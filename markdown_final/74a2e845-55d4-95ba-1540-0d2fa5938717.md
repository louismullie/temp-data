# Medical Question & Answer

**Sample ID**: 74a2e845-55d4-95ba-1540-0d2fa5938717
**Dataset Index**: 366222

---

## Question

21. Find the number of positive integers n such that
n + 2 n² + 3 n³ + ⋯ + 2005 n²⁰⁰⁵
is divisible by n − 1.

---

## Answer

> Let's see… What do we have here? The user is asking how many positive integers (n) make the sum (S(n) = n + 2n^2 + 3n^3 + \\cdots + 2005n^{2005}) divisible by (n - 1). Let's break this down step-by-step. First, I need to think about how to simplify the divisibility condition modulo (n - 1). Then, I should evaluate the resulting expression modulo (n - 1). Next, I will translate that into a congruence on (n) and solve it. Finally, I will count the positive integer solutions and verify edge cases like (n = 1) to ensure correctness.

> Let me first confirm the core idea: if (n - 1) divides (S(n)), then (S(n) ≡ 0 (mod n − 1)). Since (n ≡ 1 (mod n − 1)), each term (kn^k ≡ k × 1^k = k (mod n − 1)), so the entire sum modulo (n − 1) becomes the sum of the first 2005 positive integers. Wait, let me verify that substitution carefully: yes, (n \\equiv 1 \\pmod{n - 1}) implies (n^k \\equiv 1 \\pmod{n - 1}), so (kn^k \\equiv k \\pmod{n - 1}), and thus (S(n) ≡ 1 + 2 + ⋯ + 2005 (mod n − 1)).

> I will now examine the sum (1 + 2 + ⋯ + 2005). Using the formula for the sum of the first (m) positive integers, this equals ([2005 × 2006/2] = 2005 × 1003). Hold on, I should double-check the arithmetic: (2005 \\times 2006 = 4,022,030), and dividing by 2 gives 2,011,015, which is indeed 2005 times 1003. So the condition simplifies to (2005 × 1003 ≡ 0 (mod n − 1)), meaning (n - 1) must divide 2,011,015.

> Next, I should review the prime factorization of 2,011,015 to count the divisors. Let me factor step-by-step: 2,011,015 ends in 5, so it is divisible by 5, giving 402,203. Then 402,203 is divisible by 17, yielding 23,659. Next, 23,659 is divisible by 19, giving 1,245.21… Wait, let me verify that division: 19 times 1,245 is 23,655, so 23,659 minus 23,655 is 4, which means 23,659 is not divisible by 19. I should correct this: 23,659 divided by 19 is approximately 1,245.21, so 19 is not a factor. Let me reconsider the next prime; 23,659 divided by 23 is 1,029.52…, not an integer. Trying 29 gives 815.83…, still not integer. Trying 31 gives 763.19…, no. Trying 37 gives 639.43…, no. Trying 41 gives 576.07…, no. Trying 43 gives 549.98…, close but not 550. Trying 47 gives 503.38…, no. Trying 53 gives 446.39…, no. Trying 59 gives 401.00…, wait, 59 times 401 is 23,659, so yes, 59 and 401 are factors. I should confirm 401 is prime: it has no divisors other than 1 and itself, so the prime factorization is (5 × 17 × 59 × 401).

> Now, to count the number of positive divisors, I add 1 to each exponent in the prime factorization and multiply: ((1 + 1)(1 + 1)(1 + 1)(1 + 1) = 2⁴ = 16). Therefore, there are 16 positive divisors of 2,011,015, which means there are 16 possible values of (n - 1) such that (n - 1 \\mid 2,011,015). Hold on, I should verify that I'm not missing any constraints; since (n) must be a positive integer, (n - 1) must be a non-negative integer, and because 2,011,015 is positive, each positive divisor corresponds to a valid (n − 1 ≥ 1), giving (n ≥ 2).

> But wait, what if (n = 1)? I need to check that edge case separately because the original sum is defined for positive integers (n), and division by (n - 1 = 0) is undefined. If (n = 1), the sum becomes (1 + 2 + \\cdots + 2005 = 2,011,015), and we are asking whether this is divisible by 0, which is not meaningful. So (n = 1) is excluded from consideration, and the valid solutions are precisely those with (n - 1) a positive divisor of 2,011,015, i.e., 16 values.

> Final answer: There are 16 positive integers (n) such that (n + 2n² + ⋯ + 2005n²⁰⁰⁵) is divisible by (n - 1).

---

The number of positive integers (n) such that (n + 2n² + 3n³ + ⋯ + 2005n²⁰⁰⁵) is divisible by (n − 1) is **16**. This is because the sum modulo (n − 1) equals (1 + 2 + ⋯ + 2005 = [2005 × 2006/2] = 2005 × 1003), so (n − 1) must divide (2005 × 1003). Since (2005 × 1003 = 5 × 17 × 59 × 401), there are (2⁴ = 16) divisors, each giving a valid (n).

---

## Step-by-step solution

### Step 1: Simplify the divisibility condition

We need (n + 2n² + 3n³ + ⋯ + 2005n²⁰⁰⁵ ≡ 0 (mod n − 1)). Since (n ≡ 1 (mod n − 1)), each term (kn^k ≡ k (mod n − 1)), so the sum modulo (n − 1) is:

1 + 2 + ⋯ + 2005 = [2005 × 2006/2] = 2005 × 1003

Thus, the condition reduces to (2005 × 1003 ≡ 0 (mod n − 1)), i.e. (n − 1 | 2005 × 1003).

---

### Step 2: Factorize 2005 · 1003

Factorizing gives:

2005 = 5 × 401

1003 = 17 × 59

Therefore:

2005 × 1003 = 5 × 17 × 59 × 401

---

### Step 3: Count the divisors

The number of positive divisors of (5 × 17 × 59 × 401) is ((1 + 1)⁴ = 16), since all prime factors appear with exponent 1.

---

### Step 4: Map divisors to valid n

Each divisor (d) of (2005 × 1003) corresponds to a unique (n = d + 1), and since (d ≥ 1), each such (n) is a positive integer satisfying the divisibility condition.

---

## Final answer

There are **16** positive integers (n) for which (n + 2n² + ⋯ + 2005n²⁰⁰⁵) is divisible by (n − 1).

---

## References

### A neural theory for counting memories [^b797eb11]. Nature Communications (2022). High credibility.

Theorem 2'

There is an absolute constant c for which the following holds. Suppose the neural count sketch sees n observations satisfying Assumption 1' with. Pick any 0 < δ < 1.
Suppose that m ≥ 2 k n and thatfor a positive integer f. Then with probability at least 1 − δ, when presented with a query x (i) with 0 ≤ f i ≤ f, the response of the neural count sketch will lie in the range f i ± 1.
Suppose that m ≥ 2 k / ϵ for some ϵ > 0 and that. Then with probability at least 1 − δ, when presented with a query x (i), the response of the neural count sketch will lie in the range f i ± ϵ n.

Note that the query x (i) need not belong to the original sequence of n observations, in which case f i = 0.

Theorem 5 gives bounds that are significantly more favorable for the 1-2-3-many sketch.

Theorem 5'

Suppose the 1-2-3-many sketch, with parameter β = 1, witnesses n observations that satisfy Assumption 1' with. Pick any 0 < δ < 1 and suppose that m ≥ 2 k N and. Then with probability at least 1 − δ, when presented with a query x (i), the response of the sketch will be e − r for some value r that is either f i or f i + 1 when rounded to the nearest integer.

Overall, these mathematical proofs provide bounds on how accurately stimuli can be tracked using the two neural count sketches.

The Drosophila mushroom body implements the anti-Hebbian count sketch

Here, we provide evidence supporting the "1-2-3-many" model from the olfactory system of the fruit fly, where circuit anatomy and physiology have been well-mapped at synaptic resolution. The evidence described below includes the neural architecture of stimulus encoding, the plasticity induced at the encoding-decoding synapse, and the response precision of the decoding (counting) neuron. The latter two we derive from a re-analysis of data detailing novelty detection mechanisms in the fruit fly mushroom body, where odor memories are stored.

---

### Mammalian skull dimensions and the golden ratio (φ) [^3a12f466]. The Journal of Craniofacial Surgery (2019). Medium credibility.

The Golden Ratio characterizes a unique relationship in the study of integers, a branch of mathematics called Number Theory. Numerically, Φ can be calculated as a ratio within the Fibonacci series as the series approaches infinity. The Fibonacci series is a well-known, infinite series in which the next term in the series is the sum of the prior 2 terms. The Fibonacci series starts as 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55… Initially, the ratios of consecutive terms in the Fibonacci series (F n / F n-1) fluctuate between 1 and 2, but rapidly converge on the value Φ as the series grows: 1/1 = 1, 2/1 = 2, 3/2 = 1.5, 5/3 = 1.667, 8/5 = 1.6, 13/8 = 1.625, 21/34 = 1.615, 34/21 = 1.619, 55/34 = 1.618,…

The division of the 40th Fibonacci term (102,334,155) by the 39th (63,245,986) matches Φ to 15 decimal places. For convenience, authors and mathematicians typically truncate Φ to 1.618. Interestingly, the reciprocal of Φ (1/Φ or 1/1.61803…) is 1 integer less than Φ (0.61803…) and has the same decimal extension as Φ, and the square of Φ (Φ 2 or 1.61803… 2) is 1 integer more than Φ (2.61803…) and also has the same decimal extension. Φ is the only number that has these 2 properties. By convention, the reciprocal of Φ is represented by the Greek lower-case letter phi (ϕ), although sometimes this convention is reversed.

---

### Mending cracks atom-by-atom in rutile tiOwith electron beam radiolysis [^9da21e37]. Nature Communications (2023). High credibility.

A closer look at the initial restructuring stages reveals interesting details about bridging locations. At an electron dose of 1.5×10 6 e Å −2, bridging from two opposite sides can be observed at two different locations (Fig. 2). The first bridging location is at the distance d 1 = 15 nm from the tip of the crack and the second one is at the same distance from the first: d 2 = 2d 1 = 30 nm (Fig. 2b and Supplementary Fig. 3). The gaps at these two locations are estimated to be 6.5 Å and 13.1 Å, respectively. These two gap widths are equal to an integral multiple of the unit-cell-length of rutile TiO 2 in this projection satisfying the relationship: where n = 1, 2, 3,… is positive integer number, α = 2.5° is crack wedge angle, and a [110] = 6.48 Å is the unit cell width of rutile TiO 2 indirections (Fig. 2c). This observation suggests that at these locations, where integer number of rutile TiO 2 unit cell lengths can fit, are preferable sites for initial bridging with minimal strain or defects. In between these bridging sites, some crystalline formations can also be seen, particularly at distances of, where(Fig. 2b). But they are not well ordered as the widths of the gaps at these locations are fractional multiples of the unit-cell-length and thus, a perfect rutile structure cannot be satisfactorily accomplished. Additionally, it is observed that the main bridging sites, at distances d n, are surrounded with widened cracks from both sides (Fig. 2b), showing that material migrates from neighboring free surfaces into the bridging sites. Simplified 3D and 2D crack models are shown in Supplementary Fig. 4.

---

### Subsampling scaling [^8e2e9a81]. Nature Communications (2017). Medium credibility.

As each event is observed independently, the probability of X sub = s is the sum over probabilities of observing clusters of X = s + k events, where k denotes the missed events and s the sampled ones (binomial sampling):

This equation holds for any discrete P (s) defined on, the set of non-negative integers. To infer P (s) from P sub (s), we develop in the following a novel 'subsampling scaling' that allows to parcel out the changes in P (s) originating from spatial subsampling. A correct scaling ansatz collapses the P sub (s) for any sampling probability p.

In the following, we focus on subsampling from two specific families of distributions that are of particular importance in the context of neuroscience, namely exponential distributions P (s) = C λ e − λs with λ > 0, and power laws P (s) = C γ s − γ with γ > 1. These two families are known to show different behaviours under subsampling:
For exponential distributions, P (s) and P sub (s) belong to the same class of distributions, only their parameters change under subsampling. Notably, this result generalizes to positive and negative binomial distributions, which include Poisson distributions.
Power-laws or scale-free distributions, despite their name, are not invariant under subsampling. Namely, if P (s) follows a power-law distribution, then P sub (s) is not a power law but only approaching it in the limit of large cluster size (s →∞).

---

### Allelic decomposition and exact genotyping of highly polymorphic and structurally variant genes [^85761b97]. Nature Communications (2018). Medium credibility.

In order to estimate the copy number of any region r spanning positions a, a + 1,…, b of a gene or pseudogene s, we first calculate the normalized copy number cn s of s, which intuitively reflects the number of copies of s at position i (when the intron/exon of the gene includes ambiguously mappable positions, those positions are ignored). Details about calculating this function are provided in Supplementary Note 1 and Supplementary Figures 1 and 2. The estimated copy number (or observed coverage) of a region r of s is denoted as cn [r], and is simply calculated as:

Now we formulate Aldy's goal in this step as an instance of ILP, i.e. integer linear programming (see Trickfor an introduction to ILP) where the goal is to find an integer linear combinationof configuration vectors from the set V such that the sumwhereis minimized. Here, non-negative integer variables z i denote the number of times (the copy number of) a configuration described by vector v i from the database, appears in the solution (we recall that k denotes the total number of possible configurations). We call this problem the Copy Number Estimation Problem (CNEP).

After finding all optimal solutions of CNEP, Aldy only reports "the most parsimonious" solution(s), i.e. those for whichis the minimum possible. In the case when multiple optimal solutions minimize the term, all will be reported in the final output of CNEP.

We illustrate the above notion of parsimony through a simple example with n = 2. Let the aggregate copy number vector be cn = [1111], and the set of potential vectors be V = { v 1, v 2, v 3, v 4 } where v 1 = [1100], v 2 = [0011], v 3 = [1000], and v 4 = [0100]. Consider the following optimal solutions to this instance of CNEP: (z 1, z 2, z 3, z 4) = (1, 1, 0, 0) and (z 1, z 2, z 3, z 4) = (0, 1, 1, 1). The first solution is more parsimonious compared to the second one since the first solution implies the presence of the whole gene and the whole pseudogene — whereas the second solution implies the presence of two structurally altered copies of the gene itself, in addition to the whole pseudogene.

---

### Transient power-law behaviour following induction distinguishes between competing models of stochastic gene expression [^b98a2450]. Nature Communications (2025). High credibility.

In Fig. 4 B–K we show histograms of the exponent (computed over all rate parameter sets) for N -state models with N = 2 − 5 and j = 1. N − 1. The distributions of exponent values are wide but the maximum value of the exponents is always less than the theoretical exponent N − j + 1 (shown by a vertical red dashed line). This is because the next-to-leading order term in the series expansion of the mean in powers of time has a negative sign (see 'Methods') and hence the leading-order term proportional to t N − j +1 always overestimates the true mean mRNA count. Hence we have shown using biologically realistic rate parameter constraints that independent of N, j and the time interval over which the data is collected, the slope calculated from log-log plots of the mean versus time is a reliable means to estimate the lower bound of the number of gene states N.

---

### Measurement of respiratory rate using wearable devices and applications to COVID-19 detection [^57a25397]. NPJ Digital Medicine (2021). Medium credibility.

In Fig. 4 b, we computed the probability of obtaining N ≥ N * measurements satisfying Z ≥ 2.326. Let us estimate the noise floor by averaging the probability in the 14 day period D −28 ≤ d < D −14. For N ✱ = 1,3, and 5, we find noise floor values equal to 13.4%, 0.88%, and 0.092%, while the peak values are respectively, 59.3%, 23.9%, and 11.1%, yielding peak-to-noise ratios of 4.42, 27.1, and 120.4 respectively. Setting the noise floor as the false positive rate, and assuming a disease prevalence of 1 per 1000 individuals per day, we obtain positive predictive values for N ✱ = 1,3,5 to be 0.440%, 2.641%, and 10.76% respectively. For symptomatic individuals presenting with a fever (Fig. 4 (c)), the P (N ≥ 1) plot peaks at 71.5%, while for symptomatic individuals who do not present with a fever, the plot peaks at 47.3%. For asymptomatic individuals (Fig. 4 (d)), the plot for N ✱ = 1 peaks at 33.3%. This is smaller than for symptomatic individuals (59.3%) and for individuals who present with a fever (71.5%).

---

### Implementation of generalized quantum measurements for unambiguous discrimination of multiple non-orthogonal coherent States [^a0b0f0e9]. Nature Communications (2013). Medium credibility.

Generalized quantum measurements implemented to allow for measurement outcomes termed inconclusive can perform perfect discrimination of non-orthogonal states, a task which is impossible using only measurements with definitive outcomes. Here we demonstrate such generalized quantum measurements for unambiguous discrimination of four non-orthogonal coherent states and obtain their quantum mechanical description, the positive-operator valued measure. For practical realizations of this positive-operator valued measure, where noise and realistic imperfections prevent perfect unambiguous discrimination, we show that our experimental implementation outperforms any ideal standard-quantum-limited measurement performing the same non-ideal unambiguous state discrimination task for coherent states with low mean photon numbers.

---

### Transient power-law behaviour following induction distinguishes between competing models of stochastic gene expression [^7aaf2587]. Nature Communications (2025). High credibility.

In this paper, we provide answers to these two questions. We consider the model (1) with one active state and a general number of inactive states, i.e. N ≥ 2. We first conclusively show that steady-state mRNA count data is not typically sufficient to distinguish between the N = 2 modeland models with N = 3, 4 and 5 gene states. In particular, we identify a mapping of rate parameters between the telegraph model and the other types of models that lead to distributions of mRNA counts that in the vast majority of cases are practically model-independent. Next, we devise a simple method to overcome the difficulties of the inference method based on steady-state data and other methods in the literature that utilise the time-dependent distribution of mRNA counts. We analytically show that following induction, the initial increase of the mean mRNA count with time is described by a power law whose exponent is solely dependent on the number of gene states between the initial inactive state and the active state. We confirm the accuracy of the theoretical result using simulations and then apply it to data collected for various cell types to provide a robust estimate of a lower bound for the total number of inactive gene states. Interestingly, our results show that this bound: is not affected by static extrinsic noise due to cell-to-cell variability; is valid for a large class of networks of gene states (of which (1) is a special case); and in principle is sufficient to accurately infer the number of gene states using bulk data, rather than single-cell data.

---

### Using GNN property predictors as molecule generators [^792b755c]. Nature Communications (2025). High credibility.

Fig. 1
Key concepts of the generative framework.

a Molecular representation for this work using an HCN molecule as an example. b A visual representation of a typical training process for a neural network in comparison to an input optimization scheme.

The adjacency matrix is constructed from a weight vector w adj containingelements. These elements are squared and populated in an upper triangular matrix with zeros on the main diagonal. The resulting matrix is then added to its transpose to obtain a positive symmetric matrix with zero trace. Elements of the matrix are then rounded to the nearest integer to obtain the adjacency matrix. The key element here is that the adjacency matrix needs to have non-zero gradients with respect to w adj, which is not the case when using a conventional rounding half-up function. To alleviate this problem we used a sloped rounding function, where [x] is the conventional rounding half-up function and a is an adjustable hyper-parameter. These steps guarantee that only valid near-integer filled adjacency matrices are constructed. However, it does not take into account any chemistry: atoms are allowed to form as many bonds as there are rows in the adjacency matrix. To avoid that, we use two strategies: (1) we penalize valence (sum of bond orders) of more than 4 through the loss function and (2) we do not allow gradients in the direction of higher number of bonds when the valence is already 4.

---

### A space-time tradeoff for implementing a function with master equation dynamics [^57fa10b0]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### Transient power-law behaviour following induction distinguishes between competing models of stochastic gene expression [^8721f3aa]. Nature Communications (2025). High credibility.

Discussion

In this paper we have shown that (i) in most cases, steady-state mRNA count data is not sufficient to estimate the number of inactive states in transcription; (ii) when the steady-state mRNA distribution of the telegraph model distribution well fits the distribution of the N -state model (N > 2), there exists a simple relationship between the rate parameters of the two models; (iii) for a short time after induction, the increase in the mean mRNA counts follows a universal power law whose exponent provides a lower bound on the number of gene states — this is valid for a large family of models of gene expression, and it is independent of the values of the rate parameters of the gene expression model and the size of static extrinsic noise. We confirm this power law by analysing published yeast and mammalian data. In agreement with estimation using other methods, our results suggest that the number of inactive eukaryotic gene states can be larger than one.

---

### Input-output maps are strongly biased towards simple outputs [^659947d0]. Nature Communications (2018). Medium credibility.

Random matrix map with bias but not simplicity bias

Finally, we provide an example of a map that exhibits strong bias that is not simplicity bias. We define the matrix map by having binary input vectors p of length n that map to binary output vectors x of same length though x i = Θ ((M ⋅ p) i), where M is a matrix and the Heaviside thresholding function Θ (y) = 0 if y < 0 and Θ (y) = 1 if y ≥ 0. This last nonlinear step, which resembles the thresholding function in simple neural networks, is important since linear maps do not show simplicity bias. In Fig. 1e, we illustrate this map for a matrix made with entries randomly chosen to be −1 or +1. The rank plot (Supplementary Fig. 17) shows strong bias, but in marked contrast to the other maps, the probability P (x) does not correlate with the complexity of the outputs. Simplicity bias does not occur because the n × n independent matrix elements mean that the mapping's complexity grows rapidly with increasing n so that the map violates our limited complexity condition (1). Intuitively: the pattern of 1s and 0s in output x is strongly determined by the particular details of the map M, and so does not correlate with the complexity of the output.

To explore in more detail how simplicity bias develops or disappears with limited complexity condition (1), we also constructed a circulant matrix where we can systematically vary the complexity of the map. It starts with a row of p positive 1 s and q − 1 s, randomly placed. The next row has the same sequence, but permuted by one. This permutation process is repeated to create a square n × n matrix. Thus the map is completely specified by defining the first row together with the procedure to fill out the rest of the matrix. In Fig. 2, we plot the ratio of the mean complexity sampled over outputs divided by the mean complexity sampled over all inputs, as a function of the complexity of the first row that that defines the matrix. If the ratiois significantly larger than one, then the map shows simplicity bias. Simplicity bias only occurs whenis very small, i.e. for relatively simple maps that respect condition (1). The output of one such simple matrix maps is shown in Fig. 1f. In Supplementary Note 12, we investigate these trends in more detail as a function of matrix type, size n, and also investigate the role of matrix rank and sparsity.

---

### Correlations as a resource in quantum thermodynamics [^075da07c]. Nature Communications (2019). High credibility.

The presence of correlations in physical systems can be a valuable resource for many quantum information tasks. They are also relevant in thermodynamic transformations, and their creation is usually associated to some energetic cost. In this work, we study the role of correlations in the thermodynamic process of state formation in the single-shot regime, and find that correlations can also be viewed as a resource. First, we show that the energetic cost of creating multiple copies of a given state can be reduced by allowing correlations in the final state. We obtain the minimum cost for every finite number of subsystems, and then we show that this feature is not restricted to the case of copies. More generally, we demonstrate that in the asymptotic limit, by allowing a logarithmic amount of correlations, we can recover standard results where the free energy quantifies this minimum cost.

---

### Dimensionless learning based on information [^63610f0f]. Nature Communications (2025). High credibility.

We have shown that IT- π offers a complete set of dimensionless learning tools, including ranking inputs by predictability, identifying distinct physical regimes, uncovering self-similar variables, and extracting characteristic scales and dimensionless parameters. IT- π is also sensitive to the norm used to quantify errors and the optimal set of dimensionless variables may vary depending on the error metric of interest (e.g. prediction of ordinary versus rare events). Although some of these features are available through other methods, none encompass them all. Even in cases where alternative methods apply, IT- π distinguishes itself by being grounded in a theorem rather than relying on heuristic reasoning. This makes IT- π independent of specific modeling assumptions.

In additional to its model-free nature, IT- π offers unique capabilities that other methods do not, such as establishing bounds on the irreducible error and evaluating model efficiency. The former allows us to precisely determine the actual number of relevant dimensionless variables, l *, which is typically overestimated by the Buckingham- π theorem. Moreover, IT- π quantifies the degree of dynamic similarity achievable with the optimal variables, rather than providing merely a binary yes-or-no answer as classical dimensional analysis does. This feature can be decisive in designing laboratory experiments for extrapolation to real-world applications. For example, consider predicting the heat flux over a rough surface as discussed in the application above. According to Buckingham- π, seven dimensionless variables would be required. If three different values must be measured to capture the scaling behavior of each variable, that would entail ~3 7 = 2187 experiments. In contrast, IT- π determined that only two dimensionless variables are necessary to achieve a dynamic similarity of 92% (i.e. an 8% normalized irreducible error). This entails a significantly reduced effort of only 3 2 = 9 experiments. The same reasoning applies to the construction of predictive modeling: models with fewer inputs require orders of magnitude less training data compared to those with high-dimensional inputs. In the previous example, this factor would be of the order of 1000.

---

### Platanus-allee is a de novo haplotype assembler enabling a comprehensive access to divergent heterozygous regions [^c7f85fb9]. Nature Communications (2019). High credibility.

(1) Anchor bubble information (see "Methods" section, Detection of anchor bubbles for haplotype synteny-based correction) to find the homologous node are assigned to the contigs. Let b 1, b 2,…, b n be the numerical sequences. If the i- th contig (i ∈ {1, 2,…, n }) is an anchor bubble, b i is the scaffold-ID of the node to which the counterpart of i th contig belongs; otherwise b i = 0.

(2) B max, the scaffold-ID of the homologous counterpart of most anchor bubbles, is determined. Let l 1, l 2,…, l n be the lengths of the n contigs. L sum (x) is the sum of all l i such that b i = x, and

Note that x is a scaffold-ID and non-zero integer.

(3) d min and d max are the minimum and maximum i such that b i = B max, respectively, and the candidate position to be divided for correction. Note that it is assumed that the region from the d min -th to d max -th contig retains synteny to the homologous counterpart.

(4) Whether the region from the d min -th to d max -th contig contains many anchor bubbles whose counter-part-scaffold-IDs are not B max. If so, d min and/or d max are corrected. For d min, the division score is calculated as follows:

If positions at which S (x) > 0 exist, the position at which S (x) is the maximum for x is the new d min. This correction procedure is applied for d max in a similar manner.

---

### Developing a clinical prediction score: comparing prediction accuracy of integer scores to statistical regression models [^91dd14ad]. Anesthesia and Analgesia (2021). Medium credibility.

Researchers often convert prediction tools built on statistical regression models into integer scores and risk classification systems in the name of simplicity. However, this workflow discards useful information and reduces prediction accuracy. We, therefore, investigated the impact on prediction accuracy when researchers simplify a regression model into an integer score using a simulation study and an example clinical data set. Simulated independent training and test sets (n = 1000) were randomly generated such that a logistic regression model would perform at a specified target area under the receiver operating characteristic curve (AUC) of 0.7, 0.8, or 0.9. After fitting a logistic regression with continuous covariates to each data set, continuous variables were dichotomized using data-dependent cut points. A logistic regression was refit, and the coefficients were scaled and rounded to create an integer score. A risk classification system was built by stratifying integer scores into low-, intermediate-, and high-risk tertiles. Discrimination and calibration were assessed by calculating the AUC and index of prediction accuracy (IPA) for each model. The optimism in performance between the training set and test set was calculated for both AUC and IPA. The logistic regression model using the continuous form of covariates outperformed all other models. In the simulation study, converting the logistic regression model to an integer score and subsequent risk classification system incurred an average decrease of 0.057–0.094 in AUC, and an absolute 6.2%-17.5% in IPA. The largest decrease in both AUC and IPA occurred in the dichotomization step. The dichotomization and risk stratification steps also increased the optimism of the resulting models, such that they appeared to be able to predict better than they actually would on new data. In the clinical data set, converting the logistic regression with continuous covariates to an integer score incurred a decrease in externally validated AUC of 0.06 and a decrease in externally validated IPA of 13%. Converting a regression model to an integer score decreases model performance considerably. Therefore, we recommend developing a regression model that incorporates all available information to make the most accurate predictions possible, and using the unaltered regression model when making predictions for individual patients. In all cases, researchers should be mindful that they correctly validate the specific model that is intended for clinical use.

---

### A useful and sustainable role for N-of-1 trials in the healthcare ecosystem [^ac786434]. Clinical Pharmacology and Therapeutics (2022). Medium credibility.

Clinicians: As allies of their patients, and seeking evidence for the treatments they prescribe, clinicians generally are attracted to the idea of N‐of‐1 trials, if they can be practically and efficiently executed without disrupting the process and economics of patient care. Clinicians also may welcome the opportunity for structured shared decision making and the engagement of the broader care team. However, the logistics and economics of the approach must be attractive for adoption in routine clinical practice. This will require easy‐to‐use processes and platforms to implement N‐of‐1 studies, and coverage of expenses by healthcare payers and/or sponsors.
Clinical researchers: Clinical epidemiologists and researchers long have been attracted to type 1 N‐of‐1 trials in chronic conditions. Besides supporting rigorous trial processes to reveal what treatments work best, when combined, sample size requirements are reduced because participants serve as their own controls. More recently, researchers have used type 2 N‐of‐1 trials to assess cutting‐edge, highly personalized treatments. As a potential addition to the drug development and evaluation toolkit, the use of N‐of‐1 trials highlights the need for standard processes and platforms, such as those used for phases I, II, and III trials, including to meet regulatory requirements to bring a new treatment to the public. This would greatly facilitate the use of these studies by academic and industry researchers. For this structuring of clinical research practices to happen, there will need to be support by the stakeholders who derive economic value from these trials, such as pharmaceutical and biotech companies and contract research organizations.
Medical product developers: For prevalent and rapidly progressing conditions, and for sponsors seeking regulatory approval for a new drug, conventional phase III randomized parallel group controlled trials are likely to remain more attractive than type 1 N‐of‐1 trials. Although aggregated N‐of‐1 trials need fewer participants than traditional parallel group trials, the perception is that N‐of‐1 trials are more difficult to stage and require more time to execute. In drug development, this could mean more expensive studies and greater opportunity costs from lost time of limited market protection. In addition, for the manufacturer, there may be more incentive for broad sales of a drug based on the average effect seen in a parallel arm trial rather than in a trial that emphasizes the heterogeneity of benefit among different patients. Thus, at this point, N‐of‐1 trials do not appear likely to supplant extant clinical trial practices. However, there are some specific circumstances where they could potentially be part of drug development. In early drug development for a new molecule being studied for the first time in patients, such as in phase Ib trials, including dose‐escalation studies in planning for phase II trials, N‐of‐1 trials potentially could be useful. N‐of‐1 trials may also be attractive as part of an overall development program for treatments of chronic conditions when a key objective is to investigate heterogeneity of treatment and/or adverse effects (especially short‐term). Also, N‐of‐1 trials could be considered for a run‐in period preceding a traditional parallel group randomized controlled trial in order to identify treatment responders.

---

### A blueprint for precise and fault-tolerant analog neural networks [^195aad13]. Nature Communications (2024). High credibility.

Analog computing has reemerged as a promising avenue for accelerating deep neural networks (DNNs) to overcome the scalability challenges posed by traditional digital architectures. However, achieving high precision using analog technologies is challenging, as high-precision data converters are costly and impractical. In this work, we address this challenge by using the residue number system (RNS) and composing high-precision operations from multiple low-precision operations, thereby eliminating the need for high-precision data converters and information loss. Our study demonstrates that the RNS-based approach can achieve ≥ 99% FP32 accuracy with 6-bit integer arithmetic for DNN inference and 7-bit for DNN training. The reduced precision requirements imply that using RNS can achieve several orders of magnitude higher energy efficiency while maintaining the same throughput compared to conventional analog hardware with the same precision. We also present a fault-tolerant dataflow using redundant RNS to protect the computation against noise and errors inherent within analog hardware.

---

### No extension of quantum theory can have improved predictive power [^ccb6494a]. Nature Communications (2011). Medium credibility.

According to quantum theory, measurements generate random outcomes, in stark contrast with classical mechanics. This raises the question of whether there could exist an extension of the theory that removes this indeterminism, as suspected by Einstein, Podolsky and Rosen. Although this has been shown to be impossible, existing results do not imply that the current theory is maximally informative. Here we ask the more general question of whether any improved predictions can be achieved by any extension of quantum theory. Under the assumption that measurements can be chosen freely, we answer this question in the negative: no extension of quantum theory can give more information about the outcomes of future measurements than quantum theory itself. Our result has significance for the foundations of quantum mechanics, as well as applications to tasks that exploit the inherent randomness in quantum theory, such as quantum cryptography.

---

### MRNAs, proteins and the emerging principles of gene expression control [^2d1f7bae]. Nature Reviews: Genetics (2020). High credibility.

Gene expression involves transcription, translation and the turnover of mRNAs and proteins. The degree to which protein abundances scale with mRNA levels and the implications in cases where this dependency breaks down remain an intensely debated topic. Here we review recent mRNA-protein correlation studies in the light of the quantitative parameters of the gene expression pathway, contextual confounders and buffering mechanisms. Although protein and mRNA levels typically show reasonable correlation, we describe how transcriptomics and proteomics provide useful non-redundant readouts. Integrating both types of data can reveal exciting biology and is an essential step in refining our understanding of the principles of gene expression control.

---

### Learning meaningful representations of protein sequences [^6646110e]. Nature Communications (2022). High credibility.

How we choose to represent our data has a fundamental impact on our ability to subsequently extract information from them. Machine learning promises to automatically determine efficient representations from large unstructured datasets, such as those arising in biology. However, empirical evidence suggests that seemingly minor changes to these machine learning models yield drastically different data representations that result in different biological interpretations of data. This begs the question of what even constitutes the most meaningful representation. Here, we approach this question for representations of protein sequences, which have received considerable attention in the recent literature. We explore two key contexts in which representations naturally arise: transfer learning and interpretable learning. In the first context, we demonstrate that several contemporary practices yield suboptimal performance, and in the latter we demonstrate that taking representation geometry into account significantly improves interpretability and lets the models reveal biological information that is otherwise obscured.

---

### Learning properties of quantum States without the IID assumption [^d59de50a]. Nature Communications (2024). High credibility.

We develop a framework for learning properties of quantum states beyond the assumption of independent and identically distributed (i.i.d.) input states. We prove that, given any learning problem (under reasonable assumptions), an algorithm designed for i.i.d. input states can be adapted to handle input states of any nature, albeit at the expense of a polynomial increase in training data size (aka sample complexity). Importantly, this polynomial increase in sample complexity can be substantially improved to polylogarithmic if the learning algorithm in question only requires non-adaptive, single-copy measurements. Among other applications, this allows us to generalize the classical shadow framework to the non-i.i.d. setting while only incurring a comparatively small loss in sample efficiency. We leverage permutation invariance and randomized single-copy measurements to derive a new quantum de Finetti theorem that mainly addresses measurement outcome statistics and, in turn, scales much more favorably in Hilbert space dimension.

---

### Machine learning in spectral domain [^b6a6f35e]. Nature Communications (2021). High credibility.

In Fig. 6, we report the results of the tests performed when operating under the deep linear configuration. Symbols are analogous to those employed in Fig. 5. In all inspected cases, the entry layer is made of N 1 = 784 elements and the output one has N ℓ = 10 nodes. The first five points, from left to right, refer to a three layers (linear) neural network. Hence, ℓ = 3 and the size of the intermediate layer is progressively increased, N 2 = 20, 80, 100, 500, 800. The total number of trained eigenvalues is N 2 + N 3, and gets therefore larger as the size of the intermediate layer grows. The successive four points of the collections are obtained by setting ℓ = 4. Here, N 2 = 800 while N 3 is varied (= 100, 200, 400, 600). The training impacts on N 2 + N 3 + N 4 parameters. Finally, the last point in each displayed curve is obtained by working with a five layers deep neural network, ℓ = 5. In particular, N 2 = 800, N 3 = 600 and N 4 = 500, for a total of N 2 + N 3 + N 4 + N 5 tunable parameters. Also in this case, the spectral algorithm performs better than conventional learning schemes constrained to operate with an identical number of free parameters. Similarly, the distribution of the weights of an equivalent perceptron trained in reciprocal space matches that obtained when operating in the space of the nodes and resting on a considerably larger number of training parameters. To sum up, eigenvalues are parameters of key importance for neural networks training, way more strategic than any other set of equivalent cardinality in the space of the nodes. As such, they allow for a global approach to the learning, with significant reflexes of fundamental and applied interest. In all cases here considered, the learning can extend to the eigenvectors: an optimised indentation of the eigen-directions contribute to enhance the overall performance of the trained device.

---

### What are the determinants of gene expression levels and breadths in the human genome? [^c0ffce99]. Human Molecular Genetics (2012). Low credibility.

Conclusions and future directions

Our study presents a novel statistical method to analyze effects of different genomic traits on two distinct aspects of gene expression. We identified several key genomic traits whose relative effects on levels and breadths of gene expression are quantified. This information is useful in decoding regulatory principles of transcription.

Although the current study analyzed only levels and spatial distribution of gene expression, our approach provides a flexible statistical tool to evaluate the roles of additional genomic traits on the regulation of gene expression. In particular, with the improved power for dissecting complex transcriptomes, information on temporal variation of gene expression is soon to accumulate. Moreover, data on genomic traits that may be intimately related to regulation of gene expression, such as epigenetic changes, will soon become available. Our statistical framework can be easily expanded to incorporate additional data.

---

### What are the determinants of gene expression levels and breadths in the human genome? [^2c69e210]. Human Molecular Genetics (2012). Low credibility.

Using this new approach, we identify which genomic traits are significant determinants of gene expression levels and breadths. Our statistical models, using information on genomic traits alone, can predict substantial amount of variation found in the levels and breadths of gene expression in the human and mouse genomes. Furthermore, we identify genomic traits that are more strongly associated with one aspect of gene expression than with the other. We also show that some genomic features exhibit species-specific patterns of associations with gene expression traits. Our study provides valuable insights into the molecular mechanisms underlying the regulation of gene expression. In addition, our method is highly applicable to many other questions in biology, to disentangle effects of different factors on biologically correlated traits.

---

### A general derivation and quantification of the third law of thermodynamics [^43738480]. Nature Communications (2017). Medium credibility.

The most accepted version of the third law of thermodynamics, the unattainability principle, states that any process cannot reach absolute zero temperature in a finite number of steps and within a finite time. Here, we provide a derivation of the principle that applies to arbitrary cooling processes, even those exploiting the laws of quantum mechanics or involving an infinite-dimensional reservoir. We quantify the resources needed to cool a system to any temperature, and translate these resources into the minimal time or number of steps, by considering the notion of a thermal machine that obeys similar restrictions to universal computers. We generally find that the obtainable temperature can scale as an inverse power of the cooling time. Our results also clarify the connection between two versions of the third law (the unattainability principle and the heat theorem), and place ultimate bounds on the speed at which information can be erased.

---

### Evaluating the evidence for exponential quantum advantage in ground-state quantum chemistry [^03db0a64]. Nature Communications (2023). High credibility.

Due to intense interest in the potential applications of quantum computing, it is critical to understand the basis for potential exponential quantum advantage in quantum chemistry. Here we gather the evidence for this case in the most common task in quantum chemistry, namely, ground-state energy estimation, for generic chemical problems where heuristic quantum state preparation might be assumed to be efficient. The availability of exponential quantum advantage then centers on whether features of the physical problem that enable efficient heuristic quantum state preparation also enable efficient solution by classical heuristics. Through numerical studies of quantum state preparation and empirical complexity analysis (including the error scaling) of classical heuristics, in both ab initio and model Hamiltonian settings, we conclude that evidence for such an exponential advantage across chemical space has yet to be found. While quantum computers may still prove useful for ground-state quantum chemistry through polynomial speedups, it may be prudent to assume exponential speedups are not generically available for this problem.

---

### Extreme purifying selection against point mutations in the human genome [^e49426ec]. Nature Communications (2022). High credibility.

Approximate model for ultraselection

Following Eq. (1), the log likelihood function is given by, where R = ∑ i Y i is the number of rare variants. When the P i values are small (as is typical), it is possible to obtain a reasonably good closed-form estimator for λ s by making use of the approximation. In this case, where N = ∑ i (1 − Y i) is the number of invariant sites andis the average value of P i at the invariant sites. It is easy to show that this approximate log likelihood is maximized at,

However, this procedure leads to a biased estimator for λ s. A correction for the bias leads to the following, intuitively simple, unbiased estimator:where M = N + R is the total number of sites andis the average value of P i at all sites. In other words, is given by 1 minus the observed number of rare variants divided by the expected number of rare variants under neutrality, which is simply the total number of sites multiplied by the average rate at which rare variants appear.

Full allele-specific model

In practice, we use a model that distinguishes among the alternative alleles at each site and exploits our allele-specific mutation rates. This model behaves similarly to the simpler one described above, but yields slightly more precise estimates in the presence of multi-allelic rare variants.

In the full model, we assume separate indicator variables, and, for the three possible allele-specific rare variants at each site, and corresponding allele-specific rates of occurrence, and(which, notably, sum to the quantity previously denoted P i). We further make the assumption that the different rare variants appear independently. Thus, the likelihood function generalizes to (cf. equation (1)), where we redefineandfor j ∈ {1, 2, 3}. Notice that, when more than one alternative allele is present, will be 1 for more than one value of j.

---

### Summary benchmarks-full set – 2024 [^e2c40ddc]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — GRADE recommendation categories are specified as: "Strong recommendation (SR): Used when the desirable effects of an intervention clearly outweigh the undesirable effects or clearly do not" and "Discretionary recommendation (DR): Used when the trade-offs are less certain — either because of low-quality evidence or because evidence suggests that desirable and undesirable effects are closely balanced".

---

### Clinical policy: critical issues in the initial evaluation and management of patients presenting to the emergency department in early pregnancy [^3b67dcc6]. Annals of Emergency Medicine (2012). Medium credibility.

β-hCG threshold data — study-level diagnostic metrics from the table. For a β-hCG threshold of 3,000 mIU/mL, Wang 1996 (Class II, N = 141) reported relative risk 1.3 (0.6–2.6), negative likelihood ratio (LR) 1.1 (0.8–1.5), and positive LR 0.8 (0.5–1.4), while Dart 1997 (Class III, N = 194) reported relative risk 2.1 (0.9–4.8), negative LR 1.4 (1.0–1.8), and positive LR 0.6 (0.3–1.1). For a 2,000 mIU/mL threshold, Condous 2005 (Class II, N = 527) reported relative risk 0.5 (0.2–1.1), negative LR 0.9 (0.8–1.0), and positive LR 2.3 (1.1–4.9); Mol 1998 (Class II, N = 262) reported relative risk 0.2 (0.0–1.3), negative LR 0.6 (0.5–0.8), and positive LR 2.3 (1.2–4.3).

---

### Probabilistic population codes for Bayesian decision making [^028a723c]. Neuron (2008). Low credibility.

When making a decision, one must first accumulate evidence, often over time, and then select the appropriate action. Here, we present a neural model of decision making that can perform both evidence accumulation and action selection optimally. More specifically, we show that, given a Poisson-like distribution of spike counts, biological neural networks can accumulate evidence without loss of information through linear integration of neural activity and can select the most likely action through attractor dynamics. This holds for arbitrary correlations, any tuning curves, continuous and discrete variables, and sensory evidence whose reliability varies over time. Our model predicts that the neurons in the lateral intraparietal cortex involved in evidence accumulation encode, on every trial, a probability distribution which predicts the animal's performance. We present experimental evidence consistent with this prediction and discuss other predictions applicable to more general settings.

---

### Measurement of the neutron charge radius and the role of its constituents [^1373d622]. Nature Communications (2021). High credibility.

The neutron is a cornerstone in our depiction of the visible universe. Despite the neutron zero-net electric charge, the asymmetric distribution of the positively- (up) and negatively-charged (down) quarks, a result of the complex quark-gluon dynamics, lead to a negative value for its squared charge radius, [Formula: see text]. The precise measurement of the neutron's charge radius thus emerges as an essential part of unraveling its structure. Here we report on a [Formula: see text] measurement, based on the extraction of the neutron electric form factor, [Formula: see text], at low four-momentum transfer squared (Q 2) by exploiting the long known connection between the N→Δ quadrupole transitions and the neutron electric form factor. Our result, [Formula: see text], addresses long standing unresolved discrepancies in the [Formula: see text] determination. The dynamics of the strong nuclear force can be viewed through the precise picture of the neutron's constituent distributions that result into the non-zero [Formula: see text] value.

---

### Abstract representations emerge naturally in neural networks trained to perform multiple tasks [^55073c20]. Nature Communications (2023). High credibility.

Methods

Abstraction metrics

Both of our abstraction methods quantify how well a representation that is learned in one part of the latent variable space (e.g. a particular context) generalizes to another part of the latent variable space (e.g. a different context). To make this concrete, in both metrics, we train a decoding model on representations from only one — randomly chosen — half of the latent variable space and test that decoding model on representations from the non-overlapping half of the latent variable space.

The classifier generalization metric

First, we select a random balanced division of the latent variable space. One of these halves is used for training, the other is used for testing. Then, we select a second random balanced division of the latent variable space that is orthogonal to the first division. One of these halves is labeled category 1 and the other is labeled category 2. As described above, we train a linear classifier on this categorization using 1000 training stimuli from the training half of the space, and test the classifier's performance on 2000 stimuli from the testing half of the space. Thus, chance is set to 0.05 and perfect generalization performance is 1.

The regression generalization metric

As above, except we train a linear ridge regression model to read out all D latent variables using 4000 sample stimulus representations from the training half of the space. We then test the regression model on 1000 stimulus representations sampled from the testing half of the space. We quantify the performance of the linear regression with its r 2 value:where X is the true value of the latent variables andis the prediction from the linear regression. Because the MSE is unbounded, the r 2 value can be arbitrarily negative. However, chance performance is r 2 = 0, which would be the performance of the linear regression always predicted the mean of X, and r 2 = 1 indicates a perfect match between the true and predicted value.

---

### Local kernel renormalization as a mechanism for feature learning in overparametrized convolutional neural networks [^526f1d1d]. Nature Communications (2025). High credibility.

Empirical evidence for global and local kernel renormalization

Extracting the renormalized kernel from the measure of a physical observable is not straightforward, and we have no way, at the moment, to directly access it in numerical experiments. One indirect experimental blueprint of the form of kernel renormalization at proportional width in different architectures is given by the similarity matrix of the internal representations before and after training. We define this observable for both FC and CNNs as:where N 1 denotes the number of neurons in the last layer (for CNNs we have N 1 = N c ⌊ N 0 / S ⌋). Note that the observable in Eq. (14) coincides with the empirical correlation of the post-activations when the activation function is odd.

We can track the effect of training by taking the difference between the similarity matrix at initialization, which is by definition the NNGP kernel (up to the Gaussian prior λ 1), and the same quantity after training: Δ K μ ν ≡ 〈 O μ ν 〉 − K μ ν, where the average is done over the Gibbs ensemble of the weights. We analytically compute this observable for FC and CNNs, as shown in Supplementary Methods 4. The final result in the two cases respectively reads (at zero temperature):where. Let us now highlight a few interesting physical implications that directly follow from these formulas and can be checked with numerical experiments:
(i) the difference between the trained and untrained similarity matrixconverges to zero in the thermodynamic limit P, N 1 → ∞ at fixed α 1 = P / N 1, as long as the terms in the square brackets are of order o (N 1). This means that in the proportional regime under consideration the effect of training on the internal representations is just a finite-size correction O (1/ N 1) and the trained similarity matrix stays very close to its value at initialization. In particular, we expect that the empirical distribution of the elements of the matrixwill be increasingly peaked around zero in finite-size scaling experiments where we consider increasing values of P and N 1 keeping their ratio α 1 = P / N 1 fixed;
(ii) by choosing a learning task with labels y μ = 0, 1 and input patterns x μ such that the matrix elements of the kernel K μ ν are on average centered in zero, the distribution of the matrix elements of the block(corresponding to the subset of training patterns with label one) should drift towards zero at a rate 1/ N 1, if one performs experiments at constant α 1, while increasing P and N 1. Interestingly, the remaining blocks of the matrixshould not be affected by this drift;
(iii) the difference between the trained and untrained convolutional similarity matrixdisplays a different behavior, due to local kernel renormalization. For instance, even if we restrict our analysis to the learning setting outlined in bullet (ii), the elements of the blockshould not identically converge to zero in the thermodynamic proportional limit. A straightforward scaling analysis in fact shows that the contribution due to the term in the square bracket does not vanish in the proportional limit: if P is the same order of magnitude of N 1, the double sum of P O(1) kernels on the rhs of Eq. (16) scales as P and it compensates the vanishing prefactor 1/ N 1, leading to a finite Δ K CNN ∝ O (P / N 1).

---

### Categorization of 34 computational methods to detect spatially variable genes from spatially resolved transcriptomics data [^7acb1dca]. Nature Communications (2025). High credibility.

To facilitate our discussion, we introduce the following mathematical notations. For each gene with expression levels (in counts or normalized) measured at n spatial spots, we useto represent its expression level at spot i = 1,…, n. When gene expression levels are considered random variables, we use Y i to emphasize the random nature. The vector notation is. For each spot i = 1,…, n, we denote its 2D spatial location as. The spatial coordinates of all n spots are represented by the matrix, where each row corresponds to a spot. When spots are annotated with L spatial domain labels, the spatial-domain indicator vector for spot i is, with; that is, d i l = 1 means that spot i belongs to spatial domain l. When spots are annotated with K cell type labels, the cell-type proportion vector for spot i is, where c i k indicates the proportion of cell type k at spot i, with. In summary, each spot i has three covariate vectors: spatial location s i, spatial-domain indicator d i, and cell-type proportions c i. We summarize the mathematical notations in Table 3.

Table 3
Mathematical notations and interpretations

---

### 2022 AHA / ACC key data elements and definitions for cardiovascular and noncardiovascular complications of COVID-19: a report of the American college of cardiology / American Heart Association task force on clinical data standards [^7c00ac0e]. Journal of the American College of Cardiology (2022). High credibility.

Maximum fraction of inspired oxygen (FiO2) is defined as the maximum molar fraction of oxygen in an inhaled gas, with permissible value type Numeric.

---

### Quantifying dissipation using fluctuating currents [^4456ae54]. Nature Communications (2019). High credibility.

Systems coupled to multiple thermodynamic reservoirs can exhibit nonequilibrium dynamics, breaking detailed balance to generate currents. To power these currents, the entropy of the reservoirs increases. The rate of entropy production, or dissipation, is a measure of the statistical irreversibility of the nonequilibrium process. By measuring this irreversibility in several biological systems, recent experiments have detected that particular systems are not in equilibrium. Here we discuss three strategies to replace binary classification (equilibrium versus nonequilibrium) with a quantification of the entropy production rate. To illustrate, we generate time-series data for the evolution of an analytically tractable bead-spring model. Probability currents can be inferred and utilized to indirectly quantify the entropy production rate, but this approach requires prohibitive amounts of data in high-dimensional systems. This curse of dimensionality can be partially mitigated by using the thermodynamic uncertainty relation to bound the entropy production rate using statistical fluctuations in the probability currents.

---

### Gene-expression measurement: variance-modeling considerations for robust data analysis [^4e2bc0ca]. Nature Immunology (2012). Medium credibility.

System-wide measurements of gene expression by DNA microarray and, more recently, RNA-sequencing strategies have become de facto tools of modern biology and have led to deep understanding of biological mechanisms and pathways. However, analyses of the measurements have often ignored statistically robust methods that account for variance, resulting in misleading biological interpretations.

---

### Unsupervised vector-based classification of single-molecule charge transport data [^6a217f37]. Nature Communications (2016). Medium credibility.

Results

Vector-based data analysis

Vector-based classification methods are powerful tools for categorizing the data and have found widespread application in such fields as genetics, robotics and neuroscience. Generally, they operate by regarding a data set, for example, an I(s) curve with N current values in the present case, as an N -dimensional vector X n (n = 1. N); a total number of M observations thus results in a data matrix X n, m (m = 1. M) or, in short X m (dropping n for convenience). The Euclidean distance | Δ X| between the two vector points may then be used as a measure for similarity between different data sets m and m ′, equation (1):

where K is an optional normalization constant (if required, so that 0 ≤ Δ X ≤ 1). If X m and X m ′ are identical, then Δ X m, m ′ is zero; if they are very different, Δ X m, m ′ is large. After calculating all combinations of distances between the M data sets, a distance or probability criterion may then be used to classify the data. The computational effort scales with (M −1)· M, according to the variation formula, and can be very significant for some of our larger data sets (for example, involving 70,000 traces, see below).

In the case of I (s) data, however, we found the mutual distance criterion to be insufficient in many cases. A different, somewhat expanded methodology was required, which as we show below, significantly improved the classification performance.

In a first step, we defined an arbitrary, N -component reference vector R, which generally depends on the data to be analysed. It could be determined self-consistently, based on some optimization parameter (for example, to maximize the variance of the existing data around R). We chose a vector with noise-free, exponentially decaying current–distance values, which is similar to the experimental data without molecular binding events (I 0 = 20 nA; decay coefficient β = 1 Å −1), Fig. 1c (dashed, magenta line).

---

### Summary benchmarks-full set – 2024 [^6f68d3ad]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — use and ethics note states that "The PPPs are intended to serve as guides in patient care, with greatest emphasis on technical aspects", that "true medical excellence is achieved only when skills are applied in a such a manner that the patients' needs are the foremost consideration", and that "The Academy is available to assist members in resolving ethical dilemmas that arise in the course of practice".

---

### Exploiting redundancy in large materials datasets for efficient machine learning with less data [^ba2f4c45]. Nature Communications (2023). High credibility.

Extensive efforts to gather materials data have largely overlooked potential data redundancy. In this study, we present evidence of a significant degree of redundancy across multiple large datasets for various material properties, by revealing that up to 95% of data can be safely removed from machine learning training with little impact on in-distribution prediction performance. The redundant data is related to over-represented material types and does not mitigate the severe performance degradation on out-of-distribution samples. In addition, we show that uncertainty-based active learning algorithms can construct much smaller but equally informative datasets. We discuss the effectiveness of informative data in improving prediction performance and robustness and provide insights into efficient data acquisition and machine learning training. This work challenges the "bigger is better" mentality and calls for attention to the information richness of materials data rather than a narrow emphasis on data volume.

---

### 8355.doc… [^820e9cc1]. aspe.hhs.gov (2025). Medium credibility.

17 Table 8 Average Impact Factors for Comparable Categories of Journals 19 Table 9 Summary of Grantee Publication History 1999 to the Present for Non-R03 related Articles 20 Table 10 Summary of Publication for Non-R03-Related Articles by Year 25 Table 11 Summary of Center for Scientific Review Study Section Selection Criteria and Associated Outcome Measures. This special review process allows for a shorter turnaround time for small projects. Successful grantees receive funding for two years, with maximum allowable direct costs of $100, 000. As stated in NCI publication PA 04–034, the R03 mechanism is distinguished from the R21 mechanism in that "new investigators use the R03 to learn the factors involved in leading an investigation in an area they may have been working in since graduate school or as a post-doc" and "there is a lesser emphasis on the continuation into the R01 for the R03 applicant".

The cohort of grantees from the subsequent program announcement, who completed their research grants in April 2004, would be less likely to have concluded their analysis and prepared manuscripts for publication. The evaluation design combines both quantitative and qualitative methodologies to assess the success of the program. Specifically, the evaluation assesses the key question: What is the impact of the R03 Program on the careers of new investigators in the field of behavioral research in cancer control. Using the citation data file, key information was assembled including a summary of R03-related publication activities, peer-review status, 5-year citation impact for the journal, and 5-year citation impact for that category of journal.

The impact factor was calculated on an annual basis and consists of the number of citations in the current year to items published in the past two years divided by the number of substantive items published in those same two years. Impact factors for journals and for their journal categories were taken from the Institute for Scientific Information and other sources such as the. Note: The total number of grants submitted is not entirely accounted for in the grant status categories due to grantee recall. Grantees were unable to recall the status of 2 NIH applications related to their R03 research; 5 NIH applications not related to their R03 work; and 19 non-NIH applications related to their R03 research.

---

### Global quantification of mammalian gene expression control [^6eab7aec]. Nature (2011). Excellent credibility.

Gene expression is a multistep process that involves the transcription, translation and turnover of messenger RNAs and proteins. Although it is one of the most fundamental processes of life, the entire cascade has never been quantified on a genome-wide scale. Here we simultaneously measured absolute mRNA and protein abundance and turnover by parallel metabolic pulse labelling for more than 5,000 genes in mammalian cells. Whereas mRNA and protein levels correlated better than previously thought, corresponding half-lives showed no correlation. Using a quantitative model we have obtained the first genome-scale prediction of synthesis rates of mRNAs and proteins. We find that the cellular abundance of proteins is predominantly controlled at the level of translation. Genes with similar combinations of mRNA and protein stability shared functional properties, indicating that half-lives evolved under energetic and dynamic constraints. Quantitative information about all stages of gene expression provides a rich resource and helps to provide a greater understanding of the underlying design principles.

---

### Quantum networks reveal quantum nonlocality [^a97c7efb]. Nature Communications (2011). Medium credibility.

The results of local measurements on some composite quantum systems cannot be reproduced classically. This impossibility, known as quantum nonlocality, represents a milestone in the foundations of quantum theory. Quantum nonlocality is also a valuable resource for information-processing tasks, for example, quantum communication, quantum key distribution, quantum state estimation or randomness extraction. Still, deciding whether a quantum state is nonlocal remains a challenging problem. Here, we introduce a novel approach to this question: we study the nonlocal properties of quantum states when distributed and measured in networks. We show, using our framework, how any one-way entanglement distillable state leads to nonlocal correlations and prove that quantum nonlocality is a non-additive resource, which can be activated. There exist states, local at the single-copy level, that become nonlocal when taking several copies of them. Our results imply that the nonlocality of quantum states strongly depends on the measurement context.

---

### An introduction to machine learning [^70a823d9]. Clinical Pharmacology and Therapeutics (2020). Medium credibility.

Different categories of loss functions

Different objective functions can be chosen to measure the distance between observed data and values predicted by the model. Some of the distance metrics used in practice can be associated to a likelihood. The likelihood indicates how probable it is to observe our data according to the selected model. The most common use of a likelihood is to find the parameters that make the model fit optimally to the data (i.e. the maximum likelihood parameter estimates). Usually, the negative logarithm of the likelihood is minimized and considered as objective function because it has favorable numerical properties. Similarly, in ML metrics, such as mean squared error, logistic objective, or cross‐entropy, are used to find optimal parameters or assess the fitness of the model.

In practice, analytical calculation of maximum likelihood or minimal loss may not be feasible, and it is often necessary to use a numerical optimization algorithm to solve for the best parameter values. Gradient descent is such an algorithm, where we first define an objective function for which we want to minimize and then iteratively update the values of the parameters in the direction with the steepest decrease (first‐order derivative) of the objective function until a convergence to a minimum distance is deemed reached. In the scenario of a nonconvex objective function, the success of finding a global minimum, as opposed to landing in some local minima, will depend on the choice of the initial set of parameter values, the learning rate (i.e. step size of each iteration) and the criterion for convergence. The reader can refer to ref. 35 for details on convex and nonconvex optimization processes. Stochastic gradient descent is an additional trick that can further speed up the optimization by randomly sampling a training dataset and summing the distances across this subset of training data points for approximating the objective function.

---

### The minimal work cost of information processing [^894e044e]. Nature Communications (2015). Medium credibility.

where the sum ranges only over those x that have a non-zero probability of occurring. In the case of deterministic mappings p (x ′| x)∈{0,1}, this corresponds to the maximum number of input states that map to a same output state. For the AND gate, provided all four states 00, 01, 10 and 11 have non-negligible probability of occurring, there are three input states mapping to the same output state, so (3) gives us simply. Also, in simple examples as considered here, the expression (3) is stable to considering an-approximation (Supplementary Note 4); this quantity is thus physically justified.

Crucially, our result reveals that the minimal work requirement in general depends on the specific logical process, and not only on the input and output states. This contrasts with traditional thermodynamics for large systems, where the minimal work requirement of a state transformation can always be written as a difference of a thermodynamical potential, such as the free energy. For example, the minimal work cost of performing specifically an AND gate may differ from that of another logical process mapping an input distribution (p 00, p 01, p 10, p 11) (with ∑ i p i = 1) to the distribution (p ′ 0, p ′ 1) = (p 00 + p 01 + p 10, p 11) (Recall that the classical counterpart of a quantum state is a probability distribution.). To see this, consider the XOR gate, which outputs a 1 exactly when both inputs are different (see Fig. 2b). The minimal work cost requirement of this gate, as given by (3), is now only kT ln 2, as in the worst case, only a single bit of information is erased (again supposing that all four input states have non-negligible probability of occurring). Now, suppose that, for some reason, the input distribution is such that p 01 + p 10 = p 11, that is, the input 11 occurs with the same probability as of either 01 or 10 appearing. Then, the XOR gate reproduces the exact same output distribution as the AND gate: in both cases, we have p ′ 0 = p 00 + p 10 + p 01 = p 00 + p 11 and p ′ 1 = p 11 = p 01 + p 10. In other words, both logical processes have the same input and output state, yet the XOR gate only requires work kT ln 2 compared with the AND gate, which requires 1.6 kT ln 2. Furthermore, we point out that this difference, which appears small in this case, may be arbitrarily large in certain scenarios (Supplementary Note 4).

---

### Improved machine learning algorithm for predicting ground state properties [^f1a00240]. Nature Communications (2024). High credibility.

The ML algorithm predicted the classical representation of the ground state for a new vector x. These predicted classical representations were used to estimate two-body correlation functions, i.e. the expectation value offor each pair of qubits 〈 i j 〉 on the lattice. Here, we are using the combination of our ML algorithm with the classical shadow formalism as described in Corollary 1, leveraging this more powerful technique to predict a large number of ground state properties.

In Fig. 2 A, we can clearly see that the ML algorithm proposed in this work consistently outperforms the ML models implemented in, which includes the rigorous polynomial-time learning algorithm based on Dirichlet kernel proposed in, Gaussian kernel regression, and infinite-width neural networks. Figure 2 A (Left) and (Center) show that as the number T of measurements per data point or the training set size N increases, the prediction performance of the proposed ML algorithm improves faster than the other ML algorithms. This observation reflects the improvement in the sample complexity dependence on prediction error ϵ. The sample complexity independs exponentially on 1/ ϵ, but Theorem 1 establishes a quasi-polynomial dependence on 1/ ϵ. From Fig. 2 A (Right), we can see that the ML algorithms do not yield a substantially worse prediction error as the system size n increases. This observation matches with thesample complexity in Theorem 1, but not with the poly(n) sample complexity proven in. These improvements are also relevant when comparing the ML predictions to actual correlation function values. Figure 3 inillustrates that for the average prediction error achieved in their work, the predictions by the ML algorithm match the simulated values closely. In this work, we emphasize that significantly less training data is needed to achieve the same prediction errorand agree with the simulated values.

---

### 2021 ACC / AHA key data elements and definitions for heart failure: a report of the American college of cardiology / American Heart Association task force on clinical data standards (writing committee to develop clinical data standards for heart failure) [^ab9498f9]. Circulation: Cardiovascular Quality and Outcomes (2021). High credibility.

History of hepatitis C infection — defined as "A viral infection caused by the hepatitis C virus" — has permissible values "Positive, fully treated", "Positive, partially treated", "Positive, untreated", "Negative", or "Unknown".

---

### Improved machine learning algorithm for predicting ground state properties [^cea9d142]. Nature Communications (2024). High credibility.

Proposition 1

(A variant of Proposition 1 in). Consider a randomized polynomial-time classical algorithmthat does not learn from data. Suppose for any smooth family of gapped 2D Hamiltoniansand any single-qubit observablecan compute ground state propertiesup to a constant error averaged over x ∈ [−1, 1] m uniformly. Then, NP-complete problems can be solved in randomized polynomial time.

This proposition states that even under the restricted settings of considering only 2D Hamiltonians and single-qubit observables, predicting ground state properties is a hard problem for non-ML algorithms. When one consider higher-dimensional Hamiltonians and multi-qubit observables, the problem only becomes harder because one can embed low-dimensional Hamiltonians in higher-dimensional spaces.

Numerical experiments

We present numerical experiments to assess the performance of the classical ML algorithm in practice. The results illustrate the improvement of the algorithm presented in this work compared to those considered in, the mild dependence of the sample complexity on the system size n, and the inherent geometry exploited by the ML models. We consider the classical ML models previously described, utilizing a random Fourier feature map. While the indicator function feature map was a useful tool to obtain our rigorous guarantees, random Fourier features are more robust and commonly used in practice. Moreover, we still expect our rigorous guarantees to hold with this change because Fourier features can approximate any function, which is the central property of the indicator functions used in our proofs. Furthermore, we determine the optimal hyperparameters using cross-validation to minimize the root-mean-square error (RMSE) and then evaluate the performance of the chosen ML model using a test set. The models and hyperparameters are further detailed in Supplementary Section 4.

---

### 2021 ACC / AHA key data elements and definitions for heart failure: a report of the American college of cardiology / American Heart Association task force on clinical data standards (writing committee to develop clinical data standards for heart failure) [^972f8cac]. Circulation: Cardiovascular Quality and Outcomes (2021). High credibility.

History of hepatitis B infection — defined as "A viral infection caused by the hepatitis B virus" — has permissible values "Positive, fully treated", "Positive, partially treated", "Positive, untreated", "Negative", or "Unknown".

---

### Local orthogonality as a multipartite principle for quantum correlations [^34afbe74]. Nature Communications (2013). Medium credibility.

In recent years, the use of information principles to understand quantum correlations has been very successful. Unfortunately, all principles considered so far have a bipartite formulation, but intrinsically multipartite principles, yet to be discovered, are necessary for reproducing quantum correlations. Here we introduce local orthogonality, an intrinsically multipartite principle stating that events involving different outcomes of the same local measurement must be exclusive or orthogonal. We prove that it is equivalent to no-signalling in the bipartite scenario but more restrictive for more than two parties. By exploiting this non-equivalence, it is then demonstrated that some bipartite supra-quantum correlations do violate the local orthogonality when distributed among several parties. Finally, we show how its multipartite character allows revealing the non-quantumness of correlations for which any bipartite principle fails. We believe that local orthogonality is a crucial ingredient for understanding no-signalling and quantum correlations.

---

### Diagnosis, management and treatment of the Alport syndrome-2024 guideline on behalf of ERKNet, ERA and ESPN [^45eede36]. Nephrology, Dialysis, Transplantation (2025). High credibility.

Regarding diagnostic investigations for Alport syndrome, more specifically with respect to hearing assessment, ERA/ERN ERKNet/ESPN 2025 guidelines recommend to consider obtaining a hearing evaluation at diagnosis or upon reaching adulthood in female patients with X-linked alport syndrome, and then every 5 years in the absence of hearing loss symptoms.

---

### Gene expression drives the evolution of dominance [^7890a1b9]. Nature Communications (2018). Medium credibility.

Simulation of theoretical dominance model

For the simulations in Fig. 5b, c, we simulated 5000 genes with random intercept and scale parameters and computed h and s of potential mutations in each gene. The cost parameter c was fixed to 0.001. The intercept parameter was sampled from a uniform distribution with values ranging from 0.9 to 1, reflecting the fact that most new mutations are effectively neutral. The scale parameter was sampled from the absolute values of a normal distribution with mean and standard deviation of 0.1, leading to variation in the levels of optimal gene expression that is slightly skewed to lower values (i.e. assuming more genes with small optimal gene expression than with large optimal gene expression).

Code availability

The code used for analysis is available at.

Data availability

The datasets analyzed during the current study are available in the European Nucleotide Archive using accession numbers PRJEB19780 and PRJNA284572.

---

### Standards of care in diabetes – 2025 [^ef149bc3]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for diabetic nephropathy, more specifically with respect to pediatric patients, ADA 2025 guidelines recommend to determine the eGFR at the time of diagnosis and annually thereafter.

---

### Interpolating numerically exact many-body wave functions for accelerated molecular dynamics [^7dff026b]. Nature Communications (2025). High credibility.

While there have been many developments in computational probes of both strongly-correlated molecular systems and machine-learning accelerated molecular dynamics, there remains a significant gap in capabilities in simulating accurate non-local electronic structure over timescales on which atoms move. We develop an approach to bridge these fields with a practical interpolation scheme for the correlated many-electron state through the space of atomic configurations, whilst avoiding the exponential complexity of these underlying electronic states. With a small number of accurate correlated wave functions as a training set, we demonstrate provable convergence to near-exact potential energy surfaces for subsequent dynamics with propagation of a valid many-body wave function and inference of its variational energy whilst retaining a mean-field computational scaling. This represents a profoundly different paradigm to the direct interpolation of potential energy surfaces in established machine-learning approaches. We combine this with modern electronic structure approaches to systematically resolve molecular dynamics trajectories and converge thermodynamic quantities with a high-throughput of several million interpolated wave functions with explicit validation of their accuracy from only a few numerically exact quantum chemical calculations. We also highlight the comparison to traditional machine-learned potentials or dynamics on mean-field surfaces.

---

### Stochastic representation of many-body quantum States [^cf5b8aa3]. Nature Communications (2023). High credibility.

The quantum many-body problem is ultimately a curse of dimensionality: the state of a system with many particles is determined by a function with many dimensions, which rapidly becomes difficult to efficiently store, evaluate and manipulate numerically. On the other hand, modern machine learning models like deep neural networks can express highly correlated functions in extremely large-dimensional spaces, including those describing quantum mechanical problems. We show that if one represents wavefunctions as a stochastically generated set of sample points, the problem of finding ground states can be reduced to one where the most technically challenging step is that of performing regression-a standard supervised learning task. In the stochastic representation the (anti)symmetric property of fermionic/bosonic wavefunction can be used for data augmentation and learned rather than explicitly enforced. We further demonstrate that propagation of an ansatz towards the ground state can then be performed in a more robust and computationally scalable fashion than traditional variational approaches allow.

---

### Machine learning in spectral domain [^bfbe7fe9]. Nature Communications (2021). High credibility.

Linear spectral learning: single-layer perceptron trained in reciprocal space

Assume N i to label the nodes assigned to layer i, and define. For the specific case here inspected the output layer is composed of ten nodes (N ℓ = 10), where recognition takes eventually place. Select one image from the training set and be n 1 (= 0, 1, 2. 9) the generic number therein displayed. We then construct a column vector, of size N, whose first N 1 entries are the intensities displayed on the pixels of the selected image (from the top-left to the bottom-right, moving horizontally), as illustrated in Fig. 1. All other entries are initially set to zero. As we shall explain in the following, our goal is to transform the inputinto an output vector with the same dimension. The last N ℓ elements of this latter vector represent the output nodes where reading is eventually performed.

To set the stage, we begin by reporting on a simplified scenario that, as we shall prove in the following, yields a single-layer perceptron. The extension to multilayered architectures will be discussed right after.

Consider the entry layer made of N 1 nodes and the outer one composed of N 2 elements. In this case N = N 1 + N 2. The input vectorundergoes a linear transformation to yieldwhere A 1 is a N × N matrix that we shall characterise in the following. Introduce matrix Φ 1: this is the identity matrixmodified by the inclusion of a sub-diagonal block N 2 × N 1, e.g. filled with uniformly distributed random numbers, defined in a bounded interval, see Fig. 2. The columns of Φ 1, hereafterwith k = 1. N, define a basis of the N-dimensional space to whichandbelong. Then, we introduce the diagonal matrix Λ 1. The entries of Λ 1 are set to random (uniform) numbers spanning a suitable interval. A straightforward calculation returns. We hence defineas the matrix that transformsinto. Because of the specific structure of the input vector, and owing to the nature of A 1, the information stored in the first N 1 elements ofis passed to the N 2 successive entries of, in a compact form which reflects both the imposed eigenvectors' indentation and the chosen non-trivial eigenvalues.

---

### 2022 AHA / ACC key data elements and definitions for cardiovascular and noncardiovascular complications of COVID-19: a report of the American college of cardiology / American Heart Association task force on clinical data standards [^e1ec7d3d]. Journal of the American College of Cardiology (2022). High credibility.

Persistently positive SARS-CoV-2 antigen or molecular test — defined as a persistently positive virological test (molecular amplification or antigen test) in a patient who is out of the presumptive infectious period of acute COVID-19 — uses permissible values Yes, No, or Unknown and maps to Centers for Disease Control and Prevention interim guidance; the note states that any decision on potential retesting for symptoms within 3 mo of an acute COVID infection should be individualized.

---

### EAU guidelines on neuro-urology [^aa3119e1]. EAU (2025). High credibility.

Regarding diagnostic investigations for neurogenic bladder, more specifically with respect to initial evaluation, EAU 2025 guidelines recommend to acknowledge individual patient disabilities when planning further investigations.

---

### Standards of care in diabetes – 2025 [^b0af56ee]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for diabetic nephropathy, more specifically with respect to laboratory testing, ADA 2025 guidelines recommend to consider alternative diagnoses.

---

### Deep learning suggests that gene expression is encoded in all parts of a co-evolving interacting gene regulatory structure [^5ce90dab]. Nature Communications (2020). High credibility.

Motif co-occurrence uncovers the regulatory rules of gene expression

To determine the functional meaning of the DNA motifs reconstructed from the deep learning relevance profiles (Fig. 3f), we analysed the informative power of the DNA motifs for predicting the specific gene expression levels. For this, we calculated the signal-to-noise ratio (SNR = μ / σ) of expression levels across genes that carried the identified motifs (Fig. 4a and Supplementary Fig. 23). The spread of expression levels of motif-associated genes was over 2.5-fold larger than its expected (median) expression level (Supplementary Fig. 24), resulting in a low median SNR of under 0.4 (Fig. 4a). Of the full 4 order-of-magnitude range of gene expression levels observed in the data (Fig. 1a), only 57% could be recovered with the motifs, as indicated by the average expression level of genes per associated motif (Fig. 4b and Supplementary Fig. 23b). The range of gene expression levels for the identified motifs were thus too dispersed and overlapped, indicating that the predictions made by the model (Figs. 1 f and 3b) were likely not based on single motif occurrences.

---

### Efficient probabilistic inference in generic neural networks trained with non-probabilistic feedback [^05d0a86f]. Nature Communications (2017). Medium credibility.

Task details

In the linear cue combination task, the objective is to combine two cues, r 1 and r 2, encoding information about the same variable, s, in a statistically optimal way. Assuming a squared error loss function, this can be achieved by computing the mean of the posterior p (s | r 1, r 2). For a uniform prior distribution, the posterior mean is given by an expression of the form:where ϕ is the vector of preferred stimuli of input neurons and 1 is a vector of ones. This expression is approximate when the prior is not uniform over the entire real line and the quality of the approximation can become particularly bad in the high-noise regime considered in this paper. Thus, in practice, we computed posterior means numerically, rather than using the above equation. The equation is still useful, however, in helping us understand the type of computation the network needs to perform to approximate optimal probabilistic inference. During training, the two cues received by the input populations were always non-conflicting: s 1 = s 2 = s and the gains of the input populations varied from trial to trial. The network was trained to minimize the mean squared error between its output and the common s indicated by the two cues.

---

### A coreduction to syngas.O-CDots-CNthree component electrocatalyst design concept for efficient and tunable CO [^7666a13e]. Nature Communications (2017). Medium credibility.

Investigation of the catalytic mechanisms

A series of controlled experiments were carried out to validate and further understand the catalytic mechanism operating with the Co 3 O 4 -CDots-C 3 N 4 proposed in a previous section (Fig. 2). Issues investigated included: the generation sites of H•, CO, and H 2; the role of H +; the roles of CDots, C 3 N 4, and Co 3 O 4; the role of the proximity between the CDots, C 3 N 4, and Co 3 O 4; ways in which the relative significance of the different reaction channels and the resulting CO/H 2 volume ratio can be manipulated; other HER-CDots-C 3 N 4 systems with different HER materials. We first studied the linear sweep voltammetry (LSV) curves of different combinations of the three components comprising the ternary Co 3 O 4 -CDots-C 3 N 4. The comparison of the different LSV curves (Fig. 4a) and the complementary study of the CO and H 2 composition generated during the electrocatalytic processes catalyzed by the different components (Fig. 4b) enabled a clear determination of both the generation sites of the CO and H 2 and the role of the three basic components used. We first discuss the LSVs showing the apparent current density (current density per geometrical area), which is also equivalent to the mass activity of the catalysts (current per gram catalyst) as shown in Fig. 4a. Further on, we also give the corresponding turn on frequencies (TOFs) and the real current densities derived from the real surface areas of the components of the different combinations of catalysts in Fig. 4a. LSVs of C 3 N 4 (Fig. 4a, curve 1, black trace, morphology of C 3 N 4 is shown in Supplementary Fig. 8a), CDots (Fig. 4a, curve 2, cyan trace, morphology of CDots is shown in Supplementary Fig. 1), Co 3 O 4 (curve 3, gray trace, morphology of Co 3 O 4 is shown in Supplementary Fig. 8b), CDots-C 3 N 4 (curve 4, blue trace, morphology of CDots-C 3 N 4 is shown in Supplementary Fig. 8c), Co 3 O 4 -C 3 N 4 (curve 5, green trace, morphology of Co 3 O 4 -C 3 N 4 is shown in Supplementary Fig. 8d), Co 3 O 4 -CDots (curve 6, purple trace, morphology of Co 3 O 4 -CDots is shown in Supplementary Fig. 8e), and Co 3 O 4 -CDots-C 3 N 4 (curve 7, red trace) were performed using a CO 2 -saturated 0.5 M KHCO 3 (pH = 7.2) solution (Fig. 4a). The LSVs of C 3 N 4 (curve 1, black trace), CDots (curve 2, cyan trace), Co 3 O 4 (curve 3, gray trace), CDots-C 3 N 4 (curve 4, blue trace), and Co 3 O 4 -C 3 N 4 (curve 5, green trace) show poor electrocatalytic performances evident by their low current densities and high onset potentials (Fig. 4a). Only the Co 3 O 4 -CDots (curve 6) and the Co 3 O 4 -CDots-C 3 N 4 (curve 7) exhibit high current densities and low onset potentials (Fig. 4a). CDots generate H 2 only, but not CO (Fig. 4b). We explain it by their trapping and stabilizing H•, which may generate a small amount of H 2 even in the absence of a HER catalyst. The LSV of Co 3 O 4 (curve 3, gray trace) shows a relatively small activity producing only H 2 (Fig. 4b). In comparison, Co 3 O 4 -CDots (curve 6, purple trace) produces a much larger amount of H 2 compared to pure CDots or pure Co 3 O 4. We attribute it to the effect of the CDots, which greatly enhance the electrocatalytic performance of Co 3 O 4 by providing H•, which is necessary for generation of H 2. We thus conclude that Co 3 O 4 is the H 2 generation site while CDots are the generation site of H• and both are needed for a large generation rate of H 2. Now, we prove that the generation site of CO is C 3 N 4. Figures 3 a, b and 4a, b indicate that significant amounts of CO are generated by either the CDots-C 3 N 4 or the Co 3 O 4 -CDots-C 3 N 4. Co 3 O 4 was shown to generate H 2 only, which leaves CDots-C 3 N 4 as the producer of CO. CDots per se produce only H 2 as previously discussed. They however are essential for CO generation by CDots-C 3 N 4 since they significantly enhance the CO 2 adsorption, adsorb H +, and stabilize H•. C 3 N 4 per se catalyzes only a very small (negligible) current (Fig. 4a, curve 1) so that the very little amount of gas produced by pure C 3 N 4 (Fig. 4b) contains more H 2 than CO. The supply of H• by the CDots is necessary to promote the generation of CO in C 3 N 4. Figure 4a shows that each catalyst component adds to the activity by enhancing one of the three reactions (H• generation, H 2 generation, and CO generation) but the complete three components composite Co 3 O 4 -CDots-C 3 N 4 is necessary for intense generation of syngas. We further studied the effect of the type of mixing of the different catalyst components on the catalytic activity. The LSVs of physical mixtures of catalysts (Co 3 O 4 + C 3 N 4, CDots + C 3 N 4, and Co 3 O 4 + CDots + C 3 N 4) were compared to those of composites of the same components chemically blended (Co 3 O 4 -C 3 N 4, CDots-C 3 N 4, and Co 3 O 4 -CDots-C 3 N 4). The chemically prepared composites had much larger activities (current densities) than their corresponding physical mixtures (Supplementary Fig. 9). We attribute the large activity of the chemically prepared composites to the close proximity between the different catalysts (active sites) in the composite materials. In contrast, physical mixing does not provide such a proximity so that the large distance between the active sites hinders the activity of the physically mixed catalysts.

---

### IPNA clinical practice recommendations for the diagnosis and management of children with steroid-sensitive nephrotic syndrome [^1363a1b7]. Pediatric Nephrology (2023). High credibility.

Regarding diagnostic investigations for nephrotic syndrome in children, more specifically with respect to initial evaluation, IPNA 2023 guidelines recommend to confirm nephrotic range proteinuria at least once by quantifying proteinuria before initiating treatment for the first episode.

---

### Philadelphia chromosome-negative classical myeloproliferative neoplasms: revised management recommendations from European LeukemiaNet [^7c5dc941]. Leukemia (2018). Medium credibility.

Regarding classification and risk stratification for primary myelofibrosis, more specifically with respect to risk assessment, ELN 2018 guidelines recommend to obtain molecular assessment during the course of the disease (at least ASXL1 mutation) for therapeutic decisions in selected patients, such as to decide on transplantation in patients with DIPSS/DIPSS-plus intermediate-1 risk category.

---

### Infective endocarditis in adults: diagnosis, antimicrobial therapy, and management of complications: a scientific statement for healthcare professionals from the American Heart Association [^4733d84e]. Circulation (2015). Medium credibility.

Regarding follow-up and surveillance for infective endocarditis, more specifically with respect to laboratory follow-up, AHA 2015 guidelines recommend to obtain ≥ 3 sets of blood cultures in patients with suspected relapses with new onset of fever or chills or other evidence of systemic toxicity.

---

### 2018 ESC guidelines for the diagnosis and management of syncope [^ff6a8c45]. European Heart Journal (2018). Medium credibility.

Regarding diagnostic investigations for syncope, more specifically with respect to neurological evaluation, ESC 2018 guidelines recommend to obtain neurological evaluation when syncope is due to autonomic failure to evaluate the underlying disease.

---

### Standards of care in diabetes – 2025 [^31be65a6]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for hypoglycemia, more specifically with respect to screening for cognitive impairment, ADA 2025 guidelines recommend to simplify diabetes treatment plans as much as possible for patients with cognitive impairment and tailor to minimize the risk of hypoglycemia.

---

### 2022 AHA / ACC key data elements and definitions for cardiovascular and noncardiovascular complications of COVID-19: a report of the American college of cardiology / American Heart Association task force on clinical data standards [^f8224a2d]. Journal of the American College of Cardiology (2022). High credibility.

FiO2/PEEP ratio is defined as the ratio of FiO2 to PEEP used to achieve desired arterial oxygenation, with permissible value type Numeric.

---

### Reflections on benefits and challenges of longitudinal organisational network analysis as a tool for health systems research and practice [^53dc86a3]. BMJ Global Health (2021). High credibility.

Dealing with data missingness

Gaps in data points are particularly problematic for network analysis, since one missing respondent at a particular time point in a group of N entities results in a loss of N-1 data points and, consequently, inferences based on a partial snapshot of the network (see figure 1, created with NetDraw). This is especially challenging when each given relationship or interaction between entities is defined by two separate data points (ie, data from both Organisation A and Organisation B about the nature, frequency or other characteristic of a particular interaction between A and B). When analysing these types of 'confirmed ties' (ie, an interaction confirmed by both parties), if one data point between a pair of nodes is missing, the other data point is effectively missing as well, since both are required to characterise the interaction. To compound the issue, it may not be clear what assumptions to use for data imputation since individual nodes are not necessarily assumed to behave like other nodes with similar characteristics. Different patterns of missingness across time points further complicate efforts to identify meaningful patterns or trends over time.

Figure 1
One-way (unconfirmed) reported linkages ('any links) at time 2 for network B in the US-based case application, colour-coded by actor response versus non-response. Of the 52 actors in the network at time 2 for network B in the US-based case example, only 21 (40%) responded; of these, only 11 actors also responded at time 1. Though respondents report their interactions with non-respondents, no information is available from non-respondents to confirm those interactions or to report on interactions with other actors. This substantially limits the ability to characterise the network with confirmed ties at each individual time point and the issue is often compounded as the number of time points increases with longitudinal analysis.

Threats to validity and reliability of measurements

When administered in survey or questionnaire format, it is possible that data will be collected from a small number of individuals in each organisation (potentially just a single person) and it is assumed that the responses from these people represent the perspective or experience of their entire unit or organisation. This introduces the possibility of several potential issues, including selecting the wrong person as the respondent or the wrong person being assigned to respond by organisational management; respondents misrepresenting the interactions of the organisation, whether intentionally or unintentionally (eg, due to recall bias); different individuals from a given organisation providing responses at different time points. Any of these issues could result in misleading findings.

---

### EAU guidelines on urolithiasis [^993f5667]. EAU (2025). High credibility.

Regarding diagnostic procedures for nephrolithiasis, more specifically with respect to stone analysis, EAU 2025 guidelines recommend to obtain stone analysis in first-time formers using a valid procedure (X-ray diffraction or infrared spectroscopy).

---

### The resolution of the genetics of gene expression [^884723ba]. Human Molecular Genetics (2009). Low credibility.

Understanding the influence of genetics on the molecular mechanisms underpinning human phenotypic diversity is fundamental to being able to predict health outcomes and treat disease. To interrogate the role of genetics on cellular state and function, gene expression has been extensively used. Past and present studies have highlighted important patterns of heritability, population differentiation and tissue-specificity in gene expression. Current and future studies are taking advantage of systems biology-based approaches and advances in sequencing technology: new methodology aims to translate regulatory networks to enrich pathways responsible for disease etiology and 2nd generation sequencing now offers single-molecular resolution of the transcriptome providing unprecedented information on the structural and genetic characteristics of gene expression. Such advances are leading to a future where rich cellular phenotypes will facilitate understanding of the transmission of genetic effect from the gene to organism.

---

### 2018 ESC guidelines for the diagnosis and management of syncope [^a1725495]. European Heart Journal (2018). Medium credibility.

Regarding diagnostic investigations for syncope, more specifically with respect to neurological evaluation, ESC 2018 guidelines recommend to obtain cognitive assessment and physical performance tests in older patients with syncope or unexplained fall.

---

### Diagnosis, management and treatment of the Alport syndrome-2024 guideline on behalf of ERKNet, ERA and ESPN [^6f42c569]. Nephrology, Dialysis, Transplantation (2025). High credibility.

Regarding diagnostic investigations for Alport syndrome, more specifically with respect to genetic testing, ERA/ERN ERKNet/ESPN 2025 guidelines recommend to recognize that no targeted gene panel is 100% sensitive for the different types of genetic changes that can cause Alport syndrome, with well-designed gene panels achieving a sensitivity of over 85% for COL4A3/4/5 gene analysis in detecting pathogenic variants.

---

### Stochastic reservoir computers [^0f0bc21f]. Nature Communications (2025). High credibility.

Reservoir computing is a form of machine learning that utilizes nonlinear dynamical systems to perform complex tasks in a cost-effective manner when compared to typical neural networks. Recent advancements in reservoir computing, in particular quantum reservoir computing, use reservoirs that are inherently stochastic. In this paper, we investigate the universality of stochastic reservoir computers which use the probabilities of each stochastic reservoir state as the readout instead of the states themselves. This allows the number of readouts to scale exponentially with the size of the reservoir hardware, offering the advantage of compact device size. We prove that classes of stochastic echo state networks form universal approximating classes. We also investigate the performance of two practical examples in classification and chaotic time series prediction. While shot noise is a limiting factor, we show significantly improved performance compared to a deterministic reservoir computer with similar hardware when noise effects are small.

---

### Stochastic representation of many-body quantum States [^3273c594]. Nature Communications (2023). High credibility.

In some cases, the Schrödinger equation can be recast in stochastic form. For example, in diffusion Monte Carlo (DMC). quantum properties can be extracted from the simulated stochastic dynamics of a set of N walkers, withdenoting a walker index and R i ≡ r 1, i ⊕… ⊕ r n, i containing the coordinates of all n particles in walker i. It is never necessary to store or evaluate wavefunctions explicitly. However, the DMC algorithm breaks down when the wavefunction has nodal surfaces: it exhibits a sign problem, meaning that the variance of the stochastic estimator (and therefore the error) grows exponentially with the system's size. The errors due to the sign problem can only be controlled when an accurate and efficiently evaluated expression for the location of the nodal surfaces is known. This suggests that one could improve the precision of a DMC calculation by preceding it with a variational calculation, which would then be used to estimate the location of the nodes. The overall accuracy is then limited by that of the variational ansatz Φ ϑ and the optimization procedure. In practice, therefore, DMC is most often employed as the final step of a VMC calculation, providing a small correction and enhancing the accuracy. This is not a problem specific to DMC: in fermionic systems, sign problems generically afflict Monte Carlo methods that circumvent the need to enumerate wavefunctions.

Here, we propose an intermediate approach that allows for full utilization of the expressive power of advanced ML models, without sacrificing either computational scaling or systematic optimization. Importantly, in our method the (anti)symmetry of fermions and bosons becomes an advantage rather than a drawback; and the sign problem is controlled.

---

### Biomarkers for adjuvant endocrine and chemotherapy in early-stage breast cancer: ASCO guideline update [^bc13db52]. Journal of Clinical Oncology (2022). High credibility.

AdjuvantOnline clinical risk classification in MINDACT — estrogen receptor (ER)-positive/human epidermal growth factor receptor 2 (HER2)-negative: For well differentiated tumors, node-negative (N–) with tumor size ≤ 3 are C-low and 3.1–5 are C-high; with 1–3 positive nodes, tumor size ≤ 2 are C-low and 2.1–5 are C-high. For moderately differentiated tumors, N– with tumor size ≤ 2 are C-low and 2.1–5 are C-high; with 1–3 positive nodes, any size is C-high. For poorly differentiated or undifferentiated tumors, N– with tumor size ≤ 1 are C-low and 1.1–5 are C-high; with 1–3 positive nodes, any size is C-high.

---

### European guideline on chronic nausea and vomiting – A UEG and ESNM consensus for clinical management [^31765349]. United European Gastroenterology Journal (2025). High credibility.

Regarding diagnostic investigations for rumination syndrome, more specifically with respect to initial assessment, ESNM/UEG 2025 guidelines recommend to consider evaluating patients with RS for a current or previous associated eating or psychiatric disorder.

---

### Bone cancer, version 2.2025, NCCN clinical practice guidelines in oncology [^cb365b27]. Journal of the National Comprehensive Cancer Network (2025). High credibility.

Regarding diagnostic investigations for Ewing's sarcoma, more specifically with respect to history and physical examination, NCCN 2025 guidelines recommend to elicit history and perform physical examination in patients presenting with suspected ES.

---

### Quantifying the dynamics of failure across science, startups and security [^6acaa77f]. Nature (2019). Excellent credibility.

Human achievements are often preceded by repeated attempts that fail, but little is known about the mechanisms that govern the dynamics of failure. Here, building on previous research relating to innovation 1–7, human dynamics 8–11 and learning 12–17, we develop a simple one-parameter model that mimics how successful future attempts build on past efforts. Solving this model analytically suggests that a phase transition separates the dynamics of failure into regions of progression or stagnation and predicts that, near the critical threshold, agents who share similar characteristics and learning strategies may experience fundamentally different outcomes following failures. Above the critical point, agents exploit incremental refinements to systematically advance towards success, whereas below it, they explore disjoint opportunities without a pattern of improvement. The model makes several empirically testable predictions, demonstrating that those who eventually succeed and those who do not may initially appear similar, but can be characterized by fundamentally distinct failure dynamics in terms of the efficiency and quality associated with each subsequent attempt. We collected large-scale data from three disparate domains and traced repeated attempts by investigators to obtain National Institutes of Health (NIH) grants to fund their research, innovators to successfully exit their startup ventures, and terrorist organizations to claim casualties in violent attacks. We find broadly consistent empirical support across all three domains, which systematically verifies each prediction of our model. Together, our findings unveil detectable yet previously unknown early signals that enable us to identify failure dynamics that will lead to ultimate success or failure. Given the ubiquitous nature of failure and the paucity of quantitative approaches to understand it, these results represent an initial step towards the deeper understanding of the complex dynamics underlying failure.

---

### What are the determinants of gene expression levels and breadths in the human genome? [^644d4e6a]. Human Molecular Genetics (2012). Low credibility.

RESULTS

Strong correlations between expression levels and breadths

We chose to analyze the Novartis Gene Atlas expression data from human and mouse transcriptomes because these data represent comprehensive information on genome-wide gene expression from the largest number of tissues currently available. Importantly, because they are obtained from human and mouse in a similar manner, we can compare results from these two taxa and infer potentially lineage-specific biological differences.

We will refer to the number of tissues in which a gene is expressed as the 'expression breadth'. The expression levels of each gene averaged over all tissues are referred to as the 'expression level'. In both human and mouse, expression breadths are extremely strongly correlated with expression levels [Pearson's correlation r = 0.81 and 0.82 for human and mouse, respectively, P < 2.2 × 10–16 for both comparisons]. An alternative measure of tissue specific patterns of gene expression is the 'tissue specificity index', which incorporates information on the maximum expression level in different tissues (see Materials and Methods). The tissue specificity index, while negatively correlated with the expression breadth (Pearson's correlation r = −0.39 and −0.40, for human and mouse, P < 2.2 × 10 −16 for both comparisons), behaves differently from the expression breadth: for example, the tissue specificity index exhibits low correlation with the expression level (r = −0.03, P = 0.003 and r = −0.13, P < 2.2 × 10 −16 for human and mouse, respectively). However, it is strongly correlated with the maximum expression level of a gene among the tissues examined (r = 0.53 and 0.61 for human and mouse, respectively, P < 2.2 × 10 −16 for both comparisons). In, we provide results of the same analyses using the tissue specificity index and the maximum level of gene expression. The major findings are highly similar to those presented in the main text.

---

### Identifying domains of applicability of machine learning models for materials science [^72c4ba76]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ν that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by σ lin (x 1, x 2) ≡ −0.3 ≤ x 1 ≤ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: σ rbf ≡ −3.3 ≤ x 11 ≤ 3.1 ∧ x 2 ≤ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA σ poly (x 1, x 2) ≡ −3.5 ≤ x 2 ≤ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### Revealing hidden patterns in deep neural network feature space continuum via manifold learning [^c4afd71e]. Nature Communications (2023). High credibility.

Pseudo labeling via histogram bin count

Consider a histogram with equally spaced bins. In particular, letdenote the bin boundaries, and let h m = t m (i +1) − t m i denote the bin widths which is uniform across all bins and is thus independent of index i. Associated with this histogram, letdenote the histogram estimation for the distance. The integrated mean squared error is then defined as followswhere the expectation is taken with respect to the binomial distribution of the number of samples that fall into the same bin as the point s. It is known that. To achieve this rate of convergence, we adopt Scott's optimal binning strategy. Let I m (d) be the bin interval that contains the pointand let t m (d) denote the left-hand endpoint of the bin I m (d). Integrating the density function over bin I m (d) yieldsUsing Taylor's expansion for the integrand, yields the following approximation of the integralWe denote the count of empirical distance values falling in the bin interval I m (d) by s m (d). Then, s m (d) has a binomial distribution. The histogram estimate then is given by a random variable defined aswith expectationThe bias can be expressed asThe variance of the histogram estimate can be derived as followsThe mean squared error between the histogram estimate and the true density value is defined byUsing Eqs. (4) and (7), we can thus writeIntegration of MSE(d) in eq. (8) results inBy optimizing the first two terms in eq. (9), the optimal choice of bin bandwidth is obtained inas followswhich, is the optimal choice for h m. We notice that this optimal choice depends on the derivative of the density p (d) which is unknown. This density itself depends on the underlying validation data-set { x 1, ⋯, x m } as well as the parameters θ * of the trained feed-forward network f (⋅; θ *). Consider a folded Gaussian distribution with zero mean, Above, σ > 0 is the variance. Plugging the folded Gaussian density function in Eq. (11) into Eq. (10) yieldsIn the sequel, we use the bin width estimate in Eq. (12). As shown in Supplementary Fig. S8, when m → ∞, the ratio of the estimateto the optimal value is near one for many heavy tail and bimodal distributions.

---

### Guidance for the Heart Rhythm Society pertaining to interactions with industry endorsed by the Heart Rhythm Society on April 26, 2011 [^f1dedb5c]. Heart Rhythm (2011). Medium credibility.

Heart Rhythm Society financial relationship disclosure categories — greater transparency is required and the value of financial relationships is now categorized as $0; $1–$10,000; $10,001–$25,000; $25,001–$50,000; $50,001–$100,000; and greater than $100,000, whereas previously categories were none; modest ($1–$10,000); or significant (more than $10,000).

---

### Controlling gene expression with deep generative design of regulatory DNA [^eef382d6]. Nature Communications (2022). High credibility.

To generate sequences that manifest desired target expression levels by connecting the functional regulatory DNA space modeled by the generator with expression levels and coding sequence information modeled by the predictor, a DNA-based activation maximization approach, was used that incorporates both the trained generator and predictor models (Fig. 2a). This approach has been tested with generative modeling of both regulatory DNA, as well as in other domains, such as image modeling, where its demonstration equals that of biological experimental validation, due to human-level performance being the benchmark there. The optimal trained generative model to use for optimization was identified at iteration 200,000 (Fig. 1c, Supplementary Fig. 2), further supported by comparing the properties of generators obtained at 6 different training iteration checkpoints (100,000, 200,000, 300,000, 500,000, 700,000 and 1,000,000) after optimization, which included the percentage of unique generated sequences, range of predicted gene expression levels and amounts of sampled sequences across the whole expression range (Supplementary Fig. 24). Optimizations were run for 100,000 iterations and, to increase the breadth of the investigated latent subspace, 10 optimization runs were performed with different initial random states. The results were merged to obtain a set of 6,062,804 unique sequences that were used for further analysis.

---

### Allele-specific gene expression differences in humans [^29553b38]. Human Molecular Genetics (2004). Low credibility.

In the last decade, the search for the genetic origins of phenotypic variation has expanded beyond the non-synonymous variants which alter the amino acid sequence of the encoded protein, and many examples of sequence variants which alter gene expression have been found. Recently, using both traditional and novel technologies, a number of surveys have been carried out to examine the frequency with which cis-acting sequence variants or other cis-acting effects, alter gene expression either in vitro or in vivo. Microarray data have shown that the expression of many genes varies markedly between individuals and allele-specific expression studies have shown that the source of much of this variation appears to be cis-acting effects. A significant proportion of the variation may originate in gene promoter regions and a large number of sequence variants which have functional effect in vitro have been found. The evidence suggests that given a large enough population, most, if not all genes may have allele-specific expression differences in at least some individuals and finding the genetic origins of each of these and linking the former to a possible phenotype must be a major long term goal of the biomedical community.

---

### A positive statistical benchmark to assess network agreement [^b2081d06]. Nature Communications (2023). High credibility.

Construction of positive and negative benchmarks based on maximum entropy framework

The positive and negative benchmarks are constructed based on the maximum entropy framework, which enables us to efficiently incorporate both hard and soft constraints. The entropy is maximized subject to these constraints, producing the most random distribution that is still in compliance with the considered constraints.

We consider two graphs G(V 1, E 1) and M(V 2, E 2). For the positive benchmark, G and M are combined to form Q(V 1 ∪ V 2, E 1 ∪ E 2), which includes all nodes and links from G and M. The graph ensemble {Gp} contains subgraphs of Q and is constructed by the maximum entropy approach to preserve the expected value of the node degrees in G. For example, to construct {Gp}, we fix the mean subgraph node degree at its original value < k i > G p = k i (G). The resulting probability of having a link between nodes i and j is given by p i j = 1/(1 + α i α j), with the expected subgraph degree of a node. The optimized α i can be found iteratively with the update rule. For the negative benchmark, G is randomized similarly, except that Q is replaced by the complete graph of nodes in G. We start with the initial conditionand perform a large number of iterations to update α i. For the negative benchmark, we stop the iterations when the maximum relative change of α i is less than 10 −6. For the positive benchmark, α i converges slower compared to the negative benchmark due to the high number of constraints. In this case, we stop iterating α i if the change in the mean overlap of the positive benchmark is less than 1 during the last 1000 iterations. α i are then used to calculate the connection probability p i j for all links in Q. We can calculate the average and variance of the (positive or negative) benchmark as Σ (i, j)∈M p i j and Σ (i, j)∈M p i j (1 − p i j), without the need to explicitly generate random samples from the ensemble. This enables an efficient evaluation of statistical significance.

---

### Summary benchmarks-full set – 2024 [^c9abe316]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — GRADE evidence quality ratings are defined for forming recommendations for care as follows: "Good quality (GQ): Further research is very unlikely to change our confidence in the estimate of effect", "Moderate quality (MQ): Further research is likely to have an important impact on our confidence in the estimate of effect and may change the estimate", and "Insufficient quality (IQ): Further research is very likely to have an important impact on our confidence in the estimate of effect and is likely to change the estimate; any estimate of effect is very uncertain".

---

### Associations of long-term nitrogen dioxide exposure with a wide spectrum of diseases: a prospective cohort study of 0. 5 million Chinese adults [^eeca5d93]. The Lancet: Public Health (2024). High credibility.

Table
Baseline characteristics of participants by quartiles of ambient NO 2 exposure averaged from 2005 to 2008

Figure 1
Annual average ambient NO 2 concentration (A) and area-specific distribution of NO 2 concentrations (B) from 2005 to 2018 across ten study areas

In panel B, study areas are ranked in descending order of annual average NO 2 levels in 2005. In the box plots, horizontal bolded lines represent medians across the whole 2005–18 period and the edges of the boxes represent IQRs. Vertical lines indicate 1·5 times the IQR. Black dots indicate outliers. NO 2 = nitrogen dioxide.

During approximately 6·5 million person-years of follow-up, the top causes of incident events were cardiovascular (n = 144 852), digestive (n = 74 729), respiratory (n = 73 232), and musculoskeletal (n = 54 409) diseases (appendix p 13). In the main models, annual NO 2 exposure showed linear or supra-linear associations with mental and behavioural (HR 1·12 [95% CI 1·05–1·21]), cardiovascular (1·04 [1·02–1·05]), respiratory (1·03 [1·01–1·05]), and musculoskeletal (1·11 [1·09–1·14]) diseases, but no clear association with other disease categories (Figure 2, Figure 3). Additional adjustment for PM 2·5 resulted in generally stronger positive associations, and the associations with cardiovascular, respiratory, and musculoskeletal diseases persisted, with suggestive evidence of additional significant associations with infectious (per 10 μg/m 3 NO 2: HR 1·08 [95% CI 1·03–1·13]), ear and mastoid process (1·23 [1·14–1·33]), digestive (1·04 [1·02–1·07]), and genitourinary (1·07 [1·04–1·11]) diseases (appendix pp 16, 21). By contrast, associations did not change materially in the main + O 3 models (Cochrane's Q test p > 0·05 for all; appendix pp 16, 21). Across all three models, NO 2 had no association with external causes (Figure 2, Figure 3; appendix p 21).

---

### Inferring histology-associated gene expression gradients in spatial transcriptomic studies [^b1051e47]. Nature Communications (2024). High credibility.

Representative examples of either hypothesis as well as for either screening approach, are displayed in Supplementary Fig. 8a, b. To be able to adopt either of these hypotheses, we posit that, if the inferred gradient in question stems from a gene whose inferred expression gradient is dependent on the spatial trajectory or the spatial annotation, it should not merely consist of randomly scattered expression values. Instead, it should display a discernible degree of gradual expression change forming a recognizable pattern. Thus, the smoother the inferred gradient, the less random it is, and vice versa. Assuming this relationship, we quantify the degree of randomness of a gradient using its total variation (TV). This metric is calculated by considering the absolute differences between adjacent expression estimates, as displayed in Supplementary Fig. 8c. The total variation is calculated according to the formula:where
TV is the total variation.
n is the number of expression estimates in the gradient.
yi represents the gene expression value at expression estimate i.

| y i+1 − y i | calculates the absolute difference between the gene expression values of adjacent expression estimates.

To assess the effectiveness of the total variation in capturing the percentage of noise or randomness introduced into a gradient, we leveraged the simulated dataset discussed above, where a certain degree of noise was introduced per simulation, represented by randomly generated expression values. Each resulting simulation was a combination of pattern-specific expression (Supplementary Figs. 9 c, 10c) and randomly generated expression, based on different noise types (Supplementary Figs. 9 d–f, 10d–f). We examined the relationship between the total variation and the degree of randomness across different underlying patterns and various types of noise. Our findings consistently demonstrated a strong linear relationship between the total variation and noise ratio across all simulation modalities using a resolution of 100 μm. The high C values of 0.77–0.92 and 0.78–0.91 in the noise types equally distributed and combined (which we consider the most realistic manifestation of noise in real-life data) indicate that this metric effectively quantifies the degree of randomness.

---

### Bumetanide [^d0437f55]. FDA (2025). Medium credibility.

WARNING

Bumetanide injection is a potent diuretic which, if given in excessive amounts, can lead to a profound diuresis with water and electrolyte depletion. Therefore, careful medical supervision is required, and dose and dosage schedule have to be adjusted to the individual patient's needs [see Dosage and Administration].

---

### Accurate estimation of cell-type composition from gene expression data [^3e83321c]. Nature Communications (2019). High credibility.

Simulation details

Counts for the simulated single-cell data set are generated using a Poisson distribution, for a total of six genes and three cell types. In the first simulation, two genes are upregulated in each cell type, where λ = 50 for an upregulated gene and λ = 5 otherwise. Fifty cells from each cell type are used to create a signature matrix, where the six genes are averaged over each cell type to create a reference gene expression profile. Between 10,001 and 15,000 cells are used to simulate bulk data, by summing up gene expression values across cell types. Specifically, 5000 bulk data sets are created by combining 5000 cells from cell types 2 and 3, and between 1 and 5000 cells from cell type 1. Overall, this creates bulk data sets with a rare cell-type proportion spanning between 0.1% and 33.3%. Bulk data simulation is repeated 10 times for each rare cell-type proportion, and all metrics reported are based on an average of these 10 samples.

In the second simulation, two genes are again upregulated in each cell type, but the mean expression level of the marker genes corresponding to the first cell type is lower, such that λ ranges from 0.05 to 10 for an upregulated gene and from 0.005 to 1 otherwise. Fifty cells from each cell type are again used to create a signature matrix, where gene expression levels are scaled for each choice of λ, for a total of 200 signature matrices. To simulate the bulk data, 5000 cells from each cell type are aggregated so that each cell type is present in equal proportion. Bulk data simulation is repeated 10 times for each choice of λ, and all metrics reported are based on an average of these 10 samples.

---

### NCCN guidelines® insights: breast cancer, version 5.2025 [^300adaa1]. Journal of the National Comprehensive Cancer Network (2025). High credibility.

Gene expression assay caveat: In other groups, genomic expression assays may be considered to help assess risk of recurrence but have not been validated to predict response to chemotherapy.

---

### Clinical practice guideline: nosebleed (epistaxis) [^c36914a6]. Otolaryngology — Head and Neck Surgery (2020). High credibility.

Regarding diagnostic investigations for epistaxis, more specifically with respect to initial assessment, AAO-HNS 2020 guidelines recommend to distinguish patients with nosebleed requiring prompt management from patients who do not, at the time of initial contact.

---

### Diagnosis, management and treatment of the Alport syndrome-2024 guideline on behalf of ERKNet, ERA and ESPN [^c59a118f]. Nephrology, Dialysis, Transplantation (2025). High credibility.

Regarding diagnostic procedures for Alport syndrome, more specifically with respect to kidney biopsy, ERA/ERN ERKNet/ESPN 2025 guidelines recommend to avoid performing a kidney biopsy in patients with a genetic diagnosis of X-linked Alport syndrome or autosomal recessive Alport syndrome.