# Medical Question & Answer

**Sample ID**: a071422f-d4c6-cf9b-176e-2462a830e7de
**Dataset Index**: 266757

---

## Question

Example 2 Given x < [5/4], find the maximum value of the function f(x) = 4x − 2 + [1/(4x − 5)].

---

## Answer

> Let's see… What do we have here? The user is asking to find the maximum value of the function f(x) = 4x − 2 + [1/(4x − 5)] given the constraint x < 5/4. Let's break this down step-by-step. First, I need to think about the domain and whether the function is well-defined. Then, I should simplify the expression with a substitution to make differentiation easier. Next, I will find critical points and check second-order conditions. After that, I need to verify behavior near the boundary and at infinity. Finally, I will confirm the maximum value and state the answer clearly with justification.

> Let me first confirm the domain and the function's behavior. The constraint is x < 5/4, which means 4x − 5 < 0, so the denominator is negative and the function is defined everywhere in the domain; I should double-check that there are no other singularities, and indeed there are none in this interval, so the function is smooth and differentiable on (−∞, 5/4).

> I will now simplify the expression to make calculus cleaner. Let u = 4x − 5, so that x = (u + 5)/4; substituting gives f(x) = 4x − 2 + [1/(4x − 5)] = (u + 5) − 2 + [1/u] = u + 3 + [1/u], and since x < 5/4, we have u < 0; this transformation will make differentiation and sign analysis more straightforward, and I should remember to convert back to x at the end.

> Next, I should find critical points by taking the derivative with respect to u and setting it to zero. The derivative is df/du = 1 − [1/u²]; setting this equal to zero yields 1 − [1/u²] = 0, so u² = 1, which gives u = ± 1; but wait, I need to ensure I only consider u < 0, so the relevant critical point is u = −1, which corresponds to x = (u + 5)/4 = (−1 + 5)/4 = 1; I should confirm that this x satisfies the original constraint, and yes, 1 < 5/4, so it is in the domain.

> Hold on, let's not jump to conclusions; I should verify whether this critical point is a maximum or minimum using the second derivative. The second derivative is d²f/du² = [2/u³]; at u = −1, this equals [2/(−1)³] = −2, which is negative, indicating a local maximum; I should also confirm that this is the only critical point in the domain, and since u = 1 is excluded by u < 0, this is indeed the unique critical point in the relevant interval.

> I need to check the function's behavior as we approach the boundary and as x goes to negative infinity. As x approaches 5/4 from the left, u approaches 0 from the left, so 1/u tends to −∞, making f(x) tend to −∞; as x approaches −∞, u approaches −∞, and the linear term dominates, so f(x) approaches −∞; this confirms that the local maximum at x = 1 is in fact the global maximum on the interval (−∞, 5/4).

> Now I will calculate the maximum value by plugging the critical point back into the original function. At x = 1, f(1) = 4(1) − 2 + [1/(4(1) − 5)] = 4 − 2 + [1/(−1)] = 2 − 1 = 1; I should double-check the arithmetic, and yes, it is correct, so the maximum value is 1.

> Final answer: The maximum value of f(x) = 4x − 2 + [1/(4x − 5)] subject to x < [5/4] is 1, attained at x = 1.

---

The maximum value of f(x) = 4x − 2 + [1/(4x − 5)] for x < [5/4] is **1**, achieved at x = 1. This is found by substituting u = 4x − 5 (so u < 0), rewriting f(x) = u + 3 + [1/u], and maximizing g(u) = u + [1/u] for u < 0. The maximum occurs at u = −1, which corresponds to x = 1, yielding f(1) = 1.

---

## Step 1: Substitution to simplify the function

Let (u = 4x - 5). Since (x < [5/4]), we have (u < 0). Then, (x = [(u + 5)/4]), and substituting into (f(x)) gives:

f(x) = 4[(u + 5)/4] − 2 + [1/u] = u + 5 − 2 + [1/u] = u + 3 + [1/u]

So, we need to maximize g(u) = u + [1/u] + 3 for u < 0.

---

## Step 2: Analyze the function g(u) = u + [1/u] for u < 0

We find the derivative:

g'(u) = 1 − [1/u²]

Setting g'(u) = 0 gives 1 − [1/u²] = 0, so u² = 1. Since u < 0, the critical point is u = −1.

Next, we check the second derivative:

g"(u) = [2/u³]

At u = −1, g"(−1) = −2 < 0, confirming a local maximum.

---

## Step 3: Evaluate the maximum value

At (u = -1):

g(−1) = −1 + [1/(−1)] + 3 = −1 − 1 + 3 = 1

Thus, the maximum value of (f(x)) is **1**.

---

## Step 4: Find the corresponding (x)-value

Since (u = 4x - 5), setting (u = -1) gives:

4x − 5 = −1 ⇒ 4x = 4 ⇒ x = 1

So, the maximum occurs at x = 1, which satisfies x < [5/4].

---

## Conclusion

The maximum value of f(x) = 4x − 2 + [1/(4x − 5)] for x < [5/4] is **1**, achieved at x = 1.

---

## References

### Size limits the sensitivity of kinetic schemes [^68145c64]. Nature Communications (2023). High credibility.

Methods

Logarithmic sensitivity, the Hill coefficient, and fold-change amplification

Here we review the relationships between the logarithmic sensitivity and other measures of sensitivity that might be reported or measured in an experiment, especially "the Hill coefficient".

If f (x) were a Hill function (1), thenWe note two simple facts about this expression. First, the logarithmic sensitivity achieves its maximum value, H, when x (and so f (x)) is very small. Second, at the midway point x = K, wherereaches its maximal value H /4, the logarithmic sensitivity is H /2.

As mentioned in the main text, in general, functions of interest will not actually be Hill functions, but it is common nevertheless to report a Hill coefficient or an "effective Hill coefficient", H eff. There are several different quantities — we will describe several below — sometimes called the effective Hill coefficient, and which give H eff = H in the case of the Hill function, but which in general are not equivalent. We will later see that the size of the support bounds them all.

First, suppose f (x) is not a Hill function, but that it is known exactly, or at least, we can find its derivative. Then one approach is to define H eff directly as the logarithmic sensitivity at some point, in analogy to how H controls the sensitivity of the Hill function. For example, where x * is the value of x at which f (x) is halfway between the smallest and largest value it can assume. This definition has been used to quantify the sensitivity of non-Hill sigmoidal functions arising from theoretical models (e.g. refs.).

In an experimental context, it is very common to fit a Hill function to data (e.g. averaged observations) that are purported to reflect a functional relationship f (x), and to report the fit parameteras the Hill coefficient. Often this is informative, but as a matter of principle, two functions can have radically different derivatives even if the function values are very close everywhere (e.g. if one function exhibits very high frequency but low amplitude oscillations). This means that, even if the fit is very good, relations based on analogy to (6), such as thatat the midpoint, can fail dramatically.

---

### Over-exploitation of natural resources is followed by inevitable declines in economic growth and discount rate [^b6b7ffb0]. Nature Communications (2019). High credibility.

We used algorithms that find the exact solutions provided that the resolutions are sufficiently fine. Specifically, to find the optimal solution numerically, our algorithm uses Stochastic Programming with backward induction (Supplementary Note 4). (Note that the model's dynamics are deterministic but the general method is still called stochastic.) To find the market solution, our algorithm also uses Stochastic Programming to solve for a given value of X. But it finds a solution multiple times, each time for a different value of X, until it finds the solution that satisfies the consisteny criterion. These algorithms are coded in C/C++ and are described in detail in Supplementary Note 4.

In turn, in the results shown in Fig. 3, system 2, as well as in Figs. 4 and 5 and in the graphical tool, we assume that the dynamics of c and f follow Eqs. 6–9, but we consider harvest functions that are not given by either the optimal solution or the market solution. In Fig. 3, system 2, we consider harvest functions that follow the market solution until t = t 1 and after t = t 1 + 10, but between these times, the non-sustainable harvest decreases gradually from its maximal level to zero. In Fig. 4, we calculate Δ sus, which is the cumulative discount that emerges if the harvest is entirely sustainable, namely, H n = 0 and H s = x 1 + x 2 if t > 0. Also, in Fig. 4a, we consider three scenarios in which the non-sustainable harvest is higher in the beginning but eventually approaches zero, while H n + H s = x 1 + x 2.

After we determine the harvest functions, the functions c (t) and f (t) are calculated according to Eqs. 6 and 9. In turn, we calculate the discount rate and the cumulative discount according to Eq. 5 (where the cumulative discount is the integral over time of the discount rate). Specifically, for the case in which only sustainable harvest is used (Δ sus in Fig. 4), the discount rates are calculated in Supplementary Note 2 and are given by Eqs. B5 and B12. The prices are given by Eq. A10, and the total product is given by Eq. A11. All of these equations are derived in Supplementary Notes 1, 2.

---

### Size limits the sensitivity of kinetic schemes [^71d1065f]. Nature Communications (2023). High credibility.

A different measure of sensitivity — the amplification of a fold-change in the input — provides a solution to this problem. Suppose that for some value x 0 of the input parameter x, scaling by a factor a scales the output by b, so f (a x 0) = b f (x 0). Then the quotientcan be thought of as a discrete approximation of the derivative defining the logarithmic sensitivity. And if f (x) is differentiable everywhere, then by the mean value theorem, there must be a value x * of x for whichThis means that careful measurement of any two points on the input-output curve (x versus f (x)) witnesses the (local, infinitesimal) logarithmic sensitivity somewhere. Importantly — unlike in the case of fitting to a Hill function — if error in the measurements is very low, then they are also telling us the derivative for some value of x very accurately.

Equation (25) leads us to another common definition of the effective Hill coefficient:where S 0.9 and S 0.1 are the values of the input variable (in our case, x) required to get 90% and 10% (respectively) of the maximum value of the output variable (in our case, f (x)). Note that (26) is like (25) with a = S 0.9 / S 0.1 and b = 9. It implies that somewhere between S 0.1 and S 0.9 there is a logarithmic sensitivity of H eff /2.

There is yet another common definition, specific to models of binding. Suppose x is the concentration of a ligand and 〈 n b 〉(x) is the expected number of sites bound by a ligand out of a total of n possible binding sites. It is common then, to takeor to report, as the Hill coefficient, the slope of a line fitted to x versusdata on a log-log plot.

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^d55d13ff]. Nature Communications (2023). High credibility.

F-statistics and p -values

The basic idea behind the F-value in classical linear regression problems is to quantify how much better a more complex model (M 1 with k linear parameters) fits the data compared to a simpler model (M 0 with j linear parameters and j < k) given the n observed data points and the corresponding sum-squared errors (SSE) (Eq. 6).

Although not a linear model by nature, the log-logistic function still meets the required assumptions of random sampling, independence of observations, residual normality, and equal variance of the errors. The basic rationality behind CurveCurator's recalibrated F-statistic is similar to the linear F-statistic above. It also quantifies how much better the fitted log-logistic model (M 1) is compared to the mean model (M 0), which describes that there is no relationship between the applied dose and the observed response. We found, however, that n/k was a more appropriate scaling factor for the 4-parameter log-logistic function.

The obtained recalibrated F-value (Eq. 7) can then be used to calculate a p -value that quantifies how often a curve with a similar or bigger F-value can be found by random chance. We observed that these F-values follow a parameterized F-distribution with degrees of freedom that diverged from the case of linear models. Using extensive simulations under the null hypothesis (5 million curves for n = 5… 50), we obtained a simple quasi-linear function to calculate the "effective "degrees of freedom as a function of n (Eqs. 8–10).

---

### Using both qualitative and quantitative data in parameter identification for systems biology models [^a956cd8d]. Nature Communications (2018). Medium credibility.

We must choose the penalty constants C i in Eq. (3) based on our confidence in the qualitative data — a larger C i gives the qualitative data more weight compared to the quantitative data. We hand-selected a value of C i = 0.03 for all i, to give roughly equal contributions from the qualitative and quantitative datasets (i.e. such that the blue and orange curves in Fig. 2d, e can be plotted using the same scale). Note that giving equal contributions to both datasets is an arbitrary choice for illustration — with real experimental data, the modeler may choose to give unequal weights based on the relative importance/credibility of the datasets.

We minimized f tot (x) by differential evolution, and found best-fit parameters of K 3 = 5100 μM −1 and K 5 = 0.060 μM −1, which are reasonably close to the ground truth.

To evaluate the strengths of combining quantitative and qualitative data, we performed uncertainty quantification. We used a variant of the profile likelihood approach, a method that is well-established for quantitative fitting. One parameter of interest is held fixed, and the objective function is minimized by varying the remaining parameters. The resulting minimum is taken as the negative log likelihood of the fixed parameter value. The minimization is repeated for many possible fixed values of the parameter to produce a curve (Fig. 2d, e). Note that our objective function is not a likelihood in the rigorous sense, but has a similar interpretation in that a lower objective value indicates the parameter value is more consistent with the data.

We performed profile likelihood analysis using f quant (x), f qual (x), and f tot (x) in turn as the objective function, and considering two free parameters K 3 and K 5 (Fig. 2d, e). We found that each dataset individually provided bounds on possible parameter values, but the tightest bounds were obtained when we combined both datasets.

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^66f1bd1f]. Nature Communications (2023). High credibility.

CurveCurator yields well-calibrated p -values using a recalibrated F-statistic

The first step to assess the statistical significance of dose–response curves is to find the best possible fit given the measured dose–response values. As the optimization surface for sigmoidal curves is non-convex (i.e. a surface with many local minima), naïve curve fitting algorithms often get stuck in local minima, leading to suboptimal fits and, thereby, overly conservative p -values (Supplementary Fig S1). CurveCurator uses a heuristic that reaches the global minimum in almost all cases in a short period of time (Supplementary Fig. S2). To obtain an empirical null distribution, we simulated 5 million dose–response curves under the null hypothesis, i.e. curves where the response is independent of the dose (i.e. no dose-dependent regulation) for a range of n data points per curve (Fig. 2a). As expected, direct application of the classical F-statistic for linear models to these null dose–response curves yielded poorly calibrated p -values (Supplementary Fig. S3). CurveCurator solved this issue by optimizing both the F-value formula and the parameters of the F-distribution as a function of n to approximate these simulated null distributions accurately (Fig. 2 b, c). The validity of this approach was confirmed by noting that p -values in real experiments in which the vast majority of curves were expected to be unresponsive formed a uniform distribution of truly non-regulated curves plus a small distribution of truly regulated curves enriched at low p -values (Fig. 2d).

---

### Optoelectronic system and device integration for quantum-dot light-emitting diode white lighting with computational design framework [^c2cc23b1]. Nature Communications (2022). High credibility.

Methods

Colour optimisation process

To find the optimal combination of peak wavelengths for the N -primary coloured white lighting system, we used a combinatorial colour optimisation process with a cost function for a given spectral radiance of the lighting system. First, the spectral radiance L (λ, Λ) for a set of peak wavelengths Λ = (λ 1, λ 2, …, λ N) of the N -primary white light is defined as a mixture of Gaussian-shaped spectral radiances for the n -th primary colour with peak wavelength λ n, normalised power β n, and its full-width at half maximum (FWHM) Δ λ n. Here, β n = D 65(λ n) for the normalised power distribution of the CIE standard illuminant D65. With the given spectral radiance L (λ, Λ), we calculate a cost function that is a function of the CRIs and chromaticity difference Δ xy according to Eq. (1). In Eq. (1), the colour properties such as CIE xy and CRIs for the spectral radiances are calculated with Colour package which is an open-source library for Python. The factor 1.5 of the first term in Eq. (1) encourages the optimiser to prioritise chromaticity over CRI. With the given cost function, the colour optimisation process simultaneously minimises chromaticity difference while maximising the CRI of the N -primary white lighting system.

In the combinatorial colour optimisation process, we used an iterative procedure with the Nelder-Mead algorithm which finds the local minimum point and value of the given cost function in an N -dimensional variable space of the peak wavelength set Λ. The optimised wavelength set Λ opt is determined to be a wavelength set having the smallest cost function, by the end of the iterative procedure, as described in Eq. (4). Here, arg min f (x m) is the x m that gives smallest f for m = 1, 2, …, N + 1. A programming function of the Nelder-Mead algorithm supported by TensorFlow, which is an open-source Python package for machine learning frameworks, was utilised in this study.

---

### Epistatic net allows the sparse spectral regularization of deep neural networks for inferring fitness functions [^7a3a3502]. Nature Communications (2021). High credibility.

Methods

Notation and background

Suppose we are given n (experimental) samples, that is, (sequence, value) pairs from a biological landscape, where x i ∈ {−1, +1} d denotes the binary encoding of d mutational sites in a variant andis its associated fitness value. We are interested in learning a function f (x) that maps all subsets of mutations to fitness values. In other words, we seek to learn a set function, wheredenotes the space of all the binary vectors of length d. A key theoremin mathematics states that any set function (also known as pseudo-Boolean function) f (x) = f (x 1, x 2, …, x d) can be represented uniquely by a multi-linear polynomial over the hypercube (x 1, x 2, …, x d) ∈ {−1, +1} d :whereis a subset of {1, 2, 3, …, d } = [d] andis the WH transform coefficient (or equivalently the epistatic coefficient) associated with the monomial (interaction). For example, the pseudo-Boolean functiondefined over d = 5 mutational sites, has three monomials with orders 2, 1, and 3 and WH coefficients 12, − 3, and 6, respectively. The WH transform of this function is sparse with k = 3 non-zero coefficients out of a total of 2 5 = 32 coefficients. Each monomial can be easily explained, for example, the first monomial in the WH transform, that is 12 x 1 x 4, indicates that mutation sites 1 and 4 are interacting and the interaction enriches fitness because the sign of the coefficient is positive. On the hand, the second monomial − 3 x 3 shows that a mutation at site 3 depletes fitness. The last monomial 6 x 1 x 2 x 5 shows a third-order interaction between mutational sites 1, 2, and 5 which also enrich fitness.

---

### Using both qualitative and quantitative data in parameter identification for systems biology models [^e33a4be7]. Nature Communications (2018). Medium credibility.

Methods

Polynomial model

A system of linear equations for computing the polynomial coefficients was implemented with the Python Numpy module. We suppose we have n + 1 equally spaced qualitative data points, which divide the range x = [0, 10] into n equally-sized intervals. Exactly two of these intervals will be bounded on one end by a "−" point and on the other by a "+" point; these two intervals must contain the intersection points x 1 and x 2.

To generate Fig. 1b, within the possible intervals for x 1 and x 2, we sampled all possible combinations of 100 (evenly spaced) values of x 1 and 100 values of x 2, and solved for the coefficients. Considering only the fits that yielded positive values for all coefficients, we report the minimum and maximum possible values for each coefficient.

Raf inhibitor model

The model shown in Fig. 2a was implemented in Python (Supplementary Software 1). We generated synthetic data points at various values of I and constant R tot = 50 μM, and populated the other species concentrations based on the equations of the model. To generate quantitative data, we calculated = (RI + RIR + 2RIRI)/ R tot, and perturbed results by adding normally distributed noise with standard deviation 0.1. To generate qualitative data, we calculated A = RR + RIR, and compared the result to the value A (0) = 15.24. If the difference from A (0) was < 1.5, the data point was labeled as "0" (within error), otherwise it was labeled as "+" or "−" appropriately.

For fitting, the objective function was created as described in Results. Minimization was performed with the Scipy function optimize.differential_evolution, with a maximum of 1000 iterations, strategy of "best1exp", and search range of [10 −4, 10 4] for each parameter.

To perform profile likelihood analysis, for each parameter, we considered 100 possible fixed values (log uniformly distributed in the range [10 2, 10 5] for K 3 and [10 −3, 10 0] for K 5). At each fixed value considered, minimization was performed by the same method as above, except that the parameter of interest was held at the fixed value. We report the resulting minimum objective function values in the profile likelihood plots (Fig. 2d, e).

---

### Fractional response analysis reveals logarithmic cytokine responses in cellular populations [^47f7d39e]. Nature Communications (2021). High credibility.

Methods

Software implementation

The methodology to perform and visualize FRA is provided as a user-friendly R-package available for download at. The package contains an installation guide and a brief user manual.

Formal definition of the FRC

Consider a series of doses x 1, …, x i, …, x m and denote a single-cell response as y. Depending on the context, y, may be a number or a vector, e.g. the level of one or more measured signaling effectors. Suppose that responses to a given dose, x i, are represented as the probability distribution, The FRC is then formally defined aswhere integration takes place over, the set of all possible responses, y. The integral quantifies the area under the curve (or under surface for multivariate data), with respect to y, defined asFor the calculations shown in Fig. 2 the integration corresponds to the calculation of the area of the gray regions in c–e. As explained in Supplementary Note 1, the FRC defined as above is closely related Rényi min-information capacity.

Formal definition of typical fractions

Having the responses represented in terms of the probability distribution, Eq. 3, we can define which responses, y, are typical to any of the doses. Precisely, we define the response, y, to be typical for dose x j if it is most likely to arise for this dose, which writes asThe above condition allows assigning any response, y, to a dose for which it is typical. Therefore, for a given dose, x i, we can identify what fraction of cells stimulated with this dose exhibits responses typical to any dose, x j, for j from 1 to m. These fractions, denoted as v ij, can be practically computed as explained below.

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^327fd22c]. Nature Communications (2023). High credibility.

Thresholding

The curve fold change for the i th curve (cfc i) is defined as the log 2 -ratio between the lowest and highest concentration using the regressed model and it quantifies the drug's effect size or efficacy (Eq. 11).

We transferred the SAM principle of differential T-statistics to the recalibrated F-statistic to obtain equivalent decision boundaries for the dose–response curve analyses. This is possible by recognizing that a dose–response curve converges in the limit to two groups (front plateau group = not affected data points, and back plateau group = affected data points), where the curve fold change is equivalent to a conventional SAM fold change between the two plateau groups. In this case, allowing for the conversion and application of the s 0 SAM principle. CurveCurator simplified this process by calculating the tuning parameter s 0 directly from the user-specified significance and fold change asymptotes (Eq. 12). Whereis the inverse cumulative density function of an F-distribution with degrees of freedom dfn and dfd as determined in the section above. This makes s 0 also a function of the number of data points, which is relevant when a curve has missing values.

The tuning parameter s 0, which defines the hyperbolic decision boundaries, can also be used to transform the curve's recalibrated F-value into the s 0 -adjusted F-value (F adj, i) based on the global s 0 value and the curve's measured fold change (cfc i) (Eq. 13).

We then transform this s 0 -adjusted F-value into a "relevance score" using the cumulative density function F X (x | dfn, dfd). For s 0 = 0.0, this simply corresponds to the p -value of the curve. For s 0 > 0.0, this can no longer be interpreted as a p -value, but it still provides an appropriate ordering of curves by both statistical and biological relevance. Additionally, a −log 10 transformation is applied for visualization purposes and to obtain an interpretable score that ranges from 0 to infinity, where 0 has no relevance (Eq. 13).

---

### Statistical analysis and optimality of neural systems [^bf829ef6]. Neuron (2021). Medium credibility.

Normative theories and statistical inference provide complementary approaches for the study of biological systems. A normative theory postulates that organisms have adapted to efficiently solve essential tasks and proceeds to mathematically work out testable consequences of such optimality; parameters that maximize the hypothesized organismal function can be derived ab initio, without reference to experimental data. In contrast, statistical inference focuses on the efficient utilization of data to learn model parameters, without reference to any a priori notion of biological function. Traditionally, these two approaches were developed independently and applied separately. Here, we unify them in a coherent Bayesian framework that embeds a normative theory into a family of maximum-entropy "optimization priors". This family defines a smooth interpolation between a data-rich inference regime and a data-limited prediction regime. Using three neuroscience datasets, we demonstrate that our framework allows one to address fundamental challenges relating to inference in high-dimensional, biological problems.

---

### Local orthorhombic lattice distortions in the paramagnetic tetragonal phase of superconducting NaFeNiAs [^102a523b]. Nature Communications (2018). Medium credibility.

The distribution of d -spacing f is commonly described as a Gaussian function with full-width-at-half-maximum (FWHM), also denoted as Δ d / d in the rest of the paper. Eq. (3) then becomes

In iron pnictides with a nonzero nematic order parameter, due to twinning, f becomes the sum of two Gaussian functions. Assuming that the two Gaussian peaks have identical FWHM, Eq. (4) becomeswhere, r and (1 − r) denotes the relative populations of the two lattice d -spacings a and b, and Δ = 2(a − b)/(a + b) = 2 δ. Therefore, the nematic order parameter can be extracted by fitting P (ϕ tot) using Eq. (5).

When δ is too small to be directly resolved by Larmor diffraction, P (ϕ tot) can be well described by either Eq. (4) or (5). In such cases, we either extract Δ d / d from Eq. (4) (Fig. 4) or extract δ by assuming at T = 50 K, δ = 0 and extract, then fit to Eq. (5) by fixingto this value (Figs. 1e, 3). Measurements of P (ϕ tot) at several different temperatures for NaFe 1− x Ni x As (x = 0.013) are shown in Supplementary Fig. 4, and fit to Eq. (5) as described.

A key feature of Eq. (5) is an oscillation in P (ϕ tot), which can be seen in raw data in Supplementary Fig. 4d–i (open symbols in Fig. 3c); in these cases, the measurement provides definitive evidence of an orthorhombic state. For other panels in Supplementary Fig. 4, due to limited range of ϕ tot, P (ϕ tot) can be equally well described by Eq. (4) (solid symbols in Fig. 3c); for such data, we cannot differentiate between a true splitting and a broadening from measurements done at a single temperature.

---

### Fractional response analysis reveals logarithmic cytokine responses in cellular populations [^db94414a]. Nature Communications (2021). High credibility.

Calculation of the FRC

Calculation of the FRC can be conveniently performed using the typical fractions, as defined above, rather than through integration of Eq. 4. Precisely, to calculate the FRC for the dose, x i, consider doses x 1, …, x i in isolation from higher doses. Then, the sum of typical fractions v 11, …, v ii is equivalent to FRC for the dose x i The equivalency of the above equation and Eq. 4 is derived in the Supplementary Methods.

---

### Rationally reduced libraries for combinatorial pathway optimization minimizing experimental effort [^86ac8beb]. Nature Communications (2016). Medium credibility.

Methods

Computational implementation of RedLibs

RedLibs was developed in the integrated development environment (IDE) XCode (version 4.6) using C ++ as programming language. Parallelization of the algorithm was implemented using OpenMPI (version 1.4.3). RedLibs uses a database of sequence combinations which were generated using the programming language R (version 2.14.1) in the IDE RStudio (version 0.98.501). For the detailed script please refer to Supplementary Software or. Additionally, the source code was deposited under: The input data for RedLibs was externally generated using the RBS Library Calculator in the 'Predict: RBS Library' mode using either six or eight consecutive N bases generating 4,096 or 65,536 sequence TIR pairs. The corresponding RBS sequences including relevant up- and downstream regions for all randomized genes in this work are provided in Supplementary Table 1.

Evaluation of library distributions

RedLibs uses the CDF of each evaluated library and compares it with the cumulative distribution function of the desired target distribution. As a criterion of evaluation the Kolmogorov–Smirnov distance d KS is used which is defined as the maximum absolute difference between the actual cumulative distribution function F actual (x) and the target distribution function F target (x):

where F (x) is derived from a distribution of n numerical values according to the following equation in which H n denotes the number of values smaller than x (ref.):

Consequently distributions with a high degree of resemblance to the target distribution will exhibit a low d KS value and vice versa. For an exemplary illustrative description of the evaluation process please refer to Supplementary Fig. 5.

---

### Seasonal changes of MéLange thickness coincide with Greenland calving dynamics [^1f091f0f]. Nature Communications (2025). High credibility.

For each glacier terminus, we digitized terminus positions using ArcticDEMs on the dates when the data was acquired. For mélange of length 15 km and width 4 km, there were ~15,000,000 data points available. For each data point in a DEM strip, we calculated its distance from terminus and the surface elevation value after applying the elevation offset. After picking specific values for the number of horizontal and vertical bins, we displayed all data points in a density map where surface elevation was plotted as a function of distance from terminus (Fig. 14 in SI). The accuracy from varying the number of bins of density maps ranged from 0.11 to 0.27 m (Fig. 15 in SI). For any specific distance from terminus, we find the elevation value that had the maximum number of data points (Fig. 14 in SI). We then connected these values along the distance from terminus as the representative mélange elevation profiles (solid blue lines in Fig. 7), Z (x). We calculated the maximum mélange elevation within 200 m from the terminus as Z 0. The value Z 0 was further divided by 1 − ρ i / ρ w to obtain the mélange thickness, H 0, which was used for calculating the buttressing force, F / W, based on Eq. (5). In Table 2 in SI, we report the thickness uncertainty arising from ArcticDEM (± 1.06 m), ice (kg/m 3) and water (kg/m 3) densities. The uncertainties in ice and water densities, mélange packing density, and the mélange thickness fed into Eq. (5) to obtain the uncertainty in the buttressing force, F / W.

---

### Evaluation of the evenness score in next-generation sequencing [^1f5f1936]. Journal of Human Genetics (2016). Low credibility.

The evenness score (E) in next-generation sequencing (NGS) quantifies the homogeneity in coverage of the NGS targets. Here I clarify the mathematical description of E, which is 1 minus the integral from 0 to 1 over the cumulative distribution function F(x) of the normalized coverage x, where normalization means division by the mean, and derive a computationally more efficient formula; that is, 1 minus the integral from 0 to 1 over the probability density distribution f(x) times 1-x. An analogous formula for empirical coverage data is provided as well as fast R command line scripts. This new formula allows for a general comparison of E with the coefficient of variation (= standard deviation σ of normalized data) which is the conventional measure of the relative width of a distribution. For symmetrical distributions, including the Gaussian, E can be predicted closely as 1-σ(2)/2⩾E⩾1-σ/2 with σ ≤ 1 owing to normalization and symmetry. In case of the log-normal distribution as a typical representative of positively skewed biological data, the analysis yields E≈exp(-σ*/2) with σ*(2) = ln(σ(2)+1) up to large σ (≤ 3), and E≈1-F(exp(-1)) for very large σ (⩾2.5). In the latter kind of rather uneven coverage, E can provide direct information on the fraction of well-covered targets that is not immediately delivered by the normalized σ. Otherwise, E does not appear to have major advantages over σ or over a simple score exp(-σ) based on it. Actually, exp(-σ) exploits a much larger part of its range for the evaluation of realistic NGS outputs.

---

### Versatile whole-organ / body staining and imaging based on electrolyte-gel properties of biological tissues [^44dc93f3]. Nature Communications (2020). High credibility.

For the c-Fos-labeled whole-brain data, we applied the "3D Find Maxima" function rather than the 3D centroid calculation for 2D Maxima objects adopted in the ClearMap algorithm. The same parameter values were used for the two datasets (MK-801 + or −) of the c-Fos-labeled brains. 0) The original stack covering the whole brain was cropped to prepare 100 partitioned stacks with 10-pixel x – y margins; (1) the "Subtract Background" function (rolling = 1) was applied to the single stack; (2) the stack was expanded three times in the three dimensions (x – y – z); (3) a 3D Minimum filter (radius x = y = z = 1) was applied; (4) a 3D Gaussian blur filter (sigma x = y = z = 1) was applied; (5) the "Set Threshold" function with a manually determined value was applied to eliminate background signals; (6) the "3D Find Maxima"function was applied with a radius size of x = y = z = 2.5 and a noise tolerance of 1. A stack with single pixels of maxima positions and a CSV file of the result table were generated. Every 120 slices, 5 marginal slices were separately calculated and integrated into the single stack generated above; (7) after calculating all 100 stacks, they were resized to the original x – y – z dimensions and then tiled after removing the 10-pixel x – y margins; and (8) the 3D Maximum filter and 3D Gaussian blur filter were finally applied to the stack to adjust the spot size to the labeled nucleus size. It took ~20 h to perform the calculations for ~8 GB of whole-brain data by using a workstation PC (64-bit Windows 10 Pro for Workstations, Intel(R) Xeon(R) E5-2687W v4 @ 3.00 GHz, two processors, 192 GB RAM, and 4× SSD RAID0 storage).

---

### High temperature singlet-based magnetism from hund's rule correlations [^d24c504c]. Nature Communications (2019). High credibility.

The CEF energy hierarchy has not been fine tuned. Perturbation strengths are scaled to set the lowest energy excitation to 10 meV, a round number that roughly matches the lowest k B T N value at which a magnetic transition is observed in U 1- x Th x Sb 2. This assignment gives a total energy scale for crystal field physics that is approximately comparable to room temperature (ΔCEF~ k B T N), as expected for this class of materials, and the associated orbital energies were found to correspond reasonably (within < ~30%) with coarse estimates from density functional theory. The crystal field parameters are listed in the first column of Table 1.

The low temperature ordered moment of M = 1.90 μ B seen by neutron scattering is matched by downward-renormalizing the moment calculated in the mean field model to 62% (see Fig. 4d shading). Within density functional theory (DFT) models, the consideration of itinerant electronic states provides a mechanism to explain most of this discrepancy. In DFT simulations, the spin component of the magnetic moment is enhanced to M S ~2 μ B, larger than the maximal value of M S ~1.4 μ B that we find in the 5 f 2 (J = 4) atomic multiplet picture. This larger DFT spin moment is directly opposed to the orbital magnetic moment, resulting in a smaller overall ordered moment. The ordered moment in the multiplet simulation could alternatively be reduced by strengthening the crystal field, but this is challenging to physically motivate, and has the opposite effect of reducing the spin moment to M S < 1 μ B.

Density functional theory + dynamical mean field theory (DFT + DMFT)

The combination of density functional theory (DFT) and dynamical mean-field theory (DMFT), as implemented in the full-potential linearized augmented plane-wave method, was used to describe the competition between the localized and itinerant nature of 5 f -electron systems. The correlated uranium 5 f electrons were treated dynamically by the DMFT local self-energy, while all other delocalized spd electrons were treated on the DFT level. The vertex corrected one-crossing approximationwas adopted as the impurity solver, in which full atomic interaction matrix was taken into account. The Coulomb interaction U = 4.0 eV and the Hund's coupling J = 0.57 eV were used for the DFT + DMFT calculations.

---

### Comparison of parameter optimization methods for quantitative susceptibility mapping [^26a0417b]. Magnetic Resonance in Medicine (2021). Medium credibility.

Purpose

Quantitative Susceptibility Mapping (QSM) is usually performed by minimizing a functional with data fidelity and regularization terms. A weighting parameter controls the balance between these terms. There is a need for techniques to find the proper balance that avoids artifact propagation and loss of details. Finding the point of maximum curvature in the L-curve is a popular choice, although it is slow, often unreliable when using variational penalties, and has a tendency to yield overregularized results.

Methods

We propose 2 alternative approaches to control the balance between the data fidelity and regularization terms: 1) searching for an inflection point in the log-log domain of the L-curve, and 2) comparing frequency components of QSM reconstructions. We compare these methods against the conventional L-curve and U-curve approaches.

Results

Our methods achieve predicted parameters that are better correlated with RMS error, high-frequency error norm, and structural similarity metric-based parameter optimizations than those obtained with traditional methods. The inflection point yields less overregularization and lower errors than traditional alternatives. The frequency analysis yields more visually appealing results, although with larger RMS error.

Conclusion

Our methods provide a robust parameter optimization framework for variational penalties in QSM reconstruction. The L-curve-based zero-curvature search produced almost optimal results for typical QSM acquisition settings. The frequency analysis method may use a 1.5 to 2.0 correction factor to apply it as a stand-alone method for a wider range of signal-to-noise-ratio settings. This approach may also benefit from fast search algorithms such as the binary search to speed up the process.

---

### Inferring time derivatives including cell growth rates using gaussian processes [^b5444dae]. Nature Communications (2016). Medium credibility.

Methods

Using a Gaussian process to fit time-series data

In the following, we will denote a Gaussian distribution with mean μ and covariance matrix Σ as(μ, Σ) and use the notation of Rasmussen and Williamsas much as possible.

Prior probability

For n data points y i at inputs x i (each x i is a time for a growth curve), we denote the underlying latent function as f (x). We define a covariance matrix k (x, x ′), which has an explicit dependence on hyperparameters θ, and obeys

where the expectations are taken over the distribution of latent functions (samples of f (x)).

We interpret equation (1) as giving the prior probability distribution of the latent functions f (X), where were we use X to denote the inputs x i, such that

where K (X, X) is the n × n matrix with components k (x i, x j). With f denoting [f (x 1). f (x n)], this prior probability can be written as

noting the dependence of k (x, x ′; θ) on the hyperparameters θ.

---

### Fractional response analysis reveals logarithmic cytokine responses in cellular populations [^3e14a7a3]. Nature Communications (2021). High credibility.

Fractional response curves

Outcomes of physiological processes, e.g. of inflammation or stress responses, depend on the number of cells with specific responses, rather than on their mean or median, which constitutes the fraction of cells with a given response as a biologically relevant variable. We proposed, therefore, to quantify dose–responses in terms of cellular fractions and show here how this can be achieved for multivariate data.

We first introduced the fractional response curve (FRC) that quantifies fractions of cells that exhibit different responses to a change in dose, or in fact any other experimental condition. For each subsequent dose, the increase of FRC reflects the fraction of cells that exhibit responses different from lower doses. Adding cumulatively distinct fractions results in counting the number of distinct response distributions.

For an illustration of FRC, in addition to the formal definition derived in Methods, we considered a simple hypothetical example involving one signaling effector and three doses, although the approach extends to a general multivariate scenario. Response distributions to three doses, x 1, x 2, x 3, which can be interpreted as control, intermediate, and high dose, are shown in Fig. 2a. When dose 1 was considered alone, fractions of cells with all possible responses sum up to 1 (Fig. 2b). Therefore, we defined the value of the FRC for dose 1 to be 1, and write r(x 1) = 1. We then asked what fraction of the cellular population exhibits different responses after the change from dose 1 to dose 2. The fraction of cells exhibiting different responses is equivalent to the overall increase in the frequency of responses (Fig. 2c, green region). The overall fractional increase, denoted as ∆ r, is calculated as the area of the green region, and ∆ r = 0.31, represents the 31% of the cellular population exhibiting different responses due to dose increase. Therefore, we defined the value of the FRC for dose 2 to be the sum of the previous value and the fractional increment, r (x 2) = r (x 1) + ∆ r = 1.31. When dose 3 was considered, the fraction of cells that exhibited different responses is again equivalent to the overall increase in the frequency of different responses, now compared with the two lower doses (Fig. 2d). As before, the overall increase, ∆ r, is equivalent to the area of the yellow region (Fig. 2d), with ∆r = 0.74, representing 74% of cells stimulated with dose 3 exhibiting responses different to populations stimulated with lower doses. Again, the value of the FRC for dose 3 was defined as the sum of the previous value and the fractional increment, r(x 3) = r(x 2) + ∆r = 2.05. Changes in the FRC show what fraction of cells exhibit different responses owing to the dose increase. Adding subsequent fractional increments, ∆r, leads to the value of FRC expressed in terms of the cumulative fraction of cells that exhibit different responses due to dose change.

---

### On the UNFOLD method [^2cbaf033]. Magnetic Resonance in Medicine (2002). Low credibility.

A graphical formalism is presented, showing that "UNaliasing by Fourier-encoding the Overlaps Using the temporaL Dimension" (UNFOLD) is equivalent to sampling k-t-space in a sheared grid pattern. Discrete regular sampling in k-t-space leads to periodic replication of the support region in x-f-space. Thus, the maximum acceleration achievable by UNFOLD is equivalent to the maximum packing of support regions in x-f-space. When the support region is separable along the x and f axes, the reconstruction can be performed separately for each k. UNFOLD can be combined with SiMultaneous Acquisition of Spatial Harmonics (SMASH) to further accelerate acquisition. However, a straightforward combination of the methods has been shown to result in a size restriction, which limits the portion of the field of view (FOV) with a larger temporal bandwidth to only a quarter of the FOV. Two solutions are presented to overcome this restriction.

---

### Evolution of new regulatory functions on biophysically realistic fitness landscapes [^e3a06c88]. Nature Communications (2017). Medium credibility.

Methods

We consider mutation rates to be low enough that a beneficial mutation fixes before another beneficial mutation arises, allowing us to assume that the population is almost always captured by a single genotype. The probability that the population occupies a particular genotypic state, evolves according to a continuous-time discrete-space Markov chain. Transition rates between states are a product between the mutation rates between different genotypes and the fixation probabilities that depend on the fitness advantage a mutant has over the ancestral genotype, r xy = 2 Nμ xy Φ y → x, where N is the population size, μ xy is the mutation rate from genotype y to x, and Φ y → x is the probability of fixation of a single copy of x in a population of y. Our model only requires us to keep track of mismatches and not full sequences (i.e. the reduced-genotypes,), which significantly reduces the genotype space dimensionality. This framework allows for calculation of the steady state distribution of genotypes, or reduced-genotypes Eq. (3) and classification of genotypes into relevant macrostates.

To calculate the neutral distribution P 0 of the reduced-genotypes (distribution in the absence of selection), we enumerate the number of possible BS sequences j that have mismatch values (k 1 j, k 2 j) with respect to two TFs that match each other at M out of L consensus positions:The neutral distribution (up to proportionality constant) equalsWe iterate this calculation for various parameter combinations (Ns, ρ, f 1,2). For each, we determine the most probable macrostate at steady state (Supplementary Note 2) as illustrated in Fig. 3.

To determine evolutionary dynamics we numerically integratein time-steps corresponding to one generation t g :where R is the Markov chain transition matrix. Again, at every time-point we determine the most probable macrostate (Supplementary Note 2), as illustrated in Fig. 4.

---

### Determination of the effective local lethal function for the nanOx model [^f3f5ac80]. Radiation Research (2020). Medium credibility.

NanOx is a biophysical model recently developed in the context of hadrontherapy to predict the cell survival probability from ionizing radiation. It postulates that this may be factorized into two independent terms describing the cell response to two classes of biological events that occur in the sequence of an irradiation: the local lethal events that occur at nanometric scale and can by themselves induce cell death, and the non-local lethal events that lead to cell death by an effect of accumulation and/or interaction at a larger scale. Here we address how local lethal events are modeled in terms of the inactivation of undifferentiated nanometric targets via an "effective local lethal function F ", which characterizes the response of each cell line to the spectra of "restricted specific energy". F is initially determined as a linear combination of basis functions. Then, a parametric expression is used to reproduce the function's main features, a threshold and a saturation, while at the same time reducing the number of free parameters. This strategy was applied to three cell lines in response to ions of different type and energy, which allows for benchmarking of the α(LET) curves predicted with both effective local lethal functions against the experimental data.

---

### Identifying domains of applicability of machine learning models for materials science [^72c4ba76]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ν that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by σ lin (x 1, x 2) ≡ −0.3 ≤ x 1 ≤ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: σ rbf ≡ −3.3 ≤ x 11 ≤ 3.1 ∧ x 2 ≤ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA σ poly (x 1, x 2) ≡ −3.5 ≤ x 2 ≤ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### Recovery coupling in multilayer networks [^c0323da7]. Nature Communications (2022). High credibility.

Recovery coupling simulations and phase space

To understand the implications of recovery coupling for multilayer network resilience, we consider the symmetric case in which the network structure, damage, and recovery parameters are the same in both systems. Since the two systems support each other, we let the repair rate of Y be influenced by the state of X in the same manner as Eq. (5): In the symmetric case f x = f y = f, leading to a single equation that governs the state of the system. If the failures are uniformly distributed, we can use percolation theory, to analytically derive the equation that governs the expected fraction of primary failures in the coupled system, where u (x) is the probability that a link does not lead to the largest connected component when a random fraction 1 − x of the nodes are removed, and is determined by the network topology. Equation (6) has one or two stable solutions depending on the value of the control parameter. The non-symmetric case has similar results, as we shown in supplementary note 1 and Supplementary Fig. 1. In contrast, the uncoupled case (2), which we recover from (6) for α = 0, has a single stable solution. The new solution describes a stable fixed point at f = 1 (all nodes failed), which persists even for high recovery rates(see Fig. 4 a). The existence of two stable solutions for f for the same recovery rateindicates that for a wide range of conditions, recovery coupled networks are resilient: they display functionality comparable to the uncoupled case and return to full functionality following small perturbations. However, a sufficiently large perturbation can force the system to cross the unstable branch, pushing it into a dynamically stable non-functional state (Fig. 4 a). This is more likely with correlated perturbations across layers, as we show in Supplementary Fig. 2. The existence of this behavior analytically predicts a "catch 22" phase that follows a sufficiently large disaster: infrastructure system X cannot be repaired because it requires resources from Y, and Y cannot be repaired because it requires resources from X. The fact that the collapsed state persists even for high repair rates and low damage rates predicts that it is harder to bootstrap a broken system than it is to maintain the functionality of one that is damaged but still working. Synthesizing elastic residual curves (Fig. 4 c) like the observations in Fig. 2 d, we find that the full coupling α = 1 reproduces the shape of the curve, while lower values of α do not, providing further evidence that the general deviation from elasticity is consistent with recovery coupling.

---

### Quantitative assessment of full-width at half-maximum and detector energy threshold in X-ray imaging systems [^c65d8c68]. European Journal of Radiology (2024). Medium credibility.

Background

The response function of imaging systems is regularly considered to improve the qualified maps in various fields. More the accuracy of this function, the higher the quality of the images.

Methods

In this study, a distinct analytical relationship between full-width at half-maximum (FWHM) value and detector energy thresholds at distinct tube peak voltage of 100 kV has been addressed in X-ray imaging. The outcomes indicate that the behavior of the function is exponential. The relevant cut-off frequency and summation of point spread function S(PSF) were assessed at large and detailed energy ranges.

Results

A compromise must be made between cut-off frequency and FWHM to determine the optimal model. By detailed energy range, the minimum and maximum of S(PSF) values were revealed at 20 keV and 48 keV, respectively, by 2979 and 3073. Although the maximum value of FWHM occurred at the energy of 48 keV by 224 mm, its minimum value was revealed at 62 keV by 217 mm. Generally, FWHM value converged to 220 mm and S(PSF) to 3026 with small fluctuations. Consequently, there is no need to increase the voltage of the X-ray tube after the energy threshold of 20 keV.

Conclusion

The proposed FWHM function may be used in designing the setup of the imaging parameters in order to reduce the absorbed dose and obtain the final accurate maps using the related mathematical suggestions.

---

### Input-output maps are strongly biased towards simple outputs [^50fca0dd]. Nature Communications (2018). Medium credibility.

Many systems in nature can be described using discrete input-output maps. Without knowing details about a map, there may seem to be no a priori reason to expect that a randomly chosen input would be more likely to generate one output over another. Here, by extending fundamental results from algorithmic information theory, we show instead that for many real-world maps, the a priori probability P(x) that randomly sampled inputs generate a particular output x decays exponentially with the approximate Kolmogorov complexity [Formula: see text] of that output. These input-output maps are biased towards simplicity. We derive an upper bound P(x)≲[Formula: see text], which is tight for most inputs. The constants a and b, as well as many properties of P(x), can be predicted with minimal knowledge of the map. We explore this strong bias towards simple outputs in systems ranging from the folding of RNA secondary structures to systems of coupled ordinary differential equations to a stochastic financial trading model.

---

### Self-activating anti-infection implant [^8ca9255c]. Nature Communications (2021). High credibility.

The Fermi level (E F) was obtained from the work function using Eq. (2): E F = −φ.

The position of the VB maximum (E VB) was obtained from Eq. (3): E VB = E F − X, in which X was obtained from the extrapolation of the onsets in the UPS spectrum.

The CB minimum potential (E CB) was obtained from Eq. (4): E CB = E VB + E BG = E F − X + E BG. Here, the bandgap energy E BG was obtained by Tauc plots.

The CB position of MoS 2 –Ti6 and HA/MoS 2 –Ti6 was determined by the UPS spectra. The work function of MoS 2 –Ti6 and HA/MoS 2 –Ti6 was calculated to be 4.77 and 4.70 eV, respectively, by using the method of a linear approximation to the UPS spectra. The Fermi level of MoS 2 –Ti6 and HA/MoS 2 –Ti6 was calculated to be −4.77 and −4.70 eV, respectively. Next, the E VB level of MoS 2 –Ti6 and HA/MoS 2 –Ti6 was −5.92 and −5.58 eV separately. The average bandgap energy value was 1.73 eV for MoS 2 –Ti6 and 1.38 eV for HA/MoS 2 –Ti6, which was obtained from the Tauc plots. According to Eq. (4), the calculated E CB level of MoS 2 –Ti6 and HA/MoS 2 –Ti6 was −4.19 and −4.20 eV, respectively.

Culturing of bacteria

Gram-positive methicillin-susceptible S. aureus (ATCC 25923), Gram-positive methicillin-resistant S. aureus (MRSA) (43300), Gram-negative E. coli (ATCC 8099), and Gram-negative P. aeruginosa (ATCC 15692) were cultured in a sterile Luria–Bertani (LB) medium (10 g L −1 of back to-tryptone, 10 g L −1 of NaCl, and 5 g L −1 of bacto-yeast extract). The bacterial counts were obtained from the spread plate of different samples. P. aeruginosa was a strictly aerobic Gram-negative bacterium.

---

### Identifiability analysis of second-order systems [^b99ab947]. Nuclear Medicine and Biology (2003). Low credibility.

Models provide a means to represent our understanding of the interaction among the components of a system. Individual instantiations of a system are captured by allowing the model to have parameters that may be different for each instantiation. Included in the model is a representation of the means to observe certain aspects of a system. Given the observations, successful estimation of the parameters requires that the parameters be identifiable. This is an important consideration for potential use of the model in diagnostic medicine or analysis and planning of experiments. Models considered here are typical of those used to describe flows and concentrations associated with ligand-receptor interactions.

---

### A dual role in regulation and toxicity for the disordered N-terminus of the toxin graT [^910277d6]. Nature Communications (2019). High credibility.

Small-angle X-ray scattering

SAXS data were collected at SWING beamline (Soleil Synchrotron, Gif-Sur-Yvette, France). This beamline has a SEC system before the measuring capillary. SEC will remove possible aggregates rendering a very homogeneous sample that will then be directly exposed to X-rays for data collection. All the experiments were performed in 50 mM Tris, pH 8.0, 250 mM NaCl and 2 mM TCEP as running buffer.

Each protein was run through a Shodex KW402.5-4F at 0.2 ml/min. Scattering curves covering a concentration range around the peak were normalized and averaged to obtain the final scattering curve. Rg values were derived from the value of I 0 which were obtained by extrapolating to q = 0 using the Guinier approximation as implemented in ATSAS suite. The molecular weights of the different entities were estimated in a concentration independent way using the I 0, Porod volume and Fisher methods.

The use of the dimensionless Kratky plot ((qR g) 2 I(q)/I(0) vs qR g) is a relatively easy way to show that a protein is completely folded, partially folded or completely unstructured. If a protein is globular it follows Guinier's law I(q)/I(0) = exp (−(qR g) 2 / 3). The corresponding dimensionless Kratky plot is a function f(x) = x 2 exp (−x 2 / 3), with x = qR g > 0 with maximum of 1.104 at qR g = √3. On the other hand, an ideally disordered protein follows Debye's law I(q)/I(0) = 2 (x 2 −1−exp (−x 2))/x 4, with x = qR g > 0. In this case the Kratky plot is described by the function f(x) = 2 (x 2 − 1−exp (−x 2))/x 2 which increases monotonically with an asymptote at f(x) = 2. Experimentally, globular proteins show a very similar normalized Kratky plot with a maximum at (√3; 1.1), while partially unstructured proteins show a maximum shifted to higher values in both axes.

---

### Dimerization deficiency of enigmatic retinitis pigmentosa-linked rhodopsin mutants [^5f32140d]. Nature Communications (2016). Medium credibility.

Analysis of scramblase reconstitution

The analysis is described in detail in Supplementary Notes 2 and 3. Briefly, end-point fluorescence reduction data from scramblase activity assays (that is, extent of fluorescence reduction 400 s after adding dithionite) were obtained for proteoliposomes generated over a range of PPR (grams of protein per mole phospholipid) values. The data were transformed according to the following equation:

where F is the percentage fluorescence reduction for a particular sample 400 s after adding dithionite, F o is the percentage reduction obtained with liposomes (45%, Supplementary Note 2), F max is the maximum percentage reduction observed for samples with high PPR (82.5%; Supplementary Note 2) and p (≥ 1 scramblase) is the probability that a particular vesicle in the ensemble contains at least one functional scramblase. The dependence of p (≥ 1 scramblase) on PPR follows Poisson statistics. Taking into account that a fraction of the vesicles is refractory to reconstitution (Supplementary Note 2), and that the vesicles have a range of sizes that can be described by a Gaussian distribution (mean radius = 88 nm and s.d. σ = 28 nm (Supplementary Figure 4)), p (≥ 1 scramblase) can be written as (Supplementary Note 3, equation (4)):

where α is a fit constant that is inversely proportional to M, the molar mass of the functional scramblase and x is the PPR*, derived from the measured PPR after taking into account the fraction of vesicles that is refractory to reconstitution (PPR✱ = PPR/0.65). Fitting data sets of p (≥ 1 scramblase) versus PPR* (Fig. 4c–f and Supplementary Fig. 5) yields α and hence the molar mass of the functionally reconstituted scramblase (Fig. 4g and Supplementary Table 2), for example, α = 17.22 × 10 −4 mol g −1 nm −2 corresponds to the functional reconstitution of opsin monomers (molar mass 41,700 g mol −1), whereas α = 8.61 × 10 −4 mol g −1 nm −2 corresponds to the functional reconstitution of opsin dimers.

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^cf66bf7c]. Nature Communications (2023). High credibility.

Dose-response curves are key metrics in pharmacology and biology to assess phenotypic or molecular actions of bioactive compounds in a quantitative fashion. Yet, it is often unclear whether or not a measured response significantly differs from a curve without regulation, particularly in high-throughput applications or unstable assays. Treating potency and effect size estimates from random and true curves with the same level of confidence can lead to incorrect hypotheses and issues in training machine learning models. Here, we present CurveCurator, an open-source software that provides reliable dose-response characteristics by computing p-values and false discovery rates based on a recalibrated F-statistic and a target-decoy procedure that considers dataset-specific effect size distributions. The application of CurveCurator to three large-scale datasets enables a systematic drug mode of action analysis and demonstrates its scalable utility across several application areas, facilitated by a performant, interactive dashboard for fast data exploration.

---

### AsymptoticMK: a web-based tool for the asymptotic McDonald-Kreitman test [^355c2e94]. G3 (2017). Low credibility.

Figure 1
A screenshot of the web page for asymptoticMK. After entering values for d and d 0, choosing an input file with binned values for x, p, and p 0, and choosing the x interval to fit, the user can click the Submit button and asymptoticMK will provide its results in a new browser window or tab.

Upon submission of the web form, asymptoticMK conducts its analysis and then opens a results page in a new browser tab, presenting a summary of the input data and the results from the analysis. The first plot on this results page shows binned polymorphism counts, p 0 (x) and p (x), for the submitted data; the second plot shows that same data normalized (i.e. the normalized SFS in the test and reference regions). A third plot shows the calculated empirical α (x) as a function of x, estimated from the input data according to Equation (2). The fourth plot shows the same α (x) data, with the best-fitting model and the asymptotic estimate of α from Equation (3) superimposed upon the data.

Below these plots, the results of the analysis are presented in two tables. The first table provides the coefficients a, b, and (for exponential fits) c of the model yielding the best fit to the data. The second table provides the estimated α asymptotic according to Equation (3), and the upper and lower limits of the 95% C. I. around that estimate, as well as the estimated α from the original nonasymptotic MK test (α original) for comparison (also estimated from all polymorphisms falling within the frequency cutoff interval specified on the input page).

For purposes of automation, the asymptoticMK web service can also be run at the command line using the Linux/Unix curl command. For example, the command

would run asymptoticMK with the given values of d and d 0, the given x cutoff interval, and polymorphism data uploaded from the local file polymorphisms.txt, and would output a simple table of results to the file MK_table.txt. Further documentation on the use of this feature is provided on the asymptoticMK web page.

---

### Competition among networks highlights the power of the weak [^6bd3a4a1]. Nature Communications (2016). Medium credibility.

Methods

Measuring the importance of nodes and networks

We use the eigenvector centrality x k for quantifying the importance of a node k in a network, which can be obtained as an iterative process that sums the centralities of all neighbours of k:

where λ is a constant, x k is the eigenvector centrality of node k and G kj are the components of the adjacency matrix, which could be both binary or weighted. In matrix notation equation (5) reads λ x = Gx so that x can be expressed as a linear combination of the eigenvectors u k of the adjacency matrix G, being λ k the set of the corresponding eigenvalues. Equation (5) can be regarded as an iterative process that begins at t = 0 with a set of initial conditions x 0. Regardless of the values of x 0, the value of x (t) at t →∞ will be proportional to the eigenvector u 1 associated with the largest eigenvalue λ 1. Therefore, the eigenvector centrality is directly obtained from the eigenvector u 1 of the adjacency matrix G, which also holds for weighted adjacency matrices. As explained in the main text, the centrality accumulated by each network is obtained as the fraction of centrality accumulated by their nodes. Finally, we use λ 1 as the measure of the network strength, as it is related to a series of dynamical properties of networks and, in turn, it increases with the number of nodes, links and the average degree of the network.

---

### Multiradionuclide evidence for the solar origin of the cosmic-ray events of ᴀ ᴅ 774 / 5 and 993 / 4 [^7e3c7f71]. Nature Communications (2015). Medium credibility.

Discussion

By knowing the approximate spectral hardness of the solar proton events, the specific yield function of 10 Be as well as the averaged 10 Be flux from the NGRIP, NEEM and WDC ice cores, we can estimate the fluence. More precisely, we applied the multiple of the 10 Be increase factor attributable to the SPEs of774/5 and 993/4 (Figs 2d–f and 3d–f and Table 1) relative to that of SPE05 (X 05), to the fluence spectrum of the latter (spectrum 2 in Fig. 4). Previous computationsinfer that SPE05 caused an annual 10 Be production increase by a factor of about 0.024 under the assumption that complete atmospheric mixing would take place before deposition. This scenario is more realistic than no mixing at all because solar protons mostly would produce 10 Be and other radionuclides in the stratosphere which ensures a homogeneous distribution of the cosmogenic radionuclide signature due to a relatively longer mean residence time as opposed to the troposphere. In comparison, our 10 Be measurements indicate increase factors of 3.4 and 1.2 (Figs 2d–f and 3d–f) implying a multiple X 05 of 141 and 51 for the SPEs of774/5 and 993/4, respectively. With a spectral hardness similar to that of SPE05, we therefore find a F 30 of 2.82 ± 0.25 × 10 10 protons per cm 2 for the SPE(s) of774/5 and of 1.02 ± 0.21 × 10 10 protons per cm 2 for the SPE(s) of993/4 (Fig. 5). In addition, we performed an alternative estimation by multiplying the specific yield function of 14 C (Fig. 1)by a scaled up differential energy spectrum of SPE05 (ref.) which could account for the amount of 14 C produced during the two events. A multiple X 05 of 119 and 68 was then needed to reproduce the same amount of radiocarbon than modelled which indicates a F 30 of 2.38 ± 0.40 × 10 10 and 1.36 ± 0.42 × 10 10 protons per cm 2 for the SPE(s) of774/5 and 993/4, respectively (Fig. 5). We also performed the same calculation but using a different 14 C yield functionin order to assess the dependence of our results on the production rate models considered. We found very similar F 30 estimates of 2.76 ± 0.46 × 10 10 protons per cm 2 (774/5) and of 1.56 ± 0.48 × 10 10 protons per cm 2 (993/4).

---

### Neural substrates of cognitive biases during probabilistic inference [^5bc1690b]. Nature Communications (2016). Medium credibility.

Models parameters

For simulations shown in Figures 6 and 7 we used a wide range of parameters to show the robustness of our results and capture inter-subject variability. For those simulations, the model's behaviour was measured over 100,000 simulated trials to accurately capture the average behaviour for a given set of parameters. For simulations of the original model (Fig. 7a–c), we sampled all combinations of parameters (total four parameters) from the following values with the constraint that q p < 2.5 q d: q p = [0.02, 0.04, 0.06, 0.08, 0.10]; q d = [0.02, 0.04, 0.06, 0.08, 0.10, 0.12]; σ D = [0.05, 0.10, 0.20, 0.30]; σ E = [0.10, 0.20, 0.30]. We limited the range to the above values to avoid extreme biases towards the more rewarding option during the choice session, which occurs when the learning rates are greater than 0.12, or when q p and q d are comparable in magnitude. For simulations of the original model with the additional concave f-I response function for the value-encoding neurons (Fig. 7d–f), we used an f-I curve function that captures the average behavioural data and sampled all combinations of other parameters (total seven parameters) from the following values: q p = [0.02, 0.04, 0.06]; q d = [0.04, 0.06, 0.08, 0.10, 0.12]; σ D = [0.05, 0.10, 0.20, 0.30]; and σ E = [0.05, 0.10, 0.15]. For simulations of the model which has all components of the full model except the modulation of the potentiation rate by reward expectation (Fig. 7g–i), we sampled all combinations of parameters (total eight parameters) from the following values: q p = [0.02, 0.04, 0.06]; q d = [0.04, 0.06, 0.08, 0.10, 0.12]; σ D = [0.05, 0.10, 0.20, 0.30]; σ E = [0.05, 0.10, 0.15]; and p ign = [0.2, 0.4, 0.6, 0.8]. For simulations of the complete model, we fixed q r at 0.1 and sampled all combinations of parameters (total ten parameters) from the following values: q p = [0.02, 0.04, 0.06]; q d = [0.04, 0.06, 0.08, 0.10, 0.12]; σ D = [0.05, 0.15, 0.25]; σ E = [0.05, 0.10, 0.15]; p ign = [0.3, 0.6]; and σ r = [0.1, 0.2, 0.3, 0.4, 0.5].

---

### Universality of quantum phase transitions in the integer and fractional quantum hall regimes [^2d7a66a5]. Nature Communications (2024). High credibility.

Fig. 4
Scaling exponent in the ES regime for ν = 3 + 2/5 to 3 + 3/7 transition.

a Plots of the T -dependence of G x x versus filling factor ν for two FQH states between ν = 3 and ν = 4. The black box marks the region where the ES analysis was carried out. b Fit of ES Eq. (4) (dotted lines) to the G x x data for the transition from ν = 3 + 2/5 and ν = 3 + 3/7. Each set of data points is for a given value of δ ν = ∣ ν – ν c ∣ with ν c = 3.416. The plots deviate from the expected ES behavior at high T (the region is marked with an ellipse). c Plots of T 0 versus δ ν CF. The dotted line is a linear fit to the data (see Eq. (5)). The slope yields the value of γ. The error bars are determined from the least square fits to the data in (b). d Plot of scaled longitudinal conductance G x x / s as a function of scaling parameter s = ∣ δ ν ∣ γ / T for PT between ν = 17/5 and ν = 24/7. The scatter points of different colors are for different values of ∣ δ ν ∣, and the solid black line is fit to Eq. (6).

An independent estimate of γ is obtained by casting Eq. (4) into a single-parameter scaling form:with the the scaling parameter s = ∣ δ ν CF ∣ γ / T. Figure 4 d shows the scaling plots of G x x / s versus s 1/2 for the PT in ES regime from ν = 3 + 2/5 to ν = 3 + 3/7. We find a near-perfect data collapse for all values of δ ν CF in the localized regime with γ ≈ 2.3, providing an independent validation of the universality of γ.

---

### Deriving disease modules from the compressed transcriptional space embedded in a deep autoencoder [^62f8c4b4]. Nature Communications (2020). High credibility.

First layer associated genes colocalized in the interactome

In order to further interpret the different layers and uncover their role in defining disease modules, we proceeded to analyze the relationship between the signature genes of each hidden node. Since cellular function is classically described in terms of biological pathways, or lately has also been abstracted to densely interconnected subregions in the interactome (so-called network communities) we analyzed the parameters in the deepAE and their connection to the global pattern of expressed genes. There are different ways one could potentially interpret parameters in a deepAE. To this end, we created a procedure to associate genes with hidden nodes, which we refer to as light-up. Briefly, a light-up input vector was defined for each hidden node by activating it to the maximum value of the activation function, clamping all other nodes at the same layer deactivated by zero values. Then we forward propagated this input vector through all layers to the output pattern response on the gene expression space ("Methods"). This resulted in a ranked list of genes for each hidden node, identifying which genes were most influenced by the activation of that node. We repeated this procedure for all hidden nodes and layers. In order to test if these lists corresponded to functional units, we analyzed their localization within the PPI network STRING. We hypothesized that genes co-influenced by a hidden node could represent protein patterns involved in the same function. Also, the STRING database captures proteins associated with the same biological function and which are known to be within the same neighborhood of their physical interactome. By first ranking the most influenced genes we systematically analyzed the cutoffs thereby showing whether a gene was considered as associated with the node by powers of two from 100 to 10,000. Next, we calculated the average shortest path distance between these genes within the STRING network, using the harmonic mean distance to include also disconnected genes.

---

### Reliable and computationally efficient maximum-likelihood estimation of "proper" binormal ROC curves [^47db8e38]. Academic Radiology (2007). Low credibility.

Rationale and Objectives

Estimation of ROC curves and their associated indices from experimental data can be problematic, especially in multireader, multicase (MRMC) observer studies. Wilcoxon estimates of area under the curve (AUC) can be strongly biased with categorical data, whereas the conventional binormal ROC curve-fitting model may produce unrealistic fits. The "proper" binormal model (PBM) was introduced by Metz and Pan to provide acceptable fits for both sturdy and problematic datasets, but other investigators found that its first software implementation was numerically unstable in some situations. Therefore, we created an entirely new algorithm to implement the PBM.

Materials and Methods

This paper describes in detail the new PBM curve-fitting algorithm, which was designed to perform successfully in all problematic situations encountered previously. Extensive testing was conducted also on a broad variety of simulated and real datasets. Windows, Linux, and Apple Macintosh OS X versions of the algorithm are available online at http://xray.bsd.uchicago.edu/krl/.

Results

Plots of fitted curves as well as summaries of AUC estimates and their standard errors are reported. The new algorithm never failed to converge and produced good fits for all of the several million datasets on which it was tested. For all but the most problematic datasets, the algorithm also produced very good estimates of AUC standard error. The AUC estimates compared well with Wilcoxon estimates for continuously distributed data and are expected to be superior for categorical data.

Conclusion

This implementation of the PBM is reliable in a wide variety of ROC curve-fitting tasks.

---

### The XZZX surface code [^8f6ac9c8]. Nature Communications (2021). High credibility.

This structured noise model thus leads to two distinct regimes, depending on which failure process is dominant. In the first regime where, we expect that the logical failure rate will decay like. We find this behaviour with systems of a finite size and at high bias where error rates are near to threshold. We evaluate logical failure rates using numerical simulations to demonstrate the behavior that characterises this regime; see Fig. 6 (a). Our data show good agreement with the scaling ansatz. In contrast, our data are not well described by a scaling.

Fig. 6
Sub-threshold scaling of the logical failure rate with the XZZX code.

a Logical failure rateat high bias near to threshold plotted as a function of code distance d. We use a lattice with coprime dimensions d × (d + 1) for d ∈ {7, 9, 11, 13, 15} at bias η = 300, assuming ideal measurements. The data were collected usingiterations of Monte-Carlo (MC) samples for each physical rate sampled and for each lattice dimension used. The physical error rates used are, from the bottom to the top curves in the main plot, p = 0.19, 0.20, 0.21, 0.22 and 0.23. Error bars represent one standard deviation for the Monte-Carlo simulations. The solid lines are a fit of the data to, consistent with Eq. (2), and the dashed lines a fit to, consistent with Eq. (3) where we would expect, see Methods. The data fit the former very well; for the latter, the gradients of the best fit dashed lines, as shown on the inset plot as a function of, give a linear slope of 0.61(3). Because this slope exceeds the value of 0.5, we conclude that the sub-threshold scaling is not consistent with. b Logical failure ratesat modest bias far below threshold plotted as a function of the physical error rate p. The data (markers) were collected at bias η = 3 and coprime d × (d + 1) code dimensions of d ∈ {5, 7, 9, 11, 13, 15} assuming ideal measurements. Data is collected using the Metropolis algorithm and splitting method presented in refs. The solid lines represent the prediction of Eq. (3). The data show very good agreement with the single parameter fitting for all system sizes as p tends to zero.

---

### The three major axes of terrestrial ecosystem function [^0958e1c0]. Nature (2021). Excellent credibility.

In summary, by analysing a consistent set of ecosystem functions across major terrestrial biomes and climate zones, we show that three key axes capture the terrestrial ecosystem functions. The first and most important axis represents maximum productivity and is driven primarily by vegetation structure, followed by mean climate. The second axis is related to water-use strategies, and is driven by vegetation height. The third axis is related to ecosystem carbon-use efficiency; it is controlled by vegetation structure, but shows a gradient related to aridity. We find that the plant functional type concept does not necessarily capture the variability of ecosystem functions, because the majority of plant functional types are evenly distributed along the water-use strategies (PC2) and carbon-use efficiency (PC3) axes. Our approach allows the overall functioning of terrestrial ecosystems to be summarized and offers a way towards the development of metrics of ecosystem multifunctionality — a measure of ecosystem functions as a whole, which is crucial to achieving a comprehensive assessment of the responses of ecosystems to climate and environmental variability, as well as biodiversity losses. The analysis focuses on relatively few critical functions related to carbon, water and energy cycling of ecosystems. To attain a fully comprehensive characterization of the key axes of terrestrial ecosystem functions, more parameters related to nutrient cycling, seed dispersal and chemical defences — among others — should be included. The concept of the key axes of ecosystem functions could be used as a backdrop for the development of land surface models, which might help to improve the predictability of the terrestrial carbon and water cycle in response to future changing climatic and environmental conditions.

---

### A geometric approach to characterize the functional identity of single cells [^69a6825a]. Nature Communications (2018). Medium credibility.

Results

The ACTION framework consists of three major components, shown in Fig. 1: (i) a robust, yet sensitive measure of cell-to-cell similarity, (ii) a geometric approach for identification of primary functions, and (iii) a statistical framework for constructing cell-type-specific TRN. Our framework starts by defining a cell similarity metric that simultaneously suppresses the shared, but highly expressed genes and enhances the signal contributed by preferentially expressed markers. The next component of our method is a geometric approach for identifying primary functions of cells. Each of these primary functions is represented by a corner of the convex hull of points defined within the functional space of cells. We refer to these corners as archetypes and the functional identity of each cell is represented by a convex combination of these archetypes. Finally, ACTION uses a novel method to orthogonalize archetypes, find key marker genes, and assess the significance of each transcriptional factor in mediating the transcriptional phenotype associated with each archetype. Finally, we use this method to construct the characteristic TRN of each cell type. In what follows, we describe, validate, and discuss each component in detail.

Fig. 1
Overview of ACTION. ACTION consists of five main steps: i A biologically inspired metric to capture similarity among cells. ii A geometric approach for identifying the set of primary functions. iii An automated mechanism for identifying the number of primary functions needed to represent all cells. iv An orthogonalization procedure for identifying key markers for each primary function. v A statistical approach for identifying key regulatory elements in the transcriptional regulatory network. These steps are grouped into three main components in the ACTION method that are each discussed in the methods section

Representing functional relationships between single cells

A fundamental component of many methods for identifying cell types is a measure for quantifying the similarity between individual cells. Most prior methods rely on traditional measures, such as linear correlation, which are not specifically targeted towards transcriptomic profiles. In contrast, we define a similarity metric, or formally a kernel, specifically designed for measuring the similarity between single-cell transcriptomes. Our approach is illustrated in Fig. 2 and the mathematical models underlying the metric are described in the Methods section, Computing ACTION metric. In summary, we first adjust the raw transcriptional profiles of cells to remove the effect of universally expressed genes by projecting them onto the orthogonal space relative to the universally expressed profile. We then boost the contribution of cell-type-specific genes using an information theoretic approach. The final similarity is then a weighted inner-product kernel between these adjusted profiles.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^d33c8072]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Quality categories for FVC or FEV1 in adults and children — Grades A–F are defined by the number of acceptable tests and repeatability thresholds. Grade A requires " ≥ 3 acceptable tests with repeatability within 0.150 L for age 2–6, 0.100 L, or 10% of highest value, whichever is greater"; Grade B requires " ≥ 2 acceptable tests with repeatability within 0.150 L for age 2–6, 0.100 L, or 10% of highest value, whichever is greater"; Grade C requires " ≥ 2 acceptable tests with repeatability within 0.200 L for age 2–6, 0.150 L, or 10% of highest value, whichever is greater"; Grade D requires " ≥ 2 acceptable tests with repeatability within 0.250 L for age 2–6, 0.200 L, or 10% of highest value, whichever is greater"; Grade E is "One acceptable test", and Grade F is "No acceptable tests".

---

### A rich conformational palette underlies human ca2.1-channel availability [^4f194867]. Nature Communications (2025). High credibility.

Data analysis

The voltage dependence of channel opening was obtained from the peak tail current at V h = −80 mV and fit to the single Boltzmann function:where V was membrane potential, I tail, max was the maximal I tail, z was the valence, V 0.5 was the half-activation potential, F was the Faraday constant, R was the gas constant, and T was temperature (294 K).

The voltage dependence of fluorescence deflections (Δ F) was obtained from the averaged fluorescence signal during the last 5 ms of the test pulse. Δ F for VSDs III and IV were fit to a Boltzmann function:where Δ F max and Δ F min were the maximal and minimal Δ F asymptotes, respectively.

In the case of the Δ F (V) curve for VSD-I (Fig. 2f), and subsequent fittings of VSDs I, III, and IV at extended holding potentials (Figs. 3f, i & 4a, d, g, j), the sum of two Boltzmann distributions was used:where Δ F total was the total fluorescence change (Δ F max_1 +Δ F max_2 −Δ F min_1 −Δ F min_2) and F 1 was the fractional amplitude of the depolarized fluorescence component [(Δ F max_1 −Δ F min_1)/Δ F total)]. To help define the parameters, only cells with > 1 V h were fit, and the following constraints were placed to reduce the number of free parameters:
Voltage-dependence parameters (V 0.5_1, z 1, V 0.5_2, and z 2) were constrained to be equal across fits of different V h for each cell.
Δ F min_1 was constrained to be equal to Δ F max_2 for each V h, in each cell.

---

### In-plane dielectric constant and conductivity of confined water [^516e5d32]. Nature (2025). Excellent credibility.

Although this phenomenological approximation ultimately gives identical ε // and σ // to our model, it has notable drawbacks. First, as discussed above, it yields unphysically large ε l f // that have no physical meaning. Second, unlike macroscale measurements, in which the MW approximation can be fitted directly to the measured effective dielectric function, our nanoscale spectra still require full-3D numerical simulations to account for the system geometry, such as the AFM tip geometry, the scan height and the nanochannels structure. Moreover, in this framework, extracting σ // depends on either ε l f // or ε h f //, both of which can only be determined from 3D numerical fitting of the low- f or high- f plateaus, respectively. By contrast, in our model, σ // is directly proportional to the cut-off frequency f c through the geometric factor α (equation (5)), which can be estimated analytically, allowing extraction of σ // without further numerical simulations.

---

### Statistical laws of stick-slip friction at mesoscale [^bc26fb1f]. Nature Communications (2023). High credibility.

Fig. 5
Statistics of the local force gradient.

Measured PDF of the local force gradientof the pinning force field. In the plot, the value ofis normalized by k 0. The measurements are made at the same scanning speed U = 100 nm/s, and under the same normal load N = 500 nN, for both the quasi-1D probe (black circles) and 2D probe (red triangles). The error bars show the standard deviation of the black circles. The solid line shows an exponential fit, to all the data with b = 0.14 ± 0.02. The inset shows the PDF P (k) of the normalized dynamic spring constant k / k 0. Source data are provided as a Source Data file.

Source Data.

Under the mean-field approximation, the stick-slip motion of the scanning probe may be envisioned as a result of its center-of-mass (at position x s) moving in a random pinning force field F i (x s) in Eq. (4) under the influence of F e (x 0; x s), where x 0 = U t. The equation of motion of x s (t) can be written aswhere m and γ are, respectively, the effective mass and damping coefficient of the scanning probe, and F i (x s) > 0 is a random pinning force field. Equation (6) is an extension of the PT model, which laid down the foundation for our understanding of stick-slip instabilities. In the original PT model. the external pinning (or frictional) force field F i (x s) was assumed to be of a sinusoidal form for a single crystalline surface. For many rough surfaces of practical interest, however, F i (x s) > 0 is a random force field. Because disorder has many different forms, modeling a realistic random force field that can be compared directly with the experimental results remains a challenging task. Here we show that because the asperity-induced frictional force acting on a mesoscale scanning probe is a running average of the individual pinning forces resulting from a finite number of asperities at the interface, the resulting force field F i (x s) is Brownian correlated, i.e. where D is a measure of the fluctuation amplitude of F i (x s) (see SI Section III.B for more details). The Brownian correlation is a general feature of the random pinning force field and has been used in the Alessandro–Beatrice–Bertotti–Montorsi (ABBM) model, to describe the avalanche dynamics of domain walls in soft magnets.

---

### FEMA: fast and efficient mixed-effects algorithm for large sample whole-brain imaging data [^6d508c39]. Human Brain Mapping (2024). Medium credibility.

3.4 Simulation 4: Type I error rate

On comparing the type I error rate for fixed effects estimation between a standard mixed model solver and FEMA, we observed comparable number of FPs across different threshold values of false positive rates, demonstrating that FEMA did not have any inflation of FPs. We observed comparable values to the expected average number of FPs (see Table 1), and these numbers were similar to the ones reported by fitlmematrix (see Figure S8 for a detailed look at the number of FPs at 5% false‐positive rate threshold).

TABLE 1
Comparison of type I error rate for fixed effects estimation between FEMA and fitlmematrix function from MATLAB.

3.5 Simulation 5: Effect of number of observations

When examining the differences between ground truth and estimated parameters from FEMA and fitlmematrix as a function of number of observations, we found that the fixed effects estimates converged to ML‐based estimates at a few hundred observations (Figure S9), while for the random effects variance parameter estimates, FEMA estimates converged to ML‐based estimates at a few thousand observations (Figure S10).

3.6 Simulation 6: Type I error rate as a function of bin values

On examining the type I error rate, we did not find any influence of the bin value on the number of FPs. On average, for every bin, the number of FPs was comparable and did not exceed the 5% FP rate threshold (Figure S11).

3.7 Empirical application

---

### Identifying domains of applicability of machine learning models for materials science [^196a563d]. Nature Communications (2020). High credibility.

Fig. 1
Domains of applicability of three 2d-models of a noisy third-degree polynomial.

Three different models, linear (top), radial basis function (rbf, center), and polynomial (poly, bottom), are shown approximating the same distribution of two independent features x 1 ~ N (0, 2) and x 2 ~ N (0, 2), and the target property, where N (μ, ϵ 2) denotes a normal distribution with mean μ and standard deviation ϵ. Test points are plotted in 3d plots against the prediction surface of the models (color corresponds to absolute error) where the DA is highlighted in gray. The distributions of individual errors for the DA (gray) and globally (black) are shown in the 2d plots of each panel with the mean error (solid) and the 95th percentile (95 perc./dashed) marked by vertical lines. Note that the global error distribution of the linear model has a considerably long tail, which is capped in the image.

---

### Scalable spatiotemporal prediction with Bayesian neural fields [^fea8687d]. Nature Communications (2024). High credibility.

Prediction queries

We can approximate the posterior (13) using a set of samples, which may be obtained from either MAP ensemble estimation or stochastic variational inference (by sampling from the ensemble of M variational distributions). We can then approximate the posterior-predictive distribution(which marginalizes out the parameters Θ) of Y (r *) at a novel field index r ✱ = (s *, t *) by a mixture model with M equally weighted components:Equipped with Eq. (23), we can directly compute predictive probabilities of events { Y (r *) ≤ y }, predictive probability densities { Y (r *) = y }, or conditional expectationsfor a probe function. Prediction intervals around Y (r *) are estimated by computing the α -quantile y α (r *), which satisfiesFor example, the median estimate is y 0.50 (s *, t *) and 95% prediction interval is [y 0.025 (s *, t *), y 0.975 (s *, t *)]. The quantiles (24) are estimated numerically using Chandrupatla's root finding algorithmon the cumulative distribution function of the mixture (23).

Temporal seasonal features

Including seasonal features (c.f. Eq. (8)), where possible, is often essential for accurate prediction. Example periodic multiples p for datasets with a variety of time units and seasonal components are listed below (Y = Yearly; Q = Quarterly; Mo = Monthly; W = Weekly; D = Daily; H; Hourly; Mi = Minutely; S = Secondly):
Q: Y = 4
Mo: Q = 3, Y = 12
W: Mo = 4.35, Q = 13.045, Y = 52.18
D: W = 7, Mo = 30.44, Q = 91.32, Y = 365.25
H: D = 24, W = 168, Mo = 730.5, Q = 2191.5, Y = 8766
Mi: H = 60, D = 1440, W = 10080, Mo = 43830, Q = 131490, Y = 525960
S: Mi = 60, H = 3600, D = 86400, W = 604800, Mo = 2629800, Q = 7889400, Y = 31557600

---

### The statistical properties of gene-set analysis [^54918f50]. Nature Reviews: Genetics (2016). Medium credibility.

The rapid increase in loci discovered in genome-wide association studies has created a need to understand the biological implications of these results. Gene-set analysis provides a means of gaining such understanding, but the statistical properties of gene-set analysis are not well understood, which compromises our ability to interpret its results. In this Analysis article, we provide an extensive statistical evaluation of the core structure that is inherent to all gene- set analyses and we examine current implementations in available tools. We show which factors affect valid and successful detection of gene sets and which provide a solid foundation for performing and interpreting gene-set analysis.

---

### An exact form for the magnetic field density of States for a dipole [^2c861285]. Magnetic Resonance Imaging (2001). Low credibility.

We present an analytical form for the density of states for a magnetic dipole in the center of a spherical voxel. This analytic form is then used to evaluate the signal decay as a function of echo time for different volume fractions and susceptibilities. The decay can be considered exponential only in a limited interval of time. Otherwise, it has a quadratic dependence on time for short echo times and an oscillatory decaying behavior for long echo times.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^d6d8fcbc]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Spirometry reporting specifies that numerical values are given for the FEV1, the FVC, and the FEV1/FVC ratio; the latter should be reported as a decimal fraction and the space for percent predicted value left blank, and if bronchodilators are given the LLN column need not be repeated with absolute and percent change given only for FEV1 and FVC. Other numerical values such as the forced inspiratory flow at 75% of FVC (FEF75%) and FEF25–75% are not recommended for routine use. Graph requirements include that for the volume–time curve the volume scale should be at least 10 mm/L, the time scale at least 20 mm/s, and 1 second prior to the start of expiration should be displayed; on the flow–volume plot the flow display should be at least 5 l/min/L/s, and the ratio of flow to volume should be 2 L/s to 1 L, and linear and log scales where values are plotted as z-scores relative to the predicted value (z = 0) give an intuitive sense of severity.

---

### Article 5. An introduction to estimation – 2: from z to T [^fb82b806]. Emergency Medicine Journal (2001). Low credibility.

Provided the sample size is large enough (that is, n greater than 100), the z statistic can be used to determine the confidence interval estimation of the population mean even when the sigma is not known. In these cases the estimation of the standard error of the mean is used. The z statistic is also valid when determining the population's proportion based upon a large sample. However, when dealing with smaller samples, the z statistic is replaced by the t statistic. This makes it possible to estimate, in a population with an unknown standard deviation: The probability of getting a sample mean greater than or equal to a particular value The value of a sample mean with a particular probability of occurring The probability of getting a sample mean between two particular values The confidence interval for the estimation of the population mean can also be determined using the t statistic.

---

### Effect of altering breathing frequency on maximum voluntary ventilation in healthy adults [^caf0af7d]. BMC Pulmonary Medicine (2018). Low credibility.

Background

Compared to other pulmonary function tests, there is a lack of standardization regarding how a maximum voluntary ventilation (MVV) maneuver is performed. Specifically, little is known about the variation in breathing frequency (f R) and its potential impact on the accuracy of test results. This study examines the effect of several preselected values for f R and one self-selected f R (f Rself) on MVV.

Methods

Ten participants performed MVV maneuvers at various f R values, ranging from 50 to 130 breaths·min -1 in 10 breaths·min -1 intervals and at one f Rself. Three identical trials with 2-min rest periods were conducted at each f R, and the sequence in which f R was tested was randomized. Ventilation and related parameters were measured directly by gas exchange analysis via a metabolic measurement system.

Results

A third-order polynomial regression analysis showed that MVV = -0.0001(f R) 3 +0.0258(f R) 2–1.38(f R)+96.9 at preselected f R and increased up to approximately 100 breaths·min -1 (r 2 = 0.982, P < 0.001). Paired t-tests indicated that average MVV values obtained at all preselected f R values, but not f Rself, were significantly lower than the average maximum value across all participants. A linear regression analysis revealed that tidal volume (V T) = -2.63(MVV)+300.4 at preselected f R (r 2 = 0.846, P < 0.001); however, this inverse relationship between V T and MVV did not remain true for the self-selected f R. The V T obtained at this f R (90.9 ± 19.1% of maximum) was significantly greater than the V T associated with the most similar MVV value (at a preselected f R of 100 breaths·min -1, 62.0 ± 10.4% of maximum; 95% confidence interval of difference: (17.5, 40.4%), P < 0.001).

Conclusions

This study demonstrates the shortcomings of the current lack of standardization in MVV testing and establishes data-driven recommendations for optimal f R. The true MVV was obtained with a self-selected f R (mean ± SD: 69.9 ± 22.3 breaths·min -1) or within a preselected f R range of 110–120 breaths·min -1. Until a comprehensive reference equation is established, it is advised that MVV be measured directly using these guidelines. If an individual is unable to perform or performs the maneuver poorly at a self-selected f R, ventilating within a mandated f R range of 110–120 breaths·min -1 may also be acceptable.

---

### Standardization of spirometry 2019 update. An official American Thoracic Society and European Respiratory Society technical statement [^e98c921b]. American Journal of Respiratory and Critical Care Medicine (2019). High credibility.

Spirometry reported values — pre- and post-bronchodilator outputs and definitions: "The following measurements are reported separately for the sets of prebronchodilator and post-bronchodilator maneuvers"; "The largest FVC and the largest FEV1, observed from all of the acceptable values are reported… Their ratio is used for FEV1/FVC", and if a bronchodilator is given, both percentage and absolute changes in FEV1 and FVC versus prebronchodilator are reported; reporting change in FEV1 as "a percentage of the predicted FEV1 or as z-scores" avoids sex and height bias; FIVC is the largest inspiratory volume immediately after forced expiration, PEF is the highest flow from a maximum forced expiratory maneuver with the largest value reported from acceptable maneuvers, and FET is "the time in seconds measured from Time 0 to the end of the expiratory plateau or the beginning of inspiration after maximal forced expiration, or the time that the patient comes off the mouthpiece".

---

### Inhibiting peptidylarginine deiminases (PAD1-4) by targeting a cadependent allosteric binding site [^f7c2507c]. Nature Communications (2025). High credibility.

Glutamate dehydrogenase (GDH) enzyme-coupled assay for PAD enzyme activity

The enzyme activity of each of the PAD isoforms was assayed by detection of the product, ammonia, via coupled enzyme detection. GDH catalyzes the conversion of ammonium, α-ketogluterate, NADH, and H + to NAD +, glutamate, and water, which is monitored spectrophotometrically at 340 nm. The PAD selectivity assay conditions were: 100 mM HESPES, pH 7.5, 50 mM NaCl, 0.25 mM CaCl 2, 1 mM substrate BAEE, 2 mM DTT, 0.01% Triton X-100, 40 μ/mL GDH (Sigma, G2626), 8.5 mM α-ketogluterate, and 0.2 mM NADH; the enzyme concentrations used were: 10 nM PAD1, 6 nM PAD2, 20 nM PAD3, or 4 nM PAD4. Serial dilutions of the compound were prepared in DMSO at 100x concentrations, then diluted into buffer to prepare a 4x stock solution immediately prior to assay. Reactions (40 μL) were prepared by combining 10 μL of [4x] BAEE, 10 μL of [4x] compound, and [2x] assay solution that contained all other reagents. Assays were performed in black/clear-bottom 384 well plates (Corning #3544), absorbance (340 nm) data were collected on an EnVision 2103 plate reader (PerkinElmer), and reactions were monitored at 140 s intervals for 50 plate repeats. An apparent molar absorptivity for NADH under the conditions of the assay was determined from a standard solution of NADH to be 4.13 mM -1. IC 50 values were determined via nonlinear regression by fitting the rate data to the equation: f(x) = a/(1 + (x/b)), where f(x) is the measured enzyme rate at inhibitor concentration, x; a is the maximum observed velocity at x = 0 (i.e. in the absence of inhibitor); and b is the IC 50. Enzyme inhibition kinetic data are graphically reported as % inhibition: 100*(1-(f(x)/a)).

---

### Accurate prediction of protein function using statistics-informed graph networks [^9bf198a7]. Nature Communications (2024). High credibility.

Introduction

Proteins bind to other molecules to facilitate nearly all essential biological activities. Consequently, understanding protein function is of paramount importance for comprehending health, disease, evolution, and the functioning of living organisms at the molecular level –. The primary sequence of a protein contains all the essential information required to fold up into a particular three-dimensional shape, thereby determining its activities within cells. The evolutionary information in massive protein sequences that are gleaned from extensive genome sequencing efforts has significantly contributed to recent advances in protein structure prediction –. This evolutionary data, especially the couplings between pairwise residues, has also been utilized to characterize protein functional sites. The evolutionary couplings have been utilized to pinpoint functional sites in proteins, capturing interactions between residues that contribute to specific functions. Indeed, the analysis of evolutionary information has allowed the identification of allosteric mechanisms in proteins, disease variants, and metamorphism in proteins that undergo reversible switches between distinct folds, often accompanied by different functions.

---

### Minimum electric-field gradient coil design: theoretical limits and practical guidelines [^13a4a404]. Magnetic Resonance in Medicine (2021). Medium credibility.

FIGURE 4
A, A region contours of constant Bz for Y gradient coil, with (red) and without (black) E max optimization, showing that gradient distortion is virtually identical despite the peak E‐field being reduced by a factor of 2.8 for the Y coil. B, Difference in the B y components of the gradient field with versus without E‐field optimization. Values are normalized to the gradient strength yielding units of length (mm) and show a highly uniform concomitant y‐directed field added to the optimized (asymmetric) solution. C, Wire pattern representing the difference between optimized (asymmetric) and nonoptimized (symmetric) solutions with same current scaling as Figure 3. Similar results are obtained for the X gradient coil

The key difference between the E‐field minimized and nonminimized solutions is the addition of a uniform concomitant B 0 field component in the x or y direction. Figure 4B shows a difference plot of the concomitant B y field, normalized to gradient strength (units of millimeters), which is responsible for the reduction in E‐field. Figure 4C shows the winding that would need to be added to the Y symmetric coil (Figure 3B) to obtain the winding pattern of the Y asymmetric coil (Figure 3D), and shows the fundamental difference between the two solutions. The winding in Figure 4C produces a uniform field transverse to the main B 0 field, resulting in a concomitant component with additional magnetic stored energy. Conceptually, this can be thought of as a second coil, which if activated, converts the symmetric coil into an asymmetric coil with reduced E‐field but otherwise identical distortion. The added concomitant field in Figure 4B is highly uniform with a 0.85% variation over the 26‐cm imaging region, resulting in minimal distortion of the desired gradient field; this concomitant field can be compensated with a phase/frequency offset correction by the scanner. Figure 4B shows that the average concomitant field of 88.5 mm within the imaging region can be directly compared with the z 0x defined by Meier et al, 29 in which a value of 127 mm was reported for an asymmetric head gradient. The z 0x for both the ESP and HG2 gradients are approximately 120 mm 30; these are both fully asymmetric designs (single eye per half coil with all return currents flowing above the head), similar to the original concept defined by Roemer. 12 The X coil of Figure 3 requires a smaller concomitant field (z 0y = 57.8 mm) than the Y coil, reflecting a decreased need to pull E‐field off the torso region due to the smaller extent of the body in the anterior–posterior direction.

---

### A meta-analysis of meta-analyses [^0aa9b520]. Fertility and Sterility (2011). Low credibility.

Meta-analyses have become an increasingly popular method of drawing conclusions when there are multiple publications addressing a particular topic. While the statistical calculations are straightforward, many of the decisions regarding the research question, populations of interest, and inclusion/exclusion criteria are subjective and have a significant impact on the conclusions. Readers must understand the objective and subjective components of meta-analysis to properly evaluate these types of analyses.

---

### Survival analysis part I: basic concepts and first analyses [^eb4f3e88]. British Journal of Cancer (2003). Low credibility.

TYPES OF 'EVENT' IN CANCER STUDIES

In many medical studies, time to death is the event of interest. However, in cancer, another important measure is the time between response to treatment and recurrence or relapse-free survival time (also called disease-free survival time). It is important to state what the event is and when the period of observation starts and finishes. For example, we may be interested in relapse in the time period between a confirmed response and the first relapse of cancer.

---

### Noise considerations for PET quantification using maximum and peak standardized uptake value [^a18f5da2]. Journal of Nuclear Medicine (2012). Low credibility.

Unlabelled

In tumor response monitoring studies with (18)F-FDG PET, maximum standardized uptake value (SUV(max)) is commonly applied as a quantitative metric. Although it has several advantages due to its simplicity of determination, concerns about the influence of image noise on single-pixel SUV(max) persist. In this study, we measured aspects of bias and reproducibility associated with SUV(max) and the closely related peak SUV (SUV(peak)) using real patient data to provide a realistic noise context.

Methods

List-mode 3-dimensional PET data were acquired for 15 min over a single bed position in twenty (18)F-FDG oncology patients. For each patient, data were sorted so as to form 2 sets of images: respiration-gated images such that each image had statistical quality comparable to a 3 min/bed position scan, and 5 statistically independent (ungated) images of different durations (1, 2, 3, 4, and 5 min). Tumor SUV(max) and SUV(peak) (12-mm-diameter spheric region of interest positioned so as to maximize the enclosed average) were analyzed in terms of reproducibility and bias. The component of reproducibility due to statistical noise (independent of physiologic and other variables) was measured using paired SUVs from 2 comparable respiration-gated images. Bias was measured as a function of scan duration.

Results

Replicate tumor SUV measurements had a within-patient SD of 5.6% ± 0.9% for SUV(max) and 2.5% ± 0.4% for SUV(peak). SUV(max) had average positive biases of 30%, 18%, 12%, 4%, and 5% for the 1-, 2-, 3-, 4-, and 5-min images, respectively. SUV(peak) was also biased but to a lesser extent: 11%, 8%, 5%, 1%, and 4% for the 1-, 2-, 3-, 4-, and 5-min images, respectively.

Conclusion

The advantages of SUV(max) are best exploited when PET images have a high statistical quality. For images with noise properties typically associated with clinical whole-body studies, SUV(peak) provides a slightly more robust alternative for assessing the most metabolically active region of tumor.

---

### A step-by-step guide to the systematic review and meta-analysis of diagnostic and prognostic test accuracy evaluations [^e04c205b]. British Journal of Cancer (2013). Low credibility.

A minimum of two reviewers perform a first-stage screening of titles and abstracts based on the research question and the study design, population, intervention and outcome to be studied. Based on the initial screening, selected full-text articles are obtained for the second-stage screening. Including two reviewers minimises the introduction of bias by either reviewer. Any study identified by either reviewer should be included. Using the full text, a second-stage screening is performed by at least two reviewers. The studies selected are then submitted for data extraction.

Data extraction

Once an appropriate group of studies has been identified, the relevant data from candidate studies must be correctly extracted. To minimise errors, the following conventions should be considered: (1) all reviewers must be trained under a consensus standard and then practice using several articles for 'calibration' (2) a consensus form or database that constrains entries to the expected range should be determined in advance; (3) at least two independent reviewers should check and extract data from a given article, and if the extracted data are not same, conflicts are resolved by reaching a consensus; and (4) to prevent bias creeping into a meta-analysis, the reviewers should not be biased in favor of (or against) well-known researchers or prominent journals as far as possible.

For studies related to biomarkers, the reviewers should also pay attention to the following matters in addition to the preceding items. Among the four types of markers, screening and diagnostic markers mainly focus on sensitivity and specificity, while prognostic and monitoring markers are usually focused on the HR and its variance, SE or CI. Consequently, we will classify the markers into two different categories and describe how to abstract relevant information: (1) if more than one marker is used in a given study, then the relevant data for each eligible marker must be individually extracted; (2) if one marker has multiple functions (i.e. one marker for one disease is used for screening, diagnosis, prognosis and/or monitoring), then the data sets corresponding to multiple functions must be extracted separately; and (3) if there are multiple markers and diseases addressed in one study, then only the relevant data from the marker(s) corresponding to each disease of interest for the author(s) should be extracted.

---

### Two-dome structure in electron-doped iron arsenide superconductors [^1e4df636]. Nature Communications (2012). Medium credibility.

Iron arsenide superconductors based on the material LaFeAsO(1-x)F(x) are characterized by a two-dimensional Fermi surface (FS) consisting of hole and electron pockets yielding structural and antiferromagnetic transitions at x = 0. Electron doping by substituting O(2-) with F(-) suppresses these transitions and gives rise to superconductivity with a maximum T(c) of 26 K at x = 0.1. However, the over-doped region cannot be accessed due to the poor solubility of F(-) above x = 0.2. Here we overcome this problem by doping LaFeAsO with hydrogen. We report the phase diagram of LaFeAsO(1-x)H(x) (x < 0.53) and, in addition to the conventional superconducting dome seen in LaFeAsO(1-x)F(x), we find a second dome in the range 0.21 < x < 0.53, with a maximum T(c) of 36 K at x = 0.3. Density functional theory calculations reveal that the three Fe 3d bands (xy, yz and zx) become degenerate at x = 0.36, whereas the FS nesting is weakened monotonically with x. These results imply that the band degeneracy has an important role to induce high T(c).

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^83b7d887]. Annals of the American Thoracic Society (2023). High credibility.

Applying imaging metrics in computational modeling to predict lung function — modeling supports the acquisition, analysis, and interpretation of images while providing data to aid in the development and refinement of physiological models. Although multiple modalities inform ventilation, perfusion, and gas uptake, no method captures all information simultaneously; the goal is to integrate multimodal data with physiological measurements to characterize how structure determines function. Quantitative linking of structure and function in the lung is stated to be achievable only through computational modeling based on physical laws. The document notes that respiratory diseases are complex and multifactorial, complicating interpretation of imaging metrics and limiting the clinical utility of imaging biomarkers; structure‑based models incorporating tissue, airway, and circulation can facilitate analysis of intersubject differences, protocol effects, and normal or abnormal physiology, and carefully constructed models can examine contributions of factors and provide mechanistic understanding beyond statistical methods alone.

---

### Survival analysis part IV: further concepts and methods in survival analysis [^2b3b3d2a]. British Journal of Cancer (2003). Low credibility.

A good summary of important issues can be found inand, and more details on the statistical methods are given in. In summary, internal validation is necessary before a model is proposed, and external validation is highly recommended before it is to used in clinical practice.

---

### Time efficient design of multi dimensional RF pulses: application of a multi shift CGLS algorithm [^c93b3fd1]. Magnetic Resonance in Medicine (2011). Low credibility.

Designing multi dimensional ratio frequency excitation pulses in the small flip angle regime commonly reduces to the solution of a least squares problem, which requires regularization to be solved numerically. Usually, regularization is carried out by the introduction of a penalty, λ, on the solution norm. In most cases, the optimal regularization parameter is not known a priori and the problem needs to be solved for several values of λ. The optimal value can be selected, typically by plotting the L-curve. In this article, a conjugate gradients-based algorithm is applied to design ratio frequency pulses in a time-efficient way without a priori knowledge of the optimal regularization parameter. The computation time is reduced considerably (by a factor 10 in a typical set up) with respect to the standard conjugate gradients for least square since just one run of the algorithm is required. Simulations are shown and the performance is compared to that of conjugate gradients for least square.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^6e468ce5]. Annals of the American Thoracic Society (2023). High credibility.

General principles of biophysical computational models — biophysical computational models can be used for forward simulations of function or inverse identification of system structure and parameters. Forward models are described as useful for sensitivity analyses to evaluate the contribution of normal subject variability to imaging metrics to define a threshold for abnormal and can also predict emergent behavior after parameter changes at the cellular or tissue level. Inverse models are described as useful for deriving information that cannot be measured directly, including distributions of airway or vascular obstruction that produce specific V˙A/Q˙ patterns. Both forward and inverse models can be used to analyze, interpret, and predict.

---

### Survival analysis part I: basic concepts and first analyses [^d320981a]. British Journal of Cancer (2003). Low credibility.

Introduction

In many cancer studies, the main outcome under assessment is the time to an event of interest. The generic name for the time is survival time, although it may be applied to the time 'survived' from complete remission to relapse or progression as equally as to the time from diagnosis to death. If the event occurred in all individuals, many methods of analysis would be applicable. However, it is usual that at the end of follow-up some of the individuals have not had the event of interest, and thus their true time to event is unknown. Further, survival data are rarely Normally distributed, but are skewed and comprise typically of many early events and relatively few late ones. It is these features of the data that make the special methods called survival analysis necessary.

This paper is the first of a series of four articles that aim to introduce and explain the basic concepts of survival analysis. Most survival analyses in cancer journals use some or all of Kaplan–Meier (KM) plots, logrank tests, and Cox (proportional hazards) regression. We will discuss the background to, and interpretation of, each of these methods but also other approaches to analysis that deserve to be used more often. In this first article, we will present the basic concepts of survival analysis, including how to produce and interpret survival curves, and how to quantify and test survival differences between two or more groups of patients. Future papers in the series cover multivariate analysis and the last paper introduces some more advanced concepts in a brief question and answer format. More detailed accounts of these methods can be found in books written specifically about survival analysis, for example,… In addition, individual references for the methods are presented throughout the series. Several introductory texts also describe the basis of survival analysis, for example, and.

---

### Spin dephasing under nonlinear gradients: implications for imaging and field mapping [^eaf2803c]. Magnetic Resonance in Medicine (2012). Low credibility.

This work examines the prototypical MR echo that would be expected for a voxel of spins evolving in a strong nonlinear field, specifically focusing on the quadratic z(2) - ½(x(2) + y(2)) field. Dephasing under nonlinear gradients is increasingly relevant given the growing interest in nonlinear imaging, and here, we report several notable differences from the linear case. Most notably, in addition to signal loss, intravoxel dephasing under gradients creating a wide and asymmetric frequency distribution across the voxel can cause skewed and nonlinear phase evolution. After presenting the qualitative and analytical origins of this difference, we experimentally demonstrate that neglecting these dynamics can lead to significant errors in sequences that assume phase evolution is proportional to voxel frequency, such as those used for field mapping. Finally, simplifying approximations to the signal equations are presented, which not only provide more intuitive forms of the exact expression but also result in simple rules to predict key features of the nonlinear evolution.

---

### Functional genomics and proteomics: application in neurosciences [^20ce2e35]. Journal of Neurology, Neurosurgery, and Psychiatry (2004). Low credibility.

The sequencing of the complete genome for many organisms, including man, has opened the door to the systematic understanding of how complex structures such as the brain integrate and function, not only in health but also in disease. This blueprint, however, means that the piecemeal analysis regimes of the past are being rapidly superseded by new methods that analyse not just tens of genes or proteins at any one time, but thousands, if not the entire repertoire of a cell population or tissue under investigation. Using the most appropriate method of analysis to maximise the available data therefore becomes vital if a complete picture is to be obtained of how a system or individual cell is affected by a treatment or disease. This review examines what methods are currently available for the large scale analysis of gene and protein expression, and what are their limitations.

---

### Sodium fluoride f 18 [^ca730735]. FDA (2024). Medium credibility.

11.2 Physical Characteristics

Fluoride F 18 decays by positron (β+) emission and has a half-life of 109.7 minutes. Ninety-seven percent of the decay results in emission of the positron with a maximum energy of 633 keV and 3% of the decay results in electron capture with subsequent emission of characteristic X-rays of oxygen. The principal photons useful for diagnostic imaging are the 511 keV gamma photons, resulting from the interaction of the emitted positron with an electron (Table 2). Fluorine F 18 atom decays to stable18O-oxygen.

* Produced by positron annihilation

[3] Kocher, D.C. Radioactive Decay Data Tables DOE/TIC-11026, 69, 1981.

The specific gamma ray constant for fluoride F 18 is 5.7 R/hr/mCi (1.35 x 10-6Gy/hr/kBq) at 1 cm. The half-value layer (HVL) for the 511 keV photons is 4.1 mm lead (Pb). A range of values for the attenuation of radiation results from the interposition of various thickness of Pb. The range of attenuation coefficients for this radionuclide is shown in Table 3. For example, the interposition of an 8.3 mm thickness of Pb with a coefficient of attenuation of 0.25 will decrease the external radiation by 75%.

Table 4 lists the fraction of radioactivity remaining at selected time intervals from the calibration time. This information may be used to correct for physical decay of the radionuclide.

* Calibration time

---

### Single photon emission computed tomography (SPECT) myocardial perfusion imaging guidelines: instrumentation, acquisition, processing, and interpretation [^097e8756]. Journal of Nuclear Cardiology (2018). Medium credibility.

Table 2 — typical performance parameters for low-energy (< 150 keV) parallel-hole collimators — reports resolution (full width at half maximum [FWHM]) at 10 cm, efficiency (%), and relative efficiency: Ultra-high resolution: 6.0 mm, 0.005, 0.3; High resolution: 7.8 mm, 0.010, 0.6; All/general purpose: 9.3 mm, 0.017, 1.0*; High sensitivity: 13.2 mm, 0.035, 2.1. A footnote specifies that a relative efficiency of 1 corresponds approximately to a collimator efficiency of 0.017%, and defines efficiency as the fraction of gamma rays and X-rays passing through the collimator per gamma ray and X-ray emitted by the source.

---

### Casein [^fd0595f4]. FDA (2009). Low credibility.

Volume desired x Concentration desired = Volume needed x Concentration available.

Example 1: If a 1:10 w/v extract is available and it is desired to use a 1:1,000 w/v extract substitute as follows:

Vd x Cd = Vn x Ca

10ml x 0.001 = Vn x 0.1

0.1 mL = Vn

Using a sterile technique, remove 0.10 mL of extract from the 1:10 vial and place it into a vial containing 9.90 mL of sterile diluent. The resulting ratio will be a 10 mL vial of 1:1,000 w/v.

Example 2: If a 10,000 pnu/ml extract is available and it is desired to use a 100 pnu/ml extract substitute as follows:

10ml x 100 = Vn x 10,000

0.1 mL = Vn

Using a sterile technique, remove 0.10 mL of extract from the 10,000 pnu/ml vial and place it into a vial containing 9.90 mL of sterile diluent. The resulting concentration will be a 10 mL vial of 100 pnu/ml.

Example 3: If a 10,000 AU/ml or BAU/ml extract is available and it is desired to use a 100 AU/ml or BAU/ml extract substitute as follows: Vd x Cd = Vn x Ca

---

### ASNC imaging guidelines / SNMMI procedure standard for positron emission tomography (PET) nuclear cardiology procedures [^5ff9bd96]. Journal of Nuclear Cardiology (2016). Medium credibility.

Imaging with 82Rb — useful-count kinetics and timing guidance indicate that about 80% of useful counts are acquired in the first 3 minutes, 95% in the first 5 minutes, and 97% in the first 6 minutes; infusion should be for a maximum of 30 seconds. After dose delivery, patients with normal ventricular function or LVEF > 50% are typically imaged starting 70 to 90 seconds after injection; those with reduced ventricular function or LVEF from 30 to 50% usually begin imaging 90 to 110 seconds after termination of infusion; those with poor function or LVEF < 30% are typically imaged at 110 to 130 seconds.

---

### Perioperative management of antiplatelet and anticoagulant therapy in patients undergoing interventional techniques: 2024 updated guidelines from the American Society of Interventional Pain Physicians (ASIPP) [^bc0e11e1]. Pain Physician (2024). High credibility.

Table 6 — pharmacokinetic and pharmacodynamic comparison of ADP-receptor inhibitors — reports the following values by agent: time to maximum effect is clopidogrel 4 hours to 4 days, prasugrel 1 hour, ticlopidine 3–5 days, ticagrelor 2.5 hours; bioavailability is clopidogrel > 50%, prasugrel ≥ 79%, ticlopidine > 80%, ticagrelor 36%; protein binding is clopidogrel 94–98%, prasugrel active metabolite ~98%, ticlopidine 98%, ticagrelor > 99.7%; plasma half-life is clopidogrel 7–8 hours (inactive metabolite), prasugrel ~7 hours (ranges from 2 hours to 15 hours), ticlopidine 12 hours for a single dose or 4–5 days for repeated dose, ticagrelor 7 hours for ticagrelor and 8.5 hours for active metabolite AR-C124910XX; time to 50% of platelet function recovery is clopidogrel 3 days, prasugrel 3 days, ticlopidine 6 days, ticagrelor 1.5 days.

---

### Interpretation of omics data analyses [^44baad2f]. Journal of Human Genetics (2021). Medium credibility.

Many methods and many tools

There are many data analyses tools available. In addition, multiple combinations of these methods in the form of data processing pipelines have been reported. All of the methods and pipelines return different outputs. These outputs represent aspects of the whole data set, and it is essentially impossible to extract all of the information contained therein. Therefore, all methods and pipelines return their own outputs, which represent their viewpoints, but lack the viewpoints of other methods. In other words, all of the methods and pipelines have advantages and disadvantages. Based on the above-mentioned considerations, the purpose and design of your study should be reconsidered. This will guide your selection and assist interpretation of the results of your analysis.

---

### Proteomics: the first decade and beyond [^2a0bf471]. Nature Genetics (2003). Medium credibility.

Proteomics is the systematic study of the many and diverse properties of proteins in a parallel manner with the aim of providing detailed descriptions of the structure, function and control of biological systems in health and disease. Advances in methods and technologies have catalyzed an expansion of the scope of biological studies from the reductionist biochemical analysis of single proteins to proteome-wide measurements. Proteomics and other complementary analysis methods are essential components of the emerging 'systems biology' approach that seeks to comprehensively describe biological systems through integration of diverse types of data and, in the future, to ultimately allow computational simulations of complex biological systems.

---

### A compendium of human gene functions derived from evolutionary modelling [^cdaff3b8]. Nature (2025). Excellent credibility.

A comprehensive, computable representation of the functional repertoire of all macromolecules encoded within the human genome is a foundational resource for biology and biomedical research. The Gene Ontology Consortium has been working towards this goal by generating a structured body of information about gene functions, which now includes experimental findings reported in more than 175,000 publications for human genes and genes in experimentally tractable model organisms 1,2. Here, we describe the results of a large, international effort to integrate all of these findings to create a representation of human gene functions that is as complete and accurate as possible. Specifically, we apply an expert-curated, explicit evolutionary modelling approach to all human protein-coding genes. This approach integrates available experimental information across families of related genes into models that reconstruct the gain and loss of functional characteristics over evolutionary time. The models and the resulting set of 68,667 integrated gene functions cover approximately 82% of human protein-coding genes. The functional repertoire reveals a marked preponderance of molecular regulatory functions, and the models provide insights into the evolutionary origins of human gene functions. We show that our set of descriptions of functions can improve the widely used genomic technique of Gene Ontology enrichment analysis. The experimental evidence for each functional characteristic is recorded, thereby enabling the scientific community to help review and improve the resource, which we have made publicly available.

---

### American Association of Clinical Endocrinologists / American college of endocrinology clinical practice guidelines for the diagnosis and treatment of postmenopausal osteoporosis-2020 update [^cafea19d]. Endocrine Practice (2020). High credibility.

Bone mineral density categories and fracture risk — osteoporosis is defined by a T-score at or below −2.5 and osteopenia as a T-score between −1.0 and −2.5 based on dual-energy x-ray absorptiometry (DXA), and these criteria are useful for classification and risk stratification but are not intended as treatment thresholds. More than 80% of fragility fractures occur in women with BMD in the "osteopenia" range, and it is now recommended that treatment decisions include consideration of fracture probability; BMD results should be combined with other clinical risk factors, and FRAX® integrates the contribution of BMD and other clinical risk factors and calculates an individual's probability of fracture over 10 years.

---

### Standardization of spirometry 2019 update. An official American Thoracic Society and European Respiratory Society technical statement [^7290ab6b]. American Journal of Respiratory and Critical Care Medicine (2019). High credibility.

Standardization of spirometry — display and digitization requirements specify acquisition, graph scaling, and start-of-test display. For digitization of the flow or volume signal, the sampling rate must be ≥ 100 Hz with a minimum resolution of 12 bits. For the flow–volume graph, expiratory flow must be plotted upward and expiratory volume toward the right, and a 2:1 aspect ratio must be maintained so that 2 L/s of flow and 1 L of volume are the same distance on their respective axes. For the start of test display, the volume–time graph must begin at the point of maximum inspiration or 1 second before Time 0, whichever occurs first, and should continue to the end of the plateau or the beginning of inspiration. Displays of flow versus volume provide more detail than volume–time graphs for the first 1 second of the FVC maneuver, whereas volume–time graphs provide more detail for the latter part of the maneuver.

---

### A systematic evaluation of single cell RNA-seq analysis pipelines [^c02edfbc]. Nature Communications (2019). High credibility.

Finally, we want to remark that paying attention to the details in a computational pipeline and in particular to normalisation pays off. The effect of using a good pipeline as compared to a naively compiled one has a similar or even greater effect on the potential to detect a biological signal in scRNA-seq data as an increase in cell numbers from 96 to 384 cells per group (Fig. 5b).

---

### A comparison of phase II study strategies [^ebb760ae]. Clinical Cancer Research (2009). Low credibility.

The traditional oncology drug development paradigm of single arm phase II studies followed by a randomized phase III study has limitations for modern oncology drug development. Interpretation of single arm phase II study results is difficult when a new drug is used in combination with other agents or when progression-free survival is used as the endpoint rather than tumor shrinkage. Randomized phase II studies are more informative for these objectives but increase both the number of patients and time required to determine the value of a new experimental agent. In this article, we compare different phase II study strategies to determine the most efficient drug development path in terms of number of patients and length of time to conclusion of drug efficacy on overall survival.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^6517cbfd]. Annals of the American Thoracic Society (2023). High credibility.

Basic stereological measurements — first-order parameters and probes identify that "First-order parameters are volume (three-dimensional [3D]), surface (two-dimensional [2D]), length (one-dimensional [1D]), and number (zero-dimensional [0D])". Measurements "are performed using geometric test probes, such as points (0D), lines (1D), planes (2D), or volumes (3D)". These probes create countable events in the image, and "Raw counts provide ratios… that are multiplied by the reference space volume to obtain absolute measures for the lung or subcompartment".

---

### Evaluation after a first seizure in adults [^b94f7748]. American Family Physician (2022). High credibility.

Regarding diagnostic investigations for first seizure in adults, more specifically with respect to initial assessment, AAFP 2022 guidelines recommend to evaluate for provoking factors after a first seizure, such as inflammatory, infectious, structural, toxic, or metabolic causes.

---

### A spectral method for assessing and combining multiple data visualizations [^d018c0dc]. Nature Communications (2023). High credibility.

Visualizing cell cycles

Our second real data example concerns visualization of a different low-dimensional structure, namely, a mixture of cycle and clusters, contained in the gene expression profile of a collection of mouse embryonic stem cells, as a result of the cell cycle mechanism. The cell cycle, or cell-division cycle, is the series of events that take place in a cell that cause it to divide into two daughter cells. Identifying the cell cycle stages of individual cells analyzed during development is important for understanding its wide-ranging effects on cellular physiology and gene expression profiles. Specifically, we consider a dataset containing n = 288 mouse embryonic stem cells, whose underlying cell cycle stages were determined using flow cytometry sorting. Among them, one-third (96) of the cells are in the G1 stage, one-third in the S stage, and the rest in the G2M stage. The raw count data were preprocessed and normalized, leading to a dataset consisting of standardized expression levels of 1147 cell-cycle-related genes for the 288 cells (Methods).

---

### The snm procedure guideline for general imaging 6.0 [^773d53ba]. SNMMI (2010). Medium credibility.

Gated imaging — electrocardiogram (ECG) gating is used to synchronize image acquisition with the patient's heart rate, and the number of frames per R-R interval should be no less than 16 for ejection fraction measurements and 32 for time-based measurements. For gated cardiac blood pool imaging, electronic zoom generally should be used to magnify the field of view to approximately 25 cm, a matrix size of 64 x 64 is sufficient, and typically a total of at least 5 million counts in the entire study will provide sufficient statistics for quantitative and functional image processing.

---

### Osteoporosis in men: an endocrine society clinical practice guideline [^2893c535]. The Journal of Clinical Endocrinology and Metabolism (2012). Medium credibility.

Osteoporosis in men — treatment decision criteria and risk thresholds state that men who have suffered fragility hip or clinical vertebral fractures are at high risk of additional fractures and should be considered for pharmacological treatment, a T-score of −2.5 or below in the spine, femoral neck, or total hip (using the young male reference range) should also be a factor in the decision to treat, and we recommend the use of FRAX or Garvan or another risk assessment tool in men who have not sustained a fragility fracture and in whom the T-score is between −1.0 and −2.5, and, at least in the United States, to recommend pharmacological therapy for men who have a 10-yr risk of greater than 3% for hip fracture or at least 20% for major osteoporosis-related fracture using FRAX.

---

### New designs for phase 2 clinical trials [^a90544dc]. Blood (2003). Low credibility.

Conventional phase 2 clinical trials are typically single-arm experiments, with outcome characterized by one binary "response" variable. Clinical investigators are poorly served by such conventional methodology. We contend that phase 2 trials are inherently comparative, with the results of the comparison determining whether to conduct a subsequent phase 3 trial. When different treatments are studied in separate single-arm trials, actual differences between response rates associated with the treatments, "treatment effects", are confounded with differences between the trials, "trial effects". Thus, it is impossible to estimate either effect separately. Consequently, when the results of separate single-arm trials of different treatments are compared, an apparent treatment difference may be due to a trial effect. Conversely, the apparent absence of a treatment effect may be due to an actual treatment effect being cancelled out by a trial effect. Because selection involves comparison, single-arm phase 2 trials thus fail to provide a reliable means for selecting which therapies to investigate in phase 3. Moreover, reducing complex clinical phenomena, including both adverse and desirable events, to a single outcome wastes important information. Consequently, conventional phase 2 designs are inefficient and unreliable. Given the limited number of patients available for phase 2 trials and the increasing number of new therapies that must be evaluated, it is critically important to conduct these trials efficiently. These concerns motivated the development of a general paradigm for randomized selection trials evaluating several therapies based on multiple outcomes. Three illustrative applications of trials using this approach are presented.

---

### Standards of care in diabetes – 2025 [^5eb43e21]. Diabetes Care (2025). High credibility.

Regarding follow-up and surveillance for diabetes mellitus type 1, more specifically with respect to continuous glucose monitoring, ADA 2025 guidelines recommend to implement early continuous glucose monitoring in adult patients with T1DM to improve glycemic outcomes and QoL and minimize hypoglycemia.

---

### The 2020 Canadian Cardiovascular Society / Canadian Heart Rhythm Society comprehensive guidelines for the management of atrial fibrillation [^a62807cc]. The Canadian Journal of Cardiology (2020). High credibility.

Regarding diagnostic investigations for atrial fibrillation, more specifically with respect to initial evaluation, CCS/CHRS 2020 guidelines recommend to identify and treat reversible and secondary causes of AF.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^681fd2f9]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Using reference data in interpretation — ATS/ERS recommend using limits of normal, with thresholds set to the fifth percentile (z-score −1.645) so that 95% of a healthy population falls within the normal range; use the upper limit where appropriate such as for lung volumes.

---

### Practice parameter update: the care of the patient with amyotrophic lateral sclerosis: drug, nutritional, and respiratory therapies (an evidence-based review): report of the quality standards subcommittee of the American Academy of Neurology [^6ea658d8]. Neurology (2009). Medium credibility.

Regarding diagnostic investigations for amyotrophic lateral sclerosis, more specifically with respect to evaluation of respiratory function, AAN 2009 guidelines recommend to consider measuring supine FVC and maximal inspiratory pressures, in addition to the erect FVC, as part of the routine respiratory monitoring of patients with ALS.