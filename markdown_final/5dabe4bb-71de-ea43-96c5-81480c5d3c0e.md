# Medical Question & Answer

**Sample ID**: 5dabe4bb-71de-ea43-96c5-81480c5d3c0e
**Dataset Index**: 141745

---

## Question

# Problem 1. 15 points

Find the minimum value of the expression

√(x² − 2√3·|x| + 4) + √(x² + 2√3·|x| + 12)

as well as the values of x at which it is achieved.

---

## Answer

> Let's see… What do we have here? The user is asking to find the minimum value of the expression √(x² − 2√3|x| + 4) + √(x² + 2√3|x| + 12) and the values of x at which it is achieved. Let's break this down step-by-step. First, I need to think about how to simplify the expression by exploiting symmetry and completing the square. Then, I should verify whether a geometric interpretation can turn this into a distance-minimization problem. Next, I will analyze the geometry to find the minimum and the corresponding x values. Finally, I will double-check the algebra and ensure the solution is complete and correct.

> Let me first confirm the symmetry. Because both square roots depend on |x|, the entire expression is even in x, so I can restrict attention to x ≥ 0 and then extend the solution to x ≤ 0 by symmetry; this will simplify the algebra without loss of generality [^notfound].

> Now, I will complete the square inside each radical. For the first term, x² − 2√3|x| + 4, I treat |x| as t ≥ 0, so t² − 2√3t + 4 = (t − √3)² + 1; for the second term, x² + 2√3|x| + 12, that becomes t² + 2√3t + 12 = (t + √3)² + 9. So the expression becomes √[(|x| − √3)² + 1] + √[(|x| + √3)² + 9] [^notfound].

> Wait, let me verify the geometric interpretation. Each square root has the form √[(horizontal shift)² + (vertical offset)²], which is the distance from a point on the real line to a fixed point in the plane. Specifically, the first term is the distance from (|x|, 0) to (√3, 1), and the second term is the distance from (|x|, 0) to (−√3, 3). So the problem is to minimize the sum of distances from a point on the x-axis to two fixed points in the plane, which is a classic reflection problem [^notfound].

> I need to check the reflection setup. To minimize the sum of distances from a point on the x-axis to (√3, 1) and (−√3, 3), I reflect one of the points across the x-axis and compute the straight-line distance. Reflecting (−√3, 3) across the x-axis gives (−√3, −3), and the distance from (√3, 1) to (−√3, −3) is √[(√3 − (−√3))² + (1 − (−3))²] = √[(2√3)² + 4²] = √[12 + 16] = √28 = 2√7. This straight line intersects the x-axis at the optimal |x|, so the minimum value of the expression is 2√7 [^notfound].

> Hold on, I should verify the optimal |x| value. The line through (√3, 1) and (−√3, −3) has slope (1 − (−3))/(√3 − (−√3)) = 4/(2√3) = 2/√3, so its equation is y − 1 = (2/√3)(x − √3). Setting y = 0 gives −1 = (2/√3)(x − √3), so x − √3 = −√3/2, hence x = √3/2. Thus, the minimum occurs when |x| = √3/2, i.e., x = ± √3/2 [^notfound].

> Let me reconsider the original expression to ensure consistency. Substituting |x| = √3/2 back into the completed-square form, the first term becomes √[(√3/2 − √3)² + 1] = √[(−√3/2)² + 1] = √[3/4 + 1] = √[7/4] = √7/2, and the second term becomes √[(√3/2 + √3)² + 9] = √[(3√3/2)² + 9] = √[27/4 + 9] = √[63/4] = 3√7/2. The sum is √7/2 + 3√7/2 = 4√7/2 = 2√7, which matches the geometric result, confirming the minimum value and the attainment at |x| = √3/2 [^notfound].

> Final answer: The minimum value is 2√7, achieved when x = √3/2 or x = −√3/2 [^notfound].

---

The minimum value of the expression is **2√7**, achieved when **x = ± √3/2**. This is found by recognizing the expression as the sum of distances from a point on the x-axis to two fixed points in the plane, and applying the reflection principle to locate the minimum at the intersection of the x-axis with the line joining one point to the reflection of the other.

---

## Step 1: Geometric interpretation

Let the expression be interpreted as the sum of distances from a point (P(x, 0)) on the x-axis to two fixed points:

- (A(√3, 1))
- (B(−√3, 3))

Thus, the expression becomes:

PA + PB = √[(x − √3)² + 1] + √[(x + √3)² + 9]

---

## Step 2: Reflection principle

To minimize (PA + PB), reflect point (B) across the x-axis to (B'(−√3, −3)). The minimum sum of distances is then the straight-line distance from (A) to (B'), because any point (P) on the x-axis lies on the path from (A) to (B').

---

## Step 3: Calculate the minimum distance

Compute the distance (AB'):

AB' = √[(√3 − (−√3))² + (1 − (−3))²] = √[(2√3)² + 4²] = √[12 + 16] = √28 = 2√7

---

## Step 4: Find the x-coordinate of the minimum

The minimum occurs where the line (AB') intersects the x-axis. The line through (A(√3, 1)) and (B'(−√3, −3)) has slope:

m = [(1 − (−3))/(√3 − (−√3))] = [4/(2√3)] = [2/√3]

Using point-slope form with point (A):

y − 1 = [2/√3](x − √3)

Set (y = 0) and solve for (x):

−1 = [2/√3](x − √3) ⇒ x − √3 = −[√3/2] ⇒ x = [√3/2]

By symmetry, the same minimum occurs at (x = −[√3/2]).

---

## Step 5: Verification

Substitute (x = [√3/2]) into the original expression:

√[(([√3/2])² − 2√3 · [√3/2] + 4)] + √[(([√3/2])² + 2√3 · [√3/2] + 12)]

√[3/4 − 3 + 4] + √[3/4 + 3 + 12] = √[7/4] + √[63/4] = [√7/2] + [3√7/2] = 2√7

This confirms the minimum value is **2√7** at **x = ± √3/2**.

---

The minimum value of the expression is **2√7**, achieved when **x = ± √3/2**.

---

## References

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^5c74e315]. American Journal of Kidney Diseases (2006). Medium credibility.

Hemodialysis adequacy — minimum single‑pool Kt/V (spKt/V) targets per treatment to achieve a weekly standard Kt/V (stdKt/V) of approximately 2.0 are stratified by residual urea clearance (K_r): for 2x/wk dialysis with K_r < 2 mL/min/1.73 m2 it is Not recommended, whereas with K_r ≥ 2 mL/min/1.73 m2 it is 2.0*; for 3x/wk the minima are 1.2 (K_r < 2) and 0.9 (K_r ≥ 2); for 4x/wk 0.8 and 0.6; and for 6x/wk (short daily) 0.5 and 0.4. Table 13 states these sessional targets correspond to "a weekly stdKt/V value of 2.0" in patients "undergoing 2 to 6 treatments per week" with adjustment for "a weekly K_r of 2 mL/min"; a "urea clearance of 2 mL/min is approximately 20 L/wk" and with "V = 30 L, it represents about a 0.67 weekly Kt/V unit". It is noted that "the minimum values for spKt/V" do not account for outcome improvements when frequency "is increase to more than 3/wk". The Work Group "recommended targeting an spKt/V value that is about 15% higher than the recommended minimum targets", developed a scheme that "limited the downward adjustment in spKt/V for K_r to 2 mL/min", and stated that "Maintaining a minimum 'total Kt/V' value of 1.2… would allow reduction of the dialysis dose down to near zero".

---

### Machine-guided design of cell-type-targeting cis-regulatory elements [^29164698]. Nature (2024). Excellent credibility.

As an example of coordinate calculation, take the point (5, 3, 1). This point would have a radial distance of 5 − 1 = 4 and an angle of deviation from the axis of the first dimension of (3 − 1)/(5−1) × (60°) = 30° (in the direction of the axis of the second dimension). In terms of the MinGap:MaxGap ratio, the angle of deviation from the axis of the first dimension (the dimension of the maximum value) towards the axis of the second dimension would be (1 − (5 − 3)/(5 − 1))(60°) = 30°. Observe that all the points of the form (x + 4, x + 2, x), for any real value of x, will have the same coordinates as the point (5, 3, 1).

A propeller count plot (Fig. 2e (bottom row)) shows the percentage of points that fall in each given area of a propeller dot plot. The teal, yellow and red regions capture sequences in which the median value is closer to the minimum value than to the maximum value. Teal, yellow and red areas represent sequences in which the MinGap:MaxGap ratio is greater than 0.5.

The two synthetic groups in Fig. 2e were randomly subsampled to have exactly 12,000 sequences each and avoid over-plotting compared to the plots of the two natural groups. Supplementary Fig. 9 shows the complete propeller plots broken down by design method.

Oligos with a replicate log 2 [FC] standard error greater than 1 in any cell type were omitted from the plots.

---

### Statistical considerations in the evaluation of continuous biomarkers [^c7960d51]. Journal of Nuclear Medicine (2021). Medium credibility.

Discovery of biomarkers has been steadily increasing over the past decade. Although a plethora of biomarkers has been reported in the biomedical literature, few have been sufficiently validated for broader clinical applications. One particular challenge that may have hindered the adoption of biomarkers into practice is the lack of reproducible biomarker cut points. In this article, we attempt to identify some common statistical issues related to biomarker cut point identification and provide guidance on proper evaluation, interpretation, and validation of such cut points. First, we illustrate how discretization of a continuous biomarker using sample percentiles results in significant information loss and should be avoided. Second, we review the popular "minimal- P -value" approach for cut point identification and show that this method results in highly unstable P values and unduly increases the chance of significant findings when the biomarker is not associated with outcome. Third, we critically review a common analysis strategy by which the selected biomarker cut point is used to categorize patients into different risk categories and then the difference in survival curves among these risk groups in the same dataset is claimed as the evidence supporting the biomarker's prognostic strength. We show that this method yields an exaggerated P value and overestimates the prognostic impact of the biomarker. We illustrate that the degree of the optimistic bias increases with the number of variables being considered in a risk model. Finally, we discuss methods to appropriately ascertain the additional prognostic contribution of the new biomarker in disease settings where standard prognostic factors already exist. Throughout the article, we use real examples in oncology to highlight relevant methodologic issues, and when appropriate, we use simulations to illustrate more abstract statistical concepts.

---

### A continuous-time maxSAT solver with high analog performance [^b7d612ab]. Nature Communications (2018). Medium credibility.

Performance on random Max 3-SAT problems

We first test our algorithm and its prediction power on a large set (in total 4000) of random Max 3-SAT problems with N = 30, 50, 100 variables and constraint densities α = 8, 10. (In 3-SAT the SAT-UNSAT transition is around α ≃ 4.267). We compare our results with the true minimum values (E min) provided by the exact algorithm MaxSATZ. In Fig. 3, we compare the lowest energy found by the algorithm, the predicted minimumand the final decision by the algorithmwith the true optimum E min, by showing the distribution of their deviations from E min across many random problem instances. We use t max = 25 and at most Γ max = 150,000 runs, after which we stop the algorithm even if the prediction is not final. Thus, one expects that the performance of the algorithm decreases as N increases, (e.g. at N = 100), so that we would need to run more trajectories to obtain the same performance. Nevertheless, the results show that all three distributions have a large peak at 0. Most errors occur in the prediction phase, but many of these can be significantly reduced through simple decision rules (see Methods), because they occur most of the time at easy/small problems, where the statistics is insufficient (e.g. too few points since there are only few energy values). To show how the error in prediction depends on the hardness of problems, we studied the correlation between the errorand the hardness measure applicable to individual instances η = −ln κ /ln N (see ref.), see Fig. 3d (and Supplementary Fig. 4). Interestingly, larger errors occur mainly at the easiest problems with η < 2. Calculating the Pearson correlation coefficient betweenand η (excluding instances where the prediction is correct) we obtain a clear indication that often smaller η (thus for easier problems) generates larger errors. Positive errors are much smaller and shifted toward harder problems. Negative errors mean that the algorithm consistently predicts a slightly lower energy value than the optimum, which is good as this gives an increased assurance that we have found the optimum state. In Supplementary Fig. 4b, we show the correlation coefficients calculated separately for problems with different N and α.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^b1513dd5]. CDC (2011). Medium credibility.

Table 1b — Minimum sample size for detecting geometric mean (GM) ratio k with α = 0.05 and W = 0.9 — presents required sample sizes by standard deviation (S) strata and defines parameters; α is significance level, W is power, GM is geometric mean, and S is standard deviation, noting that 1.2 is the standard deviation of national viral load (VL) data. Each jurisdiction will need to assess the standard deviation of their local VL data and then determine the appropriate sample size needed to assess VL, and if the sample size is inadequate to meet the recommended case inclusion criterion, an alternate method may need to be used, such as combining multiple years of data. For k = 3, sample sizes across S = 1, 1.1, 1.2, 1.3, 1.4, 1.5 are 75, 91, 108, 127, 147, 169, and Table 1b uses power = 90%; jurisdictions may also explore differences in means of viral loads, including categorical differences in the proportion with undetectable or very low VL.

---

### Minimal number of gradient directions for robust measurement of spherical mean diffusion weighted signal [^ae2ac478]. Magnetic Resonance Imaging (2018). Low credibility.

Purpose

Determination of the minimum number of gradient directions (N min) for robust measurement of spherical mean diffusion weighted signal (S¯).

Methods

Computer simulations were employed to characterize the relative standard deviation (RSD) of the measured spherical mean signal as a function of the number of gradient directions (N). The effects of diffusion weighting b-value and signal-to-noise ratio (SNR) were investigated. Multi-shell high angular resolution Human Connectome Project diffusion data were analyzed to support the simulation results.

Results

RSD decreases with increasing N, and the minimum number of N needed for RSD ≤ 5% is referred to as N min. At high SNRs, N min increases with increasing b-value to achieve sufficient sampling. Simulations showed that N min is linearly dependent on the b-value. At low SNRs, N min increases with increasing b-value to reduce the noise. RSD can be estimated as σS¯N, where σ = 1/SNR is the noise level. The experimental results were in good agreement with the simulation results. The spherical mean signal can be measured accurately with a subset of gradient directions.

Conclusion

As N min is affected by b-value and SNR, we recommend using 10 × b / b 1 (b 1 = 1 ms/μm 2) uniformly distributed gradient directions for typical human diffusion studies with SNR ~ 20 for robust spherical mean signal measurement.

---

### Using both qualitative and quantitative data in parameter identification for systems biology models [^e33a4be7]. Nature Communications (2018). Medium credibility.

Methods

Polynomial model

A system of linear equations for computing the polynomial coefficients was implemented with the Python Numpy module. We suppose we have n + 1 equally spaced qualitative data points, which divide the range x = [0, 10] into n equally-sized intervals. Exactly two of these intervals will be bounded on one end by a "−" point and on the other by a "+" point; these two intervals must contain the intersection points x 1 and x 2.

To generate Fig. 1b, within the possible intervals for x 1 and x 2, we sampled all possible combinations of 100 (evenly spaced) values of x 1 and 100 values of x 2, and solved for the coefficients. Considering only the fits that yielded positive values for all coefficients, we report the minimum and maximum possible values for each coefficient.

Raf inhibitor model

The model shown in Fig. 2a was implemented in Python (Supplementary Software 1). We generated synthetic data points at various values of I and constant R tot = 50 μM, and populated the other species concentrations based on the equations of the model. To generate quantitative data, we calculated = (RI + RIR + 2RIRI)/ R tot, and perturbed results by adding normally distributed noise with standard deviation 0.1. To generate qualitative data, we calculated A = RR + RIR, and compared the result to the value A (0) = 15.24. If the difference from A (0) was < 1.5, the data point was labeled as "0" (within error), otherwise it was labeled as "+" or "−" appropriately.

For fitting, the objective function was created as described in Results. Minimization was performed with the Scipy function optimize.differential_evolution, with a maximum of 1000 iterations, strategy of "best1exp", and search range of [10 −4, 10 4] for each parameter.

To perform profile likelihood analysis, for each parameter, we considered 100 possible fixed values (log uniformly distributed in the range [10 2, 10 5] for K 3 and [10 −3, 10 0] for K 5). At each fixed value considered, minimization was performed by the same method as above, except that the parameter of interest was held at the fixed value. We report the resulting minimum objective function values in the profile likelihood plots (Fig. 2d, e).

---

### Concise whole blood transcriptional signatures for incipient tuberculosis: a systematic review and patient-level pooled meta-analysis [^bbb1bb94]. The Lancet: Respiratory Medicine (2020). High credibility.

Figure 4
Diagnostic accuracy of eight best performing transcriptional signatures for incipient tuberculosis shown in receiver operating characteristic space, stratified by months to disease

Dashed lines represent positive-predictive values of 5%, 10%, and 15%, based on 2% pre-test probability. Grey shading indicates 95% CIs for each signature. Cutoffs derived from two standard scores above the mean of control population. The number of samples included for each signature, at each timepoint, is indicated in the appendix 1 (p 19). Point estimates and 95% CIs are also shown in the appendix 1 (p 20).

On the basis of a pre-test probability of 2% at the prespecified cutoffs, all eight best performing signatures achieved a positive predictive value marginally above the WHO benchmark of 5.8% for a 0–24-month period, ranging from 6.8% for Suliman2 to 9.4% for Kaforou25, with corresponding negative-predictive values of 98.4% and 98.6% (appendix 1 p 21). For the 0–3-month period, positive predictive values ranged from 11.2% for Gliddon3 to 14.4% for Zak16, with corresponding negative-predictive values of 99.0% and 99.3% (appendix 1 p 21).

Sensitivities and specificities of the eight equivalent signatures using cutoffs defined by the maximal Youden index for each time interval were smaller than the minimum WHO target product profile criteria for a 0–24-month period but met or approximated the minimum criteria over 0–3 months (appendix 1 pp 22–23). Restricting inclusion of incipient tuberculosis cases to those with documented microbiological confirmation and including only one blood RNA sample per participant (by randomly sampling) produced no significant change to the main results (appendix 1 pp 24–25). Reanalysis of the receiver operating characteristic curves using mutually exclusive periods of 0–3, 3–6, 6–12, and 12–24 months magnified the difference in performance between the intervals, with performance declining more markedly with increasing interval to disease (appendix 1 pp 26–27). AUCs in the 12–24-month interval ranged from 0.60 (95% CI 0.50–0.70) to 0.67 (0.60–0.75) for the eight equivalent signatures. Finally, our two-stage meta-analysis approach showed similar findings to the primary analysis (appendix 1 pp 28–29).

---

### Cancers attributable to inadequate physical exercise in the UK in 2010 [^c78d9977]. British Journal of Cancer (2011). Low credibility.

The minimum target would envisage all those doing less than 30 min, 5 days a week, to move to this minimum (but no increase in exercise levels in those individuals already achieving the target level).

The National Diet and Nutrition Survey provides tables showing reported levels of physical exercise, by age group and sex, for adults aged 19–64 in a sample of households in Great Britain in 2000–2001. There are four categories: moderate exercise of 30 min duration for 5 or more days a week, 1–2 days, 3–4 days and < 1 day per week. In all, 36% of men and 26% of women aged 19–64 were already at the target level of at least 30 min moderate physical exercise on at least 5 days a week. For those of age > 65, equivalent data were obtained from the Health Survey for England by averaging the values in the surveys of 1997 and 2003. The middle category (1–4 days moderate exercise per week) was split into two (1–2 and 3–4), based on the ratios observed at ages 50–64 in the 2000–2001 National Diet & Nutrition Survey.

The results for all adult age groups (ages 19 and above) are shown in Table 1.

Assuming that exercise of moderate intensity is equivalent to 6 METS, so that 30 min of moderate exercise consumes 3 MET-hours, we estimate the deficit in MET hours below the recommended level of 15 per week (3 × 5). Thus, for the proportion of the population exercising moderately 3–4 days a week, the deficit is, on average, 4.5 MET-hours per week (15−[3 × 3.5]), for those exercising 1–2 days 10.5 MET-hours per week and for those exercising less than 1 day per week 13.5 MET-hours (15−[3 × 0.5]) per week. These values are shown in the first row of Table 1.

Population-attributable fractions (PAFs) were calculated for each sex-age group in Table 1 according to the usual formula:

where p x is the proportion of population in exercise category x and ERR x the excess RR in exercise category x.

ERR x is calculated as

where R m is the increase in risk for a deficit of 1 MET-hour per week and M x is the deficit in MET-hours per week (less than 15) in exercise category x.

---

### Size limits the sensitivity of kinetic schemes [^71d1065f]. Nature Communications (2023). High credibility.

A different measure of sensitivity — the amplification of a fold-change in the input — provides a solution to this problem. Suppose that for some value x 0 of the input parameter x, scaling by a factor a scales the output by b, so f (a x 0) = b f (x 0). Then the quotientcan be thought of as a discrete approximation of the derivative defining the logarithmic sensitivity. And if f (x) is differentiable everywhere, then by the mean value theorem, there must be a value x * of x for whichThis means that careful measurement of any two points on the input-output curve (x versus f (x)) witnesses the (local, infinitesimal) logarithmic sensitivity somewhere. Importantly — unlike in the case of fitting to a Hill function — if error in the measurements is very low, then they are also telling us the derivative for some value of x very accurately.

Equation (25) leads us to another common definition of the effective Hill coefficient:where S 0.9 and S 0.1 are the values of the input variable (in our case, x) required to get 90% and 10% (respectively) of the maximum value of the output variable (in our case, f (x)). Note that (26) is like (25) with a = S 0.9 / S 0.1 and b = 9. It implies that somewhere between S 0.1 and S 0.9 there is a logarithmic sensitivity of H eff /2.

There is yet another common definition, specific to models of binding. Suppose x is the concentration of a ligand and 〈 n b 〉(x) is the expected number of sites bound by a ligand out of a total of n possible binding sites. It is common then, to takeor to report, as the Hill coefficient, the slope of a line fitted to x versusdata on a log-log plot.

---

### Tofacitinib in rheumatoid arthritis: lack of early change in disease activity and the probability of achieving low disease activity at month 6 [^26a97e12]. Arthritis Care & Research (2019). Medium credibility.

Assessments

The Clinical Disease Activity Index (CDAI), as the primary analysis, and the 4‐component Disease Activity Score in 28 joints using the ESR (DAS28‐ESR) were assessed at baseline (prior to the first study dose), and at months 1, 3, 6, 9, and 12 (or at the end‐of‐study visit). LDA and remission criteria, respectively, were defined as CDAI score of ≤ 10 and ≤ 2.8 and as DAS28‐ESR of ≤ 3.2 and < 2.6 29. The proportion of patients who failed to achieve a number of different thresholds of improvement in disease activity was assessed. Improvement thresholds were a decrease from baseline in CDAI of ≥ 3, ≥ 6, ≥ 9, and ≥ 12, and a decrease from baseline in DAS28‐ESR of ≥ 0.3, ≥ 0.6, ≥ 0.9, ≥ 1.2, ≥ 1.5, and ≥ 1.8 12. An improvement of ≥ 6 in CDAI after 4 weeks of treatment with baricitinib was found to be the minimum level predictive of a response at later time points 12, and an improvement of ≥ 1.2 in DAS28‐ESR and a baseline value of > 5.1 are deemed a moderate response in patients with RA 30. These values were therefore considered as the key thresholds for improvement at months 1 and 3.

---

### The standardization of the Polish version of the Alberta infant motor scale [^3e3d157d]. BMC Pediatrics (2023). Medium credibility.

Results

All participants were analyzed together on account of no significant differences between infants born before 39 weeks and at/after 39 weeks of pregnancy. The characteristics of participants is listed in Table 1.

Table 2 presents the mean and the standard deviation of the AIMS total scores in Polish and Canadian infants, and the minimum and maximum AIMS scoring in the Polish sample in every age range.

In Polish infants, the maximal score was noted for the first time in the group of 11- < 12 months (in 8% of participants). Then it was achieved by 25% of individuals in the group of 13- < 14 months, 84% of participants in the group of 14- < 15 months, and all infants older than 17 months. In analyzing ranges between the minimum and maximum AIMS scores, the biggest diversities were shown in age groups of 7- < 8 months (range of 25 points), 8- < 9 months (range of 19 points), 9- < 10 months (range of 23 points).

The mean AIMS total scores in the Polish population were significantly lower than the Canadian values in the groups of 0- < 1, 1- < 2, 4- < 5, 5- < 6, 6- < 7, 13- < 14, and 15- < 16 months of age. Small to large effect size was noted, with the majority of moderate.

Table 3 presents the percentile ranks in the Polish AIMS scores and the comparison with the Canadian norms. The significant differences were found in the 5th percentile in 3- < 4, 15- < 16, 16- < 17 months of age, in the 10th percentiles in 15- < 16 and 16- < 17 months of age, in the 25th percentile in 6- < 7 and 15- < 16 months of age, in the 50th percentile in 6- < 7 and 13- < 14 months of age, in the 75th percentile in 0- < 1, 1- < 2, 3- < 4, 5- < 6, 6- < 7, 11- < 12 months of age, in the 90th percentile in 14- < 15, 15- < 16, 16- < 17 months of age. Figure 1 shows the percentile of the AIMS score curves for Polish infants.

---

### Quantitative assessment of full-width at half-maximum and detector energy threshold in X-ray imaging systems [^c65d8c68]. European Journal of Radiology (2024). Medium credibility.

Background

The response function of imaging systems is regularly considered to improve the qualified maps in various fields. More the accuracy of this function, the higher the quality of the images.

Methods

In this study, a distinct analytical relationship between full-width at half-maximum (FWHM) value and detector energy thresholds at distinct tube peak voltage of 100 kV has been addressed in X-ray imaging. The outcomes indicate that the behavior of the function is exponential. The relevant cut-off frequency and summation of point spread function S(PSF) were assessed at large and detailed energy ranges.

Results

A compromise must be made between cut-off frequency and FWHM to determine the optimal model. By detailed energy range, the minimum and maximum of S(PSF) values were revealed at 20 keV and 48 keV, respectively, by 2979 and 3073. Although the maximum value of FWHM occurred at the energy of 48 keV by 224 mm, its minimum value was revealed at 62 keV by 217 mm. Generally, FWHM value converged to 220 mm and S(PSF) to 3026 with small fluctuations. Consequently, there is no need to increase the voltage of the X-ray tube after the energy threshold of 20 keV.

Conclusion

The proposed FWHM function may be used in designing the setup of the imaging parameters in order to reduce the absorbed dose and obtain the final accurate maps using the related mathematical suggestions.

---

### Contemporary issue: health and safety of female wrestlers [^c2819a17]. Current Sports Medicine Reports (2024). High credibility.

Female wrestlers — body fat distribution and MWW threshold reconsideration shows that even in trained, highly fit female wrestlers body fat values are between 18% and 22% and it is extremely rare that the 12% threshold is approached; from Table 2 data, a sample size weighted mean of 20.3% with a pooled standard deviation of 4.2% in a sample of 326 female wrestlers yields a lower boundary of 17.9% body fat, with 12% clearly falling in the unexpected region. In line with these data, the authors state, "we encourage SGB to reevaluate using 12% fat as the minimum set for all female wrestlers" and that "a value like 18% body fat might be a more reasonable minimum", adding that "If 18% were adopted, 0.82 in the MWW calculation would replace 0.88", and clarifying that "biological females with less than 18% body fat at the time of weight class determination would not be disqualified", although preseason values under 18% would prohibit further reductions to reach a lower class.

---

### Rigorous location of phase transitions in hard optimization problems [^d98ebea5]. Nature (2005). Excellent credibility.

It is widely believed that for many optimization problems, no algorithm is substantially more efficient than exhaustive search. This means that finding optimal solutions for many practical problems is completely beyond any current or projected computational capacity. To understand the origin of this extreme 'hardness', computer scientists, mathematicians and physicists have been investigating for two decades a connection between computational complexity and phase transitions in random instances of constraint satisfaction problems. Here we present a mathematically rigorous method for locating such phase transitions. Our method works by analysing the distribution of distances between pairs of solutions as constraints are added. By identifying critical behaviour in the evolution of this distribution, we can pinpoint the threshold location for a number of problems, including the two most-studied ones: random k-SAT and random graph colouring. Our results prove that the heuristic predictions of statistical physics in this context are essentially correct. Moreover, we establish that random instances of constraint satisfaction problems have solutions well beyond the reach of any analysed algorithm.

---

### Statistical analysis and optimality of neural systems [^bf829ef6]. Neuron (2021). Medium credibility.

Normative theories and statistical inference provide complementary approaches for the study of biological systems. A normative theory postulates that organisms have adapted to efficiently solve essential tasks and proceeds to mathematically work out testable consequences of such optimality; parameters that maximize the hypothesized organismal function can be derived ab initio, without reference to experimental data. In contrast, statistical inference focuses on the efficient utilization of data to learn model parameters, without reference to any a priori notion of biological function. Traditionally, these two approaches were developed independently and applied separately. Here, we unify them in a coherent Bayesian framework that embeds a normative theory into a family of maximum-entropy "optimization priors". This family defines a smooth interpolation between a data-rich inference regime and a data-limited prediction regime. Using three neuroscience datasets, we demonstrate that our framework allows one to address fundamental challenges relating to inference in high-dimensional, biological problems.

---

### Implementing quantum dimensionality reduction for non-markovian stochastic simulation [^4ea471be]. Nature Communications (2023). High credibility.

Methods

Stochastic processes and minimal-memory classical modelling

A discrete-time stochastic processconsists of a sequence of random variables X t, corresponding to events drawn from a set, and indexed by a timestep. The process is defined by a joint distribution of these random variables across all timesteps, whererepresents the contiguous (across timesteps) series of events between timesteps t 1 and t 2. We consider stochastic processes that are bi-infinite, such thatand, and stationary (time-invariant), such that P (X 0: L) = P (X t: t + L) ∀ t, L ∈. Without loss of generality, we can take the present to be t = 0, such that the past is given by, and the future. Note that we use upper case for random variables and lower case for the corresponding variates.

A (causal) model of such a (bi-infinite and stationary) discrete-time stochastic process consists of an encoding functionthat maps from the set of possible past observationsto a set of memory states –. The model also requires an update rulethat produces the outputs and updates the memory state accordingly. We then designate the memory cost D f of the encoding as the logarithm of the dimension (i.e. the number of (qu)bits) of the smallest system into which these memory states can be embedded. For classical (i.e. mutually orthogonal) memory states, this corresponds to. For quantum memory states, which may, in general, be linearly dependent.

Let us, for now, restrict our attention to statistically-exact models, such that (f, Λ) must produce outputs with a distribution that is identical to the stochastic process being modelled. Under such a condition, the provably-memory minimal classical model of any given discrete-time stochastic process is known and can be systematically constructed. These models are referred to as the ε - machine of the process, which employs an encoding function f ε based on the causal states of the process. This encoding function satisfiesand given initial memory state, the evolution produces output x 0 with probabilityand updates the memory to state. The memory states are referred to as the causal states of the process, and the associated cost D μ is given by the logarithm of the number of causal states.

---

### Evaluating molecular biomarkers for the early detection of lung cancer: when is a biomarker ready for clinical use? An official American Thoracic Society policy statement [^1c6ced37]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Accuracy guidance for biomarker clinical validation — formulas specify minimal accuracy thresholds tied to action after positive versus negative results. The committee "agreed that it would be helpful to provide guidance about the minimal accuracy, as assessed in the clinical validation phase, that could lead to a positive clinical impact". For tests where "a positive biomarker result leads to an action where a negative biomarker result is associated with standard of care for the population", the threshold is "sensitivity/(1 − specificity) ≥ [(1 − prevalence)/prevalence] × harm/benefit", where harm/benefit is "the ratio of the net harm of a falsely positive test result to the net benefit of a true-positive test result". For tests where "a negative test leads to an action other than standard of care for the population", the threshold is "specificity/(1 − sensitivity) ≥ [prevalence/(1 − prevalence)] × harm/benefit", where harm/benefit is "the ratio of the net harm of a falsely negative test result to the net benefit of a true-negative test result". "Sensitivity/(1 − specificity) is known as the positive likelihood ratio, and specificity/(1 − sensitivity) is 1 divided by the negative likelihood ratio", and "Prevalence refers to the percentage of cases in the intended use population". The harm/benefit term "can be articulated in one of two ways: 1/N, where in the first scenario N is the maximum number of control subjects testing positive that is tolerated to benefit one case subject testing positive".

---

### Casein [^fd0595f4]. FDA (2009). Low credibility.

Volume desired × Concentration desired = Volume needed × Concentration available.

Example 1: If a 1:10 w/v extract is available and it is desired to use a 1:1,000 w/v extract substitute as follows:

Vd × Cd = Vn × Ca

10ml × 0.001 = Vn × 0.1

0.1 mL = Vn

Using a sterile technique, remove 0.10 mL of extract from the 1:10 vial and place it into a vial containing 9.90 mL of sterile diluent. The resulting ratio will be a 10 mL vial of 1:1,000 w/v.

Example 2: If a 10,000 pnu/ml extract is available and it is desired to use a 100 pnu/ml extract substitute as follows:

10ml × 100 = Vn × 10,000

0.1 mL = Vn

Using a sterile technique, remove 0.10 mL of extract from the 10,000 pnu/ml vial and place it into a vial containing 9.90 mL of sterile diluent. The resulting concentration will be a 10 mL vial of 100 pnu/ml.

Example 3: If a 10,000 AU/ml or BAU/ml extract is available and it is desired to use a 100 AU/ml or BAU/ml extract substitute as follows: Vd × Cd = Vn × Ca

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^59838d2f]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Sample size metrics — tolerance intervals versus confidence intervals: Although performance is often stated in terms of confidence intervals (CI), the CI of the mean only gives an estimate of the population mean with a stated level of confidence and does not give an indication of the performance of any given sample; to estimate the distribution of the underlying population and the performance of individual samples, the tolerance intervals should be used, with the lower tolerance interval for a normally distributed population determined as x̄ ± k × s, where s is the sample standard deviation (SD) and k is a correction factor, and for two-sided 95% confidence with n = 20 the k value is 2.75, approaching the z-score as the number of samples increases.

---

### Maximizing NMR sensitivity: a guide to receiver gain adjustment [^19dc56cc]. NMR in Biomedicine (2025). Medium credibility.

Unexpectedly, we found a strong dip in SNR at 9.4 T of about 40% for all measured nuclei except 1 H at = 18. As the signal increases linearly, this effect is caused by changes in noise intensity. The plateau was reached at > 30 for 1 H andfor the other nuclei.

These findings showed that the most sensitive RG for X‐nuclei is between 10 and 18 and should be used in most cases without any automatic RG adjustment. Lower RG values are needed when the signal exceeds the threshold, while higher RG values do not provide any benefits. The 1 H channel showed the most gain in SNR up to≈ 30, with minor improvements thereafter but with a potential threat of signal compression. This finding revealed that choosing a bigger RG is not necessarily the right strategy to improve SNR.

3.2 Optimal Settings for a Hyperpolarization Observation

Having measured(Figure 2) and the RRT (for 1 H andfor other nuclei at the 9.4 T system, Figure 1), one can estimate the signal intensity and SNR for a known sample for any parameters: and.

Using Equations (1) and (5), we calculated the signal and SNR for pure water (55.51 M) and D 2 O (55.21 M) for = 0–90° and RG = 0–101 (Figure 3). Note that the concentration of the 1 H and 2 H spins is twice as large as the concentration of the molecules. For more than half of the combinations ofand, the signal exceeded(white areas in Figure 3, right). These results indicate thatandshould be chosen with care.

Note that if there are no constraints on the flip angle, maximizingis more beneficial for SNR than maximizing RG (Figure S2, SI). For water, the maximum SNR was obtained with = 90° andof 4.9 (1 H) and 12.8 (2 H) — values far below the maximum RG of 101.

In hyperpolarization experiments, andare usually given, whileandcan be adjusted as needed. We calculated the signal and SNR as a function ofandfor a 90 mM 13 C sample polarized to 35% and a 40 mM 15 N sample polarized to 15% — similar values were achieved recently with [1‐ 15 N]nicotinamide (Figure 3). To preserve polarization, the flip anglein hyperpolarized metabolic experiments is typically low. Here, we set the limits to 5° (13 C) and 10° (15 N).

---

### Miniature origami robot for various biological micromanipulations [^03cd1422]. Nature Communications (2025). High credibility.

Contact operation force control

The thin film pressure sensor PXS-S6ST-50K (GuanZhou PuHui Technology Co. Ltd) is fixed between the moving platform and the needle to measure the normal force along the needle. During the cell contact operation process, we primarily focus on three values: the current value of the sensor F t, the minimum value recorded in this operation F m i n, and the maximum value F m a x.

For the cell touching operation, we set an upper limit for the contact force F U, when F t achieve or above the upper limit, the needle will hold on for a while and retract. The algorithm is shown in Algorithm 1. During the cell puncturing process, precise control of the needle is critical to ensure that cells are punctured rather than cut through. With force measurement, we are able to monitor the change in the contact force. A rapid decrease implies that the cell has already been punctured. Based on this judgment, we set the puncture force threshold R = 0.18, meaning that when F t is below 82% of the maximum puncturing force (F m a x − F m i n), the needle tip will retract. This ensures that only the cell wall is punctured without penetrating the entire cell. The process is demonstrated in Algorithm 2.

Algorithm 1

Cell touching

1: while Touching command starts do

2: if F t < F U then (Contact force not achieve the upper limit)

3: Needle tip keeps going down

4: else

5: Needle tip stays in the current position for a while

6: Needle tip lifts

7: break

8: end if

9: end while

Algorithm 2

Cell puncturing

1: while Puncturing command starts do

2: if F t < F U then (Before puncturing, ensure contact with the cell)

3: Needle tip keeps going down

4: else

5: if (F m a x − F t) ≥ R (F m a x − F m i n) then (A rapid decrease in contact force implies that the cell has been punctured)

6: Needle tip lifts

7: break

8: else

9: Needle tip keeps going down

10: end if

11: end if

12: end while

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^bb9b7f12]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Δ Φ, the probability distribution at a point Φ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Φ ∈ {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters α and β, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since α and β are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Φ ∈ [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(α, β) and x 2 = 1 − x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Φ, it may seem natural to select α and β by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### Assessment of the minimum clinically important difference in pain, disability, and quality of life after anterior cervical discectomy and fusion: clinical article [^e960f0f5]. Journal of Neurosurgery: Spine (2013). Low credibility.

Object

Treatment effectiveness following spine surgery is usually gauged with the help of patient-reported outcome (PRO) questionnaires. Although these questionnaires assess pain, disability, and general health state, their numerical scores lack direct, clinically significant meaning. Thus, the concept of minimum clinically important difference (MCID) has been introduced, which indicates the smallest change in an outcome measure that reflects clinically meaningful improvement to patients. The authors set out to determine anterior cervical discectomy and fusion (ACDF)-specific MCID values for the visual analog scale (VAS), Neck Disability Index (NDI), 12-Item Short-Form Health Survey (SF-12), and EQ-5D (the EuroQol health survey) in patients undergoing ACDF for cervical radiculopathy.

Methods

Data on 69 patients who underwent ACDF for cervical radiculopathy were collected in the authors' web-based, prospective registry during the study enrollment period. Patient-reported outcome questionnaires (VAS-neck pain [NP]), VAS-arm pain [AP], NDI, SF-12, and EQ-5D) were administered preoperatively and 3 months postoperatively, allowing 3-month change scores to be calculated. Four established calculation methods were used to calculate anchor-based MCID values using the North American Spine Society (NASS) patient satisfaction scale as the anchor: 1) average change, 2) minimum detectable change (MDC), 3) change difference, and 4) receiver operating characteristic (ROC) curve analysis.

Results

Sixty-one patients (88%) were available at follow-up. At 3 months postoperatively, statistically significant improvement (p < 0.001) was observed for the following PROs assessed: VAS-NP (2.7 ± 3.3), VAS-AP (3.7 ± 3.6), NDI (23.2% ± 19.7%), SF-12 physical component score (PCS; 10.7 ± 9.7), and EQ-5D (0.20 ± 0.23 QALY). Improvement on the SF-12 mental component score (MCS) trended toward significance (3.4 ± 11.4, p = 0.07). The 4 MCID calculation methods generated a range of MCID values for each of the PROs: VAS-NP 2.6–4.0, VAS-AP 2.4–4.2, NDI 16.0%-27.6%, SF-12 PCS 7.0–12.2, SF-12 MCS 0.0–7.2, and EQ-5D 0.05–0.24 QALY. The maximum area under the curve (AUC) was observed for NDI (0.80), and the minimum AUC was observed for SF-12 MCS (0.66) and EQ-5D (0.67). Based on the MDC approach, the MCID threshold was 2.6 points for VAS-NP, 4.1 points for VAS-AP, 17.3% for NDI, 8.1 points for SF-12 PCS, 4.7 points for SF-12 MCS, and 0.24 QALY for EQ-5D. The mean improvement in patient scores at 3 months surpassed the MCID threshold for VAS-NP, NDI, and SF-12 PCS but not for VAS-AP, SF-12 MCS, and EQ-5D.

Conclusions

The ACDF-specific MCID is highly variable depending on the calculation technique used. The MDC approach seems to be most appropriate for MCID calculations in the ACDF population, as it provided a threshold value above the 95% confidence interval of nonresponders (greater than the measurement error) and was closest to the average change of most PROs reported by responders. When the MDC method was applied with the NASS patient satisfaction scale as the anchor, the MCID thresholds were 2.6 points for VAS-NP, 4.1 points for VAS-AP, 17.3% for NDI, 8.1 points for SF-12 PCS, 4.7 points for SF-12 MCS, and 0.24 QALY for EQ-5D.

---

### The minimum clinically important difference for patient health questionnaire-9 in minimally invasive transforaminal interbody fusion [^b12afe2a]. Spine (2021). Medium credibility.

Study Design

Retrospective cohort.

Objective

To investigate and establish minimum clinically important differences (MCID) for Patient Health Questionnaire-9 (PHQ-9) among patients undergoing minimally invasive transforaminal lumbar interbody fusion (MIS TLIF).

Summary Of Background

Spine surgery is linked to postoperative improvements in anxiety, depression, and mental health. These improvements have been documented using patient-reported outcome measures such as PHQ-9. Few studies evaluated the clinical significance of PHQ-9 for lumbar spine surgery.

Methods

Patients who underwent single-level, primary MIS TLIF from 2015 to 2017 were retrospectively reviewed in a prospective database. Patients with incomplete preoperative and 2-year postoperative PHQ-9 surveys were excluded. Demographic and perioperative characteristics were recorded. PHQ-9, 12-Item Short Form (SF-12), and Veterans RAND 12-Item Health Survey (VR-12) Mental Component Summary (MCS) were collected at preoperative, 6-week, 12-week, 6-month, 1-year, and 2-year intervals. MCID was calculated using anchor and distribution-based methods. SF-12 served as an anchor. MCID was assessed using mean change methodology, four receiver operating characteristic curve assessments, and standard error measurement. Cutoff values were selected from receiver operating characteristic curve analysis. MCID achievement rates for all patient-reported outcome measures were calculated.

Results

A total of 139 patients met inclusion criteria, with a mean age of 55 years and 39% females. The most common spinal pathology was radiculopathy (92%). MCID analysis revealed the following ranges of values: 2.0 to 4.8 (PHQ-9), 6.7 to 12.1 (SF-12 MCS), and 7.5 to 15.9 (VR-12 MCS). Final MCID thresholds were 3.0 (PHQ-9), 9.1 (SF-12 MCS), and 8.1 (VR-12 MCS). MCID achievement at 2-years for PHQ-9, SF-12 MCS, and VR-12 MCS was 89.2%, 85.6%, and 84.9% respectively.

Conclusion

Our 2-year postoperative MCID analysis is the first mental health calculation from an MIS TLIF cohort. We report a 2-year MCID value for PHQ-9 of 3.0 (2.0–4.8). MCID values for mental health instruments are important for determining overall success of lumbar spine surgery. Level of Evidence: 3.

---

### 2018 ACC / HRS / NASCI / SCAI / SCCT expert consensus document on optimal use of ionizing radiation in cardiovascular imaging: best practices for safety and effectiveness [^8c6a7f8c]. Catheterization and Cardiovascular Interventions (2018). Medium credibility.

Computed tomography (CT) dose optimization — scan length, x-ray beam intensity (tube potential and tube current), rotation time, and x-ray beam filtration — includes the following considerations: Scan length "should be kept to a minimum to encompass only the anatomy of interest and not expose structures that are not relevant to the examination's purpose". X-ray beam intensity is "determined by both the x-ray tube potential (in units of kV) and the x-ray tube current (in units of mA)". For tube potential, "the most important single factor in controlling radiation dose is adjustment of x-ray tube voltage", and "at a constant tube current, a decrease of tube voltage from 120 to 100 kV reduces the radiation dose by almost 40%"; in most scanners, "the x-ray tube voltage may be adjusted between 70 to 140 kilovolts (kV)", with a commonly used scale of "120 kV for patients with body mass index ≥ 30 kg/m2, 100 kV for body mass index 21 to 29 kg/m2, and 80 kV for body mass index < 21 kg/m2", and "in extreme cases (body mass index ≥ 40 kg/m2) the maximum tube potential of 150 kV may be necessary". For tube current, "The radiation dose is linearly proportional to the tube current. Image noise is inversely proportional to the square root of the tube current", and modulation "may be applied longitudinally as well as circumferentially. This approach can reduce radiation exposure of thoracic CT examinations by 20% without increasing image noise". "The time required for the gantry to perform 1 rotation is a selectable parameter" and "Exposure increases linearly with rotation time", while x-ray beam filtration "selectively attenuate[s] low-energy x-rays that do not significantly contribute to the image but do contribute to radiation dose" and "increase[s] the mean energy of the x-rays while not altering the maximum energy".

---

### Region-specific denoising identifies spatial co-expression patterns and intra-tissue heterogeneity in spatially resolved transcriptomics data [^fe14a52f]. Nature Communications (2022). High credibility.

Low-rank matrix completion

Suppose the observed gene expression matrix for region r is, whereis a sparse matrix with M rows (spots) and N columns (genes). The task is to estimate, which represents the denoised gene expression matrix for region r. We adapted the low-rank-matrix completion algorithm through singular value decomposition used by McImpute.

Given the assumption that the number of cell types within a functional region is small, we expectto have a low-rank. To achieve this goal, the task is turned into a low-rank matrix completion problem by solving the following objective function:

The first term in Eq. (2) minimizes the error between the non-missing gene expression values ofandwith a projection functionthat returns values inat the indices of non-missing values in. The second term in Eq. (2) minimizes the rank of the denoised gene expression matrix. The objective function is a linear combination of these two terms regularized by a non-zero tuning parameter. Theoretically, a largerwill give us a lower-ranked denoised gene expression matrix whose values on the non-missing indices might deviate from the ground truth. On the other hand, a smallwill result in a relatively high-rank denoised matrix with a lower error on the non-missing indices.

However, minimizing the rank of a matrix is non-convex. To transform it to a convex problem with a globally optimal solution, we relaxed Eq. 2 as:where we transformed the term 2 in Eq. (2)as a nuclear norm of, which can be calculated by summing up the singular values obtained through singular value decomposition. Specifically, Eq. (3) can be further transformed aswhere. Using the inequality, wheredenotes the singular value vector for matrix X, Eq. (4) can be rewritten as:

Therefore, by taking the derivative of Eq. (5), is solved by soft thresholding the singular values ofwith a threshold equal to.

To tune the parameterto strike the balance between low matrix singularity and low error on non-missing indices, we find the maximalthat achieves a fixed low error calculated by the sum of the absolute difference between denoised and observed values on non-missing indices.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^d6d8fcbc]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Spirometry reporting specifies that numerical values are given for the FEV1, the FVC, and the FEV1/FVC ratio; the latter should be reported as a decimal fraction and the space for percent predicted value left blank, and if bronchodilators are given the LLN column need not be repeated with absolute and percent change given only for FEV1 and FVC. Other numerical values such as the forced inspiratory flow at 75% of FVC (FEF75%) and FEF25–75% are not recommended for routine use. Graph requirements include that for the volume–time curve the volume scale should be at least 10 mm/L, the time scale at least 20 mm/s, and 1 second prior to the start of expiration should be displayed; on the flow–volume plot the flow display should be at least 5 l/min/L/s, and the ratio of flow to volume should be 2 L/s to 1 L, and linear and log scales where values are plotted as z-scores relative to the predicted value (z = 0) give an intuitive sense of severity.

---

### Just-in-time transcription program in metabolic pathways [^e6adb9ca]. Nature Genetics (2004). Medium credibility.

A primary goal of systems biology is to understand the design principles of the transcription networks that govern the timing of gene expression. Here we measured promoter activity for approximately 100 genes in parallel from living cells at a resolution of minutes and accuracy of 10%, based on GFP and Lux reporter libraries. Focusing on the amino-acid biosynthesis systems of Escherichia coli, we identified a previously unknown temporal expression program and expression hierarchy that matches the enzyme order in unbranched pathways. We identified two design principles: the closer the enzyme is to the beginning of the pathway, the shorter the response time of the activation of its promoter and the higher its maximal promoter activity. Mathematical analysis suggests that this 'just-in-time' (ref. 5) transcription program is optimal under constraints of rapidly reaching a production goal with minimal total enzyme production. Our findings suggest that metabolic regulation networks are designed to generate precision promoter timing and activity programs that can be understood using the engineering principles of production pipelines.

---

### Defining the clinically meaningful outcomes for arthroscopic treatment of femoroacetabular impingement syndrome at minimum 5-year follow-up [^02637b5e]. The American Journal of Sports Medicine (2020). Medium credibility.

Background

Minimal clinically important difference (MCID), substantial clinical benefit (SCB), and patient acceptable symptomatic state (PASS) have gained prominence as important variables in the orthopaedic outcomes literature. In hip preservation surgery, much attention has been given to defining early clinically meaningful outcome; however, it is unknown what represents meaningful patient-reported outcome improvement in the medium to long term.

Purpose

(1) To define MCID, PASS, and SCB at a minimum 5 years after hip arthroscopy for femoroacetabular impingement syndrome (FAIS) and (2) to evaluate the time-dependent nature of MCID, PASS, and SCB.

Study Design

Case series; Level of evidence, 4.

Methods

Patients undergoing primary hip arthroscopy for FAIS between January 2012 and March 2014 were included. Clinical and demographic data were collected in an institutional hip preservation registry. MCID, PASS, and SCB were calculated for each outcome score including the Hip Outcome Score-Activities of Daily Living subscale (HOS-ADL), Hip Outcome Score-Sport-Specific subscale (HOS-SS), modified Harris Hip Score (mHHS), and International Hip Outcome Tool (iHOT-12) at 1, 2, and 5 years. MCID was calculated by use of a distribution-based method, whereas PASS and SCB were calculated by use of an anchor method.

Results

The study included 283 patients with a mean ± SD age of 34.2 ± 11.9 years. The mean changes in 1-year, 2-year, and 5-year scores, respectively, required to achieve MCID were as follows: HOS-ADL (8.8, 9.7, 10.2); HOS-SS (13.9, 14.3, 15.2); mHHS (6.9, 9.2, 11.4); and iHOT-12 (15.1, 13.9, 15.1). The threshold scores for achieving PASS were as follows: HOS-ADL (89.7, 88.2, 99.2); HOS-SS (72.2, 76.4, 80.9); mHHS (84.8, 83.3, 83.6); and iHOT-12 (69.1, 72.2, 74.3). Last, the threshold scores for achieving SCB scores were as follows: HOS-ADL (89.7, 91.9, 94.6); HOS-SS (78.1, 77.9, 85.8); mHHS (86.9, 85.8, 94.4); and iHOT-12 (72.6, 76.8, 87.5). More patients achieved MCID, SCB, and PASS at 2-year compared with 1-year follow-up; however, by 5 years, fewer patients had achieved clinically meaningful outcome (minimum 1-, 2-, and 5-year follow-up, respectively: MCID, 82.6%, 87.3%, 79.3%; PASS, 67.6%, 74.9%, 67.5%; SCB, 62.3%, 67.2%, 56.6%).

Conclusion

The greatest proportion of patients achieved MCID, PASS, and SCB at 2-year follow-up after arthroscopic treatment of FAIS compared with 1- and 5-year time points. Improvements were maintained out to 5-year follow-up, although the proportion of patients achieving clinical significance was slightly decreased.

---

### AASLD practice guidance on the use of TIPS, variceal embolization, and retrograde transvenous obliteration in the management of variceal hemorrhage [^0e6c243a]. Hepatology (2024). High credibility.

Regarding therapeutic procedures for variceal hemorrhage, more specifically with respect to TIPS, technical considerations, AASLD 2023 guidelines recommend to dilate the TIPS progressively (starting at 8 mm of diameter) to the minimum diameter needed to achieve a portosystemic pressure gradient < 12 mmHg when TIPS is performed for variceal hemorrhage (treatment of acute hemorrhage or prevention of recurrence).

---

### Transient power-law behaviour following induction distinguishes between competing models of stochastic gene expression [^6513fe13]. Nature Communications (2025). High credibility.

What features of transcription can be learnt by fitting mathematical models of gene expression to mRNA count data? Given a suite of models, fitting to data selects an optimal one, thus identifying a probable transcriptional mechanism. Whilst attractive, the utility of this methodology remains unclear. Here, we sample steady-state, single-cell mRNA count distributions from parameters in the physiological range, and show they cannot be used to confidently estimate the number of inactive gene states, i.e. the number of rate-limiting steps in transcriptional initiation. Distributions from over 99% of the parameter space generated using models with 2, 3, or 4 inactive states can be well fit by one with a single inactive state. However, we show that for many minutes following induction, eukaryotic cells show an increase in the mean mRNA count that obeys a power law whose exponent equals the sum of the number of states visited from the initial inactive to the active state and the number of rate-limiting post-transcriptional processing steps. Our study shows that estimation of the exponent from eukaryotic data can be sufficient to determine a lower bound on the total number of regulatory steps in transcription initiation, splicing, and nuclear export.

---

### An official American Thoracic Society workshop report: developing performance measures from clinical practice guidelines [^369e6d46]. Annals of the American Thoracic Society (2014). Medium credibility.

Agency for Healthcare Research and Quality performance measure attributes — Desirable features of performance measures include relevance to stakeholders and addressing important aspects of health, with evidence of a need for the measure; evidence should be explicitly stated, results should be reproducible and truly measure what they purport to measure, and specifications should explicitly define the numerator and denominator with understandable data collection requirements; necessary data sources should be available within the measurement timeframe and data collection costs justified by potential improvement in care or health.

---

### 2018 ACC / HRS / NASCI / SCAI / SCCT expert consensus document on optimal use of ionizing radiation in cardiovascular imaging: best practices for safety and effectiveness [^a51cc398]. Catheterization and Cardiovascular Interventions (2018). Medium credibility.

X-Ray fluoroscopy — detector dose setting benchmarks state that typical x-ray system calibrations are expressed as the dose reaching the detector in nGy per pulse and current x-ray fluoroscopic systems display this value in image metadata; for state-of-the-art equipment with the detector in a 22-cm zoom mode (typical zoom for coronary angiography), representative factory default image detector dose settings are Low-dose fluoroscopy: 20 nGy/pulse, Standard-dose fluoroscopy: 40 nGy/pulse, and Cine acquisition: 200 nGy/pulse. These should be considered benchmark values, and units delivering substantially greater doses should be recalibrated or renovated to achieve similar doses, or, if not remediable, retired.

---

### Forensic age assessment by 3.0 T MRI of the wrist: adaption of the vieth classification [^b54be9d4]. European Radiology (2022). Medium credibility.

Observed minimum ages in the distal radial epiphysis

In male individuals, the minimum ages for stages 2, 3, 4, 5, and 6 of the epiphyseal-diaphyseal fusion of the distal radius' epiphysis were 12.05, 13.69, 15.35, 16.59, and 19.19 years respectively (compare Table 3).

Table 3
Relevant age values of the distal radius' epiphysis ossification stages for male volunteers (in years)

Please note: Values close to 12 years of life, to 24 years of life and deduced data of those values are presumably affected by the cohort's age range

In female individuals, the minimum ages for stages 2, 3, 4, 5, and 6 of the epiphyseal-diaphyseal fusion of the distal radius' epiphysis were 12.11, 12.30, 14.44, 15.94, and 18.86 years respectively (compare Table 4).

Table 4
Relevant age values of the distal radius' epiphysis ossification stages for female volunteers (in years)

Please note: Values close to 12 years of life, to 24 years of life and deduced data of those values are presumably affected by the cohort's age range

Observed minimum ages in the distal ulnar epiphysis

In male volunteers, the minimum ages for stages 2, 3, 4, 5, and 6 of the epiphyseal-diaphyseal fusion of the distal ulna's epiphysis were 12.05, 14.14, 15.18, 16.59, and 18.50 years respectively (compare Table 5).

Table 5
Ossification stages of the distal ulna's epiphysis in relation to age for male volunteers (in years)

Please note: The values bordering to 12 years of life and to 24 years of life as well as derived data of those values are likely influenced by the cohort's age range

In female volunteers, the minimum ages for stages 2, 3, 4, 5, and 6 of the epiphyseal-diaphyseal fusion of the distal ulna's epiphysis were 12.11, 12.30, 14.44, 15.49, and 17.49 years respectively (compare Table 6).

Table 6
Relevant age values of the distal ulna's epiphysis ossification stages for female volunteers (in years)

Please note: Values close to 12 years of life, to 24 years of life and deduced data of those values are presumably affected by the cohort's age range

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^cbffc27e]. CDC (2011). Medium credibility.

Appendix G. Mathematical formula for sample size — sample size required to detect the difference of GM between two subpopulation groups defines a z test with null hypothesis GM1 = GM2 or GMmax/GMmin = 1 and alternative GM1 ≠ GM2 or GMmax / GMmin > 1, where GMmax = max(GM1, GM2) and GMmin = min(GM1, GM2). It considers power W (say 80% chance) to detect that one group's GM is at least k fold higher and gives the p-value as p = Prob(Z ≥ z) = 1 − φ−1(z) with Z ~ Normal (0,1). Given α (e.g., 0.05) and target ratio k, power is W = Prob (Z > φ−1(1 − α) − log10(k)/(S√(1/n1 + 1/n2))), where S is the expected standard deviation of log10(VL) in the population of interest; when n1 = n2 = n, W = Prob (Z > φ−1(1 − α) − log10(k)/(S√(2/n))). The required sample size is n = 2[(φ−1(1 − α) − φ−1(1 − W))S/log10(k)]², and tables are provided for W = 0.8 and W = 0.9 with α = 0.05.

---

### An official American Thoracic Society workshop report: developing performance measures from clinical practice guidelines [^53d0f733]. Annals of the American Thoracic Society (2014). Medium credibility.

American Thoracic Society workshop — limitations and cautions include that excluding weak recommendations can miss improvement opportunities, since "it is possible that some weak recommendations could improve patient outcomes if broadly applied" and "weakly recommended treatments may improve outcomes on average, even if they harm some patients"; there may be value in reducing variation, but "this approach carries the risk that meaningful opportunities for performance improvement are missed", and codifying weak recommendations "carries greater risks, including wasted resources" and punishing "legitimate practice variation". The approach applies primarily to broad-use measures, as local tracking "may not need to follow such stringent criteria", though developers "should be cognizant of opportunity costs" when considering uniform implementation of weak recommendations. Although GRADE-based recommendations are "subjective and conditional on the varying judgments of the developers", the system documents judgments and values that developers can consider when selecting recommendations to turn into measures.

---

### Defining thresholds and predictors for achieving the patient acceptable symptom state for patient-reported outcome measures after revision hip arthroscopy [^7d10f083]. The American Journal of Sports Medicine (2023). Medium credibility.

Background

The Patient Acceptable Symptom State (PASS) after primary hip arthroscopy has been determined; nonetheless, the PASS still needs to be defined for revision hip arthroscopy.

Purpose

To define minimum 2-year follow-up PASS thresholds for the modified Harris Hip Score (mHHS), Nonarthritic Hip Score (NAHS), Hip Outcome Score-Sports Specific Subscale (HOS-SSS), visual analog scale (VAS) for pain, and International Hip Outcome Tool-12 (iHOT-12) after revision hip arthroscopy, and to identify predictors of achieving the PASS.

Study Design

Case-control study; Level of evidence, 3.

Methods

Data were prospectively collected and retrospectively reviewed for all patients who underwent revision hip arthroscopy between April 2017 and July 2020. Patients were included if they had baseline and minimum 2-year follow-up scores for the mHHS, NAHS, HOS-SSS, VAS for pain, and iHOT-12. PASS was calculated using the anchor-based method. Receiver operating characteristic curve analysis was used to determine the thresholds for the PASS. A multivariate logistic regression was used to identify predictors for achieving the PASS.

Results

A total of 318 patients who underwent revision hip arthroscopy met the inclusion criteria. Of those patients, 292 (91.8%) had baseline and minimum 2-year follow-up. Of this group, 68 patients (72.1% female and 27.9% male; mean age, 32.9 years) answered the PASS anchor question. Achievement PASS rates were 58.8%, 41.2%, 52.9%, 60.3%, and 52.9% for the mHHS, NAHS, HOS-SSS, VAS, and iHOT-12, respectively. The area under the curve (AUC) values for the PASS for mHHS, NAHS, HOS-SSS, VAS, and iHOT-12 were 0.912, 0.888, 0.857, 0.903, and 0.871, respectively, indicating excellent discrimination. The PASS for the mHHS was 76 (sensitivity, 0.809; specificity, 0.905), for the NAHS was 86.3 (sensitivity, 0.660; specificity, 1), for the HOS-SSS was 64.3 (sensitivity, 0.745; specificity, 0.905), for the VAS was 3 (sensitivity, 0.830; specificity, 0.905), and for the iHOT-12 was 64.3 (sensitivity, 0.745; specificity, 0.905). Body mass index (BMI) was identified as a significant predictor of achieving PASS for the NAHS (OR, 0.967; 95% CI, 0.940–0.996; P = 0.027), as patients with a BMI ≤ 25.4 had 1.03 times higher odds ratio of achieving PASS for the NAHS.

Conclusion

After revision hip arthroscopy, the minimum 2-year follow-up PASS thresholds for the mHHS, NAHS, HOS-SSS, VAS for pain, and iHOT-12 were 76, 86.3, 64.3, 3, and 64.3, respectively. The odds ratio of achieving PASS for the NAHS was 1.03 times higher for patients with a BMI ≤ 25.4.

---

### Breathing-adapted imaging techniques for rapid 4-dimensional lung tomosynthesis [^c12056f0]. Advances in Radiation Oncology (2023). Medium credibility.

Figure 2
Simulated sinusoidal breathing waveforms and associated imaging techniques plots for D = 1 mm and f = 5. Left: 1 cm displacement, 12 bpm. Right: 4 cm displacement, 25 bpm. (A) Breathing displacement (cm) versus time. (B) Arc duration (seconds). (C) Frame rate (Hz). (D) Pulse duration (ms). (E) Normalized tube current (mA/mAs AEC). Abbreviation: bpm = breaths per minute.

Figure 3 presents plots of extreme imaging technique values derived from the sinusoidal-based model of Lujan et al with velocity computed as it was for Fig. 2 with breathing displacements of 10, 20, 30, and 40 mm and respiration rates of 12, 15, 20, and 25 bpm.

Figure 3
Imaging technique extreme values derived from simulated sinusoidal breathing waveforms using instantaneous analytical velocity at 12, 15, 20, and 25 breaths per minute and 10 to 40 mm breathing displacements for D = 1 mm and f = 5. (A) Minimum arc duration (seconds). (B) Maximum frame rate (Hz). (C) Minimum pulse duration (ms). (D) Maximum normalized tube current (mA/mAs AEC).

Table 1 presents the extreme scanning parameters for D = 1 and = 5 for volunteers and simulated breathing cycles: (1) volunteers' extreme values of shortest arc duration of 0.066 to 0.267 seconds, fastest frame rate of 228 to 921 Hz, shortest x-ray pulse duration of 1.076 to 4.368 ms, and maximum tube current, normalized toof 18.8 to 76.2 s –1; (2) simulated sinusoidal waveforms scanning parameters, using the average velocity (Equation 2), of shortest arc duration 0.034 to 0.253 seconds, fastest frame rate 241 to 1801 Hz, shortest x-ray pulse duration 0.545 to 4.141 ms, and maximum tube current, normalized to10.0 to 150.3 s –1; (3) simulated sinusoidal waveforms scanning parameters, using the instantaneous velocity (Equation 10), of shortest arc duration 0.029 to 0.245 seconds, fastest frame rate 249 to 2074 Hz, shortest x-ray pulse duration 0.472 to 4.007 ms, and maximum tube current, normalized to20.4 to 173.6 s –1.

---

### Quantification of network structural dissimilarities [^3cf663e0]. Nature Communications (2017). Medium credibility.

Identifying and quantifying dissimilarities among graphs is a fundamental and challenging problem of practical importance in many fields of science. Current methods of network comparison are limited to extract only partial information or are computationally very demanding. Here we propose an efficient and precise measure for network comparison, which is based on quantifying differences among distance probability distributions extracted from the networks. Extensive experiments on synthetic and real-world networks show that this measure returns non-zero values only when the graphs are non-isomorphic. Most importantly, the measure proposed here can identify and quantify structural topological differences that have a practical impact on the information flow through the network, such as the presence or absence of critical links that connect or disconnect connected components.

---

### The XZZX surface code [^8f6ac9c8]. Nature Communications (2021). High credibility.

This structured noise model thus leads to two distinct regimes, depending on which failure process is dominant. In the first regime where, we expect that the logical failure rate will decay like. We find this behaviour with systems of a finite size and at high bias where error rates are near to threshold. We evaluate logical failure rates using numerical simulations to demonstrate the behavior that characterises this regime; see Fig. 6 (a). Our data show good agreement with the scaling ansatz. In contrast, our data are not well described by a scaling.

Fig. 6
Sub-threshold scaling of the logical failure rate with the XZZX code.

a Logical failure rateat high bias near to threshold plotted as a function of code distance d. We use a lattice with coprime dimensions d × (d + 1) for d ∈ {7, 9, 11, 13, 15} at bias η = 300, assuming ideal measurements. The data were collected usingiterations of Monte-Carlo (MC) samples for each physical rate sampled and for each lattice dimension used. The physical error rates used are, from the bottom to the top curves in the main plot, p = 0.19, 0.20, 0.21, 0.22 and 0.23. Error bars represent one standard deviation for the Monte-Carlo simulations. The solid lines are a fit of the data to, consistent with Eq. (2), and the dashed lines a fit to, consistent with Eq. (3) where we would expect, see Methods. The data fit the former very well; for the latter, the gradients of the best fit dashed lines, as shown on the inset plot as a function of, give a linear slope of 0.61(3). Because this slope exceeds the value of 0.5, we conclude that the sub-threshold scaling is not consistent with. b Logical failure ratesat modest bias far below threshold plotted as a function of the physical error rate p. The data (markers) were collected at bias η = 3 and coprime d × (d + 1) code dimensions of d ∈ {5, 7, 9, 11, 13, 15} assuming ideal measurements. Data is collected using the Metropolis algorithm and splitting method presented in refs. The solid lines represent the prediction of Eq. (3). The data show very good agreement with the single parameter fitting for all system sizes as p tends to zero.

---

### A model to predict survival in patients with end-stage liver disease [^c686fc48]. Hepatology (2001). Low credibility.

The clinical calculator "Model for End-Stage Liver Disease-Sodium (MELD-Na)" for hepatocellular carcinoma, hepatic encephalopathy, portopulmonary hypertension, hepatorenal syndrome, variceal hemorrhage, ascites, portal hypertension, liver transplantation, acute fatty liver of pregnancy, primary biliary cholangitis, primary sclerosing cholangitis, acute-on-chronic liver failure, acute liver failure, alcohol-related liver disease and liver cirrhosis.

The MELD-Na (UNOS/OPTN) score is used to estimate the severity of chronic liver disease and prioritize liver transplant candidates. It incorporates sodium levels, creatinine, bilirubin, and INR to assess short-term mortality risk. The formula includes adjustments for patients on dialysis and limits the score between 6 and 40

The MELD-Na calculator is utilized to evaluate the severity of liver disease and estimate mortality risk by using key laboratory values and clinical factors. It outputs a MELD-Na Score which is associated with a specific mortality risk percentage. Here's how the process works:

Input Data:

- Serum creatinine level, which can be in mg/dL (0 to 20) or converted from µmol/L (0 to 1768).
- Total bilirubin level, reported in mg/dL (0 to 50) or converted from µmol/L (0 to 855).
- International Normalized Ratio (INR), ranging from 0 to 10.
- Serum sodium level in either mEq/L or mmol/L, ranging from 100 to 180.
- Dialysis status in the past 7 days, which is a simple "Yes" or "No".

Computation Steps:

- First, if creatinine and bilirubin are provided in µmol/L, conversion to mg/dL is performed by dividing the creatinine value by 88.4 and bilirubin by 17.1.
- Then, initial adjustments are made where the minimum acceptable values for creatinine, bilirubin, and INR are set to 1.0. For creatinine, if the patient has undergone dialysis or the creatinine level exceeds 4.0 mg/dL, the value is restricted to a maximum of 4.0 mg/dL.
- Serum sodium levels are modified to ensure they fall within the 125 to 137 mEq/L range.
- The initial MELD score, termed MELD(i), is calculated using the formula:
0.957 × ln(creatinine) + 0.378 × ln(bilirubin) + 1.120 × ln(INR) + 0.643
This score is then multiplied by 10 and rounded.
- If the MELD(i) score exceeds 11, a sodium adjustment is applied to obtain the MELD-Na score:
MELD-Na = MELD(i) + 1.32 × (137 − sodium) − (0.033 × MELD(i) × (137 − sodium))
- The resulting MELD-Na score is capped at a maximum value of 40 to prevent excessively high values.

Interpreting the Results:

- A MELD-Na score of 9 or less indicates a low mortality risk of 1.9%.
- Scores between 10 and 19 suggest a moderate risk with a mortality rate of 6.0%.
- For scores between 20 and 29, the mortality risk increases to 19.6%.
- A score in the range of 30 to 39 corresponds to a substantial risk of 52.6%.
- Finally, a MELD-Na score of 40 or greater reflects the highest mortality risk at 71.3%.

By following these steps, the calculator systematically provides an objective assessment of liver disease severity and correlates it with associated mortality risks, aiding clinicians in clinical decision-making.

---

### Confidence reports in decision-making with multiple alternatives violate the Bayesian confidence hypothesis [^b245f51e]. Nature Communications (2020). High credibility.

Inference on a given trial. We assume that the observer knows the mean and standard deviation of each category based on the exemplar dots, and that the observer assumes that the three categories have equal probabilities. The posterior probability of category C given the measurement x is then p (C | x) ∝ p (x | C) = N (x; m C, (σ s 2 + σ 2) I). Instead of the true posterior p (C | x), the observer makes the decisions based on q (C | x), a noisy version of the posterior probability. We obtain a noisy posterior q (C | x) by drawing from a Dirichlet distribution. The Dirichlet distribution is a generalization of the beta distribution. Just like the beta distribution is a continuous distribution over the probability parameter of a Bernoulli random variable, the Dirichlet distribution is a distribution over a vector that represents the probabilities of any number of categories. The Dirichlet distribution is parameterized as

Γ represents the gamma function. p is a vector consisting of the three posterior probabilities, p = (p 1, p 2, p 3) = (p (C = 1 | x), p (C = 2 | x), p (C = 3 | x)). q is a vector consisting of the three posterior probabilities perturbed by decision noise, q = (q 1, q 2, q 3) = (q (C = 1 | x), q (C = 2 | x), q (C = 3 | x)). The expected value of q is p. The concentration parameter α is a scalar whose inverse determines the magnitude of the decision noise; as α increases, the variance of q decreases. To make a category decision, the observer chooses the category that maximizes the posterior probability:

---

### Evaluating molecular biomarkers for the early detection of lung cancer: when is a biomarker ready for clinical use? An official American Thoracic Society policy statement [^4009df9d]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

American Thoracic Society policy statement — analytic performance characteristics and result types are defined. Limit of detection is "The lowest assay level at which the presence of the analyte is detected with reliability in repeated measurement". Linearity is "The ability to provide results directly proportional to the amount of analyte in the test sample within a given measuring range". Measurement accuracy is "The closeness of agreement between a measurement result and an accepted reference value; an aggregate of trueness and precision". Performance around the cutoff is "The measurement accuracy of an assay at biomarker levels near the threshold chosen to distinguish a positive and negative result for the intended use of the test". Precision is "The closeness of agreement of replicate test results under stipulated conditions", with repeatability defined as "Precision when replicate measurements are taken under the same conditions (within a run)" and reproducibility as "Precision when one of the conditions being varied is the laboratory for an in vitro diagnostic measurement". A qualitative result is "A biomarker result consisting of a set number of possible responses (often two)", whereas a quantitative result is "A biomarker result that is numerical in amount or level of a physical quantity", and semiquantitative denotes "Results of a test that fall into an approximate range of values". Signature refers to "Multiple variables combined to provide a single result", and medical tests are "Results of a clinical, imaging, or laboratory-based assay that are used alone or with other information to help assess a subject's health condition of interest, or target condition".

---

### MRNAs, proteins and the emerging principles of gene expression control [^2d1f7bae]. Nature Reviews: Genetics (2020). High credibility.

Gene expression involves transcription, translation and the turnover of mRNAs and proteins. The degree to which protein abundances scale with mRNA levels and the implications in cases where this dependency breaks down remain an intensely debated topic. Here we review recent mRNA-protein correlation studies in the light of the quantitative parameters of the gene expression pathway, contextual confounders and buffering mechanisms. Although protein and mRNA levels typically show reasonable correlation, we describe how transcriptomics and proteomics provide useful non-redundant readouts. Integrating both types of data can reveal exciting biology and is an essential step in refining our understanding of the principles of gene expression control.

---

### An analysis of multirules for monitoring assay quality control [^b37bd68a]. Laboratory Medicine (2020). Medium credibility.

Background

Multirules are often employed to monitor quality control (QC). The performance of multirules is usually determined by simulation and is difficult to predict. Previous studies have not provided computer code that would enable one to experiment with multirules. It would be helpful for analysts to have computer code to analyze rule performance.

Objective

To provide code to calculate power curves and to investigate certain properties of multirule QC.

Methods

We developed computer code in the R language to simulate multirule performance. Using simulation, we studied the incremental performance of each rule and determined the average run length and time to signal.

Results

We provide R code for simulating multirule performance. We also provide a Microsoft Excel spreadsheet with a tabulation of results that can be used to create power curves. We found that the R4S and 10x rules add very little power to a multirule set designed to detect shifts in the mean.

Conclusion

QC analysts should consider using a limited-rule set.

---

### Approaches to working in high-dimensional data spaces: gene expression microarrays [^4f0907bf]. British Journal of Cancer (2008). Low credibility.

Although feature selection is integral to each of these analytical tasks, an exhaustive search of all 2 n −1 possible feature subsets is prohibitive for large n. Thus, practical feature selection techniques are of necessity heuristic, with an inherent accuracy/complexity tradeoff. Moreover, while multivariate analysis methods based on complex criterion functions may reveal subtle joint marker effects, they are also prone to overfitting. Additionally, high dimensionality compromises the ability to validate marker discovery, which requires accurately measuring true and false discovery rates. These issues have prompted the development of a variety of novel statistical methods for estimating (or controlling for) false discoveries.

---

### Daptomycin (Cubicin RF) [^51211821]. FDA (2025). Medium credibility.

12.1 Mechanism of Action

Daptomycin is an antibacterial drug [see Clinical Pharmacology (12.4)].

12.2 Pharmacodynamics

Based on animal models of infection, the antimicrobial activity of daptomycin appears to correlate with the AUC/MIC (area under the concentration-time curve/minimum inhibitory concentration) ratio for certain pathogens, including S. aureus. The principal pharmacokinetic/pharmacodynamic parameter best associated with clinical and microbiological cure has not been elucidated in clinical trials with CUBICIN RF.

CUBICIN Administered over a 30-Minute Period in Adults

The mean and standard deviation (SD) pharmacokinetic parameters of daptomycin at steady-state following intravenous (IV) administration of CUBICIN over a 30-minute period at 4 to 12 mg/kg every 24h to healthy young adults are summarized in Table 12.

Daptomycin pharmacokinetics were generally linear and time-independent at CUBICIN doses of 4 to 12 mg/kg every 24h administered by IV infusion over a 30-minute period for up to 14 days. Steady-state trough concentrations were achieved by the third daily dose. The mean (SD) steady-state trough concentrations attained following the administration of 4, 6, 8, 10, and 12 mg/kg every 24h were 5.9 (1.6), 6.7 (1.6), 10.3 (5.5), 12.9 (2.9), and 13.7 (5.2) mcg/mL, respectively.

CUBICIN Administered over a 2-Minute Period in Adults

Following IV administration of CUBICIN over a 2-minute period to healthy adult volunteers at doses of 4 mg/kg (N = 8) and 6 mg/kg (N = 12), the mean (SD) steady-state systemic exposure (AUC) values were 475 (71) and 701 (82) mcg∙h/mL, respectively. Values for maximum plasma concentration (Cmax) at the end of the 2-minute period could not be determined adequately in this study. However, using pharmacokinetic parameters from 14 healthy adult volunteers who received a single dose of CUBICIN 6 mg/kg IV administered over a 30-minute period in a separate study, steady-state Cmaxvalues were simulated for CUBICIN 4 and 6 mg/kg IV administered over a 2-minute period. The simulated mean (SD) steady-state Cmaxvalues were 77.7 (8.1) and 116.6 (12.2) mcg/mL, respectively.

---

### Article 5. An introduction to estimation – 2: from z to T [^fb82b806]. Emergency Medicine Journal (2001). Low credibility.

Provided the sample size is large enough (that is, n greater than 100), the z statistic can be used to determine the confidence interval estimation of the population mean even when the sigma is not known. In these cases the estimation of the standard error of the mean is used. The z statistic is also valid when determining the population's proportion based upon a large sample. However, when dealing with smaller samples, the z statistic is replaced by the t statistic. This makes it possible to estimate, in a population with an unknown standard deviation: The probability of getting a sample mean greater than or equal to a particular value The value of a sample mean with a particular probability of occurring The probability of getting a sample mean between two particular values The confidence interval for the estimation of the population mean can also be determined using the t statistic.

---

### An expectation-maximization algorithm for the analysis of allelic expression imbalance [^80a63f76]. American Journal of Human Genetics (2006). Low credibility.

A significant proportion of the variation between individuals in gene expression levels is genetic, and it is likely that these differences correlate with phenotypic differences or with risk of disease. Cis-acting polymorphisms are important in determining interindividual differences in gene expression that lead to allelic expression imbalance, which is the unequal expression of homologous alleles in individuals heterozygous for such a polymorphism. This expression imbalance can be detected using a transcribed polymorphism, and, once it is established, the next step is to identify the polymorphisms that are responsible for or predictive of allelic expression levels. We present an expectation-maximization algorithm for such analyses, providing a formal statistical framework to test whether a candidate polymorphism is associated with allelic expression differences.

---

### Approaches to working in high-dimensional data spaces: gene expression microarrays [^ab1261e3]. British Journal of Cancer (2008). Low credibility.

PREDICTIVE CLASSIFICATION

Performance of a predictive model depends on the interrelationship between sample size, data dimensionality, and model complexity. The accuracy of learned models tends to deteriorate in high dimensions, a phenomenon called the 'curse of dimensionality'. This phenomenon is illustrated for classification by an example by. Consider two equally probable, normally distributed classes with common variance in each dimension. For the feature indexed by n = 1,2,3…, class 1 has mean 1/ n 1/2 and class 2 has mean −1/ n 1/2. Thus, each additional feature has some class discrimination power, albeit diminishing as n increases. Trunk evaluated error rates for the Bayes decision rule, applied as a function of n, when the variance is assumed known but the class means are estimated based on a finite data set. Trunk found that (1) the best test error was achieved using a finite number of features; (2) using an infinite number of features, test error degrades to the accuracy of random guessing; and (3) the optimal dimensionality increases with increasing sample size. These observations are consistent with the 'bias/variance dilemma'. Simple models may be biased but will have low variance. More complex models have greater representation power (low bias) but overfit to the particular training set (high variance). Thus, the large variance associated with using many features (including those with modest discrimination power) defeats any possible classification benefit derived from these features (Figure 1). With severe limits on available samples in microarray studies, complex models using high-feature dimensions will severely overfit, greatly compromising classification performance. Computational learning theory provides distribution-free bounds on generalisation accuracy in terms of a classifier's capacity, related to model complexity. Relevance of these bounds to the microarray domain is discussed e.g. by.

---

### Anthropogenic ecosystem disturbance and the recovery debt [^9575411e]. Nature Communications (2017). Medium credibility.

The same approach was used to estimate the recovery debt value with the linear approximation. To estimate the area representing the recovery debt value, we used a linear function that merges any two points with positive slope. After calculation, the resulting formula is. We also used a linear function that merges any two points with negative slope and after calculation the resulting formula is. Finally, when X r = 0, we approximated the recovery debt ratio as.

The presence of zero values in the outcome measures could produce abnormally high or low values of our recovery debt estimations. This issue occurred in two situations: (i) when X r = 0 (scenario e) and (ii) when X s or X e = 0 (n = 628). To select the best approach to tackle this issue, we tested nine different strategies (Supplementary Table 3) to calculate the r parameter required to estimate the recovery debt. From them, six included the use of a constant value added to the numerator and denominator (that is, 0.01, 0.05, 0.1, 0.5 and 1) of the formula used to calculate r and four included the use of an amount specific for each outcome measure. These specific amounts were (i) an amount of the same order of magnitude than X s and X e located at the beginning of the order of magnitude (for example, 0.1, 1 and 10), (ii) the same than (i) but with the median magnitude (for example, 0.5, 5 and 50), (iii) an amount one order of magnitude larger than X s and X e located at the beginning of the order of magnitude and (iv) the same as (iii) but with the median magnitude. For example, if X s = 0.81, the four amounts were 0.1, 0.5, 1 and 5, respectively. We compared the data distribution (median and 95% confidence interval) of each strategy with the rest of the database without the zero values using Mann–Whitney rank sum tests. Only the approach using an amount of the same order of magnitude than X s and X e at the median magnitude had a nonsignificant data distribution (P > 0.05) different from the rest of the database and thus this was the strategy we used in the two situations where we needed to address zero values. This strategy was not used in the rest of the database not affected by zero values. To further ensure that there were no differences between this approach and an approach that simply excludes all zero values, we compared the recovery debt excluding and including outcome measures with zero values and we did not find qualitative differences that could lead to different conclusions (Supplementary Fig. 7).

---

### Solving the where problem and quantifying geometric variation in neuroanatomy using generative diffeomorphic mapping [^fd44f32f]. Nature Communications (2025). High credibility.

The statistical interpretation allows us to accommodate images with non reference signals, such as missing tissue, tracer injection sites, or other anomalies. At each pixel, the identity of the signal type is modeled as missing data, and maximum likelihood estimators are computed using an Expectation Maximization algorithm, which alternates between the E step: compute posterior probability π i (x) that each pixel corresponds to the reference image rather than one of the non-reference types, and the M step: update parameters by solving a posterior weighted version of the above:As an EM algorithm, this approach is guaranteed to be monotonically increasing in likelihood. An example of posterior weights are shown in the right hand column of Fig. 2 b.

Our approach uses mixtures of Gaussians to model variability in data, to allow large outliers to be accommodated by additional components, even though the Gaussian distribution itself does not have long tails. The Gaussian model allows for closed form expression (in terms of matrix inverse) for contrast transformation parameters. Other groups have used long tailed distributions to model variability and outliers in a robust manner, most notably the exponential distribution for l1 optimization. Techniques such as iteratively reweighted least squares can be applied as in Reuter et al. which lead lead to a weighted least squares problem which is similar to ours.

Nonconvex optimization with low to high dimensional subgroups and resolutions

This registration problem is highly nonconvex, and allows for many local minima. To provide robustness in our solution, we solve a sequence of lower dimensional subproblems, initializing the next with the solution to the previous. (i) 2D slice to slice rigid alignment maximizing similarity to neighbors(ii) 3D affine only alignment, registration using the full model at (iii) low (200 μm), (iv) medium (100 μm), and (v) high (50 μm) resolution. Time varying velocity fields are discretized into 5 timesteps and integrated using the Semi Lagrangian method. For most subproblems, spatial transformation parameters are estimated by gradient descent, and intensity transformation parameters are updated by solving a weighted least squares solution at each iteration. For subproblems that include linear registration only, parameters are estimated using Reimannian gradient descent (discussed in ref.and similar to a second order Gauss–Newton optimization scheme).

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^345386c9]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Selecting and reporting reference values — procedural guidance states that PFT laboratories must select appropriate reference values generated from high-quality data from a large sample of healthy asymptomatic individuals who have never smoked or had other respiratory illness or significant exposures, reports should identify the source of reference values, any change in selected equations should be noted and prior percent predicted values recalculated if possible, pediatric–adult equation discontinuity is discouraged, and extrapolation beyond the age range should not be done during growth, increases uncertainty in the elderly, and must be noted in technician comments.

---

### Chromosome-scale and haplotype-resolved genome assembly of a tetraploid potato cultivar [^73462c60]. Nature Genetics (2022). High credibility.

Linkage-based grouping of contigs

The read count (denoted by r) at each coverage marker of size W (in bp) for each of the selected 717 pollen genomes (Supplementary Fig. 2), which were collected using bedtools (v.2.29.0), was firstly normalized using n r = r × (10 4 / W) × (10 6 / N), where N was the total number of reads aligned to the assembly of 6,366 contigs. Meanwhile, the average read count, m r, for all coverage markers with more than 7 × (N /10 6) reads was calculated for each pollen genome. The coverage (genotype) at a marker for each pollen was set to n r / m r (which would be rounded to 0, 1 or 2). In general, coverages across 717 pollen read sets at the same window marker were used to build up a PAP: X = x 1 × 2… x 717, where x i was in {0, 1}, {0, 1, 2}, {1, 2} or {2}, depending on whether the marker type was haplotig, diplotig, triplotig or tetraplotig. The correlation between two PAPs X and Y was calculated as cor XY = sum[(x i - m x) × (y i - m y)] / [sqrt(sum(x i - m x) 2) × sqrt(sum(y i - m y) 2)], where m x and m y were the respective average values. Initially, any pair of haplotigs/vertices (with sizes ≥ 100 kb) was connected by an edge, if the highest correlation value between the PAPs of the markers at two ends of the two haplotigs was > 0.55 (Supplementary Fig. 3). This graph-based clusteringled to 48 groups representing the 48 haplotypes (Extended Data Fig. 2). If the lowest correlation value between any pair of the markers of any two groups was less than –0.25, they could be determined as homologous linkage groups (same chromosome, different haplotypes). With this, the 48 groups were clustered into 12 chromosomes, each with four different haplotypes.

---

### Mean daily temperatures predict the thermal limits of malaria transmission better than hourly rate summation [^a2566da6]. Nature Communications (2025). High credibility.

Sensitivity and uncertainty analysis

We performed two types of sensitivity analysis and an uncertainty analysis on each version of the suitability model to determine which traits were most important for determining the thermal optimum and limits for transmission and how each trait contributed to the uncertainty in S (T). First, we used a partial derivative approach, calculating ∂S/∂x·∂x/∂T across the temperature (T) gradient for each trait (x). This approach only works for the models without rate summation (i.e. model 1: constant and model 2: empirical fluctuating) because it uses the derivatives of the quadratic and Brière functions and their fitted parameters (T min, T max, and q) for each trait. Second, we held each trait constant while allowing all others to vary with temperature. Finally, we calculated the HPD interval (highest posterior density interval, the smallest interval of predicted trait value encompassing 95% of the probability density in the posterior distribution) across the temperature gradient for S (T) using the full posterior distributions for all traits (i.e. full uncertainty) and for S (T) with each trait given its mean value (i.e. removing the uncertainty for one trait at a time). We then compared the relative size of the HPD in both conditions for each trait.

---

### The basic reproduction number of SARS-CoV-2 in wuhan is about to die out, how about the rest of the world? [^cc93c9ef]. Reviews in Medical Virology (2020). Medium credibility.

The virologically confirmed cases of a new coronavirus disease (COVID-19) in the world are rapidly increasing, leading epidemiologists and mathematicians to construct transmission models that aim to predict the future course of the current pandemic. The transmissibility of a virus is measured by the basic reproduction number (R 0), which measures the average number of new cases generated per typical infectious case. This review highlights the articles reporting rigorous estimates and determinants of COVID-19 R 0 for the most affected areas. Moreover, the mean of all estimated R 0 with median and interquartile range is calculated. According to these articles, the basic reproduction number of the virus epicentre Wuhan has now declined below the important threshold value of 1.0 since the disease emerged. Ongoing modelling will inform the transmission rates seen in the new epicentres outside of China, including Italy, Iran and South Korea.

---

### ACR-AAPM-SPR practice parameter for diagnostic reference levels and achievable doses in medical X-ray imaging [^43dcb27d]. ACR/AAPM/SPR (2023). High credibility.

Table 4 — Patient size–based DRL and AD values for pediatric CT chest without contrast are organized by effective diameter (Eff Dia) and provide CTDIvol, SSDE, and DLP. Representative rows include 12- < 16 cm: CTDIvol AD 1.7 and DRL 1.9, SSDE AD 3.7 and DRL 4.2, DLP AD 29 and DRL 41; 24- < 28 cm: CTDIvol AD 3.5 and DRL 4.5, SSDE AD 4.8 and DRL 6.5, DLP AD 122 and DRL 154; and 32- < 36 cm: CTDIvol AD 5.7 and DRL 7.4, SSDE AD 6.0 and DRL 7.9, DLP AD 213 and DRL 249.

---

### Society of Surgical Oncology consensus statement: assessing the evidence for and utility of gene expression profiling of primary cutaneous melanoma [^bc85398a]. Annals of Surgical Oncology (2025). High credibility.

Society of Surgical Oncology consensus process — voting threshold and update cadence specify that consensus on recommendations required "a minimum of 80% agreement" and that "Review and any necessary updates are planned for within 3–5 years of publication".

---

### Evaluating molecular biomarkers for the early detection of lung cancer: when is a biomarker ready for clinical use? An official American Thoracic Society policy statement [^f50ffd58]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Appendix 1: Definitions — biomarker evaluation terminology is clarified, defining a biomarker as a characteristic that is objectively measured and evaluated as an indicator of biological processes or responses, and specifying analytical validity as acceptable performance in measuring or detecting biomarker characteristics. The appendix further defines clinical validity as the demonstrated association of a test result with the presence or absence of the target condition, and clinical utility as a biomarker's ability to improve clinical outcomes when measured and used as directed for its intended use. Additional key terms include context of use (what the test measures, why, and in what populations it should be used), diagnostic accuracy (agreement between the new test and the reference standard), companion diagnostic (a biomarker essential for the safe and effective use of a therapy), and confidence intervals (an interval about a point estimate that quantifies statistical uncertainty due to variability in the subject/sample selection process). The appendix also defines related concepts such as clinical performance, clinical reference standard, diagnostic marker, and lead time.

---

### Experimental noise cutoff boosts inferability of transcriptional networks in large-scale gene-deletion studies [^6acf6e02]. Nature Communications (2018). Medium credibility.

Generating a comprehensive map of molecular interactions in living cells is difficult and great efforts are undertaken to infer molecular interactions from large-scale perturbation experiments. Here, we develop the analytical and numerical tools to quantify the fundamental limits for inferring transcriptional networks from gene knockout screens and introduce a network inference method that is unbiased with respect to measurement noise and scalable to large network sizes. We show that network asymmetry, knockout coverage and measurement noise are central determinants that limit prediction accuracy, whereas the knowledge about gene-specific variability among biological replicates can be used to eliminate noise-sensitive nodes and thereby boost the performance of network inference algorithms.

---

### Duration of salmeterol-induced bronchodilation in mechanically ventilated chronic obstructive pulmonary disease patients: a prospective clinical study [^b24abe83]. Critical Care (2008). Low credibility.

Figure 1
Percentage change (Δ-difference %) in peak airway pressure (Ppk), minimum inspiratory resistance (R int), maximum inspiratory resistance (Rrs), and intrinsic positive end-expiratory pressure (PEEPi) after salmeterol administration.

Figure 2
Effect of salmeterol on airway pressures and inspiratory resistance mean values. (a) Mean values (± 1 SD) of peak airway pressure (Ppk) and intrinsic positive end-expiratory pressure (PEEPi) at baseline and up to 12 hours after salmeterol administration. (b) Mean values (± 1 SD) of minimum inspiratory resistance (R int) and maximum inspiratory resistance (Rrs) at baseline and up to 12 hours after salmeterol administration. *Significantly different from baseline values (P < 0.05, Wilcoxon signed rank test). SD, standard deviation.

Figure 3
Individual patient values of minimum inspiratory resistance (R int) before and after salmeterol administration. Baseline values (before salmeterol) are indicated at time 0. The closed circles connected by the thick solid line represent the mean R int value for the whole patient group. -◆-: Patient #1; -■-: Patient #2; -x-: Patient #3; -✷-: Patient #4; -○-: Patient #5; -●-: Patient #6; -□-: Patient #7; -△: Patient #8; — x —: Patient #9; -◇-: Patient #10.

Figure 4
Time after salmeterol administration that minimum inspiratory resistance (R int) remained less than 85% of baseline in each patient.

---

### Single photon emission computed tomography (SPECT) myocardial perfusion imaging guidelines: instrumentation, acquisition, processing, and interpretation [^097e8756]. Journal of Nuclear Cardiology (2018). Medium credibility.

Table 2 — typical performance parameters for low-energy (< 150 keV) parallel-hole collimators — reports resolution (full width at half maximum [FWHM]) at 10 cm, efficiency (%), and relative efficiency: Ultra-high resolution: 6.0 mm, 0.005, 0.3; High resolution: 7.8 mm, 0.010, 0.6; All/general purpose: 9.3 mm, 0.017, 1.0*; High sensitivity: 13.2 mm, 0.035, 2.1. A footnote specifies that a relative efficiency of 1 corresponds approximately to a collimator efficiency of 0.017%, and defines efficiency as the fraction of gamma rays and X-rays passing through the collimator per gamma ray and X-ray emitted by the source.

---

### Identifying domains of applicability of machine learning models for materials science [^72c4ba76]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ν that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by σ lin (x 1, x 2) ≡ −0.3 ≤ x 1 ≤ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: σ rbf ≡ −3.3 ≤ x 11 ≤ 3.1 ∧ x 2 ≤ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA σ poly (x 1, x 2) ≡ −3.5 ≤ x 2 ≤ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### Principles of analytic validation of immunohistochemical assays: guideline update [^cda8439c]. Archives of Pathology & Laboratory Medicine (2024). High credibility.

Supplemental Digital Content — AMSTAR (Assessing the Methodological Quality of Systematic Reviews) appraisal of methodological quality presents checklist items marked with "√" or "x": "Quality assessed & documented" is "√" and "x"; "Quality used appropriately for conclusion" is "√" and "x"; "Methods to combine used appropriately" is "√" and "√"; "Publication bias assessed" is "x" and "x"; and "COI (conflict of interest)" is "√" and "√". The AMSTAR SCORE /11 is 9 and 5.

---

### Experimental noise cutoff boosts inferability of transcriptional networks in large-scale gene-deletion studies [^361afac2]. Nature Communications (2018). Medium credibility.

Assuming that the observed node activities follow a multivariate normal distribution, we can find estimates for the unknown orthogonal matrix U, the singular values Σ, and the observational noise σ by maximising the log-likelihood functionunder the constraint, with U k the k -th column vector of U and δ lk the Kronecker delta. It fact, it suffices to constrain the norm of the vectors, as the corresponding maximum likelihood solution leads to an eigenvalue problem with U k as eigenvectors, which can always be made orthogonal. We can therefore define the likelihood function byHere, anddenote maximum likelihood estimates of the covariance matrix. From this definition of y ref follows that the initially introduced perturbation vector, u, must satisfy. We further defined with λ k a Lagrange multiplier and denoted by tr(.) the trace of a matrix. In the following calculations, we substitute S by the unbiased sample covariance matrix, S → P (P − 1) −1 S. Note that V must disappear in the likelihood function as the covariance matrix of u is invariant under any orthogonal transformation u → V T u.

The maximum of the log-likelihood function is determined by the conditions, and, which results inshowing that maximum likelihood estimates of, andare determined by the sample covariance matrix S. If N 0 > 1 and the full-rank sample covariance matrix is significantly different from a block-diagonal form — e.g. the network is not separable in subnetworks–the orientations of the corresponding N 0 eigenvectors are determined by sampling noise in the space orthogonal to remaining N − N 0 eigenvectors. In case that we have less data points than nodes in the network — e.g. the number of perturbed nodes times their replicates is smaller than the network size — some of the N 0 smallest eigenvalues become exactly zero and as a consequence the noise level, is underestimated. Although a maximum likelihood solution exists in this case, it is necessary to regularise the covariance matrix, witha regularisation parameter, as a correct estimate of the noise level is essential for statistical analysis. Note that the derivation of the maximum likelihood solution is mathematically equivalent to the derivation of principle component analysis from a probabilistic perspective.

---

### 2021 ACC / AHA key data elements and definitions for heart failure: a report of the American college of cardiology / American Heart Association task force on clinical data standards (writing committee to develop clinical data standards for heart failure) [^24c1d525]. Circulation: Cardiovascular Quality and Outcomes (2021). High credibility.

ACC/AHA heart failure data standards — chemotherapy dose units list permissible values including mg/m², mg/kg, and mg/d.

---

### Quantifying threshold scores for patient satisfaction after massive rotator cuff repair for the interpretation of mid-term patient-reported outcomes [^2d144ef0]. Arthroscopy (2024). Medium credibility.

Purpose

To establish minimal clinically important difference (MCID) and patient acceptable symptomatic state (PASS) values for 4 patient-reported outcomes (PROs) in patients undergoing arthroscopic massive rotator cuff repair (aMRCR): American Shoulder and Elbow Surgeons (ASES) score, Subjective Shoulder Value (SSV), Veterans Rand-12 (VR-12) score, and the visual analog scale (VAS) pain. In addition, our study seeks to determine preoperative factors associated with achieving clinically significant improvement as defined by the MCID and PASS.

Methods

A retrospective review at 2 institutions was performed to identify patients undergoing aMRCR with minimum 4-year follow-up. Data collected at the 1-year, 2-year, and 4-year time points included patient characteristics (age, sex, length of follow-up, tobacco use, and workers' compensation status), radiologic parameters (Goutallier fatty infiltration and modified Collin tear pattern), and 4 PRO measures (collected preoperatively and postoperatively): ASES score, SSV, VR-12 score, and VAS pain. The MCID and PASS for each outcome measure were calculated using the distribution-based method and receiver operating characteristic curve analysis, respectively. Pearson and Spearman coefficient analyses were used to determine correlations between preoperative variables and MCID or PASS thresholds.

Results

A total of 101 patients with a mean follow-up of 64 months were included in the study. The MCID and PASS values at the 4-year follow-up for ASES were 14.5 and 69.4, respectively; for SSV, 13.7 and 81.5; for VR-12, 6.6 and 40.3; and for VAS pain, 1.3 and 1.2. Greater infraspinatus fatty infiltration was associated with failing to reach clinically significant values.

Conclusions

This study defined MCID and PASS values for commonly used outcome measures in patients undergoing aMRCR at the 1-year, 2-year, and 4-year follow-up. At mid-term follow-up, greater preoperative rotator cuff disease severity was associated with failure to achieve clinically significant outcomes.

Level Of Evidence

Level IV, case series.

---

### Patients undergoing hip arthroscopy with concomitant periacetabular osteotomy demonstrate clinically meaningful improvement at 2 years using the patient-reported outcome measurement information system and international hip outcome tool 12 [^6cbdb834]. Arthroscopy (2025). Medium credibility.

Purpose

To report the minimal clinically important difference (MCID) and substantial clinical benefit (SCB) values using the Patient-Reported Outcome Measurement Information System (PROMIS) and International Hip Outcome Tool 12 (iHOT-12) in patients undergoing hip arthroscopy (HA) with concomitant periacetabular osteotomy (PAO), HA + PAO for acetabular dysplasia, and intra-articular pathology with a minimum 2-year follow-up.

Methods

Data from patients who underwent HA + PAO were prospectively collected and retrospectively analyzed. Inclusion criteria consisted of patients who had a diagnosis of hip dysplasia or hip instability and had a minimum 2-year patient-reported outcome measure follow-up. Data were collected electronically preoperatively and postoperatively at 6 months, 1 year, and 2 years. Outcome measures analyzed were the iHOT-12 and PROMIS computer adaptive tests: Physical Function (PF), Pain Interference (PI), and Global Physical Health (GPH). MCID and SCB were calculated for these measures.

Results

In total, 106 patients were included in the study with an average age of 23.5 ± 6.6 years, an average body mass index of 24.3, and the majority being female (94%). The values for MCID were calculated to be 40.9, 40.7, 60.2, and 43.5, and the percentage achieving MCID at 2 years was 82.6%, 82.9%, 79.6%, and 80.1% for the iHOT-12, PROMIS-PF, PROMIS-PI, and PROMIS-GPH respectively. The 1-year and 2-year SCB scores for ≥ 80% satisfaction and percent achieving were as follows, respectively: iHOT-12, 71.8 (60.3%), 61.9 (65.1%); PROMIS-PF, 47.1 (64.2%), 47.2 (71.7%); PROMIS-PI, 50.6 (48.1%), 52.3 (49.1%); and PROMIS-GPH, 49.3 (54.7%), 49.3 (55.7%). The 1-year and 2-year SCB scores for 100% satisfaction and percent achieving were as follows, respectively: iHOT-12, 80.2 (44.3%), 81.3 (47.2%); PROMIS-PF, 50.7 (46.2%), 50.3 (56.6%); PROMIS-PI, 52.4 (34.9%), 52.4 (49.1%); and PROMIS-GPH, 52.5 (36.8%), 49.3 (55.7%).

Conclusions

This study reports values for MCID and SCB for PROMIS and iHOT-12 at a 2-year follow-up in patients undergoing HA + PAO for hip dysplasia or instability. The percentage of patients achieving MCID ranged from 79.6% to 82.9% at the 2-year follow-up. In addition, the percentage of patients achieving SCB at the 2-year follow-up for ≥ 80% satisfaction ranged from 49.1% to 71.2%, and the percentage for 100% satisfaction ranged from 49.1% to 56.6%.

Level Of Evidence

Level IV, therapeutic case series.

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^cf66bf7c]. Nature Communications (2023). High credibility.

Dose-response curves are key metrics in pharmacology and biology to assess phenotypic or molecular actions of bioactive compounds in a quantitative fashion. Yet, it is often unclear whether or not a measured response significantly differs from a curve without regulation, particularly in high-throughput applications or unstable assays. Treating potency and effect size estimates from random and true curves with the same level of confidence can lead to incorrect hypotheses and issues in training machine learning models. Here, we present CurveCurator, an open-source software that provides reliable dose-response characteristics by computing p-values and false discovery rates based on a recalibrated F-statistic and a target-decoy procedure that considers dataset-specific effect size distributions. The application of CurveCurator to three large-scale datasets enables a systematic drug mode of action analysis and demonstrates its scalable utility across several application areas, facilitated by a performant, interactive dashboard for fast data exploration.

---

### NCCN guidelines® insights: squamous cell skin cancer, version 1.2022 [^37e6faeb]. Journal of the National Comprehensive Cancer Network (2021). High credibility.

Definitive or adjuvant skin brachytherapy — RT dosing and examples specify GOAL BED10 at least 60 Gy; custom surface mold applicators have prescription depth ≤ 5 mm with common examples 3 Gy x 16 fractions (daily) and 5 Gy x 8 fractions (2 times per week [for non-cosmetically sensitive areas]); interstitial brachytherapy has prescription depth > 5 mm with an example 4.4 Gy x 10 fractions (BID [6 hours minimum between fractions]); shielded single channel surface applicators (Leipzig or Valencia) have prescription depth ≤ 3 mm with an example 3 Gy x 12–16 fractions (daily).

---

### Gene regulatory network inference from sparsely sampled noisy data [^c4f997e1]. Nature Communications (2020). High credibility.

The GP framework can handle combinatorial effects, meaning nonlinearities that cannot be decomposed into f (x 1, x 2) = f 1 (x 1) + f 2 (x 2). This is an important property for modelling a chemical system — such as gene expression — where reactions can happen due to combined effects of reactant species. For example, the dynamics corresponding to a chemical reaction x 1 + x 2 → x 3 cannot be modelled by.

In a classical variable selection problem from input–output data η j = f (ξ j) + v j, for j = 1, …, N, where, the task is to find out which components of ξ does the function f depend on. In the GP framework, variable selection can be done by a technique known as "automatic relevance determination", based on estimating the hyperparameters of the covariance function of f. For BINGO, we have chosen the squared exponential covariance function for each component f i :The hyperparameters β i, j ≥ 0 are known as inverse length scales, sincecorresponds to a distance that has to be moved in the direction of the j th coordinate for the value of function f i to change considerably. If β i, j = 0, then f i is constant in the corresponding direction. The mean function for f i is m i (x) = b i − a i x i where a i and b i are regarded as nonnegative hyperparameters corresponding to mRNA degradation and basal transcription, respectively.

---

### The interdependent network of gene regulation and metabolism is robust where it needs to be [^34456ef9]. Nature Communications (2017). Medium credibility.

In a first step, we sample some full cascade trajectories in order to check our expectation of different responses of the system to small perturbations applied in either G R or G M; two rather large values of q are chosen and the raw number of unaffected vertices is logged during the cascade. Indeed, already this first approach implies a different robustness of the gene regulatory and the metabolic domains in terms of the transmission of perturbation cascades to the other domains. Cascades initiated in the metabolic domain of the network tend to be rather restricted to this domain, while the system seems much more susceptible to small perturbations applied in the gene regulatory domain. This effect can be seen both in the overall sizes of the aggregated cascades as well as in the domain which shows the largest change with respect to the previous time step (largest set of newly affected vertices), which we indicate by black markers in Fig. 3. More sample trajectories are shown in Supplementary Figs. 1–5 and, although they illustrate occasionally large fluctuations between the behaviors of single trajectories, they are consistent with this first observation. They also show that considerably larger metabolic perturbations are needed for large cascades and back-and-forth propagation between domains to emerge.

Fig. 3
Sample trajectories of the integrative E. coli network. Initial perturbations differ in size, q = 1 − p = 0.01 (a, b) and q = 0.03 (c, d), as well as in localization: perturbations are initiated in the gene regulatory domain (a, c) and the metabolic domain (b, d). In each panel, the vertex states are plotted against the update steps according to Eqs. (1) and (2). The vertices are grouped according to the domain affiliation, starting from gene regulatory domain (top, green) via protein-interface domain (middle, blue) towards metabolic domain (bottom, red). Within each domain they are arranged with respect to the update step, in which they were affected in the respective run (vertex ordering within the domains is thus different from run to run). Affected vertices are shown in the color of their domain, unaffected vertices are shown last for each run and in white. The horizontal dashed lines denote the domain boundaries and the black square indicates the domain with the largest number of newly affected vertices in each step. Connections between black squares are only shown to guide the eye

---

### Estimating the statistical significance of gene expression changes observed with oligonucleotide arrays [^ba8afa58]. Human Molecular Genetics (2002). Low credibility.

We present a simple method to assign approximate P-values to gene expression changes detected with Affymetrix oligonucleotide arrays and software. The method pools data for groups of genes and a small number of like-to-like comparisons in order to estimate the significance of changes observed for single genes in comparisons of experimental interest. Statistical significance levels are based on the observed variability in the fractional majority of probe pairs that indicate increasing or decreasing differential expression in comparisons of technical replicates. From this reference distribution or error model, we compute the expected frequency for fractional majorities in comparisons for N ≥ 2. These computed distributions are the source of P-value estimates for changes seen in the experimental comparisons. The method is intended to complement the Affymetrix software and to rationalize gene selection for experimental designs involving limited replication.

---

### Global quantification of mammalian gene expression control [^6eab7aec]. Nature (2011). Excellent credibility.

Gene expression is a multistep process that involves the transcription, translation and turnover of messenger RNAs and proteins. Although it is one of the most fundamental processes of life, the entire cascade has never been quantified on a genome-wide scale. Here we simultaneously measured absolute mRNA and protein abundance and turnover by parallel metabolic pulse labelling for more than 5,000 genes in mammalian cells. Whereas mRNA and protein levels correlated better than previously thought, corresponding half-lives showed no correlation. Using a quantitative model we have obtained the first genome-scale prediction of synthesis rates of mRNAs and proteins. We find that the cellular abundance of proteins is predominantly controlled at the level of translation. Genes with similar combinations of mRNA and protein stability shared functional properties, indicating that half-lives evolved under energetic and dynamic constraints. Quantitative information about all stages of gene expression provides a rich resource and helps to provide a greater understanding of the underlying design principles.

---

### Validating whole slide imaging for diagnostic purposes in pathology: guideline from the college of American pathologists pathology and laboratory quality center [^ddcb0ec9]. Archives of Pathology & Laboratory Medicine (2013). Medium credibility.

Supplemental Table 12 — Evidence to Decision Summary of Recommendation 3 reports response distributions as follows: For "Is the problem a priority?", Yes was 70% (7/10) with 20% (2/10) Probably No, 10% (1/10) Probably Yes, and 0 for No, Varies, and Don't Know. For "How accurate is the test?", Very Inaccurate 0, Inaccurate 10% (1/10), Accurate 30% (3/10), Very Accurate 30% (3/10), Varies 20% (2/10), and Don't Know 10% (1/10). For "How substantial are the desirable anticipated effects?", Trivial 0, Small 0, Moderate 20% (2/10), Large 40% (4/10), Varies 20% (2/10), and Don't Know 20% (2/10). For "What is the overall certainty of the evidence of test accuracy?", Very Low 10% (1/10), Low 10% (1/10), Moderate 20% (2/10), High 40% (4/10), Very High 20% (2/10), and No Included Studies 0. For "What is the overall certainty of the evidence of effects of the management that is guided by the test results?", Very Low 10% (1/10), Low 20% (2/10), Moderate 30% (3/10), High 30% (3/10), Very High 10% (1/10), and No Included Studies 0. For "How certain is the link between test results and management decisions?", Very Low 10% (1/10), Low 20% (2/10), Moderate 10% (1/10), High 40% (4/10), Very High 10% (1/10), and No Included Studies 10% (1/10). For "What is the overall certainty of the evidence of effects of the test?", Very Low 10% (1/10), Low 0, Moderate 40% (4/10), High 40% (4/10), Very High 10% (1/10), and No Included Studies 0. For "What is the overall certainty of the evidence for any critical or important direct benefits, adverse effects or burden of the test?", Very Low 10% (1/10), Low 20% (2/10), Moderate 30% (3/10), High 40% (4/10), Very High 0, and No Included Studies 0.

---

### A doubly stochastic renewal framework for partitioning spiking variability [^7cd52800]. Nature Communications (2025). High credibility.

Fig. 2
Estimation of spiking irregularity on synthetic data with known ground truth ϕ.

a We generated synthetic data from an ensemble of doubly stochastic renewal point processes { g (⋅), λ (t)}, where g (⋅) is the gamma distribution and the ground-truth value of ϕ ranges from 0.1 to 1. The firing rate λ (t) is either constant within a trial, sampled across trials from a uniform distribution with the width w (upper row), or a drift-diffusion process with sticky boundaries and the diffusion coefficient D (lower row). The parameters w and D control the trial-to-trial variability of the firing rate. We varied w from 10 to 30 Hz, and D from 5 to 13 Hz 2 /ms. b The ground-truth ϕ (x -axis) versus estimated ϕ (y -axis) for the ensemble of doubly stochastic renewal point processes with uniform (upper row) and drift-diffusion (lower row) firing rate fluctuations. The deterministic time rescaling (DTR) method always overestimates ϕ with error increasing for larger trial-to-trial variability of the firing rate (left). The minimum ratio (MR) method can underestimate or overestimate ϕ depending on the mean and variance of the firing rate, bin size, and the ground truth ϕ itself, producing unpredictable estimation errors (center). The doubly stochastic renewal (DSR) method accurately estimates ϕ in all cases (right). Every point in the scatter plots is the average over 20 simulations for a fixed ϕ and fixed D or w. Each simulation had 100 trials. c Estimation error (root-mean-square error, RMSE, across 20 simulations) for ϕ estimated by the three methods for different values of w (upper row) and D (lower row), which control the trial-to-trial variability of the firing rate. The RMSE increases with the firing rate variability for both DTR and MR methods, whereas the RMSE is consistently low and independent of the firing rate variability for our DSR method. Source data are provided as a Source data file.

---

### Restricted maximum-likelihood method for learning latent variance components in gene expression data with known and unknown confounders [^ce205dd3]. G3 (2022). Medium credibility.

Random effects models are popular statistical models for detecting and correcting spurious sample correlations due to hidden confounders in genome-wide gene expression data. In applications where some confounding factors are known, estimating simultaneously the contribution of known and latent variance components in random effects models is a challenge that has so far relied on numerical gradient-based optimizers to maximize the likelihood function. This is unsatisfactory because the resulting solution is poorly characterized and the efficiency of the method may be suboptimal. Here, we prove analytically that maximum-likelihood latent variables can always be chosen orthogonal to the known confounding factors, in other words, that maximum-likelihood latent variables explain sample covariances not already explained by known factors. Based on this result, we propose a restricted maximum-likelihood (REML) method that estimates the latent variables by maximizing the likelihood on the restricted subspace orthogonal to the known confounding factors and show that this reduces to probabilistic principal component analysis on that subspace. The method then estimates the variance-covariance parameters by maximizing the remaining terms in the likelihood function given the latent variables, using a newly derived analytic solution for this problem. Compared to gradient-based optimizers, our method attains greater or equal likelihood values, can be computed using standard matrix operations, results in latent factors that do not overlap with any known factors, and has a runtime reduced by several orders of magnitude. Hence, the REML method facilitates the application of random effects modeling strategies for learning latent variance components to much larger gene expression datasets than possible with current methods.

---

### Clinical bioinformatician body of knowledge-bioinformatics and software core: a report of the Association for Molecular Pathology [^a9322a42]. The Journal of Molecular Diagnostics (2025). High credibility.

Algorithm development life cycle — clinical machine learning (ML) models in bioinformatics are organized into design and planning, data wrangling, model development and optimization, and clinical testing, beginning with model design and planning and applying validation testing before clinical use, with cycles restarting as assay requirements evolve. Design and planning activities include to "Define value to be predicted", "Identify data set for model training", and "Capture variability expected in production within data sets". Data wrangling tasks include "Data structuring into organized formats", "Data cleaning to remove/correct invalid values and handle missing values", "Data transformation, normalization, and standardization", "Fold data into mutually exclusive training and validation test sets", and "Feature engineering of predictive features for modeling (eg, metrics generated by an upstream pipeline)". Model development and optimization covers "Determine machine learning model class by considering data labels and model complexity vs explainability" (including "Supervised ML (eg, regression, classification)" and "Unsupervised ML (eg, clustering, anomaly detection)"), "Feature selection to identify predictive features", "Iterative model training and testing", and "Optimization to reduce overfitting, such as cross-validation and measurement of learning curves". Clinical testing emphasizes "Calculation of key model evaluation metrics (eg, RMSE, confusion matrix, etc.)", "Evaluation of model bias", "Demonstration of model robustness", and "Model validation (refer to Validation)", with RMSE defined as root mean squared error (RMSE).

---

### Basal cell skin cancer, version 2.2024, NCCN clinical practice guidelines in oncology [^4613451d]. Journal of the National Comprehensive Cancer Network (2023). High credibility.

NCCN basal cell skin cancer — brachytherapy dosing specifies prescription depth bands and example regimens: custom surface mold applicators use prescription depth ≤ 5 mm with common examples 3 Gy x 16 fractions (daily) and 5 Gy x 8 fractions (2 times per week [for non-cosmetically sensitive areas]). Interstitial brachytherapy: prescription depth > 5 mm, with GOAL BED10 at least 60 Gy, and an example regimen of 4.4 Gy x 10 fractions (BID [6 hours minimum between fractions]). Shielded single channel surface applicators (Leipzig or Valencia) specify prescription depth ≤ 3 mm with a common example of 3 Gy x 12–16 fractions (daily).

---

### Opportunities and challenges associated with clinical diagnostic genome sequencing: a report of the Association for Molecular Pathology [^ac422a46]. The Journal of Molecular Diagnostics (2012). Medium credibility.

Clinical diagnostic genome sequencing — base-calling quality scoring uses a Phred score generated on a logarithmic scale (Q = − 10 log E) to estimate error probability, where a Phred of 10 corresponds to a 1/10 probability of error and a Phred of 20 corresponds to 1/100. For sequencing information, the Clinical Laboratory Standards Institute has recommended filtering on a Phred score of 20 to 30, resulting in an error rate of 1/100 to 1/1000, and when interrogating variants in population-level situations such as microbes or tumor samples, higher thresholds may be necessary.

---

### ACR-AAPM-SPR practice parameter for diagnostic reference levels and achievable doses in medical X-ray imaging [^180a661b]. ACR/AAPM/SPR (2023). High credibility.

Pediatric CT diagnostic reference levels — Table 3 provides patient age-based achievable dose (AD) and diagnostic reference level (DRL) values by examination with CTDIvol (mGy), size-specific dose estimate (SSDE, mGy), and dose–length product (DLP, mGy-cm); these benchmarks are the result of an analysis of the top 10 pediatric CT examinations, including more than 1.5 million examinations performed at 1625 facilities from 2016 to 2020, and although the ICRP recommends weight-based pediatric DRLs, this was not possible because the ACR Dose Index Registry does not collect patient weight. Representative values include: Head, age 6–18 y — CTDIvol AD 46 and DRL 55; DLP AD 748 and DRL 910. Sinuses without contrast material, age 2– < 6 y — CTDIvol AD 6.7 and DRL 12; DLP AD 94 and DRL 219. Maxillofacial area without contrast material, age 1– < 2 y — CTDIvol AD 7.0 and DRL 15; DLP AD 127 and DRL 286. Neck soft tissue with contrast material, age 10– < 15 y — CTDIvol AD 7.8 and DRL 11; DLP AD 198 and DRL 270. Cervical spine without contrast material, age 15–18 y — CTDIvol AD 19 and DRL 34; DLP AD 425 and DRL 707. Chest, age 0– < 1 y — CTDIvol AD 1.2 and DRL 1.7; SSDE AD 2.9 and DRL 3.8; DLP AD 22 and DRL 27.

---

### Experimental noise cutoff boosts inferability of transcriptional networks in large-scale gene-deletion studies [^86f43065]. Nature Communications (2018). Medium credibility.

Solving the matrix equation, Eq. (3), for A giveswith Σ + the pseudoinverse of Σ. As the matrix A has full rank, we complement Σ + with an unknown diagonal matrix Σ 0 that has non-zero values where Σ + has zero values and vice versa and complement BV with an unknown orthogonal matrix W. Note that by construction, Σ + U T and Σ + U T map from complementary subspaces and thereby ensure that A has full rank. The fact that V, W and Σ 0 cannot be determined from S shows that A cannot be computed from a single covariance matrix. A more general case arises when measurement noise is independent but not isotropic, σ 2 I → σ 2 D, with D = diag(r 1, r 2, …, r N) a diagonal matrix with known positive elements that contains scaled noise variances, resulting inA transformation to isotropic noise is possible by multiplying both sides of Eq. (12) by, which changes the result Eq. (11) towith U the eigenvectors of.

---

### Experimental noise cutoff boosts inferability of transcriptional networks in large-scale gene-deletion studies [^2ddd1736]. Nature Communications (2018). Medium credibility.

PRC

We aim at inferring direct interactions between N observable molecular components, such as transcripts or proteins, by measuring their concentrations. We define byan N dimensional vector that represents the logarithm of these concentrations, which is the natural scale where experimental data are reported. We assume that the available data set has been generated from P perturbation experiments, which may also include experimental replicates. We further assume that the molecular targets of the perturbations are known, as it is the case for gene knockout or knockdown experiments. The elements of the interaction matrixdefine the strengths of the directed interactions among the molecular components, for example, A ij quantifies the direct impact of component j on component i. Given the available experimental data, our aim is to correctly classify the off-diagonal elements of A as zero or non-zero to obtain the structural organisation of the interaction network. We assume that the observed component abundance on log-scale, y obs, differs from the true value, y, by additive measurement noise, which is characterised by zero mean, and variance, with I N the N dimensional identity matrix. We assume that the observed data can by described to sufficient accuracy by a linear stationary modelwith A negative definite to ensure stability. Equations of this type typically arise from linear expansion of a non-linear model around a reference state, y ref. Linear models are usually preferred for network inference a on larger scale, as the amount of data often limit model complexity and the fact that linear models can give surprisingly good resuits for non-linear cases. The perturbation vector u reflects perturbations that persist long enough to propagate through the network, such as mutations that affect gene activity. Here, u is defined such that for u = 0 the system approaches the reference state y = y ref. Note that the reference state, y ref is not necessarily the unperturbed state but could be also defined as the average over perturbed and unperturbed states. We assume that the perturbation forces are sampled from a standard normal distribution, with meanand covariance matrix. The identity matrix is a consequence of the fact that we can absorb the associated standard deviations of the perturbative forces, u, in the matrix. We introduce normal distributed perturbations for mathematical convenience, as this implies that also y is normal distributed and the resulting maximum likelihood approach is analytically solvable. In general, only the positions of the non-zero elements of B are known from the experimental setup but their actual values are unknown. Using a linear model that operates on log-scale of physical quantities implies that only perturbations can be modelled that act multiplicatively on molecular concentrations. Fortunately, most enzymatic reactions typically fall into this class, such as sequestration and inhibition by other components and also knockout and knockdown experiments can be described on multiplicative level. From Eq. (1) we can derive a relation between the interaction matrix A and the covariance matrix of observed component abundancesWe exploit Eq. (2) to infer directed networks from correlation data. Here, we assume that component abundances are obtained from averaging over a large number of cells. In this case, fast fluctuating perturbations that arise from thermal noise and can be observed only on single-cell level average out. To infer the interaction matrix, A, we start with singular value decomposition of the matrix product A −1 B with U and V orthogonal matrices and Σ a diagonal matrix containing the singular values. The negative definite matrix A has full rank and hence is invertible. In the following, we show that it is possible to infer the strength of a directed link between a sender node j and a receiver node i, if all direct perturbations on receiver node i are removed from the data set and if a significant partial correlation between i and j exists. Removing the perturbation data for node i implies that the matrix B has at least one zero entry. As a consequence, N 0 ≥ 1 singular values are zero–as in general not all nodes are perturbed–and the corresponding rows of U span the left nullspace of A −1 B. In the absence of fast fluctuating perturbations, γ = 0, we can rewrite the covariance matrix as

---

### Revealing the range of equally likely estimates in the admixture model [^4446a58e]. G3 (2025). Medium credibility.

Many ancestry inference tools, including structure and admixture, rely on the admixture model to infer both, allele frequencies p and individual admixture proportions q for a collection of individuals relative to a set of hypothetical ancestral populations. We show that under realistic conditions the likelihood in the admixture model is typically flat in some direction around a maximum likelihood estimate (q^, p^). In particular, the maximum likelihood estimator is non-unique and there is a complete spectrum of possible estimates. Common inference tools typically identify only a few points within this spectrum. We provide an algorithm which computes the set of equally likely (q~, p~), when starting from (q^, p^). It is analytic for K = 2 ancestral populations and numeric for K > 2. We apply our algorithm to data from the 1000 genomes project, and show that inter-European estimators of q can come with a large set of equally likely possibilities. In general, markers with large allele frequency differences between populations in combination with individuals with concentrated admixture proportions lead to small areas with a flat likelihood. Our findings imply that care must be taken when interpreting results from STRUCTURE and ADMIXTURE if populations are not separated well enough.

---

### An official American Thoracic Society workshop report: developing performance measures from clinical practice guidelines [^0802211b]. Annals of the American Thoracic Society (2014). Medium credibility.

Process-based performance measure development — When developing process-based measures, considerations include the purpose of measurement, the importance of the clinical problem, ease of measurement, variability in delivered care, opportunity for improvement, and potential unintended consequences; successful measures are described as important, scientifically sound, and feasible. After selecting a care process, steps include writing an indicator statement, specifying eligibility and the desired action in plain language, defining the denominator and numerator, and specifying how patient preferences are handled, with options such as removing refusals from the numerator, removing refusals from both the numerator and denominator, or adding refusals to both as a marker that the indicator was offered but refused.

---

### Abatacept reduces disease activity and ultrasound power Doppler in ACPA-negative undifferentiated arthritis: a proof-of-concept clinical and imaging study [^fa4eb012]. Rheumatology (2017). Low credibility.

Objectives

No proven treatment exists for ACPA-negative undifferentiated arthritis (UA). The aim of this study was to evaluate whether abatacept is effective in treating poor prognosis, ACPA-negative UA, including its effect on power Doppler on US (PDUS).

Methods

A proof-of-concept, open-label, prospective study of 20 patients with DMARD-naïve, ACPA-negative UA (⩾2 joint synovitis) and PDUS ⩾ 1 with clinical and 20-joint US (grey scale/PDUS) assessments at baseline, 6, 12, 18 and 24 months. All patients received 12 months of abatacept (monotherapy for minimum first 6 months). The primary end point was a composite of the proportion of patients that at 6 months achieved DAS44 remission, a maximum of one swollen joint for at least 3 consecutive months and no radiographic progression (over 0–12 months).

Results

Twenty of the 23 patients screened were enrolled [14 female; mean (sd) age 53.4 (11.2) years, symptom duration 7.5 (0.9) months]. Two (10%) achieved the composite primary end point. A reduction in the mean (sd) DAS44 was observed from a baseline value of 2.66 (0.77) to 2.01 (0.81) at 6 months and to 1.78 (0.95) at 12 months. The DAS44 remission rates were 6/20 (30%; 95% CI: 15, 51%) at 6 months and 8/20 (40%; 95% CI: 22, 62%) at 12 months. A striking decrease in the median (interquartile range; IQR) total PDUS score was noted from 10 (4–23) at baseline to 3 (2–12) and 3 (0–5) at 6 and 12 months, respectively.

Conclusion

This report is a first in potentially identifying an effective therapy, abatacept monotherapy, for poor-prognosis, ACPA-negative UA, supported by a clear reduction in PDUS. These data justify evaluation in a controlled study.

---

### Comparison of parameter optimization methods for quantitative susceptibility mapping [^26a0417b]. Magnetic Resonance in Medicine (2021). Medium credibility.

Purpose

Quantitative Susceptibility Mapping (QSM) is usually performed by minimizing a functional with data fidelity and regularization terms. A weighting parameter controls the balance between these terms. There is a need for techniques to find the proper balance that avoids artifact propagation and loss of details. Finding the point of maximum curvature in the L-curve is a popular choice, although it is slow, often unreliable when using variational penalties, and has a tendency to yield overregularized results.

Methods

We propose 2 alternative approaches to control the balance between the data fidelity and regularization terms: 1) searching for an inflection point in the log-log domain of the L-curve, and 2) comparing frequency components of QSM reconstructions. We compare these methods against the conventional L-curve and U-curve approaches.

Results

Our methods achieve predicted parameters that are better correlated with RMS error, high-frequency error norm, and structural similarity metric-based parameter optimizations than those obtained with traditional methods. The inflection point yields less overregularization and lower errors than traditional alternatives. The frequency analysis yields more visually appealing results, although with larger RMS error.

Conclusion

Our methods provide a robust parameter optimization framework for variational penalties in QSM reconstruction. The L-curve-based zero-curvature search produced almost optimal results for typical QSM acquisition settings. The frequency analysis method may use a 1.5 to 2.0 correction factor to apply it as a stand-alone method for a wider range of signal-to-noise-ratio settings. This approach may also benefit from fast search algorithms such as the binary search to speed up the process.

---

### Statistical challenges in the development and evaluation of marker-based clinical tests [^cdc98f03]. BMC Medicine (2012). Low credibility.

Exciting new technologies for assessing markers in human specimens are now available to evaluate unprecedented types and numbers of variations in DNA, RNA, proteins, or biological structures such as chromosomes. These markers, whether viewed individually, or collectively as a 'signature', have the potential to be useful for disease risk assessment, screening, early detection, prognosis, therapy selection, and monitoring for therapy effectiveness or disease recurrence. Successful translation from basic research findings to clinically useful test requires basic, translational, and regulatory sciences and a collaborative effort among individuals with varied types of expertise including laboratory scientists, technology developers, clinicians, statisticians, and bioinformaticians. The focus of this commentary is the many statistical challenges in translational marker research, specifically in the development and validation of marker-based tests that have clinical utility for therapeutic decision-making.

---

### Optimal deployment of limited vaccine supplies to control mpox outbreaks [^bfcefb49]. NPJ Vaccines (2025). Medium credibility.

Fig. 4
Predicting the risk threshold with a time-varying force of infection and different cumulative incidence.

Here we compute the risk threshold, which is the fold-higher risk in a high-risk group compared to a lower risk group above which a second dose to higher risk individuals is the favoured strategy and below which a first dose to lower risk individuals is favoured. We assume there is an unknown time-varying force of infection, and a given cumulative incidence rate (in the high-risk group) over a 2 year time interval (x-axis). We first calculate this assuming no waning of vaccine effectiveness by using only the estimates of vaccine effectiveness from our meta-analysis of primary studies(A). The solid line is the estimated risk threshold for each cumulative incidence, and the shaded region is the credible interval around this estimate. We also use our previous model of predicted waning of vaccine effectiveness over time, and with different dose spacingto estimate the risk threshold (B − D). In these cases, the exact risk threshold will depend on the exact time-varying force of infection, which we have chosen to leave unspecified. Thus instead of computing an exact risk-threshold we compute a range within which the actual risk threshold must be contained regardless of the exact time-varying force of infection. This range is given by the coloured region (full opacity). The credible intervals of this range are indicated by the shaded region and dashed lines. Note that above a cumulative incidence of ~75% there is no single risk threshold, thus we have truncated our estimates here.

---

### Fibroscan versus simple noninvasive screening tools in predicting fibrosis in high-risk nonalcoholic fatty liver disease patients from Western India [^4c8c99d2]. Annals of Gastroenterology (2015). Low credibility.

Table 2
Indications of liver biopsy in our study

The biopsy sample was considered adequate in all cases. The median cumulative sample size was 2.4 cm (min 2.0 cm and max 3.2 cm). The histopathologist had examined a minimum of 12 portal tracts. Twenty eight patients had only steatosis with or without portal/lobular inflammation, while rest had either ballooning, Mallory-Denk bodies or fibrosis - changes consistent with nonalcoholic steatohepatitis (NASH). The number of patients with stage 0 (no fibrosis), stage 1 (perivenular/central fibrosis), stage 2 (perivenular with periportal fibrosis), stage 3 (bridging fibrosis) and stage 4 (cirrhosis) fibrosis were 31, 29, 12, 29 and 9 respectively.

Fibroscan was carried out successfully in all NAFLD patients. However, obese patients required an increased number of readings to achieve its criteria for successful reading of 60%. Getting a proper window was difficult in obese patients, though we managed to get successful readings in all of them. The median Fibroscan value in patients in whom a biopsy was not advised was 4.6 kPa. The difference in liver stiffness measurements between this group and the ones having undergone liver biopsy showed a statistical significant difference (P < 0.001). None of these individuals had a value above 12 kPa. Median Fibroscan values for stages 0/1, 2, 3 and 4 were 8, 9.1, 12 and 20 kPa respectively (Fig. 2). There was a statistically significant difference in liver stiffness measurements in patients with stage 0/1/2 fibrosis as compared to stage 3/4 fibrosis (P < 0.05). With increase in severity of liver fibrosis there was stepwise increase in Fibroscan values (P = 0.000038 by Kruskal-Wallis test). The AUROC for Fibroscan for detecting stage 3/4 fibrosis in NAFLD was 0.91. The sensitivity, specificity, PPV and NPV of Fibroscan at 12 kPa for stage 3/4 fibrosis were 0.9, 0.8, 0.72 and 0.93 respectively (Table 3).

---

### Optimal gene expression analysis by microarrays [^15b18110]. Cancer Cell (2002). Low credibility.

DNA microarrays make possible the rapid and comprehensive assessment of the transcriptional activity of a cell, and as such have proven valuable in assessing the molecular contributors to biological processes and in the classification of human cancers. The major challenge in using this technology is the analysis of its massive data output, which requires computational means for interpretation and a heightened need for quality data. The optimal analysis requires an accounting and control of the many sources of variance within the system, an understanding of the limitations of the statistical approaches, and the ability to make sense of the results through intelligent database interrogation.

---

### Minimum 10-year follow-up of anatomic total shoulder arthroplasty and ream-and-run arthroplasty for primary glenohumeral osteoarthritis [^e0e20e60]. Journal of Shoulder and Elbow Surgery (2024). Medium credibility.

Background

Reports on long term outcomes and failures of shoulder arthroplasty are uncommon. The purpose of this study is to present minimum 10-year outcomes in consecutive patients undergoing ream-and-run and anatomic total shoulder arthroplasty (TSA) for primary glenohumeral arthritis.

Methods

This study analyzed consecutive patients who had undergone a ream-and-run or TSA with minimum 10-year follow-up. Pain scores and Simple Shoulder Test (SST) values were obtained preoperatively and at a minimum of 10 years postoperatively via e-mail or mail-in response. Percentage of maximum possible improvement (%MPI) was also calculated.

Results

Of 127 eligible patients, 63 (50%) responded to a 10-year survey. This included 34 patients undergoing ream-and-run arthroplasty and 29 patients undergoing TSA. The ream-and-run patients were significantly younger than the TSA patients (60 ± 7 vs. 68 ± 8, P < .001), predominantly male (97% vs. 41%, P < .001), and had a lower American Society of Anesthesiologists classification (P = 0.018). In the ream-and-run group, the mean pain score improved from a preoperative value of 6.5 ± 1.9 to 0.9 ± 1.3 (P < .001), and the mean SST score improved from 5.4 ± 2.4 to 10.3 ± 2.1 at 10-year follow-up (P < .001). Twenty-eight (82%) achieved an SST improvement above the minimally clinically important difference (MCID) of 2.6. Four patients (12%) underwent single-stage exchange to another hemiarthroplasty, whereas 1 (3%) required manipulation under anesthesia. In the TSA group, the pain score improved from a preoperative value of 6.6 ± 2.2 to 1.2 ± 2.3 (P < .001), and the SST score improved from 3.8 ± 2.6 to 8.9 ± 2.6 at 10-year follow-up (P < .001). Of the 29 patients who underwent a TSA, 27 (93%) achieved an SST improvement above the MCID of 1.6. No patient in the TSA group required reoperation.

Conclusion

Although the characteristics of the patients differ between the 2 groups, excellent functional results can be obtained with the ream-and-run arthroplasty and TSA for glenohumeral osteoarthritis.

---

### Machine learning enables completely automatic tuning of a quantum device faster than human experts [^fb7ba5a8]. Nature Communications (2020). High credibility.

Variability is a problem for the scalability of semiconductor quantum devices. The parameter space is large, and the operating range is small. Our statistical tuning algorithm searches for specific electron transport features in gate-defined quantum dot devices with a gate voltage space of up to eight dimensions. Starting from the full range of each gate voltage, our machine learning algorithm can tune each device to optimal performance in a median time of under 70 minutes. This performance surpassed our best human benchmark (although both human and machine performance can be improved). The algorithm is approximately 180 times faster than an automated random search of the parameter space, and is suitable for different material systems and device architectures. Our results yield a quantitative measurement of device variability, from one device to another and after thermal cycling. Our machine learning algorithm can be extended to higher dimensions and other technologies.

---

### Experimental noise cutoff boosts inferability of transcriptional networks in large-scale gene-deletion studies [^6663808d]. Nature Communications (2018). Medium credibility.

Residual bootstrapping

We make use of the 748 measured wild-type experimental replicates to determine the natural variation among biological replicates, with, x in the expression of gene i in wild-type replicate n, the expression level of gene i measured after pooling over wild-type replicates, and 〈.〉 n denoting the average over replicates. To generate the bootstrap samples we randomly select 200 different δ in from the replicates for each gene i, and add these values to the log fold changes of the perturbed expression levels, withand the average is taken over the two replicates for each knockout.

Sparsity constraints by removing noisy nodes

As network inference typically comes with an insufficient amount of independent perturbations and experimental replicates we run into the problem of overfitting the data. In this case, noisy information from many network nodes is collected to explain the response of a given target node. L 1 -norm regularised regression (Lasso) systematically removes many links, where each link explains only a small part of the variation of the target node, in favour of few links, where each link contributes significantly. In our approach we remove noisy nodes and thus their potential outgoing links, where the critical noise level is determined by the variability among biological replicates. In the presence of noise, our algorithm removes weakly responding nodes from the network. We thereby assume that the existence of many indirect interactions between source and target node by first distributing the signal of the source node among many weakly responding nodes and then collecting these weak signals to generate a significantly responding target node is much less likely than the existence of a single direct interaction. However, in the noise-free case we run into the same problem as Lasso to determine the right cutoff (regularisation parameter).

Synthetic data

Synthetic data were generated using our own model and GeneNetWeaver — an open access software that has been designed for benchmarking network inference methods. With GeneNetWeaver, networks were generated from a model that closely resembles the structure of the yeast regulatory network, and steady state levels of node activities were computed using ordinary differential equations. In our data generating model, we first generated scale-free networks with an exponent of 2.5 and an average degree of 2. Then, we solved a system of ordinary differential equations with non-linear regulatory interactions between nodes to obtain steady state values of node activities, e.g. transcript levels. For both models, logarithmic fold changes of node activities were calculated (transcriptional levels upon perturbation relative to wild levels), and gaussian noise was added.