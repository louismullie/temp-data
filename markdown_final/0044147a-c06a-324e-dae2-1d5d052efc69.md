# Medical Question & Answer

**Sample ID**: 0044147a-c06a-324e-dae2-1d5d052efc69
**Dataset Index**: 104342

---

## Question

\section*{Problem 3–051033}

Determine all real numbers a, b and c for which the following holds: a + bc = (a + b)(a + c).

---

## Answer

> Let's see… What do we have here? The user is asking to determine all real numbers (a, b, c) that satisfy the equation (a + bc = (a + b)(a + c)). Let's break this down step-by-step. First, I need to think about expanding the right-hand side and simplifying the equation. Then, I should factor the resulting expression to isolate cases. Next, I will analyze the two cases that emerge and characterize the solution sets. Finally, I will verify the solutions and present a concise summary.

> Let me first expand the right-hand side carefully: ((a + b)(a + c) = a² + ac + ab + bc). So the original equation becomes (a + bc = a² + ab + ac + bc). Wait, let me verify the expansion — yes, that's correct, and I can already see the (bc) terms will cancel on both sides, which simplifies things nicely [^notfound].

> I will now subtract (bc) from both sides to get (a = a² + ab + ac). Hold on, I should double-check that cancellation — yes, (bc) cancels, leaving (a = a² + ab + ac). Next, I should move everything to one side to prepare for factoring: (a² + ab + ac - a = 0), which simplifies to (a² + a(b + c - 1) = 0). Let me factor this as (a(a + b + c - 1) = 0), and I should confirm that this factorization is accurate — yes, distributing (a) gives back the previous line [^notfound].

> Now, I need to consider the two cases that arise from the zero-product property. Case 1: (a = 0). Case 2: (a + b + c - 1 = 0), which is equivalent to (a + b + c = 1). I should confirm that these are mutually exclusive and exhaustive — yes, they cover all possibilities given the factorization [^notfound].

> For Case 1, when (a = 0), I should substitute back into the original equation to verify consistency. The left side becomes (0 + bc = bc), and the right side becomes ((0 + b)(0 + c) = bc). So (bc = bc), which is always true. Therefore, when (a = 0), (b) and (c) can be any real numbers. Let me write that explicitly: the solution set is ({(0, b, c) | b, c ∈ ℝ}) [^notfound].

> For Case 2, when (a + b + c = 1), I should confirm that this single constraint is sufficient. Let me substitute (a = 1 - b - c) into the original equation. The left side is ((1 - b - c) + bc), and the right side is ((1 - b - c + b)(1 - b - c + c) = (1 - c)(1 - b) = 1 - b - c + bc). Comparing both sides, I get (1 - b - c + bc = 1 - b - c + bc), which is an identity. So any triple ((a, b, c)) with (a + b + c = 1) satisfies the equation, with (a) not necessarily zero. I should double-check that I'm not over-constraining (b) and (c) — indeed, they remain free subject only to the sum condition [^notfound].

> But wait, what if both conditions are satisfied simultaneously? If (a = 0) and (a + b + c = 1), then (b + c = 1), which is a subset of Case 1. So the two cases overlap when (a = 0) and (b + c = 1), but that does not create a contradiction; it just means the union of the two sets captures all solutions without duplication. I should confirm that my final characterization reflects this union — yes, it does [^notfound].

> Final answer: All real solutions are given by either (a = 0) with (b) and (c) arbitrary, or by (a + b + c = 1) with (a) not necessarily zero. In set notation, the solution set is ({(a, b, c) ∈ ℝ³ | a = 0 or a + b + c = 1}). I should double-check that this matches the derivation — yes, it cleanly summarizes both cases and their overlap [^notfound].

---

The equation (a + bc = (a + b)(a + c)) simplifies to (a(a + b + c - 1) = 0), so the solutions are all real triples where either (a = 0) (with (b, c) arbitrary) or (a + b + c = 1) (with (a ≠ 0) allowed). Thus, the complete solution set is:

{(a, b, c) ∈ ℝ³ | a = 0 or a + b + c = 1}

---

## Step-by-step solution

### Step 1: Expand the right-hand side

Start by expanding the right-hand side of the equation:

(a + b)(a + c) = a² + ac + ab + bc

So the equation becomes:

a + bc = a² + ab + ac + bc

---

### Step 2: Simplify the equation

Subtract (bc) from both sides:

a = a² + ab + ac

Rearrange terms to bring everything to one side:

a² + ab + ac - a = 0

Factor out (a):

a(a + b + c - 1) = 0

---

### Step 3: Analyze the factored equation

The equation (a(a + b + c - 1) = 0) implies two cases:

- **Case 1**: (a = 0)
- **Case 2**: (a + b + c - 1 = 0 ⇒ a + b + c = 1)

---

### Step 4: Describe the solution sets

- **Case 1**: If (a = 0), the original equation reduces to (bc = bc), which is always true. Thus, (b) and (c) can be any real numbers. The solution set is ({(0, b, c) | b, c ∈ ℝ}).

- **Case 2**: If (a + b + c = 1), then (a) can be any real number, and (b) and (c) are constrained by (b + c = 1 - a). The solution set is ({(a, b, c) | a + b + c = 1}).

---

### Step 5: Combine the cases

The complete solution set is the union of the two cases:

{(a, b, c) ∈ ℝ³ | a = 0 or a + b + c = 1}

---

## Verification of solutions

It is straightforward to verify that both cases satisfy the original equation:

- For (a = 0), the equation becomes (bc = bc), which is always true.
- For (a + b + c = 1), substituting (a = 1 - b - c) into the original equation yields (1 - b - c + bc = (1 - c)(1 - b)), which simplifies to (1 - b - c + bc = 1 - b - c + bc), confirming the identity.

---

## Geometric interpretation

Geometrically, the solution set consists of two components:

- The plane (a = 0) (the (yz)-plane), where (b) and (c) are unrestricted.
- The plane (a + b + c = 1), which intersects the coordinate axes at ((1, 0, 0)), ((0, 1, 0)), and ((0, 0, 1)).

These two planes intersect along the line where (a = 0) and (b + c = 1).

---

## Final answer

All real solutions ((a, b, c)) to (a + bc = (a + b)(a + c)) are those for which either (a = 0) or (a + b + c = 1). In set notation:

{(a, b, c) ∈ ℝ³ | a = 0 or a + b + c = 1}

---

## References

### Quantum theory based on real numbers can be experimentally falsified [^83529f33]. Nature (2021). Excellent credibility.

Fig. 1
Simulating single-site and multipartite quantum experiments through real quantum theory.

Left: a single-site quantum experiment. A complex quantum system in state ρ is probed via the measurement { Π r } r. One way to reproduce the measurement statistics of this experiment using real quantum theory requires adding an extra real qubit: the state ρ is then replaced by the real statein equation (1), while every measurement operator is replaced by the real measurement operator. Using the fact that probabilities are real, and thus, it is straightforward that. It is noted that this construction doubles the Hilbert space dimension of the original complex quantum system (when finite). This is not surprising, as a complex number is defined by two real numbers, and may just be seen as yet another example of how complex numbers simplify the calculation of experimental predictions, as in electromagnetism. Right: a multipartite quantum experiment. A complex Bell scenario consists of two particles (or systems) distributed between Alice and Bob, who perform local measurements, labelled by x and y, and get results a and b. By postulates (1 ¢)−(4), a complex Hilbert space is assigned to each particle, and the Hilbert space describing the overall bipartite system is defined by the tensor product of these. The state of the two particles is thus described by an operator ρ AB acting on the joint space, whereas operatorsandacting on each local Hilbert space describe the local measurements. The observed measurement statistics or correlations are described by the conditional probability distribution. One way to reproduce these statistics using real quantum theory consists of assigning an extra real qubit to each particle. The quantum state is replaced by the real state, and the local measurements are replaced by the same transformation as before for a single system. The observed statistics are again recovered, that is.

---

### An analytical theory of balanced cellular growth [^a5efc002]. Nature Communications (2020). High credibility.

Proof: The active stoichiometric matrix A may have more rows than columns. In this case, m + 1 > n, and the rows for exactly n metabolites are linearly independent, as row and column rank must equal. As a consequence, the remaining m + 1 − n metabolite concentrations are linearly dependent on the concentrations of the n independent metabolites. These dependent concentrations are not free variables, and hence they can be put aside and dealt with separately.

We decompose the linear system of equations represented by constraint (11) into two parts, rearranging the rows of A into matrices B, C such that B contains the rows for the independent reactants. As A has full column rank, choosing linearly independent rows results in a square matrix B of full rank (#rows(B) = rank(B) = rank(A)). Let b be the subvector of reactant concentrations a that correspond to the rows of B, and let c be the subvector of the remaining reactant concentrations corresponding to the rows of C. We can then split the mass balance constraint (11) into two separate equations: B is a square matrix of full rank, so there is always a unique inverse B −1. Multiplying both sides of the first equation by B −1 from the left, we obtain the desired equation for v. Inserting this result into the second equation results in the desired equation for c. □

---

### A continuous-time maxSAT solver with high analog performance [^87d3d16d]. Nature Communications (2018). Medium credibility.

Algorithm description

Here, we give a simple, nonoptimized variant of the algorithm (see flowchart in Supplementary Fig. 2). Better implementations can be devised, for example with better fitting routines, however the description below is easier to follow and works well. Given a SAT problem, we first determine the b parameter as described previously. Step 1: initially we set, Γ min, = and t max. Unless specified otherwise, in our simulations we used Γ min = 100, Γ max = 2 × 10 6, t max = 50. Step 2: to initialize our statistics, we run Γ min trajectories up to t max, each from a random initial condition. For every such trajectory ω we update the p (E, t) distributions as function of the energies of the orthants visited by ω. We record the lowest energy value found. Step 3: starting from Γ = Γ min + 1 and up to Γ max, we continue running trajectories in the same way and for each one of them check: (a) If, set, update p (E, t) and go to Step 4. (b) If Γ just reached, go to Step 4. (c) If Γ = Γ max, output "Maximum number of steps reached, increase Γ max ", output the lowest energy value found, the predictedand the quality of fit for, then halt. Step 4: using the p (E, t) distributions, estimate the escape rates κ (E) as described in the corresponding Methods section. Step 5: the κ (E) curve is extrapolated to the E − 1 value obtaining κ (E − 1) and then using this we predict(as described in another Methods section). Further extrapolating the κ (E) curve to κ = 0 we obtain(see the corresponding Methods section). Step 6: we check the consistency of the prediction defined here as saturation of the predicted values. We call it consistent, ifhas not changed during the last 5 predictions. If it is not consistent yet, we continue running new trajectories (Step 4). If the prediction is consistent, we check for the following halting conditions: (i) Ifthen we decide the global optimum has been found:and skip to Step 7. (ii) If the fitting is consistently predicting(usually it is very close,) we check the number of trajectories that has attained states with, i.e. = . If it is large enough (e.g. > 100), we decide to stop running new trajectories and setand go to Step 7. (iii) Ifthen we most probably have not found the global optimum yet and we go to Step 4. We added additional stopping conditions that can shorten the algorithm in case of easy problems, see Methods corresponding section, but these are not so relevant. Step 7: the algorithm ends and outputs, values, the Boolean variables corresponding to the optimal state found, along with the quality of fit.

---

### Approximations to the expectations and variances of ratios of tree properties under the coalescent [^cf6c6a02]. G3 (2022). Medium credibility.

We have found that approximations for fixed n and in the limit asare quite accurate in predicting the expected values seen in coalescent simulations of the ratios (Fig. 2). For the variances, the approximations are generally less accurate, although in most cases, graphs of the approximations and simulated values have similar shape (Fig. 4). These approximations are obtained from a Taylor approximation for the variance of a ratio (equation 4), and higher-order approximations of this variance could potentially be applied by use of Taylor's theorem; as the order of the approximation increases, however, the complexity of the resulting formula also increases. For those variances for which the approximation and simulation are not close in Fig. 4, we advise caution in using the variances in settings in which a precise approximation is needed.

---

### Thinking outside the curve, part I: modeling birthweight distribution [^b60d1510]. BMC Pregnancy and Childbirth (2010). Low credibility.

b. Simulation study on calibrating confidence intervals

For our second simulation study we generated 25 overlapping data sets of size 50,000 from design C in Table 3, the degree of overlap consistent with a population of 200,000. For each of various C between 2.0 and 5.0, we used Equation (7) to form confidence intervals for the mixture parameters p 1, p 2, p 3, p 4, μ 1, μ 2, μ 3, μ 4, σ 1, σ 2, σ 3, σ 4. We recorded how many of the mixture parameters were contained in their respective confidence intervals. This was repeated nine more times, and we tabulated how many of the 120 = 12 × 10 confidence intervals contained their targets. Confidence intervals were also formed using Equation (6) for comparative purposes. The above steps were repeated with overlapping data sets consistent with a population of 1,000,000 and with nonoverlapping data sets consistent with an effectively infinite population.

The results are summarized in Table 5. With an effectively infinite population, only 81.7% of the confidence intervals formed using Equation (6) contained their targets at C = 5.0. The confidence intervals formed using Equation (7) contained their targets 95.0% of the time at C = 2.5. The adjustment suggested by Equation (8) appears reasonable: φ = .05 = 50,000/1,000,000 and N rep = 25 yield C φ = 1.315 C 0, which accords with the 95.8% capture of mixture parameters at C = 3.5 ≈ 1.315 × 2.5 with a population of 1,000,000.

Table 5
Confidence Interval Coverage Probabilities in Simulation Studies

The row with " C " = 2 and "Population size" = 200,000 identifies the numbers and percentages of confidence intervals containing their targets of mixture parameters, based on 10 repetitions in each of which 25 samples of size 50000 were simulated from a 4-component normal mixture with 12 parameters; results under the heading of "Bias adjustment included" are based on Equation (7) with C = 2, results under the heading of "Bias adjustment omitted" are based on Equation (6) with C = 2, and the 25 samples of size 50000 had overlap consistent with a population size of 200,000. Other rows correspond to different choices of C and/or population sizes.

---

### Anomalous frozen evanescent phonons [^d42644f1]. Nature Communications (2024). High credibility.

Decomposition of displacement field

For the simplified finite-length mass-and-spring model in Fig. 5b, we denote the axial (-direction) displacement of each mass bywith. We first focus on the masses in the bulk, i.e. Each of these masses is connected to two immediate neighbors and two-th nearest neighbors on both sides. Thus, the force-balance equation for the massreads:

Apparently, this set of equations always supports the linear non-Bloch solution

andare constants. Additionally, multiple frozen evanescent phonons of the following form are possible

Here, the complex wavenumbersatisfies the following condition

Equation (18) has infinitely many solutions for. Here, we only need to considerwith its real part inside of the first Brillouin zone, as other solutions are simply shifted by an integer multiple of, which does not influence the displacement solution Eq. (17).

For the considered parameter settings in Fig. 5c, we have the following solutions for:

The displacement field in Eq. (17) forwith a positive imaginary part represents a frozen evanescent phonon that exponentially decays to the right, while the one with a negative imaginary part stands for a frozen evanescent phonon exponentially decaying to the left.

The general displacement field of the finite-length system in Fig. 5b is a linear combination of the above solutions, i.e.

The displacement field Eq. (22) automatically ensures the force-balance equation Eq. (15) for the masses in bulk, i.e. The unknown complex coefficients, and, can be determined from the boundary conditions applied to the mass-and-spring chain.

For the single-site loading as in Fig. 5c, the left most mass and the right most mass have prescribed displacements,… In addition, masses, and massesmust be in force-balance, too. For the two-sites loading condition in Fig. 5d, the massis similarly prescribed with displacement. Other conditions are the same as the single-site loading conditions. It can be easily verified that the number of the boundary conditions, exactly matches the number of the unknown coefficients. Thus, the unknown coefficients can be determined. Afterward, the total displacement field Eq. (22), as well as individual parts, including the linear part and the frozen evanescent phonons, can be obtained. The analytical formula for the unknown coefficients is very lengthy and thus omitted here.

---

### Management guidelines for children with thyroid nodules and differentiated thyroid cancer [^c311b8f2]. Pediatrics (2018). Medium credibility.

Radioiodine (131I) therapy in pediatric PTC — recommendation numbers and ratings are listed for key decisions: "Which children might benefit from therapeutic 131I?" is 17 B; "How should a child be prepared for 131I?" is 18 A; "What should be considered for administration of 131I?" includes 19(A) C and 19(B) F; "How is the activity of therapeutic 131I determined?" is 20 I; "Should a posttreatment whole-body scan be obtained?" is 21 B; and "What are the acute and long-term risks of 131I therapy in children?" is 22 C.

---

### Time and classical equations of motion from quantum entanglement via the page and wootters mechanism with generalized coherent States [^50e7c9ea]. Nature Communications (2021). High credibility.

Most discussions about time in QM are aimed at understanding what is the status of time in the quantum description, as if there were no problem as far as one stays classical. However, if one believes that there do not exist quantum systems and classical ones, but rather that some quantum systems behave in a way that, under certain conditions, is efficiently described by the laws of classical physics, then there must be just one time. In other terms, the procedure used to identify what time is in QM must have a well-defined classical limit, fully consistent with classical physics and the way time enters the classical equations of motion (e.o.m.). We construct such a procedure, demonstrating that it consistently produces not only the Schrödinger equation for quantum systems but also the Hamilton e.o.m. for classical ones, with the parameter playing the role of time being the same in both cases. We tackle the quantum-to-classical crossover via the large- N approach based on Generalized Coherent States (GCS) from refs. –, where it is demonstrated that the theory describing a quantum system for which GCS can be constructed flows into a well-defined classical theory if few specific conditions upon its GCS hold in the N → ∞ limit (N quantifies the number of microscopic quantum components, sometimes referred to as the number of degrees of freedom or dynamical variables, in the literature). By "classical limit", we will hereafter mean the large- N limit with the above conditions on GCS enforced.

In this work, we consider a quantum composite system made of two non-interacting and entangled objects, dubbed clock and evolving system, whose quantum state is expressed via a parametric representation with GCS for the clock (see Fig. 1); from this representation, a real parameter φ emerges, with features that make it a good candidate for being recognized as time. We then take the classical limit for the clock only (see Fig. 2) and derive an equation for the physical states of the evolving system, which is the Schrödinger equation, once the above-mentioned parameter φ is given the role of time. We also obtain an inequality that provides relevant clues for understanding the origin, nature, and meaning of the energy–time uncertainty relation. Finally, we introduce GCS for the evolving system, take its classical limit (see Fig. 3), and get to our most relevant result: the Hamilton e.o.m. of classical physics are derived, with the same parameter φ as time.

---

### A space-time tradeoff for implementing a function with master equation dynamics [^57fa10b0]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### Evolution and emergence of infectious diseases in theoretical and real-world networks [^e614d467]. Nature Communications (2015). Medium credibility.

(i) triple closure,

(ii) deconvolution of pairs,

(iii) detailed balance,

(iv) deconvolution of individuals,

where N is the total population size and E is the total number of edges. Whenever a disease state occurs without a subscript, it implies that it includes the sum over all degrees. We then arrive at a simplified set of equations,

[SI] is the number of edges between susceptible and infected individuals. These equations can be used to describe the time course of the infection among individuals of each degree and the equilibrium state.

Deriving a closed and reduced set of pair-wise equations requires making approximations about the types of higher-order correlations between connected individuals introduced by the epidemic. Triple closure and deconvolution of pairs and individuals are examples of such approximations. It is difficult to formulate exactly when these assumptions hold a priori, but previous studies have shown that they usually agree very well with simulations. In contrast, the detailed balance approximation depends only on the network structure and is assured in a configuration model. In networks with other methods of edge creation, such as preferential attachment, this simplification may fail. As stated above, these approximations assume that the network clustering, φ, is zero, although corrections can be made to account for non-zero values.

---

### An analytical theory of balanced cellular growth [^98f39440]. Nature Communications (2020). High credibility.

Without dependent reactants, A is a square matrix with a unique inverse A −1, and x ≡ [P, a] T is the corresponding vector of independent concentrations. Multiplying both sides of the mass balance constraint (1) by A −1, we obtain (Theorem 5)The right-hand side of the mass balance constraint (1) quantifies how much of each component x i needs to be produced to offset the dilution that would otherwise occur through the exponential volume increase at rate μ.quantifies the proportion of flux v j invested into offsetting the dilution of component i, and we thus name A −1 the investment (or dilution) matrix; see Supplementary Fig. 1 for examples. In contrast to the mass-normalized stoichiometric matrix A, which describes local mass balances, A −1 describes the structural allocation of reaction fluxes into offsetting the dilution of all downstream cellular components, carrying global, systems-level information.

From the kinetic equation (Eq. (3)), p j = v j / k j (a), and inserting v j from the investment equation (Eq. (4)) giveswhere ∑ i sums over the total protein and individual reactant concentrations (Theorem 6). Substituting these expressions into the total protein sum (Eq. (2)) and solving for μ results in the growth equation (Theorem 7)

As detailed in "Methods" (Theorems 5–7), a corresponding result also holds for BGSs with dependent reactants. Thus, for any active matrix A with full column rank (in particular for all active matrices of EGSs) and for any corresponding concentration vector x, there are unique and explicit mathematical solutions for the fluxes v, individual protein concentrations p, and growth rate μ. If μ (Eq. (6)) and all individual protein concentrations p j (Eq. (5)) are positive, the cellular state is a BGS; otherwise, no balanced growth is possible at these concentrations.

---

### Generating conjectures on fundamental constants with the ramanujan machine [^75e8911d]. Nature (2021). Excellent credibility.

Fundamental mathematical constants such as e and π are ubiquitous in diverse fields of science, from abstract mathematics and geometry to physics, biology and chemistry 1,2. Nevertheless, for centuries new mathematical formulas relating fundamental constants have been scarce and usually discovered sporadically 3–6. Such discoveries are often considered an act of mathematical ingenuity or profound intuition by great mathematicians such as Gauss and Ramanujan 7. Here we propose a systematic approach that leverages algorithms to discover mathematical formulas for fundamental constants and helps to reveal the underlying structure of the constants. We call this approach 'the Ramanujan Machine'. Our algorithms find dozens of well known formulas as well as previously unknown ones, such as continued fraction representations of π, e, Catalan's constant, and values of the Riemann zeta function. Several conjectures found by our algorithms were (in retrospect) simple to prove, whereas others remain as yet unproved. We present two algorithms that proved useful in finding conjectures: a variant of the meet-in-the-middle algorithm and a gradient descent optimization algorithm tailored to the recurrent structure of continued fractions. Both algorithms are based on matching numerical values; consequently, they conjecture formulas without providing proofs or requiring prior knowledge of the underlying mathematical structure, making this methodology complementary to automated theorem proving 8–13. Our approach is especially attractive when applied to discover formulas for fundamental constants for which no mathematical structure is known, because it reverses the conventional usage of sequential logic in formal proofs. Instead, our work supports a different conceptual framework for research: computer algorithms use numerical data to unveil mathematical structures, thus trying to replace the mathematical intuition of great mathematicians and providing leads to further mathematical research.

---

### The more you test, the more you find: the smallest P-values become increasingly enriched with real findings as more tests are conducted [^4d8d5bd6]. Genetic Epidemiology (2017). Low credibility.

The increasing accessibility of data to researchers makes it possible to conduct massive amounts of statistical testing. Rather than follow specific scientific hypotheses with statistical analysis, researchers can now test many possible relationships and let statistics generate hypotheses for them. The field of genetic epidemiology is an illustrative case, where testing of candidate genetic variants for association with an outcome has been replaced by agnostic screening of the entire genome. Poor replication rates of candidate gene studies have improved dramatically with the increase in genomic coverage, due to factors such as adoption of better statistical practices and availability of larger sample sizes. Here, we suggest that another important factor behind the improved replicability of genome-wide scans is an increase in the amount of statistical testing itself. We show that an increase in the number of tested hypotheses increases the proportion of true associations among the variants with the smallest P-values. We develop statistical theory to quantify how the expected proportion of genuine signals (EPGS) among top hits depends on the number of tests. This enrichment of top hits by real findings holds regardless of whether genome-wide statistical significance has been reached in a study. Moreover, if we consider only those "failed" studies that produce no statistically significant results, the same enrichment phenomenon takes place: the proportion of true associations among top hits grows with the number of tests. The enrichment occurs even if the true signals are encountered at the logarithmically decreasing rate with the additional testing.

---

### Detecting the ultra low dimensionality of real networks [^7635a2e9]. Nature Communications (2022). High credibility.

Fig. 2
Relation between densities of edge triangles, squares and pentagons for different values of γ.

Panels (A – C) show the projection of the phase space in the subspace (C s, C t), panels (D – F) show the projection in the subspace (C p, C t), and panels (G – I) show the projection in the subspace (C p, C s). In plots (A – F), the dashed red line represents the β = 2 D limit separating the small-world and large-world phases. In these plots, the area on the left of the dashed red line corresponds to β < 2 D (small-world phase) and the area on the right corresponds to β > 2 D (large-world phase if γ > 3). Each point represents an average over 10 network realizations. Standard errors are smaller than the symbols themselves.

There are several interesting patterns that can be observed in the phase space (C t, C s, C p). First, all the curves tend to collapse when there is only a small level of clustering, thus becoming dimension independent. This is to be expected because in this case the topological equivalent of the triangle inequality breaks down, so that the network loses its metric character. In addition, all the curves tend to be closer together — and so tend towards dimension independence — as γ → 2. This implies that, beyond the fact that a metric space may be needed to explain the observed levels of clustering, its dimensionality is not very important when degrees are strongly heterogeneous and networks are dominated by very big hubs. In turn, this explains why highly heterogeneous real networks are extremely well described by themodel.

---

### Transient power-law behaviour following induction distinguishes between competing models of stochastic gene expression [^bbab52ab]. Nature Communications (2025). High credibility.

The power-law exponent contains useful information on the number of gene states over biologically relevant timescales

To test more directly the practical utility of the theory, we use simulations to mimic the measurement of the mean mRNA count from experimental data. We assumed five measurements of the mean mRNA at regularly spaced time points and that the final measurement was taken at a time much earlier than that of the median half-life of mRNA in mammalian cells (several hours). The parameters of the N -state model were then chosen such that the mean mRNA at the largest time point was within the biologically meaningful range (less than a few tens of mRNA molecules per cell). For further details on the selection procedure see Supplementary Note 2.

For each of the rate parameter sets, we integrated the time-evolution equations for the mean mRNA counts of the N -state model with an initial condition of zero mRNA and starting from inactive state j to calculate the mean mRNA at each of the five-time points. Then we used a linear regression on the log-log plot of the mean versus time to estimate the slope and hence exponent of the power law. In all cases, the coefficient of determination of the best linear fit through the data points was larger than 0.99 thus firmly establishing the existence of a power law — however as clear from the example shown in Fig. 4 A, because of the length of the time interval used, the fitted power law was different than the theoretical one and hence it is of interest to understand the relationship between the two.

---

### The measurement postulates of quantum mechanics are operationally redundant [^d7941401]. Nature Communications (2019). High credibility.

Understanding the core content of quantum mechanics requires us to disentangle the hidden logical relationships between the postulates of this theory. Here we show that the mathematical structure of quantum measurements, the formula for assigning outcome probabilities (Born's rule) and the post-measurement state-update rule, can be deduced from the other quantum postulates, often referred to as "unitary quantum mechanics", and the assumption that ensembles on finite-dimensional Hilbert spaces are characterized by finitely many parameters. This is achieved by taking an operational approach to physical theories, and using the fact that the manner in which a physical system is partitioned into subsystems is a subjective choice of the observer, and hence should not affect the predictions of the theory. In contrast to other approaches, our result does not assume that measurements are related to operators or bases, it does not rely on the universality of quantum mechanics, and it is independent of the interpretation of probability.

---

### A general derivation and quantification of the third law of thermodynamics [^6246fb55]. Nature Communications (2017). Medium credibility.

A number of recent works analyse a process closely related to cooling to absolute zero: erasing information or generating pure states. In refs, it is shown that, regardless of the amount of time invested, these processes are strictly impossible if the reservoir is finite-dimensional. However, strict unattainability in the sense of Nernst is not really a physically meaningful statement. Rather, one wants to obtain a finite bound to how close one can get to the desired state with a finite amount of resources or within a given time. Some interesting steps in this direction are taken in ref. where they obtain a bound in terms of the dimension of the reservoir, but not one that can be translated into time. It also requires the dimension of the reservoir to be finite, an assumption that is not needed to derive our unattainability result here, and something which rarely holds in real setups. In fact, we shall see that the physical mechanism which enforces our third law is not dimension, but the profile of the density of states of the reservoir. On the other hand, argues that for a qubit, one can produce a pure state to arbitrary accuracy as the time invested increases. This, however, requires that the work injected in the bath fluctuates around its mean value by an unbounded amount (this is also necessary in ref.). A fact that becomes more relevant when cooling systems much larger than a qubit.

In the present article, we bound the achievable temperature by resources such as the volume of the reservoir and the largest value by which the work can fluctuate. This in itself can be said to constitute a version of the third law. However, we also argue that, in any process implemented in finite time, these two resources must remain finite too. When the scaling of these resources with time has a standard form (explained below), and the heat bath consists of radiation, our third law provides the following relation between the lowest achievable temperatureand time t

We believe this to be the first derivation of a quantitative lower bound on the temperature in terms of cooling resources, confirming the general validity of the models and conjecture in ref.(although we do not require the system to be continually thermal, we are able to get a bound which is more general than the differential equation postulated there).

---

### Dioptric power and refractive behaviour: a review of methods and applications [^f44382b1]. BMJ Open Ophthalmology (2022). High credibility.

The subscript index i = 1 to N is used for the sample measurements, and S are the sample mean and variance–covariance matrix, respectively. (Coordinate vector h could be used in equation 35 instead of vector f.)

Where the population distribution is not known, another method would be to use the Chebyshev inequalityto estimate the probability that specific measurements differ from their mean by more than a specified number of SD. Stellato et al also derive a multivariate form of the Chebyshev inequality that uses the Euclidean norm or Mahalanobis distance for outlier detectionbut, for simplicity here, the univariate Chebyshev inequality is:

and equation 36 provides the probability that a scalar and random variable with distributiondiffers from its mean (µ) by more than a specified number (λ) of SDs (σ) with the condition, that is, the number of SD is real and positive, and that the probability is less than or equal to the minimum as specified in equation 36. There are some difficulties with this inequality as we are usually not sure of the true values of the population means and variances, and thus they need to be estimated via sampling and this process might not always provide satisfactory estimates. Also, the inequality is sensitive to sample size and the number of samples concerned in estimation of the means and SD. (These limitations would, however, apply also to that of figure 4 where Mahalanobis distances (equation 35) are provided for the sample concerned.

---

### An analytical theory of balanced cellular growth [^b0d02f80]. Nature Communications (2020). High credibility.

The maximal balanced growth rate μ * will be a function of ρ. In analogy to the marginal net benefits of cellular components, we define the marginal benefit of the cellular density as the relative fitness increase facilitated by a small increase in ρ,

Using the method of Lagrange multipliers with the growth equation (Eq. (6)) as the objective function, we derive necessary conditions at optimal growth, which we term balance equations:(Theorem 10). Again, the presentation here assumes that there are no dependent reactants, while a corresponding result is derived for the general case with dependent reactants in "Methods" ("Optimal density-constrained balanced growth states"). Both with and without dependent reactants, the optimal state is perfectly balanced: the marginal net benefits of all independent cellular concentrations x i are identical. Thus, if the dry weight density ρ could increase by a small amount (such as 1 mg l −1), then the marginal fitness gain that could be achieved by increasing protein concentration by this amount is identical to that achieved by instead increasing the concentration of any reactant α by the same amount. This should not be surprising: if the marginal net benefit of concentration x i was higher than that of, growth could be accelerated by increasing x i at the expense of.

Equation (10) together with Eq. (9) describes a system of n + 1 equations for n + 1 unknowns, the independent concentrations x i. In realistic cellular systems, this set of equations has a finite number of discrete solutions. Thus, growth rate optimization can be replaced by searching for the solution of the balance equations. If the optimization problem is convex, the conditions given by Eq. (10) are necessary and sufficient, and the solution is unique.

---

### A comparative analysis of statistical methods to estimate the reproduction number in emerging epidemics, with implications for the current coronavirus disease 2019 (COVID-19) pandemic [^731a7292]. Clinical Infectious Diseases (2021). Medium credibility.

Background

As the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) pandemic continues its rapid global spread, quantification of local transmission patterns has been, and will continue to be, critical for guiding the pandemic response. Understanding the accuracy and limitations of statistical methods to estimate the basic reproduction number, R0, in the context of emerging epidemics is therefore vital to ensure appropriate interpretation of results and the subsequent implications for control efforts.

Methods

Using simulated epidemic data, we assess the performance of 7 commonly used statistical methods to estimate R0 as they would be applied in a real-time outbreak analysis scenario: fitting to an increasing number of data points over time and with varying levels of random noise in the data. Method comparison was also conducted on empirical outbreak data, using Zika surveillance data from the 2015–2016 epidemic in Latin America and the Caribbean.

Results

We find that most methods considered here frequently overestimate R0 in the early stages of epidemic growth on simulated data, the magnitude of which decreases when fitted to an increasing number of time points. This trend of decreasing bias over time can easily lead to incorrect conclusions about the course of the epidemic or the need for control efforts.

Conclusions

We show that true changes in pathogen transmissibility can be difficult to disentangle from changes in methodological accuracy and precision in the early stages of epidemic growth, particularly for data with significant over-dispersion. As localized epidemics of SARS-CoV-2 take hold around the globe, awareness of this trend will be important for appropriately cautious interpretation of results and subsequent guidance for control efforts.

---

### Transient power-law behaviour following induction distinguishes between competing models of stochastic gene expression [^bc0f8ccf]. Nature Communications (2025). High credibility.

We seek to understand what information about the number of gene states N can be deduced from the the moments of the mRNA counts measured a short time after induction. To this end, in Methods, starting from the chemical master equation (CME) for the N -state model, we use perturbation theory to show that in the limit of short times, the mean 〈 m (t)〉 of mRNA counts at time t is given by:In this equation both time and the rate parameters are non-dimensional since they are rescaled by the mRNA degradation rate parameter, i.e. t ↦ t d, k i ↦ k i / d and ρ ↦ ρ / d.

The expression for the mean, Eq. (3), suggests a simple method to extract information about the number of inactive states. For short times after induction, a log-log plot of the mean mRNA count versus time leads to a straight line with slope equal to N − j + 1. Since j = 1. N − 1, the slope provides a lower bound on the total number of gene states N. Specifically the slope estimates the the number of rate-limiting steps between the initial inactive state j and the active state N. This result also means that it is difficult to distinguish N -state models for which j = N − 1 from the 2-state telegraph model because in both cases the measured power law exponent is 2. In Fig. 3 A we confirm the derived power law by direct numerical integration of the time-evolution equations for the first moment of the mRNA counts of the N = 5-state model — since the system (1) is composed of purely first-order reactions, these equations are simply the deterministic rate equations.

---

### The making of the standard model [^0fc9a4ce]. Nature (2007). Excellent credibility.

A seemingly temporary solution to almost a century of questions has become one of physics' greatest successes.

---

### A general derivation and quantification of the third law of thermodynamics [^43738480]. Nature Communications (2017). Medium credibility.

The most accepted version of the third law of thermodynamics, the unattainability principle, states that any process cannot reach absolute zero temperature in a finite number of steps and within a finite time. Here, we provide a derivation of the principle that applies to arbitrary cooling processes, even those exploiting the laws of quantum mechanics or involving an infinite-dimensional reservoir. We quantify the resources needed to cool a system to any temperature, and translate these resources into the minimal time or number of steps, by considering the notion of a thermal machine that obeys similar restrictions to universal computers. We generally find that the obtainable temperature can scale as an inverse power of the cooling time. Our results also clarify the connection between two versions of the third law (the unattainability principle and the heat theorem), and place ultimate bounds on the speed at which information can be erased.

---

### Mathematical modeling is more than fitting equations [^994275ea]. The American Psychologist (2014). Low credibility.

Comments on the comments made by Brown et al. (see record 2013-24609-001). The article by Brown et al. regarding the Fredrickson and Losada (see record 2005-11834-001) article discussed the use of differential equations in science and repeated our earlier observation (Luoma, Hämäläinen, & Saarinen, 2008) that there is lack of justification for the use of the Lorenz equations in the latter article. In this comment we want to point out that Brown et al. presented a very narrow view on mathematical modeling in behavioral research. We describe how the conceptual use of mathematical models is essential in many fields.

---

### State estimation of a physical system with unknown governing equations [^794152e0]. Nature (2023). Excellent credibility.

State estimation is concerned with reconciling noisy observations of a physical system with the mathematical model believed to predict its behaviour for the purpose of inferring unmeasurable states and denoising measurable ones 1,2. Traditional state-estimation techniques rely on strong assumptions about the form of uncertainty in mathematical models, typically that it manifests as an additive stochastic perturbation or is parametric in nature 3. Here we present a reparametrization trick for stochastic variational inference with Markov Gaussian processes that enables an approximate Bayesian approach for state estimation in which the equations governing how the system evolves over time are partially or completely unknown. In contrast to classical state-estimation techniques, our method learns the missing terms in the mathematical model and a state estimate simultaneously from an approximate Bayesian perspective. This development enables the application of state-estimation methods to problems that have so far proved to be beyond reach. Finally, although we focus on state estimation, the advancements to stochastic variational inference made here are applicable to a broader class of problems in machine learning.

---

### Bayesian strategy selection identifies optimal solutions to complex problems using an example from GP prescribing [^103d3a03]. NPJ Digital Medicine (2020). Medium credibility.

The algorithm begins by assuming a stochastic distribution of reward for each arm, to be initially non-informative — each is a Gaussian with a mean reward of 0 with a standard deviation of 0.3. This is because we expect the reward to lie in the interval [−1,1]. At each iteration, based on the exploitation-exploration trade-off policy and the current reward distributions of arms, an arm is chosen. We choose 13 arms (strategies), one for each clinic. For each clinic, the chosen strategy is implemented and used by all GPs in that clinic. It is important to note that a strategy may be played in more than one clinic and may also be repeated over time to reduce the uncertainty over its reward.

The outcome for each clinic for a particular strategy at the end of the intervention week is the fraction of the number of conversations (for all GPs) that included a discussion of physical activity, y, out of all the conversations where a discussion of physical activity could be relevant, n. Conversations deemed irrelevant included difficulties with a disability, patients who were infants, and patients with a drug use problem. The outcome of strategy i at clinic c, in week τ of the trial is thus given by (see Eq. (1)):The reward for a strategy for a particular clinic in week τ is then computed as the difference between the outcome for the tested strategyand the outcome for the baseline strategy for that clinic,(see Eq. (2)):Formally, the algorithm then proceeds as follows: Let μ i denote the mean reward of the i-th arm (strategy). We use a prior distribution on μ as p(μ i) = , where we select m 0 = 0 for each strategy to start with a prior belief that each strategy is the baseline strategy. The standard deviation s 0 is set to 0.3 to cover the possibilities of mean reward μ i to lie between [−1, 1] with high probability (greater than 0.999). We also assume the reward observation model to be Gaussian distributed as, whereis an i.i.d. Gaussian random variable with zero mean and standard deviation 1. The data collected from all the clinics up to week t is denoted asfor all c, wheredenotes the strategy used in week τ at clinic c. After observing data D t, the posterior distribution of μ i is given aswith mean m t and standard deviation s t given as (see Eq. (3)):

---

### An analytical theory of balanced cellular growth [^a7d1bc0f]. Nature Communications (2020). High credibility.

Discussion

At the heart of our mathematical derivations is A −1, the inverse of the mass-normalized active stoichiometric matrix A of any given EGS (or, more generally, any given BGS with linearly independent reactions). A −1 provides important information on the cellular efficiency. As seen from Eq. (4), quantifies which proportion of reaction flux v j is required to offset the dilution of the downstream cellular component i (either total protein P or reactant α). These non-local, structural mass-balance constraints lead to an explicit dependence of reaction fluxes on the cellular concentrations (Eq. (4), Theorem 5). Independently of this, fluxes also depend on concentrations through reaction kinetics (constraint (3)). Combining these two relationships leads to explicit expressions for the individual protein concentrations p j and for the growth rate μ, casting them as functions of the concentrations x = [P, a] T. Accordingly, A −1 accounts for all systems-level contributions to the marginal costs and benefits of cellular concentrations x i, while the kinetic functions k j (a) account for local effects. The insight that optimal, density-constrained states of balanced growth are EGS allowed us to derive the balance equations (Eq. (10)); furthermore, as any BGS can be expressed as a weighted average of EGSs (Theorem 3), our results allow a general characterization of the solution space of balanced growth.

While computational limitations restricted previous studies of balanced growth to specific models with 2–7 reactions, we here provide general results for arbitrarily complex cellular systems. Except for the maximal cellular dry weight density constraint (9), the balanced growth model proposed by Molenaar et al.and utilized subsequently for the analysis of schematic models,– is based on assumptions identical to those made for GBA, constraints (1), (2) and (3). Previous authors (with the exception of Faizi et al.) assumed a limit on total protein ("macromolecular") concentrations, while we assume a joint limitation of all cellular solutes (Eq. (9)). The latter choice is justified by the approximate constancy of the cellular dry mass density across growth conditions –, and by an observed relationship between enzyme and substrate concentrations that is consistent with natural selection on the parsimonious use of a limited dry mass density.

---

### State estimation of a physical system with unknown governing equations [^b2864b1a]. Nature (2023). Excellent credibility.

Example 5: high-dimensional, spatially extended differential-equation discovery with low-rank observation matrices

We now consider the problem of recovering the underlying governing equations for the Lorenz '96 system with 1,024 states using a low-rank observation matrix. The Lorenz '96 model is a set of coupled, chaotic, ODEs designed to be a simplified model of the climate along a line of constant latitude. The governing equations for this system are given by, for which the boundary conditions are assumed to be periodic (k = 1, 2, …, 1,024).

We generated observation matrices of rank r using the expression, in which eachis a random vector sampled from a standard normal distribution. We studied the performance of our approach in which the rank of the observation matrix is 256, 512 and 1,024. For each experiment, we used 512 snapshots over the time interval of 0 to 10 corrupted by noise that is 2% of the range of the system. We make the assumption that the dynamics are given by, in whichreturns all quadratic polynomial functions that are a function of x i and its two closest neighbours to the left and right of the node. Although we found that we were able to exactly recover the underlying functional form of the governing equations with an observation matrix whose rank was half the dimensionality of the state, further work is required to theoretically establish conditions under which the governing equations can be exactly recovered. These results are summarized in Extended Data Fig. 2.

Because many real-world systems for which governing equations are challenging to derive from first principles are both (1) high-dimensional and (2) challenging to measure, we believe this to be a useful result. Our method has a computational cost that scales linearly in the state dimension and can be applied given an arbitrary observation function. This result opens the door for equation discovery in systems that were previously believed to be too large and/or difficult to measure.

---

### Allometric cascade as a unifying principle of body mass effects on metabolism [^0bbbcb77]. Nature (2002). Excellent credibility.

The power function of basal metabolic rate scaling is expressed as aM(b), where a corresponds to a scaling constant (intercept), M is body mass, and b is the scaling exponent. The 3/4 power law (the best-fit b value for mammals) was developed from Kleiber's original analysis and, since then, most workers have searched for a single cause to explain the observed allometry. Here we present a multiple-causes model of allometry, where the exponent b is the sum of the influences of multiple contributors to metabolism and control. The relative strength of each contributor, with its own characteristic exponent value, is determined by the control contribution. To illustrate its use, we apply this model to maximum versus basal metabolic rates to explain the differing scaling behaviour of these two biological states in mammals. The main difference in scaling is that, for the basal metabolic rate, the O(2) delivery steps contribute almost nothing to the global b scaling exponent, whereas for the maximum metabolic rate, the O(2) delivery steps significantly increase the global b value.

---

### The mechanics of landslide mobility with erosion [^f4b3436e]. Nature Communications (2021). High credibility.

Erosion can significantly increase the destructive power of a landslide by amplifying its volume, mobility and impact force. The threat posed by an erosive landslide is linked to its mobility. No mechanical condition has yet been presented for when, how and how much energy erosive landslides gain or lose. Here, we pioneer a mechanical model for the energy budget of erosive landslides that controls enhanced or reduced mobility. Inertia is related to an entrainment velocity, is a fundamentally new understanding. This ascertains the true inertia of erosive landslides, making a breakthrough in correctly determining the landslide mobility. Erosion velocity, which regulates the energy budget, determines the enhanced or reduced mobility. Newly developed energy generator offers the first-ever mechanical quantification of erosional energy and a precise description of mobility. This addresses the long-standing question of why many erosive landslides generate higher mobility, while others reduce mobility. We demonstrate that erosion and entrainment are different processes. Landslides gain energy and enhance mobility if the erosion velocity exceeds the entrainment velocity. Energy velocity delineates distinct excess energy regimes. Newly introduced mobility scaling and erosion number deliver the explicit measure of mobility. Presented dynamical equations correctly include erosion induced net momentum production.

---

### Optimal enzyme utilization suggests that concentrations and thermodynamics determine binding mechanisms and enzyme saturations [^440ef9eb]. Nature Communications (2023). High credibility.

Petersen linearization

After approximating elementary displacements from equilibrium, the rate equation contained the bilinear terms arising from the products,… We could approximate these continuous products using the following derivation, which is only shown for the productfor simplicity. The same linearization scheme also applies to the remaining nonlinearities of the formand.

The productis bilinear, though is a product of a binary and a continuous variable. Assuming a constantwe could apply the Petersen linearization scheme, to the bilinearity. Replacingwith another non-negative variable, where s stands for the index of the binary variable and i stands for the elementary step, we could represent the bilinear product by one new variable and three new constraints:

Note that when N e > 3, we need an additional linearization to account for the product of two binary variables. As an example, consider the random-ordered Bi-Uni mechanism given in Eq. (3). By approximating 4 elementary displacement variableswe can express all 6 elementary displacements as explained in the previous section. To describe the reaction rate from the product dissociation step, we can use the following equations:

Note that the product-dissociation stepis not in the branched pathway of the reaction mechanism. We then needed to represent γ 4 from the approximated elementary displacements as.

We can rewrite the constraints above using the following equations:Hereandare the approximations by a piecewise-constant function as described by Eqs. (20)–(24). As both approximations contain binary variables, their product needed to be considered. This product could be linearized by representing it with a new binary variable and three new constraints as follows:where, δ s and λ p are binary variables. δ s and λ p are the binary variables in the binary expansion forandrespectively. After this linearization, the remaining bilinearity was of the form continuous × binary, which could be linearized using Petersen's theorem, (Eqs. (25)–(26)).

Using the change of variables and the piecewise-constant approximation as described above, we translated the nonlinear optimization problem given in Eq. (15) to a MILP, which can be summarized as follows:

Note that, the overall thermodynamic constraint was dropped in the final formulation, as approximations of the independent elementary displacement variableswere performed accordingly as explained before.

---

### A multi-step model of Parkinson's disease pathogenesis [^e2cef97a]. Movement Disorders (2021). Medium credibility.

Box 1
Armitage and Doll Multistep Model – Heuristic Argument*

If disease development depends on one step, incidence in a given year will be proportional to the chance of undergoing that step:If two‐steps are required, then incidence is the product of the chance of undergoing the first step by age t and the rate of undergoing the second step:And for n‐steps:Taking log of both sides returns the equation for a straight line:The slope is one less than the number of steps required to develop the disease, and the intercept represents the combined probability of undergoing these steps.

*see Webster 2019 for full derivation.

A multistep model of pathogenesis could account for many of the epidemiological observations made in PD. These include the variability in the expression of disease and age of onset in carriers of disease‐causing mutations, and the multiple environmental and genetic associations that confer a risk of developing PD. A multistep model could also explain the phenotypic variability seen in PD, if it is assumed that at least some steps apply to specific neuronal populations rather than the nervous system as a whole. Furthermore, the predictions arising from such a model can be used to test specific hypotheses about basic observations in PD, such as whether the higher incidence and prevalence of PD in males observed in most parts of the world relates to differential environmental exposures by sex.

---

### A general derivation and quantification of the third law of thermodynamics [^314e2820]. Nature Communications (2017). Medium credibility.

Discussion

We hope the present work puts the third law on a footing more in line with those of the other laws of thermodynamics. These have already been long established, although they've recently been reformulated within the context of other resource theories. Namely, as described in ref. the first law (energy conservation), and unitarity (or microscopic reversibility) describe the class of operations which are allowed within thermodynamics. The zeroth law, is the fact that the only state which one can add to the theory without making it trivial, are the equivalence class of thermal states at temperature T. This allows the temperature to emerge naturally. The second law(s), tells us which state transformations are allowed under the class of operations. For macroscopic systems with short-range interactions, there is only one function, the entropy, which tells you whether you can go from one state to another, but in general there are many constraints. The third law quantifies how long it takes to cool a system. We propose to generalize it further: While the second laws tell us which thermodynamical transitions are possible, generalized third laws quantify the time of these transitions. In this context, it would be interesting to explore the time and resource costs of other thermodynamical transitions. It would also be interesting to explore the third law in more restricted physical settings, as well as to other resource theory frameworks, in particular, those discussed in ref.

It is worth noting that scaling we find, for example, the inverse polynomial of equation (1), is more benign than what one might have feared, and does not exclude obtaining lower temperatures with a modest increase of resources. However, it is also stronger than that envisioned when the third law was original formulated. Consider for example, the cooling protocol of Fig. 1 proposed by Nernst. It is unphysical, since it requires an infinite number of heat baths, each one at a lower and lower temperature, and with the final heat baths at close to zero temperature. However, it allows the temperature of the system to decrease exponentially in the number of steps, something which we are able to rule out when one doesn't have colder and colder reservoirs.

---

### Calibrated rare variant genetic risk scores for complex disease prediction using large exome sequence repositories [^bcae2ba0]. Nature Communications (2021). High credibility.

Methods

Single gene simulations

We simulated an additive rare variant association model for a single gene with differing probabilities of mutation based on 3 confounding parameters: (1) sequencing false negative (SFN) rates, (2) sequencing false positive (SFP) rates, and (3) population-specific factors (PSF). SFN is a real number between 0 and 1 that represents the proportion of true variant calls missed by sequencing. SFP is a real number ≥ 0 and corresponds to the ratio of called false variants to the number of (true) variants in the reference population. The PSF parameter is also a real number ≥ 0 and is defined as the ratio of (true) variants in a gene relative to a reference sample. PSF represents the effect of population substructure, where PSF = 1 when the test and reference populations are perfectly matched. The probability of mutation P for the gene is given by Eq. (1):where CMAF Gene is the baseline cumulative minor allele frequency (CMAF) (defined as the aggregate sum of the MAF) of the simulated gene in the absence of any genetic effect, sequencing effects (i.e. SFN and SFP), or population effects (i.e. PSF). Please note use of CMAF Gene implicitly assumes variants are rare and LD negligible. When a genetic effect exists, Eq. (1) can be written as:where P represents the probability of mutation given an individual is a case and is therefore dependent upon the OR, which is the true odds ratio, or genetic effect of the association. It is important to note that the OR represents the ratio of the frequency of rare alleles in a case population to a control population using the approximation of rare exposures:wherefor rare exposures (i.e. the CMAF attributed to the aggregate rare allele frequency within a gene). The equation for OR thus simplifies towhich we also empirically confirmed.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^e2a04562]. Nature Communications (2021). High credibility.

With DDEs, multiple steps ("cascades") within a network can be rigorously simplified into a single step with delay (see below), an approach which has been explored in a variety of biological contexts. This makes interpretation of the phenomenology simpler than with ODEs and reduces the number of equations and parameters in the model. This idea can be visualized by depicting regulatory networks as directed graphs, with nodes representing biological species, and pointed vs. blunt arrows indicating activation vs. repression, respectively (Fig. 1). DDE models allow a single arrow to faithfully capture many biochemical steps, expanding the available dynamics in a model with a reduced set of equations and parameters. DDE regulatory models have in fact been used widely to model a range of biological phenomena such as development, and hematopoiesis. For instance, a 1-variable ODE such as Eq. (1) can only produce exponential growth or decay. A corresponding DDE, such as Eq. (2), can also oscillate (with amplitude approaching zero or infinity) and, if nonlinear, lead to stable oscillations, bistability and chaos.

A key challenge in using DDEs is their mathematical complexity relative to ODEs. For example, while Eq. (1) is 1-dimensional because one initial condition (x at t = 0) determines all future behavior, Eq. (2) is infinite-dimensional, because x must be specified at all times − τ ≤ t ≤ 0 to predict a unique solution. Concretely, the solution to Eq. (1) can be written as a single exponential, while Eq. (2) generally must be specified as a sum of an infinite number of exponential terms to satisfy initial conditions.

Despite the challenges, much progress has been made in analytical understanding of DDEs, and numerical methods exist for simulation. We thus see an opportunity to use DDEs to recapitulate dynamics found in ODE solutions of network motif behavior with fewer genes and thus fewer modeling parameters and equations (Fig. 1), a type of "modeling simplicity".

---

### High-order michaelis-menten equations allow inference of hidden kinetic parameters in enzyme catalysis [^21a9eeb4]. Nature Communications (2025). High credibility.

Single-molecule measurements provide a platform for investigating the dynamical properties of enzymatic reactions. To this end, the single-molecule Michaelis-Menten equation was instrumental as it asserts that the first moment of the enzymatic turnover time depends linearly on the reciprocal of the substrate concentration. This, in turn, provides robust and convenient means to determine the maximal turnover rate and the Michaelis-Menten constant. Yet, the information provided by these parameters is incomplete and does not allow access to key observables such as the lifetime of the enzyme-substrate complex, the rate of substrate-enzyme binding, and the probability of successful product formation. Here we show that these quantities and others can be inferred via a set of high-order Michaelis-Menten equations that we derive. These equations capture universal linear relations between the reciprocal of the substrate concentration and distinguished combinations of turnover time moments, essentially generalizing the Michaelis-Menten equation to moments of any order. We demonstrate how key observables such as the lifetime of the enzyme-substrate complex, the rate of substrate-enzyme binding, and the probability of successful product formation, can all be inferred using these high-order Michaelis-Menten equations. We test our inference procedure to show that it is robust, producing accurate results with only several thousand turnover events per substrate concentration.

---

### Stability and complexity in model meta-ecosystems [^5a1c6798]. Nature Communications (2016). Medium credibility.

Results

Stability criterion for random matrices

The stability criteria for matrix J is generally complicated, but can be simplified under some conditions. Assuming that both S and n are large and that d is also sufficiently large, we obtain the following stability criterion (Fig. 2c):

whereis the effective number of ecologically independent patches in the meta-ecosystem. When elements of the random matrices are perfectly correlated among patches (that is, ρ = 1), equation (3) simplifies to May's formula, that is, equation (1). By contrasts, when elements of the random matrices are all completely independent (ρ = 0), the stability criterion then reduces to, that is, the maximal admissible complexity parameteris multiplied by the number of patches.

The effect of the effective number of patches n e in equation (3) results from the statistical thinning of the variance of the 'average ecosystem' obtained by linking all ecosystems through dispersal (that is, as an instance of the central limit theorem). In other words, meta-ecosystems are stabilized because the main effect of dispersal is to make stability dependent on interaction coefficients averaged over many ecosystems, rather than on local interaction coefficients per se. By way of being average quantities, these meta-ecosystem interaction coefficients have lower variance, and hence correspond to an empirical spectral distribution with a smaller radius. These results indicate that dispersal can stabilize meta-ecosystem dynamics in proportion to its effective ecological size (the number of effectively independent habitat patches). To the degree that different local ecosystems have similar conditions (ρ increases and thus n e decreases) however, this effect is weakened and the effect of dispersal disappears if all the local ecosystems are identical (that is, when n e = 1 and ρ = 1) and we recover May's equation (1).

When d is small (and S and n large), however, the effect of dispersal is very different (Supplementary Note 1 and Fig. 2b). The criterion corresponding to equation (3) in this situation is

In this case, stability increases with d, regardless of the value of n e and it does so additively (at least in cases where d is very small) in comparison with the criterion for isolated ecosystems. In this case, environmental heterogeneity is not important — emigration alone is the factor improving the stability of meta-ecosystems and its importance will depend on its magnitude relative to the intraspecific density dependence. Analytical results hold even in presence of heterogeneity among species in dispersal rates (Supplementary Note 1).

---

### CLIMB: high-dimensional association detection in large scale genomic data [^c4e27f7e]. Nature Communications (2022). High credibility.

These association vectors can be appreciated as an alternative to binarization or ternarization of genomic signals, since they assign binary or ternary labels to the data. A label directly reflects the pattern of condition specificity of the observations in its associated cluster. Further, as a mixture modeling approach, these labels naturally allow for heterogeneity in signals, resulting in greater model flexibility.

Yet, a remaining challenge is that models that leverage these association vectors suffer from computational intractability for even a modest number of conditions. To understand this issue, consider D conditions: Letbe the set of all 3 D possible configurations of association vector H, such that an observation described by an association vector with h [i] = 1(h [i] = − 1) has a positive (negative) association in condition i. It is clear that this model formulation becomes computationally prohibitive even for single-digit D because the total number of possible association vectors grows exponentially with D, possibly resulting in the number of model parameters exceeding the number of observations. In response to this, several restrictive assumptions are imposed. For example, Amar et al.somewhat alleviate computational burden by assuming all associations must be positive, and estimate partial latent associations for subgroups of conditions with a heuristic approach. This heuristic reduces statistical power and resolution when testing for consistent findings, and cannot provide a single unified clustering of observations since it is not a true joint analysis. Moreover, this approach does not distinguish an observation that is significant in opposite directions in two conditions from an observation that exhibits consistent direction of association across conditions. Alternatively, Urbut et al.make computational gains by assuming all observations come from a uni-modal distribution centered over zero, but this restriction does not always hold in practice.

---

### Optimizing enzyme inhibition analysis: precise estimation with a single inhibitor concentration [^19f27dc6]. Nature Communications (2025). High credibility.

Inhibition constants can be estimated through model fitting, but they are often determined by using the simplified Cheng-Prusoff equation (Eq. 4) based solely oninformation, as follows:This approach is known for its simplicity and high accuracy. However, since the Cheng-Prusoff equation varies depending on the inhibition type, estimating inhibition constants solely fromdata requires prior knowledge. On the other hand, the 50-BOA uses the general Cheng-Prusoff equation (Eq. 2), which does not require inhibition type, as a regularization term (Eq. 3) in the estimation process. Incorporating the Cheng-Prusoff equation into the fitting process leads to more precise estimations than those from the canonical inhibition model-based method.

Previous studies also have investigated optimal experimental designs for estimating inhibition constants, resulting in the recommendation of D-optimal design. D-optimal design identifies experimental conditions that maximize the determinant of the information matrix, whose inverse approximates the volume of the confidence ellipsoid for parameter estimates. It recommends measuring initial velocities multiple times at the theoretical minimum number of experimental points — equal to the number of parameters — to maximize the determinant of the information matrix. However, this design depends on the unknown true parameter values, the very target of the estimation. As a result, this approach requires prior estimates of parameters that are close to the true values to achieve precise estimation. Furthermore, the design aims to minimize the overall volume of the confidence ellipsoid rather than focusing on the confidence intervals of individual parameters. As a result, it can lead to precise estimation of one parameter at the expense of others, as noted in previous research. In this study, we addressed these limitations of D-optimal design. Our approach provides a straightforward experimental design based on known values, such as, rather than relying on unknown true parameters. Additionally, by incorporatinginformation as a regularization term in the optimization process, we achieved precise estimation for both inhibition constants while requiring only the theoretical minimum number of experimental points, which is equal to the number of parameters being estimated (i.e. twovalues and onevalue).

---

### Observing electron localization in a dissociating hmolecule in real time [^90b75d20]. Nature Communications (2017). Medium credibility.

Dissociation of diatomic molecules with odd number of electrons always causes the unpaired electron to localize on one of the two resulting atomic fragments. In the simplest diatomic molecule H 2 + dissociation yields a hydrogen atom and a proton with the sole electron ending up on one of the two nuclei. That is equivalent to breaking of a chemical bond-the most fundamental chemical process. Here we observe such electron localization in real time by performing a pump-probe experiment. We demonstrate that in H 2 + electron localization is complete in just 15fs when the molecule's internuclear distance reaches 8 atomic units. The measurement is supported by a theoretical simulation based on numerical solution of the time-dependent Schrödinger equation. This observation advances our understanding of detailed dynamics of molecular dissociation.

---

### A general model for ontogenetic growth [^10660e44]. Nature (2001). Excellent credibility.

Several equations have been proposed to describe ontogenetic growth trajectories for organisms justified primarily on the goodness of fit rather than on any biological mechanism. Here, we derive a general quantitative model based on fundamental principles for the allocation of metabolic energy between maintenance of existing tissue and the production of new biomass. We thus predict the parameters governing growth curves from basic cellular properties and derive a single parameterless universal curve that describes the growth of many diverse species. The model provides the basis for deriving allometric relationships for growth rates and the timing of life history events.

---

### The value of understanding feedbacks from ecosystem functions to species for managing ecosystems [^8044a087]. Nature Communications (2019). High credibility.

Ecosystem dynamics and transition probabilities

The ecosystem dynamics are captured in the transition probability matrix in MDP. Let P be the transition probability matrix representing the dynamics of the system from time step t to time step t + 1.represents the conditional probability of the ecosystem transitioning from state x t to x t +1 given action a t is implemented at time t. We assume that species j could be present or absent at each time step. This transition probability is also conditional on the baseline probability of survival of species j, the feedback strength α (the percentage of the ecosystem function going back to a species), the predation strength b, the feedback structure f and the food web matrix M representing the prey-predator interactions of our system. To model this transition probability, we assume that, knowing the state at time t, x t, the state of species j at time t + 1 is independent of the state of the other species at time t + 1. So we can define the transition probability P as the product of J individual species' transition probabilities:

Survival probability of a species will increase with the number of extant preys N prey (j, x t, M) and ecosystem function available N EF (j, x t, f), and will decrease with the number of extant predators N predator (j, x t, M). We assume that N prey (j, x t, M), N EF (j, x t, f), and N predator (j, x t, M) are maximum at the initial time step where all species are present (i.e. x t = x 0 = [1,1,1,1]). Formally, we defined the transition probability when species j is not under protection (a t ≠ j) as the product of four terms:

In this way, under the most favourable condition where species j has no predator, no prey loss and receive maximum level of ecosystem function, the above equation reduces to its baseline probability of survival. However, species j survival probability will decrease when at least one of the following three events happen — prey loss, predator presence, or insufficient functional support (see 'Using Markov Decision Processes to model species dynamics and protection actions effects' in the Supplementary Methods).

---

### 2013 ACC / AHA guideline on the assessment of cardiovascular risk: a report of the American college of cardiology / American Heart Association task force on practice guidelines [^38029cee]. Journal of the American College of Cardiology (2014). Medium credibility.

ACC/AHA cardiovascular risk assessment — Table 8 describes estimating an individual's 10-year risk of incident hard atherosclerotic cardiovascular disease (ASCVD) using race-sex group–specific coefficients and a worked example. The example profile assumes an individual 55 years of age (for which the Ln[Age] = 4.01), with a total cholesterol of 213 mg/dL, HDL-C (high-density lipoprotein cholesterol) of 50 mg/dL, and an untreated systolic BP (blood pressure) of 120 mm Hg who is not a current smoker and does not have diabetes. The calculation is performed in steps: the natural log (Ln) of age, total cholesterol, HDL-C, and systolic BP is calculated; appropriate interaction terms are derived (for example, Ln[Age]×Ln[Total Cholesterol]); the resulting values are multiplied by coefficients from the equation for the specific race-sex group to yield "Coefficient × Value" terms that are summed as an "Individual Sum". The estimated risk is formally calculated as "1 minus the survival rate at 10 years ("Baseline Survival" in Table A), raised to the power of the exponent of the "Coefficient × Value" sum minus the race- and sex-specific overall mean "Coefficient × Value" sum", and the white men example equates to a 5.3% probability of a first hard ASCVD event within 10 years.

---

### The use of opioids in the management of chronic pain: synopsis of the 2022 updated U.S. department of veterans affairs and U.S. department of defense clinical practice guideline [^6e0d0c24]. Annals of Internal Medicine (2023). High credibility.

VA/DoD Clinical Practice Guideline for the Use of Opioids in the Management of Chronic Pain — 2017 clinical practice guideline recommendations for acute pain include: "We recommend alternatives to opioids for mild-to-moderate acute pain", "We suggest use of multimodal pain care including non-opioid medications as indicated when opioids are used for acute pain", and "If take-home opioids are prescribed, we recommend that immediate-release opioids are used at the lowest effective dose with opioid therapy reassessment no later than 3–5 days to determine if adjustments or continuing opioid therapy is indicated", with strengths "a) Strong for b) Weak for c) Strong for", and the 2017 CPG recommendation category "Reviewed, New-added". The page adds "Note: Patient education about opioid risks and alternatives to opioid therapy should be offered", and the disposition lists "a) Not reviewed, Deleted b) Not reviewed, Deleted c) Not reviewed, Deleted" with recommendation numbers "a) N/A b) N/A c) N/A".

---

### Semiparametric estimation of the relationship between ROC operating points and the test-result scale: application to the proper binormal model [^9a9d14d8]. Academic Radiology (2011). Low credibility.

Rationale and Objectives

Semiparametric methods provide smooth and continuous receiver operating characteristic (ROC) curve fits to ordinal test results and require only that the data follow some unknown monotonic transformation of the model's assumed distributions. The quantitative relationship between cutoff settings or individual test-result values on the data scale and points on the estimated ROC curve is lost in this procedure, however. To recover that relationship in a principled way, we propose a new algorithm for "proper" ROC curves and illustrate it by use of the proper binormal model.

Materials and Methods

Several authors have proposed the use of multinomial distributions to fit semiparametric ROC curves by maximum-likelihood estimation. The resulting approach requires nuisance parameters that specify interval probabilities associated with the data, which are used subsequently as a basis for estimating values of the curve parameters of primary interest. In the method described here, we employ those "nuisance" parameters to recover the relationship between any ordinal test-result scale and true-positive fraction, false-positive fraction, and likelihood ratio. Computer simulations based on the proper binormal model were used to evaluate our approach in estimating those relationships and to assess the coverage of its confidence intervals for realistically sized datasets.

Results

In our simulations, the method reliably estimated simple relationships between test-result values and the several ROC quantities.

Conclusion

The proposed approach provides an effective and reliable semiparametric method with which to estimate the relationship between cutoff settings or individual test-result values and corresponding points on the ROC curve.

---

### Thinking outside the curve, part I: modeling birthweight distribution [^49eb3a50]. BMC Pregnancy and Childbirth (2010). Low credibility.

Results

1. Pragmatics for mixture modeling

a. Finite normal mixture models

Many phenomena cannot be accurately described via a normal distribution. When no other commonly used probability distribution seems appropriate, a finite normal mixture model is often reasonable. We now briefly describe the model. Readers interested in theoretical developments may consult references and works cited therein.

Let f (x;μ,σ) denote the probability density for the normal distribution with mean μ and standard deviation σ. A finite normal mixture model with k components has probability density

A common way to interpret Equation (1) is to imagine that the full population consists of k subpopulations. The proportion of individuals in the full population belonging to subpopulation j is p j. In subpopulation j, measurements are normally distributed with mean μ j and standard deviation σ j.

The mixture components may or may not represent subpopulations with obvious biological definitions outside the statistical model. For example, in a 2-component normal mixture describing birthweights for white singletons in the United States, there is not an obvious biological characterization for the two components: we may say that the component with the smaller mean reflects compromised pregnancies, but we cannot immediately attribute the compromised pregnancies to a specific biological mechanism.

Ideally, modeling with finite normal mixtures may lead to discoveries of subpopulations with biological definitions that were not immediately obvious, although the mixture components themselves may still only be approximations to such subpopulations.

---

### A scalable Bayesian functional GWAS method accounting for multivariate quantitative functional annotations with applications for studying Alzheimer disease [^3a2b5b50]. HGG Advances (2022). Medium credibility.

Simulation study design

We conducted simulation studies to validate the performance of BFGWAS_QUANT. Continuous phenotypes were simulated using the real whole-genome sequence (WGS) data of chromosomes 19 (122,745 SNPs with minor allele frequency [MAF] > 0.01) for 1,893 samples from the ROS/MAP cohort, and Mount Sinai Brain Bank (MSBB) study. Phenotypes were simulated based on the multivariate linear additive model (Equation 1) with true genetic effect sizesgenerated based on the hierarchical logistic model with multivariate quantitative functional annotations (Equation 2). Scenarios with various numbers of true causals and heritability were considered.

Besides the real cis -eQTL-based functional annotations of Allcis-QTL, 95%CredibleSet, and MaxCPP, we also considered a fourth artificial annotation randomly generated from N (0, 1) as a negative control. With chosen annotation enrichment parameters, we first calculated casual probabilitiesfor all considered 122,745 SNPs by Equation 2, wherewas chosen to ensure the total number of true causal SNPs fall in (5, 10) withor (15, 30) with. Second, a vector of binary indicatorof true causal SNPs was generated from the corresponding Bernoulli distribution with probability for. Third, genetic effect sizes were taken as 0 for SNPs withor generated from a normal distribution for SNPs with. Finally, phenotypes were generated from Equation 1 with simulated genetic effect sizes and random errorsto ensure that a target total heritabilitywas equally explained by all true causal SNPs. Four scenarios were considered, including one with relatively sparse true causals in the range of (5, 10) and one with the number of true causals in the range of (15, 30) with respect to two different heritability values (0.25, 0.5).

---

### Single-molecule theory of enzymatic inhibition [^d57cf9e7]. Nature Communications (2018). Medium credibility.

Uncompetitive inhibition at the single-enzyme level

We now turn to employ the same type of analysis to uncompetitive inhibition (Fig. 4). Interestingly, the situation here is very different from the competitive case analyzed above, and strong deviations from the classical behavior are observed. To show this, we follow a path similar to that taken above and obtain a generalized equation for the turnover rate of a single enzyme in the presence of uncompetitive inhibition (Supplementary Methods):where. Equation (3) should be compared to Eq. (1) in the limit, and we once again see that both exhibit the same characteristic 1/[S] dependence. The dependence on inhibitor concentration is, however, different from that in Eq. (1) as Eq. (3) also includes two additional factors, and, whose emergence is a direct result of non-Markovian stochastic fluctuations at the single-enzyme level.andcould be understood in terms of average life times and transition probabilities (Methods), but are otherwise complicated functions of [I]. We nevertheless note thatalways; and that in the Markovian case, i.e. when the schemes presented in Figs. 4 and 1 (middle) coincide, for all [I]. Equation (3) then reduces to Eq. (1) in the limit, but in all other cases analyzed this is no longer true. In particular, Eq. (3) predicts that the classical, Markovian, theory of uncompetitive inhibition will inevitably break down when catalysis times come from a non-exponential distribution.

Fig. 4
A generic scheme for uncompetitive inhibition at the single-enzyme level. Transition rates have once again been replaced with generally distributed transition times to generalize the Markovian scheme in Fig. 1

---

### A multi-step model of Parkinson's disease pathogenesis [^0a462123]. Movement Disorders (2021). Medium credibility.

Bayesian regression models defined in Stan and fit in RStan were used to determine the model parameters (slopes, intercepts, inflexion points). The median of the posterior distribution is given as the parameter estimate, along with credible intervals defined by 2.5% and 97.5% percentile quantiles from the posterior distribution. Full data, model specification, and analysis code are available at.

The Armitage–Doll model is:whereis incidence, is age, is the product of the exposure risks for each step, andis the number of slow stages (steps). Taking the log of both sides gives the linear equation:where.

The basic Armitage–Doll model cannot account for a reduction in incidence at very old ages (> 80 years for Parkinson's), as is observed in most cancers and in Parkinson's. We therefore restricted fitting the basic Armitage–Doll model to a maximum of age 80 years, based on the visual inspection of the incidence data and previous approaches.

A linear regression, taking into account the uncertainty in the age‐specific incidence, was used to examine the fit of the standard Armitage–Doll model. A Bayesian R‐squared measure, based on the median age‐specific values, was used to determine how well the data points fitted the model. Then, to test the hypothesis that the data set contained two separate populations with differing number of steps required to develop Parkinson's, we extended the Bayesian regression model to a broken‐stick model to allow an unknown breakpointafter which the slope increased:wherefor

Finally, to test for sex differences, the regression model was further extended to allow both the intercept and slope (but not breakpoint) to vary by sex. Bayes factors were used to determine if the extended models provided a better fit of the data. Briefly, Bayes factors give the ratio of the likelihood of the data given one potential model to the likelihood of the data given a second, alternative, model. Conventionally, the degree of evidence for a particular model is considered moderate (3–10), strong (10–100), or decisive (> 100). Informative priors, based on estimated parameter values from prior applications of the Armitage‐Doll model and the scale of the observed data, were used in the models. These informative priors limited the scale of plausible parameters to moderately tight intervals and were used to allow the calculation of Bayes factor values and to improve nonlinear model convergence.

---

### Nonlinear optical components for all-optical probabilistic graphical model [^9f412763]. Nature Communications (2018). Medium credibility.

Multiplication

Inserting the saturable absorption equationin the differential equation for the nonlinear absorption, and solving leads toHere I sat is the saturation peak irradiance, α 0 is the weak field absorption, L is the thickness of SA material and I in and I out are the input and output peak irradiance, respectively. A numerical solution of Eq. (3) and its fit with an exponential function are plotted in Fig. 3a. Including the TPA term in the nonlinear absorption differential equationd I / d Z = − α 0 I − βI 2, leads to an explicit analytical solution, where β is the TPA coefficient and. A numerical solution of Eq. (4) is plotted in Fig. 3b as well as its fit with a natural logarithm function. The result of combinations for 29 identical logarithm inputs and an exponentiation gives the multiplication of the inputs as illustrated in Fig. 3c. The ideal multiplication result is plotted as a linear fit in Fig. 3c. Note that the peak irradiance in Eq. (3) and (4) can be replaced with energy per pulse (fluence or photon number as well) without any change in concept of their comparison with the exponential and logarithm functions. We use energy per pulse (E) for simulation as the experimental data were measured in terms of energy per pulse. In Fig. 3a and Fig. 3b we need to limit the range of fitting in order to get maximum overlap of the exponential and natural logarithm fit functions with SA and TPA solutions. Also the normalized-root-mean-square error (NRMSE) should be less than 1% and is defined asLimiting the ranges also comes from the natural behavior of the SA and TPA process where Eq. (3) and (4) start from zero for no input energy. However, we know that e 0 = 1 and ln(0) is undefined. Therefore, bounding the input intensity range for fitting is necessary for convergence and adequate fitting of the solutions of the TPA and SA equations with the target functions. The criteria are the maximum error acceptable to reproduce the function.

---

### [^bcb72458]. Developmental Medicine and Child Neurology (2019). Medium credibility.

C 青少年和成年人

本章节专门介绍针对青少年和成人DCD患者的术语、诊断标准、评估和干预措施。

我们有足够的证据表明，许多儿童在进入青春期和成年期后并没有摆脱他们的问题。 23, 291, 292, 293 然而，他们在日常生活、教育、工作和休闲中遇到的问题会随着他们的情况而改变。例如，处于各种就业环境中的个人和接受高等教育的人，面临的问题和挑战是不同的。

这种情况的影响也会因个人的因素和社会支持网络的性质和程度而不同。

---

### Emergence of collective oscillations in massive human crowds [^de803956]. Nature (2025). Excellent credibility.

Simulation parameters

Without loss of generality, we can set γ = 1 and α = 1, which amounts to defining our units of length and time. The model has then six control parameters, which are β, γ p, k, η, σ and σ p. To integrate the equations of motion, we set δ t = 0.001, so that the damping coefficients in equations (1) and (2) verify k / γ ≳ 20 δ t and 1/ γ p ≳ 100 δ t for the typical values of γ p and k that we consider. This value of δ t also ensures that the angular frequency Ω ⋆ always remains in our simulation window. The number of steps n is set so that the system explores exhaustively its phase space. Finally, the number of steps n st = 1.5 × 10 5 is set so that the system reaches the limit cycle when β > β c.

The n run = 100 simulations are initialized on one of the limit cycles in the absence of noise when β > β c, namely, with φ drawn uniformly in the range, and, wherewith equal probability for the sign of Ω ⋆. Otherwise, the simulations are initialized from u = 0 and p = 0.

For Fig. 3a–c, we used k = 0.027, γ = 1.00, γ p = 18.00, β / β c = 1.10, η = 0.45, σ = 0.00 and σ p = 2.00. For Fig. 3f, we used γ = 1.00, γ p = 18.00, η = 0.45, σ = 2.00 and σ p = 2.00.

---

### Adding flexibility to clinical trial designs: an example-based guide to the practical use of adaptive designs [^606c397c]. BMC Medicine (2020). Medium credibility.

Of the thirty-four patients recruited, 18 were randomised to idarubicin and cytarabine, randomisation to troxacitabine and idarubicin stopped after five patients, and randomisation to troxacitabine and cytarabine stopped after 11 patients. Success rates were 55% (10 of 18 patients) with idarubicin and cytarabine, 27% (three of 11 patients) with troxacitabine and cytarabine, and 0% (zero of five patients) with troxacitabine and idarubicin.

Advantages RAR can increase the overall proportion of patients enrolled in the trial who benefit from the treatment they receive while controlling the statistical operating characteristics. This mitigates potential ethical conflicts that can arise during a trial when equipoise is broken by accumulating evidence and makes the trial more appealing to patients possibly improving trial recruitment. The main motivation for RAR designs is to ensure that more trial participants receive the best treatments; it is possible to use such methods to optimise other characteristics of the trial. In a multi-armed context, RAR can shorten the development time and more efficiently identify responding patient populations.

Disadvantages RAR designs have been criticised for a number of reasons although many of the raised concerns can be addressed. Logistics of trial conduct is a noticeable obstacle in RAR due to the constantly changing randomisation; requiring more complex randomisation systems may in turn impact things such as drug supply and manufacture. When the main advantage pursued is patient benefit, this may compromise other characteristics; for example, a two-arm RAR trial will require larger sample sizes than a traditional fixed design with equal sample sizes in both arms; methods to account for such compromise have been proposed.

Choosing an approach from the variants of RAR can be challenging; in most cases, balancing the statistical operating characteristics and randomly assigning patients in an ethical manner is required. Most RAR methods require the availability of a reliable short-term outcome (although the exact form of the data may vary); however, this can result in bias, requiring the use of extra correction methods for estimation purposes. Another statistical concern is control of the type I and type II error rates. As discussed above, this is possible but requires intensive simulations or the use of specific theoretical results; this creates an additional burden at the design stage, requiring additional time and support.

---

### New creatinine-and cystatin C-based equations to estimate GFR without race [^7bffb662]. The New England Journal of Medicine (2021). Excellent credibility.

The clinical calculator "CKD-EPI Creatinine-Cystatin C Equation for eGFR (2021)" for diabetic nephropathy, chronic kidney disease, immunoglobulin A nephropathy, membranous nephropathy, focal segmental glomerulosclerosis, anemia of chronic kidney disease, anti-glomerular basement membrane disease, hepatorenal syndrome, immunoglobulin light chain amyloidosis, infection-related glomerulonephritis, kidney transplantation and membranoproliferative glomerulonephritis.

The 2021 CKD-EPI Creatinine-Cystatin C equation combines both serum creatinine and cystatin C measurements to estimate GFR. This combination provides improved accuracy over single-marker equations, especially in patients where creatinine-based estimates alone may be less reliable.

The 2021 CKD-EPI Creatinine-Cystatin C calculator is designed to estimate an individual's kidney function through the calculation of the estimated glomerular filtration rate (eGFR) and to assign a chronic kidney disease (CKD) stage based on key health parameters. By inputting a person's age, gender, serum creatinine, and cystatin C levels, the calculator determines their eGFR and corresponding CKD stage. Here is how it works:

- **Serum Creatinine**: This is initially adjusted to mg/dL if provided in µmol/L, using a straightforward conversion: creatinine (mg/dL) = creatinine (µmol/L) / 88.4.

- **Gender-Specific Parameters**: The calculator uses different parameters for males and females due to physiological differences:

- **For females**:
- A = 0.7
- B depends on whether creatinine is ≤ 0.7 (value -0.219) or > 0.7 (value -0.544)
- C = 0.8
- D depends on whether cystatin C is ≤ 0.8 (value -0.323) or > 0.8 (value -0.778)
- **For males**:
- A = 0.9
- B depends on whether creatinine is ≤ 0.9 (value -0.144) or > 0.9 (value -0.544)
- C = 0.8
- D depends on whether cystatin C is ≤ 0.8 (value -0.323) or > 0.8 (value -0.778)

- **Calculating eGFR**: With these values, the eGFR is calculated using the formula:
eGFR = 135 × (creatinine/A)^B × (cystatin C/C)^D × 0.996^age
If the individual is female, this result is further adjusted by multiplying the calculated eGFR by 0.963 to account for differences in muscle mass and body composition.

- **CKD Stage Interpretation**:
- eGFR ≥ 90 indicates Stage 1 (Normal or High renal function)
- eGFR 60–89 indicates Stage 2 (Mildly Decreased function)
- eGFR 45–59 indicates Stage 3a (Moderately Decreased function)
- eGFR 30–44 indicates Stage 3b (Moderately Decreased function)
- eGFR 15–29 indicates Stage 4 (Severely Decreased function)
- eGFR < 15 indicates Stage 5 (Kidney Failure)

By evaluating the inputs of age, gender, creatinine, and cystatin C levels, this calculator yields a rounded eGFR and a concise assessment of kidney health through the assigned CKD stage. This interpretation aids in diagnosing and managing potential kidney health issues, based on recognized medical guidelines.

---

### A genomic selection index applied to simulated and real data [^2c34a14a]. G3 (2015). Low credibility.

PSI vs. GSI when the generation interval is equal in both indices

Some of the results shown in Tables 2 and Table 3 andandoccurred when the PSI generation interval (L PSI) was greater than the GSI generation interval (L PSI). What would happen if L PSI = L GSI? In this case, if the number of markers is very small, then Equation (4) will give lower values than Equation (2) and PSI efficiency will be greater than GSI efficiency. However, if the number of markers is very large, the PSI and GSI responses will be very similar.

This argument also holds true for theinequality and the PSI and GSI expected genetic gain per selection cycle for observed and unobserved traits. For example, note that in, we have assumed thatand. Suppose now that. In this case, theinequality will not hold true because in all selection cycles. That is, theinequality will change its direction. Finally, it is evident that if, PSI will be more efficient than GSI even in the hypothetical case when the number of molecular marker is infinite. In conclusion, GSI will be more efficient than PSI in terms of unit of time only if; in this case, theinequality is true. In all other cases, PSI will be more efficient than GSI.

In this study, we applied the theory of GSI to simulated and real data and compared its efficiency with PSI efficiency by using three different criteria: the ratio of the GSI response over the PSI response, the PSI and GSI expected genetic gain per selection cycle for observed and unobserved traits, respectively, and the Technow inequality. In all three cases, for simulated and real data, GSI efficiency was higher than PSI efficiency per unit of time in all selection cycles. We thus concluded that GSI is an efficient choice when the purpose of a breeding program is to select individuals using GS.

---

### Statistical mechanics for metabolic networks during steady state growth [^8aeef127]. Nature Communications (2018). Medium credibility.

Competitive growth dynamics

The second possibility is that the Boltzmann distribution emerges from competitive growth dynamics. Since its historical origins in statistical physics, much research has been devoted to uncovering the dynamical roots of Boltzmann distributions, whose study highlighted important concepts and applications, ranging from ergodicity to fluctuation-response relations. The same questions naturally arise in the context of its application in metabolism. It has been shown that the maximum entropy distribution at a fixed average growth rate is recovered independently and justified dynamically as the steady state of logistic growth. Since the logistic growth is the standard model used to experimentally fit optical density curves, this link also provides a possible interpretation of the maximum entropy parameter β, as we discuss below.

Consider a population of initial size N 0 in a medium with carrying capacity N C and assume that the intrinsic growth rates of individuals, λ i, are sampled independently from a distribution q (λ), defined over the feasible polytope. In the simplest setting, upon neglecting growth state transitions, the number n i of cells with growth rate λ i will evolve in time according to

Then, with

Under a mean field approximation, the steady states of these dynamics are distributions with maximum entropy form at a fixed average growth rate, where the asymptotic optimization parameter, is given implicitly by the equation

Equation (13) can be viewed as a relationship between quantities that can be independently estimated for a specific experimental setup: the inoculum size (N 0) and carrying capacity (N C) on the one hand, as well as the typical value of β, via Eq. (8) or direct fitting of measured metabolic fluxes, on the other.

Taken together, the two mechanisms, active regulation and competitive growth dynamics, need not be exclusive, and can operate concurrently. A simple diagnostic that could provide insight into the relative importance of both mechanisms is to examine whether the relationship of Eq. (13) is satisfied. If it were, it would suggest that the Boltzmann distribution is dynamical in origin. If, on the other hand, the values of β inferred from fitting the maximum entropy model were higher than those derived from the N C / N 0 ratio and Eq. (13), additional active regulation may be at work. In the Results section and Supplementary Methods, we provide estimates of these quantities for the experiments under consideration.

---

### Evolutionary dynamics of any multiplayer game on regular graphs [^916cb3ba]. Nature Communications (2024). High credibility.

General replicator equations

The evolution of frequencies x 1, x 2, …, x n can be deduced through the microscopic strategy update process. Specifically, in an infinite population, i.e. N → ∞, a single unit of time comprises N elementary steps, ensuring that each individual has an opportunity to update their strategy. During each elementary step, the frequency of i -players increases by 1/ N when a focal j -player (where j ≠ i) is chosen to update its strategy and is replaced by an i -player. Similarly, the frequency of i -players decreases by 1/ N when a focal i -player is selected to update its strategy and the player who takes the position is not an i -player. Based on this perception, we derive a simple form of the replicator equations for i = 1, 2, …, n in the weak selection limit (Supplementary Note 2) :We find that Eq. (4) offers an intuitive understanding, if we introduce the following two concepts: (1), the expected accumulated payoff of the i -player (zero steps away on the graph), and (2), the expected accumulated payoff of the i -player's neighbors (one step away on the graph). These concepts suggest that. Under pairwise comparison, the reproduction rate of i -players is dependent on how their accumulated payoff exceeds that of their neighbors. In essence, the evolution of x i is the competition between an individual and its first-order neighbors, which aligns with the results obtained by a different theoretical framework in two-strategy systems. We further extend it to n -strategy systems in the framework of pair approximation. We also verify that the death-birth rule is essentially the competition between an individual and its second-order neighbors for n -strategy systems (Supplementary Information).

---

### Renal mass and localized renal cancer: evaluation, management, and follow-up: AUA guideline: part I [^ecbf6aeb]. The Journal of Urology (2021). High credibility.

Renal mass and localized renal cancer — AUA recommendation nomenclature links statement types Strong Recommendation, Moderate Recommendation, and Conditional Recommendation to Evidence Strength A, Evidence Strength B, and Evidence Strength C. For Strong Recommendation, benefits exceed risks and "Net benefit (or net harm) is substantial", with applicability to "most patients in most circumstances"; confidence modifiers are "future research is unlikely to change confidence" for A, "better evidence could change confidence" for B, and for C the net benefit "appears substantial", "better evidence is likely to change confidence", and this category is "rarely used to support a Strong Recommendation". For Moderate Recommendation, "Net benefit (or net harm) is moderate" for A and B with applicability to "most patients in most circumstances", and confidence shifts from "future research is unlikely to change confidence" (A) to "better evidence could change confidence" (B); with C the net benefit "appears moderate" and "better evidence is likely to change confidence". For Conditional Recommendation, benefit–risk is equivocal or unclear: A and B specify "Benefits = Risks/Burdens" and that the "Best action depends on individual patient circumstances" or "Best action appears to depend on individual patient circumstances", with confidence moving from "Future research unlikely to change confidence" (A) to "Better evidence could change confidence" (B), while C indicates "Balance between Benefits & Risks/Burdens unclear", "Alternative strategies may be equally reasonable", and "Better evidence likely to change confidence".

---

### Methods for pragmatic randomized clinical trials of pain therapies: IMMPACT statement [^f1288bd8]. Pain (2024). Medium credibility.

Abstract

Pragmatic, randomized, controlled trials hold the potential to directly inform clinical decision making and health policy regarding the treatment of people experiencing pain. Pragmatic trials are designed to replicate or are embedded within routine clinical care and are increasingly valued to bridge the gap between trial research and clinical practice, especially in multidimensional conditions, such as pain and in nonpharmacological intervention research. To maximize the potential of pragmatic trials in pain research, the careful consideration of each methodological decision is required. Trials aligned with routine practice pose several challenges, such as determining and enrolling appropriate study participants, deciding on the appropriate level of flexibility in treatment delivery, integrating information on concomitant treatments and adherence, and choosing comparator conditions and outcome measures. Ensuring data quality in real-world clinical settings is another challenging goal. Furthermore, current trials in the field would benefit from analysis methods that allow for a differentiated understanding of effects across patient subgroups and improved reporting of methods and context, which is required to assess the generalizability of findings. At the same time, a range of novel methodological approaches provide opportunities for enhanced efficiency and relevance of pragmatic trials to stakeholders and clinical decision making. In this study, best-practice considerations for these and other concerns in pragmatic trials of pain treatments are offered and a number of promising solutions discussed. The basis of these recommendations was an Initiative on Methods, Measurement, and Pain Assessment in Clinical Trials (IMMPACT) meeting organized by the Analgesic, Anesthetic, and Addiction Clinical Trial Translations, Innovations, Opportunities, and Networks.

---

### Transient power-law behaviour following induction distinguishes between competing models of stochastic gene expression [^63bddcbb]. Nature Communications (2025). High credibility.

Fig. 4
Synthetic data generated using biologically relevant rate parameter values for mammalian cells validates the practical use of the short-time mean mRNA count data to estimate a lower bound for the number of gene states N.

Measurements of the mean mRNA count at 5 regularly spaced times were simulated for approximately two million parameter sets through direct numerical integration of the moment equations of the chemical master equation (CME) of the N -state model. These time points varied for each parameter set and were chosen such that at the final time point the mean mRNA count per cell was in the biologically realistic range (a few tens). The final time point was also much less than the median half-life of mRNA (several hours). A An example curve for one rate parameter set showing deviations of the true power law (dashed blue) from the theoretical power law (solid black) for biologically relevant times. Note that time is non-dimensional since it is multiplied by the mRNA degradation rate: hence t = 0.1 corresponds to about one hour for mammalian cells. B – K For each parameter set, linear regression was used to estimate the power-law exponent from the slope of the log-log plot of the mean versus time. Distributions of the exponent are shown by blue histograms. The model diagram in each figure illustrates the active state (solid green circle), inactive states (open circles) and initial inactive state (solid red square) for each system. Note that N (the total number of gene states) varies between 2 and 5 while the initial inactive state j varies between 1 and N − 1. The exponent N − j + 1 estimated by our theory (Eq. (3)) is shown by a vertical red dashed line — in all cases the maximum value of the numerical exponent is below the theoretical value which itself is a lower bound for the number of gene states N. See Supplementary Note 2 for simulation details.

---

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^4165d96c]. American Journal of Kidney Diseases (2006). Medium credibility.

Appendix — methods for adding residual clearance to hemodialyzer clearance notes that dialyzer clearances (spKt/V) required to achieve a stdKt/V of 2.0 volumes per week are tabulated across treatment times from 2 to 8 hours and schedules from 2 to 7 treatments per week, with values determined using a formal 2-compartment mathematical model of urea kinetics and similar results obtainable using a simplified equation; the approach gives results similar to the third data column of Table 18.

---

### Metastable orientational order of colloidal discoids [^57cb27ec]. Nature Communications (2015). Medium credibility.

Figure 2a illustrates the effect of c / c * and φ on the degree of orientational ordering of colloidal discoids (f ordered) in both experiments and simulations. All experimental data are reported at a waiting time of t w = 120 min, and simulations are reported for number of sweeps, t MC, between 40 and 500 million. The coexistence boundary (solid line in Fig. 2a) is defined when f clust > 0.2 (a phase diagram with f clust is in Supplementary Fig. 3). Samples to the left of the solid line at low φ exist as suspensions of mobile free particles or small clusters (Fig. 2b, c), in which little to no ordering can be observed. Samples to the right or on the coexistence boundary are large aggregates. At the highest c / c * and φ (c / c* ⩾2.7, φ ⩾0.02) within the phase diagram, we find strands with high orientational order in both experiments and simulations (0.2 ≤ f ordered ≤ 0.6), shown in Fig. 2d–g. At intermediate c / c * (1.0 ≤ c / c * ≤ 2.7), the self-assembled structures become increasingly disordered (Fig. 2h–n). This type of heterogeneous structure interspersed with large voids and regions of dense packing is reminiscent of gels formed with weak, short-range attractions. The degree of orientational order continues to decrease gradually as c / c * decreases, reaching f ordered < 0.1 at the phase boundary. Structures at the phase boundary contain large condensates/clusters that coexist with mobile particles (Fig. 2l–n).

---

### Why are we regressing? [^fd8ada5d]. The Journal of Foot and Ankle Surgery (2012). Low credibility.

In this first of a series of statistical methodology commentaries for the clinician, we discuss the use of multivariate linear regression.

---

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^ba34bdc0]. American Journal of Kidney Diseases (2006). Medium credibility.

On-line clearance (CPG 2.4) — For ongoing assessment of hemodialysis (HD) adequacy, 'the dose can be assessed more frequently by measuring conductivity (or ionic) clearance across the dialyzer membrane', a method that 'does not require consumables or blood sampling and can be used with each dialysis treatment to predict the delivered Kt/V in real time before the treatment is finished'; when applied as described, 'conductivity clearance can be used safely as a substitute for the blood-side urea method for measuring dialysis dose'. Required practices include that 'multiple ionic clearance measurements must be performed throughout the treatment' and 'to calculate K/V, time on dialysis and V must be determined accurately'. The guideline cautions that 'anthropometric formulas for V frequently were shown to overestimate V in HD patients on average by approximately 15%', and outlines an alternative in which 'modeled V, measured monthly from urea kinetic modeling' is used while 'conductivity clearance is measured during the modeled dialysis', thereby 'eliminating the necessity to record blood flow, dialysate flow, and dialyzer urea mass transfer-area coefficient (KoA) to calculate K and V'.

---

### Physics-informed learning of governing equations from scarce data [^d1208a9b]. Nature Communications (2021). High credibility.

Harnessing data to discover the underlying governing laws or equations that describe the behavior of complex physical systems can significantly advance our modeling, simulation and understanding of such systems in various science and engineering disciplines. This work introduces a novel approach called physics-informed neural network with sparse regression to discover governing partial differential equations from scarce and noisy data for nonlinear spatiotemporal systems. In particular, this discovery approach seamlessly integrates the strengths of deep neural networks for rich representation learning, physics embedding, automatic differentiation and sparse regression to approximate the solution of system variables, compute essential derivatives, as well as identify the key derivative terms and parameters that form the structure and explicit expression of the equations. The efficacy and robustness of this method are demonstrated, both numerically and experimentally, on discovering a variety of partial differential equation systems with different levels of data scarcity and noise accounting for different initial/boundary conditions. The resulting computational framework shows the potential for closed-form model discovery in practical applications where large and accurate datasets are intractable to capture.

---

### Learning the intrinsic dynamics of spatio-temporal processes through latent dynamics networks [^84b98618]. Nature Communications (2024). High credibility.

Introduction

Mathematical models based on differential equations, such as Partial Differential Equations (PDEs) and Stochastic Differential Equations (SDEs), can yield quantitative predictions of the evolution of space-dependent quantities of interest in response to external stimuli. Pivotal examples are given by fluid dynamics and turbulence, wave propagation phenomena, the deformation of solid bodies and biological tissues, molecular dynamics, price evolution of financial assets, epidemiology. However, the development of traditional modeling-and-simulation approaches carry several mathematical and computational challenges. Model development requires a deep understanding of the physical processes, the adoption of physics first principles or empirical rules, and their translation into mathematical equations. The values of parameters and of boundary and initial conditions required to close the model are often unknown, increasing the intrinsic dimensionality of the solution space. Finally, the computational cost that accompanies the (possibly many-query) numerical approximation of such mathematical models may be prohibitive and hinder their use in relevant applications.

In recent years, we are witnessing the introduction of a new paradigm, namely data-driven modeling –, as opposed to traditional physics-based modeling, enabled by recent advances in optimization, high-performance computing, GPU-based hardware, artificial neural networks (NNs) and Machine/Deep Learning in general. Data-driven modeling methods hold promise in overcoming the limitations of traditional physics-based models, either as a replacement for them or in synergy with them. On the one hand, data-driven techniques are employed to learn a model directly from experimental data. On the other hand, instead, they are used to build a surrogate for a high-fidelity model – the latter being typically based on the numerical approximation of systems of differential equations – from a dataset of precomputed high-fidelity simulation snapshots. This paradigm is successful in many-query contexts, that is when the computational resources spent in the offline phase (generation of the training data and construction of the data-driven surrogate model) are repaid by a large number of evaluations of the trained model (online phase), as is the case of sensitivity analysis, parameter estimation and uncertainty quantification. Another case of interest is when real-time responses are needed, like, e.g. in clinical scenarios.

---

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^b177e6e8]. American Journal of Kidney Diseases (2006). Medium credibility.

Recommendations for hemodialysis adequacy — quality assurance emphasizes that "The Work Group continues to recommend comparisons of prescribed with delivered dose as a quality assurance aid", and that "Guideline 4 provides a minimum Kt/V threshold below which action should be taken to prevent underdialysis". When the values are "significantly different (> 15% difference), troubleshooting should be done" to identify causes such as "AR, an error in dialysis timing, inadequate blood pump occlusion or calibration, faulty dialysate pump, error in blood sampling, or inadequate performance of the dialyzer (eg, because of clotting during dialysis or excessive reuse)". Operational details include that "V is determined preferably by averaging previous values of modeled V, but also can be determined by using an anthropometric formula, eg, Watson" and that "Comparison of modeled V with a previously determined patient-specific value for V is equivalent to comparing delivered with prescribed clearance".

---

### Advanced statistics: linear regression, part I: simple linear regression [^18239641]. Academic Emergency Medicine (2004). Low credibility.

Simple linear regression is a mathematical technique used to model the relationship between a single independent predictor variable and a single dependent outcome variable. In this, the first of a two-part series exploring concepts in linear regression analysis, the four fundamental assumptions and the mechanics of simple linear regression are reviewed. The most common technique used to derive the regression line, the method of least squares, is described. The reader will be acquainted with other important concepts in simple linear regression, including: variable transformations, dummy variables, relationship to inference testing, and leverage. Simplified clinical examples with small datasets and graphic models are used to illustrate the points. This will provide a foundation for the second article in this series: a discussion of multiple linear regression, in which there are multiple predictor variables.

---

### Correction [^26cbbc41]. Journal of Radiology Case Reports (2017). Low credibility.

[This corrects the article on p. 8 in vol. 10.].

---

### Dynamical patterns and nonreciprocal effective interactions in an active-passive mixture through exact hydrodynamic analysis [^6dd1c504]. Nature Communications (2025). High credibility.

The formation of dynamical patterns is one of the most striking features of nonequilibrium physical systems. Recent work has shown that such patterns arise generically from forces that violate Newton's third law, known as nonreciprocal interactions. These nonequilibrium phenomena are challenging for modern theories. Here, we introduce a model mixture of active (self-propelled) and passive (diffusive) particles amenable to exact mathematical analysis. We exploit state-of-the-art methods to derive exact hydrodynamic equations for the particle densities, which reveal effective nonreciprocal couplings between the active and passive species. We study the resulting collective behavior, including the linear stability of homogeneous states and phase coexistence in large systems. This reveals a novel phase diagram with the spinodal associated with active phase separation protruding through the associated binodal, heralding the emergence of dynamical steady states. We analyze these states in the thermodynamic limit of large system size, showing, for example, that sharp interfaces may travel at finite velocities, but traveling phase-separated states are forbidden. The model's mathematical tractability enables precise new conclusions beyond those available by numerical simulation of particle models or field theories.

---

### Eye on the B-ALL: B-cell receptor repertoires reveal persistence of numerous B-lymphoblastic leukemia subclones from diagnosis to relapse [^a7a6e329]. Leukemia (2016). Low credibility.

Using our bespoke computational pipeline (MRDARCY — see Materials and methods), we identified B-ALL clonotypic BCRs in the diagnostic sample and searched diluted or serial patient samples for identical or related BCRs, allowing for a set number of base-pair (bp) mismatches (≤ 8 bp in this study). MRDARCY was used to identify clonotypic sequences (clusters representing ⩾2.5% of the entire repertoire, above the 95th percentile of the healthy range) in the primary samples from six patients treated on the UKALL2003 protocol and to determine the percentage of matching BCR sequences in longitudinal RNA samples, also studied for MRD using fusion gene qPCR. BCR sequencing concurred closely with qPCR transcript levels (red/green versus blue lines, Figure 1a), demonstrating strong correlations between the percentage of clonotypic B-ALL BCRs and qPCR T/C ratios (R 2 values > 0.87, Supplementary Table S3), while B-ALL clonotypic BCR sequences were detected in all qPCR-positive samples. Detection of very low-level B-ALL sequences in some qPCR-negative samples was reproducible in replicates of the same sample, with the exception of one sample (Supplementary Table S4, patient 1611, day 19 sample), from which clonotypic sequences were detected at low frequency (0.0656%) in only one of two repeats in keeping with stochastic 'loss' of a rare variant. Likewise, the percentage of matching BCR sequences in DNA samples (UKALL XI) correlated with the % blasts in PB (Figure 1b). Notably, we detected clonotypic sequences in 6 of 10 patients at day 28, where blasts were not detected (< 1%). Therefore, our results show that BCR sequencing is highly sensitive as a method for MRD detection and at least as good as fusion gene qPCR.

---

### Utilization of real-world data in assessing treatment effectiveness for diffuse large B-cell lymphoma [^764a3a43]. American Journal of Hematology (2023). Medium credibility.

A case in point for RWE supporting drug approval is the 2014 FDA accelerated approval of blinatumomab for the treatment of adults with R/R B‐precursor acute lymphoblastic leukemia. Approval was based on data from a single‐arm, phase 2 study which demonstrated a CR rate, or CR with partial hematologic recovery, of 43%. To support the application, an RWD historical control group of 694 patients was used to demonstrate a comparative CR rate of 24%. In a subsequent phase 3 RCT, a significant median OS benefit with blinatumomab (7.7 months) was demonstrated over the standard of care (4.0 months).

RWE can augment clinical trial data in regulatory submissions as a less constrained pool of patients, who more accurately reflect real‐world circumstances during treatment and can be compared with patients assessed by more stringent eligibility criteria in a clinical trial. Trial designs using RWD represent a logical, much‐needed step in the development of the regulatory approval system, and further acceptance of RWE by regulatory bodies has the potential to make drug development more efficient and generate a profound change in the drug approval process. This has been encouraged by the 21st Century Care Act, which seeks to minimize the limitations of clinical and observational studies, in order to support more efficient drug development through the advancement and refinement of innovative hybrid study designs incorporating RWD. Examples of novel study designs, such as prospective non‐interventional studies and hybrid study designs, being considered by the FDA for regulatory decision‐making include the following: RCTs using RWD to capture clinical and safety information, including pragmatic and large simple trials; single‐arm trials supplemented with data from external comparators derived from RWD, such as discussed here; RWD that fulfill post‐marketing requirements for further evaluation of safety or effectiveness to support a regulatory decision. Such study design can overcome the challenges of RCTs, which can be time‐consuming, costly, require large numbers of patients, and produce evidence with limited generalizability in clinical practice. However, while traditional statistical techniques may reduce the impact of limitations inherent to RWD, novel methodologies and applications are needed to ensure that RWD being generated is fit for regulatory purposes. Incorporation of RWD as part of regulatory submissions will require the introduction of procedures or oversight, such that an excess of submissions for ineffective or harmful drugs does not impede the review of submissions for treatments that may benefit patients. In addition, to ensure rigorous design and analysis, standardized parameters are needed in order to make studies fully reproducible (e.g. via standardized database structuring) and facilitate consistent reporting of study designs and results. To this end, an extension of the CONSORT checklist, CONSORT‐ROUTINE, was published in 2021 with the aim of establishing guidance to improve the reporting quality of studies using cohorts, registries, EHRs, and administrative databases by setting standards. It describes a minimum set of information that should be reported and provides a checklist to facilitate compliance. Consistent use of this reporting structure in the future should improve transparency and build confidence in the use of RWD and RWE.

---

### Appropriate systemic therapy dosing for obese adult patients with cancer: ASCO guideline update [^2ddf0964]. Journal of Clinical Oncology (2021). High credibility.

Regarding specific circumstances for obesity, more specifically with respect to patients with cancer, ASCO 2021 guidelines recommend to calculate body surface area using any of the standard formulae. Insufficient evidence to support one formula for calculating body surface area over another.

---

### The cochrane collaboration and the cochrane library [^8b61c032]. Otolaryngology — Head and Neck Surgery (2007). Low credibility.

The Cochrane Collaboration (www.cochrane.org) is an international, independent, not-for-profit organization dedicated to making up-to-date, accurate information about the effects of health care readily available worldwide. I review the origins, development, and structure of the organization and describe the important components of its output, The Cochrane Library.

---

### Pediatric and adult brain death / death by neurologic criteria consensus guideline [^85c54388]. Neurology (2023). High credibility.

Regarding diagnostic investigations for brain death, more specifically with respect to neurological evaluation (numbers of examinations), AAN/AAP/CNS/SCCM 2023 guidelines recommend to review the medical record to determine that the patient has sustained a catastrophic, permanent brain injury with a mechanism of brain injury known to lead to BD/death by neurologic criteria and that confounders to the examination have been excluded before performing a BD/death by neurologic criteria examination.

---

### A rational approach to the clinical use of cumulative effective dose estimates [^0571c08e]. AJR: American Journal of Roentgenology (2011). Low credibility.

Objective

This article will address ongoing efforts to track cumulative dose estimates of patient radiation exposure that have the potential to transform radiation protection research.

Conclusion

There is no clear consensus on how to use this information clinically. A rational consideration of this question reveals that until the linear no-threshold model is supplanted, cumulative dose estimates are of little clinical relevance and never constitute a logical reason to avoid an imaging evaluation that is otherwise medically indicated.

---

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^25430baa]. American Journal of Kidney Diseases (2006). Medium credibility.

Research recommendations of interest — Less critical issues include the development of prediction instruments to allow estimation of time to symptomatic kidney failure on the basis of serial GFR estimates, and less critical questions include measurement of patient preferences about the tradeoffs between the burdens and benefits of earlier therapy; investigation of dialysis creatinine kinetics would help assess the effect of muscle mass on outcome and compare somatic with visceral body mass as risk factors for survival, and studies of large patient populations to correlate urine output with RKF would help determine whether urine volume-related cutoff values for ignoring RKF are useful; although potential insults listed in CPR Table 16 are known to injure normal and partially damaged native kidneys, little is known about the threshold concentrations and duration of exposure required to incite each insult in patients with CKD stage 5, it is unlikely that controlled clinical trials will appear in the near future; therefore, observational studies are encouraged; the benefits of RKF may relate more to renal mass than urine volume, this possibility should be considered in outcome studies, and correlating kidney size with RKF may help determine whether RKF is predictable based on size.

---

### Brief resolved unexplained events (formerly apparent life-threatening events) and evaluation of lower-risk infants [^7e69974f]. Pediatrics (2016). Medium credibility.

Guideline definitions for key action statements (Table 4) — a strong recommendation is defined as "A particular action is favored because anticipated benefits clearly exceed harms (or vice versa) and quality of evidence is excellent or unobtainable", with the implication "Clinicians should follow a strong recommendation unless a clear and compelling rationale for an alternative approach is present". A moderate recommendation is defined as "A particular action is favored because anticipated benefits clearly exceed harms (or vice versa) and the quality of evidence is good but not excellent (or is unobtainable)", implying "Clinicians would be prudent to follow a moderate recommendation but should remain alert to new information and sensitive to patient preferences". A weak recommendation (based on low-quality evidence) is defined as "A particular action is favored because anticipated benefits clearly exceed harms (or vice versa), but the quality of evidence is weak", with the implication "Clinicians would be prudent to follow a weak recommendation but should remain alert to new information and very sensitive to patient preferences". A weak recommendation (based on balance of benefits and harms) is defined as "Weak recommendation is provided when the aggregate database shows evidence of both benefit and harm that appear to be similar in magnitude for any available courses of action", implying "Clinicians should consider the options in their decision-making, but patient preference may have a substantial role".

---

### MD-ALL: an integrative platform for molecular diagnosis of B-acute lymphoblastic leukemia [^7fd860d3]. Haematologica (2024). Medium credibility.

With the technical, biological, and clinical considerations applied in the MD-ALL platform, we developed a decision-tree-based pipeline to integrate multiple aspects of information acquired from RNA-seq to accurately determine B-ALL subtypes and the associated confidence score (Figure 4). Basically, a step-by-step process is taken for each sample to determine the subtype based on the GEP and signature genetic lesions, and then assigns the confidence score.

Figure 3.
Integrative summary of B-acute lymphoblastic leukemia classification by MD-ALL. (A) Gene expression profile (GEP) -based subtype prediction by support vector machine (SVM) and PhenoGraph models. Different numbers of feature genes are used in the prediction models to evaluate classification robustness. The test sample was consistently predicted as the Ph subtype. (B) The test sample is mapped to a predefined uniform manifold approximation and projection (UMAP) space for visualizing GEP-based classification. The UMAP uses 1,058 features genes. The test sample clusters with the Ph/Ph-like group, which agrees with the SVM and PhenoGraph prediction. (C) Expression of a specific gene across different B-acute lymphoblastic leukemia (B-ALL) subtypes. Ph-like (CRLF2) is shown as a separate group here for confirming CRLF2 rearrangements. Users can specify a gene to examine its expression for validating genetic lesions (e.g. overexpression of CRLF2 in CRLF2 -rearranged cases) or potential subtypes. (D) Summary of Molecular Diagnosis of Acute Lymphoblastic Leukemia (MD-ALL) to assist B-ALL classification. The genetic lesions, which include fusions, mutations, large-scale copy number variation (CNV), are integrated with GEP-based prediction by PhenoGraph and SVM to assist the classification of the test sample's B-ALL subtype.

---

### Correction [^1ef813eb]. JAAD Case Reports (2021). Medium credibility.

[This corrects the article DOI: 10.1016/j.jdcr.2020.10.038.].

---

### Lower glycolysis carries a higher flux than any biochemically possible alternative [^f729e83d]. Nature Communications (2015). Medium credibility.

To overcome this issue and to investigate in more detail, we analysed which of our candidate pathways achieved the highest flux at different points in the parameter space. This allows us to understand how the performance of a given pathway depends on the intracellular environment. Our results show that different candidate pathways perform best in different regions of the parameter space. In particular, pathway performance is very sensitive to the cellular energy state, as measured by the ratio of the ATP and ADP concentrations ([ATP]/[ADP]), and redox state, represented by the ratio [NAD] [Pi]/[NADH] (Fig. 2c, e). Focusing on the glycolytic pathways (Fig. 2b) we see that, remarkably, the natural trunk pathway (black dots) outperforms all the alternatives in the region of parameter space close to that found in living cells (red box). This suggests that the glycolytic trunk pathway represents a maximal flux solution for the conversion of G3P to pyruvate, under typical intracellular conditions. Similarly, for the gluconeogenic pathways (Fig. 2d), the pps pathway found in nature (red dots) outperforms the alternatives under typical physiological conditions (red box).

Alternative flux calculations

The flux calculation used above makes the simplifying assumption that all enzymes are perfect catalystsand thus it neglects the important physical feature of substrate saturation. Any attempt at applying a generic choice of kinetic equations to a system of reactions is problematic, and more complicated forms give rise to a greater number of parameters that must be specified. However, one can test the effect that using alternative forms has on the overall outcome, and we find that the above results still hold when two forms of reversible Michaelis–Menten kinetics are assumed (Supplementary Figs 3, 4 and Methods), capturing the behaviour of substrate and product inhibition. Thus, our results are not dependent on the detailed assumptions made in modelling the enzyme kinetics.

---

### A simple geometrical description of the trueFISP ideal transient and steady-state signal [^7102e878]. Magnetic Resonance in Medicine (2006). Low credibility.

An intuitive approach is presented for assessment of the TrueFISP signal behavior in the transient phase and the steady state, based on geometrical considerations in combination with the Bloch equations. Short formulations are derived for the zenith and phase angle determining the direction of the magnetization vector for which a smooth monoexponential decay is obtained even at considerable off-resonance frequencies, thus compactly defining the target of various preparation schemes proposed in literature. A pictorial explanation is provided to illustrate how the interplay between RF excitation and relaxation governs the TrueFISP transient phase and steady state. Closed form expressions are developed that describe the signal evolution, accounting for the influence of T(1), T(2), flip angle, and resonance frequency offset in agreement with recently published studies. These results are obtained directly from basic assumptions, without the need for abstract mathematical treatment or further approximations. The validity of the conceptual framework and the analytical description is verified by simulations based on the Bloch equations as well as with MR phantom experiments. The theory may be used for contrast calculations and has the potential to facilitate improved parameter quantification with magnetization prepared TrueFISP experiments accounting for off-resonance effects.

---

### Correction [^1ca77ef3]. JAAD Case Reports (2021). Medium credibility.

[This corrects the article DOI: 10.1016/j.jdcr.2020.12.023.].

---

### Hepatitis B immune globulin (human) (nabi-HB) [^20aca79a]. FDA (2025). Medium credibility.

HOW SUPPLIED

Nabi-HB, Hepatitis B Immune Globulin (Human), is supplied as:

---

### An analytical theory of balanced cellular growth [^5311f9a5]. Nature Communications (2020). High credibility.

The cellular density ρ changes when external osmolarity is modified.is the slope of the log-log-scale plot of μ vs. ρ across different external osmolarities. While increases in ρ may have strong effects on diffusion and thus on enzyme kinetics, reductions in ρ due to decreased external osmolarity are within the scope of our model. The very limited available experimental data (three data points from ref. Supplementary Fig. 3) suggest ρ η ρ ≈ 0.66, the same as our rough estimate from the minimal cell model. An otherwise identical model that limits total protein density. P instead of dry mass density predicts a much weaker dependency of growth rate on osmolarity, with P η P = 0.36 ("Methods").

---

### Primer on logistic regression for emergency care researchers [^12c58eb1]. The Journal of Emergency Medicine (2022). Medium credibility.

Background

Logistic regression plays a fundamental role in the production of decision rules, risk assessment, and in establishing cause and effect relationships. This primer is aimed at novice researchers with minimal statistical expertise.

Objective

Introduce the logit equation and provide a hands-on example to facilitate understanding of its benefits and limitations.

Discussion

This primer reviews the mathematical basis of a logit equation by comparing and contrasting it with the simple straight-line (linear) equation. After gaining an understanding of the meaning of beta coefficients, readers are encouraged to download a free statistical program and database to produce a logistic regression analysis. Using this example, the narrative then discusses commonly used methods to describe model fitness, including the C-statistic, chi square, Akaike and Bayesian Information Criteria, McFadden's pseudo R 2, and the Hosmer-Lemeshow test. The authors provide a how-to discussion for variable selection and estimate of sample size. However, logistic regression alone can seldom establish causal inference without further steps to explore the often complex relationship amongst variables and outcomes, such as with the use of a directed acyclic graphs. We present key elements that generally should be considered when appraising an article that uses logistic regression. This primer provides a basic understanding of the theory, hands-on construction, model analysis, and limitations of logistic regression in emergency care research.

Conclusions

Logistic regression can provide information about the association of independent variables with important clinical outcomes, which can be the first step to show predictiveness or causation of variables on the outcomes of interest. © 2022 Elsevier Inc.

---

### Correction [^b6b88c7c]. JAAD Case Reports (2021). Medium credibility.

[This corrects the article DOI: 10.1016/j.jdcr.2020.12.019.].

---

### Correction [^751b1e37]. JAAD Case Reports (2020). Medium credibility.

[This corrects the article DOI: 10.1016/j.jdcr.2020.06.022.].

---

### Correction [^deb8704b]. JAAD Case Reports (2020). Medium credibility.

[This corrects the article DOI: 10.1016/j.jdcr.2020.02.038.].

---

### Multisociety statement on coronavirus disease 2019 (COVID-19) vaccination as a condition of employment for healthcare personnel [^8f9de826]. Infection Control and Hospital Epidemiology (2022). High credibility.

Real-world effectiveness — A growing number of real-world studies show similar overall vaccine effectiveness for preventing symptomatic and asymptomatic COVID-19, although vaccine effectiveness is likely lower for immunocompromised persons. Several studies assessing vaccine breakthrough infections in the general population indicate that COVID-19 vaccines reduce the risk of severe disease, hospitalization, and death, and vaccinated persons have decreased risk of SARS-CoV-2 infection and reduced risk for secondary infection and/or likelihood for transmission to contacts, including household contacts.

---

### A simplified formula for T1 contrast optimization for short-TR steady-state incoherent (spoiled) gradient echo sequences [^c91a4f57]. Magnetic Resonance Imaging (2007). Low credibility.

There are certain instances in practical magnetic resonance imaging where T1 changes by a small amount relative to a neighboring pixel or between scans for a given pixel. The source of these small changes in T1 can be caused either by changes in tissue water content or by the uptake of a contrast agent. For short repetition time (TR) spoiled gradient echo imaging, we show that a robust and a simple, easy to use back-of-the-envelope expression for the flip angle that optimizes contrast under these conditions is given by radical 3theta E in radians or (180/pi) radical6TR/T1 in degrees. We show that for a TR/T1 ratio of up to 0.3 and for a T1 change of up to ± 50%, this approximation to the optimal flip angle produces a contrast to within 6% of the theoretical maximum value and that these predictions are in good agreement with experiment.

---

### Forecasting the cumulative number of COVID-19 deaths in China: a boltzmann function-based modeling study [^d4729286]. Infection Control and Hospital Epidemiology (2020). Medium credibility.

The COVID-19 outbreak is ongoing in China. Here, Boltzmann function-based analyses reveal the potential total numbers of COVID-19 deaths: 3,260 (95% confidence interval [CI] 3187–3394) in China; 110 (95% CI, 109–112) in Hubei Province; 3,174 (95% CI, 3095–3270) outside Hubei; 2,550 (95% CI, 2494–2621) in Wuhan City; and 617 (95% CI, 607–632) outside Wuhan.

---

### Prevalence of rare diseases: bibliographic data [^b230863d]. Orphanet Report Series (2024). Medium credibility.

Worldwide, the prevalence of c is estimated at 0.00029 per 100,000 population.

---

### KDIGO clinical practice guideline for lipid management in CKD: summary of recommendation statements and clinical approach to the patient [^2a1d5c4b]. Kidney International (2014). Medium credibility.

Estimated GFR from cystatin C (eGFRcys and eGFRcr-cys) — guidance for use and reporting. If cystatin C is measured, we suggest that health professionals (Recommendation 1.4.3.6, Grade 2C) use a GFR estimating equation to derive GFR from serum cystatin C rather than relying on the serum cystatin C concentration alone and understand clinical settings in which eGFRcys and eGFRcr-cys are less accurate. We recommend that clinical laboratories that measure cystatin C should (Recommendation 1.4.3.7, Grade 1B) measure serum cystatin C using an assay with calibration traceable to the international standard reference material, report eGFR from serum cystatin C in addition to the serum cystatin C concentration in adults and specify the equation used whenever reporting eGFRcys and eGFRcr-cys, and report eGFRcys and eGFRcr-cys in adults using the 2012 CKD-EPI cystatin C and 2012 CKD-EPI creatinine-cystatin C equations, respectively, or alternative cystatin C–based GFR estimation equations if they have been shown to improve accuracy of GFR estimates compared to the 2012 CKD-EPI cystatin C and 2012 CKD-EPI creatinine-cystatin C equations. When reporting serum cystatin C, we recommend reporting serum cystatin C concentration rounded to the nearest 100th of a whole number when expressed as conventional units (mg/L).

---

### The FENa test. use in the differential diagnosis of acute renal failure [^e441b083]. JAMA (1976). Excellent credibility.

The clinical calculator "Fractional Excretion of Sodium (FENa)" for chronic kidney disease.

The Fractional Excretion of Sodium (FENa) is a diagnostic tool used to differentiate between prerenal and intrinsic causes of acute kidney injury (AKI). It quantifies the percentage of sodium excreted compared to sodium filtered by the kidneys, based on serum and urine sodium and creatinine levels.

The Fractional Excretion of Sodium (FENa) calculator is a tool designed to assist in diagnosing the cause of acute kidney injury by interpreting certain biochemical inputs. Here's how it works:

Inputs required:

- Serum Creatinine (SCr)
- Serum Sodium (SNa)
- Urine Sodium (UNa)
- Urine Creatinine (UCr)

The inputs can be in different units:

- Serum and urine creatinine can be input in mg/dL or µmol/L. If µmol/L is used, these values are converted to mg/dL by dividing by 88.4.
- Sodium values are equivalent in mEq/L and mmol/L, so no conversion is necessary for these.

The calculation for FENa involves: FeNa = [100 × SCr × UNa/(SNa × UCr)]

This formula computes the percentage of filtered sodium excreted in the urine.

Interpretation of the FENa result is key:

- If FENa is less than 1%, it suggests prerenal causes. This typically means the kidneys are actively conserving sodium due to low blood volume or perfusion issues, such as in dehydration or heart failure.
- A FENa value between 1% and 2% falls into a gray zone or indeterminate category. It requires additional clinical assessment and context as it does not clearly point to a specific cause.
- If FENa is greater than 2%, it implies intrinsic renal causes, signifying potential damage to the kidneys that impacts sodium reabsorption, such as acute tubular necrosis.

By providing these input values, the FENa calculator assists clinicians in evaluating and distinguishing between different causes of renal impairment.