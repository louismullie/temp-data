# Medical Question & Answer

**Sample ID**: 77f1c1aa-2c54-66cd-e89c-4371fbd6d289
**Dataset Index**: 224682

---

## Question

Let $ f(x)\equal{}5x^{13}\plus{}13x^5\plus{}9ax$. Find the least positive integer $ a$ such that $ 65$ divides $ f(x)$ for every integer $ x$.

---

## Answer

> Let's see… What do we have here? The user is asking for the least positive integer (a) such that (65) divides (f(x) = 5x^{13} + 13x^5 + 9ax) for every integer (x). Let's break this down step-by-step. First, I need to think about the divisibility condition modulo 65 and how it decomposes via the Chinese Remainder Theorem into modulo 5 and modulo 13. Then, I should verify what each congruence implies about the coefficients and the unknown (a). Next, I will solve the resulting system of congruences for (a) and confirm the smallest positive solution. Finally, I should double-check that the chosen (a) indeed makes (f(x) \\equiv 0 \\pmod{65}) for all integers (x).

> Let me first confirm the decomposition. Since (65 = 5 \\times 13) and 5 and 13 are coprime, the condition (65 \\mid f(x)) for all integers (x) is equivalent to (f(x) \\equiv 0 \\pmod{5}) and (f(x) \\equiv 0 \\pmod{13}) for all integers (x). I need to ensure both congruences are satisfied simultaneously before combining them into a single solution for (a) [^notfound].

> Now, I will examine the modulo 5 condition. I should confirm how each term behaves modulo 5. Notice that (x^{13} \\equiv x \\pmod{5}) by Fermat's Little Theorem, and similarly (x^5 \\equiv x \\pmod{5}), so the expression simplifies to (5x^{13} + 13x^5 + 9ax \\equiv 0x + 3x + 4ax \\equiv (3 + 4a)x \\pmod{5}). For this to be zero for all (x), the coefficient must vanish, so (3 + 4a \\equiv 0 \\pmod{5}), which gives (4a \\equiv -3 \\equiv 2 \\pmod{5}). Multiplying by the inverse of 4 modulo 5, which is 4, yields (a \\equiv 8 \\equiv 3 \\pmod{5}). Wait, let me verify the inverse step: 4 times 4 is 16, which is 1 mod 5, so yes, multiplying both sides by 4 gives (a \\equiv 8 \\equiv 3 \\pmod{5}) [^notfound].

> Next, I should review the modulo 13 condition. By Fermat's Little Theorem, (x^{13} \\equiv x \\pmod{13}) and (x^5) remains as is, so (f(x) \\equiv 5x + 13x^5 + 9ax \\equiv 5x + 0 + 9ax \\equiv (5 + 9a)x \\pmod{13}). For this to be identically zero modulo 13, I need (5 + 9a \\equiv 0 \\pmod{13}), which simplifies to (9a \\equiv -5 \\equiv 8 \\pmod{13}). I should double-check the inverse of 9 modulo 13; 9 times 3 is 27, which is 1 mod 13, so multiplying both sides by 3 gives (a \\equiv 24 \\equiv 11 \\pmod{13}) [^notfound].

> Hold on, I should verify the system of congruences before proceeding. I have (a \\equiv 3 \\pmod{5}) and (a \\equiv 11 \\pmod{13}). Let me solve this using the Chinese Remainder Theorem. Express (a = 5k + 3) and substitute into the second congruence: (5k + 3 \\equiv 11 \\pmod{13}), so (5k \\equiv 8 \\pmod{13}). The inverse of 5 modulo 13 is 8, since (5 \\times 8 = 40 \\equiv 1 \\pmod{13}), so (k \\equiv 8 \\times 8 = 64 \\equiv 12 \\pmod{13}). Thus (k = 13m + 12) and (a = 5(13m + 12) + 3 = 65m + 63). The least positive integer solution is when (m = 0), so (a = 63). Hmm, wait a minute, I initially thought the smallest positive solution might be 63, but I should confirm that 63 is indeed positive and satisfies both original congruences: (63 \\equiv 3 \\pmod{5}) and (63 \\equiv 11 \\pmod{13}), which it does [^notfound].

> I will now examine whether (a = 63) makes (f(x)) divisible by 65 for all integers (x). With (a = 63), modulo 5 we have ((3 + 4 \\times 63)x = (3 + 252)x = 255x \\equiv 0x \\pmod{5}), and modulo 13 we have ((5 + 9 \\times 63)x = (5 + 567)x = 572x \\equiv 0x \\pmod{13}). Since both congruences hold for all (x), (65 \\mid f(x)) for all integers (x). I should double-check that no smaller positive (a) satisfies both congruences; because the general solution is (a = 65m + 63), the next smaller candidate would be negative, confirming 63 is the least positive integer [^notfound].

---

The least positive integer (a) such that (65 \\mid f(x) = 5x^{13} + 13x^5 + 9ax) for all integers (x) is **63**. This is found by requiring (f(x) \\equiv 0 \\pmod{5}) and (f(x) \\equiv 0 \\pmod{13}) for all (x), which yields (a \\equiv 3 \\pmod{5}) and (a \\equiv 11 \\pmod{13}); the smallest positive solution is **63**.

---

## Step 1: Factorize 65 and apply the Chinese remainder theorem

Since (65 = 5 \\times 13) and 5 and 13 are coprime, (65 \\mid f(x)) for all integers (x) if and only if both (5 \\mid f(x)) and (13 \\mid f(x)) for all integers (x).

---

## Step 2: Analyze modulo 5

We need (f(x) \\equiv 0 \\pmod{5}) for all integers (x). By Fermat's little theorem, (x^5 \\equiv x \\pmod{5}) and (x^{13} \\equiv x \\pmod{5}), so:

[
f(x) = 5x^{13} + 13x^5 + 9ax \\equiv 0 \\cdot x + 3x + 4ax \\equiv (3 + 4a)x \\pmod{5}.
]

For this to be 0 for all (x), the coefficient must be 0:

[
3 + 4a \\equiv 0 \\pmod{5} \\implies 4a \\equiv -3 \\equiv 2 \\pmod{5}.
]

Multiplying by the modular inverse of 4 modulo 5 (which is 4):

[
a \\equiv 8 \\equiv 3 \\pmod{5}.
]

---

## Step 3: Analyze modulo 13

We need (f(x) \\equiv 0 \\pmod{13}) for all integers (x). By Fermat's little theorem, (x^{13} \\equiv x \\pmod{13}), so:

[
f(x) = 5x^{13} + 13x^5 + 9ax \\equiv 5x + 0 + 9ax \\equiv (5 + 9a)x \\pmod{13}.
]

For this to be 0 for all (x), the coefficient must be 0:

[
5 + 9a \\equiv 0 \\pmod{13} \\implies 9a \\equiv -5 \\equiv 8 \\pmod{13}.
]

Multiplying by the modular inverse of 9 modulo 13 (which is 3):

[
a \\equiv 24 \\equiv 11 \\pmod{13}.
]

---

## Step 4: Solve the system of congruences

We have:

[
a \\equiv 3 \\pmod{5} \\quad \\text{and} \\quad a \\equiv 11 \\pmod{13}.
]

Express (a = 5k + 3) and substitute into the second congruence:

[
5k + 3 \\equiv 11 \\pmod{13} \\implies 5k \\equiv 8 \\pmod{13}.
]

Multiply by the inverse of 5 modulo 13 (which is 8):

[
k \\equiv 64 \\equiv 12 \\pmod{13}.
]

Thus, (k = 13m + 12) and:

[
a = 5(13m + 12) + 3 = 65m + 63.
]

The least positive integer occurs when (m = 0), so (a = 63).

---

## Step 5: Verification

Let (a = 63). Then:

- **Modulo 5**: (3 + 4 \\cdot 63 = 255 \\equiv 0 \\pmod{5}).
- **Modulo 13**: (5 + 9 \\cdot 63 = 572 \\equiv 0 \\pmod{13}).

Since both congruences hold, (65 \\mid f(x)) for all integers (x).

---

The least positive integer (a) is **63**.

---

## References

### A neural theory for counting memories [^b797eb11]. Nature Communications (2022). High credibility.

Theorem 2'

There is an absolute constant c for which the following holds. Suppose the neural count sketch sees n observations satisfying Assumption 1' with. Pick any 0 < δ < 1.
Suppose that m ≥ 2 k n and thatfor a positive integer f. Then with probability at least 1 − δ, when presented with a query x (i) with 0 ≤ f i ≤ f, the response of the neural count sketch will lie in the range f i ± 1.
Suppose that m ≥ 2 k / ϵ for some ϵ > 0 and that. Then with probability at least 1 − δ, when presented with a query x (i), the response of the neural count sketch will lie in the range f i ± ϵ n.

Note that the query x (i) need not belong to the original sequence of n observations, in which case f i = 0.

Theorem 5 gives bounds that are significantly more favorable for the 1-2-3-many sketch.

Theorem 5'

Suppose the 1-2-3-many sketch, with parameter β = 1, witnesses n observations that satisfy Assumption 1' with. Pick any 0 < δ < 1 and suppose that m ≥ 2 k N and. Then with probability at least 1 − δ, when presented with a query x (i), the response of the sketch will be e − r for some value r that is either f i or f i + 1 when rounded to the nearest integer.

Overall, these mathematical proofs provide bounds on how accurately stimuli can be tracked using the two neural count sketches.

The Drosophila mushroom body implements the anti-Hebbian count sketch

Here, we provide evidence supporting the "1-2-3-many" model from the olfactory system of the fruit fly, where circuit anatomy and physiology have been well-mapped at synaptic resolution. The evidence described below includes the neural architecture of stimulus encoding, the plasticity induced at the encoding-decoding synapse, and the response precision of the decoding (counting) neuron. The latter two we derive from a re-analysis of data detailing novelty detection mechanisms in the fruit fly mushroom body, where odor memories are stored.

---

### Input-output maps are strongly biased towards simple outputs [^50fca0dd]. Nature Communications (2018). Medium credibility.

Many systems in nature can be described using discrete input-output maps. Without knowing details about a map, there may seem to be no a priori reason to expect that a randomly chosen input would be more likely to generate one output over another. Here, by extending fundamental results from algorithmic information theory, we show instead that for many real-world maps, the a priori probability P(x) that randomly sampled inputs generate a particular output x decays exponentially with the approximate Kolmogorov complexity [Formula: see text] of that output. These input-output maps are biased towards simplicity. We derive an upper bound P(x)≲[Formula: see text], which is tight for most inputs. The constants a and b, as well as many properties of P(x), can be predicted with minimal knowledge of the map. We explore this strong bias towards simple outputs in systems ranging from the folding of RNA secondary structures to systems of coupled ordinary differential equations to a stochastic financial trading model.

---

### An introduction to machine learning [^70a823d9]. Clinical Pharmacology and Therapeutics (2020). Medium credibility.

Different categories of loss functions

Different objective functions can be chosen to measure the distance between observed data and values predicted by the model. Some of the distance metrics used in practice can be associated to a likelihood. The likelihood indicates how probable it is to observe our data according to the selected model. The most common use of a likelihood is to find the parameters that make the model fit optimally to the data (i.e. the maximum likelihood parameter estimates). Usually, the negative logarithm of the likelihood is minimized and considered as objective function because it has favorable numerical properties. Similarly, in ML metrics, such as mean squared error, logistic objective, or cross‐entropy, are used to find optimal parameters or assess the fitness of the model.

In practice, analytical calculation of maximum likelihood or minimal loss may not be feasible, and it is often necessary to use a numerical optimization algorithm to solve for the best parameter values. Gradient descent is such an algorithm, where we first define an objective function for which we want to minimize and then iteratively update the values of the parameters in the direction with the steepest decrease (first‐order derivative) of the objective function until a convergence to a minimum distance is deemed reached. In the scenario of a nonconvex objective function, the success of finding a global minimum, as opposed to landing in some local minima, will depend on the choice of the initial set of parameter values, the learning rate (i.e. step size of each iteration) and the criterion for convergence. The reader can refer to ref. 35 for details on convex and nonconvex optimization processes. Stochastic gradient descent is an additional trick that can further speed up the optimization by randomly sampling a training dataset and summing the distances across this subset of training data points for approximating the objective function.

---

### The eighty five percent rule for optimal learning [^0684215b]. Nature Communications (2019). High credibility.

Researchers and educators have long wrestled with the question of how best to teach their clients be they humans, non-human animals or machines. Here, we examine the role of a single variable, the difficulty of training, on the rate of learning. In many situations we find that there is a sweet spot in which training is neither too easy nor too hard, and where learning progresses most quickly. We derive conditions for this sweet spot for a broad class of learning algorithms in the context of binary classification tasks. For all of these stochastic gradient-descent based learning algorithms, we find that the optimal error rate for training is around 15.87% or, conversely, that the optimal training accuracy is about 85%. We demonstrate the efficacy of this 'Eighty Five Percent Rule' for artificial neural networks used in AI and biologically plausible neural networks thought to describe animal learning.

---

### A machine learning automated recommendation tool for synthetic biology [^5caa5c39]. Nature Communications (2020). High credibility.

Optimization-suggesting next steps

The optimization phase leverages the predictive model described in the previous section to find inputs that are predicted to bring us closer to our objective (i.e. maximize or minimize response, or achieve a desired response level). In mathematical terms, we are looking for a set of N r suggested inputs, that optimize the response with respect to the desired objective. Specifically, we want a process that:
i. optimizes the predicted levels of the response variable;
ii. can explore the regions of input phase space (in Eq. (1)) associated with high uncertainty in predicting response, if desired;
iii. provides a set of different recommendations, rather than only one.

We are interested in exploring regions of input phase space associated with high uncertainty, so as to obtain more data from that region and improve the model's predictive accuracy. Several recommendations are desirable because several attempts increase the chances of success, and most experiments are done in parallel for several conditions/strains.

In order to meet these three requirements, we define the optimization problem formally aswhere the surrogate function G (x) is defined as:depending on which mode ART is operating in (see the "Key capabilities" section). Here, y * is the target value for the response variable, y = y (x), and Var(y) denote the expected value and variance, respectively (see "Expected value and variance for ensemble model" in Supplementary Information), denotes Euclidean distance, and the parameter α ∈ [0, 1] represents the exploitation-exploration trade-off (see below). The constraintcharacterizes the lower and upper bounds for each input feature (e.g. protein levels cannot increase beyond a given, physical, limit). These bounds can be provided by the user (see details in the "Implementation" section in Supplementary Information); otherwise, default values are computed from the input data as described in the "Input space set" section in Supplementary Information.

---

### The eighty five percent rule for optimal learning [^74499c58]. Nature Communications (2019). High credibility.

If the decision boundary is set to 0, such that the model chooses option A when h > 0, option B when h < 0 and randomly when h = 0, then the noise in the representation of the decision variable leads to errors with probabilitywhere F (x) is the cumulative density function of the standardized noise distribution, p (x) = p (x |0, 1), and β = 1/ σ quantifies the precision of the representation of Δ and the agent's skill at the task. As shown in Fig. 1b, this error rate decreases as the decision gets easier (Δ increases) and as the agent becomes more accomplished at the task (β increases).

The goal of learning is to tune the parameters ϕ such that the subjective decision variable, h, is a better reflection of the true decision variable, Δ. That is, the model should aim to adjust the parameters ϕ so as to decrease the magnitude of the noise σ or, equivalently, increase the precision β. One way to achieve this tuning is to adjust the parameters using gradient descent on the error rate, i.e. changing the parameters over time t according towhere η is the learning rate and ∇ ϕ ER is the derivative of the error rate with respect to parameters ϕ. This gradient can be written in terms of the precision, β, asNote here that only the first term on the right hand side of Eq. (5) depends on the difficulty Δ, while the second describes how the precision changes with ϕ. Note also that Δ itself, as the 'true' decision variable, is independent of ϕ. This means that the optimal difficulty for training, that maximizes the change in the parameters, ϕ, at this time point, is the value of the decision variable Δ * that maximizes ∂ ER/ ∂β. Of course, this analysis ignores the effect of changing ϕ on the form of the noise — instead assuming that it only changes the scale factor, β, an assumption that likely holds in the relatively simple cases we consider here, although whether it holds in more complex cases will be an important question for future work.

---

### Degenerate boundaries for multiple-alternative decisions [^a8d28202]. Nature Communications (2022). High credibility.

Rewards are generated according to the Bayes risk represented over single trials (equation 12), when stochastic decision trajectories encounter a decision boundary. A reward landscape can be generated by systematically varying decision boundary parameters and averaging the rewards generated over a large number of simulated trajectories for each set of parameters. Averaging over many reward outcomes reduces variance producing a smooth surface and simulates the expected reward, which coincides with sampling rewards to learn the decision boundary under equation (11). So each point in the reward landscape is the mean reward obtained from a distribution of rewards for that boundary shape (typically from 100,000 samples across the parameter ranges); differing landscapes with c / W discretized in 1000 steps across its range from 0 to 0.2 were then considered. Each landscape has a point of mean reward that is an absolute maximum, which we will denote by; however, the landscape is very noisy even after this averaging, with many other points with similar mean rewards. Therefore, we consider a set of parameters corresponding to multiple points x ∈ X on the landscape, with mean rewards r x and standard deviations σ x according to the distribution of rewards for those parameters. This set of parameters is selected such that the maximum mean reward value of the landscape falls within a δ σ x of the point, so that. Using some example reward landscapes, we found that as δ decreases, the area designated as maximum also decreases as expected. However, below δ = 0.02, the area of the maximal region does not appear to change, but the points selected became more sparse, so we use this value.

Reporting summary

Further information on research design is available in the Nature Research Reporting Summary linked to this article.

---

### A continuous-time maxSAT solver with high analog performance [^1795c2c9]. Nature Communications (2018). Medium credibility.

Discussion

In summary, we presented a continuous-time dynamical system approach to solve a quintessential discrete optimization problem, MaxSAT. The solver is based on a deterministic set of ordinary differential equations and a heuristic method that is used to predict the likelihood that the optimal solution has been found by analog time t. The prediction part of the algorithm exploits the statistics of the ensemble of trajectories started from random initial conditions, by introducing the notion of energy-dependent escape rate and extrapolating this dependence to predict both the minimum energy value (lowest number of unsatisfied clauses) and the expected time needed by the algorithm to reach that value. This statistical analysis is very simple; it is quite possible that more sophisticated methods can be used to better predict minima values and time lengths. Due to its general character, the presented approach can be extended to other optimization problems as well, to be presented in forthcoming publications.

---

### Abstract representations emerge naturally in neural networks trained to perform multiple tasks [^8f36d123]. Nature Communications (2023). High credibility.

The dimensionality of representations in the multi-tasking model

First, we consider a deep network trained to perform P balanced classification tasks on a set of D latent variables. We focus on the activity in the layer just prior to readout, which we refer to as the representation layer and denote as r (x) for a particular x ∈ X. This representation layer is connected to the P output units by a linear transform W. In our full multi-tasking model, we then apply a sigmoid nonlinearity to the output layer. To simplify our calculation, we leave that out here. The network is trained to minimize error, according to a loss function which can be written for a particular sample x as:where A is a P × D matrix of randomly selected partitions (and it is assumed to be full rank) and the sum is over the P tasks. To gain an intuition for how r (x) will change during training, we write the update rule for r (x) (to be achieved indirectly by changing preceding weights, though we ignore the side effects that would arise from these weight changes), Thus, we can see that, over training, r (x) will be made to look more like a linear transform of the target function, sign(A x). Next, to link this to abstract representations, we first observe that A x produces an abstract representation of the latent variables. Then, we show that sign(A x) has approximately the same dimensionality as A x. In particular, the covariance matrixhas the elements, where A i is the i th row of A and when x i are random variables with an equal probability of being positive or negative (both the Gaussian and uniform distributions we use here have this property). To find the dimensionality of sign(A x) we need to find the dimensionality of M. First, the distribution of dot products between random vectors is centered on 0 and the variance scales as 1/ D, where D is the dimensionality of the latent variables as usual. Thus, we can Taylor expand the elements of the covariance matrix around A i A j = 0, which yieldsWe identify this as a scalar multiplication of the covariance matrix for the linear, abstract target. Further, we know that the rank of this matrix is. So, this implies that the matrix M also has rank approximately. Deviations from this approximation will produce additional non-zero eigenvalues, however, they are expected to be small.

---

### Autoencoders for sample size estimation for fully connected neural network classifiers [^07fda1d4]. NPJ Digital Medicine (2022). Medium credibility.

Introduction

Supervised learning with deep neural networks has achieved state of the art performance in a diverse range of applications. An adequate number of labeled samples is essential for training these systems but most real-world data is unlabeled. Label generation can be cumbersome, expensive and is a major barrier to the development and testing of such systems.

Ideally, when confronted with a task and unlabeled data, one would like to estimate how many examples need to be labeled to train a neural network for that task. In this paper, we take a step towards addressing this problem.

Consider a fully connected neural network f of pre-specified dimensions and a dataset X, which is initially unlabeled, but for which labels y can be obtained when needed. We define the minimum convergence size (MCS) for f on X to be the smallest number n such that a subset X n of n examples drawn at random from X can be labeled and used to train f as a non-trivial classifier, that is, one whose area-under-the-curve (AUC) on a held-out test set is greater than 0.5:Given that outcomes are balanced, an A U C > 0.5 implies that a model is able to identify some signal in the underlying data, and if that AUC is on the test dataset, this means that the signal identified by the model can generalize to unseen data. In this scenario, below the MCS, we would expect to see little or no correlation between sample size and model performance measured by AUC, whereas above the MCS we would expect to see a positive correlation.

We propose a method for empirically determining the MCS for f on X using only unlabeled data, and we call this estimate the Minimum Convergence Sample Estimate (MCSE). We do this by first constructing an autoencoder g, wherein the encoder part has a similar number of parameters and hidden layers as f. We train g on increasingly larger (unlabeled) subsets X i of X. This may permit similarities in layer-wise learning between f and g. Under these circumstances, we empirically show that, at each step i, the reconstruction loss L of g is related to the generalization performance of f trained on a similarly sized sample. We also demonstrate how this can be used to determine the MCSE for f on X (Fig. 1).

Fig. 1
Minimum convergence sample estimation can be used to approximate the number of labels required for generalizable performance.

---

### Functional control of oscillator networks [^a1b632bf]. Nature Communications (2022). High credibility.

We employ structural (i.e. interconnections between brain regions) and functional (i.e. time series of recorded neural activity) data from ref. Structural connectivity consists of a sparse weighted matrix A whose entries represent the strength of the physical interconnection between two brain regions. Functional data comprise time series of neural activity recorded through functional magnetic resonance imaging (fMRI) of healthy subjects at rest. Because the phases of the measured activity have been shown to carry most of the information contained in the slow oscillations recorded through fMRI time series, we follow the steps in ref.to obtain such phases from the data by filtering the time series in the [0.04, 0.07] Hz frequency range (Supplementary Information). Next, since frequency synchronization is thought to be a crucial enabler of information exchange between different brain regions and homeostasis of brain dynamics, we focus on functional patterns that arise from phase-locked trajectories, as compatible with our analysis. For simplicity, we restrict our study to the cingulo-opercular cognitive system, which, at the spatial scale of our data, comprises n = 12 interacting brain regions.

We identify time windows in the filtered fMRI time series where the signals are phase-locked, and compute two matrices for each time window: a matrixof Pearson correlation coefficients (also known as functional connectivity), and a functional pattern(as in equation (2)) from the phases extracted by solving the nonconvex phase synchronization problem. Strikingly, we find that ∥ F − R ∥ 2 ≪ 1 consistently (see Supplementary Information and Supplementary Fig. 7 b), thus demonstrating that our definition of the functional pattern is a good replacement for the classical Pearson correlation arrangements in networks with oscillating states.

Building upon the above finding, we test whether the oscillators' natural frequencies embody the parameter that links the brain network structure to its function (i.e. structural and functional matrices). We set ω = B D (x) δ, where x are phase differences obtained from the previous step, and integrate the Kuramoto model in Eq. (1) with random initial conditions close to x. We show in Fig. 8 that the assignment of natural frequencies according to the extracted phase differences promotes spontaneous synchronization to accurately replicate the empirical functional connectivity F.

---

### Biased expectations about future choice options predict sequential economic decisions [^9b0d2bac]. Communications Psychology (2024). Medium credibility.

Next, the model works backwards through the sequence, iteratively using the aforementioned formula forwhen computing each respective action value Q for taking the option and declining the option for each t. Whenever the reward value of taking the current option is considered, the reward function R assigns reward values to options based on their ranks. h represents the relative rank of the current option.

In contrast, the reward value of sampling again is simply the cost to sample C.

This customisable R function allowed us to examine how the Ideal Observer changes its sampling strategy under the different reward payoff schemes used in our studies. Pilot full, Study 1 full, Study 2 and both conditions in Study 3 all involved instructing participants to try to choose the best price possible. In study conditions using these instructions, we implemented a continuous payoff function (resembling that of the classic Gilbert & Mosteller formulation), in which the relative rank of each choice would be rewarded commensurate with the value of its associated option. In Pilot baseline and the baseline, squares, timing, and prior conditions of Study 1, we adapted the payoff scheme to match participants' instructions that they would be paid £0.12 for the best rank, £0.08 for the second-best rank, £0.04 for the third best rank and £0 for any other ranks. Lastly, in the payoff condition of Study 1, we programmed the reward payoff function to match participants' reward of 5 stars for the best rank, 3 stars for the second-best rank, one star for the third-best rank and zero stars for any other ranks.

Another feature added to our implementation of the Ideal Observer, compared to the Gilbert & Mosteller base model, is the ability to update the model's generating distribution from its experience with new samples in a Bayesian fashion, instead of this generating distribution being specified in advance and then fixed throughout the paradigm. This Bayesian version of the optimality model treats option values as samples from a Gaussian distribution with a normal-inverse- χ2 prior. The prior distribution is initialised before experiencing any options with four parameters: the prior mean μ 0, the degrees of freedom of the prior mean κ, the prior variance σ 2 0 and the degrees of freedom of the prior variance ν. The μ 0 and σ 2 0 parameters of this prior distribution are then updated by the model following presentation of each newly sampled option value as each sequence progresses.

---

### Complex modeling with detailed temporal predictors does not improve health records-based suicide risk prediction [^ac54580a]. NPJ Digital Medicine (2023). Medium credibility.

Variables describing trend in X over time:
D18 (# of months with X) × [(difference between the earliest month and most recent month with X] + 1). Those who do not have X observed (ever) or only have one occurrence, set value to 0.
D19 Maximum # of days with X in any month minus the minimum count of days with X in any month.
D20 Maximum # of days with X in any month.
D21 Number of months in which days with X exceeds Y, where Y is the entire visit sample's average monthly days with X. Calculate Y by averaging over all months with at least one X.
D22 Number of months in which days with X exceeds Y, where Y is person's average monthly days with X as of this visit. Only consider X that occurred while person was enrolled. If X not observed while enrolled, set to −5.
D23 Number of months in which days with X exceeds Y, where Y is person's average monthly days with X as of this visit. Only consider X that occurred while person was enrolled prior to visit. If X was not observed during that time, set to 65.
D24 Proportion of months enrolled in which days with X exceeds Y, where Y is entire visit sample's average monthly days with X. Calculate Y by averaging over all months with at least one X.
D25 Proportion of months enrolled in which days with X exceeds Y, where Y is person's average monthly days with X as of this visit. Only consider X that occurred while person was enrolled prior to visit up to the full past 60 months.
D26 Total days with X in last month minus monthly average for prior 2–12 months. Only consider X that occurred while person was enrolled prior to visit. If not enrolled ≥ 2 months, set to −5.
D27 Monthly average of days with X in last 2 months minus monthly average over prior 3–12 months. Only consider X that occurred while person was enrolled prior to visit. If not enrolled ≥ 3 months, set to −5.
D28 Monthly average of days with X in last 3 months minus monthly average over prior 4–12 months. Only consider X that occurred while person was enrolled prior to visit. If not enrolled ≥ 4 months, set to −5.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^aeeaae45]. Nature Communications (2025). High credibility.

Fitting models to data is an important part of the practice of science. Advances in machine learning have made it possible to fit more-and more complex-models, but have also exacerbated a problem: when multiple models fit the data equally well, which one(s) should we pick? The answer depends entirely on the modelling goal. In the scientific context, the essential goal is replicability: if a model works well to describe one experiment, it should continue to do so when that experiment is replicated tomorrow, or in another laboratory. The selection criterion must therefore be robust to the variations inherent to the replication process. In this work we develop a nonparametric method for estimating uncertainty on a model's empirical risk when replications are non-stationary, thus ensuring that a model is only rejected when another is reproducibly better. We illustrate the method with two examples: one a more classical setting, where the models are structurally distinct, and a machine learning-inspired setting, where they differ only in the value of their parameters. We show how, in this context of replicability or "epistemic uncertainty", it compares favourably to existing model selection criteria, and has more satisfactory behaviour with large experimental datasets.

---

### Complex modeling with detailed temporal predictors does not improve health records-based suicide risk prediction [^41a298b9]. NPJ Digital Medicine (2023). Medium credibility.

For the full list of ICD-9-CM and ICD-10-CM codes see:

Variables summarizing total count of days with X in specific time periods:
Total count of days with X in last 1 month.
Total count of days with X in last 3 months.
Total count of days with X in last 12 months.
Total count of days with X in last 24 months.
Total count of days with X in last 60 months.

Variables that describe the past "rate" of X:
D06 Total days with X in last 3 months divided by number of months enrolled in those months.
D07 Total days with X in last 12 months divided by number of months enrolled in those months.
D08 Total days with X in last 24 months divided by number of months enrolled in those months.
D09 Total days with X in the past 60 months divided by number of months enrolled in those months.

Variables capturing information on how recently X occurred:
D10 Most recent occurrence of X (months prior to visit). Those who do not have X observed (ever), set value for most recent occurrence to −5 months.
D11 Most recent occurrence of X (months prior to visit). Those who do not have X observed (ever), set value for most recent occurrence to 65 months.
D12 Most recent month for which X was not observed. Those who do not have X observed (ever), set value for most recent month without X to −5.
D13 Most recent month for which X was not observed. Those who do not have X observed (ever), set value for most recent month without X to 65.

Variables describing earliest occurrence of X:
D14 Earliest occurrence of X (months prior to visit). Those who do not have X observed (ever), set value for most recent occurrence to −5 months.
D15 Earliest occurrence of X (months prior to visit). Those who do not have X observed (ever), set value for most recent occurrence to 65 months.
D16 Difference between the earliest month and most recent month with occurrence of X. Those with only 1 occurrence set difference to 0. Those who do not have X observed (ever), set difference to −5.
D17 Difference between the earliest month and most recent month with occurrence of X. Those with only 1 occurrence set difference to 0. Those who do not have X observed (ever), set difference to 65.

---

### Identifying domains of applicability of machine learning models for materials science [^9c5f816f]. Nature Communications (2020). High credibility.

A predictive ML model is then a functionaiming to minimize the expected error (also called prediction risk)measured by some non-negative loss function l that quantifies the cost incurred by predicting the actual property value y with f (x). Examples for loss functions are the squared error, the absolute error, and, for non-zero properties, the relative error. Here P denotes some fixed probability distribution that captures how candidate materials are assumed to be sampled from the materials class (this concept, while commonly assumed in ML, is an unnecessary restriction for high-throughput screening as we discuss in more detail below). Since the true prediction risk is impossible to compute directly without perfect knowledge of the investigated materials class, models are evaluated by the test error (or empirical risk)defined as the average of the individual errors (losses) e i (f) = l (f (x i), y i) on some test set of m reference data points. The samples in this test set are drawn independently and identically distributed according to P and are also independent of the model — which means in practice that it is a random subset of all available reference data that has been withheld from the ML algorithm. In order to reduce the variance of this estimate, a common strategy is cross-validation, where this process is repeated multiple times based on partitioning the data into a number of non-overlapping "folds" and then to use each of these folds as test sets and the remaining data as a training set to fit the model.

This test error properly estimates the model performance globally over the whole representation space X (weighted by the distribution P used to generate the test points). This is an appropriate evaluation metric for selecting a model that is required to work well on average for arbitrary new input materials that are sampled according to the same distribution P. This is, however, not the condition of high-throughput screening. Here, rather than being presented with random inputs, we can decide which candidate materials to screen next. This observation leads to the central idea enabled by the DA analysis proposed in this work: if the employed model is particularly applicable in a specific subdomain of the materials class, and if that subdomain has a simple and interpretable shape that permits to generate new materials from it, then we can directly focus the screening there.

---

### Knowledge graph-based recommendation framework identifies drivers of resistance in EGFR mutant non-small cell lung cancer [^f7823c7d]. Nature Communications (2022). High credibility.

Results

Re-ranking of CRISPR hits can be approached as multi-objective optimization

We framed re-ranking of CRISPR hits as a multi-objective optimization problem. In this setting, diverse lines of evidence that support gene's relevance are treated as multiple objectives (Fig. 1). In other words, the formal goal is to simultaneously optimize k objectives, reflected in k objective functions: f 1 (x), f 2 (x). f k (x). Individual functions form a vector function F (x):where x = [x 1, x 2. x m] ∈ Ω; x represents the decision variable, Ω-decision variable space. Therefore, multi-objective optimization can be defined as minimization (or maximization) of the objective function set F (x). With multiple competing objectives a singular best solution often cannot be found. However, one can identify a set of optimal solutions based on the notion of Pareto dominance. A solution x 1 dominates solution x 2 if the following two conditions are true:
solution x 1 is not worse than x 2 according to all objectives;
solution x 1 is strictly better than solution x 2 according to at least one objective.

Fig. 1
Recommendation system takes into account diverse types of evidence to suggest promising drivers of drug resistance in NSCLC.

The evidence from specially built knowledge graph, literature, clinical and pre-clinical datasets is aggregated and formalized as objectives in multi-objective optimization (MOO) task. Recommended solutions (genes) represent the optimal trade-offs between the conflicting objectives. A subset of recommended genes is passed for the experimental validation.

If both conditions are true, we can say that x 1 dominates x 2, which is equal to x 2 being dominated by x 1. In other words, dominant solutions can not be improved any further without compromising at least one of the other objectives. A set of such dominant solutions forms a Pareto front, which combines the best trade-off combinations between competing objectives. Therefore, by computing Pareto fronts on diverse sets of objectives defined based on CRISPR screen data and additional supporting evidence we can narrow down the number of promising markers of EGFR TKI resistance (Fig. 1).

---

### A space-time tradeoff for implementing a function with master equation dynamics [^57fa10b0]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### Phylogenetically informed predictions outperform predictive equations in real and simulated data [^8b798c0c]. Nature Communications (2025). High credibility.

Inferring unknown trait values is ubiquitous across biological sciences-whether for reconstructing the past, imputing missing values for further analysis, or understanding evolution. Models explicitly incorporating shared ancestry amongst species with both known and unknown values (phylogenetically informed prediction) provide accurate reconstructions. However, 25 years after the introduction of such models, it remains common practice to simply use predictive equations derived from phylogenetic generalised least squares or ordinary least squares regression models to calculate unknown values. Here, we use a comprehensive set of simulations to demonstrate two- to three-fold improvement in the performance of phylogenetically informed predictions compared to both ordinary least squares and phylogenetic generalised least squares predictive equations. We found that phylogenetically informed prediction using the relationship between two weakly correlated (r = 0.25) traits was roughly equivalent to (or even better than) predictive equations for strongly correlated traits (r = 0.75). A critique and comparison of four published predictive analyses showcase real-world examples of phylogenetically informed prediction. We also highlight the importance of prediction intervals, which increase with increasing phylogenetic branch length. Finally, we offer guidelines to making phylogenetically informed predictions across diverse fields such as ecology, epidemiology, evolution, oncology, and palaeontology.

---

### Concerns regarding… [^70dc23c9]. EClinicalMedicine (2024). Medium credibility.

Controversy about hyperparameter optimization of various machine learning models

In this study, the process of hyperparameter optimization is not described, even though different combinations of hyperparameters are crucial for model building and validation.

In the field of machine learning, using default hyperparameters may not be suitable for specific datasets. Lack of hyperparameter optimization can prevent the model from fully exploiting feature information, leading to less stable performance, high heterogeneity, and insufficient robustness, which negatively affects model building and generalization. Different combinations of hyperparameters significantly impact the predictive efficacy of the model. Failure to adequately perform hyperparameter optimization may result in models that do not achieve optimal predictive efficacy. This causes selection bias in the model and ultimately leads to incorrect decisions.

Machine learning is prone to model overfitting and poor generalization if the built-in hyperparameters are not properly set. Therefore, hyperparameter optimization in the field of machine learning is a scientific, rigorous, and reliable practice.

---

### Efficient optimization with higher-order ising machines [^fd66b526]. Nature Communications (2023). High credibility.

A prominent approach to solving combinatorial optimization problems on parallel hardware is Ising machines, i.e., hardware implementations of networks of interacting binary spin variables. Most Ising machines leverage second-order interactions although important classes of optimization problems, such as satisfiability problems, map more seamlessly to Ising networks with higher-order interactions. Here, we demonstrate that higher-order Ising machines can solve satisfiability problems more resource-efficiently in terms of the number of spin variables and their connections when compared to traditional second-order Ising machines. Further, our results show on a benchmark dataset of Boolean k-satisfiability problems that higher-order Ising machines implemented with coupled oscillators rapidly find solutions that are better than second-order Ising machines, thus, improving the current state-of-the-art for Ising machines.

---

### A guidance to intelligent metamaterials and metamaterials intelligence [^2f701ffd]. Nature Communications (2025). High credibility.

For inverse design, its solution cannot be directly evaluated, instead requiring the exploration of a suitable solution from a formidably large design space. For metamaterials with a few design parameters, people can easily search for a top-performing structure via parameter sweeping and fine-tuning, in tandem with empirical guidance. However, this "quasi-brute-force" search can hardly be generalized into a feasible strategy and the resultant design is innately flawed by the trial-and- error manner. In place of this painstaking search, optimization algorithms are widely adopted, including gradient-based approaches (e.g. topology optimization – and adjoint method –) and heuristic algorithms (e.g. binary search, genetic algorithmsand simulated annealing –). These algorithms need to monitor the output in real time, and then send it back to optimization algorithms to form a closed-loop network. For each new customer-specific target, the network should repeatedly update to search for an optimal solution in a prescribed rule. This process calls for a gargantuan number of EM numerical simulations and inevitably encounters many failed attempts, causing a great waste of computing resources. Probably, the ultimate optimization results and serendipity are satisfactory, but they are limited by the convergence speed, random search, and easily fall into a local optimum.

---

### Symmetry breaking in optimal transport networks [^30e99d16]. Nature Communications (2024). High credibility.

Engineering multilayer networks that efficiently connect sets of points in space is a crucial task in all practical applications that concern the transport of people or the delivery of goods. Unfortunately, our current theoretical understanding of the shape of such optimal transport networks is quite limited. Not much is known about how the topology of the optimal network changes as a function of its size, the relative efficiency of its layers, and the cost of switching between layers. Here, we show that optimal networks undergo sharp transitions from symmetric to asymmetric shapes, indicating that it is sometimes better to avoid serving a whole area to save on switching costs. Also, we analyze the real transportation networks of the cities of Atlanta, Boston, and Toronto using our theoretical framework and find that they are farther away from their optimal shapes as traffic congestion increases.

---

### Bayesian strategy selection identifies optimal solutions to complex problems using an example from GP prescribing [^d1a4b1f1]. NPJ Digital Medicine (2020). Medium credibility.

Another aspect to be mindful of is that Thompson sampling trades between the type-1 error rate and statistical power, implying that to achieve a greater statistical power (relative to a same sample-size equal allocation trial), it is likely to inflate the type-1 error rate. By not trying the null strategy sufficiently, the type-1 error rate may exceed the acceptable threshold. Thus it is judicious to monitor the type-1 error rate and if it exceeds the significance level, adjustments may be required in the allocation algorithm. Recent workis starting to explore adaptive sampling approaches that can control the type-1 error rate, and further work is required to develop such techniques for a Thompson sampling setting.

We did not assess intervention fidelity, that is, whether the intervention was delivered as conceived and in accordance with the materials and instructions provided to participants. Given that the aims were to measure the effectiveness of the strategies in supporting GP-patient interactions where physical activity was discussed, failure to implement the strategies in consultations was ultimately what was tested, in essence assessing the utility of each strategy.

This case study has demonstrated an efficient process providing clear results in a short period of time. Further work should develop methods to understand interactions and sequencing of various and multiple combinations of interventions. Subsequent lines of investigation should also begin to consider the question of fidelity. Given this study demonstrates a process for supporting conversations about PA between GPs and patients, an important next step is to examine the quality and content of those conversations and their impact on measured patient PA.

While the reasons for low incidence of GP patient interaction about PA are complex, these interactions occur in a relatively stable setting with a clear, well-defined outcomes. Further work on other complex problems in similarly well-defined domains, or further investigations of the GP-patient relationship in expanded settings (i.e. with greater numbers of prioritised actions, or broader participant groups) represent exciting directions for future work.

The new frontier of public health is not what behaviours need to change, but rather how to change them. This study shows how new methods can be used to test and optimise implementation of intervention in multiple settings by engaging key actors in a system and moving quickly to the best set of interventions. For PA specifically, data suggested more than 80% of adults visit their GP at least once per year. Rapid identification of appropriate strategies that increase the discussion of PA between GPs and their patients provides new potential avenues for the improvement of public health.

---

### A note on competing risks in survival data analysis [^5a987e3d]. British Journal of Cancer (2004). Low credibility.

Equivalently, we can consider estimating the incidence of mortality among patients with breast cancer. In the above example, the estimated incidence of mortality at 1 year is five out of 100 patients, that is, 5%. The probability of dying at 2 years is the probability of living past the first year and dying during the second year. Note that 95 of 100 patients survived past the first year, and 10 of these 95 patients died in the second year. Therefore, the estimated incidence of mortality at 2 years is 95/100 * 10/95 = 10%. The cumulative incidence of mortality up to a given time is the probability that an individual dies by that time. This is the sum of mortality incidences occurring up to that time. Therefore, the cumulative incidence of mortality at 2 years is the sum of mortality incidences in the first and second years, that is, 5+10 = 15%. This is simply the converse of survival. In other words, the cumulative incidence of an event at a given time is one minus the overall survival probability at that time.

---

### Enhancing fairness in AI-enabled medical systems with the attribute neutral framework [^275a590a]. Nature Communications (2024). High credibility.

The CheXpert dataset consists of 224,316 chest X-ray images from 65,240 patients who underwent radiographic examinations at Stanford Health Care in both inpatient and outpatient centers between October 2002 and July 2017. The dataset includes only frontal-view X-ray images, as lateral-view images are removed to ensure dataset homogeneity. This results in the remaining 191,229 frontal-view X-ray images from 64,734 patients (Supplementary Table S1). Each X-ray image in the CheXpert dataset is annotated for the presence of 13 findings (Supplementary Table S2). The age and sex of each patient are available in the metadata.

In all three datasets, the X-ray images are grayscale in either ".jpg" or ".png" format. To facilitate the learning of the deep learning model, all X-ray images are resized to the shape of 256×256 pixels and normalized to the range of [−1, 1] using min-max scaling. In the MIMIC-CXR and the CheXpert datasets, each finding can have one of four options: "positive", "negative", "not mentioned", or "uncertain". For simplicity, the last three options are combined into the negative label. All X-ray images in the three datasets can be annotated with one or more findings. If no finding is detected, the X-ray image is annotated as "No finding".

Regarding the patient attributes, the age groups are categorized as " < 60 years" or " ≥ 60 years". The sex attribute includes two groups: "male" or "female". In the MIMIC-CXR dataset, the "Unknown" category for race is removed, resulting in patients being grouped as "White", "Hispanic", "Black", "Asian", "American Indian", or "Other". Similarly, the "Unknown" category for insurance type is removed and patients are grouped as "Medicaid", "Medicare", or "Other". The amount and proportion of X-ray images under attributes and cross-attributes for the three datasets are shown in Supplementary Tables S1, S3 – S5.

All three large-scale public chest X-ray datasets are divided into training datasets, validation datasets, and test datasets using an 8:1:1 ratio (Supplementary Table S6). To prevent label leakage, X-ray images from the same patient are not assigned to different subsets.

---

### Global strategy for asthma management and prevention [^ff163c6f]. GINA (2024). High credibility.

Asthma step-down to minimum effective treatment: Once good asthma control has been maintained for 2–3 months, consider stepping down gradually to find the lowest treatment that controls symptoms and exacerbations, provide a written asthma action plan, monitor closely with a follow-up visit, and do not completely withdraw ICS unless needed temporarily to confirm the diagnosis of asthma.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^68cfa501]. Journal of the American College of Cardiology (2025). High credibility.

Cost-effectiveness thresholds — framework and updates explain that the 2014 ACC/AHA statement provided per–capita GDP–based thresholds such that an intervention with an ICER < $50 000 per QALY gained should be considered high value, $50 000 to < $150 000 (1-3x per capita GDP) per QALY gained should be considered intermediate value, and ≥ $150 000 per QALY gained should be considered low value. The World Health Organization's program suggested that costs less than 3 times the national annual GDP per capita per disability-adjusted life year averted should be considered cost-effective, whereas costs less than 1 times should be considered highly cost-effective; however, experts have argued this approach lacks theoretical justification, and World Health Organization experts now also advise against the use of per capita GDP-based cost effectiveness thresholds and instead suggest considering cost-effectiveness information alongside other contextual information in a transparent multicriterion decision process. The writing committee reviewed alternative approaches, including relying on US-based studies; for example, a health opportunity cost analysis estimated that at 2019 prices, for every $1000000 increase in health care expenditures 1860 persons were projected to become uninsured with 81 QALYs lost due to mortality and 15 QALYs lost due to morbidity, implying a cost-effectiveness threshold of $104 000 per QALY ($51 000 to $209 000 per QALY) in 2019 US dollars, and in the probabilistic analysis 40% of the simulations suggested the threshold was less than $100 000 per QALY.

---

### Solving olympiad geometry without human demonstrations [^68872631]. Nature (2024). Excellent credibility.

Measuring the improvements made on top of the base symbolic deduction engine (DD), we found that incorporating algebraic deduction added seven solved problems to a total of 14 (DD + AR), whereas the language model's auxiliary construction remarkably added another 11 solved problems, resulting in a total of 25. As reported in Extended Data Fig. 6, we find that, using only 20% of the training data, AlphaGeometry still achieves state-of-the-art results with 21 problems solved. Similarly, using less than 2% of the search budget (beam size of 8 versus 512) during test time, AlphaGeometry can still solve 21 problems. On a larger and more diverse test set of 231 geometry problems, which covers textbook exercises, regional olympiads and famous theorems, we find that baselines in Table 1 remain at the same performance rankings, with AlphaGeometry solving almost all problems (98.7%), whereas Wu's method solved 75% and DD + AR + human-designed heuristics solved 92.2%, as reported in Extended Data Fig. 6b.

Notably, AlphaGeometry solved both geometry problems of the same year in 2000 and 2015, a threshold widely considered difficult to the average human contestant at the IMO. Further, the traceback process of AlphaGeometry found an unused premise in the translated IMO 2004 P1, as shown in Fig. 5, therefore discovering a more general version of the translated IMO theorem itself. We included AlphaGeometry solutions to all problems in IMO-AG-30 in the Supplementary Information and manually analysed some notable AlphaGeometry solutions and failures in Extended Data Figs. 2–5. Overall, we find that AlphaGeometry operates with a much lower-level toolkit for proving than humans do, limiting the coverage of the synthetic data, test-time performance and proof readability.

---

### Abstract representations emerge naturally in neural networks trained to perform multiple tasks [^55073c20]. Nature Communications (2023). High credibility.

Methods

Abstraction metrics

Both of our abstraction methods quantify how well a representation that is learned in one part of the latent variable space (e.g. a particular context) generalizes to another part of the latent variable space (e.g. a different context). To make this concrete, in both metrics, we train a decoding model on representations from only one — randomly chosen — half of the latent variable space and test that decoding model on representations from the non-overlapping half of the latent variable space.

The classifier generalization metric

First, we select a random balanced division of the latent variable space. One of these halves is used for training, the other is used for testing. Then, we select a second random balanced division of the latent variable space that is orthogonal to the first division. One of these halves is labeled category 1 and the other is labeled category 2. As described above, we train a linear classifier on this categorization using 1000 training stimuli from the training half of the space, and test the classifier's performance on 2000 stimuli from the testing half of the space. Thus, chance is set to 0.05 and perfect generalization performance is 1.

The regression generalization metric

As above, except we train a linear ridge regression model to read out all D latent variables using 4000 sample stimulus representations from the training half of the space. We then test the regression model on 1000 stimulus representations sampled from the testing half of the space. We quantify the performance of the linear regression with its r 2 value:where X is the true value of the latent variables andis the prediction from the linear regression. Because the MSE is unbounded, the r 2 value can be arbitrarily negative. However, chance performance is r 2 = 0, which would be the performance of the linear regression always predicted the mean of X, and r 2 = 1 indicates a perfect match between the true and predicted value.

---

### Learning protocols for the fast and efficient control of active matter [^a9cb71db]. Nature Communications (2024). High credibility.

In Fig. 1 b we show, for the choice D 1 = 2, that heat-minimizing protocols learned by the neural network vary between a step-like jump at the final time, for small values of t f, and a step-like jump at the initial time, for large values of t f (all protocols shown in this work are provided in the Supplementary Data 1 file). For intermediate values of t f we observe a range of protocol types. These protocols include non-monotonic and rapidly-varying forms, and show jump discontinuities at initial and final times. In Sec. S3A, we discuss the effect of the model parameters on the range of t f for which these non-trivial protocols result in a lower value of 〈 Q 〉 than the step protocols.

The heat associated with the final-time step protocol is just that associated with the initial steady state, and isThe heat associated with the initial-time jump protocol can be calculated from Eqs. (2) and (3), and iswhereNote that x ss is given in Eq. (10). For large t f we havewhich is the heat associated with the final steady state.

In Fig. 1 c we show that the heat values associated with the trained neural-network protocols interpolate, as a function of t f, between the values Q 1 and Q 2. Our conclusion is that this optimization problem is solved by protocols that are rapidly varying, have a variety of functional forms, and display jump discontinuities. As shown in the inset of Fig. 1 c and in Fig. S2, these protocols produce values of heat considerably smaller than those associated with the protocols derived in ref. (In the latter figure we also show that it is possible to construct smooth but rapidly-varying protocols that can produce values of heat arbitrarily close to the discontinuous protocols identified by the learning procedure).

The protocols just described are valid solutions to the heat-minimization problem defined in ref. However, some of them are not meaningful in experimental terms. For instance, for small values of t f, the heat-minimizing protocol is a step function at the final time. This protocol is a solution to the stated problem, but effects no change of the system's microscopic coordinates. All the heat associated with the subsequent transformation of the system is ignored, simply because we have stopped the clock.

---

### Machine learning enables completely automatic tuning of a quantum device faster than human experts [^fd71e035]. Nature Communications (2020). High credibility.

Searching for the hypersurface

In each iteration, the algorithm first locates the hypersurface in gate voltage space. To do this, it selects a search direction, specified by a unit vector u which during the first 30 iterations of the algorithm is selected randomly from a hypersphere, restricted to the octant where all gate voltages are negative. The gate voltages are then scanned along a ray beginning at the origin o and parallel to u (Fig. 4 a). During this scan, the current is monitored; when it falls below a threshold of 20% of full scale, this is taken as defining a location v (u) on the hypersurface.

Fig. 4
Characterising the boundary hypersurface using machine learning.

Each panel illustrates a step of the algorithm presented in Fig. 3. The gate voltage space, restricted to two dimensions for illustration, is divided into regions of near-zero (blue) and non-zero current (pink), separated by a boundary hypersurface. a Locating the hypersurface. The gate voltages are scanned along a ray (violet arrow) starting at the origin (white circle) and defined by direction u. By monitoring the current, the intersection with the hypersurface is measured. b To determine whether a region should be pruned, the algorithm scans each gate voltage individually toward the bottom of its range from a location just inside the hypersurface as shown. If only one scan intersects the hypersurface (as in the inset), future exploration of that region is inhibited by displacing the origin as shown. c Based on a short 1D scan, the location is classified according to whether it shows current peaks indicating Coulomb blockade. d If peaks are found, a 2D scan (violet square) is performed in the plane of V 3 and V 7, and is possibly repeated at higher resolution. e From the first thirty measurements (green and yellow circles), the algorithm builds a model of the hypersurface and assigns a probabilitythat peaks will be found. f To refine the model, the algorithm generates a set of candidate search locations (squares), each weighted by its corresponding value of, and selects one at random. A new scan is then performed in the corresponding direction to generate a new measurement of the hypersurface location. Steps d – f are then repeated indefinitely. Inset: Scheme for numerically sampling the hypersurface using simulated Brownian motion. Each point represents a simulated particle moving inside the enclosed volume. The collisions between the particles and the modelled hypersurface generate a set of candidate search locations.

---

### Limits on fundamental limits to computation [^98c96d2d]. Nature (2014). Excellent credibility.

An indispensable part of our personal and working lives, computing has also become essential to industries and governments. Steady improvements in computer hardware have been supported by periodic doubling of transistor densities in integrated circuits over the past fifty years. Such Moore scaling now requires ever-increasing efforts, stimulating research in alternative hardware and stirring controversy. To help evaluate emerging technologies and increase our understanding of integrated-circuit scaling, here I review fundamental limits to computation in the areas of manufacturing, energy, physical space, design and verification effort, and algorithms. To outline what is achievable in principle and in practice, I recapitulate how some limits were circumvented, and compare loose and tight limits. Engineering difficulties encountered by emerging technologies may indicate yet unknown limits.

---

### Exact controllability of complex networks [^406b29e5]. Nature Communications (2013). Medium credibility.

Controlling complex networks is of paramount importance in science and engineering. Despite the recent development of structural controllability theory, we continue to lack a framework to control undirected complex networks, especially given link weights. Here we introduce an exact controllability paradigm based on the maximum multiplicity to identify the minimum set of driver nodes required to achieve full control of networks with arbitrary structures and link-weight distributions. The framework reproduces the structural controllability of directed networks characterized by structural matrices. We explore the controllability of a large number of real and model networks, finding that dense networks with identical weights are difficult to be controlled. An efficient and accurate tool is offered to assess the controllability of large sparse and dense networks. The exact controllability framework enables a comprehensive understanding of the impact of network properties on controllability, a fundamental problem towards our ultimate control of complex systems.

---

### American Geriatrics Society identifies five things that healthcare providers and patients should question [^27b85b7d]. Journal of the American Geriatrics Society (2013). Medium credibility.

U.S. older-adult demographics and multimorbidity — With the eldest of the nation's 77 million baby boomers already 65 and the youngest reaching that milestone in 2019, older adults will make up a growing share for the next 4 decades; now roughly 40 million, U.S. residents age 65 and older will reach an estimated 78.9 million in 2050, and currently 80% of adults age 65 and older have at least one chronic health condition. The evidence base supporting the use of many common tests and treatments for older adults is described as inadequate, in part because older adults are significantly underrepresented in clinical trials.

---

### Understanding the infection severity and epidemiological characteristics of mpox in the UK [^160dbc20]. Nature Communications (2024). High credibility.

Infection to first positive test

The posterior estimate for the mean time from infection to first positive test was 15.14 days (95% CrI: 13.75, 16.65) (Fig. 2 and Table 2). The gamma distribution had the lowest LOO cross validation score however, the results are not strong evidence that that gamma had substantially lower average out-of-sample error from the Weibull or lognormal distributions. The full results for each distribution can be seen in Supplementary Table 4. The estimated values of the cumulative distribution function for the time from infection to first positive test can be seen in Supplementary Table 5, the median was 13.77 days (95% CrI: 12.36, 15.19) and at the 95th percentile the posterior estimate was 29.70 days (95% CrI: 26.80, 33.62). The data for the time from infection to first positive test can be seen in Supplementary Fig. 3.

Fig. 2
Posterior distribution for the time from infection to first positive test fit to the data of 86 cases using a gamma distribution.

Left: a violin plot of the mean and standard deviation, with a box and whisker plot including the minima, 1st quartile, median, 3rd quartile, and maxima of the posterior distribution. Right: the cumulative distribution function with 95% credible intervals.

Table 2
Summary statistics of the time from infection to first positive test, fit data from 86 cases using a gamma distribution

---

### Summary benchmarks-full set – 2024 [^e2c40ddc]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — GRADE recommendation categories are specified as: "Strong recommendation (SR): Used when the desirable effects of an intervention clearly outweigh the undesirable effects or clearly do not" and "Discretionary recommendation (DR): Used when the trade-offs are less certain — either because of low-quality evidence or because evidence suggests that desirable and undesirable effects are closely balanced".

---

### Applied machine learning as a driver for polymeric biomaterials design [^a5ee5c89]. Nature Communications (2023). High credibility.

While success with these strategies has required considerable data (900–14,000 polymers), narrowing the scope of the inverse design problem to one class of polymers has proven successful on datasets as small as 171 polymers. These current inverse design approaches have proceeded via similar workflows; thus, investigation into other inverse design algorithms (e.g. generative adversarial networks) represents a way to expand this field. Ultimately, inverse design has the potential to accelerate the discovery of state-of-the-art materials, but is contingent on the creation of larger, more comprehensive datasets and/or the development of methods which can tolerate smaller, potentially imbalanced datasets.

ML for polymer chemistry in medical applications

---

### As-3 [^f4572ab3]. FDA (2025). Medium credibility.

Storage and handling

Store at room temperature (25 °C/77 °F). Avoid excessive heat.

Protect from freezing

---

### Privacy-preserving quality control of neuroimaging datasets in federated environments [^38c3f4b7]. Human Brain Mapping (2022). Medium credibility.

2 METHODS

In the centralized problem of data embedding, we are given the task of producing a dataset of N samples, where, from a dataset, where, such that m ≪ n. For the convenience of visualization, m is usually set to m = 2. The goal is for the embedding Y to give a "faithful" embedding of the data X in the sense that similar points in X will be mapped to close points in Y.

2.1 Background:

In t‐SNE, the distances between the points in Y must be as close to the distances between the corresponding points in X, where preserving the closeness of nearby points is weighted more heavily than far points (Laurens & Geoffrey). In the first step, t‐SNE converts the high‐dimensional Euclidean distances between datapoints into conditional probabilities, called pairwise affinities, that represent similarities between data points (see Algorithm 1). The algorithm takes a scalar parameter called the perplexity ρ. To compute similarity of a datapoint x j to datapoint x i, the algorithm first computes the weight of x j given by a Gaussian kernel centered at x i with bandwidth (variance) σ i (ρ) 2, where it identifies the value of σ i separately for each datapoint by performing a binary search across a range of values until it can match the user‐specified perplexity. The similarity is a conditional probability distribution p j | i formed by renormalizing the N likelihoods into a probability mass function:

---

### The spatial complexity of optical computing: toward space-efficient design [^0f1f6839]. Nature Communications (2025). High credibility.

While this block-diagonalization approach is not expected to be applicable to all computational tasks, we find that relevant tasks solvable by neural networks are amenable to block-diagonalization, leading to a substantial reduction in spatial complexity. To optimize the trade-off between n MZI and the ONNs' performance, we propose a two-step pruning method. First, we include in the loss function an extra term, that penalizes all off-block-diagonal entries. No parameter is removed at this stage. Once the weights are trained, they are loaded into specially designed block-diagonal-structured (BDS) linear layers, where only entries in the diagonal blocks are registered as learnable parameters, while all other entries remain zero and are not updatable. The model with BDS linear layers is further trained on the same dataset to fully adapt to the block diagonal structure (see Methods).

---

### Forecasting life expectancy, years of life lost, and all-cause and cause-specific mortality for 250 causes of death: reference and alternative scenarios for 2016–40 for 195 countries and territories [^16c2d6ac]. Lancet (2018). Excellent credibility.

Our choice of defining the better and worse health scenarios based on the 85th and 15th percentiles of rates of change is arbitrary. We could have selected more ambitious rates of change (ie, the 95th and fifth percentiles), as they also have been used in the past. We selected the 85th and 15th percentiles to balance the considerations of what is possible and replicable across many contexts, as well as the fact that achieving these percentiles for all drivers at once and consistently for 24 years is quite unlikely by the laws of probability. Furthermore, the fact that annual rates of change for a risk factor or an intervention have been achieved in 15% of countries does not mean that success can be replicated everywhere else. However, rates achieved in 15% of countries are clearly not impossible and can serve as a guide for what is achievable.

These scenarios are meant to define the scope of what is generally possible. However, for specific drivers such as tobacco, there might also be value in analysing the effect of much faster rates of progress (ie, the 99th percentile). The case of tobacco is illustrative, since only 20 countries had rates of change in the tobacco SEV between 1990 and 2016 faster than 2% (eg, the USA and Canada), so these tobacco policy success stories do not define the more conservative 85th percentile. We purposefully developed our forecasting models to support more detailed alternative scenarios for specific risks, with the ultimate goal of enabling any user to explore the effect of change on any independent driver at any given rate of change. Computational speed currently precludes such endless possibilities, but our modelling platform supports this future endeavour.

---

### Reconstructing missing complex networks against adversarial interventions [^e779f6dd]. Nature Communications (2019). High credibility.

Introduction

The interaction structure largely determines the operation modes, collective behaviors, and global functionality of complex systems. Consequently, it is crucial to discover the best identification and recovery strategies for sabotaged networks subject to unknown structural interventions or camouflages. Due to its broad applicability, this problem draws interdisciplinary attention in research areas ranging from network science –, social science, system engineering, to ecology, systems biology, network medicine, neuroscience, and network securitycommunities. This challenge raises intense research interest in recent years –. By contrast, very few works discuss and incorporate the statistical influence of the interventions. Most prior works assume that the structural intervention entails a sequence of randomly distributed removals of nodes and links in the network. Under such assumption, constructing an unbiased estimator for the nodal or edge property (but not both at the same time) is shown to be possibleand can be approached by solving a matrix completion problem (e.g. low-rank matrix factorization –, convex optimization, spectral methods). These methods become mathematically infeasible when nodes and edges are simultaneously removed. Extending existing approaches to deal with such problems requires additional information that links the known and unknown part of the network (e.g. group membership, and node similarity) and become obsolete when such information is not available. Alternatively, model-based approaches are adopted in these settings by learning a probabilistic connection between the observed and the latent network structure. These probabilistic links are parametrized and identified in a maximum likelihood sense.

---

### Requirements for fault-tolerant factoring on an atom-optics quantum computer [^e49abf65]. Nature Communications (2013). Medium credibility.

Quantum information processing and its associated technologies have reached a pivotal stage in their development, with many experiments having established the basic building blocks. Moving forward, the challenge is to scale up to larger machines capable of performing computational tasks not possible today. This raises questions that need to be urgently addressed, such as what resources these machines will consume and how large will they be. Here we estimate the resources required to execute Shor's factoring algorithm on an atom-optics quantum computer architecture. We determine the runtime and size of the computer as a function of the problem size and physical error rate. Our results suggest that once the physical error rate is low enough to allow quantum error correction, optimization to reduce resources and increase performance will come mostly from integrating algorithms and circuits within the error correction environment, rather than from improving the physical hardware.

---

### Changes in semantic memory structure support successful problem-solving and analogical transfer [^5231072f]. Communications Psychology (2024). Medium credibility.

Introduction

In our daily life, we constantly deal with problems, ranging from the most mundane (e.g. what to cook for dinner given the ingredients at our disposal), to professional activities (e.g. how to reorganize our current plans to meet a new deadline), up to major societal challenges (e.g. how to find innovative solutions against global warming). How do we find new solutions to problems? While the ability to solve problems is a critical skill for adapting to new situations and innovating, the mechanisms underlying the problem-solving process remain largely unknown.

Among the new problems we face each day, some are well-defined (e.g. playing a jigsaw puzzle). The initial state (i.e. the number of independent pieces) and goal state (i.e. assembling the pieces so it looks like the picture model) are clear, and the solver can apply a set of operations (i.e. interlocking the pieces as a function of their shape) to reach the goal. However, for many of our problems (e.g. organizing work activities during the COVID-19 pandemic), the problem space is ambiguous. No heuristics or existing rules could be applied to transform the initial state into the goal state. Such "ill-defined" problemsthus require additional mental processes, which have been tightly linked to creative thinking –. Ill-defined problem-solving (or creative problem-solving) is often referred to as insight solving, where the solution comes to mind suddenly and effortlessly, with a "Eureka" phenomenon –. According to the Representational Change Theory, solving such problems involves restructuring the initial problem mental representational space, which presumably entails combining elements related to the problem in a new way. In theory, restructuring allows one to change perspective, reframe the problem, or escape its implicitly imposed constraints, leading to creative associations. For instance, consider the following problem: "A man walks into a bar and asks for a glass of water. The bartender points a shotgun at the man. The man says, 'Thank you', and walks out". The problem is ill-defined because the path to finding the solution is to be discovered, and the goal state is vague. Solving this problem first requires asking the right question: in which context would a shotgun and a glass of water help somebody? Rather than relying on obvious associations (e.g. a glass of water is related to thirst), solvers must fill the missing link between the relevant elements of the problem (a shotgun induces fear, and fear can be a remedy for hiccups, as can drinking a glass of water). Hence, restructuring the initial representation of a given problem would allow one to see this link and find its solution.

---

### Standards of care in diabetes – 2025 [^31be65a6]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for hypoglycemia, more specifically with respect to screening for cognitive impairment, ADA 2025 guidelines recommend to simplify diabetes treatment plans as much as possible for patients with cognitive impairment and tailor to minimize the risk of hypoglycemia.

---

### Computing high-degree polynomial gradients in memory [^e06f9829]. Nature Communications (2024). High credibility.

Fig. 2
Conjunctive normal form gain computation.

a Considered 4 th -degree CNF Boolean function, i.e. 4-SAT problem. b – e The main idea of the gain computing shows (b) crossbar memory array implementation and (c – e) three in-memory computing operations. The approach is similar to the polynomial pseudo gradient computation. Specifically, sums of clause literals are first computed similarly to the inverted monomial approach (Supplementary Fig. S4a) in the forward vector-by-matrix multiplication pass, as shown in panel (c). The sum values are compared to clause orders in backward steps (d) and (e) to identify break and make type clauses. Unit inputs, that can be scaled by clause factor when clauses in CNF are weighted (e.g. used in weighted SAT problems), are applied for the identified break and make clauses in the backward vector-by-matrix multiplication. The results are then properly gated at the periphery to compute make and break values for each variable. The corresponding variable gains (i.e. negative pseudo partial derivatives) are found by subtracting the outputs of two backward passes, as shown in panel (a). Values shown in blues correspond to x 1 = 1, x 2 = 0, x 3 = 1, x 4 = 0, i.e. the same example of assignment as in Fig. 1. Also note that the considered 4-SAT problem is equivalent to the polynomial function in Fig. 1a when assuming a 1 = 1, a 2 = −1, a 3 = 1, and a 4 = − 1.

Finally, another variation (Supplementary Fig. S5) is due to the use of crossbar arrays based on active, three-terminal memory devices, such as 1T1R (Supplementary Fig. S1d) or floating gate (Supplementary Fig. S1e) memory. An additional "gate" signal in such crossbar arrays (Supplementary Fig. S5a) can be used to condition the dot-product terms, which in turn allows for computing the make-and-break backward passes using a single crossbar in a single step (instead of two as shown in Supplementary Fig. S4e) for high-throughput gain computation of CNF-form Boolean functions - see Supplementary Note 5 for details.

---

### Estimated therapy costs and downstream cost consequences of iBASIS-video interaction to promote positive parenting intervention vs usual care among children displaying early behavioral signs of autism in Australia [^e1921bcc]. JAMA Network Open (2023). High credibility.

Break-Even Cost and Return on Investment

We estimated that the cost of iBASIS-VIPP to the third-party payer (NDIS) would be offset by downstream savings at age 5.3 years, or 4 years after delivery of the preemptive intervention. By age 13 years, we estimated a savings to the third-party payer of A $3.08 (US $3.08) for each A $1.00 (US $1.00) invested in iBASIS-VIPP program delivery (A $15 826 divided by A $5131).

Sensitivity Analysis

Results of the 1-way sensitivity analysis are reported in Table 4, and findings of the probabilistic sensitivity analysis are shown in the Figure. The largest impact on estimated NPV cost (savings) was clinical trial outcome, varying from an extra cost of A $10 214 (US $7180 [95% CL]) per child to a cost savings of A $33 431 (US $23 502 [−95% CL]). The NPV estimate was also sensitive to the model period. Modeling to age 17 years (18th birthday) increased the cost savings to A $19 979 (US $14 045). Plausible changes in all other attributes had a smaller impact on estimated NPV cost.

Figure.
Probabilistic Sensitivity Analysis of Estimated Net Present Value Cost Savings per Child for the iBASIS–Video Interaction to Promote Positive Parenting Intervention vs Usual Care

On the x-axis, negative values indicate additional cost and positive values indicate cost savings. Dollars are expressed as Australian dollars. The vertical dashed line indicates the break-even point.

From the probabilistic sensitivity analysis, we estimated an 89% likelihood that NPV is at least 0 — that is, downstream cost savings at least equal to intervention cost. This means there was an 88.9% chance that iBASIS-VIPP would deliver costs savings (or no net cost impost) for the NDIS, the dominant third-party payer. The likelihood of generating at least any specified cost savings (read off the x-axis) is described by the y-axis (Figure). For example, we estimated a 74.2% likelihood that NPV cost savings were at least A $5000 (US $3515) (Figure). Further details of the modeling are provided in eAppendices 3 to 11 in Supplement 2.

---

### Using health data to highlight milestones: a cookbook for non-profit program managers [^e727f412]. CDC (2012). Medium credibility.

Web-based Injury Statistics Query and Reporting System (WISQARS) — fatal injury data include "Geography: national and by state" and "Under Intent of the Injury select Unintentional", with report options that provide "Cost of Injury Reports: Select Deaths, then generate the medical and work loss costs for each type of injury", "Fatal Injury Mapping: goes down to county level", and "Fatal Injury Report: generates number of deaths, crude rate, and age-adjusted rate"; the YPLL (Years of Potential Life Lost) option "calculates Years of Potential Life Lost based on life expectancy of 65–85 years".

---

### Standards of care in diabetes – 2025 [^ef149bc3]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for diabetic nephropathy, more specifically with respect to pediatric patients, ADA 2025 guidelines recommend to determine the eGFR at the time of diagnosis and annually thereafter.

---

### Global strategy for asthma management and prevention [^d042f862]. GINA (2024). High credibility.

Global Strategy for Asthma Management and Prevention — decision point: Does asthma become uncontrolled when treatment is stepped down? YES: if asthma symptoms become uncontrolled or an exacerbation occurs when high-dose treatment is stepped down, the diagnosis of severe asthma has been confirmed; restore the patient's previous dose to regain good asthma control, and refer to a specialist or severe asthma clinic, if possible, if not done already. NO: if symptoms and exacerbations remain well controlled despite treatment being stepped down, the patient does not have severe asthma; continue optimizing management.

---

### Guidelines for prevention and treatment of opportunistic infections in HIV-infected adults and adolescents: recommendations from CDC, the National Institutes of Health, and the HIV Medicine Association of the Infectious Diseases Society of America [^783e8234]. MMWR: Recommendations and Reports (2009). Medium credibility.

Pneumococcal vaccination — HIV infection: Administer 1 dose of 13-valent pneumococcal conjugate vaccine (PCV13) followed by 1 dose of 23-valent pneumococcal polysaccharide vaccine (PPSV23) at least 2 months later, then administer a second dose of PPSV23 at least 5 years after the first dose, and if the most recent PPSV23 was before age 65 years, at age 65 years or older administer another dose of PPSV23 at least 5 years after the last PPSV23 dose.

---

### Global strategy for the diagnosis, management, and prevention of chronic obstructive pulmonary disease (2025 report) [^d800692f]. GOLD (2025). High credibility.

COPD follow-up checklist — instructions outline a structured process for follow-up encounters. Identify dates, diagnosis and whether the follow-up is being done in-person, by phone or remotely. Review baseline symptoms and whether there have been changes in dyspnea, cough, sputum volume and color (from least to most purulent: mucus; mucopurulent; purulent), and identify maintenance pharmacological and non-pharmacological treatment and whether the patient is observing treatment as prescribed. For COVID-19, assess whether the patient has any symptoms and would need to be tested and have at hand local numbers for referral to testing and treatment; if the patient has already been tested identify when the results will be obtained or whether the result was positive or negative and, if positive, whether a follow-up test is planned and dates; verify precautions (face masks, hand washing, social distancing, or shielding if necessary). In the action plan section, describe if the patient has a written action plan, See example of an action plan from the Living well with COPD program.(801), whether education for the plan has been done, whether the plan includes a prescription to be self-administered at home or requires calling a contact person / physician to obtain the prescription, and when it was used the last time and if used appropriately. Record recent admissions and ER visits, dates and where they took place. Go over self-management behaviors, covering what is pertinent to the patient treatable traits (dyspnea and/or exacerbation), and describe whether strategies are integrated in daily life (yes), not at all, and whether the patient is unsure "cannot tell". Identify with the patient the main issues of the call. Up to a maximum of 3 items that can be covered for the duration of the call. Finalize by describing the interventions done during the remote visit, the ones to be put in place and agreed by the patient, the plan, including whether the patient needs to be referred to other services, healthcare professionals, etc., and when the next follow-up will take place (describe whether will it be in-person or remote).

---

### Guidelines for the prevention and treatment of opportunistic infections in adults and adolescents with HIV [^93b40bc4]. HIV.gov (2025). High credibility.

HIV pneumococcal vaccination — previously received PCV13 and PPSV23: If < 65 years when received dose of PPSV23, administer PCV21 or PCV20 0.5 mL IM x 1 at least 5 years after the last pneumococcal vaccine (CIII). If ≥ 65 years when received dose of PPSV23, no further doses of PPSV23 are required, and shared decision-making is recommended regarding administration of PCV21 or PCV20 for adults aged ≥ 65 years who have completed both PCV13 and PPSV23; if given, administer at least 5 years after last pneumococcal vaccine dose (CIII). PPSV23 is no longer recommended as preferred booster dose for patients who previously started the vaccine series.

---

### Pregnancy at age 35 years or older: ACOG obstetric care consensus no. 11 [^a8349cf7]. Obstetrics and Gynecology (2022). High credibility.

ACOG Obstetric Care Consensus — pregnancy at age 35 years or older — defines the focus as pregnancy with anticipated delivery at age 35 years or older and notes that this is an arbitrary threshold, with some risks not influencing outcomes until later ages (ie, 40 years and older). Historically, advanced maternal age has been defined as women who are 35 years or older at estimated date of delivery, and to promote precision the document will use phrasing such as "pregnancy with anticipated delivery at [a specific age or age range] or older". For risk stratification, recent studies commonly divide ages 35 years and older into 5-year increments: 35–39 years, 40–44 years, 45–49 years, and 50 years and older.

---

### Global strategy for asthma management and prevention [^ffa193f1]. GINA (2024). High credibility.

Box 3–5 — Treating potentially modifiable risk factors: Allergen exposure if sensitized — consider trial of simple avoidance strategies if there is evidence for their effectiveness (see p.61); consider cost; consider step up of asthma treatment if exposure is unavoidable; and consider adding SLIT in symptomatic HDM-sensitive adults or adolescents with partly-controlled asthma despite ICS, provided FEV1 is > 70% predicted.

---

### Standards of care in diabetes – 2025 [^ba548edd]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for diabetes mellitus type 1, more specifically with respect to screening for PAD, ADA 2025 guidelines recommend to obtain screening for PAD with ankle-brachial index testing in asymptomatic patients with diabetes aged ≥ 65 years and microvascular disease in any location, foot complications, or any end-organ damage from diabetes if PAD diagnosis would change management.

---

### Colorectal cancer screening and prevention [^390b568a]. American Family Physician (2025). High credibility.

Regarding screening and diagnosis for colon cancer, more specifically with respect to indications for screening, high-risk individuals, family history, AAFP 2025 guidelines recommend to obtain CRC screening in patients with ≥ 1 first-degree relatives with CRC or adenomatous polyps, starting at 40 years of age or 10 years before the age of the youngest relative at the time of their diagnosis.

---

### Past-future information bottleneck for sampling molecular reaction coordinate simultaneously with thermodynamics and kinetics [^3e726f35]. Nature Communications (2019). High credibility.

Variational inference and neural network architecture

Typically, both the encoder P (χ | X) and the decoder P (X Δ t | χ) can be implemented by fitting deep neural networksto data in form of time-series of X. Our work stands out in three fundamental ways to typical implementations of the information bottleneck principle and in general of artificial intelligence (AI) methods to sampling biomolecules –. First, we use a stochastic deep neural network to implement the decoder P (X Δ t | χ), but use a simple deterministic linear encoder P (χ | X) (see Fig. 1). The simple encoder ensures that the information bottleneck or RC we learn is actually physically interpretable, which is notably hard to achieve in machine learning. On the other hand, by introducing noise in the decoder, we can control the capacity of the model to ensure that the neural network can delineate useful feature from useless information instead of just memorizing the whole dataset. Second, now that our encoder is a simple linear model, we completely drop the complexity term in Eq. (1) and set γ = 0. Due to a reduced number of variables, this leads to a simpler and more stable optimization problem. Finally, the rare event nature of processes in biomolecules makes it less straightforward to use of information bottleneck/AI methods for enhanced sampling. Here we develop a framework on the lines of ref. which makes it possible to maximize the objective function in Eq. (1) through the use of simulations that are progressively biased using importance sampling as an increasingly accurate information bottleneck variable is learnt.

---

### EAU guidelines on chronic pelvic pain [^a3a6957c]. EAU (2025). High credibility.

Regarding screening and diagnosis for interstitial cystitis/bladder pain syndrome, more specifically with respect to diagnosis, EAU 2025 guidelines recommend to diagnose patients with symptoms according to the EAU definition, after primary exclusion of specific diseases, with primary BPS by subtype and phenotype.

---

### X-linked hypophosphatemia management in adults: an international working group clinical practice guideline [^86938644]. The Journal of Clinical Endocrinology and Metabolism (2025). High credibility.

Regarding screening and diagnosis for X-linked hypophosphatemia, more specifically with respect to differential diagnosis, XLH-IWG 2025 guidelines recommend to obtain an evaluation of other possible causes of hypophosphatemia considering the differential.

---

### Global strategy for asthma management and prevention [^66b4d55d]. GINA (2024). High credibility.

Asthma strategy implementation — essential elements required to implement a health-related strategy enumerate "Steps in implementing an asthma strategy into a health system" that include "Develop a multidisciplinary working group", "Assess the current status of asthma care delivery, outcomes e.g., exacerbations, admissions, deaths, care gaps and current needs", "Select the material to be implemented, agree on main goals, identify key recommendations for diagnosis and treatment, and adapt them to the local context or environment", "In treatment recommendations, consider environmental issues (planetary health) in addition to patient health", "Identify barriers to, and facilitators of, implementation", and "Select an implementation framework and its component strategies". The plan continues with "Develop a step-by-step implementation plan:" to "Select target populations and evaluable outcomes, and specify data coding requirements (if relevant)", "Identify local resources to support implementation", "Set timelines", "Distribute tasks to members", "Evaluate outcomes", and "Continually review progress and results to determine if the strategy requires modification".

---

### The value of understanding feedbacks from ecosystem functions to species for managing ecosystems [^8044a087]. Nature Communications (2019). High credibility.

Ecosystem dynamics and transition probabilities

The ecosystem dynamics are captured in the transition probability matrix in MDP. Let P be the transition probability matrix representing the dynamics of the system from time step t to time step t + 1.represents the conditional probability of the ecosystem transitioning from state x t to x t +1 given action a t is implemented at time t. We assume that species j could be present or absent at each time step. This transition probability is also conditional on the baseline probability of survival of species j, the feedback strength α (the percentage of the ecosystem function going back to a species), the predation strength b, the feedback structure f and the food web matrix M representing the prey-predator interactions of our system. To model this transition probability, we assume that, knowing the state at time t, x t, the state of species j at time t + 1 is independent of the state of the other species at time t + 1. So we can define the transition probability P as the product of J individual species' transition probabilities:

Survival probability of a species will increase with the number of extant preys N prey (j, x t, M) and ecosystem function available N EF (j, x t, f), and will decrease with the number of extant predators N predator (j, x t, M). We assume that N prey (j, x t, M), N EF (j, x t, f), and N predator (j, x t, M) are maximum at the initial time step where all species are present (i.e. x t = x 0 = [1,1,1,1]). Formally, we defined the transition probability when species j is not under protection (a t ≠ j) as the product of four terms:

In this way, under the most favourable condition where species j has no predator, no prey loss and receive maximum level of ecosystem function, the above equation reduces to its baseline probability of survival. However, species j survival probability will decrease when at least one of the following three events happen — prey loss, predator presence, or insufficient functional support (see 'Using Markov Decision Processes to model species dynamics and protection actions effects' in the Supplementary Methods).

---

### Time spent in prior hospital stay and outcomes for ventilator patients in long-term acute care hospitals [^b1177046]. BMC Pulmonary Medicine (2021). Medium credibility.

Table 3
Relationship between Time to LTACH and Outcomes

Our estimation of ventilator weaning probability using IV yielded that an additional day spent in STACH after endotracheal intubation is associated with 11.6% reduction in the odds of weaning off the ventilator in LTACH (Table 3). If we define ventilator weaning as weaning off of ventilator and being alive for at least 7 days post discharge from LTACH, the odds of weaning decreases by 14.3% for each additional day in STACH after endotracheal intubation. When we exclude patients discharged from LTACH to hospice, we find that an additional day in STACH is associated with a 13.4% reduction in odds of weaning off ventilator at LTACH.

Because the models are non-linear, the effects of TTL on outcomes vary at different points in the TTL distribution. To better understand the relationship between TTL and the probability of weaning from ventilator at LTACH, we examined the average predicted probability of weaning if patients were discharged to an LTACH at 13, 18, and 24 days after endotracheal intubation, which correspond to the 25 th, 50 th, and 75 th percentile of the TTL distribution in the study cohort (Table 4). Based on instrumental variable estimation results, the average predicted probability of weaning is 54.5% at the median time to LTACH of 18 days post endotracheal intubation. The average adjusted probability of weaning decreases by 2.5 percentage points to 52.0% at 19 days. The average adjusted probability of being weaned decreases by 27 percentage points from 66.7 to 39.7% when time to LTACH increases from 13 to 24 days. We consistently observed reductions in the average adjusted probability of weaning with an increase in time to LTACH under alternative definitions of weaning. For example, when weaned patients are defined as those who wean from the ventilator at LTACH and remain alive for at least 7 days post discharge from LTACH, the probability of weaning decreases by 31 percentage points from 65.1 to 34.1% between 13 and 24 TTL days. When the study cohort excludes patients discharged to hospice, the average predicted probability of weaning decreases by 29.7 percentage points from 65.1 to 35.4% between 13 and 24 TTL days.

---

### Global strategy for asthma management and prevention [^340986fc]. GINA (2024). High credibility.

Childhood asthma remission — research needs note that clinical questions about remission off treatment in children focus on risk factors for asthma persistence and recurrence (including clinical, pathological, and genetic factors), the effect of risk reduction strategies on the likelihood of remission, whether monitoring after remission to allow early identification of asthma recurrence improves outcomes, and whether progression to persistent airflow limitation can be prevented. Clinical questions about remission on treatment (e.g., in children with severe asthma treated with biologic therapy) include whether inhaled anti-inflammatory therapy can be down-titrated.

---

### Summary benchmarks-full set – 2024 [^c9abe316]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — GRADE evidence quality ratings are defined for forming recommendations for care as follows: "Good quality (GQ): Further research is very unlikely to change our confidence in the estimate of effect", "Moderate quality (MQ): Further research is likely to have an important impact on our confidence in the estimate of effect and may change the estimate", and "Insufficient quality (IQ): Further research is very likely to have an important impact on our confidence in the estimate of effect and is likely to change the estimate; any estimate of effect is very uncertain".

---

### Using machine learning to estimate unobserved COVID-19 infections in north America [^f740e30a]. The Journal of Bone and Joint Surgery: American Volume (2020). Medium credibility.

Background

The detection of coronavirus disease 2019 (COVID-19) cases remains a huge challenge. As of April 22, 2020, the COVID-19 pandemic continues to take its toll, with > 2.6 million confirmed infections and > 183,000 deaths. Dire projections are surfacing almost every day, and policymakers worldwide are using projections for critical decisions. Given this background, we modeled unobserved infections to examine the extent to which we might be grossly underestimating COVID-19 infections in North America.

Methods

We developed a machine-learning model to uncover hidden patterns based on reported cases and to predict potential infections. First, our model relied on dimensionality reduction to identify parameters that were key to uncovering hidden patterns. Next, our predictive analysis used an unbiased hierarchical Bayesian estimator approach to infer past infections from current fatalities.

Results

Our analysis indicates that, when we assumed a 13-day lag time from infection to death, the United States, as of April 22, 2020, likely had at least 1.3 million undetected infections. With a longer lag time-for example, 23 days-there could have been at least 1.7 million undetected infections. Given these assumptions, the number of undetected infections in Canada could have ranged from 60,000 to 80,000. Duarte's elegant unbiased estimator approach suggested that, as of April 22, 2020, the United States had up to > 1.6 million undetected infections and Canada had at least 60,000 to 86,000 undetected infections. However, the Johns Hopkins University Center for Systems Science and Engineering data feed on April 22, 2020, reported only 840,476 and 41,650 confirmed cases for the United States and Canada, respectively.

Conclusions

We have identified 2 key findings: (1) as of April 22, 2020, the United States may have had 1.5 to 2.029 times the number of reported infections and Canada may have had 1.44 to 2.06 times the number of reported infections and (2) even if we assume that the fatality and growth rates in the unobservable population (undetected infections) are similar to those in the observable population (confirmed infections), the number of undetected infections may be within ranges similar to those described above. In summary, 2 different approaches indicated similar ranges of undetected infections in North America.

Level Of Evidence

Prognostic Level V. See Instructions for Authors for a complete description of levels of evidence.

---

### Efficient learning of ground and thermal States within phases of matter [^c7a60b05]. Nature Communications (2024). High credibility.

We consider two related tasks: (a) estimating a parameterisation of a given Gibbs state and expectation values of Lipschitz observables on this state; (b) learning the expectation values of local observables within a thermal or quantum phase of matter. In both cases, we present sample-efficient ways to learn these properties to high precision. For the first task, we develop techniques to learn parameterisations of classes of systems, including quantum Gibbs states for classes of non-commuting Hamiltonians. We then give methods to sample-efficiently infer expectation values of extensive properties of the state, including quasi-local observables and entropies. For the second task, we exploit the locality of Hamiltonians to show that M local observables can be learned with probability 1 - δ and precision ε using < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:mi > N < /mml:mi > < mml:mo ≥ < /mml:mo > < mml:mi > O < /mml:mi > < mml:mfenced > < mml:mrow > < mml:mi > log < /mml:mi > < mml:mfenced > < mml:mrow > < mml:mfrac > < mml:mrow > < mml:mi > M < /mml:mi > < /mml:mrow > < mml:mrow > < mml:mi > δ < /mml:mi > < /mml:mrow > < /mml:mfrac > < /mml:mrow > < /mml:mfenced > < mml:msup > < mml:mrow > < mml:mi > e < /mml:mi > < /mml:mrow > < mml:mrow > < mml:mi > polylog < /mml:mi > < mml:mrow > < mml:mo > (< /mml:mo > < mml:mrow > < mml:msup > < mml:mrow > < mml:mi > ε < /mml:mi > < /mml:mrow > < mml:mrow > < mml:mo > - < /mml:mo > < mml:mn > 1 < /mml:mn > < /mml:mrow > < /mml:msup > < /mml:mrow > < mml:mo >) < /mml:mo > < /mml:mrow > < /mml:mrow > < /mml:msup > < /mml:mrow > < /mml:mfenced > < /mml:math > samples - exponentially improving previous bounds. Our results apply to both families of ground states of Hamiltonians displaying local topological quantum order, and thermal phases of matter with exponentially decaying correlations.

---

### Challenges and opportunities for high-quality battery production at scale [^92389e3d]. Nature Communications (2025). High credibility.

The first consideration is inspection philosophy (Fig. 6a). At a high level, two inspection philosophies are full inspection (100% sampling rate) and sampling-based inspection (< 100% sampling rate). In full inspection, an inspection test is used as an in-process pass/fail check. Assuming the test is accurate, full inspection obviously prevents defective cells from continuing downstream. Full inspection is often suitable for inexpensive diagnostics where inspecting all or most cells is achievable, such as in-line vision, but this approach may add too much operating cost for expensive tests. In contrast, the philosophy of sampling-based inspection is to use the insights from inspection tests to root cause issues and estimate the escape rate of defective cells. Sampling-based inspection strategies have been studied and tested for nearly a century and can be quite sophisticated. A core assumption of sampling-based inspection is that cell production issues can be traced to one or a couple of suspect process steps or equipment, which is often but not always the case. As a result, careful sampling, monitoring, and analysis can be used to pinpoint many cell failure issues. This approach is often suitable for more expensive diagnostics where 100% detection would add an unacceptably high operating expense. For sampling-based detection, rapid analysis, feedback, and response are essential to ensure that the insights prevent a small issue from intensifying. In other words, the quality team must remain vigilant to prevent a defective process from remaining defective for days (as opposed to being resolved in hours).

---

### Understanding the infection severity and epidemiological characteristics of mpox in the UK [^3bebf608]. Nature Communications (2024). High credibility.

After modelling the ascertainment rate, we then adjust the observed case time series for the estimated ascertainment rates to produce the infection incidence time series. We use the estimated time delay distributions for the time to detection and hospitalisation to temporally fit the estimated incidence and observed hospitalisations. The model calculates the expected number of admissions for each exposure group based on the estimated temporal incidence. The temporal probabilities for the ascertainment of the infections in each exposure subgroups are then adjusted iteratively based on how well the expected hospitalisations match the observed hospitalisations. If the estimated incidence aligns with the observed hospitalisations, it indicates that the model is capturing the dynamics of incidence accurately. The adjustment for the ascertainment probabilities aims to find a balance where the model-predicted number of infections corresponds closely to the hospitalisations. Therefore, the model is employing a feedback loop, adjusting ascertainment probabilities to achieve a coherent relationship between estimated incidence, expected hospitalisations, and observed hospitalisations. In essence, iteratively optimising to enhance the precision of its predictions in response to real-world hospitalisation outcomes. This iterative process refines the model's understanding of how infections are ascertained and recorded in the context of hospitalisations rather than simply using estimated cumulative totals. This allows us to gain further insight into the ascertainment rates and consequently the IHR.

We estimated the modelled IHR to be 4.13% (95% CrI: 3.04, 5.02), compared to the overall sample CHR with binomial uncertainty of 5.10% (95% CrI: 4.38, 5.86). The posterior distribution of the IHR, the overall ascertainment rate, the estimated total number of infections, and the estimated total number of non-ascertained infections can be seen in Fig. 5. The modelling estimates that 74.65% (95% CrI: 55.78, 86.85) of infections over the period analysed in the UK were ascertained (Table 5).

Fig. 5
The posterior density for several key parameters of interest: the infection hospitalisation risk, the overall ascertainment rate, the total number of infections and the total non-ascertained infections.

---

### Mechanical metamaterials and beyond [^6aa01675]. Nature Communications (2023). High credibility.

AI-driven inverse design and prediction

Data-driven techniques have been recently used for inverse design of mechanical metamaterials and optimizing their complex microstructures –. Traditional experimental, theoretical and computational research paradigms have encountered technical bottlenecks in design, analysis and fabrication of mechanical metamaterials due to the vast design space. Applications of the data-driven methods, particularly AI-based approaches, in mechanical metamaterials are mainly in the two directions of performance prediction and inverse design, as shown in Fig. 3c. AI is used to describe the complex relationships between inputs (e.g. material and structural level parameters) and outputs (e.g. mechanical characteristics and beyond). AI models have been recently developed to assess the structural properties of mechanical metamaterials, such that to address the technical challenges of fabricability in industrialized fabrication, complexity in microstructural validation, designability, and optimization in performance control.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^943345bf]. Journal of the American College of Cardiology (2025). High credibility.

Figure 3 — health opportunity cost approach to US cost-effectiveness thresholds reports 50000 simulations with varying input values, with 23902/50000 (48%) thresholds < $100000/QALY and 7006/50000 (14%) thresholds > $150000/QALY; the base-case estimate is $104,000/QALY in 2019 US dollars with a (95% UI, $51,000–$209,000/QALY), and updating to 2024 US dollars suggests a cost-effectiveness threshold of $117000 per QALY.

---

### Improved machine learning algorithm for predicting ground state properties [^0c6418ab]. Nature Communications (2024). High credibility.

Theorem 1

(Sample and computational complexity). Givenand a training data setof sizewhere x ℓ is sampled from an unknown distributionandfor any observable O with eigenvalues between −1 and 1 that can be written as a sum of geometrically local observables. With a proper choice of the efficiently computable hyperparameters δ 1, δ 2, and B, the learned function h * (x) = w * ⋅ ϕ (x) satisfieswith probability at least 1 − δ. The training and prediction time of the classical ML model are bounded by.

The output y ℓ in the training data can be obtained by measuringfor the same observable O multiple times and averaging the outcomes. Alternatively, we can use the classical shadow formalism –, that performs randomized Pauli measurements on ρ (x ℓ) to predictfor a wide range of observables O. We can also combine Theorem 1 and the classical shadow formalism to use our ML algorithm to predict ground state representations, as seen in the following corollary. This allows one to predict ground state propertiesfor a large number of observables O rather than just a single one. We present the proof of Corollary 1 in Supplementary Section 3B.

Corollary 1

Givenand a training data setof sizewhere x ℓ is sampled from an unknown distributionand σ T (ρ (x ℓ)) is the classical shadow representation of the ground state ρ (x ℓ) using T randomized Pauli measurements. For, then the proposed ML algorithm can learn a ground state representationthat achievesfor any observable O with eigenvalues between −1 and 1 that can be written as a sum of geometrically local observables with probability at least 1 − δ.

We can also show that the problem of estimating ground state properties for the class of parameterized Hamiltoniansconsidered in this work is hard for non-ML algorithms that cannot learn from data, assuming the widely believed conjecture that NP-complete problems cannot be solved in randomized polynomial time. This is a manifestation of the computational power of data studied in. The proof of Proposition 1 inconstructs a parameterized Hamiltonian H (x) that belongs to the family of parameterized Hamiltonians considered in this work and hence establishes the following.

---

### Defining the time-limited trial for patients with critical illness: an official American Thoracic Society workshop report [^d28d917f]. Annals of the American Thoracic Society (2024). High credibility.

Time-limited trials — framing to promote collaboration outlines in Table 3 that they "Should Be" "Framed by uncertainty", "Transparent", "Individualized and adaptable", "Iterative", "Oriented to patients' goals and priorities", "Trials of high-quality, standard-of-care therapies", and "A collaborative process between patients, surrogates, and clinicians", and "Should Not Be" "Framed by notions of "poor prognosis", "Coercive", "Prescriptive or binding", "Time pressured or finite", "Oriented to one specific outcome", "Lower quality care", or "Determined exclusively by clinicians".

---

### Crowding in the emergency department: challenges and recommendations for the care of children [^1412c806]. Pediatrics (2023). High credibility.

Solutions to emergency department (ED) crowding — Patient safety is emphasized, and consensus language indicates the problem cannot be solved within the ED alone; specifically, "At the heart of ED crowding is the concern for patient safety", "there is a consensus that the solution to ED crowding cannot be solved within the ED alone", and "ED crowding at a local level is a hospital system issue, requiring hospital leadership to recognize the input, throughput, and output variables that may be adjustable outside of the ED". Complementing this systems framing, the page states that ED crowding "is multifactorial and cannot be solved solely by those who manage and provide care in EDs".

---

### A computational toolbox for the assembly yield of complex and heterogeneous structures [^827ee6b2]. Nature Communications (2023). High credibility.

Discussion

Many structural and functional properties of biological systems rely on heterogeneous self assembly. Increasingly, building blocks with highly tuned geometries and interactions can be made experimentally, presenting an opportunity to design complex self-assembling systems with properties of living matter –. However, for increasing numbers of individual components, the yield of the desired product is diluted by an exponential number of off-target assemblies. Even if each of the off-target assemblies forms with a lower probability than the desired product, the multiplicity of these off-target assemblies can cause a yield catastrophe in which the desired product forms with negligible yield. An efficient method for predicting assembly yield would enable practitioners to design self-assembling systems of increasing complexity while mitigating this yield catastrophe.

In this work, we have developed a combined analytical/computational approach to calculate the equilibrium assembly yields of complexes comprised of heterogeneous building blocks with arbitrary geometries. Our approach involves two novel calculations: (i) computing the partition function of each (predefined) candidate complex (see Eq. (9)) and (ii) given these partition functions, calculating equilibrium assembly yields (see Eqs. (10) and (11)). The classical statistical mechanics methodology we put forth can be realized by modern automatic differentiation techniques enabled by advances in machine learning. While previously, the efficient calculation of the entropic parts of the partition function has been intractable except for simple cases, these advances make this calculation possible even for structures with complicated building blocks or with a large number of different components.

Our theoretical framework provides significant conceptual and practical improvements over simulation-based methods, the only other general-purpose method for yield prediction. Calculations via our approach require a drastically reduced computational cost compared to simulations (i.e. seconds or minutes vs. hours, weeks, or months). Moreover, simulation-based methods can be fraught with additional difficulties such as issues in simulating finite concentrations or efficiently sampling from equilibrium distributions. Lastly, since our method involves the direct calculation of the partition function, our approach can also be used to compute other statistical properties of an equilibrium thermodynamic system (e.g. heat capacity, energy fluctuations) without complicated modifications to the calculation. Indeed, since the gradient of a solution to Eqs. (10) and (11) can be computed implicitly, our method could enable inverse design with respect to related thermodynamic equilibrium properties.

---

### Flortaucipir f-18 (Tauvid) [^da74189e]. FDA (2024). Medium credibility.

2.4 Image Display

The goal of the read is to identify and locate areas of flortaucipir activity in the neocortex that are greater than the background activity (background activity is defined as up to 1.65-fold the measured cerebellar average). For optimal display, select a color scale with a rapid transition between two distinct colors and adjust the scale so that the transition occurs at the 1.65-fold threshold. Examine the posterolateral temporal (PLT), occipital, parietal, and frontal regions bilaterally. Neocortical activity in either hemisphere contributes to image interpretation. Activity in white matter or regions outside the brain does not contribute to image interpretation. To help identify the PLT, consider subdividing the temporal lobe into four quadrants as instructed below. Activity in the anterior and medial temporal lobe does not contribute to image interpretation of a positive TAUVID pattern.

Image Display and Orientation

Display images in the transverse, sagittal, and coronal planes. Reorient images to remove head tilt in the transverse and coronal plane. Use a sagittal slice just off the midline to align the inferior frontal and inferior occipital poles in the horizontal plane.

Select and Adjust the Color Scale

To create a visual threshold for positivity:

Draw a region of interest around the cerebellum in the transverse plane.
Select the plane to go through the cerebellum at the maximum cross-sectional area of the cerebellum.
Record the mean activity or cerebellar counts (MCC). The region of interest should be drawn with the scan in gray scale and in the transverse plane as seen in the example in Figure 1.

Figure 1: Example of Cerebellar Region of Interest

Select a color scale for image display that has a rapid transition between two distinct colors in the general range of 25% to 60% of maximum intensity.
Set the upper contrast value (UCV) of the color scale. Use the following formula to set the visual threshold of 1.65 x MCC to match the rapid transition in the color scale:

If additional guidance on image display is needed, refer to the TAUVID User Guide for PET Image Display available by request from the manufacturer.

---

### Global strategy for asthma management and prevention [^121c9240]. GINA (2024). High credibility.

After starting initial controller treatment, review the child's response after 2–3 months or earlier depending on clinical urgency, consult Box 4–12 for recommendations for ongoing treatment and other key management issues, and step down treatment once good control has been maintained for 3 months.

---

### Environmental assessment and exposure control of dust mites: a practice parameter [^b9767f1a]. Annals of Allergy, Asthma & Immunology (2013). Medium credibility.

House dust mites — water balance and environmental thresholds — are about 75% water by weight and maintain water balance through uptake of water vapor when relative humidity (RH) is at least approximately 65%, with water loss when RH decreases below approximately 55%. The critical lowest humidity is temperature dependent and ranges from 55% to 75% RH over 15°C to 35°C, with D pteronyssinus and D farinae appearing to thrive best at 75% to 80% RH and 25°C to 30°C (77–86°F). These mites can maintain a positive water balance only at an ambient RH of at least 50% and live deep within soft substrates where moisture is retained; it is not uncommon to find thousands of mites in a single gram of house dust.

---

### An event-based architecture for solving constraint satisfaction problems [^7f341a4d]. Nature Communications (2015). Medium credibility.

Constraint satisfaction problems are ubiquitous in many domains. They are typically solved using conventional digital computing architectures that do not reflect the distributed nature of many of these problems, and are thus ill-suited for solving them. Here we present a parallel analogue/digital hardware architecture specifically designed to solve such problems. We cast constraint satisfaction problems as networks of stereotyped nodes that communicate using digital pulses, or events. Each node contains an oscillator implemented using analogue circuits. The non-repeating phase relations among the oscillators drive the exploration of the solution space. We show that this hardware architecture can yield state-of-the-art performance on random SAT problems under reasonable assumptions on the implementation. We present measurements from a prototype electronic chip to demonstrate that a physical implementation of the proposed architecture is robust to practical non-idealities and to validate the theory proposed.

---

### Identifying domains of applicability of machine learning models for materials science [^196a563d]. Nature Communications (2020). High credibility.

Fig. 1
Domains of applicability of three 2d-models of a noisy third-degree polynomial.

Three different models, linear (top), radial basis function (rbf, center), and polynomial (poly, bottom), are shown approximating the same distribution of two independent features x 1 ~ N (0, 2) and x 2 ~ N (0, 2), and the target property, where N (μ, ϵ 2) denotes a normal distribution with mean μ and standard deviation ϵ. Test points are plotted in 3d plots against the prediction surface of the models (color corresponds to absolute error) where the DA is highlighted in gray. The distributions of individual errors for the DA (gray) and globally (black) are shown in the 2d plots of each panel with the mean error (solid) and the 95th percentile (95 perc./dashed) marked by vertical lines. Note that the global error distribution of the linear model has a considerably long tail, which is capped in the image.

---

### An additive gaussian process regression model for interpretable non-parametric analysis of longitudinal data [^03f8e4da]. Nature Communications (2019). High credibility.

Methods

Notation

We model target variables (gene/protein/bacteria/etc;) one at a time. Let us assume that there are P individuals and there are n i time-series measurements from the i th individual. The total number of data points is thus. We denote the target variable by a column vector y = (y 1, y 2. y N) T and the covariates by X = (x 1, x 2. x N), where x i = (x i 1, x i 2. x id) T is a d -dimensional column vector and d is the number of covariates. We denote the domain of the j th variable byand the joint domain of all covariates is. In general, we use a bold font letter to denote a vector, an uppercase letter to denote a matrix, and a lowercase letter to denote a scale value.

Gaussian process

GP can be seen as a distribution of nonlinear functions. For inputs, GP is defined aswhere μ (x) is the mean and k (x, x ′) is a positive-semidefinite kernel function that defines the covariance between any two realizations of f (x) and f (x ′) bywhich is called kernel for short. The mean is often assumed to be zero, i.e. and the kernel has parameters θ, i.e. k (x, x ′| θ). For any finite collection of inputs X = (x 1, x 2. x N), the function values f (X) = (f (x 1), f (x 2). f (x N)) T have joint multivariate Gaussian distributionwhere elements of the N -by- N covariance matrix are defined by the kernel [K X, X (θ)] i, j = k (x i, x j | θ).

We use the following hierarchical GP modelwhere π (ϕ) defines a prior for the kernel parameters (including), is the noise variance, and I is the N -by- N identity matrix. For a Gaussian noise model, we can marginalise f analytically

---

### Dextrose (dextrose 10%) [^a9a7d390]. FDA (2025). Medium credibility.

CHARACTER

This product is colorless or almost colorless clear liquid.

---

### AGA clinical practice guideline on management of gastroparesis [^6a5fe598]. Gastroenterology (2025). High credibility.

Regarding screening and diagnosis for gastroparesis, more specifically with respect to differential diagnosis, AGA 2025 guidelines recommend to consider evaluating potential coexisting conditions that may contribute to symptoms when treating patients with GP.

---

### Physics-informed learning of governing equations from scarce data [^d1208a9b]. Nature Communications (2021). High credibility.

Harnessing data to discover the underlying governing laws or equations that describe the behavior of complex physical systems can significantly advance our modeling, simulation and understanding of such systems in various science and engineering disciplines. This work introduces a novel approach called physics-informed neural network with sparse regression to discover governing partial differential equations from scarce and noisy data for nonlinear spatiotemporal systems. In particular, this discovery approach seamlessly integrates the strengths of deep neural networks for rich representation learning, physics embedding, automatic differentiation and sparse regression to approximate the solution of system variables, compute essential derivatives, as well as identify the key derivative terms and parameters that form the structure and explicit expression of the equations. The efficacy and robustness of this method are demonstrated, both numerically and experimentally, on discovering a variety of partial differential equation systems with different levels of data scarcity and noise accounting for different initial/boundary conditions. The resulting computational framework shows the potential for closed-form model discovery in practical applications where large and accurate datasets are intractable to capture.

---

### Identifying domains of applicability of machine learning models for materials science [^72c4ba76]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ν that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by σ lin (x 1, x 2) ≡ −0.3 ≤ x 1 ≤ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: σ rbf ≡ −3.3 ≤ x 11 ≤ 3.1 ∧ x 2 ≤ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA σ poly (x 1, x 2) ≡ −3.5 ≤ x 2 ≤ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### Summary benchmarks-full set – 2024 [^6f68d3ad]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — use and ethics note states that "The PPPs are intended to serve as guides in patient care, with greatest emphasis on technical aspects", that "true medical excellence is achieved only when skills are applied in a such a manner that the patients' needs are the foremost consideration", and that "The Academy is available to assist members in resolving ethical dilemmas that arise in the course of practice".

---

### Fine-mapping and selective sweep analysis of QTL for cold tolerance in drosophila melanogaster [^16ae7da8]. G3 (2014). Low credibility.

Figure 2
Evidence of positive selection and candidate SNPs in the 124-kb region under deletion Df(1)ED6906. (A) Likelihood (CLR) profile along the 124-kb on the X-chromosome using SNP data of two pooled European D. melanogaster from the Netherlands and France. Two significance thresholds are depicted. The solid line corresponds to the average of the top 1% CLR values for the X-chromosome in Europe and the dashed red line represents the significance threshold from simulations of equivalent subgenomic regions. (B) Model-based F ST values for 7316 SNPs from a dataset including two European and five African samples (see Materials and Methods). Top SNPs (above the false discovery rate of 5%) are marked with position and a thin continuous line across panels.

Using the same CRL test, we also analyzed in detail, the second highly significant deletion uncovered by the quantitative complementation test (between coordinates 7.85 and 7.98 Mb). However, we could not find evidence for selective sweeps (see).

Because a large fraction of the region of low variation in Europe (particularly the coding regions of genes, and; see the gene model below) (Figure 2B) contains no or very little variation, the CLR tests cannot be used to identify the targets of selection. Instead, we utilized genetic differentiation between African and European populations to obtain model-based F ST coefficients for each SNP within the 124-kb region of interest (Figure 2B). We considered SNP data from seven populations along a gradient across Africa and Europe: South Africa, Southeast Africa, Rwanda, Cameroon, Ethiopia, France, and the Netherlands. Using BayeScan, we obtained a pattern of F ST values from a dataset of 7316 SNPs with an average F ST of 0.2621 and revealed four outlier SNPs that show the highest differentiation across populations at an FDR of 5% (Figure 2B). These four SNPs are located within the 40-kb-long fragment enriched for SNPs showing significant CLR values between positions 65,000 and 105,000 (Figure 2A). The 65-kb-long and 19-kb-long flanking regions to the left and to the right of the 40-kb fragment, respectively, are enriched for SNPs showing below-average F ST values (Figure 2B). However, none of these SNPs with low differentiation across populations is significant at the 5% FDR.

---

### Clinical genetic counseling and translation considerations for polygenic scores in personalized risk assessments: a practice resource from the National Society of Genetic Counselors [^28d8464d]. Journal of Genetic Counseling (2023). High credibility.

Polygenic score (PGS) development and validation for clinical use — Ordering providers should have a basic understanding of PGS test development and downstream testing restrictions in order to support appropriate test utilization, and there are no best practices or standards for developing a PGS, such that PGS for the same condition are often not the same. Regulatory requirements for genetic tests to make it to market often require demonstrated analytic and clinical validity, and for PGS and integrated PGS risk models analytic and clinical validity may be restricted to the model parameters used during development. From genome-wide association studies (GWAS) to PGS, the basic steps are to (1) conduct a GWAS to identify variants in a training/source data set, (2) select disease-associated variants that meet prespecified thresholds and combine these to create a PGS, and (3) validate the PGS using an independent dataset; GWAS evaluates associations using "common" variants with "common" population prevalence (≥ 1% frequency in a population).

---

### Next generation reservoir computing [^398d0bfe]. Nature Communications (2021). High credibility.

Reservoir computing is a best-in-class machine learning algorithm for processing information generated by dynamical systems using observed time-series data. Importantly, it requires very small training data sets, uses linear optimization, and thus requires minimal computing resources. However, the algorithm uses randomly sampled matrices to define the underlying recurrent neural network and has a multitude of metaparameters that must be optimized. Recent results demonstrate the equivalence of reservoir computing to nonlinear vector autoregression, which requires no random matrices, fewer metaparameters, and provides interpretable results. Here, we demonstrate that nonlinear vector autoregression excels at reservoir computing benchmark tasks and requires even shorter training data sets and training time, heralding the next generation of reservoir computing.

---

### Causal networks for climate model evaluation and constrained projections [^afadf6f0]. Nature Communications (2020). High credibility.

PCMCI causal discovery method

PCMCI is a time series causal discovery method further described in reference. Commonly, causal discovery for time series is conducted with Granger causality, which is based on fitting a multivariate autoregressive time series model of a variable Y on its own past, the past of a potential driver X, and all the remaining variables' past (up to some maximum time delay τ max). Then X Granger-causes Y if any of the coefficients corresponding to different time lags of X is non-zero (typically tested by an F -test). As analysed in reference, Granger causality, due to a too high-model complexity given finite sample size, has low detection power for causal links (true-positive rate) if too many variables are used and for strong autocorrelation, both of which are relevant in our analysis. PCMCI avoids conditioning on all variables by an efficient condition-selection step (PC) that iteratively performs conditional independence tests to identify the typically few relevant necessary conditions. In a second step, this much smaller set of conditions is used in the momentary conditional independence (MCI) test that alleviates the problem of strong autocorrelation. In general, both the PC and MCI step can be implemented with linear or nonlinear conditional independence tests. Here, we focus on the linear case and utilise partial correlation (ParCorr). A causal interpretation rests on a number of standard assumptions of causal discovery as discussed in reference, such as the Causal Markov assumption, Faithfulness, and stationarity of the causal network over the time sample considered. The free parameter of PCMCI is the maximum time delay τ max, here chosen to include atmospheric timescales over which we expect dependencies to be stationary. The pruning hyper-parameter pc- α in the PC condition-selection step is optimised using the Akaike information criterion (among pc- α = 0.05, 0.1, 0.2, 0.3, 0.4, 0.5). PCMCI yields a p -value (based on a two-sided t -test) for every pair of components at different lags. We defined links in the networks using a strict significance level of 10–4 in the main paper. However, very similar results are found for other more relaxed or even stricter significance levels; as demonstrated extensively in the Supplementary Material.

---

### Expectancy-related changes in firing of dopamine neurons depend on hippocampus [^e9b75611]. Nature Communications (2024). High credibility.

Surprisingly, largely similar firing changes were evident in dopamine neurons recorded in the HC-lesioned rats, particularly on forced choice trials (Fig. 4c, d). A direct comparison of the data between control and lesioned groups (group x value x early/late x trial) revealed a significant main effect of value (F 1,108 = 19.8, p = 2.1e-5) and significant interactions between value x early/late (F 1,108 = 12.7, p = 5.5e-4), value x trial (F 4,432 = 3.87, p = 0.0042) and value x early/late x trial (F 4,432 = 2.64, p = 0.33). However, there were no significant main effects nor interactions involving group (F's < 1.5, p 's > 0.10). Thus, dopamine neurons recorded from rats with ipsilateral HC lesions showed normal changes in firing in response to presentation of the differently-valued cues on forced choice trials. The only exception to this was in activity to the free-choice cue in later trials, which was significantly lower than that to the high-valued cue (ANOVA, F 1,65 = 10.24, p = 0.002) and significantly higher than that to the low-valued cue (ANOVA, F 1,65 = 6.85, p = 0.011; Supplementary Fig. 2b).

Hippocampal lesions disrupt hierarchical segregation of states available in different blocks

The neural results show that HC is necessary for normal error signaling by VTA dopamine neurons; dopamine neurons recorded in rats with ipsilateral HC lesions failed to signal prediction errors to changes in reward. However, the same neurons showed roughly normal error signals to the presentation of the predictive cues in our task. To better understand what hippocampus might contribute to this surprising pattern of results, we developed two different temporal-difference reinforcement learning models to describe the task and then monitored the error output of the models caused by changes in several parameters in an attempt to recreate these neural findings — one model inspired by our prior results on the effects of OFC lesions and a second model inspired by the failure of this model, which employed a state space better reflecting the complexity of the task, particularly the block structure.

---

### Emerging opportunities and challenges for the future of reservoir computing [^3548f511]. Nature Communications (2024). High credibility.

Reservoir computing originates in the early 2000s, the core idea being to utilize dynamical systems as reservoirs (nonlinear generalizations of standard bases) to adaptively learn spatiotemporal features and hidden patterns in complex time series. Shown to have the potential of achieving higher-precision prediction in chaotic systems, those pioneering works led to a great amount of interest and follow-ups in the community of nonlinear dynamics and complex systems. To unlock the full capabilities of reservoir computing towards a fast, lightweight, and significantly more interpretable learning framework for temporal dynamical systems, substantially more research is needed. This Perspective intends to elucidate the parallel progress of mathematical theory, algorithm design and experimental realizations of reservoir computing, and identify emerging opportunities as well as existing challenges for large-scale industrial adoption of reservoir computing, together with a few ideas and viewpoints on how some of those challenges might be resolved with joint efforts by academic and industrial researchers across multiple disciplines.

---

### Machine learning enables completely automatic tuning of a quantum device faster than human experts [^fb7ba5a8]. Nature Communications (2020). High credibility.

Variability is a problem for the scalability of semiconductor quantum devices. The parameter space is large, and the operating range is small. Our statistical tuning algorithm searches for specific electron transport features in gate-defined quantum dot devices with a gate voltage space of up to eight dimensions. Starting from the full range of each gate voltage, our machine learning algorithm can tune each device to optimal performance in a median time of under 70 minutes. This performance surpassed our best human benchmark (although both human and machine performance can be improved). The algorithm is approximately 180 times faster than an automated random search of the parameter space, and is suitable for different material systems and device architectures. Our results yield a quantitative measurement of device variability, from one device to another and after thermal cycling. Our machine learning algorithm can be extended to higher dimensions and other technologies.

---

### Using qualitative comparative analysis and theory of change to unravel the effects of a mental health intervention on service utilisation in Nepal [^9ba2f03b]. BMJ Global Health (2018). Medium credibility.

During the process of applying QCA together with ToC, we learnt several lessons. First, using a conceptual framework, such as ToC, to guide QCA is important to identify the outcome and condition sets as well as providing a theoretical basis for how to treat logical remainders. Second, the number of conditions needs to be kept to around 5–7 to avoid the 'limited diversity problem'which occurs when there are too many conditions in relation to the number of cases which prevent Boolean minimisation of the truth tables and result in causal pathways with many conditions and low coverage. This is similar to the problem in frequentist statistics of including too many explanatory variables in a multivariate linear regression without an adequate sample size. To avoid this problem, we aggregated similar indicators prior to data analysis, for example, proportion of trained prescribers and non-prescribers and the number of trained home-based care workers and FCHVs. Other approaches to avoid limited diversity include (1) a comprehensive approach where all conditions are included iteratively in the analysis until the most parsimonious solution is reached or (2) a two-step approach where an analysis of ths distal conditions is conducted first, followed by the proximal conditions. Third, data calibration is one of the key steps in QCA and should be conducted with a theoretical understanding of the values chosen, that is, that 1 is fully in the set, 0 is fully out of the set and that 0.5 is neither in nor out of the set. The calibration of data should be conducted using thresholds that are meaningful to the programme. Although tools exist in STATA and other software to calibrate continuous variables, the distribution of the resulting calibrated data should be checked to ensure that the interpretation of the data is in line with the theoretical understanding. Because of the iterative nature of QCA, it is possible and may be necessary to recalibrate the data following the analysis.