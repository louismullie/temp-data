# Medical Question & Answer

**Sample ID**: e02b9e4e-a6aa-bad0-4a62-4129423d171b
**Dataset Index**: 372199

---

## Question

2・111 n is a positive integer, A is a set of subsets of the set {1,2, …, n}, such that no element in A contains another element in A, find the maximum number of elements in A.

---

## Answer

> Let's see… What do we have here? The user is asking for the maximum size of a family of subsets of {1, 2, …, n} such that no subset in the family contains another subset in the family. Let's break this down step-by-step. First, I need to think about how to state this as a mathematical problem and identify the relevant theorem. Then, I should verify the statement of Sperner's theorem and its proof sketch to ensure correctness. Next, I will consider small cases to build intuition. After that, I should confirm the general formula and its asymptotics. Finally, I will double-check that the provided references indeed support the claim and note any nuances or exceptions I need to keep in mind.

> Let me first confirm the problem framing. This is the classic question of the largest antichain in the Boolean lattice of subsets of an n-element set, where an antichain is a family in which no set is a subset of another. This is precisely what Sperner's theorem addresses, so I should anchor on that result to avoid reinventing the wheel.

> Wait, let me verify the exact statement of Sperner's theorem. Sperner's theorem asserts that the maximum size of such an antichain is the binomial coefficient C(n, ⌊n/2⌋), which counts the subsets of size ⌊n/2⌋; if n is even, this is C(n, n/2), and if n is odd, both C(n, (n−1)/2) and C(n, (n+1)/2) achieve the same maximum size, so either middle layer suffices.

> I will now examine the proof sketch to ensure I'm not missing a subtlety. The standard proof uses the LYM inequality or a symmetric chain decomposition to show that the sum of the fractions |Aₖ|/C(n, k) over all k is at most 1, where Aₖ is the number of sets of size k in the antichain; since C(n, k) is maximized at k = ⌊n/2⌋, the largest possible antichain cannot exceed C(n, ⌊n/2⌋), and equality is achieved by taking all subsets of that size, confirming the bound is tight.

> Hold on, let's not jump to conclusions; I should sanity-check with small cases. For n = 1, the maximum is 1 (either the empty set or {1}), and C(1, 0) = C(1, 1) = 1, which matches. For n = 2, the maximum is 2, achieved by either the pair {{1}, {2}} or the pair {{1}, {1,2}} wait, that's wrong; {{1}, {1,2}} violates the antichain condition because {1} is a subset of {1,2}, so the correct antichain of size 2 is {{1}, {2}} or {{1}, {2}}, and C(2, 1) = 2, which fits. For n = 3, the maximum is 3, achieved by all 2-element subsets, and C(3, 1) = C(3, 2) = 3, again consistent. For n = 4, the maximum is 6, achieved by all 2-element subsets, and C(4, 2) = 6, which checks out.

> Next, I should review the general formula and its asymptotics. The exact answer is C(n, ⌊n/2⌋), and by Stirling's approximation this behaves asymptotically as [2ⁿ/√(πn/2)], which illustrates that the largest antichain comprises a substantial fraction of all subsets, roughly on the order of [2ⁿ/√n].

> But wait, what if the question is implicitly asking for a construction or an algorithmic perspective rather than just the cardinality? I need to ensure I state the constructive part clearly: the extremal antichain is simply the collection of all subsets of size floor(n/2), and no larger antichain exists, which is the complete content of Sperner's theorem in this context.

> I should double-check that the references I have at hand actually support this. The documents provided do not include a primary source for Sperner's theorem itself, so I should flag that while the mathematical claim is standard and can be found in combinatorics texts, the specific citations here are lacking; nonetheless, the reasoning and small-case checks align with the canonical result, and I will explicitly note the absence of a direct citation in this packet to maintain accuracy.

> Final answer: The maximum number of elements in such a set A is the binomial coefficient C(n, ⌊n/2⌋), that is, the number of subsets of {1, 2, …, n} of size ⌊n/2⌋. For even n this is C(n, n/2), and for odd n both C(n, (n−1)/2) and C(n, (n+1)/2) achieve the same maximum.

---

The maximum number of elements in a set (A) of subsets of ({1, 2, …, n}) such that no element in (A) contains another is given by **Sperner's theorem** as the binomial coefficient C(n, ⌊n/2⌋). This is the size of the largest antichain in the power set of ({1, 2, …, n}), achieved by taking all subsets of size ⌊n/2⌋.

---

## Sperner's theorem

Sperner's theorem states that the maximum size of an antichain in the power set of a set with (n) elements is C(n, ⌊n/2⌋). An antichain is a family of subsets where no subset is contained in another. The theorem also shows that this maximum is achieved by taking all subsets of size ⌊n/2⌋.

---

## Proof sketch

The proof uses the **LYM inequality**, which states that for an antichain (A),

Σ(k = 0..n) [|Aₖ|/C(n, k)] ≤ 1,

where (Aₖ) is the set of subsets in (A) of size (k). Since C(n, k) is maximized at (k = ⌊n/2⌋), the largest possible antichain has size C(n, ⌊n/2⌋), achieved when (A) consists of all subsets of size (⌊n/2⌋).

---

## Examples

- **For (n = 1)**: C(1, 0) = 1 or C(1, 1) = 1.
- **For (n = 2)**: C(2, 1) = 2.
- **For (n = 3)**: C(3, 1) = 3 or C(3, 2) = 3.
- **For (n = 4)**: C(4, 2) = 6.

---

## Asymptotic behavior

Using Stirling's approximation, the maximum size behaves as

C(n, ⌊n/2⌋) ~ [2ⁿ/√(πn/2)].

---

The maximum number of elements in (A) is C(n, ⌊n/2⌋).

---

## References

### Identifying outbreak risk factors through case-controls comparisons [^117Tyv8y]. Communications Medicine (2025). Medium credibility.

Box 1 Minimally sufficient sets of potential causal factors

Consider any case in which the objective is to study what leads to a positive versus negative outcome (i.e. an outbreak versus contained infection). Suppose also that a set, can be described, of elements 1 throughof possible causal factors that we hypothesize might together determine the outcome. A sufficient set, is any subsetthat, taken together, leads to a positive outcome. Mathematically, ifwhere each element, then a sufficient set is a subsetsuch thatthen the studied outcome is guaranteed to be positive. Note, that this means that the satisfaction of each element of the specific set is required for an outbreak, not the total number of elements in the sufficient set. Further, M is minimally sufficient if for any, then the studied outcome is guaranteed to be negative (i.e. a subset is minimally sufficient if the exclusion of any one of the elements changes the outcome from positive to negative). Note that one minimally sufficient set cannot be a subset of another, but there is otherwise no restriction on overlap between minimally sufficient sets, nor on the number of elements contained in the minimally sufficient set. For example, ifthen we could have, andall as minimally sufficient sets. If those are all of the minimally sufficient sets contained within S, then a maximal set of minimally sufficient subsets exists, and, therefore, a full description of all the ways in which we could have a positive outcome.

Influential contributors as case controls

Evidence-based epidemiological analysis of outbreak risk focuses on factors that may limit or foster a disease outbreak (Box 2). Abiotic factors such as temperature, humidity, and precipitation can define the distribution and abundance of a pathogen. Contextual or geographic factors capturing socioeconomic conditions, aspects of the built environment, or human demography and behavior (e.g. lockdowns) may also influence pathogen transmission risk.

---

### The definition and measurement of heterogeneity [^111CA6t8]. Translational Psychiatry (2020). Medium credibility.

A definition of heterogeneity and measurement in categorical systems

A system's heterogeneity is the degree to which it diverges from a state of perfect conformity. A "system" has three components (Fig. 1a): (A) a set, "event space", or "sample space"of distinct potential observations which one can also think of as "elements", "partitions", "groups", or "categories", (B) a measure of distancebetween any two potential elements x i and x j in, and (C) a measure of abundance of each element in. If the abundance function sums to 1 over the entire set, then the abundance measure is a probability distribution.

Fig. 1
Illustration of system components and influence on heterogeneity of samples.

Panel a depicts a categorical system comprised of a set four categories (equivalently "elements" or "partitions") connected by undirected edges whose lengths are proportional to the distance between categories. In this case, the distances between categories are all equal (symmetric), and the within-category distance is 0 (as evident in the depicted distance matrix). These properties define the set as categorical. The size of the nodes represents their relative abundance, which is also shown in the corresponding bar chart. Panel b demonstrates samples from nine categorical systems with varying number of categories (2, 3, and 4) and varying levels of inequality in the abundance distribution. Systems in the upper row have the highest level of inequality in abundance, whereas the systems shown in the bottom row have perfectly even abundance distributions. Together, these plots demonstrate that heterogeneity increases with both (A) increases in the number of categories and (B) more evenly distributed abundance across categories.

In this section, we consider only categorical systems since they are an excellent starting point for developing intuition about the measurement of heterogeneity. Categorical systems are effectively defined by the following distance function (the discrete metric):Like the case–control assumptions, this function states that (A) there are no inter-individual differences within a category, and (B) all categories are maximally different, thus meaning no two categories are more similar than any other two.

---

### Segmentation-free measurement of locomotor frequency in caenorhabditis elegans using image invariants [^113KY4bj]. G3 (2024). Medium credibility.

Ensemble feature invariants for determining locomotor frequency

We define an ensemble vector combining the 8 Hu's moment invariants with the N MSER-derived invariants, forming a vector withelements:

Next, we create an invariant ensemble matrixthat includes the time dependence of the vector sequence:

Each component variablewas measured on different scales and with widely different ranges in magnitude. We thus standardized them via z -score normalization:

where T denotes the number of video frames.

Following the standardization of the invariant ensemble time sequence, we computed its autocorrelation, defined by

whereand.

We used the following peak-finding method to determine the locomotor period: (1) In, identify local maxima (peaks, including the one at), and local minima (troughs). (2) Concatenate the peak list and the trough list, and reorder the combined list ascendingly based on the extrema's τ values, which gives uswhereand. Here, all the peaks and troughs should automatically alternate in the list according to the Intermediate Value Theorem. (3) Within the list, compute all the identified local extrema's topographic prominence, a measure of how much the peak or trough stands out relative to other peaks or troughs ("). (4) From the list, find the family P of the least number of subsets of E that meet the following criteria: (a) The family P does not contain empty set or any set with only one element; (b) If the family P is not empty, each set in the family P does not contain any element with the prominence value smaller than 0.14; (c) If the family P is not empty, each set in the family P must contain at least 2 elements with their subscripts being adjacent integers; (d) If the family P is not empty, the intersection of any 2 sets is empty; (e) The union of all sets in P is equal to, where X is the set containing all elements with their prominence values smaller than 0.14. (5) After identifying the family P, the worm's locomotor frequency in the targeted video is defined as: (a) If the family P is empty, is defined to be 0; (b) If the family P is not empty, we definewhere the set, andis calculated as, wheredenotes the cardinality of the set.

---

### An analytical theory of balanced cellular growth [^116KbC9U]. Nature Communications (2020). High credibility.

To make the presentation concise, our development of GBA assumes (i) that all proteins contribute to growth by acting as catalysts or transporters; (ii) that there is a 1-to-1 correspondence between proteins and reactions; (iii) that proteins are not used as reactants; (iv) that all catalysts are proteins; and (v) that cells are optimized for growth. Supplementary Note 2 outlines how to remove these simplifications.

Due to the explicit inclusion of the major physicochemical constraints on cellular growth, GBA models promise to provide a mechanistic understanding of microbial resource allocation and physiology at a depth not achievable with alternative optimization-based models. In principle, exploitation of the balance equations (Eq. (10)) may allow the numerical optimization of cellular systems of realistic size, encompassing hundreds of protein and reactant species. However, several challenges must be overcome before GBA models can be used to make detailed quantitative predictions of genome-scale resource investment and physiology.

The first challenge is the identification of the set of active reactions in a given cellular state, leading to the active stoichiometric matrix A. The optimal state is an EFM of the linearized problem (Theorem 9), and thus a direct way to achieve this would be to compute all EFMs of the full stoichiometric matrix compatible with balanced growth (i.e. all support-minimal subsets of reactions that are capable of producing their own reactants plus protein), to apply GBA to each of them, and to then select the EFM resulting in the highest growth rate. While this approach works well for small, schematic models as those in refs.– and may be feasible for coarse-grained models with a few dozen reactions, the number of biomass-producing EFMs in genome-scale networks is too large for them to be calculated exhaustively on current computers. As an approximate alternative, one could restrict this analysis to a subset of candidate EFMs, e.g. based on FBA with molecular crowdingand on parsimonious FBA(where fluxes could be scaled by the maximal enzyme turnover rates, k cat) or chosen to represent known physiological states (e.g. yield-maximizing vs. overflow metabolism), or one might analyze only EFMs with a pre-specified maximal number of reactions.

---

### An additive selection of markers to improve diagnostic accuracy based on a discriminatory measure [^114roCBd]. Academic Radiology (2013). Low credibility.

Rationale and Objectives

The estimation of the area under the receiver operating characteristic (ROC) curve (AUC) often relies on the assumption that the truly positive population tends to have higher marker results than the truly negative population. The authors propose a discriminatory measure to relax such an assumption and apply the measure to identify the appropriate set of markers for combination.

Materials and Methods

The proposed measure is based on the maximum of the AUC and 1-AUC. The existing methods are applied to estimate the measure. The subset of markers is selected using a combination method that maximizes a function of the proposed discriminatory score with the number of markers as a penalty in the function.

Results

The properties of the estimators for the proposed measure were studied through large-scale simulation studies. The application was illustrated through a real example to identify the set of markers to combine.

Conclusion

Simulation results showed excellent small-sample performance of the estimators for the proposed measure. The application in the example yielded a reasonable subset of markers for combination.

---

### Focal seizures induce spatiotemporally organized spiking activity in the human cortex [^112WqYka]. Nature Communications (2024). High credibility.

We then created a matrix in rank space for each seizure in the following manner. First, we enumerated, in lexicographic order, the set of all 2-element subsets of micro-electrodes {1, …, n }, yielding the set of tuples p: = {(1, 2), (1, 3), …, (2, 3), (2, 4), …, (n − 1, n)}, of length. We then created a sequence matrix withcolumns, corresponding to the number of tuples in the set p, and k rows, corresponding to the number of combined bursts:Each column of this matrix, therefore, reflects a distinct pair of electrodes rather than a single electrode. We populated each value in each row with either a 0 or a 1, depending on whether the first member of the micro-electrode pair outranked the second in that particular burst. Hence, for each burst i containing a set of timestamps, and each pair of distinct electrode indices (j 0, j 1) ∈ p :This approach forces the dimensionality reduction procedure to consider relative rank information, rather than raw ordinal or continuous data. When considering null versions of sequences, we used the same procedure applied to a random version of a particular set of bursts. In these random sequences, we shuffled MUA timestamps at active electrodes were shuffled. We again in-filled missing values for normalized ranks with random values from the uniform random distribution on, prior to creation of A.

---

### Subset-based method for cross-tissue transcriptome-wide association studies improves power and interpretability [^116L21Xn]. HGG Advances (2024). Medium credibility.

whereis the inverse standard normal cumulative distribution function. We then denotedas the converted z value for a specific gene expression in tissue, as the entire set of tissues (such as all 48 tissues in GTEx v7), as a non-empty subset of tissues in, andas the number of tissues in subset. The transformed z values quantify the evidence of association on a Gaussian scale. Here, tissue-wise correlation implies that, if two tissues are highly correlated, their corresponding z values should be similar. This transformation innately disregards the directionality of the same in order to increase computational efficiency (see below).

Under the null hypothesis that none of the tissues (or contexts) are associated with the trait, we construct a test statistic that maximizes the average association among all possible subsets of tissues.

where, the number of tissues;, the entire set of tissues;, a subset of tissues; and, the number of tissues in subset.

We expect all the significant associations to be converted to positive z values through the above transformation, disregarding the directionality of the association. In fact the above test statistic has been shown to be equivalent to the likelihood ratio test and is a consistent variable selection when all tissues/contexts are equally weighted.

Since there arenon-empty possible subset, calculation of this test statistics is computationally expensive, especially with a large number of reference tissues. To select the subset of tissues more efficiently, we simplified the enumeration process by sorting all z values in decreasing order:
Calculate the test statistics, starting with the largest z value:

---

### MINE: maximally informative next experiment-toward a new GWAS experimental design and methodology [^1141v4BZ]. G3 (2025). Medium credibility.

Each element of the correlation matrix can be obtained from the covariances:

The matrix determinant represents the ellipsoid uncertainty volume, and since both matrices are square and positive semidefinite, then the determinant is computed in the following way from the eigenvalues of the covariance or correlation matrix:

The λ and χ represent an eigenvalue from the covariance and correlation matrices, respectively calculated from the model predictions. These determinants also have the interpretation as a volume completed by the average covariance or correlation between 2 randomly chosen members of the ensemble (Fig. 2b).

Suboptimal algorithms for computing MINE

The objective of this algorithm is to reduce the size of each subset to be analyzed; the size should be a number that divides the total number of accessions to be planted. In the case here, every subset was chosen to be of size 3 because 81 accessions were planted (81 is divisible by 3). When all subsets are analyzed, the top n are chosen until completing the number of accessions to plant. Its description is as follows:

Algorithm 2.
(Suboptimal Algorithm)

As an example, identify all triples of accessions in the list of 343 accessions available in BAP. Compute the MINE score det(D) for each one. Sort from high to low. Pick 81 accessions at the top of the list.

Monte Carlo

This MINE computation algorithm intends to select a subset of size equal to the number of accessions to plant, and the rest of the accessions go into a pool as candidates for later choices (but currently categorized as not for use). After many steps, 1 accession in the subset and 1 accession in the pool are swapped, and the MINE score is calculated to decide if the change is an improvement by the MINE criterion. The algorithm is described as follows:

Algorithm 3.
(Monte Carlo algorithm)

As an example, randomly pull 81 accessions from the pool of 343 accessions in BAP to be used. Score the 81 accessions for their det(D) score. Randomly swap with BAP pool m times using the Metropolis Algorithm 1.

---

### Quantum algorithms for topological and geometric analysis of data [^113ad7gE]. Nature Communications (2016). Medium credibility.

Topological methods for analysis face challenges: a data consisting of n data points possesses 2 n possible subsets that could contribute to the topology. Performing methods of algebraic topology on simplicial complexes eventually requires matrix multiplication or diagonalization of matrices of dimensionto extract topological features at dimension k. For small k, such operations require time polynomial in n; however, to extract high-dimensional features, matrix multiplication and diagonalization lead to problem solution scalings that grow exponentially in the size of the complex. A variety of mathematical methods have been developed to cope with the resulting combinatorial explosion, notably mapping the complex to a smaller complex with the same homology, and then performing the matrix operations on the reduced complex. Even in such cases, the initial reduction must identify all simplices in the original complex, and so can scale no better than linearly in the number of simplices. Consequently, even with only a few hundred data points, creating the persistent homology for Betti numbers at all orders of k is a difficult task. In particular, the most efficient classical algorithms for estimating Betti numbers at order k (the number of k -dimensional gaps, holes and so on), have computational complexity either exponential in k or exponential in n (refs,), so that estimating Betti numbers to all orders scales exponentially in n, and algorithms for diagonalizing the combinatorial Laplacian (that reveal not only the Betti numbers but additional geometric structure) at order k have computational complexity as, where n is the number of vertices in the (possibly reduced) complex. That is, the best classical algorithms for estimating Betti numbers to all ordersand for diagonalizing the full combinatorial Laplacian grow exponentially in the number of vertices in the complex.

---

### A GPU-based computational framework that bridges neuron simulation and artificial intelligence [^117Gqs2H]. Nature Communications (2023). High credibility.

Mathematical scheduling problem

Based on the simulation accuracy and computational cost, we formulate the parallelization problem as a mathematical scheduling problem:

Given a tree T = { V, E } and a positive integer k, where V is the node-set and E is the edge set. Define partition P (V) = { V 1, V 2,… V n }, | V i | ≤ k, 1 ≤ i ≤ n, where | V i | indicates the cardinal number of subset V i, i.e. the number of nodes in V i, and for each node v ∈ V i, all its children nodes { c | c ∈children(v)} must in a previous subset V j, where 1 ≤ j < i. Our goal is to find an optimal partition P * (V) whose computational cost | P * (V)| is minimal.

Here subset V i consists of all nodes that will be computed at i -th step (Fig. 2e), so | V i | ≤ k indicates that we can compute k nodes each step at most because the number of available threads is k. The restriction "for each node v ∈ V i, all its children nodes { c | c ∈children(v)} must in a previous subset V j, where 1 ≤ j < i " indicates that node v can be processed only if all its child nodes are processed.

DHS implementation

We aim to find an optimal way to parallelize the computation of solving linear equations for each neuron model by solving the mathematical scheduling problem above. To get the optimal partition, DHS first analyzes the topology and calculates the depth d (v) for all nodes v ∈ V. Then, the following two steps will be executed iteratively until every node v ∈ V is assigned to a subset: (1) find all candidate nodes and put these nodes into candidate set Q. A node is a candidate only if all its child nodes have been processed or it does not have any child nodes. (2) if | Q | ≤ k, i.e. the number of candidate nodes is smaller or equivalent to the number of available threads, remove all nodes in Q and put them into V * i, otherwise, remove k deepest nodes from Q and add them to subset V i. Label these nodes as processed nodes (Fig. 2d). After filling in subset V i, go to step (1) to fill in the next subset V i+1.

---

### Statistical ensembles without typicality [^1168qHr9]. Nature Communications (2018). Medium credibility.

Maximum-entropy ensembles are key primitives in statistical mechanics. Several approaches have been developed in order to justify the use of these ensembles in statistical descriptions. However, there is still no full consensus on the precise reasoning justifying the use of such ensembles. In this work, we provide an approach to derive maximum-entropy ensembles, taking a strictly operational perspective. We investigate the set of possible transitions that a system can undergo together with an environment, when one only has partial information about the system and its environment. The set of these transitions encodes thermodynamic laws and limitations on thermodynamic tasks as particular cases. Our main result is that the possible transitions are exactly those that are possible if both system and environment are assigned the maximum-entropy state compatible with the partial information. This justifies the overwhelming success of such ensembles and provides a derivation independent of typicality or information-theoretic measures.

---

### Improving the sensitivity of cluster-based statistics for functional magnetic resonance imaging data [^116JNgQY]. Human Brain Mapping (2021). Medium credibility.

1.4 How to combine different test statistics?

A crucial advantage of the randomization framework is that it allows to combine cluster statistics with different sensitivity profiles. A researcher may not know whether to expect small or large clusters. In that case, it is possible to analyze the data using different cluster definitions, for example, by varying the CDT, the neighbor definition, and/or the number of required neighboring voxels. Within the randomization framework, the results can be combined over these different cluster definitions. For each of the different cluster definitions (CDT, neighbor definition, etc.), the randomization step results in a distribution of optimum (i.e. maximum or minimum) cluster magnitudes (size or sum). These randomized optimum cluster magnitudes can each be transformed into p‐values by comparing them to their corresponding randomization distribution (see Figure 3). By definition, and separately for each of the cluster definitions, the probability distribution of these p‐values is uniform (see Figure 3, Step 2). Similarly, each observed cluster magnitude can also be transformed into a p‐value by comparing it to its corresponding randomization distribution. After transforming the cluster‐definition‐specific magnitudes into p‐values, these transformed magnitudes can be meaningfully combined in a single randomization distribution. This is realized by taking the minimum p‐value over all cluster definitions. This min(p) randomization distribution is constructed in a loop over draws from the randomization distribution: for every draw, evaluate which of the cluster definitions (statistics) has the smallest p ‐value, and use the resulting value as a realization of the min(p) randomization distribution. This min(p) randomization distribution is the final distribution that is used for decision‐making: if the observed min(p)‐value is less than theth percentile of the min(p) randomization distribution, then we reject the null hypothesis of statistical independence between biological data and the explanatory variable. By using the min(p) randomization distribution for decision‐making (instead of the cluster‐definition‐specific randomization distributions), we correct for multiple testing (one test per cluster definition).

---

### Real-space observation of ergodicity transitions in artificial spin ice [^113bGCNi]. Nature Communications (2023). High credibility.

Entropy determination

Since artificial spin ice typically only samples thousands of configurations from 2 N potential configurations and temperature resolution is limited, the sampling is inadequate to directly tabulate entropy. Previous studies of artificial spin ice have shown that the conditional entropy of local configurations can effectively upper bound experimental entropy. The process is as follows: a subset of all spins, is chosen to sample the entropy. Within that set, another subset, is selected and will be used as the spins that "reveal" information and provide tighter bounds on the entropy. The conditional entropy, or the "surprise factor" of revealing new information from, is defined as

The experimental spin configurations estimate all probabilities by counting: the probabilities of existing in each of the subset states, the probability when the inner subset, is in a specific state, and the conditional probability that the bigger subset state exists given the smaller subset state. The conditional probability is calculable via Bayes' theorem. Information theory connects conditional entropy with an upper bound on entropy per spin. Specifically, the entropy per bit is upper bounded by the conditional entropy "revealed" by the additional spins in the larger subset divided by the number of additional spins. That is, for the correct σ Λ and σ Γ, n Γ s ≤ S (σ Λ |σ Γ) whereis the number of spins in the inner subset. With this lattice, we elected to choose each vertex as sampling subsets and spin subsets, and sub-subsetsshown in Fig. 3a. The sampling sets individual entropy densities are plotted in Fig. 3b as well as the average of all the entropy densities. Conditional probabilities and the resulting entropy estimations were computed from experimental spin configurations at all temperatures recorded.

---

### Identifying domains of applicability of machine learning models for materials science [^112amy14]. Nature Communications (2020). High credibility.

Table 1
Features used for discovery of domain of applicability (DA) selectors.

The DA optimization and validation can be performed as a by-product from the labels and ML predictions of the test set. However, just as for the ML-model fitting itself, we can only estimate these quantities based on empirical data. For that purpose, it is sensible to also split the test data into two parts: a DA identification set for optimizing the empirical impact and a DA validation set for obtaining an unbiased performance estimate of the identified DA (see Fig. 2 for an illustration of the overall workflow). Technically, the data points withheld in the DA validation set mimic novel independent sample points that can be used to evaluate both the coverage of the DA, as well as, the reduction in model error. As an extension of this, one can also repeat the DA optimization/validation on several splits (cross-validation) to reduce the variance of the coverage and model error estimates and, moreover, to assess the stability of the DA selector elements.

Fig. 2
Workflow for domain of applicability (DA) identification and validation for an ML model.

The DA is described by a selector (σ f) that is comprised of logical conjunctions of a representation space (here symbolized by a single dimension x for simplicity but may be multidimensional). The selector is identified by applying subgroup discovery (SGD) to the individual ML-model errors for subset of test set (DA identification set). An unbiased estimate of the model performance within the DA is obtained on the remaining samples of the test set that were left out of the DA identification (DA validation set).

For ease of notation we assume the DA identification set consists of the first k points of the test set. We end up with the following objective function for the SGD algorithm:where s denotes the number of points in the DA identification set selected by σ and I (σ) = { i: 1 ≤ i ≤ k, σ (x i) = true} denotes the set of selected indices itself. Here, we focus on DA identification based on the relative error, as it is less correlated with the target values than the absolute error. Thus, this choice promotes DAs that contain a representative distribution of target values and, by extension, more distinct and thus more characteristic DAs for the different models (see Supplementary Note 2 for a discussion of the DAs resulting from using the absolute error).

---

### Integrating genomics and metabolomics for scalable non-ribosomal peptide discovery [^112JZ4Zf]. Nature Communications (2021). High credibility.

Table 3
Number of core NRPs of SurugamideAL (assembly line corresponding to cyclic surugamides A–D) according to their adenylation scores.

Only values of s with non-zero number of cores and corresponding to the top 1000 high-scoring core NRPs are shown.

Problem 2. Given an assembly line A and a positive integer N, generate candidateCoreNRPs A (N), defined as all core NRPs of A with adenylation scores at least thresholdScore A (N).

NRPminer follows a graph-theoretic approach to quickly generate candidateCoreNRPs A (N) by using the computed values of numCoreNRPs. Let G (A) be the acyclic directed graph with nodes corresponding to pairs of positive integers i ≤ n and s ≤ maxScore A, such that numCoreNRPs A (i, s) > 0, denoted by. For every node(i = 1, …, n) and everysuch that numCoreNRPs A (i−1, s−S A (i, a)) > 0, there exists a directed edge fromto. Let Source beand let Sink be the set of all nodessuch that thresholdScore A (N). We call each directed path in G (A) from Source to the nodes in Sink as a candidate path of G(A).

Each candidate path of G (A) corresponds to a distinct core NRP of A with adenylation score at least thresholdScore A (N) and vice versa. Therefore, the problem of finding all core NRPs of A with adenylation score at least thresholdScore A (N) corresponds to the problem of finding all candidate paths of G (A). While enumerating all paths with n nodes in a directed acyclic graph can grow exponentially large (as there can be exponentially many such paths), but due to our choice of thresholdScore A (N), the number of candidate paths of G (A) is bound by 10 5 (or N if). NRPminer uses the default value N = 1000. Moreover, n ≤ 20 (only assembly lines made up of up to 20 A-domains are considered) and k 1.200000000000000e+01

---

### Coverage bias in small molecule machine learning [^115R9Vox]. Nature Communications (2025). High credibility.

Subsampling molecular structures

The biomolecular structure set contains 718,097 molecular structures. From this set, we uniformly sampled a subset of cardinality 20,000: Each of the 718,097 molecular structures has exactly the same probability to be drawn; similarly, each subset of cardinality 20,000 has exactly the same probability to be drawn. We later noticed that six of these 20,000 molecular structures are single ions, for example, a single iron ion. These molecular structures resulted in execution errors when computing the MCES, but are also of no interest for our analysis. To this end, we discarded these six structures before computing distances, embeddings, and running times. Consequently, our subsampled set contains 19,994 uniformly sampled molecular structures. The number of nodes n in the subsampled molecular graphs ranges from 1 to 139, with an average of 33.8 nodes (quartiles 20, 28, 40). Recall that we do not consider hydrogen atoms in our computations. The number of bonds (edges without multiplicities) m ranges from 0 to 153, with an average of 35.8 bonds (quartiles 21, 30, 43). The complexity of an instance can be roughly measured by the product m 1 ⋅ m 2, for number of bonds m 1, m 2. This product ranges from 0 to 22,950, with an average of 1279.5 (quartiles 520, 900, 1593). We report quartiles (25%, median, 75%) as our subset contains both very small and very large molecular structures. In the extreme case, a few "molecular structures" consist of a single atom only.

An instance consists of a pair of biomolecular structures. To avoid proliferating running times in our running time evaluation, we uniformly subsampled a set of 20,000 instances. To allow that resulting running times are comparable with other computations in this paper, we used the 19,994 molecular structure from above to generate the instances. We generated 199,870,021 pairs of molecular structures; from this set, we uniformly sampled 20,000 instances. For the subsample, the product m 1 ⋅ m 2 ranges from 0 to 14,124, with an average of 1273.0 (quartiles 525, 910, 1584). The list of molecular structures (both the complete set of biomolecules, the subsample with 19,994 molecular structures, and the 20,000 subsampled structures pairs) is available for download.

---

### Antibiotic-mediated microbial community restructuring is dictated by variability in antibiotic-induced lysis rates and population interactions [^111AKHTN]. Nature Communications (2025). High credibility.

For community formulation and simulations, the number of strains in a population, N was set to 30. Each subpopulation initiates with the same size of 0.01/ N subpopulations at time t = −4. Constant exponential growth rates (G m, i) were assigned to each subpopulation within the range of 0.7–1.5/hr. The lysis rates were assigned by a set of a i and b i to the i- th subpopulation. For no G / L variability, a i and b i were set to be constant (a and b). For x% of G / L variability, x% of a random number was added to the constants. That is, a i and b i were a i = a + x% random number, b i = b + x% random number. Simulations with variability share one random number set (30 random numbers were generated to have mean ~0 and standard deviation ~1). Fractions of each subpopulation at t = −4, 0, and 4 (F I, F G, and F A) were retrieved for enrichment calculation. For non-treated population simulations with dilution factors, we resized the subpopulation by multiplying the dilution factor by all of the endpoint S i after the first phase simulation. All mathematical simulations were performed in MATLAB R2021a, with ode45 and 'NonNegative' option.

The interaction matrix was generated by the published code, which requires the number of subpopulations, the ratio of the negative and positive interactions (50:50 was set in the community simulations), and maximum strength of negative and positive interactions as input of the code. The maximum positive interaction term was set to be half of the maximum negative interactions to reflect the higher negative interactions observed in the same publication. Two sets of uniformly distributed random numbers between 0 and 1 were generated and multiplied by the maximum positive and negative caps, respectively. The output of the code is an N -by-N matrix where the pairwise interactions of j -th subpopulation to the i -th subpopulation are assigned to the elements I ij. Diagonal elements are 0. Note that simulations in Fig. 3c, d share the same interaction matrix, the entire matrix was scaled to greater interaction strength. The same conclusion was drawn in the repeated simulations using a new random interaction matrix (Fig. S 11).

Reporting summary

Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.

---

### Dissection of gene expression datasets into clinically relevant interaction signatures via high-dimensional correlation maximization [^115F7VBa]. Nature Communications (2019). High credibility.

Framework

For the mathematical description of SDCM, we use the language of vector spaces in its established notation known from quantum mechanics.

The complete measured dataset of m genes (or, more generally, features) and n samples (e.g. tumor samples from patients) is represented as a single matrix, whereis the signal space.

The gene space V g is a vector space spanned by m basis genes. Everyhas coordinatesin the gene reference order, whereis the Kronecker delta, i.e. ifand zero otherwise. We use the upper index g to indicate elements of this space. For each sample, all genes have been measured. Thus, samples are points in this vector space spanned by all basis genes. Sample j is represented by the vectorwith expression valuesfor gene indices.

Similarly, the sample space V s is a vector space spanned by n basis samples. Everyhas coordinatesin the sample reference order. The upper index s indicates elements of this space. For each gene, all samples have been measured and hence genes are points in this vector space spanned by all basis samples. Gene i is represented by the vectorwith expression valuesfor sample indices.

Model

In general terms, we aim to detect interactions in subsets of genes that exist in subsets of samples. The size of subsets is initially unknown, the participation intensity in these interactions may vary from gene to gene and from sample to sample, and subsets of different interactions may be overlapping.

We assume that the signal is a linear superposition ofsignature signals E k plus a noise term η:

This superposition model is ambiguous without further constraints. In order to maximize the range of detectable interactions, we decided to only weakly restrict the functional form of the E k. We require that a gene order I k and a sample order J k exist, such that every row and every column of the reordered matrixis monotonic. This signature model can fit any interaction following a curve in the gene space (or sample space) that is monotonic over the signature axis in the same space, as explained in the main text. Formally, this bi-monotonicity of all E k is defined by:whereis the set of all permutations of a finite set X.

To determine signatures, we utilize a maximization principle as additional constraint. As detailed in step 2 below, we iteratively maximize the signature functionalthat quantifies both the signature size and correlations of genes and samples to signature axes based on our measure for interactions defined next.

---

### Detecting heritable phenotypes without a model using fast permutation testing for heritability and set-tests [^111teeTW]. Nature Communications (2018). Medium credibility.

Reducing the number of sampled permutations with SAMC

A major computational hurdle of permutation testing is the potentially large number of random permutations that need to be used in order to estimate small p -values accurately. To cope with this computational burden, we use an efficient p -value evaluation procedure based on the Stochastic Approximation Markov Chain Monte Carlo (SAMC) algorithm. We give an overview of the method and its properties here. For the full description, see Supplementary Note 4.

In the context of heritability testing, we utilize SAMC as follows. Given an estimate H 2, we want to calculate its p -value, i.e. the probability a randomly permuted phenotype obtains a higher heritability estimate. We divide the interval [0,1] to D + 1 intervals, where the interval [0, H 2] is divided into D equally sized intervals, and [H 2,1] is an additional interval. This induces a partitioning of the permutation space to D + 1 subsets. Each subset is the set of permutations of the phenotype for which the estimated heritability value falls in the corresponding interval. Then, the p -value is exactly the size of the subset corresponding to [H 2,1], divided by n !, the number of permutations of size n.

The SAMC algorithm estimates the size (i.e. probability) of each subset in the partition. Starting with an arbitrary initial permutation, each SAMC iteration consists of two steps: (1) Given the current random permutation, sample a new random permutation according to a certain target distribution, using the Metropolis-Hastings (MH) sampling algorithm; (2) Given the subset in which the new permutation falls, update the partition probability estimates, and the target distribution of step 1. The update rule for subset probability estimates follows the stochastic approximation algorithm, which ensures that the estimates can be improved continuously as the simulation goes on. Importantly, the number of random permutations required for convergence is much smaller than the estimated p -value.

Additionally, in order to determine in which subset a permutation falls, we need not calculate the heritability estimate of the respective permuted phenotype. Instead, we can check the derivatives at the endpoints of the interval. If the derivative is positive at the left endpoint and negative at the right endpoint, then we know a maximum exists within that interval. Using the derivative allows us to avoid the heritability estimation step, as before. When the algorithm converges, its estimate for the last subset will be our estimate of the p -value.

---

### BEARscc determines robustness of single-cell clusters using simulated technical replicates [^115K781u]. Nature Communications (2018). Medium credibility.

Estimating the background distribution of BEARscc metrics

To compute null distributions for the stability, promiscuity and score metrics, random clusters were generated with varying numbers of cells m in a data set, where each cell was assigned to an arbitrary reference cluster with size n. We then computed 100 random association matrices by taking each possible cell-to-cell association and assigning a 0 or 1 with equal probability, i.e. a cell was equally likely to associate or not with any cell. The noise consensus matrix was computed from these 100 random association matrices. From the noise consensus, the score, stability, and promiscuity metrics were calculated both per cell and per cluster. This computation was repeated 100 times for each set m and n parameters to describe the null distribution of BEARscc metrics at the cell and cluster level.

Evaluation of BEARscc runtime

To estimate the run-time requirements of BEARscc, a random sampling of 2000 samples and 20,000 genes was taken from the brain whole tissue counts matrix with replacement. This large count matrix was subset to an increasing number of genes (63, 504, 700, 1008, 1800 and 2016) and cells (6, 12, 25, 36, 50, 72) to generate counts matrices with increasing numbers of elements. The BEARscc functions estimate_noiseparameters and simulate_replicates were run on each count matrix on a standard desktop PC (Intel i7 with 2.9 GHz). The microbenchmark R package was used to measure the execution time for each counts matrix subset.

Estimation of cluster number k

To determine the cluster number, k, from the hierarchical clustering of the noise consensus, the resulting dendrogram was cut multiple times to form N clusterings with cluster numbers k = 1 to k = N clusters. The average score metric was computed for each clustering, and k was chosen by taking the k with the maximum average score metric. Evaluating all possible k from 1 to the number of cells in the experiment is computationally expensive and unlikely to be biologically meaningful. In this work, N was capped at 0.1 times the number of cells in the experiment: N = 10 for the brain and blanks control, N = 30 for the murine intestine experiment, and N = 300 for the murine brain data.

---

### Enhanced energy-constrained quantum communication over bosonic gaussian channels [^116x885T]. Nature Communications (2020). High credibility.

Theorem 2 Consider a correlated N-mode thermal state with N = (M, N − M) and such that and let, where M ∈ {1, ⋯, N }. The following two-way quantum state transmission rate can be achieved by using and hybridizing forward and backward strategies :Since x can be any rational number in (0, 1] and the set of rational numbers is a dense subset of the set of real numbers, we have the following improved lower bound for the two-way energy-constrained quantum capacity of a thermal-loss channel:where the maximization is performed subject to 0 ≤ x ≤ 1 and.

The proof of Theorem 2 is given in the Methods section. Our new bound in Eq. (19) is at least as tight as the previous bound in Eq. (16) because the previous bound can be realized by plugging in x = 1 andor x = 0 and. Moreover, we show that in the noisy channel (near-zero capacity) regime, correlated multimode thermal states outperform single-mode thermal states of the same energy and thus our bound is strictly tighter than the previous bound.

To demonstrate that our new bound can be strictly tighter than the previous bound, we take a family of thermal-loss channelswith. Then, we compute the new bound in Eq. (19) for each η = 1 − γ for three different maximum allowed average photon numbers per channel use, i.e.(Fig. 4 a),(Fig. 4 b), and(Fig. 4 c). As can be seen from the top panel of Fig. 4, the coherent information(blue lines) is larger than, equal to, and smaller than the reverse coherent information(yellow lines) for, and, respectively. In all cases, our new bound obtained by using correlated multimode thermal states (red lines) can be strictly tighter than the previous bound in the large loss probability regime, where the two-way quantum capacity almost vanishes. In this regime, the best two-way quantum state transmission rate is achieved by mixing forward (coherent information) and backward (reverse coherent information) strategies, as can be seen from the bottom panels of Fig. 4.

---

### Artificial-intelligence-driven discovery of catalyst genes with application to COactivation on semiconductor oxides [^113xwcYB]. Nature Communications (2022). High credibility.

These requirements are implemented in the following quality functions that are maximized during the search for subgroups. In particular, for OCO-angle minimization we use:and for C–O bond maximization the following quality function was applied:where Y is the whole dataset, Z — a subgroup, s — size (number of data points), min and max – minimal or maximal value of the target property, α g and l g are the gas-phase values of OCO-angle and C–O bond distance, 180° and 1.17 Å, respectively, and θ cut is the Heaviside step function which is equal 1 if all data points in the subgroup satisfy the cutoff condition and 0 otherwise. Thus, larger values of the quality function F (Z) are obtained for those subgroups in which minimal (maximal) value of a target property is close to the maximal (minimal) value of the whole sampling with respect to the gas-phase value of CO 2 molecule. The use of maximum/minimum instead of a median is done to ensure that a target property is optimal for as many members of a subgroup as possible. The gas-phase reference values are usually significantly different from the "chemisorption" subset. Therefore, the term in squared brackets in Eqs. (1) and (2) can noticeably contribute only when the sizes of candidate subgroups are similar.

---

### Six distinct NF κ B signaling codons convey discrete information to distinguish stimuli and enable appropriate macrophage responses [^114xV5bT]. Immunity (2021). Medium credibility.

Channel capacity calculation and signaling codon identification

As there are ~9.3 × 10 16 seven-dimensional combinations of 918 features (Table S3) and each channel capacity calculation takes ~90 s per combination, evaluating channel capacity of all combinations of features would take ~2.3 × 10 15 h (~2.7 × 10 11 years) to compute and is therefore is computationally infeasible. To narrow the search space, we utilized a feature selection approach. Since the channel capacities of individual features combine nonlinearly, there is no guarantee a high-ranking feature in low dimensional space will also be a subset of a high-ranking feature vector in high-dimensional space. Consequently, we utilized a forward feature selection approach that balances channel capacity rankings in lower dimensional space and diversity of candidates. Channel capacity calculations are performed on single dimensional features, ranked, and a subset of features above a threshold are selected to maximize diversity. As such 1D candidates are combined to form a set of 2D feature vectors. Channel capacity calculations are calculated on the 2D feature vectors, ranked and selected as in the 1D case. This iterative ranking and selection processes are repeated until additional dimensions offer no gain in channel capacity (Table S4).

Algorithmic detailed: We used Shannon's information theoretic framework to correlate the stimulus condition to dynamical features extracted from temporal trajectories of NFκB activity.

Controlling for different sample sizes

Jackknife resampling was used to control for different sample sizes by calculating channel capacity for differently-sized subsets and extrapolating to an infinite sample size.

Setting threshold

t 1 ←0.3
If d > 6 then t ←[t,0.1*1 d −6]

Compute channel capacity by optimizing over marginal distribution of X For j = 1… k c j ← I (x j; Y) q j ← argmax PX I (x j; Y)
Select a subset of feature vectors whose channel capacity values exceeds t i X *←{ x j | cj > t i }} Q *: = argmax PX I (X *; Y)
Select a subset of feature vectors that maximizes diversity of marginal distributions Select feature vector that yields the maximum channel capacity Construct a set of feature vectors containing theand feature vectors whose marginal distributions, q j, are most orthogonal to, Form = 2… n c

Machine learning classification

---

### Integrating genomics and metabolomics for scalable non-ribosomal peptide discovery [^117Nf31x]. Nature Communications (2021). High credibility.

For many organisms, the total number of possible core NRPs is prohibitively large, making it infeasible to conduct search against massive spectral repositories. Currently, even the fastest state-of-the-art spectral search methods are slow for searching millions of input spectra against databases with over 10 5 peptides in a modification-tolerant manner as the runtime grows exceedingly large when the database size grows. Supplementary Tables S2 and S7 shows that for 24 (22) out for 27 organisms in XPF dataset and 9 (7) out of 20 organisms in SoilActi dataset, the total number of core NRPs exceed 10 5 (10 6). Therefore, to enable scalable peptidogenomics for NRP discovery, for each constructed assembly line NRPminer, selects a set of candidate core NRPs. To do so, NRPminer starts by finding the number of core NRPs of A according to their adenylation scores (Problem 1) and then it uses these numbers for generating all core NRPs of A with adenylation scores higher than a threshold (Problem 2).

Problem 1. Given A = A 1, …, A n and a positive integer s, find the number of all core NRPs of A with adenylation score equal to s.

Letwhere || shows the number of amino acids in A i. For any positive integers i and s satisfying,1 ≤ i ≤ n and s ≤ maxScore A, let numCoreNRPs A (i, s) denote the number of core NRPs, of assembly line A 1. A i withequal to s. Let numCoreNRPs A (0, s) = 0 for any positive integer s, and numCoreNRPs A (i, s) = 0 for any integer s < 0, across all possible values of i. Then, for any positive integers i and s satisfying 1 ≤ i ≤ n and 0 < s ≤ maxScore A, we have

---

### Quantitative multislice and jointly optimized rapid CEST for in vivo whole-brain imaging [^115ZSdSh]. Magnetic Resonance in Medicine (2025). Medium credibility.

2.2.2 Slice permutations embedding

For N slices, the space of possible slice orderings is equivalent to the space of permutations of N elements, which is equal to a factorial of N (i.e. N !). The size of this space, particularly for moderate to large N, precludes a brute‐force search for the globally optimal slice ordering. Moreover, finding an optimal acquisition schedule for the Q‐MAJOR sequence requires joint optimization of both the continuous‐valued scan parameters (B 1sat, FA, TR, and T sat) and the discrete temporal slice ordering simultaneously, as they are interdependent and jointly affect the evolution of the magnetization. However, the set of slice ordering in each measurement is an unordered vector and does not lend itself to optimization using typical gradient descent algorithms. To enable the use of the dual network optimization framework shown in Figure 2, the unordered permutation space was first transformed into an ordered vector space. This was accomplished by ordering each permutation using inversion tables as in the following approach,: Given a vector of N integersan ordered vectorcan be defined such thatis equal to the number of elements of u, occurring beforethat are greater than. For example (taken from Turrini), if u = [5, 6, 1, 3, 2, 4, 8, 7] then v = [2, 3, 2, 2, 0, 0, 1, 0]. Because the space of permutation is finite (albeit large), the resulting ordered vectorcan be mapped to the set of integers. This embedding simplifies the choice of a slice ordering π s to a selection of integers representing the permutation for that measurement.

---

### Generalization in quantum machine learning from few training data [^1172TLFi]. Nature Communications (2022). High credibility.

Similarly, Theorem 5 implies Theorems 2, 3, and 4. Namely, if we take G T = 1 andfor all, and evaluate the bound for K = T, we recover Theorem 2. Choosing G T = 1 andfor all, the bound of Theorem 5 becomes that of Theorem 3. Finally, we can obtain Theorem 4 by bounding the minimum in Theorem 5 in terms of the expression evaluated at K = T.

Now that we have established that Theorem 5 indeed implies generalization bounds for all the different scenarios depicted in Fig. 1, we outline its proof. The first central ingredient to our reasoning are metric entropy bounds for the class of all n -qubit CPTP maps that a QMLM as described in Theorem 5 can implement, where the distance between such maps is measured in terms of the diamond norm. Note: The trivial metric entropy bound obtained by considering this class of maps as compact subset of an Euclidean space of dimension exponential in n is not sufficient for our purposes since it scales exponentially in n. Instead, we exploit the layer structure of QMLMs to obtain a better bound. More precisely, we show: If we fix a QMLM architecture with T trainable 2-qubit maps and a number of maps 0 ≤ K ≤ T, and we assume (data-dependent) optimization distances Δ 1 ⩾… ⩾Δ T, then it suffices to take (ε / K M)-covering nets for each of the sets of admissible 2-qubit CPTP maps for the first K trainable maps to obtain a-covering net for the whole QMLM. The cardinality of a covering net built in this way, crucially, is independent of n, but depends instead on K, M, and T. In detail, its logarithm can effectively be bounded as. This argument directly extends from the 2-local to the κ-local case, as we describe in Supplementary Note 3.

---

### Memorability of photographs in subjective cognitive decline and mild cognitive impairment: implications for cognitive assessment [^1114Y1tD]. Alzheimer's & Dementia (2019). Medium credibility.

2.4 Analyzing dissimilarity of MCI, SCD, and healthy individuals: Differentiating groups

An equally important question is whether there is a set of images in which consistencies in memory performance reliably differ between memory-impaired populations and healthy individuals. If such images exist, then they could form an optimized test to distinguish memory-impaired individuals from healthy controls with high efficiency.

To explore this question, we conducted an analysis we call the Iterative Image Subset (IIS) Analysis to compare the groups. Here, we describe the analysis comparing MCI to HC; however, the same analysis was also conducted with SCD versus HC. First, the HC participant pool was randomly downsampled so that the same number of HCs were used in the analysis as MCI individuals. The entire pool of participants was then split into two random halves (group A and group B). HR on the memory task was calculated for each image for the HC (HR GroupA, Healthy) and for the MCI individuals (HR GroupA, MCI) in group A. Using this performance metric, we formed three subsets of images. The number of images used in each subset was selected iteratively for all possible subset sizes, ranging from 0% to 100% of images (835 images) in 1% increments, to determine the optimal image subset size. Only images with at least 4 individuals' data were included in the analysis. The three resulting subsets were as follows:

1. "H > M", the top set of images where HCs outperformed MCI (i.e. maximizing HR GroupA, Healthy - HR GroupA, MCI; note that it is "H > S" for a comparison with SCD)
2. "H < M", the top set of images where MCI outperformed HCs (i.e. maximizing HR GroupA, MCI - HR GroupA, Healthy)
3. "H = M", the top set of images where HCs performed most similarly to MCI (i.e. minimizing | HR GroupA, Healthy - HR GroupA, MCI |).

---

### A deep-learning framework for multi-level peptide-protein interaction prediction [^111sgAkk]. Nature Communications (2021). High credibility.

Residue-level structural and physicochemical properties

We first define an alphabet of 21 elements to describe different types of amino acids (i.e. 20 canonical amino acids and a letter "X" for unknown or non-standard ones). Each type of amino acid is encoded with an integer between 1 and 21. For each amino-acid sequence S = (a 1, a 2. a n), we generate an n × 1 array, in which in the corresponding residue position, each element is an integer representing the amino-acid type.

In addition, although our problem setting assumes that 3D structure data are unavailable, previous studies have suggested that the predicted structures of the amino-acid sequences could still provide useful information. Here, for each amino-acid sequence S = (a 1, a 2. a n), we use SSProto generate an n × 1 array, in which each element is an integer representing the combination of secondary structure class and amino-acid type at the corresponding position (see Supplementary Note 7).

Furthermore, the hydrophobicity, hydrophilicity, and polarity of the R groups of individual amino acids can affect the tendency of the interactions between residues. For each amino-acid sequence S = (a 1, a 2. a n), we generate an n × 1 array, in which each element is an integer representing the combination of the polarity and hydropathy properties of the residue at the corresponding position (see Supplementary Note 7).

Protein evolutionary information

PSSMs are popular representations of protein sequences, which can detect remote homology of the protein sequences. For each protein sequence S = (a 1, a 2. a n) of length n, we use PSI-BLASTto generate a normalized position-specific scoring matrix, an n × 20 array S, in which each element S i, j stands for the probability of the j th amino-acid type at position i in the protein sequence (see Supplementary Note 7).

Intrinsic disorder tendencies to form contacts

It has been reported that the intrinsic disorder-based features in peptide and protein sequences play a crucial role in protein–peptide interactions. Here, for individual residues in the peptide and protein sequences, we first employ IUpred2A, to predict its intrinsic disorder properties. For an amino-acid sequence S of length m, we construct an m × 3 arrays representing three types of disorder scores for individual residues (see Supplementary Note 7).

---

### Brain songs framework used for discovering the relevant timescale of the human brain [^114bzBfW]. Nature Communications (2019). High credibility.

For extracting the spacetime motifs, we applied the method for detecting neuronal assemblies from spike data introduced by Lopes-dos-Santos and colleaguesbut here we used at the whole-brain level (after the binning of the binarised data). This method first determines the number of neuronal assemblies using eigenvalue analysis for determining the statistical significance of assembly patterns, as introduced by Peyrache and colleagues. The key idea is to estimate the number of assemblies (subsets of brain regions with correlated activity, i.e. what we here call spacetime motifs) by finding the number of principal components of the event matrix with significantly large eigenvalues. For that, we compute the principal components by finding the eigenvectors and corresponding eigenvalues of the covariance matrix of the event matrix ee T /N B (where e is the z -scored event matrix with elements e ib and N B is the number of time bins). The number of spacetime motifs is given by the number of significant components, i.e. significant larger eigenvalues compared to the null hypothesis given by the Marčenko–Pastur distribution. Indeed, Marčenko and Pasturdemonstrated that the eigenvalues of the correlation matrix of a normal random matrix M with statistically independent rows follow a probability function described by:with q = N columns / N rows ≥ 1, where ρ 2 is the variance of the elements of M (in our case ρ 2 = 1 due to z -score normalization), N columns is the number of columns and N rows the number of rows. λ max and λ min are the maximum and minimum bounds, respectively, and are calculated as:This probability function has finite support given by the interval λ min ≥ λ ≥ λ max. Thus, if the rows of M are statistically independent, the probability of finding an eigenvalue outside these bounds is zero. In other words, the variance of the data in any axis cannot be larger than λ max when brain regions are uncorrelated. Therefore, λ max can be used as a statistical threshold for detecting assembly activity. That is, the number of eigenvalues above λ max is used to estimate the number of spacetime motifs (assemblies in the event matrix).

---

### Diffusion in hierarchical systems: a simulation study in models of healthy and diseased muscle tissue [^114ziNHV]. Magnetic Resonance in Medicine (2017). Low credibility.

Acquisition Subset Optimization

Optimization searches were performed for a subset of M combinations of gradient strength and diffusion time that maximize the entropy difference between the baseline and each microstructural scenario. We tested each combination of size M from the global set, excluding repetitions and re‐orderings.

The optimization proceeded as follows:
For all combinations of M indices 1.1 Calculate the entropy of the baseline signal from the subset 1.2 For each microstructural scenario 1.2.1 Calculate the entropy of the microstructural scenario using only those acquisitions in the subset 1.2.2 Calculate the percentage change in entropy between scenario and baseline 1.2.3 Record the largest change
Report the subsets and entropy changes for all scenarios

We optimized subsets of sizes M = 5 and 6 from a total set of 205.

Optimization code was written in C+ + 11 and run on the same computing setup as the previous section.

---

### Guidelines for T cell nomenclature [^113TsDuM]. Nature Reviews: Immunology (2025). High credibility.

Advances in T cell biology have revealed heterogeneity among T cell populations that is not captured by existing general nomenclature. This issue has caused an ad hoc broadening of core T cell subset definitions and the invention of new subset designations that have not been uniformly delineated. To address this issue, in this Consensus Statement, we propose guidelines that serve three goals. First, they advocate that primary research reports define the experimental basis by which relevant subsets are designated in the methods section of each study. Second, they provide standardized definitions for existing subset designations in popular use, and common experimental criteria for defining each subset are noted. Last, they present an alternative 'modular nomenclature' paradigm. The newly proposed modular nomenclature eschews conceptualization of antigen-experienced T cells as belonging to a few idealized subsets, and the nomenclature instead simply indicates individual biological properties present in a T cell population with brief descriptors. Collectively, these guidelines intend to enhance transparency in the literature while facilitating clearer communication of findings and concepts to researchers, students and clinicians.

---

### NeST: nested hierarchical structure identification in spatial transcriptomic data [^114PCTVQ]. Nature Communications (2023). High credibility.

Spatial gene expression in tissue is characterized by regions in which particular genes are enriched or depleted. Frequently, these regions contain nested inside them subregions with distinct expression patterns. Segmentation methods in spatial transcriptomic (ST) data extract disjoint regions maximizing similarity over the greatest number of genes, typically on a particular spatial scale, thus lacking the ability to find region-within-region structure. We present NeST, which extracts spatial structure through coexpression hotspots-regions exhibiting localized spatial coexpression of some set of genes. Coexpression hotspots identify structure on any spatial scale, over any possible subset of genes, and are highly explainable. NeST also performs spatial analysis of cell-cell interactions via ligand-receptor, identifying active areas de novo without restriction of cell type or other groupings, in both two and three dimensions. Through application on ST datasets of varying type and resolution, we demonstrate the ability of NeST to reveal a new level of biological structure.

---

### MVsim is a toolset for quantifying and designing multivalent interactions [^114885Gy]. Nature Communications (2022). High credibility.

MVsim informs the use of multispecificity for molecular recognition and therapeutic targeting

Multispecificity is a potent molecular design element that is widely used in drug discovery and cell engineering. By leveraging two or more distinct binding epitopes, multispecific interactions are employed to engineer highly avid and selective molecular recognition for use in such applications as bispecific therapeutic antibodies, and chimeric antigen receptor T cells. Multisite recognition additionally enables higher-order information processing, allowing these multispecific systems to generate differential outputs to varying combinations of inputs. Because the network model of multivalency computes multivalent binding as the cooperative sum of its composite interactions, MVsim is well-suited to the study of such multipartite interactions.

For example, multispecific interactions can be designed to maximally exploit any degree of variation in the type and number of surface receptors and antigens within a population for the purposes of selective targeting. In this regard, we directed MVsim to address a design question: given a population of three distinct types of antigenic cell surfaces (Fig. 4c), what are the optimal ligand designs that can singly, doubly, and triply interrogate the population? MVsim demonstrates that the composition of the target receptor serves as a generally useful guide for ligand design, as seen, for example, in the relative selectivites of mono-, bi-, and trispecific Ligands 1, 3, and 7, respectively, for Receptor 3 (Fig. 4c). Moreover, selective recognition can be further tuned using designed linkages that leverage the spatial proximity between receptor target surfaces; Ligand 2b (rigid linkage) has greater selectivity than Ligand 2a (flexible linkage) for Receptor 2 (Fig. 4c).

The information-coding capacity of multivalent interactions can also effect the temporal ordering of ligand binding to a single receptor target when multiple multivalent ligands are introduced simultaneously (Supplementary Fig. 7a–c), a phenomenon that is not possible in a comparable monovalent system (Supplementary Fig. 7d–f). Exploration of the simulated parameter space in this system revealed a multivalent design leveraging kinetics, avidity, and stoichiometry that enables serial phases of dominant ligand engagement by exploiting the cumulative effects of concurrent binding afforded by multispecificity, the cooperative, competitive binding of multi-ligand dynamics, and the generation of effective dissociation rate constants via multivalency (Supplementary Fig. 7c).

---

### Statistical analysis and optimality of neural systems [^115tvWXX]. Neuron (2021). Medium credibility.

Normative theories and statistical inference provide complementary approaches for the study of biological systems. A normative theory postulates that organisms have adapted to efficiently solve essential tasks and proceeds to mathematically work out testable consequences of such optimality; parameters that maximize the hypothesized organismal function can be derived ab initio, without reference to experimental data. In contrast, statistical inference focuses on the efficient utilization of data to learn model parameters, without reference to any a priori notion of biological function. Traditionally, these two approaches were developed independently and applied separately. Here, we unify them in a coherent Bayesian framework that embeds a normative theory into a family of maximum-entropy "optimization priors". This family defines a smooth interpolation between a data-rich inference regime and a data-limited prediction regime. Using three neuroscience datasets, we demonstrate that our framework allows one to address fundamental challenges relating to inference in high-dimensional, biological problems.

---

### Principled approach to the selection of the embedding dimension of networks [^114bonWg]. Nature Communications (2021). High credibility.

Principled selection of a suitable value for the embedding dimension

The observed behavior of the loss function L (d) indicates that embedding algorithms may generate sufficiently accurate geometric descriptions with a relatively small value of d. Assuming that the plateau value L ∞ of the loss function L (d) corresponds to the best geometric description that the embedding algorithm can achieve, we indicate withthe minimal d value such that the difference between L (d) and the optimum L ∞ is less than ϵ. As already stated, a key implicit assumption here is that the empirically trained max-dimensional embedding is the best achievable by the algorithm. d o (ϵ) is then the best choice for the embedding dimension that can be made to achieve a geometric representation that uses less training time and computational resources but is still sufficiently similar (i.e. within an ϵ margin) to the best achievable representation of the network. Our formulation of the problem does not explicitly account for the fact that the network at hand may be a specific realization of some underlying stochastic model which the embedding method may rely on. Rather, Eq. (1) defines a low-rank approximation problem, meaning that we aim at finding the low-dimensional (i.e. low-rank) approximation that best describes the high-dimensional (i.e. full-rank) embedding matrix. We differentiate from the standard setting considered in low-rank approximation problems in two main respects. First, we do not rely on a standard metric of distance (e.g. Frobenius distance) between the low- and the full-rank embedding matrices. Instead, we make use of a measure of collective congruence between the embeddings, i.e. Eq. (6). The normalized embedding loss function is equivalent to a metric of distance between matrices only for spectral embedding methods such as LE. However, the equivalence is not immediate for arbitrary embedding methods. Second and more important, in a standard low-rank approximation, one generally aims at finding the best low-rank reduction of a full-rank matrix by constraining the maximum rank of the approximation. In our case instead, we seek the best rank reduction constrained by a maximum amount of tolerated discrepancy between the low- and the high-dimensional embeddings.

---

### Application of combinatorial optimization strategies in synthetic biology [^112voWfY]. Nature Communications (2020). High credibility.

In the first wave of synthetic biology, genetic elements, combined into simple circuits, are used to control individual cellular functions. In the second wave of synthetic biology, the simple circuits, combined into complex circuits, form systems-level functions. However, efforts to construct complex circuits are often impeded by our limited knowledge of the optimal combination of individual circuits. For example, a fundamental question in most metabolic engineering projects is the optimal level of enzymes for maximizing the output. To address this point, combinatorial optimization approaches have been established, allowing automatic optimization without prior knowledge of the best combination of expression levels of individual genes. This review focuses on current combinatorial optimization methods and emerging technologies facilitating their applications.

---

### A data-driven framework for identifying patient subgroups on which an AI / machine learning model may underperform [^115CJfgy]. NPJ Digital Medicine (2024). Medium credibility.

Finding the worst-performing subset in

Once the uncertainty set has been defined, SA finds the subset inthat has the worst average performance under a modeland for a particular loss function ℓ. Formally, this is defined by the following optimization problem:In words, SA tries to find S inthat maximizes the expected loss on the distribution of the evaluation data subset S. This is referred to as the worst-performing subset of size α. For details regarding how this optimization problem is solved, we refer readers to the SA paper.

The worst-performing subset will likely be a mixture of different subgroups, with the prevalence of each of the constituent subgroups different from its prevalence in the evaluation dataset. As an example, suppose Black females under 18 make up 8% of patients in the evaluation dataset. It is possible for the worst-performing subset of size 0.1 to consist of 60% Black females under 18. By examining the makeup of worst-performing subsets as a function of size, we can determine subgroup characteristics that are associated with poor model performance.

---

### Diverse and asymmetric patterns of single-neuron projectome in regulating interhemispheric connectivity [^115DtHoX]. Nature Communications (2024). High credibility.

Fig. 4
Network dynamics modeling of multiple heterotopic downstream regions.

A Diagram of the network model with self-projections and heterotopic projections. B Definition of heterogeneity parameter h. Within an example model encompassing four regions (N = 4), each region contains three heterotopic downstream targets. Neuron subsets, each comprising six neurons (N = 6), project to respective downstream regions. The total number of projecting neurons in an upstream region ranges from its minimum value (m min = n) when neuron subsets completely overlap, to its maximum value (m max = n (N - 1)) when there is no overlap between subsets. Parameter h, normalized from m, scales from 0 (complete overlap) to 1 (no overlap). The diagram on the right illustrates how h and m regulate the overlap between neuron subsets, where each small circle represents a projecting neuron, and large gray contours delineate different neuron subsets. C Adjusting h changes the heterogeneity level. In simulations with N = 30 regions, the distribution function representing heterogeneity is shown for different h values (0.01, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8). An increase in h shifts the distribution P (N = i) leftward, indicating a rise in heterogeneity. D The graphs show single neuron activities (gray lines) and the mean neural activity (red lines) under a noise level of 0.05, revealing how heterogeneity influences network dynamics. In a completely non-heterogeneous scenario (h = 0), the network alternates between asynchronous and synchronous states. At h = 0.5, state alternation frequency decreases, and at h = 1, the network maintains at one state. E The duration of state transitions decreases as heterogeneity increases. Durations are normalized relative to the maximum transition duration observed at different noise levels (0.01, 0.02, 0.05, 0.1, 0.2). F, G Both the normalized strength and variability of inter-regional correlations during asynchronous phases exhibit a decline as heterogeneity increases. Source data of (C, E – G) are provided as a Source Data file.

---

### Dynamics of evoked responses in hippocampal pathways are encoded by the duration of vigilance States [^111rt1NJ]. Nature Communications (2025). High credibility.

The binary data of the presence/absence of each vigilance state was then averaged with a symmetrical sliding window of 30 min (moving average), which resulted in time-series representing the average time spent in each state at each instant. A moving average has been described as a calculation to analyze data points by creating a series of averages of different selections of the full data set. Given a series of numbers and a fixed subset size, the first element of the moving average is obtained by taking the average of the initial fixed subset of the number series. Then the subset is modified by "shifting forward"; that is, excluding the first number of the series and including the next value in the subset. We performed moving averages with the movmean function in MATLAB TM.

Recording setup and electrical stimulations

The details of the recording setups are described in ref.and. Rats remained connected to the setup during the whole duration of the experimental protocols. Electrical stimulations consisted of a monophasic 200 μs pulse delivered by an isolated pulse stimulator (model 2100, AM-Systems, USA). Before all experimental protocols, an input-output curve was established to choose the stimulation intensity used for the whole experiment. This intensity had to elicit a fEPSP with an amplitude corresponding to 75% of the maximal amplitude on the electrodes. During the experimental protocol, stimulations were elicited every 30 s for 24 h. The signal was acquired at a frequency of 5000 Hz.

---

### Effect of varying number of OSEM subsets on PET lesion detectability [^116uhPgE]. Journal of Nuclear Medicine Technology (2013). Low credibility.

Unlabelled

Iterative reconstruction has become the standard for routine clinical PET imaging. However, iterative reconstruction is computationally expensive, especially for time-of-flight (TOF) data. Block-iterative algorithms such as ordered-subsets expectation maximization (OSEM) are commonly used to accelerate the reconstruction. There is a tradeoff between the number of subsets and reconstructed image quality. The objective of this work was to evaluate the effect of varying the number of OSEM subsets on lesion detection for general oncologic PET imaging.

Methods

Experimental phantom data were taken from the Utah PET Lesion Detection Database, modeling whole-body oncologic (18)F-FDG PET imaging of a 92-kg patient. The experiment consisted of 24 scans over 4 d on a TOF PET/CT scanner, with up to 23 lesions (diameter, 6–16 mm) distributed throughout the thorax, abdomen, and pelvis. Images were reconstructed with maximum-likelihood expectation maximization (MLEM) and with OSEM using 2–84 subsets. The reconstructions were repeated both with and without TOF. Localization receiver-operating-characteristic (LROC) analysis was applied using the channelized nonprewhitened observer. The observer was first used to optimize the number of iterations and smoothing filter for each case that maximized lesion-detection performance for these data; this was done to ensure that fair comparisons were made with each test case operating near its optimal performance. The probability of correct localization and the area under the LROC curve were then analyzed as functions of the number of subsets to characterize the effect of OSEM on lesion-detection performance.

Results

Compared with the baseline MLEM algorithm, lesion-detection performance with OSEM declined as the number of subsets increased. The decline was moderate out to about 12–14 subsets and then became progressively steeper as the number of subsets increased. Comparing TOF with non-TOF results, the magnitude of the performance drop was larger for TOF reconstructions.

Conclusion

PET lesion-detection performance is degraded when OSEM is used with a large number of subsets. This loss of image quality can be controlled using a moderate number of subsets (e.g., 12–14 or fewer), retaining a large degree of acceleration while maintaining high image quality. The use of more aggressive subsetting can result in image quality degradations that offset the benefits of using TOF or longer scan times.

---

### Synthesizing theories of human language with Bayesian program induction [^111mw3yZ]. Nature Communications (2022). High credibility.

Conceptually, this meta-theorizing corresponds to estimating a prior, M, over language-specific theories, and performing hierarchical Bayesian inference across many languages. Concretely, we think of the meta-theory M as being a set of schematic, highly reusable phonological-rule templates, encoded as a probabilistic grammar over the structure of phonological rules, and we will estimate both the structure and the parameters of this grammar jointly with the solutions to textbook phonology problems. To formalize a set of meta-theories and define a prior over that set, we use the Fragment Grammars formalism, a probabilistic grammar learning setup that caches and reuses fragments of commonly used rule subparts. Assuming we have a collection of D data sets (e.g. from different languages), notated, our model constructs D grammars, along with a meta-theory M, seeking to maximizewhere P (M) is a prior on fragment grammars over SPE-style rules. In practice, jointly optimizing over the space of M s and grammars is intractable, and so we instead alternate between finding high-probability grammars under our current M, and then shifting our inductive bias, M, to more closely match the current grammars. We estimate M by applying this procedure to a training subset comprising 30 problems, chosen to exemplify a range of distinct phenomena, and then applied this M to all 70 problems. Critically this unsupervised procedure is not given access to any ground-truth solutions to the training subset.

---

### Oligonucleotide subsets selection by single nucleotide resolution barcode identification [^1169WKi7]. Nature Communications (2025). High credibility.

Effective subset selection from complex oligonucleotide libraries is crucial for genomics, synthetic biology, and DNA data storage. The polymerase chain reaction, foundational for amplifying target subsets is limited by primer design and length for specificity, which constrains the scalability of oligo libraries and increases the synthesis burden for primers. We introduce an oligo subset selection methodology that utilizes sequence-specific cyclic nucleotide synthesis and blocking of the template oligos. This approach eliminates the need for primers for selective hybridization and enables the encoding and selection of hundreds of subsets with barcode lengths of fewer than five nucleotides. Moreover, cyclic selection enables a hierarchical data structure in the oligo library, enhancing the programmability. This advancement offers a scalable and cost-effective solution for handling complex oligo libraries.

---

### [^111kQZXj]. Developmental Medicine and Child Neurology (2019). Medium credibility.

5.2.3 书写

对于DCD患者，尤其是DCD儿童，书写是一个特殊的问题；实际上许多患儿都专门针对书写问题接受了治疗。

指南第一版发布后，尚没有DCD儿童书写干预的论文发表，因此第一版这方面的内容暂时不变。仅一项病例研究 264 报道了书写技能训练的相关内容，但只涉及3名儿童。经过强化训练后（每周2次，每次 45分钟；以及持续13周的家庭作业），其中2名儿童有所改善。

虽然智能手机、平板电脑和笔记本电脑等现代技术使用有所增加，并且在某种程度上（取决于文化背景和社会环境）儿童对书写的依赖程度有所下降，但由书写障碍引发的参与问题仍是DCD儿童面临的一个严峻考验。操作键盘和鼠标也是精细运动能力，这可能会给DCD儿童造成问题，他们可能需要接受训练。为防止DCD儿童书写或操作电脑问题带来的对学业成绩的影响（被贴上差生或学习成绩差的标签），可建议进行键盘和平板电脑操作的同步训练。

---

### [^112heC4B]. Developmental Medicine and Child Neurology (2019). Medium credibility.

4.2.4 临床检查：包含标准③和②

除病史采集，临床检查是排除其他导致运动障碍的医学状况的必要手段。神经行为检查的目的是排除其他运动障碍性疾病以符合标准③。同时应该进行全面的临床检查，以确认运动障碍不是由心理社会状况（例如剥夺、儿童虐待）和/或一般医学状况（例如脑瘫、偏瘫或肌营养不良）引起的。

排除由皮质脊髓束、小脑、锥体外系或神经肌肉等引起的神经系统疾病；神经代谢异常或获得性神经系统疾病（产前、围产期、产后）；周围神经系统疾病的体征。自上一版指南发布以来，尚未发现进行轻度神经功能障碍或神经软体征对DCD患者诊断存在价值。

由于注意力缺陷、学习障碍和孤独谱系障碍是DCD常见的共患病，建议对所有患有DCD的儿童进行行为和认知评估。如果存在适应性行为不良或情感问题，则必须根据各类疾病的指南进行进一步检查。

如果有正常的学习表现和学业成绩，则无需通过客观测量（例如智商测试）来评估认知功能。但若存怀疑，建议进行智力测验。

---

### SARS-CoV-2 gene content and COVID-19 mutation impact by comparing 44 sarbecovirus genomes [^112gNpgx]. Nature Communications (2021). High credibility.

To find regions that were significantly enriched for missense mutations in conserved amino acids, we first defined a null model as follows. For each mature protein, we counted the number of missense mutations and the number of conserved amino acids and randomly assigned each SNV to a conserved amino acid in the same mature protein (using Python's random.randint function), allowing multiplicity. For any positive integer n, we found the largest number of mutations that had been assigned to any set of n consecutive conserved amino acids within the same mature protein across the whole genome. Doing this 100,000 times gave us a distribution of the number of missense mutations in the most enriched set of n consecutive conserved amino acids in the genome. Comparing the number of actual missense mutations in any particular set of n consecutive conserved amino acids to this distribution gave us a nominal p -value for that n. We applied this procedure for each n from 1 to 100 and multiplied the resulting p -values by a Bonferroni correction of 100 to calculate a corrected p -value for a particular region to be significantly enriched. We note that these 100 hypotheses are correlated because enriched regions of different lengths can overlap, so a Bonferroni correction is overly conservative and our reported p -value of 0.012 understates the level of statistical significance. To find significantly depleted regions we applied a similar procedure with every n from 1 to 1000, but did not find any depleted regions with nominal P -value < 0.05 even without multiple hypothesis correction.

---

### Identity domains capture individual differences from across the behavioral repertoire [^115pihCd]. Nature Neuroscience (2019). High credibility.

Personality traits can offer considerable insight into the biological basis of individual differences. However, existing approaches toward understanding personality across species rely on subjective criteria and limited sets of behavioral readouts, which result in noisy and often inconsistent outcomes. Here we introduce a mathematical framework for describing individual differences along dimensions with maximum consistency and discriminative power. We validate this framework in mice, using data from a system for high-throughput longitudinal monitoring of group-housed male mice that yields a variety of readouts from across the behavioral repertoire of individual animals. We demonstrate a set of stable traits that capture variability in behavior and gene expression in the brain, allowing for better-informed mechanistic investigations into the biology of individual differences.

---

### Combining mechanistic and machine learning models for predictive engineering and optimization of tryptophan metabolism [^113wkkWk]. Nature Communications (2020). High credibility.

While mechanistic models require a priori knowledge of the living system of interest, and ML-guided predictions require ample multivariate experimental data for training, the combination of mechanistic and ML models holds promise for improved performance of predictive engineering of cells by uniting the advantages of the causal understanding of mechanism from mechanistic models, with the predictive power of ML. Metabolic pathways are known to be regulated at multiple levels, including transcriptional, translational, and allosteric levels. To cost-effectively move through the design and build steps of complex metabolic pathways, combinatorial optimization of metabolic pathways, in contrast to sequential genetic edits, has been demonstrated to effectively facilitate the searching for global optima for outputs of interest (i.e. production). Searching global optima using combinatorial approaches involves facing an exponentially growing number of designs (known as the combinatorial explosion) and requires efficient building of multiparameterized combinatorial libraries. However, this challenge can be mitigated by using intelligently designed condensed libraries that allow uniform discretization of multidimensional spaces: e.g. by using well-characterized sets of DNA elements controlling the expression of candidate genes at defined levels as opposed to using more less-/non-characterized random elements. As cellular metabolism is regulated at multiple levels, an efficient search strategy for global optima using combinatorial approaches should also take this into consideration, e.g. by using mechanistic models, "omics data repositories", and a priori biological understanding. Still it should be noted, that even with intelligent choice of design parameters and efficient library construction, there is no guarantee mathematical models will reach such a global optimum.

---

### Identification of influential probe types in epigenetic predictions of human traits: implications for microarray design [^113MD6DZ]. Clinical Epigenetics (2022). Medium credibility.

The performances of the four subsets of non-mQTL probes were weaker for biochemical measures than complex traits. For biochemical measures, relative effect sizes were 19.1–38.7% of the magnitude of estimates from 'all available probes'. The corresponding estimates were 47.5–74.2% for complex traits (Additional file 1: Table S4). Incremental R 2 estimates were comparable to maximal R 2 estimates from the literature achieved with similar, linear DNAm-based predictors. These analyses are distinct from the earlier variance component analyses and reflect the performance of DNAm-based predictors in samples external to those in which they were developed (Additional file 1: Table S5). Incremental R 2 estimates from the four probe subsets were also not significantly different from sub-sampled sets of equal size (Additional file 1: Table S6).

Subsets of probes capture similar amounts of variation in chronological age as total array content

Using REML, 'all available probes' captured 100% of variability in chronological age (n probe = 393,654). Subsets that contained 115,746, 50,000 and 20,000 probes also captured 100% of the variance. The subset containing the 10,000 most variable non-mQTL probes captured only 92.1% (se = 0.9%, Additional file 1: Table S7).

An epigenetic age predictor based on 'all available probes' explained 91.7% of the variance in chronological age in the test sample (n = 2578). The R 2 estimates from four subsets (in order of decreasing size) were 87.4%, 87.7%, 85.7% and 83.9%, respectively (Additional file 1: Table S8). The estimates were not significantly different from those in randomly sampled subsets with an equivalent number of loci.

---

### Allelic decomposition and exact genotyping of highly polymorphic and structurally variant genes [^114Qs2QW]. Nature Communications (2018). Medium credibility.

In order to estimate the copy number of any region r spanning positions a, a + 1, …, b of a gene or pseudogene s, we first calculate the normalized copy number cn s of s, which intuitively reflects the number of copies of s at position i (when the intron/exon of the gene includes ambiguously mappable positions, those positions are ignored). Details about calculating this function are provided in Supplementary Note 1 and Supplementary Figures 1 and 2. The estimated copy number (or observed coverage) of a region r of s is denoted as cn [r], and is simply calculated as:

Now we formulate Aldy's goal in this step as an instance of ILP, i.e. integer linear programming (see Trickfor an introduction to ILP) where the goal is to find an integer linear combinationof configuration vectors from the set V such that the sumwhereis minimized. Here, non-negative integer variables z i denote the number of times (the copy number of) a configuration described by vector v i from the database, appears in the solution (we recall that k denotes the total number of possible configurations). We call this problem the Copy Number Estimation Problem (CNEP).

After finding all optimal solutions of CNEP, Aldy only reports "the most parsimonious" solution(s), i.e. those for whichis the minimum possible. In the case when multiple optimal solutions minimize the term, all will be reported in the final output of CNEP.

We illustrate the above notion of parsimony through a simple example with n = 2. Let the aggregate copy number vector be cn = [1111], and the set of potential vectors be V = { v 1, v 2, v 3, v 4 } where v 1 = [1100], v 2 = [0011], v 3 = [1000], and v 4 = [0100]. Consider the following optimal solutions to this instance of CNEP: (z 1, z 2, z 3, z 4) = (1, 1, 0, 0) and (z 1, z 2, z 3, z 4) = (0, 1, 1, 1). The first solution is more parsimonious compared to the second one since the first solution implies the presence of the whole gene and the whole pseudogene — whereas the second solution implies the presence of two structurally altered copies of the gene itself, in addition to the whole pseudogene.

---

### Machine learning guided aptamer refinement and discovery [^111YFBvG]. Nature Communications (2021). High credibility.

Clustering approach

The conceptual graph algorithm for identifying clusters is the following:
Add all unique sequences as nodes to a graph.
For each pair of nodes, calculate Levenshtein distance and add an edge to the graph if the distance is ≤ 5.
Assign a unique cluster ID to each connected component of the resultant graph.

However, because the input data may be quite diverse (> 1 billion sequences), the naive algorithm presented above is intractable. We implement an approximation of the above algorithm in the following way:

First, we split the input data into two distinct subsets. One subset was used for an all-pairs comparison to define initial clusters, and the other was projected into the clusters defined by the all-pairs subset. To maximize the accuracy of all-pairs cluster detection, its input sequences were multi-read sequences; i.e. those for which the sum of all read counts in all conditions is greater than one. If this subset of sequences is sufficiently small, the subset is then padded with singleton sequences until either the entire dataset is exhausted or its size is as large as is computationally tractable. In practice, we set the size of the all-pairs subset to 300,000,000.

Second, we converted the two subsets of sequences to vectors representing the counts of all 6-mers in each sequence. These feature vectors were used to identify candidate neighbors, both for the all-pairs subset (comparing it to itself) and the projection subset (projecting each of those against the all-pairs data) using an inverted index algorithm and cosine distance. Finally, we filtered the candidate neighbors to the set of true neighbors by performing explicit calculation of Levenshtein distance. We used the true neighbors in the all-pairs subset to generate a set of candidate clusters, and then added the projected sequences into those clusters.

---

### A systematic examination of international funding flows for Ebola virus and zika virus outbreaks 2014–2019: donors, recipients and funding purposes [^114pVgvy]. BMJ Global Health (2021). High credibility.

Pledges, commitments and disbursements

Funding commitments are not documented in the literature as much as pledges and disbursements — only $1.3 billion was recorded to have been committed by donors for Ebola and no Zika funding commitments were recorded at all (figures 1–3). Pledges were greater than disbursements for Ebola, according to the literature, and commitments were greater than disbursements according to the GIDA and the FTS (figure 1). This is the same for GIDA-recorded Zika funding, but the opposite for Zika literature transactions, with over $1 billion more disbursed than pledged (figure 3). Also, $1.5 billion more is recorded in the 'literature by donor' subset than the 'literature by recipient' subset for Zika. Although for Ebola, the amounts may be more similar, depending on whether one considers the minimum or maximum values for the two subsets.

---

### Identifying undercompensated groups defined by multiple attributes in risk adjustment [^114euu2Q]. BMJ Health & Care Informatics (2021). High credibility.

Tree-based methods are an intuitive way of forming groups because each decision tree splits the sample into a set of mutually exclusive groups. These groups are defined based on the intersections of the input variables used for predicting the outcome. At each split in the tree, a variable is selected that best partitions the variable space in order to reduce variation within each new node. By using the component variablesto predict, we naturally create groups inthat are interactions of the component variables inused to split the tree. For each tree, we find the set of groups that are formed in the terminal nodes. We aggregate this information across all trees, recording the number of times a group appears and its mean estimated residualacross trees and years. See figure 1 for an illustration of this process.

Figure 1
Illustration of random forests for group importance.

The hyperparameters of the random forests algorithm are important for defining group criteria (in addition to algorithmic performance). Through hyperparameter specification, we set the minimise size of groups by requiring a minimum size for terminal nodes in the decision tree. We also control the complexity of the groups by limiting the depth of the tree and the number of terminal nodes. The deeper the tree grows, the more attributes the groups will contain. Recall that typically only a subset of randomly selected variables is considered at each split. By selecting a smaller number of variables at each split, we can decorrelate not only variables across trees but also groups, ensuring that the same groups are not favoured by every tree. And finally, the more trees we grow, the more stable our results will be, but this can be computationally costly and after a certain number of trees only trivially improves stability.

---

### Guide RNA structure design enables combinatorial CRISPRa programs for biosynthetic profiling [^115pwh2X]. Nature Communications (2024). High credibility.

For large combinatorial libraries of genetic circuits, higher-throughput screening methods like biosensing technologies would be needed to screen through the added diversity. For design spaces too large for current screening methods, data-driven and model-guided approaches like ART can be used to explore the full design space, informed by experimental efforts focused only on the most likely subsets of design parameters (Supplementary Fig. 17). An optimal subset size depends on the complexity of the pathway to be optimized, but the experimental CRISPRa profiling approach can ease the construction of these subsets.

Iterative cycles of model-guided optimization and data-driven model refinement present a promising path forward for rapid generation and optimization of biosynthetic pathways. The value of this approach is especially demonstrated when used together with combinatorial CRISPRa/i programs to access model predictions and build iteratively improved strains. Optimized metabolic engineering programs can help realize a circular bioeconomy that decreases our reliance on fossil feedstocks for production of industrial chemicals and materials. To help meet this challenge, synthetic biologists can use the tools presented in this work to rapidly optimize strains for bioproduction of valuable chemicals from renewable feedstocks.

---

### Mind the gap: performance metric evaluation in brain-age prediction [^115kn153]. Human Brain Mapping (2022). Medium credibility.

Effects of age range and sample size

As shown in Figure 6, all performance metrics improved with increasing sample size across subsets with different age ranges. Across all sample fractions, the effects of age range corresponded to the trends in Figure 5; lower uncorrected r and R 2 values in subsets with a narrower age range due to restricted prediction range and lower variance, and lower RMSE and MAE values in subsets with a narrower age range due to restricted prediction range. Age‐bias corrected metrics improved for all models, as shown in Figure 7.

FIGURE 6
Performance metrics calculated in UK Biobank subsets with different age range and sample size. Predictions are based on 10‐fold cross‐validation models run within each age‐range subset (age range is equal for training and test sets within each subset). The x ‐axes show the age range for each subset, while the y ‐axes indicate the subset sizes in fractions of the maximum number of participants available with the narrowest age range; N for each sample fraction: 0.025 = 451, 0.05 = 902, 0.1 = 1,805, 0.25 = 4,512, 0.5 = 9,025, 0.75 = 13,538, 1 = 18,050

FIGURE 7
Age‐bias corrected performance metrics calculated in UK Biobank subsets with different age range and sample size. Predictions are based on 10‐fold cross‐validation models run within each age‐range subset (age range is equal for training and test sets within each subset). The x ‐axes show the age range for each subset, while the y ‐axes indicate the subset sizes in fractions of the maximum number of participants available with the narrowest age range (N for each sample fraction is provided in Figure 6)

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^112Romk3]. Nature Communications (2025). High credibility.

Fig. 2
The matrix-tree theorem.

A Computing the steady-state occupancy π 1 by summing weights over directed spanning trees. Directed spanning trees are subgraphs containing all graph nodes but no cycles, with edges oriented toward a root node. In each directed spanning tree, the input forces make a positive, negative, or zero contribution to the tree weight. The structural vectorsare shown below each tree; these quantities enter into Equation (3) below. B Schematic illustration of the high-dimensional space of feature vectors ψ (i; θ) and χ (i, F). The depicted arrangement of vectors could solve a binary classification problem.

We define the input multiplicityas the number of edges affected per input variable, which we assume to be the same for each input. To focus on the functional way in which the input driving enters the steady-state probabilities, the driving contributions can be factored out in the algebraic expressions for the numerator and denominator of Equation (1). This has been previously been used to make analytical progress for M = D = 1 in, for example, refs. –. This equivalent formulation of Eq. (1) suggests that steady states of Markov jump processes implement a rational polynomial function of exponentiated input variables. Defining, we rewrite the matrix-tree expression for π i for general D and M We use the multi-index, whereis the set of D input labels and each componentof the multi-index runs over the values, to enumerate themonomials. These monomials y μ (F) in Equation (2) combinatorially depend on the different mixtures μ of input driving, representing a net total μ a of signed contributions from the input force F a, μ b such contributions for F b, and so on for each input. The coefficients, which are functions of the parameters θ, are the sums of weights over all directed spanning trees rooted at node i which have the corresponding mixture μ of signed input contributions. The monomial coefficientsthus represent learnable amplitudes of each polynomial basis function y μ (F). The coefficients in the denominator are defined as. Classification will be successful if, for F ρ drawn from class ρ, the coefficientsand monomials y μ (F ρ) are large for the same μ. In the subsequent sections of the paper and in the Supplementary Information we use the formulation in Equation (2) to show how the classification ability of a non-equilibrium Markov processes may be systematically modulated.

---

### A geometrical solution underlies general neural principle for serial ordering [^115bcxB8]. Nature Communications (2024). High credibility.

The second variation involves the number of learning trials in which a specific item of the sequence appears (Supplementary Fig. 1). This presentation schedule eventually leads to the number of learning trials decreasing with the serial position k of the item as N L (k) = N L (1) − β N (k − 1). The network size is the same, and the sensory noise during the testing phase is σ S = 0.3, with β N ranging from 2 to 5. Statistics were collected from 100 independent simulations.

The third variation focuses on a probabilistic reward schedule in the transitive inference task Fig. 4 d–f. For each learning trial (couples with symbolic distance ∣SD∣ = 1), the reward for a correct answer is given based on a probability that is higher for higher ranks. Specifically, given a couple of symbols { S k, S k +1 }, the probability of receiving a reward is calculated as:where CDF is the cumulative distribution function of the standard normal distribution, and σ is the scaling factor of the reward schedule. This approach is equivalent to considering the response target for a trial { S k, S k +1 } as the sign of a random value η extracted from a Gaussian distribution. For this pair of items the rewarded scenario corresponds to η > 0 (right motor choice). The probability that a Gaussian random variableis positive is given by CDF(μ / σ), leading to Eq. (27). Note that 1 − CDF(− μ / σ) = CDF(μ / σ) so that the same reward probability holds for both pairs { S k +1, S k } and { S k, S k +1 }. Even in this case, the network size is N = 100, and the number of learning and testing trials per pair was 10000. σ ranged from 0 to 0.35.

Dimensionality analysis

Figure 8 depicts the outcomes of the principal component analysis (PCA) conducted on the network activity simulated during the testing phase of the task. To perform PCA, the sampling matrix T × N is initially centered at zero by subtracting its time averages. In this context, T represents the number of time steps of the test phase, and N stands for the number of nodes in the network. Subsequently, the associated covariance matrix and its eigenvectors, which embody the principal components, are computed. Notably, the network state projected onto the eigenvector with the maximum eigenvalue is the projection onto the first principal component.

---

### Quantitative constraints on the scope of negative selection [^111FL94G]. Trends in Immunology (2003). Low credibility.

Maturing T cells with a high affinity for self-antigens presented in the thymus are deleted in the process of negative selection. Although the expression of various "tissue-specific" antigens has been described in the thymus, it is still controversial what fraction of all self-antigens induces tolerance by this mechanism. We demonstrate that the limited duration of the negative selection phase imposes a constraint on the number of self-peptides that can be reliably selected against. The analysis supports the theory that negative selection is confined to the subset of peptides produced by dendritic cells.

---

### Complexity of avian evolution revealed by family-level genomes [^1136cRS1]. Nature (2024). Excellent credibility.

Bayesian molecular dating was conducted using MCMCtreev.4.9h, with approximate likelihood calculationand under the GTR + G model. The analyses included all calibration priors, a minimum bound on root age based on an uncontroversial neornithine fossiland a soft maximum bound at 86.5 Ma. Nodes without calibrations followed a birth–death process prior(λ = μ = 1, sampling fraction ρ = 0.1), which gives an approximately uniform kernel. We used a relaxed clock with lognormally distributed rates across branches and a gamma-Dirichlet prior on rates across the three subsets of loci. During Markov chain Monte Carlo sampling, samples were drawn every 2,500 steps over a total of 5.5 × 10 7 steps following 5 × 10 6 burn-in, run twice.

We performed four additional analyses with alternative settings (Extended Data Fig. 6): (1) uniform calibration priors with ranges spanning the 95% probability density of the original calibration prior, adding a soft maximum bound with a 2.5% tail of probability; (2) a Jurassic age bound with a relaxed maximum age bound of 201.3 Ma on the root; (3) a calibration subset of 23 calibrations that were considered to be the most reliable (Supplementary Information); and (4) a set of 10,494 loci randomly selected from the 63,430 set, split into two equal groups of 5,247 and randomly partitioned into three subsets of 1,749 loci.

Subsetting analyses

Taxon sampling

To investigate the effect of sampling multiple species across orders (which represent the most contentious branches), we successively reduced taxon sampling to 50, 25, 10,… 2 or 1 species per order. We randomly selected species from the existing GTs of the 63,430 locus set, retaining all if fewer than the desired number were available. We then scored the main tree against the taxon-reduced GTs to compute the normalized quartet support for the three topologies around each branch. These analyses showed substantial impact only for Accipitriformes, in which fewer than 50 species were required to recover the main relationship. Because only Passeriformes had fewer than 50 taxa, we inferred that their sampling affected the position of Accipitriformes. To test this we removed 1, 3. 171 of the 173 Passeriformes in random order and computed quartet scores with GTs restricted to that subset. Two replicates produced indistinguishable results.

---

### Optimality guarantees for crystal structure prediction [^1153oHaw]. Nature (2023). Excellent credibility.

Crystalline materials enable essential technologies, and their properties are determined by their structures. Crystal structure prediction can thus play a central part in the design of new functional materials 1,2. Researchers have developed efficient heuristics to identify structural minima on the potential energy surface 3–5. Although these methods can often access all configurations in principle, there is no guarantee that the lowest energy structure has been found. Here we show that the structure of a crystalline material can be predicted with energy guarantees by an algorithm that finds all the unknown atomic positions within a unit cell by combining combinatorial and continuous optimization. We encode the combinatorial task of finding the lowest energy periodic allocation of all atoms on a lattice as a mathematical optimization problem of integer programming 6,7, enabling guaranteed identification of the global optimum using well-developed algorithms. A single subsequent local minimization of the resulting atom allocations then reaches the correct structures of key inorganic materials directly, proving their energetic optimality under clear assumptions. This formulation of crystal structure prediction establishes a connection to the theory of algorithms and provides the absolute energetic status of observed or predicted materials. It provides the ground truth for heuristic or data-driven structure prediction methods and is uniquely suitable for quantum annealers 8–10, opening a path to overcome the combinatorial explosion of atomic configurations.

---

### Network structure influences the strength of learned neural representations [^114QfT8k]. Nature Communications (2025). High credibility.

Dimensionality

To estimate dimensionality, we adapted the procedure taken in Tang et al. We take our ability to split our data between arbitrary groupings of classes as a measure of its intrinsic dimensionality: high-dimensional data will allow more separable groupings than low-dimensional data. We begin by enumerating over both the number of classes, m, and permutations of m classes. For each value of m, there arecombinations of trial types, and for each combination, there are 2 m −1 unique ways to divide those trial types into two groups. Dimensionality is estimated as the fraction of these assignments which we find to be linearly separable. In practice, this means that for a given value of m, we choose m out of 15 trial types, and then assign a binary label ("+" or " − ") to each of the m trial types. We then train an SVM to separate the "+" from the " − " trial types, and calculate the fraction of assignments above a threshold. However, in following ref. we note that, for fMRI data, this full calculation is computationally intractable and the between-subject differences can be understood from the mean classification score, rather than thresholding a classification as correct. We therefore choose a representative subsample of possible combinations and binary assignments for each value of m.

---

### General-purpose machine-learned potential for 16 elemental metals and their alloys [^114jVhH6]. Nature Communications (2024). High credibility.

A multiple-loss evolutionary training algorithm for many-component systems

While the increase in the number of trainable parameters does not significantly affect the inference speed, it considerably increases the number of iterations required for training, particularly with the approach used for neuroevolution potential version 3 (NEP3). It turns out that the training algorithm must be modified to achieve better performance for many-element systems. For training NEP models, we use the separable natural evolution strategy (SNES) approach, which is a powerful black-box optimization algorithm that is particularly suitable for problems with many possible solutions. It maintains a mean value and a variance for each trainable parameter that is updated according to the rank of a population of solutions. The rank is determined according to the loss function to be minimized. The loss function L is constructed using predicted and reference data for energies, forces, and virials, and is a function of the trainable parameters, i.e. The rank (or "fitness") is of crucial importance in evolutionary algorithms, as it determines the relative weight of a solution in the population. However, using a single loss function can lead to ambiguity in rank assignment: Even if the total loss of solution X is smaller than that of solution Y, it does not guarantee that solution X is more accurate for all the subsystems in a many-element system. For example, solution X might offer higher accuracy for Au systems but lower accuracy for Ag systems. To account for this observation, we define multiple loss functions for many-element systems. Since we are concerned with alloys, we cannot define a set of loss functions that have no common terms at all, but we can make a definition that minimizes the common parts. Naturally, we define the loss function for element I as the parts in Eq. (6) that are contributed by structures containing element I. For illustration, consider an explicit example with three elements, denoted A, B, and C, respectively. The loss function for element A can be calculated by considering the chemical compositions A, A B, and A C only, excluding B, C, and B C. This loss function is used when training the parameters related to element A, which are w A, c A A, c A B, and c A C (Fig. 1 b). Using this multi-loss evolutionary algorithm, the training converges much faster than using a single-loss function. The efficiency improvement in training becomes more significant with an increasing number of elements, and is crucial for being able to develop models such as UNEP-v1.

---

### Rationally reduced libraries for combinatorial pathway optimization minimizing experimental effort [^114ZCa4j]. Nature Communications (2016). Medium credibility.

This lack of knowledge prohibits strictly rational design methods, which are replaced by the creation of diversity and subsequent screening/selection, a process known as directed evolution. This can lead to the emergence of significantly enhanced functions as demonstrated in a number of compelling cases, and consequently be employed to identify optimal absolute and relative abundance of all enzymes constituting a pathway or the 'metabolic sweet spot' within a complex expression level space of dimensionality m (number of proteins) and resolution n (expression levels tested per protein, Fig. 1). To alter and fine-tune expression levels several methods have been developed targetting gene dosage, transcriptional regulation, codon usageand protein half-life. A promising alternative is the tailoring of translation rates by engineering the ribosomal-binding site (RBS). The latter has several compelling advantages: the number of base changes to access a large range of expression levels is small, it allows independent adjustment for individual proteins in operonsand RBS tailoring is applicable for a wide range of different hosts. RBS engineering has been exploited by recruiting a set of experimentally pre-characterized RBSs for combinatory optimization of expression levels. However, the effective RBS strength varies strongly for different genes due to differences predominantly in the 5′-coding region, casting doubts on the general applicability of this strategy. Furthermore, each individual RBS combination needs separate cloning, which aggregates into a substantial effort when many components are optimized at high resolution. Both limitations can be overcome by a library design in which parts of the RBS are randomized. However, such an approach becomes very quickly limited by combinatorial explosion e.g. if a degenerate Shine–Dalgarno (SD) sequence of six or eight random bases (N s) is used: already a three-gene pathway leads to 6.9 × 10 10 ((4 6) 3 for N 6) or 2.8 × 10 14 (N 8) combinations, respectively. These numbers cannot be comprehensively evaluated experimentally and furthermore this pool would be highly redundant for weak RBSs whereas potentially more productive combinations including intermediate or strong RBSs would remain scarce.

---

### Principles and open questions in functional brain network reconstruction [^115zr8J2]. Human Brain Mapping (2021). Medium credibility.

Partial information on the structure could in principle be dealt with by resorting to maximum entropy models (Squartini, Caldarelli, Cimini, Gabrielli, & Garlaschelli, 2018) (cf. Section 2.2.2). These models yield ensembles of graphs whose topology is maximally random, given a chosen set of structural properties used as constraints, naturally providing a null distribution for quantities that are not directly constrained (Jaynes, 1957), which can be used to approximate an unknown under‐sampled probability distributions.

Thinking of observed data as the output of a process obeying detailed balance and with pairwise couplings, the model parameters can be inferred using equilibrium statistics, and likelihood maximization. In general, though, the generating process is unknown, and what is sought is a statistical description of the data in terms of a simpler model matching some property of the observed data, from the mean activity of individual populations and the correlations between them (Meshulam et al. 2017; Roudi, Tyrcha, & Hertz, 2009; Schneidman et al. 2006; Tang et al. 2008; Tkačik, Schneidman, Berry II, & Bialek, 2009; Tkačik et al. 2013), to specific combinations of activity and silence (Ganmor, Segev, & Schneidman, 2011) or higher‐order correlations (see Yeh et al. 2010, Savin & Tkačik, 2017, and Nguyen et al. 2017 for critical reviews of maximum entropy methods in neuroscience). Time‐dependent variants could provide tractable null models for the time‐varying dynamics of neural activity as an alternative to latent linear dynamical systems models. These methods prove unwieldy for very high‐dimensional spaces (Roudi et al. 2015), but can in principle be used at sufficiently coarse‐grained scales.

---

### Antigen microarrays: descriptive chemistry or functional immunomics? [^113NuX2C]. Trends in Immunology (2010). Low credibility.

Advances in protein microarray technology allow the generation of high content, reliable information about complex, multilevel protein interaction networks. Yet antigen arrays are used mostly only as devices for parallel immune assays describing multitudes of individual binding events. We propose here that the huge amount of immunological information hidden in the plasma of an individual could be better revealed by combining the characterization of antibody binding to target epitopes with improved estimation of effector functions triggered by these binding events. Furthermore, we could generate functional immune profiles characterizing general immune responsiveness of the individual by designing arrays incorporating epitope collections from diverse subsets of antibody targets.

---

### Identifying molecules as biosignatures with assembly theory and mass spectrometry [^116h5Pet]. Nature Communications (2021). High credibility.

Results

Defining molecular assembly

Our approach to life detection is based on the molecular assembly number (MA) which is derived from the theory of assembly pathways. Assembly pathways are sequences of joining operations, that start with basic building blocks (in this case bonds) and end with a final product. In these sequences, sub-units generated within the sequence can combine with other basic or compound sub-units later in the sequence, to recursively generate larger structures (see Fig. 1A). Assembly pathways have been formalized mathematically using directed multigraphs (graphs where multiple edges are permitted between two vertices) with objects as vertices and objects as edge labels, however for the results here the formal details are unnecessary (see the Supplementary Information for more details). Generating many assembly pathways from a pool of basic units will result in a combinatorial explosion in the diversity of structures. The molecular assembly number (MA) therefore captures the specificity of the molecule in the face of this combinatorial explosion and provides an agnostic measure of the likelihood for any molecular structure to be produced more than once. There will normally be multiple assembly pathways to create a given molecule. The MA of an object is the length of the shortest of those pathways, i.e. the smallest number of joining operations requires to construct the object, where objects created in the process can subsequently be reused. Thus, it is a simple integer metric to indicate the number of steps required in this idealized case to construct the molecule.

---

### G-STRATEGY: optimal selection of individuals for sequencing in genetic association studies [^114QCDgX]. Genetic Epidemiology (2016). Low credibility.

In a large-scale genetic association study, the number of phenotyped individuals available for sequencing may, in some cases, be greater than the study's sequencing budget will allow. In that case, it can be important to prioritize individuals for sequencing in a way that optimizes power for association with the trait. Suppose a cohort of phenotyped individuals is available, with some subset of them possibly already sequenced, and one wants to choose an additional fixed-size subset of individuals to sequence in such a way that the power to detect association is maximized. When the phenotyped sample includes related individuals, power for association can be gained by including partial information, such as phenotype data of ungenotyped relatives, in the analysis, and this should be taken into account when assessing whom to sequence. We propose G-STRATEGY, which uses simulated annealing to choose a subset of individuals for sequencing that maximizes the expected power for association. In simulations, G-STRATEGY performs extremely well for a range of complex disease models and outperforms other strategies with, in many cases, relative power increases of 20–40% over the next best strategy, while maintaining correct type 1 error. G-STRATEGY is computationally feasible even for large datasets and complex pedigrees. We apply G-STRATEGY to data on high-density lipoprotein and low-density lipoprotein from the AGES-Reykjavik and REFINE-Reykjavik studies, in which G-STRATEGY is able to closely approximate the power of sequencing the full sample by selecting for sequencing a only small subset of the individuals.

---

### Jumper enables discontinuous transcript assembly in coronaviruses [^111sRDbu]. Nature Communications (2021). High credibility.

Proposition 2

Let G be a segment graph, T b e a transcript and r be a read. Then, π (T) ⊇ π (r) if and only if.

Hence, we may rewrite the likelihoodaswhere, c = [c 1, …, c k], and wherebe the subset of indices i corresponding to transcriptswhereand. Note that the only difference between Eq. (2) and the formulation in Eq. (1) is the way that the candidate transcripts of origin for a given read are described. In Eq. (1), they are described as paths in the segment graph whereas in Eq. (2), they are described by sets of pairwise non-overlapping discontinuous edges in the segment graph. This leads to the following theorem.

Theorem 1

For any alignment, transcripts and abundances c, Eqs. (1) and (2) are identical.

Although we have described the formulation for single-end reads, this characterization is applicable to paired-end and even synthetic long reads. Moreover, our implementation provides support for both single-end and paired-end read samples with a fixed read length. The above characterization using discontinuous edges allows us to reduce the number of terms in the likelihood function since multiple reads can be characterized by the same characteristic discontinuous edges. We describe this in detail in the next section.

JUMPER: a progressive heuristic for the DTA problem

To solve the DTA problem, we use the results of the previous section to write a more concise form of the likelihood. Specifically, letbe the set of pairs of characteristic discontinuous edges generated by the reads in alignment. Let d = { d 1, ⋯, d m }, where d i is the number of reads that map to pair. Using that reads r with identical characteristic discontinuous edges (σ ⊕ (r), σ ⊖ (r)) have identical probabilities, we obtain the following mathematical program for the log-likelihood(see Supplementary Note A.2 for derivation).

Observe that the first sum (over reads) is concave and the second sum (over transcripts) is convex. Since we are maximizing, our objective function would ideally be concave. In Supplementary Note B.1, we prove the following lemma, which enables us to remove the second term using a scaling factor for the relative abundances c that does not alter the solution space.

---

### Robust classification of salient links in complex networks [^113Rf3F2]. Nature Communications (2012). Medium credibility.

Complex networks in natural, social and technological systems generically exhibit an abundance of rich information. Extracting meaningful structural features from data is one of the most challenging tasks in network theory. Many methods and concepts have been proposed to address this problem such as centrality statistics, motifs, community clusters and backbones, but such schemes typically rely on external and arbitrary parameters. It is unknown whether generic networks permit the classification of elements without external intervention. Here we show that link salience is a robust approach to classifying network elements based on a consensus estimate of all nodes. A wide range of empirical networks exhibit a natural, network-implicit classification of links into qualitatively distinct groups, and the salient skeletons have generic statistical properties. Salience also predicts essential features of contagion phenomena on networks, and points towards a better understanding of universal features in empirical networks that are masked by their complexity.

---

### Efficient classical sampling from gaussian boson sampling distributions on unweighted graphs [^112AGUzX]. Nature Communications (2025). High credibility.

Results

Gaussian Boson sampling for graph problems

Boson sampling is a quantum computing model where N identical photons pass through a M -mode linear interferometer and are detected in output modes. In the standard Boson sampling paradigm, the probability of a given output configurationis related to the permanent of a submatrix of the interferometer's M × M unitary matrix T, which we call T S :whereis the set of all permutations on [N] ≔ {1, 2, …, N }, and T S is a matrix composed of the intersecting elements of the columns and the rows of T determined by the input positions and output, respectively.

Gaussian Boson Sampling (GBS) is a variant that uses Gaussian states with squeezing parametersas inputs instead of single photons. In GBS, the output photon-number distribution is determined by a matrix function called the Hafnian. The Hafnian of a 2 n × 2 n matrix A is defined asSpecifically, the Hafnian of the adjacency matrix of an unweighted graph equals to the number of its perfect matchings. The probability of measuring a specific photon number patternin an M -mode GBS experiment can be expressed in closed-form as, where σ is the 2 M × 2 M covariance matrix of the Gaussian state and A S is the submatrix by selecting the intersection of columns and rows only according to outputfrom the sampling matrix A = B ⊕ B * with

Given an arbitrary undirected graph with potentially complex-valued symmetric adjacency matrix Δ, we aim to engineer B = c Δ with an appropriate rescaling parameter c. It is possible to find such a T by the Takagi-Autonne decomposition (see in ref.and Section 4.4 of) when, where { λ j } is the eigenvalue set of A. Subsequently, the sampling matrix becomes A = c Δ ⊕ c Δ *.

When, with dominating probability all the click number n i ≤ 1. Then, all the factorials in (3) disappear since 0! = 1! = 1. On the other hand, the covariance matrix σ and the sampling matrix A is related byThus, the probability of outputting the subgraph with vertex set S is given by

---

### Accurate high throughput alignment via line sweep-based seed processing [^111JSLNp]. Nature Communications (2019). High credibility.

Fig. 5
Contradicting seeds. A scenario with two contradicting seeds s and s ′. While extending the chain, chaining,– is forced to choose one of both seeds. Abouelhoda et al.use gap cost estimations, while Shibuya et al.use accumulative seed lengths for making this decision. In contrast to our approach, however, both techniques cannot guarantee that the selected seed belongs to the best alignment

Alignments in repetitive regions of a genome demand for long seeds because short seeds can show unmanageable amounts of repetitive occurrences in these regions (Supplementary Fig. 7). In contrast, noisy reads demand for short seeds because exact matches of an inappropriately chosen size might not lead to sufficiently many seeds, which are required to guarantee the discovery of the correct position. Therefore, fixed-length seeds require sequencer specific fine-tuning for meeting both requirements simultaneously (k-mer sizes must be adapted to sequencing techniques). Variable-length seeds, as e.g. SMEMs, do not require such fine-tuning. Additionally, variable-length seeds become longer and decrease in quantity with increasing read quality, while fixed-size seeds are expected to increase in quantity. (With fixed-size seeds, the information about a long perfect match has to be decomposed into many small pieces.)

Maximally spanning seeds constitute a subset of SMEMs. Although being a subset, maximally spanning seeds can deliver similar accuracy to SMEMs thanks to their high relevance (Supplementary Fig. 8). This relevance aspect is now inspected more in detail. A seed s is called relevant with respect to a query Q if and only if s overlaps with the reference interval of Q (i.e. the seed leads to the correct position on the reference). Let S be a set of seeds and let S ′ ⊆ S be the set of relevant seeds in S. We define the relevance rate of S as the ratio | S ′|/| S |. For simulated UON reads (mean error sampled from HG002 (AJ Son) by Zook et al.), maximally spanning seeds deliver a relevance rate of 4.4% (i.e. 4.4% of all seeds lead to the correct position on the genome), while for SMEMs the rate is 1.5% merely. With simulated PacBio reads, the relevance rates are 5.0% (maximally spanning) and 1.7% (SMEMs).

---

### The T cell subsetting challenge [^114GvPPf]. Trends in Immunology (2025). Medium credibility.

Exploiting specific T cell subset properties bears potential for T cell therapies but is complicated by inconsistencies in T cell subset definitions and markers. Here, we discuss causes for the definition and classification complexities to provide a handle for how to navigate the T cell subset jungle.

---

### Analysis of missense variants in the human genome reveals widespread gene-specific clustering and improves prediction of pathogenicity [^116jZsNF]. American Journal of Human Genetics (2022). Medium credibility.

For testing set 2, we used as PLP variants all disease-causing missense mutations (DM) from the HGMD database (v.2020.2) that were added since 2017 and were neither in ClinVar nor were included in the training set or another testing set (n = 14,327). Since the HGMD database does not contain BLB variants, we used all missense variants from gnomAD that were (1) absent from the training set, (2) not used to train the positional score, (3) absent from both HGMD and ClinVar, and (4) present only in genes for which we defined at least one PLP variant. In order to have a similar number of BLB and PLP variants, we used an AF threshold of > 0.000177 in gnomAD (maximum value of exome and genome subsets), resulting in the selection of 13,248 BLB variants (Table S1). For the analysis of AUCs as a function of time (Figure S3 B), we used only HGMD variants published during the year considered.

For testing set 3, we used variants from the DoCM database as PLP variants, excluding variants from the training set and variants used to build the positional and amino acid change features (n = 205). Since the DoCM database only contains pathogenic variants, BLB variants were selected according to the same criteria described for testing set 2 (n = 207, Table S1).

For all testing sets, AUCs of MutScore were compared to other tools using the DeLong test in the "roc.test" function from the package pROC (v1.17.0.1) with default arguments, except for "method = delong".

---

### Universality, criticality and complexity of information propagation in social media [^1121P3SM]. Nature Communications (2022). High credibility.

Selection of the temporal resolution

We follow the same procedure as in Ref. Given a time series { t 1, t 2,… }, we define an avalanche starting at t b as a sequence of events { t b, t b +1, …, t b + S −1 } such that t b − t b −1 > Δ, t b + S − t b + S −1 > Δ and t b + i − t b + i −1 ≤ Δ for all i = 1, …, S, where Δ is the resolution parameter. The size S of an avalanche is the number of events within it and the duration T is the time lag between the first and last event in the avalanche, i.e. T = t b + S −1 − t b. Depending on the value of Δ, the same time series is composed of different avalanches.

We identify the optimal resolution Δ * as the critical point of a one-dimensional percolation model that is used to describe the time series. Each time series in a data set is considered as an instance of the one-dimensional percolation model. We measure the size S M of the largest avalanche within each time series. We define the percolation strength P ∞ and its associated susceptibility χ, respectively, aswhere 〈 S M 〉 andare, respectively, the first and second moments of the distribution of the size of the largest avalanche S M across all time series in a data set. Δ * is computed as the resolution maximizing χ, i.e. As time series with only one event introduces an offset in the measure of P ∞ and are not informative with respect to the optimal resolution Δ *, i.e. S M = 1 for any Δ in these time series, we remove them from the sample and compute P ∞ and χ considering only time series composed of at least two events.

Values of the optimal resolution Δ * are reported in SI A. Note that the avalanche statistics reported in Fig. 2 is obtained considering all avalanches, excluding the largest one of each time series. This choice is due to the well-known fact that in percolation theory the largest cluster respects different statistics than that of finite clusters.

---

### Combinatorial code governing cellular responses to complex stimuli [^117Bcp7U]. Nature Communications (2015). Medium credibility.

Cells adapt to their environment through the integration of complex signals. Multiple signals can induce synergistic or antagonistic interactions, currently considered as homogenous behaviours. Here, we use a systematic theoretical approach to enumerate the possible interaction profiles for outputs measured in the conditions 0 (control), signals X, Y, X+Y. Combinatorial analysis reveals 82 possible interaction profiles, which we biologically and mathematically grouped into five positive and five negative interaction modes. To experimentally validate their use in living cells, we apply an original computational workflow to transcriptomics data of innate immune cells integrating physiopathological signal combinations. Up to 9 of the 10 defined modes coexisted in context-dependent proportions. Each interaction mode was preferentially used in specific biological pathways, suggesting a functional role in the adaptation to multiple signals. Our work defines an exhaustive map of interaction modes for cells integrating pairs of physiopathological and pharmacological stimuli.

---

### Methodological considerations in quantification of oncological FDG PET studies [^112WctFZ]. European Journal of Nuclear Medicine and Molecular Imaging (2010). Low credibility.

Tomographic reconstruction

Analytical algorithms (e.g. filtered backprojection, FBP) are almost completely replaced by iterative statistical algorithms (e.g. ordered subset expectation maximization, OSEM) for tomographic reconstruction of acquired coincidence events to quantifiable images. Iterative reconstruction has a number of potential advantages that make it attractive in comparison with analytical methods. Analytical algorithms have the intrinsic, limiting assumption that measured data are perfectly consistent with the object, which is never true in practice due to noise and other physical factors (e.g. attenuation). In contrast, iterative algorithms can incorporate a priori information such as the statistical distribution of the coincidences and position-dependent spatial resolution. Important adjustable parameters for tomographic reconstruction are the matrix size (the number of voxels in the transaxial plane of the reconstructed images) and the reconstruction smoothing filter. For iterative algorithms also the number of iterations (the number of times the estimate of the real object is updated) and the number of subsets (the number of projections updated simultaneously in each iteration, in OSEM iterative reconstruction only) need to be defined.

---

### A multiresolution framework to characterize single-cell state landscapes [^114q64Mb]. Nature Communications (2020). High credibility.

The independent application of single-level decompositions at increasing levels of resolution inevitably results in high redundancy: the most dominant patterns are recurrently recovered. To systematically reduce such redundancy, and to facilitate interpretation, we developed an algorithm that prunes patterns found at multiple resolutions, identifying as a result a nonredundant, multiresolution set of cell-state patterns. Briefly, we build and cluster a pattern graph to define groups of similar patterns across different levels of resolution into equivalent classes. For each class, we then define a representative pattern by giving priority to the lowest resolution (smallest k) at which a pattern of the class was discovered (Methods). Within this framework, the size of each class provides an approximate measure of the pattern dominance. This procedure commonly reduces a multilevel space of ~400 patterns (for k = 2, …, 30) to a multiresolution space of ~20 states. For example, we applied this algorithm to the PBMC data and reduced the 464 patterns resulting from factorizations from k = 2 to k = 30 into a set of 21 dominant multiresolution patterns. Annotation of these dominant patterns enables a fast and intuitive way to interpret the underlying transcriptomic states.

---

### Complexity of avian evolution revealed by family-level genomes [^115FV7oM]. Nature (2024). Excellent credibility.

Data quantity

Of the 63,430 loci included in the main analysis we randomly selected subsets of increasing numbers of GT up to maximally half of the available GTs (1,000, 2,000,… 32,000). Each subset was repeated 50 times and an ASTRAL tree was estimated for each. The subset topology was compared to the main tree by counting the number of differing branches (Robinson–Foulds distance/2) using TreeCmpv.2.0 and calculating the proportion of highly supported branches (posterior probability ≥ 0.95). We recorded whether each clade of the main tree was present in subset trees and counted how many different sister groups were present across the 50 replicates of each subset. We performed the same analyses for the other data types, maximally sampling about half of the available loci. This included exons (50 times sampling 1,000, 2,000,… 8,000 GTs), introns (1,000, 2,000,… 32,000) and UCEs (1,000, 2,000). We also performed the analyses using all non-coding (80,047 windows, introns and UCEs totalling 129,878 loci) GTs (1,000, 2,000,… 64,000).

Data type

We compared topological differences between trees for each data type, also controlling for the number of GTs used. We subsampled loci at random (50 times). The highest number of GT subsets present across all data types was 2,000 (limited by the number of UCEs). To show the effect of increasing loci we also performed the analysis for 8,000 loci, omitting comparisons with UCEs. We calculated mean pairwise Robinson–Foulds distances between resulting species trees.

---

### AI-based detection and classification of distal radius fractures using low-effort data labeling: evaluation of applicability and effect of training set size [^111cybLH]. European Radiology (2021). Medium credibility.

Fig. 2
Flowchart shows the training and test architecture. Top — Set-up of training subsets and their sizes. All subsets were in accordance with the predefined sizes, except the biggest subset, which contained all radiographs available for each category. Middle — Set-up to develop artificial intelligence (AI) algorithms. Performance was evaluated with area under the receiver operating characteristics curve (AUC), Youden's J statistic (J), and accuracy. Bottom — Set-up to determine the radiology resident's performance and performance evaluation on test set A (AI only) and B (AI and radiology residents). *Metal and cast detection training sets were limited to 9,000 images and included both views simultaneously; therefore, the model was used directly, avoiding the splitting and averaging of predictions steps (see middle set-up). DCNN = deep convolutional neural network

Artificial intelligence algorithms

The selected DCNN architecture was ResNet 18, pre-trained on ImageNet using the DL framework Pytorch (version 1.2). Trainings and tests were run on a NVIDIA GTX 2080TI (NVIDIA) with 11GB of RAM. The batch size was set to 24. An optimizer stochastic gradient descent (SGD) was used with an initial learning rate of 0.001 and a momentum of 0.9. The learning rate was reduced by 0.1 every seven epochs. Data was augmented with a horizontal flip and, with a probability of 50%, application of an affine transformation with up to five degrees and scaling between 90 and 110%. Training was performed in 15 epochs. For training and tests, the images were center cropped to a size of 1024 × 1024 pixels and resized to 224 × 224 pixels with a subsequent normalization of pixel values according to the ImageNet mean and standard deviation.

Two models trained with one set can perform substantially different (Fig. 3). To receive a good estimate on the maximal potential of a training subset size, ten models were trained on each subset, leading to a total of 620 models.

Fig. 3
Artificial intelligence models performance for distal radius fracture detection, classification, and cast and metal detection on test set A. The performance was measured with the area under the receiver operating characteristics curve (AUC). The graph shows the effect of an incrementally increased training subset size between 500 (subset 1) and 9,000 (subset 12) radiographs on model performance, and the possible performance variation per training subset

---

### Compartments revealed in food-web structure [^1144zwvz]. Nature (2003). Excellent credibility.

Compartments in food webs are subgroups of taxa in which many strong interactions occur within the subgroups and few weak interactions occur between the subgroups. Theoretically, compartments increase the stability in networks, such as food webs. Compartments have been difficult to detect in empirical food webs because of incompatible approaches or insufficient methodological rigour. Here we show that a method for detecting compartments from the social networking science identified significant compartments in three of five complex, empirical food webs. Detection of compartments was influenced by food web resolution, such as interactions with weights. Because the method identifies compartmental boundaries in which interactions are concentrated, it is compatible with the definition of compartments. The method is rigorous because it maximizes an explicit function, identifies the number of non-overlapping compartments, assigns membership to compartments, and tests the statistical significance of the results. A graphical presentation reveals systemic relationships and taxa-specific positions as structured by compartments. From this graphic, we explore two scenarios of disturbance to develop a hypothesis for testing how compartmentalized interactions increase stability in food webs.

---

### Systems biology approaches to unravel lymphocyte subsets and function [^113PdxGG]. Current Opinion in Immunology (2023). Medium credibility.

Single-cell technologies have revealed the extensive heterogeneity and complexity of the immune system. Systems biology approaches in immunology have taken advantage of the high-parameter, high-throughput data and analyzed immune cell types in a 'bottom-up' data-driven method. This approach has discovered previously unrecognized cell types and functions. Especially for human immunology, in which experimental manipulations are challenging, systems approach has become a successful means to investigate physiologically relevant contexts. This review focuses on the recent findings in lymphocyte biology, from their development, differentiation into subsets, and heterogeneity in their functions, enabled by these systems approaches. Furthermore, we review examples of the application of findings from systems approach studies and discuss how now to leave the rich dataset in the curse of high dimensionality.

---

### Fundamental limits to learning closed-form mathematical models from data [^115B4Dpa]. Nature Communications (2023). High credibility.

Given a finite and noisy dataset generated with a closed-form mathematical model, when is it possible to learn the true generating model from the data alone? This is the question we investigate here. We show that this model-learning problem displays a transition from a low-noise phase in which the true model can be learned, to a phase in which the observation noise is too high for the true model to be learned by any method. Both in the low-noise phase and in the high-noise phase, probabilistic model selection leads to optimal generalization to unseen data. This is in contrast to standard machine learning approaches, including artificial neural networks, which in this particular problem are limited, in the low-noise phase, by their ability to interpolate. In the transition region between the learnable and unlearnable phases, generalization is hard for all approaches including probabilistic model selection.

---

### Foundations for the design and implementation of synthetic genetic circuits [^111q46mi]. Nature Reviews: Genetics (2012). Medium credibility.

Synthetic gene circuits are designed to program new biological behaviour, dynamics and logic control. For all but the simplest synthetic phenotypes, this requires a structured approach to map the desired functionality to available molecular and cellular parts and processes. In other engineering disciplines, a formalized design process has greatly enhanced the scope and rate of success of projects. When engineering biological systems, a desired function must be achieved in a context that is incompletely known, is influenced by stochastic fluctuations and is capable of rich nonlinear interactions with the engineered circuitry. Here, we review progress in the provision and engineering of libraries of parts and devices, their composition into large systems and the emergence of a formal design process for synthetic biology.

---

### Physical networks as network-of-networks [^115YWWbo]. Nature Communications (2024). High credibility.

Network growth

To study the effect of physicality on network evolution, we define a model of network growth relying on the network-of-networks representation. We start with an emptyand we place a single physical nodeoccupying a subset of the sites. We add the rest of the nodes iteratively: At time step t we add a new nodethat is seeded at a random unoccupied site and grows until it hits an already existing nodeand a link (t − s) is formed. The growth of nodeis driven by some random or deterministic process; and we assume that the physical nodes are characterized by a fractal dimension d f ∈ [1, d]. We add N physical nodes or until all ofis occupied; in the latter case we call the physical network saturated.

---

### Revisiting thymic positive selection and the mature T cell repertoire for antigen [^111rJXwe]. Immunity (2014). Low credibility.

To support effective host defense, the T cell repertoire must balance breadth of recognition with sensitivity for antigen. The concept that T lymphocytes are positively selected in the thymus is well established, but how this selection achieves such a repertoire has not been resolved. Here we suggest that it is direct linkage between self and foreign antigen recognition that produces the necessary blend of TCR diversity and specificity in the mature peripheral repertoire, enabling responses to a broad universe of unpredictable antigens while maintaining an adequate number of highly sensitive T cells in a population of limited size. Our analysis also helps to explain how diversity and frequency of antigen-reactive cells in a T cell repertoire are adjusted in animals of vastly different size scale to enable effective antipathogen responses and suggests a possible binary architecture in the TCR repertoire that is divided between germline-related optimal binding and diverse recognition.

---

### Learning interpretable network dynamics via universal neural symbolic regression [^1155DQvS]. Nature Communications (2025). High credibility.

Performance measures

The performance measures for evaluating the methods in this work are as follows:

Adjusted R 2 score

Adjusted R 2 offsets the impact of the number of nodes N on R 2, so that the true value range is (0, 1), and the larger the better. The score can be calculated as:where. X i (t) andare the true and predictive states of node i at time t, respectively.is the average of X i (t) over all node and N is the number of nodes.

NED

We adopt the normalized Euclidean distance (NED) to finely evaluate the difference between the predictive states generated by the inferred equation and the true states for each node i, defined as follows:whererepresents the maximum Euclidean distance between pairs of true states over the time interval [0, T]. X i (t) andare the true and predictive states of node i at time t, respectively.andare their respective derivatives.

Recall

From the perspective of the symbolic form of equations, we use recall to evaluate the accuracy of the discovered equations, specifically whether a certain function term is present or not. Let ξ true and ξ pre denote the true and inferred coefficients under a candidate library of function terms. For example, a library is [1, x, x 2, x 3, x 4], If a true equation is y = 1 + x, then ξ true = [1, 1, 0, 0, 0]. And if an inferred equation is y = 1 + x 2, then ξ pre = [1, 0, 1, 0, 0]. Recall measures the percentage of coefficients successfully identified among the true coefficients and is defined as:where ⊙ represents the element-by-element product of two vectors, ∣ ⋅ ∣ 0 denotes the number of non-zero elements in the vector.

More performance measures, including MRE (Mean Relative Error), MAE (Mean Absolute Error), L2 error, and Precision, can be found in Supplementary Section IV.

---

### Accelerated search for materials with targeted properties by adaptive design [^1177You6]. Nature Communications (2016). Medium credibility.

Evaluation of regressor and selector combinations

As there are many ways to choose the regressor:selector pairs, we first need to choose the best regressor:selector combination. The approach we took to selecting the regressor:selector pair was to use cross-validation on the full set of training compounds we counted the average number of samples needed to find the best Δ T when trained on the randomly chosen subsets of the training data. For example, (a) we randomly selected (without replacement) s = 2 samples of the training data, (b) trained using those two samples and the known Δ T for those samples, (c) chose the next samples from the full training set using a given regressor:selector pair and (d) counted the total number of tries needed to find the best sample in the training set. We repeated this process 2,000 times using different pairs of randomly picked samples. When counting the number of tries to find the best sample in the training set, we included the random picks in the count, for example, if we picked the best compound on the first random sample, we counted one. If we did not find the best sample in the first two random picks and had to run the regressor:selector pair three times to find the best compound, then we counted five. Therefore, every random selection of two training samples gave us a count between 1 and 22, inclusive. The average of these counts was calculated over all 2,000 cross-validation runs to obtain the average number of tries for that regressor:selector combination.

---

### Population structure in a comprehensive genomic data set on human microsatellite variation [^114ycHki]. G3 (2013). Low credibility.

While the 8 MZ pairs we identified in the Pacific Islander data set might indeed represent twins (and), it is perhaps more likely that they are pairs of duplicate samples. Following the exclusion of 53 individuals from MZ and inter-population first-degree relative pairs (some of whom appeared in more than one such pair), the combined human data set contained 5795 individuals from 267 populations (mean sample size 21.7, SD 16.8, minimum 3) with genotypes at 645 loci (Figure 1; subset MS5795 henceforth).

Next, by removing from MS5795 a member of every intra-population first-degree relative pair, we created subset MS5547, a set of 5547 individuals. Finally, by the additional removal from MS5547 of a member of every intra-population second-degree relative pair, we created subset MS5435, consisting of 5435 individuals.

In MS5795, the sample size for Gujaratis is 252 individuals, much greater than for the other populations (maximum 61, mean 20.8, SD 9.1). Rosenberg et al. had used a subset of 50 Gujaratis to make the Gujarati sample size similar to those of other populations. We therefore propose three further subsets that restrict the Gujarati population to only the 49 individuals studied by Rosenberg et al. that appear in MS5795; the restrictions of MS5795, MS5547, and MS5435 generate data sets MS5592, MS5362, and MS5255, respectively.

---

### A versatile computational algorithm for time-series data analysis and machine-learning models [^111yhXM7]. NPJ Parkinson's Disease (2021). Medium credibility.

Now consider the 3x3 neighborhood around each vector element being constructed from.represents our one-dimensional time-series vector that can be used for time-delay embedding with delay t and embedding dimension m to reconstruct the phase-space trajectory. However, instead of considering the Euclidean norm between data points:we will consider vectors V i and V j in relation to their local 3x3 neighborhood such that:where it can be seen that≠even when = (Fig. 1a). It follows then, that directional and curvature information can be captured by different inequality patterning around the 3x3 neighborhood when computed for allby constructing a new matrix that represents an 8-bit binary code for each point-pair's local neighborhood:where g 0 representsand g n = { g 1, …, g 8 } are its eight connected neighbors. Each neighbor which is larger or equal to g 0 is set to 1, while each neighbor that is smaller than g 0 is set to 0. A binary code is thus created by moving around the central point g 0 (here counterclockwise) where a single integer value is calculated based on the sum of the binary code elements (0 or 1) multiplied by the eight 2 p positional weights (increments of powers of 2: 2 0, 2 1, 2 2, 2 3, 2 4, 2 5, 2 6, 2 7) starting with the preceding points (i.e.). This represents 8-bit binary coding where there are 2 8 (256) different possible integer values, ranging from 0 to 255. This newly created matrix now allows for the identification of graded changes in phase-space trajectories that can be used to identify recurring sets of local topological features embedded within the dynamic signal (LoTRA plot; see Fig. 1b, c for example).

---

### Influence of reconstruction iterations on 18F-FDG PET / CT standardized uptake values [^115BFct4]. Journal of Nuclear Medicine (2005). Low credibility.

Unlabelled

The goal of this study was to evaluate the effect on the average standardized uptake value (avgSUV) and maximum standardized uptake value (maxSUV) of changing the number of iterations in the reconstruction process on studies acquired with PET/CT.

Methods

Data from 50 human tumors were acquired on a PET/CT scanner, using the CT portion for attenuation correction. Reconstruction was performed using the 2-dimensional reconstruction method of ordered-subsets expectation maximization (OSEM) with 28 subsets and with 1, 2, 3, 4, 5, 10, 20, and 40 iterations. The standardized uptake value (SUV) of the studies was analyzed by positioning a region of interest tightly around the tumor and reproducing the same area on all same-study iterations for SUV measurements.

Results

The differences in mean avgSUV and mean maxSUV were statistically different across different iteration groups. SUV data demonstrated that the avgSUV measurements have the most significant differences between 1 versus 2 iterations and 2 versus 3 iterations. The P values for these comparisons were less then 0.001. For maxSUV, all differences had P values less than 0.001. There also was a systematic increase in the SUVs as the number of iterations increased. The avgSUV increased at early iterations (less than 5), with just 50%-60% increasing after 5 iterations. However, maxSUV increased systematically at early iterations, and this trend continued as the number of iterations increased.

Conclusion

The OSEM algorithm converges sooner for avgSUV than for maxSUV. The likely reason is that avgSUV depends on low-frequency features that are recovered with fewer iterations. The differences in maxSUV were likely due to noise, which increased with the number of iterative updates, and to increased resolution and recovery of high-frequency features (i.e., tumor heterogeneity) with a larger number of iterations. Factors that determine the quantitative accuracy of iterative reconstruction may have played an additional role. Given the continued change in maxSUV with iterations, great care must be taken in selecting the number of iterative updates when using it to assess tumors and their response to chemotherapy and radiation therapy. Because 2–5 iterations with 8–28 subsets are being used in clinical settings, these data are pertinent when comparing the SUVs of a tumor before and after therapy.

---

### Comparison of strategies for selecting single nucleotide polymorphisms for case / control association studies [^115dbxSU]. Human Genetics (2003). Low credibility.

It is widely believed that a subset of single nucleotide polymorphisms (SNPs) is able to capture the majority of the information for genotype-phenotype association studies that is contained in the complete compliment of genetic variations. The question remains, how does one select that particular subset of SNPs in order to maximize the power of detecting a significant association? In this study, we have used a simulation approach to compare three competing methods of site selection: random selection, selection based on pair-wise linkage disequilibrium, and selection based on maximizing haplotype diversity. The results indicate that site selection based on maximizing haplotype diversity is preferred over random selection and selection based on pair-wise linkage disequilibrium. The results also indicate that it is more prudent to increase the sample size to improve a study's power than to continuously increase the number of SNPs. These results have direct implications for designing gene-based and genome-wide association studies.

---

### Solving olympiad geometry without human demonstrations [^114bykrK]. Nature (2024). Excellent credibility.

Synthetic data generation rediscovers known theorems and beyond

We find that our synthetic data generation can rediscover some fairly complex theorems and lemmas known to the geometry literature, as shown in Fig. 4, despite starting from randomly sampled theorem premises. This can be attributed to the use of composite actions described in Extended Data Table 1, such as 'taking centroid' or 'taking excentre', which — by chance — sampled a superset of well-known theorem premises, under our large-scale exploration setting described in Methods. To study the complexity of synthetic proofs, Fig. 4 shows a histogram of synthetic proof lengths juxtaposed with proof lengths found on the test set of olympiad problems. Although the synthetic proof lengths are skewed towards shorter proofs, a small number of them still have lengths up to 30% longer than the hardest problem in the IMO test set. We find that synthetic theorems found by this process are not constrained by human aesthetic biases such as being symmetrical, therefore covering a wider set of scenarios known to Euclidean geometry. We performed deduplication as described in Methods, resulting in more than 100 millions unique theorems and proofs, and did not find any IMO-AG-30 theorems, showing that the space of possible geometry theorems is still much larger than our discovered set.

Fig. 4
Analysis of the generated synthetic data.

Of the generated synthetic proofs, 9% are with auxiliary constructions. Only roughly 0.05% of the synthetic training proofs are longer than the average AlphaGeometry proof for the test-set problems. The most complex synthetic proof has an impressive length of 247 with two auxiliary constructions. Most synthetic theorem premises tend not to be symmetrical like human-discovered theorems, as they are not biased towards any aesthetic standard.

Source Data

Language model pretraining and fine-tuning

We first pretrained the language model on all 100 million synthetically generated proofs, including ones of pure symbolic deduction. We then fine-tuned the language model on the subset of proofs that requires auxiliary constructions, accounting for roughly 9% of the total pretraining data, that is, 9 million proofs, to better focus on its assigned task during proof search.

---

### Mathematical discoveries from program search with large language models [^111dJ5J4]. Nature (2024). Excellent credibility.

Bin packing

Combinatorial optimization is a subfield of mathematics that plays an important role across a wide range of areas, from theoretical computer science to practical problems in logistics and scheduling. Whereas many combinatorial optimization problems are provably hard to solve for large instances, it is typically possible to achieve strong performance using heuristics to guide the search algorithm. The choice of a heuristic is crucial for obtaining strong performance, but designing a good heuristic is difficult in practice. In this section, we show that FunSearch can be used to discover effective heuristics for one of the central problems in combinatorial optimization: bin packing.

The goal of bin packing is to pack a set of items of various sizes into the smallest number of fixed-sized bins. Bin packing finds applications in many areas, from cutting materials to scheduling jobs on compute clusters. We focus on the online setting in which we pack an item as soon as it is received (as opposed to the offline setting in which we have access to all items in advance). Solving online bin packing problems then requires designing a heuristic for deciding which bin to assign an incoming item to.

Heuristics for online bin packing are well studied and several variants exist with strong worst case performance –. However, they often show poor performance in practice. Instead, the most commonly used heuristics for bin packing are first fit and best fit. First fit places the incoming item in the first bin with enough available space, whereas best fit places the item in the bin with least available space where the item still fits. Here, we show that FunSearch discovers better heuristics than first fit and best fit on simulated data.

To achieve this, we define a heuristic as a program that takes as input an item and an array of bins (containing the remaining capacity of each bin) and returns a priority score for each bin. The 'solve' function picks the bin with the highest score according to the heuristic (Fig. 2b). FunSearch is then used to evolve this heuristic, starting from best fit.

---

### SHEA position statement on pandemic preparedness for policymakers: pandemic data collection, maintenance, and release [^113Gj8j6]. Infection Control and Hospital Epidemiology (2024). High credibility.

Data organization — national data sets with detailed cross-referencing ontology should be created and must clearly explain how different data elements relate to each other and are organized, such that different data sources can be accurately and efficiently connected; data will come from a variety of sources, including state and local health departments and individual healthcare facilities, and identifiers need to be available for cross-linkage; curated data elements should be organized, and a data dictionary, with clear and reproducible definitions, should be available for review; the removal of identifiers from data elements is permanent, meaning de-identified data elements in one data set can no longer be linked data elements in another data set to allow for accurate, granular analysis; aggregated data is often insufficiently precise to allow for the granular, detailed analysis needed to inform clinical care and infection prevention decisions; careful data management planning is required to ensure that relevant identifiers that can be used to cross-reference and link different data sets and data sources are maintained with important cybersecurity measures, and failure to accurately and precisely match various data elements can lead to uninterpretable findings.

---

### Influence maximization in boolean networks [^111F6x5Q]. Nature Communications (2022). High credibility.

The methods developed in this paper suffer from some limitations. The computational time of the unconstrained optimization algorithm grows cubically with the system size. This is certainly an improvement over several existing methods, yet it allows for the analysis of relatively small systems only. Further, constrained optimization requires prior knowledge of the targeted fixed points. Acquiring information on all attractors generally requires a time that grows exponentially with the system size, generating a clear limitation to the applicability of our method. However, we note that many biological networks have known attractors related to specific phenotypes and our method can be applied to these without having to formulate the entire attractor landscape, which is an advantage over methods that require this calculation (e.g.); as with these methods, attractors can be found via sampling for networks that are too large for exhaustive computation. Finally, as it is currently formulated, our method is useful for the study of fixed points only but not of limit cycles. The method can be generalized to the study of these more complicated attractors, but only via a non-trivial generalization of our currently proposed metric of dynamical influence.

In spite of the above limitations, there are some immediate extensions to this work that deserve future attention. For example, our methods can be easily adapted to networks with more than two states per node. Also, our definition of dynamical influence and our algorithm for the selection of optimal driver sets can be extended to study the effect of short-term perturbations, i.e. the state of the seeds is only initially set to a given value but can be altered by the dynamics of the network. Finally, our method may be used in a variety of applications to approximate node influence in Boolean networks that are too large to calculate exact solutions.

---

### Machine learning-guided co-optimization of fitness and diversity facilitates combinatorial library design in enzyme engineering [^114ZiTHN]. Nature Communications (2024). High credibility.

Results

Overview of MODIFY, an ML algorithm to co-optimize library fitness and diversity

We developed MODIFY (Fig. 1), an ML-guided framework to design high-fitness, high-diversity enzyme libraries for the engineering of new enzyme functions. Given a set of specified residues in a parent enzyme, MODIFY affords combinatorial mutant libraries to strike a balance between maximal expected fitness and diversity, without requiring functionally characterized mutants as prior knowledge (Fig. 1 a). MODIFY applies a novel ensemble ML model that leverages protein language models (PLMs) and sequence density models to make zero-shot fitness predictions and employs a Pareto optimization scheme to design libraries with both high expected fitness and high diversity. High levels of expected fitness ensure the effective sampling of functional enzyme variants, while the high diversity of designed enzyme libraries spanning a wide sequence space allows the exploration of new enzyme variants. Balancing fitness and diversity is achieved by solving the optimization problem: with parameter λ balancing between prioritizing high-fitness variants (exploitation) and generating a more diverse sequence set (exploration). In this way, MODIFY traces out an optimal tradeoff curve known as the Pareto frontier, on which each point represents an optimal library where neither desiderata can be further improved without compromising the other (Fig. 1 b). To refine the library, enzyme variants sampled from the library are further filtered based on protein foldability and stability (Fig. 1 c). Applying MODIFY to design a high-quality starting library for both new-to-nature borylation and silylation, we identified a generalist biocatalyst with substantially altered loop dynamics from top-performing MODIFY variants (Fig. 1 d).