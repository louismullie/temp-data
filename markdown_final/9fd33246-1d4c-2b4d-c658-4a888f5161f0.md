# Medical Question & Answer

**Sample ID**: 9fd33246-1d4c-2b4d-c658-4a888f5161f0
**Dataset Index**: 48405

---

## Question

During a semester, a student received scores of 76, 80, 83, 71, 80, and 78 on six tests. What is the student's average score for these six tests?

Answer Choices:
A. 76
B. 77
C. 78
D. 79

---

## Answer

> Let's see… What do we have here? The user is asking how to calculate the average of six test scores and select the correct option from the provided choices. Let's break this down step-by-step. First, I need to think about the formula for the arithmetic mean. Then, I should verify the sum of the six scores. Next, I will divide by the number of tests to compute the average. Finally, I will compare the result to the answer choices and confirm the correct option.

> Let me first confirm the formula so I don't jump to conclusions: the arithmetic mean equals the sum of all scores divided by the number of scores, which in this case is six tests, so average = (76 + 80 + 83 + 71 + 80 + 78) ÷ 6 [^9cb5f2bd].

> Wait, let me verify the sum before dividing. Adding the scores stepwise: 76 + 80 = 156, 156 + 83 = 239, 239 + 71 = 310, 310 + 80 = 390, 390 + 78 = 468. So the total is 468, not 467 or 469, which I briefly worried about, but the math checks out to 468 [^notfound].

> Now I will divide the total by 6 to get the average: 468 ÷ 6 = 78. Hold on, I should double-check that division. 6 × 78 = 468, which matches the sum, so the average is indeed 78 [^notfound].

> Comparing to the options, 78 corresponds to choice C. I should confirm there isn't a trick in the phrasing or rounding; the question asks for the average score and the calculation is exact, so no rounding is needed and C is correct [^notfound].

---

The student's average score is **78**. To calculate the average, add the six test scores (76 + 80 + 83 + 71 + 80 + 78 = 468) and divide by 6, which equals 78. Therefore, the correct answer is **C. 78**.

---

## Calculation of the average score

The formula for the average (mean) is:

[
\\text{Average} = \\frac{\\text{Sum of all scores}}{\\text{Number of scores}}
]

Substitute the given scores:

[
\\text{Sum} = 76 + 80 + 83 + 71 + 80 + 78 = 468
]

[
\\text{Average} = \\frac{468}{6} = 78
]

---

## Verification of the answer

To ensure accuracy, let's verify the calculation:

- 76 + 80 = 156
- 156 + 83 = 239
- 239 + 71 = 310
- 310 + 80 = 390
- 390 + 78 = 468

Divide the total by 6:

[
\\frac{468}{6} = 78
]

The calculation is correct, confirming the average score is **78**.

---

## Conclusion

The student's average score for the six tests is **78**, which corresponds to option **C**.

---

## References

### Common measures or common metrics? A plea to harmonize measurement results [^bb146002]. Clinical Psychology & Psychotherapy (2022). Medium credibility.

T scores are interval‐scaled, assuming this is a reasonable assumption or given for the original scores. In contrast, percentile rank scores are not equidistant and should be considered as ordinal scores. For instance, a high percentile rank score (PR = 75) corresponds to a T score of 57, which expresses a modest distance from the mean of 50. In the high range the T scores 65, 70, 75 and 80 correspond to 93.3, 97.7, 99.4 and 99.9 (see Figure 1). Thus, percentile rank scores have markedly inequal units, especially at the extremes, which results in underestimation of differences between scores near the mean and overestimation of differences at the extremes.

Figure 3 shows, for a selection of raw BDI‐II scores, their relation with T scores and clinical percentile ranks (only the even raw scores are shown). Dutch data were used to obtain T scores and percentile ranks from a clinical sample (de Beurs et al.). The relation between normal distributed T scores and percentile ranks follows a sigmoid curve, which reflects the stretching of the percentile rank metric relative to the T score metric at the extremes (due to the bell shape of the normal distribution percentile rank intervals are closer together at the middle of the scale than they are at the extremes, see also Figure 1).

FIGURE 3
Selected raw scores on the BDI‐II with T scores (x axis) and percentile rank scores relative to the clinical population (y axis); horizontal and vertical grid lines are based on raw scores

When percentile rank scores are erroneously regarded as equidistant, conclusions about the test result are often wrong. The biased perception of percentile ranks was demonstrated in a study by Bowman with third‐year undergraduate students, who appeared inclined to overestimate percentile ranks above 80 or below 20 as quite extreme test result (whereas under the normal curve PR = 80 corresponds to T = 58.4, that is, less than 1 SD removed from the mean, see Figure 1). Furthermore, given the prevailing view around admissible transformations and their link with arithmetical and statistical operations with percentile ranks, simple operations are not permissible, such as calculating an average over a set of scores, or calculating a difference score between repeated assessments. For some discussion of the topic, see Meijer and Oosterloo. Interestingly, this drawback interferes with the advantage of percentile scores that they are generally more intuitive to interpret.

---

### Community norms for the symptom questionnaire (SQ-48): normalised T-Scores and percentile rank order scores [^4c543767]. Clinical Psychology & Psychotherapy (2025). Medium credibility.

Percentiles indicate the percentage of people in the normative reference group who scored at or below a particular score. As they describe an individual's relative standing within a population in a score from 1 to 100, percentiles are easily understood and communicated to individuals who may not be familiar with statistical concepts (Crawford and Garthwaite), for example, 'Your percentile scores is 75, indicating that 75 percent of the general population has a similar or lower score'. However, despite their utility, percentile scores are not entirely without their shortcomings. Percentiles represent ranks and do not have equal intervals. Nevertheless, percentile scores are still frequently misinterpreted as equidistant, as was demonstrated in an experimental study with third‐year undergraduate students (Bowman). Percentiles are nor equidistant: When raw scores are normally distributed, the relation between T ‐scores and percentiles is a sigmoid curve. Around the mean, percentiles increase four times faster than T ‐scores (at T 50: PR = 4*T‐150); at the extremes of a scale, this is reversed (e.g. at T 27: PR ≈ 0.25*T‐5.1 and at T 74: PR ≈ 0.25*T + 80; See Figure 1). Consequently, in the vicinity of the mean, a unit increase in T ‐score corresponds to an approximate four‐unit increase in percentile rank. Conversely, at the lower and upper extremities of the scale, the relationship between T ‐score and percentile rank is inversely proportional. Hence, only T ‐scores, and not PR‐scores, accurately reflect the valence of test score changes during treatment.

---

### A mindfulness-based intervention to increase resilience to stress in university students (the mindful student study): a pragmatic randomised controlled trial [^380f76ed]. The Lancet: Public Health (2018). Medium credibility.

Discussion

Our findings suggest that the MSS course maintains wellbeing and engenders resilience to accumulation of stress during the academic year, notably during the summer examination period — our primary outcome. Although the average baseline CORE–OM score of all participants was slightly higher than reported student norms (0·76 score points), scores for MSS participants at the time of maximum stress (examinations) had fallen towards those norms, whereas scores in the support as usual group indicated increasing distress. The primary outcome results were maintained across intention-to-treat and per-protocol analyses, implying effectiveness, even with only 59% of participants attending at least half the sessions. The moderate effect size translated into about five of 30 students in each group being prevented from indicating distress scores during the examination period at a clinically significant level (a number needed to treat of six). This finding suggests that mindfulness is more effective than other preventive interventions.

Distress in the MSS group was lower beyond the period of the mindfulness courses and into the examination period than at baseline, which is consistent with mindfulness building psychological resilience to academic distress, as are results for perceived problems affecting study or experience. Wellbeing results, and interaction tests showing that the effect might be larger in people who had more stress, support this hypothesis.

Previous trials indicate that mindfulness courses shortly before university examinations might improve performance. Our analysis of undergraduate examination results following MSS earlier in the year suggests a more nuanced effect: more students achieved either higher or lower grades after the MSS course. At a borderline level of statistical significance, we view these results of a planned secondary analysis with caution. However, the effect is consistent with the well known, inverted-U relationship between arousal and performance first described by Yerkes and Dodson. The value of such an effect is, inevitably, personal and beyond the resolution of the trial; for instance, an MSS participant achieving a low mark might, in fact, have intermitted their studies but for mindfulness training. Low statistical power for analysis of intermissions precluded further comment. Nevertheless, we believe that this finding warrants further study.

---

### Development of a tool to measure student perceptions of equity and inclusion in medical schools [^9c68bb37]. JAMA Network Open (2024). High credibility.

Figure.
School-Averaged Overall Promoting Diversity, Group Inclusion, and Equity Tool Scores Across Groups of Multiple Marginalized Identities

Average overall Year 2 Questionnaire (A) and Graduation Questionnaire (B) tool scores by institution for all students and across students who identified with 0, 1, 2, 3, or 4 marginalized identities. Overall, 22 of 134 (16.4%) and 34 of 129 (26.3%) medical schools did not have any student identifying with 4 concurrent marginalized identities in the Year 2 Questionnaire and Graduate Questionnaire, respectively.

Mean (SD) tool scores for female students were lower than scores for male students (80.05 [10.1] vs 81.21 [10.8]; η 2 = 0.003; P < .001); scores for Asian (79.68 [10.21]), Black (77.62 [11.15]), and Hispanic (79.94 [10.99]) students reported lower scores than White students (81.28 [9.73]) (η 2 = 0.008; P < .001); scores for LGB students were lower than scores for non-LGB students (78.17 [10.85] vs 80.81 [10.01]; η 2 = 0.005; P < .001); and scores for students from low-income backgrounds were lower than scores from students not from low-income backgrounds (79.91 [10.69] vs 80.89 [9.83]; η 2 = 0.001; P < .001) in analyzing scores from the Y2Q time point (Table 3). Similarly, mean (SD) tool scores calculated at the GQ time point were lower for female students compared with male students (81.82 [10.19] vs 83.32 [10.41]; η 2 = 0.004; P < .001); lower for Asian (82.0 [10.86]), Black (80.56 [11.28]), and Hispanic (82.18 [10.96]) students compared with White students (82.97 [9.91]) (η 2 = 0.003; P < .001); lower for LGB students compared with non-LGB students (79.64 [11.36] vs 82.77 [10.22]; η 2 = 0.005; P < .001); and lower for students from low-income backgrounds compared with students not from low-income backgrounds (81.85 [11.05] vs 82.81 [10.06]; η 2 = 0.001; P < .001, Table 3). Similar differences in scores were present in the mean tool scores among most individual factors for the Y2Q and GQ (eTable 1 and eTable 2 in Supplement 1).

---

### Evaluation of the assessment and grading of medical students on a neurology clerkship [^0a4e6ea4]. Neurology (2008). Low credibility.

Objective

To describe a clinical encounter (Bedside Examination Exercise [BEE]) used for assessment and teaching in the Massachusetts General Hospital neurology clerkship; to compare results of the BEE with the Harvard Medical School Subjective Evaluation Form (SEF) and National Board of Medical Examiners Shelf examination (Shelf); and to develop a grading system that assesses multiple skills and reflects proficiency.

Methods

The BEE was administered to 409 students. Final grades were compared with those of 71 students evaluated with the SEF alone. We compared results on the SEF, BEE, and Shelf examination in another 132 students. A composite score was developed, weighted SEF 70%, BEE 15%, and Shelf 15%, to derive the final grade.

Results

The BEE helped limit grade inflation, but did not predict final grade determined by the SEF. Grades from the three test instruments had normal distributions, but different means and SDs: SEF 84% ± 10.3%; BEE 83% ± 9.3%; Shelf 69% ± 8.4%. There was poor agreement among individual students between different tests, even within core competencies. The 70-15-15 composite score had a normal distribution, 81% ± 8.5%.

Conclusions

The Bedside Examination Exercise (BEE) is useful for assessing and teaching clinical skills. No single test instrument predicts results of another with acceptable accuracy. Use of complimentary assessment tools (BEE, Subjective Evaluation Form, and Shelf) lessens uncertainty in deriving the composite score, and facilitates evaluation of different attributes. The composite score enables a five-tier grading system that recognizes proficiency, rewards excellence, and provides meaningful feedback. This approach could be generalized to other clerkships.

---

### APA guidelines for psychological assessment and evaluation [^9cb5f2bd]. APA (2020). High credibility.

Psychometric and measurement knowledge — Guideline 5 states that psychologists who provide psychological testing, assessment, and evaluation demonstrate knowledge in and seek to appropriately apply psychometric principles and measurement science as well as the effects of external sources of variability such as context, setting, purpose, and population. The rationale emphasizes that descriptive statistics are the foundational components of test construction and interpretation, noting common measures of central tendency and measures of variation. It adds that critical evaluation of instruments requires working knowledge of the fundamental theories and techniques of test construction. The section explains that individual results of most tests are derived from item responses that are grouped to form scales and are reported as raw scores or transformed and presented as normative comparative or standardized scores. On reliability/precision and measurement error, it states that the reliability/precision of test scores depends on how much scores vary across replications and that psychologists are encouraged to become familiar with approaches to reliability/precision estimation and factors that influence the appropriate index(es). On validity, it defines validity as "the degree to which evidence and theory support the interpretations of test scores for proposed uses of tests," clarifies that validity is not an inherent property of a test but concerns support for use for a particular purpose, and notes that in evaluating tests, psychologists should become suitably aware of the five sources of validity.

---

### How can we teach medical students to choose wisely? A randomised controlled cross-over study of video-versus text-based case scenarios [^ebc18812]. BMC Medicine (2018). Low credibility.

Background

The Choosing Wisely campaign highlights the importance of clinical reasoning abilities for competent and reflective physicians. The principles of this campaign should be addressed in undergraduate medical education. Recent research suggests that answering questions on important steps in patient management promotes knowledge retention. It is less clear whether increasing the authenticity of educational material by the inclusion of videos further enhances learning outcome.

Methods

In a prospective randomised controlled cross-over study, we assessed whether repeated video-based testing is more effective than repeated text-based testing in training students to choose appropriate diagnostic tests, arrive at correct diagnoses and identify advisable therapies. Following an entry exam, fourth-year undergraduate medical students attended 10 weekly computer-based seminars during which they studied patient case histories. Each case contained five key feature questions (items) on the diagnosis and treatment of the presented patient. Students were randomly allocated to read text cases (control condition) or watch videos (intervention), and assignment to either text or video was switched between groups every week. Using a within-subjects design, student performance on video-based and text-based items was assessed 13 weeks (exit exam) and 9 months (retention test) after the first day of term. The primary outcome was the within-subject difference in performance on video-based and text-based items in the exit exam.

Results

Of 125 eligible students, 93 provided data for all three exams (response rate 74.4%). Percent scores were significantly higher for video-based than for text-based items in the exit exam (76.2 ± 19.4% vs. 72.4 ± 19.1%, p = 0.026) but not the retention test (69.2 ± 20.2% vs. 66.4 ± 20.3%, p = 0.108). An additional Bayesian analysis of this retention test suggested that video-based training is marginally more effective than text-based training in the long term (Bayes factor 2.36). Regardless of presentation format, student responses revealed a high prevalence of erroneous beliefs that, if applied to the clinical context, could place patients at risk.

Conclusion

Repeated video-based key feature testing produces superior short-term learning outcome compared to text-based testing. Given the high prevalence of misconceptions, efforts to improve clinical reasoning training in medical education are warranted. The Choosing Wisely campaign lends itself to being part of this process.

---

### School bus rebate program and student educational performance test scores [^6fe865db]. JAMA Network Open (2024). High credibility.

Key Points

Question

Is the replacement of older, more polluting school buses with newer, cleaner alternatives associated with better student educational performance?

Findings

In this study of 1941 school districts that were randomly allocated funding under the 2012–2016 School Bus Rebate Programs, district-average student test scores in reading and language arts and math did not improve among selected districts overall. In secondary analyses, however, districts that were awarded funding to replace the oldest and highest-polluting buses experienced significantly greater improvements in district-average student test scores compared with those not selected for replacement funding.

Meaning

These findings suggest that funding the replacement of older, more polluting school buses may be an actionable way for school districts to improve educational performance.

---

### The educational value of using cumulative sum charts [^ee3bc25a]. Anaesthesia (2012). Low credibility.

Various workplace-based assessment tools are available, but none have been shown to improve performance in procedural skills. This study aimed to assess the impact of using one such tool, cumulative sum charts, on procedural skill ability. A single-blind randomised controlled trial was conducted on 82 final year medical students. Control group students received the usual teaching; in addition to this, intervention group students were provided with cumulative sum charts to log their cannulation attempts over a 7-month period. At the end of the year, students from both groups undertook a validated test of automaticity of cannulation skill. Students in the intervention group obtained median (IQR [range]) scores of 68.2 (60.5–74.3 [42.7–81.1]) vs 62.2 (52.2–68.8 [40.7–80.5]) for the control group (p = 0.013). The effect size was moderate (Cohen's d = 0.608). This study therefore provides support for the hypothesis that use of cumulative sum charts improves performance when learning procedural skills.

---

### Education and midlife cognitive functioning: evidence from the high school and beyond cohort [^1b3227cd]. Alzheimer's & Dementia (2025). Medium credibility.

2 METHODS

2.1 Sample selection

High School and Beyond (HS&B:80) began in 1980 as a nationally representative probability sample of 30,030 high school sophomores and 28,240 high school seniors from a probability sample of 1,020 U.S. public and private high schools. From the initial sample of 58,270 students, a random subset of 14,830 sophomores and 12,000 seniors have been re‐interviewed multiple times. Panelists were almost all born between 1961 and 1965 and were thus between 56 and 60 when last contacted in 2021. Note that all reported sample sizes have been rounded to the nearest 10 in accordance with the requirements of restricted‐use data licenses.

The 1980 student, parent, teacher, and school administrator questionnaires gathered data on students' educational experiences; school curricular offerings; school composition and resources; student‐level outcomes (e.g. grades, coursework); and family socioeconomic background (e.g. parental education and occupation, family composition, siblings, parenting practices, and parents' educational and occupational expectations for their children). Both the sophomore and senior cohorts completed standardized multiple‐choice achievement tests in reading, vocabulary, and math. Secondary and post‐secondary transcripts have been appended to student records.

Senior and sophomore cohort panel members were re‐surveyed in 1982, 1984, and 1986; sophomores were re‐surveyed in 1992 and 2014; and seniors were re‐surveyed in 2015. HS&B:80 surveys have had remarkably high response rates — ranging from ∼90% in the 1980s to ∼65% in the 2014 and 2015 follow‐ups. In 2021, all surviving members of the HS&B:80 cohort were again recontacted. Fieldwork included a multimode survey that assessed neurocognitive functioning and asked about a variety of other topics. Sample members initially completed either a telephone or Web survey; they were offered the other mode if they did not initially respond. Some sample members were invited to complete the telephone survey after the Web survey; these were people who either (a) scored in the bottom quartile of the distribution of either of the cognitive tasks administered on the Web survey or (b) were among 25% of Web survey completers scoring above the bottom quartile and selected at random. Of surviving eligible HS&B:80/21 sample members, 13,980 completed surveys in 2021 — a response rate of 58%.

---

### Web-based alcohol screening and brief intervention for university students: a randomized trial [^bc47ce0c]. JAMA (2014). Excellent credibility.

Importance

Unhealthy alcohol use is a leading contributor to the global burden of disease, particularly among young people. Systematic reviews suggest efficacy of web-based alcohol screening and brief intervention and call for effectiveness trials in settings where it could be sustainably delivered.

Objective

To evaluate a national web-based alcohol screening and brief intervention program.

Design, Setting, and Participants

A multisite, double-blind, parallel-group, individually randomized trial was conducted at 7 New Zealand universities. In April and May of 2010, invitations containing hyperlinks to the Alcohol Use Disorders Identification Test-Consumption (AUDIT-C) screening test were e-mailed to 14,991 students aged 17 to 24 years.

Interventions

Participants who screened positive (AUDIT-C score ≥ 4) were randomized to undergo screening alone or to 10 minutes of assessment and feedback (including comparisons with medical guidelines and peer norms) on alcohol expenditure, peak blood alcohol concentration, alcohol dependence, and access to help and information.

Main Outcomes and Measures

A fully automated 5-month follow-up assessment was conducted that measured 6 primary outcomes: consumption per typical occasion, drinking frequency, volume of alcohol consumed, an academic problems score, and whether participants exceeded medical guidelines for acute harm (binge drinking) and chronic harm (heavy drinking). A Bonferroni-corrected significance threshold of .0083 was used to account for the 6 comparisons and a sensitivity analysis was used to assess possible attrition bias.

Results

Of 5135 students screened, 3422 scored 4 or greater and were randomized, and 83% were followed up. There was a significant effect on 1 of the 6 prespecified outcomes. Relative to control participants, those who received intervention consumed less alcohol per typical drinking occasion (median 4 drinks [interquartile range {IQR}, 2–8] vs 5 drinks [IQR 2–8]; rate ratio [RR], 0.93 [99.17% CI, 0.86–1.00]; P = 0.005) but not less often (RR, 0.95 [99.17% CI, 0.88–1.03]; P = 0.08) or less overall (RR, 0.95 [99.17% CI, 0.81–1.10]; P = 0.33). Academic problem scores were not lower (RR, 0.91 [99.17% CI, 0.76–1.08]; P = 0.14) and effects on the risks of binge drinking (odds ratio [OR], 0.84 [99.17% CI, 0.67–1.05]; P = 0.04) and heavy drinking (OR, 0.77 [99.17% CI, 0.56–1.05]; P = 0.03) were not significantly significant. In a sensitivity analysis accounting for attrition, the effect on alcohol per typical drinking occasion was no longer statistically significant.

Conclusions and Relevance

A national web-based alcohol screening and brief intervention program produced no significant reductions in the frequency or overall volume of drinking or academic problems. There remains a possibility of a small reduction in the amount of alcohol consumed per typical drinking occasion.

Trial Registration

anzctr.org.au Identifier: ACTRN12610000279022.

---

### 7th international conference on malignancies in AIDS and other… [^e1860207]. NCI (2025). Medium credibility.

April 28–29, 2003 Natcher Conference Center Bethesda, Maryland ABSTRACT INDEX Aboulafia 87, 90 Anderson 72 Astrow 11 Ayers 86 Biberfeld 7 Biggar 58 Boue 89 Broker S2. Brown 60 Bryant 88 Butler 4 Casper 5 Casper 5, 12 Chow 16 Crabb Breen 8 Dalla Favera S13 de Sanjose 26 DeFalco 81 Dezube
64. McGhee 83 Melief S5 Mesri 36 Montaner 35 Morgan 69 Muderspach S6 Mullerat 78, 79 Mutlu 53 Noonan 61 Orem 28 Orth S9 Otieno 29. Rubin 91 Samaniego 39, 40 Sandulyak 84 Sastry 68 Schlegel S12 Sharp S1 Shipp S14 Simonelli 32 Song 76 Srinivasan 27 Strickler S6 Sugaya 55 Tang 38 Tary-Laehmann 82 Taylor 73 Teo 59 Usoro 85 Von Roenn 63 Widmer 48 Wilkin 74 Woodberry 50 Wu S4 Yuan 9 Zheng
67. DC are CD80/CD86 on the DC, costimulating CD28 on the CD8 cells. For maintenance and full expansion of CD8+ T cells, interaction of 4–1 BBL family) on DC with 4–1 BB on CD8+ CTL is also important. In the absence of CD80/CD86 costimulation, the 4–1.

7th International Conference on Malignancies in AIDS and Other Immunodeficiencies April 28–29, 2003 Natcher Conference Center Bethesda, Maryland S12 THE PAPILLOMAVIRUS E6 PROTEIN AND C-MYC CO­ MODULATE THE hTERT PROMOTER AND INDUCE CELLULAR T.

---

### APA guidelines for psychological assessment and evaluation [^14acf250]. APA (2020). High credibility.

APA Guidelines — validity evidence and application of psychological test scores: Psychologists are encouraged to demonstrate that the scores generated are directly, demonstrably, and consistently related to the outcome or purpose for which the test instrument is used, and conclusions and/or recommendations resulting from use of instruments are expected to be fair; minimize bias; and are consistent with applicable standards of practice, policies, and laws. Psychologists strive to develop an understanding of strategies for obtaining, evaluating, and establishing each source of validity evidence, the limits of any one source, and how different sources can be integrated into a comprehensive validity argument to support the use of a test for a particular purpose. Psychologists strive to understand and know when to apply the various methods for representing test information and are also encouraged to be aware of the advantages and disadvantages of automated scoring. There is no absolute standard for establishing that an inference one desires to make based on test scores is "valid" for a particular purpose; thus, psychologists strive to be able to locate and evaluate available validity evidence to judge the suitability of using a test for an intended purpose and seek to consider the positive and negative consequences of test administration and score use so the testing process will provide more good than harm to examinees. When psychologists evaluate the validity of inferences drawn from test scores, it is typically for use of scores in a given setting or with a given group of individuals, so psychologists strive to remain cognizant of the specific setting in which a test will be used, particularly if this setting differs from the original setting or purpose for which the test was designed, and when test purposes or populations differ from the publisher's standardization sample, psychologists are encouraged to evaluate the validity evidence for this specific use.

---

### Effect of teaching Bayesian methods using learning by concept vs learning by example on medical students' ability to estimate probability of a diagnosis: a randomized clinical trial [^d417589e]. JAMA Network Open (2019). High credibility.

Statistical Analysis

First, initial pretest and posttest probability estimates were analyzed using descriptive statistics to assess overall performance in revising probability estimates. Next, a score for subjective change from pretest probability to posttest was computed, and a bayesian change score was calculated using the student's pretest probability estimate and the calculated posttest probability based on published estimates of sensitivity and specificity. The differences between subjective and bayesian change scores were then analyzed using a mixed-model analysis of variance with the experimental group as a between-subject factor and the case as a within-subject factor. A separate analysis was performed on the 12 cases with the same 3 diagnostic categories as in the learning phase (learning cases) and the 4 cases with a new diagnostic category (new cases) to determine the extent to which the learned skills were generalizable to a diagnosis and a diagnostic test result that had not been previously encountered. Separate analyses were conducted for revisions based on positive or negative diagnostice test results. Filler cases were not analyzed.

For the primary analysis, revisions resulting from positive and negative test results were combined by reversing the sign for the revisions from the negative test results. A mixed-model analysis of variance was again used, with 1 between-participant factor (intervention group, 3 levels) and 1 within-participant factor (case). Analysis was performed on all 16 cases, on the 12 cases with the same diagnosis as the learning phase, and on the 4 cases with a new diagnostic category. Results are reported as mean (SE). Timing of student responses by experimental group was also analyzed. Analyses were performed using SPSS, version 25 (IBM Corp). Two-tailed, unpaired P < .05 was considered statistically significant.

---

### Reading disability is characterized by reduced print-speech convergence [^e27eca5f]. Child Development (2024). Medium credibility.

METHODS

Participants

We recruited 129 right‐handed native Chinese speakers from local public elementary schools and associate degree colleges, including 65 fifth‐grade children and 64 college students. The experiment protocol was approved by the Institutional Review Boards of the Sun Yat‐sen University.

The inclusion criteria for RD were (1) the standard score on the Raven matrices test was above 80; and (2) the z score was below −1.5 on at least one of three reading tests: an untimed character naming test, a sentence reading fluency test, and a 1‐min character naming test. For the first two tests, published norms for fifth‐grade children are available (Song et al; Xue et al.). For adults, we developed our own norms on these two tests. We tested 215 college students (mean age: 19.83 years) on the sentence reading fluency test (mean score: 1379.88 characters, standard deviation: 377.21) and 203 college students (mean age: 19.83 years) on the untimed character naming test (mean score: 140.02 characters, standard deviation: 6.41). For the 1‐min character naming test, we tested 201 college students (mean age: 20.75 years) (mean score: 100.45, standard deviation: 17.70 for regular characters; mean score: 80.91, standard deviation: 19.10 for irregular characters) and 217 fifth‐grade typical children (mean age: 10.89 years) (mean score: 71.18, standard deviation: 18.11 for regular characters; mean score: 48.72, standard deviation: 18.29 for irregular characters) to develop the norms. For developing the norms, we recruited new participants who did not participate in other parts of the study.

---

### School bus rebate program and student educational performance test scores [^09f8b314]. JAMA Network Open (2024). High credibility.

Importance

Students who ride older school buses are often exposed to high levels of exhaust during their commutes, which may adversely affect health and school attendance. As a result, the US Environmental Protection Agency (EPA) has awarded millions of dollars to school districts to replace older, highly polluting school buses with newer, cleaner buses.

Objective

To leverage the EPA's randomized allocation of funding under the 2012–2016 School Bus Rebate Programs to estimate the association between replacing old, highly polluting buses and changes in district-average standardized test scores.

Design, Setting, and Participants

This study examined changes in reading and language arts (RLA) and math test scores among US school district applicants to the EPA's 2012–2016 national School Bus Rebate Programs 1 year before and after each lottery by selection status. Data analysis was conducted from January 15 to July 30, 2023.

Exposure

Selection to receive EPA funding to replace older school buses with newer, cleaner alternatives.

Main Outcomes and Measures

School district changes in RLA and math test scores among students in grades 3 through 8 before and after the EPA funding lotteries by selection status were measured using an intention-to-treat approach.

Results

This study included 1941 school district applicants to the 2012–2106 EPA School Bus Rebate Programs. These districts had a mean (SD) of 14.6 (33.7) schools per district, 8755 (23 776) students per district, and 41.3% (20.2%) of students with free lunch eligibility. Among the applicants, 209 districts (11%) were selected for the clean bus funding. District-average student test scores did not improve among selected districts overall. In secondary analyses, however, districts replacing the oldest, highest polluting buses (ie, pre-1990) experienced significantly greater improvements in district-average test scores in the year after the lottery for RLA and math (SD improvement in test scores, 0.062 [95% CI, 0.050–0.074] and 0.025 [95% CI, 0.011–0.039], respectively) compared with districts without replacements.

Conclusions and Relevance

In this study, the EPA funding was not associated with student test scores overall, but in secondary analyses, the replacement of the oldest school buses was associated with improved educational performance. These findings support prioritizing clean bus replacement of the oldest buses as an actionable way for improving students' educational performance.

---

### Teledidactic versus hands-on teaching of abdominal, thoracic, and thyroid ultrasound-the TELUS II study [^384adbed]. Journal of General Internal Medicine (2024). Medium credibility.

Image Analysis Results Using B-QUIET

The images taken in the pre-test were not used for evaluation. So, the image rating according to the B-QUIET scheme refers exclusively to images acquired in the final exam. The students who were assigned to the face-to-face group achieved an average of 33.93 (SD ± 3.71, 77.11%) out of a possible 44 points. Students in the teledidactic group scored an overall mean of 35.43 (SD ± 3.19, 80.52%) points per US image; they, therefore, scored 1.5 points more on average, which corresponds to a difference of 3.4% out of a possible score of 44 points (Table 2). Considering the categories of resolution, depth, and gain, which are included in the B-QUIET scheme among others, the students lost points, especially when adjusting the depth correctly. Students in the face-to-face group received an average of 2.01 (SD 0.87) points out of a possible four in the depth category, compared to 2.9 (SD 0.77) points in gain and 2.85 (SD 0.80) points in resolution. Similarly, students in the teledidactic group only obtained 2.06 (SD 1.22) points in the depth category, 2.94 (SD 0.88) points in resolution, and 2.88 (SD 0.76) points in adjusting the gain. The t -test for independent samples shows that there is a significant difference in the FAST (p 0.015) and aorta (p 0.017) modules. In both modules, students in the teledidactic group performed better, scoring 33.59 (± 2.61) in FAST, whereas students in the face-to-face group scored an average of 30.95 (± 1.76). In the aorta module, the teledidactic group scored 35.41 (± 2.61) points, and students assigned to the face-to-face group, in comparison only 32.35 (± 3.08). Similar to the OSAUS exam results, no significant difference was found between the teledidactic and face-to-face group for the other modules (lung (B) p 0.225, kidney and urinary tract (C) p 0.160, thyroid (E) p 0.160).

---

### The effectiveness of a primer to help people understand risk: two randomized trials in distinct populations [^3bdf877c]. Annals of Internal Medicine (2007). Low credibility.

Background

People need basic data interpretation skills to understand health risks and to weigh the harms and benefits of actions meant to reduce those risks. Although many studies document problems with understanding risk information, few assess ways to teach interpretation skills.

Objective

To see whether a general education primer improves patients' medical data interpretation skills.

Design

Two randomized, controlled trials done in populations with high and low socioeconomic status (SES).

Setting

The high SES trial included persons who attended a public lecture series at Dartmouth Medical School, Hanover, New Hampshire; and the low SES trial included veterans and their families from the waiting areas at the White River Junction Veterans Affairs Medical Center, White River Junction, Vermont.

Participants

334 adults in the high SES trial and 221 veterans and their families in the low SES trial were enrolled from October 2004 to August 2005. Completion rates for the primer and control groups in each trial were 95% versus 98% (high SES) and 85% versus 96% (low SES).

Intervention

The intervention in the primer groups was an educational booklet specifically developed to teach people the skills needed to understand risk. The control groups received a general health booklet developed by the U.S. Department of Health and Human Services Agency for Health Care Research and Quality.

Measurements

Score on a medical data interpretation test, a previously validated 100-point scale, in which 75 points or more is considered "passing". Secondary outcomes included 2 other 100-point validated scores (interest and confidence in interpreting medical statistics) and participants' ratings of the booklet's usefulness.

Results

In the high SES trial, 74% of participants in the primer group received a "passing grade" on the medical data interpretation test versus 56% in the control group (P = 0.001). Mean scores were 81 and 75, respectively (P = 0.0006). In the low SES trial, 44% versus 26% "passed" (P = 0.010): Mean scores were 69 and 62 in the primer and control groups, respectively (P = 0.008). The primer also significantly increased interest in medical statistics by 6 points in the high SES trial (a 4-point increase vs. a 2-point decrease from baseline) (P = 0.004) and by 8 points in the low SES trial (a 6-point increase vs. a 2-point decrease from baseline) (P = 0.004) compared with the control booklet. The primer, however, did not improve participants' confidence in interpreting medical statistics beyond the control booklet (a 2-point vs. a 4-point increase in the high SES trial [P = 0.36] and a 2-point versus a 6-point increase in the low SES trial [P = 0.166]). The primer was rated highly: 91% of participants in the high SES trial found it "helpful" or "very helpful", as did 95% of participants in the low SES trial.

Limitations

The primarily male low SES sample and the primarily female high SES sample limits generalizability. The authors did not assess whether better data interpretation skills improved decision-making.

Conclusion

The primer improved medical data interpretation skills in people with high and low SES. ClinicalTrials.gov registration number: NCT00380432.

---

### School readiness [^1239569a]. Pediatrics (2019). High credibility.

School readiness testing — misconceptions and exclusion practices are delineated, noting that six fundamental misconceptions are identified and that historical misuse of assessments led to delayed entry. In a 1988 national survey, 10% to 50% of children in various states who were eligible to enter kindergarten on the basis of age did not enter because of readiness test scores, but since that time there has been increased recognition that school readiness assessment should not be used to exclude age-eligible children from kindergarten, and in 2010 only 6% of children in kindergarten were delayed entry.

---

### The UKCAT-12 study: educational attainment, aptitude test performance, demographic and socio-economic contextual factors as predictors of first year outcome in a cross-sectional collaborative study of 12 UK medical schools [^573f78a3]. BMC Medicine (2013). Low credibility.

Predictive value of background variables, after taking prior educational attainment into account

Prior educational attainment correlates with a wide range of background variables (TR Table 5). An important question, though, concerns the extent to which background variables continue to predict outcome after educational attainment has been taken into account. OverallMark was regressed on the 22 background variables, with an alpha set at 0.001 to account for repeated testing. Four background measures were significant, in order of entry: Ethnic2, non-White students performing less well (beta = −.126, P < 10–14); being a mature student, students over the age of 21 performing better than non-mature students (beta = .057, P < .001); UKCATdayOfTakingPctileRank, students who took UKCAT late performing less well (beta = −.089, P < 10–7); and DFES. AvePointEntry, students from high-attaining secondary schools performing less well (beta = −.085, P < 10–7). Figure 3 explores DFES. AvePointEntry in more detail. Figure 3 c shows that average points per exam entry are substantially lower in non-selective schools than selective schools. The average points are divided into four groups (boundaries 205, 230 and 250), with almost no selective secondary schools in the lowest group and almost no non-selective secondary schools in the highest group. Effects upon overall score were estimated with a regression model in which there were significant effects of three best A-level grades (beta = .205, P < .001) and secondary school-level average points (beta = −.085, P = 0.005), and an almost significant effect of selective secondary schooling (beta = −.056, P = 0.059), the fitted regression lines being shown in Figure 3 a. There was no evidence of interactions. Actual mean scores are shown for candidates whose secondary schools were in the four groups of average points, and it can be seen, particularly for entrants with AAA grades in both non-selective and selective secondary schools that overall scores at medical school are lower in those from secondary schools with higher average point scores. From the regression lines it can be estimated that one grade at A-level (the difference between AAA and AAB) is equivalent to 85 points on the average point score, so that an entrant with ABB at A-level from a secondary school with an average score of 175 (at about the 1st percentile of the non-selective schools) performs similarly at medical school to a candidate with AAA at A-level from a secondary school with an average score of 265 (at the 99th percentile of the selective secondary schools).

---

### Why questionnaire scores are not measures: a question-raising article [^2b8ac89e]. American Journal of Physical Medicine & Rehabilitation (2023). Medium credibility.

Repeatability Inflates Reliability if Items Are Too Easy or Too Difficult

Repeatability (not reliability) is easier to achieve if the difference between subjects' abilities and item difficulties is large. If a test is too difficult or easy for most subjects, total scores will be highly repeatable across raters and time points. Paradoxically, indexes based on repeatability of raw scores will be higher the less information the scale provides. If one already knows that a subject will not pass (or fail) a given item, that item is uninformative.

Things may be even subtler than that: repeatability may be inflated by local gaps in the ruler, thus being different depending on the overall ability of the individual subject. Suppose the actual "ruler" representing a questionnaire is depicted in Figure 3. In order of increasing difficulty, items are labeled from 0 to 6. Score 3 encases a wide range of ability levels. This situation will make this score highly "repeatable" across raters, time, etc. However, the measures' precision is low. Rasch analysis does not inflate reliability by this kind of forced repeatability.

FIGURE 3
A ruler with irregularly spaced "ticks," representing items marking ranges of "ability," flagged by ordered numerals. The position of the bottom symbols along the trait continuum gives subjects' ability levels. Subjects "trapped" within the "3" ability interval (degree symbols) will likely receive the same score regardless of the raters, the time of assessment, etc. given that a massive change in ability is required to change to a score of 2 or 4. By greater force, subjects scoring "2" will hardly leap to score "4," and subjects achieving "4" will hardly leap to score "2." Randomness will hardly lead to a score change. A "reliability" index will be inflated while giving less information on the actual ability level of subjects: what seems a virtue is a fault.

---

### APA guidelines for psychological assessment and evaluation [^42edaf3b]. APA (2020). High credibility.

Psychological assessment — standardized administration, scoring accuracy, and procedural/legal responsibilities — states that standardized administration is most important for validity, which may be compromised when adherence to standardization is not met; important points include a high level of knowledge of instructions, awareness of instructions on feedback, query, prompts, and response to questions, facility with supplemental material and audio/visual components, inclusion of behavioral observations, and compliance with equipment and placement of materials. Scoring of responses lends itself to error and is unforgiving, and examiners strive to be alert to miscalculation of base and ceiling, simple arithmetic errors, reversal and discontinuance mistakes, failure to record examinee comments in addition to scores, and imprecise timing on timed tests. Knowledge about procedural requirements, confidentiality of information, communication of results, and security are important, and psychologists are encouraged to understand the legal and ethical issues related to the release of test data; psychologists strive to be able to explain results and limitations to diverse audiences and to include in reports the purpose of the test and the setting in which it occurred.

---

### Measuring human capital using global learning data [^326af7c1]. Nature (2021). Excellent credibility.

We compare the average scores for the same subject (reading), schooling level (primary) and time period (2000–2010) and find a correlation coefficient of 0.984 (Extended Data Fig. 4). This comparison indicates that even as we expand coverage to 164 countries in our database, we maintain high consistency with alternative measures for the subset of proficiencies, school levels and subjects for which there is overlap.

Of note, the assessments included in this database are conducted at school. Testing, and thus learning, data could be affected by enrolment patterns, and we advise users of the data to analyse learning outcomes alongside enrolment trends. For example, average test scores could be driven by lower-performing students entering the system rather than learning progress for those who were already in school. Although this is a potential concern when analysing average scores, there are several reasons why harmonized learning outcomes are still useful. First, primary enrolment rates are relatively high in all countries, reaching 90% on average. Second, learning measured with school-based tests is likely to yield a conservative upper bound of learning in a given country. As most countries at the bottom of the distribution of measured learning are also those with relatively low enrolments, it is unlikely that new school entrants would alter conclusions related to cross-country patterns — the lowest performing countries would probably be revealed to be performing even worse.

Comparison of trends in schooling and learning

Expected years of schooling versus learning basic skills

Related to Extended Data Fig. 1. We define two key variables for analysis in this figure. First, we define our learning variable. Our measure of quality comprises the primary HLO scores, which measure the degree to which students acquire basic skills in primary school. Second, we define the schooling variable. The expected years of schooling measure is constructed by UNESCO (United Nations Educational, Scientific and Cultural Organization) and is a function of enrolment patterns and the number of years of schooling a given country formally provides. This measure is often interpreted by the international education community as a measure of a strong education system in which students attend many years of school. As the expected number of years of schooling is partially a function of enrolment, which we also use as a measure of schooling at times in this paper, these two measures are highly correlated. For countries with data available for both measures, we find an average correlation coefficient of 0.72 across them.

---

### Predictive validity of the UKCAT for medical school undergraduate performance: a national prospective cohort study [^b267c8ce]. BMC Medicine (2016). Low credibility.

However, despite having four subscales, the UKCAT may best be conceptualised as testing two main dimensions of cognitive functioning, namely verbal and non-verbal reasoning. Thus, the total score (consisting of three non-verbal and only one verbal scale score) may put too much emphasis on non-verbal performance. Rescoring so that an average of the non-verbal scales is combined with the verbal reasoning score may be a fairer way to obtain a more balanced metric of ability.

Although the magnitude of the effect of UKCAT scores on performance was relatively small, our estimates of NNR suggest some considerable practical utility of the UKCAT as a tool in helping to select out candidates more likely to require at least one resit at medical school. The NNR value of 1.18 (when a cut-off representing the average UKCAT total score for applicants was used), though derived relatively crudely, also suggests that, in a highly competitive selection process, the use of the UKCAT as a 'screening tool' for subsequent academic performance may be acceptable. We also reported the likely impact of having either relatively high or low thresholds for the UKCAT, when using the test in this manner. Indeed, across medical schools and time, a variety of cut-points have been used by universities in the UKCAT consortium who use the test scores in this manner, mainly to guide the decision about whether to invite a candidate for interview. However, the median scores used as a threshold by institutions have tended to rise over time, and tend to be slightly above the average score obtained by applicants sitting the test. Our findings suggest that the use of higher thresholds may reduce the risk of future adverse academic outcomes in students further, but at the cost of rejecting a higher ratio of candidates who would have been likely to have done well. Conversely, lower score thresholds reduce the risk of rejecting this latter group of candidates, but will increase the risk that applicants are admitted who are at higher risk of later academic problems. Thus, the choice of threshold would be a subjective one, decided on by medical school admissions teams. No doubt the competition ratio that a specific university encounters would play a role in such decision making; those with more fierce competition for places may be tempted to set a higher threshold, with the opportunity of identifying candidates less likely to do well academically appearing to offset the attendant risks of rejecting acceptable candidates. Thus, we can see that, in the context of medical school applications, where the competition ratio at individual medical schools is approximately 11:1, an NNR of roughly one may be acceptable to admissions teams (though possibly not to candidates), especially given the direct and indirect costs of resits and failures to progress. However, it should be highlighted that selection tests such as the UKCAT are not intended to be used in isolation but in conjunction with other selection criteria. Thus, the estimated NNR in this case only reflects the effectiveness of the UKCAT when used alone, rather than in conjunction with other selection approaches. It may be that the use of other selection criteria (such as performance in multiple mini interviews) would further reduce the value to one that reflects a greater utility in selection. Further, there may be genuine uncertainty over the eventual predictive validity of certain selection tests. Thus, it may be that an approach based on Bayesian principles may be useful. Bayes theorem allows us to increase the accuracy of our probabilistic predictions by conditioning our new observations on previous data or knowledge. The approach also allows us to adjust for uncertainty of how applicable our previous knowledge is to the current issue. Thus, a Bayesian framework may eventually allow us to estimate the impact of combining a variety of selection tests in the admissions process, even allowing for our uncertainty regarding predictive validity.

---

### Why questionnaire scores are not measures: a question-raising article [^e83922fd]. American Journal of Physical Medicine & Rehabilitation (2023). Medium credibility.

From Observations to a Scale

Cumulative questionnaires (or "scales") consist of a list of observable behaviors (i.e. the questionnaire's items) considered as the manifestations (effects) of a shared latent property. When the number of behaviors expressed by an individual is counted (such as when a questionnaire's total score is calculated, see hereinafter), three assumptions are implicitly accepted: (1) that all observations are caused by the same variable; (2) that all observations are caused by a single variable; and (3) that the larger the sum score, the more of the latent variable is there. Remember with this regard the oranges' example reported previously.

In the most straightforward "dichotomous" items, a 0/1 labelling (no/yes, pass/fail) is assigned to absence/presence of a given behavior (e.g. wrong/correct answer to an algebraic question in a knowledge of math scale; being dependent/independent in walking in a disability questionnaire, etc.). By "observed behavior," a "yes = 1" endorsement by the subject or, passing a given test, is intended. The symbol "1" is simply a label. "How many times" a "1" is observed, that is, how many times a subject manifests these behaviors across items (the cumulated, total scale score), provides the estimate of subjects' overall "ability." Conversely, "how many times" an item receives a 1/yes/pass label across subjects provides an estimate of item "difficulty" (the lower the item score across subjects, the "more difficult" the item).

Together with dichotomous items, "polytomous" items are also widespread (e.g. "with someone's help/with orthotic aid/with supervision/autonomous" = 0/1/2/3 or, "fully disagree/partially disagree/moderately agree/fully agree" = 0/1/2/3). If they are made of polytomous items, questionnaires are dubbed "rating scales." The concept of "counting" does not change. The total scale score comes from summing "how many times" a "1" was observed, plus "how many times" a "2" was observed, etc. The difference lies in the "weight" assigned to the counts. Observing the behavior labeled "2" counts twice as much, compared with observing the behavior scored 1, etc.

---

### Feasibility of a paediatric radiology escape room for undergraduate education [^ca8dcd3e]. Insights Into Imaging (2020). Medium credibility.

Discussion

A paediatric radiology themed escape room is a feasible, enjoyable and educational method for delivering undergraduate radiology education. All students improved their knowledge of paediatric radiology immediately after the teaching session, with sustained improvement after a 2-week period compared to baseline.

When comparing prior studies relating to healthcare education-themed escape rooms [4,6, 7, 10, 12, 22, 23] (Table 4), only Eukel et al. have objectively measured student knowledge prior to and immediately after the teaching session. They found that the average test score (based on a 23 multiple choice question test) improved significantly from an average score of 56 to 81% after the session (p < 0.01). Whilst our results are similar in finding an increase in knowledge immediately after the escape room, the authors did not test the students' knowledge retention at a later date. Our results showed that whilst a third of students scored lower on their '2 weeks post-session test' than immediately after the teaching, all students maintained a higher test score than before the teaching, indicating a level of knowledge improvement and retention.

---

### Predicting real-life cognitive scores from functional connectivity [^e3ef77aa]. Human Brain Mapping (2025). Medium credibility.

Applicants' performance in each domain is graded on a scale from 50 to 150. The global score is scaled and graded on a range from 200 to 800. Importantly, the grading is performed relative to all psychometric examinees since the very first test was administered in 1983. This ensures that the test date does not affect the score, allowing comparison of examinees' performance across test sessions.

The Psychometric Entrance Test has demonstrated high test–retest reliability, with estimates around 0.9 when the retest occurs within 1 year of the initial assessment. The predictive validity of the test is also noteworthy, with a correlation coefficient of 0.43 with first‐year grade point average (GPA) and 0.41 with GPA at the end of undergraduate studies (Allalouf et al.). Additionally, approximately 90% of examinees complete a preparation course before taking the test (Allalouf et al.), suggesting that Psychometric Entrance Test scores may reflect learning outcomes in real‐life settings.

To evaluate potential associations between test performance and demographic factors in our sample, we conducted the following analyses: first, sex‐related differences in performance were assessed using independent‐samples t‐tests for each of the four psychometric scores, with p ‐values FDR‐corrected for four comparisons. In addition, we tested whether performance was correlated with participants' age at the time of the exam or with the time difference between the imaging session and the exam. Correlations' p ‐values were also FDR‐corrected for four comparisons.

Outliers in age and psychometric scores were defined as values more than three standard deviations above or below the mean. To ensure that prediction results were not driven by extreme values, we repeated the prediction of each psychometric score after excluding age outliers and outliers of the target score. For the global score, we additionally excluded participants who were outliers in any of the domain‐specific scores, given the dependence between global and domain‐specific performance. We tested whether outlier exclusion significantly affected prediction accuracy using a permutation test (see comparison of model performance). p ‐values were FDR‐corrected for 16 comparisons (four target scores × four prediction models).

---

### Interplay between children's cognitive profiles and within-school social interactions is nuanced and differs across ages [^1885445e]. Communications Psychology (2025). Medium credibility.

In terms of achievement, children groups are very clearly defined across all tasks. Group I is associated to high achievement, group II is associated to low achievement, and group III is associated to average achievement (Fig. 1 c). In contrast to tasks, children are not always associated to a single student group. However, their group memberships follow a very structured pattern (Fig. 1 b). We find many children who belong almost exclusively to group II (low achievement), but find no children who belong exclusively to groups I or III. Furthermore, we find that children do not mix groups I and II (low and high achievement), but that many students are either split between groups II and III (low and average achievement) or between groups I and III (high and average achievement). In terms of gender, we find equivalent membership patterns for groups II and III, and only a significant, but very small increase of boys in group II (low achievement) group [two-sided Mann–Whitney U -test, p < 0.001; Hodges–Lehmann estimator < 0.001 for group II (low), and p > 0.05 for groups I and III] (see Supplementary Fig. S16). This observation is consistent with studies that find small but significant advantages for girls in compulsory education settings (see ref.for a comprehensive meta-analysis).

In cognitive terms, our latent classification of children and tasks supports the idea of a single core dimension of cognitive ability or the so-called positive manifold of cognitive testing –. Furthermore, we find that this latent structure describes very well the scoring patterns of students, despite individual variability. Indeed, while 35% of the students have scores that are in high discrepancy with those of their assigned group (Supplementary Fig. S15), 92% of these only show high discrepancy in at most two tasks (Supplementary Fig. S15), showing a good agreement between model expectations and observed data. Therefore, by grouping students along a single achievement axis, our clustering approach reproduces the psychometric construct of the g-factor, and sets it as the defining factor for cognitive ability.

---

### Association of MCAT scores obtained with standard vs extra administration time with medical school admission, medical student performance, and time to graduation [^762f9b66]. JAMA (2015). Excellent credibility.

Importance

Individuals with documented disabilities may receive accommodations on the Medical College Admission Test (MCAT). Whether such accommodations are associated with MCAT scores, medical school admission, and medical school performance is unclear.

Objective

To determine the comparability of MCAT scores obtained with standard vs extra administration time with respect to likelihood of acceptance to medical school and future medical student performance.

Design, Setting, and Participants

Retrospective cohort study of applicants to US medical schools for the 2011–2013 entering classes who reported MCAT scores obtained with standard time (n = 133,962) vs extra time (n = 435), and of students who matriculated in US medical schools from 2000–2004 who reported MCAT scores obtained with standard time (n = 76,262) vs extra time (n = 449).

Exposures

Standard or extra administration time during MCAT.

Main Outcomes and Measures

Primary outcome measures were acceptance rates at US medical schools and graduation rates within 4 or 5 years after matriculation. Secondary outcome measures were pass rates on the United States Medical Licensing Examination (USMLE) Step examinations and graduation rates within 6 to 8 years after matriculation.

Results

Acceptance rates were not significantly different for applicants who had MCAT scores obtained with standard vs extra time (44.5% [59,585/133,962] vs 43.9% [191/435]; difference, 0.6% [95% CI, -4.1 to 5.3]). Students who tested with extra time passed the Step examinations on first attempt at significantly lower rates (Step 1, 82.1% [344/419] vs 94.0% [70,188/74,668]; difference, 11.9% [95% CI, 9.6% to 14.2%]; Step 2 CK, 85.5% [349/408] vs 95.4% [70,476/73,866]; difference, 9.9% [95% CI, 7.8% to 11.9%]; Step 2 CS, 92.0% [288/313] vs 97.0% [60,039/61,882]; difference, 5.0% [95% CI, 3.1% to 6.9%]). They also graduated from medical school at significantly lower rates at different times (4 years, 67.2% [285/424] vs 86.1% [60,547/70,305]; difference, 18.9% [95% CI, 15.6% to 22.2%]; 5 years, 81.6% [346/424] vs 94.4% [66,369/70,305]; difference, 12.8% [95% CI, 10.6% to 15.0%]; 6 years, 85.4% [362/424] vs 95.8% [67,351/70,305]; difference, 10.4% [95% CI, 8.5% to 12.4%]; 7 years, 88.0% [373/424] vs 96.2% [67,639/70,305]; difference, 8.2% [95% CI, 6.4% to 10.1%]; 8 years, 88.4% [375/424] vs 96.5% [67,847/70,305]; difference, 8.1% [95% CI, 6.3% to 9.8%]). These differences remained after controlling for MCAT scores and undergraduate grade point averages.

Conclusions and Relevance

Among applicants to US medical schools, those with MCAT scores obtained with extra test administration time, compared with standard administration time, had no significant difference in rate of medical school admission but had lower rates of passing the USMLE Step examinations and of medical school graduation within 4 to 8 years after matriculation. These findings raise questions about the types of learning environments and support systems needed by students who test with extra time on the MCAT to enable them to succeed in medical school.

---

### Problem-based learning in radiology achieves similar results in classroom and metaverse settings [^1ab529a3]. Insights Into Imaging (2025). Medium credibility.

Fig. 4
Scatter diagram that correlates the score obtained by each team in the peer evaluation and in the teacher evaluation in the real life and second life subgroups. The Pearson correlation coefficient for the 4 years of the study was 0.424 (p = 0.01); covariance = 1.129

Table 2
Comparison of evaluations and difficulty between face-to-face and virtual environments

Values in italics indicate statistically significant differences comparing both gropus (p < 0.05)

a Teacher evaluation and peer evaluation were normalized to 10 points

b Case difficulty was scored up to 5 points

Of the 607 completed questionnaires, 298 were from the RL group (85.6%) and 309 from the MV group (90.4%). Table 3 shows the PBL evaluation questionnaire results comparing both groups. Responses on the 5-point Likert scale had mean values ranging from 3.79 to 4.41, with significant differences in only three of the nine assessments. Overall satisfaction averaged 7.71 out of 10, with no significant differences between both the groups. Prior PBL knowledge was reported by 86 students (29.1%) in the RL group and 229 students (74.1%) in the MV group.

Table 3
Students' perception of the PBL experience comparing the real life and metaverse groups.

a The test performed was the Mann–Whitney's U -test

b This statement was rated out of 10 points. The rest of them responded in a 5-point Likert scale from 1 (strongly disagree) to 5 (strongly agree)

Regarding the MV, most students agreed (rating 4 and 5) that they were familiar with the platform beforehand (83.1%), had sufficient Internet or computer access (78.9% and 71.3%, respectively), found avatar management easy (73.9%), and considered the island environment attractive (72.2%). Disagreement (ratings 1 and 2) with the eight statements was below 20% (Fig. 5). Various aspects of the MV platform received average scores above 7.3 out of 10, with the teacher, project organization, educational content, and classmate interaction rated highest, averaging over 8.2 (Fig. 6).

---

### Teaching medical students to prioritize preventive services [^33e03338]. Family Medicine (2006). Low credibility.

Background

Clinicians cannot provide all recommended preventive services in a single office visit and must learn to prioritize. This skill is not overtly addressed in medical school.

Methods

We designed a workshop to teach third-year medical students to prioritize preventive services during an office visit. In a prospective controlled trial, we compared performance on a standardized patient case.

Results

Students performed well, but there was no significant difference between intervention and control groups' mean scores on the standardized patient encounter.

Conclusions

Our brief intervention failed to increase students' scores on a standardized patient case requiring preventive services prioritization.

---

### Faculty evaluations correlate poorly with medical student examination performance in a fourth-year emergency medicine clerkship [^b4e76fa6]. The Journal of Emergency Medicine (2017). Low credibility.

Background

Clerkship directors routinely evaluate medical students using multiple modalities, including faculty assessment of clinical performance and written examinations. Both forms of evaluation often play a prominent role in final clerkship grade. The degree to which these modalities correlate in an emergency medicine (EM) clerkship is unclear.

Objective

We sought to correlate faculty clinical evaluations with medical student performance on a written, standardized EM examination of medical knowledge.

Methods

This is a retrospective study of fourth-year medical students in a 4-week EM elective at one academic medical center. EM faculty performed end of shift evaluations of students via a blinded online system using a 5-point Likert scale for 8 domains: data acquisition, data interpretation, medical knowledge base, professionalism, patient care and communication, initiative/reliability/dependability, procedural skills, and overall evaluation. All students completed the National EM M4 Examination in EM. Means, medians, and standard deviations for end of shift evaluation scores were calculated, and correlations with examination scores were assessed using a Spearman's rank correlation coefficient.

Results

Thirty-nine medical students with 224 discrete faculty evaluations were included. The median number of evaluations completed per student was 6. The mean score (± SD) on the examination was 78.6% ± 6.1%. The examination score correlated poorly with faculty evaluations across all 8 domains (ρ 0.074–0.316).

Conclusion

Faculty evaluations of medical students across multiple domains of competency correlate poorly with written examination performance during an EM clerkship. Educators need to consider the limitations of examination score in assessing students' ability to provide quality patient clinical care.

---

### Assessment of college students' awareness and knowledge about conditions relevant to metabolic syndrome [^3882282d]. Diabetology & Metabolic Syndrome (2014). Low credibility.

Total score in relation to gender, major of study, year in school, family history, BMI, and ethnicity

Results of the Poisson Regression model regressing total score on gender, major of study, any family history, overweight/obese, ethnicity, and year in school as covariates, indicated that gender, major of study and year in school were significant factors associating with differences in total score. Controlling for covariates mentioned, females scored 6% higher than male students (p = 0.01), Health Science major students scored 7% higher (p = 0.0011) than non- Health Science major students. Compared to 1 st year students, 5 th year had 11% higher scores (p = 0.002). Family history of disease, BMI and ethnicity were not significantly associated with total score (Table 5). Table 6 shows the average percent correct answers per condition, which ranged from 62.7% (lowest percentage on adiposity questions) to 77.3% (highest percentage on stroke questions).

Table 5
Poisson Regression Model - Total Score*

*overall model p < 0.001.

*OR (odd ratio).

Table 6
Summary of Average Scores and Percentage of Correct Answers Per Condition as Questionnaire Topic Areas (N = 243)

* Standard deviation.

Figures 1, 2, 3, 4 and 5 explore visually the trends for average percent correct answers per condition for various characteristics. Females scored higher than males on most conditions, except for "Diabetes" and "Cholesterol" sections, where males had similar average score as females (Figure 1). Health Science major students consistently scored higher than non-Health Science major students (Figure 2). Those students in the 5 th -year of study scored higher than those students in the 1 st, 2 nd, 3 rd, or 4 th - years; however, there was an overlap between the other grade years (Figure 3). While those students with family history of some diseases generally scored higher than those students with no-family history, this difference was not statistically significant for most of the tests (Figure 4). There was no difference between the knowledge of healthy weight students and that of obese or overweight students (Figure 5).

Figure 1
Students' knowledge based on gender.

Figure 2
Students' knowledge based on major of study.

Figure 3
Students' knowledge based on year in school.

Figure 4
Students' knowledge based on family history.

Figure 5
Students' knowledge based on weight status (healthy weight vs. overweight/obese).

---

### Summative assessments are more powerful drivers of student learning than resource intensive teaching formats [^07ceb8d9]. BMC Medicine (2013). Low credibility.

Student enrolment, data collection and analysis

At 4 weeks before the start of the module, medical students were informed about the study by email. On the first day of the module, all students were asked whether they would provide written consent to participate in the study, and consenting students completed the entry questionnaire and the ECG entry examination. The ECG exit examination was scheduled during the final week of the module, 3 days before the summative end-of-module examination. In order to avoid contamination between student cohorts, all test materials were collected after each assessment.

Descriptive analyses of demographic variables, student self-ratings and scores in all ECG examinations as well as the summative end-of-module examination were conducted separately for each of the six study groups, and differences between groups were assessed by χ 2 tests (dichotomous variables) and analysis of variance (ANOVA; continuous variables). Student ratings on six-point scales were dichotomized by collapsing the two most positive options and the remaining four options into positive and neutral/negative categories, respectively. The primary outcome for this study was the correct identification in the exit examination of at least three out of the five diagnoses listed above. Student self-reports of having spent more than 2 h/week on independent ECG self-study and of having used additional ECG learning material during the module were used as secondary outcomes. Multivariate regression analyses adusting for sex, age, performance levels, and initial self-ratings were used to predict primary and secondary outcomes. Formative examinations and the lowest level of teaching intensity (self-directed learning) were used as reference for these analyses, and results are given as odds ratios and 95% confidence intervals. The interaction between teaching intensity and assessment consequences was tested by adding interaction terms to the models. To validate the primary measure of student performance, we also conducted a sensitivity analysis in which we used an ANOVA to examine the effects of teaching format and assessment consequences on the percentage score in the ECG exit exam. Statistical analysis was performed using SPSS 19.0 (SPSS Inc. Chicago, IL, USA). Data are presented as mean ± standard deviation or percentages (n), as appropriate. Significance levels were set to P < 0.05. This study was approved by the local Ethics Committee (Ethik-Kommission der Medizinischen Fakultät der Georg-August-Universität Göttingen; application numbers 23/2/09, 18/8/09 and 1/3/10).

---

### Timing of emergency medicine student evaluation does not affect scoring [^28de2767]. The Journal of Emergency Medicine (2016). Low credibility.

Background

Evaluation of medical students rotating through the emergency department (ED) is an important formative and summative assessment method. Intuitively, delaying evaluation should affect the reliability of this assessment method, however, the effect of evaluation timing on scoring is unknown.

Objective

A quality-improvement project evaluating the timing of end-of-shift ED evaluations at the University of Arizona was performed to determine whether delay in evaluation affected the score.

Methods

End-of-shift ED evaluations completed on behalf of fourth-year medical students from July 2012 to March 2013 were reviewed. Forty-seven students were evaluated 547 times by 46 residents and attendings. Evaluation scores were means of anchored Likert scales (1–5) for the domains of energy/interest, fund of knowledge, judgment/problem-solving ability, clinical skills, personal effectiveness, and systems-based practice. Date of shift, date of evaluation, and score were collected. Linear regression was performed to determine whether timing of the evaluation had an effect on evaluation score.

Results

Data were complete for 477 of 547 evaluations (87.2%). Mean evaluation score was 4.1 (range 2.3–5, standard deviation 0.62). Evaluations took a mean of 8.5 days (median 4 days, range 0–59 days, standard deviation 9.77 days) to complete. Delay in evaluation had no significant effect on score (p = 0.983).

Conclusions

The evaluation score was not affected by timing of the evaluation. Variance in scores was similar for both immediate and delayed evaluations. Considerable amounts of time and energy are expended tracking down delayed evaluations. This activity does not impact a student's final grade.

---

### Predictive validity of the UKCAT for medical school undergraduate performance: a national prospective cohort study [^073b3dd0]. BMC Medicine (2016). Low credibility.

From Table 6, we can see that, if the UKCAT score is used as a screening threshold, with z = 0 set at the cut-point for candidate rejection, 46% (4903/10,693; in effect the positive predictive value of the test) of those who screen positive are estimated to fail at least one of their year's at first attempt. In contrast, only 31% (4370/13,930) of those that are above the screening threshold are estimated to fail at least one of their year's at first attempt. Thus, the absolute risk reduction resulting from the screening process is 15%. This equates to a 'number needed to treat', or rather in this case, 'number needed to screen', of seven. This could be viewed as an overall index of the effectiveness of the UKCAT as a selection method. We can also calculate hypothesised values for the UKCAT at this selected 'screening' threshold for sensitivity (53%) and specificity (62%).

We can also see that, using a mid-range cut-point, in order to reject 4903 candidates who would be likely to fail at least one exam, we would need to reject 5790 candidates who would probably pass all their end of year assessments first time. This ratio (5790:4903) could be termed as the NNR; in this case 1.18 (approximately one, if rounded down). This could be conceptualised as the number of acceptable candidates we would need to reject in order to screen out one 'poor' candidate. This number rises to 1.5 if a higher threshold (z ≤ –1, i.e. one standard deviation above the mean) is used. In contrast, the NNR reduces to around 0.8 if a lower threshold is selected (z ≤ 1). However, as can be seen from Table 6, when this lower cut-point is selected, the absolute number of higher risk candidates rejected is fairly low at 1465. Moreover, at this threshold, the estimated specificity of the test is also very low at around 15% (Table 5).

---

### Measuring learning gain during a one-day introductory bronchoscopy course [^7b9446f6]. Surgical Endoscopy (2011). Low credibility.

For the cohort one year later (comprising 18 first-year pulmonary and critical care trainees), baseline knowledge and technical skill were similar to the earlier cohort. All measures of learning gain were again significantly increased, thereby corroborating findings from the initial cohort (Table 3). Mean scores of cognitive knowledge significantly increased from 39% (7.8/20 ± 2.4) to 66% (13.1/20 ± 2.5) (p = 0.021). Absolute gain for the class was 27% (5.3/20 ± 2.9) and relative gain was 69%. No negative gains were noted. The class average normalized gain 〈g〉 was 44%. The average of the single-student normalized gains g(ave) was 60% (SD ± 21).

Table 3
Pre-/post-test scores and learning gain for two cohorts

a Robustness of educational intervention defined if 〈g〉 greater than 30%

Mean scores for technical skill also increased significantly from 41% (6.6/16 ± 3.2) to 76% (12.2/16 ± 2.4) (p = 0.015). Absolute gain for the class was 35% (5.6/16 ± 2.9) and relative gain was 85% (Table 3). Again, no negative gains were noted. The class average normalized gain 〈g〉 for technical skills was 60%, which, along with the large 〈g〉 for cognitive knowledge, confirmed the effectiveness of the educational intervention. The average of the single-student normalized gains g(ave) was 60% (SD = ± 22).

---

### Effectiveness of the "teaching the teachers" diagnostic US training program [^b13e871e]. Academic Radiology (2001). Low credibility.

Rationale and Objectives

The "Teaching the Teachers" training program was meant to establish standardized ultrasound education programs worldwide, reaching the largest possible number of physicians. The authors performed this study to evaluate the results of this training.

Materials and Methods

An open-ended test question format (ie, uncued testing) that would evoke responses from physicians in a manner ensuring the highest fidelity with a real clinical setting was selected. An examination was administered at the beginning and the end of the program and then again 6 months later to assess baseline knowledge, changes in knowledge, and knowledge retention, respectively.

Results

Scores on entry and end-of-program examinations were available for 112 participants. The mean entry test score was 35%, and the mean end-of-program examination score was 73%. All changes in scores were statistically significant (P < .001) as determined with paired t tests. Follow-up examinations were available for 27 of the 112 participants at the time of the analysis. On these examinations, mean total test scores increased by nearly 4 percentage points. Although follow-up test scores were available for only 27 participants, these mean test scores were comparable to those reported for the entire group on the end-of-program examination.

Conclusion

The examinations administered at the end of the 3-month program showed marked improvement compared with the baseline assessment. That this improvement remained stable over 6 months indicates the success of the educational process.

---

### Clinical practice guidelines: diagnosis, treatment, and management of children and adults with central auditory processing disorder [^b6a6f4bc]. AAA (2010). Medium credibility.

Central auditory processing disorder (C)APD — professional awareness and reporting practices — The most effective means for audiologists to educate other professionals is through well written reports that clearly describe (1) the various test procedures administered and test results obtained, (2) the overall implications of the test findings, and (3) the recommendations for intervention to remediate and/or compensate for the deficits identified. While raw scores may not be essential in such reports, the audiologist should interpret the scores relative to age-appropriate norms and explain the potential social, educational, and medical implications and recommendations for improving the individual's auditory and communicative functions and learning. Audiologists may also increase awareness and understanding of (C)APD by providing in-services and workshops at meetings of related professionals, by enhancing teachers' knowledge of the disorder and intervention strategies, and by using one-on-one discussions, school presentations, and provision of written materials; those connected with teaching hospitals can provide lectures to medical students and residents across specialties and participate in grand rounds.

---

### Assessing the quality of clinical teachers: a systematic review of content and quality of questionnaires for assessing clinical teachers [^0050e3d0]. Journal of General Internal Medicine (2010). Low credibility.

Measurement Characteristics of the Instruments

The measurement characteristics of the validated instruments have been summarized in Table 2. The content validity of most instruments was based on the literature and the input of experts and residents/students. In 17 studies, a previously developed instrument served as a basis for the development of the new instrument. Irby's questionnaire (1986) was mentioned four times for the development of an instrument., Not all studies documented what previously developed instrument had been used. Five studies reported the use of a learning theory for questionnaire construction., The number of available evaluations varied from 30 to 8,048 (median 506). Instruments contained 1 to 58 items, with Likert scale points ranging from four to nine. Information about feasibility in terms of costs, time needed for filling in the questionnaire, or minimum number of questionnaires needed was reported in eight instruments.

Table 2
Measurement Characteristics of Instruments for Measuring Clinical Teachers

Source of items

1 = previously developed instrument, 2 = analysis of literature, 3 = observations, 4 = expert opinions,

5 = learning theory

Content validity

1 = measurement aim described, 2 = target population described, 3 = clear framework of overall concept, 4 = item construction described, 5 = target population involved, 6 = items tested

Feasibility

Information provided about: 1 = duration of the test, 2 = costs, 3 = minimum number of respondents needed

Abbreviations

VAS = Visual Analog Scale, FA = Factor analysis, ICC = Intra Class Correlation, Inter Item C = Inter Item Correlation, G-Study = Generalizability study, SEM = Standard Error of Mean, ROC = Receiver Operating Characteristic curve, OTS = overall teaching score

Studies represented a variety of validity evidence procedures, with the most common one being the determination of internal consistency by internal structure by factor analysis and/or Cronbach's α (20 studies). Less common validation methods were determining inter-rater and intra-class correlations, Pearson correlation coefficients, Spearman Brown formula, and studies using the statistical generalizability theory. In some studies, scores were compared to the overall teaching score or scores on other instruments. Some studies reported on hypotheses formulated in advance or compared scores of different respondent groups.,

The reported purposes of clinical teacher evaluations are summarized in Table C (see online Appendix Table C). Not all authors documented how their instrument was to be used. Although providing feedback is the evaluation aim mentioned most frequently, 14 authors reported that the instrument was or would be used for summative purposes such as promotion, tenure, or resource allocation.

---

### Effects of homicide timing on test scores: quasi-experimental evidence from two cities in Colombia [^0446c546]. Child Development (2023). Medium credibility.

This study examines the effect of homicides around schools on the standardized test scores of fifth and ninth graders (N = 4729; M age = 12.71years, SD age = 2.13) using a quasi-experimental design in two Colombian cities. Exposure to homicides occurring within 7days of the test and within 500m of the school decreases test scores by 0.10SD. Effects show a greater sensitivity to timing than distance, becoming null as the time to the testing date increases but remaining consistent across larger radii. Since students in the study are on average exposed to 12.1 homicides per year, even short-lived learning losses can accumulate to impair learning for substantial portions of the school year. Findings are discussed, considering previous empirical work.

---

### Integrating evidence-based medicine skills into a medical school curriculum: a quantitative outcomes assessment [^eb19c5fb]. BMJ Evidence-Based Medicine (2021). High credibility.

Objectives

This research project aims to determine the potential differential impact of two curricular approaches to teaching evidence-based medicine (EBM) on student performance on an EBM assignment administered during the first year of clerkship. A meaningful result would be any statistically significant difference in scores on the assignment given to measure student performance.

Design

In order to assess and compare student learning under the different curricula, the principal investigator and a team of five faculty members blinded to assignment date and other possibly identifying details used a modified version of the previously validated Fresno rubric to retrospectively grade 3years' worth of EBM assignments given to students in clerkship rotations 1–3 (n = 481) during the Internal Medicine clerkship. Specifically, EBM performance in three separate student cohorts was examined.

Setting

The study took place at a large Midwestern medical school with nine campuses across the state of Indiana.

Participants

Study participants were 481 students who attended the medical school and completed the Internal Medicine clerkship between 2017 and 2019.

Interventions

Prior to the inception of this study, our institution had been teaching EBM within a discrete 2-month time period during medical students' first year. During a large-scale curricular overhaul, the approach to teaching EBM was changed to a more scaffolded, integrated approach with sessions being taught over the course of 2years. In this study, we assess the differential impact of these two approaches to teaching EBM in the first 2years of medical school.

Main Outcome Measures

We used clerkship-level EBM assignment grades to determine whether there was a difference in performance between those students who experienced the old versus the new instructional model. Clerkship EBM assignments given to the students used identical questions each year in order to have a valid basis for comparison. Additionally, we analysed average student grades across the school on the EBM portion of step 1.

Results

Four hundred and eighty-one assignments were graded. Mean scores were compared for individual questions and cumulative scores using a one-way Welch Analysis of Variance test. Overall, students performed 0.99 of a point better on the assignment from year 1 (Y1), prior to EBM curriculum integration, to year 3 (Y3), subsequent to EBM integration (p ≤ 0.001). Statistically significant improvement was seen on questions measuring students' ability to formulate a clinical question and critically appraise medical evidence. Additionally, on the United States Medical Licensing Examination (USMLE) step 1, we found that student scores on the EBM portion of the examination improved from Y1 to Y3.

Conclusions

Results of this study suggest that taking a scaffolded, curriculum-integrated approach to EBM instruction during the preclinical years increases, or at the very least does not lessen, student retention of and ability to apply EBM concepts to patient care. Although it is difficult to fully attribute students' retention and application of EBM concepts to the adoption of a curricular model focused on scaffolding and integration, the results of this study show that there are value-added educational effects to teaching EBM in this new format. Overall, this study provides a foundation for new research and practice seeking to improve EBM instruction.

Trial Registration Number

IRB approval (Protocol number 1907054875) was obtained for this study.

---

### Changes in medical student and doctor attitudes toward older adults after an intervention: a systematic review [^dc7abd71]. Journal of the American Geriatrics Society (2013). Low credibility.

Results

Twenty-seven studies met the eligibility criteria for the present review.–The main points from each study are presented in Table 2. Studies were explored in terms of their quality, design, and findings.

Table 2
Characteristics of Included Studies

Design and Quality of Studies

A number of methodological weaknesses were revealed that threatened the internal validity of reported findings. For example, 15 of the 27 studies did not employ a comparison group. Without data from a comparison group, it cannot be safely concluded that changes in attitude scores were not the result of effects such as maturation (naturally occurring psychological processes during the interval between test administrations), testing (the influence of taking the test multiple times), or history (events occurring outside of the intervention during the interval between test administrations).

Seven of the 27 studies demonstrated other methodological weaknesses that might have compromised results., Examples of these problems included the use of paired statistical tests on data that were not exclusively paired, conducting more than 20 t -tests on the same data set without correcting the family-wise error rate,–comparing a 2-year intervention group with changes in a comparison group over 1 year, high attrition levels, and significantly altering the response format of a previously established measure without piloting beforehand.

There were significant problems with the validity of attitude measures. The Maxwell-Sullivan Attitude Survey was never formally validated. Palmore's Bias scores derive from the author's Facts on Aging Questionnaires, which are measures of knowledge about older adults, not measure of attitudes. In addition, the Aging Semantic Differentialwas designed for and validated in a general population rather than using healthcare professionals. Only the University of California at Los Angeles *(UCLA) Geriatrics Attitude Scale, which was employed in six studies. has been validated on a sample of healthcare professionals. Five studies used a locally developed questionnaire to measure attitudes and did not report any details on the validity of the measure., Overall, 10 of the 27 studies used a measure that had never reported details of its validity statistics in any population.

Many studies failed to discriminate between attitudes toward older adults in general and older patients. As a result, studies frequently contained a mismatch between the focus of the intervention, which addressed older patients, and the terminology and focus of the questionnaire, which addressed older adults in general. Of the established measures, only the UCLA Geriatric Attitudes Scaleand the Maxwell-Sullivan Attitudes Surveymeasure attitudes toward older patients.

---

### The mentee perspective: evaluating mentorship of medical students in the field of orthopaedic surgery [^8eaaa848]. Journal of the American Academy of Orthopaedic Surgeons: Global Research & Reviews (2023). Medium credibility.

Knowing the hurdles to find mentorship, it is important to assess sources of mentorship for medical students. There has been a large proliferation of mentorship avenues in recent times with increased use of social media and student interest groups in orthopaedic surgery. There has also been an increasing demand for mentorship from peers at every stage of training, an important part of medical and graduate medical education. When examining different sources of mentorship in our study, a few key patterns emerge which underline our general results. On average, peers and older students scored significantly higher across the six mentorship categories (access, quality, personalization, communication, comfort, and facilitating matching) in comparison with home program directors and department chairs who scored lower (Table 5). This, in some respects, reflects the issue of access: It is easier for students to connect with other students for mentorship but can be difficult to connect with a busy program director or department chair. In addition, mentees likely find it easier to communicate with and gather personalized advice for their stage of training from peers a few years above them.

---

### Assessment of vitamin D-related knowledge, attitudes and practices among sultan qaboos university students in Oman: a cross-sectional study [^a2c395a4]. BMJ Public Health (2024). High credibility.

Discussion

To the best of the authors' knowledge, this is the first study to evaluate university students' vitamin D-related knowledge, attitudes and practices in Oman, an important topic in light of the local population's extremely high prevalence of vitamin D insufficiency. With a mean subscale score of 77.3, the findings indicated that SQU students generally possessed a moderate degree of vitamin D-related general knowledge; however, nutritional knowledge related to dietary sources of vitamin D was poor, with a mean subscale score of 45.1. In turn, the students demonstrated moderate attitudes and average practices, as indicated by mean subscale scores of 64.4 and 60.0, respectively. In terms of socio-demographic associations, female students reported significantly better general knowledge and attitude scores compared with males, while the latter had better practice scores, as did students who lived off-campus. Students enrolled in science-based colleges also showed significantly better general knowledge scores compared with those in non-science-based colleges.

Overall, our respondents demonstrated a moderate level of general knowledge, a finding consistent with the results of a similar survey conducted among female adults recruited from public healthcare centres in Iran. On the other hand, better general knowledge scores were reported by a sample of adults in the UK. Specifically, 68.2% of SQU students correctly stated that the majority of necessary vitamin D is created when the skin is directly exposed to the sun; however, only 40.1% were aware that vitamin D insufficiency is a significant health problem in Oman. A good proportion of students knew that elderly, pregnant and lactating women are at risk of vitamin D deficiency. Furthermore, 83.2% correctly indicated that bone pain and fatigue are symptoms related to vitamin D deficiency; in contrast, only 53.5% of patients attending a primary care centre in Kuwait were aware of this fact. This discrepancy in findings could be due to the high number of students enrolled in medical degree programmes in the current study. However, only 29% of students agreed that vitamin D supplement intake requirements differ throughout the year because of seasonal variations, although 76.1% knew that such requirements differ according to age group.

---

### Improved medical student satisfaction and test performance with a simulation-based emergency medicine curriculum: a randomized controlled trial [^3be92b71]. Annals of Emergency Medicine (2009). Low credibility.

Study Objective

We determine the effect of a simulation-based curriculum on fourth-year medical student test performance and satisfaction during an emergency medicine clerkship.

Methods

This was a randomized controlled study using a crossover design for curriculum format and an anonymous end-of-rotation satisfaction survey. Students were randomized into 2 groups. One group started the rotation with simulation and the other with group discussion. Midrotation, they each crossed over to the opposite format. All students subsequently completed the same multiple choice examination. We assessed paired samples of the number of questions missed for material taught in each format. Students rated satisfaction with a 5-point Likert scale framed as attitude toward simulation compared with group discussion. Scores ranged from 5, signifying strong agreement with a statement, to 1, signifying strong disagreement.

Results

Ninety students (99%) completed the multiple choice test. Significantly fewer questions were missed for material presented in simulation format compared with group discussion, with a mean difference per student of 0.7 (95% confidence interval [CI] 0.3 to 1.0; P = 0.006). This corresponds to mean scores of 89.8% for simulation and 86.4% for group discussion. Eighty-eight (97%) students completed the satisfaction survey. Students rated simulation as more stressful (mean 4.1; 95% CI 3.9 to 4.3), but also more enjoyable (mean 4.5; 95% CI 4.3 to 4.6), more stimulating (mean 4.7; 95% CI 4.5 to 4.8), and closer to the actual clinical setting (mean 4.6; 95% CI 4.4 to 4.7) compared with group discussion.

Conclusion

A simulation-based curriculum yielded measurable benefits. Students demonstrated a small improvement in learning and were more satisfied with the simulation-based curriculum compared with group discussion.

---

### APA guidelines for psychological assessment and evaluation [^efd4623a]. APA (2020). High credibility.

APA Guidelines for Psychological Assessment and Evaluation — selection, administration, and scoring of tests — states that psychologists who conduct psychological testing, assessment, and evaluation endeavor to select (a) assessment tools that demonstrate sufficient validity evidence for their uses, sufficient score reliability, and sound psychometric properties and (b) measures that are fair and appropriate for the evaluation purpose, population, setting, and context at hand. Application guidance highlights that psychometric characteristics "include but are not limited to standardization, reliability, and validity" and emphasizes accounting for diversity and limitations, as psychologists are "encouraged to be knowledgeable about and account for the impact of test results in diverse populations and across different settings, as well as the limitations of measures even when guidelines are followed for test selection". Specific application steps include: "Make sure the tool is accessible, appropriate for the purpose, and in the language that one would need to use, within the context of evidence-based practice"; "Become familiar with the administration instructions, scoring, and accurate completion of test protocols and record forms"; and "Practice and observe the administration of an instrument with an appropriate colleague". Additional steps are to "Read and evaluate reviews of the tests or instruments and research studies that used them, including meta-analyses, systematic reviews, and specialized practice parameters, in addition to edited handbooks and volumes on assessment" and to "Read the test or instrument publisher's technical manual(s) and review carefully the design, standardization sample(s) characteristics, descriptive statistics of standardization sample(s), composition and independence of scales, and supporting evidence of validity and reliability". Resource and networking activities include "Explore publishers' websites for up-to-date information on their tests and instruments" and "Join a professional listserv where intellectual discussions take place".

---

### Development of validated instrument to measure medical student learning in clinical urology: a step toward evidence based education [^b76ac40f]. The Journal of Urology (2004). Low credibility.

Purpose

To date published efforts to assess and improve medical student learning in urology have been limited due to the lack of an assessment tool with which to measure student learning. We report the development of a validated measure of medical student learning in urology.

Materials and Methods

Four core topics in clinical urology were selected as the focus of the test development, namely prostate cancer, screening with prostate specific antigen, benign prostatic hyperplasia and erectile dysfunction. Detailed curricula and multiple choice questions were created for each topic. Content validity of the curriculum and 28 item examination was established by a panel of 2 urologists and 2 medical physicians. Instrument reliability was determined by administering the test on line to third-year surgery students. Test construct validity was established through its administration to 19 urology residents and attending physicians.

Results

Reliability of the 28-item test instrument was measured by Cronbach's alpha at 0.76 and its 1-week test-retest reliability was 0.72. All urology experts performed well on the test. Mean urological expert scores were significantly higher than mean student post-test scores (24.9 ± 2.1 vs 17.8 ± 3.8, 2-tailed t test p < 0.001). Urological experts with greater urological training had higher scores than those with less residency training.

Conclusions

This study documents the development of a validated measure of medical student learning in urology. This validated instrument has the potential to improve educational quality control at medical schools and facilitate the development of effective, evidence based teaching methods.

---

### Effect of a smartphone intervention as a secondary prevention for use among university students with unhealthy alcohol use: randomised controlled trial [^346a0fa7]. BMJ (2023). Excellent credibility.

Objective

To estimate the effects of providing access to an alcohol intervention based on a smartphone.

Design

Randomised controlled trial.

Setting

Four higher education institutions in Switzerland.

Participants

1770 students (≥ 18 years) who screened positive for unhealthy alcohol use (ie, a score on the alcohol use disorders identification test-consumption (AUDIT-C) of ≥ 4 for men and ≥ 3 for women) were randomly assigned by 1:1 allocation ratio in blocks of 10.

Intervention

Providing access to a brief, smartphone based alcohol intervention.

Outcome Measures

The primary outcome studied was number of standard drinks per week at six months and the secondary outcome was number of heavy drinking days (past 30 days). Additional outcomes were maximum number of drinks consumed on one occasion, alcohol related consequences, and academic performance. Follow-up assessments occurred at months three, six, and 12. Data were analysed by intention to treat and by using generalised linear mixed models with random intercepts for the recruitment site and participants nested within the recruitment site, and with intervention (v control), time (three months v six months; 12 months v six months), and baseline outcome values as fixed effects.

Results

Between 26 April 26 2021 and 30 May 2022, 1770 participants (intervention group (n = 884); control group (n = 886)) were included. Mean age was 22.4 years (standard deviation 3.07); 958 (54.1%) were women; and 1169 (66.0%) were undergraduate students, 533 (30.1%) were studying for a master's degree, 43 (2.4%) were studying for a doctorate, and 25 (1.4%) were students of other higher education programme. The baseline mean number of standard drinks per week was 8.59 (standard deviation 8.18); the baseline number of heavy drinking days was 3.53 (4.02). Of 1770 participants, follow-up rates were 1706 (96.4%) at three months, 1697 (95.9%) at six months, and 1660 (93.8%) at 12 months. Of 884 students randomly assigned to the intervention group, 738 (83.5%) downloaded the smartphone application. The intervention had a significant overall effect on the number of standard drinks per week (incidence rate ratio 0.90 (95% confidence interval 0.85 to 0.96)), heavy drinking days (0.89 (0.83 to 0.96)), and the maximum number of drinks consumed on one occasion (0.96 (0.93 to 1.00), P = 0.029), indicating significantly lower drinking outcomes in the intervention group than in the control group during the follow-up period. The intervention did not affect alcohol related consequences or academic performance.

Conclusions

Providing access to the smartphone application throughout the 12 month follow-up was effective at limiting the average drinking volume of university students who had self-reported unhealthy alcohol use at baseline.

Trial Registration

ISRCTN 10007691.

---

### Inconsistency between overall and subgroup analyses [^9b703599]. General Psychiatry (2022). Medium credibility.

Introduction

Consider the following hypothetical example. Suppose the fourth grade students of two schools (1 and 2) in a school district took a state maths exam. The principals of these two schools wanted to know whether there was a difference in the performances between the two schools. They calculated the overall average score, the average score of girls and the average score of boys in each school. The result is presented as scenario B in table 1. After looking at the average scores of girls and boys, respectively, the principal of school 1 was very happy as they were both one point higher than those in school 2. However, after looking at the overall average score of these two schools, the principal was very confused as the overall average score of school 2 was higher than that of school 1. Is there anything wrong in calculating the overall average score? What is the reason for the inconsistency between the overall average scores and the average scores stratified by gender?

Table 1
Average scores of a maths exam in two schools

Before figuring out the reason for the inconsistency, let us take a look at scenario A in table 1. In this scenario, the overall average score, as well as the average scores of girls and boys in school 1 are all higher than those in school 2. A closer examination shows that the proportions of girls are different in scenarios A and B. In scenario A, 48% of students are girls in both schools. In scenario B, 40% and 60% of students are girls in the two schools, respectively. Is the difference in the proportions of girls sufficient to create this inconsistency? The answer is negative. In scenario C in table 1, although the proportions of girls are different in the two schools, the overall average scores and the average scores by gender are higher in school 1.

---

### Student-and school-level factors associated with mental health and well-being in early adolescence [^5b91491b]. Journal of the American Academy of Child and Adolescent Psychiatry (2024). Medium credibility.

School-level characteristics refer to the broader school context, school community, and operational school features. Hence, school-level characteristics are aggregates across students within a school, providing complementary information about the impact of a student's school environment on their mental health beyond their individual characteristics. Data were obtained by linking publicly available government data to the school's postal code, unless otherwise specified. We selected measures that were directly comparable across all 4 nations within the United Kingdom (England, Northern Ireland, Scotland, and Wales). Otherwise, we mapped existing measures onto their English equivalent (eg, school quality ratings). The broader school context summarizes wider socioeconomic factors in the school's catchment area, including urbanicity (urban vs rural school location) and area-level deprivation (Index of Multiple Deprivation, decile rating range 1 [most deprived] to 10 [least deprived]). School community factors refer to characteristics of the student population, including the percentage of students who were eligible for free school meals (school-level economic deprivation), the percentage of students receiving support for special educational needs or disabilities, and students who were White British (all range 0%-100%), as well as the average age of students in a school. Operational school features included the total number of students within a school, student-to-teacher ratio, and school sex (mixed or female-only schools). The most recent official school inspection rating before trial arm allocation was used to obtain an ordinal rating of school quality. As the approach to the measurement of school quality differed in public (independent schools) and private schools and across the nations, we mapped all school inspection rating systems onto the following 3 categories: requires improvements, good, and outstanding. Quality of social-emotional learning (SEL) provision was assessed through a semistructured interview with the senior leadership team or a staff member with overall responsibility for teaching SEL using a list of 16 quality indicators specifically designed for the trial. A higher school rating (range 0–16) reflected better SEL provision. School attainment was obtained from publicly available government data referring to the average attainment of students within a school. The attainment score is calculated based on the student's achievement across 8 subjects, including English and mathematics (double weighted) and 6 further subjects. The Alaska School Climate and Connectedness Survey (SCCS)was used to assess teacher-rated (subscales School Leadership and Involvement, Staff Attitudes, and Respectful Climate) and student-rated (subscales School Leadership and Involvement, Respectful Climate, Peer Climate, and Caring Adults) school climate, with higher total scores (range 1–5) representing a better school climate. We conducted confirmatory factor analyses using robust maximum likelihood estimation to confirm that our reduced set of subscales is a valid measure of the underlying school climate concept. Here, school climate was defined as teachers' and students' experiences in a school, including feeling safe, connected, and welcomed. It includes connections, partnerships, and conditions for learning (see Supplement 2, available online, for each subscale, the psychometric properties, and relations to students' mental health over time). For analysis, overall scores were calculated by taking the mean across teachers (or students for the school-level measure) within a school to obtain a composite school-level measure of teacher-rated (or student-rated) school climate. Student-rated school climate refers to the student's views of the school climate analyzed both at the student and at the school level. Student-level effects capture each student's unique perception of the school climate and school-level effects capture the average perception of the school climate among students within a school.

---

### ACC appropriate use criteria methodology: 2018 update: a report of the American college of cardiology appropriate use criteria task force [^bff81262]. Journal of the American College of Cardiology (2018). Medium credibility.

ACC Appropriate Use Criteria — rating tabulation states that final scores are reported in discrete categories — Appropriate Care, May Be Appropriate Care, and Rarely Appropriate Care — as well as with their numerical median rating, with anonymized individual scores available in an online appendix to the AUC document.

---

### Relationship between pathology curricular approaches and performance in the United States medical licensing examination (USMLE), step 1: a national cross-sectional study [^6d508316]. Human Pathology (2003). Low credibility.

A collaborative multi-institutional study was conducted to study the relationship between characteristics of pathology instruction and student performance in the United States Medical Licensing Examination (USMLE), Step 1. Detailed descriptions of pathology instruction, such as curriculum format, style of instruction, and design of instruction of various areas of pathology for the 1996–1997 academic year were matched with Step 1 total scores and pathology subscores for 10,159 students from 88 schools who took the June 1997 examination. Hierarchical linear modeling was used for analysis while controlling for students' MCAT-bpv scores (Medical College Admission Test scores, average of biological and physical sciences and verbal reasoning). The Step 1 total scores correlate with Step 1 pathology subscores, and both correlate with students' MCAT-bpv scores. The MCAT-bpv scores account for most of the variation in Step 1 scores. Of all of the curricular variables analyzed, 2 possible factors that significantly influence the impact of MCAT-bpv scores on Step 1 scores are (1) multidisciplinary format of instruction and (2) discipline-based general pathology instruction in year 2. Overall, the multidisciplinary format reduced the impact of MCAT-bpv scores on both the Step 1 total score and the pathology subscore by 1 point per MCAT-bpv. Overall, the general pathology instruction in year-2 increased that impact on Step 1 total score by 1 point per MCAT-bpv. The slope became less steep with multidisciplinary format and more steep with year-2 general pathology instruction. As a result, students with higher MCAT-bpv scores tend to benefit from year-2 general pathology instruction, whereas those with lower MCAT-bpv scores appear to benefit from a multidisciplinary format. These differences become more apparent as the MCAT-bpv scores move away from the mean in either direction, indicating that scores of most students are not affected by curricular variables. Overall, there is no significant difference in the school means of Step 1 total scores and pathology subscores of schools with different curricular approaches. This is most likely due to the finding that the students' MCAT-bpv scores, and not curricular variables, are the major predictors of Step 1 scores, and all schools have a mix of students with various MCAT-bpv scores.

---

### Timing of surgery and internal medicine clerkships and surgery shelf examination scores [^e173a2f1]. The Journal of Surgical Research (2019). Medium credibility.

Background

Many medical students believe that third-year clerkship rotation sequence affects their success. We hypothesized that students who completed the internal medicine clerkship before the surgery clerkship received higher surgery shelf examination scores compared with the students who did not.

Materials and Methods

Deidentified academic data including preclinical data and National Board of Medical Examiners shelf examination scores for surgery for all third-year medical students at a single institution from 2012 to 2017 were analyzed. Students who did not complete all six core clerkships during the standard third-year time frame were excluded. Data were analyzed using 2-tailed t-tests and Z-scores.

Results

Four hundred and twenty four students were included in the study. Average undergraduate grade point average, Medical College Admission Test scores, and United States Medical Licensing Examination Step 1 scores showed no significant differences between groups. In aggregate, average shelf examination scores of students who completed the internal medicine clerkship before the surgery clerkship were significantly higher than those of students who did not. When the average shelf examination scores for the two groups were analyzed by individual rotation slot, no significant difference was found between the two groups.

Conclusions

Initially, it appeared that students who completed the internal medicine clerkship before the surgery clerkship scored higher on their surgery shelf examinations. When the data were analyzed by individual rotation slot, we found no difference between the students who had already completed the internal medicine clerkship and those who had not. Experience over the year rather than completion of the internal medicine rotation was associated with higher surgery shelf examination scores.

---

### A 6-item scale for overall, emotional, and social loneliness: confirmatory tests on survey data [^ad04ccff]. Research on Aging (2006). Low credibility.

The clinical calculator "De Jong Gierveld Loneliness Scale" for social isolation in the elderly.

The De Jong Gierveld Loneliness Scale is a clinical calculator used to measure feelings of loneliness in adults, particularly in the elderly population. This scale is widely used in research and clinical settings to assess both emotional and social loneliness. The scale consists of 6 items. The higher scores indicate greater levels of loneliness.

The clinical utility of the De Jong Gierveld Loneliness Scale lies in its ability to provide a quantitative measure of loneliness, which can be useful in identifying individuals who may be at risk for associated health problems, such as depression or cardiovascular disease. It can also be used to monitor the effectiveness of interventions aimed at reducing loneliness.

There are no specific exclusion criteria for the use of the De Jong Gierveld Loneliness Scale. However, it may not be suitable for use in individuals with cognitive impairment or severe mental health disorders, as they may have difficulty understanding or accurately responding to the scale's items.

The De Jong Gierveld Loneliness Scale is a tool designed to evaluate a person's sense of loneliness through responses to six questions related to feelings of social connection and support. Each response is converted to a numerical score, which is then summed to quantify the individual's level of loneliness, where a higher score indicates more loneliness.

For questions about feeling emptiness, missing people, and feeling rejected, the responses are scored straightforwardly:

- "Yes" or "More or less" results in a score of 1.
- "No" results in a score of 0.

On questions regarding reliance, trust, and closeness to others, the scoring is reversed:

- "Yes" results in a score of 0.
- "More or less" yields a score of 1.
- "No" results in a score of 1.

Here is a breakdown of the scoring:

- **I experience a general sense of emptiness**:
- **Yes**: 1
- **More or less**: 1
- **No**: 0

- **I miss having people around me**:
- **Yes**: 1
- **More or less**: 1
- **No**: 0

- **I often feel rejected**:
- **Yes**: 1
- **More or less**: 1
- **No**: 0

- **There are plenty of people I can rely on when I have problems**:
- **Yes**: 0
- **More or less**: 1
- **No**: 1

- **There are many people I can trust completely**:
- **Yes**: 0
- **More or less**: 1
- **No**: 1

- **There are enough people I feel close to**:
- **Yes**: 0
- **More or less**: 1
- **No**: 1

The total score, ranging from 0 to 6, determines the interpretation of loneliness:

- A score of 0 indicates the individual is "Not lonely".
- A score of 1 suggests a "Loneliness score is 1" (least lonely).
- A score of 2 simply shows "Loneliness score is 2".
- A score of 3 indicates "Loneliness score is 3".
- A score of 4 corresponds to "Loneliness score is 4".
- A score of 5 represents "Loneliness score is 5".
- A score of 6 signals "Loneliness score is 6" (most lonely).

Thus, through straightforward summation, the scale provides a succinct description of the individual's level of loneliness.

---

### The impact of the COVID-19 pandemic on college students in USA: two years later [^65fd3a34]. Psychiatry Research (2022). Medium credibility.

Coronavirus disease 19 (COVID-19) has posed an unprecedented public health challenge for most countries, and the repeated outbreaks of this disease have created a largest disruption of education systems. The intent of this work was to examine the intersection of COVID-19 fear and mental health consequences among college students in Northern Michigan, a region of the U.S. severely affected by the pandemic. This study was conducted from January 17 to February 25, 2022, two years later since the outbreak of the pandemic, and 151 college students (female, 76) were involved. Participants' potential psychological symptoms, anxiety, happiness, learning difficulty, and demographic information were surveyed. We have three main findings. First, 60.3% of students had tested positive for COVID-19, and more female than male students were affected (female, 69.7%; male, 50.7%). Second, there was a high prevalence of mental problems among college students, with 95.7% of the sample experiencing moderate or severe mood disorders. Third, respondents' education was severely affected by the pandemic, averaging a score of 7.6 on a scale of 10 when asked how much their learning quality was affected. They showed increased fear, stress, and decreased happiness, and these were associated with their learning quality change. Given the impact would be far-reaching, not only college students' mental health but also their learning difficulties should be monitored during the pandemic. These findings are alarming and timely, and their implications are discussed.

---

### Student attendance and academic performance in undergraduate obstetrics / gynecology clinical rotations [^e5375bd5]. JAMA (2013). Excellent credibility.

Importance

Student attendance is thought to be an important factor in the academic performance of medical students, in addition to having important regulatory, policy, and financial implications for medical educators. However, this relationship has not been well evaluated within clinical learning environments.

Objective

To evaluate the relationship between student attendance and academic performance in a medical student obstetrics/gynecology clinical rotation.

Design, Setting, and Participants

A prospective cohort study of student attendance at clinical and tutorial-based activities during a full academic year (September 2011 to June 2012) within a publicly funded university teaching hospital in Dublin, Ireland. Students were expected to attend 64 activities (26 clinical activities and 38 tutorial-based activities) but attendance was not mandatory. All 147 fourth-year medical students who completed an 8-week obstetrics/gynecology rotation were included.

Exposures

Student attendance at clinical and tutorial-based activities, recorded using a paper-based logbook.

Main Outcomes and Measures

The overall examination score (out of a possible 200 points) was obtained using an 11-station objective structured clinical examination (40 points), an end-of-year written examination comprising 50 multiple-choice questions (40 points) and 6 short-answer questions (40 points), and an end-of-year long-case clinical/oral examination (80 points). Students were required to have an overall score of 100 points (50%) and a minimum of 40 points in the long-case clinical/oral examination (50%) to pass.

Results

The mean attendance rate was 89% (range, 39%-100% [SD, 11%], n = 57/64 activities). Male students (84% attendance, P = 0.001) and students who failed an end-of-year examination previously (84% attendance, P = 0.04) had significantly lower rates. There was a positive correlation between attendance and overall examination score (r = 0.59 [95% CI, 0.44–0.70]; P < .001). Both clinical attendance (r = 0.50 [95% CI, 0.32–0.64]; P < .001) and tutorial-based attendance (r = 0.57 [95% CI, 0.40–0.70]; P < .001) were positively correlated with overall examination score. The associations persisted after controlling for confounding factors of student sex, age, country of origin, previous failure in an end-of-year examination, and the timing of the rotation during the academic year. Distinction grades (overall score of ≥ 60%) were present only among students with attendance rates of 80% or higher. The odds of a distinction grade increased with each 10% increase in attendance (adjusted odds ratio, 5.52; 95% CI, 2.17–14.00). The majority of failure grades (6/10 students; 60%) occurred in students with attendance rates lower than 80%. The adjusted odds ratio for failure with attendance rates of 80% or higher was 0.11 (95% CI, 0.02–0.72).

Conclusions and Relevance

Among fourth-year medical students completing an 8-week obstetrics/gynecology clinical rotation, attendance at clinical and tutorial-based activities was positively correlated with overall examination scores. Further research is needed to understand whether the relationship is causal, and whether improving attendance rates can improve academic performance.

---

### Contact dermatitis and patch testing education: a work group report from the allergic skin diseases committee of the AAAAI [^822bf379]. The Journal of Allergy and Clinical Immunology: In Practice (2025). High credibility.

Patch testing (PT) survey — among respondents, 18 (78.26) reported having a faculty member who performs/interprets PT. Fellows performed PT in 15 (75). Monthly PT volume most commonly was 3–6 patients in 9 (47.37) respondents, followed by 0–2 in 6 (31.58). A dedicated patch test clinic was not present in 15 (78.95) respondents. The preloaded T.R.U.E. test was used by 10 (52.63), and among responses on proportion of use, 76%–100% of patch tests were T.R.U.E. test for 5 (55.56). Use of allergens from an expanded series (eg, the NACDG) was reported by 12 (66.67), with proportions of tests from expanded series at 51%–75% in 4 (33.33) and 76%–100% in 4 (33.33). Custom PT was performed by 14 (77.78), and a personalized avoidance treatment plan after positive PT was provided by 15 (83.33). In the prior 3 years, 4 (21.05) reported presenting a poster or abstract on contact dermatitis at a national meeting.

---

### The UKCAT-12 study: educational attainment, aptitude test performance, demographic and socio-economic contextual factors as predictors of first year outcome in a cross-sectional collaborative study of 12 UK medical schools [^49820901]. BMC Medicine (2013). Low credibility.

Table 1
Summary of variables in the analysis

Variables are indicated by their SPSS variable names (in bold) to reduce ambiguity.

1. MEDICAL SCHOOL OUTCOME DATAMedical schools provided information on overall outcome on a four-point scale (passed all exams at first attempt; passed after re-sitting exams; repeating the first year; and leaving the course), which we called OutcomeFirstYear4pt. Average percentage marks on assessments were also available for most students (OverallMark), and for many students separate marks were also available for 'theory' and 'skills' assessments (TheoryMark and SkillsMark; see Table 1). The overall, theory and skills marks were all based on marks attained at the first attempt.

2. PRIOR EDUCATIONAL ATTAINMENT, AND SO ONInformation on prior educational attainment was provided by UCAS, consisting of Scottish Higher and Advanced Higher results for students from Scotland (collectively SQA qualifications), and A-level, AS-level and GCSE results for other students (collectively GCE qualifications). Educational qualifications are always complex to analyze, because different candidates take different examinations with different structures and grading schemes, and candidates have chosen to study different subjects. Four medical schools were from Scotland and eight from the rest of the UK, entrants to the former mostly, but not entirely, taking Scottish Highers rather than A-levels. No easy solution is possible for the difficult problem of equating the two different sets of results, and we have followed the approach of Tiffin et al. in converting Scottish Highers and A-levels to z-scores, which can then be combined. The Technical Report describes an extensive set of preliminary analyses of the wide range of different measures of attainment (see Table 1). Briefly, each of the 42 derived scores for A-levels, AS-levels and GCSEs was correlated with OverallScore (TR Table 1 a, b). Multiple regressions suggested that only a subset of eight measures (TR Table 2) showed independent predictions of outcome. Missing values for these eight measures were replaced by expectation-minimization (EM) imputation, the resulting 8 x 8 correlation matrix factor analyzed, the first principle component extracted, (which has a mean of zero and SD of one), and scores on that were used as an optimal summary measure of attainment at A-level, AS-level and GCSE. A similar process was carried out for the 51 derived measures of Highers, 'Highers Plus' and Advanced Highers, each of which was correlated with OverallScore (TR Table 3). Ten independent predictors were found, missing values replaced by imputation, and the first principle component extracted (TR Table 4). Since the principle components for the two analyses were both on standardized scales, they could be combined to provide an optimal summary measure of Educational Attainment for the majority of students. Educational attainment measures differ between medical schools and between cohorts, but because the main interest in this study is prediction within medical schools, we have standardized Educational Attainment within cohorts and medical schools, resulting in the variable we call zEducational Attainment. It should be noted that educational qualifications were only available in most cases for non-mature students (age less than 21). Although statistically optimal, and hence good for assessing underlying processes using as much information as possible, we realize that zEducational Attainment does not reflect the current selection processes, and therefore we also report results for the more conventional measures of three best A-levels, four best AS-levels, nine best GCSEs, five best Scottish Highers, five best Scottish "Highers Plus" (which includes finer definition of bands within grades), and the best Scottish Advanced Higher.

---

### Clinical practice guidelines: childhood hearing screening [^03f0c904]. AAA (2011). Medium credibility.

Childhood hearing loss — educational attainment, employment, and cost — includes evidence that school districts spend 2.4 times more on average for each student enrolled in a program for the deaf and hard of hearing than for a child who does not receive special education services. The page reports that children who are deaf attained median reading scores at the 4.0 grade level by the age of 17 or 18 years, and that of students who are deaf or hard of hearing who are accepted into higher education, 70% withdraw from college before earning a college degree. Employment statistics note that data from the 2000 U.S. census indicate that the total unemployment rate for 16–64 years is 60% for persons with severe sensory disabilities, that less than one-third of adults who are deaf and under the age of 35 who want to work can find a job, and that wages earned by males who are deaf are 77% of the national wage average whereas the wages earned by females who are deaf are 88% of the national wage average within every occupational group.

---

### APA guidelines for psychological assessment and evaluation [^67e43912]. APA (2020). High credibility.

Application — Profession-wide competencies and selection of tests profession-wide and specialty-specific competencies "are recognized and reinforced by various entities in psychology," and "Assessment is identified as a profession-wide competency in these and other quality assurance measures." Such competencies "are evaluated by the criteria of whether they are observable, measurable, and quantifiable," and "This consistency is necessary to maintain continuity and objectivity across and within competencies." Assessment competence "entails several functional competencies that include but are not limited to selection, use, interpretation, report of results, and use of results in response to the purpose of the assessment." Under "Selection of tests or evaluation measures," psychologists "seek to become knowledgeable about the psychometric characteristics of test instruments as well as other factors likely to impact the applicability of specific test instruments and evaluation measures to the assessment question at hand."

---

### Association of third-year medical students' first clerkship with overall clerkship performance and examination scores [^aaa54980]. JAMA (2010). Excellent credibility.

Context

Anecdotal experience has suggested that third-year medical students whose first clerkship is internal medicine may have superior performance throughout the academic year.

Objective

To determine whether the order of clerkships by specialty is associated with student performance.

Design, Setting, and Participants

Clerkship performance records of medical students at all 4 campuses of the University of Illinois College of Medicine who completed their third-year core clerkships from July 2000 through June 2008 (N = 2236) were reviewed.

Main Outcome Measures

Analysis of covariance was used to test for between-group differences (by first clerkship) in mean National Board of Medical Examiners subject examination scores (range, 0–100), preceptor ratings of clerkship clinical performances (range, 12–30), total overall clerkship grades (range, 12–30), and United States Medical Licensing Examination (USMLE) Step 2 scores, adjusted for sex, campus, and USMLE Step 1 score.

Results

First clerkship specialty was significantly associated with mean subject examination scores (family medicine, 71.96 [95% confidence interval {CI}, 70.90–72.98], internal medicine, 73.86 [95% CI, 73.33–74.39], obstetrics/gynecology, 72.36 [95% CI, 71.64–73.04], pediatrics, 73.11 [95% CI, 72.38–73.84], psychiatry, 72.17 [95% CI, 71.52–72.81], surgery, 72.37 [95% CI, 71.73–73.02]; P < .001) and overall clerkship grades (family medicine, 24.20 [95% CI, 23.90–24.90], internal medicine, 25.33 [95% CI, 25.07–25.60], obstetrics/gynecology, 24.68 [95% CI, 24.32–25.05], pediatrics, 24.92 [95% CI, 24.59–25.27], psychiatry, 24.61 [95% CI, 24.33–25.01], surgery 24.97 [95% CI, 24.64–25.30]; P = 0.01). There was no significant association with preceptor ratings or USMLE Step 2 scores. Pairwise comparisons for mean total overall clerkship grades showed a significant difference for students taking internal medicine first compared with obstetrics/gynecology (mean difference, 0.65; 95% CI, 0.18–1.12), psychiatry (mean difference, 0.66; 95% CI, 0.20–1.12), and family medicine (mean difference, 0.93; 95% CI, 0.37–1.50).

Conclusion

Among students at 4 campuses of a US medical school, clerkship order was significantly associated with performance on clerkship subject examinations and overall grades but not with clerkship clinical performance or USMLE Step 2 scores.

---

### Assessing the value of an optional radiation oncology clinical rotation during the core clerkships in medical school [^56bb71a6]. International Journal of Radiation Oncology, Biology, Physics (2012). Low credibility.

Purpose

Few medical students are given proper clinical training in oncology, much less radiation oncology. We attempted to assess the value of adding a radiation oncology clinical rotation to the medical school curriculum.

Methods and Materials

In July 2010, Jefferson Medical College began to offer a 3-week radiation oncology rotation as an elective course for third-year medical students during the core surgical clerkship. During 2010 to 2012, 52 medical students chose to enroll in this rotation. The rotation included outpatient clinics, inpatient consults, didactic sessions, and case-based presentations by the students. Tests of students' knowledge of radiation oncology were administered anonymously before and after the rotation to evaluate the educational effectiveness of the rotation. Students and radiation oncology faculty were given surveys to assess feedback about the rotation.

Results

The students' prerotation test scores had an average of 64% (95% confidence interval [CI] 61–66%). The postrotation test scores improved to an average of 82% (95% CI, 80–83%; 18% absolute improvement). In examination question analysis, scores improved in clinical oncology from 63% to 79%, in radiobiology from 70% to 77%, and in medical physics from 62% to 88%. Improvements in all sections but radiobiology were statistically significant. Students rated the usefulness of the rotation as 8.1 (scale 1–9; 95% CI, 7.3–9.0), their understanding of radiation oncology as a result of the rotation as 8.8 (95% CI, 8.5–9.1), and their recommendation of the rotation to a classmate as 8.2 (95% CI, 7.6–9.0).

Conclusions

Integrating a radiation oncology clinical rotation into the medical school curriculum improves student knowledge of radiation oncology, including aspects of clinical oncology, radiobiology, and medical physics. The rotation is appreciated by both students and faculty.

---

### Predictive validity of the UKCAT for medical school undergraduate performance: a national prospective cohort study [^edec6f59]. BMC Medicine (2016). Low credibility.

Background

For many years, access to medical school worldwide has been determined primarily on educational attainment, measured by secondary (high) school grades. This has been an accepted approach as it generally reliably predicts later academic performance in medical school and later postgraduate clinical education. In the UK, as elsewhere, there is a high competition ratio for places at medical school, with around 11 high achieving applicants for each place. Internationally, this competition has encouraged the development of aptitude tests as part of the selection of future doctors. Specifically, aptitude tests seek to quantify the cognitive (and increasingly personal) attributes that are considered desirable in a future doctor and important to successfully completing both undergraduate and postgraduate training as well as practising effectively.

In England, secondary school academic achievement and entry to university is usually focussed on General Certificate of Education Advanced-level grades ('A-levels'). There has been a fairly continual increase in the average grades awarded at A-level over the last two decades. This 'grade inflation' means that prior (or predicted) educational attainment measures are losing their discriminatory power to identify future ability to perform in medical school. Thus, candidates are restricted in the scores attainable from secondary school qualifications such that they cannot demonstrate capability beyond other applicants achieving the same grades. UK universities tend to accept only the first top three grades achieved at A-levels, reducing the variation in accepted entry qualifications further. These top grades are converted from alphabetic form (e.g. A–E) grades to a 'tariff' points score by the Universities and Colleges Admissions Service (UCAS) for the UK. Each grade is worth a set amount of tariff points, irrespective of the subject. However, for standard entry (as opposed to graduate entry or extended courses with a 'pre-medicine' year of study) medical schools usually require at least two of the sciences to have been studied at advanced level and also a certain level of achievement at mathematics. Thus, at the time of the study, the maximum achievable UCAS tariff score for 'best of three' A-levels was 360 points (equivalent to three A grades, each worth 120 points). In Scotland and Ireland the situation differs slightly. In Scotland, school students wishing to apply for the more competitive university courses typically sit 'Scottish Higher' exams in five subjects in their fifth year, followed by an additional two or three at 'Advanced Higher' level in their sixth year (the latter being more comparable with A-levels). At the time of this study, most Scottish medical schools would have required mostly 'A' grades at Higher level. UK medical schools outside Scotland generally also required Scottish Certificate of Education candidates to study up to three subjects at Advanced Higher or Higher level, with either AAA or AAB grades, and with subject specification (generally Chemistry and Biology), but this varied amongst medical schools. In Ireland, students intending to study medicine would tend to take seven Irish Leaving Certificate Examinations (ILEs), with an emphasis on science subjects. Medical school entry requirements, at the time, for applicants with Irish advanced educational qualifications, would have been 'A' grades for the best of six of the subjects taken; for ILEs the A grade is split into two levels, A1 and A2, with A1 being the highest achievable grade worth 90 UCAS tariff points. Usually, this needed to include at least two science subjects. Most Irish medical schools also required a specified standard in English, mathematics, Irish, and a third language.

---

### Association between a medical school admission process using the multiple mini-interview and national licensing examination scores [^74f3dd43]. JAMA (2012). Excellent credibility.

Context

There has been difficulty designing medical school admissions processes that provide valid measurement of candidates' nonacademic qualities.

Objective

To determine whether students deemed acceptable through a revised admissions protocol using a 12-station multiple mini-interview (MMI) outperform others on the 2 parts of the Canadian national licensing examinations (Medical Council of Canada Qualifying Examination [MCCQE]). The MMI process requires candidates to rotate through brief sequential interviews with structured tasks and independent assessment within each interview.

Design, Setting, and Participants

Cohort study comparing potential medical students who were interviewed at McMaster University using an MMI in 2004 or 2005 and accepted (whether or not they matriculated at McMaster) with those who were interviewed and rejected but gained entry elsewhere. The computer-based MCCQE part I (aimed at assessing medical knowledge and clinical decision making) can be taken on graduation from medical school; MCCQE part II (involving simulated patient interactions testing various aspects of practice) is based on the objective structured clinical examination and typically completed 16 months into postgraduate training. Interviews were granted to 1071 candidates, and those who gained entry could feasibly complete both parts of their licensure examination between May 2007 and March 2011. Scores could be matched on the examinations for 751 (part I) and 623 (part II) interviewees.

Intervention

Admissions decisions were made by combining z score transformations of scores assigned to autobiographical essays, grade point average, and MMI performance. Academic and nonacademic measures contributed equally to the final ranking.

Main Outcome Measures

Scores on MCCQE part I (standardized cut-score, 390 [SD, 100]) and part II (standardized mean, 500 [SD, 100]).

Results

Candidates accepted by the admissions process had higher scores than those who were rejected for part I (mean total score, 531 [95% CI, 524–537] vs 515 [95% CI, 507–522]; P = 0.003) and for part II (mean total score, 563 [95% CI, 556–570] vs 544 [95% CI, 534–554]; P = 0.007). Among the accepted group, those who matriculated at McMaster did not outperform those who matriculated elsewhere for part I (mean total score, 524 [95% CI, 515–533] vs 546 [95% CI, 535–557]; P = 0.004) and for part II (mean total score, 557 [95% CI, 548–566] vs 582 [95% CI, 569–594]; P = 0.003).

Conclusion

Compared with students who were rejected by an admission process that used MMI assessment, students who were accepted scored higher on Canadian national licensing examinations.

---

### Suicidal ideation and attempts among students in grades 8, 10, and 12-Utah, 2015 [^fe196d7c]. MMWR: Morbidity and Mortality Weekly Report (2018). Low credibility.

§§§ For supportive family environment, a mean score was calculated from the following three items, which were on a 4-point Likert scale. Statements included the following: "My parents ask me what I think before most family decisions affecting me are made"; "If I had a personal problem, I could ask my mom or dad for help"; and "My parents give me lots of chances to do fun things with them." A higher mean score indicated stronger family environments, with a possible range of 1–12 (α = 0.84).

---

### Scoring system: use and not use from the future to present [^b703335b]. Critical Care (2024). Medium credibility.

In conclusion, models such as Bayesian networks are currently rarely used, even though they represent a highly promising tool for the future of medicine. Until these techniques are ready for prime time, we believe it is still useful to have simple, pragmatic and easy-to-use tools to help doctors anticipate patients' needs.

---

### NASPGHAN guidelines for training in pediatric gastroenterology [^b66c6e5e]. Journal of Pediatric Gastroenterology and Nutrition (2013). Medium credibility.

NASPGHAN pediatric gastroenterology — endoscopy trainee assessment instrument outlines procedural, cognitive, and postprocedural domains with anchored rating scales and a total score metric. Procedural skills include Able to perform other required therapeutic intervention (eg, polypectomy) and Overall scope navigation — includes tip deflection, advancement withdrawal, and torque, each scored with Instructor intervention required, Prompting required, Independent and 1 2 3 4 5. Cognitive items include Appropriately recognizes anatomic landmarks (eg, appendix, ileocecal valve) and Recognizes abnormalities with the anchors Unsatisfactory Average Outstanding and 1 2 3 4 5. Postprocedural assessment covers Provides postprocedural effective communication to patient and family, including endoscopic findings and management plan if necessary and Recognizes and appropriately treats complications using Unsatisfactory Average Outstanding and 1 2 3 4 5. An Overall Assessment of Trainee's Performance uses Unsatisfactory Average Outstanding with 1 2 3 4 5 6 7 8 9 10, and the form records a Total Score (maximum 100; maximum of 95 if no therapy performed) plus documentation fields, including Was this reviewed with trainee? Yes________ No________ and signature/date lines.

---

### A simple practical patient-reported clinic satisfaction measure for young adults [^607ae670]. British Journal of Cancer (2010). Low credibility.

Testing the questionnaire

The brief questionnaire was then tested by comparing the scores with the total satisfaction from the independent data set. As reported previously, this had a median satisfaction score of 64 (range 43–80), a mean age of 23.7 years and 52% female participants.

In this 'validation' data set, the brief questionnaire again provided good correlation with the overall score (Pearson's ρ = 0.82, P < 0.001, 95% CI 0.76–0.87: comparing validation and derivation sets, P = 0.16). In this data set, the median score of the brief questionnaire was 9 (range 5–12). There was no systematic difference in correlation between the two centres (centre 1 ρ = 0.81 (95% CI 0.74–0.89), centre 2 ρ = 0.83 (95% CI 0.71–0.88), P = 0.67). The 'diagnostic' ability of a cutoff of ⩾9 to identify the lowest decile was also comparable (sensitivity 75%, specificity 78%, scores of 9 or more, predictive value of 3% for 'dissatisfaction' Figure 2).

---

### Teaching students to question [^b5260be1]. American Journal of Obstetrics and Gynecology (2004). Low credibility.

Objective

The purpose of the study was to teach medical students the strategies of critical questioning to determine the effect on students' critical thinking skills, confidence in their ability to ask questions, and interaction between student and instructor.

Study Design

Workshops were developed to teach medical students how to systematically ask critical questions. Sixty-two consenting students in their third-year obstetrics and gynecology clerkship were divided according to alternate rotations to either attend the workshops (n = 28) or not (n = 41).

Results

Medical students who attended the workshops scored higher on the California Critical Thinking Skills Test mean total score (study group 25.1 [± 0.7 SEM] vs control group 22.9 [± 0.6 SEM], P = 0.028), subscales of inference (12.6 [± 0.3 SEM] vs 11.2 [± 0.3 SEM], P = 0.003), and of deductive reasoning (12.7 [± 0.4 SEM] vs 10.9 [± 0.3 SEM], P = 0.001).

Conclusion

Teaching students to ask critical questions improves critical thinking as measured by the California Critical Thinking Skills Test.

---

### Inconsistency between overall and subgroup analyses [^50ad8f76]. General Psychiatry (2022). Medium credibility.

Notations

We used the example in table 1 to develop our notation. However, our results apply to both continuous and categorical outcomes. Let Y i denote the score of a randomly selected student from school i, and p i denote the proportion of girls in school i, i = 1, 2. We define the following quantities: a 10 = average score of girls in school 1, a 11 = average score of boys in school 1, a 20 = average score of girls in school 2, a 21 = average score of boys in school 2.

Then the overall average scores of these two schools are a 1 = p 1 a 10 +(1−p 1) a 11, and a 2 = p 2 a 20 +(1−p 2) a 21, respectively. They are the weighted averages of the subgroup averages.

We also define some differences in the score:

(1) The differences between girls and boys within each school: d 1 = a 10 −a 11, d 2 = a 20 – a 21.

(2) The difference in girls (boys) between the two schools (subgroup differences):

Δ 0 = a 10 –a 20,

Δ 1 = a 11 –a 21.

(3) The overall difference between the two schools:

Δ = a 1 – a 2.

It is easy to prove that

Δ 0 –Δ 1 = d 1 – d 2.

---

### Teaching important basic EEG patterns of bedside electroencephalography to critical care staffs: a prospective multicenter study [^2baeef56]. Neurocritical Care (2021). Medium credibility.

Background

Continuous electroencephalography (cEEG) is commonly recommended for neurocritical care patients. Routine implementation of such monitoring requires the specific training of professionals. The aim of this research was to evaluate the effectiveness of a training program on initiation of the basic interpretation of cEEG for critical care staff in a prospective multicenter study.

Methods

After completion of a pretest, participants (senior physicians, fellows, residents, medical students, and nurses) recruited in six French ICUs participated in a face-to-face electroencephalogram (EEG) training program followed by additional e-learning sessions at day 1 (post-course), day 15, day 30, and day 90, based on training tests followed by illustrated and commented answers. Each test was designed to evaluate knowledge and skills through correct recognition of ten predefined EEG sequences covering the most common normal and abnormal patterns. The primary objective was to achieve a success rate > 80% correct answers at day 90 by at least 75% of the participants.

Results

Among 250 participants, 77/108 (71.3%) who completed the full training program achieved at least 80% correct answers at day 90. Paired comparisons between the scores obtained at each evaluation showed an increase over time. The rate of correct answers at day 90 was > 80% for all common predefined EEG sequences, except for the recognition of periodic and burst-suppression patterns and reactivity, which were identified in only 42.6% (95% CI 36.4–48.8), 60.2% (54.1–66.3), and 70.4% (64.7–76.1) of the tests, respectively.

Conclusions

A training strategy for the basic interpretation of EEG in ICUs, consisting of a face-to-face EEG course supplemented with reinforcement of knowledge by e-learning, was associated with significant resignation and an effectiveness of training allowing 71% of learners to accurately recognize important basic EEG patterns encountered in critically ill patients.

Trial Registration

ClinicalTrials.gov number: NCT03545776.

---

### Medical student selection [^e5304b0c]. The Surgical Clinics of North America (2021). Medium credibility.

Medical school admissions committees are tasked with fulfilling the values of their institutions through careful recruitment. Making accurate predictions regarding the enrollment behavior of admitted students is critical to intentionally formulating class composition and impacts long-term physician representation.

---

### Interpretation and use of a decision support tool for multiple treatment options: a combined randomised controlled trial and survey of medical students [^07db1631]. BMJ Evidence-Based Medicine (2024). High credibility.

Outcome measures

Interpretation of evidence was measured using five multiple choice questions. Each question had 4–6 response categories with 1–2 correct answers. We also measured the proportion of students that recommended treatment in congruence with the trustworthy guideline; the panel issued a strong recommendation for adding SLGT-2 inhibitors to standard therapy for patients in this risk group. The students were at no point informed of this recommendation.

We measured perceived usefulness and self-assessed understandability of the tool using 7-point Likert scale ranging from 'strongly disagrees to 'strongly agree' with a neutral option in the middle, corresponding to ratings 1–7.

Overall preference for MATCH-IT tool A or B and preference for design alternatives (ie, preference for colour coding vs no colour coding; and preference for displaying an open full table vs a closed table on entry) was measured using a 7-point Likert scale ranging from 'strong preference for A' to 'strong preference for B'. Ratings 1–3 were in favour of version B, rating 4 was indifference between the two versions and 5–7 were in favour of version A. See online supplemental file 2 for the full questionnaire with questions and response alternatives.

Statistical analysis

We planned to report aggregated results for the whole sample in addition to comparison of the randomised groups. For data assessing interpretation of evidence, we created a composite variable of responses to multiple choice questions. Here, we pragmatically defined four out of five correct answers to constitute correct overall interpretation. For the ordinal variables (perceived usefulness and self-assessed understandability, preferences), we visually inspected the distributions. These distributions were all either U-shaped or clustered strongly towards one side. For the perceived usefulness and self-assessed understandability variables, we, therefore, dichotomised to useful/not useful and understandable/not understandable. For the treatment recommendation variable and preference variables, we wanted to capture the proportion of students who provided no treatment recommendation or who had no preference. We, therefore, divided these variables into three categories.

To compare the two randomised groups, we used the Mann-Whitney U non-parametric test for outcomes with ordinal variables that had three or more categories. We used χ 2 tests for dichotomised outcomes. We used STATA for all statistical analysis.

We did not perform a formal power calculation to detect a statistically significantly difference a priori. We knew that approximately 200 students would attend the lecture and hypothesised that at least 140 students would participate in the study.

---

### Exploring the knowledge and practices on road safety measures among motorbikers in dhaka, Bangladesh: a cross-sectional study [^45a10a44]. Injury Prevention (2025). Medium credibility.

Variation in knowledge and practices of the respondents

A significant difference in respondents' knowledge and practice with socio-demographic characteristics was observed (table 4). Results found that the average knowledge score was significantly higher among younger adults aged 18–29 years than that among the older population aged 50 and above (mean knowledge score: 0.80 (0.79–0.82) vs 0.75 (0.69–0.82); p = 0.04). Similarly, the mean knowledge score was higher among participants with secondary, higher secondary and tertiary education compared with the primary education group. Moreover, knowledge score was higher among non-married (0.81 (0.79–0.83) vs 0.78 (0.77–0.79); p = 0.006) and non-smokers (0.80 (0.79–0.82) vs 0.78 (0.77–0.79)); p = 0.026] than their counterparts (table 4).

Table 4
Independent sample t-test and one-way ANOVA test for knowledge and practice score

Regarding practices, the mean practice score was associated with educational status, religion and smoking status. Similar to knowledge level, the average practice score was higher among participants with secondary, higher secondary and tertiary education compared with the primary group. Additionally, average practice score was significantly higher (mean practice score: among Muslims (0.77 (0.77–0.78) vs 0.73 (0.69–0.77); p = 0.025)).

---

### Teaching medical students to help patients manage their weight: outcomes of an eight-school randomized controlled trial [^e1b51800]. Journal of General Internal Medicine (2021). Medium credibility.

Associations of WMC training with outcomes

At MME schools, student participation in the three intervention components was positively associated with total OSCE score, with the largest difference for the web course (0.43 items difference on average), although the associations were not statistically significant (Table 4). MME students who completed two of the three components had the highest mean OSCE score at 9.02 (standard error 0.39), and those completing no components had the lowest mean OSCE score at 8.22 (standard error 0.44), although differences were not statistically significant (p = 0.28). MME student participation in the three intervention components also was positively associated with perceived WMC skills, with statistically significant associations for all but web course participation. Among students at both MME and TE schools, time spent in WMC learning activities in the first two years ranging from 1 (< 30 minutes) to 8 (> 5 hours) was significantly positively associated with total OSCE score and total perceived WMC skills; an increase of 1 category (e.g. < 30 minutes to 30–59 minutes) was associated with an average increase of 0.08 in the OSCE score and of 0.04 in total perceived WMC skills. The corresponding associations with time in MS3 year also were positive but smaller and not statistically significant for the OSCE score.

Table 4.
Associations of WMC training with MS3 Total OSCE Scores and Perceived WMC skills

(a) Adjusted for student's Year 1 perceived WMC score

---

### Assessment of vitamin D-related knowledge, attitudes and practices among sultan qaboos university students in Oman: a cross-sectional study [^27298c80]. BMJ Public Health (2024). High credibility.

Section A includes 11 questions relating to general knowledge of vitamin D. Participants can give responses of 'yes', 'no' or 'I don't know', scored as 2, 0 or 1, respectively, with total scores for this section ranging from 0 to 22. Section B comprises five questions related to nutritional knowledge, with possible responses of 'yes', 'no' or 'I don't know' scored as 0, 2 or 1, respectively, except for item 12 which is inversely scored, resulting in a total score range of 0–10. Section C consists of 12 questions assessing vitamin D-related attitudes on a 5-Likert scale, with possible responses ranging from 'strongly disagree' to 'strongly agree' allocated scores of 1–5, respectively, for a total score range of 12–60. Finally, section D includes 10 questions relating to the frequency of various vitamin D-related practices, with possible responses of 'never', 'rarely', 'sometimes', 'often' or 'always' scored on a 5-point Likert scale from 1 to 5, with four items scored inversely, for a total score range of 10–50. For each section, the total score was proportionately transformed to a range of 0−100.

For the purposes of the present study, per cent scores of ≥ 80% indicate adequate level of knowledge, positive attitude (≥ 90%) and good prevention practice (≥ 75%) in line with Bloom's cut-off values for evidence-based medicine.

Data collection

The researchers employed a convenience sampling method, approaching students within their college environments, such as libraries and campuses. The data was collected between September and December 2022. Those who agreed to take part signed a consent form and completed a questionnaire, which typically required approximately 10 min to fill out.

Patient and public involvement

The participants solely completed the questionnaire and had no involvement in its design or any other process in the study.

Data analysis

The Statistical Package for the Social Sciences (SPSS) software, V.23 (IBM, Armonk, New York, USA) was used to analyse collected data. Data related to the demographic characteristics, knowledge, attitudes and practices of the participants were presented using descriptive statistics (ie, frequencies and percentages). Means and SDs were used to present continuous variables. Comparisons of quantitative and categorical variables were conducted using a Mann-Whitney U test. The level of statistical significance was set at p ≤ 0.05.

---

### The effect of a 13-hour curriculum to improve residents' teaching skills: a randomized trial [^27981c2a]. Annals of Internal Medicine (2004). Low credibility.

Background

Although resident physicians often teach, few trials have tested interventions to improve residents' teaching skills. A pilot trial in 2001–2002 found that 13 trained resident teachers taught better than did untrained control residents.

Objective

To determine whether a longitudinal residents-as-teachers curriculum improves residents' teaching skills.

Design

Randomized, controlled trial from May 2001 to February 2002 (pilot trial) and March 2002 to April 2003.

Setting

4 generalist residencies affiliated with an urban academic medical center.

Participants

62 second-year residents: 23 in the 2001–2002 pilot trial and 39 more in 2002–2003; 27 of the 39 participants were medicine residents required to learn teaching skills.

Intervention

A 13-hour curriculum in which residents practiced teaching and received feedback during 1-hour small-group sessions taught twice monthly for 6 months.

Measurements

A 3.5-hour, 8-station, objective structured teaching examination that was enacted and rated by 50 medical students before and after the intervention. Two trained, blinded raters independently assessed each station (inter-rater reliability, 0.75).

Results

In the combined results for 2001–2003, the intervention group (n = 33) and control group (n = 29) were similar in sex, specialty, and academic performance. On a 1 to 5 Likert scale, intervention residents outscored controls on overall improvement score (post-test-pretest difference, 0.74 vs. 0.07; difference between intervention and control groups, 0.68 [95% CI, 0.55 to 0.81]; P < 0.001) by a magnitude of 2.8 standard deviations and on all 8 individual stations. The intervention residents improved 28.5% overall, whereas the scores of control residents did not increase significantly (2.7%). In 2002–2003, 19 intervention residents similarly outscored 19 controls (post-test-pretest difference, 0.83 vs. 0.14; difference between intervention and control groups, 0.69 [CI, 0.53 to 0.84]; P < 0.001). Twenty-seven medicine residents required to learn teaching skills achieved scores similar to those of volunteers.

Limitations

The study was conducted at a single institution. No "real life" assessment with which to compare the results of the objective structured teaching examination was available.

Conclusions

Generalist residents randomly assigned to receive a 13-hour longitudinal residents-as-teachers curriculum consistently showed improved teaching skills, as judged by medical student raters. Residents required to participate improved as much as volunteers did.

---

### Safety of eyeglasses wear for visual acuity among middle school students in northwestern rural China: a cluster-randomised controlled trial [^b6acb2ad]. BMJ Open Ophthalmology (2020). High credibility.

Introduction

Uncorrected refractive error can lead to a variety of broader issues. It is estimated by the WHO that uncorrected refractive error results in the global loss of hundreds of billions of US dollars each year due to reduced productivity. In addition to loss of visual function, refractive error is also associated with reduced educational performance in children. As of 2004, 6.4 million children aged 5–15 years living in China were affected by refractive error, which accounts for 50% of all cases of refractive error in the world.

Refractive error can be effectively and inexpensively treated with eyeglasses. However, among middle school students in rural China, as few as 30% of students needing eyeglasses own and wear them. Studies in China suggest that factors leading to poor compliance include discomfort or inconvenience, lack of perceived needand fear of harm to vision. More specifically, vision knowledge of children, parents and teachers was normally distributed around a low to medium average. Parents scored an average of 34% on a short knowledge test on vision care. Teachers performed slightly better, with an average score of 56%. Students had the lowest level of baseline knowledge, with an average score of only 29%. Some of this low scoring may be attributed to a belief prevalent in rural China that wearing eyeglasses can worsen uncorrected visual acuity (VA).

Previous studieshave been inconclusive about the impact of eyeglasses wear on uncorrected VA. The majority of these studies have identified correlations between eyeglasses wear and uncorrected VA, but have not identified a causal link. Only one previous trial has addressed causality between wearing eyeglasses and VA. Using a cluster-randomised, investigator-masked, controlled trial, this study found no evidence that receiving free eyeglasses worsened uncorrected VA among primary school students in western rural China. However, little is known about the effects of eyeglasses wear on uncorrected VA among middle school students. In rural western China, middle school students may have higher rates of refractive error (50%vs 24%) and eyeglasses ownership (31%vs 15%) on account of their age. Additionally, unlike primary school students, middle school students are more likely to choose whether to wear eyeglasses based on peer influences and social stigma. In the light of the previous finding, we sought to replicate the previously proven results in a different cohort.

---

### Are people with negative diabetes screening tests falsely reassured? Parallel group cohort study embedded in the ADDITION (cambridge) randomised controlled trial [^20442a85]. BMJ (2009). Excellent credibility.

Outcome measures

Perceived personal risk — Participants were asked to estimate their chance of getting diabetes at some time in their life. Responses were given as a percentage score on an 11 point scale ranging from 0% to 100%.

Perceived comparative risk — Participants were asked to indicate their chance of getting diabetes, compared with other people of their age. Response options provided on a five point rating scale were (1) much lower, (2) a little lower, (3) about the same, (4) a little higher, and (5) much higher.

Behavioural intention — Participants were asked to rate three statements of intention to be more physically active, to eat a lower fat diet, and to reduce the amount of dietary sugar in the next 12 months. Response options provided on a five point rating scale were (1) strongly disagree, (2) disagree, (3) neither agree nor disagree, (4) agree, and (5) strongly agree.

Self rated health — A single item asked about the participant's general health, with response options of (1) excellent, (2) very good, (3) good, (4) fair, and (5) poor.

Sample size

We determined the sample size in a previous substudy to quantify the effects of screening on anxiety and depression. The observed 95% confidence interval for the primary outcome, personal risk, in this study incorporates the observed clustering of the outcome by practice and provides the basis of an indicative post hoc calculation of detectable effect sizes. We would have 80% power to detect a difference between screened and control groups in mean personal risk of 6.5 scale units at baseline (standardised difference of 0.30 SD) and 5.75 units at later time points (equivalent to effect size differences of 0.3 SD and 0.27 SD).

---

### Examination of the test-retest reliability of a computerized neurocognitive test battery [^eb2ac44d]. The American Journal of Sports Medicine (2014). Low credibility.

Background

Test-retest reliability is a critical issue in the utility of computer-based neurocognitive assessment paradigms employing baseline and postconcussion tests. Researchers have reported low test-retest reliability for the Immediate Post Concussion Assessment and Cognitive Testing (ImPACT) across an interval of 45 and 50 days.

Purpose

To re-examine the test-retest reliability of the ImPACT between baseline, 45 days, and 50 days.

Study Design

Descriptive laboratory study.

Methods

Eighty-five physically active college students (51 male, 34 female) volunteered for this study. Participants completed the ImPACT as well as a 15-item memory test at baseline, 45 days, and 50 days. Intraclass correlation coefficients (ICCs) were calculated for ImPACT composite scores, and change scores were calculated using reliable change indices (RCIs) and regression-based methods (RBMs) at 80% and 95% confidence intervals (CIs).

Results

The respective ICCs for baseline to day 45, day 45 to day 50, baseline to day 50, and overall were as follows: verbal memory (0.76, 0.69, 0.65, and 0.78), visual memory (0.72, 0.66, 0.60, and 0.74), visual motor (processing) speed (0.87, 0.88, 0.85, and 0.91), and reaction time (0.67, 0.81, 0.71, and 0.80). All ICCs exceeded the threshold value of 0.60 for acceptable test-retest reliability. All cases fell well within the 80% CI for both the RCI and RBM, while 1% to 5% of cases fell outside the 95% CI for the RCI and 1% for the RBM.

Conclusion

Results suggest that the ImPACT is a reliable neurocognitive test battery at 45 and 50 days after the baseline assessment. The current findings agree with those of other reliability studies that have reported acceptable ICCs across 30-day to 1-year testing intervals, and they support the utility of the ImPACT for the multidisciplinary approach to concussion management.

Clinical Relevance

This study suggests that the computerized neurocognitive test battery, ImPACT, is a reliable test for postconcussion serial assessments. However, when managing concussed athletes, the ImPACT should not be used as a stand-alone measure.

---

### Clinical practice guidelines: diagnosis, treatment, and management of children and adults with central auditory processing disorder [^4eb50e47]. AAA (2010). Medium credibility.

Central auditory processing disorder (C)APD — test performance concepts and evidence grading are defined, including that the false positive rate is 1 minus the specificity (e.g., 1 − 80% specificity = 20% false positive rate), test efficiency combines specificity and sensitivity as the overall number of true positives and true negatives divided by the total tested, validity reflects whether evidence and theory support intended test score interpretations, and reliability refers to the stability of a test score and is essential to validity. The guideline uses a numerical level-of-evidence scale ranging from 1 to 5, with 1 indicating the strongest level of support and 5 the weakest; although the model rates group studies as superior to individual case studies, case and retrospective studies (classified as level 3 or 4 evidence) can often provide appropriate evidence for an individual profile. Clinicians should take a pragmatic approach to evaluating evidence, and the guideline focuses on four areas: 1) patient history and selection criteria, 2) diagnosis, 3) intervention, and 4) professional issues, education, and training.

---

### Guidelines for assessment of and intervention with persons with disabilities [^dfbb2207]. The American Psychologist (2012). Medium credibility.

K-12 versus college responsibility by issue — the following table outlines common issues faced by students with disabilities and the primary responsible party for both K-12 and for college, with responsibilities as follows: Identification — School vs Student; Assessment — School vs Student; Programming — School/Parent vs Student/College; Advocacy — School/Parent vs Student; Decision Making — Placement Team vs Student; Transition Plan — Placement Team vs Student.

---

### US medical student performance on the NBME subject examination in internal medicine: Do clerkship sequence and clerkship length matter? [^3c8a8ef4]. Journal of General Internal Medicine (2015). Low credibility.

Background

Prior to graduation, US medical students are required to complete clinical clerkship rotations, most commonly in the specialty areas of family medicine, internal medicine, obstetrics and gynecology (ob/gyn), pediatrics, psychiatry, and surgery. Within a school, the sequence in which students complete these clerkships varies. In addition, the length of these rotations varies, both within a school for different clerkships and between schools for the same clerkship.

Objective

The present study investigated the effects of clerkship sequence and length on performance on the National Board of Medical Examiner's subject examination in internal medicine.

Participants

The study sample included 16,091 students from 67 US Liaison Committee on Medical Education (LCME)-accredited medical schools who graduated in 2012 or 2013.

Main Measures

Student-level measures included first-attempt internal medicine subject examination scores, first-attempt USMLE Step 1 scores, and five dichotomous variables capturing whether or not students completed rotations in family medicine, ob/gyn, pediatrics, psychiatry, and surgery prior to taking the internal medicine rotation. School-level measures included clerkship length and average Step 1 score.

Design

Multilevel models with students nested in schools were estimated with internal medicine subject examination scores as the dependent measure. Step 1 scores and the five dichotomous variables were treated as student-level predictors. Internal medicine clerkship length and average Step 1 score were used to predict school-to-school variation in average internal medicine subject examination scores.

Key Results

Completion of rotations in surgery, pediatrics and family medicine prior to taking the internal medicine examination significantly improved scores, with the largest benefit observed for surgery (coefficient = 1.58 points; p value < 0.01); completion of rotations in ob/gyn and psychiatry were unrelated to internal medicine subject examination performance. At the school level, longer internal medicine clerkships were associated with higher scores on the internal medicine examination (coefficient = 0.23 points/week; p value < 0.01).

Conclusions

The order in which students complete clinical clerkships and the length of the internal medicine clerkship are associated with their internal medicine subject examination scores. Findings may have implications for curriculum re-design.

---

### Composite rating scales [^2fbad343]. Journal of the Neurological Sciences (2010). Low credibility.

Rating scales are instruments that are very frequently used by clinicians to perform patient assessments. Typically, rating scales grade the attribute on an ordinal level of measurement, i.e., a rank ordering, meaning that the numbers assigned to the different ranks (item scores) do not represent 'real numbers' or 'physical magnitudes'. Single-item scales have some advantages, such as simplicity and low respondent burden, but they may also suffer from disadvantages, such as ambiguous score meanings and low responsiveness. Multi-item scales, in contrast, seem more adequate for assessment of complex constructs, allowing for detailed evaluation. Total scores representing the value of the construct may be quite precise and thus the responsiveness of the scale may be high. The most common strategy for obtaining the total score is the sum of the item scores, a strategy that constitutes one of the most important problems with these types of scales. A summative score of ordinal figures is not a 'real magnitude' and may have little sense. This paper is a review of the theoretical frameworks of the main theories used to develop rating scales (Classical Test Theory and Item Response Theory). Bearing in mind that no alternative is perfect, additional research in this field and judicious decisions are called for.

---

### An interuniversity competition for medical students to learn radiology in the second life metaverse [^ac1af947]. Journal of the American College of Radiology (2024). Medium credibility.

Purpose

The aim of this study was to evaluate an interuniversity competition online to learn radiology held in a 3-D virtual world, the Second Life metaverse, by analyzing the results of the game and students' perceptions.

Methods

Medical students voluntarily participated in teams of four, for 6 weeks, successively covering radiologic anatomy and radiologic semiology of the chest, abdomen, and musculoskeletal. Each week, participants had 4.5 days to study self-learning presentations and 2.5 days to complete an individual multiple-choice test and a team task, the results of which determined the game's ranking. Participants were asked to complete a cognitive-load test, a perception questionnaire, and a postexposure knowledge test.

Results

The competition was repeated for 2 years (editions), in 2020 and 2021. Seventy-five of 102 teams (73.5%) registered completed the game; 76% of them included third-year students. The average percentage of correct answers in the individual tests and team tasks was 74.2 ± 15.1 and 71.6 ± 14.7 respectively, without significant differences between both competitions. In general, the experience was valued positively (scores > 8 on a 10-point scale). A lower perception score was found in 2021 among students from universities other than the organizing university, showing a positive correlation with the in-game score.

Conclusions

An interuniversity competition in the Second Life metaverse for undergraduate learning radiology is feasible and reproducible. Participating medical students considered it interesting and useful and also identified this activity during the 2 years of the coronavirus disease 2019 pandemic as a playful learning and social interaction experience.

---

### Short social media videos as a supplementary educational resource in neuroanatomy: a nonrandomized clinical trial [^c428a2fd]. JAMA Network Open (2025). High credibility.

Regression Analysis Findings

A multiple regression analysis was performed to assess the association of student group (intervention vs control), gender, previous anatomy grade, and grade point average with current neuroanatomy scores. The model indicated a weak but significant association between the variables and neuroanatomy scores (R = 0.342; F = 5.304; P < .001).

However, the model's explanatory power was limited. The coefficient of determination (R 2) was 0.117, with the adjusted R 2 at 0.095, indicating that only 9.5% of the variance in neuroanatomy scores was explained by the variables in the model. Furthermore, the SE of the estimate was 20.71, highlighting a substantial margin of error in the estimation of individual scores.

An examination of the individual variables revealed that students in the control group had significantly higher scores, a mean of 10.6 (3.7) points above the intervention group (P = 0.001). Grade point average also had a significant positive association, where each 1-unit increase was associated with a 0.9-point increase in the neuroanatomy score (P = 0.002). Conversely, gender and previous anatomy grade did not have a statistically significant association with scores.

---

### Development and validation of the emergency department assessment of chest pain score and 2 h accelerated diagnostic protocol [^cee243db]. Emergency Medicine Australasia (2014). Low credibility.

The clinical calculator "Emergency Department Assessment of Chest Pain Score (EDACS)" for chest pain.

The Emergency Department Assessment of Chest Pain Score (EDACS) is a clinical calculator used in the emergency department setting to stratify the risk of patients presenting with chest pain. It is specifically designed to identify patients at low risk of major adverse cardiac events (MACE) within 30 days, who are safe for early discharge without further diagnostic testing.

The EDACS applies to adult patients presenting to the emergency department with symptoms suggestive of acute coronary syndrome. It incorporates variables such as age, sex, risk factors for coronary artery disease, duration of pain, pain characteristics, and initial ECG changes to generate a score.

The clinical utility of the EDACS is to aid in the safe and efficient disposition of patients in the emergency department, reducing unnecessary admissions and investigations, and improving patient flow.

Exclusion criteria for the use of EDACS include patients with obvious non-cardiac causes of chest pain, known coronary artery disease, or those who are hemodynamically unstable.

The Emergency Department Assessment of Chest Pain Score (EDACS) is a tool designed to assess the risk of chest pain in patients by calculating a numerical score based on various clinical inputs. This score helps determine the clinical pathway by indicating the level of risk associated with the patient's chest pain. Here's a guide to how the EDACS works:

The calculator uses six key inputs, each assigned a set of points based on specific conditions:

- **Age (years)**:
- **18–45**: 2 points
- **46–50**: 4 points
- **51–55**: 6 points
- **56–60**: 8 points
- **61–65**: 10 points
- **66–70**: 12 points
- **71–75**: 14 points
- **76–80**: 16 points
- **81–85**: 18 points
- **≥ 86**: 20 points

- **Gender**:
- **Male**: 6 points
- **Female**: 0 points

- Known coronary artery disease or ≥ 3 risk factors (only for patients aged 18–50 years):
- **Yes**: 4 points
- **No**: 0 points

- **Symptoms and signs**:
- **Diaphoresis**: 3 points
- **Chest pain radiating to arm or shoulder**: 5 points
- **Chest pain occurring or worsening with inspiration**: 4 points
- **Chest pain reproduced by palpation**: 6 points

- **New ischemia on electrocardiogram**:
- **Yes**: 16 points
- **No**: 0 points

- **Troponins (initial and 2-hour)**:
- **Both negative**: 0 points
- **Either positive**: 16 points

For each of these inputs, the corresponding points are summed up to give a total score. This cumulative score ranges from a minimum of 5 to a maximum of 68.

Next, interpret the score:

- **Scores less than 16 suggest a low risk**: "Safe for discharge to early outpatient follow-up (or proceed to earlier inpatient testing)".
- **Scores of 16 or higher indicate the patient is not at low risk**: "Not eligible for discharge, proceed with usual care with further observation and troponin testing".

This interpretation helps healthcare providers decide on the appropriate management strategy, ensuring that patients with higher risk receive adequate observation and testing, while those deemed low-risk can be safely discharged or directed to follow-up care.

---

### APA guidelines for psychological assessment and evaluation [^8b48c615]. APA (2020). High credibility.

Guideline 12 — Psychologists who conduct psychological testing, assessment, and evaluation seek to consider the unique issues that may arise when test instruments and assessment approaches designed for specific populations are used with diverse populations — states that psychological assessment instruments and interpretive methods are culture specific because they are developed from a specific sociocultural context, most often Western, and that this context is linked to the population used in the test standardization and may be less applicable to groups with other characteristics and cultures. It notes that assessment techniques and standardized instruments designed for a specific population have their validity and reliability tied to this normative group and cannot be readily made to apply to other groups even when they undergo adaptations and validity, reliability, and measurement equivalence have been tested across culturally diverse sample groups. Psychologists endeavor to use only properly up-to-date normed, standardized, and translated measures chosen specifically based on the client's ethnic group, acculturation level, language proficiency, education level, socioeconomic status, age, disability status, and other relevant demographic factors. For many areas of assessment, including cognitive function, it is important for psychologists to strive to use age-appropriate test norms; for example, slowed processing speed is a normal part of cognitive aging, and average scores for older adults on tests requiring fast performance, such as the Trail Making Test, would be interpreted as indicating impairment if norms for younger people were used. Psychologists are encouraged to recognize that a psychological test needs to have established research-based validity and reliability in the culture and language in which it was constructed before it can be considered appropriate for adaptation in other languages and cultures, which may be necessary for clients with less acculturation to the Western society and whose English is not the dominant language. When a test administration or protocol requires translation to match the language dominance of the examinee because a test version in the client's language and with appropriate cultural characteristics has not been developed, strict guidelines have been recommended by the International Test Commission (2017); translations can include trained professionals conducting forward and back translations focused on meaning including local and cultural context, and those translated tests or protocols, if not normed, may produce less meaningful results. Application emphasizes that although it is nearly impossible to match a test or assessment instrument to all the diverse characteristics of the client/patient/examinee, psychologists endeavor to familiarize themselves with the characteristics and cultural origin of their instruments in order to decide whether the test would be fairly assessed and fairly compared with the normative group.

---

### Clinical trials of potential cognitive-enhancing drugs in schizophrenia: what have we learned so far? [^a4078420]. Schizophrenia Bulletin (2013). Low credibility.

Completed Trials.

Among the 61 completed trials (figure 2), 34(55.7%) used double blind, randomized, placebo-controlled add-on design; 7(11.5%) used randomized, double-blind, placebo-controlled crossover design; 3(4.9%) used add-on open-label design; and 17(27.9%) were monotherapy trials. No clear preference or consistency in the primary neurocognitive outcome measure was observed. Results of 50.0% of add-on trials and 22.0% of trials using other designs were available in the public domain.

Ongoing Trials.

Among the 57 ongoing trials, 31(68.4%) are add-on trials; 5(8.8%) are crossover trials, 7(12.3%) are open label, and 6(10.5%) are monotherapy trials. In 79.5% (N = 31) of the ongoing add-on trials and in 88.9% (N = 16) of ongoing trials using other designs, recruitment began since 2007. The MCCB,(either alone or in combination with another neurocognitive assessment battery) is the primary outcome measure in 53.8% (N = 21) of ongoing add-on trials and in 38.9% (N = 7) of ongoing trials using other designs.

Characteristics of Completed and Ongoing Add-on Trials

Sample Size.

A similar distribution of sample sizes was observed for both completed trials and ongoing trials (figure 3). However, only 17.6% of completed and 35.9% of ongoing trials report a sample size that was or is anticipated to be sufficient to produce statistical power to detect a medium (d = 0.5) effect size, which requires 71 subjects per group (using 2-arm trial with drug and placebo) assuming the primary outcome measure has excellent test-retest reliability (ICC = .90) as with the MCCB composite score(see online supplementary tables 2 and 4).

Fig. 3.
Total sample size in completed and ongoing add-on trials.

Trial Duration.

While the trial duration was ≤ 8 weeks in the majority of completed trials (58.8%), there is a pattern of longer duration among ongoing trials, with 66.7% being > 8 weeks long. Nevertheless, despite a moderate shift toward longer trial duration, the length of 33.3% of ongoing trials is ≤ 8 weeks.

---

### ACFAS scoring scale user guide [^715ba2da]. The Journal of Foot and Ankle Surgery (2005). Medium credibility.

ACFAS Scoring Scale — validation process and current status are as follows: The ACFAS Scoring Scale has undergone several tests to validate the design of this tool, with validation parameters including reliability (test-retest); construct validity (subjective vs objective correlation); multiple rater effects; and criterion validity. Modules 1 and 2 of the ACFAS Scoring Scale have been validated; modules 3 and 4 are currently pending validation.

---

### Guidelines for psychological practice with girls and women [^3edd2b8b]. The American Psychologist (2007). Medium credibility.

Psychological assessment and standardized testing in girls and women — gendered scales, score norms, and cultural applicability — include cautions that psychologists are cautioned to find research on gender differences in various assessment tools before relying on tests that use non-gendered scoring. Regarding standardized clinical scales pertaining to gender (e.g. Scale 5 on the MMPI-2), there has been criticism suggesting that they caricature gender roles, see gender as dichotomous and unchanging, and show little correlation with gender identity or gender role-related behavior. In assessing girls and women, psychologists should also be aware of the history of achievement tests favoring boys over girls and men over women through language and examples that favor experiences more familiar to boys and men, and should also be aware that there are biases regarding the reference to which the client uses in responding to various questions, with women often comparing themselves to a generic male rather than other women. It is also important to note that while normative samples may be comparable to the U.S. population and provide norms for the "average American," the average American is often assumed to be White; as such, it may be inappropriate to apply these to African Americans, Asian Americans, Latino(as), indigenous individuals, or people of other non-White racial and ethnic groups. Additional concerns include that some women score significantly higher on the D scale (depression) and the FBS (Fake Bad Scale FBS) and that using non-gendered T scores could result in biases, and that other scales had been originally produced to define a construct in men, and then later used to assess women.

---

### School-aged children who are not progressing academically: considerations for pediatricians [^e7e6fac9]. Pediatrics (2019). High credibility.

American Academy of Pediatrics clinical report on school-aged children who are not progressing academically outlines its scope and aims: pediatric primary care providers may be consulted when a child is not making expected progress in school, and the report describes the current state of the science regarding educational achievement to inform pediatricians' decisions about further evaluation. It reviews commonly available options for psychological evaluation and/or treatment, medical referrals, and/or recommendations for referral for eligibility determinations at school, and strategies for collaborating with families, schools, and specialists to best serve children and families.

---

### NASPGHAN guidelines for training in pediatric gastroenterology [^bc7bf717]. Journal of Pediatric Gastroenterology and Nutrition (2013). Medium credibility.

Pediatric gastroenterology endoscopy trainee evaluation — technical, cognitive, and postprocedural competencies are itemized and scored with anchored rating scales and an overall total. Technical tasks include "Able to pass endoscope from bulb to second part of duodenum," "Able to retroflex instrument to examine fundus/gastric cardia," maintaining a clear field that "includes insufflation, suction, and irrigation as required," "Able to perform mucosal biopsy," "Able to perform other required therapeutic intervention — list," and "Overall scope navigation — includes tip deflection, advancement withdrawal, and torque." For technical items, anchors are "Instructor intervention required," "Prompting required," and "Independent." Cognitive items include "Appropriately recognizes anatomic landmarks (GE junction, Z- line, etc)" and "Recognizes abnormalities," and postprocedural elements include "Provides postprocedural effective communication to patient and family, including endoscopic findings and management plan if necessary" and "Recognizes and appropriately treats complications," which display "Unsatisfactory Average Outstanding" with the numeric scale "1 2 3 4 5." An "Overall Assessment of Trainee's Performance" uses "Unsatisfactory Average Outstanding" with the scale "1 2 3 4 5 6 7 8 9 10," and the form specifies "Total Score_____(maximum 100; maximum of 95 if no therapy performed)."

---

### Suicidal ideation and attempts among students in grades 8, 10, and 12-Utah, 2015 [^ca92fa74]. MMWR: Morbidity and Mortality Weekly Report (2018). Low credibility.

§§ For prosocial behaviors, a mean score was calculated from the following three items, which were on an 8-point Likert scale. Items asked included how many times in the past year "Have you participated in clubs, organizations, or activities at school?"; "Have you done extra work on your own for school?"; and "Have you volunteered to do community service?" A higher mean score indicated stronger prosocial behaviors, with a possible range of 1–24 (α = 0.70).

¶¶ For supportive community level environment, a mean score was calculated from the following three items, which were on a 4-point Likert scale. Statements included the following: "My neighbors notice when I am doing a good job and let me know about it"; "There are people in my neighborhood who are proud of me when I do something well"; and "There are people in my neighborhood who encourage me to do my best." A higher mean score indicated a more supportive community level environment, with a possible range of 1–12 (α = 0.90).

* For supportive school environment, a mean score was calculated from the following five items, which were on a 4-point Likert scale. Statements included the following: "In my school, students have lots of chances to help decide things like class activities and rules"; "There are lots of chances for students in my school to talk with a teacher one-on-one"; "My teachers notice when I am doing a good job and let me know about it"; "I have lots of chances to be part of class discussions or activities"; and "Teachers ask me to work on special classroom projects." A higher mean score indicates a more supportive school environment, with a possible range of 1–20 (α = 0.69).

††† For supportive peer environment, a mean score was calculated from five items, which were on a 5-point Likert scale. Items asked in the past year included how many of your best friends have "Participated in school clubs"; "Made a commitment to stay drug-free"; "Tried to do well in school"; "Have liked school"; and "Regularly attended religious services." A higher mean score indicated a more supportive peer environment, with a possible range of 1–25 (α = 0.79).

---

###: vol. 66, no. 31 [^3ceea67d]. MMWR: Morbidity and Mortality Weekly Report (2017). Low credibility.

[This corrects the article on p. 835 in vol. 66, PMID: 28796762.].

---