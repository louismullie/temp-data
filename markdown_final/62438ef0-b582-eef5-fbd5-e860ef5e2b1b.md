# Medical Question & Answer

**Sample ID**: 62438ef0-b582-eef5-fbd5-e860ef5e2b1b
**Dataset Index**: 335619

---

## Question

Example 1.8.1. Suppose that a, b, c are three positive real numbers satisfying
(a + b + c)([1/a] + [1/b] + [1/c]) = 13.

Find the minimum value of
P = (a² + b² + c²)([1/a²] + [1/b²] + [1/c²]).

---

## Answer

> Let's see… What do we have here? The user is asking to find the minimum value of (P = (a² + b² + c²)([1/a²] + [1/b²] + [1/c²])) given that ((a + b + c)([1/a] + [1/b] + [1/c]) = 13), where (a, b, c) are positive real numbers. Let's break this down step-by-step. First, I need to think about how to simplify the constraint and the objective using symmetric sums. Then, I should verify whether symmetry can reduce the problem to two variables. Next, I will express everything in terms of a single parameter and find the critical points. After that, I need to check the second derivative to confirm a minimum. Finally, I will compute the exact minimum value and ensure it is attainable with positive reals.

> Let me first confirm the notation and the constraint. Let (S₁ = a + b + c), (S₋₁ = [1/a] + [1/b] + [1/c]), (S₂ = a² + b² + c²), and (S₋₂ = [1/a²] + [1/b²] + [1/c²]). The given condition is (S₁ S₋₁ = 13), and we want to minimize (P = S₂ S₋₂). I should double-check that these definitions align with the problem statement, which they do.

> Now, I need to expand the constraint and the objective in terms of elementary symmetric sums. Wait, let me verify the expansions carefully. The constraint expands to (3 + ([a/b] + [b/a]) + ([a/c] + [c/a]) + ([b/c] + [c/b]) = 13), so the sum of pairwise ratios is 10. Similarly, the objective expands to (3 + ([a²/b²] + [b²/a²]) + ([a²/c²] + [c²/a²]) + ([b²/c²] + [c²/b²])), so I need to minimize the sum of squared pairwise ratios. Hold on, I should verify that both expansions are correct; yes, they are.

> Next, I should consider symmetry. Let me think about whether the minimum occurs when two variables are equal. By symmetry and convexity arguments, it is reasonable to assume (b = c), which reduces the problem to two variables. I should confirm that this assumption does not exclude the true minimum; given the symmetry of the expressions, the extremum should indeed occur when two variables coincide, so I will proceed with (b = c) and check the result afterward.

> With (b = c), I will now express everything in terms of (x = [a/b]). The constraint becomes ((a + 2b)([1/a] + [2/b]) = 13), which simplifies to ((x + 2)([1/x] + 2) = 13). Expanding this gives (1 + 2x + [2/x] + 4 = 13), so (2x + [2/x] = 8), and thus (x + [1/x] = 4). I should double-check that algebra; yes, it is correct.

> Now, I will express the objective in terms of (x). The objective becomes (P = (a² + 2b²)([1/a²] + [2/b²]) = (x² + 2)([1/x²] + 2)). Expanding this, I get (1 + 2x² + [2/x²] + 4 = 5 + 2(x² + [1/x²])). Wait, let me verify the expansion; yes, that is correct.

> I need to relate (x² + [1/x²]) to the constraint. From (x + [1/x] = 4), squaring both sides gives (x² + 2 + [1/x²] = 16), so (x² + [1/x²] = 14). Substituting this into the expression for (P), I obtain (P = 5 + 2(14) = 33). Hold on, I should verify that substitution; yes, it is correct.

> But wait, what if this is a maximum rather than a minimum? I should confirm the nature of the critical point. Let me consider (P(x) = 5 + 2(x² + [1/x²])) subject to (x + [1/x] = 4). Since (x² + [1/x²] = (x + [1/x])²–2 = 14) is constant on the constraint, (P) is constant at 33 for all (x) satisfying (x + [1/x] = 4). Hmm, wait a minute, that would imply every such (x) gives the same value, which suggests I should reconsider whether I am truly at an extremum or if the function is constant along the constraint. Let me verify by checking the second derivative or by parameterizing differently; indeed, because (x^2 + 1/x^2) is fixed by (x + 1/x = 4), (P) is constant along the constraint, so 33 is both the minimum and maximum under this symmetric assumption.

> I should confirm that this value is attainable with positive reals. Solving (x + [1/x] = 4) yields the quadratic (x²–4x + 1 = 0), so (x = 2 ± √3), both positive. Thus, for example, setting ([a/b] = 2 + √3) and (b = c = 1) gives valid positive reals satisfying the constraint and achieving (P = 33). I need to ensure that this construction respects the original equation; yes, it does.

> Hold on, let's not jump to conclusions about globality. I initially thought this might be the global minimum, but I should double-check whether asymmetric configurations could yield a smaller (P). Let me reconsider the problem without assuming (b = c). The expressions are symmetric and convex in the relevant ratios, and the constraint surface is compact in a suitable quotient space, so the extremum should occur at a symmetric point; thus, 33 is indeed the global minimum. I should confirm that this reasoning aligns with standard inequalities; yes, symmetry and convexity support that the minimum is attained when two variables are equal, confirming 33 as the global minimum.

> Final answer: The minimum value of (P) is 33, attained when two of the variables are equal and the third is in the ratio 2 ± √3 to them. I should double-check that no smaller value is possible; given the derivation and the symmetry argument, 33 is the minimum.

---

The minimum value of (P = (a² + b² + c²)([1/a²] + [1/b²] + [1/c²])) subject to ((a + b + c)([1/a] + [1/b] + [1/c]) = 13) is **33**, achieved when two variables are equal and the third is in a specific ratio (e.g. (a = b), (c = (2 ± √3)a)).

---

## Step 1: Simplify the constraint

Let (S = a + b + c) and (T = [1/a] + [1/b] + [1/c]). The constraint is (ST = 13). Expanding (ST):

ST = 3 + ([a/b] + [b/a]) + ([a/c] + [c/a]) + ([b/c] + [c/b]) = 13

Thus, the sum of pairwise ratios is:

[a/b] + [b/a] + [a/c] + [c/a] + [b/c] + [c/b] = 10

---

## Step 2: Express the objective function

Expand (P):

P = (a² + b² + c²)([1/a²] + [1/b²] + [1/c²]) = 3 + ([a²/b²] + [b²/a²]) + ([a²/c²] + [c²/a²]) + ([b²/c²] + [c²/b²])

Let (x = [a/b] + [b/a]), (y = [a/c] + [c/a]), (z = [b/c] + [c/b]). Then:

- **Constraint**: (x + y + z = 10)
- **Objective**: (P = 3 + (x²–2) + (y²–2) + (z²–2) = x² + y² + z²–3)

---

## Step 3: Minimize (P) using symmetry

By symmetry, the minimum occurs when two variables are equal. Let (a = b), so (x = 2) and (y = z). Then:

2 + 2y = 10 ⇒ y = 4

Thus, (x = 2), (y = z = 4), and:

P = 2² + 4² + 4² − 3 = 4 + 16 + 16 − 3 = 33

---

## Step 4: Verify the solution

With (a = b), the constraint becomes:

(2a + c)([2/a] + [1/c]) = 13

Let (t = [c/a]). Then:

(2 + t)(2 + [1/t]) = 13 ⇒ 4 + [2/t] + 2t + 1 = 13 ⇒ 2t + [2/t] = 8 ⇒ t + [1/t] = 4

Solving (t² − 4t + 1 = 0) gives (t = 2 ± √3), so (c = (2 ± √3)a). For these values, (P = 33) is achieved.

---

## Conclusion

The minimum value of (P) is **33**, achieved when two variables are equal and the third is in the ratio (2 ± √3) to them.

---

## References

### Stable between-subject statistical inference from unstable within-subject functional connectivity estimates [^791af2c6]. Human Brain Mapping (2019). Medium credibility.

2.2 Example case for a single pair of variables

Before coming to a complete description we consider a toy example to make the point above more concrete. We wish to assess if there is a linear relationship between two variables, a and b. The first one, a, with values a n, is Gaussian distributed (mean 0, standard deviation [SD] 1); the second one, b, is a corrupted version of a by the introduction of random noise:

where ε n are independent, Gaussian distributed random variables (mean 0, SD = 1), and κ ≥ 0. We generate replicates of b based on independent realizations of noise ε n and κ, where κ is randomly sampled from a uniform distribution between 0 and c. We choose c to define the expected strength of the relationship between a and b. We then run permutation testing on each data set. We evaluate the power of the permutation combining method to detect a relationship between a and b for different values of c > 0. Even when κ is randomly small on some replicates, it may be large on others (allowing to detect the underlying relationship in these cases).

For the purpose of illustration, we generated 1,000 data sets using N = 100, each with a different value of κ sampled from a uniform distribution and performed permutation testing for each of them. We repeat this for three different values of c: 0.0, 0.1, and 0.2. Figure 1 shows histograms of correlation coefficients between a and b across data sets (top), and histograms of p values (bottom). If the empirical distribution of p values is basically flat, as is the case when c = 0.0, then there is no evidence of a relationship between a and b. However, when c = 0.1 or c = 0.2, then the distribution of p values gets increasingly skewed toward zero despite the generally low correlations. Therefore, if a and b were experimental replications of some pair of unobserved processes, we could intuitively say that there are signs of correlation between these processes in the c = 0.1 and c = 0.2 cases. However, neither p mean or p gmean (data not shown in the figure) are below 0.05; they are higher than 0.2 in all cases, emphasizing again the point that p mean or p gmean are not p values and, thus, the need for a permutation procedure to learn their null distribution.

---

### Revealing the predictability of intrinsic structure in complex networks [^dbc2a588]. Nature Communications (2020). High credibility.

Methods

Compression algorithm

Our compression scheme builds upon the seminal paper of, which is a two-step lossless compression of graphs. First we encode a network into two binary sequences B 1, B 2, through traversing all nodes on the network from an initial node, and encode nodes' neighbors according to specific rules. In the second stage, both B 1, B 2 are compressed by an improved arithmetic encoder based on recursive splitting proposed by K. Skrettingto exploit the dependencies between the symbols of the sequence. After that we obtain two compressed binary sequences, and we define the compression length of the network L to be the total length of these two sequences:where, are the length ofand, respectively. More details about the algorithm is provided in Supplementary Note 2.

Upper and Lower Bounds of p 1 and P C

Here we demonstrate how to identify the upper and lower bounds of link prediction precision p 1 and P C. The basic idea is to transform these problems into optimization problems of certain boundary conditions. Firstly, we calculate the network's BPAA performance entropythrough its shortest compression length L * given by Eq. (3). Its un-normalized value is, and by the definition of entropy it can be written asThere are two constraints for p i s. The first is that they sum up to unity, i.e.and:as they are arranged in decreasing order.

With these boundary conditions, p 1 is maximized when the other probabilities are equal. Then it can be found that the upper bound of p 1 is given by Eq. (10).

And the minimum value of p 1 is when there are as many p i s as close to p 1 as possible, leading to a lower bound given by Eq. (11).

For predictability of the top C intervals, the upper bound corresponds to the case when the last N ∕2 − C probabilities are the same, yielding

For the lower bound of P C, the value of p C can be introduced to construct the minimization problem. It has the boundary condition of 0 ≤ p C ≤ P C ∕ C. The approximate lower bound is the solution of the following:The more detailed mathematics is provided in Supplementary Note 10.

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^66f1bd1f]. Nature Communications (2023). High credibility.

CurveCurator yields well-calibrated p -values using a recalibrated F-statistic

The first step to assess the statistical significance of dose–response curves is to find the best possible fit given the measured dose–response values. As the optimization surface for sigmoidal curves is non-convex (i.e. a surface with many local minima), naïve curve fitting algorithms often get stuck in local minima, leading to suboptimal fits and, thereby, overly conservative p -values (Supplementary Fig S1). CurveCurator uses a heuristic that reaches the global minimum in almost all cases in a short period of time (Supplementary Fig. S2). To obtain an empirical null distribution, we simulated 5 million dose–response curves under the null hypothesis, i.e. curves where the response is independent of the dose (i.e. no dose-dependent regulation) for a range of n data points per curve (Fig. 2a). As expected, direct application of the classical F-statistic for linear models to these null dose–response curves yielded poorly calibrated p -values (Supplementary Fig. S3). CurveCurator solved this issue by optimizing both the F-value formula and the parameters of the F-distribution as a function of n to approximate these simulated null distributions accurately (Fig. 2 b, c). The validity of this approach was confirmed by noting that p -values in real experiments in which the vast majority of curves were expected to be unresponsive formed a uniform distribution of truly non-regulated curves plus a small distribution of truly regulated curves enriched at low p -values (Fig. 2d).

---

### What does COVID-19 testing results really say? The real statistics concealed behind the accessible data [^1ae5c113]. Journal of Medical Virology (2021). Medium credibility.

Accurate and comprehensive testing is crucial for practitioners to portray the pandemic. Without testing there is no data; yet, the exact number of infected people cannot be determined due to the lack of comprehensive testing. The number of seropositive for SARS-CoV-2 infection is obviously relative to the extent of testing. However, the true number of infections might be still far higher than the reported values. To compare the countries based on the number of seropositive for SARS-CoV-2 infection is misleading, as there may not be enough tests being carried out to properly monitor the outbreak. In this paper, we closely look through the COVID-19 testing results. Herein, we try to draw conclusions based on the reported data: first, the presence of a possible relationship between COVID-19 transition and patients' age will be assessed. Then, the COVID-19 case fatality rate (CFR) is compared with the age-demographic data for different countries. Based on the results, a method for estimating a lower bound (minimum) for the number of actual positive cases will be developed and validated. Results of this study have shown that CFR is a metric reflecting the spread of the virus, but is a factor of the extent of testing and does not necessarily show the real size of the outbreak. Moreover, no large difference in susceptibility by age has been found. The results suggest the similarity between the age distribution of COVID-19 and the population age-demographic is improving over the course of the pandemic. In addition, countries with lower CFRs have a more similar COVID-19 age distribution, which is a result of more comprehensive testing. Finally, a method for estimation of the real number of infected people based on the age distributions, reported CFRs, and the extent of testing will be developed and validated.

---

### Universal patterns in egocentric communication networks [^f59d619a]. Nature Communications (2023). High credibility.

Fig. 1
Tie strengths are heterogeneous and driven by cumulative advantage.

a Real-time contact sequence between ego and its k alters (left) and timeline of communication activity a (right), for selected ego in the CNS call dataset, (data description in SI Section S1). Times are relative to the observation length, so close-by events appear as single lines (left) or sudden increases in a (right). The sequence is divided into two consecutive intervals with the same number of events (I 1 and I 2). With time, some alters communicate more than others. b Aggregated ego network (left) and alter activity distribution p a (right) for (a). The distribution has minimum activity a 0, mean t, and standard deviation σ. c Complementary cumulative distribution function (CCDF)of number of alters with at least activity a, for egos in each quartile range of the dispersion distribution p d and k ≥ 10, in the Mobile (call) dataset,– (all systems in SI Fig. S3). For larger dispersions, egos communicate with alters heterogeneously. d Dispersion distribution p d for data in (c), showing more heterogeneous egos (all channels in SI Fig. S2). e Relative probability π a − 〈1/ k 〉 that alter with activity a is contacted, averaged over time and egos in each quartile range of the dispersion distribution p d in (d) (all systems in SI Fig. S6). The baseline π a = 〈1/ k 〉 means alters are contacted at random (each a value corresponds to at least 30 egos and is normalized by the maximum activity a m in the ego subset). For heterogeneous egos, the increasing tendency indicates cumulative advantage: alters with high prior activity receive more events. f CCDFof number of egos having at least dispersion d, for 8.6M egos in 16 communication channels (SI Table S1 and SI Fig. S2; shown only for egos with more than 10 events). g Relative connection kernel π a − 〈1/ k 〉 for all datasets (each a value corresponds to at least 50 egos with k ≥ 2; see SI Figs. S4 – S6). Increasing trends indicate cumulative advantage in all channels.

---

### Death by p-value: the overreliance on p-values in critical care research [^bfbf92b9]. Critical Care (2025). Medium credibility.

The p-value has changed from a versatile tool for scientific reasoning to a strict judge of medical information, with the usual 0.05 cutoff frequently deciding a study's significance and subsequent clinical use. Through an examination of five critical care interventions that demonstrated meaningful treatment effects yet narrowly missed conventional statistical significance, this paper illustrates how rigid adherence to p-value thresholds may obscure therapeutically beneficial findings. By providing a clear, step-by-step illustration of a basic Bayesian calculation, we demonstrate that clinical importance can remain undetected when relying solely on p-values. These observations challenge current statistical paradigms and advocate for hybrid approaches-including both frequentist and Bayesian methodologies-to provide a more comprehensive understanding of clinical data, ultimately leading to better-informed medical decisions.

---

### Beyond the P: II: precluding a puddle of P values [^1cd5f00e]. Journal of Cataract and Refractive Surgery (2004). Low credibility.

This is the second in a 4-part series discussing the proper use, interpretation, and limitations of P values.

---

### Fundamental limits to learning closed-form mathematical models from data [^5954e9c9]. Nature Communications (2023). High credibility.

Fig. 1
Probabilistic model selection makes quasi-optimal predictions about unobserved data.

We select two models m *, whose expressions are shown at the top of each column. a, b From each model, we generate synthetic datasets D with N points (shown, N = 100) and different levels of noise s ϵ (shown, s ϵ = 1). Here and throughout the article, the values of the independent variables x 1 and x 2 are generated uniformly at random in [− 2, 2]. Vertical lines show the observation error ϵ i for each point in D. For a model not drawn from the prior and data generated differently, see Supplementary Fig. S1. c, d For each dataset D (with dataset sizes N ∈ {25, 50, 100, 200, 400}), we sample models from p (m ∣ D) using the Bayesian machine scientist, select the MDL model (maximum p (m ∣ D)) among those sampled, and use this model to make predictions on a test dataset, generated exactly as D. We show the prediction root mean squared error (RMSE) of the MDL model onas a function of N and s ϵ. For comparison, we also show the predictions from an artificial neural network (ANN, dotted lines; Methods). Since s ϵ is the irreducible error, predictions on the diagonal RMSE = s ϵ are optimal. e, f We plot the prediction RMSE scaled by the irreducible error s ϵ; optimal predictions satisfy RMSE/ s ϵ = 1 (dashed line).

---

### A practical guide for understanding confidence intervals and P values [^c0a7d4a0]. Otolaryngology — Head and Neck Surgery (2009). Low credibility.

The 95 percent confidence interval about the mean demarcates the range of values in which the mean would fall if many samples from the universal parent population were taken. In other words, if the same observation, experiment, or trial were done over and over with a different sample of subjects, but with the same characteristics as the original sample, 95 percent of the means from those repeated measures would fall within this range. This gives a measure of how confident we are in the original mean. It tells us not only whether the results are statistically significant because the CI falls totally on one side or the other of the no difference marker (0 if continuous variables; 1 if proportions), but also the actual values so that we might determine if the data seem clinically important. In contrast, the P value tells us only whether the results are statistically significant, without translating that information into values relative to the variable that was measured. Consequently, the CI is a better choice to describe the results of observations, experiments, or trials.

---

### Identifying unreliable predictions in clinical risk models [^f788a45e]. NPJ Digital Medicine (2020). Medium credibility.

Results

Prediction unreliability is a function of class imbalance

The unreliability metric, is a function of the risk model, the prevalence of the outcome of interest in the overall population, P (y = 1), and the relative likelihood ratio of the positive and negative classes arising from the generative models,(see Methods and Supplementary Methods). To understand how each of these quantities affects the unreliability estimate, we computedfor a range of input parameters and then calculated the average value ofas a function of the risk model prediction. We consider two limiting cases: (i) when there is no class imbalance in the training data (, Fig. 1a), and (ii) when the positive class (y = 1) is in the minority (, Fig. 1b). When there is no class imbalance on average, values of the clinical risk score that are close to 0 or 1 are slightly more unreliable than values close to 0.5, which is equal to the prevalence of the outcome in the population (Fig. 1a). By contrast, on average, when there is significant class imbalance, unreliable predictions are more likely to occur in patients who are predicted to be at high risk (Fig. 1b). In other words, when there is a relatively small number of patients who belong to the positive class, and consequently few positive examples for the clinical model to learn from, positive predictions are, on average, more likely to have high values of.

Fig. 1
Unreliability scores are a function of class imbalance.

a Average unreliability as a function of the classifier prediction, when there is no class imbalance in the data. b Average unreliability for different predictions, in the setting of large class imbalance. Calculated expected values assume a uniform distribution for.

---

### Introduction to clinical decision making [^9046941c]. Seminars in Nuclear Medicine (2010). Low credibility.

In the last few years there has been a remarkable increase in the amount of clinical data in the average hospital chart, and more and more problem-solving algorithms have been developed. We need better "thinking tools" to help us handle the flow of information. The term "clinical decision making" is used to describe a systematic way to handle data and algorithms to decide on a best course of action. This introductory article discusses some of the problems in establishing a decision criterion, both for a population and for an individual patient. Comparing the probabilities and utilities of various diagnostic outcomes (true positive, false positive, etc.) leads to a diagnostic strategy. The article also discusses conditional probability. Bayes' theorem, and likelihood ratios.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^dee6cacc]. CDC (2011). Medium credibility.

Community viral load sample size calculations — sample size to detect differences in geometric mean (GM) viral load depends on power and standard deviation (S). The table specifies "α = 0.05 and W = 0.8", with S columns 1 to 1.5; for k = 3 the minimum sample sizes by S are 54, 66, 78, 92, 106, and 122, and "1.2 is the standard deviation of national VL data", so at S = 1.2 the needed sample size is 78.

---

### Extraction of accurate cytoskeletal actin velocity distributions from noisy measurements [^703919c4]. Nature Communications (2022). High credibility.

Displacement distributions were then calculated for twenty timescales (2, 4. 38, and 40 s) from the simulated particles. Within each estimated pdf, bins with zero counts were set to 1/(N ⋅ w), where N is the number of particles in the distribution and w = 30 nm is the width of the bin; this prevents infinite likelihoods but overestimates rare events, as it represents an approximate upper bound on the expected value of that bin. To calculate likelihoods for our experimentally measured displacements, probability densities were linearly interpolated from the two closest points. To keep computational times reasonable, we calculated likelihoods from a randomly sampled subset of our data: for each of the twenty timescales, if there were more than 1000 displacements measured, a random sample of 1000 were chosen, resulting in ~20,000 observations included in the fit sample. Negative log likelihoods were minimized using MATLAB's genetic algorithm global optimization function. Due to the rough nature of the fit landscape (in part due to the stochasticity of the simulations), ten optimizations were completed for each dataset and the best fit (highest likelihood) was chosen for each. For each population, best fit parameters from different experimental replicates were averaged and the standard error of the mean on each parameter is presented (Fig. 4 g).

The resulting velocity distributions from each fit parameter were generated using similar simulations, but without the addition of Gaussian noise to each localization. The discrete probability of zero displacement was calculated analytically as P (s = 0) = e − μ τ, where μ is the average jump rate and τ is the observation timescale. For values > 0, probability distributions were calculated numerically from simulations with 100,000 particles.

For the drug perturbations presented in Supplementary Fig. 10, 95% confidence intervals were estimated using the profile likelihood method as described in ref. In short, each parameter was systematically varied in turn above and below its best fit value. At each sampled value, the other two parameters were again optimized using MATLAB's genetic algorithm, and the log-likelihood of this optimized set of parameters was recorded. These log-likelihood values were then fit to a parabola as a function of the varied parameter with its minimum fixed at the log-likelihood of the best fit parameter. The 95% confidence intervals were then estimated by the domain over which the parabolic fit to the log 10 -likelihoods rises by 0.8338 from the minimum.

---

### Inconsistency between overall and subgroup analyses [^eed221ea]. General Psychiatry (2022). Medium credibility.

In table 3, we use numerical examples to show that if p 1 ≠ p 2 and d 1 d 2 ≠0, all combinations of 1–9 and 13 in table 2 may occur.

Table 3
Numerical examples of Δ 0, Δ 1 and Δ when p 1 ≠ p 2 and d 1 d 2 ≠0

The following theorem gives a more general sufficient condition of consistency than the first three cases discussed above.

Theorem: given Δ 0 and Δ 1, for any p 1 and p 2 between 0 and 1, there always exists a p between 0 and 1 such that Δ = Δ 0 p +Δ 1 (1− p) if and only if p 1 = p 2 or d 1 d 2 ≤ 0.

The proof of this theorem is available on request. Note that d 1 d 2 = 0 implies d 1 d 2 ≤ 0.

Unfortunately, if we are only given the information that p 1 ≠ p 2 and d 1 d 2 > 0, we cannot determine whether the inconsistency will happen. For example, combinations 1 and 3 satisfy the condition of p 1 ≠ p 2 and d 1 d 2 > 0. In combination 1, the overall difference is consistent with the subgroup differences, while it is not in combination 3.

---

### Power of data in quantum machine learning [^03099894]. Nature Communications (2021). High credibility.

Given some set of data, if s K (N) is found to be small relative to N after training for a classical ML model, this quantum model f (x) can be predicted accurately even if f (x) is hard to compute classically for any given x. In order to formally evaluate the potential for quantum prediction advantage generally, one must take s K (N) to be the minimal over efficient classical models. However, we will be more focused on minimally attainable values over a reasonable set of classical methods with tuned hyperparameters. This prescribes an effective method for evaluating potential quantum advantage in practice, and already rules out a considerable number of examples from the literature.

From the bound, we can see that the potential advantage for one ML algorithm defined by K 1 to predict better than another ML algorithm defined by K 2 depends on the largest possible separation betweenandfor a dataset. The separation can be characterized by defining an asymmetric geometric difference that depends on the dataset, but is independent of the function values or labels. Hence evaluating this quantity is a good first step in understanding if there is a potential for quantum advantage, as shown in Fig. 1. This quantity is defined bywhere ∣∣. ∣∣ ∞ is the spectral norm of the resulting matrix and we assume Tr(K 1) = Tr(K 2) = N. One can show that, which implies the prediction error bound. A detailed derivation is given in Supplementary Section C and an illustration of g 12 can be found in Fig. 2. The geometric difference g (K 1 ∣∣ K 2) can be computed on a classical computer by performing a singular value decomposition of the N × N matrices K 1 and K 2. Standard numerical analysis packagesprovide highly efficient computation of a singular value decomposition in time at most order N 3. Intuitively, if K 1 (x i, x j) is small/large when K 2 (x i, x j) is small/large, then the geometric difference g 12 is a small value ~1, where g 12 grows as the kernels deviate.

---

### Automated NMR resonance assignments and structure determination using a minimal set of 4D spectra [^a7a647f2]. Nature Communications (2018). Medium credibility.

Fig. 1
Comparison of uncorrelated and correlated chemical shifts probabilities and their power in amino acid-type prediction. a The 1D probability distributions for the C α and H α atoms of Trp (left) and their joint probability distribution (right). b The 2D probability distribution of correlated C α –H α chemical shifts of Trp (left) and the corresponding smoothed 2D probability density map (right) after applying a Gaussian kernel function. Graphs were generated from the VASCO database with a bin size of 0.04 p.p.m. for protons and 0.2 p.p.m. for carbons. The color gradient scaling differs for (a) uncorrelated and (b) correlated distributions. c Pie charts displaying the ranking of amino acid-type predictions in the current dataset (four proteins) using uncorrelated (left) or correlated distributions (right) of 13 C– 1 H chemical shifts. d Analysis of common (correct or wrong) predictions made by using uncorrelated (red) or correlated distributions (blue) of 13 C– 1 H chemical shifts. Horizontal axis plots the mean value and standard error of correct predictions (P 1) to the second probable (P 2), and vertical axis plots the mean value and standard error of wrong predictions (Pn) to the first probable (P 1). Pairwise t -test shows that the improvement in predictions made by using correlated instead of uncorrelated 13 C– 1 H chemical shifts (arrows) is statistically significant for the correct ones (P -value = 0.00016) but not for the wrong ones (P -value = 0.2175). The discrepancy is attributed to the relatively small number of wrong predictions available for the test (405 vs 88)

---

### How do you design randomised trials for smaller populations? A framework [^9ac3f0db]. BMC Medicine (2016). Low credibility.

Exploring less common approaches to reducing sample size

We now consider some less standard approaches to bringing the sample size requirements closer to the numbers it is feasible to recruit in a reasonable time frame.

Step 3: Relaxing α by a small amount, beyond traditional values

The much-criticised 5% significance level is used widely in much applied scientific research, but is an arbitrary figure. It is extremely rare for clinical trials to use any other level. It may be argued that this convention has been adopted as a compromise between erroneously concluding a new treatment is more efficacious and undertaking a trial of an achievable size and length. Settings where traditionally sized trials are not possible may be just the area where researchers start to break this convention, for good reason.

In considering the type I error, it is critical to consider the question: 'What are the consequences of erroneously deciding to use a new treatment routinely if it is truly not better?'

Taking the societal perspective as before, we might consider the probability of making a type I error, thus erroneously burdening patients with treatments that do not improve outcomes, or even worsen them, while potentially imposing unnecessary toxicity.

First, for conditions where there are only enough patients available to run one modestly sized randomised trial in a reasonable time frame, research progress will be relatively slow, and making a type I error may be less of a concern than a type II error. In contrast, making several type I errors in a common disease could lead in practice to patients taking several ineffective treatments; for a disease area where only one trial can run at any given time, the overall burden on patients is potentially taking one ineffective treatment that does not work.

Thus, if we take the societal perspective with the trials in Table 1 then, if each trial was analysed with α = 0.05 and we see (hypothetically) 40% positive results, then the expected number of false positive trials is given in the final column. We also assumed 10% and 70% positive results, with qualitatively similar conclusions.

---

### Understanding quantum machine learning also requires rethinking generalization [^0feeb9f5]. Nature Communications (2024). High credibility.

Theorem 3

(Conditioning as a convex program 1). Let ρ 1, …, ρ N be unknown, linearly-independent quantum states on n qubits, with. For any i ∈ [N], letbe approximations of ρ i, each of which can be efficiently prepared using a PQC. Assume the computation ofin polynomial time for any choice of i, j and k. Call σ = (σ 1, …, σ N). The real numbersdefine the auxiliary statesasand the matrix of inner productswith entriesThen. Further, one can then decide in polynomial time whether, given ρ 1, …, ρ N, σ, and, there exists a specification ofsuch thatis well-conditioned in the sense that. And, if there exists such a specification, a convex semi-definite problem (SDP) outputs an instance of α ← SDP(ρ, σ, κ) for whichis well-conditioned. If it exists, one can also find in polynomial time the α with the smallestornorm.

Proof

The inequalityfollows from Gershgorin's circle theorem, given that all entries ofare bounded between [0, 1]. In particular, the largest singular value of the matrixreaches the value N when all entries are 1.

The expressionis a linear constraint on α and, for i, j ∈ [N], whilein matrix ordering is a positive semi-definite constraint.is equivalent with, whilemeans that the smallest singular value ofis lower bounded by κ, being equivalent withfor an invertible. The test whether such ais well-conditioned hence takes the form of a semi-definite feasibility problem. One can additionally minimize the objective functionsandboth again as linear or convex quadratic and hence semi-definite problems. Overall, the problem can be solved as a semi-definite problem, that can be solved in a run-time with low-order polynomial effort with interior point methods. Duality theory readily provides a rigorous certificate for the solution.

---

### Fast cavity-enhanced atom detection with low noise and high fidelity [^44c6271d]. Nature Communications (2011). Medium credibility.

For most applications, however, it is not enough to detect the bright state (logical 1) efficiently; the detector must also be able to identify the dark state (logical 0) correctly. A more useful figure of merit is thus the fidelity, which is the probability of a correct measurement result. Let us take the detection of ⩾ K photons as indicating logical 1, and < K as logical 0. Then, for Poissonian distributions, the single-photon fidelity is F K = 1 = (1− P)e −BT + P [1−e − (S+B)T], where B is the background photon counting rate and P is the probability that the state being measured is logical 1. The first (second) term is the probability of having logical 0 (1) and identifying it correctly. The four logical possibilities are shown schematically in (Fig. 4a). The red curve in (Fig. 4b) shows the expected value of F K = 1 in our experiment over a data set for which 〈 N eff 〉 = 1.24 and P = 1/2. The fidelity rises quickly as the detection of logical 1 becomes increasingly successful but eventually falls, due to false positives from the background. Superimposed on this curve are our measured values of the fidelity versus detection time, which agree well with our expectations. In general, the maximum fidelity F 1max increases with S/B, reaching its peak at a time T 1max proportional to 1/ S for fixed S/B.

Table 1 compares our values of F 1max and T 1max with those for other atom detection experiments. The highest fidelity by far is that of ref. whereas our high signal rates result in the shortest detection time. It is worth noting that the measurements in refs, are non-destructive, whereas the rest are carried out on resonance. Lossless fluorescence detection of single trapped atoms in free space has been observed with 95% (98.6%) accuracy in 0.3 ms (1.5 ms). Additionally, recent refinements to our cavity manufacturing process have increased the finesse by two orders of magnitude. This suggests the possibility of single-atom strong coupling with g > (κ,γ) and C 1 in the hundreds, allowing non-destructive measurements in our system as well.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^b1513dd5]. CDC (2011). Medium credibility.

Table 1b — Minimum sample size for detecting geometric mean (GM) ratio k with α = 0.05 and W = 0.9 — presents required sample sizes by standard deviation (S) strata and defines parameters; α is significance level, W is power, GM is geometric mean, and S is standard deviation, noting that 1.2 is the standard deviation of national viral load (VL) data. Each jurisdiction will need to assess the standard deviation of their local VL data and then determine the appropriate sample size needed to assess VL, and if the sample size is inadequate to meet the recommended case inclusion criterion, an alternate method may need to be used, such as combining multiple years of data. For k = 3, sample sizes across S = 1, 1.1, 1.2, 1.3, 1.4, 1.5 are 75, 91, 108, 127, 147, 169, and Table 1b uses power = 90%; jurisdictions may also explore differences in means of viral loads, including categorical differences in the proportion with undetectable or very low VL.

---

### Triazolam [^59f91090]. FDA (2025). Medium credibility.

WARNING: RISKS FROM CONCOMITANT USE WITH OPIOIDS; ABUSE, MISUSE, AND ADDICTION; and DEPENDENCE AND WITHDRAWAL REACTIONS

Concomitant use of benzodiazepines and opioids may result in profound sedation, respiratory depression, coma, and death. Reserve concomitant prescribing of these drugs in patients for whom alternative treatment options are inadequate. Limit dosages and durations to the minimum required. Follow patients for signs and symptoms of respiratory depression and sedation [see Warnings and Precautions (5.1), Drug Interactions (7.1)].
The use of benzodiazepines, including triazolam, exposes users to risks of abuse, misuse, and addiction, which can lead to overdose or death. Abuse and misuse of benzodiazepines commonly involve concomitant use of other medications, alcohol, and/or illicit substances, which is associated with an increased frequency of serious adverse outcomes. Before prescribing triazolam and throughout treatment, assess each patient's risk for abuse, misuse, and addiction [see Warnings and Precautions (5.2)].
The continued use of benzodiazepines, including triazolam, may lead to clinically significant physical dependence. The risks of dependence and withdrawal increase with longer treatment duration and higher daily dose. Abrupt discontinuation or rapid dosage reduction of triazolam after continued use may precipitate acute withdrawal reactions, which can be life-threatening. To reduce the risk of withdrawal reactions, use a gradual taper to discontinue triazolam or reduce the dosage [see Dosage and Administration (2.3), Warnings and Precautions (5.3)].

WARNING: RISKS FROM CONCOMITANT USE WITH OPIOIDS; ABUSE, MISUSE, AND ADDICTION; and DEPENDENCE AND WITHDRAWAL REACTIONS

See full prescribing information for complete boxed warning.

Concomitant use of benzodiazepines and opioids may result in profound sedation, respiratory depression, coma, and death. Reserve concomitant prescribing of these drugs in patients for whom alternative treatment options are inadequate. Limit dosages and durations to the minimum required. Follow patients for signs and symptoms of respiratory depression and sedation (5.1, 7.1).
The use of benzodiazepines, including triazolam, exposes users to risks of abuse, misuse, and addiction, which can lead to overdose or death. Before prescribing triazolam and throughout treatment, assess each patient's risk for abuse, misuse, and addiction (5.2).
Abrupt discontinuation or rapid dosage reduction of triazolam after continued use may precipitate acute withdrawal reactions, which can be life-threatening. To reduce the risk of withdrawal reactions, use a gradual taper to discontinue triazolam or reduce the dosage (2.3, 5.3).

---

### Substantial underestimation of SARS-CoV-2 infection in the United States [^f686ef8a]. Nature Communications (2020). High credibility.

Correction for incomplete testing

Sampled variates from the procedure described above were used as inputs to the bias correction. To correct for incomplete testing, we defined the following formulas to estimate, for each state, the number of SARS-CoV-2 infections among untested individuals.is the estimated number of untested individuals who would have moderate to severe symptoms and test positive if tested.is the estimated number of untested individuals who would have mild or no symptoms and test positive if tested.

The quantitiesandare, marginally, binomial random variables, with parametersand probability parameter. In our analysis, we set them equal to their expectation because the binomial variance is negligible in comparison to other sources of uncertainty, and the population size is large enough for the random variable to be highly concentrated around its expectation value. Incorporation of this variation would simply involve additional resampling at this stage.

Correction for imperfect test accuracy

Finally, to estimate the number of SARS-CoV-2 infections (N *) correcting for imperfect test accuracy (i.e. sensitivity (S e) or specificity (S p) < 1), we used the following formula:where N + is the sum of the number of confirmed cases and the estimated number of infections among untested individuals correcting for incomplete testing.

Supplementary Fig. 6 presents a visual depiction of the sampling procedure described above.

Simulation results

To fully propagate uncertainty into the final state-specific estimates of case counts N *, we repeated the process described above 10 4 times, obtaining a distribution of expected cases in each state on each day as the primary estimate. To characterize uncertainty in our model, we report the 2.5th and 97.5th quantiles of the sampled distributions.

Proportion of infections attributable to under-testing

We calculated the proportion of the difference between observed case counts and the estimated number of infections attributable to imperfect test accuracy as the difference between N * with S e and S p set to our prior values and with S e = 1 and S p = 1 divided by the difference between N * and the observed case counts. We calculated the proportion attributable to incomplete testing as the 1 − the proportion attributable to imperfect test accuracy. To obtain national estimates of these percentages, we obtained the median and 2.5th and 97.5th quantiles of the state-specific distributions and weighted by state population.

---

### Statistical ensembles without typicality [^c8e48c72]. Nature Communications (2018). Medium credibility.

Maximum-entropy ensembles are key primitives in statistical mechanics. Several approaches have been developed in order to justify the use of these ensembles in statistical descriptions. However, there is still no full consensus on the precise reasoning justifying the use of such ensembles. In this work, we provide an approach to derive maximum-entropy ensembles, taking a strictly operational perspective. We investigate the set of possible transitions that a system can undergo together with an environment, when one only has partial information about the system and its environment. The set of these transitions encodes thermodynamic laws and limitations on thermodynamic tasks as particular cases. Our main result is that the possible transitions are exactly those that are possible if both system and environment are assigned the maximum-entropy state compatible with the partial information. This justifies the overwhelming success of such ensembles and provides a derivation independent of typicality or information-theoretic measures.

---

### Preparedness and response for chikungunya virus introduction in the americas [^254022b8]. CDC (2011). Medium credibility.

Appendix C — IgM antibody capture enzyme-linked immunosorbent assay (ELISA) interpretation specifies that all patient P/N values greater than or equal to 2.0 should be reported as presumptive IgM-positive, as long as they meet the requirements listed above; if an early acute CSF or serum is negative, a convalescent serum specimen must be requested and tested before that patient is reported as negative, since without testing of a convalescent specimen a negative result may reflect testing before antibody has risen to detectable levels, with IgM detectable eight days post-onset of symptoms and persisting for at least 45 days, often for as long as 90 days; the positive P/N cut-off value of 2.0 is empirical, P/N values between 2.0 and 3.0 should be considered suspect false-positives and further tests should be performed, and the P/N value for a specimen at the screening dilution of 1:400 is not quantitative.

---

### Practice guidelines for the prevention, detection, and management of respiratory depression associated with neuraxial opioid administration: an updated report by the American society of anesthesiologists task force on neuraxial opioids and the American Society of Regional Anesthesia and Pain Medicine [^c7d4883b]. Anesthesiology (2016). Medium credibility.

American Society of Anesthesiologists neuraxial opioid guideline — evidence grading and opinion methodology: Category B observational evidence permits directional inference, and for studies that report statistical findings, "the threshold for significance is P < 0.01"; Category B levels comprise "observational comparisons… with comparative statistics" (Level 1), "noncomparative observational studies with associative statistics" (Level 2), "noncomparative observational studies with descriptive statistics" (Level 3), and "case reports" (Level 4). Opinion-based inputs include formal surveys and informal sources; survey types are designated "Category A: Expert Opinion" and "Category B: Membership Opinion", with additional "Category C: Informal Opinion". Survey responses "are recorded using a five-point scale and summarized based on the median values", with median categories defined as follows: "Strongly Agree: Median score of 5 (at least 50% of the responses are 5)… Agree: Median score of 4 (at least 50% of the responses are 4 or 4 and 5)… Equivocal: Median score of 3 (at least 50% of the responses are 3, or no other response category or combination of similar categories contain at least 50% of the responses)… Disagree: Median score of 2 (at least 50% of responses are 2 or 1 and 2)… Strongly Disagree: Median score of 1 (at least 50% of responses are 1)". When equal numbers of distinct responses occur, "the median value is determined by calculating the arithmetic mean of the two middle values".

---

### Extensive benchmarking of a method that estimates external model performance from limited statistical characteristics [^691db016]. NPJ Digital Medicine (2025). Medium credibility.

One-sided positivity tests

The following tests are performed before running the weighting algorithm, to assess one-sided positivity:
Verify that Φ int and μ ext contain the same features and feature-outcome transformations and do not contain missing values.
Unary variables: In case one of the columns of Φ int has a constant value across all units, make sure that the difference between this value and the corresponding entry in μ ext is less than a threshold (default 0.01).
Binary variables: First, if the external statistics of binary features are exactly the same as one of the internal binary values, we assume that the external cohort is composed of a sub-population with this value and use only the corresponding sub-population in the internal cohort for weighting. Second, we test if univariate weighting on this variable will not result in assigning most of the weights to a few internal units. Specifically, let n 0 and n 1 be the number of internal units with feature values 0 and 1. Let p 0 and p 1 be the proportion of such units in the external dataset. Then, we assume that whenmost weights will be assigned to a few samples and the variance of any weighted estimator will be too large.
Continuous variables: Make sure that the external statistics is within the range of the internal cohort values. We allow a small slack (default 0.01).

Weighting algorithm pre-processing and parameter settings

Before running this algorithm, the features in the internal dataset are normalized by subtracting the mean and dividing by the standard deviations. The means and standard deviation are maintained and used later to apply the same transformation to the external statistics. This step maintains the relationships between the internal cohort and the external statistics while improving numerical stability and standardizing decision threshold. The convergence of the algorithm is examined by testing if the L 2 norm of the distance between the weighted internal and the external statistics is less than a threshold (default value 10 −5). The maximum number of iterations in this benchmark was set to 2000. If the weighting algorithm converges, we test that the maximum standardized mean difference between the weighted internal statistics and the external ones is less than a threshold (default 0.05).

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^d55d13ff]. Nature Communications (2023). High credibility.

F-statistics and p -values

The basic idea behind the F-value in classical linear regression problems is to quantify how much better a more complex model (M 1 with k linear parameters) fits the data compared to a simpler model (M 0 with j linear parameters and j < k) given the n observed data points and the corresponding sum-squared errors (SSE) (Eq. 6).

Although not a linear model by nature, the log-logistic function still meets the required assumptions of random sampling, independence of observations, residual normality, and equal variance of the errors. The basic rationality behind CurveCurator's recalibrated F-statistic is similar to the linear F-statistic above. It also quantifies how much better the fitted log-logistic model (M 1) is compared to the mean model (M 0), which describes that there is no relationship between the applied dose and the observed response. We found, however, that n/k was a more appropriate scaling factor for the 4-parameter log-logistic function.

The obtained recalibrated F-value (Eq. 7) can then be used to calculate a p -value that quantifies how often a curve with a similar or bigger F-value can be found by random chance. We observed that these F-values follow a parameterized F-distribution with degrees of freedom that diverged from the case of linear models. Using extensive simulations under the null hypothesis (5 million curves for n = 5… 50), we obtained a simple quasi-linear function to calculate the "effective "degrees of freedom as a function of n (Eqs. 8–10).

---

### Striatal arbitration between choice strategies guides few-shot adaptation [^5fbb64f2]. Nature Communications (2025). High credibility.

Results

Unexpected event-driven few-shot adaptation hypothesis

In a context-switching environment with binary choices and probabilistic reward (Fig. 1 a left), a subject who infers the current task context as T 1 (p 1 > p 2; marked as a red square in the reward probability plot) is likely to choose the action A 1 since it leads to the outcome state S 1 with a higher reward probability (p 1). Suppose a reward is not offered, contrary to its expectation. Although this unexpected outcome can be regarded as a noisy event due to the probabilistic nature of the reward function, it could also indicate the context switching to T 2 (p 2 > p 1; marked as a blue square in the reward probability plot). This cognitive process might urge action A 2 at the trial right after, which can lead to highly rapid adaptation. Such contextual behavior patterns contradict the prediction of the conventional RL models that choices are made based on values of decision variables accumulated over several past trials.

Fig. 1
Specialized action-selection strategy after experiencing the unexpected event.

a Alternative role of the unexpected event. Left for an example reversal learning task with two contexts and two actions. p 1 / p 2 is the probability of receiving a reward after arriving at the outcome state S 1 / S 2, respectively. Right for the reward probability plot depicting the context reversal. At the task context T 1, p 1 > p 2. On the other hand, in the task context T 2, p 1 < p 2. b, c Key terminology for behavioral measures. b The definition of action support/conflict events. c The two-step task; Left for the definition of positive/negative actions with the task diagram. Here, the current task context is T 1 (p 1 > p 2). Right for the summary table of 4 event types.

---

### Unbounded number of channel uses may be required to detect quantum capacity [^d09ea295]. Nature Communications (2015). Medium credibility.

Transmitting data reliably over noisy communication channels is one of the most important applications of information theory, and is well understood for channels modelled by classical physics. However, when quantum effects are involved, we do not know how to compute channel capacities. This is because the formula for the quantum capacity involves maximizing the coherent information over an unbounded number of channel uses. In fact, entanglement across channel uses can even increase the coherent information from zero to non-zero. Here we study the number of channel uses necessary to detect positive coherent information. In all previous known examples, two channel uses already sufficed. It might be that only a finite number of channel uses is always sufficient. We show that this is not the case: for any number of uses, there are channels for which the coherent information is zero, but which nonetheless have capacity.

---

### Natural statistics support a rational account of confidence biases [^470169fa]. Nature Communications (2023). High credibility.

Fig. 4
Latent ideal observer accounts for dissociations between decisions and confidence.

a Denoising variational autoencoder (VAE) used to extract low-dimensional latent representation of training data. An image x was passed through a DNN encoder q (distinct from the encoder f used in the supervised neural network model), which output the parameters (means and variances) of the latent posterior q (z ∣ x), a two-dimensional Gaussian distribution. This distribution was sampled from, yielding z, which was then passed through a DNN decoder h, yielding, a denoised reconstruction of the input x. The VAE was regularized based on the divergence of the latent posterior from a unit normal prior (with means equal to 0 and variances equal to 1), encouraging efficient low-dimensional encodings of the high-dimensional inputs. b Latent ideal observer model. After training the VAE, Gaussian distributions were fit to the latent representations resulting from the training images for classes s1 and s2. The distributions were used to construct an ideal observer model that computed confidence according to p (c o r r e c t ∣ z), the probability of being correct given the low-dimensional embedding z. Concentric ellipses represent distributions based on the average parameters of those extracted from 100 trained VAEs. The latent ideal observer accounted for both versions of the PE bias, including (c) the version involving manipulation of contrast and noise (Fig. 3 a; two-sided paired t -tests, accuracy: p = 0.97, confidence: p = 1.3 × 10 −60), and (d) the version involving superimposed stimuli presented at different contrast levels (Fig. 3 c; two-sided paired t-tests, accuracy: p = 0.82, confidence: p = 1.3 × 10 −62). Confidence for correct and incorrect trials is shown in Supplementary Fig. S3. e The latent ideal observer also accounted for the dissociation between type-1 and type-2 sensitivity. Results in (c) and (d) reflect probability density over 100 ideal observers (each based on distributions extracted by a separate trained VAE), with mean accuracy/confidence in each condition represented by circular markers, and maxima/minima represented by the upper/lower line; Results in (e) reflect mean d'/meta-d' over 100 ideal observers ± the standard of the mean; dotted black lines represent chance performance; ns indicates p > 0.05, ✱✱ p < 0.0001. Source data are provided as a Source Data file.

---

### Diagnosis: making the best use of medical data [^aa53c8a5]. American Family Physician (2009). Low credibility.

To take the best possible care of patients, physicians must understand the basic principles of diagnostic test interpretation. Pretest probability is an important factor in interpreting test results. Some tests are useful for ruling in disease when positive or ruling out disease when negative, but not necessarily both. Many tests are of little value for diagnosing disease, and tests should be ordered only when the results are likely to lead to improved patient-oriented outcomes.

---

### Net risk reclassification p values: valid or misleading? [^b023a91d]. Journal of the National Cancer Institute (2014). Low credibility.

Background

The Net Reclassification Index (NRI) and its P value are used to make conclusions about improvements in prediction performance gained by adding a set of biomarkers to an existing risk prediction model. Although proposed only 5 years ago, the NRI has gained enormous traction in the risk prediction literature. Concerns have recently been raised about the statistical validity of the NRI.

Methods

Using a population dataset of 10000 individuals with an event rate of 10.2%, in which four biomarkers have no predictive ability, we repeatedly simulated studies and calculated the chance that the NRI statistic provides a positive statistically significant result. Subjects for training data (n = 420) and test data (n = 420 or 840) were randomly selected from the population, and corresponding NRI statistics and P values were calculated. For comparison, the change in the area under the receiver operating characteristic curve and likelihood ratio statistics were calculated.

Results

We found that rates of false-positive conclusions based on the NRI statistic were unacceptably high, being 63.0% in the training datasets and 18.8% to 34.4% in the test datasets. False-positive conclusions were rare when using the change in the area under the curve and occurred at the expected rate of approximately 5.0% with the likelihood ratio statistic.

Conclusions

Conclusions about biomarker performance that are based primarily on a statistically significant NRI statistic should be treated with skepticism. Use of NRI P values in scientific reporting should be halted.

---

### Pointwise error estimates in localization microscopy [^58128e53]. Nature Communications (2017). Medium credibility.

Laplace approximation

An alternative way to approximate the uncertainty of the fit parameters is to Taylor expand the likelihood around the maximum-likelihood parameters θ * to second order, that is

The first-order term is 0, since θ * is a local maximum. This approximates the likelihood by a Gaussian with covariance matrix given by the inverse Hessian, that is, Σ = [∂ 2 ln L / ∂θ 2] −1. In a Bayesian setting, this expresses the (approximate) posterior uncertainty about the fit parameters. The estimated uncertainties (posterior variances) are given by the diagonal entries of the covariance matrix, for example. We compute the Hessian numerically using Matlab's built-in optimization routines, and use the log of the scale parameters b, N, σ for fitting, since they are likely to have a more Gaussian-like posterior.

Prior distributions

For MAP localizations in the main text, we used weak normal priors with standard deviation ln(30) centred on the true value for the log background intensity. For the spot width, we define σ = σ 0 (1+Δ σ), where σ 0 = 0.21 λ /NA is the width of a focused spot, and put exponential priors with mean value one on Δ σ. For asymmetric Gaussian spots with two principal widths, we define analogous excess widths Δ σ 1,2 and use independent exponential priors with mean value 1/2. Other parameters were left with flat priors (for details, see Supplementary Note 3).

---

### Preparedness and response for chikungunya virus introduction in the americas [^b0e1ba93]. CDC (2011). Medium credibility.

Appendix C — IgG enzyme-linked immunosorbent assay (ELISA) interpretation for chikungunya states that for a single late specimen obtained later than 4–5 days post-onset of symptoms with a positive IgG-ELISA and negative MAC-ELISA, the distinction between current and past infections cannot be made. A negative IgG-ELISA plus a negative MAC-ELISA indicates the lack of any recent or past infections if the sample was collected 7 days post illness onset, whereas a more acute sample cannot rule out infection. P/N interpretation is empirical with a positive P/N cut-off value of 2.0; P/N values between 2.0 and 3.0 should be considered suspect false-positives and further tests should be performed to determine the status. The P/N value at the screening dilution of 1:400 is not an indication of absolute antibody concentration and is not quantitative.

---

### Fundamental limits to learning closed-form mathematical models from data [^a345300f]. Nature Communications (2023). High credibility.

Given a finite and noisy dataset generated with a closed-form mathematical model, when is it possible to learn the true generating model from the data alone? This is the question we investigate here. We show that this model-learning problem displays a transition from a low-noise phase in which the true model can be learned, to a phase in which the observation noise is too high for the true model to be learned by any method. Both in the low-noise phase and in the high-noise phase, probabilistic model selection leads to optimal generalization to unseen data. This is in contrast to standard machine learning approaches, including artificial neural networks, which in this particular problem are limited, in the low-noise phase, by their ability to interpolate. In the transition region between the learnable and unlearnable phases, generalization is hard for all approaches including probabilistic model selection.

---

### Beractant (Survanta) [^be34b226]. FDA (2020). Medium credibility.

Administration

For endotracheal administration using a 5 French end-hole catheter:

1. Slowly withdraw the entire contents of the vial into a plastic syringe through a large-gauge needle (e.g., at least 20 gauge).

2. Attach the premeasured 5 French end-hole catheter to the syringe. Fill the catheter with SURVANTA. Discard excess SURVANTA through the catheter so that only the total dose to be given remains in the syringe.

3. When administering SURVANTA using a 5 French end-hole catheter, administer in four quarter-dose aliquots. Each quarter-dose is administered with the infant in a different position:

Head and body inclined 5–10° down, head turned to the right
Head and body inclined 5–10° down, head turned to the left
Head and body inclined 5–10° up, head turned to the right
Head and body inclined 5–10° up, head turned to the left

4. **First quarter-dose aliquot of SURVANTA suspension**:

a. Position the infant appropriately in one of the four recommended positions.

b. Insert the 5-French end-hole catheter into the endotracheal tube. The tip of the catheter should protrude just beyond the end of the endotracheal tube above the infant's carina. SURVANTA should not be instilled into a mainstem bronchus.

c. Gently inject the first quarter-dose aliquot through the catheter over 2–3 seconds.

d. After the first aliquot is instilled, remove the catheter from the endotracheal tube and manually ventilate the infant for at least 30 seconds or until clinically stable. Ventilate with sufficient oxygen to prevent cyanosis and sufficient positive pressure to provide adequate air exchange and chest wall excursion.

5. When the infant is stable, reposition the infant for instillation of the next quarter-dose.

6. Instill each remaining quarter-dose using the same procedures.

7. After instillation of the final quarter-dose, remove the catheter without flushing it. Do not suction the infant for 1 hour after dosing unless signs of significant airway obstruction occur.

---

### Locomotion-induced ocular motor behavior in larval xenopus is developmentally tuned by visuo-vestibular reflexes [^984fdff4]. Nature Communications (2022). High credibility.

Fig. 5
Vestibulo-ocular and spino-ocular motor signal integration depends on the swim frequency.

a Movements of the left (Leye) and right eye (Reye) during horizontal sinusoidal head rotation (green) and swim-related tail undulations (blue) at stage 52 (a 1); note that the spontaneously reduced swim frequency within the illustrated episode (marked by the red dashed line in a 2) produced two different types of resultant ocular motion. b Representative example of ocular motion of the left eye during a single horizontal head rotation cycle and concurrent tail undulations (b 1), depicting the variation in eye motion magnitude (b 2) and change of eye position eccentricity (= angular position of the eye relative to the longitudinal axis of the head; b 3). c Quantification of ocular motion parameters as scatter plots of Δ motion magnitude (c 1) and Δ position eccentricity (c 2) as function of the swimming frequency (n = 32 swimming episodes from N = 15 animals at different developmental stages). Two-tailed p values were calculated to estimate the Pearson coefficient correlation (R) significance (p = 0.3023 for the Δ motion magnitude plot and p = 0.0023 for the Δ motion eccentricity plot). d Box plots of Δ motion magnitude (d 1) and Δ position eccentricity (d 2) as function of the vestibular stimulus frequency at 0.5 Hz (n = 10 swimming episodes), 1 Hz (n = 13 swimming episodes) and 2 Hz (n = 14 swimming episodes) from N = 4 animals. Upper and lower error bars of box plots represent, respectively, maximum and minimum values. Bounds of box and center lines represent, respectively, the values of 50% of the central region (75 and 25% percentile) and the median values. Superimposed full circles/squares represent mean values. p -values, calculated using the Kruskal-Wallis test were equal to 0.6754 and 0.0423 respectively for box plots (d 1) and (d 2). e Histogram of Δ magnitude (e 1) and Δ eccentricity (e 2) variations during spino-ocular motor coupling at stage 49 (N = 5), stage 52 (N = 5), and stage 58 (N = 5). The Kruskal-Wallis test was performed to test the significance and resulted in p = 0.1829 in e 1 and p = 0.0343 in e 2. f Relative occurrence of eccentricity modulation during ocular motion (left axis) and average swim frequency (black dots and spheres, right axis) between stage 49 and stage 58 (Kruskal-Wallis test, p = 0.0007). Data are presented as mean values ± SEM and statistical significance is indicated as ✱✱ p < 0.001, ✱✱ p < 0.01, ✱ p < 0.05, ns non-significant in (c – f). Source data are provided as a Source Data file.

---

### Elacestrant (Orserdu) [^25e5bc7d]. FDA (2024). Medium credibility.

The dosage of elacestrant PO for treatment of breast cancer in postmenopausal female adults with ESR1 mutation (advanced or metastatic, ER-positive, HER2-negative, progression after ≥ 1 line of endocrine therapy) is 345 mg PO daily until disease progression or unacceptable toxicity

---

### Understanding statistical terms: 2 [^351bd399]. Drug and Therapeutics Bulletin (2009). Low credibility.

An increasing number of statistical terms appear in journal articles and other medical information. A working knowledge of these is essential in assessing clinical evidence. With this in mind, we are producing a series of explanatory articles covering various statistical terms and their uses. This, the second article in the series, will focus on some of the most common terms used in reporting the results of randomised controlled trials.1.

---

### Efficient experimental quantum fingerprinting with channel multiplexing and simultaneous detection [^6ac8b2ed]. Nature Communications (2021). High credibility.

Figure 6 shows an example of the distributions of the total counts at D 1 for both the same-input case (blue curve) and different-input (red curve) cases. As indicated, the probability distributions for the two cases are away from each other. For most of the time, the total counts for different-input cases C 1, D are larger than the counts for the equal-input cases C 1, E. Therefore, Charlie could choose a threshold total count C 1, th and compare C 1, th with the detected photon counts at D 1. If the number of the detected counts is smaller (larger) than C 1, th, Charlie concludes that Alice and Bob have the same (different) inputs. The errors exist when C 1, E is actually larger than the threshold, or C 1, D is smaller than the threshold. The error probability for Charlie's decision is indicated by Eq. (8). As long as P error is smaller than the tolerable error probability ϵ, Charlie's conclusion is acceptable.

Fig. 6
Probability distribution of the total counts at detector D 1 for the equal-input case (blue curve) and different-input case (red curve).

In this figure, the total number of pulses M is 5 × 10 9. δ and ν in Eq. (6) and Eq. (7) are 0.22 and 97%, respectively. The detector's dark-count rate is 100 Hz and η = 0.25 is considered as the detector efficiency (25%). The detection window is 500 ps. The average photon number μ is 1000. Note that this figure just shows an example of the probability distributions. Hence, the value of μ is not necessarily optimal and the corresponding error probability might be large.

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^a549d473]. Nature Communications (2023). High credibility.

Curves were classified as significantly up or down-regulated if a curve's relevance score is above the decision boundary.

For many applications, it is also useful to know which dose–response curves show clear independence of the doses to obtain a high-quality negative data set. It is fundamentally impossible to prove the absence of an effect statistically. Thus, we developed a heuristic approach using the null model to classify only a clear non-responsive line to be not regulated. A clear non-responder has a mean-model intercept close to 1.0 and is allowed to maximally diverge +- fc lim /2. Additionally, the variance around the null model should be low and is quantified by the root-mean-squared error (RMSE). By default, the maximally tolerated variance is an RMSE of 0.1 but can be adjusted by the user. Optionally, the user can add additional criteria, such as a p -value threshold, to be even more restrictive to the non-classification.

False discovery rate estimation

The false discovery rate (FDR) was estimated using a target-decoy approach. Decoy curves were generated based on the sample variance distribution estimated from the input data directly (Eq. 15). From this variance distribution, decoys were constructed similarly to the null curves (Eq. 1).

The decoy curves are then subject to the identical analysis pipeline. Finally, the target-decoy relevance score allows the calculation of a q -value for each curve using the target-decoy approach, as well as the overall FDR corresponding to the user's pre-defined thresholds (Eq. 15). FDR estimation is enabled using the — fdr command line option.

Statistics and reproducibility

No new experimental data were generated for this study. Study designs, including sample sizes, were taken directly from the respective original studies. For the simulated curve data, no statistical method was used to determine sample size. The 5 million curves for each number of curve data points n was chosen as the number of curves that produced sufficient smooth characterization of p -value estimation behavior up to 10 −5. No data were excluded from the analyses. Unless otherwise stated, p -values in this study were generated using the recalibrated F-statistic and multiple testing correction was performed by applying a target-decoy strategy on the relevance score. The CurveCurator version used throughout this manuscript is v0.2.1.

Reporting summary

Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.

---

### Fundamental limits to learning closed-form mathematical models from data [^bfc95d4c]. Nature Communications (2023). High credibility.

Sampling models with the Bayesian machine scientist

For a given dataset D, each model m has a description lengthgiven by Eq. (4). The Bayesian machine scientistgenerates a Markov chain of models { m 0, m 1, …, m T } using the Metropolis algorithm as described next.

Each model is represented as an expression tree, and each new model m t +1 in the Markov chain is proposed from the previous one m t by changing the corresponding expression tree: changing an operation or a variable in the expression tree (for example, proposing a new model m t +1 = θ 0 + x 1 from m t = θ 0 * x 1); adding a new term to the tree (for example, m t +1 = θ 0 * x 1 + x 2 from m t = θ 0 * x 1); or replacing one block of the tree (for example, from m t = θ 0 * x 1) (see ref.for details). Once a new model m t +1 is proposed from m t, the new model is accepted using the Metropolis rule.

Note that the only input to the Bayesian machine is the observed data D. In particular, the observational noise s ϵ is unknown and must be estimated, via maximum likelihood, to calculate the Bayesian information criterion B (m) of each model m.

Artificial neural network benchmarks

For the analysis of predictions on unobserved data, we use as benchmarks the following artificial neural network architectures and training procedures. The networks consist of: an input layer with two inputs corresponding to x 1 and x 2; (ii) four hidden fully connected feed-forward layers, with 10 units each and ReLU activation functions; and (iii) a linear output layer with a single output y.

Each network was trained with a dataset D containing N points, just as in the probabilistic model selection experiments. Training errors and validation errors (computed on an independent set) were calculated, and the training process stopped when the validation error increased, on average, for 100 epochs; this typically entailed training for 1000–2000 epochs. This procedure was repeated three times, and the model with the overall lowest validation error was kept for making predictions on a final test set. The predictions onare those reported in Fig. 1.

---

### Bayesian strategy selection identifies optimal solutions to complex problems using an example from GP prescribing [^d1a4b1f1]. NPJ Digital Medicine (2020). Medium credibility.

Another aspect to be mindful of is that Thompson sampling trades between the type-1 error rate and statistical power, implying that to achieve a greater statistical power (relative to a same sample-size equal allocation trial), it is likely to inflate the type-1 error rate. By not trying the null strategy sufficiently, the type-1 error rate may exceed the acceptable threshold. Thus it is judicious to monitor the type-1 error rate and if it exceeds the significance level, adjustments may be required in the allocation algorithm. Recent workis starting to explore adaptive sampling approaches that can control the type-1 error rate, and further work is required to develop such techniques for a Thompson sampling setting.

We did not assess intervention fidelity, that is, whether the intervention was delivered as conceived and in accordance with the materials and instructions provided to participants. Given that the aims were to measure the effectiveness of the strategies in supporting GP-patient interactions where physical activity was discussed, failure to implement the strategies in consultations was ultimately what was tested, in essence assessing the utility of each strategy.

This case study has demonstrated an efficient process providing clear results in a short period of time. Further work should develop methods to understand interactions and sequencing of various and multiple combinations of interventions. Subsequent lines of investigation should also begin to consider the question of fidelity. Given this study demonstrates a process for supporting conversations about PA between GPs and patients, an important next step is to examine the quality and content of those conversations and their impact on measured patient PA.

While the reasons for low incidence of GP patient interaction about PA are complex, these interactions occur in a relatively stable setting with a clear, well-defined outcomes. Further work on other complex problems in similarly well-defined domains, or further investigations of the GP-patient relationship in expanded settings (i.e. with greater numbers of prioritised actions, or broader participant groups) represent exciting directions for future work.

The new frontier of public health is not what behaviours need to change, but rather how to change them. This study shows how new methods can be used to test and optimise implementation of intervention in multiple settings by engaging key actors in a system and moving quickly to the best set of interventions. For PA specifically, data suggested more than 80% of adults visit their GP at least once per year. Rapid identification of appropriate strategies that increase the discussion of PA between GPs and their patients provides new potential avenues for the improvement of public health.

---

### Understanding statistical terms: 1 [^50f69b79]. Drug and Therapeutics Bulletin (2009). Low credibility.

An increasing number of statistical terms appear in journal articles and other medical information. A working knowledge of these is essential in assessing clinical evidence. With this in mind, we are producing a series of explanatory articles covering various statistical terms and their uses. This, the first article in the series, will focus on some of the most common terms used in randomised controlled trials.

---

### Article 5. An introduction to estimation – 2: from z to T [^fb82b806]. Emergency Medicine Journal (2001). Low credibility.

Provided the sample size is large enough (that is, n greater than 100), the z statistic can be used to determine the confidence interval estimation of the population mean even when the sigma is not known. In these cases the estimation of the standard error of the mean is used. The z statistic is also valid when determining the population's proportion based upon a large sample. However, when dealing with smaller samples, the z statistic is replaced by the t statistic. This makes it possible to estimate, in a population with an unknown standard deviation: The probability of getting a sample mean greater than or equal to a particular value The value of a sample mean with a particular probability of occurring The probability of getting a sample mean between two particular values The confidence interval for the estimation of the population mean can also be determined using the t statistic.

---

### Accelerated estimation and permutation inference for ACE modeling [^f0578266]. Human Brain Mapping (2019). Medium credibility.

3.2.1 Simulation setting

In this set of simulations, we use n = 20, 60, 100, using only twins and equal number of MZ and DZ pairs. The signal is generated with four parameter settings shown in Table 2 consisting of different values of heritability and shared environmental variance.

Table 2
Four parameter settings of heritability h 2 and parameters A, C, E

The simulated images are 2D, with 128 × 128 pixels. Two spatial configurations of signal were considered, a single large region or nine separate regions, having similar number of signal pixels (1,020 vs. 1,024); see Figure 7. The set ofimages were first created by filling each pixel with i.i.d. standard Gaussian noise, and then inducing the signal with the Cholesky decomposition of the desired variance/covariance structure. Spatial Gaussian smoothing kernels with full width at half maximum (FWHM) of 0, 1.5, 3, or 6 pixels were applied to blur these images in order to accommodate spatial dependence across neighbouring voxels. A total ofimages were generated for each simulation setting.

Figure 7
Illustration of the 2D simulated signal shapes. Focal signal (left) with one large circle in the middle; distributed signal (right) with nine identical circular regions

3.2.2 ROC analysis

The ROC curves plot true positive rate (TPR; y ‐axis) against FPR with varying threshold levels. A standard ROC analysis is suitable for only a single outcome, while we have 128 2 outcomes. Hence we use an alternative, free‐response ROC approach (Chakraborty & Winter, 1990). As described in (Smith & Nichols, 2009), a free‐response ROC, consisting of a y ‐axis representing the probability of true detection, averaged over pixels with A > 0, and an x ‐axis representing the probability of any false detections, is deployed.

We summarize the ROC curve by a normalized area under the curve (AUC), with a larger value for better performance. Since we are mostly concerned about FPR values between 0 and 0.05, corresponding to a family‐wise error rate of 5%, the normalised AUC is, maintaining a"perfect" AUC of 1. We calculate this free‐response ROC curve for both voxel‐ and cluster‐wise inferences. For cluster‐wise inference, we set the voxel‐level cluster‐forming threshold to α = 0.05 or LRT statistic value u α = 2.71.

---

### Clinical decision rules: how to use them [^62ab325a]. Archives of Disease in Childhood: Education and Practice Edition (2010). Low credibility.

The first of this pair of papers outlined what a clinical decision rule is and how one should be created. This section examines how to use a rule, by checking that it is likely to work (examining how it has been validated), understanding what the various numbers that tell us about "accuracy" mean, and considers some practical aspects of how clinicians think and work in order to make that information usable on the front lines.

---

### Predicting what will happen when we act. what counts for warrant? [^58aa35d9]. Preventive Medicine (2011). Low credibility.

To what extent do the results of randomized controlled trials inform our predictions about the effectiveness of potential policy interventions? This crucial question is often overlooked in discussions about evidence-based policy. The view I defend is that the arguments that lead from the claim that a program works somewhere to a prediction about the effectiveness of this program as it will be implemented here rests on many premises, most of which cannot be justified by the results of randomized controlled trials. Randomized controlled trials only provide indirect evidence for effectiveness, and we need much more than just randomized- controlled-trial results to make reliable predictions.

---

### Carbachol (Miostat) [^da222cd1]. FDA (2023). Medium credibility.

Warnings and precautions regarding the use of carbachol intraocular (also known as Miostat):
- **AF**: use caution in patients with hyperthyroidism. To prevent AF in hyperthyroid patients, closely monitor cardiac function and thyroid status before using Carbachol. Discontinue the medication, manage the cardiac condition, and collaborate with a cardiologist for targeted intervention and treatment adjustments.
- **Airway obstruction**: use extreme caution in patients with bronchial asthma. Carefully assess patients for bronchial asthma history before use and consider alternative treatments if indicated. Discontinue the medication, administer bronchodilators, and seek immediate medical intervention to manage the complication in patients with bronchial asthma.
- **Exacerbation of bladder outflow obstruction**: use caution in patients with pre-existing urinary tract obstruction. To avert worsening of urinary tract obstruction, exercise caution with Carbachol use, assess urinary tract health, and consider alternative treatments for patients with obstruction risk. Discontinue the medication, relieve obstruction, and collaborate with a urologist for appropriate intervention.
- **Exacerbation of corneal abrasion**: use caution in patients with pre-existing corneal abrasion. Exercise caution when using Carbachol ophthalmic solution in patients with corneal abrasions, ensuring minimal drug penetration and considering alternative treatments if needed. If systemic toxicity occurs due to excessive Carbachol penetration in patients with corneal abrasions, discontinue its use, manage systemic symptoms, and consult a medical professional for appropriate intervention.
- **Exacerbation of Parkinson's disease**: use caution in patients with pre-existing Parkinson's disease. Use Carbachol cautiously, assess for Parkinson's history, and consider alternative treatments if indicated. Discontinue the medication, manage Parkinson's symptoms, and consult a neurologist for specialized guidance.
- **Exacerbation of peptic ulcer disease**: use caution in patients with active peptic ulcer disease or gastrointestinal spasm. Screen patients for ulcer history, employ gastroprotective measures and consider alternative therapies if needed. Discontinue the medication, administer ulcer-healing treatments, and consult a gastroenterologist for specialized guidance.
- **Hypotension, cardiac arrhythmias, hypertension**: use extreme caution in patients with a history of cardiac disease. Carefully assess patients for cardiac conditions, administer the lowest effective dose, and monitor cardiovascular function during treatment. Discontinue the medication, provide appropriate cardiac support, and consult a cardiologist for tailored management.

---

### The history of risk: a review [^43e42677]. World Journal of Emergency Surgery (2017). Low credibility.

Blaise Pascal, the "father of the modern theory of decision-making" (Fig. 4), constructed a systematic method for analyzing the probability of future outcomes using a simple triangle. Pascal's triangle was published in his seminal work Traité du triangle arithmétique in 1653 and introduced a pragmatic and simple solution to the historic question on how to divide the stakes in an unfinished game. In the triangle, each number is the sum of the two numbers directly above (Fig. 5). The first and last number in each row are always a number 1, since the non-depicted, omitted numbers next to each number 1 represent a zero (0 + 1 = 1). The basic concept of Pascal's triangle is that the top number in row 0 shows a probability of an event that cannot fail to happen, with only one possible outcome (100%). This is ensued by a 50:50 probability in row 1 (1–1), a 25:50:25 probability in row 2 (1-2-1), and etc.

---

### Correction [^26cbbc41]. Journal of Radiology Case Reports (2017). Low credibility.

[This corrects the article on p. 8 in vol. 10.].

---

### A framework for evaluating clinical artificial intelligence systems without ground-truth annotations [^f3b9d969]. Nature Communications (2024). High credibility.

Evaluate classifier

After training g ϕ, we evaluated it on a held-out set of data comprising data points with ground-truth labels (from both class 0 and class 1) (Fig. 1 b, Step 5). The intuition here is that a classifier which can successfully distinguish between these two classes, by performing well on the held-out set of data, is indicative that the training data and the corresponding ground-truth labels are relatively reliable. Since the data points from class 1 are known to be correct (due to our use of ground-truth labels), then a highly-performing classifier would suggest that the class 0 pseudo-labels of the remaining data points are likely to be correct. In short, this step quantifies how plausible it is that the sampled unlabelled data points belong to class 0.

As presented, this approach determines how plausible it is that the sampled set of data points in some probability interval, s ∈ (s 1, s 2], belongs to class 0. It is entirely possible, however, that a fraction of these sampled data points belong to the opposite class (e.g. class 1). We refer to this mixture of data points from each class as class contamination. We hypothesised (and indeed showed) that the degree of this class contamination increases as the probability output, s, by an AI system steers away from the extremes (s ≈ 0 and s ≈ 1). To quantify the extent of this contamination, however, we also had to determine how plausible it was that the sampled set of data points belong to class 1, as we outline next.

---

### Editorial [^33402631]. Journal of Psychiatric Research (2024). Medium credibility.

N/A.

---

### Brentuximab vedotin (Adcetris) [^4d012d7d]. FDA (2025). Medium credibility.

Warnings and precautions regarding the use of brentuximab vedotin IV (also known as Adcetris):
- **Anaphylaxis**: use caution in patients with a history of infusion-related reactions.
- **Chronic pulmonary disease**: use caution risk factors for pulmonary toxicity.
- **Colitis**: use caution in patients with a history of gastrointestinal disease.
- **Gastrointestinal perforation, gastrointestinal bleeding**: use caution in patients with a history of gastrointestinal disease or a history of lymphoma with gastrointestinal involvement. Prior to initiating Brentuximab vedotin, assess patients for risk factors of GI complications and consider dose adjustments or alternative treatments if necessary. If serious GI complications like perforation, bleeding, or erosion occur, discontinue Brentuximab vedotin, initiate appropriate medical interventions, and manage the patient's condition.
- **Hematologic disorder**: use caution risk factors for myelosuppression.
- **Hyperglycemia**: use caution pre-existing diabetes mellitus.
- **Increased blood glucose**: use caution in patients with pre-existing diabetes mellitus. Monitor blood glucose levels regularly during Brentuximab vedotin treatment, especially in patients at risk for hyperglycemia, and manage blood sugar levels appropriately. Implement appropriate diabetes management strategies, which may include adjusting medications or insulin dosages under medical supervision.
- **Neutropenia, thrombocytopenia, anemia**: use extreme caution with advanced age, a history of hematologic disorder, and who received brentuximab vedotin in combination with chemotherapy. Obtain CBCs prior to each brentuximab dose and more frequently if grade 3 or 4 neutropenia occurs; monitor all patients for fever. Therapy interruption, dosage reduction, or discontinuation may be necessary in patients who develop grade 3 or 4 neutropenia.
- **Peripheral neuropathy**: use caution in patients with risk factors for Peripheral neuropathy. Monitor patients for symptoms of neuropathy (e.g., hypoesthesia, hyperesthesia, paresthesia, discomfort, burning sensation, neuropathic pain, or weakness). Therapy interruption, dosage reduction, or discontinuation may be necessary in patients who develop new or worsening peripheral neuropathy.
- **Peripheral neuropathy**: use caution in patients with symptoms of neuropathy.
- **Pneumonitis, ILD, ARDS**: use extreme caution with a history of pulmonary illness or concomitant use of brentuximab with bleomycin-containing chemotherapy. Monitor patients for signs and symptoms of pulmonary toxicity such as cough and dyspnea. Evaluate patients who develop new or worsening pulmonary symptoms; hold brentuximab until symptoms improve. Most cases resolved following corticosteroid treatment.
- **Progressive multifocal leukoencephalopathy**: use extreme caution in patients with prior immunosuppressive therapies or immunosuppressive disease. Evaluate patients who develop new neurological, cognitive, or behavioral signs and symptoms such as changes in mood or behavior; confusion; memory impairment; changes in vision, speech, or walking. Hold therapy if PML is suspected; discontinue brentuximab in patients with confirmed PML.
- **Tumor lysis syndrome**: use extreme caution in patients with rapidly proliferating tumors and/or a high tumor burden. Monitor patients for signs of TLS (e.g., serum electrolytes, uric acid, serum creatinine) prior to and during therapy. Institute appropriate prophylactic and treatment (e.g., hydration, uric acid lowering therapy) as necessary.

---

### Guidelines for the echocardiographic assessment of the right heart in adults and special considerations in pulmonary hypertension: recommendations from the American Society of Echocardiography [^b3ff0d7d]. Journal of the American Society of Echocardiography (2025). High credibility.

Supplemental appendix — methods for deriving right-sided echocardiographic reference values describe a systematic PubMed search for studies of healthy men or women ≥ 18 years of age without known cardiovascular disease or significant acute or chronic comorbidity, extracting study-level means, SDs, and counts by sex; a Bayesian hierarchical model produced pooled means and SDs used to compute lower (mean − 1.96 SDs) and upper (mean + 1.96 SDs) reference values and moderate (mean ± 3 SDs) and severe (mean ± 4 SDs) cutoffs. For each statistic, the posterior median and 95% equal-tailed credible interval were computed; the model was stratified by measurement technique when relevant, sex-stratified meta-analysis was not feasible due to insufficient studies, and the analysis used a Python codebase with the JAGS package, with study-level data publicly available online.

---

### MOCCASIN: a method for correcting for known and unknown confounders in RNA splicing analysis [^feb63f39]. Nature Communications (2021). High credibility.

Fig. 2
Removal of batch effects from simulated RNA-Seq data with MOCCASIN.

RNA-Seq samples from mouse Aorta and Cerebellum were simulated using BEERS while injecting G % of the genes in half the samples with a batch effect of C % expression change of the main isoform (see main text). a Cumulative distribution of the difference (|dPSI|) from simulated ground truth after batch signal injection (G = 20%, C = 60%) either before MOCCASIN (blue) or after correction with increasing numbers of samples for each of the four batch/condition combinations: 1 × 4 (4 total, purple), 2 × 4 (8 total, brown), 3 × 4 (12 total, yellow), and 4 × 4 (16 total, orange). All plots are derived from the same representative sample (SRR1158528) to maintain a fixed base for comparison, with similar plots observed for other perturbed samples (data not shown). Total number of LSVs: 21566. b, c The number of LSVs (Y -axis) detected as differential (|E(dPSI)| > 0.2) for the batch 1 (N = 4) versus batch 2 (N = 4) signal (b, left) and the aorta (N = 4) vs cerebellum (N = 4) signal (c, right) across a range of increasingly significant p -values (X -axis, Student's t test, −log 10 scale). Number of samples used is 4 per batch/tissue combination (same as in the orange line in a). The green points ("Ground Truth") are from the simulated data with no batch signal injection and the blue points ("Before MOCCASIN") are from the same data after batch signal injection (G = 20%, C = 60%). Both blue and green points serve as reference points for MOCCASIN correction of the batch signal. Orange and gray represent, respectively, the results after MOCCASIN correction when the batches are known or unknown. d, e Assessing false positive rate (FPR) for the batch signal (d, left) and false discovery rate (FDR) for the tissue signal (e, right) for a range of G values (2, 5, 20%) and C effect size (2, 10, 60%). Number of samples same as in (b, c). Here positive events where considered as those changing by at least 20% with high confidence by MAJIQ (P(|dPSI| > 0.2) > 0.95). Under these definitions small effect sizes (C = 2,10%) represent perturbations that are not expected to affect the positive event set much. f Heatmaps of E(PSI) from simulated data without batch effect (ground truth, left), with simulated batch effect (G = 20%, C = 60%) without correction (middle), and after applying MOCCASIN with 1 known confounding factor (right). Each column is a sample, and each row is an LSV (N = 4941). The colored bars above the samples denote the sample's tissue (8 aorta samples in purple, and 8 cerebellum samples in green) and batch (8 batch 1 sample in red and 8 batch 2 samples blue).

---

### The case for open computer programs [^0f6e8786]. Nature (2012). Excellent credibility.

Scientific communication relies on evidence that cannot be entirely included in publications, but the rise of computational science has added a new layer of inaccessibility. Although it is now accepted that data should be made available on request, the current regulations regarding the availability of software are inconsistent. We argue that, with some exceptions, anything less than the release of source programs is intolerable for results that depend on computation. The vagaries of hardware, software and natural language will always ensure that exact reproducibility remains uncertain, but withholding code increases the chances that efforts to reproduce results will fail.

---

### Spontaneous variability in gamma dynamics described by a damped harmonic oscillator driven by noise [^e861d0a6]. Nature Communications (2022). High credibility.

Fig. 4
Gamma-half-cycle amplitudes and durations are positively correlated in gamma-oscillatory epochs.

a For each dataset listed on the x-axis, the three bars show the correlation between the amplitude of a gamma half cycle and the duration of the same (center, red), previous (left, white), and next (right, white) gamma half cycle. On the right, this is shown for the average across all datasets. This was calculated for each time point across trials and averaged across time points for gamma-oscillatory epochs. The data used correspond to the period during the presentation of the visual stimulus. For individual datasets, a null distribution was produced by randomizing the order of duration values across trials, and the resulting means and 99.9% confidence intervals are shown as dots and vertical lines (numbers of trials = 278, 5740, 672, 1075, 672, 320, 142 for respective datasets). For the average across datasets, shown on the right, we performed a two-sided t -test and show the resulting confidence intervals as vertical lines on the observed mean (red bar: p < 6 × 10 −3, white bars for preceding cycle: p = 0.5, white bars for succeeding cycle: p = 0.35). In addition we performed a two-sided non-parametric permutation test (red bar: p < 0.05; white bars: p > 0.05). b Correlation between the amplitude of a gamma half-cycle and the duration of gamma half-cycles before and after it for three different datasets. Note that in monkey I, this is limited to ± 2 cycles, because the signal-to-noise ratio was lower, resulting in shorter gamma-oscillatory epochs. Importantly, all three example datasets show a central peak, despite the fact that they show different longer-term correlations. The gray lines and gray-shaded areas depict the means and 99.9% confidence regions, after randomizing the order of duration values across trials. c Same as a, but showing the correlations between residuals of the regression across adjacent amplitude triplets and the residuals of the regression across adjacent duration triplets (numbers of trials = 278, 5740, 672, 1075, 672, 320, 142 for respective datasets; red bar: p = 23 × 10 −4, two-sided t-test across datasets; p < 0.05, permutation test for individual datasets; white bars p = 0.066 and p = 0.97, respectively for preceding and succeeding cycles, two-sided t -test across datasets; p > 0.05, permutation test for individual datasets). a – c: n = 6 biologically independent experiments. Source data are provided in the Source Data file.

---

### Association between presence of HLA-B✱5701, HLA-DR7, and HLA-DQ3 and hypersensitivity to HIV-1 reverse-transcriptase inhibitor abacavir [^a300e945]. Lancet (2002). Excellent credibility.

Background

The use of abacavir — a potent HIV-1 nucleoside-analogue reverse-transcriptase inhibitor — is complicated by a potentially life-threatening hypersensitivity syndrome in about 5% of cases. Genetic factors influencing the immune response to abacavir might confer susceptibility. We aimed to find associations between MHC alleles and abacavir hypersensitivity in HIV-1-positive individuals treated with abacavir.

Methods

MHC region typing was done in the first 200 Western Australian HIV Cohort Study participants exposed to abacavir. Definite abacavir hypersensitivity was identified in 18 cases, and was excluded in 167 individuals with more than 6 weeks' exposure to the drug (abacavir tolerant). 15 individuals experienced some symptoms but did not meet criteria for abacavir hypersensitivity. p values were corrected for comparisons of multiple HLA alleles (p(c)) by multiplication of the raw p value by the estimated number of HLA alleles present within the loci examined.

Findings

HLA-B✱5701 was present in 14 (78%) of the 18 patients with abacavir hypersensitivity, and in four (2%) of the 167 abacavir tolerant patients (odds ratio 117 [95% CI 29–481], p(c) < 0.0001), and the HLA-DR7 and HLA-DQ3 combination was found in 13 (72%) of hypersensitive and five (3%) of tolerant patients (73 [20–268], p(c) < 0.0001). HLA-B✱5701, HLA-DR7, and HLA-DQ3 were present in combination in 13 (72%) hypersensitive patients and none of the tolerant patients (822 [43–15 675], p(c) < 0.0001). Other MHC markers also present on the 57.1 ancestral haplotype to which the three markers above belong confirmed the presence of haplotype-specific linkage disequilibrium, and mapped potential susceptibility loci to a region bounded by C4A6 and HLA-C. Within the entire abacavir-exposed cohort (n = 200), presence of HLA-B✱5701, HLA-DR7, and HLA-DQ3 had a positive predictive value for hypersensitivity of 100%, and a negative predictive value of 97%.

Interpretation

Genetic susceptibility to abacavir hypersensitivity is carried on the 57.1 ancestral haplotype. In our population, withholding abacavir in those with HLA-B✱5701, HLA-DR7, and HLA-DQ3 should reduce the prevalence of hypersensitivity from 9% to 2.5% without inappropriately denying abacavir to any patient.

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^d027f21a]. Nature Communications (2025). High credibility.

Understanding the effect of heterogeneity is fundamental to numerous fields. In community ecology, classical theory postulates that habitat heterogeneity determines niche dimensionality and drives biodiversity. However, disparate heterogeneity-diversity relationships have been empirically observed, generating increasingly complex theoretical developments. Here we show that spurious heterogeneity-diversity relationships and subsequent theories arise as artifacts of heterogeneity measures that are mean-biased for bounded continuous variables. To solve this, we derive an alternative mean-independent measure of heterogeneity for beta and gamma distributed variables that disentangles statistical dispersion from mean. Using the mean-independent measure of heterogeneity, true monotonic positive heterogeneity-diversity relationships, consistent with classical theory, are revealed in data previously presented as evidence for both hump-shaped heterogeneity-diversity relationships and theories of an area-heterogeneity trade-off for biodiversity. This work sheds light on the source of conflicting results that have hindered understanding of heterogeneity relationships in broader ecology and numerous other fields. The mean-independent measure of heterogeneity is provided as a solution, essential for understanding true mean-independent heterogeneity relationships in wider research.

---

### Dissection of molecular and histological subtypes of papillary thyroid cancer using alternative splicing profiles [^70b66986]. Experimental & Molecular Medicine (2022). Medium credibility.

Fig. 2
Association of the NUMA1_17515 exon skipping event with biological features of papillary thyroid cancer (PTC).

a Correlations of the NUMA1_17515 event with thyroid differentiation scores (TDSs) and ERK and RAS activity scores. Each dot represents an individual patient sample. Pearson's correlation coefficient (r) and the linear regression line are shown for each plot. b Percent spliced-in (PSI) values for molecular (left) and histological (right) subtypes. The p values were determined by ANOVA. c Correlation between NUMA1_17515 PSI values and NUMA1 expression levels. Histological subtypes are indicated by different colors. d Association of NUMA1_17515 PSI values with progression of extrathyroidal extension (left) and regional lymph node metastasis (right). Extrathyroidal extension: 0 = none; 1 = minimum (T3); 2 = advanced (T4a and T4b). Regional lymph node metastasis: 0 = negative; 1 = positive. e Correlations of the NUMA1_17515 event with TDSs, histological subtype, and NUMA1 expression level in the Seoul National University Hospital (SNUH) thyroid cancer dataset.

---

### Clinical policy: critical issues in the evaluation and management of emergency department patients with suspected Non-ST-elevation acute coronary syndromes [^4ba2dde4]. Annals of Emergency Medicine (2018). Medium credibility.

Appendix C — Likelihood ratios and number needed to treat provides interpretive thresholds: LR (+) "1.0" with LR (−) "1.0" "Does not change pretest probability"; LR (+) "1–5" with LR (−) "0.5–1" "Minimally changes pretest probability"; LR (+) "10" with LR (−) "0.1" "May be diagnostic if the result is concordant with pretest probability"; LR (+) "20" with LR (−) "0.05" "Usually diagnostic"; and LR (+) "100" with LR (−) "0.01" "Almost always diagnostic even in the setting of low or high pretest probability". It also defines number needed to treat: "Number needed to treat (NNT): number of patients who need to be treated to achieve 1 additional good outcome; NNT = 1/absolute risk reduction×100", and clarifies that "absolute risk reduction is the risk difference between 2 event rates (ie, experimental and control groups)".

---

### Maximum likelihood estimation of signal amplitude and noise variance from MR data [^bfd9a45c]. Magnetic Resonance in Medicine (2004). Low credibility.

In MRI, the raw data, which are acquired in spatial frequency space, are intrinsically complex valued and corrupted by Gaussian-distributed noise. After applying an inverse Fourier transform, the data remain complex valued and Gaussian distributed. If the signal amplitude is to be estimated, one has two options. It can be estimated directly from the complex valued data set, or one can first perform a magnitude operation on this data set, which changes the distribution of the data from Gaussian to Rician, and estimate the signal amplitude from the obtained magnitude image. Similarly, the noise variance can be estimated from both the complex and magnitude data sets. This article addresses the question whether it is better to use complex valued data or magnitude data for the estimation of these parameters using the maximum likelihood method. As a performance criterion, the mean-squared error (MSE) is used.

---

### Blood levels of nicotinic acid negatively correlate with hearing ability in healthy older men [^60395505]. BMC Geriatrics (2023). Medium credibility.

(A, B and C) Multiple linear regression analysis with hearing threshold (dB HL) at each frequency as dependent variable and NA (μM) (A), NAM (μM) (B) or NAR (μM) (C) as independent variables, adjusted for age

The values analyzed excluding outliers are presented in parentheses. ✱ p < 0.05; ✱✱ p < 0.01; ✱✱ p < 0.001. R: Right, L: Left

In addition, the same analyses were performed on the data set excluding outliers. Outliers were defined as values outside of the third quartile + 1.5 × quartile range as the maximum and the first quartile -1.5 × quartile range as the minimum. Even after excluding outliers, Spearman's rank correlation tests still presented positive associations between NA levels and right or left-ear hearing thresholds at 1000 Hz (right: r = 0.454, p = 0.004, left: 1000 Hz (r = 0.359, p = 0.027)),2000 Hz (right: r = 0.522, p < 0.001, left: r = 0.641, p < 0.001) and 4000 Hz (r = 0.335, p = 0.032) (Fig. 2 A). Age-adjusted multiple linear regression analysis (dependent variable: right-ear hearing threshold, independent variable: NA and age) also revealed that NA was a still independent predictor of elevated right-ear hearing thresholds (1000 Hz (right): p = 0.004, β = 2657, 95% CI = 934 to 4379; 1000 Hz (left): p = 0.028, β = 2419, 95% CI = 274 to 4563; 2000 Hz (right): p = 0.001, β = 3782, 95% CI = 1585 to 5979; 2000 Hz (left): p < 0.001, β = 5968, 95% CI = 3777 to 8159; 4000 Hz (left): p = 0.024, β = 3743, 95% CI = 519 to 6966) (Table 2).

---

### Dietary thiols accelerate aging of C. elegans [^33914a54]. Nature Communications (2021). High credibility.

Fig. 6
GSH restriction upregulates mitochondrial ROS signaling in C. elegans.

a Genes induced by acivicin and by life-extending mitochondrial mutations overlap significantly. Numbers represent the expression log 2 fold change compared to wt or untreated worms. See full list of differentially regulated genes in Supplementary Data 2. Data for gene expression in daf-2, clk-1, isp-1 and nuo-6 are from. Up- and down-regulated genes are indicated, respectively, in red and blue. Genes which are regulated in daf-2 worms are highlighted in gray. *-genes regulated by Hg 2+, CH 3 HgCl or Cd 2+. Statistical significance for the overlap between sets upregulated more than two folds by: acivicin (34 genes) and nuo-6 (1285 genes) – 22 genes overlap, RF = 10.3, p value = 7.93e−19; acivicin and clk-1 (328 genes) – 13 genes overlap, RF = 23.9, p value = 2.48e−15; acivicin and isp-1 (609 genes) – 19 genes overlap, RF = 18.8, p value = 9.01e−21. RF – representation factor is the number of overlapping genes divided by the expected number of overlapping genes and p is normal approximation of hypergeometric probability. b, c Representative fluorescent images (left panels) and quantifications (right panels) demonstrating the tbb-6 counter-regulation by acivicin (n = 70) and NAC (n = 30–55) (b) and the tbb-6 induction by acetaminophen (AAPh, 20 mM, n = 35), acivicin (75 µM, n = 35), dinitrophenol (DNP, 100 µM, n = 35), and cadmium (Cd 2+, 50 µM, n = 65) (c). L4 stage worms were transferred to plates supplemented with chemicals as indicated, and incubated for 24 h at 20 °C before imaging. n = 30–70 worms over three independent experiments. See also Supplementary Tables 8 and 9. d Representative fluorescent and bright light images (left panels) and quantifications (right panel) demonstrating mitochondrial ROS staining by MitoTracker CM-H 2 X in control and acivicin-treated worms (n = 79). Worms developed till stage L3 on control and acivicin supplemented plates. See also Supplementary Table 10. In b, c and d the box plots indicate median (middle line), 25th, 75th percentile (box) and 5th and 95th percentile (whiskers) as well as maximum, minimum and mean (single points). In all graphs p values are: n.s. not significant; ✱ p < 0.05; ✱✱ p < 0.01; ✱✱ p < 0.001; ✱✱ p < 0.0001; two-tailed t -tests.

---

### Modelling insights into the COVID-19 pandemic [^de45a0e2]. Paediatric Respiratory Reviews (2020). Medium credibility.

Coronavirus disease 2019 (COVID-19) is a newly emerged infectious disease caused by the severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2) that was declared a pandemic by the World Health Organization on 11th March, 2020. Response to this ongoing pandemic requires extensive collaboration across the scientific community in an attempt to contain its impact and limit further transmission. Mathematical modelling has been at the forefront of these response efforts by: (1) providing initial estimates of the SARS-CoV-2 reproduction rate, R 0 (of approximately 2–3); (2) updating these estimates following the implementation of various interventions (with significantly reduced, often sub-critical, transmission rates); (3) assessing the potential for global spread before significant case numbers had been reported internationally; and (4) quantifying the expected disease severity and burden of COVID-19, indicating that the likely true infection rate is often orders of magnitude greater than estimates based on confirmed case counts alone. In this review, we highlight the critical role played by mathematical modelling to understand COVID-19 thus far, the challenges posed by data availability and uncertainty, and the continuing utility of modelling-based approaches to guide decision making and inform the public health response. †Unless otherwise stated, all bracketed error margins correspond to the 95% credible interval (CrI) for reported estimates.

---

### Work-relatedness [^d363de6d]. Journal of Occupational and Environmental Medicine (2018). Medium credibility.

Work-relatedness — evaluating the evidence explains that for epidemiologic surveillance a "highly sensitive, relatively nonspecific case definition is frequently employed", which "may increase the rate of screening yield for study, but generally produces a high rate of false-positives", and that if surveillance suggests an association, "more formal research can be done" with a research-study definition that is "much more rigid, traditionally implying 95% confidence that the purported causal relationship is not statistically spurious", with "The methodology for inferring a causal association" provided in Table 1.

---

### Deep learning reconstruction algorithm and high-concentration contrast medium: feasibility of a double-low protocol in coronary computed tomography angiography [^30daa5b6]. European Radiology (2025). Medium credibility.

Objective image quality

Comprehensive objective image quality results are reported in Table 2 and depicted in Fig. 2, with a total of 6120 datasets subjected to analysis.

Table 2
Objective image quality scores

Data are mean ± SD. Parametric continuous data are expressed means ± SD

CNR contrast-to-noise ratio, LM left main, LAD left anterior descending branch, LCX left circumflex branch, SNR signal-to-noise ratio, RCA right coronary artery

a Hounsfield units

Fig. 2
Box-and-whisker plots for quantitative image quality show the distribution of coronary arteries attenuation values (A), SNR (B), and CNR (C) of groups A, B, and C. Boxes represent the middle 50% of the data, solid lines represent the median, and whiskers represent minimum and maximum values. Group B yielded significantly higher attenuation values, SNR, and CNR (all p < 0.001)

Group B yielded higher attenuation values (517.3 ± 26.3 HU) than group A and C (461.1 ± 28.9 HU and 460 ± 24.3 HU, respectively; p ≤ 0.001); no significant differences were found between group A and C (p = 0.788). Similarly, group B yielded also lower noise (18.5 ± 5.4) than group A and C (23.6 ± 6.7 and 20.7 ± 7.3, respectively; all p < 0.001).

Group B yielded also higher overall SNR (30.5 ± 11.5) and CNR (27.8 ± 11) compared to group A (21.6 ± 9.2 and 19.6 ± 8.7) and group C (24.4 ± 12.1 and 22.5 ± 9.5, all p ≤ 0.001). Group C achieved higher CNR than group A (22.5 ± 9.5 vs 19.6 ± 8.7; p = 0.039), while the SNR of the two groups was comparable (24.4 ± 12.1 vs 21.6 ± 9.2; p = 0.091).

---

### Machine learning in nuclear medicine: part 1-introduction [^19d93384]. Journal of Nuclear Medicine (2019). Medium credibility.

This article, the first in a 2-part series, provides an introduction to machine learning (ML) in a nuclear medicine context. This part addresses the history of ML and describes common algorithms, with illustrations of when they can be helpful in nuclear medicine. Part 2 focuses on current contributions of ML to our field, addresses future expectations and limitations, and provides a critical appraisal of what ML can and cannot do.

---

### General conditions for predictivity in learning theory [^e1e7f28e]. Nature (2004). Excellent credibility.

Developing theoretical foundations for learning is a key step towards understanding intelligence. 'Learning from examples' is a paradigm in which systems (natural or artificial) learn a functional relationship from a training set of examples. Within this paradigm, a learning algorithm is a map from the space of training sets to the hypothesis space of possible functional solutions. A central question for the theory is to determine conditions under which a learning algorithm will generalize from its finite training set to novel examples. A milestone in learning theory was a characterization of conditions on the hypothesis space that ensure generalization for the natural class of empirical risk minimization (ERM) learning algorithms that are based on minimizing the error on the training set. Here we provide conditions for generalization in terms of a precise stability property of the learning process: when the training set is perturbed by deleting one example, the learned hypothesis does not change much. This stability property stipulates conditions on the learning map rather than on the hypothesis space, subsumes the classical theory for ERM algorithms, and is applicable to more general algorithms. The surprising connection between stability and predictivity has implications for the foundations of learning theory and for the design of novel algorithms, and provides insights into problems as diverse as language learning and inverse problems in physics and engineering.

---

### Why is amyloid-β PET requested after performing CSF biomarkers? [^14c193fa]. Journal of Alzheimer's Disease (2020). Medium credibility.

Table 1
Patient population stratified by binarized CSF Aβ 42 and tau status

Education is staged by Verhage classification (1–7). MRI scans were regarded 1) medial temporal atrophy (MTA)-positive if the left-right averaged MTA ≥ 1 for a patient under the age of 65, or ≥ 1.5 for patient age between 65 and 75, 2) posterior cortical atrophy (PCA)-positive if the left-right averaged PCA ≥ 1 for a patient under the age of 65, and 3) global cortical atrophy (GCA)-positive if the GCA ≥ 1 for a patient under the age of 65. Cognitive domain z-scores were derived using the mean and standard deviation values from a group of healthy controls. A, B, C, D indicate difference (p < 0.05) from other groups: A) difference from Aβ–/tau–; B) difference from Aβ–/tau+; C) difference from Aβ+/tau–; D) difference from Aβ+/tau+.

Fig.2
CSF Aβ 42, and tau/p-tau values relative to their cut-offs. We present standardized CSF values, created by calculating the percentage of the CSF value relative to its concurrent cut-off. Values of < 100% represent pathologically decreased CSF Aβ 42 and values of > 100% indicate pathologically increased CSF tau (A) and p-tau (B).

---

### Estimating global, regional, and national daily and cumulative infections with SARS-CoV-2 through nov 14, 2021: a statistical analysis [^5284c7ec]. Lancet (2022). Excellent credibility.

R effective estimation in the past

Using daily infections, we directly estimated R effective in the past by location and day, where R effective at time t is:

The assumptions required for this estimation are the duration from infection to being infectious and the period of infectiousness, collectively represented as θ. We used ranges of 3–5 days for both assumptions to generate estimates of R effective in the past. These estimates are useful for identifying the effect of different non-pharmaceutical interventions on transmission in different settings. An R effective lower than 1·0 indicates that the epidemic is shrinking, whereas an R effective higher than 1·0 indicates that the epidemic is growing.

We compared R effective to an estimate of total immunity in the population of location l at time t (presented as weekly averages), where this value is calculated as:

The proportion of the population effectively vaccinated is a function of doses administered and brand-specific efficacy and is discounted for existing natural immunity at the time of delivery.

Role of the funding source

The funders of the study had no role in the study design, data collection, data analysis, data interpretation, or the writing of the report.

---

### Clinical policy: critical issues related to opioids in adult patients presenting to the emergency department [^64899fd9]. Annals of Emergency Medicine (2020). High credibility.

Appendix C — Likelihood ratios (LR) and number needed to treat (NNT) provides interpretive thresholds: "1.0 1.0 Does not change pretest probability"; "1–5 0.5–1 Minimally changes pretest probability"; "10 0.1 May be diagnostic if the result is concordant with pretest probability"; "20 0.05 Usually diagnostic"; and "100 0.01 Almost always diagnostic even in the setting of low or high pretest probability". It defines abbreviations and NNT via "LR, likelihood ratio". and "Number needed to treat (NNT): number of patients who need to be treated to achieve 1 additional good outcome; NNT = 1/absolute risk reduction×100, where absolute risk reduction is the risk difference between 2 event rates (ie, experimental and control groups)".

---

### Solving the diagnostic challenge: a patient-centered approach [^df1cd6b6]. Annals of Family Medicine (2018). Low credibility.

Arriving at an agreed-on and valid explanation for a clinical problem is important to patients as well as to clinicians. Current theories of how clinicians arrive at diagnoses, such as the threshold approach and the hypothetico-deductive model, do not accurately describe the diagnostic process in general practice. The problem space in general practice is so large and the prior probability of each disease being present is so small that it is not realistic to limit the diagnostic process to testing specific diagnoses on the clinician's list of possibilities. Here, new evidence is discussed about how patients and clinicians collaborate in specific ways, in particular, via a process that can be termed inductive foraging, which may lead to information that triggers a diagnostic routine. Navigating the diagnostic challenge and using patient-centered consulting are not separate tasks but rather synergistic.

---

### Clinical genetic counseling and translation considerations for polygenic scores in personalized risk assessments: a practice resource from the National Society of Genetic Counselors [^28d8464d]. Journal of Genetic Counseling (2023). High credibility.

Polygenic score (PGS) development and validation for clinical use — Ordering providers should have a basic understanding of PGS test development and downstream testing restrictions in order to support appropriate test utilization, and there are no best practices or standards for developing a PGS, such that PGS for the same condition are often not the same. Regulatory requirements for genetic tests to make it to market often require demonstrated analytic and clinical validity, and for PGS and integrated PGS risk models analytic and clinical validity may be restricted to the model parameters used during development. From genome-wide association studies (GWAS) to PGS, the basic steps are to (1) conduct a GWAS to identify variants in a training/source data set, (2) select disease-associated variants that meet prespecified thresholds and combine these to create a PGS, and (3) validate the PGS using an independent dataset; GWAS evaluates associations using "common" variants with "common" population prevalence (≥ 1% frequency in a population).

---

### Calculating APC with weighting… [^670161cc]. seer.cancer.gov (2001). Low credibility.

\^{2}} } \). Then the estimate of the slope of the line is \
- \left\left}{\left
- \left^{2}} } \). Then the APC is just 100 ×. To estimate the standard error of m, s, we use the square root of: \^{2}}{n
- 2} \left \left
- \left^{2}} \right) }\). \} \).

---

### Extraction of accurate cytoskeletal actin velocity distributions from noisy measurements [^e02eea03]. Nature Communications (2022). High credibility.

Consequently, for small displacements, the localization error overwhelms the measured distance. Figure 2 a, b shows simulations of measured distances between two positions with a true distance s = 5 σ x y, as well as for a true distance of s = 0 (a stationary point) with σ x y = 1. In both cases, the distribution of measured distances was in good agreement with the analytical solution given by a noncentral χ distribution. For the longer distance, the average measured distance is roughly correct, but for a stationary particle (s = 0), all measurements are greater than the true displacement of zero, giving a peaked distance distribution (with peak at). For both true distances, the distribution of measured distances is broader than the (Dirac delta-function) true distribution.

In order to quantitatively characterize the localization error, we treated chemically fixed cells with SiR-actin and tracked the actin fiducials through time in the same way as our live-cell measurements. The resulting distribution of measured displacements was peaked at non-zero values, as predicted (Fig. 2 c). However, this distribution is not well-fit by a single noncentral χ with s = 0, due to the varying localization errors of individual puncta. We are able to capture this variation by a mixture of two noncentral χ distributions, giving three fit parameters: σ x y 1 and σ x y 2, the σ s contributed from each of the two noncentral χ distributions, and f 1, the relative weighting of the noncentral chi with the smaller σ. Here and hereafter we report maximum likelihood estimates for fit parameters; MLE allows us to maximally leverage the information in tracks (n = ~70,000–360,000 displacements in fixed cells from each experimental dataset), rather than fitting to the binned histogram. On average across all experiments, the best fit parameters were σ x y 1 = 23 nm, σ x y 2 = 50 nm, and f 1 = 0.57, defining the localization error in our system (Fig. 2 c, Supplementary Fig. 1).

---

### Elective female genital cosmetic surgery: ACOG committee opinion, number 795 [^2996ee1b]. Obstetrics and Gynecology (2020). High credibility.

Variability of female genitalia measurements — selected normative values from Table 2 (Mean [in mm], Standard Deviation, Minimum [in mm], Maximum [in mm]) include: Width of clitoris — Mean 4.62, Standard Deviation 2.538, Minimum 1, Maximum 22; Length of clitoris — Mean 6.89, Standard Deviation 4.965, Minimum 0.5, Maximum 34; Length of labia minora (right) — Mean 42.1, Standard Deviation 16.35, Minimum 6, Maximum 100; Width of labia minora (right) — Mean 13.4, Standard Deviation 7.875, Minimum 2, Maximum 61.

---

### Corrections [^e34f1524]. The Lancet: Respiratory Medicine (2016). Medium credibility.

[This corrects the article DOI: 10.1016/S2213-2600(15)00283–0.].

---

### Implementation evaluation of multiple complex early years interventions: an evaluation framework and study protocol [^bf774844]. BMJ Paediatrics Open (2019). High credibility.

Data collection

Data for the implementation evaluation will be derived from a number of sources:

Quantitative data collected by intervention teams

Prior to the implementation of each intervention, a service design process takes place in collaboration with commissioners, intervention delivery teams, academic researchers and other stakeholders including health professionals and community representatives to ensure each intervention meets the needs of the local population. During this process, recruitment targets and process and outcome data to be collected by intervention teams throughout the delivery period and submitted quarterly to the research team are also agreed. A guide to the service evaluation process and templates including a minimum dataset for implementation evaluation is available on our website.

Satisfaction questionnaires

We have developed a brief six-item satisfaction questionnaire to capture participants' satisfaction across all interventions (see online supplementary additional file 1). Questions are based on the key constructs of commonly used patient satisfaction surveys, but have been adapted following advice from our Community Research Advisory Group (CRAG), composed of local parents and volunteers alongside intervention team managers, commissioners and the research team to ensure acceptability for the local community. This process resulted in a questionnaire that is brief and uses visual cues and simple language that can be easily understood and translated into other languages.

Semi-structured interviews and focus groups

Semistructured interviews and/or focus groups, where appropriate, will be undertaken with intervention participants and delivery teams to allow more in-depth exploration of elements of the conceptual model. Topic guides will be based on the Theoretical Domains Framework (TDF). The TDF encompasses a comprehensive range of constructs from theories of behaviour change including beliefs about capabilities, knowledge, skills, emotions and social influences. Furthermore, use of the TDF provides a firm theoretical basis to allow understanding of the mechanisms of action as well as the barriers and facilitators of implementation. It has been extensively applied to investigate and address implementation problems. While the interview questions may differ by intervention, use of the TDF ensures the underlying theoretical concepts explored in all interviews are explored using a consistent approach.

All studies will include data from sources 1 and 2. In-depth qualitative work may be triggered in response to issues identified by interventions such as difficulties in engaging families from particular ethnic groups, low completion rates and priorities highlighted by the commissioning team.

Eligibility

Inclusion criteria for all participants are listed in box 1.

Box 1
Inclusion criteria for all participants

Exclusion criteria

For qualitative studies, participants who have completed an interview/focus group within the past 12 months will not be approached to take part in a second study to avoid unnecessary burden.

---

### The Infectious Diseases Society of America guidelines on the diagnosis of COVID-19: antigen testing (January 2023) [^d77aaaa3]. Clinical Infectious Diseases (2024). High credibility.

Pretest probability assumptions for symptomatic and asymptomatic individuals — Prevalence, defined by surveillance nucleic acid amplification test (NAAT) results over the last 14 days, was used to set baseline risks; for asymptomatic cases the panel applied 1%, 5%, and 10% pretest probability, and for symptomatic patients used 5%, 20%, to 50% pretest probability, with these values chosen based on reported prevalence sources.

---

### Directly measuring mean and variance of infinite-spectrum observables such as the photon orbital angular momentum [^a1aca62f]. Nature Communications (2015). Medium credibility.

The standard method for experimentally determining the probability distribution of an observable in quantum mechanics is the measurement of the observable spectrum. However, for infinite-dimensional degrees of freedom, this approach would require ideally infinite or, more realistically, a very large number of measurements. Here we consider an alternative method which can yield the mean and variance of an observable of an infinite-dimensional system by measuring only a two-dimensional pointer weakly coupled with the system. In our demonstrative implementation, we determine both the mean and the variance of the orbital angular momentum of a light beam without acquiring the entire spectrum, but measuring the Stokes parameters of the optical polarization (acting as pointer), after the beam has suffered a suitable spin-orbit weak interaction. This example can provide a paradigm for a new class of useful weak quantum measurements.

---

### Predicting future risk of asthma exacerbations using individual conditional probabilities [^9109e332]. The Journal of Allergy and Clinical Immunology (2011). Low credibility.

Background

Determination of future risk of exacerbations is a key issue in the management of asthma. We previously developed a method to calculate conditional probabilities (π) of future decreases in lung function by using the daily fluctuations in peak expiratory flow (PEF).

Objective

We aimed to extend calculation of π values to individual patients, validated by using electronically recorded data from 2 past clinical trials.

Methods

Twice-daily PEF data were analyzed from 78 patients with severe (study A) and 61 patients with poorly controlled (study B) asthma. For each patient, the π value was calculated from 5000 PEF data points simulated based on the correlation and distribution properties of observed PEF. Given an initial PEF, the π value was defined as the probability of a decrease in PEF to less than 80% of predicted value on 2 consecutive days within a month. These probabilities were then compared with actual occurrences of such events and clinically defined exacerbations within the following month.

Results

π Values were related to actual occurrences of decreases in PEF (adjusted R(2) > 0.800 for both studies). Every increase of 10% in π value was associated with an odds ratio of having a future exacerbation of 1.24 (95% CI, 1.07–1.43) for study A and 1.13 (95% CI, 1.02–1.26) for study B, with better sensitivity and specificity than clinic-measured FEV(1).

Conclusion

These results from 2 independent datasets with differing asthmatic populations and differing exacerbation criteria provide support that clinically relevant quantification of individual future risk of exacerbations is possible.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^64462d9d]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Lower limit of normal (LLN) and fixed cutoffs — percent predicted values should not be used to define abnormality because the true LLN is age- and/or height-dependent and varies among individuals; fixed cutoffs such as 80% predicted for forced vital capacity (FVC) or 0.70 for the FEV1/FVC ratio are based on middle-aged adults, can yield erroneous decisions in children and in older or shorter adults, and introduce sex bias. The LLN does not necessarily need to be the fifth percentile and can be adjusted with outcome data; caution is warranted near dichotomous boundaries, especially with results from a single occasion. In clinical testing where disease is suspected, the pretest probability is higher and the false positive rate is much lower than the lowest 5% implied by percentile thresholds.

---

### Correction [^d71782a7]. JAAD Case Reports (2021). Medium credibility.

[This corrects the article DOI: 10.1016/j.jdcr.2020.09.014.].

---

### Measurements of the size and correlations between ions using an electrolytic point contact [^ebb36881]. Nature Communications (2019). High credibility.

For each current trace, the data acquired within 15 s after a change in voltage was expunged to guarantee a steady-state reading of current with no capacitive influence. Two properties were then extracted from these traces; the mean open pore current I 0 and the amplitude, S 1/ f (1 Hz). To determine, the PSD was plotted as a function ofand a weighted fit of the 1/ f noise component of the trace was performed using a force fit slope of β = −1. The fit was preferentially weighted to low frequencies such that a hard cutoff on the higher frequency bound was unnecessary. Specifically, every two decades, the weight dropped an order of magnitude, so a PSD value at 100 Hz was 10 times less significant to the fit than the PSD recorded at 1 Hz and so on. Separately, the mean logarithmic PSD was determined in the intermediate range 1–5 KHz, where pink noise was not evident for the range of bias voltages used here. The parameter S 0 was defined as the mean PSD in this range. The intercept of these two lines was found and iterated to minimize the residuals to the piece-wise fit using custom MATLAB code and produce optimal S 1/ f (1 Hz) values.

---

### Correction [^751b1e37]. JAAD Case Reports (2020). Medium credibility.

[This corrects the article DOI: 10.1016/j.jdcr.2020.06.022.].

---

### Correction [^1ef813eb]. JAAD Case Reports (2021). Medium credibility.

[This corrects the article DOI: 10.1016/j.jdcr.2020.10.038.].

---

### Correction [^deb8704b]. JAAD Case Reports (2020). Medium credibility.

[This corrects the article DOI: 10.1016/j.jdcr.2020.02.038.].

---

### Efficient probabilistic inference in generic neural networks trained with non-probabilistic feedback [^a45b572b]. Nature Communications (2017). Medium credibility.

Training conditions

The "all g " conditions in different tasks were as follows. In cue combination and coordinate transformation tasks, all 25 pairs of the form (g 1, g 2) with g 1, g 2 ∈ {0.25, 0.5, 0.75, 1, 1.25} were presented an equal number of times. In Kalman filtering, g was uniformly drawn between 0.3 and 3 at each time step. In binary categorization, the six gain values, g ∈ {0.148, 0.36, 0.724, 1.128, 1.428, 1.6}, were presented an equal number of times. These gain values were calculated from the mean noise parameter values reported for the human subjects in ref. In causal inference, all 25 pairs of the form (g 1, g 2) with g 1, g 2 ∈ {0.5, 1, 1.5, 2, 2.5} were presented an equal number of times. In stimulus demixing, following, c was uniformly and independently drawn between 2 and 9 for each source. In visual search, g was randomly and independently set to either 0.5 or to 3 for each stimulus.

The "restricted g " conditions in different tasks were as follows. In cue combination and coordinate transformation tasks, the two pairs (g 1, g 2) ∈ {(0.25, 0.25), (1.25, 1.25)} were presented an equal number of times. In Kalman filtering, g was randomly and independently set to either 0.3 or to 3 at each time step. In binary categorization, g was always 1.68. This gain value corresponds to 100% contrast as calculated from the mean noise parameter values for the human subjects reported in ref. In causal inference, pairs of the form (g 1, g 2) ∈ {(0.5, 0.5), (2.5, 2.5)} were presented an equal number of times. In stimulus demixing, c was either set to 2 for all sources or else set to 9 for all sources. Similarly, in visual search, g was either set to 0.5 for all stimuli or else set to 3 for all stimuli.

---

### Correction [^b6b88c7c]. JAAD Case Reports (2021). Medium credibility.

[This corrects the article DOI: 10.1016/j.jdcr.2020.12.019.].

---

### Correction [^d1cb12dc]. JAAD Case Reports (2021). Medium credibility.

[This corrects the article DOI: 10.1016/j.jdcr.2020.11.020.].

---

### Correction [^0b79cba8]. JAAD Case Reports (2019). Medium credibility.

[This corrects the article DOI: 10.1016/j.jdcr.2019.02.034.].

---

### Correction [^1ca77ef3]. JAAD Case Reports (2021). Medium credibility.

[This corrects the article DOI: 10.1016/j.jdcr.2020.12.023.].

---

### Correction [^7a505b51]. JAAD Case Reports (2020). Medium credibility.

[This corrects the article DOI: 10.1016/j.jdcr.2019.11.012.].

---

### Using qualitative comparative analysis and theory of change to unravel the effects of a mental health intervention on service utilisation in Nepal [^9ba2f03b]. BMJ Global Health (2018). Medium credibility.

During the process of applying QCA together with ToC, we learnt several lessons. First, using a conceptual framework, such as ToC, to guide QCA is important to identify the outcome and condition sets as well as providing a theoretical basis for how to treat logical remainders. Second, the number of conditions needs to be kept to around 5–7 to avoid the 'limited diversity problem'which occurs when there are too many conditions in relation to the number of cases which prevent Boolean minimisation of the truth tables and result in causal pathways with many conditions and low coverage. This is similar to the problem in frequentist statistics of including too many explanatory variables in a multivariate linear regression without an adequate sample size. To avoid this problem, we aggregated similar indicators prior to data analysis, for example, proportion of trained prescribers and non-prescribers and the number of trained home-based care workers and FCHVs. Other approaches to avoid limited diversity include (1) a comprehensive approach where all conditions are included iteratively in the analysis until the most parsimonious solution is reached or (2) a two-step approach where an analysis of ths distal conditions is conducted first, followed by the proximal conditions. Third, data calibration is one of the key steps in QCA and should be conducted with a theoretical understanding of the values chosen, that is, that 1 is fully in the set, 0 is fully out of the set and that 0.5 is neither in nor out of the set. The calibration of data should be conducted using thresholds that are meaningful to the programme. Although tools exist in STATA and other software to calibrate continuous variables, the distribution of the resulting calibrated data should be checked to ensure that the interpretation of the data is in line with the theoretical understanding. Because of the iterative nature of QCA, it is possible and may be necessary to recalibrate the data following the analysis.