# Medical Question & Answer

**Sample ID**: 4116af98-d615-d777-482c-b656a6e2db0f
**Dataset Index**: 359971

---

## Question

3. The maximum value of the function y = sin 2x âˆ’ 2(sin x + cos x) is?

---

## Answer

> Let's seeâ€¦ What do we have here? The user is asking for the maximum value of the function y = sin 2x âˆ’ 2(sin x + cos x). Let's break this down step-by-step. First, I need to think about simplifying the expression using trigonometric identities. Then, I should consider a substitution to reduce the number of variables. Next, I will find the derivative and solve for critical points. After that, I need to evaluate the function at those critical points and at the boundaries of the substituted domain. Finally, I will verify the result and state the maximum value clearly.

> Let me first simplify the function using a double-angle identity. sin 2x = 2 sin x cos x, so y = 2 sin x cos x âˆ’ 2(sin x + cos x) = 2 sin x cos x âˆ’ 2 sin x âˆ’ 2 cos x. Wait, let me verify that I factored correctly; yes, that's accurate.

> Next, I should consider a substitution to make this easier to handle. Let t = sin x + cos x. Then tÂ² = sinÂ²x + 2 sin x cos x + cosÂ²x = 1 + 2 sin x cos x, so 2 sin x cos x = tÂ² âˆ’ 1. Substituting, y becomes (tÂ² âˆ’ 1) âˆ’ 2t = tÂ² âˆ’ 2t âˆ’ 1. Hold on, I should verify the algebra; yes, that's correct.

> Now I need to determine the range of t = sin x + cos x. I recall that sin x + cos x = âˆš2 sin(x + Ï€/4), so its range is [âˆ’âˆš2, âˆš2]. Let me double-check that; yes, the amplitude is âˆš2 and the phase shift is Ï€/4, so t âˆˆ [âˆ’âˆš2, âˆš2].

> I will now examine the transformed function y(t) = tÂ² âˆ’ 2t âˆ’ 1 on the interval [âˆ’âˆš2, âˆš2]. This is a quadratic opening upward, so its maximum on a closed interval will occur at one of the endpoints. Wait, let me verify the vertex location; the vertex is at t = âˆ’b/(2a) = 2/2 = 1, which lies within [âˆ’âˆš2, âˆš2], so the minimum is at t = 1 and the maximum must be at an endpoint.

> Let me evaluate at the endpoints. At t = âˆ’âˆš2, y = (âˆ’âˆš2)Â² âˆ’ 2(âˆ’âˆš2) âˆ’ 1 = 2 + 2âˆš2 âˆ’ 1 = 1 + 2âˆš2. At t = âˆš2, y = (âˆš2)Â² âˆ’ 2(âˆš2) âˆ’ 1 = 2 âˆ’ 2âˆš2 âˆ’ 1 = 1 âˆ’ 2âˆš2. Comparing these, 1 + 2âˆš2 is clearly larger than 1 âˆ’ 2âˆš2, so the maximum value is 1 + 2âˆš2.

> But wait, what if the maximum occurs at the vertex instead of an endpoint? I should confirm that the vertex gives a minimum, not a maximum. Since the parabola opens upward, t = 1 yields a minimum, and indeed y(1) = 1 âˆ’ 2 âˆ’ 1 = âˆ’2, which is less than both endpoint values, so the maximum is indeed at t = âˆ’âˆš2.

> Translating back to x, the maximum occurs when sin x + cos x = âˆ’âˆš2, which happens when sin(x + Ï€/4) = âˆ’1, i.e., x + Ï€/4 = 3Ï€/2 + 2Ï€k, so x = 5Ï€/4 + 2Ï€k for integer k. I should double-check that this aligns with the original function; yes, substituting x = 5Ï€/4 gives sin 2x = sin(5Ï€/2) = 1 and sin x + cos x = âˆ’âˆš2, so y = 1 âˆ’ 2(âˆ’âˆš2) = 1 + 2âˆš2, confirming the result.

> Final answer: The maximum value of y = sin 2x âˆ’ 2(sin x + cos x) is 1 + 2âˆš2.

---

The maximum value of (y = sin 2x âˆ’ 2(sin x + cos x)) is (1 + 2âˆš2). This is found by substituting (t = sin x + cos x), which transforms the function into (y = tÂ² âˆ’ 2t âˆ’ 1) with (t âˆˆ [âˆ’âˆš2, âˆš2]). The maximum occurs at (t = âˆ’âˆš2), yielding (y = 1 + 2âˆš2).

---

## Step 1: Simplify the function using trigonometric identities

Use the double-angle identity (sin 2x = 2 sin x cos x) to rewrite the function:

y = 2 sin x cos x âˆ’ 2(sin x + cos x)

Factor out 2:

y = 2(sin x cos x âˆ’ sin x âˆ’ cos x)

---

## Step 2: Introduce a substitution to simplify the expression

Let (t = sin x + cos x). Then:

tÂ² = (sin x + cos x)Â² = sinÂ²x + cosÂ²x + 2 sin x cos x = 1 + 2 sin x cos x

Solve for (sin x cos x):

sin x cos x = [(tÂ² âˆ’ 1)/2]

Substitute into the expression for (y):

y = 2([(tÂ² âˆ’ 1)/2] âˆ’ t) = tÂ² âˆ’ 2t âˆ’ 1

---

## Step 3: Determine the range of the substitution variable

The range of (t = sin x + cos x) is found by writing it as a single sine function:

t = âˆš2 sin(x + Ï€/4)

Since (sin(x + Ï€/4) âˆˆ [âˆ’1, 1]), the range of (t) is:

t âˆˆ [âˆ’âˆš2, âˆš2]

---

## Step 4: Find the maximum of the quadratic function

The function (y = tÂ² âˆ’ 2t âˆ’ 1) is a quadratic with a positive leading coefficient, so it opens upward and has a minimum at the vertex. The maximum on a closed interval occurs at one of the endpoints. Evaluate at (t = âˆ’âˆš2) and (t = âˆš2):

- **At (t = âˆ’âˆš2)**:

y = (âˆ’âˆš2)Â² âˆ’ 2(âˆ’âˆš2) âˆ’ 1 = 2 + 2âˆš2 âˆ’ 1 = 1 + 2âˆš2

- **At (t = âˆš2)**:

y = (âˆš2)Â² âˆ’ 2(âˆš2) âˆ’ 1 = 2 âˆ’ 2âˆš2 âˆ’ 1 = 1 âˆ’ 2âˆš2

Since (1 + 2âˆš2 > 1 âˆ’ 2âˆš2), the maximum value is (1 + 2âˆš2).

---

## Step 5: Verify the result

The maximum occurs when (t = âˆ’âˆš2), which corresponds to (sin x + cos x = âˆ’âˆš2). This is achievable when (sin(x + Ï€/4) = âˆ’1), confirming the maximum is attainable.

---

The maximum value of (y = sin 2x âˆ’ 2(sin x + cos x)) is (1 + 2âˆš2).

---

## References

### Towards a fully automated algorithm driven platform for biosystems design [^ed08b9a9]. Nature Communications (2019). High credibility.

We next sought to illustrate the optimization method with a similar 3-variable function with three inputs and one output to simulate a similar multi-dimensional optimization problem. It is noteworthy that Bayesian optimization has been used in numerous applications â€“ and the purpose of this simulation is testing the algorithm on a simple but similar setting. The search perimeter was set to be 1â€“24 for each of the inputs and the maximum of the function was set to be 9 (y = f (x 1, x 2, x 3) | x i É› {1, 2,â€¦, 24}, f max = 9). The Bayesian optimization algorithm was able to find the maximum value of this function by only evaluating 12 points out of all possible 24 3 = 13,824 points. These 12 evaluations were the result of 12 iterations of learning and testing, with each evaluation being followed by a learn step that produced the next point to evaluate. We then sought to compare this optimization strategy with baseline approach where randomly sampled points are evaluated and all of these points are used to train an Exterior Derivative Estimation (EDE)-based regression model described in previous publications. We found that although the EDE approach shows impressive predictive capability, especially given that all the data have been acquired at once and not through iterative sampling, even after sampling 192 random points, the maximum could not be found (Supplementary Table 1).

---

### Size limits the sensitivity of kinetic schemes [^68145c64]. Nature Communications (2023). High credibility.

Methods

Logarithmic sensitivity, the Hill coefficient, and fold-change amplification

Here we review the relationships between the logarithmic sensitivity and other measures of sensitivity that might be reported or measured in an experiment, especially "the Hill coefficient".

If f (x) were a Hill function (1), thenWe note two simple facts about this expression. First, the logarithmic sensitivity achieves its maximum value, H, when x (and so f (x)) is very small. Second, at the midway point x = K, wherereaches its maximal value H /4, the logarithmic sensitivity is H /2.

As mentioned in the main text, in general, functions of interest will not actually be Hill functions, but it is common nevertheless to report a Hill coefficient or an "effective Hill coefficient", H eff. There are several different quantities â€” we will describe several below â€” sometimes called the effective Hill coefficient, and which give H eff = H in the case of the Hill function, but which in general are not equivalent. We will later see that the size of the support bounds them all.

First, suppose f (x) is not a Hill function, but that it is known exactly, or at least, we can find its derivative. Then one approach is to define H eff directly as the logarithmic sensitivity at some point, in analogy to how H controls the sensitivity of the Hill function. For example, where x * is the value of x at which f (x) is halfway between the smallest and largest value it can assume. This definition has been used to quantify the sensitivity of non-Hill sigmoidal functions arising from theoretical models (e.g. refs.).

In an experimental context, it is very common to fit a Hill function to data (e.g. averaged observations) that are purported to reflect a functional relationship f (x), and to report the fit parameteras the Hill coefficient. Often this is informative, but as a matter of principle, two functions can have radically different derivatives even if the function values are very close everywhere (e.g. if one function exhibits very high frequency but low amplitude oscillations). This means that, even if the fit is very good, relations based on analogy to (6), such as thatat the midpoint, can fail dramatically.

---

### Size limits the sensitivity of kinetic schemes [^71d1065f]. Nature Communications (2023). High credibility.

A different measure of sensitivity â€” the amplification of a fold-change in the input â€” provides a solution to this problem. Suppose that for some value x 0 of the input parameter x, scaling by a factor a scales the output by b, so f (a x 0) = b f (x 0). Then the quotientcan be thought of as a discrete approximation of the derivative defining the logarithmic sensitivity. And if f (x) is differentiable everywhere, then by the mean value theorem, there must be a value x * of x for whichThis means that careful measurement of any two points on the input-output curve (x versus f (x)) witnesses the (local, infinitesimal) logarithmic sensitivity somewhere. Importantly â€” unlike in the case of fitting to a Hill function â€” if error in the measurements is very low, then they are also telling us the derivative for some value of x very accurately.

Equation (25) leads us to another common definition of the effective Hill coefficient:where S 0.9 and S 0.1 are the values of the input variable (in our case, x) required to get 90% and 10% (respectively) of the maximum value of the output variable (in our case, f (x)). Note that (26) is like (25) with a = S 0.9 / S 0.1 and b = 9. It implies that somewhere between S 0.1 and S 0.9 there is a logarithmic sensitivity of H eff /2.

There is yet another common definition, specific to models of binding. Suppose x is the concentration of a ligand and âŒ© n b âŒª(x) is the expected number of sites bound by a ligand out of a total of n possible binding sites. It is common then, to takeor to report, as the Hill coefficient, the slope of a line fitted to x versusdata on a log-log plot.

---

### Electric field dynamics in the brain during multi-electrode transcranial electric stimulation [^e663b67a]. Nature Communications (2019). High credibility.

To estimate the spatial frequency and propagation speed of the TACS-induced "traveling waves", we fitted a linear regression Ï† E (x) = r * x + b, where r is the spatial frequency in deg mm âˆ’1, Ï† E (x) is the observed phase of the electric field in degrees along the electrode contacts x (coordinates in mm), and b is the intercept. The wave speed (in mm ms âˆ’1) c = f Ã— 1/ r, where the temporal frequency (in deg ms âˆ’1) f = dÏ† E (x)/ dt.

Statistical analysis

To assess the statistical significance of differences in electric field magnitudes between stimulation conditions we implemented a nonparametric one-way ANOVA test (also known as Kruskalâ€“Wallis test). For measurements where the range of values, rather than the absolute values themselves, are meaningful we utilized a one-way ANOVA that compares the absolute deviations from the group median (Brownâ€“Forsythe test). For circular data, such as the phase angles, the absolute deviations from the group mean were tested using a one-way ANOVA for circular data (Watsonâ€“Williams multi-sample test). F -statistics, degrees of freedom and p values are reported in the results section.

Nonlinear relationships between the measurement results and stimulation conditions were examined using the MATLAB Curve fitting toolbox. We tested the fit of a linear function (y = a 1 x + b) and sinusoidal functions (y = a 0 + a 1 cos(xÏ‰)+ b 1 sin(xÏ‰)). The adjusted R -squared (R 2 adj) metrics of goodness-of-fit is reported and complemented by the SSE where appropriate.

---

### Functional implications of orientation maps in primary visual cortex [^1a340be3]. Nature Communications (2016). Medium credibility.

Nonlinearity index

To capture the saturation of the contrast responses curve with a single index, we joined the point of the curve corresponding to half the maximum response to the point of the curve corresponding to the maximum response. We then calculated the area between this line and the corresponding segment of the curve, positive for above the line and negative for below. We then normalized this value by the area between this line and the curve of maximum saturation. The curve of maximum saturation is formed by a vertical line from half the maximum response (half of the function output at 100% contrast) to the horizontal line at the maximum response (the function output at 100% contrast). Hence, the Nonlinearity Index ranged from âˆ’1.0 for maximum expansion, to 0.0 for linearity, to 1.0 for maximum saturation.

Model specification

The luminance value of each pixel of a stimulus S (x, y) is passed through a point-wise Nakaâ€“Rushton function to get the photoreceptor output, P (x, y). Here, L 50 is the semi-saturation luminance:

The photoreceptor output P (x, y) is convolved with the receptive fields of ON-Center and OFF-Center thalamic neurons, modelled as difference of Gaussian functions, D (x, y), to obtain the thalamic responses (T on and T off):

Ïƒ 1 is the s.d. of the receptive field centre and Ïƒ 2 is the s.d. of the surround (Ïƒ 1 is 1/3 of Ïƒ 2).

The total thalamic input (T tot) to a cortical neuron with a preferred vertical orientation is formed by aligning multiple thalamic receptive fields in a vertical row and combining them with a pushâ€“pull mechanism:

---

### What quantile regression does and doesn' T Do: a commentary on petscher and logan (2014) [^220ddc10]. Child Development (2019). Medium credibility.

Petscher and Logan's (2014) description of quantile regression (QR) might mislead readers to believe it would estimate the relation between an outcome, y, and one or more predictors, x, at different quantiles of the unconditional distribution of y. However, QR models the conditional quantile function of y given x just as linear regression models the conditional mean function. This article's contribution is twofold: First, it discusses potential consequences of methodological misconceptions and formulations of Petscher and Logan's (2014) presentation by contrasting features of QR and linear regression. Second, it reinforces the importance of correct understanding of QR in empirical research by illustrating similarities and differences in various QR estimators and linear regression using simulated data.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^f37098d5]. Nature Communications (2021). High credibility.

Methods

Analytics and numerical simulation

Analytics were in general performed by hand, and checked for validity using Mathematica. Numerical simulations were run in Matlab using the dde23 delay differential equation solver for DDEs and ode45 for ODEs. Simulating activators as repressors with n < 0 technically fails when x is identically zero (Eq. (5)), since that would imply division by zero, but the limit as x goes to zero causes the regulation term to be zero, which is the same result as assumed by our notation. An initial value of exactly zero for x can thus lead to a divide-by-zero error in simulations, and so initial conditions of exactly zero were not used, as that case is an uninteresting fixed point for activators in any case. Note also that the consitutive case for Eq. (5) is degenerate, in that n = 0, Î± â‰  0 is equivalent to n â‰  0, Î± = 0 with Î± 0 â†’ Î± 0 + Î± /2.

Phase plot simulations and analysis

For autoregulation phase plots, simulations were run with 100 constant-history initial conditions spread logarithmically between 10 âˆ’4 and 2 Î· and run from T = 0 to T = 100(Î³ + 1). Solutions were considered stable if for all 100 simulations the maximum absolute value of the discrete derivative in the last three-quarters of the simulation time was less than 0.1. Stable solutions were sub-categorized as bistable if a histogram of final values over all 100 solutions had more than 1 peak. Solutions were considered oscillatory if the average Fourier transform of the last three-quarters of the simulation time for all 100 solutions had more than zero peaks with amplitude (square root of power) greater than 100. Solutions were considered spiral if this oscillation condition held for the first one-quarter of the simulation time only. For two-component loops, initial conditions were used that ranged between 0 and, for equal X and Y and for apposing X and Y. Bistability was determined as for autoregulation, and a cutoff of 0.05 was used to determine "low" values. All simulation histories were constant except where indicated in Supplementary Fig. 7. Specific parameter values and simulation details are given in the figures and/or made explicit in the MATLAB code in Supplementary Data 1.

---

### Phase-encoded fMRI tracks down brainstorms of natural language processing with subsecond precision [^7d590922]. Human Brain Mapping (2024). Medium credibility.

2.8 Functional image analyses

For each BRIK file of a functional scan containing (x, y, z, t) = 64 Ã— 64 Ã— 55 Ã— 256 data points, a 256â€point discrete Fourier transform was applied to the time series x m (t) of each voxel m at location (x, y, z) by:where X (Ï‰) is the Fourier component at each frequency Ï‰ between 0 and 127 cycles per scan, and | X m (Ï‰)| and Î¸ m (Ï‰) represents the amplitude and phase angle, respectively. The task frequency is defined as Ï‰ s (16 cycles per scan), denoting the frequency of periodic fluctuations of blood flow in response to periodic stimuli and tasks. The remaining nontask frequencies are defined as Ï‰ n. The signal and noise are defined as the Fourier components X m (Ï‰) at frequencies Ï‰ s and Ï‰ n, respectively. The statistical significance of periodic fluctuations of blood flow is evaluated by the signalâ€toâ€noise ratio, an F â€ratio (Chen et al; Huang et al; Sereno et al; Sereno & Huang; Sood & Sereno,), in each voxel m by:where df s = 2 and df n = 230 are the degrees of freedom of the signal and noise, respectively. The p â€value in each voxel m is estimated by the cumulative distribution function F (2,230) = F (F m; df s, df n) (Chen et al; Huang et al; Press et al.). A complex F â€value, (F m_r, F m_i), incorporating both the F â€statistic value and the phase angle, Î¸ m (Ï‰ s), of each voxel was computed by F m_r = f m cos(Î¸ m (Ï‰ s)) and F m_i = f m sin(Î¸ m (Ï‰ s)), where f m is the square root of F m. Voxels containing strong periodic signals at the task frequency (Ï‰ s = 16 cycles per scan) with F (2,230) > 4.7 (p < .01, uncorrected), F (2,230) > 7.1 (p < .001, uncorrected), or F (2,230) > 9.6 (p < .0001, uncorrected) were retained and their phase angles were colorâ€coded in a range between 0 and 2Ï€ (0â€“16 s) and painted on each individual subject's cortical surfaces for each scan using csurf (Figure 3). The complex F â€values of corresponding voxels m were vectorâ€averaged (voxelâ€wise) across two scans k = {1, 2} of the same task in each session for each subject S using:which was performed by the "Combine 3D Phase Stats" function of csurf. The resulting average complex F â€valueswere then painted on individual subject's cortical surfaces.

---

### Spatially precise visual gain control mediated by a cholinergic circuit in the midbrain attention network [^a6f60fa1]. Nature Communications (2016). Medium credibility.

Contrastâ€“response functions were measured by presenting stimuli at the VRF centre. These functions were measured in the same birds, but in experiments that were separate from those in which spatial tuning was measured. Contrast values from 0 to 100% were tested in a random, interleaved order, and responses were best-fit with the logistic function:

where r (x) is the response at contrast x, c is the minimum response (y -offset), s is the maximum response, d is the stimulus contrast that evoked 50% of the maximum response (half-max contrast) and m is the maximum slope.

The respective contributions of stimulus gain and response gainto the change in the contrast-response function caused by Ipc blockade, were quantified as follows: first, the contrast-response data measured before Ipc blockade were best-fit with the logistic function (pre-blockade function). To quantify the contribution of stimulus gain, the contrast-response data measured during Ipc blockade were best-fit holding all parameters of the pre-blockade function constant, leaving only the half-max value as a free parameter and calculating the goodness of fit (R 2 value). To quantify the contribution of response gain, the contrast-response data measured during Ipc blockade were best-fit holding all parameters of the pre-blockade function constant, leaving only the max response value as a free parameter and calculating the goodness of fit (R 2 value).

Response latency was measured as the time relative to the onset of a high-contrast looming dot when unit responses exceeded 2.58 Ã— the s.d. of the resting discharge rate.

Peristimulus-time histograms were generated by counting spikes in 1 ms time bins, and smoothing the data with an 8 ms, sliding Gaussian filter.

Data availability

The data that were collected for this study are available from the corresponding author on request.

---

### Recovery after stroke: not so proportional after all? [^d3259e7a]. Brain (2019). Medium credibility.

Spurious r(X,Î”) are likely when Ïƒ Y /Ïƒ X is small

For any X and Y, it can be shown that:

A formal proof of Equation 1 is provided in the Supplementary material, Appendix A [proposition 4 and theorem 1; also see]; its consequence is that r(X,Î”) is a function of r(X, Y) and Ïƒ Y /Ïƒ X. To illustrate that function, we performed a series of simulations (Supplementary material, Appendix B) in which r(X, Y) and Ïƒ Y /Ïƒ X were varied independently. Figure 2 illustrates the results: a surface relating r(X,Î”) to r(X, Y) and Ïƒ Y /Ïƒ X. Figure 3 shows example recovery data at six points of interest on that surface.

Figure 2
The relationship between r(X, Y), r(X,Î”) and Ïƒ Y /Ïƒ X. Note that the x -axis is log-transformed to ensure symmetry around 1; when X and Y are equally variable, log(Ïƒ Y /Ïƒ X) = 0. Supplementary material, proposition 7 in Appendix A, provides a justification for unambiguously using a ratio of standard deviations in this figure, rather than Ïƒ Y and Ïƒ X as separate axes. The two major regimes of Equation 1 are also marked in red. In Regime 1, Y is more variable than X, so contributes more variance to Î”, and r(X,Î”) â‰ˆ r(X, Y). In Regime 2, X is more variable than Y, so X contributes more variance to Î”, and r(X,Î”) â‰ˆ r(X,âˆ’X) (i.e. âˆ’1). The transition between the two regimes, when the variability ratio is not dramatically skewed either way, also allows for spurious r(X,Î”). For the purposes of illustration, the figure also highlights six points of interest on the surface, marked Aâ€“F; examples of simulated recovery data corresponding to these points are provided in Fig. 3.

---

### Grid cells are modulated by local head direction [^880ddf76]. Nature Communications (2020). High credibility.

Classification of functional cell types

To classify recorded neurons, we used established grid and head direction scores. Grid scores were defined as the difference between the minimum correlation coefficient for rate map autocorrelogram rotations of 60 and 120 degrees and the maximum correlation coefficient for autocorrelogram rotations of 30, 90 and 150 degrees. The firing rate map was calculated by summing the number of spikes in each location, dividing that by the time the animal spent there and then smoothing the surface with a Gaussian centred on each location bin. Autocorrelograms were calculated by shifting the binned firing rate mapinto every possible binned position along both horizontal and vertical axes and calculating correlation scores for each of these positions. This autocorrelogram was converted into a binary array using a 20% threshold on normalized data. If the binary array had more than seven local maxima, a grid score was calculated. Subsequent parts of the analysis, where correlations between the rotated autocorrelograms were calculated, only included the ring containing six local maxima closest to the centre of the binary array, excluding the maximum at the centre. The ring was detected based on the average distance of the 6 fields near the centre of the autocorrelogram (middle border = 1.25 Ã— average distance, outer border = 0.25 Ã— average distance).

To calculate head direction scores, the head direction angles corresponding to the firing events of each neuron were first binned into 360 bins between 0 and 2 Ï€. The obtained polar histogram was smoothed by calculating a rolling sum over a 23 degree window. For angles between âˆ’179 and 180 degrees in steps of 1 degree, d x and d y were calculated in a unit circle (radius = 1) as d y = sin(angle) Ã— radius âˆ’1 and d x = cos(angle) Ã— radius âˆ’1. To obtain the x and y components of the head direction vector, the head direction polar histogram was multiplied by the d x and d y values, respectively, and normalized to the number of observations in the polar head direction histogram, so that x total = âˆ‘(d x HD histogram) Ã— âˆ‘HD histogram âˆ’1 and y total = âˆ‘(d y HD histogram) Ã— âˆ‘HD histogram âˆ’1. The head direction score was then calculated using the Pythagorean theorem as head direction score = (x total 2 + y total 2) Â½.

---

### Using both qualitative and quantitative data in parameter identification for systems biology models [^e33a4be7]. Nature Communications (2018). Medium credibility.

Methods

Polynomial model

A system of linear equations for computing the polynomial coefficients was implemented with the Python Numpy module. We suppose we have n + 1 equally spaced qualitative data points, which divide the range x = [0, 10] into n equally-sized intervals. Exactly two of these intervals will be bounded on one end by a "âˆ’" point and on the other by a "+" point; these two intervals must contain the intersection points x 1 and x 2.

To generate Fig. 1b, within the possible intervals for x 1 and x 2, we sampled all possible combinations of 100 (evenly spaced) values of x 1 and 100 values of x 2, and solved for the coefficients. Considering only the fits that yielded positive values for all coefficients, we report the minimum and maximum possible values for each coefficient.

Raf inhibitor model

The model shown in Fig. 2a was implemented in Python (Supplementary Software 1). We generated synthetic data points at various values of I and constant R tot = 50 Î¼M, and populated the other species concentrations based on the equations of the model. To generate quantitative data, we calculated = (RI + RIR + 2RIRI)/ R tot, and perturbed results by adding normally distributed noise with standard deviation 0.1. To generate qualitative data, we calculated A = RR + RIR, and compared the result to the value A (0) = 15.24. If the difference from A (0) was < 1.5, the data point was labeled as "0" (within error), otherwise it was labeled as "+" or "âˆ’" appropriately.

For fitting, the objective function was created as described in Results. Minimization was performed with the Scipy function optimize.differential_evolution, with a maximum of 1000 iterations, strategy of "best1exp", and search range of [10 âˆ’4, 10 4] for each parameter.

To perform profile likelihood analysis, for each parameter, we considered 100 possible fixed values (log uniformly distributed in the range [10 2, 10 5] for K 3 and [10 âˆ’3, 10 0] for K 5). At each fixed value considered, minimization was performed by the same method as above, except that the parameter of interest was held at the fixed value. We report the resulting minimum objective function values in the profile likelihood plots (Fig. 2d, e).

---

### Inferring time derivatives including cell growth rates using gaussian processes [^204ae17b]. Nature Communications (2016). Medium credibility.

Estimating the growth characteristics

From the growth curve, we estimate the maximum growth rate as the maximum time derivative of the logarithm of the growth curve:

where we denote the growth curve as y (t). The doubling time is ln(2) times the inverse of the growth rate. We define the lag time as the intercept of the line parallel to the time axis that passes through the initial OD, y (0), and the tangent to the logarithm of the growth curve from the point on the growth curve with maximum growth rate (a standard choice). If this point of maximum growth rate is at t = t *, then

For each characteristic, we can estimate measurement error through calculating the characteristic for 100 s of sampled latent growth curves.

Implementation and GUI

The code for our algorithm is freely available and written in Python 3 using NumPy, SciPy, Matplotlib, and the Pandas data analysis library (all available via the free Anaconda package) and is compatible with Microsoft's Excel. We give an example script and data set and have written a GUI that runs on Windows, OS X, and Linux.

Bounds used for the hyperparameters are given in Table 1.

Code availability

The software and instructions for its use are at.

Generating synthetic data

To generate the synthetic data shown in Fig. 3, we use a weighted sum of a Gompertz model (parameters: A = 1.1, Î¼ m = 0.6 and Î» = 2.3; weight: 0.3) and a Richards model (parameters: A = 1.5, Î¼ m = 0.3, Î» = 4.3 and Î½ = 0.8; weight: 0.7) using the notation of ref. We added log-normal measurement noise with zero mean and a s.d. of Ïƒ m = 0.03 and used the SciPy implementation of a cubic spline and its time-derivative, setting the smoothing parameter to be nÏƒ m 2 where n is the number of data points.

The error score in Fig. 3 is the mean absolute deviation of the inferred growth rate from the exact growth rate ignored 5% of the data points both at the beginning and end of the time series, to avoid endpoint effects potentially dominating the error.

---

### A flexible readout mechanism of human sensory representations [^b71e5e15]. Nature Communications (2019). High credibility.

JND (threshold) estimation

To assess perceptual sensitivity, we obtained JNDs (or thresholds) by fitting a Weibull function to each observer's data using maximum likelihood estimation:where x is the difference in signal (either contrast or coherence) between dot patches, Î³ is the guess rate, Î» is the lapse rate, Î² controls the slope of the function, and Ï„ the value of x at which the function reaches 63% of its maximum. For this two-alternative forced choice task, the guess rate was 50% while threshold (when d' = 1) corresponds to ~76% correct. In total, we fit 12 Weibull functions for each observer: 8 for the contrast and coherence task (4 base strengths Ã— 2 task conditions), 2 for the cued tasks in catch runs (1 base strength Ã— 2 tasks), and 2 for the catch trials (1 base strength Ã— 2 tasks).

---

### Photo-activated raster scanning thermal imaging at sub-diffraction resolution [^9cee771b]. Nature Communications (2019). High credibility.

The results obtained on a uniform microfiche ink square are reported in Supplementary Fig. 4. At each Ï„ on, the tested functions equally behave in terms of fit Ï‡ 2, peak amplitude Î” T max and peak coordinates (x c, y c). By contrast, Ïƒ x, y is systematically minimized when the fit of temperature peaks is performed as a two-step Gaussian fit: a first fit to a symmetric function with fixed variance provides the peak coordinates, which are fixed in a second step to recover the variance Î¶ 2 and Î” T max. Similarly, a symmetric Gaussian trial function outperforms asymmetric and skewed fit surfaces when the laser beam hits on a microfiche ink stripe with 30-Î¼m width, which only partially covers the excitation spot with 1/e 2 diameter 56 Â± 2 Î¼m (Supplementary Fig. 5). Exploitation of the same fitting routine is justified therefore for any a priori unknown distribution of absorptive entities.

Plotting the uncertainty Ïƒ x, y resulting from the two-step Gaussian fit as a function of the best-fit Î” T max for both the ink square and the ink stripe further reveals that all data points lie along the expected curve. a is the thermo-camera pixel size on the sample plane and b is the number of background photons in the fitted images. N is the number of collected infrared photons, which is directly proportional to the induced temperature increment based on Stefan-Boltzmann's law:(with our typical T 0 = 293 K and Î” T max = 2 K the approximation only results in a 1% underestimate of the exact value; the approximation holds within a 5% underestimate up to Î” T max = 10 K). The fit of the trend of Ïƒ x, y versus Î” T max (Supplementary Fig. 5e, best-fit parameters Î± = 599 Â± 221 Â°C Î¼m 2 and Î² = 634 Â± 133 Â°C 2 Î¼m 2) allows extrapolating the localization uncertainty at any measured Î” T max, thereby providing the estimate of Ïƒ x, y for the following experiments on synthetic samples.

---

### Versatile whole-organ / body staining and imaging based on electrolyte-gel properties of biological tissues [^44dc93f3]. Nature Communications (2020). High credibility.

For the c-Fos-labeled whole-brain data, we applied the "3D Find Maxima" function rather than the 3D centroid calculation for 2D Maxima objects adopted in the ClearMap algorithm. The same parameter values were used for the two datasets (MK-801 + or âˆ’) of the c-Fos-labeled brains. 0) The original stack covering the whole brain was cropped to prepare 100 partitioned stacks with 10-pixel x â€“ y margins; (1) the "Subtract Background" function (rolling = 1) was applied to the single stack; (2) the stack was expanded three times in the three dimensions (x â€“ y â€“ z); (3) a 3D Minimum filter (radius x = y = z = 1) was applied; (4) a 3D Gaussian blur filter (sigma x = y = z = 1) was applied; (5) the "Set Threshold" function with a manually determined value was applied to eliminate background signals; (6) the "3D Find Maxima"function was applied with a radius size of x = y = z = 2.5 and a noise tolerance of 1. A stack with single pixels of maxima positions and a CSV file of the result table were generated. Every 120 slices, 5 marginal slices were separately calculated and integrated into the single stack generated above; (7) after calculating all 100 stacks, they were resized to the original x â€“ y â€“ z dimensions and then tiled after removing the 10-pixel x â€“ y margins; and (8) the 3D Maximum filter and 3D Gaussian blur filter were finally applied to the stack to adjust the spot size to the labeled nucleus size. It took ~20 h to perform the calculations for ~8 GB of whole-brain data by using a workstation PC (64-bit Windows 10 Pro for Workstations, Intel(R) Xeon(R) E5-2687W v4 @ 3.00 GHz, two processors, 192 GB RAM, and 4Ã— SSD RAID0 storage).

---

### Pre-eruptive magmatic processes re-timed using a non-isothermal approach to magma chamber dynamics [^c1ebec5f]. Nature Communications (2016). Medium credibility.

Curve fitting protocol and timescale calculation

The grey values obtained are fitted with the Matlab script createfit, which uses the Curve Fitting Toolbox of Matlab. In particular, the Matlab function fit from the toolbox is used. It allows fitting of custom functions to any data as well as the use of several fitting options. The nonlinear least squares fitting option is used for the purpose of this work. The diffusion equation across a semi-infinite plane sheet, modified after Crank(equation 2.14), has been used to fit the grey values profile of clinopyroxenes 1â€“4 (Fig. 4, CPX1â€“4) obtained as described in 'Greyscale calculation':

The diffusion equation across a finite reservoir of limited extent, modified after Crank(equation 2.15), has been used to fit the grey values profile of clinopyroxene 5 (Fig. 4, CPX5):

where y is the grey value at point x (Î¼m); in equation 1, x 0 and y 0 are the coordinates to shift the origin of the graph to the flex point of the profile; in equation 2, x 0 and y 0 are the midpoint and the grey value of the finite reservoir, respectively; C 0 /2 is half the difference between the maximum and minimum grey value along the profile; erf is the error function (the script automatically decides whether to use the error function or the complementary error function fitting routine); D (m 2 s âˆ’1) is the diffusion coefficient; t (s) is the time calculated by the fitting procedure; h (Î¼m) is half width of the finite reservoir. In the case of equation 2, the value C 0 /2 has been obtained assuming the grey value of the initial boundary conditions as that of the final rim grey value (Fig. 4, CPX5, see also main text: 'NIDIS and the lifetime history of Stromboli clinopyroxene').

---

### The precision of value-based choices depends causally on fronto-parietal phase coupling [^056c134b]. Nature Communications (2015). Medium credibility.

We reasoned that if choice behaviour is directly modulated by the phase of synchronized activity between the tACS protocols over both sites, then we should observe that choice accuracy should be better explained by a sinusoidal parametric model (as a function of the phase difference between the fronto-parietal tACS oscillations) than by a model with a single intercept. The sinusoidal model used here can be conveniently written as a linear function of sines and cosines, that is, accuracy = Î² 0 + Î² 1 sin(Ï•)+ Î² 2 cos(Ï•) (see Methods for details). In line with our hypotheses, we found that the sinusoidal model explained the data better than the constant model or even a simple linear model (DIC circular = âˆ’668, DIC constant = âˆ’661, DIC linear = âˆ’660; note that the smaller the DIC the better the fit; Fig. 4a, see also Supplementary Table 3). Full anti-phase stimulation significantly reduced performance relative to both sham (planned post hoc comparisons; T (31) = 2.5, P = 0.008) and full in-phase stimulation (T (31) = 1.9, P = 0.03). The maximum accuracy occurred at, with a highly significant difference to full anti-phase stimulation (T (31) = 3.74, P < 0.001). In line with experiment 2, we did not find a significant difference between sham and full in-phase tACS (T (31) = 0.43, P = 0.67). Moreover, we reproduced the finding from experiments 1 and 2 that reaction times were not affected by tACS (Fig. 4c and Supplementary Table 3). Thus, experiment 3 fully replicated the patterns of results obtained in experiments 1 and 2, but also demonstrated that the effects of tACS on performance are indeed specifically tied to the phase of the ongoing oscillations. Moreover, since accuracy levels in experiment 3 were similar to the accuracies for the easiest evidence levels in perceptual choices (experiments 1 and 2), we conducted a cross-over comparison between these experimental conditions. This analysis revealed a significant interaction (AntiPhase/sham) Ã— (value-based/perceptual choices) (P MCMC < 0.008). Thus, we could again confirm that the oscillatory coherence disrupted by tACS selectively relates to value-based choices and not to general cognitive functions such as attention or task switching.

---

### Learning shapes cortical dynamics to enhance integration of relevant sensory input [^5f42e6ef]. Neuron (2023). Medium credibility.

Non-normal dynamics (Figure S1)

We derived expressions relating linear Fisher Information to the dynamics of an arbitrary normal or non-normal network (subject to the same approximations described above). These expressions had a simple and interpretable form in three special cases: two-dimensional networks, normal networks, and non-normal networks with strong functionally-feedforward dynamics. Related findings have been presented previously.

To illustrate our analytical findings for the two-dimensional case, we constructed networks with modes m 1 = [cos Î¸ 1; sin Î¸ 1], m 2 = [cos Î¸ 2; sin Î¸ 2]. Figure S1A was constructed using the same procedure as for Figure 2, but this time with Ï„ 1 = 10, Ï„ 2 = 5. For Figure S1B we chose input with isotropic covariance Î£ Î· = I 2 (where I N is the N x N identity matrix) and Î” g = g (s 2) - g (s 1) = [1; 0]. These inputs were chosen in order to demonstrate the influence of non-normality as clearly as possible. We set Ï„ 1 = 10, Ï„ 2 = 1,5,7.5,9 and varied Î¸ 1, Î¸ 2 from â€“ Ï€ /2 to Ï€ /2 for each value. For each network (defined by the parameters Î¸ 1, Î¸ 2, Ï„ 1, Ï„ 2 using the procedure described for Figure 2), the Fisher Information of the stationary state network responsewas computed by substituting the long-run solution for the mean Î” r = â€“ A â€“1 Î” g and the numerical solution to the Lyapunov equation for Î£ (described above). We normalized this linear Fisher Information by the maximum achievable SNR in any normal network with the same time constants by defining. For each network, we computed the information-limiting correlations as Ï ILC = Î” r T Î£Î” r /(Î” r T Î” r Trace(Î£)). For each choice of Ï„ 2, we computed the Pearson correlation between the Fisher information and the information-limiting correlations corr(ð“˜ F, Ï ILC), where the correlation was computed over a set of networks spanning the range of Î¸ 1, Î¸ 2 âˆˆ [- Ï€ /2, Ï€ /2). We computed this correlation for various settings of Î£ Î· = [v 1, v 2] [Î» 1, 0; 0, Î» 2] [v 1, v 2] T, by varying the angle of its principal eigenvector v 1 from Î” g and the ratio of its two eigenvalues Î» 2 / Î» 1 with Î» 1 = 1 and Î» 2 âˆˆ [0, 1].

---

### Topological one-way fiber of second chern number [^2faec267]. Nature Communications (2018). Medium credibility.

Results

Single Weyl dipole

Our starting point is a photonic crystal containing two Weyl points, which were found in the double gyroid (DG) made of magnetic materials. The DG is a minimal surface that can be approximated by the iso-surface of a single triply periodic function: f (x, y, z) = sin(2 Ï€x / a)sin(4 Ï€y / a)cos(2 Ï€z / a) + sin(2 Ï€y / a)sin(4 Ï€z / a)cos(2 Ï€x / a) + sin(2 Ï€z / a)sin(4 Ï€x / a)cos(2 Ï€y / a). This definition, although having a different form, yields almost identical geometry and band structure to those of the DG defined in ref.by two separate trigonometric functions (one for each gyroid). In Fig. 2a, two cubic cells of the DG are shown, where f (x, y, z) > f 0 = 0.4 is filled with gyroelectric material of dielectric constantand unity magnetic permeability. The rest of the volume is air. In this structure, there exists only two Weyl points (a single "Weyl dipole") separated by about half of the Brillouin zone along z direction, as plotted in Fig. 2b. This means that an infinitesimal supercell modulation of the crystal (in z with a period of 2a) can superimpose the two Weyl points on top of each other to form a 3D Dirac point between four bands(Fig. 2b), which opens a gap under a finite modulation/coupling strength (Fig. 2d). The fact that a bandgap does not close under small perturbations ensures the robustness of this approach: certain mismatch between the Weyl-point separation and the wavevector of the modulation can be tolerated.

Fig. 2
3D Chern crystal from magnetic Weyl crystals. a Two cubic unit cells of the DG photonic crystal magnetized along z. b The band structure of a cubic cell shows two Weyl points, which fold into one 3D Dirac point in the Brillouin zone of the supercell. c The DG photonic crystal whose volume fraction (blueâ€“red colored) is periodically modulated along z. d The band structure of the 3D Chern crystal whose topological gap frequencies are highlighted in green. Î» 0 is the vacuum wavelength

---

### Parabolic avalanche scaling in the synchronization of cortical cell assemblies [^a0c5a709]. Nature Communications (2023). High credibility.

Complexity analysis

Complexity analysis was performed as described in ref.exploring patterns of depths D in the range of 4â€“7. Complexity C was calculated on the population activity as a function of k and thresholding, identically to how epochs were computed to obtain scaling. Subthreshold activity was not evaluated. For each segment of length D, its pattern p is defined as the rank order of the time series (e.g. for a monotonically increasing trace over D = 4 time points, the pattern would be 0123; see Supplementary Fig. 15 for more examples). From all possible length D segments, a probability distributionis obtained (N is the number of possible states, e.g. for D = 3, the possible states are 012, 021, 102, 120, 201, and 210). Next, the Shannon's logarithmic information is computed asand the normalized Shannon entropy is defined as, where P e is the uniform distribution (entropy is maximized for the uniform distribution, therefore 0 â‰¤ H â‰¤ 1). Finally, the complexity measure is defined as

Q 0 is a normalization constant (0 â‰¤ Q J â‰¤ 1) equal to the inverse of the maximum possible value of J [P, P e].

Temporal profiles

Temporal profiles were calculated on individual recordings after thresholding and temporal coarse-graining had been applied. Avalanches were obtained as described for our experimental data and grouped based on duration. Population activity during all avalanches of a certain duration was averaged over all avalanches to get the temporal profile for that duration.

To calculate the profile collapse exponent, Ï‡ coll, temporal profiles were first x-rescaled to fit between 0 and 1, then linearly interpolated to N = 500 points. Every interpolated temporal profile in the desired duration range was then y-rescaled by d Ï‡ coll âˆ’ 1. The exponent Ï‡ coll was chosen to minimize the RMS error between all collapsed shapes in the desired duration range. To calculate temporal profiles at a certain coarse-graining k using the original experimental frame rate, avalanche epochs were calculated at the coarse-grained resolution as described above. Then, these epochs were mapped back onto the raster at the original frame rate and temporal profiles were calculated on these remapped epochs. The standard deviations from the mean rescaled shapes were visualized as shaded area if not stated otherwise.

Neural simulations

---

### Optimal free descriptions of many-body theories [^f77fa88f]. Nature Communications (2017). Medium credibility.

Methods

Optimization

The optimization to find Ïƒ andin (3) is performed by a Monte Carlo basin-hopping strategyusing the Nelderâ€“Mead simplex algorithm for local minimization within basins of the cost function. This global strategy was selected to counteract an observed tendency for local methods to get trapped in local minima. The initial guess for this search is found as follows. The normalization constant E 0 is the lowest entanglement energy of the input entanglement spectrum { E }. We iteratively construct an approximate set of single-particle entanglement energies starting from an empty set. First, we take the lowest remaining level in the spectrum and subtract E 0 to produce a new single-particle level k. Then we remove the many-body levels, which are closest to the new combinatorial levels generated according to (2) by the additional single-particle level. This process is repeated until the input spectrum is exhausted. We can also introduce a truncation of the entanglement spectrum cutting off high entanglement energies, making the construction of the initial guess terminate faster. The minimization of D (Ïƒ, Ïƒ f) to identify the optimal free model for the Ising Hamiltonian (5) is calculated using a local Nelderâ€“Mead method.

Finite-size scaling

We perform the finite-size scaling according to an ansatz (4). The parameters of the collapse were estimated using the method of ref. From the scaling ansatz (4) and for a trial set of scaling parameters g c, Î½ and Î¶, the scaled values x L = (g âˆ’ g c) L 1/ Î½ andare calculated from each unscaled data point. From this collection of scaled data points (x L, y L) across all L, we implicitly define a so-called master curve that best represents them. This curve y (x) is defined around a point x as the linear regression calculated by taking the scaled data points immediately left and right of x for each system size L. We characterize the deviation of the scaled points (x L, y L) from the master curve y (x L) using the Ï‡ 2 statistic. This measure is used as the cost function for an optimization problem over the scaling parameters g c, Î½, Î¶ and Î¸, which can be solved using the same techniques as the previous problems.

---

### Identifying domains of applicability of machine learning models for materials science [^72c4ba76]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector Î½ that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by Ïƒ lin (x 1, x 2) â‰¡ âˆ’0.3 â‰¤ x 1 â‰¤ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: Ïƒ rbf â‰¡ âˆ’3.3 â‰¤ x 11 â‰¤ 3.1 âˆ§ x 2 â‰¤ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA Ïƒ poly (x 1, x 2) â‰¡ âˆ’3.5 â‰¤ x 2 â‰¤ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### Quantitative assessment of full-width at half-maximum and detector energy threshold in X-ray imaging systems [^c65d8c68]. European Journal of Radiology (2024). Medium credibility.

Background

The response function of imaging systems is regularly considered to improve the qualified maps in various fields. More the accuracy of this function, the higher the quality of the images.

Methods

In this study, a distinct analytical relationship between full-width at half-maximum (FWHM) value and detector energy thresholds at distinct tube peak voltage of 100 kV has been addressed in X-ray imaging. The outcomes indicate that the behavior of the function is exponential. The relevant cut-off frequency and summation of point spread function S(PSF) were assessed at large and detailed energy ranges.

Results

A compromise must be made between cut-off frequency and FWHM to determine the optimal model. By detailed energy range, the minimum and maximum of S(PSF) values were revealed at 20 keV and 48 keV, respectively, by 2979 and 3073. Although the maximum value of FWHM occurred at the energy of 48 keV by 224 mm, its minimum value was revealed at 62 keV by 217 mm. Generally, FWHM value converged to 220 mm and S(PSF) to 3026 with small fluctuations. Consequently, there is no need to increase the voltage of the X-ray tube after the energy threshold of 20 keV.

Conclusion

The proposed FWHM function may be used in designing the setup of the imaging parameters in order to reduce the absorbed dose and obtain the final accurate maps using the related mathematical suggestions.

---

### Inferring time derivatives including cell growth rates using gaussian processes [^d99eaf3f]. Nature Communications (2016). Medium credibility.

Inferring the first and second time derivatives

To determine the time derivative of the data, we use that the derivative of a Gaussian process is another Gaussian process. We can therefore adapt standard techniques for Gaussian process to allow time derivatives to be sampled too.

Building on the work of Boyle, we let g (x) and h (x) be the first and second derivatives with respect to x of the latent function f (x). If f (x) is a Gaussian process then so are both g (x) and h (x). Writing âˆ‚ 1 and âˆ‚ 2 for the partial derivatives with respect to the first and second arguments of a bivariate function, we have

and that

as well as

following ref.

Consequently, the joint probability distribution for y and f *, g * and h * evaluated at points X * is again Gaussian (cf. equation (7)):

where we write K = K (X, X) and K âœ± = K (X *, X *) for clarity.

The covariance function is by definition symmetric: k (x i, x j) = k (x j, x i) from equation (1). Therefore, and so

for all positive integers k and. Consequently, the covariance matrix in equation (13) is also symmetric.

Conditioning on y now gives that the distribution P (f *, g *, h *| X, y, Î¸, X *) is Gaussian with mean

and covariance matrix

Equation (16) includes equation (9) and shows that

which gives the error in the estimate of the first derivative. Similarly,

is the error in estimating the second derivative.

Using an empirically estimated measurement noise

Although our derivation is given for a Gaussian process where the measurement errors in the data are independent and identically distributed with a Gaussian distribution of mean zero, the derivations are unchanged if the measurement noise has a different s.d. for each time point.

When the magnitude of the measurement noise appears to change with time, we first empirically estimate the relative magnitude of the measurement noise by the variance across all replicates at each time point. We then smooth this estimate over time (with a Gaussian filter with a width of 10% of the total time of the experiment, but the exact choice is not important) and replace the identity matrix, I, in equations (6), (15) and (16) by a diagonal matrix with the relative measurement noise on the diagonal in order to make predictions.

---

### Physical deep learning with biologically inspired training method: gradient-free approach for physical hardware [^078402ba]. Nature Communications (2022). High credibility.

Fig. 2
BP vs augmented DFA.

Training curve of 4-layer MLP with 800 hidden nodes for a augmented DFA and b BP with various g(a):sin(a), cos(a), triangle(a), tanh(a) functions, and 40 random Fourier series. Color differences indicate differences in activation function [f (a) = sin(a), cos(a), tanh(a)]. Bold line and shaded area indicate the average and maximum-minimum region, respectively. c Test error distribution of four-layer fully connected neural network as a function of the correlation coefficients between f ' (a) and g (a), Î·. Blue and red boxplots in c are the results for the model trained by BP and augmented DFA, respectively. Î· was scanned by using various g (a):sin(a), cos(a), triangle(a), tanh(a) functions and a random Fourier series. The whiskers, plots, and filled area show the minimum and maximum values, average values, and density of data distribution, respectively. d Test error of four-layer RC with 400 nodes as a function of Î·. The spectral radius of reservoir weight was set to 0.9 because the performance of the deep RC was maximized in this region (see Supplementary Information S13). Red and blue plots in d are the results for the model trained by BP and augmented DFA. Î· was scanned by using g (a) = sin(a + Î¸) with Î¸ = 0, 15, 30, 45, 60, 75, 80, 85, 90, 95, 100, 105, 120, 135, 150, 165, 180Â°. As a reference, cos(Î¸) is displayed as a second x axis. Data in this figure were obtained using standard CPU/GPU computation. Each experiment was repeated five times.

---

### A daptomycin-xylitol-loaded polymethylmethacrylate bone cement: how much xylitol should be used? [^2e0d4269]. Clinical Orthopaedics and Related Research (2013). Low credibility.

Background

The rate of release of an antibiotic from an antibiotic-loaded polymethylmethacrylate (PMMA) bone cement is low. This may be increased by adding a particulate poragen (eg, xylitol) to the cement powder. However, the appropriate poragen amount is unclear.

Questions/Purposes

We explored the appropriate amount of xylitol to use in a PMMA bone cement loaded with daptomycin and xylitol.

Methods

We prepared four groups of cement, each comprising the same amount of daptomycin in the powder (1.36 g/40 g dry powder) but different amounts of xylitol (0, 0.7, 1.4, and 2.7 g); the xylitol mass ratio (X) (mass divided by mass of the final dry cement-daptomycin-xylitol mixture) ranged from 0 to 6.13 wt/wt%. Eight mechanical, antibiotic release, and bacterial inhibitory properties were determined using three to 22 specimens or replicates per test. We then used an optimization method to determine an appropriate value of X by (1) identifying the best-fit relationship between the value of each property and X, (2) defining a master objective function incorporating all of the best fits; and (3) determining the value of X at the maximum master objective function.

Results

We found an appropriate xylitol amount to be 4.46 wt/wt% (equivalent to 1.93 g xylitol mixed with 1.36 g daptomycin and 40 g dry cement powder).

Conclusions

We demonstrated a method that may be used to determine an appropriate xylitol amount for a daptomycin-xylitol-loaded PMMA bone cement. These findings will require in vivo confirmation.

Clinical Relevance

While we identified an appropriate amount of xylitol in a daptomycin-xylitol-loaded PMMA bone cement as a prophylactic agent in total joint arthroplasties, clinical evaluations are needed to confirm the effectiveness of this cement.

---

### Non-equilibrium critical scaling and universality in a quantum simulator [^a55212f1]. Nature Communications (2025). High credibility.

Fig. 4
Unscaled (a) and scaled (b) fluctuations after a double quench.

a We plot the unscaled fluctuations along y direction at the predicted critical points for system sizes of N = 10â€“50 ions. The second quench is applied when the fluctuations following the first quench reach their maxima and the time t 2 is counted after the second quench. b We apply the same scaling collapse technique as for the single quench to find the best scaling exponents (Î± 2, Î¶ 2) for the experimental data. See Supplementary Fig. 4 for numerical simulations of the data. We observe that the critical fluctuations do not monotonically grow for increasing system sizes, as would be expected from the scaling relations. This effect can be attributed to the imperfect switching time between the first and second quench; a nearly perfect collapse can be reproduced numerically using the precise switch times (SI Sec. VII.D). We also report exponents found by a power-law fit of the maximum fluctuations which agree more closely with the analytical prediction (Inset b.). While determining the experimental critical exponents, we have excluded 50 ion data (gray) [see main text for details]. The error bars of the experimental data are statistical fluctuation around the mean value.

Experimental decoherence causes the observed fluctuations to be damped for both single and double quenches. We see that the unscaled 50 ion fluctuations after the double quench are significantly damped (Fig. 3 a). The major sources of decoherence, which scale with the system size, remain within acceptable thresholds for system sizes N < 50, but these errors start to dominate for N â‰¥ 50 ("Methods"). This effect is more adverse for the double-quench sequence than the single-quench, since the former involves longer evolution under two quenches. For completeness, we have included all the 50 ion data in Fig. 4 a, c, but excluded it in determining the best collapse exponent. Fitting the maximum amplitudes of the fluctuations toyields exponent Î± 2 = 0.69(9), with tighter error bounds (Inset Fig. 4 c). Errors in identifying the peak fluctuation result in erroneous switch time between the two quenches, contributing to imperfect exponents. This effect can be reproduced in the simulation with exact experimental parameters, and correction for such errors in further simulations results in exponents that are well in agreement with the analytically predicted non-equilibrium values (SI Sec. VII.D).

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^d6d8fcbc]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Spirometry reporting specifies that numerical values are given for the FEV1, the FVC, and the FEV1/FVC ratio; the latter should be reported as a decimal fraction and the space for percent predicted value left blank, and if bronchodilators are given the LLN column need not be repeated with absolute and percent change given only for FEV1 and FVC. Other numerical values such as the forced inspiratory flow at 75% of FVC (FEF75%) and FEF25â€“75% are not recommended for routine use. Graph requirements include that for the volumeâ€“time curve the volume scale should be at least 10 mm/L, the time scale at least 20 mm/s, and 1 second prior to the start of expiration should be displayed; on the flowâ€“volume plot the flow display should be at least 5 l/min/L/s, and the ratio of flow to volume should be 2 L/s to 1 L, and linear and log scales where values are plotted as z-scores relative to the predicted value (z = 0) give an intuitive sense of severity.

---

### Is there a better way to predict death using heart rate recovery? [^c61a7bda]. Clinical Cardiology (2006). Low credibility.

Background

Heart rate recovery (HRR) during exercise testing is an independent predictor of prognosis. The relative predictive power of computational analysis of HRR as a function of resting and maximum heart rate (HR) compared with direct measurement of the drop in HR has not been determined.

Hypothesis

We aimed to improve on the prognostic value of HRR by the use of mathematical representations of HRR kinetics.

Methods

In all, 2,193 patients who underwent exercise testing, coronary angiography, and clinical evaluation were followed up for 10.2 Â± 3.6 years. Mathematical functions were used to model HRR as a function of resting (HR(Rest)), maximum HR (HR(Peak)) and time (t): (a) HRR = HR(Rest) + (HR(peak) - HR(Rest)) X e(-kt) and (b) HRR = HR(Rest) + (HR(peak) - HR(Rest)) e(-kt2)

Results

Equation (b) provided the best fit of the recovery HR curve. An abnormal HRR at 2 min was a better predictor of mortality than HRR at 1, 3, or 5 min. At 2 min, HRR also predicted mortality better than computational models of HRR, relating HRR as a function of maximum and resting HRs. After adjusting for univariately significant predictors of mortality, HRR, age, exercise capacity, and maximum HR were chosen in order as the best predictors of mortality.

Conclusion

Even though the computational models of HRR and the determination of HRR at different time intervals were significant predictors of mortality, the simple discrete measure of HRR at 2 min was the best predictor of mortality. At 2 min, HRR outperformed age, METs, and maximum exercise HR in predicting all-cause mortality.

---

### Principled approach to the selection of the embedding dimension of networks [^e899ba67]. Nature Communications (2021). High credibility.

If we repeat the same analysis using the LINE algorithm, although the rate of decay of the function L (d) is not identical for the two embedding algorithms, we find analogous behavior with L (d) smoothly decaying toward an asymptotic value as d increases (Fig. 1 a, b). However, a faster decay doesn't necessarily imply that the corresponding algorithm is able to represent more efficiently the network than the other algorithm. The reference embeddings used in the definition of L (d) are in fact algorithm dependent, and the quantitative comparison between the L (d) functions of two different algorithms may be not informative.

Similar observations can be made when we perform the geometric embedding of synthetic networks. In Fig. 2, we display L (d) for networks generated according to the SB model. We still observe a saturation of the function L (d). Graph clustering reaches maximal performance at slightly smaller d values than those necessary for the saturation of L (d), and slowly decreases afterwards. The drop of performance for values of the embedding dimension that are too large has been already observed in other papers, see for example refs. Results based on LE embedding provide similar insights (Supplementary Figs. 3â€“6): L (d) saturates quickly as d increases; the performance in the graph clustering task is optimal before the saturation point of L (d); further, the saturation point is compatible with the presence of a significant gap in the sorted Laplacian eigenvalues.

Fig. 2
Geometric embedding of synthetic networks.

a Normalized loss as a function of the embedding dimension in SB networks. We generate graph instances composed of N = 256 nodes and C = 16 groups with p i n = 0.2 and p o u t = 0.02. Embedding is performed using node2vec. The blue curve in panel a is the best fit curve of Eq. (2) (, and R 2 = 5.93 Ã— 10 âˆ’3) with the data points. b Test of performance in recovering the ground-truth community structure in SB graphs as a function of the dimension of the embedding. We measure the recovery accuracy with normalized mutual information (NMI). Symbols refer to average values of NMI computed over 10 instances of the SB model; the shaded region identifies the range of values corresponding to one standard deviation away from the mean.

---

### Pattern electroretinogram in ocular hypertension, glaucoma suspect and early manifest glaucoma eyes: a systematic review and meta-analysis [^ed9261d7]. Ophthalmology Science (2023). Medium credibility.

with x being the observed value, Î¼ being the mean in the sample, and Ïƒ being its SD. As such, 1 SD equals 1 z-score.

A P value < 0.05 was considered statistically significant.

Based on the type of electrode adopted for the PERG measurements, studies were classified into 2 groups (i.e. invasive, noninvasive), and a subanalysis was conducted accordingly.

Cochran's Q was calculated as a measure of heterogeneity and checked by P value. We also reported I 2 statistic results, which quantify heterogeneity regardless of the number of included studies. The maximum-likelihood estimator was used to estimate the between-study variance (Ï„ 2). The influence analysis was performed using the "Influence Analysis" function in R, and a Baujat plot was consequently created. The "find.outliers" function was used to detect any outliers presenting their 95% CI lying outside the 95% CI of the pooled effect.

---

### Improved security bound for the round-robin-differential-phase-shift quantum key distribution [^68502abf]. Nature Communications (2018). Medium credibility.

Theorem. For the RRDPS protocol with L-pulse packet, where each packet contains N photons (L â©¾ N + 1), Eve's information can be bounded bywhere, non-negative real parameters x i satisfying. Moreover, if the error rate of raw key bits is E, these parameters x i (i âˆˆ {1, 2. N + 1}) must satisfy the constraintand,

Corollary. If the photon-number L â‰¤ N âˆ’ 2, I AE < 1 always holds.

Based on this theorem, the upper bound of I AE is generalized to find the maximum value of a given function under a constraint defined by E. If we ignore this constraint, we obtainwithout monitoring signal disturbance. Alternatively, if we retain this constraint, a tighter estimation may be achieved. It is remarkable that searching such a maximum value can be effective and concise through a numerical method, since its function is convex. A different, less tight bound improvement was recently reported in ref.

Potential improvements made by our theory

For a QKD protocol, the mutual information between Alice and Bob is given by I AB = 1 âˆ’ h 2 (E). Thus, there is a maximum value E max of error rate E, which satisfies, when E = E max. Obviously, ifholds, I AB will be no larger than, and no secret key bits can be generated. Thus, E max is the maximum value of tolerable error rate of a QKD protocol. We first compare E max of RRDPS between the original method and our new formulae. In Table 1, we list the results for the cases in which a single-photon source is equipped. One can see that our formulae can increase E max, especially when L is small. It is remarkable to note that for the case L = 3, with our formulae E max can be up to 5%, while the original RRDPS protocol cannot generate secure key bits at all. More importantly, the notable difference between columns two and three in Table 1 implies that our theory leads to increased E max compared to the original RRDPS even when signal disturbance monitoring is still turned off.

---

### Feasibility of dynamic T*-based oxygen-enhanced lung MRI at 3T [^66508cb8]. Magnetic Resonance in Medicine (2024). Medium credibility.

2.4 Data analysis

For motion correction, nonâ€linear image registration was performed on the dynamic time series data using Advanced Normalization Tools (ANTs). Subsequently, the lung parenchyma, excluding central major vasculature, was manually segmented from registered images. For an initial exploration of the data, first, image registration and density correction were performed as described below. Secondly, averaged hyperoxia images (61st to 210th) were subtracted from averaged normoxia images (10th to 60th). Last, mean PSE maps was calculated from the subtracted images normalized to the averaged normoxia images.

For our main data analysis, the dynamic series were fitted using exponential functions to characterize oxygen washâ€in (encompassing the downslope between the plateau regions of the curve) and washâ€out (the upslope and return to baseline). The baseline for the exponential fit was defined as the averaged signal intensity across all normoxia time points before O 2 inhalation as described in Eq. 1. The curve was fitted with the two functional forms described in Eq. 2 for the downslope and Eq. 3 for the upslope, where A 1 (x) and A 2 (x) are the baseline and fitted negative maximum hyperoxia intensity (or plateau value) at position x, respectively, and Ï„, tp1, and tp2 are the fitted washâ€in time and the provided gas switching time points (i.e. tp1, air to O 2; tp2, O 2 to air). Maximum PSE maps were produced by the subtraction of the baseline from the negative maximum hyperoxia value (A 1 â€“ A 2), normalized to the baseline A 1. We additionally defined a breathing rateâ€normalized washâ€in time, Ï„â€nBR as the product of Ï„ and the average breathing rate over the dynamic series.

---

### Inferring time derivatives including cell growth rates using gaussian processes [^b5444dae]. Nature Communications (2016). Medium credibility.

Methods

Using a Gaussian process to fit time-series data

In the following, we will denote a Gaussian distribution with mean Î¼ and covariance matrix Î£ as(Î¼, Î£) and use the notation of Rasmussen and Williamsas much as possible.

Prior probability

For n data points y i at inputs x i (each x i is a time for a growth curve), we denote the underlying latent function as f (x). We define a covariance matrix k (x, x â€²), which has an explicit dependence on hyperparameters Î¸, and obeys

where the expectations are taken over the distribution of latent functions (samples of f (x)).

We interpret equation (1) as giving the prior probability distribution of the latent functions f (X), where were we use X to denote the inputs x i, such that

where K (X, X) is the n Ã— n matrix with components k (x i, x j). With f denoting [f (x 1). f (x n)], this prior probability can be written as

noting the dependence of k (x, x â€²; Î¸) on the hyperparameters Î¸.

---

### J-shapedness: an often missed, often miscalculated relation: the example of weight and mortality [^9e991138]. Journal of Epidemiology and Community Health (2014). Low credibility.

We present three considerations in analysing the association between weight and mortality, as well as other relations that might be non-linear in nature. First, authors must graphically plot their independent and dependent variables in a continuous manner. Second, authors should assess the shape of that relation, and note its shape. If it is non-linear, and specifically, J-shaped or U-shaped, careful consideration should be given to using the 'best' statistical model, of which multivariate fractional polynomial regression is a reasonable choice. Authors should also refrain from truncating their data to avoid dealing with non-linear relations.

---

### Principled approach to the selection of the embedding dimension of networks [^0e29bd7c]. Nature Communications (2021). High credibility.

There could be several ways of finding the solution of Eq. (1). Here, given the smoothness of the empirically observed loss functions, we opted for a parametric approach. Specifically, we found thatwhere s and Î± are fitting parameters, represents well the data in various combinations of embedding methods and embedded networks. Equation (2) is meant to describe the mathematical behavior of the loss function only in the range d âˆˆ [1, d r). In particular in the definition of Eq. (2), L âˆž appears as the value of the normalized loss function in the limit of infinitely large embedding dimensions. However, such a limit has no physical meaning and is used only to facilitate the mathematical description of the empirical loss function in the regime d â‰ª d r. Best fits, obtained by minimizing the mean-squared error, of the function of Eq. (2) are shown in Figs. 1 a, b and 2 a. We performed a systematic analysis on a corpus of 135 real-world networks. We found that best estimatesof the asymptotic value L âˆž for the normalized embedding loss function are generally close to zero (Supplementary Table 1). Best estimatesof the factor s, regulating the rate of the power-law decay toward L âˆž, seem very similar to each other, irrespective of the network that is actually embedded. Measured values ofindicate only a mild dependence on the underlying network (see Fig. 3). For uncorrelated clouds of points in d dimension, the central limit theorem allows us to predict that L (d) ~ 1/ d 1/2. Our best estimatesof the decay exponent Î± are generally greater than those expected for uncorrelated clouds of data points, indicating that the embedding algorithm correctly retains network structural information even in high-dimensional space. In systematic analyses performed on random networks constructed using either the ER and the BA models, we find that the size of the network is not an important factor in determining the plateau value L âˆž. The decay exponent Î± and the rate s are positively correlated with density of the embedded network, but their values become constant in the limit of large network sizes (see Fig. 4).

---

### The SNMMI and EANM practice guideline for tele-nuclear medicine 2.0 [^cf27d78f]. Journal of Nuclear Medicine Technology (2014). Medium credibility.

Tele-nuclear medicine data visualization capabilities â€” The remote station should allow the same (or equivalent) display and processing functions as those used for interpretation or consultation at an on-site physician workstation, and small displays (smart phones or tablets) may facilitate consultation with colleagues but will have limited utility for interpretation or reporting. Display control includes the ability to display image data in gray scale or color and scale the intensity to fewer levels (e.g., 256 levels) for on-screen display. Planar image display abilities include the ability to display complete images ranging in size from 64 x 64 to 1,024 x 1,024, display a 1,024 x 1,024 or 512 x 1024 whole-body image centered within a smaller-width frame (e.g., 256 pixels wide), simultaneously display whole-body images (e.g., 1024 x 256) and spot images (e.g., 256 x 256), and display a sequence of images scaled to a common maximum pixel value.

---

### Semiparametric estimation of the relationship between ROC operating points and the test-result scale: application to the proper binormal model [^9a9d14d8]. Academic Radiology (2011). Low credibility.

Rationale and Objectives

Semiparametric methods provide smooth and continuous receiver operating characteristic (ROC) curve fits to ordinal test results and require only that the data follow some unknown monotonic transformation of the model's assumed distributions. The quantitative relationship between cutoff settings or individual test-result values on the data scale and points on the estimated ROC curve is lost in this procedure, however. To recover that relationship in a principled way, we propose a new algorithm for "proper" ROC curves and illustrate it by use of the proper binormal model.

Materials and Methods

Several authors have proposed the use of multinomial distributions to fit semiparametric ROC curves by maximum-likelihood estimation. The resulting approach requires nuisance parameters that specify interval probabilities associated with the data, which are used subsequently as a basis for estimating values of the curve parameters of primary interest. In the method described here, we employ those "nuisance" parameters to recover the relationship between any ordinal test-result scale and true-positive fraction, false-positive fraction, and likelihood ratio. Computer simulations based on the proper binormal model were used to evaluate our approach in estimating those relationships and to assess the coverage of its confidence intervals for realistically sized datasets.

Results

In our simulations, the method reliably estimated simple relationships between test-result values and the several ROC quantities.

Conclusion

The proposed approach provides an effective and reliable semiparametric method with which to estimate the relationship between cutoff settings or individual test-result values and corresponding points on the ROC curve.

---

### The snm procedure guideline for general imaging 6.0 [^85398c18]. SNMMI (2010). Medium credibility.

Nuclear medicine computer system components â€” The camera head or associated image processing system performs functions including image size, position and zoom; energy correction; spatial distortion correction; other corrections (scatter correction, dead time correction, depth of interaction correction, sensitivity correction); and digital position computation; the interface handles the data in two basic modes: 1) Frame mode: complete images or matrices are available to the attached computer; and 2) List mode: data are passed on to the attached computer as a list of event x, y coordinates, to which time information and energy information may be also attached; for cardiac studies in particular, time lapse averaging is required, such that each image acquired at some specific time within the cardiac cycle is added to other acquired at similar times.

---

### The snm procedure guideline for general imaging 6.0 [^638aaec4]. SNMMI (2010). Medium credibility.

SNM Procedure Guideline for General Imaging â€” matrix size and pixel depth in nuclear medicine acquisitions notes that matrix size is almost always a power of 2 with typical values 64 x 64, 128 x 128, 256 x 256 and 512 x 512, and that non-square matrix sizes also exist for whole-body studies. Each pixel can be represented with a single byte (pixel values ranging from 0 to 255 counts) or with 16 bit words (pixel values ranging up to a maximum of 32k or 64k). Overflow occurs when the number of counts recorded at some given position exceeds the maximum number of counts, and overflow is more likely to occur when using a byte matrix.

---

### SNMMI procedure standard / EANM practice guideline for diuretic renal scintigraphy in adults with suspected upper urinary tract obstruction 1.0 [^2432fc8a]. Seminars in Nuclear Medicine (2018). Medium credibility.

20 minute to maximum (20 min/max) count ratio â€” definition, normal values, and threshold: The 20 min/max count ratio is the ratio of the kidney counts at 20 minutes to the maximum (peak) counts normalized for time and provides an index of transit time and parenchymal function. For 99mTc-MAG3, the normal 20 min/max count ratio for cortical ROIs averages 0.19, with SDs of 0.07 and 0.04 for the right and left kidneys, respectively. If the patient is not dehydrated and the cortical ROI 20 min/max count ratio exceeds 0.35 (> 2â€“3 SDs above the mean), kidney function is likely to be abnormal, and these ratios can be useful in monitoring patients with suspected urinary tract obstruction and renovascular hypertension.

---

### A dual role in regulation and toxicity for the disordered N-terminus of the toxin graT [^910277d6]. Nature Communications (2019). High credibility.

Small-angle X-ray scattering

SAXS data were collected at SWING beamline (Soleil Synchrotron, Gif-Sur-Yvette, France). This beamline has a SEC system before the measuring capillary. SEC will remove possible aggregates rendering a very homogeneous sample that will then be directly exposed to X-rays for data collection. All the experiments were performed in 50 mM Tris, pH 8.0, 250 mM NaCl and 2 mM TCEP as running buffer.

Each protein was run through a Shodex KW402.5-4F at 0.2 ml/min. Scattering curves covering a concentration range around the peak were normalized and averaged to obtain the final scattering curve. Rg values were derived from the value of I 0 which were obtained by extrapolating to q = 0 using the Guinier approximation as implemented in ATSAS suite. The molecular weights of the different entities were estimated in a concentration independent way using the I 0, Porod volume and Fisher methods.

The use of the dimensionless Kratky plot ((qR g) 2 I(q)/I(0) vs qR g) is a relatively easy way to show that a protein is completely folded, partially folded or completely unstructured. If a protein is globular it follows Guinier's law I(q)/I(0) = exp (âˆ’(qR g) 2 / 3). The corresponding dimensionless Kratky plot is a function f(x) = x 2 exp (âˆ’x 2 / 3), with x = qR g > 0 with maximum of 1.104 at qR g = âˆš3. On the other hand, an ideally disordered protein follows Debye's law I(q)/I(0) = 2 (x 2 âˆ’1âˆ’exp (âˆ’x 2))/x 4, with x = qR g > 0. In this case the Kratky plot is described by the function f(x) = 2 (x 2 âˆ’ 1âˆ’exp (âˆ’x 2))/x 2 which increases monotonically with an asymptote at f(x) = 2. Experimentally, globular proteins show a very similar normalized Kratky plot with a maximum at (âˆš3; 1.1), while partially unstructured proteins show a maximum shifted to higher values in both axes.

---

### A geometric approach to characterize the functional identity of single cells [^bc528430]. Nature Communications (2018). Medium credibility.

This operation projects z i to the orthogonal complement of the space spanned by the universally expressed genes. We then concatenate the column vectorsto create a adjusted cell signature matrix.

Next, to enhance the signal contributed by preferentially expressed genes, we propose an information theoretic approach that is inspired by the work of Schug et al. The main idea is to use Shannon's entropy to measure the informativeness of genes. If a gene is uniformly utilized across cells, it contains less information as opposed to the case in which it is selectively active in a few cells. To this end, we start with the positive projection of adjusted cell signatures, in which case we replace all negative values with zeros. Then, we normalize this matrix to construct a stochastic matrix P (every row sums to one). Let p i be the row vector associated with the i th gene. We compute the uniformity, or normalized entropy, of p i as: u (i) = âˆ’âˆ‘ j p ij log(p ij)/log(n), where p ij is an entry in the matrix P and n is the number of genes. This value is always between zero and one and is used as a basis to boost contributions from the most informative genes. A detailed comparison of our entropy-based method with dispersion and Gini index is provided in the Supplementary Note 2.

To scale genes according to their specificity, we compute a coefficient that controls the contribution of each gene. This coefficient is greater than one (scales up) for cell-type-specific genes and less than one (scales down) for universally expressed genes, respectively. To do so, we note that the distribution of the entropy values follows a bimodal distribution, with separate peaks for the cell-type-specific and universally expressed genes. To identify the critical point where these two population separate from each other, we fit a mixture of two Gaussians over the distribution of the values and use it to identify this transition point, denoted by, which is the point of equal probability from each Gaussian. Then for each gene i, we define a scaling factor as. Finally, we compute the kernel matrix as follows:

In this formulation, if we denote, then K is a dot-product kernel defined as Y T Y. We will refer to Y as the adjusted transcriptional profile of cells, and K as the cell similarity kernel, or ACTION metric.

---

### Standardization of spirometry 2019 update. An official American Thoracic Society and European Respiratory Society technical statement [^7290ab6b]. American Journal of Respiratory and Critical Care Medicine (2019). High credibility.

Standardization of spirometry â€” display and digitization requirements specify acquisition, graph scaling, and start-of-test display. For digitization of the flow or volume signal, the sampling rate must be â‰¥ 100 Hz with a minimum resolution of 12 bits. For the flowâ€“volume graph, expiratory flow must be plotted upward and expiratory volume toward the right, and a 2:1 aspect ratio must be maintained so that 2 L/s of flow and 1 L of volume are the same distance on their respective axes. For the start of test display, the volumeâ€“time graph must begin at the point of maximum inspiration or 1 second before Time 0, whichever occurs first, and should continue to the end of the plateau or the beginning of inspiration. Displays of flow versus volume provide more detail than volumeâ€“time graphs for the first 1 second of the FVC maneuver, whereas volumeâ€“time graphs provide more detail for the latter part of the maneuver.

---

### A probabilistic Bayesian approach to recovermap and phase images for quantitative susceptibility mapping [^b146d367]. Magnetic Resonance in Medicine (2022). Medium credibility.

Purpose

Undersampling is used to reduce the scan time for high-resolution three-dimensional magnetic resonance imaging. In order to achieve better image quality and avoid manual parameter tuning, we propose a probabilistic Bayesian approach to recover Râ‚‚* map and phase images for quantitative susceptibility mapping (QSM), while allowing automatic parameter estimation from undersampled data.

Theory

Sparse prior on the wavelet coefficients of images is interpreted from a Bayesian perspective as sparsity-promoting distribution. A novel nonlinear approximate message passing (AMP) framework that incorporates a mono-exponential decay model is proposed. The parameters are treated as unknown variables and jointly estimated with image wavelet coefficients.

Methods

Undersampling takes place in the y-z plane of k-space according to the Poisson-disk pattern. Retrospective undersampling is performed to evaluate the performances of different reconstruction approaches, prospective undersampling is performed to demonstrate the feasibility of undersampling in practice.

Results

The proposed AMP with parameter estimation (AMP-PE) approach successfully recovers Râ‚‚* maps and phase images for QSM across various undersampling rates. It is more computationally efficient, and performs better than the state-of-the-art lâ‚-norm regularization (L1) approach in general, except a few cases where the L1 approach performs as well as AMP-PE.

Conclusion

AMP-PE achieves better performance by drawing information from both the sparse prior and the mono-exponential decay model. It does not require parameter tuning, and works with a clinical, prospective undersampling scheme where parameter tuning is often impossible or difficult due to the lack of ground-truth image.

---

### ACR-ACNM-SPR practice parameter for the performance of renal scintigraphy [^70080c6b]. ACNM/ACR/SPR (2022). High credibility.

Renal scintigraphy image processing â€” Processing includes correct drawing of ROIs, background correction, generation of time-activity curves, and other quantitative indices, which may include relative perfusion, split function, time-to-peak or Tmax, half-life, diuretic half-time, 20 minute/maximum count ratio, postvoid renal counts, and camera renal clearance rates.

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^fe47f6c1]. Nature Communications (2025). High credibility.

Derivation of Î´ 2 from the standard beta distribution

For continuous variables constrained between the interval [0, 1], the beta distribution is a flexible model capable of describing a wide array of distribution shapes, including bell-shaped, left-skewed, right-skewed and multi-modal (U-shaped) distributions (Fig. 2 i). The beta probability density function (pdf) is typically defined by two positive shape parameters, p and q both on the open interval (0, âˆž), with the function indexed by continuous values of x constrained between the closed interval [0, 1]. The pdf for the beta distribution iswhere the two exponential functions, x p âˆ’1 (1âˆ’ x) q âˆ’1, describe all distribution shapes modelled by the beta distribution (Fig. 2 i), Î“(.) is the gamma function, and Î“(p + q)/[Î“(p)Î“(q)] normalises the beta density function such that the area below the function is equal to 1. Notably, as p and q diverge from one another and their sum remains fixed (constant), the central tendency of the distribution shifts to the left (p < q) or to the right (p > q). To be exact, the mean (Î¼) and 1 âˆ’ Î¼ can be found withandwhere Î¼ is constrained between the open interval (0, 1). The interval constraining Î¼ is found with limits of the interval of p and q asand. Note Î¼ = 0.5 when p = q. Meanwhile, as the sum of p and q decreases the dispersion of the distribution increases, irrespective of Î¼. Specifically, the dispersion can be described withwhere Î´ 2 is also constrained between the open interval (0, 1). The interval constraining Î´ 2 is found with limits of the interval of p and q asand. Note that Î´ 2 is equivalent to the shape parameter described by Damgaard and Irvine, which is better interpreted as dispersion and equivalent to the inverse of the precision parameter described by Ferrari and Cribari-Neto. It can, thus, be shown that the combination of Î¼ and Î´ 2 are capable of describing all possible values of p and q, and by extension all possible beta distributions, whereand

---

### Control nucleation for strong and tough crystalline hydrogels with high water content [^b28620a1]. Nature Communications (2024). High credibility.

SAXS measurement

SAXS was conducted on a Xenocs Xeuss 2.0 System by using an incident Cu-K Î± X-ray beam that was perpendicular to the sample plane. X-ray wavelength, spot size and distance between specimen and detector were 0.154 nm, 172 Ã— 172 Î¼m 2 and 1185 mm, respectively. Samples were square slices with a side length of 10 mm. Scattering patterns were collected by a Pilatus 300 k detector. The distance distribution function, P (r), was analyzed by SASfit software.

(1) The inter-crystallite distance (R d) was obtained from the Bragg expression:Where q max is the scattering vector at which Iq 2 shows a maximum value.

(2) The radius of gyration (R g) could be calculated at low q according to Guinier equation:Where I (q) was the scattering intensity and I 0 was the zero angle scattering intensity. R g was used to represent the average size of crystallites.

(3) The pair-distribution function P (r) could be calculated through a Fourier transform of the scattering curve:

(4) The orientation order parameter (f) was calculated to describe the orientation degree of crystal domains. Its value ranges from 0 to 1, where the former corresponds to an isotropic structure and the latter corresponds to a perfect orientation structure along the director. A Maier-Saupe distribution function was used to fit the azimuthal angle profile:where I 0 is the free baseline intensity, Ï† is the azimuthal angle, Ï† 0 is the azimuth at the position of maximal intensity, and Ï‰ is a parameter that determines the width of the distribution. After the fitting, parameter Ï‰ was obtained, and the orientation factor f can be determined using the following formula:where the function P 2 (cos Ï†) is the second-order Legendre polynomial of cos Ï† and often referred to as the Hermans orientation function:

---

### Comparison of parameter optimization methods for quantitative susceptibility mapping [^26a0417b]. Magnetic Resonance in Medicine (2021). Medium credibility.

Purpose

Quantitative Susceptibility Mapping (QSM) is usually performed by minimizing a functional with data fidelity and regularization terms. A weighting parameter controls the balance between these terms. There is a need for techniques to find the proper balance that avoids artifact propagation and loss of details. Finding the point of maximum curvature in the L-curve is a popular choice, although it is slow, often unreliable when using variational penalties, and has a tendency to yield overregularized results.

Methods

We propose 2 alternative approaches to control the balance between the data fidelity and regularization terms: 1) searching for an inflection point in the log-log domain of the L-curve, and 2) comparing frequency components of QSM reconstructions. We compare these methods against the conventional L-curve and U-curve approaches.

Results

Our methods achieve predicted parameters that are better correlated with RMS error, high-frequency error norm, and structural similarity metric-based parameter optimizations than those obtained with traditional methods. The inflection point yields less overregularization and lower errors than traditional alternatives. The frequency analysis yields more visually appealing results, although with larger RMS error.

Conclusion

Our methods provide a robust parameter optimization framework for variational penalties in QSM reconstruction. The L-curve-based zero-curvature search produced almost optimal results for typical QSM acquisition settings. The frequency analysis method may use a 1.5 to 2.0 correction factor to apply it as a stand-alone method for a wider range of signal-to-noise-ratio settings. This approach may also benefit from fast search algorithms such as the binary search to speed up the process.

---

### Simultaneous spatiotemporal transcriptomics and microscopy of Bacillus subtilis swarm development reveal cooperation across generations [^a13825d4]. Nature Microbiology (2023). High credibility.

Spectral representation of spatiotemporal gene expression

The transcriptome measurements and the microscopy-based measurement of phenotypic properties were sampled at a set of radial spaceâ€“time points, whereis the time andis the radial position from the centre of the swarm at which the sample was acquired. For each gene, we have a sample vectorwith length, where theth entry is the gene expression at the point. Similarly, for each phenotypic property, we have a sample vectorwith length.

To form a spectral representation across the three replicates, we fit a common domain to the three replicates. Experimentally, the radial position of the boundaryof the swarm at each timewas determined automatically by detecting the presence and location of bacteria in the microscopy field of view and moving the microscope stage until the field of view was split between colonized agar containing bacteria and uncolonized agar in approximately equal proportions. We simultaneously fitted a boundary of the formto all replicates by first minimizing the loss function, where superscript (n) denotes the index of the three different replicates. This exponential fit approximates the experimental data very well (Extended Data Fig. 9a).

To obtain non-dimensional data, we rescaled data as follows. Letbe the index corresponding to the largest; we defined the time shiftand scaled the initial value. The other time shifts are given by

We then non-dimensionalized the data (variables) usingand. The domain boundary is then given byandwhereis maximum non-dimensional time present in all three replicates. Data points that lay outside the domain after rescaling were not used in the spectral representation. The non-dimensionalized domain and how the individual sampling data points are distributed within this domain are shown in Extended Data Fig. 9b.

We built a domain-specific orthogonal polynomial basisby applying Gramâ€“Schmidt orthogonalizationto the monomial set, under the inner product

The spaceâ€“time dependence of each gene was compressed by expanding each sample vector, whereis the lengthvector formed by evaluatingat each non-dimensional spaceâ€“time point. The coefficients were fitted using least squares on the matrix equation, for each gene and property.

To combine information from all replicates, we fitted a single coefficient vector for the replicates by stacking the least square problems on top of each other to form a single linear regression problem, from which a smooth average spatiotemporal gene expression was formed,

The same procedure was also used to produce average properties.

---

### A geometric approach to characterize the functional identity of single cells [^7218b9f4]. Nature Communications (2018). Medium credibility.

Fig. 2
Workflow of ACTION cell-to-cell similarity metric. Computation of the ACTION metric consists of three main steps. i The effect of universally expressed genes is masked out to construct an adjusted transcriptional profile. ii A gene expression specificity vector is computed that assigns weights to each gene based on its informativeness. iii The ACTION kernel is computed as the weighted dot product of adjusted transcriptional vectors

To establish the superiority of our metric, we compare it against an alternate measure specifically designed for single-cell analysis, SIMLR. SIMLR combines a number of distance metrics to learn a joint similarity score that maximizes the block diagonal structure of the resulting matrix. We also compare ACTION with the normalized dot product resulting from two nonlinear dimension-reduction techniques: multidimensional scaling (MDS) and Isomap. While ACTION is a non-parametric method, the other methods have one or more parameters that need to be specified by the user. For SIMLR, we need to specify the true number of cell types. For all methods other than ACTION, we must specify the dimension of the low-dimensional subspace. To give them the best chance at competing with ACTION, we evaluate ten different values for the dimension of projected subspace (from 5 to 50 with increments of 5) and report the best results obtained over all configurations.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^f7d7cfa0]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

American Thoracic Society pulmonary function report â€” core spirometry reporting elements emphasize that a uniform format for the presentation of PFT results in reports to users and in the medical record can reduce potential miscommunication or misunderstanding, and only information with validated clinical application should be included. The normal limit(s) of each test parameter should be displayed, and, consistent with other laboratory values, the measured value should be shown before reference values, ranges, or normal limits. Report and/or display of the displacement of the result from a predicted value in standard deviation units (z-score) can help in understanding abnormality. For spirometry, many parameters can be calculated but most do not add clinical utility and should not be routinely reported; only FVC, FEV1, and FEV1/FVC need be routinely reported. Measurement of slow VC and calculation of FEV1/VC are a useful adjunct in patients with suspected airflow obstruction. Reporting FEV1/FVC (or FEV1/VC) as a decimal fraction, and not reporting it as a percentage of the predicted value for this ratio, will help to minimize miscommunication. The nitrogen washout plot for multibreath tests and the tracings for plethysmograph tests can be shown graphically to aid quality assessment.

---

### Estimation in medical imaging without a gold standard [^82dda7fe]. Academic Radiology (2002). Low credibility.

Rationale and Objectives

In medical imaging, physicians often estimate a parameter of interest (eg, cardiac ejection fraction) for a patient to assist in establishing a diagnosis. Many different estimation methods may exist, but rarely can one be considered a gold standard. Therefore, evaluation and comparison of different estimation methods are difficult. The purpose of this study was to examine a method of evaluating different estimation methods without use of a gold standard.

Materials and Methods

This method is equivalent to fitting regression lines without the x axis. To use this method, multiple estimates of the clinical parameter of interest for each patient of a given population were needed. The authors assumed the statistical distribution for the true values of the clinical parameter of interest was a member of a given family of parameterized distributions. Furthermore, they assumed a statistical model relating the clinical parameter to the estimates of its value. Using these assumptions and observed data, they estimated the model parameters and the parameters characterizing the distribution of the clinical parameter.

Results

The authors applied the method to simulated cardiac ejection fraction data with varying numbers of patients, numbers of modalities, and levels of noise. They also tested the method on both linear and nonlinear models and characterized the performance of this method compared to that of conventional regression analysis by using x-axis information. Results indicate that the method follows trends similar to that of conventional regression analysis as patients and noise vary, although conventional regression analysis outperforms the method presented because it uses the gold standard which the authors assume is unavailable.

Conclusion

The method accurately estimates model parameters. These estimates can be used to rank the systems for a given estimation task.

---

### Joint EANM / SNMMI / IHPBA procedure guideline for [Tc] Tc-mebrofenin hepatobiliary scintigraphy SPECT / CT in the quantitative assessment of the future liver remnant function [^185330a4]. HPB (2023). High credibility.

Preoperative assessment for major liver resection â€” The clinical indication flow chart for HBS presents a bilirubin threshold of "Bilirubin < 50 Î¼mol/L (2.92 mg/dL)" and, as described in the figure legend, considers "the maximum serum bilirubin levels, the cut-off values and suggested regenerative procedure to achieve sufficient FLRF for safe resection". Footnotes specify a special-case cut-off, "In patients suspected of perihilar cholangiocarcinoma a cut-off value of 8.5%/min is kept", and a volumetric context, "The volumetric threshold for healthy liver parenchyma is roughly on 25%. For diseased liver parenchyma, this is 40%".

---

### Psychophysical analysis of monitor display functions affecting observer diagnostic performance of CT image on liquid crystal display monitors [^542b453d]. European Radiology (2005). Low credibility.

The aim of the present study was to propose suitable display functions for CT image representation on liquid crystal display (LCD) monitors by analyzing the characteristics of the monitor's typical display functions using psychophysical analysis. The luminance of the LCD monitor was adjusted to a maximum of 275 cd/m2 and 480 cd/m2. Three types of postcalibrated display functions (i.e. GSDF, CIELAB, and Exponential gamma 2.2) were evaluated. Luminance calculation of a new grayscale test pattern (NGTP) was done for the conversion of the digital driving level (DDL) into the CT value. The psychophysical gradient delta of display functions for the CT value was evaluated and compared via statistical analysis. The delta value of GSDF and CIE decreased exponentially; however, the delta value of Exponential gamma 2.2 showed a convex curve with a peak at a specific point. There was a statistically significant difference among the delta values of the three types of display functions on the 480 cd/m2 maximum via Kruskal Wallis test (P < 0.001). The GSDF was suitable for observation of abdominal and lung CT images; however, the display function combined the Exponential gamma 2.2 and the GSDF functions and was ideal for observation of brain CT images by psychophysical analysis.

---

### A simple new visualization of exercise data discloses pathophysiology and severity of heart failure [^8d8fc56b]. Journal of the American Heart Association (2012). Low credibility.

Background

The complexity of cardiopulmonary exercise testing data and their displays tends to make assessment of patients, including those with heart failure, time consuming.

Methods and Results

We postulated that a new single display that uses concurrent values of oxygen uptake / ventilation versus carbon dioxide output / ventilation ratios ([Formula: see text]-versus-[Formula: see text]), plotted on equal X-Y axes, would better quantify normality and heart failure severity and would clarify pathophysiology. Consecutive [Formula: see text]-versus-[Formula: see text] values from rest to recovery were displayed on X-Y axes for patients with Class II and IV heart failure and for healthy subjects without heart failure. The displays revealed distinctive patterns for each group, reflecting sequential changes in cardiac output, arterial and mixed venous O(2) and CO(2) content differences, and ventilation ([Formula: see text]). On the basis of exercise tests of 417 healthy subjects, reference formulas for highest [Formula: see text] and [Formula: see text], which normally occur during moderate exercise, are presented. Absolute and percent predicted values of highest [Formula: see text] and [Formula: see text] were recorded for 10 individuals from each group: Those of healthy subjects were significantly higher than those of patients with Class II heart failure, and those of patients with Class II heart failure were higher than those of patients with Class IV heart failure. These values differentiated heart failure severity better than peak [Formula: see text], anaerobic threshold, peak oxygen pulse, and [Formula: see text] slopes. Resting [Formula: see text]-versus-[Formula: see text] values were strikingly low for patients with Class IV heart failure, and with exercise, increased minimally or even decreased. With regard to the pathophysiology of heart failure, high [Formula: see text] values during milder exercise, previously attributed to ventilatory inefficiency, seem to be caused primarily by reduced cardiac output rather than increased [Formula: see text].

Conclusion

[Formula: see text]-versus-[Formula: see text] measurements and displays, extractable from future or existing exercise data, separate the 3 groups (healthy subjects, patients with Class II heart failure, and patients with Class IV heart failure) well and confirm the dominant role of low cardiac output rather than excessive [Formula: see text] in heart failure pathophysiology. (J Am Heart Assoc. 2012;1:e001883 doi: 10.1161/JAHA.112.001883.).

---

### A synaptic learning rule for exploiting nonlinear dendritic computation [^8451416b]. Neuron (2021). Medium credibility.

(C) The approximations in (B) accurately predict the voltage gradients computed from numerical integration of (Equation 13), (Equation 14), (Equation 15), (Equation 16), (Equation 17) (fitted on 75% of the simulated data and tested on the remaining 25%). For visibility, scatterplot shows randomly sampled points from bins of 0.1 mV nS âˆ’1 width along the x axis (up to 100 points per bin). R 2 values are computed from the correlation between actual and approximated values over all held-out data.

(D) The voltage at a synapse at the time of somatic spikes depends on multiple factors, allowing their implicit representation in the learning rule. Shown is the semipartial correlation computed from a linear model fitted on 75% of the data and tested on the remaining 25%.

See also Figure S1.

---

### The snm procedure guideline for general imaging 6.0 [^0e7310d4]. SNMMI (2010). Medium credibility.

SNM Procedure Guideline for General Imaging 6.0 â€” field uniformity testing for scintillation cameras states that field uniformity should be tested each day a scintillation camera is used, using a solid- or liquid-filled uniform flood source to evaluate system (collimator on) uniformity; an acceptable alternative is to remove the collimator and produce an image using a distant (5 times the maximum detector dimension) point source, and the point source must be on the central axis. For planar imaging, images for small-field cameras must contain at least 1.25M counts, large-field circular, 2.5M counts and large rectangular field cameras, 4M counts, and all imaging parameters and the technologist's initials should be recorded.

---

### Epistatic net allows the sparse spectral regularization of deep neural networks for inferring fitness functions [^acdf7bc6]. Nature Communications (2021). High credibility.

If the fitness function is measured (known) for all the combinatorial p = 2 d inputs x i, then we can use the Fast WH Transform (FWHT)to find the WH coefficients intime complexity. The problem is so-called fully determined in such a scenario. However, as discussed in the introduction, in inferring fitness functions, we typically face problems where the number of observed samples (sequences) n is much smaller than the total number of possible sequences, that is, n â‰ª p = 2 d; in other words, we are in an underdetermined regime. In full generality, we assume that the data is generated according to a noisy nonlinear modelwhere Î¸ are the parameters of the model, Îµ e is a random variable drawn from a Gaussian distribution with zero mean and variance. Under this setting the maximum likelihood estimate is

We denote a deep neural network (DNN) by g Î¸ (x), where Î¸ is a vector of all the weights in DNN. The DNN, g Î¸ (x), takes in a binary input vector x i and predicts the output. Letdenote a matrix which comprises all the p = 2 d enumeration of the binary sequence x i of length d in its rows. We slightly abuse the notation and letdenote the real-valued vector of DNN outputs over all these binary sequences. We call this high-dimensional vector the DNN landscape. In order to find the WH transform of the DNN we can multiply the DNN landscape, g Î¸ (X), by the WH matrix. The WH matrix H can be defined using the recursive equationwhere H 2 is the 2 Ã— 2 mother WH matrix defined asand âŠ— denotes the Kronecker product. The WH matrix is a symmetric unitary matrix; in other words, (1/2 d) H H = I. Each of the 2 d columns of H corresponds to a monomial in the pseudo-Boolean representation of set functions and equivalently corresponds to one of the terms in WH transform. In biology literature, this coefficient is known as an epistatic interaction when. The WH transform of the DNN can be calculated as. Note that in this manuscript we assume H is properly normalized to be unitary.

---

### Dynamical regimes of diffusion models [^a7936fcc]. Nature Communications (2024). High credibility.

Speciation time

In order to show the existence of regimes I and II, and compute the speciation time, we focus on the following protocol which consists of cloning trajectories. We consider a backward trajectory starting at time t f â‰« 1 from a point x f drawn from a random Gaussian distribution where all components are independent with mean zero and unit variance. This trajectory evolves backward in time, through the backward process until time t < t f. At this time the trajectory has reached the point y (t), at which cloning takes place. One generates for Ï„ < t two clones, starting from the same x 1 (t) = x 2 (t) = y (t), and evolving as independent trajectories x 1 (Ï„) and x 2 (Ï„), i.e. with independent thermal noises. We compute the probability Ï• (t) that the two trajectories ending in x 1 (0) and x 2 (0) are in the same class. Defining P (x 1, 0âˆ£ y, t) as the probability that the backward process ends in x 1, given that it was in y at time t, the joint probability of finding the trajectory in y at time t and the two clones in x 1 and x 2 at time 0 is obtained as Q (x 1, x 2, y, t) = P (x 1, 0âˆ£ y, t) P (x 2, 0âˆ£ y, t) P (y, t). Then Ï• (t) is the integral of Q over x 1, x 2, y with the constraint (x 1 â‹… m)(x 2 â‹… m) > 0. This simplifies into a one-dimensional integral (see SI Appendix):where G (y, u, v) is a Gaussian probability density function for the real variable y, with mean u and variance v, and, Î“ t = Î” t + Ïƒ 2 e âˆ’2 t. The probability Ï• (t) that the two clones end up in the same cluster is a decreasing function of t, going from Ï• (0) = 1 to Ï• (âˆž) = 1/2. In the large d limit, the scaling variable controlling the change of Ï• iswhich can be rewritten asby using. This explicitly shows that speciation takes place at the time scale t S on a window of time of order one. As detailed in SI Appendix, this expression for t S coincides with the one obtained from the general criterion (5). We show in Fig. 2 the analytical result from (6) and direct numerical results obtained for increasingly larger dimensions. This comparison shows that our analysis is accurate already for moderately large dimensions. In the limit of infinite d, the analytical curve in Fig. 2 suddenly jumps from one to zero at t / t S = 1, corresponding to a symmetry-breaking phase transition (or a threshold phenomenon) on the time scale t S. In the numerics, following finite size scaling theory, we have defined the speciation time as the crossing point of the curves for different d, which corresponds approximatively to Ï• (t S) = 0.775 and indeed is of the orderfor d â†’ âˆž. As it happens in mean-field theories of phase transitions, the large dimensional limit allows to obtain a useful limiting process. In our case, this leads to a full characterization of the asymptotic backward dynamics. At its beginning, i.e. in regime I, the overlap with the centers of the Gaussian model, Â± m â‹… x (t), is of order. Defining, one can obtain a closed stochastic Langevin equation on q in a potential V (q, t) (see SI Appendix), where Î· (t) is square root of two times a Brownian motion, andAt large d, this potential is quadratic at times, and it develops a double well structure, with a very large barrier, when. The trajectories of q (t) are subjected to a force that drives them toward plus and minus infinity. The barrier between positive and negative values of q becomes so large that trajectories commit to a definite sign of q: this is how the symmetry breaking takes place dynamically at the time scale t S, in agreement with the previous cloning results. Regime II corresponds to the scaling limit q â†’ âˆž, where m â‹… x (t) becomes of order d. In this regime, the rescaled overlap m â‹… x (t)/ d concentrates, and its sign depends on the set of trajectories one is focusing on. Moreover, the stochastic dynamics of the x i correspond to the backward dynamics for a single Gaussian centered in Â± m. This shows that the dynamics generalizes, see SI Appendix (and alsofor similar results).

---

### The snm procedure guideline for general imaging 6.0 [^ca75ae44]. SNMMI (2010). Medium credibility.

Dynamic imaging â€” time per frame should be selected based on temporal resolution needs; for quantitative functional studies, shorter times are preferred to measure rapidly changing physiologic parameters, whereas longer times are generally used when imaging statistics for each frame are being optimized. For computer acquired images, a matrix size of 64 x 64 or 128 x 128 may be suitable depending on the type of study, and if there is any question, word mode should be used to avoid pixel saturation that may occur in byte mode after choosing between word (often two-byte) and byte mode acquisitions. For high-count rate studies, dead time effects may be important and count rate loss should be ascertained by dead time measurements.

---

### Scalable spatiotemporal prediction with Bayesian neural fields [^fea8687d]. Nature Communications (2024). High credibility.

Prediction queries

We can approximate the posterior (13) using a set of samples, which may be obtained from either MAP ensemble estimation or stochastic variational inference (by sampling from the ensemble of M variational distributions). We can then approximate the posterior-predictive distribution(which marginalizes out the parameters Î˜) of Y (r *) at a novel field index r âœ± = (s *, t *) by a mixture model with M equally weighted components:Equipped with Eq. (23), we can directly compute predictive probabilities of events { Y (r *) â‰¤ y }, predictive probability densities { Y (r *) = y }, or conditional expectationsfor a probe function. Prediction intervals around Y (r *) are estimated by computing the Î± -quantile y Î± (r *), which satisfiesFor example, the median estimate is y 0.50 (s *, t *) and 95% prediction interval is [y 0.025 (s *, t *), y 0.975 (s *, t *)]. The quantiles (24) are estimated numerically using Chandrupatla's root finding algorithmon the cumulative distribution function of the mixture (23).

Temporal seasonal features

Including seasonal features (c.f. Eq. (8)), where possible, is often essential for accurate prediction. Example periodic multiples p for datasets with a variety of time units and seasonal components are listed below (Y = Yearly; Q = Quarterly; Mo = Monthly; W = Weekly; D = Daily; H; Hourly; Mi = Minutely; S = Secondly):
Q: Y = 4
Mo: Q = 3, Y = 12
W: Mo = 4.35, Q = 13.045, Y = 52.18
D: W = 7, Mo = 30.44, Q = 91.32, Y = 365.25
H: D = 24, W = 168, Mo = 730.5, Q = 2191.5, Y = 8766
Mi: H = 60, D = 1440, W = 10080, Mo = 43830, Q = 131490, Y = 525960
S: Mi = 60, H = 3600, D = 86400, W = 604800, Mo = 2629800, Q = 7889400, Y = 31557600

---

### Distance-to-agreement investigation of tomotherapy's bony anatomy-based autoregistration and planning target volume contour-based optimization [^a0c57732]. International Journal of Radiation Oncology, Biology, Physics (2013). Low credibility.

Purpose

To compare Tomotherapy's megavoltage computed tomography bony anatomy autoregistration with the best achievable registration, assuming no deformation and perfect knowledge of planning target volume (PTV) location.

Methods and Materials

Distance-to-agreement (DTA) of the PTV was determined by applying a rigid-body shift to the PTV region of interest of the prostate from its reference position, assuming no deformations. Planning target volume region of interest of the prostate was extracted from the patient archives. The reference position was set by the 6 degrees of freedom (dof)-x, y, z, roll, pitch, and yaw-optimization results from the previous study at this institution. The DTA and the compensating parameters were calculated by the shift of the PTV from the reference 6-dof to the 4-dof-x, y, z, and roll-optimization. In this study, the effectiveness of Tomotherapy's 4-dof bony anatomy-based autoregistration was compared with the idealized 4-dof PTV contour-based optimization.

Results

The maximum DTA (maxDTA) of the bony anatomy-based autoregistration was 3.2 Â± 1.9 mm, with the maximum value of 8.0 mm. The maxDTA of the contour-based optimization was 1.8 Â± 1.3 mm, with the maximum value of 5.7 mm. Comparison of Pearson correlation of the compensating parameters between the 2 4-dof optimization algorithms shows that there is a small but statistically significant correlation in y and z (0.236 and 0.300, respectively), whereas there is very weak correlation in x and roll (0.062 and 0.025, respectively).

Conclusions

We find that there is an average improvement of approximately 1 mm in terms of maxDTA on the PTV going from 4-dof bony anatomy-based autoregistration to the 4-dof contour-based optimization. Pearson correlation analysis of the 2 4-dof optimizations suggests that uncertainties due to deformation and inadequate resolution account for much of the compensating parameters, but pitch variation also makes a statistically significant contribution.

---

### KDOQI clinical practice guidelines and clinical practice recommendations for diabetes and chronic kidney disease [^71730538]. American Journal of Kidney Diseases (2007). Medium credibility.

KDOQI â€” Effect of lipid-lowering treatments on kidney function and albuminuria in diabetes summarizes study-level data by diabetes type, listing treatment dose, comparator, outcomes, baseline values, net effects and P values. In type 1 diabetes, Hommel, 1992 (3 mo, N 21) evaluated simvastatin 12.5 mg versus placebo for Î” GFR with baseline value 72 (64) and net effect -3 (P NS). In type 2 diabetes, Nagai, 2000 (4 y, N 71) compared pravastatin 10 mg with bezafibrate 400 mg for Î” ACR (mg/g) with baseline 102 (108) and net effect +3.3 (P NS). Tonolo, 2000 (10 mo, N 26) compared simvastatin 20 mg with cholestyramine 18 g for Î” AER (mg/d) with baseline 154 (154) and net effect -38 (P nd c). Smulders, 1997 (1 y, N 15) tested gemfibrozil 1,200 mg versus placebo for Î” ACR (%) with baseline 8.9 (14.2) and net effect -29% (P NS). Footnotes clarify that treatment dose is the "Maximum dose", baseline values are for the "treatment (comparator) arm", that "P value significant in the treatment arm for before vs after treatment", and that "In placebo arm, there was a significant decrease in GFR over 24 months compared to baseline (-10 mL/min, P = 0.025)".

---

### Analysis of multi-condition single-cell data with latent embedding multivariate regression [^5cef3f91]. Nature Genetics (2025). High credibility.

To construct the function space, we recall some concepts from differential geometry. Given the whole numbers G and P, the set of all orthonormal real matrices of dimension G Ã— P is a differentiable manifold, the Stiefel manifold. For our application, it is appropriate to consider two matrices equivalent if they span the same linear subspace of. The set of all such equivalence classes is again a differentiable manifold, called the Grassmann manifold Gr(G, P). Accordingly, elements of Gr(G, P) are P -dimensional linear subspaces of the ambient space. Computationally, we represent an element of Gr(G, P) by an orthonormal matrix, that is, by one of the members of the equivalence class.

We then constructas the set of all functions R that have domain and codomain as in equation (3) and can be written aswhere B is a three-dimensional real-valued tensor of size G Ã— P Ã— K. The expressionis the exponential map on the Grassmann manifold. It takes a point o âˆˆ Gr(G, P) and a tangent vector at that point and returns a new point on the Grassmann manifold. Thus, given a choice of the point o, which we call the base point, and of the design matrix X, the set of all possible B induces, and fitting equation (4) is achieved by fitting B. We use the terms tangent vector and tangent space in their standard meaning in differential geometry and represent tangent vectors with G Ã— P matrices. The name exponential map derives from the fact that, for some Riemannian manifolds, the exponential map coincides with the matrix exponential; however, this is not the case for Grassmann manifolds. Here, the exponential map for a base point o and a tangent vector represented byiswhere U and V are the left and right singular matrices of the singular-value decomposition of A (A = U diag(d) V T).

The argument of the exponential map in equation (5) is a linear combination of the slices of B. Each slice of B :: k represents a tangent vector and so are their linear combinations (a tangent space is a vector space).

We analogously parameterize the offset function Î³ (x) = âˆ‘ k Î“: k x k, where. Accordingly, fitting Î³ is just ordinary linear regression.

---

### Approximations to the expectations and variances of ratios of tree properties under the coalescent [^8355e038]. G3 (2022). Medium credibility.

Properties of gene genealogies such as tree height (H), total branch length (L), total lengths of external (E) and internal (I) branches, mean length of basal branches (B), and the underlying coalescence times (T) can be used to study population-genetic processes and to develop statistical tests of population-genetic models. Uses of tree features in statistical tests often rely on predictions that depend on pairwise relationships among such features. For genealogies under the coalescent, we provide exact expressions for Taylor approximations to expected values and variances of ratios Xn/Yn, for all 15 pairs among the variables {Hn, Ln, En, In, Bn, Tk}, considering n leaves and 2 â‰¤ k â‰¤ n. For expected values of the ratios, the approximations match closely with empirical simulation-based values. The approximations to the variances are not as accurate, but they generally match simulations in their trends as n increases. Although En has expectation 2 and Hn has expectation 2 in the limit as nâ†’âˆž, the approximation to the limiting expectation for En/Hn is not 1, instead equaling Ï€2/3â€“2â‰ˆ1.28987. The new approximations augment fundamental results in coalescent theory on the shapes of genealogical trees.

---

### The effect of gradient nonlinearities on fiber orientation estimates from spherical deconvolution of diffusion magnetic resonance imaging data [^19d816ea]. Human Brain Mapping (2021). Medium credibility.

Due to the relatively large gradient deviations in xâ€ and yâ€ direction and relatively small gradient deviation in zâ€direction, larger differences of FOD peak orientation in the xâ€ and yâ€fibers (top two rows) than in the zâ€fibers (bottom row) can be seen. When the fiber is along zâ€ direction, the distributions of the angular difference from CSD and dRL estimation are similar, regardless of gradient nonlinearity effects. For the simulations of xâ€ and yâ€direction fibers, gradient distortions generally increase the median of the angular deviation distribution of FOD peaks over 1Â° in dRL estimation.

Employing the dRLâ€mod framework did improve the angular accuracy of FOD estimation in xâ€ and yâ€directed fibers where the gradient deviations are over 10%. The distribution of angular deviations of dRLâ€mod FOD peaks matches well with the distribution of FODs estimated from signals without any gradient deviations, with medians of the distributions moving from [1.26Â° 1.31Â° 1.03Â°] of [x, y, z] orientation fibers with dRLâ€uni estimation to [0.9Â° 0.87Â° 0.92Â°] with dRLâ€mod estimation at b = 3,000 s/mm 2 and b = 2,000 s/mm 2. The medians of the angular deviations in FOD estimation are shown in supplementary material Table S1.

CSD slightly outperforms dRL in accuracy of FOD peak orientation estimation, while the angular deviations of CSD FOD peak orientations did not change much with or without the gradient deviation. Modifying the response function did not significantly improve the CSD estimation, with similar angular deviations from the ground truth within approximately 2Â° and a median of 0.8Â°. However, the angular deviations with the MRtrix CSD implementation in supplementary Figure S1 top row are showing smaller angular deviations using CSDâ€mod compared to CSDâ€uni in the presence of negative gradient deviations. At both b = 3,000 s/mm 2 and b = 1,000 s/mm 2, MRtrix CSDâ€mod performs better than MRtrix CSDâ€uni with negative gradient deviations, while the differences between CSDâ€uni and CSDâ€mod are not obvious with positive gradient deviations.

---

### The optimal MR acquisition strategy for exponential decay constants estimation [^e2d26e09]. Magnetic Resonance Imaging (2008). Low credibility.

Estimating the relaxation constant of an exponentially decaying signal from experimental MR data is fundamental in diffusion tensor imaging, fractional anisotropy mapping, measurements of transverse relaxation rates and contrast agent uptake. The precision of such measurements depends on the choice of acquisition parameters made at the design stage of the experiments. In this report, chi(2) fitting of multipoint data is used to demonstrate that the most efficient acquisition strategy is a two-point scheme. We also conjecture that the smallest coefficient of variation of the decay constant achievable in any N-point experiment is 3.6 times larger than that in the image intensity obtained by averaging N acquisitions with minimal exponential weighting.

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^f4bdcdc4]. Nature Communications (2025). High credibility.

Derivation of Î´ L from the standard gamma distribution

Due to its flexibility, the standard gamma distribution has been used as a model for many zero-bounded variables in a variety of fields. The gamma distribution is commonly defined with parameters for shape (k) and dispersion (Î´ L) where the pdf, indexed by x, iswith x, k, Î´ L > 0. The product of two exponential functions, describes all distribution shapes modelled by the pdf, whilenormalises the area below the pdf to be equal to one.

The mean and variance of the gamma distribution are, respectively, andwhere Î¼ > 0 and Ïƒ 2 > 0. It is clear from this formulation alone that the variance changes linearly with the mean while Î´ L is fixed and that the rate of this change is scaled by the dispersion of the distribution (Equation (22); see Fig. 2 e). It is also clear that the standard deviation (Ïƒ) has a square root relationship with the mean when the dispersion is fixed.

The solution to derive a mean-independent measure of dispersion for approximately gamma distributed variables is to simply retrieve the distribution's dispersion parameter, Î´ L. Recall that the variance had linear dependence on the mean (Equation (22)), which can be removed with the reciprocal of Î¼. As such, the mean-independent measure of dispersion, Î´ L, can be derived from the mean and variance withproving the mean independence of Î´ L for gamma distributed variables, where Ïƒ 2 and Î¼ can be estimated from a sample.

---

### Global strategy for the diagnosis, management, and prevention of chronic obstructive pulmonary disease (2025 report) [^f361165a]. GOLD (2025). High credibility.

Spirometry â€” considerations in performing spirometry (Figure 2.4) state that forced spirometry is the most reproducible and objective measurement of airflow obstruction, and peak expiratory flow measurement alone cannot be reliably used as the only diagnostic test because of its weak specificity. Preparation includes that spirometers should produce hard copy or have a digital display of the expiratory curve to permit detection of technical errors or have an automatic prompt to identify an unsatisfactory test and the reason for it. During performance, the pause between inspiration and expiration should be less than one second, the recording should go on long enough for a volume plateau to be reached, which may take more than 15 seconds in severe disease, and both FVC and FEV1 should be the largest value obtained from any of three technically satisfactory curves and the FVC and FEV1 values in these three curves should vary by no more than 5% or 150 mL, whichever is greater; the FEV1/FVC ratio should be taken from the technically acceptable curve with the largest sum of FVC and FEV1. For bronchodilation testing, possible dosage protocols are 400 mcg short-acting beta2-agonist, 160 mcg short-acting anticholinergic, or the two combined; FEV1 should be measured 10â€“15 minutes after a short-acting beta2-agonist is given, or 30â€“45 minutes after a short-acting anticholinergic or a combination of both classes of drugs, and patients already on bronchodilator treatment, in whom spirometry is requested for monitoring purposes do not need to stop their regular treatment for spirometry. For evaluation, spirometry measurements are evaluated by comparison of the results with appropriate reference values based on age, height and sex, and the presence of a post-bronchodilator FEV1/FVC < 0.7 confirms the presence of non-fully reversible airflow obstruction.

---

### Determination of the effective local lethal function for the nanOx model [^f3f5ac80]. Radiation Research (2020). Medium credibility.

NanOx is a biophysical model recently developed in the context of hadrontherapy to predict the cell survival probability from ionizing radiation. It postulates that this may be factorized into two independent terms describing the cell response to two classes of biological events that occur in the sequence of an irradiation: the local lethal events that occur at nanometric scale and can by themselves induce cell death, and the non-local lethal events that lead to cell death by an effect of accumulation and/or interaction at a larger scale. Here we address how local lethal events are modeled in terms of the inactivation of undifferentiated nanometric targets via an "effective local lethal function F ", which characterizes the response of each cell line to the spectra of "restricted specific energy". F is initially determined as a linear combination of basis functions. Then, a parametric expression is used to reproduce the function's main features, a threshold and a saturation, while at the same time reducing the number of free parameters. This strategy was applied to three cell lines in response to ions of different type and energy, which allows for benchmarking of the Î±(LET) curves predicted with both effective local lethal functions against the experimental data.

---

### Synthetic analog computation in living cells [^c53dfa6f]. Nature (2013). Excellent credibility.

A central goal of synthetic biology is to achieve multi-signal integration and processing in living cells for diagnostic, therapeutic and biotechnology applications. Digital logic has been used to build small-scale circuits, but other frameworks may be needed for efficient computation in the resource-limited environments of cells. Here we demonstrate that synthetic analog gene circuits can be engineered to execute sophisticated computational functions in living cells using just three transcription factors. Such synthetic analog gene circuits exploit feedback to implement logarithmically linear sensing, addition, ratiometric and power-law computations. The circuits exhibit Weber's law behaviour as in natural biological systems, operate over a wide dynamic range of up to four orders of magnitude and can be designed to have tunable transfer functions. Our circuits can be composed to implement higher-order functions that are well described by both intricate biochemical models and simple mathematical functions. By exploiting analog building-block functions that are already naturally present in cells, this approach efficiently implements arithmetic operations and complex functions in the logarithmic domain. Such circuits may lead to new applications for synthetic biology and biotechnology that require complex computations with limited parts, need wide-dynamic-range biosensing or would benefit from the fine control of gene expression.

---

### Input-output maps are strongly biased towards simple outputs [^50fca0dd]. Nature Communications (2018). Medium credibility.

Many systems in nature can be described using discrete input-output maps. Without knowing details about a map, there may seem to be no a priori reason to expect that a randomly chosen input would be more likely to generate one output over another. Here, by extending fundamental results from algorithmic information theory, we show instead that for many real-world maps, the a priori probability P(x) that randomly sampled inputs generate a particular output x decays exponentially with the approximate Kolmogorov complexity [Formula: see text] of that output. These input-output maps are biased towards simplicity. We derive an upper bound P(x)â‰²[Formula: see text], which is tight for most inputs. The constants a and b, as well as many properties of P(x), can be predicted with minimal knowledge of the map. We explore this strong bias towards simple outputs in systems ranging from the folding of RNA secondary structures to systems of coupled ordinary differential equations to a stochastic financial trading model.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^bb9b7f12]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Î” Î¦, the probability distribution at a point Î¦ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Î¦ âˆˆ {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters Î± and Î², which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since Î± and Î² are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Î¦ âˆˆ [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(Î±, Î²) and x 2 = 1 âˆ’ x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Î¦, it may seem natural to select Î± and Î² by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### Comparison of tumor volumes derived from glucose metabolic rate maps and SUV maps in dynamic 18F-FDG PET [^a82a97ef]. Journal of Nuclear Medicine (2008). Low credibility.

Unlabelled

Tumor delineation using noninvasive medical imaging modalities is important to determine the target volume in radiation treatment planning and to evaluate treatment response. It is expected that combined use of CT and functional information from 18F-FDG PET will improve tumor delineation. However, until now, tumor delineation using PET has been based on static images of 18F-FDG standardized uptake values (SUVs). 18F-FDG uptake depends not only on tumor physiology but also on blood supply, distribution volume, and competitive uptake processes in other tissues. Moreover, 18F-FDG uptake in tumor tissue and in surrounding healthy tissue depends on the time after injection. Therefore, it is expected that the glucose metabolic rate (MRglu) derived from dynamic PET scans gives a better representation of the tumor activity than does SUV. The aim of this study was to determine tumor volumes in MRglu maps and to compare them with the values from SUV maps.

Methods

Twenty-nine lesions in 16 dynamic 18F-FDG PET scans in 13 patients with non-small cell lung carcinoma were analyzed. MRglu values were calculated on a voxel-by-voxel basis using the standard 2-compartment 18F-FDG model with trapping in the linear approximation (Patlak analysis). The blood input function was obtained by arterial sampling. Tumor volumes were determined in SUV maps of the last time frame and in MRglu maps using 3-dimensional isocontours at 50% of the maximum SUV and the maximum MRglu, respectively.

Results

Tumor volumes based on SUV contouring ranged from 1.31 to 52.16 cm3, with a median of 8.57 cm3. Volumes based on MRglu ranged from 0.95 to 37.29 cm3, with a median of 3.14 cm3. For all lesions, the MRglu volumes were significantly smaller than the SUV volumes. The percentage differences (defined as 100% x (V MRglu - V SUV)/V SUV, where V is volume) ranged from -12.8% to -84.8%, with a median of -32.8%.

Conclusion

Tumor volumes from MRglu maps were significantly smaller than SUV-based volumes. These findings can be of importance for PET-based radiotherapy planning and therapy response monitoring.

---

### Article 5. An introduction to estimation â€“ 2: from z to T [^fb82b806]. Emergency Medicine Journal (2001). Low credibility.

Provided the sample size is large enough (that is, n greater than 100), the z statistic can be used to determine the confidence interval estimation of the population mean even when the sigma is not known. In these cases the estimation of the standard error of the mean is used. The z statistic is also valid when determining the population's proportion based upon a large sample. However, when dealing with smaller samples, the z statistic is replaced by the t statistic. This makes it possible to estimate, in a population with an unknown standard deviation: The probability of getting a sample mean greater than or equal to a particular value The value of a sample mean with a particular probability of occurring The probability of getting a sample mean between two particular values The confidence interval for the estimation of the population mean can also be determined using the t statistic.

---

### American Clinical Neurophysiology Society technical standards for electrical stimulation with intracranial electrodes for functional brain mapping and seizure induction [^895068d1]. Journal of Clinical Neurophysiology (2025). High credibility.

Electrical stimulation mapping (ESM) settings and charge density â€” ESM settings should allow focal and reproducible perturbation of neurological function while avoiding neuronal injury, yet the propagation of current and cellular interactions are not sufficiently known to inform specific settings. Charge density is defined as charge delivered (current x duration) per unit surface area of the electrode contact, and there is paucity of human data regarding its relationship to neuronal injury. A maximum charge density of 52â€“57 Î¼C/cm2/phase is suggested for SDE ESM; it is difficult to extrapolate this value to SEEG, in part because up to 87% of the current may be shunted by CSF in case of SDE unlike that for SEEG. For comparison, the FDA approved limit for deep brain stimulation is 30 Î¼C/cm2/phase.

---

### Iodinated I-131 albumin (volumex) [^d0a70598]. FDA (2022). Medium credibility.

2. Plot the 12, 18, 24, 30 and 36 minute sample counts (net cpm/mL) on semilog graph paper using the average count value of each sample and determine the radioconcentration at injection time (zero time) by drawing a straight line through the 12, 18, 24, 30 and 36 minute points to zero time. The x ordinate of the

graph is the sample withdrawal time and the logarithmic y ordinate is

radioconcentration in net cpm/mL.

3. **Calculate patient's blood volume (in mL) using the following formula**:

Net cpm/mL reference solution blood

Net cpm/mL patient's blood sample x DF = volume

(in mL)

Sample Blood Volume Calculations

Volume of Blood sample aliquot = 1.0 mL

Volume of Reference Solution aliquot = 1.0 mL

Net counts at zero time = 48,100

Net counts obtained from reference solution aliquot = 52, 430

If, for example, DF, dilution factor of the reference solution = 4000

Using the above formula gives = 52,430 x 4000 = 4360 mL

48,100

Serial Blood Volume Determinations

Iodinated I 131 Albumin Injection is administered in sufficiently low dosage to permit repetitions as often as required by clinical circumstances. It must be remembered that it is always necessary to correct for background radioactivity remaining in the blood from former determinations. Therefore, for each determination after the first one, a background blood sample must be taken just before the iodinated I 131 albumin is injected.

---

### Recommendations for the evaluation of left ventricular diastolic function by echocardiography: an update from the American Society of Echocardiography and the European association of Cardiovascular imaging [^02c9ad9d]. Journal of the American Society of Echocardiography (2016). Medium credibility.

Left ventricular diastolic function â€” recommended variables and interpretation are as follows: The four recommended variables and abnormal cutoffs are annular eâ€² velocity (septal eâ€² < 7 cm/sec, lateral eâ€² < 10 cm/sec), average E/eâ€² ratio > 14, LA volume index > 34 mL/m^2, and peak TR velocity > 2.8 m/sec. LV diastolic function is normal if more than half of the available variables do not meet these cutoff values, diastolic dysfunction is present if more than half meet them, and the study is inconclusive if half meet them. Average E/eâ€² ratio is recommended for simplification, and when only one annulus is available a lateral E/eâ€² ratio > 13 or a septal E/eâ€² > 15 is considered abnormal. LA maximum volume index is recommended rather than LA anteroposterior diameter by Mâ€‘mode.

---

### ACR-AAPM-SPR practice parameter for diagnostic reference levels and achievable doses in medical X-ray imaging [^a7ff7c7f]. ACR/AAPM/SPR (2023). High credibility.

Documentation â€” Reporting should be in accordance with the ACR Practice Parameter for Communication of Diagnostic Imaging Findings.

---

### Exploring non-equilibrium processes and spatio-temporal scaling laws in heated egg yolk using coherent X-rays [^82b9f8d8]. Nature Communications (2023). High credibility.

Fig. 3
Kinetics of LDL aggregation at high temperatures.

a I (q) profiles collected at a sample temperature of T = 85 Â°C as a function of isothermal waiting time, t w as indicated by the colour bar. The error bars indicate the standard deviation. The dashed line represents ~ q âˆ’4. b The correlation length estimated from the scattering profiles as a function of t w for sample temperatures T â‰¥ 75 Â°C. The error bars are obtained from the error in q peak estimation (see Supplementary Note 7). The kinetics of LDL aggregation shows two growth regimes: stage-I - exponential growth phase, and stage-II - power-law growth phase. The solid lines are sigmoidal fits of log 10 (Î¶) vs log 10 (t w) using Eq. (5). The light-coloured shaded area around the solid sigmoidal fit lines represents the 95% confidence band. The green dashed lines in stage-II are power-law fits. c The correlation length as a function of normalised t w with respect to. For convenience, double x and y scales are given in b, c. The temperature dependence ofis shown in an Arrhenius plot in the inset of c. The error bar inindicate the parameter uncertainty obtained from the fits using least-squares minimisation. The green line represents an Arrhenius fit with an activation energy of 410 Â± 20 kJ/mol (â‰ˆ98 kcal/mol or â‰ˆ4.2 eV). The horizontal dashed line in b, c indicates the boundary between stage-I and stage-II. Source data are provided as a Source Data file.

---

### A geometric approach to characterize the functional identity of single cells [^00e6c55b]. Nature Communications (2018). Medium credibility.

Computing ACTION metric as a measure of similarity between cells

The transcriptome of each cell consists of genes that are expressed at different levels and have different specificity with respect to the underlying cell types. Universally expressed genes correspond to the subset of genes responsible for mediating core cellular functions. These functions are needed by all cells to function properly, which result in ubiquitous expression of these genes across all cell types. While fundamental to cellular function, these genes are not informative with respect to the identity of cells. On the other hand, cell-type-specific genes are preferentially expressed in one or a few selected cell types to perform cell-type-specific functions. Unlike universally expressed genes, cell-type-specific genes are, typically, weakly expressed, but are highly relevant for grouping cells according to their common functions. Our goal here is to define a similarity measure between cells that suppresses universally expressed genes and enhances the signal contained in cell-type-specific genes.

To suppress the ubiquitously high expression of universally expressed genes, we adopt a method that we developed recently for bulk tissue measurements and extend it to single-cell analysis. This method projects a standardized representation of expression profiles of cells onto the orthogonal subspace of universally expressed genes. Let us denote the raw expression profile of cells using matrix, where each row corresponds to a gene and each column represents a cell. We use x i to denote the expression profile of i th cell. In addition, let us denote the signature vector of universally expressed genes by v. As a first order estimate, a universally expressed signature is computed by taking the average expression over all cells:; that is, v i is the average expression of gene i across all samples. This choice is motivated by the fact that highly expressed genes are more consistently expressed, whereas lowly expressed genes show exhibit higher variability. To this end, by orthogonalizing with respect to the mean value, we significantly reduce the effect of universally expressed genes, while preserving the variation of lowly expressed, but preferential ones. After estimating this baseline expression, we z -score normalize the profile of each cell: where Î¼ i and Ïƒ i are the mean and sample standard deviation of the entries in the i th cell profile. Similarly, we z -score normalize the signature vector of universally expressed genes, v, to create a new vector z v. Finally, we project out the impact of the universally expressed gene expressions on each cell's profile as follows:

---

### Addressing sex as a biological variable in preclinical models of lung disease: an official American Thoracic Society research statement [^044a47b9]. American Journal of Respiratory and Critical Care Medicine (2025). High credibility.

Figure 1 logic tree â€” delineating mechanisms behind sex differences in experimental lung disease phenotypes outlines a pathway from the question 'Does the phenotype show a sex difference?' to 'Discriminate effects of gonadal hormones vs. sex chromosomes', including 'Do changes in levels of gonadal hormones (gonadectomy/ hormone replacement) explain the sex difference?' with mechanistic questions ('Which hormones cause sex difference? Which receptors mediate hormone effect? When does hormone act? Where is hormone produced? Which cell type is the target of hormone effect?'). The algorithm incorporates the four core genotypes (FCG) model â€” 'Use FCG model to test for both sex chromosome and gonadal hormone effects' â€” and downstream steps ('Identify downstream molecular effects of hormones in specific cell types mediating sex differences' and 'Measure interactions of hormones and sex chromosome genes'). For sex chromosome mechanisms, it directs 'Use XY* model to discover X effect or Y effect', 'Manipulate expression of specific X or Y genes to discover which genes account for sex chromosome effect', and 'Identify downstream molecular effects of sex chromosome genes in specific cell types mediating sex differences'; if no difference is observed, 'Consider sex-specific effects that counterbalance each other'.

---

### Influence of noise correlation in multiple-coil statistical models with sum of squares reconstruction [^47d0e721]. Magnetic Resonance in Medicine (2012). Low credibility.

Noise in the composite magnitude signal from multiple-coil systems is usually assumed to follow a noncentral Ï‡ distribution when sum of squares is used to combine images sensed at different coils. However, this is true only if the variance of noise is the same for all coils, and no correlation exists between them. We show how correlations may be obviated from this model if effective values are considered. This implies a reduced effective number of coils and an increased effective variance of noise. In addition, the effective variance of noise becomes signal-dependent.

---

### Thermalization and criticality on an analogue-digital quantum simulator [^49e547d9]. Nature (2025). Excellent credibility.

Fig. 3
Critical coarsening in the XY model.

a, Left, experimental schematic of qubit frequencies (blue) and coupling (yellow). Right, phase diagram. Dynamics become diabatic (dashed black) with increased temperature (T) when inverse remaining time (red) exceeds gap (green; Î” âˆ âˆ£ g âˆ’ g c âˆ£ Î½ z). QCP and CCP denote the quantum and classical critical phases, respectively. b, The final energy density approaches the ground state value (Îµ gs, grey) and Kosterlitzâ€“Thouless transition value (Îµ KT, black) as t r is increased. Red circles and squares indicate single-qubit (SQ) and Bell basis measurements, respectively. Blue, MPS simulation. Purple shading indicates where classical critical behaviour is expected. c, Average correlation,(found from averaging (âŸ¨ X i X j + Y i Y j âŸ© âˆ’ âŸ¨ X i âŸ©âŸ¨ X j âŸ© âˆ’ âŸ¨ Y i âŸ©âŸ¨ Y j âŸ©)/2 over all pairs i, j separated by r) measured at various t r. d, Decay of radially averaged correlations. Green and purple curves show examples of exponential and power-law fits, respectively, performed up to a distance of 6 to avoid finite-size effects at longer distances. Error bars represent one standard deviation estimated from bootstrapping (N reps = 5 Ã— 10 4 repetitions). e, Ratio between r.m.s. errors from power-law and exponential fits decreases for g m t r > 15. f, Power-law exponent, Î³, approaches expected value at Kosterlitzâ€“Thouless transition (1/4; black line). g, Vortex density proxy, n V, decreases to minimum of 2 Ã— 10 âˆ’2. h, Correlation length increases with t r. Both simulation results (blue) and experimental data (red) show substantially more superlinear growth than KZ predictions (dashed black). Diamonds, correlation lengths extracted at expected freezing point (i). i, Correlation length during the ramp, shown with and without rescaling (main and inset, respectively) and two-sided logarithmic axes for âˆ£ Ï„ âˆ£ > Ï„ KZ. Î¾ KZ is found from fittingwith Î² = 0.9(1) (difference from Î² = Î½ = 0.67 expected to be due to finite-size effects). Dashed coloured lines show the theoretically expected scaling, f (x) = âˆ£ x âˆ£ âˆ’ Î½ with Î½ = 0.67 for x < âˆ’1 (purple) and a heuristic f (x) = x Î· with Î· = 1 for x > 1 (teal). The increase in Î¾ beyond the freezing point (diamond) causes deviation from KZ predictions.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^f4eb04eb]. Annals of the American Thoracic Society (2023). High credibility.

13N-nitrogen positron emission tomography (PET) tracer kinetics â€” relationships for specific ventilation (sV) and perfusion (Q) are outlined: In a single-compartment voxel, the washout plateau at first appearance is proportional to regional perfusion; tracer washout is proportional to regional sV and the slope of washout is equal to â€“1/sV; the area under the curve is proportional to the Q/sV ratio. In two-compartment behavior, compartment 1 has high sV and compartment 2 low sV; during washout there is a steep initial slope in compartment 1, total blood flow in the voxel (Q1 + Q2) is reflected in the plateau, and the plateau is apportioned between compartments based on the back-extrapolated point to the onset of washout in compartment 2.

---

### A geometric approach to characterize the functional identity of single cells [^b6395f5f]. Nature Communications (2018). Medium credibility.

Single-cell transcriptomic data has the potential to radically redefine our view of cell-type identity. Cells that were previously believed to be homogeneous are now clearly distinguishable in terms of their expression phenotype. Methods for automatically characterizing the functional identity of cells, and their associated properties, can be used to uncover processes involved in lineage differentiation as well as sub-typing cancer cells. They can also be used to suggest personalized therapies based on molecular signatures associated with pathology. We develop a new method, called ACTION, to infer the functional identity of cells from their transcriptional profile, classify them based on their dominant function, and reconstruct regulatory networks that are responsible for mediating their identity. Using ACTION, we identify novel Melanoma subtypes with differential survival rates and therapeutic responses, for which we provide biomarkers along with their underlying regulatory networks.

---

### The snm procedure guideline for general imaging 6.0 [^773d53ba]. SNMMI (2010). Medium credibility.

Gated imaging â€” electrocardiogram (ECG) gating is used to synchronize image acquisition with the patient's heart rate, and the number of frames per R-R interval should be no less than 16 for ejection fraction measurements and 32 for time-based measurements. For gated cardiac blood pool imaging, electronic zoom generally should be used to magnify the field of view to approximately 25 cm, a matrix size of 64 x 64 is sufficient, and typically a total of at least 5 million counts in the entire study will provide sufficient statistics for quantitative and functional image processing.

---

### Single photon emission computed tomography (SPECT) myocardial perfusion imaging guidelines: instrumentation, acquisition, processing, and interpretation [^097e8756]. Journal of Nuclear Cardiology (2018). Medium credibility.

Table 2 â€” typical performance parameters for low-energy (< 150 keV) parallel-hole collimators â€” reports resolution (full width at half maximum [FWHM]) at 10 cm, efficiency (%), and relative efficiency: Ultra-high resolution: 6.0 mm, 0.005, 0.3; High resolution: 7.8 mm, 0.010, 0.6; All/general purpose: 9.3 mm, 0.017, 1.0*; High sensitivity: 13.2 mm, 0.035, 2.1. A footnote specifies that a relative efficiency of 1 corresponds approximately to a collimator efficiency of 0.017%, and defines efficiency as the fraction of gamma rays and X-rays passing through the collimator per gamma ray and X-ray emitted by the source.

---

### Casein [^fd0595f4]. FDA (2009). Low credibility.

Volume desired x Concentration desired = Volume needed x Concentration available.

Example 1: If a 1:10 w/v extract is available and it is desired to use a 1:1,000 w/v extract substitute as follows:

Vd x Cd = Vn x Ca

10ml x 0.001 = Vn x 0.1

0.1 mL = Vn

Using a sterile technique, remove 0.10 mL of extract from the 1:10 vial and place it into a vial containing 9.90 mL of sterile diluent. The resulting ratio will be a 10 mL vial of 1:1,000 w/v.

Example 2: If a 10,000 pnu/ml extract is available and it is desired to use a 100 pnu/ml extract substitute as follows:

10ml x 100 = Vn x 10,000

0.1 mL = Vn

Using a sterile technique, remove 0.10 mL of extract from the 10,000 pnu/ml vial and place it into a vial containing 9.90 mL of sterile diluent. The resulting concentration will be a 10 mL vial of 100 pnu/ml.

Example 3: If a 10,000 AU/ml or BAU/ml extract is available and it is desired to use a 100 AU/ml or BAU/ml extract substitute as follows: Vd x Cd = Vn x Ca

---

### Structural and functional insights into the delivery of a bacterial rhs pore-forming toxin to the membrane [^4d6c432e]. Nature Communications (2023). High credibility.

Small-angle X-ray scattering study of Tse5, Tse5-Î”CT, Tse5-D1141A, and Tse5-K47G-P48A

Synchrotron X-ray scattering data for purified wild-type Tse5 and variants Tse5-K47G-P48A, Tse5-D1141A and Tse5-Î”CT were collected on an EigerX 4 M (Dectris) pixel detector at Diamond Light Source B21 beamline (UK). Data collection and structural parameters are reported in Supplementary Table 3. The scattering patterns were measured with a 3-s exposure time per frame in a continuous mode using an in-line Agilent HPLC system connected to a Shodex KW-403 column (exclusion limits: 10â€“700 kDa), equilibrated in 50 mM pH 8.0, 150 mM NaCl and 2 mM DTT, and running at 0.16 mlÂ·min âˆ’1. Proteins were injected at 2.5 mg mL âˆ’1.

To check for radiation damage, the accumulated frames corresponding to eluted peaks were compared as a time series, and no radiation damage was observed. Using the sample-to-detector distance of 3.72 m, the range of momentum transfer values is 0.0045 < q < 0.34 Ã… âˆ’1 (q = 4Ï€Â·sin(Î¸)/ Î» where 2 Î¸ is the scattering angle, and Î» = 1 Ã… is the X-ray wavelength).

Data were processed using standard procedures by the programme packages ScÃ…tter IV and PRIMUS 3.1. The forward scattering [I (0)] was evaluated using the Guinier approximationassuming the intensity is represented as I (q) = I (0)exp(-(qR g) 2/3) for a minimal range of momentum transfer values (q < 1.3/ R g). The maximum dimensions (d max), the interatomic distance distribution functions [P (r)], and the radii of gyration (R g) were computed using GNOM. The molecular mass was estimated by a Bayesian approach(a concentration-independent method), providing a probability estimate and credibility interval of the particles.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^89cdccc2]. Annals of the American Thoracic Society (2023). High credibility.

V_A/Q distribution modeling â€” derived from cumulative plots of ventilation or perfusion versus V_A/Q â€” is binned into 50 equally spaced log-scale compartments, with shunt and dead space handled separately. Specifically, V_A/Q ratios < 0.005 (shunt) and > 100 (dead space) are calculated as separate components, and relative dispersion and gravitational gradients may also be assessed.

---

### Perfect linear optics using silicon photonics [^5e7cfabb]. Nature Communications (2024). High credibility.

In order to ease the understanding of the programming procedure, we indicatively illustrate in Fig. 2b the sum ofandexperimental values for the 4 constituent paths of the 1st Xbar column with r = [1,2,3,4]. These values were calculated via the HA model when a single node is driven in the rangeand the remaining 3 EAMs are driven at 0 V bias, revealing the divergence from the expected performance when using the standalone EAM metrics (baseline model). Figure 2c schematically captures the effectiveness of the developed HA programming model, by putting in juxtaposition the experimentally measured Y 1 output for the #256 linear transformation sets (black dashed line) with respect to the theoretically calculated Y 1 output when using the baseline programming model (gray curve) and the theoretically calculated Y 1 output when using the HA programming model (orange curve), revealing the excellent matching between the experimental curve and the curve calculated via the HA model. A quantitative representation of the model's accuracy in describing the underlying hardware parameters is presented in Fig. 2d, where the error of the baseline and HA programming model outputs with respect to the experimentally obtained response is plotted with the gray and orange histograms, respectively. Fitting a Gaussian distribution into the acquired error values results in mean and standard deviation values that equal âˆ’5.45 and 2.04, and ~2 * 10 âˆ’3 and 0.41 for the baseline and HA models respectively, revealing the lack of a biased-error parameter in our HA-model and a significantly lower standard deviation. The same procedure was followed for the remaining 3 Xbar columns and 12 constituent EAM-based nodes, revealing a small deviation of < 2% in the derived mean and standard deviation values.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^59d10d7c]. Annals of the American Thoracic Society (2023). High credibility.

Ventilationâ€“perfusion (V A /Q) distribution modeling is binned into 50 equally spaced (log scale) compartments, with V A /Q ratios < 0.005 (shunt) and > 100 (dead space) separately calculated, and relative dispersion and gravitational gradients may also be assessed.