# Medical Question & Answer

**Sample ID**: 00af11d2-9ea8-9427-1ce0-a310c7e368ab
**Dataset Index**: 345924

---

## Question

15. (1999 American High School Mathematics Examination) Given six points on a circle, any combination of four chords can form a convex quadrilateral. The probability that they form a convex quadrilateral is:
A. [1/15]
B. [1/91]
C. [1/273]
D. [1/455]
E. [1/1365]

---

## Answer

> Let's see… What do we have here? The user is asking for the probability that four randomly chosen chords from six points on a circle form a convex quadrilateral. Let's break this down step-by-step. First, I need to think about how many total ways there are to choose four chords from six points. Then, I should verify how many of those combinations actually yield a convex quadrilateral. Finally, I will compute the probability and check which option matches, making sure I haven't overcounted or miscounted anywhere along the way.

> Let me first confirm the total number of ways to choose four chords from six points on a circle. Wait, let me verify the wording: "any combination of four chords" suggests we are selecting a set of four chords, not four vertices. With six points on a circle, the total number of chords is C(6, 2) = 15, so the number of ways to choose four chords is C(15, 4). Let me calculate that: C(15, 4) = 15 × 14 × 13 × 12 / (4 × 3 × 2 × 1) = 1365, so the denominator should be 1365.

> Hold on, let's not jump to conclusions about what constitutes a convex quadrilateral here. I need to ensure I'm interpreting the geometric condition correctly. Four chords will form a convex quadrilateral if and only if their eight endpoints are distinct and alternate around the circle, which means the chords are non-intersecting and their endpoints are interleaved. Let me think about this: if the endpoints are labeled in cyclic order as A, B, C, D, E, F, then a convex quadrilateral arises when the four chords connect pairs that do not cross, such as AB, CD, EF, but wait, that's only three chords, not four. Hmm, wait a minute, I should correct that: I meant to say that the four chords must be non-intersecting and their eight endpoints must be distinct and alternate, which implies the chords are a non-crossing matching of eight distinct vertices. But with six points total, I cannot pick eight distinct vertices, so I need to reconsider the setup.

> Let me reconsider the problem statement carefully. The phrase "any combination of four chords can form a convex quadrilateral" is ambiguous. Initially, I thought it meant choosing four chords from the 15 possible chords, but that cannot guarantee a quadrilateral at all, let alone a convex one, because chords can share endpoints and may not even define a four-sided figure. I should confirm the intended interpretation: the standard combinatorics interpretation is that we choose four distinct vertices from the six points and then consider the quadrilateral formed by connecting them in cyclic order; that quadrilateral is convex if and only if the four vertices are in convex position on the circle, which is always true for four distinct points on a circle. Wait, that would make every such quadrilateral convex, which would imply probability 1, which is clearly not among the options. Hold on, I should verify the actual intent: the intended meaning is that we choose four chords that are the sides of a quadrilateral, i.e., we choose four vertices and then the four chords that connect them in order, and we want the probability that this quadrilateral is convex, which, for points on a circle, is equivalent to the four vertices being distinct and in cyclic order without self-intersections. But since any four distinct points on a circle are in convex position, the non-convex case would arise only if the "quadrilateral" is degenerate in some way, which typically means the four chords are not the sides of a simple quadrilateral. Let me think about this differently: the usual combinatorics question is to choose four vertices and ask how many of the resulting quadrilaterals are convex, and on a circle that is all of them, so that cannot be the question. Alternatively, the question might be about the probability that four randomly chosen chords are the sides of a convex quadrilateral, which requires that the four chords are precisely the sides of a quadrilateral whose vertices are four distinct points in cyclic order. I should double-check the classic formulation: the standard problem is to choose four vertices and ask for the probability that the quadrilateral they form is convex, and on a circle that probability is 1, which again does not match the options. Hmm, wait a minute, perhaps the intended interpretation is that we choose four chords at random from the 15 possible chords, and we want the probability that these four chords are the sides of some convex quadrilateral, i.e., that there exist four distinct vertices on the circle such that the four chords are exactly the sides of the quadrilateral they define in cyclic order. Let me proceed with that interpretation and see if it yields one of the given answers.

> I will now examine the favorable cases under this interpretation. For four chords to be the sides of a convex quadrilateral, they must be non-intersecting and their eight endpoints must be distinct and alternate around the circle. But wait, with only six points on the circle, I cannot have eight distinct endpoints, so this is impossible. Hold on, I should verify whether the problem is actually about choosing four vertices rather than four chords. Let me pivot to the classic formulation: choose four distinct vertices from six, and ask for the probability that the quadrilateral they form is convex. On a circle, any four distinct points are in convex position, so the probability would be 1, which is not an option. I need to check the original AHSME context: the canonical version of this problem is to choose four vertices and ask for the probability that the quadrilateral is convex, and the answer is 1, but since 1 is not among the choices, I suspect the question has been misstated or the intended interpretation is different. Let me consider another possibility: perhaps "four chords" refers to the two diagonals and two sides, or some other subset that can be non-convex in configuration, but that seems inconsistent with standard usage. Given the options, I should reconsider whether the question is actually asking for the probability that four randomly chosen vertices form a convex quadrilateral, in which case the answer is 1, but since that is not offered, I may need to infer the intended counting from the provided choices.

> Let me think about the arithmetic with the given options. The denominator 1365 matches C(15, 4), which is the total number of ways to choose four chords from 15, so the question likely intends that we are choosing four chords and counting how many of those selections correspond to the sides of a convex quadrilateral. But wait, as I noted, with only six points, four chords cannot have eight distinct endpoints, so strictly speaking, zero selections would satisfy the literal geometric condition, which would imply probability 0, again not an option. I should double-check whether the intended interpretation is that the four chords are the sides of a quadrilateral defined by four of the six points, in which case the number of such quadrilaterals is C(6, 4) = 15, and if we assume that only one of the C(15, 4) chord selections corresponds to a convex quadrilateral, the probability would be 1/1365, which is option E. But hold on, that assumption is unjustified: there are 15 different convex quadrilaterals (one for each choice of four vertices), and for each such quadrilateral, there is exactly one set of four chords that are its sides, so there are 15 favorable outcomes, not 1. Thus, the probability would be 15/1365 = 1/91, which is option B. Let me verify: 15/1365 simplifies to 1/91, yes, so if the intended interpretation is that we choose four chords and want them to be the sides of some convex quadrilateral, the probability is 1/91.

> Final answer: B. 1/91.

---

The probability that four randomly chosen chords from six points on a circle form a convex quadrilateral is **1/91**. This is because there are C(15, 4) = 1365 ways to choose four chords, and only 15 of these combinations correspond to the sides of a convex quadrilateral (one for each set of four vertices). Thus, the probability is 15/1365 = **1/91**.

---

## Total number of ways to choose four chords

With six points on a circle, the total number of chords is C(6, 2) = 15. The number of ways to choose four chords is C(15, 4) = 1365.

---

## Number of ways to form a convex quadrilateral

A convex quadrilateral is formed by selecting four distinct points on the circle and connecting them in order. The number of ways to choose four points from six is C(6, 4) = 15. Each set of four points defines exactly one convex quadrilateral, so there are 15 valid combinations.

---

## Probability calculation

The probability is the ratio of favorable outcomes to total outcomes:

Probability = [Number of favorable outcomes/Total number of outcomes] = [15/1365] = [1/91]

---

## Conclusion

The probability that four randomly chosen chords from six points on a circle form a convex quadrilateral is **1/91**.

---

## References

### Quantitative lung morphology: semi-automated measurement of mean linear intercept [^117TumPD]. BMC Pulmonary Medicine (2019). Medium credibility.

Optimization of number of chords to sample to accurately estimate MLI actual

Application of the method yielded a data set of 118,716 chords measured across the computer-generated images of uniform-radius circles. A repeated random subsampling algorithm was implemented on this dataset to estimate MLI measured using N randomly sampled chords from the pool of all chords measured per image (numbering N max); N was an integer on the interval [2, N max] and for each N, 5 random subsamples were used. The number of chords sampled per image was then analyzed via ROC curve to determine the ability of N chords to accurately estimate MLI actual as p ≥ 0.05 by one-sample t-test, Fig. 3 a. AUC ROC is a nearly monotonic function of r stratum, Fig. 3 b. This non-monotonicity is likely due to higher relative noise when measuring smaller radii circles. We then used Youden's Index to estimate the optimal number of chords to sample to accurately measure MLI actual, Table 1. The optimal number of chords per r stratum was used to calculate MLI measured, Fig. 4. Finally, a reliability analysis was performed on MLI measured, stratified by r, using a two-way mixed effects model to determine the reliability of the measurements. We demonstrated excellent reliability, with ICC = 0.9998 and p < 0.0001, Fig. 4.

Fig. 3
ROC Curves to Optimize the Number of Chords to Sample. a ROC curves were generated to test the sensitivity and specificity of number of chords in estimating MLI actual per radius stratum. b AUC ROC values and 95% confidence intervals as a classification performance measure

Table 1
Optimal number of chords to estimate MLI actual

Red shading — failure to estimate MLI actual (AUC ROC < 0.5)

Yellow shading — fair estimation (0.6 < AUC ROC < 0.8)

Green shading — good estimation (AUC ROC > 0.8)

Fig. 4
Dot Plot of MLI measured. Dot plot of intraclass correlation. We demonstrate high reliability with ICC = 0.9998. Vertical rule lines represent MLI actual. Represents significant deviation from MLI actual by one-sample t-test, p < 0.05. Data for MLI actual corresponding to radii of 5 or 10 pixels not shown due to poor AUC ROC as discussed in Table 1

---

### Confidence reports in decision-making with multiple alternatives violate the Bayesian confidence hypothesis [^112kJxsV]. Nature Communications (2020). High credibility.

Experiment 2

In Experiment 2, we aimed to test whether the findings in Experiment 1 could be generalized to other stimulus configurations, where the centers of the categories varied in a two-dimensional space. We tested four conditions in which the centers of the three groups varied along both horizontal and vertical axis (Fig. 1c). We sampled the target dot positions uniformly within a circular area centered on the screen. In addition, the distribution of the categories used in Experiment 2 allowed us to probe confidence reports in a wider range of posterior distributions (Supplementary Fig. 1B). For example, we can probe the confidence report when the target dot had the same distance to all three categories in Experiment 2, but not in Experiment 1.

The "psychometric curve" is now a heat map in two dimensions (Fig. 5). The fits to these psychometric curves showed different patterns among the three models: When the three groups formed an equilateral triangle (Fig. 5, the first and second columns), the confidence (as a function of target location) estimated by the Entropy model exhibited contours that were more convex than that in the data. In the last two conditions (Fig. 5, the third and fourth columns), compared to the other two models, the Difference model showed stronger resemblance to the data, as the model exhibited an extended low confidence region at the side where two categories were positioned closely. The results of model comparisons were consistent with Experiment 1. The Difference model outperformed the Max model by a group-summed AIC score of 541 (95% CI [371, 735]) and the Entropy model by 1631 (95% CI [1179, 2159]) (Fig. 4b). The model with both sensory and inference noise explained the data the best, and the inference noise had a stronger influence on the model fit than the sensory noise (Supplementary Fig. 4B, Supplementary Fig. 5B, Supplementary Tables 1 and 2).

Fig. 5
Experiment 2.

a The mean confidence report as a function of target positions. b Model fit averaged across individuals. The red crosses in each panel represent the center of each of the three categories.

Consistent with Experiment 1, we found that the Difference model outperformed the Max and the Entropy model when we only fitted the confidence reports (Supplementary Fig. 6B). Models that considered other sources of variability or used heuristic decision rules did not perform better than the Difference model (Supplementary Fig. 10B).

---

### The pupil responds spontaneously to perceived numerosity [^115o4UMY]. Nature Communications (2021). High credibility.

To match the total number of pixels in the arrays (and hence luminance), as well as the covered area (convex hull), stimuli were precalculated offline through multiple steps. In the first step, the isolated stimuli were created: coordinates were randomly generated for twice the number of the desired dot positions, half for the dots and the other half for the lines, with the constraint that neither dots nor lines could be closer than 0.5°. In a second step, the connected stimuli were created by connecting randomly chosen couples of dots, with line length as detailed above. In the third step, the total number of displayed pixels and the convex hull of the isolated dots was modified to match those of the connected stimuli. For the stimuli of experiment 2, dots were enlarged so that the overall ink of the isolated stimulus matched those of the dumbbells (which contained the connectors). For stimuli of experiment 1, the length of the isolated lines was iteratively adjusted until it matched the total number of pixels of the connected stimuli (which would otherwise contain less pixels due to the over imposition of the lines with the dots at the point of connection between the two). Then the position of the isolated dot pairs was iteratively modified until the convex hull matched that of the connected stimuli (which would otherwise have had higher probability of appearing sparser than the isolated stimuli). The final convex hull was 513 deg 2.

---

### General theory for packing icosahedral shells into multi-component aggregates [^117WFsE4]. Nature Communications (2025). High credibility.

Results

Mapping icosahedra into paths

Here, we develop our theoretical framework, whose starting point is the well-known approach of Caspar and Klug (CK) –, which was originally proposed for rationalising and predicting the architecture of icosahedral viral capsids. Specifically, they developed a general method for the construction of individual icosahedral shells, which is based on cutting and folding leaflets from the two-dimensional hexagonal lattice and produces achiral and chiral arrangements of particles on the icosahedral surface.

The CK construction is shown in Fig. 1 a. A segment is drawn between points of coordinates (0, 0) and (h, k) with respect to the basis vectors of the hexagonal lattice; h and k are integer non-negative numbers so that the segment always connects two lattice points. The segment is the base of an equilateral triangle, which is replicated 20 times to form a leaflet, which is then cut and folded to generate an icosahedral shell with a well-defined surface lattice.

---

### Degenerate boundaries for multiple-alternative decisions [^113CS7yb]. Nature Communications (2022). High credibility.

Sequential Bayesian inference can be rewritten in a form that more closely resembles a sum of evidence by taking the logarithm of equation (8):For each of the n -choices, the accumulated evidence is the log posterior, with n distinct evidence incrementsand a common subtractive log-marginal term. Calculation of this log-marginal is necessary for multiple (n > 3) choice optimal decision-making, but can be avoided for n = 2 by using the SPRT as mentioned above. A similarterm in a theory of basal ganglia function has been interpreted as suppressing evidence accumulation when choices are ambiguous –.

Within this framework, using that probabilities sum to unity ∑ i P i (t) = 1, the evidence accumulation takes place within an (n − 1)-dimensional linear subspace spanning n vertices { P i = 1, P j ≠ i = 0} of an N -dimensional unit hypercube. This is most easily seen for N = 3 choices, where the decision variables P = (P 1, P 2, P 3) vary within a triangle passing through three vertices {(1, 0, 0), (0, 1, 0), (0, 0, 1)} of the unit cube (Fig. 1, top right panel). So rather than representing evidence accumulation trajectories as a random walk constrained to two dimensions, for multiple alternatives the random walk moves in higher dimensions (a line for two alternatives, a plane/triangle for three, and so on). For simplicity, this vector is initialized with equal prior probabilities 1/ n and updated sequentially until one of the elements reaches a boundary, P i > θ.

Both the SPRT and two-choice sequential Bayesian inference give optimal decisions in that they give the fastest decisions for a given level of accuracy on single trials of known difficulty. Formally, they optimize a cost function, the Bayes risk, that is linear in the mean error rate e and mean decision time T over many trialswith error costs W i > 0 and cost of time c, scaling the expected error and decision time,… This cost function represents the trade-off between speed and accuracy of decision-making: slow but accurate or fast but inaccurate decisions are both costly, and so a balance must be found. For equal decision costs, W 0 = W 1, there is a single threshold that is free parameter, which for optimal decision-making is tuned according to the relative cost c / W of accuracy and time.

---

### Navigable networks as nash equilibria of navigation games [^1168EwBf]. Nature Communications (2015). Medium credibility.

Results

Geometrical considerations

The network construction game that we employ is very general and applies to any set of points in any geometry. The latent geometry of numerous real networks is not Euclidean but hyperbolic as shown in ref. Specifically, the model in ref.extends the preferential attachment mechanism of network growth by observing that in many real networks the probability of establishing a connection depends not only on popularity of nodes, that is, their degrees, but also on similarity between nodes. Similarity is modelled in ref.as a distance between nodes on the simplest compact space, the circle. The connection probability thus depends both on node degrees (popularity) and on the distance between nodes on the circle (similarity). The node degrees are then mapped to radial coordinates of nodes, thus moving nodes from the circle to its interior, the disk. One can then show that the resulting connection probability depends only on the hyperbolic (versus Euclidean) distance between nodes on the disk, and that the resulting graphs are random geometric graphsgrowing over the hyperbolic plane. As shown in earlier work, these graphs are maximally random, that is, maximum-entropy graphs that have power-law degree distributions and strong clustering. In other words, power-law degree distributions, coupled with strong clustering, are manifestations of latent hyperbolic geometry in networks. If this geometry is not hyperbolic but Euclidean, then the resulting random geometric graphs still have strong clustering, but their degree distributions are Poisson distributions that do not have any fat tails. The model in ref.has been validated against long histories of growth of several real networks, predicting their growth dynamics with a remarkable precision. It is then not surprising that as a consequence the same model also reproduces a long list of structural properties of these networks.

---

### Degenerate boundaries for multiple-alternative decisions [^114vUgg6]. Nature Communications (2022). High credibility.

The general decision threshold parameterization is thenwhereTaking the product of all components of P (t) j accomplishes two things. Firstly, it reduces threshold dimensionality by one, collapsing the decision space to the number of alternatives with non-zero belief. For example, for n = 4, if one alternative has a zero probability, the decision space and thresholds collapse to the n = 3 case (the planes shown in Fig. 1), constraining how the decision boundaries intersect with the outer faces. Secondly, it introduces the simplest form of nonlinearity, as the product of all elements P (t) j parameterizes curved boundaries.

Overall, the functionmodulates the curve parameterized by the product of posterior probabilities. The variableis the maximum absolute difference between the components of the vector P (t) j, which with the addition of a parameter β allows a range of boundary shapes, such as those in equations (15–18) below. Summing these components over all subspaces generalizes (14) to n -choices, for which it is normalized by the number of 3D subspaces using the multiplier.

The general parameterization, equation (14), has three parameters: θ is the intersection of the boundary with the edges in posterior probability space, because if there are only two choices with non-zero belief values, then the right-hand side of equation (14) is zero leaving F = θ; α directly tunes the amplitude of the function f or of the curve if f = 1; and β is a free parameter whose purpose depends on the function f. Clearly, we could include additional parameters, but consider only these three for tractability.

There is no prevailing method for learning arbitrary functions with stochastic rewards, so we use the general form given by equation (14) and a range of functions f over which to explore the optimality of nonlinear higher-dimensional decision boundaries. The considered functions are:Examples are shown in Fig. 2. The constant-valued boundaries are dashed in all subsequent examples for comparison. Note that setting α = 0 or β = 0 for any of (16–18) recovers the flat case (15).

---

### Structural puzzles in virology solved with an overarching icosahedral design principle [^113SY3FU]. Nature Communications (2019). High credibility.

Methods

The construction of the polyhedral models and their duals is described below.

Construction of polyhedral designs

Consider two lines intersecting at an angle of 60° at the centre of one of the hexagons in the hexagonal (sub)lattice of a given Archimedean lattice. Counting steps between midpoints of adjacent hexagons along these lines via the integer coordinatesand, thencharacterises the positions of other hexagons in the (sub)lattice with respect to the original one, i.e. Using the line connecting the midpoints of these hexagons as the edge of an equilateral triangle of an icosahedral face (Supplementary Fig. 1), the position of the remainder of that surface is uniquely determined, andthus defines a planar embedding of an icosahedral surface into the Archimedean lattice (see examples in Fig. 2). The corresponding polyhedral shape in three dimensions is an icosahedron, obtained via identification of edges of the planar embedding. The numbers of pentagonal, hexagonal, triangular and square faces in the Archimedean lattice overlapping with this icosahedral surface for different values ofandare provided in Supplementary Tables 1–4 for the hexagonal, trihexagonal, snub hexagonal and rhombitrihexagonal lattice, respectively. In particular, an icosahedral face given bycontains either no additional face (hexagonal case), one triangle (trihexagonal case), four triangles (snub hexagonal case), or one triangle and a square (rhombitrihexagonal case), that each form the start of an infinite series of polyhedra.

Construction of the dual lattices

For each polyhedron in the above classification, we construct a dual polyhedron. For this, vertices are positioned at the centres of the polyhedral faces, and vertices associated with adjacent faces connected by straight lines. Since Archimedean lattices have a single type of vertex environment, these dual polyhedra each have a single type of face that corresponds to the fundamental domain of a Laves lattice. These faces are triangles, rhombs, florets and kites for the hexagonal, trihexagonal, snub hexagonal and rhombitrihexagonal lattice, respectively. Using again the planar embedding of an icosahedral surface into the associated Archimedean lattice, we determine the numbers of each such face for polyhedra characterised byandas above; their numbers are listed in Supplementary Table 5.

---

### Clustering by measuring local direction centrality for data with heterogeneous density and weak connectivity [^114dws6F]. Nature Communications (2022). High credibility.

DCM calculation in high-dimensional space

DCM calculation requires to map the KNNs onto the unit hyperspherical surface drawn by their center point firstly, then subdivides the hyperspherical surface and measures the generalized angles of each subdivision unit. In 2D space, KNNs are mapped onto a unit circle. They subdivide the circle into multiple arcs and each arc corresponds to a central angle. DCM measures the variance of these angles. While in 3D space, KNNs are mapped onto a unit spherical surface. They connect neighboring points to form a spherical triangulation and DCM is extended as the variance of the solid angles of the triangles. For subdividing a hyperspherical surface in a higher-dimensional space, we adopt Qhull algorithmto construct the convex complex of KNNs. Since all the KNNs have been mapped onto the hyperspherical surface, they are guaranteed to be the vertices of the convex complex (Repeat KNNs are not included). In d -dimensional space, each facet of the convex complex is a (d- 1)-simplex and corresponds to a subdivision unit (Simplex here denotes the simplest figure that contains d + 1 given points in d -dimensional space and that does not lie in a space of lower dimension). For instances, a 2D and 3D convex complex consists of multiple line segments (1-simplex) and triangles (2-simplex), which subdivides the circle and spherical surface into arcs and spherical triangles respectively (Supplementary Fig. 8).

After subdividing, the generalized angles could be measured. Natively, the angles are equivalent to volumes of the corresponding subdivision units (e.g. arc length in 2D circle, area of the spherical triangle in 3D sphere). However, it is difficult to calculate the volumes of subdivision units in high-dimensional space due to the computational complexity of multiple integral. Therefore, we measure the volume of each simplices and then allocate the global volume error to each subdivision unit evenly for an approximate calculation. Although there are errors between the true and calculated volumes of subdivision units, thesort orders based on the two kinds of volumes are the same, since the volumes of subdivision units increase monotonically with the corresponding simplices. Thus, athreshold can be searched to distinguish the internal and boundary points effectively.

---

### The burden of proof studies: assessing the evidence of risk [^111DkJmb]. Nature Medicine (2022). Excellent credibility.

Spline ensemble

To make the risk function estimates robust to knot placement, we created 50 models based on random knot placement samples. Spline estimates depend on the choice of spline parameters, including spline degree, number of knots and knot placement. To mitigate the effect of spline parameter selection on results, we developed an ensemble approach over knot placement, so that the modeler only had to specify the spline degree and number of knots.

Given the degree and number of knots, we automatically sampled a set of knot placements for a feasible knot distribution. For each knot placement, we fit a spline (including nonlinear measurements, shape constraints and trimming), evaluated each resulting model by computing its fit and curvature, and aggregated the final model as a weighted combination of the ensemble.

Sampling knots from simplex

We prefixed a minimal set of rules that describe a feasible set from which to sample knots, and uniformly sample from this set. Given a number of knots, the rules specify feasible ranges for each knot and feasible gaps between knots. Given an intervaldelimited by terminal knots (which are always the minimum and maximum of the data), the feasible region of the interior knotsis denoted by

We enforced the rules

The set of knot placements that satisfy these four rules form a closed polyhedron (a volume in high-dimensional space delineated by hyperplanes). We calculated the vertices of the polyhedron using the double description method in ref. and uniformly sampled knot placements from within the polyhedron. Each knot placement yielded a model, fit using the trimmed constrained spline approach.

Ensemble performance evaluation

Once the ensemble was created, we scored the resulting risk curves using two criteria: model fit (measured using the log-likelihood) and total variation (measured using the highest order derivative). These scores balanced competing objectives of fit and generalizability. Once we had these scores, denoted as s 1 and s 2, we normalized them to the range [0,1]:and applied a logistic transformation. The transformation was used to make the scoring meaningful even in the presence of spurious curves in a large ensemble. We then multiplied the scores to down-weight models that are low under either criterion (fit or total variation). The final weights are normalized to sum to 1.

Using a weighted combination of these metrics, we weighted the 50 models to create the ensemble model.

---

### Articulation points in complex networks [^116DVPwf]. Nature Communications (2017). Medium credibility.

The phenomenon that n AP displays a unimodal behaviour and the fact that c AP > c p can be explained as follows. The process of increasing the mean degree c of an ER network can be considered as the process of randomly adding links into the network. When the mean degree c is very small (nearly zero), there are only isolated nodes and dimers (that is, components consisting of two nodes connected by one link), and thus n AP →0. With c gradually increasing but still smaller than c p, the network is full of finite connected components (FCCs), most of which are trees (Fig. 3c). Hence, in the range of 0 < c < c p most of the nodes (except isolated nodes and leaf nodes) are APs, and adding more links to the network will increase the number of APs (Fig. 3c). When c > c p, the GCC develops and occupies a finite fraction of nodes in the network (highlighted in light blue in Fig. 3d–f). In this case, we can classify the links to be added to the network into two types: (I) links inside the GCC (yellow dashed lines); and (II) links that connect the GCC with an FCC or connect two FCCs (turquoise dashed lines). The probability that an added link is type-I (or type-II) as a function of the mean degree c is shown in Fig. 3a (light blue region). Adding type-I links to the network will never induce new APs, and may even convert the existing APs (see Fig. 3d, e, nodes in black boxes) back to normal nodes. By contrast, adding type-II links will never decrease the number of APs and could convert normal nodes (see Fig. 3d, e, nodes in orange boxes) to APs. The contributions of these two types of links to n AP compete with each other. At the initial stage of this range of c (c > c p), since the GCC is still small, most of the added links are type-II (Fig. 3a, turquoise dashed line), and thus n AP continues to increase (Fig. 3a, red line). At certain point c AP, where the peak of n AP locates, the contribution of type-I links to n AP overwhelms that of type-II links, hence n AP begins to decrease. When the mean degree c is large enough, the network itself becomes a bicomponent without any AP.

---

### Identifying a common backbone of interactions underlying food webs from different ecosystems [^1153dUsB]. Nature Communications (2018). Medium credibility.

Fig. 3
Ranking of species from our dataset of 411 food webs based on the average similarity between their role and the roles of the species to which they are paired across all 84,255 alignments. The top panel a shows the observed role similarity for all species when compared to food webs from either the same (circles) or different (squares) ecosystem types. The bottom panel b shows the alignment transitivity observed for all species across all food webs. The red dotted line represents the expected alignment transitivity for shuffled alignments, where the number of pairings per alignment was maintained. In both panels, every point indicates the median across at least 250 species with the exception of the last point which is the median across 30 species, and the error bars characterize the interquartile range

Our observation that every network has a set of species that align much better than the rest could be indicative of the existence of a backbone of interactions underlying all these communities. However, this observation is still not a sufficient condition for the backbone to exist. Instead, we identified two necessary conditions for the presence of a backbone of interactions: (i) the best-aligned species from all networks should tend to be paired to each other; and (ii) they should also form a connected component in their own network. To test the first condition, we studied the transitivity of species' alignments, which is a measure of how coherent a species' pairings are across alignments (Fig. 1b; Methods). We observed that the best-aligned species show a significantly higher alignment transitivity than would be expected at random (Fig. 3). This implies that the best-aligned species for the different food webs are in fact paired with each other more often than expected by chance, satisfying condition (i). Next, we indirectly tested the second condition by studying the path likelihood between species, which is a measure of how connected a set of species is within a network (Methods). For every network, we compared the subweb formed by the set of best-aligned species to structures formed by equally sized random subsets of species. We found that the best-aligned species tend to present a high path probability (Supplementary Fig. 3; Connectance and path likelihood section of Supplementary Note 1), which implies that those species are also more connected and likely to form a connected component than expected by chance, satisfying condition (ii).

---

### A general method for the creation of dilational surfaces [^113TYUgo]. Nature Communications (2019). High credibility.

Dilational structures can change in size without changing their shape. Current dilational designs are only suitable for specific shapes or curvatures and often require parts of the structure to move perpendicular to the dilational surface, thereby occupying part of the enclosed volume. Here, we present a general method for creating dilational structures from arbitrary surfaces (2-manifolds with or without boundary), where all motions are tangent to the described surface. The method consists of triangulating the target curved surface and replacing each of the triangular faces by pantograph mechanisms according to a tiling algorithm that avoids collisions between neighboring pantographs. Following this algorithm, any surface can be made to mechanically dilate and could, theoretically, scale from the fully expanded configuration down to a single point. We illustrate the method with three examples of increasing complexity and varying Gaussian curvature.

---

### Network statistics of the whole-brain connectome of drosophila [^111nSibu]. Nature (2024). Excellent credibility.

Pairwise distances between neurons

To determine the connection probability distribution as a function of distance between neurons, we first had to distil the available spatial information into a handful of points. This was the only practical way to enable distance comparisons between all neurons — a total of 14 billion pairs. For each neuron, we defined two coordinates based on the location of their incoming and outgoing synapses. We computed the average 3D position of all of the neuron's incoming synapses to approximate the position of the neuron's dendritic arbour, and did the same to approximate the position of the neuron's axonal arbour. We then computed for all neuron pairs the pairwise distances between the axonal arbour of neuron A and the dendritic arbour of neuron B. Binning by distance and comparing the number of true connections to the number of neuron pairs allowed us to compute connection probability as a function of space (Extended Data Fig. 1d).

NND model

Informed by the distribution of connection probability as a function of distance, we constructed a NND model with two zones of probability — a 'close' zone (0 to 50 μm) where connections are possible with a relatively high probability (p close = 0.00418) and a 'distant' zone (more than 50 μm) where connections occur with lower probability (p distant = 0.00418) (Extended Data Fig. 1e). The probabilities in these two zones were derived from the real network. Then, for every neuron pair (around 14 billion pairs), we generate a random number drawn uniformly from between 0 and 1. The distance between the two neuronal arbours sets the probability of forming an edge between the pairs, p close or p distant. If the random number is below the probability threshold, a connection is formed between these two neurons in the model.

---

### On convex decision regions in deep network representations [^112zQYTG]. Nature Communications (2025). High credibility.

An extensive body of work concerns the geometry of input space representations in ReLU networks and has led to a detailed understanding of the role of so-called 'linear regions': In networks based on ReLU nonlinearity, the network's output is a piecewise linear function over convex input space polyhedra, formed by the intersection of neuron half-spaces –. Interestingly, in typical networks (e.g. at initialization or after training), the linear regions are much less in number and simpler in structure compared to theoretical upper bounds –. During training, the collection of convex linear regions is transformed and combined into decision regions through the later network layers. The resulting decision regions are, therefore, unions of convex sets and may in general be non-convex or non-connected, as noted in ref. Our two measures of convexity, Euclidean and graph-based, probe different mechanisms of generalization, the former is associated with generalization by linear interpolation, while the latter is associated with generalization by interpolation along the data manifolds. As both mechanisms may be relevant for explaining generalization in cognitive systems, we are interested in the abundance and potential role of both measures of convexity. Note that a region may be convex in terms of the Euclidean measure but not the graph-based (e.g. a decision region formed by disconnected but linearly separated subsets) and a region may be graph-convex, but not Euclidean-convex (e.g. a general shaped connected region in the data manifold). For visual examples, see Fig. 2 a.

Convexity measurement workflows

We developed and presented two workflows for measuring graph convexity and Euclidean convexity respectively in latent spaces of neural networks.

We are interested in measuring the approximate convexity of a decision region, here, a subset of nodes in a graph. A decision region for a specific class is a set of points that the model classifies as belonging to this class. We measure convexity per layer and per class separately.

We first create a graph containing the representations of the data points of all classes. The points are nodes in the graph and the Euclidean distances between the nodes are the weights of the edges. To handle manifold-based representation, for each node, we create an undirected edge only to the nearest neighbors (K = 10). This procedure creates a sparse undirected weighted graph with positive weights (i.e. distances) only.

---

### A neurophysiological basis for aperiodic EEG and the background spectral trend [^112a8gx2]. Nature Communications (2024). High credibility.

Embedding synapses onto postsynaptic dendrites

Dipole synchrony has been previously modelled either by separating inhibitory and excitatory input into somatic and apical compartments, respectively, or by having counterphase input into the basal and apical compartments; both of these models can be thought of as optimal mappings of two anticorrelated populations of synapses. Inspired by these models, we developed a procedure to generate dipole synchrony given any presynaptic topology. First, the pairwise correlation between each pair of presynaptic neurons was determined by the spike time tiling coefficient (STTC), a measure of spike train correlation which accounts for the likelihood of spikes overlapping by chance. The pairwise correlations among all presynaptic neurons were computed based on 40 s long simulations of the presynaptic networks. The Uniform Manifold Approximation and Projection (UMAP) algorithm, a dimensionality reduction technique, was then used to optimally project the presynaptic network onto a sphere such that the angle between presynaptic neurons with high STTCs was minimized. After this embedding step, the dendrites of the postsynaptic neurons were orthogonally projected onto the sphere. Finally, the following procedure was run until all connections were formed between pre- and post-synaptic neurons: (1) a postsynaptic dendrite segment was chosen randomly (with replacement) with a probability proportional to its surface area; (2) the presynaptic neuron closest on the spherical embedding was chosen and a connection formed.

To model suboptimal synapse placement, we randomly perturbed the spherical embedding before mapping the synapses. Each point in the embedding was perturbed by a distancealong a randomly chosen bearing, whereHere, X is what we refer to as the optimality index (Fig. 4e). By construction, when, the points on the sphere are not perturbed at all, while when, all the points on the sphere are perturbed by a distance sampled from the function. Consequently, for, the distribution of points on the sphere is uniformly random.

This procedure can be thought of as a model of dipole correlation that generalizes beyond dichotomous input. Alternatively, this procedure can be considered in terms of observations from recent studies, which have reported that functionally related synaptic inputs cluster within individual dendritic branches, and that input from similar presynaptic populations target similar dendritic compartments in the postsynaptic population –. These experimental observations hint at a more continuous mapping of synaptic input along dendrites than a strictly binary apical-basal compartment paradigm, which is precisely what is achieved when the above mapping algorithm is applied to a continuous presynaptic network topology, such as the planar subcritical network used in Fig. 5.

---

### Confidence reports in decision-making with multiple alternatives violate the Bayesian confidence hypothesis [^113VUVhw]. Nature Communications (2020). High credibility.

Fig. 2
Models.

a Generative model. Target position is represented by s. Two sources of variability are considered in the model: First, observers have access to noisy measurement x, a Gaussian distribution centered at s with a standard deviation σ. Second, given the same measurement x, the posterior distribution varies across trials due to decision noise, modeled by Dirichlet distribution, of which spread (represented by the shade of the ternary plot) is controlled by a parameter α (see Methods). On each trial, a decisionand a confidence c are read out from the posterior distribution of that trial. b We use ternary plots to represent all possible posterior distributions. For example, a point at the center represents a uniform posterior distribution; at the corners of the ternary plot, the posterior probability of one category is one while the posterior for the other two categories are zeros. c The bar graphs illustrate how confidence is read out from posterior probabilities in each model. For the purposes of these plots, we did not include decision noise here. The color of each ternary plot represents the confidence as a function of posterior distribution for each model. The color is scaled for each ternary plot (independently) to take the whole range of the color bar.

We introduce three models of confidence reports: the Max model, the Entropy model and the Difference model. Each of these models contains two steps: (a) mapping the posterior distribution (q) to a real-valued internal confidence variable; (b) applying three criteria to this confidence variable to divide its space into four regions, which, when ordered, map to the four confidence ratings. The second step accounts for every possible monotonic mapping from the internal confidence variable to the four-point confidence rating. The three models differ only in the first step.

The Max model corresponds to the Bayesian confidence hypothesis. In this model, the confidence variable is the probability that the chosen category is correct, or in other words, it is the highest of the three posterior probabilities (Fig. 2c). In this model, the observer is least confident when the posterior distribution is uniform. Importantly, after the posterior distribution is computed, the posterior probability of the unchosen options does not further contribute to the computation of confidence.

---

### On the shape of the population ROC curve [^116L36Hn]. Academic Radiology (2013). Low credibility.

Rationale and Objectives

Human observers often do not produce empirical operating points near the northeast corner of the receiver operating characteristic (ROC) plot, and thus the local shape of the population ROC curve is unknown.

Materials and Methods

We call attention to occult abnormalities and propose that considerations by human observers of the prior probability of occult abnormalities can cause the shape of the local population ROC curve to be convex, a straight line, or concave near the northeast corner of the ROC plot. We further conducted a set of simulated detection-task (without-search) experiments with human observers and, mathematically, with an ideal observer and a model observer. In the experiments, we used signals, pseudo-signals that were similar to signals, and random image noise. The relative frequency of occult signals was controlled in the experiments.

Results

In the simulated experiments, the population ROC curve of the ideal observer was always convex, but those of the model observer and of human observers were convex, a straight-line, or concave, depending on the relative frequency of occult signals. The population ROC curve for the model observer was identical to that for the ideal observer when knowing the relative frequency of occult signals was not important for the ideal observer, and it was similar to that for human observers otherwise.

Conclusion

Observer consideration of the prior probability of occult abnormalities is important in ROC studies and could cause unexpected shapes of the local population ROC curve. Absence of empirical operating points near the northeast corner of the ROC plot may be caused by occult abnormalities.

---

### On the convexity of ROC curves estimated from radiological test results [^116kpchX]. Academic Radiology (2010). Low credibility.

Rationale and Objectives

Although an ideal observer's receiver operating characteristic (ROC) curve must be convex-ie, its slope must decrease monotonically-published fits to empirical data often display "hooks". Such fits sometimes are accepted on the basis of an argument that experiments are done with real, rather than ideal, observers. However, the fact that ideal observers must produce convex curves does not imply that convex curves describe only ideal observers. This article aims to identify the practical implications of nonconvex ROC curves and the conditions that can lead to empirical or fitted ROC curves that are not convex.

Materials and Methods

This article views nonconvex ROC curves from historical, theoretical, and statistical perspectives, which we describe briefly. We then consider population ROC curves with various shapes and analyze the types of medical decisions that they imply. Finally, we describe how sampling variability and curve-fitting algorithms can produce ROC curve estimates that include hooks.

Results

We show that hooks in population ROC curves imply the use of an irrational decision strategy, even when the curve does not cross the chance line, and therefore usually are untenable in medical settings. Moreover, we sketch a simple approach to improve any nonconvex ROC curve by adding statistical variation to the decision process. Finally, we sketch how to test whether hooks present in ROC data are likely to have been caused by chance alone and how some hooked ROCs found in the literature can be easily explained as fitting artifacts or modeling issues.

Conclusion

In general, ROC curve fits that show hooks should be looked on with suspicion unless other arguments justify their presence.

---

### The subject-fixated coaxially sighted corneal light reflex: a clinical marker for centration of refractive treatments and devices [^112pJSDb]. American Journal of Ophthalmology (2014). Low credibility.

Purpose

To describe the inconsistencies in definition, application, and usage of the ocular reference axes (optical axis, visual axis, line of sight, pupillary axis, and topographic axis) and angles (angle kappa, lambda, and alpha) and to propose a precise, reproducible, clinically defined reference marker and axis for centration of refractive treatments and devices.

Design

Perspective.

Methods

Literature review of papers dealing with ocular reference axes, angles, and centration.

Results

The inconsistent definitions and usage of the current ocular axes, as derived from eye models, limit their clinical utility. With a clear understanding of Purkinje images and a defined alignment of the observer, light source/fixation target, and subject eye, the subject-fixated coaxially sighted corneal light reflex can be a clinically useful reference marker. The axis formed by connecting the subject-fixated coaxially sighted corneal light reflex and the fixation point, the subject-fixated coaxially sighted corneal light reflex axis, is independent of pupillary dilation and phakic status of the eye. The relationship of the subject-fixated coaxially sighted corneal light reflex axis to a refined definition of the visual axis without reference to nodal points, the foveal-fixation axis, is discussed. The displacement between the subject-fixated coaxially sighted corneal light reflex and pupil center is described not by an angle, but by a chord, here termed chord mu. The application of the subject-fixated coaxially sighted corneal light reflex to the surgical centration of refractive treatments and devices is discussed.

Conclusion

As a clinically defined reference marker, the subject-fixated coaxially sighted corneal light reflex avoids the shortcomings of current ocular axes for clinical application and may contribute to better consensus in the literature and improved patient outcomes.

---

### Principled approach to the selection of the embedding dimension of networks [^114ZezTL]. Nature Communications (2021). High credibility.

Link prediction

For the link prediction task, we use the repeated random sub-sampling validation by applying the following procedure 10 times. We report results corresponding to the AUCROC score (the area under a receiver operating characteristic curve) over these repetitions. We first hide 10% of randomly chosen edges of the original graph. The hidden edges in the original graph are regarded as the "positive" sample set. We sample an equal amount of disconnected vertex pairs as the "negative" sample set. The union of the "positive" and "negative" sample sets form our test set. The training set consists of the remaining 90% of connected node pairs and an equal number of randomly chosen disconnected node pairs. We use the training set to learn the embedding and determine the embedding of the nodes. Furthermore, we use the training set to learn the probability of connection between pairs of nodes given their embedding. Specifically, for every pair of nodes i and j in the training set, we evaluate the Hadamard product e i j = V i,⋅ ⊙ V j,⋅. Note that e i j is a d -dimensional vector. We assume that the probability of connection between nodes i and j is given bywhere θ is a d -dimensional parameter vector, andis the dot product between the vectors e i j and θ. The best estimate of the entries of vector θ are obtained from the training set via logistic regression. In absence of training when all components of the vector θ are identical and positive, the probability of Eq. (4) is proportional to the dot product between the vectors V i,⋅ and V j,⋅. We use Eq. (4) to rank edges of the test set and evaluate the AUCROC score of the link prediction task.

---

### Quantifying entanglement in a 68-billion-dimensional quantum state space [^115aiAgW]. Nature Communications (2019). High credibility.

Adaptive sampling algorithm

For each configuration, experimental data are stored in nodes in a quad-tree decomposition of P whose levels describe increasingly fine detail. The i th node corresponds to a square area ofat locationwith span. Nodes are sampled by placing the corresponding, one-dimensional local patterns on the DMDs and generating a coincidence histogram during acquisition time T a = 0.5 s. Coincidences C i are counted within a 1-ns coincidence window centered on the coincidence peak; accidental coincidences A i are counted in a 1-ns window displaced 2 ns from the coincidence window. Coincidence and accidental values are appended to a list each time the node is sampled. The estimated count rate, whereis a calibrated, relative fiber coupling efficiency. Optionally, A i can be subtracted from C i for accidental removal. Uncertainty is computed by assuming Poissonian counting statistics for C i and A i and applying standard, algebraic propagation of error through the calculation of the entanglement quantity (Eq. (3)).

The data collection algorithm consists of a partitioning phase followed by an iterative phase. During partitioning, the algorithm repeatedly iterates through a scan-list of leaves of the tree. Node i is considered stable when sgn(αR T − R i) is known to at least β standard deviations of certainty, where splitting threshold α (0 ≤ α ≤ 1) and stability criterion β are user-chosen heuristics. Stable nodes are no longer measured. If a node is stable and R i ≥ αR T, the node is split into four equal-sized sub-quadrants, which are initially unstable and added to the scan-list. Optionally, a maximum resolution (maximum tree depth) may be set.

The transition to the iterative phase occurs when the percentage of unstable leaves is < Γ, a user-chosen parameter. At this point, stability is ignored and all leaf nodes are scanned repeatedly and guaranteed to have the same total acquisition time. Various final stopping criteria can be used; we chose a fixed total run time. Note that heuristic parameters α, β, and γ may be changed during operation if desired. For the data shown in this manuscript, α = 0.002, β = 2, and Γ = 0.15 with a 30-h runtime.

The probability distributionis computed by uniformly distributing the estimated count rate (with or without accidental subtraction) from each leaf node across its constituent elements in, followed by normalization.

---

### On convex decision regions in deep network representations [^1123ZLwZ]. Nature Communications (2025). High credibility.

The consistency of sample/graph-based geodesic estimates has been discussed in connection with the introduction of ISOMAPand more generally in. Graph connectivity-based estimates of geodesics from sample data are implemented using two procedures: The neighborhood graph can be determined by a distance cutoff ϵ, so that any points within the Euclidean distance ϵ are considered neighbors, or by K-nearest neighbors (KNN) based on the Euclidean distance. Consistency of these estimates, i.e. that the sample-based shortest paths converge to geodesics, is most straightforward to prove for the former approach. The consistency proof for the ϵ -based procedure is based on the smoothness of the metric, a uniformly bounded data distribution (1/ c < p (x) < c) and scaling of the distance cutoff or K so the connectivity (number of edges per node) increases for large samples (cutoff decays slowly ϵ → 0 as sample size N → ∞).

In finite samples, a (too) large connectivity will bias the geodesics, while a (too) small connectivity can lead to disconnected graphs and noisy estimates. We explore the role of the way we construct the graph. The consistency proof holds for a graph constructed with a distance cutoff ϵ. We compare it to graphs constructed by keeping K -nearest neighbors and symmetrizing the graph. We chose the number of nearest neighbors to be 3, 5, 10 and 20. The respective ϵ values were chosen to keep approximately the same number of edges in the graph.

Figure 2 d shows that the graph constructed with a distance cutoff ϵ is very disconnected, and the scores computed with this type of neighborhood are biased towards zero (see Fig. 2 e). If we skip the disconnected pairs and compute the score only from existing paths, the results are very close to the scores based on K -nearest neighbors (see Fig. 2 f). All experiments also demonstrate that the role of the size of K is negligible. Therefore, we choose a KNN-based approach with K = 10 in the complete set of experiments.

---

### On convex decision regions in deep network representations [^115WzzaD]. Nature Communications (2025). High credibility.

We now sample N s pairs of points within the given predicted class label and compute the shortest path in the graph between the pairs using Dijkstra's algorithm. For each path, we compute a score between 0 and 1. The path score is defined as the proportion of the number of nodes on the shortest path, without the endpoints, inside the decision region (i.e. with the same predicted label). If an edge directly connects the pair of nodes, the score is 1. If the points are not connected, the score is 0. We average the scores for all paths within one class to get a convexity score per class per layer. To get one convexity score per layer, we average over all paths regardless of class. Uncertainties are calculated as the standard error of the mean, where we set n as the number of points in the given class. This is likely a conservative estimate since the mean is based on many more pairs than there are points.

The runtime complexity of this procedure is, where N denotes the number of data points, D is the dimensionality of the representations and N c is the number of classes. See Supp. Mat. 1.4 for details.

Euclidean convexity is also measured for each layer and class separately. To measure the convexity of class C in layer l, we first extract hidden representations at l of all points predicted to belong to class C. We sample N s pairs of points. For each pair, we compute the N p = 10 equidistant points on the linear segment connecting the representations of the two endpoints. We feed the interpolated points to the rest of the network (after layer l). The score for each pair is then the proportion of the interpolated points that are also predicted to belong to class C. Finally, to get the score of Euclidean convexity for the whole class, we average the scores over all the pairs of points. To get the Euclidean convexity for each layer, we average over all classes.

---

### Diffusion MRI tractography of the locus coeruleus-transentorhinal cortex connections using GO-ESP [^1154x8Eg]. Magnetic Resonance in Medicine (2022). Medium credibility.

ESP expands upon the concept of a random walk, a process by which a randomly moving object (or walker) moves away from its starting position. This can also be thought of as a path made of a series of random steps on a lattice. In a generic random walk, a step from one point in any direction is equiprobable. However, consider the situation where some points on the lattice are less accessible than others (perhaps due to structural barriers), a biased random walk. In this case, a step from one point to any other is not equiprobable but is based on some probability distribution influenced by prior information about the relationship between points. GO‐ESP expands this concept to dMRI by using a type of biased random walk called the maximum entropy random walk. In this case, our "points" are dMRI voxels, and the prior information about the relationship between these voxels (ie, which voxels are more accessible and more likely to be physically connected) takes the form of a coupling matrix which characterizes the intervoxel interactions (in this work, these interactions are computed as the inner product of the full diffusion tensors). Once the coupling matrix is known, the probability of stepping from one voxel to another (the TP) can be calculated. Thus, the TP depends not only on the diffusion profile within each voxel, but the similarity of the diffusion profiles between voxels.

The TPs describe the probabilities of various pathways within the data, or trajectories that a random walker may follow. Over time, biased random walkers will tend to localize in more accessible regions, and this final distribution provides the novel dependent measure of GO‐ESP, the equilibrium probability (EP). In other words, regions of higher EP represent locations that are more accessible, that is, there are a greater number of ways that these regions can be reached (higher entropy).

---

### Concurrence of form and function in developing networks and its role in synaptic pruning [^1144JBMi]. Nature Communications (2018). Medium credibility.

Monte Carlo simulations

The initial conditions for the neural states are random. For the network topology we can draw an initial degree sequence from some distribution p (k, t = 0), and then place edges between nodes i and j with a probability proportional to k i (0) k j (0), as in the configuration model. Time evolution is then accomplished in practice via computer simulations as follows. First, the number of links to be created and destroyed is chosen according to two Poisson distributions with means Nu (κ) and Nd (κ), respectively. Then, as many times as needed according to this draw, we choose a node i with probability π (I i) to be assigned a new edge, to another node randomly chosen; and similarly we choose a new node j according to η (I j) to lose an edge from one of its neighbours, randomly chosen. This procedure uses the BKL algorithm to assure proper evolution towards stationarity. This way, each node can then gain (or lose) an edge via two paths: either through the process with probability π (I i) for a gain (or η (I i) for a loss), or when it is randomly connected to (or disconnected from) an already chosen node. Therefore, the effective values of the second factors in Eq. 1 areand, where the 1/2 factor is included to assure normalization.

---

### On demand synthesis of hollow fullerene nanostructures [^115uSFQK]. Nature Communications (2019). High credibility.

Assembly and interconnection of hollow units

In the TEM images, we note that the nanobowls, bottles, and cucurbits often arrange closely in a circle/curve with their openings pointing inward (Fig. 5a, b and Supplementary Fig. 2). This unusual alignment cannot be explained by random aggregation of the particles during drying. We speculate that they were assembled around a xylene droplet, which connects with the xylene domain inside the hollow units. This extra droplet may form at the initial stage of drying and eventually evaporated to leave a circular arrangement.

Fig. 5
Connection of hollow units. TEM images of a, b C 60 nanocucurbits and bottles arranged in a circle; c dimers of connected nanobowls; d connection between a dimer of nanobowls and a nanobottle; e dimers of connected nanocucurbits; f – h dimers, trimers, and tetramers of nanobottles, respectively; i large area dimers of nanobottles, and j typical connected nanobottles. k Schematics illustrating the connection of nanobottles via the droplet at their opening. Scale bars are 200 nm

We attempted to cause C 60 growth around this hypothetical droplet. Water-doped DMF (1% V/V) was used to reduce the solubility of C 60, and the nanobowls used as the starting material were enriched to four times of the normal concentration, to enhance the probability of aggregation. After second addition of C 60 solution in m -xylene, interconnected structures were obtained. As shown in Fig. 5c, the dimers of nanobowls contain a shared hollow compartment as the node connecting the two nanobowls. When the nanobowls were replaced with nanobottles, connected trimers and tetramers were obtained in addition to dimers (Fig. 5g, h and Supplementary Fig. 12B), probably because the presence of bottlenecks reduced the mutual steric hindrance. When the nanobottles were not enriched, only dimers were formed (Fig. 5f), agreeing with the concentration effect. In the products, the monomers are often well separated from each other surrounding the central node (Fig. 5g–j), with no direct contact between them. It is a clear proof that the monomers did not aggregate by direct contact, but via the mediating droplets at their openings.

---

### De novo design of high-affinity binders of bioactive helical peptides [^111iZsoW]. Nature (2024). Excellent credibility.

Methods

Computational methods

Parametric design of groove-shaped scaffold library and use for binder design

The parametric groove-shaped scaffold library was sampled using a random sampling approach, for which key parameterswere selected randomly from specific distributions. An even distribution of bundle 'lengths' was sampled, for which each parametric helix was 15–19 residues long. A supercoiling value was randomly selected from a biased distribution favouring more supercoiled scaffolds, given that these scaffolds were more likely to fail in the subsequent looping step (Fig. 1b and Supplementary Fig. 1). This biased sampling strategy was chosen to achieve a more uniform distribution of supercoiling within the final scaffold library, with sufficient numbers of highly supercoiled bundles. An average helix neighbour distance value was randomly selected from a Gaussian distribution informed by native protein helical bundle geometries (Fig. 1b and Supplementary Fig. 1). The distance of each helix from its neighbours was independently randomly selected from a much tighter Gaussian distribution centred at the preselected average helix neighbour distance value, to provide some noise within a given scaffold to helix distances and allow for heterogeneous amino acid selections (Supplementary Fig. 2). Values for helix phase and z displacement were randomly sampled for each helix. The 'groove' consisting of three helices was first sampled as a helical bundle using the Crick parameterization of α-helical coiled coils, around an imaginary central helix where the target was to later be docked. Next, the two buttressing helices were sampled with the same parameterization, but moved radially outwards with randomly sampled helix neighbour distances as well as an additional randomly sampled tilt. This process was used to sample a set of 200,000 arrangements of 5 helices. Next, the Rosetta ConnectChainsMoverwas used to loop this set into approximately 135,000 successful scaffold backbones. These backbones were designed and filtered using Rosetta(including flexible backbone design) to yield a final library of 18,000 scaffolds. Backbones were filtered on metrics including buried nonpolar surface area per residue, Rosetta score per residue, percentage alanine, exposed hydrophobics per residue, and Rosetta 'holes'. This library was used to design binders to different helical peptide targets using an adapted version of the miniprotein binder design computational pipeline used in ref. in which only the binder interface was designed and the target was restricted to only rotamer repacking.

---

### Confidence reports in decision-making with multiple alternatives violate the Bayesian confidence hypothesis [^115cBwRJ]. Nature Communications (2020). High credibility.

In Experiment 1, the vertical position of the target dot was sampled from a normal distribution (2° std), and the horizontal position of the target dot was sampled uniformly between the center of the leftmost and rightmost categories plus a 0.2° extension to the left and the right. In Experiment 2, the target dot was uniformly sampled from a circular area (2.6° radius) positioned at the center of the screen. No feedback was provided in Experiment 1 and Experiment 2.

In Experiment 3, in each trial, we randomly chose one of the three categories with equal probability as the true category. We then positioned the target dot by sampling from the distribution of the true category. A feedback regarding the true category was provided at the end of each trial: After participants reported their confidence, all exemplar dots disappeared except that the exemplar dots from the true category remained on the screen for an extra 500 ms. In each experiment, participants completed one 1-hr session (84 trials per configuration in Experiment 1 and 120 trials per configuration in Experiment 2 and 3). All the trials in one session were separated into eight blocks with equal number of trials. Different configurations were randomized and interleaved within each block.

Participants were well informed about the structure of the stimuli. We told observers that the distributions of the three groups are circular and symmetric, and the three groups have the same spread (standard deviation) throughout the experiments, and only differed in their centers. In Experiment 1 and 3, participants were informed that the centers of the three groups only varied horizontally.

Models

Generative model. The target belongs to category C ∈{1, 2, 3}. The two-dimensional position s of a target in category C is drawn from a two-dimensional Gaussian p (s | C) = N (s; m C, σ s 2 I), where m C is the center of category C, σ s 2 is the variance of the stimulus distribution, and I is the two-dimensional identity matrix. We assume that the observer makes a noisy sensory measurement x of the target position. We model the sensory noisy using a Gaussian distribution centered at s with covariance matrix σ 2 I. Thus, the distribution of x given category C is p (x | C) = N (x; m C, (σ s 2 + σ 2) I).

---

### An empirical comparison of discrete ratings and subjective probability ratings [^116dReBU]. Academic Radiology (2002). Low credibility.

Rationale and Objectives

The authors compared receiver operating characteristic (ROC) data from a five-category discrete scale with that from a 101-category subjective probability scale to determine how well the latter categories define the ROC curve.

Materials and Methods

The authors analyzed data from a pilot study performed for another purpose in which 10 radiologists provided both a five-point confidence rating and a subjective probability rating of abnormality for each interpretation. ROC operating points were plotted for a five-category scale and a 101-category scale to determine how well the observed points covered the range of false-positive probabilities. ROC curves were fitted to the subjective probability data according to the standard ROC model.

Results

For these data, subjective probability ratings were somewhat more effective in populating the range of false-positive probability with ROC points. For three observers, the ROC curves inappropriately crossed the chance line. For another four, prevention of such crossing seemed to depend on one or two ROC points near the upper right corner of the ROC space, points based on discriminations within the discrete category "no abnormality to report".

Conclusion

Subjective probability rating should provide substantially better coverage of the ROC space with operating points, preventing inappropriate crossing of the chance line. Unfortunately, the protection offered by subjective probability ratings was unreliable and depended on ROC points derived from discriminations not directly related to apparent abnormality. The use of proper ROC models to fit data may offer a better solution.

---

### Overcurvature describes the buckling and folding of rings from curved origami to foldable tents [^112dFzVo]. Nature Communications (2012). Medium credibility.

Creased paper rings are a third example of overcurved structures (Fig. 2c). In curved origami, paper is creased along closed curved paths, resulting in the out-of-plane bending of the folding path due to torsion. Simultaneously, the conservation of arc length by the two sides of the crease imposes an increase of the curvature of the crease by a factor O p = 1/cos(δ /2), where δ is the angle between the normals to the two sides of the crease (Supplementary Methods; Supplementary Figs S2 and S3; Supplementary equation S19). We creased a flat circular ring of paper along its median circle, and adjusted the degree of overcurvature by tuning the crease angle δ and holding it constant. After observations, the paper ring was cut and the number of turns in the formed coil was quantified to compute O p. This overcurvation process corresponds to increasing the local pre-set curvature while keeping the length constant.

Finally, to give a human-scale example of overcurved structures, arcs of wood of radius of curvature R = 1 m were combined with clamping collars to form closed continuous rings of square cross-section, with different magnitudes of the pre-set overcurvature. Representative images are shown in Fig. 2d and in Supplementary Fig. S4.

The overcurvation scenario is similar for all observed systems: the conformation starts from a circle at O p = 1 and buckles to a saddle shape with two pairs of upper and lower lobes for larger pre-set overcurvatures. The opposite lobes of the saddle then overlap close to O p = 2.5, and the ring finally debuckles to form a triple-coiled loop for O p = 3. All conformations have D 2d symmetry (for creased paper, this holds true only for the crease line). Although some issues are specific to each case, such as the cross-section rotation angle or the formation of wrinkles and lateral creases, the general scenario is identical for these systems, which cover almost five orders of magnitude in spatial scale, and for which the physical origin of overcurving differs sensibly. In the sequel, we concentrate on the general conformation of the centre lines of the rings (or of the crease lines for paper strips), and ignore details such as small wrinkles and lateral creases.

---

### Machine learning enables completely automatic tuning of a quantum device faster than human experts [^117Uk7n4]. Nature Communications (2020). High credibility.

Searching for the hypersurface

In each iteration, the algorithm first locates the hypersurface in gate voltage space. To do this, it selects a search direction, specified by a unit vector u which during the first 30 iterations of the algorithm is selected randomly from a hypersphere, restricted to the octant where all gate voltages are negative. The gate voltages are then scanned along a ray beginning at the origin o and parallel to u (Fig. 4 a). During this scan, the current is monitored; when it falls below a threshold of 20% of full scale, this is taken as defining a location v (u) on the hypersurface.

Fig. 4
Characterising the boundary hypersurface using machine learning.

Each panel illustrates a step of the algorithm presented in Fig. 3. The gate voltage space, restricted to two dimensions for illustration, is divided into regions of near-zero (blue) and non-zero current (pink), separated by a boundary hypersurface. a Locating the hypersurface. The gate voltages are scanned along a ray (violet arrow) starting at the origin (white circle) and defined by direction u. By monitoring the current, the intersection with the hypersurface is measured. b To determine whether a region should be pruned, the algorithm scans each gate voltage individually toward the bottom of its range from a location just inside the hypersurface as shown. If only one scan intersects the hypersurface (as in the inset), future exploration of that region is inhibited by displacing the origin as shown. c Based on a short 1D scan, the location is classified according to whether it shows current peaks indicating Coulomb blockade. d If peaks are found, a 2D scan (violet square) is performed in the plane of V 3 and V 7, and is possibly repeated at higher resolution. e From the first thirty measurements (green and yellow circles), the algorithm builds a model of the hypersurface and assigns a probabilitythat peaks will be found. f To refine the model, the algorithm generates a set of candidate search locations (squares), each weighted by its corresponding value of, and selects one at random. A new scan is then performed in the corresponding direction to generate a new measurement of the hypersurface location. Steps d – f are then repeated indefinitely. Inset: Scheme for numerically sampling the hypersurface using simulated Brownian motion. Each point represents a simulated particle moving inside the enclosed volume. The collisions between the particles and the modelled hypersurface generate a set of candidate search locations.

---

### Arrays of individually controlled ions suitable for two-dimensional quantum simulations [^111brWzC]. Nature Communications (2016). Medium credibility.

A precisely controlled quantum system may reveal a fundamental understanding of another, less accessible system of interest. A universal quantum computer is currently out of reach, but an analogue quantum simulator that makes relevant observables, interactions and states of a quantum model accessible could permit insight into complex dynamics. Several platforms have been suggested and proof-of-principle experiments have been conducted. Here, we operate two-dimensional arrays of three trapped ions in individually controlled harmonic wells forming equilateral triangles with side lengths 40 and 80μm. In our approach, which is scalable to arbitrary two-dimensional lattices, we demonstrate individual control of the electronic and motional degrees of freedom, preparation of a fiducial initial state with ion motion close to the ground state, as well as a tuning of couplings between ions within experimental sequences. Our work paves the way towards a quantum simulator of two-dimensional systems designed at will.

---

### Engineering zero modes in transformable mechanical metamaterials [^111vQgyF]. Nature Communications (2023). High credibility.

Fig. 1
Design concept and 3D tessellation for transformable mechanical metamaterial.

a 2D transformable tessellation consisting of periodically arranged equilateral 4-bar linkages, whose typical configurations include completely folded (CF), completely extended (CE) and partially folded (PF) configurations, and are further interpreted as continuous media with different numbers of zero modes (pink colored). b 3D realization of the transformable metamaterial via modular origami technique. c The decoupled motion in three orthogonal planes. d A 4 × 4 × 4 tessellation of 3D transformable metamaterial.

Although all possible 2D and 3D extremal materials were theoretically conceived by Milton and Cherkaev in 1995, there is still an omnipresent challenge needs to be answered: can we develop a design strategy to physically engineer a 3D zero-mode metamaterial, which can freely transform elasticity tensors among all seven types of 3D extremal materials covering the solid null-mode to the gas-like hexa-mode? The key to answer the question is the realization of the decoupled 4-bar linkage motions in the three orthogonal planes in the 3D system. First, the modular origami technique is introduced to build a 3D linkage (Fig. 1b). By adding four pink cubes into the 3D linkage, the required connecting spaces are provided for the following orthogonal design. However, with all compliant joints (marked in red short lines) being arranged along the z-axis and forming a 2D closed loop, an 8-bar linkage forms instead, which confines all motions within the x-y plane. To break the in-plane confinement, we rotate the two rhomboids (A1 and A2) and arrange their joints along the y-axis. Hence, a 4-bar linkage supporting decoupled in-plane and out-of-plane motions is formed. Next, the same design strategy is applied to the other two orthogonal planes, and then a cubic unit is constructed. Thanks to the decoupled orthogonal design, switch among the three basic configurations (CE, CF and PF) can be performed along the three orthogonal directions independently (Supplementary Movie 2), and the desired 3D unit configuration can therefore be achieved through a superposition of the basic configurations (Fig. 1c). In fact, all possible configurations of the cubic unit can fulfill a 3D workspace (Supplementary Note 3). Finally, by using the cubic symmetry of the unit, we can easily tessellate it along the three orthogonal directions to construct a 3D metamaterial. Figure 1d shows a 4×4×4 tessellation built by the modular origami technique.

---

### 2023 U.S. department of veterans affairs and U.S. department of defense clinical practice guideline for the management of headache [^114Cv7iS]. Annals of Internal Medicine (2024). High credibility.

Algorithm — this clinical practice guideline (CPG) algorithm is designed to facilitate understanding of the clinical pathway and decision-making process used in managing patients with Headache, and the format represents a simplified flow that helps foster efficient decision making by providers; it includes steps of care in an ordered sequence, decisions to be considered, decision criteria recommended, and actions to be taken; the algorithm is a step-by-step decision tree, with standardized symbols to display each step and arrows connecting numbered boxes indicating the order in which the steps should be followed; shape meanings are specified: rounded rectangles represent a clinical state or condition, hexagons represent a decision point in the process of care formulated as a question that can be answered "Yes" or "No", rectangles represent an action in the process of care, and ovals represent a link to another section within the algorithm; Sidebars 1–7 provide more detailed information, and Appendix K contains alternative text descriptions of the algorithms.

---

### Diverse synaptic plasticity mechanisms orchestrated to form and retrieve memories in spiking neural networks [^1115Nvmx]. Nature Communications (2015). Medium credibility.

Details of feed-forward network simulations

To characterize the effect of our excitatory plasticity rule on a single postsynaptic neuron, we simulated two simple feed-forward networks without inhibition (Fig. 2b, c; Supplementary Fig. 3). More precisely, we simulated a single postsynaptic neuron that received 80 plastic excitatory input from each of two populations of Poisson neurons. The neurons in one population fired at 10 Hz and the initial weight w 0 took different values in the interval (0.2,0.35) as stated in Fig. 2b. Neurons in the second Poisson population fired at 1 Hz and the initial weight was initialized at a value of w ctl (t = 0) = 0.1.

In Fig. 2e we used a similar set-up with 1,000 presynaptic Poisson inputs (initial weight w 0 = 0.05 and) all firing at a constant background rate of 10 Hz. The stimulus set consisted of 10 Gaussian firing rate profiles in the presynaptic index with fixed s.d. σ = 50, but different centres. Stimulation onset was at t = 100 s mean stimulation interval T On = 20 s (T Off = 100 ms).

Balanced network model

We used a balanced network model consisting of 4,096 excitatory and 1,024 inhibitory integrate and fire neurons. The connectivity within the network was random sparse with an overall connection probability of 10%. Neurons in the excitatory population received additional input from an external population of equal size that provided noisy background input. For Figs 3 and 4 these input connections were pre-structured so that cells from within a circular area (radius R = 8) in the 64 × 64 input space projected to individual network neurons (Fig. 3a). The centre of the circle was chosen randomly within the input space for each postsynaptic neuron. For Supplementary Figs 6 and 7 input connections were initialized with random sparse connectivity with 5% connection probability. All excitatory afferent connections relayed stimuli from the external Poisson input population to the network (see 'Stimulation paradigm' above).

In simulations involving plasticity, all afferent connections to excitatory cells in the network were plastic. Moreover, plasticity was always active, also during periods when the network was cued with partial stimuli (for example, Fig. 4).

---

### The management of major depressive disorder: synopsis of the 2022 U.S. department of veterans affairs and U.S. department of defense clinical practice guideline [^112Mp2J5]. Annals of Internal Medicine (2022). High credibility.

Algorithm for major depressive disorder (MDD) — this CPG's algorithm is designed to facilitate understanding of the clinical pathway and decision making process used in managing patients with MDD; it represents a simplified flow of the management of patients with MDD and helps foster efficient decision making by providers; it includes an ordered sequence of steps of care, decisions to be considered, recommended decision criteria, and actions to be taken; the algorithm is a step-by-step decision tree with standardized symbols and arrows connecting numbered boxes indicating the order in which steps should be followed, and sidebars provide more detailed information; shape legend: rounded rectangles represent a clinical state or condition, hexagons represent a decision point formulated as a question that can be answered "Yes" or "No", rectangles represent an action in the process of care, and ovals represent a link to another section within the algorithm; Appendix G contains alternative text descriptions of the algorithm.

---

### Phase nucleation in curved space [^116wF1bb]. Nature Communications (2015). Medium credibility.

Nucleation and growth is the dominant relaxation mechanism driving first-order phase transitions. In two-dimensional flat systems, nucleation has been applied to a wide range of problems in physics, chemistry and biology. Here we study nucleation and growth of two-dimensional phases lying on curved surfaces and show that curvature modifies both critical sizes of nuclei and paths towards the equilibrium phase. In curved space, nucleation and growth becomes inherently inhomogeneous and critical nuclei form faster on regions of positive Gaussian curvature. Substrates of varying shape display complex energy landscapes with several geometry-induced local minima, where initially propagating nuclei become stabilized and trapped by the underlying curvature.

---

### Diffusion sampling schemes: a generalized methodology with nongeometric criteria [^113JB3cU]. Magnetic Resonance in Medicine (2023). Medium credibility.

Purpose

The aim of this paper is to show that geometrical criteria for designing multishell q -space sampling procedures do not necessarily translate into reconstruction matrices with high figures of merit commonly used in the compressed sensing theory. In addition, we show that a well-known method for visiting k-space in radial three-dimensional acquisitions, namely, the Spiral Phyllotaxis, is a competitive initialization for the optimization of our nonconvex objective function.

Theory and Methods

We propose the gradient design method WISH (WeIghting SHells) which uses an objective function that accounts for weighted distances between gradients within M-tuples of consecutive shells, with M ranging between 1 and the maximum number of shells S. All the M -tuples share the same weight ω_M. The objective function is optimized for a sample of these weights, using Spiral Phyllotaxis as initialization. State-of-the-art General Electrostatic Energy Minimization (GEEM) and Spherical Codes (SC) were used for comparison. For the three methods, reconstruction matrices of the attenuation signal using MAP-MRI were tested using figures of merit borrowed from the Compressed Sensing theory (namely, Restricted Isometry Property -RIP- and Coherence); we also tested the gradient design using a geometric criterion based on Voronoi cells.

Results

For RIP and Coherence, WISH got better results in at least one combination of weights, whilst the criterion based on Voronoi cells showed an unrelated pattern.

Conclusion

The versatility provided by WISH is supported by better results. Optimization in the weight parameter space is likely to provide additional improvements. For a practical design with an intermediate number of gradients, our results recommend to carry out the methodology here used to determine the appropriate gradient table.

---

### Bioinspired 3D structures with programmable morphologies and motions [^116Qz2Th]. Nature Communications (2018). Medium credibility.

Multimodular 3D structures

The modular assembly of target metrics can create 3D structures with broad morphological and functional diversity (Fig. 4, Supplementary Figure 12). However, there is an intrinsic problem in assembling modules in the metric space. Each module can randomly adopt the direction of deformation (e.g. upward or downward)or the orientation with respect to other modules due to the symmetric nature of metrics. Thus, a multimodular Ω can in general form multiple different conformations presumably with the same elastic energy (as shown in previous workand Supplementary Figure 12). To tackle this problem, we introduced the concept of transitional components, designed to control the direction of deformation and the orientation of modular components (Fig. 4a–j). A saddle-like structure with K < 0 (e.g. Figs. 2b, 3j) has the principle curvatures with the same sign along its parallel edges. We thus postulated that modular components with K > 0 that share the parallel edges of a saddle-like structure (K < 0), or a parallel transitional component (e.g. small circles with dashed white lines in Fig. 4f–i), would deform in the same direction as the parallel edges. On the other hand, modular components with K > 0 that share the perpendicular edges of a saddle-like structure (K < 0), or a perpendicular transitional component (e.g. large circles with dashed white lines in Fig. 4f–i), would deform in the opposite directions. Figure 4a–d shows examples of multimodular structures with directional control, where the arrows indicate the programmed orientation of each module. Placing the parallel and perpendicular transitional components between modules (indicated by the small and large circles with white dashed lines in Fig. 4f–i, respectively) led to the multimodular structures with designed morphologies (Fig. 4a–d). In contrast, the modules in a control structure without transitional components tend to deform in the same direction, implying slight variations in shrinkage through the thickness; the variations make a specific direction energetically favorable for all modules (Fig. 4e, j).

---

### Bioinspired rational design of bi-material 3D printed soft-hard interfaces [^115mGsL3]. Nature Communications (2023). High credibility.

Durable interfacing of hard and soft materials is a major design challenge caused by the ensuing stress concentrations. In nature, soft-hard interfaces exhibit remarkable mechanical performance, with failures rarely happening at the interface. Here, we mimic the strategies observed in nature to design efficient soft-hard interfaces. We base our geometrical designs on triply periodic minimal surfaces (i.e. Octo, Diamond, and Gyroid), collagen-like triple helices, and randomly distributed particles. A combination of computational simulations and experimental techniques, including uniaxial tensile and quad-lap shear tests, are used to characterize the mechanical performance of the interfaces. Our analyses suggest that smooth interdigitated connections, compliant gradient transitions, and either decreasing or constraining strain concentrations lead to simultaneously strong and tough interfaces. We generate additional interfaces where the abovementioned toughening mechanisms work synergistically to create soft-hard interfaces with strengths approaching the upper achievable limit and enhancing toughness values by 50%, as compared to the control group.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^112kB1Tn]. Annals of the American Thoracic Society (2023). High credibility.

Unbiased stereological sampling — principles and methods note that "Unbiased sampling designs guarantee statistically representative samples and generally must be spatially randomized", and for anisotropic structures some parameters "require spatial orientation randomization". Named approaches include "systematic uniform random sampling", "isotropic uniform random sampling", "stratified sampling", "fractionator sampling", and "proportionator sampling". Importantly, "Voxel-by-voxel automated image analysis ensures neither accuracy nor precision of the results, nor does it obviate the need to strictly follow stereological sampling principles".

---

### Rigorous location of phase transitions in hard optimization problems [^116ZYG3W]. Nature (2005). Excellent credibility.

It is widely believed that for many optimization problems, no algorithm is substantially more efficient than exhaustive search. This means that finding optimal solutions for many practical problems is completely beyond any current or projected computational capacity. To understand the origin of this extreme 'hardness', computer scientists, mathematicians and physicists have been investigating for two decades a connection between computational complexity and phase transitions in random instances of constraint satisfaction problems. Here we present a mathematically rigorous method for locating such phase transitions. Our method works by analysing the distribution of distances between pairs of solutions as constraints are added. By identifying critical behaviour in the evolution of this distribution, we can pinpoint the threshold location for a number of problems, including the two most-studied ones: random k-SAT and random graph colouring. Our results prove that the heuristic predictions of statistical physics in this context are essentially correct. Moreover, we establish that random instances of constraint satisfaction problems have solutions well beyond the reach of any analysed algorithm.

---

### Mathematical estimates of recovery after loss of activity: II. long-range connectivity facilitates rapid functional recovery [^1167zRfq]. Critical Care Medicine (2008). Low credibility.

Objective

To model the effects of system connectedness on recovery of dysfunctional tissues.

Design

One-dimensional elementary cellular automata models with small-world features, where the center-input for a few cells comes not from itself but, with a given probability, from another cell. This probability represents the connectivity of the network. The long-range connections are chosen randomly to survey the potential influences of distant information flowing into a local region.

Setting

MATLAB and Mathematica computing environments.

Patients

None.

Interventions

None.

Measurements and Main Results

We determined the recovery rate of the entropy after perturbing a uniformly dormant system. We observed that the recovery of normal activity after perturbation of a dormant system had the characteristics of an epidemic. Moreover, we found that the rate of recovery to normal steady-state activity increased rapidly even for small amounts of long-range connectivity. Findings obtained through numerical simulation were verified through analytical solutions.

Conclusions

This study links our hypothesis that multiple organ function syndromes represent recoupling failure with a mathematical model showing the contribution of such coupling to reactivation of dormant systems. The implication is that strategies aimed not at target tissues or target organs but rather at restoring the quality and quantity of interconnections across those tissues and organs may be a novel therapeutic strategy.

---

### Computational and neural mechanisms of statistical pain learning [^116fvR9x]. Nature Communications (2022). High credibility.

We designed the task such that the statistics of the sequence could occasionally and suddenly change (i.e. they were volatile), which meant that the sequences of 1500 stimuli included sub-sequences of stimuli (mean 25 ± 4 stimuli per sub-sequence). Participants were not explicitly informed when these changes happened. Figure 1 a illustrates an example of a snapshot of a typical sequence, showing a couple of 'jump' points where the probabilities change. The sequence statistics were Markovian and, thus, incorporated two types of information. First, they varied in terms of the relative frequency of high and low-intensity stimuli (from 15 to 85%), to test whether the frequency statistics can be learned. Second, sequences also contained an additional aspect of predictability, in which the conditional probability of a stimulus depended on the identity of the previous stimulus (i.e. the transition probability of high or low pain following a high pain stimulus, and the transition probability of high or low pain following a low pain stimulus; Fig. 1 c). By having different transition probabilities between high and low stimuli within sub-sequences, it is possible to make a more accurate prediction of a forthcoming stimulus intensity over-and-above simply learning the general background statistics. Thus, we were able to test whether participants learn the frequency or the transition probabilities between different intensities, as shown previously with visual stimuli. For this reason, our design mirrored a well-studied task used to probe statistical learning with visual stimuli. From a mathematical point of view, the frequency can always be derived from the transition probabilities, but not vice versa. Therefore, participants were not asked to rate the frequency of the stimuli because it can be simply derived from their transition probability ratings; in contrast, transition probability estimates cannot be derived from frequency estimates.

---

### Cutting edge or blunt instrument: how to decide if a stepped wedge design is right for you [^112pSeFT]. BMJ Quality & Safety (2021). High credibility.

When might I consider doing a stepped wedge trial?

Research designs are shaped as much by practical constraints as by abstract schemes, and it is always a good idea to start with the constraints and work towards a design, rather than start with a design and try to fit it to constraints. These constraints will be unique to each research context, and box 1 lists some areas to think about. Still, there are some common features of settings where a stepped wedge trial might be considered as a possible design, and we now review these.

Box 1
Practical constraints on the design of a longitudinal cluster randomised trial

Are there limits on the time available to complete the evaluation, on the number of clusters, or on the number of participants (or the rate at which you can recruit participants) at each cluster? These constraints put limits on the overall scale of the evaluation, or force trade-offs between different design characteristics.
How will participants and their data be sampled in your study: as a series of cross-sectional surveys, as a continuous stream of incident cases, as a cohort followed over time, or some other way? Does the timescale divide into cycles, seasons or milestones that influence how you will sample participants and data?
Is there a limit on how many clusters can implement the intervention at the same time in the evaluation? If this is constrained by research resources (eg, if there are only enough trained research staff to implement the intervention one cluster at a time) then implementation must be staggered in some way.
If implementation is to be staggered, is there a minimum 'step length'? If the same team delivers the intervention in different clusters at different steps, then bear in mind it may take some time to get the intervention fully operational at a site, and the team will also need time to relocate from one cluster to the next.

---

### Generating brain-wide connectome using synthetic axonal morphologies [^112hLL7F]. Nature Communications (2025). High credibility.

Synthesizing in the mouse brain

We now present how we used the axonal projections analysis presented in section "Axonal projections analysis" as input data for the synthesis algorithm in section "Long-range axon synthesis" to synthesize LRAs in the mouse brain.

Initial morphologies synthesis

First of all, we synthesized neuronal morphologies made of somata, dendrites, and grafted reconstructed local axons in the isocortex, with the same methodology as in refs. These local axons were copied from previous experimental reconstructions and grafted to the synthesized cells. Since mostly pyramidal cells of the mouse cortex project to distal regions, we filtered pyramidal cells of the brain region s a and would replace their local axon with a synthesized LRA. We synthesized LRAs for all pyramidal cells in the regions for which we synthesize, but that can be changed to a provided portion of them.

Source points

The somata of the filtered pyramidal cells were used as the source points of the synthesis algorithm. We assigned a GMM cluster for each source point by randomly picking a cluster c with probability p c from the clusters C of source s a.

Target points

We computed the probability of an axon to target a brain region b as the number of axons targeting b, divided by the total number of axons in the cluster. For a picked target brain region, a numberof target points were randomly placed inside b. All the targets of axon a were then connected with a better edge weight for targets inside fiber tracts to form the trunk. In this way, the trunk would preferentially follow the fiber tracts.

Tufts selection

Finally, tufts were selected for each target point with probability computed based on the representativity scores of their group g, and then synthesized with the NeuroTS software.

Creating connections

Once all the morphologies were fully synthesized, we made axo-dendritic connections as described in ref. In a few words, touches were detected based on the physical proximity of neurite branches and filtered based on a minimum inter-bouton interval. Touches were then pruned based on physiological synapse density and converted into synapses. Connections and connectivity matrices were analyzed using the ConnectomeUtilities software.

Reporting summary

Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.

---

### VA / DoD clinical practice guideline for the non-surgical management of hip&knee osteoarthritis [^112ybdRr]. VA/DoD (2020). High credibility.

Algorithm — hip and knee osteoarthritis (OA) management is presented as a step-by-step decision tree to facilitate understanding of clinical pathways and decision-making, with standardized symbols and ordered flow. This clinical practice guideline includes an algorithm designed to facilitate understanding of the clinical pathways and decision-making processes used in managing patients with OA and includes an ordered sequence of steps of care, recommended observations and examinations, decisions to be considered, and actions to be taken. For each VA/DoD clinical practice guideline, a corresponding clinical algorithm is depicted by a step-by-step decision tree; standardized symbols are used to display each step, and arrows connect the numbered boxes indicating the order in which the steps should be followed. Shape legend for interpreting the algorithm: rounded rectangles represent a clinical state or condition, hexagons represent a decision point formulated as a question that can be answered Yes or No, rectangles represent an action in the process of care, and ovals represent a link to another section within the guideline.

---

### A scaling law for random walks on networks [^112AGjuv]. Nature Communications (2014). Medium credibility.

The dynamics of many natural and artificial systems are well described as random walks on a network: the stochastic behaviour of molecules, traffic patterns on the internet, fluctuations in stock prices and so on. The vast literature on random walks provides many tools for computing properties such as steady-state probabilities or expected hitting times. Previously, however, there has been no general theory describing the distribution of possible paths followed by a random walk. Here, we show that for any random walk on a finite network, there are precisely three mutually exclusive possibilities for the form of the path distribution: finite, stretched exponential and power law. The form of the distribution depends only on the structure of the network, while the stepping probabilities control the parameters of the distribution. We use our theory to explain path distributions in domains such as sports, music, nonlinear dynamics and stochastic chemical kinetics.

---

### Invariant and smooth limit of discrete geometry folded from bistable origami leading to multistable metasurfaces [^113EPzDR]. Nature Communications (2019). High credibility.

Fig. 4
Folded configurations of three triangulated hypar origami with different panel widths display invariant hypar geometry (same patterns as in Fig. 1). The three origami patterns have the same number of square creases (counted from the center outwards), denoted by n. Every other square crease is a mountain, and thus there are 10 mountain creases in total. Two panels on the both sides of a mountain crease are assigned the same width. All three concentrically pleated origami can fold approximately into the same hypar shape, as indicated by the red quadratic curves. a Standard hypar pattern: panel width d is a constant for all corrugations. b Functionally graded hypar pattern: panel width d increases as n increases. c Random hypar pattern: panel width d is a random variable for each corrugation. Since the hypar patterns are triangulated, folding is achieved by rigid origami simulation. The left images show the 3D views of the folded hypar origami. The middle images show the side views. The right images present quantitative measures. The blue dots (each dot corresponds to a mountain) in the charts show how (d / L) vanishes as n increases. The green dots show the residual (or error) of Eq. (2) when n is finite

Connecting geometry with mechanics

With an analytical description of the global homogenized geometry of the hypar pattern, we can associate the mechanical behavior of the pattern with its geometry by expressing the system energy in terms of the folding kinematics. First, let us consider how a hypar origami folds into a stable saddle shape. We start from a flat hypar pattern, and forcefully fold up the pattern. After the origami is folded into shape, the folding creases undergo inelastic deformation, which shifts the neutral (i.e. stress free) angles of the folding hinges from zero to the angles at a folded state with shape coefficient k = k 0 (see Eq. 10). However, the panels undergo elastic deformation, as we observe that they return to the initial flat configuration once cut out from a folded hypar origami. Thus, we assume that the bending hinges still have their neutral angles equal to zero, which corresponds to the state at k = 0. A stable configuration is drawn by minimizing the system total energy (E T).

---

### The need for rotational margins in intensity-modulated radiotherapy and a new method for planning target volume design [^1118H59x]. International Journal of Radiation Oncology, Biology, Physics (2005). Low credibility.

Purpose

The effect of rotational errors on the coverage of clinical target volumes (CTVs) is examined. A new planning target volume (PTV) construction that considers the individual paths traced by movements of the target boundary points is developed.

Methods and Materials

A standard uniform margin expansion was compared with a PTV constructed from the space swept out by a concave moving target. A new method formed the PTV by aggregating the separate convex hulls taken of the positions of the individual target boundary points in a sampling of CTV displacements.

Results

A 0.5-cm uniform margin adequate for translations was inadequate given CTV rotation about a fixed off-center axis. A PTV formed of the target's swept-out area was 22% smaller than needed for coverage by a uniform margin, but computationally is not readily extended to translations combined with rotations about a shifting axis. Forming instead the union of convex hulls of the boundary points in a sampling of CTV displacements represented these movements in the PTV design and retained the target's concave shape.

Conclusions

Planning target volumes should accommodate target rotation. The union of convex hulls of the boundary point positions in a sampling of displacements can effectively represent multiple sources of deviations while preserving target concavities.

---

### A deterministic neuromorphic architecture with scalable time synchronization [^116K8SAv]. Nature Communications (2025). High credibility.

Network configurations

In the lattice network studied in Fig. 3, each core consists of 200 neurons, spiking with a probability of 0.01 at each time step. If one time step models 1 ms of neural activity, this configuration corresponds to a spike rate of 10 Hz. The total wall-clock time required to run 100 time steps is measured for the configured networks. For the configuration with 16,384 cores, 20 time steps are simulated as a reference due to limitations in computational resources.

The recurrent neural network studied in Fig. 4 consisted of 16 recurrently-connected neural populations (P1-P16), with each population containing 200 neurons. Populations 1 through 16 have biases of 1 through 16, respectively, with a fixed threshold of 100. Higher bias results in denser spiking activity. Connections are formed uniformly at random within populations with a probability of 0.1, and between populations with a probability of 0.05. All synaptic connections have a weight of one. The 16 populations are mapped onto an 8 × 8 core layout in adjacent sectional order.

The navigation problem of Fig. 5 uses 2D grid mazes of varying sizes: 400 × 400, 800 × 800, 1600 × 1600, and 3200 × 3200. Each maze contains obstacles occupying 40% of the grid points. Obstacle positions are selected uniformly at random, with the remaining locations forming feasible paths. The source and destination points are also selected uniformly at random. The grid is mapped to a graph, where each non-obstacle point is represented by a node, and edges are added between adjacent nodes. Due to computational resource constraints, simulations are limited to a maximum of 100 time steps.

---

### Functional analysis of multiple genomic signatures demonstrates that classification algorithms choose phenotype-related genes [^113xmjHw]. The Pharmacogenomics Journal (2010). Low credibility.

Signature genes are interconnected, but connectivity does not correlate with signature performance

An important measure of functional connectivity between genes is the ability of their protein products to form DI networks. The size of the DI network and the probability (P -value) of assembling a DI network with the same number of nodes from a random set of genes serves as a quantitative measure of interconnectivity (see Materials and methods for details). We built DI networks for all 262 signatures and 13 unions (Supplementary Table 2; Supplementary File 4). The ability of signature gene lists to generate DI networks was dependent on the phenotype and the statistical method used to derive the signature. No significant DI networks were assembled from any signature for end points A, B, H and L. Conversely, between 45 and 85% of signatures assembled into statistically significant DI networks for end points C, D, E and K. Signatures generated by certain analysis groups (likely reflecting the methodology used) formed DI networks more consistently than other teams' signatures. Over 40% of signatures derived by the SAI, Tsinghua and ZJU teams formed networks. Signatures from other teams, however, did not assemble into DI networks regardless of the end point (DKFZ, GT, JHSPH).

Signature unions for 10 of the 13 end points formed statistically significant DI networks (Supplementary File 4). No networks were formed by the unions of end points B, H and L. Altogether, 22 out of 33 teams generated signatures capable of forming statistically significant DI networks, indicating that the genes comprising most signatures were interconnected to a significantly higher degree than expected for a random gene list of the same size. Therefore, we conclude that the predictive models used tended to select interconnecting genes. We saw no correlation between signature performance and interconnectivity between its genes, however, so there does not appear to be any relationship between the biological interconnectedness of signature genes and their predictive power.

---

### As-3 [^117FFFuc]. FDA (2025). Medium credibility.

Storage and handling

Store at room temperature (25 °C/77 °F). Avoid excessive heat.

Protect from freezing

---

### VA / DoD clinical practice guideline for management of posttraumatic stress disorder and acute stress disorder [^1157im8G]. DoD/VA (2023). High credibility.

Algorithm — VA/DoD Clinical Practice Guideline for Management of Posttraumatic Stress Disorder — states that this clinical practice guideline's algorithm is designed to facilitate understanding of the clinical pathway and decision-making process used in managing patients with PTSD and represents a simplified flow that helps foster efficient decision making by providers; it includes steps of care in an ordered sequence, decisions to be considered, decision criteria recommended, and actions to be taken. The algorithm is a step-by-step decision tree in which standardized symbols display each step and arrows connect the numbered boxes indicating the order in which the steps should be followed; sidebars 1–11 provide more detailed information to assist in defining and interpreting elements in the boxes. The legend specifies that rounded rectangles represent a clinical state or condition, hexagons represent a decision point in the process of care formulated as a question that can be answered Yes or No, rectangles represent an action in the process of care, and ovals represent a link to another section within the algorithm, and Appendix H contains alternative text descriptions of the algorithms.

---

### Standards of care in diabetes – 2025 [^1177SxqY]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for diabetic nephropathy, more specifically with respect to pediatric patients, ADA 2025 guidelines recommend to determine the eGFR at the time of diagnosis and annually thereafter.

---

### Standards of care in diabetes – 2025 [^113RQNuE]. Diabetes Care (2025). High credibility.

Regarding follow-up and surveillance for diabetes mellitus type 1, more specifically with respect to continuous glucose monitoring, ADA 2025 guidelines recommend to implement early continuous glucose monitoring in adult patients with T1DM to improve glycemic outcomes and QoL and minimize hypoglycemia.

---

### Clotrimazole [^115eJtJD]. FDA. Low credibility.

The dosage of clotrimazole OTIC for treatment of otomycosis in adults is 1 vial OTIC BID for 14 days (1%/0.17 mL)

---

### Standards of care in diabetes – 2025 [^112GkM3T]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for hypoglycemia, more specifically with respect to screening for cognitive impairment, ADA 2025 guidelines recommend to simplify diabetes treatment plans as much as possible for patients with cognitive impairment and tailor to minimize the risk of hypoglycemia.

---

### Appropriate use of drug testing in clinical addiction medicine [^111HuaQD]. Journal of Addiction Medicine (2017). Medium credibility.

Random testing — preference and implementation notes that whatever the frequency, clinical consensus favors unannounced drug testing over scheduled drug testing, because a fixed schedule (e.g., every Monday) offers patients increased opportunity to engage in sample tampering, and even when frequency is within a test's normal window of detection (e.g., a urine immunoassay screen for amphetamines every Monday and Thursday) it is possible for a patient to engage in substance use on Thursday night and not produce a positive result on Monday morning. The purest form of randomization is to have a set probability (e.g., 15%) that a patient could be tested on any given day, which is akin to rolling a die every day and testing whenever a 6 appears; while this eliminates known safe periods, the length of time a patient may go between testing can be quite long, and many addiction treatment providers randomly select a day from a fixed interval.

---

### Roflumilast (Zoryve) [^116oLut2]. FDA (2024). Medium credibility.

The dosage of roflumilast TOP for treatment of plaque psoriasis in adults is 1 application(s) TOP daily (0.3% cream or foam)

---

### Validation of monte carlo estimates of three-class ideal observer operating points for normal data [^111zPxr4]. Academic Radiology (2013). Low credibility.

Rationale and Objectives

Traditional two-class receiver operating characteristic (ROC) analysis is inadequate for the complete evaluation of observer performance in tasks with more than two classes.

Materials and Methods

Here, a Monte Carlo estimation method for operating point coordinates on a three-class ROC surface is developed and compared with analytically calculated coordinates in two special cases: (1) univariate and (2) restricted bivariate trinormal underlying data.

Results

In both cases, the statistical estimates were found to be good in the sense that the analytical values lay within the 95% confidence interval of the estimated values about 95% of the time.

Conclusions

The statistical estimation method should be key in the development of a pragmatic performance metric for evaluation of observers in classification tasks with three or more classes.

---

### Understanding the underlying systems dynamics contributing to the continued predominance of the unhealthy motorway food environment in the Netherlands: identifying leverage points and actions for change [^1174q7HB]. BMC Medicine (2025). Medium credibility.

GMB workshop 1

The first GMB workshop aimed to create a CLD, to identify and illustrate the system that contributes to the continued predominance of the unhealthy motorway food environment. This involved identifying the most important factors and their interconnections that shape this system. The workshop started with an introductory opening and explanation of the food environment followed by multiple GMB techniques that were based on scripts. First, the participants individually wrote down their hopes and fears to elicit and establish their expectations for the workshop. These expectations were then briefly discussed in a plenary session. Next, participants were divided into seven groups, each consisting of five to six participants. The groups consisted of participants from different stakeholder groups and were led by a member of the research team who also took notes. First, participants individually identified factors influencing the continued predominance of the unhealthy motorway food environment according to the Graphs Over Time script, with all identified factors being documented. Second, participants were instructed to discuss and choose the five most important factors in their group. After this, each group named their most important factors in the plenary session, until a final list of about 20 to 30 factors was reached. In this plenary session, the factors were discussed in terms of their influence on the unhealthy motorway food environment. The discussion concluded by questioning whether these factors were indeed the most important which was confirmed by the group. Third, participants made a connection circle in a plenary session, in which the connections between these factors were identified, resulting in the creation of the concept CLD. Participants were asked to indicate whether connections were either accompanied by a plus (+) or a minus (−) sign. A plus sign indicated that when the initial factor increased, the receiving factor also increased, and similarly when the initial factor decreased the receiving factor also decreased (positive relationship). Contrarily, a minus sign indicates that when the initial factor increased, the receiving factor decreased, or when the initial factor decreased the receiving factor increased (negative relationship). These factors formed a feedback loop when one factor influenced other factors, either reinforcing (+) or balancing (−), thereby creating a continuous cycle with the initial factor. Whilst these loops represent key dynamics, additional system dynamics may exist. This concept CLD offered a visual overview of the identified factors influencing a complex problem, illustrating the interconnections between these factors and feedback loops according to stakeholders. The full programme of GMB techniques used in workshop 1 can be found in more detail in Supplementary Table 1, Additional file 1. Systems Thinking In Community Knowledge Exchange (STICKE) software was used to form the identified factors and connections into a CLD.

---

### VA / DoD clinical practice guideline for the primary care management of asthma [^113xMKLJ]. DoD/VA (2025). High credibility.

VA/DoD Clinical Practice Guideline for the Primary Care Management of Asthma — algorithm legend and scope: This clinical practice guideline (CPG) algorithm is designed to facilitate understanding of the clinical pathway and decision-making process used in the primary care management of asthma and represents a simplified flow that helps foster efficient decision making; it includes an ordered sequence of steps of care, recommended observations and examinations, decisions to be considered, and actions to be taken. The algorithm is a step-by-step decision tree in which standardized symbols are used to display each step and arrows connect the numbered boxes indicating the order in which the steps should be followed; Sidebars A-J provide more detailed information to assist in defining and interpreting elements in the boxes. Symbol meanings are defined as: rounded rectangles represent a clinical state or condition, hexagons represent a decision point in the guideline formulated as a question that can be answered "Yes" or "No", rectangles represent an action in the process of care, and ovals represent a link to another section within the algorithm.

---

### Understanding quantum machine learning also requires rethinking generalization [^111QczzY]. Nature Communications (2024). High credibility.

Theorem 3

(Conditioning as a convex program 1). Let ρ 1, …, ρ N be unknown, linearly-independent quantum states on n qubits, with. For any i ∈ [N], letbe approximations of ρ i, each of which can be efficiently prepared using a PQC. Assume the computation ofin polynomial time for any choice of i, j and k. Call σ = (σ 1, …, σ N). The real numbersdefine the auxiliary statesasand the matrix of inner productswith entriesThen. Further, one can then decide in polynomial time whether, given ρ 1, …, ρ N, σ, and, there exists a specification ofsuch thatis well-conditioned in the sense that. And, if there exists such a specification, a convex semi-definite problem (SDP) outputs an instance of α ← SDP(ρ, σ, κ) for whichis well-conditioned. If it exists, one can also find in polynomial time the α with the smallestornorm.

Proof

The inequalityfollows from Gershgorin's circle theorem, given that all entries ofare bounded between [0, 1]. In particular, the largest singular value of the matrixreaches the value N when all entries are 1.

The expressionis a linear constraint on α and, for i, j ∈ [N], whilein matrix ordering is a positive semi-definite constraint.is equivalent with, whilemeans that the smallest singular value ofis lower bounded by κ, being equivalent withfor an invertible. The test whether such ais well-conditioned hence takes the form of a semi-definite feasibility problem. One can additionally minimize the objective functionsandboth again as linear or convex quadratic and hence semi-definite problems. Overall, the problem can be solved as a semi-definite problem, that can be solved in a run-time with low-order polynomial effort with interior point methods. Duality theory readily provides a rigorous certificate for the solution.

---

### Mapping the optimal route between two quantum States [^116Gks7P]. Nature (2014). Excellent credibility.

A central feature of quantum mechanics is that a measurement result is intrinsically probabilistic. Consequently, continuously monitoring a quantum system will randomly perturb its natural unitary evolution. The ability to control a quantum system in the presence of these fluctuations is of increasing importance in quantum information processing and finds application in fields ranging from nuclear magnetic resonance to chemical synthesis. A detailed understanding of this stochastic evolution is essential for the development of optimized control methods. Here we reconstruct the individual quantum trajectories of a superconducting circuit that evolves under the competing influences of continuous weak measurement and Rabi drive. By tracking individual trajectories that evolve between any chosen initial and final states, we can deduce the most probable path through quantum state space. These pre- and post-selected quantum trajectories also reveal the optimal detector signal in the form of a smooth, time-continuous function that connects the desired boundary conditions. Our investigation reveals the rich interplay between measurement dynamics, typically associated with wavefunction collapse, and unitary evolution of the quantum state as described by the Schrödinger equation. These results and the underlying theory, based on a principle of least action, reveal the optimal route from initial to final states, and may inform new quantum control methods for state steering and information processing.

---

### Roflumilast (Zoryve) [^111TASQF]. FDA (2024). Medium credibility.

The dosage of roflumilast TOP for treatment of plaque psoriasis in both children (in patients 6–12 years) is 1 application(s) TOP daily (0.3% cream)

---

### AGA clinical practice guideline on management of gastroparesis [^11542B3n]. Gastroenterology (2025). High credibility.

Regarding diagnostic investigations for gastroparesis, more specifically with respect to gastric emptying studies, methodology, AGA 2025 guidelines recommend to perform a 4-hour study in all cases, as some patients with normal emptying at 2 hours may be reclassified as delayed at 4 hours.

---

### The temporal dynamics of group interactions in higher-order social networks [^1134Wb8Q]. Nature Communications (2024). High credibility.

Fig. 4
Fitting the empirical group-change probability and measuring the signal of social memory.

The different panels refer to out-of-class interactions from the CNS (A – C) and the DyLNet (D – F) data sets. A, D Probability for a node belonging to a group of any size to leave it for a different one after exactly τ timestamps. Points are binned empirical results, dashed lines represent a power-law fit of the form b τ β — values reported in each panel. Fitted exponents β are then used to estimate the group-size-dependent constants b k, as given in Eq. (1), with another power-law fit. In B, E the resulting values for b k are plotted (points and 95% CI error bars) together with a logistic fit (dashed lines). C, F Distribution of fraction of nodes — composing a newly chosen group — that were previously known to the focal node. The resulting distributions of values, averaged over the different time steps, are plotted in comparison to two null scenarios where the group to join is chosen at random, or at random given the target size.

We thus assume in the model that the probability that a node i in a group of size k has a change of group decreases with the time τ that node i has spent in that group (residence time) aswhere β is a real-valued exponent that modulates the impact of the residence time, which we obtain by fitting the empirical distributions (dashed lines in Fig. 4 A, D). With probability 1 − p k (τ) the agent i stays in the same group.

In Equation (1), b k is a constant that depends on the size of the current group of node i. It is indeed reasonable to assume that the probability of leaving a group also depends on the size of the group itself, as size is a crucial factor that determines a group's sociological form, and its ability to sustain a single conversation — leading to the phenomenon known as schisming. To gain insights on the dependency of b k on the group size k, we fit each P ↷ (k, τ) using a power-law function of the form b k τ − β (with β taken from the fit shown in Fig. 4 A, D). The resulting values of b k are reported in Fig. 4 B, E. They show a monotonic increase with k, which we fit to a logistic form

---

### The skeleton structure – an improved compression algorithm with perfect reconstruction [^114bYSNW]. Journal of Digital Imaging (2001). Low credibility.

This article presents an improved morphological image representation that can be used for image compression, obtaining very high compression rates. The new image representation described in this work is called skeleton structure and is a natural extension of the morphologic structure. This article will present its theoretical background, introduce the new representation, and show some application examples.

---

### A deep learning method for replicate-based analysis of chromosome conformation contacts using siamese neural networks [^111RPJSC]. Nature Communications (2023). High credibility.

Twins testing

To test the network performance we use the Euclidean embedding distance as our learnt metric. To calculate a measure of mean performance we use the train and validation distance distributions to create a threshold for classification into replicate/condition pairs and count the rate of correct classification for replicates (replicate rate) and conditions (condition rate). The threshold is calculated using point where the replicate and condition distributions overlap. The mean performance is then the average of the replicate and condition rates. This is to take into account the in-balance between the replicate and condition distributions.

Another measure of network performance is given by separation index, this is derived from the 1 - integral of the overlap between the replicate and condition probability density distributions. This gives a metric for performance which is 1 if the replicate and condition distributions do not overlap and 0 if there is a complete overlap between the distributions.

Finally, we use the integrated gradients on individual window pairs to understand where the network places importance. To do this we deep copy the network weights into a standard CNN and then use the IntegratedGradients function from the captum packageto compare the windows.

Feature extraction

To extract features from our chromatin conformation maps we feed our windows through the convolutional layers of the network. We feed each of our windows each individually into the convolutional layers and then subtract the output and amalgamate across the genome taking the mean values across locations and filters. We then take the mean across all replicate groupings and the mean across all condition groups to produce one replicate and one condition map.

We calculate the threshold by taking the 0.95th percentile of values obtained by the application of the same method on the replicate map. We label featuresand remove any which are smaller than 50 kb in both dimensions or greater than 2.56 Mb in size. We then apply a convex hull to our extracted features. These are re-scaled and fed as matrices into a kmeans algorithm with k = 10. Since the orientations are not separated, stripes pointing up or down stream are grouped into two separate clusters, similarly asymmetric contact domains are grouped according to orientations and there are several clusters which correspond to domain-like events. These clusters are grouped together by manual inspection to form the three groups discussed in Fig. 8.

Reporting summary

Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.

---

### VA / DoD clinical practice guideline for the diagnosis and treatment of low back pain [^116qspm3]. DoD/VA (2022). High credibility.

VA/DoD low back pain (LBP) algorithm — This clinical practice guideline's algorithm is designed to facilitate understanding of the clinical pathway and decision making process used in managing patients with LBP and represents a simplified flow that helps foster efficient decision making by providers; it includes an ordered sequence of steps of care, decisions to be considered, and recommended decision criteria. The algorithm is a step-by-step decision tree with standardized symbols; rounded rectangles represent a clinical state or condition, hexagons represent a decision point in the process of care formulated as a question that can be answered "Yes" or "No", rectangles represent an action in the process of care, ovals represent a link to another section within the algorithm, and arrows connect the numbered boxes indicating the order in which the steps should be followed. Appendix I contains alternative text descriptions of the algorithm.

---

### Functional specificity of local synaptic connections in neocortical networks [^114wgSSu]. Nature (2011). Excellent credibility.

Neuronal connectivity is fundamental to information processing in the brain. Therefore, understanding the mechanisms of sensory processing requires uncovering how connection patterns between neurons relate to their function. On a coarse scale, long-range projections can preferentially link cortical regions with similar responses to sensory stimuli. But on the local scale, where dendrites and axons overlap substantially, the functional specificity of connections remains unknown. Here we determine synaptic connectivity between nearby layer 2/3 pyramidal neurons in vitro, the response properties of which were first characterized in mouse visual cortex in vivo. We found that connection probability was related to the similarity of visually driven neuronal activity. Neurons with the same preference for oriented stimuli connected at twice the rate of neurons with orthogonal orientation preferences. Neurons responding similarly to naturalistic stimuli formed connections at much higher rates than those with uncorrelated responses. Bidirectional synaptic connections were found more frequently between neuronal pairs with strongly correlated visual responses. Our results reveal the degree of functional specificity of local synaptic connections in the visual cortex, and point to the existence of fine-scale subnetworks dedicated to processing related sensory information.

---

### Standards of care in diabetes – 2025 [^11566QPS]. Diabetes Care (2025). High credibility.

Regarding follow-up and surveillance for diabetes mellitus type 1, more specifically with respect to continuous glucose monitoring, ADA 2025 guidelines recommend to assess skin reactions, either due to irritation or allergy, and address to aid in successful use of devices.

---

### 2020 focused update of the 2017 ACC expert consensus decision pathway on the management of mitral Regurgitation: a report of the American college of cardiology solution set oversight committee [^1178U3UB]. Journal of the American College of Cardiology (2020). High credibility.

Mitral regurgitation severity assessment — comprehensive transthoracic echocardiography: Evaluation of MR severity requires a comprehensive TTE and no single echocardiographic parameter should be the sole arbiter; the effects of chronic MR on left ventricle (LV) and left atrium (LA) and pulmonary artery pressure must be considered in an integrative fashion. Clinicians often use color flow Doppler (CFD) to form an initial impression, which should only be a starting point requiring further confirmation using a Bayesian approach that integrates multiple factors. After the initial impression, one should consider whether LA and LV sizes are normal and whether MR is holosystolic; if MR appears severe from a large CFD jet but LA/LV sizes are normal and MR is limited to late systole, the impression is most likely an overestimate. One should consider common reasons for overestimation such as high MR driving velocity and MR duration limited to very early or very late systole. When multiple specific parameters for mild or severe MR align with the initial impression, MR can be correctly graded with high probability, a scenario relatively common in practice, especially with mild MR and a structurally normal mitral valve (MV).

---

### Clinical practice guideline for management of tinnitus: recommendations from the US VA / DOD clinical practice guideline work group [^116H5PEs]. JAMA Otolaryngology — Head & Neck Surgery (2025). High credibility.

VA/DoD Clinical Practice Guideline for Tinnitus — algorithm overview and symbol legend: This clinical practice guideline (CPG) algorithm is designed to facilitate understanding of the clinical pathway and decision-making process used in managing patients with tinnitus and represents a simplified flow of the initial evaluation of tinnitus and management and improvement of quality of life (QoL) with tinnitus to help foster efficient decision making by providers. It includes steps of care in an ordered sequence, decisions to be considered, decision criteria recommended, and actions to be taken. The algorithm is a step-by-step decision tree, and standardized symbols display each step while arrows connect the numbered boxes indicating the order in which the steps should be followed. The shape legend specifies that rounded rectangles represent a clinical state or condition, hexagons represent a decision point in the process of care formulated as a question that can be answered "Yes" or "No"., rectangles represent an action in the process of care, and ovals represent a link to another section within the algorithm; Appendix L contains alternative text descriptions of the algorithms.

---

### CONE: community oriented network estimation is a versatile framework for inferring population structure in large-scale sequencing data [^117NykRC]. G3 (2017). Low credibility.

Choosing the tuning parameter

The neighborhood selection scheme is comparatively easy to use and implement as nice statistical properties are obtained by adjusting a single penalty parameter λ. Various common model selection methods could be used to identify the optimal value of the tuning parameter, including k-fold cross-validation, Akaike's information criterion, or the Bayesian information criterion (see, e.g. pp. 230–245). In practice it is well known that cross-validation and information criteria are often impractical in graph construction because their application usually produces rather dense graphs with many false-positive edges.

In this article we represent two methods that can be used to choose a proper value for the tuning parameter, one using a stability approach and the other, modularity approach, which is similar to the so-called "elbow" method, commonly applied in hierarchical clustering (see e.g.;, pp. 187–190).

Finding the optimal tuning parameter — The stability approach:

used a stability approach to regularization selection (StARS) to choose the most stable graph. The StARS is a general way of obtaining an undirected graph-selection criterion when the graph estimation is based on the penalized likelihood minimization problem as in Equation 3. StARS guarantees that the graph estimate will be consistent, provided that some moderate conditions are met (see theorem 4.2 in). Our regularization selection procedure is based strictly on the methods described in the work of.

Suppose we want to choose the "best" value for the tuning parameter λ in Equation 3 over a sequence of different positive values. With StARS, we use subsampling to measure the uncertainty regarding the presence of edges between the nodes in the graph estimatefor each fixed value of λ. Then we calculate the total instability of the estimated graph based on the subsamples. Finally, we choose an estimatefor λ that gives the optimal stability among the sparse graphs based on a special instability measureassociated with the corresponding graph estimate. Assuming the sequence of the instability measure is monotonic, we can choosewith a threshold parameter β. We useas a default value for our StARS procedure, as proposed in. The selection of λ using the quantityis illustrated for the case of a data set in Figure 1.

---

### Recommendations for noninvasive evaluation of native valvular regurgitation: a report from the American Society of Echocardiography developed in collaboration with the Society for Cardiovascular Magnetic Resonance [^116w5fr5]. Journal of the American Society of Echocardiography (2017). Medium credibility.

Mitral regurgitation imaging — significant secondary mitral regurgitation (MR) in dilated cardiomyopathy is shown with marked tenting of the mitral valve (MV) and parasternal short-axis color Doppler demonstrating regurgitation along the entire length of valve coaptation. Pulsed-wave (PW) Doppler at the tips of the MV leaflets shows high E velocity, and continuous-wave (CW) Doppler has a triangular spectral shape in significant MR. Proximal isovelocity surface area (PISA) from apical three- and two- (commissural) chamber views shows nonhemispheric flow convergence and a noncircular vena contracta (VC), and the legend cautions that assumption of a hemispheric flow convergence or circular VC would underestimate MR.

---

### Emergent geometry and duality in the carbon nucleus [^112STb83]. Nature Communications (2023). High credibility.

Results and discussion

Spectrum and electromagnetic properties

We present results obtained from nuclear lattice simulations using two different interactions. The first is an interaction that is independent of spin and isospin. We call it an SU(4) interaction due to the symmetry with respect to the four nucleonic degrees of freedom. The second interaction is based on the framework of chiral effective field theory, carried out to next-to-next-to-leading order (N2LO). We have calculated the 12 C spectrum up to excitation energies of about 15 MeV. The results are plotted in the left panel of Fig. 1 for different values of the angular momentum and parity. For comparison, we show the experimental data (black stars). The error bars are one standard deviation estimates due to stochastic errors and Euclidean time extrapolation uncertainties. The lattice results using the SU(4) and N2LO chiral interactions are in good agreement with each other and in good agreement with experimental data. The triangular shapes surrounding the data points indicate intrinsic shapes and are explained later in our discussion. In the right panel of Fig. 1, we show the form factors for the ground state and the Hoyle state in the top figure (b), and the transition form factor from the ground state to the Hoyle state in the bottom figure (c). For comparison, we show the experimental data for the ground state and transition form factors. and the agreement is fairly good.

Fig. 1
Spectrum of 12 C and charge form factors.

a Spectrum of 12 C using N2LO interaction (red squares) and SU(4) interaction (blue circles) in comparison with experimental data (black stars). The error bars correspond to one standard deviation errors. The gray shaded regions indicate decay widths for cases where it has been measured. The triangular shapes indicate the intrinsic shape of each nuclear state, either equilateral or obtuse triangle arrangements of alpha clusters. The dotted lines for some equilateral triangles indicate significant distortions or large-amplitude displacements of the alpha clusters. b, c Absolute value of the charge form factor F (q) using the SU(4) interaction. b The ground state (red squares) and Hoyle state (green circles), and c the transition from the ground state to the Hoyle state (red squares). The error bars correspond to one standard deviation errors. Experimental data (purple stars) are shown for comparison.

---

### Standards of care in diabetes – 2025 [^113EK77G]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for diabetes mellitus type 1, more specifically with respect to pediatric patients (physical activity), ADA 2025 guidelines recommend to advise frequent glucose monitoring before, during, and after exercise, via a blood glucose meter and/or continuous glucose monitoring, to prevent, detect, and treat hypoglycemia and hyperglycemia associated with exercise.

---

### Minimum electric-field gradient coil design: theoretical limits and practical guidelines [^111BTS9R]. Magnetic Resonance in Medicine (2021). Medium credibility.

Force and torque constraints can be added using the known main B 0 field distribution, integrating over the current distribution for each basis function and entering the results into matrix H or C. One row is added for each direction of force or torque, to obtain an overall magnitude constraint. Eddy currents are computed in a similar manner; the step response for time‐dependent eddy currents on the surface of a conducting cylinder — typically the thermal shield in the magnet — is computed at multiple points in space and time. Each point in space and time is entered as a separate inequality constraint equation. It is usually sufficient to constrain eddy currents at time = 0 following a step change in gradient current, reducing the size of the problem.

Finally, and of primary significance to the present work, E‐field constraints can be handled in a similar manner to those for force and torque. At gradient frequencies of interest, the time evolution of E‐fields in human tissues is short, and therefore proportional to instantaneous slew rate and not the detailed waveform shape in time. Thus, given a body model, the E‐field is computed from each gradient coil basis function for unit slew rate at a series of points over the body surface, such as using the methods we have described recently. 17 Calculation at points on the interior of the model is not required, as the maximum magnitude always occurs on the surface of a uniform body model (see Supporting Information). A magnitude constraint is created on the surface by placing a series of rotated inequality constraints on the vector E‐field tangent to the surface. For example, we choose 32 directions at each point spaced over 360º, which constrains the magnitude to within 0.5% of the desired value.

---

### VA / DoD clinical practice guideline for the management of upper limb amputation rehabilitation [^115Azfi9]. DoD/VA (2022). High credibility.

VA/DoD upper limb amputation rehabilitation algorithm — overview and symbol legend: The algorithm is designed to facilitate understanding of the clinical pathway and decision making process used in managing patients with upper limb amputation (ULA) and represents a simplified flow including an ordered sequence of steps of care, decisions to be considered, recommended decision criteria, and actions to be taken. It is a step-by-step decision tree with standardized symbols; arrows connect the numbered boxes indicating the order in which the steps should be followed, and sidebars provide more detailed information. Shape meanings are defined as follows: rounded rectangles represent a clinical state or condition; hexagons represent a decision point formulated as a question that can be answered "Yes" or "No"; rectangles represent an action in the process of care; and ovals represent a link to another section within the algorithm. Appendix N contains alternative text descriptions of the algorithm.

---

### American College of Gastroenterology guidelines update: diagnosis and management of celiac disease [^112DWwEN]. The American Journal of Gastroenterology (2023). High credibility.

Celiac disease gluten challenge steps within the diagnostic algorithm include specific dosing and timing: The algorithm specifies 3 g gluten daily for 2 weeks, then 3 g gluten daily for up to 6 additional weeks, with repeat serology at end of challenge and repeat serology 2 to 6 weeks after end of challenge.

---

### Elacestrant (Orserdu) [^111yBi3v]. FDA (2024). Medium credibility.

The dosage of elacestrant PO for treatment of breast cancer in postmenopausal female adults with ESR1 mutation (advanced or metastatic, ER-positive, HER2-negative, progression after ≥ 1 line of endocrine therapy) is 345 mg PO daily until disease progression or unacceptable toxicity

---

### Imlunestrant [^1149tV4C]. FDA. Low credibility.

Labeled indications for Imlunestrant include:

- Treatment of breast cancer in female adults with ESR1 mutation (advanced or metastatic, ER-positive, HER2-negative, progression after ≥ 1 line of endocrine therapy)

---

### ESICM / ESCMID task force on practical management of invasive candidiasis in critically ill patients [^113BCSYz]. Intensive Care Medicine (2019). High credibility.

Regarding medical management for invasive candidiasis, more specifically with respect to step-down therapy, ESCMID/ESICM 2019 guidelines recommend to discontinue antifungal therapy in patients with suspected (but not proven) invasive candidiasis with negative blood cultures and/or other negative culture specimens taken from suspected infectious foci before starting antifungal therapy.

---

### 5% dextrose [^114dDV57]. FDA (2020). Medium credibility.

OVERDOSAGE

In the event of overhydration or solute overload, re-evaluate the patient and institute appropriate corrective measures. (See WARNINGS, PRECAUTIONS, and ADVERSE REACTIONS .)

---

### Standards of care in diabetes – 2025 [^113MRkAD]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for hypertension, more specifically with respect to patients with diabetes mellitus, lifestyle modifications, ADA 2025 guidelines recommend to advise lifestyle interventions in patients with BP > 120/80 mmHg, consisting of weight loss when indicated, a DASH-style eating pattern (including reducing sodium and increasing potassium intake), moderation of alcohol intake, smoking cessation, and increased physical activity.

---

### Enhancing haptic continuity in virtual reality using a continuity reinforcement skeleton [^112JsT73]. Nature Communications (2025). High credibility.

Fig. 4
2D Continuity Reinforcement Skeleton (CRS) device.

a Structure of 2D CRS device. b The 2D surfaces constructed by a grid of beams while generating a non-developable curvature. c Comparison of the shapes with and without the CRS when showing a specific target shape. d Display of the spread of ripples and displacement field of z from Digital Image Correlation (DIC) method. e Display of the movement of points and displacement field of z from DIC method. f Theoretical curves of distortion for two types of pixels displays and the experimentally measured distortion.

Figure 4d and e show the effect of displaying ripple spreading and movement of a circular point. The top half is the actual image, while the bottom half shows the target shape and the displacement field measured by the DIC method. Comparing the displacement fields shows that the surface formed by the CRS experiences continuous motion instead of sequential rise and fall of the pixels (see Supplementary Movie 4).

To quantify the reinforcement effect, it is necessary to extend the concept of haptic distortion to 2D cases. Let the 2D point contact be a rotated sinusoidal surface shown in the target shape of Fig. 4c and use it as the benchmark to illustrate display quality. The equation of the target shape can be found in Supplementary Note 2, andis the peak position of the contact point which is uniformly and randomly distributed in the display region. The peak position distortion can be obtained aswhereandare the coordinates of the actual peak position and are functions of. For square pixels; for hexagon pixels. It is also possible to define the shape distortion in 2D asin whichis the actual shape, S is the range covered by the ideal shape. Square pixels have an empirical formula. For hexagon pixels,(see Supplementary Note 2).

Based on the theoretical and experimental results, we can obtain the distortion curves, see Fig. 4f, g. The blue solid lines and the orange dashed lines represent the theoretical distortion curves for square and hexagonal pixel points, respectively, while the blue crosses and the red circles represent the experimentally measured distortion with and without the CRS (see Supplementary Note 3 for experiment data processing). It is clear that the CRS can reduce the shape and position distortions by half.

---

### Addressing early childhood emotional and behavioral problems [^111F8tFt]. Pediatrics (2016). Medium credibility.

Examples of evidence-based treatments for existing diagnoses in young children — This report focuses on programs that target current diagnoses or clear clinical problems (rather than risk) in infants and toddlers.

---

### Revealing the predictability of intrinsic structure in complex networks [^115k8YCB]. Nature Communications (2020). High credibility.

Structure prediction is an important and widely studied problem in network science and machine learning, finding its applications in various fields. Despite the significant progress in prediction algorithms, the fundamental predictability of structures remains unclear, as networks' complex underlying formation dynamics are usually unobserved or difficult to describe. As such, there has been a lack of theoretical guidance on the practical development of algorithms for their absolute performances. Here, for the first time, we find that the normalized shortest compression length of a network structure can directly assess the structure predictability. Specifically, shorter binary string length from compression leads to higher structure predictability. We also analytically derive the origin of this linear relationship in artificial random networks. In addition, our finding leads to analytical results quantifying maximum prediction accuracy, and allows the estimation of the network dataset potential values through the size of the compressed network data file.

---

### February, 2024 SATISH IYENGAR department of statistics… [^116vfdQ1]. FDA (2025). Medium credibility.

00190–3. Iyengar S. On the exceedances of exchangeable random variables. Sankhya B, 83: 26–35, doi:

10. 1007/s13571-021-00252-3. Simsek B, Iyengar S. Computing tail areas for a high-dimensional Gaussian mixture. Applicable Analysis and Discrete Mathematics, 13, 871–882. Gaunt R, Iyengar S, Olde Dalhuis A, Simsek B. An asymptotic expansion for the normalizing constant of the Conway-Maxwell-Poisson distribution. Annals of the Institute of Statistical Mathematics, 71: 163–180. Simsek B, Iyengar S. On certain mathematical problems in two-photon laser scanning microscopy. Mathematical Modeling and Analysis, 22, 587–600. Saunders D, Tsui LK, Iyengar S. Lower tail dependence of hitting times of two-. Zhao M, Iyengar S. Nonconvergence in logistic and Poisson models for neural spiking. Neural Computation, 22, 1231–1244. Paninski L, Brown EN, Iyengar S, Kass RE. Statistical models of spike trains. In. probabilities.

SIAM J Scientific Computing, 19, 1234–1244. Iyengar S, Liao Q. Modeling neural activity using the generalized inverse Gaussian distribution. Biological Cybernetics, 77, 289–295. Iyengar S. Parameter estimation for a diffusion approximation to Stein's model. Com-. Iyengar S. Evaluation of normal probabilities of symmetric regions. SIAM J Scientific and Statistical Computing, 9, 418–424. Iyengar S, Greenhouse JB. Selection models and the file-drawer problem. Statistical Science, 3, 109–135. Iyengar S. On a lower bound for the multivariate normal Mills' ratio. Annals of Prob- ability, 14, 1399–1403. Iyengar S. Hitting lines with two-dimensional Brownian motion. SIAM J Applied Mathematics, 45, 983–989.

---

### Diabetes technology-continuous subcutaneous insulin infusion therapy and continuous glucose monitoring in adults: an endocrine society clinical practice guideline [^115BvrPk]. The Journal of Clinical Endocrinology and Metabolism (2016). Medium credibility.

Regarding follow-up and surveillance for diabetes mellitus type 1, more specifically with respect to continuous glucose monitoring, ES 2016 guidelines recommend to provide education, training, and ongoing support to help achieve and maintain individualized glycemic goals in adult patients with T1DM using continuous subcutaneous insulin infusion and continuous glucose monitoring.

---

### Sodium chloride [^112q8B3X]. FDA (2025). Medium credibility.

OVERDOSAGE

In the event of overhydration or solute overload, re-evaluate the patient and institute appropriate corrective measures. (See WARNINGS, PRECAUTIONS, and ADVERSE REACTIONS .)

---

### Standards of care in diabetes – 2025 [^1121wCy2]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for diabetes mellitus type 1, more specifically with respect to pediatric patients (physical activity), ADA 2025 guidelines recommend to advise physical activity in all young patients with T1DM with the goal of 60 minutes of moderate- to vigorous-intensity aerobic activity daily, with vigorous muscle-strengthening and bone-strengthening activities at least 3 days per week.

---

### Pharmacological management of osteoporosis in postmenopausal women: an endocrine society guideline update [^114dfNJ7]. The Journal of Clinical Endocrinology and Metabolism (2020). High credibility.

Regarding medical management for postmenopausal osteoporosis, more specifically with respect to antiresorptive and anabolic therapy, PTH analogs, ES 2020 guidelines recommend to initiate teriparatide or abaloparatide for up to 2 years to reduce the risk of vertebral and non-vertebral fractures in postmenopausal patients with osteoporosis at very high risk of fractures, such as severe or multiple vertebral fractures. (1, Moderate) Switch to an antiresorptive therapy to maintain bone density gains in postmenopausal patients with osteoporosis completing a course of teriparatide or abaloparatide.