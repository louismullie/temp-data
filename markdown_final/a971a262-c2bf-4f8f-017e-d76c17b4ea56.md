# Medical Question & Answer

**Sample ID**: a971a262-c2bf-4f8f-017e-d76c17b4ea56
**Dataset Index**: 175726

---

## Question

$32 \cdot 26$ Let $n$ be a natural number between 100 and 200, then the number of $n$ that satisfies $7 n+2$ is a multiple of 5 is
(A) 10.
(B) 11.
(C) 20.
(D) 21.
(2nd "Five Sheep Cup" Junior High School Mathematics Competition, 1990)

---

## Answer

> Let's see… What do we have here? The user is asking how many natural numbers n between 100 and 200 make 7n + 2 a multiple of 5, and to choose the correct option among 10, 11, 20, or 21. Let's break this down step-by-step. First, I need to think about translating the divisibility condition into a modular equation. Then, I should solve that congruence for n modulo 5. Next, I will determine the smallest and largest values of n in the 100–200 range that satisfy the congruence. After that, I need to count how many such numbers there are using an arithmetic sequence approach. Finally, I will double-check the calculation and match the answer to the choices provided.

> Let me first translate the condition into modular arithmetic. Saying 7n + 2 is a multiple of 5 is equivalent to 7n + 2 ≡ 0 (mod 5), which simplifies to 7n ≡ −2 (mod 5). Wait, let me verify the sign: yes, −2 mod 5 is the same as 3 mod 5, so 7n ≡ 3 (mod 5) is equivalent and often easier to work with.

> Next, I should simplify the coefficient modulo 5. Since 7 ≡ 2 (mod 5), the congruence becomes 2n ≡ 3 (mod 5). Hold on, I should verify the inverse of 2 modulo 5. The multiplicative inverse of 2 mod 5 is 3 because 2 × 3 = 6 ≡ 1 (mod 5), so multiplying both sides by 3 gives n ≡ 9 (mod 5), which simplifies to n ≡ 4 (mod 5). So, n must be of the form 5k + 4 for some integer k.

> Now I need to find the smallest and largest values of n in the range 100 ≤ n ≤ 200 that satisfy n ≡ 4 (mod 5). Let me think about the lower bound first. If n = 5k + 4 ≥ 100, then 5k ≥ 96, so k ≥ 19.2, which means the smallest integer k is 20 and the smallest n is 5 × 20 + 4 = 104. I should confirm: 104 is indeed between 100 and 200 and 104 mod 5 is 4, so that checks out.

> For the upper bound, I need n = 5k + 4 ≤ 200, so 5k ≤ 196 and k ≤ 39.2, which means the largest integer k is 39 and the largest n is 5 × 39 + 4 = 199. Let me verify: 199 is within the range and 199 mod 5 is 4, so that is correct as well.

> Next, I should count how many integers of the form 5k + 4 lie between 104 and 199 inclusive. This is an arithmetic sequence with first term 104, last term 199, and common difference 5. The number of terms is given by (last − first)/difference + 1, which equals (199 − 104)/5 + 1 = 95/5 + 1 = 19 + 1 = 20. Hmm, wait a minute, I initially thought the answer might be 21, but that would be if I included 204, which is outside the range; so correcting that, the correct count is 20.

> Let me double-check the options. The choices are 10, 11, 20, or 21, and my calculation consistently gives 20 valid values of n, so the correct choice is C. I should confirm there are no off-by-one errors: starting at 104 and ending at 199 with step 5 yields exactly 20 terms, so I am confident in the result.

---

The number of natural numbers (n) between 100 and 200 such that (7n + 2) is a multiple of 5 is **20**. This is found by solving the congruence (7n + 2 \\equiv 0 \\pmod{5}), which simplifies to (n \\equiv 4 \\pmod{5}). The valid (n) values form an arithmetic sequence from 104 to 199 with a common difference of 5, and there are 20 such terms.

---

## Step 1: Translate the condition into a congruence

The condition that (7n + 2) is a multiple of 5 can be written as:

[
7n + 2 \\equiv 0 \\pmod{5}
]

Subtract 2 from both sides:

[
7n \\equiv -2 \\pmod{5}
]

Since (-2 \\equiv 3 \\pmod{5}), this becomes:

[
7n \\equiv 3 \\pmod{5}
]

---

## Step 2: Simplify the congruence

Note that (7 \\equiv 2 \\pmod{5}), so the congruence simplifies to:

[
2n \\equiv 3 \\pmod{5}
]

To solve for (n), multiply both sides by the modular inverse of 2 modulo 5. The inverse of 2 modulo 5 is 3, because (2 \\times 3 = 6 \\equiv 1 \\pmod{5}):

[
n \\equiv 3 \\times 3 \\pmod{5}
]

[
n \\equiv 9 \\pmod{5}
]

[
n \\equiv 4 \\pmod{5}
]

---

## Step 3: Identify the range of valid (n) values

We need natural numbers (n) such that (100 \\leq n \\leq 200) and (n \\equiv 4 \\pmod{5}). The smallest such (n) is 104, and the largest is 199. These values form an arithmetic sequence:

[
104, 109, 114, \\ldots, 199
]

---

## Step 4: Count the number of terms

The number of terms in this sequence is given by:

[
\\text{Number of terms} = \\frac{\\text{Last term} - \\text{First term}}{\\text{Common difference}} + 1
]

[
\\frac{199–104}{5} + 1
]

[
\\frac{95}{5} + 1
]

[
19 + 1
]

[
20
]

---

## Conclusion

There are **20** natural numbers between 100 and 200 for which (7n + 2) is a multiple of 5. The correct answer is **C. 20**.

---

## References

### Humans learn generalizable representations through efficient coding [^7017d27f]. Nature Communications (2025). High credibility.

Efficient coding automatically extracts rewarding features throughout learning simplified representations

Experiment 2 extended the standard paradigm to examine both functional-based and perceptual-based generalizations in humans. The experiment featured two primary modifications. First, we manipulated the stimuli's perceptual cues — shape, color, and appendage — to ensure each feature provided a different amount of information about the environment's rewards. We designed three experimental conditions, each with a distinct rewarding configuration (Fig. 1E):
In the consistent condition, the alien stimuli with the same color were associated with the same actions, making the color the most rewarding feature.
In the control condition, the colors of the stimuli were mutually different, and all features were equally rewarding. This condition, like Experiment 1, only tested the functional-based generalization and the state abstraction ability of an agent.
In the conflict condition, stimuli with the same color were associated with different actions, making shapes and appendages the rewarding features, while the color cue yielded a negative reward.

These three conditions also indicated three levels of difficulty in rewarding feature extraction. In the consistent and conflict condition, the four stimuli shared two colors, making color cues more frequent and salient. For example, while the "cylinder" shape was associated with rewards twice during training, the color "red" might have been rewarded four times. The consistent condition was the easiest because this salient feature yielded positive rewards, while the conflict condition was the most difficult because the agent needed to first suppress the color cue, the salient feature, before being able to detect rewarding ones.

The second primary modification in Experiment 2 was the incorporation of a probe stimulus during the testing stage; this stimulus was entirely new and had not been encountered during training. This probe was used to assess humans' ability to extract informative and rewarding features at a behavioral level. A more detailed introduction to the use of this probe design follows below, along with the presentation of our model's predictions. All other aspects of the experiment remained identical to those in Experiment 1.

---

### Type 2 diabetes therapies: a STEPS approach [^fc25c424]. American Family Physician (2019). Medium credibility.

A concise way to evaluate pharmacotherapy options for type 2 diabetes mellitus is to use the five patient-oriented STEPS criteria: safety, tolerability, efficacy, price, and simplicity. The first-line treatment option, metformin, is safe and fairly well-tolerated, has excellent long-term efficacy for patient-oriented outcomes, is moderately priced, and has a simple dosing regimen. However, most patients with type 2 diabetes require more than one medication. The STEPS approach can help choose subsequent medications if metformin does not provide adequate glycemic control.

---

### Cost-effectiveness of universal asymptomatic preoperative SARS-CoV-2 polymerase chain reaction screening: a cost-utility analysis [^1d10ead6]. Clinical Infectious Diseases (2024). Medium credibility.

Modeling Assumptions

Several modeling assumptions were made in the analysis. First, we assumed that the related expenses would be added to the surgical cost if a PPC occurred. Second, due to the lack of appropriate data, the cost of treating PPCs and their treatment duration were assumed to be the same as those of inpatient COVID-19 care, regardless of surgical procedures. Third, utilities were calculated by subtracting the disutility due to a PPC, where the disutility due to COVID-19 was extrapolated from the baseline value; the reduction in utility due to surgery or primary diseases was not considered. Fourth, the baseline value of the utilities in this model was set at 1.0. The baseline health conditions of preoperative patients are very diverse according to their age, the severity of primary diseases, and comorbidities, and the average utility value of preoperative patients was not available. Fifth, we assumed that the utility would return to its baseline level once acute COVID-19 or a PPC had healed. In other words, we did not consider the possibility of suffering from "long COVID" (defined as a multisystemic condition with often severe symptoms following SARS-CoV-2 infection). Sixth, the highly sensitive PCR test can detect residual SARS-CoV-2 for a prolonged period after disease resolution; therefore, a positive test does not equate to active infection or contagiousness. To simplify the model, however, the positive PCR test in the base case was assumed to be the first positive and not the detection of residual SARS-CoV-2 virus.

---

### Design of calibration-free RF pulses for T-weighted single-slab 3D turbo-spin-echo sequences at 7T utilizing parallel transmission [^cc8febed]. Magnetic Resonance in Medicine (2024). Medium credibility.

Purpose

T < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:msub > < mml:mo > < /mml:mo > < mml:mrow > < mml:mn > 2 < /mml:mn > < /mml:mrow > < /mml:msub > < /mml:mrow > < mml:annotation > $$ {}_2 $$ < /mml:annotation > < /mml:semantics > < /mml:math > -weighted turbo-spin-echo (TSE) sequences are a fundamental technique in brain imaging but suffer from field inhomogeneities at ultra-high fields. Several methods have been proposed to mitigate the problem, but were limited so far to nonselective three-dimensional (3D) measurements, making short acquisitions difficult to achieve when targeting very high resolution images, or needed additional calibration procedures, thus complicating their application.

Methods

Slab-selective excitation pulses were designed for flexible placement utilizing the concept of k < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:msub > < mml:mo > < /mml:mo > < mml:mrow > < mml:mi > T < /mml:mi > < /mml:mrow > < /mml:msub > < /mml:mrow > < mml:annotation > $$ {}_T $$ < /mml:annotation > < /mml:semantics > < /mml:math > -spokes. Phase-coherent refocusing universal pulses were subsequently optimized with the Gradient Ascent Pulse Engineering algorithm and tested in vivo for improved signal homogeneity.

Results

Implemented within a 3D variable flip angle TSE sequence, these pulses led to a signal-to-noise ratio (SNR) improvement ranging from 10% to 30% compared to a two-dimensional (2D) T2w TSE sequence employing < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:msubsup > < mml:mrow > < mml:mi > B < /mml:mi > < /mml:mrow > < mml:mrow > < mml:mn > 1 < /mml:mn > < /mml:mrow > < mml:mrow > < mml:mo > + < /mml:mo > < /mml:mrow > < /mml:msubsup > < /mml:mrow > < mml:annotation > $$ {\mathrm{B}}_1^{+} $$ < /mml:annotation > < /mml:semantics > < /mml:math > -shimmed pulses. < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:msubsup > < mml:mrow > < mml:mi > B < /mml:mi > < /mml:mrow > < mml:mrow > < mml:mn > 1 < /mml:mn > < /mml:mrow > < mml:mrow > < mml:mo > + < /mml:mo > < /mml:mrow > < /mml:msubsup > < /mml:mrow > < mml:annotation > $$ {\mathrm{B}}_1^{+} $$ < /mml:annotation > < /mml:semantics > < /mml:math > field inhomogeneities could be mitigated and artifacts from < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:msub > < mml:mrow > < mml:mi > B < /mml:mi > < /mml:mrow > < mml:mrow > < mml:mn > 0 < /mml:mn > < /mml:mrow > < /mml:msub > < /mml:mrow > < mml:annotation > $$ {\mathrm{B}}_0 $$ < /mml:annotation > < /mml:semantics > < /mml:math > deviations reduced. The concept of universal pulses was successfully applied.

Conclusion

We present a pulse design method which provides a set of calibration-free universal pulses (UPs) for slab-selective excitation and phase-coherent refocusing in slab-selective TSE sequences.

---

### ScDisInFact: disentangled learning for integration and prediction of multi-batch multi-condition single-cell RNA-sequencing data [^cd6dc253]. Nature Communications (2024). High credibility.

Training algorithm of scDisInFact

We update the model parameter in an alternating manner using stochastic gradient descent. For each iteration, the parameter update of scDisInFact is separated into two steps. We first fix the parameters of the unshared encoder and classifier and update the parameters of the shared encoder and decoder through stochastic gradient descent. The loss function (Eq. (6)) is then simplified into:Then we fix the parameters of the shared encoder, and update the parameters of the unshared encoder, classifier, and decoder through stochastic gradient descent. The loss function (Eq. (6)) is then simplified into:The algorithm iterates until the objective function (Eq. (6)) converges. We trained scDisInFact using Adam optimizer, and set the learning rate to be 5 × 10 −4 and the batch size to be 64. Supplementary Fig. 16 shows the training loss curves of scDisInFact on all testing datasets in the manuscript.

Condition-associated key gene (CKG) detection

The weight matrix W in the first layer of each unshared encoder is used to extract the CKGs of its corresponding condition type (Supplementary Fig. 1 a). Each column vector of W is connected to one input gene, and we used the ℓ 2 -norm of each column vector as the score of the corresponding gene. For gene i, the corresponding score s i is calculated as s i = ∥ w i ∥ 2, where w i is the i th column vector of W. A higher s i score means that gene i is more likely to be a CKG.

---

### Optimizing genetics online resources for diverse readers [^73cd2fa8]. Genetics in Medicine (2020). Medium credibility.

Natural language processing

We applied NLP methods to improve readability for a set of conditions pertaining to the American College of Medical Genetics and Genomics (ACMG TM) 59 conditions. These are typically penetrant genetic conditions with actionable information that are reported as incidental or secondary findings in clinical genomic sequencing. We applied NLP methods in a step-wise manner by first removing medical jargon and then replacing the complex condition name (steps 1 and 2 in Fig. 4). We compared readability scores before and after text optimization. When we programmatically processed the text with step 1 for the ACMG TM conditions (n = 28), the average reading grade level moderately improved from 12.8 to 12.3. By replacing repeats of complex condition names (step 2), the score lowered to 11.6 (Kruskal–Wallis, p < 0.05) (Fig. 4). We also identified a set of long, complex words found in genetic resources that could be problematic for patients (Table S1). Many of these words are scientific terms that cannot be easily replaced or shortened. Since preliminary text processing methods (steps 1 and 2) only modestly lowered the reading grade level, we performed novel curation of informational resources to generate new educational content using NLP tools. Our text processing method (step 3) consisted of creating a new template for simplified genetic information by first bulk downloading of online health educational resources including GHR and MedlinePlus, as well as information from ClinGen actionability. For each condition, we extracted key medical details such as a short one-sentence description of condition, gene associated with the condition, risk associated with the condition, clinical actionability, and known inheritance, and integrated these details into a standardized template. This resulted in a simple structured summary of the condition (Fig. 5). After this step, the mean reading level of text decreased to 9.3 grade (Kruskal–Wallis, p < 0.0001) (Fig. 4). Structured summaries could be generated in a scalable fashion for consumer health information.

---

### Using qualitative comparative analysis and theory of change to unravel the effects of a mental health intervention on service utilisation in Nepal [^3e00f462]. BMJ Global Health (2018). Medium credibility.

Data analysis

The data were analysed using pooled QCAin fsQCA V.2.5where data are pooled for all facilities and outcomes and which therefore treats each observation for each facility and each time point as an independent observation.

We conducted each of the following analysis steps for the outcomes 'high' and 'low mental health service utilisation' as QCA is asymmetrical. We first identified necessary conditions from the list of conditions outlined in table 2 using the Necessary Conditions option in fsQCA and XY plots. Necessary conditions are conditions for which the outcome is almost always a result of a causal condition. As outlined by Garson, we defined conditions as necessary when the consistency, or degree to which the outcome was a result of the condition, was greater than 0.90.

Second, we constructed a truth table with the remaining conditions to determine sufficient causal pathways. A truth table is an intermediate step in the QCA analysis that shows all possible configurations of conditions, and the number of cases that exhibit each causal configuration, together with a consistency score for the causal configuration. The abbreviated truth tables containing only the causal configurations for which data were available for the outcomes of high and low mental health service utilisation are presented in online supplementary file 3. We applied a frequency threshold of 2 and used a standard consistency threshold of 0.8and the standard analysis option in fsQCA.

fsQCA uses Boolean miniminisation to simplify the combinations of conditions and provide sufficient combinations of conditions associated with the outcome. fsQCA provides three types of solution according to the simplifying assumptions used: (1) a complex solution, which minimises the solution based on the available data only; (2) an intermediate solution, which uses theoretical knowledge to determine which logical remainders to include; and (3) a parsimonious solution, which finds the simplest solution without evaluating the plausibility. For the intermediate solution, logical remainders were coded according to the programme theory outlined in the ToC. Logical remainders are the causal configurations in the truth table for which there were no available data.

We calculated solution consistency and coverage based on the findings. Coverage refers to the degree to which the causal condition explains the outcome in the sample. There is no specific cut-off for coverage because a lower coverage may indicate a more uncommon causal pathway. We present both the intermediate and complex solutions of sufficient conditions from the truth table analysis in the table and only the intermediate solutions in the narrative.

Permission to collect data was obtained from the managers of all 10 facilities to obtain facility-level data.

---

### Cost-effectiveness analysis of current treatment options for eosinophilic esophagitis [^26e66c30]. The American Journal of Gastroenterology (2025). Medium credibility.

Introduction

The management strategies for eosinophilic esophagitis include proton pump inhibitors (PPIs), swallowed topical corticosteroids (tCSs), elimination diets, and the biologic agent dupilumab, although there remains little guidance on the selection of initial treatment. We performed cost-effectiveness analyses to compare these approaches of first-line therapy.

Methods

A Markov model was constructed from a payer perspective to evaluate the cost-effectiveness of first-line therapies for eosinophilic esophagitis, including PPI, tCS, and 6-food elimination diet (SFED), with crossover in treatments for primary and secondary nonresponse. The primary outcome was incremental cost-effectiveness ratio at 2 and 5-year time horizons. Secondary analyses included modeling from a societal perspective that also accounted for patient-specific costs, as well as a separate simplified model comparing dupilumab with tCS and PPI.

Results

In the base-case scenario (5-year time horizon), the average costs were SFED: $15,296.81, PPI: $16,153.77, and tCS: $20,975.33 as initial therapy, with SFED being the dominant strategy (more effective/less costly), while PPI offered the lowest cost on a 2-year time horizon. From a societal perspective, PPI was the dominant initial strategy on both 2 and 5-year time horizons. Among pharmacologic therapies, PPI was the most cost-effective first-line option. Dupilumab was not cost-effective relative to tCS, unless the quarterly cost is reduced from $7,311 to $2,038.50 per price threshold analysis under permissive modeling conditions.

Discussion

SFED was the most effective/least costly first-line therapy from the payer perspective while PPI was more cost-effective from the societal perspective. PPI is also the most cost-effective pharmacologic strategy. Dupilumab requires substantial cost reductions to be considered cost-effective first-line pharmacotherapy.

---

### Overcoming the design, build, test bottleneck for synthesis of nonrepetitive protein-RNA cassettes [^ee2c8053]. Nature Communications (2021). High credibility.

In the analysis carried out in this paper, we chose to reduce the parameter space to a 3-dimensional space consisting of the following components: the slope (m) and goodness of fit (R 2) to a simple linear fit of the rescaled fluorescenceto inducer concentration values. The third component is the standard deviation (std) ofcomputed at the three highest concentration induction bins. We term this vector:

Based on the 3-dimensional space (R 2, m, and std) we conducted a multivariant Gaussian fit for the positive and negative control populations (Fig. 2), which in turn allowed us to compute the 3-dimensional pdf(pos, n) and pdf(neg, n). Finally, we computed the R score for each non-control variant by averaging the score over as many barcodes which passed our filters (each variant appeared in our library 5 times). The results of this computation are presented in the heatmaps of Fig. 2 and Fig. S3, which are arranged in accordance with decreasing R score. Up to this point, we have developed the R score to sort the different variants, but did not dive into what it means physically or from a binding perspective. The approach relied on mapping the behavior of the positive binding controls and non-binding negative controls in some 3-dimensional parameter space, and computing the likelihood that a given variant would belong to one or the other group. The R score is the log of the ratio of the two computations. In principle, R score can be computed from any number of probability density functions. We could have used the original 6D space consisting of the 6 inducer concentrations, or chosen any other combination. In the computation below, we will map the 6D space to a 1D space of binding affinities that can be in principle computed from each 6-vector using a Hill function fit. In the case of such a mapping, we can replace eqn. 7 and 8 in the paper with the following terms:

---

### Simplifying switch to second-line antiretroviral therapy in sub saharan Africa: predicted effect of using a single viral load to define efavirenz-based first-line failure [^8d4f9ab7]. AIDS (2019). Medium credibility.

Results

Table 1 shows the range of characteristics of the setting scenarios in 2018, just before the consideration of the change in strategy for defining first-line EFV-based ART failure.

Over a 3-year follow-up period (Table 2), the strategy of using a single viral load more than 1000 to define failure of EFV-based first-line ART (simplified strategy), instead of two consecutive values (current strategy), is predicted to result in a 18% (90% range 6–30%) reduction of the AIDS death rate among people with previous or current viral load more than 1000 while on ART (Fig. 1). The simplified strategy is also predicted to reduce the rate of AIDS conditions by a median of 31% (90% range 8–49%) among people on first-line ART with a viral load more than 1000 copies/ml in the past 6 months. For a country of 10 million adults with 880 000 people with HIV, this is estimated to lead to a median of 1322 (90% range 67–3513) AIDS deaths averted per year over 3 years. For South Africa this would represent 10 215 deaths averted annually (refer to Supplementary Table 1). For a country of 10 million adults under the current strategy, we estimate 301 (90% range 33–1338) switches per year among those without resistance being present under the base case scenario, compared with 7285 (90% range 3538–14 653) such switches using the single viral load strategy (refer to Supplementary Table 2).

---

### First and second stage labor management: ACOG clinical practice guideline no. 8 [^24837231]. Obstetrics and Gynecology (2024). High credibility.

Strength and quality labels — For STRONG recommendations, ACOG recommends and benefits clearly outweigh harms and burdens and most patients should receive the intervention, whereas ACOG recommends against when harms and burdens clearly outweigh the benefits and most patients should not receive the intervention; for CONDITIONAL recommendations, ACOG suggests and the balance of benefits and risks will vary depending on patient characteristics and their values and preferences with individualized, shared decision making recommended. Quality of evidence categories include HIGH, MODERATE, LOW, and VERY LOW, with HIGH supported by randomized controlled trials, systematic reviews, and meta-analyses without serious methodologic flaws or limitations and by very strong evidence from observational studies and with high confidence in the accuracy of the findings, while VERY LOW reflects unsystematic clinical observations or very indirect evidence from observational studies; intermediate categories have corresponding definitions. Good Practice Points are ungraded and are incorporated when clinical guidance is deemed necessary in the case of extremely limited or nonexistent evidence and are based on expert opinion as well as review of the available evidence.

---

### Elacestrant (Orserdu) [^25e5bc7d]. FDA (2024). Medium credibility.

The dosage of elacestrant PO for treatment of breast cancer in postmenopausal female adults with ESR1 mutation (advanced or metastatic, ER-positive, HER2-negative, progression after ≥ 1 line of endocrine therapy) is 345 mg PO daily until disease progression or unacceptable toxicity

---

### Clustering knowledge and dispersing abilities enhances collective problem solving in a network [^9f543c96]. Nature Communications (2019). High credibility.

The severity of this problem's complexity is not additive or linear in nature, but interdependent on all of the sub-problems that need to be solved for this to work. Furthermore, adopting a solution might have unintended, if not non-linear, consequences to the system's overall outcome that were not obvious or foreseeable. For instance, allowing students to use their mobile phones in schools might seem like a good idea to improve class engagement through other media, but an unintended consequence might be distraction or discouraging deeper critical thinking. The objective of the NK space is to capture the complex interaction among activities that yield performance. The NK problem space's popularity in modeling human decision-making stems from its verisimilitude with the complex and multidimensional problems that face problem-solving tasks, and because researchers can easily generate a large number of statistically similar problem spaces for robustness checks. (We note, however, in order to make sure our results are not an artifact of the idiosyncrasies of the NK problem space, we replicated all of our results in another rugged problem space, the Traveling Salesperson Problem — findings available in Supplemental Methods in Supplementary Fig. 2 through 9 and 11).

---

### Spatiotemporal analysis of event-related fMRI to reveal cognitive States [^81cc24bb]. Human Brain Mapping (2020). Medium credibility.

3.4.2 State recovery with multiple conditions

Except for the simulations of the data from the real experiment, both data and models have just had a single condition. We simulated two experiments where the sojourn times of some states varied as a function of 4 conditions. Past experiments (e.g. Anderson & Fincham, 2014; Anderson et al. 2015; Anderson, Pyke, & Fincham, 2016; Anderson, Zhang, et al. 2016), where we have used HSMM‐MVPA, have had multiple conditions and we were interested in what states these conditions affect. Again, 100 data sets were generated with a SNR of.1 and the results for these two models are shown in Figure 15:
Top panels: There were 4 states and the mean sojourn times of states 1, 3, and 4 did not vary with conditions and were 3, 4, and 5 s, respectively. State 2 lasted an average of.5, 2.5, 4.5, and 6.5 in the four conditions.
Bottom panels: There were 5 states and the mean sojourn times of states 2, 3, and 4 did not vary with conditions and were 3, 4, and 5 s, respectively. States 1 and 5 either were 1 or 4 s. Crossing these possibilities yielded the four conditions.

---

### Standards of care in diabetes – 2025 [^31be65a6]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for hypoglycemia, more specifically with respect to screening for cognitive impairment, ADA 2025 guidelines recommend to simplify diabetes treatment plans as much as possible for patients with cognitive impairment and tailor to minimize the risk of hypoglycemia.

---

### The problem with pay-for-performance schemes [^756a6421]. BMJ Quality & Safety (2019). High credibility.

'The Problem with… ' series covers controversial topics related to efforts to improve healthcare quality, including widely recommended but deceptively difficult strategies for improvement and pervasive problems that seem to resist solution.

---

### A last-in first-out stack data structure implemented in DNA [^a299e2f0]. Nature Communications (2021). High credibility.

Model predictions of DNA stack with two signal types

Next, we extrapolated the parameterised model to exhaustively predict the (ideal) operation of the DNA stack with two signal types X and Y, this time over longer sequences of recording and popping operations.

The model was subject to five operation sequences each containing 20 record operations and 20 pop operations in total (Supplementary Note 10.1). In every case, approximately equal numbers of X and Y signals were recorded, and the stack always finished empty (under ideal operation). Four sequences, denoted seqN, were periodic, with N records always followed by N pops for N = 1, 5, 10, 20. One sequence, denoted seqR, arranged the 20 record and 20 pop operations randomly. Additionally, we tested the model with three increasingly stringent washing procedures. Procedure W1 was our experimental protocol (μ = 0.1, ϕ 0 = 0.33), procedure W2 was approximately twice as efficient (μ = 0.05, ϕ 0 = 0.15) and W3 was approximately twice as efficient again (μ = 0.02, ϕ 0 = 0.05).

For all operations sequences and all washing efficiencies, the model suggested that instead of detecting the most common stack species in the system, the most robust way to read out signals stored was to detect the majority signal popped into supernatant (Xr or Yr) following the addition of read strands (Supplementary Note 10.1). The model suggested that under imperfect washing, the stack population actually becomes de-synchronised from the target stack structure quickly (faster for W1 than for W3, as expected). However, despite this de-synchronisation of the stack population, the majority popped signal in supernatant remains correct for some time because the signals popped into supernatant derive from only the last signals stored on stacks in the population. That is, the stack population is simply required to have the majority of stacks terminating with the correct end signal, it is not required for all stacks in the population to be identical and synchronised (a much stricter condition). The fact that majority popped signal is the most robust system read out is indeed convenient, since the popped Xr and Yr complexes would be how the DNA stack is eventually linked to downstream nucleic acid circuits.

---

### RF coil design strategies for improving SNR at the ultrahigh magnetic field of 10.5T [^6f9778ff]. Magnetic Resonance in Medicine (2025). Medium credibility.

Purpose

Toward pushing the boundaries of ultrahigh fields for human brain imaging, we wish to evaluate experimentally achievable SNR relative to ultimate intrinsic SNR (uiSNR) at 10.5T, develop design strategies toward approaching the latter, quantify magnetic field-dependent SNR gains, and demonstrate the feasibility of whole-brain, high-resolution human brain imaging at this uniquely high field strength.

Methods

A dual row 16-channel self-decoupled transmit (Tx) and receive (Rx) array was developed for 10.5T using custom Tx/Rx switches. A 64-channel receive-only array was built to fit into the 16-channel Tx/Rx array. Electromagnetic modeling and experiments were used to define safe operational power limits. Experimental SNR was evaluated relative to uiSNR at 10.5T and 7T.

Results

The 64-channel Rx array alone captured approximately 50% of the central uiSNR at 10.5T, while an identical array developed for 7T captured about 76% of uiSNR at 7T. The 16-channel Tx/80-channel Rx configuration brought the fraction of uiSNR captured at 10.5T to levels comparable to the 64-channel Rx array at 7T. SNR data displayed an approximate < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:msubsup > < mml:mi > B < /mml:mi > < mml:mn > 0 < /mml:mn > < mml:mn > 2 < /mml:mn > < /mml:msubsup > < /mml:mrow > < mml:annotation > $$ {\mathrm{B}}_0^2 $$ < /mml:annotation > < /mml:semantics > < /mml:math > dependence over a large central region when evaluated in the context of uiSNR. Whole-brain, high-resolution < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:msubsup > < mml:mi > T < /mml:mi > < mml:mn > 2 < /mml:mn > < mml:mo > * < /mml:mo > < /mml:msubsup > < /mml:mrow > < mml:annotation > $$ {\mathrm{T}}_2^{\ast } $$ < /mml:annotation > < /mml:semantics > < /mml:math > -weighted and T 1 -weighted anatomical and gradient-recalled-echo BOLD-EPI functional MRI images were obtained at 10.5T for the first time with such an advanced array.

Conclusion

We demonstrated the ability to approach the uiSNR at 10.5T over the human brain, achieving large SNR gains over 7T, currently the most commonly used ultrahigh-field platform. Whole-brain, high-resolution anatomical and EPI-based functional MRI data were obtained at 10.5T, illustrating the promise of greater than 10T fields in studying the human brain.

---

### Equivalent gambling warning labels are perceived differently [^b5723dd8]. Addiction (2020). Medium credibility.

Figure 2
Mean perceived chance of winning in experiment 1. Perceived chances of winning: 7 = very high chance of coming out ahead, 4 = neither high nor low chance of coming out ahead, 1 = very low chance of coming out ahead. Error bars represent 95% confidence intervals

An additional model was run to observe if these effects remained if gamblers' characteristics were taken into account. The model included fixed effects of PGSI and gambling frequency. We tested for the presence of significant two‐way interactions between magnitude, condition, PGSI and gambling frequency. An analysis of variance (ANOVA) table is displayed in Table 1. As can be seen, the only new statistically significant interaction term was between PGSI and condition (F = 5.34, P = 0.021). Closer inspection of the marginal effects revealed a trend such that those with higher PGSI scores gave higher responses in the house‐edge condition (marginal trend = 0.21, 95% CIs = −0.06; 0.48), but lower responses in the return‐to‐player condition (marginal trend = −0.21, 95% CIs = −0.46; 0.04.

Table 1
Mixed‐model analysis of variance (ANOVA) table.

Overall, 47.4% of participants responded correctly to the multiple‐choice question of return‐to‐player understanding. As can be seen in Table 2, the most‐commonly given incorrect answers were: '90% of people who play this game will win something' (23.8%) and: 'This game will give out a prize 9 times in 10' (23.8%).

Table 2
Responses to the measure of warning label understanding.

Discussion

Participants rated their perceived chances of winning as higher in the return‐to‐player condition than the house‐edge condition. Perceived chances of winning are subjective, however, and hence there is no 'correct' response to experiment 1. Experiment 2 was designed to address this limitation, by assessing whether participants would answer the four‐alternative multiple‐choice question correctly more often with a house‐edge than return‐to‐player label.

---

### How deep learning solved my seizure detection problems [^45e96990]. Epilepsy Currents (2020). Medium credibility.

[Box: see text].

---

### Multi-response mendelian randomization: identification of shared and distinct exposures for multimorbidity and multiple related disease outcomes [^a9f263c6]. American Journal of Human Genetics (2023). Medium credibility.

We simulate the following scenarios.
- **Scenario I — null**: there are no causal exposures for any of the outcomes and no confounder.
- **Scenario II — confounding**: there are 30% of exposures with a non-zero direct causal effect and there is a joint confounder for all outcomes.
- **Scenario III — undirected pleiotropy**: residual correlation between outcomes is induced by a shared undirected pleiotropic pathway that can increase or decrease the level of the responses.
- **Scenario IV — directed pleiotropy**: residual correlation between outcomes is induced by a shared directed pleiotropic pathway that only increases the level of the responses.
- **Scenario V — dependence**: outcomes are simulated with correlated errors, mimicking the effect of non-genetic factors that contribute to their correlation.

In the following, we consider for simplicity only the case of 100% overlapping samples in the genetic associations with the outcomes. Full details regarding the simulation study setup are presented in appendix A. An overview of the simulations setting and the open parameters (andin Equation 5, the effect of the unmeasured confounderon the exposures, and, the correlation between individual-level responses' errors in Equation 5) that vary across the simulation scenarios are shown in Table S2. Fixed parameters, including the number of subjects in the individual-level data, the number of IVs, the range of the simulated direct causal effects, the heritability of the exposures, and the proportion of variance explained when simulating the responses, are also detailed in the same table. In supplemental information, we also provide details regardinghyper-parameters setting, including the number of MCMC iterations after burn-in, as well as technical details of the alternative methods and their software implementations we used in the simulation study.

---

### Interacting with volatile environments stabilizes hidden-state inference and its brain signatures [^ddd36df5]. Nature Communications (2021). High credibility.

The main task (divided in 8 blocks of 72 trials, 4 of each condition) was preceded by a short practice period (2 blocks of 54 trials, 1 of each condition) in which the concentration κ of stimulus distributions was increased to 1.5 (instead of 0.5 in the main task) such that participants could understand the structure of the task (in particular, the presence of reversals in the two conditions) with lower uncertainty regarding the generative category c of each stimulus sequence. Performance in the practice blocks was also used to assess participants' understanding of the two conditions. All tested participants were able to perform both conditions at near-ceiling performance after 54 trials of practice.

Each condition (n = 4 blocks) consisted of 2 types of blocks as follows: "more stable" blocks (n = 2) that contained 6 hidden-state episodes (i.e. a hazard rate h of 1/12) and "more volatile" blocks that contained 12 hidden-state episodes (i.e. a hazard rate h of 1/6). The shorter practice blocks (54 trials instead of 72) contained 6 hidden-state episodes (i.e. a hazard rate h of 1/9, in between the values used in the more stable and more volatile blocks). The eight blocks of the main task were organized in pairs of blocks of the "observation" and "action" conditions, one of each volatility, whose order was counterbalanced both within and between participants. The counter-balancing of the different aspects of the task across participants required to test a multiple of n = 8 participants. Given the absence of prior effect sizes for the difference between "observation" (Cb) and "action" (Ob) conditions, we chose a sample size (n = 24), which exceeded the average sample size used in human MEG studies at the time of data collection. All results presented in the main text collapse across the two types of blocks within each condition, unless noted otherwise.

---

### Global strategy for asthma management and prevention [^2d5a224e]. GINA (2024). High credibility.

Asthma medications — controller denotes medication targeting both domains of asthma control (symptom control and future risk), and in the past 'controller' was largely used for ICS-containing medications prescribed for regular daily treatment so 'controller' and 'maintenance' became almost synonymous; to avoid confusion, 'ICS-containing treatment' and 'maintenance treatment' have been substituted as appropriate where the intended meaning was unclear.

---

### Coagulation factor viia recombinant human (sevenfact) [^33889fad]. FDA (2025). Medium credibility.

2.2 Reconstitution

Follow the procedures below for reconstitution of SEVENFACT.
Calculate the amount of SEVENFACT required and select the appropriate SEVENFACT packages containing the matching pre-filled syringe of sterile Water for Injection, and the vial adapters.
Reconstitute each vial with the pre-filled syringe provided with each vial of SEVENFACT.

Overview of SEVENFACT P ackage:

Figure 1 Vial with SEVENFACT lyophilized powder

Lyophilized Powder Drug Vial

Figure 2 Syringe plunger rod and pre-filled syringe with Water for Injection diluent

Syringe Plunger Rod Pre-filled syringe with Diluent

Figure 3 SEVENFACT 1 mg and 2 mg vial adapter and SEVENFACT 5 mg vial adapter

Vial Adapters* and Packaging

1 mg and 2 mg vial adapter 5 mg vial adapter

* **N ote**: Each SEVENFACT kit will contain only one vial adapter.

The instructions below serve as a general guideline for reconstitution of SEVENFACT.

Reconstitution:

1. Based on the prescribed dose, take out the number of SEVENFACT kits (each kit containing one vial of SEVENFACTpowder and one pre-filled Water for Injection diluent syringe with one vial adapter for needleless reconstitution), an infusion set (not supplied in the kit) and an alcohol swab (not supplied in the kit). Check the expiration date on the side of the box(es) for the SEVENFACT kit(s).

---

### Clustering knowledge and dispersing abilities enhances collective problem solving in a network [^a4ac9110]. Nature Communications (2019). High credibility.

NK spaces are analytically useful for agent-based models but are conceptually abstract and difficult to ground to real-world examples. Here, we can instead understand an NK problem's complexity in terms of how incremental contributions made by any subset of activities are also contingent on other activities. Thus, a solution that an agent comes up within the NK space — captured by a unique sequence of 1s and 0s — reflects a basket of activities people undertake to solve a real-world problem, where a 1 represents the presence of an activity and a 0 its absence.

There are many examples of social collectives solving a common problem that are analogous to the NK space setup. In these instances, the solutions and performances are visible at least some subset of other actors, and there are no positive or negative externalities from one actor to another (beyond the informational spillover). For instance, consider national ministries of health of various countries. They all have different approaches to healthcare (e.g. insurance markets like in Germany or Austria or a single-public option like in Canada or the U.K.). And the improvement or decline of one country's healthcare does not adversely affect or hamper the healthcare of another. The interest of some countries in the policies employed by other countries may be limited by physical adjacency or similar political systems or cultures, so their information about the "problem space" is more localized.

---

### An event-based architecture for solving constraint satisfaction problems [^f2e7014a]. Nature Communications (2015). Medium credibility.

Constraint satisfaction problems (CSPs) are a fundamental class of problems in computer science with wide applicability in areas such as channel coding, circuit optimizationand scheduling. Algorithms for solving CSPs are typically run on classical von Neumann computing platforms that were not explicitly designed for these types of problems. This paper addresses the question: how can we implement a more efficient computing substrate whose architecture and dynamics better reflect the distributed nature of CSPs?

Many dynamical systems that have been proposed for solving CSPs violate the 'physical implementability' condition. Non-physicality arises from the use of variables that can grow without bounds as the system is searching for solutions. On the other hand, there is a long, well-established tradition of studying physically realizable dynamical systems, for example, in the form of artificial neural networks, to solve CSPs or 'best-match problems'. Early attempts in this field used attractor networks, such as Hopfield networks, to solve NP hard (non-deterministic polynomial-time hard) problems such as the travelling salesman problem. These attractor networks, however, would often get stuck at locally optimal solutions. To overcome this problem, stochastic mechanisms were proposed, which require explicit sources of noise to force the network to continuously explore the solution space. While noise is an inextricable part of any physical system, dynamically controlling its power to balance 'exploratory' versus 'greedy' search, or to move the network from an exploratory phase to a greedy one according to an annealing schedule, is not a trivial operation and puts an additional overhead on the physical implementation.

---

### Efficacy and safety of a structured de-escalation from antipseudomonal β-lactams in bloodstream infections due to enterobacterales (SIMPLIFY): an open-label, multicentre, randomised trial [^294f951d]. The Lancet: Infectious Diseases (2024). High credibility.

The study SIMPLIFY (original research) was published by Luis Eduardo López-Cortés and colleagues in 2024 in the journal Lancet Infect Dis. This study is related to the following diseases: Catheter-related bloodstream infection, Antimicrobial-resistant Gram-negative infections and Sepsis and septic shock. In the SIMPLIFY (original research) study, the trial question was: is structured de-escalation from an antipseudomonal β-lactam to a narrower-spectrum drug noninferior to continued antipseudomonal drugs in patients with Enterobacterales bacteremia? In the SIMPLIFY (original research) study, the study design was: multi-center, open label, RCT. In the SIMPLIFY (original research) study, the population was: 331 patients. The inclusion criteria were patients with bacteremia caused by Enterobacterales susceptible to one of the de-escalation options and treated empirically with an antipseudomonal β-lactam. The key exclusion criteria were life expectancy < 30 days; pregnancy or lactation; severe neutropenia; treatment of infection > 28 days. In the SIMPLIFY (original research) study, the interventions were: n = 164 de-escalation (de-escalate to a narrower-spectrum drug) n = 167 continued antipseudomonal drugs (to continue with the empiric antipseudomonal β-lactams). In the SIMPLIFY (original research) study, the primary outcome was: difference not exceeding non-inferiority margin in the rate of clinical cure 3–5 days after end of treatment (90.2% vs. 88.6%; ARD 1.6, 95% CI -5 to 8.2). In the SIMPLIFY (original research) study, the secondary outcomes were: no significant difference in clinical cure at day 60 (92.8% vs. 90%; ARD 2.8, 95% CI -3.4 to 9) No significant difference in the rate of relapses until day 60 (5.9% vs. 11.3%; ARD 5.54, 95% CI -0.8 to 11.6) No significant difference in death at day 60 (4.6% vs. 5.6%; ARD 1, 95% CI -3.9 to 5.9). In the SIMPLIFY (original research) study, the safety outcomes were: no significant difference in adverse events. In the SIMPLIFY (original research) study, the conclusion was: in patients with bacteremia caused by Enterobacterales susceptible to one of the de-escalation options and treated empirically with an antipseudomonal β-lactam, de-escalation was noninferior to continued antipseudomonal drugs with respect to the rate of clinical cure 3–5 days after end of treatment.

---

### Flexible multitask computation in recurrent networks utilizes shared dynamical motifs [^dbf04629]. Nature Neuroscience (2024). High credibility.

Analysis of fixed points for interpolated inputs

After input interpolation (see 'Input interpolation' for details), we wanted to compare fixed points across input conditions to track their positions and stability in high-dimensional space. However, there were often multiple fixed points, making it difficult to track an individual fixed point across input conditions. We focused on the fixed point closest to the state at the end of a task period of interest (except in Fig. 4e, f, where we focused on the closest unstable fixed point because it appeared more relevant to the nonlinear dynamics — the closest fixed point was stable and was also shared across tasks). Our reasoning was that if the state evolved toward a particular fixed point, it was likely relevant for computation. After identifying fixed points during rule input interpolation, we ran the network forward from the beginning of the context period for each interpolated rule input and identified the fixed point closest to the network state at the end of the task period of interest (stimulus or response period). We refer to this closest fixed point as the 'relevant' fixed point for a given interpolated input. We calculated the Euclidean distance between relevant fixed points associated with adjacent interpolated input conditions (α 1, α 2) as

We also tracked the stability of the relevant fixed point for each interpolated input. To do this, we performed eigenvalue decomposition on the Jacobian of the RNN state transition function at the relevant fixed point

The eigenvalue with the maximum real value is informative about whether the relevant fixed point is stable. By tracking the stability over input interpolation, we could identify bifurcations in the dynamical landscape.

To examine the relevant dynamical motif for a given task period, we defined a 'relevant fixed point' to be the fixed point closest to the state at the end of the task period. If the input interpolation between α = 0 and α = 1 resulted in approximately the same location of the relevant fixed point and approximately the same local dynamics around the relevant fixed point, then we defined the relevant fixed point as being functionally the same across inputs, and, therefore, the dynamical motif was shared across input conditions.

Alternatively, if the interpolation between α = 0 and α = 1 resulted in a bifurcation of the fixed point structure, then we defined the dynamical motifs to be distinct. We highlight that our definition of distinct motifs was limited in that a different path for consecutive input interpolation might not result in a bifurcation. It will be of great interest to explore ambiguous cases of shared and distinct motifs in future work.

---

### Evaluation after a first seizure in adults [^dcaa0cee]. American Family Physician (2022). Medium credibility.

The disease first seizure in adults.

---

### Rigorous location of phase transitions in hard optimization problems [^d98ebea5]. Nature (2005). Excellent credibility.

It is widely believed that for many optimization problems, no algorithm is substantially more efficient than exhaustive search. This means that finding optimal solutions for many practical problems is completely beyond any current or projected computational capacity. To understand the origin of this extreme 'hardness', computer scientists, mathematicians and physicists have been investigating for two decades a connection between computational complexity and phase transitions in random instances of constraint satisfaction problems. Here we present a mathematically rigorous method for locating such phase transitions. Our method works by analysing the distribution of distances between pairs of solutions as constraints are added. By identifying critical behaviour in the evolution of this distribution, we can pinpoint the threshold location for a number of problems, including the two most-studied ones: random k-SAT and random graph colouring. Our results prove that the heuristic predictions of statistical physics in this context are essentially correct. Moreover, we establish that random instances of constraint satisfaction problems have solutions well beyond the reach of any analysed algorithm.

---

### Large differences in regional precipitation change between a first and second 2 K of global warming [^130088b8]. Nature Communications (2016). Medium credibility.

Results

2 K intervals and the roles of linear and nonlinear mechanisms

We examine change relative to pre-industrial conditions, rather than relative to a recent historical period. This is done because it simplifies the separation of different physical mechanisms, by reducing pattern changes due to a changing balance of aerosol forcing (which has seen more attention elsewhere). In all CMIP5 scenarios, the change in aerosol forcing relative to pre-industrial levels is small by the end of the century, but this is not true for changes relative to recent historical periods. The historical climate change, from pre-industrial to the recent past, may be considered to be a separate problem that may be tackled using different information, such as real-world observations.

We first study two routes to a first 2 K of global mean warming relative to pre-industrial conditions. This is reached, in the CMIP5 ensemble mean, by mid-century under the reference scenario RCP8.5. This, the fastest route to a first 2 K in CMIP5 projections, is denoted 2 K (Fast).

An alternative, mitigation route to a first 2 K is obtained by averaging the RCP2.6 and RCP4.5 scenarios: the ensemble mean warming is slightly < 2 K above pre-industrial levels under RCP2.6, and slightly > 2 K under RCP4.5. Taking this mean reduces noise from internal variability and produces a scenario that reaches 2 K in the ensemble mean by the end of the century. This route is denoted 2 K (Mit). Potential issues with averaging RCP2.6 and RCP4.5 are discussed in the Supplementary Note 1.

These two routes to a first 2 K have different warming and forcing histories (compare yellow and blue lines in Fig. 1). As a result, they may show different precipitation patterns due to linear mechanisms (that is, from different timescales of response). Further pattern differences may arise because the mid-century aerosol burden in 2 K (Fast) is higher than that at the end of the century — in 2 K (Mit).

---

### Climate change modulates the stratospheric volcanic sulfate aerosol lifecycle and radiative forcing from tropical eruptions [^25df485e]. Nature Communications (2021). High credibility.

In an ideal setup, the eruptive column model would be run at every time step of the UKESM1 model during the eruption. However, such set-up would require full integration of the eruptive column model into the climate model which is beyond the scope of our study. Using a simplified approach, our study provides the first line of evidence that such integration could be valuable to accurately predict volcanic forcing in different climates. Our simplified approach mostly has two limitations. First, it neglects the variability of atmospheric conditions at sub-daily timescales during the eruption. Hourly atmospheric profiles were not outputted in our UKESM simulations, but using hourly profiles from the ERA5 reanalysisat the location of Mount Pinatubo and running the eruptive column model employed for all July 1st profiles in the last 20 years, we find that over 85% of the variability in simulated plume height is associated with interannual variability (as opposed to hourly variability). We conclude that our design enables us to sample well the atmospheric conditions of each climate state. Second, our approach neglects the potential impacts of the early plume development on atmospheric conditions, which could in turn modulate the height reached by the eruptive column. The few studies that have quantified the local, instantaneous response of atmospheric conditions to volcanic eruptions suggest a warming temperature response near the plume top region for plumes composed mostly of SO 2, but a cooling temperature response for those composed mostly of ash. Accounting for such effects would thus require ash to be co-emitted with SO 2 which is not currently possible with UKESM. However, atmospheric conditions would likely be affected below the spreading plume, downwind of the vent, and it thus remains unclear whether such effects would be of critical importance for modeling the column rise accurately.

---

### Metal-rich stars are less suitable for the evolution of life on their planets [^56e7ab84]. Nature Communications (2023). High credibility.

The Sun-Earth evolution

A quantitative assessment of the biological impact of these factors is challenging because additional protection (e.g. water bodies, shadowing by rocks, pigment formation) and biological repair mechanisms are not known. Therefore we considered the Earth-Sun system as a paradigm to guide the interpretation of our results for other systems. We first investigated how life on land of our planet steered through the conditions mentioned above and then how these conditions are affected by the effective temperature and metallicity of the host star.

Figure 2 illustrates the development of the Earth's atmosphere and surface UV fluxes over the last 600 Myr (million years before present). The geological isotope records indicate that the level of atmospheric O 2 (and CO 2, see Supplementary Table 1) went through substantial fluctuations – (Fig. 2a). The largest change in O 2, known as the Paleozoic oxygenation event, happened around 470 Myrwhen Earth's atmosphere went from almost anoxic to oxygenated conditions. While this event is absent in the reconstruction of Berner et al.(B6, blue curve in Fig. 2a) data (probably because of simplified assumptions about sulfur geochemistry), it is evident in the more recent reconstruction of Lenton et al. 2016(L16, light blue curve in Fig. 2a).

---

### The problem with using patient complaints for improvement [^f5d11f12]. BMJ Quality & Safety (2018). Medium credibility.

'The Problem with… ' series covers controversial topics related to efforts to improve healthcare quality, including widely recommended, but deceptively difficult strategies for improvement and pervasive problems that seem to resist solution.

---

### Cutting edge or blunt instrument: how to decide if a stepped wedge design is right for you [^471f52f2]. BMJ Quality & Safety (2021). High credibility.

When might I consider doing a stepped wedge trial?

Research designs are shaped as much by practical constraints as by abstract schemes, and it is always a good idea to start with the constraints and work towards a design, rather than start with a design and try to fit it to constraints. These constraints will be unique to each research context, and box 1 lists some areas to think about. Still, there are some common features of settings where a stepped wedge trial might be considered as a possible design, and we now review these.

Box 1
Practical constraints on the design of a longitudinal cluster randomised trial

Are there limits on the time available to complete the evaluation, on the number of clusters, or on the number of participants (or the rate at which you can recruit participants) at each cluster? These constraints put limits on the overall scale of the evaluation, or force trade-offs between different design characteristics.
How will participants and their data be sampled in your study: as a series of cross-sectional surveys, as a continuous stream of incident cases, as a cohort followed over time, or some other way? Does the timescale divide into cycles, seasons or milestones that influence how you will sample participants and data?
Is there a limit on how many clusters can implement the intervention at the same time in the evaluation? If this is constrained by research resources (eg, if there are only enough trained research staff to implement the intervention one cluster at a time) then implementation must be staggered in some way.
If implementation is to be staggered, is there a minimum 'step length'? If the same team delivers the intervention in different clusters at different steps, then bear in mind it may take some time to get the intervention fully operational at a site, and the team will also need time to relocate from one cluster to the next.

---

### Exenatide [^a44dacd5]. FDA (2025). Medium credibility.

Section 2 Read and follow the directions in this section only after you've read Section 1 — What You Need To Know About Your Exenatide Injection Pen.

GETTING STARTED

Set up your new pen just before you use it the first time. For routine use, do not repeat this one-time-only new pen setup. If you do, you will run out of Exenatide Injection before 30 days of use.

ONE-TIME-ONLY NEW PEN SETUP

STEP 1 Check the Pen

STEP 2 Attach the Needle

STEP 3 Dial the Dose

Note: If you cannot turn the dose knob away from you to the, see Commonly Asked Questions, number 7, in Section 4 of these Instructions for Use.

STEP 4 Prepare the Pen

Note: If you do not see liquid after 4 times, see Commonly Asked Questions, number 3, in Section 4 of these Instructions for Use.

STEP 5 Complete New Pen Setup

Section 3 Now that you have done the one-time-only new pen setup, follow Section 3 for all of your injections.

ROUTINE USE

STEP 1 Check the Pen

STEP 2 Attach the Needle

STEP 3 Dial the Dose

Note: If you cannot turn the dose knob away from you to the, see Commonly Asked Questions, number 7, in Section 4 of these Instructions for Use.

---

### Effectiveness of oral step-down therapy and early oral switch for bloodstream infections caused by enterobacterales: a post hoc emulation trial of the SIMPLIFY trial [^94e801c2]. International Journal of Infectious Diseases (2025). Medium credibility.

The study SIMPLIFY (post-hoc analysis) was published by Emanuele Rando and colleagues in 2025 in the journal Int J Infect Dis. This study is related to the following diseases: Catheter-related bloodstream infection, Antimicrobial-resistant Gram-negative infections and Sepsis and septic shock. In the SIMPLIFY (post-hoc analysis) study, the trial question was: what is the effect of early oral switch in patients with bloodstream infections caused by Enterobacterales? In the SIMPLIFY (post-hoc analysis) study, the study design was: multi-center, open label, RCT. In the SIMPLIFY (post-hoc analysis) study, the population was: 303 patients (124 female, 179 male). The inclusion criteria were patients with bloodstream infections caused by Enterobacterales who achieved clinical stability by day 5. The key exclusion criteria were life expectancy < 30 days; pregnancy or lactation; isolation of carbapenemase-producing Enterobacterales; severe neutropenia; planned duration of treatment > 28 days. In the SIMPLIFY (post-hoc analysis) study, the interventions were: n = 110 early oral switch (switched to oral therapy on day 5) n = 193 continued IV therapy (either switched later or remained on intravenous therapy for full treatment duration). In the SIMPLIFY (post-hoc analysis) study, the primary outcome was: no significant difference in clinical cure at test of cure (95.4% vs. 91.7%; RR 1.04, 95% CI 0.98 to 1.1). In the SIMPLIFY (post-hoc analysis) study, the secondary outcomes were: significant increase in clinical cure at day 60 (96.3% vs. 84.9%; RR 1.13, 95% CI 1.06 to 1.21) No significant difference in death at day 60 (2.7% vs. 5.2%; RR 1.02, 95% CI 0.98 to 1.07) Significant decrease in the rate of recurrence until day 60 (10.9% vs. 24.3%; RR 0.45, 95% CI 0.24 to 0.8). In the SIMPLIFY (post-hoc analysis) study, the safety outcomes were: no significant difference in adverse events. In the SIMPLIFY (post-hoc analysis) study, the conclusion was: in patients with bloodstream infections caused by Enterobacterales who achieved clinical stability by day 5, early oral switch was equivalent to continued IV therapy with respect to clinical cure at test of cure.

---

### IMPULSE: a scalable algorithm for design of minimum specific absorption rate parallel transmit RF pulses [^4308a5a9]. Magnetic Resonance in Medicine (2019). Medium credibility.

Purpose

Managing local specific absorption rate (SAR) in parallel transmission requires ensuring that the peak SAR over a large number of voxels (> < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:msup > < mml:mrow > < mml:mn > 10 < /mml:mn > < /mml:mrow > < mml:mn > 5 < /mml:mn > < /mml:msup > < /mml:math >) is below the regulatory limit. The safety risk to the patient depends on cumulative (not instantaneous) SAR thus making a joint design of all RF pulses in a sequence desirable. We propose the Iterative Minimization Procedure with Uncompressed Local SAR Estimate (IMPULSE), an efficient optimization formulation and algorithm that can handle uncompressed SAR matrices and optimize pulses for all slices jointly within a practical time frame.

Theory and Methods

IMPULSE optimizes parallel transmit pulses for small-tip-angle slice selective excitation to minimize a single cost function incorporating multiple quantities (local SAR, global SAR, and per-channel power) averaged over the entire multislice scan subject to a strict constraint on excitation accuracy. Pulses for an 8-channel 7T head coil were designed with IMPULSE and compared with pulses designed using generic optimization algorithms and VOPs to assess the computation time and SAR performance benefits.

Results

IMPULSE achieves lower SAR and shorter computation time compared with a VOP approach. Compared with the generic sequential quadratic programming algorithm, computation time is reduced by a factor of 5–6 by using IMPULSE. Using as many as 6 million local SAR terms, up to 120 slices can be designed jointly with IMPULSE within 45 s.

Conclusions

IMPULSE can handle significantly larger number of SAR matrices and slices than conventional optimization algorithms, enabling the use of uncompressed or partially compressed SAR matrices to design pulses for a multislice scan in a practical time frame.

---

### Colorectal cancer screening and prevention [^390b568a]. American Family Physician (2025). High credibility.

Regarding screening and diagnosis for colon cancer, more specifically with respect to indications for screening, high-risk individuals, family history, AAFP 2025 guidelines recommend to obtain CRC screening in patients with ≥ 1 first-degree relatives with CRC or adenomatous polyps, starting at 40 years of age or 10 years before the age of the youngest relative at the time of their diagnosis.

---

### Clustering knowledge and dispersing abilities enhances collective problem solving in a network [^77613a31]. Nature Communications (2019). High credibility.

Here, however, we find quite different results: intermixing hampers systemic performance. Furthermore, there is a tradeoff in systemic performance over time, as seen in Fig. 6b. Networks with some form of intermixing perform better in the short-run. However, the network with the minimal possible intermixing performs worse in the short-run but best in the long-run. In other words, this suggests an "all or nothing" trade-off over time: the configuration with the least amount of intermixing possible (minimal) performs worse in the short-run, but better in the long-run, while any gradation of increased intermixing yields the same rank-ordering in performance as their counterpart networks in the diversity of ability simulations.

Agents in the intermixed network setups explore for solutions early on in the simulation, which is a double-edged sword. In the short-run, with more intermixing, agents quickly turn to exploitation, as they merely take solutions that are marginally better relative to other setups but are still mediocre in absolute terms. Said differently, agents in setups with at least some intermixing quickly coalesce to whatever the few agents that did explore the problem space found, which are often not the best solutions possible. So, exploring the problem space using worse solutions often leads to modest gains. However, the agents in setups with minimal intermixing are more commonly exploring, rather than exploiting, because their neighbors' initial solutions are less optimal than their counterparts in intermixed networks. This is because there is minimal exposure to diversity in these minimal setups, which inadvertently produces poor solutions in the short-run but allows for more exploration to find better solutions in the long-run. In other words, by finding better solutions through exploration, agents more often uncover pathways to better solutions earlier on. The end result is that the long-run performance of setups with minimal intermixing is best.

---

### ACR appropriateness criteria® chronic elbow pain [^3cfc658d]. Journal of the American College of Radiology (2022). High credibility.

Regarding diagnostic investigations for lateral elbow tendinopathy, more specifically with respect to diagnostic imaging, ACR 2022 guidelines recommend to obtain elbow radiography as the initial imaging in patients with chronic elbow pain.

---

### Imlunestrant [^7b5c5001]. FDA. Low credibility.

Labeled indications for Imlunestrant include:

- Treatment of breast cancer in female adults with ESR1 mutation (advanced or metastatic, ER-positive, HER2-negative, progression after ≥ 1 line of endocrine therapy)

---

### Flexible multitask computation in recurrent networks utilizes shared dynamical motifs [^6231fc78]. Nature Neuroscience (2024). High credibility.

Identifying dynamical motifs in 15 task networks

To quantify shared structure across many tasks in a single network and to compare shared structure across multiple networks, we developed a modified version of the task variance metric described by Yang et al. We were motivated to study task periods because changes in the inputs reconfigure the RNN's dynamics across task periods. For example, when the stimulus input turns off in some tasks, the network goes from processing a stimulus to maintaining a memory of the stimulus. Task periods, therefore, provide the relevant granularity to identify the dynamical motifs that perform distinct computations.

We divided tasks into task periods and computed the variance across stimulus conditions for each unit, normalized across all task periods ('Task variance analysis' in Methods). The result was a matrix of each unit's normalized variance for each task period of every task (Fig. 3a), which we refer to as the variance matrix in subsequent analyses. We sorted the rows and columns of this matrix based on similarity ('Clusters' in Methods). Clusters of units a–z were identified by performing hierarchical clustering on the columns and rows of the variance matrix and identifying a distance criterion to maximize the ratio of intercluster to intracluster distances (Fig. 3a, Extended Data Fig. 4 and 'Clusters' in Methods).

---

### Multimorbidity progression and the heterogeneous impact of healthy ageing risk factors: a multicohort study [^77dd3d25]. BMJ Public Health (2025). High credibility.

Results

Of the 75 874 individuals involved in the primary study, the median age was 64 years (IQR, 57–73 years), 42 477 (55.98%) were females. The average follow-up time was 7.58 years equating to 574 817 person-years. Overall, 5338 individuals with 0 conditions (4 our list of 11 diseases) individuals developed 1 condition, 13 900 individuals with less than 2 conditions developed multimorbidity (≥ 2 conditions) and 11 682 died within the study period (online supplemental table 5).

Health transitions across and by cohorts over 8 years

The probabilities of disease accumulation are relatively higher when increasing one condition compared with having more than one condition (figure 1), with 0→ 1: 0.29 (95% CI 0.28 to 0.29), 1→ 2: 0.27 (95% CI 0.27 to 0.28) and 2→ ≥ 3: 0.44 (95% CI 0.43 to 0.45). Moreover, when comparing a sequence of transitions with the same starting states, the accumulation probabilities declined as the number of diseases of the ending states increased (figure 1). These findings suggest that multiple diseases tend to accumulate gradually over time, rather than occurring suddenly. We emphasised that there were non-zero reduction probabilities between two successive health states, although much smaller than the accumulation probabilities. This finding indicates that solely examining net accumulation, as the literature has done, may not furnish a comprehensive understanding of the multimorbidity development over time.

---

### N of 1, two contemporary arm, randomised controlled clinical trial for bilateral epicondylitis: a new study design [^35cb8546]. BMJ (2011). Excellent credibility.

Conclusions

At end of the trial, both arms were almost pain-free, but the drop in pain in the treated arm was greater than in the control arm. This allows several different conclusions to be drawn on efficacy of platelet lysate in chronic refractory epicondylitis:

1) It is effective, since drop in pain was steeper in the treated arm
2) It is not effective, and improvement was due to the natural course of lateral epicondylitis
3) It is effective, and the parallel improvement in the untreated arm could have been mediated by the improvement in the treated arm, by allowing a more equal distribution of workload
4) Both arms benefited from participation into a clinical trial
5) More studies are needed.

---

### Facile construction of fully sp-carbon conjugated two-dimensional covalent organic frameworks containing benzobisthiazole units [^38f7c031]. Nature Communications (2022). High credibility.

Discussion

In summary, a new condition of the Knoevenagel reaction was first explored for constructing the sp 2 -carbon conjugated COFs materials which shows good universality and success rate. Three fully sp 2 -carbon conjugated 2D COFs containing benzobisthiazole units as an electron-withdrawing group were successfully prepared via Knoevenagel polycondensation under this mild condition. The electron-deficient benzobisthiazoles units and cyano-vinylene linkages were both integrated into the π conjugated COFs skeleton. Further introduction of functional building blocks such as triazine and benzotrithiophene could successfully adjust the physical properties of the COFs for photocatalysis applications. It is noted that BTH-3 containing electron-riched benzotrithiophene as a donor with a strong D-A effect exhibited an attractive photocatalytic HER of 15.1 mmol h −1 g −1 under visible light irradiation, much higher than that of the triazine-containing BTH-1 (10.5 mmol h −1 g −1) and the benzene-containing BTH-2 (1.2 mmol h −1 g −1). This research first provides a facile way for the construction of sp 2 -carbon conjugated COFs. The conventional intractable preparation of the sp 2 -carbon conjugated COFs was greatly simplified. Second, the fully π conjugated COFs materials with D-A system show excellent photocatalytic hydrogen evolution performance and will definitely promote the development of COFs in photocatalysis.

---

### Practical guidance for the evaluation and management of drug hypersensitivity: specific drugs [^4b3e6cbe]. The Journal of Allergy and Clinical Immunology: In Practice (2020). High credibility.

Paclitaxel desensitization — two-bag/8-step protocol: Under the same paclitaxel infusion context, the two-bag example lists Solution 1 as 250 mL at 0.118 mg/mL with 18.75 mL infused (dose 2.213 mg) and Solution 2 as 250 mL at 1.167 mg/mL with 250 mL infused (291.787 mg). Step examples shown include step 1 using solution 1 at 5 mL/h for 15 min with 1.25 mL infused (0.148 mg; cumulative 0.148 mg) and step 4 using solution 1 at 40 mL/h for 15 min with 10 mL infused (1.18 mg; cumulative 2.213 mg).

---

### ACOEM practice guidelines: elbow disorders [^51466a24]. Journal of Occupational and Environmental Medicine (2013). Medium credibility.

Summary of recommendations — basis and use: These recommendations are based on critically appraised higher quality research evidence and on expert consensus only when higher quality evidence was unavailable or inconsistent, and the more detailed indications, specific appropriate diagnoses, temporal sequencing, prior testing or treatment, and contraindications — which are elaborated in more detail for each test or treatment in the guideline — should be reviewed for recommendations in clinical practice or medical management; these recommendations are not simple "yes/no" criteria, and the evidence supporting them is in nearly all circumstances developed from typical patients, not unusual situations or exceptions.

---

### Guidelines for the use of antiretroviral agents in pediatric HIV infection [^b0631b97]. HIV.gov (2025). High credibility.

Abacavir (ABC) neonatal dosing — birth to < 1 month of age uses simplified weight-band dosing for the 20 mg/mL oral solution twice daily: for 2 kg to < 3 kg, 0.4 mL provides 8 mg (4.0–2.8 mg/kg); for 3 kg to < 4 kg, 0.5 mL provides 10 mg (3.3–2.6 mg/kg); for 4 kg to < 5 kg, 0.6 mL provides 12 mg (3.0–2.4 mg/kg). The table states that simplified weight-band dosing exceeds recommended mg/kg ABC dosing in neonates and infants and that neonatal ABC dose is based on birth weight and does not require weight-based adjustment during the first month of life.

---

### Validation of a simplified comorbidity evaluation predicting clinical outcomes among patients with coronavirus disease 2019 – A multicenter retrospective observation study [^7b038518]. The American Journal of Emergency Medicine (2022). Medium credibility.

Objectives

We compared and validated the performance accuracy of simplified comorbidity evaluation compared to the Charlson Comorbidity Index (CCI) predicting COVID-19 severity. In addition, we also determined whether risk prediction of COVID-19 severity changed during different COVID-19 pandemic outbreaks.

Methods

We enrolled all patients whose SARS-CoV-2 PCR tests were performed at six different hospital Emergency Departments in 2020. Patients were divided into three groups based on the various COVID-19 outbreaks in the US (first wave: March-May 2020, second wave: June-September 2020, and third wave: October-December 2020). A simplified comorbidity evaluation was used as an independent risk factor to predict clinical outcomes using multivariate logistic regressions.

Results

A total of 22,248 patients were included, for which 7023 (32%) patients tested COVID-19 positive. Higher percentages of COVID-19 patients with more than three chronic conditions had worse clinical outcomes (i.e., hospital and intensive care unit admissions, receiving invasive mechanical ventilations, and in-hospital mortality) during all three COVID-19 outbreak waves.

Conclusions

This simplified comorbidity evaluation was validated to be associated with COVID clinical outcomes. Such evaluation did not perform worse when compared with CCI to predict in-hospital mortality.

---

### Community mitigation guidelines to prevent pandemic influenza-United States, 2017 [^cc95be86]. MMWR: Recommendations and Reports (2017). Medium credibility.

Refined assessment — scaled measures of influenza virus transmissibility and clinical severity provide ordinal categories with explicit cut points: For transmissibility (scale of 1–5), symptomatic attack rate, community is ≤ 10%, 11%–15%, 16%–20%, 21%–24%, or ≥ 25%; symptomatic attack rate, school is ≤ 20%, 21%–25%, 26%–30%, 31%–35%, or ≥ 36%; symptomatic attack rate, workplace is ≤ 10%, 11%–15%, 16%–20%, 21%–24%, or ≥ 25%; household secondary attack rate, symptomatic is ≤ 5%, 6%–10%, 11%–15%, 16%–20%, or ≥ 21%; R0 (basic reproductive number) is ≤ 1.1, 1.2–1.3, 1.4–1.5, 1.6–1.7, or ≥ 1.8; and peak percentage of outpatient visits for influenza-like illness is 1%–3%, 4%–6%, 7%–9%, 10%–12%, or ≥ 13%. For clinical severity (scale of 1–7), case-fatality ratio is categorized as < 0.02%, 0.02%–0.05%, 0.05%–0.1%, 0.1%–0.25%, 0.25%–0.5%, 0.5%–1%, and > 1%; case-hospitalization ratio as < 0.5%, 0.5%–0.8%, 0.8%–1.5%, 1.5%–3%, 3%–5%, 5%–7%, and > 7%; and deaths-hospitalizations ratio as ≤ 3%, 4%–6%, 7%–9%, 10%–12%, 13%–15%, 16%–18%, and > 18%.

---

### Roflumilast (Zoryve) [^e2de6cd7]. FDA (2024). Medium credibility.

The dosage of roflumilast TOP for treatment of plaque psoriasis in adults is 1 application(s) TOP daily (0.3% cream or foam)

---

### Data-driven quantum chemical property prediction leveraging 3D conformations with uni-mol [^b7f80dbe]. Nature Communications (2024). High credibility.

Model configuration

Similar to both Graphormerand Transformer-M, Uni-Mol+ comprises 12 layers with an atom representation dimension of d x = 768 and a pair representation dimension of d p = 256. The hidden dimension of FFN in the atom representation track is set to 768, while that of the pair representation track is set to 256. Additionally, the hidden dimension in the OuterProduct is d o = 32, and the hidden dimension in the TriangularUpdate is d t = 32 as well. The number of conformation optimization iterations R is set to 1, indicating that the model iterates twice in total (once for conformation optimization and once for quantum chemistry property prediction). For the training strategy, we specified a standard deviation of υ = 0.2 for random noise and employed a particular sampling method for q. Specifically, q was set to 0.0 with probability 0.8, set to 1.0 with probability 0.1, and uniformly sampled from [0.4, 0.6] with probability 0.1. With this setting, the number of parameters of Uni-Mol+ is about 52.4M.

Setting for PCQM4MV2

We used the AdamW optimizer with a learning rate of 2 e − 4, a batch size of 1024, (β 1, β 2) set to (0.9, 0.999), and gradient clipping set to 5.0 during training, which lasted for 1.5 million steps, with 150K warmup steps. Additionally, an exponential moving average (EMA) with a decay rate of 0.999 was utilized. The training took approximately 5 days, utilizing 8 NVIDIA A100 GPUs. The inference on the 147k test-dev set took approximately 7 minutes, utilizing 8 NVIDIA V100 GPUs.

---

### Multi-time-point data preparation robustly reveals MCI and dementia risk factors [^c8a94d99]. Alzheimer's & Dementia (2020). Medium credibility.

2.2.2 Progression analyses

Three comparisons were assessed for progression analyses: remained healthy (n = 5431) versus healthy‐to‐MCI (n = 543), remained healthy (n = 5431) versus realthy‐to‐dementia (n = 400), and remained MCI (n = 1141) versus MCI‐to‐dementia (n = 809). Figure 2 depicts the data preparation process for the progression groups, and Figure 3 depicts the transitions between diagnostic groups over time. The length of time from the first visit to subsequent diagnosis varied among the progression groups, ranging between 3.10 and 7.00 years (Table S1 in supporting information). Individuals with single observations, dementia at baseline, and those with alternating diagnosis between visits were excluded (n = 21,090). Moreover, those with missing values at baseline (n = 560) or only having complete observations for a single visit (n = 4871) were also excluded (Figure 2). Next, a multi‐time‐point approach was used to determine the status of risk factors given the data from multiple visits as explained in Section 2.2.2.1.

FIGURE 3
Transitions between different diagnostic groups over time in the National Alzheimer's Coordinating Center dataset

---

### Addressing early childhood emotional and behavioral problems [^0988ef83]. Pediatrics (2016). Medium credibility.

Examples of evidence-based treatments for existing diagnoses in young children — This report focuses on programs that target current diagnoses or clear clinical problems (rather than risk) in infants and toddlers.

---

### Selecting appropriate toys for young children in the digital era [^dd9f979e]. Pediatrics (2019). High credibility.

Toys and child development — general considerations states that "Toys are important in early child development in relation to their facilitation of cognitive development, language interactions, symbolic and pretend play, problem-solving, social interactions, and physical activity, with increasing importance as children move from infancy into toddlerhood".

---

### Standards of care in diabetes – 2025 [^9a2ec52c]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for hypoglycemia, more specifically with respect to elderly patients, ADA 2025 guidelines recommend to individualized goal levels for the treatment of hypertension in most older patients.

---

### Evaluation after a first seizure in adults [^b94f7748]. American Family Physician (2022). High credibility.

Regarding diagnostic investigations for first seizure in adults, more specifically with respect to initial assessment, AAFP 2022 guidelines recommend to evaluate for provoking factors after a first seizure, such as inflammatory, infectious, structural, toxic, or metabolic causes.

---

### Hydrocodone bitartrate (hysingla ER) [^0c03a0bc]. FDA (2023). Medium credibility.

Warnings and precautions regarding the use of hydrocodone bitartrate ER PO (also known as Hysingla ER):
- **Adrenal insufficiency**: use caution in patients taking the drug for a prolonged period (> 1 month).
- **Central sleep apnea**: use caution in patients taking higher doses.
- **Decreased serum hydrocodone level**: use caution in patients taking CYP3A4 inducers (such as rifampin, carbamazepine, or phenytoin) or discontinuing CYP3A4 inhibitors (such as macrolide antibiotics, azole antifungals, or protease inhibitors).
- **Erectile dysfunction, infertility**: use caution in patients taking the drug for a prolonged period.
- **Exacerbation of increased ICP**: use caution in patients with increased ICP, brain tumor, or head injury.
- **Growth suppression**: use caution in patients with chronic corticosteroid therapy. Monitor growth regularly in pediatric patients receiving chronic corticosteroid therapy. Reassess the need for hydrocodone regularly and adjust the corticosteroid therapy as appropriate.
- **Hypotension, syncope**: use caution in patients with reduced blood volume or taking other CNS depressants. Monitor BP after initiation and dose titration. Avoid use in patients with circulatory shock.
- **Mask symptoms of head injury**: use caution in patients with head injury. Avoid use in patients with impaired consciousness or coma.
- **Opioid overdose**: use caution in patients taking CNS depressants or with a history of opioid use disorder or prior opioid overdose. Consider prescribing naloxone based on the patient's risk factors for overdose.
- **Opioid withdrawal syndrome**: do not discontinue abruptly in patients physically dependent on opioids.
- **Opioid withdrawal syndrome**: use caution in patients taking CYP3A4 inducers (such as rifampin, carbamazepine, or phenytoin) or discontinuing CYP3A4 inhibitors (such as macrolide antibiotics, azole antifungals, or protease inhibitors).
- **Opioid withdrawal syndrome**: use extreme caution in patients taking mixed agonist/antagonist analgesics (such as pentazocine, nalbuphine, or butorphanol) or partial agonist analgesics (such as buprenorphine).
- **Prolonged QT interval**: use caution in patients with congestive HF, bradyarrhythmia, electrolyte abnormalities, or taking drugs prolonging QT interval. Avoid use in patients with congenital long QT syndrome. Do not exceed 90 mg BID in patients developing QT prolongation.
- **Seizure**: use caution in patients with seizure disorder.
- **Serotonin syndrome**: use caution in patients taking serotonergic drugs.
- **Serotonin syndrome**: use extreme caution in patients taking MAOIs or within 14 days of stopping treatment.
- **Somnolence**: use extreme caution in patients performing activities requiring mental alertness, such as driving or operating machinery.
- **Sphincter of Oddi dysfunction**: use caution in patients with biliary tract disease and acute pancreatitis.

---

### Hourly step recommendations to achieve daily goals for working and older adults [^588d8957]. Communications Medicine (2024). Medium credibility.

Figure 3 shows the predicted mean step counts and 95% CI for the rest of the day, given the step counts accumulated from 6 p.m. to 11 p.m. adjusting for age group, BMI group and sex. Overall, there was little difference in the conditional predicted mean step counts for the rest of the days between the weekdays and the weekends. On weekdays, participants who accumulated 6000 steps by 6 p.m. had a 0.692 probability of achieving 10,000 steps (all the goals) by the end of the day (refer to the Goal 3 diagonal line in Fig. 3, Table 3, and Supplementary Table S2). Similarly, participants who had accumulated 7000 steps by 7 p.m. 8000 steps by 8 p.m. 8500 steps by 9 p.m. and 9500 steps by 10 p.m. achieved 10,000 mean steps by the end of the day with a probability of 0.910, 0.982, 0.700, and 1, respectively. On weekends, participants who had accumulated 6500 steps by 6 p.m. 7000 steps by 7 p.m. 8000 steps by 8 p.m. 8500 steps by 9 p.m. and 9500 steps by 10 p.m. achieved 10,000 mean steps by the end of the day with probability 0.852, 0.583, 0.947, 0.717 and 1, respectively. The step counts required to achieve 5000 and 7500 steps by the end of the day can be found in Supplementary Tables S3 and S4, respectively.

---

### KDIGO clinical practice guideline for the care of kidney transplant recipients [^2cd3ee78]. American Journal of Transplantation (2009). Medium credibility.

KDIGO GRADE system for grading quality of evidence — structure and upgrade/downgrade rules — starts by study design (Randomized trials = High; Observational study = Low; Any other evidence = Very Low), allows reductions for study quality, consistency, directness and other limitations with specified level changes, and allows raises for strength of association or other factors, with strong and very strong associations defined by risk ratio thresholds. Specifically, reductions include "−1 level if serious limitations" or "−2 levels if very serious limitations", "−1 level if important inconsistency", "−1 level if some uncertainty" or "−2 levels if major uncertainty", and "−1 level if sparse or imprecise data" or "−1 level if high probability of reporting bias". Raises include "+1 level if strong, a no plausible confounders" or "+2 levels if very strong, b no major threats to validity", plus "+1 level if evidence of a dose response gradient" or "+1 level if all residual plausible confounders would have reduced the observed effect". Strong and very strong associations are defined as "significant RR of > 2 (< 0.5)" and "significant RR of > 5 (< 0.2)", respectively.

---

### How to write a problem solving in clinical practice paper [^a300cd92]. Archives of Disease in Childhood: Education and Practice Edition (2013). Low credibility.

An overview of the concept of problem solving in clinical practice, and how to go about writing an article to be submitted.

---

### Triaging and initial treatment in the patient with suspected type 2 diabetes [^23f6b893]. PES (2023). High credibility.

Insulin titration protocol: weaning — manage insulin reduction with the following steps and thresholds. If still on MDI, stop short acting insulin and contact approximately every 3 days to review glucoses. Wait until fasting numbers are < 100 mg/dl for at least 2–3 days in a 5–7 day period. If so, cut back long-acting insulin dose by 20%/10 units, but may consider 30%/15–20 units if glucoses are 80 mg/dl or less, and consider reliability of patient contact when making the decision to more aggressively wean insulin. Once the dose is down to 0.1–0.2 units/kg/day, consider stopping, and consider using a small decrease in insulin as a motivational factor to continue to maintain contact. If patient was lost to follow-up between onset visit and follow-up visit, but has an HbA1c < 6% 3 months+ after diagnosis and is on full-dose metformin (or pioglitazone), stop long-acting insulin with careful monitoring after. If HbA1c is 6.5–7% at the next visit, add a second agent.

---

### Technical update on HIV-1 / 2 differentiation assays [^50eb7933]. CDC (2016). Medium credibility.

HIV-1/HIV-2 differentiation testing — alternatives when the FDA-approved antibody differentiation immunoassay cannot be used states that there may be circumstances under which a laboratory is unable to adopt the FDA-approved HIV-1/HIV-2 antibody differentiation immunoassay, and in this situation a laboratory has alternatives for that step in the algorithm, some of which could delay turnaround time for test results. Send specimens to another laboratory that offers the FDA-approved supplemental HIV antibody differentiation assay. Refer to CDC/APHL laboratory testing guidance section I, "Alternative Testing Sequences When Tests in the Recommended Algorithm Cannot be Used".

---

### 2013 ACC / AHA guideline on the assessment of cardiovascular risk: a report of the American college of cardiology / American Heart Association task force on practice guidelines [^d0e143d1]. Journal of the American College of Cardiology (2014). Medium credibility.

Table 5 — Distribution of estimated 10-year risk of a first hard atherosclerotic cardiovascular disease (ASCVD) event in the CVD-free, nonpregnant US population aged 40 to 79 years is organized by predicted 10-year risk categories < 2.5%, 2.5%–4.9%, 5.0%–7.4%, 7.5%–9.9%, 10.0%–14.9%, 15.0%–19.9%, and ≥ 20.0%. In the overall population, the percentages (95% CI) are 33.4 (31.2–35.5), 21.0 (19.4–22.7), 12.7 (11.4–14.0), 7.4 (6.5–8.3), 8.9 (8.1–9.6), 6.3 (5.6–7.1), and 10.2 (9.5–11.0), with corresponding n of 33 534 000, 21 151 000, 12 766 000, 7 470 000, 8 940 000, 6 380 000, and 10 300 000. Among men, the percentages (95% CI) are 17.4 (15.2–19.7), 22.7 (20.3–25.1), 15.6 (13.8–17.4), 10.1 (8.5–11.6), 12.1 (10.7–13.5), 8.8 (7.4–10.2), and 13.3 (12.1–14.4), with n of 8 386 000, 10 950 000, 7 511 000, 4 847 000, 5 849 000, 4 248 000, and 6 388 000. Among women, the percentages (95% CI) are 48.0 (44.8–51.3), 19.5 (17.3–21.6), 10.0 (8.3–11.8), 5.0 (3.8–6.2), 5.9 (5.1–6.7), 4.1 (3.4–4.7), and 7.5 (6.5–8.4), with n of 25 148 000, 10 200 000, 5 256 000, 2 622 000, 3 091 000, 2 131 000, and 3 912 000. These estimates were derived by applying the Pooled Cohort Equations to NHANES 2007–2010 with N = 5367 and weighting to 100 542 000 US population.

---

### Diagnosis, management and treatment of the Alport syndrome-2024 guideline on behalf of ERKNet, ERA and ESPN [^45eede36]. Nephrology, Dialysis, Transplantation (2025). High credibility.

Regarding diagnostic investigations for Alport syndrome, more specifically with respect to hearing assessment, ERA/ERN ERKNet/ESPN 2025 guidelines recommend to consider obtaining a hearing evaluation at diagnosis or upon reaching adulthood in female patients with X-linked alport syndrome, and then every 5 years in the absence of hearing loss symptoms.

---

### Baveno VII-renewing consensus in portal hypertension [^33aaab5c]. Journal of Hepatology (2022). High credibility.

Regarding classification and risk stratification for portal hypertension, more specifically with respect to prognosis, Baveno VII 2022 guidelines recommend to monitor patients with liver stiffness measurement values of 7–10 kPa and ongoing liver injury on a case-by-case basis for changes indicating progression to compensated advanced chronic liver disease.

---

### Colorectal cancer screening and prevention [^c270f773]. American Family Physician (2025). High credibility.

Regarding screening and diagnosis for colon cancer, more specifically with respect to indications for screening, general population, aged 76–85 years, AAFP 2025 guidelines recommend to consider obtaining screening for CRC in adults aged 76–85 years at average risk based on overall health status, prior screening history, and patient preferences.

---

### Multiple testing corrections for imputed SNPs [^9b1e235c]. Genetic Epidemiology (2011). Low credibility.

Multiple testing corrections are an active research topic in genetic association studies, especially for genome-wide association studies (GWAS), where tests of association with traits are conducted at millions of imputed SNPs with estimated allelic dosages now. Failure to address multiple comparisons appropriately can introduce excess false-positive results and make subsequent studies following up those results inefficient. Permutation tests are considered the gold standard in multiple testing adjustment; however, this procedure is computationally demanding, especially for GWAS. Notably, the permutation thresholds for the huge number of estimated allelic dosages in real data sets have not been reported. Although many researchers have recently developed algorithms to rapidly approximate the permutation thresholds with accuracy similar to the permutation test, these methods have not been verified with estimated allelic dosages. In this study, we compare recently published multiple testing correction methods using 2.5M estimated allelic dosages. We also derive permutation significance levels based on 10,000 GWAS results under the null hypothesis of no association. Our results show that the simpleM method works well with estimated allelic dosages and gives the closest approximation to the permutation threshold while requiring the least computation time.

---

### Six steps in quality intervention development (6SQuID) [^221654f2]. Journal of Epidemiology and Community Health (2016). Low credibility.

Conclusion

In order to improve the effectiveness of public health interventions, a systematic approach to intervention development is required, as well as rigorous evaluation. However, little practical guidance exists for public health practitioners and researchers that explains the essential stages of intervention development. We argue that this process can be broken down into six key steps: defining and understanding the problem; identifying modifiable determinants; deciding on the mechanisms of change; clarifying how these will be delivered; testing and adapting the intervention; and collecting initial evidence of effectiveness. This model imposes somewhat arbitrary cut-offs in the process of intervention development and suggests a linear progression. In practice developers often return to an earlier step in the sequence before reaching step 6, and subsequently 'definitive trials' can lead to further revisions of the intervention. However, we hope that if each of these six steps is carefully addressed in the design of interventions better use will be made of scarce public resources by avoiding the costly evaluation, or implementation, of unpromising interventions.

What is already known on this subject?

There is little practical guidance for researchers or practitioners on how best to develop public health interventions. Existing models are generally orientated towards individual behaviour change and some are highly technical and take years to implement.

What this study adds?

This paper provides a pragmatic six-step guide to develop interventions in a logical, evidence-based way to maximise likely effectiveness. If each step is carefully addressed, better use will be made of scarce public resources by avoiding the costly evaluation, or implementation, of unpromising interventions.

---

### Direct high-throughput deconvolution of non-canonical bases via nanopore sequencing and bootstrapped learning [^6ea11409]. Nature Communications (2025). High credibility.

Generation of a complex template library to enable training of XNA basecaller models

To obtain a basecaller for XNAs (i.e. capable of natively basecalling canonical as well as non-canonical bases) we needed training data with enough reads and diversity of sequence contexts containing NCBs such that a neural network model with sufficient generalization capability could be developed. Since nanopore signal levels are known to be affected by the identity of at least the 6 adjacent bases in the pore during sequencing (Fig. 2A), we designed a complex library where a NCB is flanked by 5-mers of the same sequence identity on both sides (4 5 = 1024 templates containing N 1 N 2 N 3 N 4 N 5 -M-N 1 N 2 N 3 N 4 N 5 sequence, where: N i = A, T, G, or C; M = Ds or Px; Supplementary Fig. 2, Supplementary Data 2, 3), such that a rolling 6-nt window places the NCB in every position within the 6-mer resulting in all possible 6144 single-NCB 6-mer sequence contexts being represented. Synthesizing such a large library would have been infeasible by employing the chemical synthesis approach that is typically used for obtaining high-fidelity XNAs, which was used for the initial proof-of-concept library here (20 templates). We therefore designed and developed a cost-effective enzymatic synthesis scheme to generate this much larger complex library (> 50× in size; Fig. 3A; "Methods").

---

### Addendum guidelines for the prevention of peanut allergy in the United States: report of the National Institute of Allergy and Infectious Diseases-sponsored expert panel [^a8d05d24]. Annals of Allergy, Asthma & Immunology (2017). Medium credibility.

Peanut introduction recipes — option 2 (thinned smooth peanut butter) specifies 2 teaspoons (9–10 g of peanut butter; approximately 2 g of peanut protein) with preparation steps: Measure 2 teaspoons of peanut butter and slowly add 2 to 3 teaspoons of hot water, stir until peanut butter is dissolved, thinned, and well blended, let cool, and increase water amount if necessary (or add previously tolerated infant cereal) to achieve consistency comfortable for the infant.

---

### NCCN guidelines® insights: non-small cell lung cancer, version 7.2025 [^79596b05]. Journal of the National Comprehensive Cancer Network (2025). High credibility.

Maintenance therapy — definitions, eligibility, and monitoring: Maintenance therapy refers to therapy given for patients with advanced NSCLC after 4 to 6 cycles of first-line therapy, and patients are only candidates if their tumors have responded or they have stable disease and their tumors have not progressed. Continuation maintenance therapy refers to the use of at least one of the agents that was given in the first-line regimen, whereas switch maintenance therapy refers to initiation of a different agent that was not included as part of the first-line regimen. Maintenance therapy is recommended in the NCCN Guidelines for select patients with tumor response or stable disease and is not recommended for all patients; it is not recommended for patients with performance status (PS) 3 to 4 or those with progression. Monitoring is recommended during maintenance therapy with response assessment with computed tomography (CT), with or without contrast, of known or high-risk sites of disease every 6 to 12 weeks.

---

### Roflumilast (Zoryve) [^11a5acd0]. FDA (2024). Medium credibility.

The dosage of roflumilast TOP for treatment of plaque psoriasis in both children (in patients 6–12 years) is 1 application(s) TOP daily (0.3% cream)

---

### A problem of persistence: still more questions than answers? [^1078e4fd]. Nature Reviews: Microbiology (2013). Medium credibility.

The current antibiotic resistance crisis has led to increased pressure to prioritize strategies to tackle the issue, with a strong focus being placed on the development of novel antimicrobials. However, one major obstacle that is often overlooked is persister cells, which are refractory to antibiotic treatment. Tackling persistence is a challenge because these cell types are extremely difficult to study and, consequently, little is known about their physiology and the factors that lead to their emergence. Here, four experts contemplate the main physiological features that define persistence and the implications of persistence for antibiotic treatment regimens, and consider what the study of bacterial persistence has taught us about the heterogeneity of bacterial populations.

---

### Changes to information in working memory depend on distinct removal operations [^2f956be8]. Nature Communications (2020). High credibility.

Data from all localizer runs were then used to re-train the WM representation classifiers and decode the central study data. Training was done with an individualized optimal penalty derived from the cross-validation analysis, and a new feature selection was performed (category: 57.81% of the original voxels; M = 8021 voxels, SD = 1653; subcategory: 71.79% of the original voxels; M = 26,615 voxels, SD = 5268). These classifiers were reliably accurate at the category level (M = 0.80, SEM = 0.012, more reliable than T (49) = 28.16, P < 0.001, d = 3.983, 95% CI [0.35, 0.4]) and the subcategory level (M = 0.28, SEM = 0.008, T (49) = 9.7, P s < 5.48e−13, d = 1.372, 95% CI [0.08, 0.12], Supplementary Table 2). These classifiers were used to decode every timepoint to construct trial-averaged decoding time series. A 13.8 s time window (30 TRs, unshifted, from the onset of each trial) was used to evaluate the trajectory of the average classifier evidence for the WM item's category in each condition (Fig. 4a). The data were baseline corrected by removing the mean target classifier evidence, separately for each condition, from the first 2.76 s (6 TRs) from all subsequent time points. This procedure had no effect on any statistical comparisons between conditions but centered the data at trial onset to 0 rather than 0.45 (classifier evidence, ranging from 0 to 1). To highlight the removal of information from WM, we then recoded these data using the classifier evidence from maintain as a baseline by subtracting these values from the classifier evidence values for the three removal conditions (Fig. 4b). For statistical tests, we focused on a 6.9 s (15 TR) time window beginning at the onset of the operation (TR 7) through the end of the longest fixation period (TR 21). This analysis window was then segmented into five contiguous blocks (1.38 s [3 TRs] per block) and a repeated-measure one-way ANOVA and pair-wise T -tests with false discovery rate (FDR) for multiple comparison correction was applied to the averaged target category classifier evidence scores in each block. To identify the removal start point for each condition, one-sample T -tests were used in each block to compare the classifier evidence scores against zero.

---

### An official multi-society statement: the role of clinical research results in the practice of critical care medicine [^6198cd13]. American Journal of Respiratory and Critical Care Medicine (2012). Medium credibility.

Variability in clinical practice — the weight given to facts and reasons for a clinical decision can differ between clinicians, resulting in variability; factors beyond study design and execution can make clinical research results more compelling and are summarized in Table 2, but no particular clinical study can be expected to satisfy all criteria; when research results are not compelling, clinicians will be more likely to weight experiential or pathophysiologic knowledge more heavily, so clinicians can reasonably come to a different clinical conclusion even when considering the same research; differences may derive from differing professional values and prior knowledge, and making the clinical decision-making process explicit will aid in identifying the sources of variability in practice.

---

### First and second stage labor management: ACOG clinical practice guideline no. 8 [^dbeb7bfb]. Obstetrics and Gynecology (2024). High credibility.

Oxytocin dosing strategies — ACOG recommends either low-dose or high-dose oxytocin strategies as reasonable approaches to the active management of labor to reduce operative deliveries (STRONG RECOMMENDATION, HIGH-QUALITY EVIDENCE).

---

### EASL-EASD-EASO clinical practice guidelines on the management of metabolic dysfunction-associated steatotic liver disease (MASLD) [^e30d71d8]. Journal of Hepatology (2024). High credibility.

Regarding classification and risk stratification for metabolic dysfunction-associated steatotic liver disease, more specifically with respect to noninvasive assessment of fibrosis, EASD/EASL/EASO 2024 guidelines recommend to use a multi-step approach in adult patients with MASLD:

- obtain an established non-patented blood-based score, such as the fibrosis-4 index, as the first step

- obtain established imaging techniques, such as liver elastography, as the second step to further clarify the fibrosis stage if fibrosis is still suspected or in high-risk groups.

---

### ACR appropriateness criteria® chronic elbow pain [^1ee319f4]. Journal of the American College of Radiology (2022). High credibility.

Regarding diagnostic investigations for lateral elbow tendinopathy, more specifically with respect to diagnostic imaging, ACR 2022 guidelines recommend to obtain elbow CT/MRI to evaluate for suspected occult stress fracture or other bone abnormality in patients with chronic elbow pain and normal or nonspecific radiographs.

---

### ACR appropriateness criteria® chronic elbow pain [^be21b521]. Journal of the American College of Radiology (2022). High credibility.

Regarding diagnostic investigations for lateral elbow tendinopathy, more specifically with respect to diagnostic imaging, ACR 2022 guidelines recommend to obtain elbow ultrasound or MRI to evaluate for suspected chronic epicondylalgia or tendon tear (including refractory to empirical treatment) in patients with chronic elbow pain and normal or nonspecific radiographs.

---

### New designs for phase 2 clinical trials [^a90544dc]. Blood (2003). Low credibility.

Conventional phase 2 clinical trials are typically single-arm experiments, with outcome characterized by one binary "response" variable. Clinical investigators are poorly served by such conventional methodology. We contend that phase 2 trials are inherently comparative, with the results of the comparison determining whether to conduct a subsequent phase 3 trial. When different treatments are studied in separate single-arm trials, actual differences between response rates associated with the treatments, "treatment effects", are confounded with differences between the trials, "trial effects". Thus, it is impossible to estimate either effect separately. Consequently, when the results of separate single-arm trials of different treatments are compared, an apparent treatment difference may be due to a trial effect. Conversely, the apparent absence of a treatment effect may be due to an actual treatment effect being cancelled out by a trial effect. Because selection involves comparison, single-arm phase 2 trials thus fail to provide a reliable means for selecting which therapies to investigate in phase 3. Moreover, reducing complex clinical phenomena, including both adverse and desirable events, to a single outcome wastes important information. Consequently, conventional phase 2 designs are inefficient and unreliable. Given the limited number of patients available for phase 2 trials and the increasing number of new therapies that must be evaluated, it is critically important to conduct these trials efficiently. These concerns motivated the development of a general paradigm for randomized selection trials evaluating several therapies based on multiple outcomes. Three illustrative applications of trials using this approach are presented.

---

### Concerns regarding… [^70dc23c9]. EClinicalMedicine (2024). Medium credibility.

Controversy about hyperparameter optimization of various machine learning models

In this study, the process of hyperparameter optimization is not described, even though different combinations of hyperparameters are crucial for model building and validation.

In the field of machine learning, using default hyperparameters may not be suitable for specific datasets. Lack of hyperparameter optimization can prevent the model from fully exploiting feature information, leading to less stable performance, high heterogeneity, and insufficient robustness, which negatively affects model building and generalization. Different combinations of hyperparameters significantly impact the predictive efficacy of the model. Failure to adequately perform hyperparameter optimization may result in models that do not achieve optimal predictive efficacy. This causes selection bias in the model and ultimately leads to incorrect decisions.

Machine learning is prone to model overfitting and poor generalization if the built-in hyperparameters are not properly set. Therefore, hyperparameter optimization in the field of machine learning is a scientific, rigorous, and reliable practice.

---

### As-3 [^f4572ab3]. FDA (2025). Medium credibility.

Storage and handling

Store at room temperature (25 °C/77 °F). Avoid excessive heat.

Protect from freezing

---

### ACR appropriateness criteria® chronic elbow pain [^a3f8448d]. Journal of the American College of Radiology (2022). High credibility.

Regarding diagnostic investigations for lateral elbow tendinopathy, more specifically with respect to diagnostic imaging, ACR 2022 guidelines recommend to obtain elbow ultrasound or MRI to evaluate for suspected nerve abnormalities in patients with chronic elbow pain and normal or nonspecific radiographs.

---

### 'Second primary breast cancers following an initial diagnosis of cancer in one breast: a methodological issue' [^f3f0fe88]. British Journal of Cancer (2008). Low credibility.

Sir,

A large number of studies have been published investigating the occurrence of second primary cancers following the diagnosis of an 'index' cancer. The standard method of analysis is to compare the observed numbers of subsequent cancers with the numbers expected on the basis of cancer incidence rates in the general population (accounting for age, sex and time period). To do this, the number of 'person-years at risk' is calculated, with the 'at risk' period for each individual starting at the date of diagnosis of the index cancer and terminating at the study cutoff date, the date of death or loss to follow-up, or the date of diagnosis of the subsequent cancer of interest. If the treatment for the index cancer entails removal of the organ in which this occurred, then it seems sensible when investigating the occurrence of second cancers at the same anatomical site to terminate the 'at-risk' period at the point when the removal occurred, as a subsequent primary cancer cannot arise in an organ that no longer exists.

---

### Practical guidance for the evaluation and management of drug hypersensitivity: specific drugs [^444379cb]. The Journal of Allergy and Clinical Immunology: In Practice (2020). High credibility.

β-Lactam desensitization (intravenous protocol) — for a target dose of 1 g, an example protocol uses three solutions each with 250 mL diluent at drug concentrations of 0.04, 0.4, and 4 mg/mL, and escalates infusion steps to deliver a cumulative dose of 1000 mg over a total infusion time of 337.9 min; the final step uses solution 3 at 80 mL/h for 172.9 min administering 922.13 mg.

---

### Clotrimazole [^b5a7188a]. FDA. Low credibility.

The dosage of clotrimazole OTIC for treatment of otomycosis in adults is 1 vial OTIC BID for 14 days (1%/0.17 mL)

---

### Managing menopause [^0a83dde9]. Journal of Obstetrics and Gynaecology Canada (2014). Medium credibility.

Regarding diagnostic investigations for menopause, more specifically with respect to evaluation of sexual dysfunction, SOGC 2014 guidelines recommend to categorize the problem according to desire, arousal, pain, or orgasm problems in order to facilitate treatment and triage care.

---

### Is it time to reconsider the indications for surgery in patients with tennis elbow? [^cdae25f1]. The Bone & Joint Journal (2023). Medium credibility.

On an individual level, a steady half-life means that the duration of symptoms or length of follow-up could be largely irrelevant and provides little prognostic value. It also means that the future course is completely unpredictable. Recovery seems to be a random process akin to the biological half-life of drugs. A molecule lasting several half-lives is no different from a molecule that is eliminated first. This does not mean that a patient should expect their symptoms to halve every four months, but rather that the probability of recovery in future persists irrespective of the time passed.

While the underlying mechanisms for the half-life pattern are unclear, these findings have relevance for everyday clinical practice and future research. In this annotation we discuss what the findings mean for the usual practice of operating on patients whose symptoms fail to settle with nonoperative treatment. Second, we consider how the findings influence the interpretation of other studies. Finally, we discuss what our advice should be to patients who are frustrated by long-standing symptoms and seek advice about whether surgery should be undertaken.

---

### An official multi-society statement: the role of clinical research results in the practice of critical care medicine [^f77c0e47]. American Journal of Respiratory and Critical Care Medicine (2012). Medium credibility.

Negotiating between various kinds of medical knowledge — no set hierarchy of knowledge to guide clinical decision making in all situations is possible, and a ranking based on study design or quality measures cannot be directly applied to clinical decisions; clinicians must continue to rely on sound clinical judgment to negotiate between potentially conflicting facts and reasons, and explicitness is to be greatly valued, as clinicians should be able to identify and articulate the sources and kinds of knowledge invoked; the initial step is identifying pertinent medical knowledge, including clinical research, pathophysiologic understanding, and clinical experience, and since no kind of knowledge is always superior, simply relying upon a randomized controlled trial or meta-analysis will be insufficient; clinicians are obligated to consider information and knowledge that might suggest action that would run counter to that suggested by clinical research, and a clinician ought to be able to concisely outline and justify the process of clinical reasoning for scrutiny and revision.

---

### Coagulation factor viia recombinant human (sevenfact) [^7272571e]. FDA (2025). Medium credibility.

1 6 HOW SUPPLIED/STORAGE AND HANDLING

How Supplied

SEVENFACT [coagulation factor VIIa (recombinant)-jncw], is supplied as a room temperature stable, white to off-white, lyophilized powder in single-dose vials, one vial per carton. The diluent for reconstitution of SEVENFACT is Water for Injection supplied as a clear colorless solution in a pre-filled syringe.
Single 1 mg, 2 mg or 5 mg vials of SEVENFACT are available in packages as indicated below.

The SEVENFACT vials are made of glass, closed with a bromobutyl rubber stopper (not made with natural rubber latex), and sealed with an aluminum cap.
The pre-filled diluent syringes are made of glass, with a siliconized bromobutyl rubber plunger (not made with natural rubber latex).

Storage and Handling

Prior to reconstitution, the SEVENFACT kit should be stored at room temperature but can be stored between 36°F to 86°F (2°C to 30°C), protected from light in the product package. Do not freeze.
After reconstitution, SEVENFACT should be stored at room temperature but can be stored between 36°F to 86°F (2°C to 30°C), for up to 4 hours. Do not freeze or store in syringes.

---

### Phase 0 / microdosing approaches: time for mainstream application in drug development? [^98d5620b]. Nature Reviews: Drug Discovery (2020). High credibility.

Phase 0 approaches - which include microdosing - evaluate subtherapeutic exposures of new drugs in first-in-human studies known as exploratory clinical trials. Recent progress extends phase 0 benefits beyond assessment of pharmacokinetics to include understanding of mechanism of action and pharmacodynamics. Phase 0 approaches have the potential to improve preclinical candidate selection and enable safer, cheaper, quicker and more informed developmental decisions. Here, we discuss phase 0 methods and applications, highlight their advantages over traditional strategies and address concerns related to extrapolation and developmental timelines. Although challenges remain, we propose that phase 0 approaches be at least considered for application in most drug development scenarios.

---

### Requirements for fault-tolerant factoring on an atom-optics quantum computer [^e49abf65]. Nature Communications (2013). Medium credibility.

Quantum information processing and its associated technologies have reached a pivotal stage in their development, with many experiments having established the basic building blocks. Moving forward, the challenge is to scale up to larger machines capable of performing computational tasks not possible today. This raises questions that need to be urgently addressed, such as what resources these machines will consume and how large will they be. Here we estimate the resources required to execute Shor's factoring algorithm on an atom-optics quantum computer architecture. We determine the runtime and size of the computer as a function of the problem size and physical error rate. Our results suggest that once the physical error rate is low enough to allow quantum error correction, optimization to reduce resources and increase performance will come mostly from integrating algorithms and circuits within the error correction environment, rather than from improving the physical hardware.

---

### Global strategy for asthma management and prevention [^66b4d55d]. GINA (2024). High credibility.

Asthma strategy implementation — essential elements required to implement a health-related strategy enumerate "Steps in implementing an asthma strategy into a health system" that include "Develop a multidisciplinary working group", "Assess the current status of asthma care delivery, outcomes e.g., exacerbations, admissions, deaths, care gaps and current needs", "Select the material to be implemented, agree on main goals, identify key recommendations for diagnosis and treatment, and adapt them to the local context or environment", "In treatment recommendations, consider environmental issues (planetary health) in addition to patient health", "Identify barriers to, and facilitators of, implementation", and "Select an implementation framework and its component strategies". The plan continues with "Develop a step-by-step implementation plan:" to "Select target populations and evaluable outcomes, and specify data coding requirements (if relevant)", "Identify local resources to support implementation", "Set timelines", "Distribute tasks to members", "Evaluate outcomes", and "Continually review progress and results to determine if the strategy requires modification".

---