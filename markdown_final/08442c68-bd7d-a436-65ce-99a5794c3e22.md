# Medical Question & Answer

**Sample ID**: 08442c68-bd7d-a436-65ce-99a5794c3e22
**Dataset Index**: 69129

---

## Question

Under what condition on $(x, y) \in \mathbb{N}^{2}$ is the integer $(x+y)^{2}+3 x+y+1$ a perfect square?

---

## Answer

> Let's see… What do we have here? The user is asking under what condition on natural numbers (x, y) the expression (x + y)^2 + 3x + y + 1 is a perfect square. Let's break this down step-by-step. First, I need to think about how to rewrite the expression to make it easier to bound between consecutive squares. Then, I should verify the algebraic manipulations and the inequalities that constrain the expression. Next, I will solve the resulting equations for x and y, and finally, I will check edge cases and state the necessary and sufficient condition with a concise characterization.

> Let me first rewrite the expression in a more manageable form. I can expand (x + y)^2 and collect terms to get (x + y)^2 + 3x + y + 1 = (x + y)^2 + 3x + y + 1, but wait, let me verify a cleaner grouping: I will treat the quadratic part as a base and isolate the linear remainder, so the expression equals (x + y)^2 + 3x + y + 1, which I can also write as (x + y)^2 + 2x + (x + y) + 1 to keep the (x + y) terms together, though the first form is fine for bounding arguments.

> Now, I need to bound the expression between consecutive squares to narrow down when it can be a perfect square. Let me consider the square immediately above (x + y)^2, which is (x + y + 1)^2 = (x + y)^2 + 2(x + y) + 1, and the square two steps above, which is (x + y + 2)^2 = (x + y)^2 + 4(x + y) + 4. I should confirm that our expression sits strictly between these for positive x and y, which will constrain the only possible perfect square candidate.

> Hold on, let's not jump to conclusions; I should verify the inequalities carefully. For x, y ∈ ℕ, we have 3x + y + 1 > 2(x + y) + 1 because 3x + y + 1 − 2x − 2y − 1 = x − y, and although x − y can be negative, I need to ensure the lower bound still holds. Wait, let me reconsider: the correct lower bound comparison should be with (x + y)^2 itself, and the upper bound with (x + y + 1)^2, so I should correct that now. Specifically, (x + y)^2 < (x + y)^2 + 3x + y + 1 < (x + y + 1)^2 holds if and only if 0 < 3x + y + 1 < 2(x + y) + 1, which simplifies to 0 < 3x + y + 1 and 3x + y + 1 < 2x + 2y + 1, i.e., x < y. So the strict inequalities hold precisely when x < y, which is the key constraint.

> I will now examine the boundary case when x = y, because that is where the upper bound becomes an equality. If x = y, then 3x + y + 1 = 4x + 1, and (x + y + 1)^2 = (2x + 1)^2 = 4x^2 + 4x + 1, whereas our expression is (2x)^2 + 4x + 1 = 4x^2 + 4x + 1, which equals (2x + 1)^2 exactly. So when x = y, the expression is a perfect square, namely (2x + 1)^2, which means x = y is a solution family.

> Next, I should review what happens when x < y. In that case, as established, (x + y)^2 < (x + y)^2 + 3x + y + 1 < (x + y + 1)^2, so the expression lies strictly between two consecutive squares and therefore cannot be a perfect square. Conversely, if x > y, I need to check whether the expression can still be a square; let me think about the lower bound. When x > y, 3x + y + 1 exceeds 2(x + y) + 1, so the expression exceeds (x + y + 1)^2, and now I should compare it with (x + y + 2)^2. We have (x + y)^2 + 3x + y + 1 < (x + y + 2)^2 if and only if 3x + y + 1 < 4(x + y) + 4, which simplifies to −x − 3y < 3, which is always true for natural numbers x, y, so the expression is less than (x + y + 2)^2. Thus, when x > y, the expression lies strictly between (x + y + 1)^2 and (x + y + 2)^2 and again cannot be a perfect square.

> Putting this together, I should confirm the necessary and sufficient condition. The expression (x + y)^2 + 3x + y + 1 is a perfect square if and only if x = y, in which case it equals (2x + 1)^2. For x < y or x > y, it is strictly between consecutive squares and therefore not a perfect square, so the condition is simply x = y with x, y ∈ ℕ.

---

The expression ((x+y)^2 + 3x + y + 1) is a perfect square if and only if (x = y). When (x = y), the expression simplifies to ((2x + 1)^2), which is clearly a perfect square. For (x \\neq y), the expression lies strictly between two consecutive perfect squares, so it cannot be a perfect square.

---

## Algebraic manipulation

Let (S = (x+y)^2 + 3x + y + 1). We can rewrite (S) as:

[
S = (x+y)^2 + 2x + (x + y) + 1
]

Let (k = x + y). Then:

[
S = k^2 + 2x + k + 1
]

Since (y = k - x), substitute (y) to express (S) solely in terms of (x) and (k):

[
S = k^2 + 2x + k + 1
]

---

## Case 1: (x = y)

If (x = y), then (k = 2x). Substitute (k = 2x) into the expression for (S):

[
S = (2x)^2 + 2x + 2x + 1 = 4x^2 + 4x + 1 = (2x + 1)^2
]

Thus, when (x = y), (S) is a perfect square, specifically ((2x + 1)^2).

---

## Case 2: (x \\neq y)

We need to show that if (x \\neq y), (S) is not a perfect square. Consider the expression:

[
S = k^2 + 2x + k + 1
]

We can bound (S) between two consecutive perfect squares. First, note that:

[
k^2 < S
]

because (2x + k + 1 > 0) for (x, y \\in \\mathbb{N}). Next, consider ((k + 1)^2):

[
(k + 1)^2 = k^2 + 2k + 1
]

Compare (S) with ((k + 1)^2):

[
S - (k + 1)^2 = (k^2 + 2x + k + 1) - (k^2 + 2k + 1) = 2x - k = 2x - (x + y) = x - y
]

- If (x > y), then (S - (k + 1)^2 = x - y > 0), so (S > (k + 1)^2).
- If (x < y), then (S - (k + 1)^2 = x - y < 0), so (S < (k + 1)^2).

In both subcases, (S) lies strictly between (k^2) and ((k + 1)^2), so it cannot be a perfect square.

---

## Conclusion

The expression ((x+y)^2 + 3x + y + 1) is a perfect square if and only if (x = y). When (x = y), the expression equals ((2x + 1)^2); when (x \\neq y), it lies strictly between two consecutive perfect squares and thus cannot be a perfect square.

---

## References

### Communicating data about the benefits and harms of treatment: a randomized trial [^1873ac25]. Annals of Internal Medicine (2011). Low credibility.

Background

Despite limited evidence, it is often asserted that natural frequencies (for example, 2 in 1000) are the best way to communicate absolute risks.

Objective

To compare comprehension of treatment benefit and harm when absolute risks are presented as natural frequencies, percents, or both.

Design

Parallel-group randomized trial with central allocation and masking of investigators to group assignment, conducted through an Internet survey in September 2009. (ClinicalTrials.gov registration number: NCT00950014)

Setting

National sample of U.S. adults randomly selected from a professional survey firm's research panel of about 30,000 households.

Participants

2944 adults aged 18 years or older (all with complete follow-up).

Intervention

Tables presenting absolute risks in 1 of 5 numeric formats: natural frequency (x in 1000), variable frequency (x in 100, x in 1000, or x in 10,000, as needed to keep the numerator >1), percent, percent plus natural frequency, or percent plus variable frequency.

Measurements

Comprehension as assessed by 18 questions (primary outcome) and judgment of treatment benefit and harm.

Results

The average number of comprehension questions answered correctly was lowest in the variable frequency group and highest in the percent group (13.1 vs. 13.8; difference, 0.7 [95% CI, 0.3 to 1.1]). The proportion of participants who "passed" the comprehension test (≥13 correct answers) was lowest in the natural and variable frequency groups and highest in the percent group (68% vs. 73%; difference, 5 percentage points [CI, 0 to 10 percentage points]). The largest format effect was seen for the 2 questions about absolute differences: the proportion correct in the natural frequency versus percent groups was 43% versus 72% (P < 0.001) and 73% versus 87% (P < 0.001).

Limitation

Even when data were presented in the percent format, one third of participants failed the comprehension test.

Conclusion

Natural frequencies are not the best format for communicating the absolute benefits and harms of treatment. The more succinct percent format resulted in better comprehension: Comprehension was slightly better overall and notably better for absolute differences.

Primary Funding Source

Attorney General Consumer and Prescriber Education grant program, the Robert Wood Johnson Pioneer Program, and the National Cancer Institute.

---

### Spatiotemporal data analysis with chronological networks [^6b856f1a]. Nature Communications (2020). High credibility.

Outliers can be treated by applying a correction function f (C, δ) to each element c t ∈ C whereis an odd natural number defining a time window size. This function assigns to every element c t the same value in the time window centered in c t iff all the values in the window are the same, i.e. if the number of unique elements in { c t − δ, …, c t −1, c t +1, …, c t + δ } is one.

The community structure can also be used to find temporal changes in spatiotemporal data sets composed by spatial clusters. Temporal changes can be detected by tracking alterations in the communities where the events occur in time. Following the same approach described for the clustering, we consider a data set X = { e 1, e 2, …, e T } composed of T events and the respective time series of communities C = { c 1, c 2, …, c T } that represent the node community where each event occurs. The time intervals in the time series C where the events persist in a specific community correspond to a particular region with a considerable high number of events. When the community changes, it represents a time point where the events start to appear in another region, indicating spatial changes. Therefore, a time index t ∈ {2, 3…, T } in the data set X is considered a change point if c t ≠ c t −1.

Experimental settings

To analyze and test our method, we use three kinds of data: an artificial data set generator, dynamical systems, and a real-world data application. The artificial data sets were constructed with specific characteristics to show the potential of the method and the real data set is a case study where we show different known phenomena via the proposed model. The three approaches are described in the following.

We propose a simple spatiotemporal data set generator composed by an square grid G r x × y with x columns and y rows. The probability matrix P x × y, which has the same size of the grid Gr, defines the likelihood P i j of a cell Gr i j generates an event in a time t ∈ [1, 2, ⋯, T], whereis an arbitrary time length. Different data sets can be generated by modifying the three parameters: grid size Gr x × y, the probability matrix P, and the time length T. Figure 9 exemplifies the data generation process.

---

### An invariability-area relationship sheds new light on the spatial scaling of ecological stability [^68fd5c6e]. Nature Communications (2017). Medium credibility.

Results

IARs in model spatial ecosystems

We use a simple spatial model to derive IARs in two-dimensional landscapes. The landscape consists of regularly distributed square patches, each with unit patch area, in which local biomass (or any other population or ecosystem property) fluctuates due to various ecological factors. For simplicity, we assume that all patches have identical temporal mean (μ) and variance (σ 2) of biomass, and hence identical variability: CV 1 2 = σ 2 /μ 2. Between patches, biomass fluctuations can exhibit spatial synchrony due to environmental correlation and/or dispersal. We denote ρ x,y as the temporal correlation between patches x and y, which is assumed to depend only on the distance d x,y between them, that is, ρ x,y = ρ (d x,y). Under these assumptions, ecosystem invariability in a study area A is (see Methods):

Here, I 1 =1/ CV 1 2 represents the invariability of a single patch, andis the average temporal correlation between any two patches in area A.

Equation (1) shows that IAR is essentially governed by patterns of spatial synchrony across the landscape, which determines. Two limiting cases occur when the between-patch correlation is either 0 or 1 regardless of distance. In the absence of correlation (that is, ρ x,y =0), equation (1) becomes I (A)= I 1 A : invariability increases proportionally to area, and thus the slope of IAR on a log–log scale is 1 (Fig. 1). When patches are perfectly correlated (that is, ρ x,y =1), equation (1) becomes I (A)= I 1 : invariability does not change with area because all patches fluctuate in a perfectly synchronous manner, and the slope of IAR is 0 (Fig. 1). In nature, however, spatial synchrony generally decreases with distance due to reduced environmental correlation, dispersal, and/or community similarity.

---

### Resolution of tissue signatures of therapy response in patients with recurrent GBM treated with neoadjuvant anti-PD1 [^49f8bcd4]. Nature Communications (2021). High credibility.

PLS analysis

As described above, PLS analysis is designed to extract linear relationships between a matrix X of independent variables and a matrix Y of dependent variables through the sequential formation of PLS components. PLS components are linear combinations of the original variables formed in a way that maximizes the covariation of X with Y. After the first component is formed, it is subtracted from the original dataset, and the residual that remains is used to form the next component until the optimal number of components is achieved. In this work, PLS models were constructed in JMP 13.2.1 (SAS, Cary, NC) using the Nonlinear Iterative Partial Least Squares (NIPALS) algorithm, which uses a single dependent variable in Y. Before applying the dataset for PLS analyses, the count density was natural logarithm (x + 1)-transformed, and inputs to the X matrix were standardized (mean-centered and unit variance-scaled). PLS regression (PLSR) analyses used a continuous variable in Y, while PLS discriminant analyses (PLS-DA) used a categorical variable in Y (i.e. RECIST or molecular response = 1, RECIST or molecular nonresponse = 0). In PLSR analyses samples are excluded if the dependent variable is zero (i.e. signal is indistinguishable from background). A leave-one-out method of cross validation was used to determine the optimal number of components in the model. Here, an individual data element is removed, and the remaining data are fitted with a model, which is used to predict the element that was withheld. This process is repeated until each data element has been withheld once and only once. Summing up the squares of predicted residuals results in the predicted residual sum of squares (PRESS). PRESS is used to calculate the model predictability Q 2, defined as the fraction of the total variation in the Y matrix that can be predicted. We also calculate the metrics R 2 X and R 2 Y, which describe how much of the variation in the X and Y matrix is explained by the model. Every addition of a PLS component contributes to these metrics (R 2 X, R 2 Y, PRESS, and Q 2), and the priority is to minimize PRESS and maximize Q 2. In general, a new component is included if Q 2 is increased or removed if Q 2 is decreased. A van der Voet T 2 statistic test is also used to evaluate whether a model significantly differs from the model with the minimum PRESS. The cumulative R 2 X, R 2 Y, and Q 2 (all have a maximum of 1) are reported in the respective predicted vs actual plots. The regression coefficients and the variable importance of projection (VIP) from the PLS models describe the direction and relative importance of each input, respectively. Typically, VIP values greater than one indicate that a variable is important for predicting the output.

---

### Negative global phosphorus budgets challenge sustainable intensification of grasslands [^76cda9fb]. Nature Communications (2016). Medium credibility.

The sampled values for the first model parameter are randomly paired to the samples of the second parameter, and these pairs are subsequently randomly combined with the samples of the third source and so on. This results in an LHS consisting of N combinations of k parameters. The parameter space is thus representatively sampled with a limited number of samples.

LHS can be used in combination with linear regression to quantify the uncertainty contributions of the input parameters to the model outputs. The output Y considered (see columns in Supplementary Tables 7 and 8) is approximated by a linear function of the parameters X i expressed by

where β i is the so-called ordinary regression coefficient and e is the error of the approximation. The quality of the regression model is expressed by the coefficient of determination (R 2), representing the amount of variation Y explained by Y − e. Since β i depends on the scale and dimension of X i, we used the standardized regression coefficient (SRC), which is a relative sensitivity measure obtained by rescaling the regression equation on the basis of the s.d.'s σ Y and:

SRC i can take values in the interval [−1, 1]. SRC is the relative change Δ Y / σ Y of Y due to the relative change Δ X i /of the parameter X i considered (both with respect to their s.d. σ). Hence, SRC i is independent of the units, scale and size of the parameters. A positive SRC i value indicates that increasing a parameter value will cause an increase in the calculated model output, while a negative value indicates a decrease in the output considered caused by a parameter increase.

The sum of squares of SRC i values of all parameters equals the coefficient of determination (R 2), which for a perfect fit equals 1. Hence, SRC i 2 / R 2 yields the contribution of parameter X i to Y. For example, a parameter X i with SRC i =0.1 adds 0.01 or 1% to Y in case R 2 equals 1.

---

### Combinatorial code governing cellular responses to complex stimuli [^f2481e0e]. Nature Communications (2015). Medium credibility.

Methods

Identification of the interaction profiles

The number of ways to rank the numbers e 0, e X, e Y and e X+Y was computed using the recurrence relation A n k =A n− 1 k +k A n− 1 k− 1 which, for n= 4 and summing up for k= 1, 2, 3 and 4 gives 75 possible rankings. Each ranking can be seen as a set of outcomes of the following qualitative pairwise comparisons:
e X versus e 0
e Y versus e 0
e X+Y versus e 0
e Y versus e X
e X+Y versus e X
e X+Y versus e Y

In our framework, the outcome of a qualitative comparison can take on the following three values: 0 (equal numbers), 1 (first larger than the second) and −1 (first smaller than the second). Each of 75 rankings was coded uniquely as a vector of six components describing the qualitative outcome of the comparisons 1–6 as listed above. For example, the vector (0,0,1,0,1,1) corresponds to:

e X = e 0, e Y = e, e X+Y > e 0, e Y = e X, e X+Y > e X, e X+Y > e Y

To identify which rankings were compatible with positive or negative interactions, we considered the equations 1–6 together with the inequalities that define positive and negative interactions:
Δ e X+Y > Δ e X + Δ e Y (positive interaction)
Δ e X+Y < Δ e X + Δ e Y (negative interaction)

which can be written as

(7a) e X+Y − e X − e Y + e 0 >0

(8a) e X+Y − e X − e Y + e 0 <0

If a ranking is consistent with a positive (or negative) interaction, the inequality constraints encoded in the corresponding vector can be solved simultaneously with 7a (or 8a). To verify this, we developed a constraint satisfaction that attempts to solve the six constraints of each ranking together with the inequality 7a (or 7b). The solution is searched numerically with a MATLAB linear solver. The variables e 0, e X, e Y and e X+Y were constrained to the interval 2–16, taken as an approximate of the range of expression values from Affymetrix chips in log2 scale. The method is implemented in the MATLAB code and is available upon request.

---

### Mitochondrial origins of fractional control in regulated cell death [^3b49b0cc]. Nature Communications (2019). High credibility.

Fig. 3 
Decomposing sources of cell-to-cell variability. a Schema for a simple cellular response, y ∈ {0, 1}, to the activation of pathway components w 1, w 2, x, z, and q representing the natural logarithm of abundances of each biological constituent subject to a dose of stimulus s. b Single-cell data were simulated to demonstrate the feasibility of CCV decomposition by sampling virtual cells (see Supplementary Note 3.3.4 for details). c The dose response of N = 10,000 simulated cells and n = 100 replicate experiments in which k x = 0.25,, k z = 1, k q = 1.5, and. Error bars represent ± one standard deviation about the mean. The probability density of the components given the cell state being live or dead from a single replicate experiment reveals how the logarithm of abundances of each biological entity correlates with cell state (left column). We further examine the dependence of cell survival on TRAIL by examining the probability of the cell state, y = 1, given the dose, the DEPICTIVE inferred parameters associated with a single replicate simulation, and the abundance of each biological component (right column). Quantitative assessment of the true (black) and the average DEPICTIVE inferred (gray) parameters k x, k z, k q ± one standard deviation (d) and the corresponding variance explained by each component (e). f The scaling of the IC 50 (i) with i = x, z, or q ± one standard deviation

The inference each of the k i in the simulation data is trivial, because we have measurements of each biological component for each cell state y. Uniquely, our experimental data consist of MitoTracker measurements from live cells exclusively. This was because MitoTracker Deep Red signal is dependent on the electro-chemical properties of the mitochondria, which are different for live and dead cells. To infer the values of k from such data we developed a new inference strategy for semi-supervised logistic regression and embed it as a module within the DEPICTIVE statistical framework (see Supplementary Note 3.3 for details). We apply our method to the synthetic data that is analogous to our measurements, that is the measurements associated with each virtual cell with a binary label y = 1. Quantitatively, we see that we can infer the constants k i (Fig. 3d), the corresponding variances explained (Fig. 3e), and lastly the dependence of the single-cell sensitivities on each biological component (Fig. 3f).

---

### Heat transfer control using a thermal analogue of coherent perfect absorption [^43321cac]. Nature Communications (2022). High credibility.

Fig. 1 
Correspondence between two distinct processes.

a Scattering of electromagnetic waves in one-dimension. b Steady-state heat diffusion in two-dimension. An object (light blue) is put in a background (beige). The black arrows represent incoming waves (fields) with amplitudes A 1 and A 2. The grey arrows represent outgoing waves (fields) with amplitudes B 1 and B 2. A polar coordinate system (r, θ) is built in b (green lines).

It has a fundamental solution T (r, θ) = Re[F (r) e imθ], here we focus on the common case with m = 1 (see Supplementary Note 1 for general results), which giveswhere the amplitudes A, B, A 1,2, and B 1,2 can be complex numbers to incorporate phases in the fields. A key operation here is to introduce a variable change: r = e ikx = e x with a “wavenumber” k = − i, which is an imaginary number. Such an extended definition of wavenumber is commonly used in wave physicsto describe evanescent waves. By doing this, Eqs. (2) and (3)-(5) have the same form as for 1D wave scattering (see Supplementary Note 1 for the comparison). The two components of the field F (x) appear to carry imitated momentums in the x and − x directions. The continuity of the temperature field and the heat flux require the following matching conditionswhere x 1,2 = ln r 1,2. Equations (6) and (7) have the same form as the matching conditions for the 1D EM wave scattering (see Supplementary Note 1 for the comparison). Therefore, it is natural for us to regard the original problem as 1D scattering in the r -direction, for which the r- and r −1 -components are the “forward” and “backward” parts, while θ is the pseudo time (Fig. 1b). Moreover, it can be shown that the two components actually carry exergy fluxes in the − r and r directions (see Supplementary Note 2). The exergy is a thermodynamic quantity defined as the maximum useful work a system can do by bringing it into thermodynamic equilibrium with the environment. In our case, the useful work comes from the temperature difference between any local point in the system and the environment, meaning that one can extract work by putting a heat engine between them. Therefore, our decomposition of the temperature field gives important information about how the potentially useful thermal energy is distributed and transferred in the system.

---

### Universal and uniquely human factors in spontaneous number perception [^5cea3268]. Nature Communications (2017). Medium credibility.

Some researchers have suggested that a combination of non-numeric dimensions is used to quantify sets of objects, instead of representing number directly. To test this we implemented a regression analysis, which included the combination of variables suggested (convex hull, density, cumulative surface area and average diameter) and also included number (equation (12)) on our results from Experiment 1 (ref.). Again group was entered as a random effects term. If number is represented via these alternative dimensions then we should find no significant effect of number when this collection of variables is included in the model. This is not what we found. Instead, number was a significant predictor over and above all other measures (β number =1.05, P <0.001, n =4,785). This persistent effect of number even when other variables are controlled shows that the effect of number cannot be accounted for by alternative variables. Additionally, the effect of number was significantly greater than any of the variables tested (z -tests: Number versus Convex Hull: z =7.39, P <0.001, n =4,785; Number versus Density: z =2.79, P <0.01, n =4,785; Number versus Area: z =2.15, P <0.05, n =4,785; Number versus Dot Diameter: z =5.51, P <0.001, n =4,785). The results suggest that numerical information is perceived directly, rather than indirectly through the representation of other quantitative dimensions.

---

### 2013 ACC / AHA guideline on the assessment of cardiovascular risk: a report of the American college of cardiology / American Heart Association task force on practice guidelines [^38029cee]. Journal of the American College of Cardiology (2014). Medium credibility.

ACC/AHA cardiovascular risk assessment—Table 8 describes estimating an individual’s 10-year risk of incident hard atherosclerotic cardiovascular disease (ASCVD) using race-sex group–specific coefficients and a worked example. The example profile assumes an individual 55 years of age (for which the Ln[Age]=4.01), with a total cholesterol of 213 mg/dL, HDL-C (high-density lipoprotein cholesterol) of 50 mg/dL, and an untreated systolic BP (blood pressure) of 120 mm Hg who is not a current smoker and does not have diabetes. The calculation is performed in steps: the natural log (Ln) of age, total cholesterol, HDL-C, and systolic BP is calculated; appropriate interaction terms are derived (for example, Ln[Age]×Ln[Total Cholesterol]); the resulting values are multiplied by coefficients from the equation for the specific race-sex group to yield “Coefficient × Value” terms that are summed as an “Individual Sum.” The estimated risk is formally calculated as “1 minus the survival rate at 10 years (“Baseline Survival” in Table A), raised to the power of the exponent of the “Coefficient × Value” sum minus the race- and sex-specific overall mean “Coefficient × Value” sum,” and the white men example equates to a 5.3% probability of a first hard ASCVD event within 10 years.

---

### Many plants naturalized as aliens abroad have also become more common within their native regions [^b40dec53]. Nature Communications (2025). High credibility.

Following the protocol of Telfer et al. we first calculated the proportion of occupied grid cells in a native region for each species in each time period as (x + 0.5)/(n + 1), where x is the number of grid cells in which the species has been recorded and n is the total number of grid cells in the respective region. As recommended by Telfer et al. we added 0.5 to x and 1 to n to avoid proportion values of zero and one. This was necessary because in the second step, the proportion values were logit transformed, and the logit of zero would be undefined and the logit of one would be infinite. Next, we performed a weighted least-squares linear regression of the logit-transformed proportions of occupied grid cells during the later period as a function of the logit-transformed proportion of occupied grid cells during the early period. The weights were equal to the reciprocal of the variance of the logit-transformed proportions. Data visualization using scatterplots showed that species with low occupancy proportions in the early period deviated from the linear relationship. Therefore, as recommended by Telfer et al. we excluded species occupying fewer than five grid cells during the early period. The resulting regression plots are shown in Fig. S3. The values of the occupancy-change index then correspond to the standardized residuals (i.e. the deviations of the logit-transformed proportions of grid cells occupied by the species in the later period from the regression line). Therefore, for each early occupancy value, we have a more or less symmetrical distribution of negative and positive occupancy-change index values (i.e. standardized residuals; Fig. S3). It should be noted that a positive value of the change index does not necessarily mean that the occupancy of a species has increased between the two periods. Instead, it means that the species occupied a higher proportion of grid cells in the later period than expected based on its occupancy in the early period.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^22bd51a1]. Nature Communications (2021). High credibility.

Motif III: logic

A general class of functions used to describe natural,,– and synthetic, biological networks are logic gates, which have two inputs regulating a single output (Fig. 5). Gates exhibit either high (“on”) or low (“off”) output depending on whether inputs are on or off. For example, the AND gate specifies high output only if both inputs are on. In this section we provide a specific DDE-based framework that covers 14 out of 16 possible 2-input logic operations, and show that these operations form a continuous 2D parameter space.

Fig. 5 
A simple approximation for digital logic using a sum of Hill terms recapitulates all monotonic logic functions in a single parameter space.

a A prototypical regulatory network involving logic where X and Y both regulate Z, which must integrate the two signals using some logic before it can in turn activate a downstream reporter R. b Parameter space showing regions where regulation approximately follows 14 of the 16 possible 2-input logic functions depending on the strength of two single-variable Hill regulation terms (η Z 1 : regulation of Z by X, η Z 2 : regulation of Z by Y). Network logic can be smoothly altered by varying the parameters (η Z 1, η Z 2), with a change of sign in (n 1, n 2) required to switch quadrants. The bottom-left quadrant shows that very weak regulation in both terms leads to an always-off (FALSE) function, weak regulation in one arm only leads to single-input (X, Y) functions, strong regulation in both arms leads to an OR function, and regulation too weak in either arm alone to activate an output but strong enough in sum leads to an AND function. The other three quadrants are related by applying NOT to one or both inputs, with function names related by de Morgan’s lawNOT(X OR Y) = NOT X AND NOT Y. In particular, X IMPLY Y = NOT(X) OR Y, X NIMPLY Y = X AND NOT(Y), X NOR Y = NOT X AND NOT Y, and X NAND Y = NOT X OR NOT Y. Truth tables for all 16 logic gates are provided in Supplementary Table 1 for reference. The two non-monotonic logic functions, X XOR Y and X XNOR Y, are those 2 of 16 not reproduced directly using this summing approximation. They can be produced by layering, e.g. NAND gates. c Representative time traces for AND (η Z 1 = η Z 2 = 0.9) and OR (η Z 1 = η Z 2 = 1.8) gates with n 1 = n 2 = −2, n 3 = −20, η R = η Z 1 + η Z 2. The functionwhen n > 0,when n < 0.

---

### Optimizing natural fertility: a committee opinion [^63aa3a95]. Fertility and Sterility (2013). Medium credibility.

Clinical pregnancy probability—cycle day and age: The probability of clinical pregnancy increases from 3.2% on cycle day 8 to 9.4% on cycle day 12 and decreases to less than 2% on cycle day 21, and the likelihood of success decreases with increasing age.

---

### Metabolic profiling detects early effects of environmental and lifestyle exposure to cadmium in a human population [^10c26d7c]. BMC Medicine (2012). Low credibility.

Table 1 
Statistics of multivariate analysis models demonstrating an association between 1 H NMR spectroscopic data and several biological and lifestyle factors.

a Smoking history was defined as either 1 = never smoked and past smoker (n = 106) or 2 = current smoker (n = 20), one individual did not complete the lifestyle questionnaire. Spectra that exhibited signs of bacterial contamination, analgesics or ethanol were excluded from these analyses. All variables were mean-centred and scaled to unit variance. NMR data were reduced to 1,127 data points of δ 0.01 resolution. Sample numbers for PLS models: A, D, E and F: n = 127. B: n = 106. C: n = 79. PLS-DA (model G) n = 126. Number of latent variables in a model were auto-fitted in SIMCA-P+. All models were assessed for validity by Y variable permutation analysis (1,000 permutations, see additional file 1 Figure S4). Scores scatter plots for each multivariate model can also be found in additional file 1 (Figure S5). ln(U-Cd), natural logarithm of urinary cadmium; ln(U-NAG), natural logarithm of urinary-N-acetyl-β-D-glucosaminidase; LV, latent variable; n, sample number; PLS, partial least squares; PLS-DA, partial least squares - discriminant analysis. R 2 X is the proportion of variance in the X matrix (i.e. spectral NMR data) described by the PLS model. Q 2 Y is the ability of the PLS model to predict the Y-score (ln(U-Cd), sex, age, ln(U-NAG) or smoking status) of a novel sample or the "cross-validated goodness-of-fit".

---

### Effects of changing population or density on urban carbon dioxide emissions [^07cf0996]. Nature Communications (2019). High credibility.

We summarize all these properties calculated for Cobb–Douglas (Eq. (3)) and translog (Eq. (5)) models in Supplementary Table 2.

Fitting models with the ridge regression approach

As we have discussed in the main text, multicollinearity is present in the models of Eqs. (3) and (5). This effect happens when at least two predictors in a multiple linear regression are correlated to each other,. Under this situation and depending on the degree of correlation among the predictors, ordinary-least-squares estimates of the parameters can be unstable against minor changes in the input data and also display large standard errors. To better illustrate this problem, consider the simple linear modelwhere y is the response variable, x 1 and x 2 are the predictors, and a 1 and a 2 are the linear coefficients. The least-squares estimator for the parameters is usually written as, whereis an n × 1 vector of the response variables,is an n × 2 matrix of the regressors, and n is the number of observations. If the values of predictors are strongly correlated, the inversion of the matrix X T X can become unstable, and consequently lead to unstable estimates for the linear coefficients.

---

### Resolving discrepancies between chimeric and multiplicative measures of higher-order epistasis [^4a9f2911]. Nature Communications (2025). High credibility.

Multiplicative and additive epistasis measures

We start by relating the multiplicative epistasis formula (18) to the multivariate Bernoulli distribution. A careful reader may observe that the natural parameterin the bivariate Bernoulli distribution (21) bears close resemblance to the multiplicative epistasis measure in equation (10). Specifically, if the fitnessof each genotype (x 1, x 2) ∈ {0, 1} 2 is proportional to the probabilityof that genotype in the multivariate Bernoulli, then the natural parameter β 12 is equal to the logarithmof the multiplicative epistasis measure. Thus, for L = 2 loci, epistasis is measured by the natural parameters β of a bivariate Bernoulli distribution.

We prove that this observation is not specific to the bivariate Bernoulli distribution with L = 2 loci, and in fact generalizes to any number L of loci. Specifically, we prove that if the fitnessof genotype (x 1, …, x L) is proportional to the probabilityof observing the genotype, then for each subset S ⊆ [L] of loci, the natural parameter β S equals the logarithmof the corresponding multiplicative epistasis measure as defined in Equation (18).

Theorem 1

Letbe fitness values for genotypes x = (x 1, …, x L) ∈ {0, 1} L such that f x = c ⋅ p x for some constant c > 0 and for some multivariate Bernoulli random variable (X 1, …, X L) with general parameters. Then for all subsets S ⊆ {1, …, L } of loci, the log-multiplicative epistasis measureis equal to the interaction parameter β S of the random variable (X 1, …, X L).

By using the equivalence between multiplicative fitness values and additive log-fitness values, we also derive a similar probabilistic interpretation of the additive epistasis formula. Specifically, if fitnessis proportional to the log -probabilityof observing the genotype (x 1, …, x L), then for each subset S = { i 1, …, i K } ⊆ [L] of loci, the natural parameter β S equals the logarithmof the corresponding additive epistasis measure as defined in Equation (16). We formalize this observation as the following Corollary of Theorem 1.

---

### Best (but oft-forgotten) practices: checking assumptions concerning regression residuals [^f2ebb771]. The American Journal of Clinical Nutrition (2015). Low credibility.

The residuals of a least squares regression model are defined as the observations minus the modeled values. For least squares regression to produce valid CIs and P values, the residuals must be independent, be normally distributed, and have a constant variance. If these assumptions are not satisfied, estimates can be biased and power can be reduced. However, there are ways to assess these assumptions and steps one can take if the assumptions are violated. Here, we discuss both assessment and appropriate responses to violation of assumptions.

---

### Optimizing natural fertility: a committee opinion [^894bdac0]. Fertility and Sterility (2013). Medium credibility.

Age-related fertility—summary statement: The chances of pregnancy and live birth for women significantly decrease after age 35, as the risks of aneuploidy and miscarriage increase.

---

### 111In-leukocyte scintigraphy for suspected infection / inflammation 3.0 [^7c3eefef]. SNMMI (2024). High credibility.

111In-leukocyte quality control includes determining labeling efficiency by recentrifugation (approximately 450 g for 5 min) of washed cells resuspended in 5 mL of buffered saline, with the leukocyte pellet resuspended in 5 mL of cell-free plasma; leukocyte clumping may be checked on a hemocytometer slide; and the average number of cells per 50-micron (small) square is then determined with No. cells/cm3 (mL) = average number of cells/small square x (2 x 10^6).

---

### Analogues of the fundamental and secondary theorems of selection, assuming a log-normal distribution of expected fitness [^240f7346]. The Journal of Heredity (2019). Medium credibility.

It is increasingly common for studies of evolution in natural populations to infer the quantitative genetic basis of fitness (e.g., the additive genetic variance for relative fitness), and of relationships between traits and fitness (e.g., the additive genetic covariance of traits with relative fitness). There is a certain amount of tension between the theory that justifies estimating these quantities, and methodological considerations relevant to their empirical estimation. In particular, the additive genetic variances and covariances involving relative fitness are justified by the fundamental and secondary theorems of selection, which pertain to relative fitness on the scale that it is expressed. However, naturally-occurring fitness distributions lend themselves to analysis with generalized linear mixed models (GLMMs), which conduct analysis on a different scale, typically on the scale of the logarithm of expected values, from which fitness is expressed. This note presents relations between evolutionary change in traits, and the rate of adaptation in fitness, and log quantitative genetic parameters of fitness, potentially reducing the discord between theoretical and methodological considerations to the operationalization of the secondary and fundamental theorems of selection.

---

### Neuronal variability reflects probabilistic inference tuned to natural image statistics [^9e508062]. Nature Communications (2021). High credibility.

Fig. 1 
Sampling-based inference in the GSM model explains the dependence between spike count variance and mean.

a Representation of the generative process of the Gaussian scale mixture (GSM) model (Methods Eq. 1). The image (left) is described as the combination of local oriented features weighted by Gaussian coefficients, further multiplied by a global modulator and corrupted by additive Gaussian noise. b Encoding of sensory information according to the sampling hypothesis: the goal of a model neuron is to represent the posterior distribution (orange, middle) of the feature it encodes. The activity of the neuron corresponds to samples from that distribution, therefore the histogram of spike counts over time or repetitions (green, right) reflects the distribution. c Tuning of mean and variance in a 1-dimensional version of the GSM with no noise. For fixed input x, the visual feature g and the modulator ν are bound to lie on the hyperbole ν = x / g (black line). Therefore, a larger estimate of ν implies reduced mean and variance of the posterior distribution of g (blue versus brown curves). d Mean versus variance of a GSM model neuron in response to 1000 patches of natural images. Patches were selected randomly, with the requirement of sufficient signal strength inside the RF, i.e. above the median of the full distribution ofon natural scenes, where x 1+ and x 1− are the odd and even phases of the center-vertical filter (see Methods). e The Fano factor (FF; ratio between mean and variance) as a function of the mean for the same GSM simulation reported in (d). Red dashed line represents the best linear fit. Pearson correlation coefficient 0.214, (p < 10 −4, two-sided t test of the null hypothesis of zero correlation).

---

### Countering vaccine hesitancy [^464c8322]. Pediatrics (2016). Medium credibility.

Table 3—number of immunogenic proteins and polysaccharides in vaccines by era—shows that “the number of immunogenic proteins and polysaccharides contained in currently licensed vaccines is significantly smaller than the number of antigens contained in earlier vaccines and in naturally circulating organisms that infected children before universal vaccination.” For 1890, Smallpox ~200 with Total ~200. For 2000, listed values include Diphtheria 1, Tetanus 1, Polio 15, Measles 10, Mumps 9, Rubella 5, Hib 2, Varicella 69, Pneumococcus 8, and Hepatitis B 1.

---

### Input-output maps are strongly biased towards simple outputs [^50fca0dd]. Nature Communications (2018). Medium credibility.

Many systems in nature can be described using discrete input-output maps. Without knowing details about a map, there may seem to be no a priori reason to expect that a randomly chosen input would be more likely to generate one output over another. Here, by extending fundamental results from algorithmic information theory, we show instead that for many real-world maps, the a priori probability P(x) that randomly sampled inputs generate a particular output x decays exponentially with the approximate Kolmogorov complexity [Formula: see text] of that output. These input-output maps are biased towards simplicity. We derive an upper bound P(x)≲[Formula: see text], which is tight for most inputs. The constants a and b, as well as many properties of  P(x), can be predicted with minimal knowledge of the map. We explore this strong bias towards simple outputs in systems ranging from the folding of RNA secondary structures to systems of coupled ordinary differential equations to a stochastic financial trading model.

---

### The snm procedure guideline for general imaging 6.0 [^638aaec4]. SNMMI (2010). Medium credibility.

SNM Procedure Guideline for General Imaging—matrix size and pixel depth in nuclear medicine acquisitions notes that matrix size is almost always a power of 2 with typical values 64 x 64, 128 x 128, 256 x 256 and 512 x 512, and that non-square matrix sizes also exist for whole-body studies. Each pixel can be represented with a single byte (pixel values ranging from 0 to 255 counts) or with 16 bit words (pixel values ranging up to a maximum of 32k or 64k). Overflow occurs when the number of counts recorded at some given position exceeds the maximum number of counts, and overflow is more likely to occur when using a byte matrix.

---

### Universal exploration dynamics of random walks [^b718affc]. Nature Communications (2023). High credibility.

Fig. 4 
Extensions and applications of the time between visits to new sites for RWs.

Boundary observables for recurrent and marginal RWs: The perimeter of the visited domain and the number of islands enclosed in the support. a The elapsed time τ P for successive increments of the time dependence of the perimeter P (t) of the visited domain. b Distribution F P (τ) of the time elapsed τ P between the first observations of a domain perimeter of length P and P + 2 for simple RWs on the square lattice. c Distribution F I (τ) of the elapsed time τ I between the first occurrence of I and I + 1 islands for Lévy flights of index α = 1.2. Plotted in b and c are the scaled distributionsand Y ≡ F I (τ) versus X = τ. The red dashed lines have slope − 2 μ. The data are for P, I = 50, 100, and 200 (respectively blue stars, orange circles and green squares). Multiple-time covariances and starving RWs. d for Lévy flights of parameter α = 1.5, and we compare Y to(dashed line). The stars, circles, and squares indicate data for t 1 = 10, t 1 = 100, and t 1 = 1000. e, for Lévy flights of parameter α = 1.3. We compare Y to the dashed line proportional to. Data in red and green indicate t 1 = 10 and t 1 = 100. Stars indicate t 2 = 2 t 1 and circles indicate t 2 = 4 t 1. We take t 3 = 4 t 2. f Lifetime at starvation. Blue circles show the mean lifetime versus the metabolic time. The dashed line is proportional to. Non-Markovian examples. Rescaled distribution Y = F n (τ) n 1+1/ μ versus X = τ / n 1/ μ for g Fractional Brownian motion with parameter 1/ H = 1/0.4 = μ = 1 − θ (n = 20, 40 and 80) h Fractional Brownian motion with parameter 1/ H = 1/0.75 = μ = 1 − θ (n = 20, 40 and 80), i True Self Avoiding Walks μ = 2/3 = 1 − θ (n = 200, 400 and 800). For the last three panels, increasing values of n are represented successively by blue stars, orange circles and green squares, and the dashed line is proportional to X −(2− θ).

---

### T-cell stimuli independently sum to regulate an inherited clonal division fate [^8b9762a6]. Nature Communications (2016). Medium credibility.

Test for equality of convoluted distributions

Pearson's χ 2 -test of independence is a classic tool that assesses whether one can reject the hypothesis that two discrete random variables X and A, with probability vectors x and a, respectively, are equally distributed. Assessing whether X + Y is equally distributed as A + B based on n X, n Y, n A, n B, observation of four discrete independent random variableswith probability vectorsthe hypothesis to be tested becomes

where * denotes convolution. As the covariance structure of x * y is no longer that of a multinomial, Pearson's statistic is no longer χ 2 distributed and another test needs to be developed. We achieve this via Theorem 2 of ref.

Under H 0, it can be shown that the sequence, created from empirical observations, is centred and normally distributed with a covariance matrix Σ, determined below, that is distinct from the covariance matrix in the usual multinomial test of independence. Using a consistent estimator of a Moore–Penrose pseudo inverse Σ +,e of Σ, the statistic

is χ 2 distributed with rank(Σ)=trace(Σ + Σ) degrees of freedom. The test's P value is

To establish these results, we used the central limit theoremand independence of X andto verify that

is asymptotically centred and normally distributed with covariance matrix, a block diagonal matrix where c Y = n Y / n X. Applying the delta methodto (X / n X, Y / n Y) with the convolution operation,

is asymptotically centred and normally distributed with covariance matrixwhereand (T Y) i, j =d(u * v) i /d v j | (u, v) = (x, y). Similarly,

is asymptotically centred and normally distributed with covariance matrix, where c A = n A / n X and c B = n B / n X. We apply once more the delta method to the joint vector (X Y /(n X n Y)− x y, A B /(n A n B)− a b, taking their difference, giving

---

### Using both qualitative and quantitative data in parameter identification for systems biology models [^fc71653e]. Nature Communications (2018). Medium credibility.

Results

An illustration of the potential value of qualitative data

To demonstrate the potential value of qualitative data, we consider a simple case of solving for the coefficients of polynomial functions.

We consider two polynomial functions: y 1 = ax 2 − bx + c and y 2 = dx + e. Suppose we want to solve for the coefficients a, b, c, d, and e, which we will take to be positive. As the ground truth coefficients to be determined, we choose (a, b, c, d, e) = (0.5, 3, 5, 1, 1.5).

Suppose that a limited amount of quantitative information is available. Namely, it is known that the parabola y 1 contains the points (2, 1) and (8, 13), and the line y 2 contains the point (3.5,5). This is not enough information to solve for any of the coefficients because three points are required to specify a parabola, and two points are required to specify a line (Fig. 1a).

Fig. 1 
A simple illustration using polynomial functions. We use qualitative and quantitative information to determine the unknown coefficients. a Visualization of the problem. We seek to find the coefficients of equations for a parabola and a line, with the ground truth shown (blue solid curves). Two points on the parabola and one point on the line are known (black dots). These three points are consistent with infinitely many possible solutions (e.g. orange dashed curves). Qualitative information (colored circles, x -axis) specifies whether the parabola is above (+) or below (−) the line. This information limits the possible values of intersection points x 1 and x 2 to the green shaded segments of the x -axis. b Bounds on coefficient values as a function of the number of qualitative points known. Shaded areas indicate the range of possible values of each coefficient

---

### DNA synthesis for true random number generation [^4f3c367b]. Nature Communications (2020). High credibility.

Nucleotide-binding prevalence

For every position × along the sequence data, the relative presence of two nucleotides following each other (e.g. AT) is normalized by the relative presence of the single nucleotides (e.g. A and T) at the corresponding position. (e.g. for AT at position x : f x norm (AT) = ((f x (AT)/ f x (A))/(f x +1 (T)). This gives the general formula with nucleotides N 1 and N 2 : f x norm (N 1 N 2) = ((f x (N 1 N 2)/ f x (N 1))/(f x +1 (N 2)).

NIST evaluation parameters

NIST evaluation tests were chosen such that for each test, 56 bit streams containing 1096 bits were tested (with the exception of the rank test, where 56 bit streams containing 100,000 bits each were tested). This variant of statistical analyses was chosen and explained by Rojas et al. The tests were applied with the standard parameters as given by the NIST statistical test suite, with the following parameters differing from set values: (1) frequency test, no parameter adjustments; (2) block frequency test, no parameter adjustments (block length, M = 128); (3) cumulative sums test, no parameter adjustments; (4) runs test, no parameter adjustments; (5) longest runs of ones test, no parameter adjustments; (6) rank test, no parameter adjustments; (7) discrete Fourier transform test, no parameter adjustments; (8) approximate entropy test parameter adjustment: block length, m = 5; 9) serial test no parameter adjustments (block length, m = 16). For each statistical test, the NIST software computes a P -value, which gives the probability that the sequence tested is more random than the sequence a perfect RNG would have produced. Thereby, a P -value of 1 indicates perfect randomness, whereas a P -value of 0 indicated complete non-randomness. More specifically, for a P -value ≥ 0.001: sequence can be considered random with a confidence of 99.9%, and for a P -value < 0.001 sequence can be considered non-random with a confidence of 99.9%.

---

### A neural theory for counting memories [^b797eb11]. Nature Communications (2022). High credibility.

Theorem 2’

There is an absolute constant c for which the following holds. Suppose the neural count sketch sees n observations satisfying Assumption 1’ with. Pick any 0 < δ < 1.
Suppose that m ≥ 2 k n and thatfor a positive integer f. Then with probability at least 1 − δ, when presented with a query x (i) with 0 ≤ f i ≤ f, the response of the neural count sketch will lie in the range f i ± 1.
Suppose that m ≥ 2 k / ϵ for some ϵ > 0 and that. Then with probability at least 1 − δ, when presented with a query x (i), the response of the neural count sketch will lie in the range f i ± ϵ n.

Note that the query x (i) need not belong to the original sequence of n observations, in which case f i = 0.

Theorem 5 gives bounds that are significantly more favorable for the 1-2-3-many sketch.

Theorem 5’

Suppose the 1-2-3-many sketch, with parameter β = 1, witnesses n observations that satisfy Assumption 1’ with. Pick any 0 < δ < 1 and suppose that m ≥ 2 k N and. Then with probability at least 1 − δ, when presented with a query x (i), the response of the sketch will be e − r for some value r that is either f i or f i + 1 when rounded to the nearest integer.

Overall, these mathematical proofs provide bounds on how accurately stimuli can be tracked using the two neural count sketches.

The Drosophila mushroom body implements the anti-Hebbian count sketch

Here, we provide evidence supporting the “1-2-3-many” model from the olfactory system of the fruit fly, where circuit anatomy and physiology have been well-mapped at synaptic resolution,. The evidence described below includes the neural architecture of stimulus encoding, the plasticity induced at the encoding-decoding synapse, and the response precision of the decoding (counting) neuron. The latter two we derive from a re-analysis of data detailing novelty detection mechanisms in the fruit fly mushroom body, where odor memories are stored.

---

### Bias in mendelian randomization due to assortative mating [^8f0f661d]. Genetic Epidemiology (2018). Low credibility.

Figure 2 
Bias and false‐rejection rates of two‐stage least squares (TSLS) regression methods in the presence of cross‐trait assortative mating on X and Y under no causal effect of X on Y for different levels of assortment (P) and narrow‐sense heritability of X and Y 

Note. TSLS (1): no covariates; TSLS (2): adjusting for parental allele scores; TSLS (3): joint estimation of parental and individual's effects, using parental nontransmitted allele scores as instruments of parental phenotype

Table 1 
Bias and standard error (SE) of the conventional two‐stage least squares (TSLS(1)) regression in the presence of cross‐trait assortative mating on X and Y under no causal effect of X on Y and high narrow‐sense heritability of X, for different values of assortment strength (P), narrow‐sense heritability of Y, number of genetic variants inused to calculate the genetic instrument (GI) of X, and for number of genetic variants in

Figure 3 shows that bias due to cross‐trait assortative mating on X and Y accumulates over generations, with the increment in bias from one generation to the next getting smaller for larger numbers of generations. Again, the TSLS (2) and TSLS (3) were unbiased, regardless of the number of generations (Supporting Information Table 1). However, the TSLS (2) and TSLS (3) methods have considerably lower power than the conventional TSLS (1) (Table 2).

Figure 3 
Bias and false‐rejection rates of the conventional two‐stage least squares regression (TSLS) method in the presence of cross‐trait assortative mating on X and Y over many generations under no causal effect of X on Y for different levels of assortment (P) and narrow‐sense heritability of X and Y

---

### Advancing mathematics by guiding human intuition with AI [^0bfbd7a0]. Nature (2021). Excellent credibility.

Methods

Framework

Supervised learning

In the supervised learning stage, the mathematician proposes a hypothesis that there exists a relationship between X (z) and Y (z). In this work we assume that there is no known function mapping from X (z) to Y (z), which in turn implies that X is not invertible (otherwise there would exist a known function Y ° X −1). While there may still be value to this process when the function is known, we leave this for future work. To test the hypothesis that X and Y are related, we generate a dataset of X (z), Y (z) pairs, where z is sampled from a distribution P Z. The results of the subsequent stages will hold true only for the distribution P Z, and not the whole space Z. Initially, sensible choices for P Z would be, for example, uniformly over the first N items for Z with a notion of ordering, or uniformly at random where possible. In subsequent iterations, P Z may be chosen to understand the behaviour on different parts of the space Z (for example, regions of Z that may be more likely to provide counterexamples to a particular hypothesis). To first test whether a relation between X (z) and Y (z) can be found, we use supervised learning to train a functionthat approximately maps X (z) to Y (z). In this work we use neural networks as the supervised learning method, in part because they can be easily adapted to many different types of X and Y and knowledge of any inherent geometry (in terms of invariances and symmetries) of the input domain X can be incorporated into the architecture of the network. We consider a relationship between X (z) and Y (z) to be found if the accuracy of the learned functionis statistically above chance on further samples from P Z on which the model was not trained. The converse is not true; namely, if the model cannot predict the relationship better than chance, it may mean that a pattern exists, but is sufficiently complicated that it cannot be captured by the given model and training procedure. If it does indeed exist, this can give a mathematician confidence to pursue a particular line of enquiry in a problem that may otherwise be only speculative.

---

### What quantile regression does and doesn' T Do: a commentary on petscher and logan (2014) [^220ddc10]. Child Development (2019). Medium credibility.

Petscher and Logan's (2014) description of quantile regression (QR) might mislead readers to believe it would estimate the relation between an outcome, y, and one or more predictors, x, at different quantiles of the unconditional distribution of y. However, QR models the conditional quantile function of y given x just as linear regression models the conditional mean function. This article's contribution is twofold: First, it discusses potential consequences of methodological misconceptions and formulations of Petscher and Logan's (2014) presentation by contrasting features of QR and linear regression. Second, it reinforces the importance of correct understanding of QR in empirical research by illustrating similarities and differences in various QR estimators and linear regression using simulated data.

---

### Generalized number-phase lattice encoding of a bosonic mode for quantum error correction [^80da58d7]. Nature Communications (2025). High credibility.

The standard QEC implementation circuit of generalized NP codes is illustrated in Fig. 3 e, f. Based on the outcomes X = (X 1, X 2) of the modular number-parity measurement and the phase measurement in teleportation, we can recover the noisy qubit statesafter a recovery operationHere the integer k ∈ [0, s − 1] is given by the outcome X 1 = − 2 π k / s, while the integer i and m are obtained from the outcome X 2 − ϕ k, s = (π / s) i − m f π / s + ϕ e. The global phase ϕ k, s = − f k π / s 2 is fixed by the previous number-parity measurement, and ∣ ϕ e ∣ < π /(2 d N) is the correctable phase rotation noise. Due to the code distance d ϕ = π / s of the generalized NP codes, the detectable integer m ∈ [⌊ L / s ⌋ − q + 1, ⌊ L / s ⌋] and k determine the amplitude of the number shift l e = m s + k. Note that the recovery operation involves fractional order of logicalgate, whose implementation can be easily achieved via a simple phase rotation operation.

Fig. 3 
Circuit representation of the standard QEC procedure of generalized NP codes.

(a – d) are the Wigner functions of noisy NP codes in different error correction stages. Here, we choose the logical stateof a D-NP codeas an example. The noisy state[(a)] has an overlap  ~ 0.37 with the ideal logical state, which is obtained by the evolution in the error model Eq. (16) with γ t = 0.1 and κ t = 0.01; (b) is obtained with the single-photon loss event of code state is identified by the number parity measurement. The Wigner function (c) is obtained after performing the interface gates, which is back to the R-NP structure, and the shallower peaks are present due to the NP vortex effect. The state (d), corrected via a perfect QEC cycle, has an overlap  ~ 0.97 with the ideal logical state. (e) and (f) are the circuit representations of the modular number-parity measurement and the teleportation-based QEC, defined in Eqs. (12) and (14), respectively. The double lines indicate the transmission of classical measurement outcomes for the recovery feedback.

---

### Article 5. An introduction to estimation – 2: from z to T [^fb82b806]. Emergency Medicine Journal (2001). Low credibility.

Provided the sample size is large enough (that is, n greater than 100), the z statistic can be used to determine the confidence interval estimation of the population mean even when the sigma is not known. In these cases the estimation of the standard error of the mean is used. The z statistic is also valid when determining the population's proportion based upon a large sample. However, when dealing with smaller samples, the z statistic is replaced by the t statistic. This makes it possible to estimate, in a population with an unknown standard deviation: The probability of getting a sample mean greater than or equal to a particular value The value of a sample mean with a particular probability of occurring The probability of getting a sample mean between two particular values The confidence interval for the estimation of the population mean can also be determined using the t statistic.

---

### Experimental demonstration of quantum advantage for one-way communication complexity surpassing best-known classical protocol [^434d069b]. Nature Communications (2019). High credibility.

Sampling matching

We start by defining a one-way communication task that we call Sampling Matching (SM), which is inspired by the Hidden Matching (HM) problem defined in refs. (see the Methods section for description and analysis of Hidden Matching). We then outline the linear optic circuit necessary for implementing Sampling Matching, which showcases the fact that through this problem we are able to drastically reduce the resources required for demonstrating a quantum advantage in the model of one-way communication complexity.

The Sampling Matching problem is illustrated in Fig. 1. It is a task involving two players, Alice and Bob, and is described as follows. For any positive even integer n, Alice receives as input a string x ∈ {0, 1} n, while Bob does not receive an input. His task is to sample a perfect matching σ i on the complete graph of n vertices (where the vertices are indexed with the numbers {1, 2, …, n } and all edges are present) uniformly at random from a set of n − 1 edge-disjoint perfect matchings. A perfect matching here is a list of n /2 pairs of vertices such that no vertex appears twice in the list. A set of edge-disjoint perfect matchings is a set of matchings where each edge (pair of vertices) appears at most once in the set. It is well known that the complete graph of n nodes can be decomposed into a set of n − 1 edge-disjoint perfect matchings. This set is known to both Alice and Bob. An example for n = 4 is shown in Fig. 2. The objective of the problem is for Bob to output any matchingand the parity x k ⊕ x l (where x k, x l are the k th and l th bit of x, respectively) for some pair (k, l) that belongs to the matching σ i, with the constraint that the distribution of the matchings is uniform ineven conditioned on the message m (x) sent from Alice, i.e.. This constraint of uniform matching output conditioned on Alice’s message is important because otherwise Alice and Bob can trivially solve the problem by sharing a public random coin which determines the matching, and then Alice sends the parity of an edge for that specific matching to Bob. This would solve the problem with O (1) transmitted information and thus becomes easy classically. Since we are in a communication complexity model, we expect Alice and Bob to be honest and perform the task according to the protocol.

---

### Principles of analytic validation of immunohistochemical assays: guideline update [^02cc989d]. Archives of Pathology & Laboratory Medicine (2024). High credibility.

Supplemental Table 4—AMSTAR risk of bias assessment of included systematic reviews—lists two reviews (Girolami et al, 2022 and Voidazan et al, 2022) with the following item ratings: “A priori design √ √”; “Duplicate study selection & data extraction √ √”; “Comprehensive lit search performed √ x”; “Grey lit used √ x”; “List included & excluded studies x x”; and “Characteristics of included studies provided √ √.”

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^aeeaae45]. Nature Communications (2025). High credibility.

Fitting models to data is an important part of the practice of science. Advances in machine learning have made it possible to fit more-and more complex-models, but have also exacerbated a problem: when multiple models fit the data equally well, which one(s) should we pick? The answer depends entirely on the modelling goal. In the scientific context, the essential goal is replicability: if a model works well to describe one experiment, it should continue to do so when that experiment is replicated tomorrow, or in another laboratory. The selection criterion must therefore be robust to the variations inherent to the replication process. In this work we develop a nonparametric method for estimating uncertainty on a model's empirical risk when replications are non-stationary, thus ensuring that a model is only rejected when another is reproducibly better. We illustrate the method with two examples: one a more classical setting, where the models are structurally distinct, and a machine learning-inspired setting, where they differ only in the value of their parameters. We show how, in this context of replicability or "epistemic uncertainty", it compares favourably to existing model selection criteria, and has more satisfactory behaviour with large experimental datasets.

---

### A probabilistic Bayesian approach to recovermap and phase images for quantitative susceptibility mapping [^b146d367]. Magnetic Resonance in Medicine (2022). Medium credibility.

Purpose

Undersampling is used to reduce the scan time for high-resolution three-dimensional magnetic resonance imaging. In order to achieve better image quality and avoid manual parameter tuning, we propose a probabilistic Bayesian approach to recover <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:semantics><mml:mrow><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:annotation>$$ {R}_2^{\ast } $$</mml:annotation></mml:semantics></mml:math> map and phase images for quantitative susceptibility mapping (QSM), while allowing automatic parameter estimation from undersampled data.

Theory

Sparse prior on the wavelet coefficients of images is interpreted from a Bayesian perspective as sparsity-promoting distribution. A novel nonlinear approximate message passing (AMP) framework that incorporates a mono-exponential decay model is proposed. The parameters are treated as unknown variables and jointly estimated with image wavelet coefficients.

Methods

Undersampling takes place in the y-z plane of k-space according to the Poisson-disk pattern. Retrospective undersampling is performed to evaluate the performances of different reconstruction approaches, prospective undersampling is performed to demonstrate the feasibility of undersampling in practice.

Results

The proposed AMP with parameter estimation (AMP-PE) approach successfully recovers <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:semantics><mml:mrow><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:annotation>$$ {R}_2^{\ast } $$</mml:annotation></mml:semantics></mml:math> maps and phase images for QSM across various undersampling rates. It is more computationally efficient, and performs better than the state-of-the-art <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:semantics><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:annotation>$$ {l}_1 $$</mml:annotation></mml:semantics></mml:math> -norm regularization (L1) approach in general, except a few cases where the L1 approach performs as well as AMP-PE.

Conclusion

AMP-PE achieves better performance by drawing information from both the sparse prior and the mono-exponential decay model. It does not require parameter tuning, and works with a clinical, prospective undersampling scheme where parameter tuning is often impossible or difficult due to the lack of ground-truth image.

---

### Species interactions can explain Taylor's power law for ecological time series [^c24c43a3]. Nature (2003). Excellent credibility.

One of the few generalities in ecology, Taylor's power law, describes the species-specific relationship between the temporal or spatial variance of populations and their mean abundances. For populations experiencing constant per capita environmental variability, the regression of log variance versus log mean abundance gives a line with a slope of 2. Despite this expectation, most species have slopes of less than 2 (refs 2, 3-4), indicating that more abundant populations of a species are relatively less variable than expected on the basis of simple statistical grounds. What causes abundant populations to be less variable has received considerable attention, but an explanation for the generality of this pattern is still lacking. Here we suggest a novel explanation for the scaling of temporal variability in population abundances. Using stochastic simulation and analytical models, we demonstrate how negative interactions among species in a community can produce slopes of Taylor's power law of less than 2, like those observed in real data sets. This result provides an example in which the population dynamics of single species can be understood only in the context of interactions within an ecological community.

---

### J-shapedness: an often missed, often miscalculated relation: the example of weight and mortality [^9e991138]. Journal of Epidemiology and Community Health (2014). Low credibility.

We present three considerations in analysing the association between weight and mortality, as well as other relations that might be non-linear in nature. First, authors must graphically plot their independent and dependent variables in a continuous manner. Second, authors should assess the shape of that relation, and note its shape. If it is non-linear, and specifically, J-shaped or U-shaped, careful consideration should be given to using the 'best' statistical model, of which multivariate fractional polynomial regression is a reasonable choice. Authors should also refrain from truncating their data to avoid dealing with non-linear relations.

---

### Efficient classical sampling from gaussian boson sampling distributions on unweighted graphs [^0b2f9fbc]. Nature Communications (2025). High credibility.

Algorithm 2

Double-loop Glauber dynamics

Input : A graph G = (V, E).

Output : A sample of vertex set S of G such that Pr[S] ∝ λ ∣ S ∣ Haf 2 (S).

1 Initialize X 0 as an arbitrary matching in G ;

2 Initialize t ← 0, set;

3 while t < T do

4 Choose a uniformly random edge e from E ;

5 if e and X t form a new matching then

6 Set X t +1 = X t ∪ { e } with probabilityand otherwise set X t +1 = X t ;7D2

7 else if e is in X t then

8 Uniformly sample a perfect matching E t in the subgraph induced by X t (by running another MCMC on). If e ∉ E t, set X t +1 = X t. If e ∈ E t, set X t +1 = X t \{ e } with probabilityand otherwise set X t +1 = X t ;

9 else

10 Set X t +1 = X t ;

11 t ← t + 1;7D2

12 Output the vertex set in matching X T ;7D2

We first verify the convergence to the desired distribution. For the edge sets of any two matchings X and X ∪ { e }, on the one hand, from Line 6 in Algorithm 2,On the other hand, if X t = X ∪ { e }, the probability Pr[e ∈ E t] in Line 8 in Algorithm 2 is equal to the ratio of the number of perfect matchings of G X ∪{ e } that contains e to the number of perfect matchings of G X ∪{ e }. The numerator equals the number of perfect matchings of G X. Thus we havewhere the second term comes from the probability of edge e being in E t. Recall that the stationary distribution of matchings should satisfythus if we take λ = c 2,Notice that the number of perfect matchings of G X is Haf(G X). Therefore, when we finally take the vertex set of the sampled matching as the output, the probability of the sampled set of vertices S is:which simulates the output distribution by GBS. We remark that our Algorithm 2 works for any undirected and unweighted graph G.

Next, we rigorously establish that the mixing time of the double-loop Glauber dynamics on dense graphs is at most a polynomial. Specifically, for dense bipartite graphs, we have:

---

### Multiple patient samples of an analyte improve detection of changes in clinical status [^0819e938]. Archives of Pathology & Laboratory Medicine (2010). Low credibility.

Context

When comparing results over time, biologic variation must be statistically incorporated into the evaluation of laboratory results to identify a physiologic change. Traditional methods compare the difference in 2 values with the standard deviation (SD) of the biologic variation to indicate whether a "true" physiologic change has occurred.

Objective

To develop methodology to reduce the effect of biologic variation on the difference necessary to detect changes in clinical status in the presence of biologic variation.

Design

The standard test for change compares the difference between 2 points with the 95% confidence limit, given as +/- 1.96 x square root of 2 x SD. We examined the effect of multiple data pairs on the confidence limit.

Results

Increasing the number of data pairs using the formula 1.96 x square root of 2 x SD, where n = number of data pairs, significantly reduces the difference between values necessary to achieve a 95% confidence limit.

Conclusions

Evaluating multiple paired sets of patient data rather than a single pair results in a substantial decrease in the difference between values necessary to achieve a given confidence interval, thereby improving the sensitivity of the evaluation. A practice of using multiple patient samples results in enhanced power to detect true changes in patient physiology. Such a testing protocol is warranted when small changes in the analyte precede serious clinical events or when the SD of the biologic variation is large.

---

### Cancer etiology and prevention principle: "1 + X" [^2bba99d4]. Cancer Research (2021). Medium credibility.

Cancer was previously thought to be an inevitable aspect of human health with no effective treatments. However, the results of in-depth cancer research suggest that most types of cancer may be preventable. Therefore, a comprehensive understanding of the disparities in cancer burden caused by different risk factors is essential to inform and improve cancer prevention and control. Here, we propose the cancer etiology and prevention principle "1 + X," where 1 denotes the primary risk factor for a cancer and X represents the secondary contributing risk factors for the cancer. We elaborate upon the "1 + X" principle with respect to risk factors for several different cancer types. The "1 + X" principle can be used for precise prevention of cancer by eliminating the main cause of a cancer and minimizing the contributing factors at the same time.

---

### The effect of environmental information on evolution of cooperation in stochastic games [^06da36c7]. Nature Communications (2023). High credibility.

When we simulate the evolutionary dynamics of this stochastic game, we observe that individuals consistently learn to cooperate when they have full information. In contrast, without information, they mostly defect (Fig. 2 b). To explain this result, we numerically compute which strategies are most likely to evolve according to the process’s invariant distribution, for each of the two cases (Fig. 2 c). In the full-information setting, individuals predominantly adopt a strategy p F = (1, 0, 0, 0; x, 0, 0, 1), where x ∈ {0, 1} is arbitrary. This strategy may be considered as a variant of the WSLS rule that has been successful in the traditional prisoner’s dilemma. In particular, it is fully cooperative with itself. We prove in Supplementary Note 3 that this strategy forms a subgame perfect (Nash) equilibrium if 2 b 1 − b 2 ≥ 2 c, which is satisfied for the parameters we use (see also Fig. 3 a). On the other hand, in the no-information setting, this strategy is no longer available. Instead, players can only sustain cooperation with the traditional WSLS rule p N = (1, 0, 0, 1). This strategy is only an equilibrium under the more stringent condition b 1 > 2 c. Because our parameters do not satisfy this condition, cooperation does not evolve in the no-information setting (Fig. 3 b). To explore how these results depend on the benefit of cooperation b 1 and on the selection strength β, Fig. 2 d shows further simulations where we systematically vary both parameters. In all considered cases, state information is beneficial because it allows individuals to give more nuanced responses.

---

### Fundamental limits to learning closed-form mathematical models from data [^5954e9c9]. Nature Communications (2023). High credibility.

Fig. 1 
Probabilistic model selection makes quasi-optimal predictions about unobserved data.

We select two models m *, whose expressions are shown at the top of each column. a, b From each model, we generate synthetic datasets D with N points (shown, N = 100) and different levels of noise s ϵ (shown, s ϵ = 1). Here and throughout the article, the values of the independent variables x 1 and x 2 are generated uniformly at random in [− 2, 2]. Vertical lines show the observation error ϵ i for each point in D. For a model not drawn from the prior and data generated differently, see Supplementary Fig. S1. c, d For each dataset D (with dataset sizes N ∈ {25, 50, 100, 200, 400}), we sample models from p (m ∣ D) using the Bayesian machine scientist, select the MDL model (maximum p (m ∣ D)) among those sampled, and use this model to make predictions on a test dataset, generated exactly as D. We show the prediction root mean squared error (RMSE) of the MDL model onas a function of N and s ϵ. For comparison, we also show the predictions from an artificial neural network (ANN, dotted lines; Methods). Since s ϵ is the irreducible error, predictions on the diagonal RMSE = s ϵ are optimal. e, f We plot the prediction RMSE scaled by the irreducible error s ϵ ; optimal predictions satisfy RMSE/ s ϵ = 1 (dashed line).

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^bb9b7f12]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if “for small enough Δ Φ, the probability distribution at a point Φ [does] not depend on the level of refinement”. Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Φ ∈ {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters α and β, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since α and β are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Φ ∈ [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(α, β) and x 2 = 1 − x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Φ, it may seem natural to select α and β by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### Causal associations between risk factors and common diseases inferred from GWAS summary data [^a2c8dae6]. Nature Communications (2018). Medium credibility.

Multi-trait conditional GWAS analysis using summary data

To test whether the effect of a risk factor (x 0) on a disease (y) depends on other risk factors (x = { x 1, x 2,…, x i }), we usually perform a joint analysis based on the model belowwhere b 0 is the effect of x 0 on y,is a t -length vector withbeing the effect of a covariate x i on y, and e is the residual. Such an analysis is equivalent to a two-step analysis with the first step to adjust both x 0 and y by x and the second step to estimate the effect of adjusted x 0 on adjusted y. We therefore can estimate the effect size of x 0 on y accounting for x by a GSMR analysis using SNP effects on x 0 and y conditioning on x.

---

### Fundamentals of research data and variables: the devil is in the details [^07fcd2d5]. Anesthesia and Analgesia (2017). Low credibility.

Designing, conducting, analyzing, reporting, and interpreting the findings of a research study require an understanding of the types and characteristics of data and variables. Descriptive statistics are typically used simply to calculate, describe, and summarize the collected research data in a logical, meaningful, and efficient way. Inferential statistics allow researchers to make a valid estimate of the association between an intervention and the treatment effect in a specific population, based upon their randomly collected, representative sample data. Categorical data can be either dichotomous or polytomous. Dichotomous data have only 2 categories, and thus are considered binary. Polytomous data have more than 2 categories. Unlike dichotomous and polytomous data, ordinal data are rank ordered, typically based on a numerical scale that is comprised of a small set of discrete classes or integers. Continuous data are measured on a continuum and can have any numeric value over this continuous range. Continuous data can be meaningfully divided into smaller and smaller or finer and finer increments, depending upon the precision of the measurement instrument. Interval data are a form of continuous data in which equal intervals represent equal differences in the property being measured. Ratio data are another form of continuous data, which have the same properties as interval data, plus a true definition of an absolute zero point, and the ratios of the values on the measurement scale make sense. The normal (Gaussian) distribution ("bell-shaped curve") is of the most common statistical distributions. Many applied inferential statistical tests are predicated on the assumption that the analyzed data follow a normal distribution. The histogram and the Q-Q plot are 2 graphical methods to assess if a set of data have a normal distribution (display "normality"). The Shapiro-Wilk test and the Kolmogorov-Smirnov test are 2 well-known and historically widely applied quantitative methods to assess for data normality. Parametric statistical tests make certain assumptions about the characteristics and/or parameters of the underlying population distribution upon which the test is based, whereas nonparametric tests make fewer or less rigorous assumptions. If the normality test concludes that the study data deviate significantly from a Gaussian distribution, rather than applying a less robust nonparametric test, the problem can potentially be remedied by judiciously and openly: (1) performing a data transformation of all the data values; or (2) eliminating any obvious data outlier(s).

---

### Principles of analytic validation of immunohistochemical assays: guideline update [^cda8439c]. Archives of Pathology & Laboratory Medicine (2024). High credibility.

Supplemental Digital Content—AMSTAR (Assessing the Methodological Quality of Systematic Reviews) appraisal of methodological quality presents checklist items marked with “√” or “x”: “Quality assessed & documented” is “√” and “x”; “Quality used appropriately for conclusion” is “√” and “x”; “Methods to combine used appropriately” is “√” and “√”; “Publication bias assessed” is “x” and “x”; and “COI (conflict of interest)” is “√” and “√”. The AMSTAR SCORE /11 is 9 and 5.

---

### A practical guide for understanding confidence intervals and P values [^c0a7d4a0]. Otolaryngology--Head and Neck Surgery (2009). Low credibility.

The 95 percent confidence interval about the mean demarcates the range of values in which the mean would fall if many samples from the universal parent population were taken. In other words, if the same observation, experiment, or trial were done over and over with a different sample of subjects, but with the same characteristics as the original sample, 95 percent of the means from those repeated measures would fall within this range. This gives a measure of how confident we are in the original mean. It tells us not only whether the results are statistically significant because the CI falls totally on one side or the other of the no difference marker (0 if continuous variables; 1 if proportions), but also the actual values so that we might determine if the data seem clinically important. In contrast, the P value tells us only whether the results are statistically significant, without translating that information into values relative to the variable that was measured. Consequently, the CI is a better choice to describe the results of observations, experiments, or trials.

---

### Estimating the success of re-identifications in incomplete datasets using generative models [^9c6329d0]. Nature Communications (2019). High credibility.

Theoretical and empirical population uniqueness

For n individuals x (1), x (2), …, x (n) drawn from X, the uniqueness Ξ X is the expected percentage of unique individuals. It can be estimated either (i) by computing the mean of individual uniqueness or (ii) by sampling a synthetic population of n individuals from the copula distribution. In the former case, we havewhere T x = [∃! i, x (i) = x] equals one if there exists a single individual i such as x (i) = x and zero otherwise. T x follows a binomial distribution B (p (x), n). Thereforeand

This requires iterating over all combinations of attributes, whose number grows exponentially as the number of attributes increases, and quickly becomes computationally intractable. The second method is therefore often more tractable and we use it to estimate population uniqueness in the paper.

For cumulative marginal distributions F 1, F 2, …, F d and copula correlation matrix Σ, the algorithm 1 (Supplementary Methods) samples n individuals from q (⋅|Σ,Ψ) using the latent copula distribution. From the n generated records (y (1), y (2), …, y (n)), we compute the empirical uniqueness

Individual likelihood of uniqueness and correctness

The probability distributioncan be computed by integrating over the latent copula density. Note that the marginal distributions X 1 to X d are discrete, causing the inversestoto have plateaus. When estimating p (x), we integrate over the latent copula distribution inside the hypercube:with ϕ Σ the density of a zero-mean multivariate normal (MVN) of correlation matrix Σ. Several methods have been proposed in the literature to estimate MVN rectangle probabilities. Genz and Bretz, proposed a randomized quasi Monte Carlo method which we use to estimate the discrete copula density.

The likelihood ξ x for an individual’s record x to be unique in a population of n individuals can be derived from p X (X = x):

Similarly, the likelihoodfor an individual’s record x to be correctly matched in a population of n individuals can be derived from. With, the number of potential false positives in the population, we have:

Note that, since records are independent, T follows a binomial distribution B (n − 1, p (x)).

We substitute the expression for ξ x in the last formula and obtain:

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^cbffc27e]. CDC (2011). Medium credibility.

Appendix G. Mathematical formula for sample size—sample size required to detect the difference of GM between two subpopulation groups defines a z test with null hypothesis GM1 = GM2 or GMmax/GMmin = 1 and alternative GM1 ≠ GM2 or GMmax / GMmin > 1, where GMmax = max(GM1, GM2) and GMmin = min(GM1, GM2). It considers power W (say 80% chance) to detect that one group’s GM is at least k fold higher and gives the p-value as p = Prob(Z ≥ z) = 1 − φ−1(z) with Z ~ Normal (0,1). Given α (e.g., 0.05) and target ratio k, power is W = Prob ( Z > φ−1(1 − α) − log10(k)/(S√(1/n1 + 1/n2)) ), where S is the expected standard deviation of log10(VL) in the population of interest; when n1 = n2 = n, W = Prob ( Z > φ−1(1 − α) − log10(k)/(S√(2/n)) ). The required sample size is n = 2[(φ−1(1 − α) − φ−1(1 − W))S/log10(k)]^2, and tables are provided for W = 0.8 and W = 0.9 with α = 0.05.

---

### Casein [^fd0595f4]. FDA (2009). Low credibility.

Volume desired x Concentration desired = Volume needed x Concentration available.

Example 1 : If a 1:10 w/v extract is available and it is desired to use a 1:1,000 w/v extract substitute as follows:

Vd x Cd = Vn x Ca

10ml x 0.001 = Vn x 0.1

0.1 ml = Vn

Using a sterile technique, remove 0.10 ml of extract from the 1:10 vial and place it into a vial containing 9.90 ml of sterile diluent. The resulting ratio will be a 10 ml vial of 1:1,000 w/v.

Example 2 : If a 10,000 pnu/ml extract is available and it is desired to use a 100 pnu/ml extract substitute as follows:

10ml x 100 = Vn x 10,000

0.1 ml = Vn

Using a sterile technique, remove 0.10 ml of extract from the 10,000 pnu/ml vial and place it into a vial containing 9.90 ml of sterile diluent. The resulting concentration will be a 10 ml vial of 100 pnu/ml.

Example 3: If a 10,000 AU/ml or BAU/ml extract is available and it is desired to use a 100 AU/ml or BAU/ml extract substitute as follows: Vd x Cd = Vn x Ca

---

### Experimental demonstration of quantum advantage for one-way communication complexity surpassing best-known classical protocol [^2c59f7d2]. Nature Communications (2019). High credibility.

Fig. 1 
The Sampling Matching problem. Alice gets an input x ∈ {0, 1} n and sends a message m (x) to Bob who outputs the pair 〈(k, l) ∈ σ i, b = x k ⊕ x l 〉 for a matching σ i, whose distribution is uniform in, even conditioned on m (x). The parity should be correct with high probability for all choices of the matching

Fig. 2 
Illustration of a set of edge-disjoint perfect matchings for size n = 4. The matching sethas three edge-disjoint perfect matchings: [σ 1 :{(1, 2), (3, 4)}; σ 2 :{(1, 3), (2, 4)}; σ 3 :{(1, 4), (2, 3}]

It is also important to note that in our one-way model the communication cost of a protocol is the number of bits Alice has to send to Bob in order to solve the problem, while the transmitted information, instead of the number of bits sent, calculates the real bits of information about the inputs that the messages carry. For example, if Alice always sends the same, long message, independent of her input, then the communication cost will be large, while the transmitted information will be zero, since no information about her input has been transmitted. Transmitted information is a resource that is important for privacy, when on top of having an efficient protocol, we want the players to solve the task without learning much about the other player's input. One can define the transmitted information as the mutual information between the messages and the inputs and can upper bound it with the logarithm of the number of different messages. The transmitted information is always at most the communication cost, since one bit carries at most one bit of information, and hence the bottleneck is always the time. Last, we remark that we define our problem in the randomized setting where Bob is allowed to use random coins and output the correct value with high probability.

---

### Optimal free descriptions of many-body theories [^f77fa88f]. Nature Communications (2017). Medium credibility.

Methods

Optimization

The optimization to find σ andin (3) is performed by a Monte Carlo basin-hopping strategyusing the Nelder–Mead simplex algorithm for local minimization within basins of the cost function. This global strategy was selected to counteract an observed tendency for local methods to get trapped in local minima. The initial guess for this search is found as follows. The normalization constant E 0 is the lowest entanglement energy of the input entanglement spectrum { E }. We iteratively construct an approximate set of single-particle entanglement energies starting from an empty set. First, we take the lowest remaining level in the spectrum and subtract E 0 to produce a new single-particle level k. Then we remove the many-body levels, which are closest to the new combinatorial levels generated according to (2) by the additional single-particle level. This process is repeated until the input spectrum is exhausted. We can also introduce a truncation of the entanglement spectrum cutting off high entanglement energies, making the construction of the initial guess terminate faster. The minimization of D (σ, σ f) to identify the optimal free model for the Ising Hamiltonian (5) is calculated using a local Nelder–Mead method.

Finite-size scaling

We perform the finite-size scaling according to an ansatz (4). The parameters of the collapse were estimated using the method of ref. From the scaling ansatz (4) and for a trial set of scaling parameters g c, ν and ζ, the scaled values x L =(g − g c) L 1/ ν andare calculated from each unscaled data point. From this collection of scaled data points (x L, y L) across all L, we implicitly define a so-called master curve that best represents them. This curve y (x) is defined around a point x as the linear regression calculated by taking the scaled data points immediately left and right of x for each system size L. We characterize the deviation of the scaled points (x L, y L) from the master curve y (x L) using the χ 2 statistic. This measure is used as the cost function for an optimization problem over the scaling parameters g c, ν, ζ and θ, which can be solved using the same techniques as the previous problems.

---

### 2018 ACC / HRS / NASCI / SCAI / SCCT expert consensus document on optimal use of ionizing radiation in cardiovascular imaging: best practices for safety and effectiveness [^5e4c4e7f]. Catheterization and Cardiovascular Interventions (2018). Medium credibility.

Background and population radiation exposure—natural background radiation averages 3.0 millisieverts (mSv) per person/year in the United States, equivalent to 150 posteroanterior chest radiographs, with a posteroanterior chest x-ray dose of 0.02 mSv and combined posteroanterior and lateral of 0.06 mSv; between 1987 and 2006, estimated per year total medical radiation exposure grew from 0.6 mSv/year (0.2 x background) to 3.2 mSv/year (1.07 x background), and the 2006 medical exposure is equivalent to 150 posteroanterior chest x-rays per person/year.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^dee6cacc]. CDC (2011). Medium credibility.

Community viral load sample size calculations—sample size to detect differences in geometric mean (GM) viral load depends on power and standard deviation (S). The table specifies “α = 0.05 and W = 0.8,” with S columns 1 to 1.5; for k = 3 the minimum sample sizes by S are 54, 66, 78, 92, 106, and 122, and “1.2 is the standard deviation of national VL data,” so at S = 1.2 the needed sample size is 78.

---

### Estimation of the risk of a qualitative phenotype: dependence on population risk [^4a647c65]. Journal of Human Genetics (2017). Low credibility.

Individual disease risk estimated based on the data from single or multiple genetic loci is generally calculated using the genotypes of a subject, frequencies of alleles of interest, odds ratios and the average population risk. However, it is often difficult to estimate accurately the average population risk, and therefore it is often expressed as an interval. To better estimate the risk of a subject with given genotypes, we built R scripts using the R environment and constructed graphs to examine the change in the estimated risk as well as the relative risk according to the change of the average population risk. In most cases, the graph of the relative risk did not cross the line of y=1, thereby indicating that the order of the relative risk for given genotypes and the population average risk does not change when the average risk increases or decreases. In rare cases, however, the graph of the relative risk crossed the line of y=1, thereby indicating that the order of the relative risk for given genotypes and the population average risk does change owing to the change in the population risk. We propose that the relative risk should be estimated for not only the point average population risk but also for an interval of the average population risk. Moreover, when the graph crosses the line of y=1 within the interval, this information should be reported to the consumer.

---

### Reporting and assessing the quality of diagnostic accuracy studies for cervical cancer screening and management [^df4ea209]. Journal of Lower Genital Tract Disease (2020). High credibility.

Analysis population—signaling question 12 notes that attrition and missing evaluations can bias results: There is potential for bias if the number of patients enrolled differs from the number of patients included in the 2 x 2 tables and results; some patients may not be included in the analysis because of missing evaluation of 1 or more tests; such differences should be clearly stated or addressed in a Consolidated Standards of Reporting Trials diagram.

---

### Fractional response analysis reveals logarithmic cytokine responses in cellular populations [^0d1364aa]. Nature Communications (2021). High credibility.

Calculation of typical fractions

The fractions of cells stimulated with dose i that have responses typical to dose j, v ij, can be easily calculated from data regardless of the number of doses and the type of experimental measurements. We have thatCalculation of typical fractions, v ij, with the above formula requires the possibility to examine the condition P (y | x j) > P (y | x k) for any experimentally observed response, y. The distributions P (y | x j) can be reconstructed from data using a variety of probability density estimators. The use of the available estimators, however, might be problematic for multivariate responses,. We, therefore, propose a more convenient strategy. We replace the condition P (y | x j) > P (y | x k) with an equivalent condition that is computationally much simpler to evaluate. Precisely, we propose to use the Bayes formulaIf we set the equiprobable prior distribution, i.e. P (x j) = 1 /m, we have that P (y | x j) is proportional to P (x j | y) and the condition P (y | x j) > P (y | x k) is equivalent toThe above strategy allows avoiding estimation of the response distributions, P (y | x j), from data. For continuous and multivariate variable y the estimation of P (x j | y) is generally simpler than estimation of P (y | x j),. Precisely, an estimatorof the distribution P (x j | y) can be built using a variety of Bayesian statistical learning methods. For simplicity and efficiency, here we propose to use logistic regression, which is known to work well in a range of applications. In principle, however, other classifiers could also be considered. The logistic regression estimators of P (x j | Y = y) arise from a simplifying assumption that log-ratio of probabilities, P (x j | Y = y) and P (x m | Y = y) is linear. Precisely,The above formulation allows fitting the logistic regression equations to experimental data, i.e. finding values of the parameters, α j and β j that best represent the data. The fitted logistic regression model allows assigning cellular responses to typical doses based on conditions given by Eq. 8. Formally, the fractions v ij defined by Eq. 6 are calculated aswhere n i is the number of cells measured for the dose x i, y l i denotes response of the l- th cell, andis equal 1 iffor anyand 0 otherwise.

---

### Identifying domains of applicability of machine learning models for materials science [^72c4ba76]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ν that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by σ lin (x 1, x 2) ≡ −0.3 ≤ x 1 ≤ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: σ rbf ≡ −3.3 ≤ x 11 ≤ 3.1 ∧ x 2 ≤ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA σ poly (x 1, x 2) ≡ −3.5 ≤ x 2 ≤ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### KDOQI clinical practice guidelines and clinical practice recommendations for diabetes and chronic kidney disease [^e7d0d7b5]. American Journal of Kidney Diseases (2007). Medium credibility.

Results—associations are summarized by both direction and strength, with magnitude and both clinical and statistical significance accounted for, and both univariate and multivariate associations presented when appropriate; metrics used include prevalence, relative effects (relative risk [RR], odds ratio [OR], hazard ratio [HR], or net change—change from baseline in the intervention group minus the change in the control group), correlation (r or r^2), and test accuracy (sensitivity, specificity, and positive and negative predictive value); the choice of metric often was limited by the reported data and for some studies only the statistical significance was reported.

---

### 2013 ACC / AHA guideline on the assessment of cardiovascular risk: a report of the American college of cardiology / American Heart Association task force on practice guidelines [^58242b99]. Journal of the American College of Cardiology (2014). Medium credibility.

Pooled Cohort Equations—equation parameters for estimating 10-year risk of hard atherosclerotic cardiovascular disease (ASCVD) are provided for each race and sex group, with an example profile of 55 years of age, total cholesterol 213 mg/dL, HDL-C 50 mg/dL, untreated systolic BP 120 mm Hg, nonsmoker, and without diabetes. For women, baseline survival is 0.9665 for White and 0.9533 for African American, and the estimated 10-y risk for the example is 2.1% and 3.0%, respectively. Representative women’s coefficients include Ln Age (y) −29.799 (White) and 17.114 (African American) and Ln HDL-C (mg/dL) −13.578 and −18.920. For men, baseline survival is 0.9144 for White and 0.8954 for African American, and the estimated 10-y risks for the example are 5.3% and 6.1%. Representative men’s coefficients include Ln Age (y) 12.344 (White) and 2.469 (African American) and Ln Total Cholesterol (mg/dL) 11.853 and 0.302. Hard ASCVD is defined as the first occurrence of nonfatal myocardial infarction or CHD death, or fatal or nonfatal stroke.

---

### Drawing statistical conclusions from experiments with multiple quantitative measurements per subject [^70dff523]. Radiotherapy and Oncology (2020). Medium credibility.

In experiments with multiple quantitative measurements per subject, for example measurements on multiple lesions per patient, the additional measurements on the same patient provide limited additional information. Treating these measurements as independent observations will produce biased estimators for standard deviations and confidence intervals, and increases the risk of false positives in statistical tests. The problem can be remedied in a simple way by first taking the average of all observations of each specific patient, and then doing all further calculations only on the list of these patient means. A more sophisticated statistical modeling of the experiment, for example in a linear mixed model, is only required if (i) there is a large imbalance in the number of observations per patient or (ii) there is a specific interest in actually identifying the various sources of variation in the experiment.

---

### 2019 ASCCP risk-based management consensus guidelines: methods for risk estimation, recommended management, and validation [^c44c6ea6]. Journal of Lower Genital Tract Disease (2020). High credibility.

Risk-based management uncertainty—confidence interval and decision confidence score: Confidence intervals for each risk estimate were calculated using a normal approximation or exact methods based on the binomial distribution, and a “decision confidence score” is defined that combines the uncertainty in the statistical precision and how close the risk estimates fall to the clinical action thresholds; mathematical details on estimating the recommendation confidence scores are presented in Appendix Part D (http://links.lww.com/LGT/A160).

---

### Mixed models for repetitive measurements: the basics [^d40754d3]. Fertility and Sterility (2024). Medium credibility.

Linear mixed models are regularly used within the field of reproductive medicine. This manuscript explains the basics of mixed models, when they could be used, and how they could be applied.

---

### Statistics for X-chromosome associations [^c3957f8d]. Genetic Epidemiology (2018). Low credibility.

In a genome-wide association study (GWAS), association between genotype and phenotype at autosomal loci is generally tested by regression models. However, X-chromosome data are often excluded from published analyses of autosomes because of the difference between males and females in number of X chromosomes. Failure to analyze X-chromosome data at all is obviously less than ideal, and can lead to missed discoveries. Even when X-chromosome data are included, they are often analyzed with suboptimal statistics. Several mathematically sensible statistics for X-chromosome association have been proposed. The optimality of these statistics, however, is based on very specific simple genetic models. In addition, while previous simulation studies of these statistics have been informative, they have focused on single-marker tests and have not considered the types of error that occur even under the null hypothesis when the entire X chromosome is scanned. In this study, we comprehensively tested several X-chromosome association statistics using simulation studies that include the entire chromosome. We also considered a wide range of trait models for sex differences and phenotypic effects of X inactivation. We found that models that do not incorporate a sex effect can have large type I error in some cases. We also found that many of the best statistics perform well even when there are modest deviations, such as trait variance differences between the sexes or small sex differences in allele frequencies, from assumptions.

---

### LinkedSV for detection of mosaic structural variants from linked-read exome and genome sequencing data [^f423756f]. Nature Communications (2019). High credibility.

If we regard (R (F i 1), L (F i 2)) as a point in a two-dimensional plane, according to Eq. (2), for almost all (98.01%) of the fragment pairs (F i 1, F i 2), ((R (F i 1), L (F i 2)) is restricted in a G × G square region with the point (b 1, b 2) being a vertex (Supplementary Fig. 14b).

We used a graph-based method to fast group the points into clusters and find square regions where the numbers of points were more than expected. First, every possible pair of endpoints (R (F 1), L (F 2)) meeting B (F 1) = B (F 2) formed a point in the two-dimensional plane. Each point indicated a pair of fragments that share the same barcode. For example, if ten fragments share the same barcode,pairs of endpoints will be generated. A point/pair of endpoints may or may not support an SV because there are two possible reasons for observing two fragments sharing the same barcode: (1) the two fragments originated from two different HMW DNA molecules but were dispersed into the same droplet partition and received the same barcode and (2) the two fragments originated from the same HMW DNA molecule but the reads were reconstructed into two fragments due to an SV. The points are sparsely distributed in the two-dimensional plane and it is highly unlikely to observe multiple points in a specific region. Next, a k – d tree (k = 2) was constructed, of which each node stores the (X, Y) coordinates of one point. A k – d tree is a binary tree that enable fast query of nearby nodes. Therefore, we could quickly find all pairs of points within a certain distance. Any two points (x 1, y 1) and (x 2, y 2) were grouped into one cluster if | x 1 − x 2 | < G and | y 1 − y 2 | < G. For each cluster, if the number of points in the cluster was more than a user-defined threshold (default: 5), it was considered as a potential region of enriched fragment endpoints. If the points in the cluster were not within a G × G square region, we used a G × G moving square to find a square region where the points are best enriched. Theoretically, the best enriched square region should contain 98.01% (0.99 × 0.99) of the points, according to Eq. (2). The predicted breakpoints were the X and Y coordinates of the right-bottom vertex of the square. The points in the square region were subjected to a statistical test describe below.

---

### Incomplete influenza A virus genomes occur frequently but are readily complemented during localized viral spread [^19b96fd0]. Nature Communications (2019). High credibility.

Individual-based model of replication

A cellular automaton model of viral spread was developed to investigate the relationships between spatial structure, prevalence of incomplete viral genomes, and viral fitness. The system consists of a 100 × 100 grid of cells. Each cell contains 0–8 distinct IAV genome segments, and additional copies of the same segment are assumed to be redundant. Virions exist on the same grid, in a bound or unbound state. When a virion infects a cell, any missing segments may be delivered, with the probability of delivery defined by P P, as derived in Figs. 1 and 2.

The simulation begins with a single productively infected cell in the middle of the grid. The following events occur at each time-step (3 min), and the frequency of each of these events is governed by the parameters listed in Table 1.
All virions not currently bound to a cell will diffuse. First, the total distance traveled is randomly drawn from the normal distributionwhere D is the diffusion coefficient, t is the length of the time-step, μ is the mean, and σ is the standard deviation. Second, the direction traveled is randomly drawn from the uniform distribution The total distance traveled is then converted to orthogonal distancesand Distance and direction of travel are calculated independently for each virion. These distances are then used to adjust the X and Y positions of the diffusing virions, which are tracked in absolute units (µm). Cells are modeled as squares measuring 30 µm × 30 µm, and so a virion with an absolute position 0 ≤ X < 30 corresponds to cell position X = 1, one with absolute position 30 ≤ X < 60 corresponds to cell position X = 2, and so on. If a virion would diffuse beyond the border of the grid, it instead emerges from the other side (e.g. a virion that would be moved to the cell position [X = 105, Y = 69] is instead placed at the position [X = 5, Y = 69]).
Free virions may attach to the cell at their current position. While attached, virions are unable to diffuse.
Bound virions may be released, or infect the cell to which they are attached. Virions that are released become free-floating, and are able to diffuse. When virions infect a cell, the number of segments added to that cell is determined by the probabilistic model described above. The number of segments added is calculated from the binomial distributionwhere S is the number of segments the cell already contains, P P is the probability an individual segment is delivered successfully, and v is the number of virions infecting the cell at the current time-step. For example, if a cell already contains five segments, additional virions infecting it may add 0–3 segments (N = 8 – 5 = 3). If four virions are infecting this cell, then the probability of a given segment being delivered is high (p = 1 – (1–0.58) 4 = 0.97), and the probability of four virions all failing to deliver a given segment ((1–0.58) 4 = 0.03) is relatively low.
Infected cells (those containing 1–8 segments) may become refractory to superinfection. Diffusing virions cannot bind to nonsusceptible cells, and any currently bound virions that attempt to infect these cells automatically fail to deliver all segments.
Productively infected cells (containing eight segments) produce virions, which are initially bound to the producer cell’s surface. The number of virions produced by each cell is independently drawn from the Poisson distribution with λ = Burst_Rate (962 virions day −1 or 2.06 virions time-step −1 (time-step = 3 min)). Each virion is placed at a random location on the producer cell as determined by the uniform distribution.
Productively infected cells may die. These cells lose all segments, cannot produce virions, and cannot be bound by diffusing virions.

---

### The XZZX surface code [^8f6ac9c8]. Nature Communications (2021). High credibility.

This structured noise model thus leads to two distinct regimes, depending on which failure process is dominant. In the first regime where, we expect that the logical failure rate will decay like. We find this behaviour with systems of a finite size and at high bias where error rates are near to threshold. We evaluate logical failure rates using numerical simulations to demonstrate the behavior that characterises this regime; see Fig. 6 (a). Our data show good agreement with the scaling ansatz. In contrast, our data are not well described by a scaling.

Fig. 6 
Sub-threshold scaling of the logical failure rate with the XZZX code.

a Logical failure rateat high bias near to threshold plotted as a function of code distance d. We use a lattice with coprime dimensions d × (d + 1) for d ∈ {7, 9, 11, 13, 15} at bias η = 300, assuming ideal measurements. The data were collected usingiterations of Monte-Carlo (MC) samples for each physical rate sampled and for each lattice dimension used. The physical error rates used are, from the bottom to the top curves in the main plot, p = 0.19, 0.20, 0.21, 0.22 and 0.23. Error bars represent one standard deviation for the Monte-Carlo simulations. The solid lines are a fit of the data to, consistent with Eq. (2), and the dashed lines a fit to, consistent with Eq. (3) where we would expect, see Methods. The data fit the former very well; for the latter, the gradients of the best fit dashed lines, as shown on the inset plot as a function of, give a linear slope of 0.61(3). Because this slope exceeds the value of 0.5, we conclude that the sub-threshold scaling is not consistent with. b Logical failure ratesat modest bias far below threshold plotted as a function of the physical error rate p. The data (markers) were collected at bias η = 3 and coprime d × (d + 1) code dimensions of d ∈ {5, 7, 9, 11, 13, 15} assuming ideal measurements. Data is collected using the Metropolis algorithm and splitting method presented in refs. The solid lines represent the prediction of Eq. (3). The data show very good agreement with the single parameter fitting for all system sizes as p tends to zero.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^b1513dd5]. CDC (2011). Medium credibility.

Table 1b—Minimum sample size for detecting geometric mean (GM) ratio k with α = 0.05 and W = 0.9—presents required sample sizes by standard deviation (S) strata and defines parameters; α is significance level, W is power, GM is geometric mean, and S is standard deviation, noting that 1.2 is the standard deviation of national viral load (VL) data. Each jurisdiction will need to assess the standard deviation of their local VL data and then determine the appropriate sample size needed to assess VL, and if the sample size is inadequate to meet the recommended case inclusion criterion, an alternate method may need to be used, such as combining multiple years of data. For k = 3, sample sizes across S = 1, 1.1, 1.2, 1.3, 1.4, 1.5 are 75, 91, 108, 127, 147, 169, and Table 1b uses power=90%; jurisdictions may also explore differences in means of viral loads, including categorical differences in the proportion with undetectable or very low VL.

---

### Continuous growth reference from 24th week of gestation to 24 months by gender [^0f68c406]. BMC Pediatrics (2008). Low credibility.

Appendix

Assuming normality, to estimate the variance, skewness and kurtosis of the population based on the cases in the range of μ ± δσ in a sample for certain values of δ > 0.

Without lost of generality, let μ = 0. Let, where φ (x ; σ) is the normal probability distribution function with mean 0 and variance σ 2. It can be deduced that

where Φ (x) is the standard normal cumulative distribution function. Letand. It can be shown that

Letand. It can be shown that

Let p = 2Φ(δ) - 1 be the proportion of observations in the range of μ ± δσ under a normal distribution. Considering a sample of n observations, x 1,. x n. Let q 1 be the (1 - p)/2-th quantile and q 2 be the (1 - (1 - p)/2)-th quantile and X r = { x i : q 1 ≤ x i ≤ q 2 } be the reduced sample of size m. An empirical estimate of, j = 2, 3, 4, is given as

The general sample variance, the skewness and the kurtosis are given as

and

respectively. Let, Skew r and Kurt r be the corresponding statistics calculated based on the reduced sample. Substituting – into –, we have the statistics for the full dataset evaluated based on the reduced sample as

and

---

### Cutting edge or blunt instrument: how to decide if a stepped wedge design is right for you [^471f52f2]. BMJ Quality & Safety (2021). High credibility.

When might I consider doing a stepped wedge trial?

Research designs are shaped as much by practical constraints as by abstract schemes, and it is always a good idea to start with the constraints and work towards a design, rather than start with a design and try to fit it to constraints. These constraints will be unique to each research context, and box 1 lists some areas to think about. Still, there are some common features of settings where a stepped wedge trial might be considered as a possible design, and we now review these.

Box 1 
Practical constraints on the design of a longitudinal cluster randomised trial

Are there limits on the time available to complete the evaluation, on the number of clusters, or on the number of participants (or the rate at which you can recruit participants) at each cluster? These constraints put limits on the overall scale of the evaluation, or force trade-offs between different design characteristics.
How will participants and their data be sampled in your study: as a series of cross-sectional surveys, as a continuous stream of incident cases, as a cohort followed over time, or some other way? Does the timescale divide into cycles, seasons or milestones that influence how you will sample participants and data?
Is there a limit on how many clusters can implement the intervention at the same time in the evaluation? If this is constrained by research resources (eg, if there are only enough trained research staff to implement the intervention one cluster at a time) then implementation must be staggered in some way.
If implementation is to be staggered, is there a minimum ‘step length’? If the same team delivers the intervention in different clusters at different steps, then bear in mind it may take some time to get the intervention fully operational at a site, and the team will also need time to relocate from one cluster to the next.

---

### The minimal work cost of information processing [^894e044e]. Nature Communications (2015). Medium credibility.

where the sum ranges only over those x that have a non-zero probability of occurring. In the case of deterministic mappings p (x ′| x)∈{0,1}, this corresponds to the maximum number of input states that map to a same output state. For the AND gate, provided all four states 00, 01, 10 and 11 have non-negligible probability of occurring, there are three input states mapping to the same output state, so (3) gives us simply. Also, in simple examples as considered here, the expression (3) is stable to considering an-approximation (Supplementary Note 4); this quantity is thus physically justified.

Crucially, our result reveals that the minimal work requirement in general depends on the specific logical process, and not only on the input and output states. This contrasts with traditional thermodynamics for large systems, where the minimal work requirement of a state transformation can always be written as a difference of a thermodynamical potential, such as the free energy. For example, the minimal work cost of performing specifically an AND gate may differ from that of another logical process mapping an input distribution (p 00, p 01, p 10, p 11) (with ∑ i p i =1) to the distribution (p ′ 0, p ′ 1)=(p 00 + p 01 + p 10, p 11) (Recall that the classical counterpart of a quantum state is a probability distribution.). To see this, consider the XOR gate, which outputs a 1 exactly when both inputs are different (see Fig. 2b). The minimal work cost requirement of this gate, as given by (3), is now only kT ln 2, as in the worst case, only a single bit of information is erased (again supposing that all four input states have non-negligible probability of occurring). Now, suppose that, for some reason, the input distribution is such that p 01 + p 10 = p 11, that is, the input 11 occurs with the same probability as of either 01 or 10 appearing. Then, the XOR gate reproduces the exact same output distribution as the AND gate: in both cases, we have p ′ 0 = p 00 + p 10 + p 01 = p 00 + p 11 and p ′ 1 = p 11 = p 01 + p 10. In other words, both logical processes have the same input and output state, yet the XOR gate only requires work kT ln 2 compared with the AND gate, which requires 1.6 kT ln 2. Furthermore, we point out that this difference, which appears small in this case, may be arbitrarily large in certain scenarios (Supplementary Note 4).

---

### Fractional quantum hall effect in the absence of landau levels [^b805bfe2]. Nature Communications (2011). Medium credibility.

Methods

Calculation of Chern number for many-body state

The Chern number of a many-body state can be obtained as:where the closed path integral is along the boundary of a unit cell 0≤ θ ≤2 π with j = x and y, respectively. The Chern number C is also the Berry phase (in units of 2 π) accumulated for such a state when the boundary phase evolves along the closed path. Equation (2) can also be reformulated as an area integral over the unit cell C =(1/2 π)∫ dθ 1 dθ 2 F, where F (θ 1, θ 2) is the Berry curvature. To determine the Chern number accurately, we divide the boundary-phase unit cell into about N tot =200 to 900 meshe. The curvature F (θ 1, θ 2) is then given by the Berry phase of each mesh divided by the area of the mesh. The Chern number is obtained by summing up the Berry phases of all the meshes.

Chern number quantization in FQHE

We find that the curvature is in general a very smooth function of θ j inside FQHE regime. For an example, the ground-state total Berry phase sums up to 0.325×2 π, slightly away from the 1/3 quantization for a system with N s =30, U =1 and V =0 at 1/3 filling. Physically, as we start from one state with momentum (k x, k y) in the GSM, it evolves to another state with a different momentum k x → k x + N e (k y → k y + N e), when the boundary phase along x (y) direction is increased from 0 to 2 π. Thus, with the insertion of a flux, states evolve to each other within the GSM. We observe that only the total Berry phase of the GSM is precisely quantized to 2 π and the total Chern number C =1 for all different choices of parameters inside either the 1/3 or 1/5 FQHE regime of Figure 1. When states in the GSM have the same momentum, the Chern number of each state is an integer due to non-crossing condition of the energy levels with the same quantum number, while the total Chern number remains the same.

---

### Modeling and forecasting the total number of cases and deaths due to pandemic [^4243a391]. Journal of Medical Virology (2022). Medium credibility.

The COVID-19 pandemic has appeared as the predominant disease of the 21st century at the end of 2019 and was a drastic start with thousands of casualties and the COVID-19 victims in 2020. Due to the drastic effect, COVID-19 scientists are trying to work on pandemic diseases and Governments are interested in the development of methodologies that will minimize the losses and speed up the process of cure by providing vaccines and treatment for such pandemics. The development of a new vaccine for any pandemic requires long in vitro and in vivo trials to use. Thus the strategies require understanding how the pandemic is spreading in terms of affected cases and casualties occurring from this disease, here we developed a forecasting model that can predict the no of cases and deaths due to pandemic and that can help the researcher, government, and other stakeholders to devise their strategies so that the damages can be minimized. This model can also be used for the judicial distribution of resources as it provides the estimates of the number of casualties and number of deaths with high accuracy, Government and policymakers on the basis of forecasted value can plan in a better way. The model efficiency is discussed on the basis of the available dataset of John Hopkins University repository in the period when the disease was first reported in the six countries till the mid of May 2020, the model was developed on the basis of this data, and then it is tested by forecasting the no of deaths and cases for next 7 days, where the proposed strategy provided excellent forecasting. The forecast models are developed for six countries including Pakistan, India, Afghanistan, Iran, Italy, and China using polynomial regression of degrees 3-5. But the models are analyzed up to the 6th-degree and the suitable models are selected based on higher adjusted R-square (R 2 ) and lower root-mean-square error and the mean absolute percentage error (MAPE). The values of R 2 are greater than 99% for all countries other than China whereas for China this R 2 was 97%. The high values of R 2 and Low value of MAPE statistics increase the validity of proposed models to forecast the total no cases and total no of deaths in all countries. Iran, Italy, and Afghanistan also show a mild decreasing trend but the number of cases is far higher than the decrease percentage. Although India is expected to have a consistent result, more or less it depicts some other biasing factors which should be figured out in separate research.

---

### Epidemiological perspectives on type 1 diabetes in childhood and adolescence in germany: 20 years of the baden-württemberg diabetes incidence registry (DIARY) [^dbce165b]. Diabetes Care (2010). Low credibility.

Objective

To predict the frequency of type 1 diabetes in childhood and adolescence (<15 years of age) in Germany for the next 20 years.

Research Design and Methods

Data on diabetes onset has been collected by means of a registry in the federal German state of Baden-Württemberg (documentation period, 1987-2006; n = 5,108; completeness of data 98.1%).

Results

The current incidence rate (2000-2006) is 19.4 per 100,000 per year (95% CI 18.6-20.2). The annual incidence rate can be expressed as a square of a linear function of the calendar year X [y = (3.05 + 0.0778 x {X-1986})(2); r(2) = 0.90]. The highest increase per year was observed in the age-groups comprising 2- and 3-year-olds (12 and 13% per year, respectively). The incidence rate for the year 2026 is estimated to be 37.9 per 100,000 per year (95% CI 33.3-42.9).

Conclusions

The increase that we found in younger children is characteristic of a left shift toward an earlier age.

---

### Executive summary of the stages of reproductive aging workshop + 10: addressing the unfinished agenda of staging reproductive aging [^76874d75]. Menopause (2012). Medium credibility.

Figure 1—hormone trajectories across the final menstrual period depict adjusted population means (95% CI) for segmented mean profiles of follicle-stimulating hormone (FSH) and estradiol in the Study of Women’s Health Across the Nation (N = 1,215), with the y axis unitless and hormone units marked on the corresponding curves.

---

### Committee opinion No 671: perinatal risks associated with assisted reproductive technology [^8e8baeac]. Obstetrics and Gynecology (2016). Medium credibility.

Risks regardless of fetal number—singleton ART pregnancies: Studies comparing singleton ART with naturally occurring pregnancies “suggest that there are at increased risk of preterm birth, low birth weight, and perinatal mortality rate,” and a meta-analysis of 15 studies (12,283 IVF singletons vs 1.9 million spontaneous singletons) reported higher odds for perinatal mortality (odds ratio [OR], 2.2; 95% confidence interval [CI], 1.6–3.0), preterm delivery (OR, 2.0; 95% CI, 1.7–2.2), low birth weight (OR, 1.8; 95% CI, 1.4–2.2), very low birth weight (OR, 2.7; 95% CI, 2.3–3.1), and small-for-gestational-age status (OR, 1.6; 95% CI, 1.3–2.0). A secondary analysis of the FASTER trial did not find ART associated with prematurity but found increased risk of cesarean delivery (OR, 2.3), placenta previa (OR, 6.0), abruptio placentae (OR, 2.4), and preeclampsia (OR, 2.7). Additional findings include higher vasa previa frequency in IVF (1/250 with IVF and 1/2,500 without IVF) and stillbirth after IVF/ICSI at 16.2/1,000 versus 2.3/1,000 in naturally occurring pregnancies; Massachusetts state data also “found an elevated risk of severe maternal morbidity” among women who became pregnant through ART.

---

### Quantum tomography of electrical currents [^53212aa3]. Nature Communications (2019). High credibility.

The wavefunctionsandcan be compared with the expected ones generated by Lorentzian voltage pulses at zero temperature. A single Lorentzian pulse of temporal width τ and carrying an integer number N e of electrons is the Slater determinant built over the 1 ≤ n ≤ N e electronic wavefunctions,, whereis a normalization constant,is the n th Laguerre polynomial. The two first onesand, plotted in red dashed lines in Fig. 5c, are very similar toand. However, they do not reproduce the discretization of the energy distribution in units of hf. This discretization is related to the periodicity of the single-electron emission, which is not captured by the expression of. The n = 1 wavefunction of the periodic train of Lorentzian pulses has been shown, to be given bywherehas quantized steps 2 πf related to the pulse periodicity. Here, we generalize this expression to the n = 2 and n = 3 wavefunctions by using the following ansatz:where. We take x 1 = 0 following refs. and then numerically deduce x 2 = 0.33 and x 3 = 0.24 from the constraint that the wavefunctions should be orthogonal: 〈 φ L, n | φ L, n ′ 〉 = δ n, n ′. Comparing the wavefunctionsandextracted from our experimental data to these theoretical predictions, we observe thatis very close to the expected wavefunction φ L, n =1 (blue dashed line in Fig. 5c) with an overlap of 0.98. Interestingly,which is emitted at higher energy strongly resembles φ L, n =2 with an overlap of 0.93 leading to this simple interpretation of temperature effects: finite temperature leads to the emission of a statistical mixture between the expected n =1 Lorentzian wavefunction and the wavefunctions corresponding to higher excitations numbers n > 1. Importantly, our observations are not related to imperfections of the emission drive or to errors of our extraction method, but only to thermal effects. This probabilistic description of the electron state stems from the nonzero entropy of the finite-temperature ground state, which reveals the statistical (non-quantum) fluctuations of the ground state. This interpretation is confirmed by numerical calculations. Applying our method on perfect periodic Lorentzian pulses calculated using Floquet scattering theory (see Supplementary Note 2), we recover that for T el = 0 K,and(pure state) but for T el = 50 mK,and, in very good agreement with our experimental results.

---

### Machine learning the dimension of a fano variety [^0ef68a27]. Nature Communications (2023). High credibility.

Data generation: weighted projective spaces

The following result characterises weighted projective spaces with terminal quotient singularities; this is [ref. Proposition 2.3].

Proposition 1

Letbe a weighted projective space of dimension at least three. Then X has terminal quotient singularities if and only iffor each k {2, …, a − 2}. Here a = a 1 + a 2 + ⋯ + a N and { q } denotes the fractional part q − ⌊ q ⌋ of.

A simpler necessary condition is given by [ref. Theorem 3.5]:

Proposition 2

Letbe a weighted projective space of dimension at least two, with weights ordered a 1 ≤ a 2 ≤ … ≤ a N. If X has terminal quotient singularities then a i / a < 1/(N − i + 2) for each i {3, …, N }.

Weighted projective spaces with terminal quotient singularities have been classified in dimensions up to four,. Classifications in higher dimensions are hindered by the lack of an effective upper bound on a.

We randomly generated 150,000 distinct weighted projective spaces with terminal quotient singularities, and with dimension up to 10, as follows. We generated random sequences of weights a 1 ≤ a 2 ≤ … ≤ a N with a N ≤ 10 N and discarded them if they failed to satisfy any one of the following:
for each i {1, …, N },, whereindicates that a i is omitted;
a i / a < 1/(N − i + 2) for each i {3, …, N };
for each k {2, …, a − 2}.

Condition 1 here was part of our definition of weighted projective spaces above; it ensures that the set of singular points inhas dimension at most N − 2, and also that weighted projective spaces are isomorphic as algebraic varieties if and only if they have the same weights. Condition 2 is from Proposition 2; it efficiently rules out many non-terminal examples. Condition 3 is the necessary and sufficient condition from Proposition 1. We then deduplicated the sequences. The resulting sample sizes are summarised in Table 1.

Table 1 
The distribution by dimension in our datasets

---

### Spin dephasing under nonlinear gradients: implications for imaging and field mapping [^eaf2803c]. Magnetic Resonance in Medicine (2012). Low credibility.

This work examines the prototypical MR echo that would be expected for a voxel of spins evolving in a strong nonlinear field, specifically focusing on the quadratic z(2) - ½(x(2) + y(2) ) field. Dephasing under nonlinear gradients is increasingly relevant given the growing interest in nonlinear imaging, and here, we report several notable differences from the linear case. Most notably, in addition to signal loss, intravoxel dephasing under gradients creating a wide and asymmetric frequency distribution across the voxel can cause skewed and nonlinear phase evolution. After presenting the qualitative and analytical origins of this difference, we experimentally demonstrate that neglecting these dynamics can lead to significant errors in sequences that assume phase evolution is proportional to voxel frequency, such as those used for field mapping. Finally, simplifying approximations to the signal equations are presented, which not only provide more intuitive forms of the exact expression but also result in simple rules to predict key features of the nonlinear evolution.

---

### Induced dipole moments in amorphous ZnCdS catalysts facilitate photocatalytic hevolution [^eaef88bb]. Nature Communications (2024). High credibility.

Amorphous semiconductors without perfect crystalline lattice structures are usually considered to be unfavorable for photocatalysis due to the presence of enriched trap states and defects. Here we demonstrate that breaking long-range atomic order in an amorphous ZnCdS photocatalyst can induce dipole moments and generate strong electric fields within the particles which facilitates charge separation and transfer. Loading 1 wt.% of low-cost Co-MoS x cocatalysts to the ZnCdS material increases the H 2 evolution rate to 70.13mmolg -1 h -1 , which is over 5 times higher than its crystalline counterpart and is stable over the long-term up to 160h. A flexible 20cm × 20cm Co-MoS x /ZnCdS film is prepared by a facile blade-coating technique and can generate numerous observable H 2 bubbles under natural sunlight, exhibiting potential for scale-up solar H 2 production.

---

### T-cell stimuli independently sum to regulate an inherited clonal division fate [^07f71eaa]. Nature Communications (2016). Medium credibility.

Estimating the CDF of a sum of independent random variables

The maximum likelihood estimator (MLE) for the cumulative distribution function (CDF)of the sum of two independent random variables, given observations of each of them, is the convolution of their empirical CDFs (eCDFs). That is, given independent and identically distributed observations ofand, we have

Thus the MLE for the CDF of X + Y given n observations of X and m observations of Y is the convolution of the eCDF of X with the eCDF of Y :

where χ (x i + y j ≤ z) is 1 if x i + y j ≤ z and 0 otherwise.

To determine the eCDF of the mDD and maxDD of a sum of two conditions assuming that the DD influence of the signals was independent, we evaluated the eCDF for each of the signals and convolved those using custom software executed in Matlab and Python to obtain the MLE CDF for the summed condition. This procedure led to Fig. 5 and Supplementary Fig. 7.

Bootstrap confidence intervals

The reported asymmetric 95% confidence Intervals (CIs) were generated by creating bootstrap samples of each individual condition, determining their eCDFs, convolving those bootstrap eCDFs across two conditions to obtain an estimate of the CDF of the impact of the summed signals, and using percentile bootstrap. Given observations x 1,…, x n and of y 1,…, y m of, say, maxDD of two conditions, we createdbootstrap samples of each data set and determined the MLE of the CDF of their sumfor k =1,…, K. To determine the lower, L (z), and upper, U (z), CIs at each point z of the MLE of the CDF we solve the following minimization problem

---

### Topological regression as an interpretable and efficient tool for quantitative structure-activity relationship modeling [^0c5de2be]. Nature Communications (2024). High credibility.

Neighborhood training model

Customarily, the weights are expressed as a deterministic function of the distances in the predictor space. In standard KNN regression, we assume that distance in the predictor space is proportional to the distance in the response space. In metric learning, a transformation of the predictor space is learned such that there is an approximate isometry between the transformed predictor space and the response space. In TR, we instead write a formal statistical model to connectwith the squared Euclidean distances in the predictor space in the following fashion:

We define the weightsand since we have I * and I to be disjoint and the responses could be assumed to be absolutely continuous, we can definewith the entries in, i.e.being real quantities. Define the squared Euclidean distance matrix in the predictor space as

We define a simple multivariate linear regression model connectingwith D X. Consider the m th row of. Observe that, this row consists of the weights used to express the m th response in I using all the responses in I *. We envision this row to be a set of repeated measurements taken on the m th point in I from the vantage points in I *. Thus, denoting the K elements in the m th row ofby, the corresponding row of predictors in D X by, and the matrix of regression coefficients bywe arrive at the following regression modelwith. Now assuming mutual independence across the N rows ofand since N > K (by construction), we can obtain the MLEs of B and Σ. Letanddenote their respective estimates. Then, for a new query point, we can computeand, using, obtain the predictions. However, observe that (1) requires (W 1, q u e r y, W 2, q u e r y, ⋯, W K, q u e r y) to generate a prediction for the query point, and simply exponentiating the output,, of (6) will yield a biased estimate of W becausedue to Jensen’s inequality. Therefore, we use the properties of the multivariate log-normal distribution to improve the estimate of W in the following way:

---

### The X-files in immunity: sex-based differences predispose immune responses [^66cdc7a7]. Nature Reviews: Immunology (2008). Medium credibility.

Despite accumulating evidence in support of sex-based differences in innate and adaptive immune responses, in the susceptibility to infectious diseases and in the prevalence of autoimmune diseases, health research and clinical practice do not address these distinctions, and most research studies of immune responses do not stratify by sex. X-linked genes, hormones and societal context are among the many factors that contribute to disparate immune responses in males and females. It is crucial to address sex-based differences in disease pathogenesis and in the pharmacokinetics and pharmacodynamics of therapeutic medications to provide optimal disease management for both sexes.

---

### Estimating the success of re-identifications in incomplete datasets using generative models [^cfb6f216]. Nature Communications (2019). High credibility.

Results

Using Gaussian copulas to model uniqueness

We consider a dataset, released by an organization, and containing a sample ofindividuals extracted at random from a population of n individuals, e.g. the US population. Each row x (i) is an individual record, containing d nominal or ordinal attributes (e.g. demographic variables, survey responses) taking values in a discrete sample space. We consider the rows x (i) to be independent and identically distributed, drawn from the probability distribution X with, abbreviated p (x).

Our model quantifies, for any individual x, the likelihood ξ x for this record to be unique in the complete population and therefore always successfully re-identified when matched. From ξ x, we derive the likelihood κ x for x to be correctly re-identified when matched, which we call correctness. If Doe’s record x (d) is unique in, he will always be correctly re-identified (and). However, if two other people share the same attribute (not unique,), Doe would still have one chance out of three to have been successfully re-identified. We modelas:and κ x as:with proofs in “Methods”.

We model the joint distribution of X 1, X 2, … X d using a latent Gaussian copula. Copulas have been used to study a wide range of dependence structures in finance, geology, and biomedicineand allow us to model the density of X by specifying separately the marginal distributions, easy to infer from limited samples, and the dependency structure. For a large sample spaceand a small numberof available records, Gaussian copulas provide a good approximation of the density using only d (d − 1)/2 parameters for the dependency structure and no hyperparameter.

The density of a Gaussian copula C Σ is expressed as:with a covariance matrix Σ, u ∈ [0, 1] d, and Φ the cumulative distribution function (CDF) of a standard univariate normal distribution.

---

### Approximations to the expectations and variances of ratios of tree properties under the coalescent [^8355e038]. G3 (2022). Medium credibility.

Properties of gene genealogies such as tree height (H), total branch length (L), total lengths of external (E) and internal (I) branches, mean length of basal branches (B), and the underlying coalescence times (T) can be used to study population-genetic processes and to develop statistical tests of population-genetic models. Uses of tree features in statistical tests often rely on predictions that depend on pairwise relationships among such features. For genealogies under the coalescent, we provide exact expressions for Taylor approximations to expected values and variances of ratios Xn/Yn, for all 15 pairs among the variables {Hn,Ln,En,In,Bn,Tk}, considering n leaves and 2≤k≤n. For expected values of the ratios, the approximations match closely with empirical simulation-based values. The approximations to the variances are not as accurate, but they generally match simulations in their trends as n increases. Although En has expectation 2 and Hn has expectation 2 in the limit as n→∞, the approximation to the limiting expectation for En/Hn is not 1, instead equaling π2/3-2≈1.28987. The new approximations augment fundamental results in coalescent theory on the shapes of genealogical trees.

---

### The biological basis of sex differences in athletic performance: consensus statement for the American College of Sports Medicine [^3b4403a2]. Medicine and Science in Sports and Exercise (2023). High credibility.

New York City marathon—sex difference in first-place running velocity correlates with participation ratio, with a positive association between the ratio of men-to-women finishers and the sex difference in velocity (r = 0.58, r2 = 0.34, P < 0.001), a linear regression of y = 10.4 + 0.98x (n = 184 pairs), prediction that a ratio of 1 yields an 11.4% sex difference, and an averaged first-place sex difference of 16.9%; illustratively, a ratio of 6 means that for every female runner in a 10-yr age group, there were six male runners, and the dataset spans 31 yr.

---

### Perfect linear optics using silicon photonics [^5e7cfabb]. Nature Communications (2024). High credibility.

In order to ease the understanding of the programming procedure, we indicatively illustrate in Fig. 2b the sum ofandexperimental values for the 4 constituent paths of the 1st Xbar column with r = [1,2,3,4]. These values were calculated via the HA model when a single node is driven in the rangeand the remaining 3 EAMs are driven at 0 V bias, revealing the divergence from the expected performance when using the standalone EAM metrics (baseline model). Figure 2c schematically captures the effectiveness of the developed HA programming model, by putting in juxtaposition the experimentally measured Y 1 output for the #256 linear transformation sets (black dashed line) with respect to the theoretically calculated Y 1 output when using the baseline programming model (gray curve) and the theoretically calculated Y 1 output when using the HA programming model (orange curve), revealing the excellent matching between the experimental curve and the curve calculated via the HA model. A quantitative representation of the model’s accuracy in describing the underlying hardware parameters is presented in Fig. 2d, where the error of the baseline and HA programming model outputs with respect to the experimentally obtained response is plotted with the gray and orange histograms, respectively. Fitting a Gaussian distribution into the acquired error values results in mean and standard deviation values that equal −5.45 and 2.04, and ~2 * 10 −3 and 0.41 for the baseline and HA models respectively, revealing the lack of a biased-error parameter in our HA-model and a significantly lower standard deviation. The same procedure was followed for the remaining 3 Xbar columns and 12 constituent EAM-based nodes, revealing a small deviation of <2% in the derived mean and standard deviation values.

---

### Budding yeast complete DNA synthesis after chromosome segregation begins [^ed47d512]. Nature Communications (2020). High credibility.

Normalized coverage for each window was computed separately for each sequenced sample and each chromosome by dividing the read coverage for each window on that chromosome by the mode of all coverage values for all windows on that chromosome (copy number or CN). To each CN we applied a local regression using a weighted linear least squares and a first degree polynomial model (with a 5 kb span) to decrease the impact of technical noise. For each mutant we calculated copy number ratio (CNR) for each 200-bp window as normalized copy number in M-phase divided by the normalized copy number in G1-phase. The percent of cells that have not yet replicated a given region of the genome (under-replication), is defined as 2×(1-CNR).

We expect the measured copy number in G1-arrested cells to be 1 across all loci in the genome. However, multiple factors, such as GC-content and mappability, make this not true. We assume that the effect of such factors will be similar between identical loci in different cell-cycle stages (G1 and M) and therefore locus-specific effects on measured read counts can be eliminated through normalization of CN in M and G1 phases. This normalized copy number is the copy number ratio (CNR). To calculate the percent of cells that have not yet replicated a given locus, from CNR, we do the following. Let us denote that X (0 ≤ X ≤ 1) is the fraction of cells that yet didn’t replicate a given locus in M-phase. Accordingly, 1 − X is the fraction of cells that have finished replication of this locus. Therefore, in a mixed population of cells, some of which have finished replication, and others that haven’t, the locus will have a total representation of X + 2 × (1 − X) copies. Next, we assume that most of the genome has completed replication in all cells before entering M phase. The majority of loci (which corresponds to the mode of coverage which is used as normalization factor) will have total representation of 2 copies in M phase. All together, it gives us the linear equation: CNR = (X + 2 × (1 − X)) / 2. Therefore, X, the fraction of cells that have not yet replicated a given locus, is X = 2 × (1 − CNR).

---

### Accurate prediction of protein folding mechanisms by simple structure-based statistical mechanical models [^017809f7]. Nature Communications (2023). High credibility.

Kinetic analysis for hen egg-white lysozyme

The microscopic rate constant of the reaction from state X to Y was given by, whereis the free energy difference between state X and the transition state and is obtained from the 2D free energy landscape.is a pre-exponential factor for crossing the transition state at order parameter. This value was calculated asconsidering internal friction,.is the degree of contact formation during the folding process. Since this number is approximately proportional to the square of the number of residues, we usedfor simplicity.was determined to simulate the experimentally observed folding rates at 293 K. The time-dependent changes in the concentrations of U, I 1, I 2, and N were calculated as described below.

The kinetic analysis was performed according to the following scheme:which gives the following matrix equation:whereand.

Under native conditions, where the free energy barriers between N and TS 3 and between N and TS 2 are large, bothandare satisfied. Then, we can neglect the unfolding of the native state, as follows:In this scheme, the concentrations of the native states formed through Pathways 1 and 2 are denoted as N p1 and N p2, respectively. Here, vector C and the matrix M in Eq. (42) are written as follows:

The matrix equation was solved analytically using Mathematica 12.2 (Wolfram), and the time-dependent changes in U, I 1, I 2, N p1, N p2, and N (= N p1 + N p2) were obtained under the following initial conditions: U(0) = 1 and I 1 (0) = I 2 (0) = N p1 (0) = N p2 (0) = 0.

---

### Mineral oil [^c1c1ab7e]. FDA. Low credibility.

The drug Mineral oil.

---

### The discriminatory value of cardiorespiratory interactions in distinguishing awake from anaesthetised states: a randomised observational study [^a52268b8]. Anaesthesia (2015). Low credibility.

We used a distance‐based classification technique, called the nearest neighbour classifier 31, where each subject is assigned a vector x based on their measured signals and derived values. The (squared) distance between two subjects x 1 and x 2 is calculated as D(x 1,x 2) 2 = (x 1 − x 2) T A (x 1 − x 2), where T indicates the transpose of the original matrix. The distance measure (i.e. the entries in the positive symmetric matrix A) is determined through a procedure described in online Appendix S3, where further details are given [38].

The results of a classifier are assessed by inspecting the ‘confusion matrix’ [38] (see Table A2 of the online supplementary information). In brief, the confusion matrix contrasts actual states/classifications (rows) with classifications/states (columns) predicted by the classifier. The diagonal entries correspond to the number (likelihood) of correct classifications and off‐diagonal entries to the number (likelihood) of misclassifications: the likelihood entries in one row sum up to 100%. Further details can be found in Appendix S3.

---

### Oxytocin [^e1cf39bf]. FDA (2019). Medium credibility.

The drug Oxytocin.

---