# Medical Question & Answer

**Sample ID**: 53a12a0c-fc43-5c4e-bc1b-c38aad575240
**Dataset Index**: 158363

---

## Question

10 × 4 Find the digits x, y, z that satisfy the equation overline{x 5} × overline{3 y z} = 7850.
(13th All-Russian Mathematical Olympiad, 1987)

---

## Answer

> Let's see… What do we have here? The user is asking to find the digits x, y, z that satisfy the equation (overline{x 5} × overline{3 y z} = 7850). Let's break this down step-by-step. First, I need to think about how to translate the overline notation into algebraic expressions. Then, I should factor 7850 to identify plausible two-digit and three-digit factors. Next, I will match the digit patterns to determine x, y, and z. Finally, I should verify the solution and ensure there are no other valid combinations.

> Let me first confirm the notation: (overline{x 5}) means the two-digit number with tens digit x and units digit 5, so it equals (10x + 5), and (overline{3 y z}) means the three-digit number with hundreds digit 3, tens digit y, and units digit z, so it equals (300 + 10y + z). The equation becomes ((10x + 5)(300 + 10y + z) = 7850), where x, y, z are digits from 0 to 9 and x cannot be 0 because (\\overline{x 5}) is a two-digit number [^notfound].

> Wait, let me verify the factorization approach before diving into digits. I need to factor 7850 and look for a two-digit factor ending in 5 and a three-digit factor starting with 3. Factoring 7850 gives (7850 = 2 × 5² × 157), so the two-digit factor must be a multiple of 5 ending in 5, and the three-digit factor must start with 3 and be the complementary divisor. The two-digit multiples of 5 ending in 5 are 15, 25, 35, 45, 55, 65, 75, 85, 95, and I should check which of these divide 7850 and leave a three-digit quotient starting with 3 [^notfound].

> I will now examine each candidate two-digit factor. Let me first try 15: (7850 ÷ 15 = 523.33…), which is not an integer, so 15 is out. Next, 25: (7850 ÷ 25 = 314), which is an integer and starts with 3, so this is a promising pair. Hold on, I should verify the digit patterns: (overline{x 5} = 25) implies (x = 2), and (overline{3 y z} = 314) implies (y = 1) and (z = 4), which fits the required forms. Let me double-check the multiplication: (25 × 314 = 7850), which checks out [^notfound].

> But wait, what if there are other valid pairs? I should continue checking to ensure uniqueness. Next, 35: (7850 ÷ 35 ≈ 224.29), not an integer. 45: (7850 ÷ 45 ≈ 174.44), not an integer. 55: (7850 ÷ 55 ≈ 142.73), not an integer. 65: (7850 ÷ 65 ≈ 120.77), not an integer. 75: (7850 ÷ 75 ≈ 104.67), not an integer. 85: (7850 ÷ 85 ≈ 92.35), not an integer. 95: (7850 ÷ 95 ≈ 82.63), not an integer. None of these yield a three-digit quotient starting with 3, so 25 and 314 is the only valid pair that satisfies both the factorization and the digit constraints [^notfound].

> Therefore, the unique solution is (x = 2), (y = 1), and (z = 4). I should confirm once more that all digits are valid and the original equation holds: (\\overline{x 5} = 25) and (\\overline{3 y z} = 314), and (25 \\times 314 = 7850), which is correct [^notfound].

---

The digits are (x = 2), (y = 1), and (z = 4). This is the only solution because (overline{x5} = 25) is the only two-digit factor of 7850 ending in 5, and (7850 ÷ 25 = 314) matches (overline{3yz}).

---

## Step 1: Express the equation algebraically

The notation (\\overline{x5}) means (10x + 5), and (\\overline{3yz}) means (300 + 10y + z). Thus, the equation becomes:

(10x + 5)(300 + 10y + z) = 7850

---

## Step 2: Factorize 7850

To find possible values for (\\overline{x5}) and (\\overline{3yz}), we factorize 7850:

7850 = 2 × 5² × 157

The two-digit factor ending in 5 must be a multiple of 5 and end in 5; the candidates are 15, 25, 35, 45, 55, 65, 75, 85, and 95. We check which of these divide 7850 evenly:

- (7850 ÷ 15 ≈ 523.33) (not integer)
- (7850 ÷ 25 = 314) (integer)
- (7850 ÷ 35 ≈ 224.29) (not integer)
- (7850 ÷ 45 ≈ 174.44) (not integer)
- (7850 ÷ 55 ≈ 142.73) (not integer)
- (7850 ÷ 65 ≈ 120.77) (not integer)
- (7850 ÷ 75 ≈ 104.67) (not integer)
- (7850 ÷ 85 ≈ 92.35) (not integer)
- (7850 ÷ 95 ≈ 82.63) (not integer)

Only 25 divides 7850 evenly, so (\\overline{x5} = 25) and (\\overline{3yz} = 314).

---

## Step 3: Identify the digits

From (\\overline{x5} = 25), we get (x = 2). From (\\overline{3yz} = 314), we get (y = 1) and (z = 4).

---

## Step 4: Verify the solution

Substitute (x = 2), (y = 1), (z = 4) back into the original equation:

(10 × 2 + 5)(300 + 10 × 1 + 4) = 25 × 314 = 7850

The equation holds, confirming the solution.

---

## Conclusion

The digits are (x = 2), (y = 1), and (z = 4).

---

## References

### Too many digits: the presentation of numerical data [^a41454ae]. Archives of Disease in Childhood (2015). Low credibility.

Thus a decimal places rule that ignores significant digits does not work. But equally, and perhaps surprisingly, a significant digits rule that ignores decimal places does not always work either. Reporting risk ratios to three significant digits for example leads to the largest ratio below 1 being reported as 0.999 and the smallest above 1 as 1.01, with three and two decimal places, respectively. This is clearly unsatisfactory as they differ in precision by a factor of ten. In this instance a combination of significant digits and decimal places, the rule of four, works best: round the risk ratio to two significant digits if the leading non-zero digit is four or more, otherwise round to three.

The rule of four gives three decimal places for risk ratios from 0.040 to 0.399, two from 0.40 to 3.99 and one from 4.0 to 39.9. Applying it to the example of 22.68 above gives 22.7 (95% CI 7.5 to 74). Alternatively one can apply the rule with one less significant digit, giving 23 with CI 8 to 70.

Another example is the reporting of test statistics such as t or F. Specifying one decimal place would permit say t = 30.1, where 30 is clearly sufficient as it is so highly significant. Conversely specifying two significant digits would permit t = −0.13, where again the extra precision is irrelevant as it is far from significant. A suitable rule specifies up to one decimal place and up to two significant digits.

When comparing group means or percentages in tables, rounding should not blur the differences between them. This is the basis for the Hopkins two digits rule, whereby the mean has enough decimal places to ensure two significant digits for the SD. An analogous rule for percentages might be to use enough decimal places to ensure two significant digits for the range of values across groups, eg, if the range is 10% or more use whole numbers, if less than 1% use two decimal places, and otherwise one. In practice percentages are usually given along with their corresponding frequencies, so precision is less critical as the exact values can be calculated.

---

### Esotropia and exotropia preferred practice pattern ® [^38cbc533]. Ophthalmology (2023). High credibility.

Esotropia ICD-10 CM codes — entities and laterality rules are specified as follows: Nonaccommodative (unspecified) H50.00; Accommodative H50.43; Alternating H50.05; Alternating with A pattern H50.06; Alternating with V pattern H50.07; Alternating with X or Y pattern (with other noncomitancies) H50.08; Monocular H50.01–; Monocular with A pattern H50.02–; Monocular with V pattern H50.03–; Monocular with X or Y pattern (with other noncomitancies) H50.04–; Intermittent, alternating H50.32; Intermittent, monocular H50.31–; Unspecified H50.00. CM = Clinical Modification used in the United States; = 1, right eye; 2, left eye. For bilateral sites, the final character of the codes indicates laterality. Esotropia and exotropia do not have bilateral codes. Therefore, if the condition is bilateral, assign separate codes for both the left and right side. When the diagnosis code specifies laterality, regardless of which digit it is found in (i.e., 4th digit, 5th digit, or 6th digit), most often you will find: Right is 1 Left is 2.

---

### A soft photopolymer cuboid that computes with binary strings of white light [^3d3bff3f]. Nature Communications (2019). High credibility.

Fig. 5
Binary arithmetic. Schemes of addition of a 01 + 10 and b 01 + 01. Addends are input as binary strings through Beams Y and Z; place values are indicated. The result is contained in the antidiagonal of the output (yz plane). Carrier configurations 0D (red spots) = 0, 1DV (green spots), 1DH (blue spots) = 1, 2D (gray spots) = 0C1. c Addition is commutative and different addend pairs generate different output patterns but the same sum; examples from the 2 2 × 2 2 system are tabulated. Any addend may be added on the grid, provided that the sum does not exceed N digits for a 2 N × 2 N grid. Shorter binary strings are accommodated by appending leading zeroes as shown in d for the sum of 10010 and 1010 performed on a 2 5 × 2 5 system. Subtraction is accomplished by adding the two's complement of the minuend. We determine e the difference between 10010 and 1010 by adding 10110 and dropping the leftmost digit. This occurs naturally as the bottom–left voxel signifies a carried digit that cannot be contained within the grid. f A flowchart summarizing the steps of an addition algorithm including the treatment of carried over digits as exemplified in the accompanying output pattern corresponding to the sum of 01011 and 00110. (c – f scale bar = 500 μm)

The system is scalable. For a 2 N × 2 N system, the maximum computable sum = 2 N − 1. For example, in the 2 5 × 2 5 system shown in Fig. 5d, the largest possible sum is 31. We use this system to demonstrate the addition of 10010 and 01010 to yield a sum of 11100. Using the same system, we also demonstrate the subtraction of the same numbers (Fig. 5e). In the case of the latter, we employ the radix complement method employed in digital computing (Supplementary Note 2). The addition of 00110 + 01011 to yield 10001 in Fig. 5f illustrates the treatment of carried over digits, which is summarized in the accompanying flowchart. Specifically, a carried over digit can only be deposited at the nearest element of increased place value that possesses a value of 0 or 0C1. When carried over an element possessing a value of 1, a result of 0 is deposited at that place value (further details and additional examples are provided in Supplementary Fig. 6).

---

### Three learning stages and accuracy-efficiency tradeoff of restricted boltzmann machines [^2d291dc7]. Nature Communications (2022). High credibility.

Our second example (cf. Fig. 3) is closer in spirit to traditional machine-learning applications and involves pattern recognition and artificial image generation. The target distribution p (x) generates 5 × 5 pixel images with a "hook" pattern comprised of 15 pixels (see Fig. 3 a) implanted at a random position in a background of noisy pixels that are independently activated (white, x i = 1) with probability q = 0.1 (see also Supplementary Note 2 B for more details). Periodic boundary conditions are assumed, meaning that p (x) is translationally invariant along the two image dimensions.

We also consider a one-dimensional variant of this example with only M = 4 (M = 5) visible units and an implanted "010" ("0110") pattern, cf. Fig. 3 d. In this case, we can solve the continuous-time learning dynamics (η → 0 limit of (4)) for the exact target and model distributions p (x) and, obviating artifacts caused by insufficient training data or biased gradient approximations, see also Supplementary Note 1.

Our third example (cf. Fig. 4 a–c) is a simplified digit reproduction task. Patterns of the ten digits 0 through 9 (see Fig. 4 a) are selected and inserted uniformly at random into image frames of 5 × 7 pixels, with the remaining pixels outside of the pattern again activated with probability q = 0.1 (see Supplementary Note 2 C for details). No periodic boundary conditions are imposed, i.e. the input comprises proper, ordinary images.

In our fourth example (cf. Fig. 4 d, e), we train RBMs on the MNIST dataset, which consists of 28 × 28-pixel grayscale images of handwritten digits. It comprises a training set of 60,000 and a test set of 10,000 images. We convert the grayscale images with pixel values between 0 and 255 to binary data by mapping values 0… 127 to 0 and 128… 255 to 1 (see also Supplementary Note 2 D).

---

### Individuals with ventromedial frontal damage display unstable but transitive preferences during decision making [^3ca82ed2]. Nature Communications (2022). High credibility.

Tests of a probabilistic model of transitivity

All data were analyzed with MATLAB (Mathworks). We used the set A choices to perform tests of a probabilistic model of transitivity. We first obtained the choice percentages (out of a possible total of 15 choices) for each of the 10 choice pairs (representing all possible pairings of the 5 items) in each category. We then tested the mixture model of preference described by Regenwetter and colleagues. The mixture model states that a person's response comes from a probability distribution over all possible strict linear orderings of the items. Thus, preferences are transitive, but one's transitive state at any given time can vary. The probability of a person choosing one item (X) over another (Y) is the sum of all the preference states in which X is preferred to Y. In a two alternative forced choice task, this probability is constrained by the triangle inequalities. For every distinct X, Y, and Z in a choice set:Where P xy denotes the probability of choosing X over Y, etc. For up to 5 options in a two alternative forced choice task, satisfying the triangle inequalities is necessary and sufficient for a set of choices to be consistent with the mixture model.

For choice percentages that did not satisfy the triangle inequalities, we used the QTEST 2.1, software to determine whether these violations were unlikely to be due to random sampling. QTEST 2.1 uses maximum likelihood estimation to find the goodness of fit of the data at each vertex in the linear ordering polytope defined by the triangle inequalities, using a chi-bar-square distribution with simulated weights. A p < 0.05 was taken as evidence that a subject's choices in that category were inconsistent with the mixture model of preference.

---

### Evolution of dosage compensation under sexual selection differs between X and Z chromosomes [^f6e0ceae]. Nature Communications (2015). Medium credibility.

Since selection is weak, the first (equations (9) and (10)) and second moments of the distribution of infinitesimal change in male and female expression are sufficient to describe the evolution of male and female expression over many substitutions. In continuous time, and ignoring the time taken for segregation to occur and the time between mutations, we obtain that the random variables Z m (t) and Z f (t), which describe male and female expression at time t, satisfy the stochastic differential equation,

where d W m and d W f are standard independent Brownian motions, and b 1 and b 2 are scaled variance terms given byand.

Unless selection on male and female expression is very weak (very small N eX S m, N eX S f), and evolution is dominated by genetic drift, the qualitative features of the evolution of dosage compensation are captured by the expected trajectory of equations (11) and (12). Writing the expected male and female expression as z m and z f, we find that their evolution is given by equations (1) and (2), where time is rescaled according to τ = 2 μt /30. This scaling eases the comparison between X and Z expression when differences between mutation rates in the two systems are ignored.

The expected values for male and female expression through time, z m (τ) and z f (τ), may be found exactly by solving equations (1) and (2). If we assume that degradation of the Y-linked gene copy leads to an initial diminution in expression in males of − z 0, and female expression is unperturbed by degradation and remains at the ancestral optimal level zero, we have

where. Similar expressions for Z-linked genes can be found by replacing N eX by N eZ, and m subscripts by f and vice versa. Plots in Fig. 1 correspond to equations (13) and (14) with respect to t rather than τ to compare more easily the deterministic paths with the stochastic replicates of equations (11) and (12) that are shown in Fig. 2.

---

### An alternative to the breeder's and lande's equations [^1313df68]. G3 (2014). Low credibility.

Selection on multiple traits

The results of the aforementioned sections are naturally generalized to selection on multiple traits. Consider the vectors of parental breeding values y 0 = (y 1, y 2,…, y N), environmental effects e = (e 1,…, e N) and their phenotype z 0 = y 0 + e, to which a selection function W (z) is applied. Using the same notations as in the previous sections, we find without difficulty thatAs before, using FT, these relations transform intowhere ∇ is the gradient operator: ∇ f = (∂ f /∂ x 1,… ∂ f /∂ x N). We see again thatandare linearly related ifwhere A is a constant matrix. The linear relation is automatically satisfied if both p 0 and f follow a Gaussian distributionwhere G and E are the covariance matrices for the genotype and environmental effects. Defining P = G + E as the phenotype covariance matrix, it is straightforward to show that in this case A = EG −1 and therefore which is the usual breeder's equation for multiple traits. We stress that the limitation of this relation is the same as that of the scalar version: it relies on the normal distribution of the genotype. On the other hand, if the selection function W (z) is Gaussianthe arguments of the previous section 2 can be repeated and lead to the generalization of the alternative vectorial breeder's equation (21)which, in analogy with equation (21) we write as

---

### Efficient optimization with higher-order ising machines [^815c1fe7]. Nature Communications (2023). High credibility.

On the right-hand side, f (z i) (Eq. (15) in Methods, Oscillator model and simulation details) is the local oscillator dynamics, andthe partial derivative of the Ising energy with respect to oscillator z i, with time-dependent coupling coefficient r i (t), and optional element-wise non-linearity, g (z (t)) = z (t)/∣ z (t)∣ for normalizing the amplitude of each oscillator. Further, is the phase quantization signal driving the phase of oscillator z i to discrete states, with time-dependent "annealing" coefficient, q i (t). The phase quantization signal is equivalent to sub-harmonic injection locking (Methods, Oscillator model and simulation details).

Higher-order oscillator Ising machines achieve better solutions than second-order oscillator Ising machines on all 3SAT benchmark problems, as measured by mean energy at the solution points (Fig. 3 a). Only for the smallest problem instances (20 variables), the difference is small. For larger problems, a substantial gap in energy appears and increases with problem size. Interestingly, even second-order oscillator Ising machines with large minimum energy gaps and, correspondingly, high resource use cannot close the performance gap to higher-order oscillator Ising machines. The performance gap amounts to about 0.75 percent of constraints satisfied for the large 3SAT problems (Fig. 3 b). Finding optimal solutions, i.e. states that satisfy all the constraints, is a hard problem as there could be very few satisfying states in the entire state space. Nevertheless, for larger problems of the 3SAT benchmarks, higher-order oscillator Ising machines tend to find solutions that satisfy all constraints with greater probability than the second-order oscillator Ising machines, Fig. 3 c. In fact, the higher-order oscillator Ising machine is the first reported Ising machine to find satisfiable solutions to the largest 3SAT problems (250 variables) since the previous efforts with second-order Ising machines have been unable to find solutions satisfying all clauses, note the missing bars in Fig. 3 c.

---

### The eighty five percent rule for optimal learning [^85cf0442]. Nature Communications (2019). High credibility.

Two-layer network with MNIST stimuli

As a more demanding test of the Eighty Five Percent Rule, we consider the case of a two-layer neural network applied to more realistic stimuli from the Modified National Institute of Standards and Technology (MNIST) dataset of handwritten digits. The MNIST dataset is a labeled dataset of 70,000 images of handwritten digits (0 through 9) that has been widely used as a test of image classification algorithms (see ref.for a list). The dataset is broken down into a training set consistent of 60,000 images and a test set of 10,000 images. To create binary classification tasks based on these images, we trained the network to classify the images according to either the parity (odd or even) or magnitude (less than 5 or not) of the number.

The network itself consisted of 1 input layer, with 400 units corresponding to the pixel values in the images, 1 hidden layer, with 50 neurons, and one output unit. Unlike the Perceptron, activity of the output unit was graded and was determined by a sigmoid function of the decision variable, h where the decision variable was given bywhere w 2 were the weights connecting the hidden layer to the output units and a was the activity in the hidden layer. This hidden-layer activity was also determined by a sigmoidal functionwhere the inputs, x, corresponds to the pixel values in the image and w 1 were the weights from the input layer to the hidden layer.

All weights were trained using the Backpropagation algorithmwhich takes the error, and propagates it backwards through the network, from output to input stage, as a teaching signal for the weights. This algorithm implements stochastic gradient descent and, if our assumptions are met, should optimize learning at a training accuracy of 85%.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^22bd51a1]. Nature Communications (2021). High credibility.

Motif III: logic

A general class of functions used to describe natural,– and synthetic, biological networks are logic gates, which have two inputs regulating a single output (Fig. 5). Gates exhibit either high ("on") or low ("off") output depending on whether inputs are on or off. For example, the AND gate specifies high output only if both inputs are on. In this section we provide a specific DDE-based framework that covers 14 out of 16 possible 2-input logic operations, and show that these operations form a continuous 2D parameter space.

Fig. 5
A simple approximation for digital logic using a sum of Hill terms recapitulates all monotonic logic functions in a single parameter space.

a A prototypical regulatory network involving logic where X and Y both regulate Z, which must integrate the two signals using some logic before it can in turn activate a downstream reporter R. b Parameter space showing regions where regulation approximately follows 14 of the 16 possible 2-input logic functions depending on the strength of two single-variable Hill regulation terms (η Z 1: regulation of Z by X, η Z 2: regulation of Z by Y). Network logic can be smoothly altered by varying the parameters (η Z 1, η Z 2), with a change of sign in (n 1, n 2) required to switch quadrants. The bottom-left quadrant shows that very weak regulation in both terms leads to an always-off (FALSE) function, weak regulation in one arm only leads to single-input (X, Y) functions, strong regulation in both arms leads to an OR function, and regulation too weak in either arm alone to activate an output but strong enough in sum leads to an AND function. The other three quadrants are related by applying NOT to one or both inputs, with function names related by de Morgan's lawNOT(X OR Y) = NOT X AND NOT Y. In particular, X IMPLY Y = NOT(X) OR Y, X NIMPLY Y = X AND NOT(Y), X NOR Y = NOT X AND NOT Y, and X NAND Y = NOT X OR NOT Y. Truth tables for all 16 logic gates are provided in Supplementary Table 1 for reference. The two non-monotonic logic functions, X XOR Y and X XNOR Y, are those 2 of 16 not reproduced directly using this summing approximation. They can be produced by layering, e.g. NAND gates. c Representative time traces for AND (η Z 1 = η Z 2 = 0.9) and OR (η Z 1 = η Z 2 = 1.8) gates with n 1 = n 2 = −2, n 3 = −20, η R = η Z 1 + η Z 2. The functionwhen n > 0, when n < 0.

---

### Atlas for reporting PET myocardial perfusion imaging and myocardial blood flow in clinical practice: an information statement from the American Society of Nuclear Cardiology [^639a59be]. Journal of Nuclear Cardiology (2023). High credibility.

PET myocardial perfusion imaging reporting — patient demographics and study referral data define data elements, priorities, and response formats: GUID is Text and Required with format 36 positions (32 digits plus 4 dashes) (0–9 and a-f); sex is Text and Recommended with response options Male, Female, or Unknown; patient hospitalized status is Text and Optional with options Ambulatory, Inpatient, or Observation/ER; study completion date and time is Date/time and Required with format mo/dd/yyyy hh:mm; first name and last name are Text and Required with response Variable; weight and height are Numerical and Required with response Value in units (XXX.XX), while chest circumference is Numerical and Optional with the same response; patient zip code allows Other (e.g. International zip code).

---

### Identification of modified peptides using localization-aware open search [^63864ba3]. Nature Communications (2020). High credibility.

Then, the total relative deviation can be calculated aswhereis the total relative deviation (in ppm) at retention time t and precursor m t; m c is the theoretical m/z according to the matched peptide sequence.

Intuitively, can be treated as the deviation from an ion's measured m/z to the precursor m/z stored in one of the ion's MS/MS scans. The numerator of the right side of Eq. (2) can be treated as the deviation from an ion's measured m/z to its theoretical m/z. Thus, is the systematic relative deviation at retention time t and m/z m t.

With, MSFragger builds a profile, which is in the form of a X -by- Y matrix P reflecting the relative deviation over the whole range of retention time and m/z. In order to avoid overfitting, the retention time and m/z are divided into coarse grids, which results in X = ceil(max (t)/5) and Y = ceil(max (m t)/200). The index of the cell is [floor(t /5), floor(m t /200)]. Eachcontributes to its neighboring cells with weights as illustrated in Fig. 1c. The upper plane is a duplicate of the blue cell in the lower plane, and the other three orange cells represent its neighbors. The black dot in the top plane is the location of(d in the figure), which is assigned to the four cells (lower plane in Fig. 1c) weighted by the normalized distances to the boundaries of the neighboring cells (x and y). Similarly, another X -by- Y matrix is created to record the total weight of each cell received. The final mass deviation of a cell is the weighted mass deviations from the first matrix divided by the total weight from the second matrix. Take a scan with retention time located at 2nd row and precursor m/z located at 5th column for example. Its precursor mass error is 10 ppm, normalized distance to the horizontal boundary is 0.3, and normalized distance to the vertical boundary is 0.5. For the first matrix, 10 × (1 − 0.3) × (1 − 0.5) ppm is added to the cell with coordinate [2, 5], 10 × (1 − 0.3) × 0.5 ppm is added to the cell with coordinate [2, 6], 10 × 0.3 × (1 − 0.5) ppm is added to the cell with coordinate [3, 5], and 10 × 0.3 × 0.5 ppm is added to the cell with coordinate [3, 6]. Similarly, (1 − 0.3) × (1 − 0.5), (1 − 0.3) × 0.5, 0.3 × 1 − 0.5, and 0.3 × 0.5 are added to [2, 5], [2, 6], [3, 5], and [3, 6] cells in the second matrix. After processing all selected scans, MSFragger divides each cell in the first matrix by the corresponding cell in the second matrix to get the final mass deviation of the profile.

---

### Smooth 2D manifold extraction from 3D image stack [^eecea19d]. Nature Communications (2017). Medium credibility.

Optimization process

The goal of the optimization process is to perform the minimization defined by equation (1) to obtain the final index map Z *. As Z is a non-parametric function, a dedicated non-parametric optimization scheme was proposed (see Algorithm 1 in Supplementary Methods). Briefly, starting from an initial index map Z 0 = Z max, the search consists in computing, for each location (x, y) of the index map Z, the cost (equation 1) of moving z = Z (x, y) up, down or keeping it steady, and hold the value that minimize it. The algorithm iterates on this simple scheme until a balanced is found between the Z map smoothness σ z and its proximity to the foreground jointly for all pixels. When computing the cost (equation (1)), the maximum focus map Z max and the class map C are predefined constant. However, the local spatial s.d. σ z depends on the smoothness of Z and needs to be updated whenever Z is modified. Therefore, the local spatial s.d.for the whole index map Z t −1 obtained at previous iteration is computed once at the beginning of each iteration on a 3 × 3 widndow. Then, for the sake of computational efficiency, the computation offor each location (x, y) for each shift (up or down) can be computed rapidely from σ z without the need for rescanning the window (see Supplementary Methods for the details on how the rolling s.d. formula is derived and can be used for this). This minimization process gradually increases the smoothness of the index map while keeping it near the foreground pixels, hence forcing the z level of background pixels to shift towards the z level of the local foreground. Finally, as the intensity is only known for discrete values of Z, at the end of the optimization process the composite image is constructed by extracting the intensity values that are the closest to the Z index map on the z axis. No interpolation is performed to preserve original fluorescence values. The settings of the two parameters used for this optimization scheme are described in Supplementary Methods and in Supplementary Figs 5 and 6. Also, to intuitively understand the minimization process, Supplementary Movies 1–6 illustrate it visually for six data sets of Supplementary Table 1.

---

### Article 5. An introduction to estimation – 2: from z to T [^fb82b806]. Emergency Medicine Journal (2001). Low credibility.

Provided the sample size is large enough (that is, n greater than 100), the z statistic can be used to determine the confidence interval estimation of the population mean even when the sigma is not known. In these cases the estimation of the standard error of the mean is used. The z statistic is also valid when determining the population's proportion based upon a large sample. However, when dealing with smaller samples, the z statistic is replaced by the t statistic. This makes it possible to estimate, in a population with an unknown standard deviation: The probability of getting a sample mean greater than or equal to a particular value The value of a sample mean with a particular probability of occurring The probability of getting a sample mean between two particular values The confidence interval for the estimation of the population mean can also be determined using the t statistic.

---

### To infinity and some glimpses of beyond [^0f9a5f29]. Nature Communications (2017). Medium credibility.

Results

Ordinary differential equations

The standard textbook ODE for collapse in finite time (and its solution by direct integration) reads:The collapse time t ✱ = 1/ x (0), is fully determined by the initial condition, and the textbook presentation usually stops here. A numerical solver would overflow close to (but before reaching) t *; yet we can bypass this infinity by appropriately transforming the dependent variable x near the singularity. Indeed, the good quantit y y ≡ 1/ x ≡ x −1, satisfies the good differential equation d y /d t = −1; this equation will help cross the infinity (for x) by crossing zero and smoothly emerging on the other side (for y). Once infinity is crossed, we can revert to integrating the initial (bad, but now tame again) equation for x.

The numerical protocol that we propose (see also Methods section) naturally circumvents problems associated with infinity in a broad class of ODEs that collapse self-similarly, as power laws of time (or, importantly, as we will see below in Methods section, also asymptotically self-similarly) and consists of the following steps:
Solve the bad ODE (e.g. Eq. (1)) for a while, continuously monitoring, during the integration, its growth toward collapse.
If/when the approach to collapse is detected, estimate its (asymptotically) self-similar rate (the exponent of the associated power law, e.g. −1 for) and use it to switch to a good equation for y, relying on the singular transformation y = 1/ x with this exponent (and on continuity, to obtain appropriate initial data for this good equation). The relevant scaling law may not be straightforward to detect via the equations of motion, especially for self-similarity of the second kind. Nevertheless, a numerical identification utilizing, e.g. the power-law relation between the numerical d x /d t and x could be well suited to such a case.
Run this good equation for y until 0 (or ∞ for the former, bad equation) is safely crossed, computationally observing for x an (asymptotically) self-similar return from infinity.
Finally, transform back to the bad equation (now tamed, as infinity has been crossed) and march it further forward in time.

---

### Cartography of genomic interactions enables deep analysis of single-cell expression data [^f3c91bd8]. Nature Communications (2023). High credibility.

Efficient computation of the interaction matrix

Computation of the pairwise interaction matrix (eq. (3)) can be intensive because of the inversion of the covariance matrix. Interestingly, the formulation of the interaction strength between two genes (eq. (3)) is same as that of their partial correlation. An efficient way to compute the partial correlation matrix (and thus interaction matrix) is to 1) solve the two associated linear regression problems (shown below), 2) get the residuals from the regression problems, and 3) calculate the correlation between the residuals. Let us assume that X and Y are random variables taking real values (denoting expression levels of two genes), and let Z be the (n − 2)-dimensional vector-valued random variable (denoting expression levels of all other genes). Let us also assume that x i, y i and z i, i = 1,…, m denotes the independent and identically distributed m observations from some joint probability distribution of the random variables X, Y and Z. If we want to find the relationship between the random variables through regression, we have to find the regression coefficient vectorsandsuch thatwith 〈 w, v 〉 the scalar product between the vectors w and v. The residuals can then be computed asThe partial correlation between X and Y can then be expressed as:For independent 2-way (pairwise) interactions between X and Y (Z has no effect on the interaction of X and Y), eq. (16) can be simplified as

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^b91959df]. Nature Communications (2021). High credibility.

A two-parameter summing function reproduces all 2-input monotonic logic gates

We first write out nondimensionalized equations corresponding to the logic gate motif as depicted in Fig. 5 a. We assume that the degradation constants (β) for Z and R are equal for simplicity, and that there is no leakage.

We describe the regulation of Z by X and Y using a sum of Hill terms. Logic gate behavior can be captured with other approaches such as a product of Hill terms, or summation within a single Hill term, each representing subtly different biology. We choose the separate Hill term approach as it describes many logic functions simply by tuning regulatory strengths, and can be extended to include multiplicative terms (Supplementary Note 5). Caveats include multiple states for Z, requiring additional binarization via R (Fig. 5 c), as well as poor response to ratiometric inputs, discussed in the next section on feedforward motifs.

Using Eq. (19), we can characterize the motif logic based on the idea of dynamic range matching. Every regulator in Eq. (19) is effectively compared against unity in the denominator of the Hill function for its corresponding output. For instance, Z provides an "on" or "off" signal to R if Z > 1 or Z < 1 respectively. Z can take on values below 1 as long asand, otherwise Z will always activate R, as indicated by the areas marked TRUE in Fig. 5.

Let us say that X and Y settle on steady-state values X * ≡ η X, Y * ≡ η Y, as inputs to our logic gate. A value of η X or η Y significantly greater than 1 is then "high" (true, 1), and "low" (false, 0) if much less than 1. We then want to determine whether Z * is greater than (true) or less than (false) 1. For example, if n 1,2 > 0 (two repressors), Table 1 gives the possible steady states of Z. If η Z 1 and η Z 2 are greater than 1, these steady states approximate a NAND gate. If they are less than 1, but sum to greater than 1, the steady states instead approximate a NOR gate.

---

### Observation and theory of X-ray mirages [^4f65a611]. Nature Communications (2013). Medium credibility.

We use the term 'X-ray mirage' in the same sense as it is understood in the optical range. Mathematically, this is a part of the total radiation that remains after subtracting from it the initial radiation under the assumption that the initial radiation propagates through a medium without distortion due to inhomogeneity of a medium. According to this definition, we can represent the amplitude A (x, y, z) as the sum of two terms A (x, y, z) = A inc (x, y, z)+ A M (x, y, z), where A inc (x, y, z) is the amplitude of an incident beam propagating in empty space and A M (x, y, z) is the amplitude of the mirage radiation emerging in the plasma. Substituting A (x, y, z) into equation (1), we can find that A M (x, y, z) satisfies the equation:

which can be regarded as an equation for the complex amplitude of X-ray mirages.

To determine what imaginary source corresponds to the output mirage radiation A M (x, y, L), as well as to determine its position and shape, it is enough to 'run' the output radiation back in the geometry where the plasma is removed (Fig. 3b). To do this, taking into account the symmetry of the wave equation with respect to time reversal, it is sufficient to solve the equation:

with the initial condition(x, y,0) = A M (x, y, z L), where(x, y, z) is the complex amplitude of an imaginary source; is the coordinate of the exit face of plasma.

For numerical calculations, we will choose the origin of the Cartesian coordinate system on the input surface of the plasma and will consider that in the x − y plane it coincides with the maximum of the gain distribution.

---

### To infinity and some glimpses of beyond [^d42883f3]. Nature Communications (2017). Medium credibility.

Methods

Complexification of ordinary differential equations

The complexified version(z = x + iy) leads to the two-dimensional dynamical system:

The real axis is an invariant subspace, retrieving our real results; yet complexification endows the dynamics with an intriguing capability: as Fig. 4a, b illustrates through the (x, y) phase plane, collapse is avoided in the presence of a minuscule imaginary part. Large elliptical-looking trajectories are traced on the phase plane, eventually returning to the neighborhood of the sole fixed point of (0, 0) — which in the real case one would characterize as semi-stable. The system of Eq. (19) can be tackled in closed form since the ODEyields 1/ z = − t + 1/ z (0). For z = x + iy (z (0) = x 0 + iy 0) we obtain the explicit orbit formulaEliminating time by dividing the two ODEs within Eq. (19) directly yields an ODE for y = y (x) (rather than the parametric forms of Eqs. (20), (21)). From this ODE, one can obtain that the quantityis an invariant of the phase plane dynamics, and thus the latter can be written as x 2 + (y − R) 2 = R 2, where. That is, the trajectory evolves along circles of radius R in the upper (resp. lower) half plane if y 0 > 0 (resp. y 0 < 0.) Approaching the axis with y 0 → 0, the curvature of these circles tends to 0 and their radius to ∞ (retrieving the real dynamics as a special case). Figure 4 through its planar projections illustrates not only the radial projection of the dynamics in the x − y plane, but the x − t and y − t dependencies.

---

### Fast kinetics of magnesium monochloride cations in interlayer-expanded titanium disulfide for magnesium rechargeable batteries [^f706aedb]. Nature Communications (2017). Medium credibility.

Fig. 6
A schematic of structural evolution of TiS 2 at different stages of intercalation. Interlayers are expanded or distorted as different amount of pillaring molecules, complex cations, and solvents are intercalated into the van der Waals gap of a host material at each stage

The following electrochemical reactions are proposed for room temperature:where Mg x Cl y z + refers to MgCl 2, Mg 2 Cl 3 +, Mg 3 Cl 5 +, etc. so that x, y, z, and n are whole numbers that satisfy y > x, z = 2 x − y = 0 or 1, and n = y − x = 1, 2, 3, etc. Equation (1) describes the reversible intercalation of MgCl + into exTiS 2 at the cathode side. This complex-ion intercalation mechanism is similar to the AlCl 4 − intercalation in graphite reported by Dai et al. except that the intercalation of MgCl + happens during the reduction of the host rather than the oxidation. Equation (2) describes the simultaneous generation of MgCl + at the Mg anode by converting Mg x Cl y z + species in the electrolyte. The overall battery reaction in Eq. (3) can continue as long as Mg x Cl y z + species are available in the electrolyte. Equation (3) depicts that MgCl 2 is converted to intercalated MgCl + without leaving anything behind because it is neutral (z = 0), whereas complex cations with y > x are converted leaving MgCl + of same z / n equivalents in the electrolyte. Therefore, the cation concentration of the electrolyte stays unchanged throughout the reaction in either case.

---

### MPicker: visualizing and picking membrane proteins for cryo-electron tomography [^1c3697ef]. Nature Communications (2025). High credibility.

To obtain the coordinate system x – y – z and describe the membrane surface in the form z = f (x, y), MPicker needs to find a viewing direction as the z -axis, from which the membrane appears flat. To determine this direction, MPicker implements two methods. The first method involves fitting a plane to the starting points and using the direction perpendicular to the fitted plane as the z -axis. The second method is to find a projection direction that maximizes the projected area of the starting points and use it as the z -axis.

Once the x – y – z coordinate system is determined, MPicker uses surface fitting to find the f (x, y) that best matches the starting points. MPicker provides two surface fitting methods. The first method is polynomial fitting, which approximates the starting points with a N -th degree bivariate polynomial:where N is specified by the user. The second method uses TPS interpolation (implemented using scipy.interpolate.RBFInterpolator). Compared to polynomial fitting, this method can better describe surfaces with complex shapes but requires more computations. Therefore, MPicker does not directly use all starting points (Supplementary Fig. 10d), but first downsamples these points (Supplementary Fig. 10e) to reduce the computation. MPicker then used these downsampled points as interpolation points and generated a continuous surface, z = f (x, y), using TPS interpolation (Supplementary Fig. 10f). The users can specify the smoothness of the fitted surface: a higher smoothness may lead to a higher error between the surface and interpolation points. However, this also means better resistance to noise in the data. In practice, MPicker also removes interpolation points with large errors as outliers and then performs another round of TPS interpolation to further increase its robustness to noise.

---

### Alternating look-locker for quantitative T, tand B3D MRI mapping [^40039e1a]. Magnetic Resonance in Medicine (2024). Medium credibility.

Purpose

To develop a new MRI method, entitled alternating Look-Locker (aLL), for quantitative T₁, T₁ρ, and B₁ 3D mapping.

Methods

A Look-Locker scheme that alternates magnetization from +Z and -Z axes of the laboratory frame is utilized in combination with a 3D Multi-Band Sweep Imaging with Fourier Transformation (MB-SWIFT) readout. The analytical solution describing the spin evolution during aLL, as well as the correction required for segmented acquisition were derived. The simultaneous B₁ and T₁ mapping are demonstrated on an agar/saline phantom and on an in-vivo rat head. T₁ρ relaxation was achieved by cyclically applying magnetization preparation (MP) modules consisting of two adiabatic pulses. T₁ρ values in the rat brain in-vivo and in a gadobenate dimeglumine (Gd-DTPA) phantom were compared to those obtained with a previously introduced steady-state (SS) method.

Results

The accuracy and precision of the analytical solution was tested by Bloch simulations. With the application of MP modules, the aLL method provides simultaneous T₁ and T₁ρ maps. Conversely, without it, the method can be used for simultaneous T₁ and B₁ mapping. T₁ρ values were similar with both aLL and SS techniques. However, the aLL method resulted in more robust quantitative mapping compared to the SS method. Unlike the SS method, the aLL method does not require additional scans for generating T₁ maps.

Conclusion

The proposed method offers a new flexible tool for quantitative mapping of T₁, T₁ρ, and B₁. The aLL method can also be used with readout schemes different from MB-SWIFT.

---

### Carp [^d8a12781]. FDA (2009). Low credibility.

Volume desired x Concentration desired = Volume needed x Concentration available.

Example 1: If a 1:10 w/v extract is available and it is desired to use a 1:1,000 w/v extract substitute as follows:

Vd x Cd = Vn x Ca

10ml x 0.001 = Vn x 0.1

0.1 mL = Vn

Using a sterile technique, remove 0.10 mL of extract from the 1:10 vial and place it into a vial containing 9.90 mL of sterile diluent. The resulting ratio will be a 10 mL vial of 1:1,000 w/v.

Example 2: If a 10,000 pnu/ml extract is available and it is desired to use a 100 pnu/ml extract substitute as follows:

10ml x 100 = Vn x 10,000

0.1 mL = Vn

Using a sterile technique, remove 0.10 mL of extract from the 10,000 pnu/ml vial and place it into a vial containing 9.90 mL of sterile diluent. The resulting concentration will be a 10 mL vial of 100 pnu/ml.

Example 3: If a 10,000 AU/ml or BAU/ml extract is available and it is desired to use a 100 AU/ml or BAU/ml extract substitute as follows: Vd x Cd = Vn x Ca

---

### Versatile whole-organ / body staining and imaging based on electrolyte-gel properties of biological tissues [^44dc93f3]. Nature Communications (2020). High credibility.

For the c-Fos-labeled whole-brain data, we applied the "3D Find Maxima" function rather than the 3D centroid calculation for 2D Maxima objects adopted in the ClearMap algorithm. The same parameter values were used for the two datasets (MK-801 + or −) of the c-Fos-labeled brains. 0) The original stack covering the whole brain was cropped to prepare 100 partitioned stacks with 10-pixel x – y margins; (1) the "Subtract Background" function (rolling = 1) was applied to the single stack; (2) the stack was expanded three times in the three dimensions (x – y – z); (3) a 3D Minimum filter (radius x = y = z = 1) was applied; (4) a 3D Gaussian blur filter (sigma x = y = z = 1) was applied; (5) the "Set Threshold" function with a manually determined value was applied to eliminate background signals; (6) the "3D Find Maxima"function was applied with a radius size of x = y = z = 2.5 and a noise tolerance of 1. A stack with single pixels of maxima positions and a CSV file of the result table were generated. Every 120 slices, 5 marginal slices were separately calculated and integrated into the single stack generated above; (7) after calculating all 100 stacks, they were resized to the original x – y – z dimensions and then tiled after removing the 10-pixel x – y margins; and (8) the 3D Maximum filter and 3D Gaussian blur filter were finally applied to the stack to adjust the spot size to the labeled nucleus size. It took ~20 h to perform the calculations for ~8 GB of whole-brain data by using a workstation PC (64-bit Windows 10 Pro for Workstations, Intel(R) Xeon(R) E5-2687W v4 @ 3.00 GHz, two processors, 192 GB RAM, and 4× SSD RAID0 storage).

---

### Phase transitions in random circuit sampling [^dbed8f5a]. Nature (2024). Excellent credibility.

We demonstrated beyond-classical RCS by performing an experiment on a 67-qubit Sycamore chip (Fig. 4). These random circuits followed the same two-dimensional ABCD-CDAB pattern as ref. The single-qubit gates were chosen randomly from the discrete set Z p X 1/2 Z − p with p ∈ {−1, −3/4, −1/2,…, 3/4}. We show in Supplementary Information section B the fidelity of the elementary operations of the random circuit. On average, we achieved a read-out error of 1.3(0.5) × 10 −2, a dressed two-qubit Pauli error rate of 5.5(1.3) × 10 −3 that can be further decomposed into a single-qubit Pauli error rate of 1.0(0.5) × 10 −3, and an intrinsic two-qubit simultaneous error of 3.5(1.4) × 10 −3. An intrinsic Pauli error rate of 3.5 × 10 −3 corresponds to an average fidelity of 99.72%. We validated the digital error model by looking at patched variations of the random circuit (inset in Fig. 4a), where slices of two-qubit gates have been removed to create patched circuits for which each patch XEB can be verified for a modest computational cost. The total fidelity was then the product of the patch fidelities. Computing XEB over full circuits is, at present, an intractable classical task. We, thus, estimated the fidelity after 32 cycles using the discrete error model, obtaining 0.1%. This increased depth was possible thanks to the substantially reduced errors compared with previous processors. We collected over 70 million sample bit strings for a single circuit at this depth. In Supplementary Information section SC1, we report fidelities for another XEB experiment, SYC-70, done on 70 qubits and with 24 cycles. In Fig. 4b, we verify these extracted fidelities with the Loschmidt echo, where we use the same circuit and its inversion to return to the initial state. We observed good agreement with the XEB experiment and the digital error model.

---

### Diffusion sampling schemes: a generalized methodology with nongeometric criteria [^59d3d131]. Magnetic Resonance in Medicine (2023). Medium credibility.

Purpose

The aim of this paper is to show that geometrical criteria for designing multishell q -space sampling procedures do not necessarily translate into reconstruction matrices with high figures of merit commonly used in the compressed sensing theory. In addition, we show that a well-known method for visiting k-space in radial three-dimensional acquisitions, namely, the Spiral Phyllotaxis, is a competitive initialization for the optimization of our nonconvex objective function.

Theory and Methods

We propose the gradient design method WISH (WeIghting SHells) which uses an objective function that accounts for weighted distances between gradients within M-tuples of consecutive shells, with M ranging between 1 and the maximum number of shells S. All the M-tuples share the same weight ω_M. The objective function is optimized for a sample of these weights, using Spiral Phyllotaxis as initialization. State-of-the-art General Electrostatic Energy Minimization (GEEM) and Spherical Codes (SC) were used for comparison. For the three methods, reconstruction matrices of the attenuation signal using MAP-MRI were tested using figures of merit borrowed from the Compressed Sensing theory (namely, Restricted Isometry Property -RIP- and Coherence); we also tested the gradient design using a geometric criterion based on Voronoi cells.

Results

For RIP and Coherence, WISH got better results in at least one combination of weights, whilst the criterion based on Voronoi cells showed an unrelated pattern.

Conclusion

The versatility provided by WISH is supported by better results. Optimization in the weight parameter space is likely to provide additional improvements. For a practical design with an intermediate number of gradients, our results recommend to carry out the methodology here used to determine the appropriate gradient table.

---

### Symbolic arithmetic knowledge without instruction [^d0ca42f7]. Nature (2007). Excellent credibility.

Symbolic arithmetic is fundamental to science, technology and economics, but its acquisition by children typically requires years of effort, instruction and drill. When adults perform mental arithmetic, they activate nonsymbolic, approximate number representations, and their performance suffers if this nonsymbolic system is impaired. Nonsymbolic number representations also allow adults, children, and even infants to add or subtract pairs of dot arrays and to compare the resulting sum or difference to a third array, provided that only approximate accuracy is required. Here we report that young children, who have mastered verbal counting and are on the threshold of arithmetic instruction, can build on their nonsymbolic number system to perform symbolic addition and subtraction. Children across a broad socio-economic spectrum solved symbolic problems involving approximate addition or subtraction of large numbers, both in a laboratory test and in a school setting. Aspects of symbolic arithmetic therefore lie within the reach of children who have learned no algorithms for manipulating numerical symbols. Our findings help to delimit the sources of children's difficulties learning symbolic arithmetic, and they suggest ways to enhance children's engagement with formal mathematics.

---

### Neural representations of the perception of handwritten digits and visual objects from a convolutional neural network compared to humans [^4b573ff8]. Human Brain Mapping (2023). Medium credibility.

We conducted searchlight RSA using (a) the RDM for each layer of our CNN model and (b) the neural RDM obtained from each center voxel by moving the center voxel across the whole brain. To this end, Spearman's rank correlation (ρ) was used as a similarity measure between the neural RDM for each voxel and the RDM for each of the CNN layers. Fisher's z‐transform was applied to the ρ values across the whole brain for each subject. A voxel‐wise one‐sample t ‐test was employed using the z ‐transformed ρ values across the 15 subjects for group inference. Multiple comparison correction of the resulting p ‐value was applied using random permutations (n = 5000). More specifically, multivoxel patterns with randomized voxel indices were used to construct a neural RDM using an individual beta‐valued map followed by RSA with the RDM for the CNN layers based on Spearman's rank correlation. The resulting ρ values across the 15 subjects were Fisher's z ‐transformed and subject to the one‐sample t ‐test. Thus, we obtained a null distribution of t ‐statistics to correct the p ‐value obtained from the RSA using intact voxel indices in the multivoxel pattern. As a result, the clusters that exhibited significant similarity (corrected p < 0.05) between the neural RDM and the RDM for each CNN layer were determined from cluster‐size correction with a minimum of 15 voxels.

---

### GRAF-pop: a fast distance-based method to infer subject ancestry from multiple genotype datasets without principal components analysis [^8851f403]. G3 (2019). Medium credibility.

In GRAF-pop, we use the expected D values calculated using Equation (3) to find the vertices of ΔEFA, and use it as the reference triangle to calculate barycentric coordinates. When there are missing genotypes, we first calculate the barycentric coordinates using the SNPs with genotypes, then map the coordinates onto the reference triangle when no genotypes are missing. Specifically, we do the following steps to normalize the genetic distances and plot the results:

For each subject group U {E, F, A}, calculate the expected genetic distances D U1, D U2, D U3, to the first three reference populations, using Equation (3) for the full set of 10,000 fingerprint SNPs.
Represent each group with a 3-D point in space by treating D U1, D U2, D U3 as the x, y, z Cartesian coordinates. Build a triangle by connecting the three points representing the three subject groups.
Rotate and translate the triangle so that it is in the plane z = 0, and side FA is parallel to the x -axis. Denote this triangle as ΔEFA 0, to be used as the reference triangle for calculating barycentric coordinates.
Given a subject i, find the subset, T, of fingerprint SNPs with genotypes for this subject.
For each subject group U {E, F, A}, calculate the expected genetic distances D U1T, D U2T, D U3T, to the first three reference populations using Equation (4) for subset T. Represent each group with a 3-D point in the space.
Calculate the genetic distances D i1, D i2 and D i3 from subject i to the first three reference populations. Represent subject i as a 3-D point, Q, by treating D i1, D i2, D i3 as the x, y, z Cartesian coordinates.
Build a triangle by connecting the three points representing the three subject groups. Rotate and translate the three points, together with point Q, so that the triangle is in the plane z = 0. Denote this triangle as ΔEFA T.
Using the x, y coordinates of point Q after transformation, calculate the barycentric coordinates (λ ie, λ if, λ ia) for subject i with respect to ΔEFA T, using Equations (5) and (6).
Convert the barycentric coordinates back to the Cartesian coordinates (x i0, y i0) using reference triangle ΔEFA 0: where (x e0, y e0), (x f0, y f0), (x a0, y a0) are the Cartesian coordinates of the three vertices of ΔEFA 0.
Plot the converted Cartesian coordinates of subject i, together with ΔEFA 0, on the x-y plane. The final x i0, y i0 values are the normalized genetic distances, called GD1 and GD2 scores in this article and in GRAF.
The z coordinate of point Q after transformation (step 7), called GD3 score, is also used for plotting results.
Calculate the genetic distances D i4 and D i5 from subject i to the last two reference populations South Asian and Mexican/Latino. The difference D i5 - D i4, called GD4 score, is plotted against GD1 scores to separate South Asians from Latin Americans.

---

### Easy-to-useshims for human brain imaging at 7 T [^fa89e01e]. Magnetic Resonance in Medicine (2025). Medium credibility.

Purpose

 B₁⁺ field inhomogeneity is a common problem in high field brain MRI (> 3 T). Parallel-transmit methods that adjust the B₁⁺ field channelwise often require valuable scan time. Group-optimized phase shims are presented to increase or attenuate the B₁⁺ field in specific brain regions, omitting personalized calibrations and potentially enabling reduced FOV acquisitions or artifact reduction.

Methods

Channelwise B₁⁺ maps were obtained for seven participants using an 8Tx/32Rx coil and a 7 T MRI scanner. Two regional shim settings (B₁⁺ shims) were calculated: one to increase the B₁⁺ field in the cerebellum and the other to increase the B₁⁺ field in the occipital lobe while attenuating the B₁⁺ field in the frontal lobe. B₁⁺ maps from five participants outside the design group were used to simulate the B₁⁺ profiles, and seven were scanned to evaluate the implementation of the B₁⁺ shims using B₁⁺ maps, 3D EPI, GRE acquisitions, and a visual fMRI experiment.

Results

Both regional shim settings successfully amplified the B₁⁺ field in the selected ROIs resulting in improved B₁⁺ yield and increased tSNR in the 3D EPI images and fMRI experiments compared to the circularly polarized shim mode. The attenuating B₁⁺ shim decreased B₁⁺ in the frontal ROI, decreasing fold-over artifacts in a reduced FOV, lowering g-factors in accelerated scans with high undersampling factors and resulted in improved BOLD responses in the visual fMRI experiment.

Conclusion

Regional B₁⁺ shim settings remove the need for time-consuming, personalized B₁⁺ measurements and calibrations. The attenuating shim allows for signal reduction within the power limits of the rf-coil, reducing artifacts while improving the B₁⁺ field in selected ROIs.

---

### Graphical analysis for phenome-wide causal discovery in genotyped population-scale biobanks [^c0fea1a6]. Nature Communications (2021). High credibility.

Introduction

Causal inference from observational data is a fundamental objective that has been receiving increasing attention in multiple domains including biology, epidemiology, and economics. Graphical models are a cornerstone of causal inference as they explicitly describe the generating process of the observed data. These models contain functions that describe how values are assigned to each variable, possibly depending on the values of other observed or unobserved variables. These dependencies can be summarized in a directed graph, where an edge X → Y means that the function that determines the value of Y depends on X 's value. If the graph is acyclic, the joint distribution of the data can be represented as a Bayesian network (BN) that specifies the conditional probabilities of nodes given their parents.

Causal discovery is a subfield of causal inference that focuses on finding evidence in data for the existence of a causal path between two or more variables. This is an essential preliminary step as it can be used to justify the assumptions made by statistical analyses. Algorithms for causal discovery identify patterns of conditional independencies (CI) with theoretical justification for refuting candidate models that are unlikely to have generated the observed data. This process requires two assumptions (1) graphical d-separation and the Causal Markov Condition (CMC), and (2) Causal Faithfulness Condition (CFC), (see Supplementary Note 1 for formal definitions). CMC states that whenever a pair of variables X and Y are separated in the graph given a set Z, then X and Y are conditionally independent given Z in every compatible distribution. CMC has been proven to hold in acyclic models and in linear models with cycles (also called feedback loops). Some results support CMC in other cyclic cases. CFC deals with the opposite direction: it assumes that a conditional independence (CI) in the observed distribution entails separation in the graph. CFC has theoretical justification in that the set of models that do not satisfy it are extremely unlikely (i.e. have a zero Lebesgue measure).

---

### Generating conjectures on fundamental constants with the ramanujan machine [^75e8911d]. Nature (2021). Excellent credibility.

Fundamental mathematical constants such as e and π are ubiquitous in diverse fields of science, from abstract mathematics and geometry to physics, biology and chemistry 1,2. Nevertheless, for centuries new mathematical formulas relating fundamental constants have been scarce and usually discovered sporadically 3–6. Such discoveries are often considered an act of mathematical ingenuity or profound intuition by great mathematicians such as Gauss and Ramanujan 7. Here we propose a systematic approach that leverages algorithms to discover mathematical formulas for fundamental constants and helps to reveal the underlying structure of the constants. We call this approach 'the Ramanujan Machine'. Our algorithms find dozens of well known formulas as well as previously unknown ones, such as continued fraction representations of π, e, Catalan's constant, and values of the Riemann zeta function. Several conjectures found by our algorithms were (in retrospect) simple to prove, whereas others remain as yet unproved. We present two algorithms that proved useful in finding conjectures: a variant of the meet-in-the-middle algorithm and a gradient descent optimization algorithm tailored to the recurrent structure of continued fractions. Both algorithms are based on matching numerical values; consequently, they conjecture formulas without providing proofs or requiring prior knowledge of the underlying mathematical structure, making this methodology complementary to automated theorem proving 8–13. Our approach is especially attractive when applied to discover formulas for fundamental constants for which no mathematical structure is known, because it reverses the conventional usage of sequential logic in formal proofs. Instead, our work supports a different conceptual framework for research: computer algorithms use numerical data to unveil mathematical structures, thus trying to replace the mathematical intuition of great mathematicians and providing leads to further mathematical research.

---

### A single inverse-designed photonic structure that performs parallel computing [^cbb42b17]. Nature Communications (2021). High credibility.

In the search for improved computational capabilities, conventional microelectronic computers are facing various problems arising from the miniaturization and concentration of active electronics. Therefore, researchers have explored wave systems, such as photonic or quantum devices, for solving mathematical problems at higher speeds and larger capacities. However, previous devices have not fully exploited the linearity of the wave equation, which as we show here, allows for the simultaneous parallel solution of several independent mathematical problems within the same device. Here we demonstrate that a transmissive cavity filled with a judiciously tailored dielectric distribution and embedded in a multi-frequency feedback loop can calculate the solutions of a number of mathematical problems simultaneously. We design, build, and test a computing structure at microwave frequencies that solves two independent integral equations with any two arbitrary inputs and also provide numerical results for the calculation of the inverse of four 5x5 matrices.

---

### The making of the standard model [^0fc9a4ce]. Nature (2007). Excellent credibility.

A seemingly temporary solution to almost a century of questions has become one of physics' greatest successes.

---

### The eighty five percent rule for optimal learning [^0684215b]. Nature Communications (2019). High credibility.

Researchers and educators have long wrestled with the question of how best to teach their clients be they humans, non-human animals or machines. Here, we examine the role of a single variable, the difficulty of training, on the rate of learning. In many situations we find that there is a sweet spot in which training is neither too easy nor too hard, and where learning progresses most quickly. We derive conditions for this sweet spot for a broad class of learning algorithms in the context of binary classification tasks. For all of these stochastic gradient-descent based learning algorithms, we find that the optimal error rate for training is around 15.87% or, conversely, that the optimal training accuracy is about 85%. We demonstrate the efficacy of this 'Eighty Five Percent Rule' for artificial neural networks used in AI and biologically plausible neural networks thought to describe animal learning.

---

### Optimal free descriptions of many-body theories [^f77fa88f]. Nature Communications (2017). Medium credibility.

Methods

Optimization

The optimization to find σ andin (3) is performed by a Monte Carlo basin-hopping strategyusing the Nelder–Mead simplex algorithm for local minimization within basins of the cost function. This global strategy was selected to counteract an observed tendency for local methods to get trapped in local minima. The initial guess for this search is found as follows. The normalization constant E 0 is the lowest entanglement energy of the input entanglement spectrum { E }. We iteratively construct an approximate set of single-particle entanglement energies starting from an empty set. First, we take the lowest remaining level in the spectrum and subtract E 0 to produce a new single-particle level k. Then we remove the many-body levels, which are closest to the new combinatorial levels generated according to (2) by the additional single-particle level. This process is repeated until the input spectrum is exhausted. We can also introduce a truncation of the entanglement spectrum cutting off high entanglement energies, making the construction of the initial guess terminate faster. The minimization of D (σ, σ f) to identify the optimal free model for the Ising Hamiltonian (5) is calculated using a local Nelder–Mead method.

Finite-size scaling

We perform the finite-size scaling according to an ansatz (4). The parameters of the collapse were estimated using the method of ref. From the scaling ansatz (4) and for a trial set of scaling parameters g c, ν and ζ, the scaled values x L = (g − g c) L 1/ ν andare calculated from each unscaled data point. From this collection of scaled data points (x L, y L) across all L, we implicitly define a so-called master curve that best represents them. This curve y (x) is defined around a point x as the linear regression calculated by taking the scaled data points immediately left and right of x for each system size L. We characterize the deviation of the scaled points (x L, y L) from the master curve y (x L) using the χ 2 statistic. This measure is used as the cost function for an optimization problem over the scaling parameters g c, ν, ζ and θ, which can be solved using the same techniques as the previous problems.

---

### State estimation of a physical system with unknown governing equations [^b2864b1a]. Nature (2023). Excellent credibility.

Example 5: high-dimensional, spatially extended differential-equation discovery with low-rank observation matrices

We now consider the problem of recovering the underlying governing equations for the Lorenz '96 system with 1,024 states using a low-rank observation matrix. The Lorenz '96 model is a set of coupled, chaotic, ODEs designed to be a simplified model of the climate along a line of constant latitude. The governing equations for this system are given by, for which the boundary conditions are assumed to be periodic (k = 1, 2,…, 1,024).

We generated observation matrices of rank r using the expression, in which eachis a random vector sampled from a standard normal distribution. We studied the performance of our approach in which the rank of the observation matrix is 256, 512 and 1,024. For each experiment, we used 512 snapshots over the time interval of 0 to 10 corrupted by noise that is 2% of the range of the system. We make the assumption that the dynamics are given by, in whichreturns all quadratic polynomial functions that are a function of x i and its two closest neighbours to the left and right of the node. Although we found that we were able to exactly recover the underlying functional form of the governing equations with an observation matrix whose rank was half the dimensionality of the state, further work is required to theoretically establish conditions under which the governing equations can be exactly recovered. These results are summarized in Extended Data Fig. 2.

Because many real-world systems for which governing equations are challenging to derive from first principles are both (1) high-dimensional and (2) challenging to measure, we believe this to be a useful result. Our method has a computational cost that scales linearly in the state dimension and can be applied given an arbitrary observation function. This result opens the door for equation discovery in systems that were previously believed to be too large and/or difficult to measure.

---

### Rigorous location of phase transitions in hard optimization problems [^d98ebea5]. Nature (2005). Excellent credibility.

It is widely believed that for many optimization problems, no algorithm is substantially more efficient than exhaustive search. This means that finding optimal solutions for many practical problems is completely beyond any current or projected computational capacity. To understand the origin of this extreme 'hardness', computer scientists, mathematicians and physicists have been investigating for two decades a connection between computational complexity and phase transitions in random instances of constraint satisfaction problems. Here we present a mathematically rigorous method for locating such phase transitions. Our method works by analysing the distribution of distances between pairs of solutions as constraints are added. By identifying critical behaviour in the evolution of this distribution, we can pinpoint the threshold location for a number of problems, including the two most-studied ones: random k-SAT and random graph colouring. Our results prove that the heuristic predictions of statistical physics in this context are essentially correct. Moreover, we establish that random instances of constraint satisfaction problems have solutions well beyond the reach of any analysed algorithm.

---

### The minimal work cost of information processing [^927873c9]. Nature Communications (2015). Medium credibility.

Classical mappings and dependence on the logical process

Our result, which is applicable to arbitrary quantum processes, applies to all classical computations as a special case. Classically, logical processes correspond to stochastic maps, of which deterministic functions are a special case. As a simple example, consider the AND gate. This is one of the elementary operations computing devices can perform, from which more complex circuits can be designed. The gate takes two bits as input, and outputs a single bit that is set to 1 exactly when both input bits are 1, as illustrated in Fig. 2a.

The logical process is manifestly irreversible, as the output alone does not allow to infer the input uniquely. If one of the inputs is zero, then the logical process effectively has to reset a three-level system to zero, forgetting which of the three possible inputs 00, 01 or 10 was given; this information can be viewed as being discarded, and hence dumped into the environment. We can confirm this intuition with our main result, using the fact that a general classical mapping is given by the specification of the conditional probability p (x ′| x) of observing x ′ at the output if the input was x. Embedding the classical probability distributions into the diagonals of quantum states, the infinity norm in expression (2) becomes simply

---

### Solving olympiad geometry without human demonstrations [^8fdca3ad]. Nature (2024). Excellent credibility.

Integrating DD and AR

DD and AR are applied alternately to expand their joint deduction closure. The output of DD, which consists of new statements deduced with deductive rules, is fed into AR and vice versa. For example, if DD deduced 'AB is parallel to CD', the slopes of lines AB and CD will be updated to be equal variables in AR's coefficient matrix A, defined in the 'Algebraic reasoning' section. Namely, a new row will be added to A with '1' at the column corresponding to the variable slope(AB) and '−1' at the column of slope(CD). Gaussian elimination and mixed-integer linear programming is run again as AR executes, producing new equalities as inputs to the next iteration of DD. This loop repeats until the joint deduction closure stops expanding. Both DD and AR are deterministic processes that only depend on the theorem premises, therefore they do not require any design choices in their implementation.

Proof pruning

Although the set of immediate ancestors to any node is minimal, this does not guarantee that the fully traced back dependency subgraph G (N) and the necessary premise P are minimal. Here we define minimality to be the property that G (N) and P cannot be further pruned without losing conclusion reachability. Without minimality, we obtained many synthetic proofs with vacuous auxiliary constructions, having shallow relation to the actual proof and can be entirely discarded. To solve this, we perform exhaustive trial and error, discarding each subset of the auxiliary points and rerunning DD + AR on the smaller subset of premises to verify goal reachability. At the end, we return the minimum proof obtainable across all trials. This proof-pruning procedure is done both during synthetic data generation and after each successful proof search during test time.

---

### CORRECTION [^a0bf88a0]. Annals of Family Medicine (2017). Low credibility.

[This corrects the article on p. 14 in vol. 15.].

---

### Dextrose 5% [^9a9aba0e]. FDA (2025). Medium credibility.

SPECIFICATION

（1）50ml:2.5g（2）50ml:5g（3）100ml：5g（4）100ml:10g（5）250ml:12.5g（6）250ml:25g（7）500ml:25g（8）500ml:50g（9）1000ml:50g（10）1000ml:100g

---

### Over-exploitation of natural resources is followed by inevitable declines in economic growth and discount rate [^b6b7ffb0]. Nature Communications (2019). High credibility.

We used algorithms that find the exact solutions provided that the resolutions are sufficiently fine. Specifically, to find the optimal solution numerically, our algorithm uses Stochastic Programming with backward induction (Supplementary Note 4). (Note that the model's dynamics are deterministic but the general method is still called stochastic.) To find the market solution, our algorithm also uses Stochastic Programming to solve for a given value of X. But it finds a solution multiple times, each time for a different value of X, until it finds the solution that satisfies the consisteny criterion. These algorithms are coded in C/C++ and are described in detail in Supplementary Note 4.

In turn, in the results shown in Fig. 3, system 2, as well as in Figs. 4 and 5 and in the graphical tool, we assume that the dynamics of c and f follow Eqs. 6–9, but we consider harvest functions that are not given by either the optimal solution or the market solution. In Fig. 3, system 2, we consider harvest functions that follow the market solution until t = t 1 and after t = t 1 + 10, but between these times, the non-sustainable harvest decreases gradually from its maximal level to zero. In Fig. 4, we calculate Δ sus, which is the cumulative discount that emerges if the harvest is entirely sustainable, namely, H n = 0 and H s = x 1 + x 2 if t > 0. Also, in Fig. 4a, we consider three scenarios in which the non-sustainable harvest is higher in the beginning but eventually approaches zero, while H n + H s = x 1 + x 2.

After we determine the harvest functions, the functions c (t) and f (t) are calculated according to Eqs. 6 and 9. In turn, we calculate the discount rate and the cumulative discount according to Eq. 5 (where the cumulative discount is the integral over time of the discount rate). Specifically, for the case in which only sustainable harvest is used (Δ sus in Fig. 4), the discount rates are calculated in Supplementary Note 2 and are given by Eqs. B5 and B12. The prices are given by Eq. A10, and the total product is given by Eq. A11. All of these equations are derived in Supplementary Notes 1, 2.

---

### Correction [^26cbbc41]. Journal of Radiology Case Reports (2017). Low credibility.

[This corrects the article on p. 8 in vol. 10.].

---

### Solving olympiad geometry without human demonstrations [^cb08ac3f]. Nature (2024). Excellent credibility.

In geometry, the symbolic deduction engine is deductive database (refs.), with the ability to efficiently deduce new statements from the premises by means of geometric rules. DD follows deduction rules in the form of definite Horn clauses, that is, Q (x) ← P 1 (x),…, P k (x), in which x are points objects, whereas P 1,…, P k and Q are predicates such as 'equal segments' or 'collinear'. A full list of deduction rules can be found in ref. To widen the scope of the generated synthetic theorems and proofs, we also introduce another component to the symbolic engine that can deduce new statements through algebraic rules (AR), as described in Methods. AR is necessary to perform angle, ratio and distance chasing, as often required in many olympiad-level proofs. We included concrete examples of AR in Extended Data Table 2. The combination DD + AR, which includes both their forward deduction and traceback algorithms, is a new contribution in our work and represents a new state of the art in symbolic reasoning in geometry.

---

### Isolation by distance in populations with power-law dispersal [^f9e29556]. G3 (2023). Medium credibility.

Simulation methods in one dimension

Our one-dimensional simulation methods are very similar to those used in two dimensions, with only three significant differences that together make them much simpler. Firstly, instead of having to track two random walks, one for each lineage, we can just track the difference between their positions. Secondly, we take space to be continuous rather than discrete, i.e. we do not round the difference in positions and do not require the difference to be exactly zero for coalescence. Note that, as mentioned in the Model section above, this means that these simulations are only meant as approximations to some underlying model that has an exact forward-time counterpart. Finally, we take the range size to be effectively infinite, limited only by the maximum size of double-precision numbers.

Explicitly, we again simulate lineage motion using a discrete time random walk,

where X t now represents the signed distance between two lineages at a given time, and the step size, Δ X t, is a real-valued random variable drawn from the dispersal distribution at each integer time t. For α < 2, we use Lévy alpha-stable distributions for dispersal, with each lineage's single-step displacement having probability density K 1 (y) given by:

Thequantity we actually simulate, Δ X t, the increment in the signed distance between the two individuals, is the sum of the two lineages' independent jumps. Thus its probability density K is the convolution of K 1 with itself:

Becausethe distributions are stable, K differs from K 1 only by an extra factor of two in the dispersal constant.

---

### An analytical theory of balanced cellular growth [^a5efc002]. Nature Communications (2020). High credibility.

Proof: The active stoichiometric matrix A may have more rows than columns. In this case, m + 1 > n, and the rows for exactly n metabolites are linearly independent, as row and column rank must equal. As a consequence, the remaining m + 1 − n metabolite concentrations are linearly dependent on the concentrations of the n independent metabolites. These dependent concentrations are not free variables, and hence they can be put aside and dealt with separately.

We decompose the linear system of equations represented by constraint (11) into two parts, rearranging the rows of A into matrices B, C such that B contains the rows for the independent reactants. As A has full column rank, choosing linearly independent rows results in a square matrix B of full rank (#rows(B) = rank(B) = rank(A)). Let b be the subvector of reactant concentrations a that correspond to the rows of B, and let c be the subvector of the remaining reactant concentrations corresponding to the rows of C. We can then split the mass balance constraint (11) into two separate equations: B is a square matrix of full rank, so there is always a unique inverse B −1. Multiplying both sides of the first equation by B −1 from the left, we obtain the desired equation for v. Inserting this result into the second equation results in the desired equation for c. □

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^488abf14]. Nature Communications (2021). High credibility.

Chaotic behaviors are prevented in monotonic regulation with linear or cyclic network topology

Due to cyclic network topology (no more than one loop) and monotonic regulation, none of the motifs so far can yield chaotic dynamics. Either non-monotonic feedback (as in the Mackey-Glass equation) or non-cyclic topologyis required for chaos. Double feedback (Fig. 8 a) is a minimal motif fulfilling both requirements, although the non-monotonicity is sufficient (Supplementary Note 8).

The governing equation for such a double feedback motif is thus given by setting the output and both inputs of the logic equation (Eq. (19)) to X (and letting K = 1 for simplicity):

Chaotic behavior is possible for non-monotonic feedback with multiple feedback and disparate delays

Simulation of Eq. (37) demonstrates chaos for some parameters of positive/negative mixed feedback, as evidenced by sustained oscillations with variable maxima (Fig. 8 b), occupying an apparently fractal region in phase space (Fig. 8 c), and a large number of peaks in frequency space (Fig. 8 d). To confirm that the dynamics are in fact chaotic, we calculated the dominant Lyapunov exponent using the Wolf method(see "Methods"), finding a positive (i.e. chaotic) value of 0.0040 ± 0.00055 bits (mean ± standard deviation). We also calculated the box dimension of a reconstructed phase space with coordinates (X (T), X (T − 10), X (T − 20)) used by the Wolf method, yielding fractional dimension ~1.81, with 95% confidence interval (1.76, 1.87). These results indicate chaos (see Supplementary Fig. 4).

---

### 2006 consensus guidelines for the management of women with abnormal cervical screening tests [^2a0e261d]. Journal of Lower Genital Tract Disease (2007). Medium credibility.

2006 Consensus Guidelines — development process and rating schema detail that approximately 12 months before the conference, 6 working groups were formed and MEDLINE searches covered English-language articles published since 2000; guidelines were voted on and approved by at least a two-thirds majority vote. To maintain consistency, terminology and the 2-part rating system were retained, using the terms recommended, preferred, acceptable, and unacceptable for interventions, letters A through E to indicate strength of recommendation, and Roman numerals I to III to designate quality of evidence.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^832976d6]. Nature Communications (2021). High credibility.

Results

Our results are divided up into 7 sections corresponding to 7 different regulatory networks of increasing complexity. An overall reference table (Table 2) is included in the discussion.

Motif 0: direct Hill regulation

We first describe how we use DDEs to model simple regulation of a gene Y by a gene X as the most basic motif, and then provide a simple yet unified mathematical framework for both activation and inhibition with delay.

Activation and inhibition can both be modeled with a single unified function

We consider a transcription factor x regulating the production of a protein y (Fig. 2). If x activates y, the production rate of y increases with increasing x, generally saturating at a maximum rate α. Often there is an additional cooperativity parameter n which determines how steeply y increases with x in the vicinity of the half-maximal x input value k. In this framework, n = 0 is constitutive production, n = 1 is a Michaelis-Menten regulation, 1 < n < 5 is a typical biological range of cooperativity, and n → ∞ is a step-function regulation. If x represses y, the same conditions hold except that the production rate of y then decreases with increasing x. A standard quantitative model for this behavior is called the Hill function, and serves as a good approximation for many regulatory phenotypes in biology, including transcription and translation rates, phosphorylation, enzymatic activity, neuronal firing, and so forth. In general there may also be a leakage rate α 0 that yields constant y production in the absence of x. The concentration of y is also generally removed at a rate β proportional to its concentration. This removal term can represent many biophysical processes, such as degradation, dilution, compartmentalization, or sequestration; for simplicity we mainly use the term "degradation".

---

### Thinking outside the curve, part II: modeling fetal-infant mortality [^27ea1e69]. BMC Pregnancy and Childbirth (2010). Low credibility.

a. Simulation study to calibrate confidence intervals

We simulated 25 overlapping data sets of size 50,000, the degree of overlap consistent with a population of 200,000, based on the specifications in Table 2. The mixture density and the risk functions were chosen to mimic the patterns actually observed for white singletons born to heavily-smoking mothers; see panel b of Figure One from the previous paper and Figure 1d of the present paper. For each of various C between 2.0 and 5.0, we used Equation (6) to form confidence intervals for mortality risks at selected birthweights, namely r 1 (μ 1 - σ 1), r 1 (μ 1), r 1 (μ 1 + σ 1), r 2 (μ 2 - σ 2), r 2 (μ 2), r 2 (μ 2 + σ 2), r 3 (μ 3 - σ 3), r 3 (μ 3), r 3 (μ 3 + σ 3), r 4 (μ 4 - σ 4), r 4 (μ 4), and r 4 (μ 4 + σ 4). Above, μ j and σ j denote the mean and standard deviation of the birthweights in component j (1 ≤ j ≤ 4). This was repeated nine more times, and we tabulated how many of the 120 = 12 × 10 confidence intervals contained their targets. Confidence intervals were also formed using Equation (5) for comparative purposes. The above steps were repeated with overlapping data sets consistent with a population of 1,000,000 and with nonoverlapping data sets consistent with an effectively infinite population.

Table 2
Mixture Model and Mortality Functions for Simulation Study

The probability density for the mixture model used in our simulation study is specified, as are the mortality risk functions associated with the mixture model components. Above, z is defined as (x - 3000)/1000, where x is birthweight in grams.

The results are summarized in Table 3. With an effectively infinite population, only 75.0% of the confidence intervals formed using Equation (5) contained their targets at C = 5.0. On the other hand, the confidence intervals formed using Equation (6) contained their targets 95.0% of the time at C = 4.0. The latter finding provided the rationale for taking C 0 = 4.0 when constructing confidence intervals for mortality risks in our examples with real data.

---

### How to find answers to clinical questions [^7cb7fcd9]. American Family Physician (2009). Low credibility.

Many barriers exist to finding answers to physicians' clinical questions. Lack of time, resources, and computer skills, as well as physicians' environment and attitudes about problem solving, all contribute to unanswered questions. Making use of computer-based information resources can give physicians a framework for answering questions and keeping their practice consistent with the best available evidence.

---

### Data-driven discovery of dimensionless numbers and governing laws from scarce measurements [^310b7972]. Nature Communications (2022). High credibility.

The optimization of dimensionless learning is different from general regression optimization approaches because only dimensionless numbers with small rational powers are preferred, such as −1, 0.5, 1, or 2, etc. Therefore, instead of searching for the best basis coefficients with a lot of decimals like other neural network-based methods, such as DimensionNet, zero-order optimization methods are used in this work. It includes grid search or pattern search-based two-level optimization and can be more efficient in finding the best basis coefficients. No gradient information and learning rate are required and the choice of grid interval is more flexible. Even though these zero-order optimization approaches can get stuck in local minima, increasing the number of initial points can easily eliminate this issue. More detailed pros and cons of different optimization methods are described in Section 4.5 of the SI.

The proposed method divides the identification process of differential equations into two steps to identify consistent parameterized governing equations efficiently. The first step is to identify a temporary governing equation in which the regression coefficients can be a constant or variable depending on how the simulation or experiment parameters are set. In the next step, dimensionless learning aims to recover the expression of the varying coefficients by leveraging the dimension of these coefficients. By combining these two steps, the proposed method can efficiently obtain a consistent dimensionally homogeneous governing equation with a small amount of data. In contrast, the standard SINDy falls short of achieving a consistent parameterized differential equation for the same system with different parameters. For example, the governing equation for the spring–mass–damper system is. If we use different parameters (damping coefficient c, spring constant k, or mass m) in this system, SINDy can only provide scalar coefficients for x andrather than the expressionsand, respectively. Other advanced SINDy approaches deal with this issue by multiplying the candidate terms by a set of predetermined parameters. Although these approaches can address this inconsistent governing equation problem, it couples the optimization of identifying candidate terms and parameterized coefficients, making the optimization more difficult. If there are many combinations of parametric derivative or non-derivative terms, this problem can become more difficult and unmanageable.

---

### Implementing quantum dimensionality reduction for non-markovian stochastic simulation [^4ea471be]. Nature Communications (2023). High credibility.

Methods

Stochastic processes and minimal-memory classical modelling

A discrete-time stochastic processconsists of a sequence of random variables X t, corresponding to events drawn from a set, and indexed by a timestep. The process is defined by a joint distribution of these random variables across all timesteps, whererepresents the contiguous (across timesteps) series of events between timesteps t 1 and t 2. We consider stochastic processes that are bi-infinite, such thatand, and stationary (time-invariant), such that P (X 0: L) = P (X t: t + L) ∀ t, L ∈. Without loss of generality, we can take the present to be t = 0, such that the past is given by, and the future. Note that we use upper case for random variables and lower case for the corresponding variates.

A (causal) model of such a (bi-infinite and stationary) discrete-time stochastic process consists of an encoding functionthat maps from the set of possible past observationsto a set of memory states –. The model also requires an update rulethat produces the outputs and updates the memory state accordingly. We then designate the memory cost D f of the encoding as the logarithm of the dimension (i.e. the number of (qu)bits) of the smallest system into which these memory states can be embedded. For classical (i.e. mutually orthogonal) memory states, this corresponds to. For quantum memory states, which may, in general, be linearly dependent.

Let us, for now, restrict our attention to statistically-exact models, such that (f, Λ) must produce outputs with a distribution that is identical to the stochastic process being modelled. Under such a condition, the provably-memory minimal classical model of any given discrete-time stochastic process is known and can be systematically constructed. These models are referred to as the ε - machine of the process, which employs an encoding function f ε based on the causal states of the process. This encoding function satisfiesand given initial memory state, the evolution produces output x 0 with probabilityand updates the memory to state. The memory states are referred to as the causal states of the process, and the associated cost D μ is given by the logarithm of the number of causal states.

---

### Abstract representations emerge naturally in neural networks trained to perform multiple tasks [^5c3b6bfe]. Nature Communications (2023). High credibility.

For the input model, we use random Gaussian processes with radial basis function kernels of different length scales. To illustrate this approach, we begin with a D = 1 normally distributed latent variable (Fig. 5 a, left), then generate random Gaussian process functions that map this variable to a scalar output (three functions in Fig. 5 a, center). The scalar outputs of many random Gaussian process functions are then used as input to the multi-tasking model (Fig. 5 a, right; and see "Random Gaussian process inputs" in Methods for more details). Where we show results from these random Gaussian process inputs, we use D = 5 rather than the D = 1 used in the example.

The length scale of the random Gaussian process kernel controls how far two points need to be from each other in latent variable space before they become uncorrelated in representation space. As a result, the length also controls how nonlinear and non-abstract the resulting input representation of the latent variables is (Fig. 5 b). In particular, a low length scale (e.g. < 1) means that the input representation is both relatively high-dimensional (Fig. 5 c, left) and non-abstract (Fig. 5 f, g, gaussian process input). Alternatively, a high length scale (e.g. > 4) produces low-dimensional (Fig. 5 c, right) and abstract representations (Fig. 5 f, g, gaussian process input). We show that the multi-tasking model achieves high classifier generalization performance for all random Gaussian process input length scales that we investigated (Fig. 5 f, multi-tasking model, top row). We also show that the multi-tasking model achieves moderate regression generalization performance for many different length scales as well, though regression generalization performance remains at chance for the shortest length scales that we investigated (Fig. 5 g, multi-tasking model, top row).

---

### A general derivation and quantification of the third law of thermodynamics [^89d2f2b2]. Nature Communications (2017). Medium credibility.

The third law of thermodynamics has a controversial past and a number of formulations due to Planck, Einstein and Nernst. Walther Nernst's first formulation of the third law of thermodynamics, now called the heat theorem, was the subject of intense discussion. Nernst claimed that he could prove his heat theorem using thermodynamical arguments while Einstein, who refuted several versions of Nernst's attempted derivation, was convinced that classical thermodynamics was not sufficient for a proof, and that quantum theory had to be taken into account. Max Planck's formulation: when the temperature of a pure substance approaches absolute zero, its entropy approaches zero; may hold for many crystalline substances, but it is not true in general, and formulations due to Einsteinand Nernstwere for some time considered to be typically true from an experimental point of view, but sometimes violated.

A modern understanding of entropy and quantum theory takes the heat theorem outside the realm of thermodynamics. Nernst's version states that: at zero temperature, a finite size system has an entropy S, which is independent of any external parameters x, that is S (T, x 1)− S (T, x 2)→0 as the temperature T →0. Since we now understand the entropy at zero temperature to be the logarithm of the ground-state degeneracy, the validity of the heat theorem is contingent on whether the degeneracy changes for different parameters of the Hamiltonian. One can easily find families of Hamiltonians, which satisfy or violate the heat theorem. Here however, we concern ourselves with the question of Nernst's unattainability principle, that Nernst introduced to support his attempted derivations of his heat theorem and counter Einstein's objections. We can understand it as saying that putting a system into its ground state requires infinite time or an infinite number of steps. Nernst argues that if the heat theorem could be violated, then it would be possible to violate the unattainability principle (Fig. 1). We will see that this is not the case. Although one can potentially cool at a faster rate in systems violating the heat theorem, we show that the unattainability principle still holds. The bound we obtain quantifies the extent to which a change in entropy at T = 0 affects the cooling rate.

---

### Estimating the success of re-identifications in incomplete datasets using generative models [^cfb6f216]. Nature Communications (2019). High credibility.

Results

Using Gaussian copulas to model uniqueness

We consider a dataset, released by an organization, and containing a sample ofindividuals extracted at random from a population of n individuals, e.g. the US population. Each row x (i) is an individual record, containing d nominal or ordinal attributes (e.g. demographic variables, survey responses) taking values in a discrete sample space. We consider the rows x (i) to be independent and identically distributed, drawn from the probability distribution X with, abbreviated p (x).

Our model quantifies, for any individual x, the likelihood ξ x for this record to be unique in the complete population and therefore always successfully re-identified when matched. From ξ x, we derive the likelihood κ x for x to be correctly re-identified when matched, which we call correctness. If Doe's record x (d) is unique in, he will always be correctly re-identified (and). However, if two other people share the same attribute (not unique,), Doe would still have one chance out of three to have been successfully re-identified. We modelas:and κ x as:with proofs in "Methods".

We model the joint distribution of X 1, X 2,… X d using a latent Gaussian copula. Copulas have been used to study a wide range of dependence structures in finance, geology, and biomedicineand allow us to model the density of X by specifying separately the marginal distributions, easy to infer from limited samples, and the dependency structure. For a large sample spaceand a small numberof available records, Gaussian copulas provide a good approximation of the density using only d (d − 1)/2 parameters for the dependency structure and no hyperparameter.

The density of a Gaussian copula C Σ is expressed as:with a covariance matrix Σ, u ∈ [0, 1] d, and Φ the cumulative distribution function (CDF) of a standard univariate normal distribution.

---

### Input-output maps are strongly biased towards simple outputs [^50fca0dd]. Nature Communications (2018). Medium credibility.

Many systems in nature can be described using discrete input-output maps. Without knowing details about a map, there may seem to be no a priori reason to expect that a randomly chosen input would be more likely to generate one output over another. Here, by extending fundamental results from algorithmic information theory, we show instead that for many real-world maps, the a priori probability P(x) that randomly sampled inputs generate a particular output x decays exponentially with the approximate Kolmogorov complexity [Formula: see text] of that output. These input-output maps are biased towards simplicity. We derive an upper bound P(x)≲[Formula: see text], which is tight for most inputs. The constants a and b, as well as many properties of P(x), can be predicted with minimal knowledge of the map. We explore this strong bias towards simple outputs in systems ranging from the folding of RNA secondary structures to systems of coupled ordinary differential equations to a stochastic financial trading model.

---

### Abstract representations emerge naturally in neural networks trained to perform multiple tasks [^27480aa5]. Nature Communications (2023). High credibility.

Importantly, each of these three components of our framework is trained in sequence to each other: The input model (Fig. 1 c) is trained first and then frozen. The input model is used to generate the training data for the multi-tasking model (Fig. 1 d), which is trained second. Then, finally, we use our abstraction metrics (Fig. 1 e) to quantify the level of abstraction present in the representation layer of the trained multi-tasking model (and in the trained standard input, as in Fig. 2).

Fig. 2
The input model.

a Schematic of the input model. Here, schematized for D = 2 latent variables. The quantitative results are for D = 5. b The 2D response fields of 25 random units from the high-d input layer of the standard input. c The same concentric square structure shown to represent the latent variables in a after being transformed by the standard input. d (left) The per-unit sparseness (averaged across units) for the latent variables (S = 0, by definition) and standard input (S = 0.97). (right) The embedding dimensionality, as measured by participation ratio, of the latent variables (5, by definition) and the standard input (~190). e Visualization of the level of abstraction present in the high-d input layer of the standard input, as measured by the classifier generalization metric (left) and regression generalization metric (right). In both cases, the y-axis shows the distance from the learned classification hyperplane (right: regression output) for a classifier (right: regression model) trained to decode the sign of the latent variable on the y-axis (right: the value of the latent variable on the y-axis) only on representations from the left part of the x-axis (trained). The position of each point on the x-axis is the output of a linear regression for a second latent variable (trained on the whole latent variable space). The points are colored according to their true category (value). f The performance of a classifier (left) and regression (right) when it is trained and tested on the same region of latent variable space (trained) or trained in one region and tested in a non-overlapping region (tested, similar to e). Both models are trained directly on the latent variables and on the input representations produced by the standard input. The gray line is chance. The standard input produces representations with significantly decreased generalization performance.

---

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^4165d96c]. American Journal of Kidney Diseases (2006). Medium credibility.

Appendix — methods for adding residual clearance to hemodialyzer clearance notes that dialyzer clearances (spKt/V) required to achieve a stdKt/V of 2.0 volumes per week are tabulated across treatment times from 2 to 8 hours and schedules from 2 to 7 treatments per week, with values determined using a formal 2-compartment mathematical model of urea kinetics and similar results obtainable using a simplified equation; the approach gives results similar to the third data column of Table 18.

---

### Longitudinal gut microbiome changes in immune checkpoint blockade-treated advanced melanoma [^b6f40802]. Nature Medicine (2024). Excellent credibility.

Without including any of the 'peripheral' independent variables, which we adjusted for (that is, center, time to/since first injection, other forms of irAEs, patient identification, age, sex and BMI), we can write our linear regression model aswhere Z and W 1–3 are binary variables dummy coded to be either 0 or 1, always with 0 as the reference category. Thus, the β 2 coefficient for Z (PFS12: 0 is PFS < 12, 1 is PFS ≥ 12) represents the value when all treatment characteristics of interest (W 1–3) are at their reference level (that is, monotherapy (W 1), no colitis (W 2) and no PPIs (W 3)) and when the independent variable X has a value of zero (that is, baseline). We can further rewrite equation (8) to illustrate that the relationship between X and Y is conditional on Z and W 1–3 as follows:where the first and second parentheses represent the intercepts and the slopes graphing Y against X.

Post hoc contrasts to compute the comparisons of interest

To create the relevant comparisons between cases and controls, we constructed so-called post hoc contrasts (linear combinations of coefficients) directly from the fitted model. To compute these, we first constructed reference grids (Supplementary Information), which contain all relevant combinations of the categorical independent variables that we wanted to average over. Based on these reference grids, we computed marginal means of cases and controls, which we then could statistically compare. Because we already mean centered all 'peripheral' independent (continuous and categorical) variables, we only consider the coefficients associated with the treatment characteristic of interest (W 1–3), which is shown in equation (8). The post hoc contrasts we computed were (1) PFS ≥ 12 versus PFS < 12 months, (2) colitis versus no colitis, (3) PFS ≥ 12 months with and without colitis, (4) patients on combination versus monotherapy with colitis, (5) PFS ≥ 12 versus PFS < 12 months on monotherapy without colitis and no PPIs, (6) PFS ≥ 12 versus PFS < 12 months on combination therapy without colitis and no PPIs and (7) PFS ≥ 12 versus PFS < 12 months on PPIs, monotherapy and without colitis. In Supplementary Information, we show the mathematical procedure to compute these post hoc contrasts for (1), (2) and (3), but the same logic applies when computing to the remaining contrasts.

---

### Implementing digital computing with DNA-based switching circuits [^ff6dc5c0]. Nature Communications (2020). High credibility.

Fan-out and fan-in of DNA switches

With the programmability of C domain on downstream switch molecules, the current signal from an upstream switch can be easily transmitted to multiple downstream switches (fan-out). We designed and tested a DSC with a two-output switch (Fig. 3a). As shown in Fig. 3b, the same DNA sequence of C domain (shown in red) on CS(x) and CS(y) allowed current transmission path of w–x and w–y. Current signal from S(w) interacted with both C domains that blocked switch x and y, resulting in activation of the two switches. To overcome signal decay, concentration ratio of upstream and downstream was optimized (Supplementary Fig. 4). We found that a ratio of 3:1 satisfied sufficient output of both downstream switches (t 1/2 < 3 min; Fig. 3c).

Fig. 3
Fan-out and fan-in of DNA switches.

a – c Switching circuit diagram (a), molecular implementation (b), and fluorescence readout (c) of a circuit with a two-output switch. d – f Switching circuit diagram (d), molecular implementation (e), and fluorescence readout (f) of a circuit with a two-input switch. Source data are provided as a Source Data file.

With the programmability of current signal strand on upstream switches, DSCs here can easily support fan-in that represents current signals from multiple upstream switches enter one downstream switch (Fig. 3d). As shown in Fig. 3e, current signals from upstream switches x and y had the same sequence and each of them was able to interact with C domain on CS(z); thus, current signals from both S(x) and S(y) were transmitted to switch z. Fluorescence kinetics showed that either input combination x–z or y–z lead to high output (t 1/2 < 3 min, Fig. 3f). This circuit also had the same function of logic gate circuit AND(OR(x, y), z), whereas the computing speed was much faster than the reported two-layer logic gate circuits in solution

---

### A multimodal cortical network of sensory expectation violation revealed by fMRI [^29eac4d7]. Human Brain Mapping (2023). Medium credibility.

The somatosensory clusters primarily extend across the OP cortex including OP1, functionally corresponding to SII (right peak: x = 56, y = −16, z = 20, t ‐value = 8.2; left peak: x = −58, y = −16, z = 20, t ‐value = 7.1), and the IC (Id4; right peak: x = 40, y = −2, z = 10, t ‐value = 7.6; left peak: x = −38, y = −6, z = 10, t ‐value = 5.2). Additional activation was found in left postcentral sulcus identified as BA 2, containing SI (left peak: x = −48, y = −26, z = 42, t ‐value = 4.3) and around the SMA/ACC (left peak: x = −2, y = 6, z = 36, t ‐value = 4.3).

The visual clusters were found bilaterally in IT and FG (FG3; right peak: x = 32, y = −54, z = −14, t ‐value = 5.2; left peak: x = −36, y = −56, z = −8, t ‐value = 4.4), and most pronounced in LOC regions functionally corresponding to higher order visual areas V4 (hOc4; right peak: x = 48, y = −74, z = 8, t ‐value = 5.8; left peak: x = −36, y = −80, z = 14, t ‐value = 4.2) and V5 (hOc5; right peak: x = 42, y = −64, z = −2, t ‐value = 5.8; left peak: x = −48, y = −70, z = 4, t ‐value = 5.5). Additional clusters are observed in the calcarine sulcus identified as early visual areas V1/V2 (hOc1/hOc2; right peak: x = 4, y = −78, z = −12, t ‐value = 4.6; left peak: x = −4, y = −86, z = 10, t ‐value = 4.2) as well as round the right IFJ (right peak: x = 40, y = 8, z = 26, t ‐value = 4.7).

---

### As-3 [^f4572ab3]. FDA (2025). Medium credibility.

Storage and handling

Store at room temperature (25 °C/77 °F). Avoid excessive heat.

Protect from freezing

---

### Comparison of surgical approaches to the hippocampal formation with artificial intelligence [^f7101d16]. Neurosurgical Review (2025). Medium credibility.

Fig. 4
With Q learning, approaches are created with more location calculations

Fig. 5
The cubic-coordinate system (x, y, z) for AI calculation. (A) The coordinate system represents a cubic graph as a set of wireframes composed of cubes. To utilize the path-finding algorithms on segmentation images, the images must be converted into graphical representations. Anatomical function can be added as another coordinate. (B) Placing MRI scans into the cubic coordinate system to match the brain parenchyma with this system can reveal the concept of digital anatomy and act as a basis for systems that can be developed and controlled

AI algorithms have evolved on two basic mechanisms classified as unsupervised and supervised. In unsupervised AI applications, the aim is not to establish a cause-and-effect relationship. In this method, the aim is to find similarities and differences in a complex and large data set. In supervised machine training applications, AI has a training and development process. In this training process outputs are given to the computer and the computer software learns the relationship between outputs.

---

### The XZZX surface code [^12141579]. Nature Communications (2021). High credibility.

Performing large calculations with a quantum computer will likely require a fault-tolerant architecture based on quantum error-correcting codes. The challenge is to design practical quantum error-correcting codes that perform well against realistic noise using modest resources. Here we show that a variant of the surface code-the XZZX code-offers remarkable performance for fault-tolerant quantum computation. The error threshold of this code matches what can be achieved with random codes (hashing) for every single-qubit Pauli noise channel; it is the first explicit code shown to have this universal property. We present numerical evidence that the threshold even exceeds this hashing bound for an experimentally relevant range of noise parameters. Focusing on the common situation where qubit dephasing is the dominant noise, we show that this code has a practical, high-performance decoder and surpasses all previously known thresholds in the realistic setting where syndrome measurements are unreliable. We go on to demonstrate the favourable sub-threshold resource scaling that can be obtained by specialising a code to exploit structure in the noise. We show that it is possible to maintain all of these advantages when we perform fault-tolerant quantum computation.

---

### Molecular breast imaging: use of a dual-head dedicated gamma camera to detect small breast tumors [^bb37212e]. AJR: American Journal of Roentgenology (2008). Medium credibility.

Molecular breast imaging (MBI) lexicon — descriptor definitions and measurement — defines background parenchymal uptake (BPU) as the degree of radiotracer uptake within the breast parenchyma in comparison to subcutaneous fat and assesses BPU visually as photopenic, minimal/mild, moderate, or marked; for identified lesions, the intensity of uptake (photopenic, mild, moderate or marked), mass or non-mass uptake, and the distribution are described, with location and size by quadrant or clock face position, as well as depth or distance from the nipple. Lesion size is measured on the image where the finding is best visualized. By definition, x is the longest lesion measurement, y is orthogonal to x on the same image, and z is orthogonal to x and y on the image not used to measure x and y. Associated findings such as nipple, axillary, or vascular uptake should be described. This lexicon should be utilized when describing and interpreting imaging findings.

---

### The eighty five percent rule for optimal learning [^74499c58]. Nature Communications (2019). High credibility.

If the decision boundary is set to 0, such that the model chooses option A when h > 0, option B when h < 0 and randomly when h = 0, then the noise in the representation of the decision variable leads to errors with probabilitywhere F (x) is the cumulative density function of the standardized noise distribution, p (x) = p (x |0, 1), and β = 1/ σ quantifies the precision of the representation of Δ and the agent's skill at the task. As shown in Fig. 1b, this error rate decreases as the decision gets easier (Δ increases) and as the agent becomes more accomplished at the task (β increases).

The goal of learning is to tune the parameters ϕ such that the subjective decision variable, h, is a better reflection of the true decision variable, Δ. That is, the model should aim to adjust the parameters ϕ so as to decrease the magnitude of the noise σ or, equivalently, increase the precision β. One way to achieve this tuning is to adjust the parameters using gradient descent on the error rate, i.e. changing the parameters over time t according towhere η is the learning rate and ∇ ϕ ER is the derivative of the error rate with respect to parameters ϕ. This gradient can be written in terms of the precision, β, asNote here that only the first term on the right hand side of Eq. (5) depends on the difficulty Δ, while the second describes how the precision changes with ϕ. Note also that Δ itself, as the 'true' decision variable, is independent of ϕ. This means that the optimal difficulty for training, that maximizes the change in the parameters, ϕ, at this time point, is the value of the decision variable Δ * that maximizes ∂ ER/ ∂β. Of course, this analysis ignores the effect of changing ϕ on the form of the noise — instead assuming that it only changes the scale factor, β, an assumption that likely holds in the relatively simple cases we consider here, although whether it holds in more complex cases will be an important question for future work.

---

### Temporal stratification of amyotrophic lateral sclerosis patients using disease progression patterns [^51bad1de]. Nature Communications (2024). High credibility.

Methods

We propose an approach to learn disease progression groups called ClusTric. This process is 3-fold: (1) finds coherent disease progression patterns using triclustering; (2) computes the representative pattern of the resulted triclusters; (3) and then clusters the patients according to their similarity to these patterns using hierarchical clustering. Figure 6 overviews the proposed method. Although this method is herein applied to ALS stratification, it can be used as a standalone approach to understanding the progression of other diseases (e.g. multiple sclerosis or Alzheimer's disease). The remaining of this section details each step of the proposed method.

Fig. 6
ClusTric method.

The proposed method is three-fold: (1) learning triclustering patterns; (2) triclustering-based data transformation; (3) finding progression groups.

Learning triclustering patterns

We start by explaining some key concepts to better understand the process of finding disease progression patterns with triclustering. In particular, we start with the definition of a three-way dataset.

Definition 1

(Three-way dataset) A three-way dataset D = { d i j k } is defined by set of subjects X = { x 1. x ∣ X ∣ }, features Y = { y 1. y ∣ Y ∣ }, and contexts Z = { z 1. z ∣ Z ∣ }, where the elements d i j k relate subject x i, feature y j, and context z k.

In the context of our work, we consider heterogeneous data that is characterized by the presence of N subjects described by a set of static features Y s t a t i c; and temporal features Y t e m p o r a l, associated with a three-way temporal dataset, where elements d i j k relate subject x i, feature y j ∈ Y t e m p o r a l, and time point z k. The number of time points ∣ Z ∣ can depend on the temporal study and be set to 1 if relevant.

Given a three-way dataset, a tricluster and a triclustering solution are defined as follows:

Definition 2

(Tricluster) Given D, a tricluster, is a subspace defined by I ⊆ X subjects, J ⊆ Y features, and K ⊆ Z contexts, where d i j k denotes the elements of.

---

### You can' T replicate what you can' T see: a call for researchers to share their data and avoid "Fragile" correlations [^842721d5]. The Journal of Orthopaedic and Sports Physical Therapy (2021). Medium credibility.

Synopsis

We suggest that a measure of a correlation's fragility is the minimum number of items that, when replaced with the group median, result in a nonsignificant correlation on reanalysis. Between January 2000 and July 2021, there were 1769 significant correlations reported in 142 papers published in this journal, and only 51 correlations (2.9%) had available data (scatter plots from which we could digitize the raw data). Twenty-six of these 51 correlations were fragile at 4 or fewer replacements. Five of the reported significant correlations were not significant when we replicated the analysis from the extracted data. J Orthop Sports Phys Ther 2021;51(12):556–558. doi:10.2519/jospt.2021.0112.

---

### A new and accurate way to identify elite athletes [^3f9e0860]. Journal of Medical Ethics (2012). Low credibility.

A simple thought experiment and web search tools are brought to bear on conventional notions of disability. A Nobel Prize winner weighs in.

---

### Quality metrics for single-photon emission computed tomography myocardial perfusion imaging: an ASNC information statement [^a5f14253]. Journal of Nuclear Cardiology (2023). High credibility.

ASNC SPECT MPI quality metrics — figure-based site assessment explains that metrics across the 8 axes are color coded to indicate whether a site has achieved standard of excellent quality (green), good quality (yellow), or failed to do so (orange), that metrics duplicated in different axes are shown with similar border dashing, and that in the example the site achieved excellent quality for axes B, E, and F and good quality for axes C, D, and G.

---

### Pediatric Hodgkin lymphoma, version 3.2021 [^77c59052]. Journal of the National Comprehensive Cancer Network (2021). High credibility.

Classic Hodgkin lymphoma primary treatment — ABVE-PC pathway for stage IIIA only — begins with ABVE-PC x 2 cycles (category 2B) (AHOD0031) followed by Response assessment w stratified as Adequate response (RER) z or Inadequate response (SER) z; for Adequate response (RER) z, ABVE-PC x 2 cycles are given with End of therapy assessment w and either See Follow-up (PHL-8) or If < CR, ISRT m, y to all sites; for Inadequate response (SER) z, ABVE-PC x 2 cycles then ISRT m, y with End of therapy assessment w and either See Follow-up (PHL-8) or If concern for persistent disease, see Therapy for relapsed or refractory disease (PHL-9).

---

### Evolution of dosage compensation under sexual selection differs between X and Z chromosomes [^996d2433]. Nature Communications (2015). Medium credibility.

Here we build a population genetic model in which dosage compensation arises through the accumulation of cis -regulatory mutations that alter the expression of X- or Z-linked genes. To be consistent with our subsequent data analysis, we define dosage compensation as the evolution of equal expression in males and females following the loss of the Y or W allele in the heterogametic sex. As correlations in expression between the sexes are typically strongly positive, we allow mutations altering expression in one sex to cause correlated change in the other sex. Therefore, mutations that increase expression in the heterogametic sex to compensate for reduced unbalanced selection between the sexes may lead to hyperexpression in the homogametic sex, resulting in sexually antagonistic fitness effects. When there is sexual conflict, the adaptive response is dominated by the sex under stronger selection. In addition, in the case of sex chromosomes, selection is greater in the homogametic sex relative to that in the heterogametic sex. As the Z chromosome resides two-thirds of the time in males while the X chromosomes resides two-thirds of the time in females, selection on males has a disproportionate effect on the Z and selection on females has a disproportionate effect on the X. Finally, the efficacy of selection on a mutation can differ according to its location on the Z or the X chromosome. Reproductive variance may vary between the sexes, and is often higher in males. This affects Z and X chromosomes differently, as the Z chromosome is more sensitive to increased male reproductive variance than the X, lowering the rate of adaptation on the Z relative to that on the X. We examine how the combination of these forces results in differences between XY and ZW systems in the evolutionary rates of dosage compensation, and find that slower and less complete Z dosage compensation evolution is associated with a positive correlation in mutational effects across the sexes, stronger selection on males and greater male reproductive variance. Our model also predicts that dosage compensation should be most pronounced in genes that are under strong selection, biased towards the heterogametic sex. Using expression variance as a measure of selection strength on expression, we find support for these predictions from the chicken transcriptome.

---

### A space-time tradeoff for implementing a function with master equation dynamics [^57fa10b0]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### 2021 ACC / AHA key data elements and definitions for heart failure: a report of the American college of cardiology / American Heart Association task force on clinical data standards (writing committee to develop clinical data standards for heart failure) [^6e9b85e0]. Circulation: Cardiovascular Quality and Outcomes (2021). High credibility.

ACC/AHA heart failure data standards — congenital cardiac lesion interventions specify that "Procedure was performed to treat a congenital cardiac lesion" with permissible values of "Yes (if yes, specify type of repair)", "No", or "Unknown", and that the "Date of congenital cardiac lesion repair" is captured as "Date, in mm/dd/yyyy".

---

### Solving olympiad geometry without human demonstrations [^2d66e152]. Nature (2024). Excellent credibility.

Proving mathematical theorems at the olympiad level represents a notable milestone in human-level automated reasoning 1–4, owing to their reputed difficulty among the world's best talents in pre-university mathematics. Current machine-learning approaches, however, are not applicable to most mathematical domains owing to the high cost of translating human proofs into machine-verifiable format. The problem is even worse for geometry because of its unique translation challenges 1,5, resulting in severe scarcity of training data. We propose AlphaGeometry, a theorem prover for Euclidean plane geometry that sidesteps the need for human demonstrations by synthesizing millions of theorems and proofs across different levels of complexity. AlphaGeometry is a neuro-symbolic system that uses a neural language model, trained from scratch on our large-scale synthetic data, to guide a symbolic deduction engine through infinite branching points in challenging problems. On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist. Notably, AlphaGeometry produces human-readable proofs, solves all geometry problems in the IMO 2000 and 2015 under human expert evaluation and discovers a generalized version of a translated IMO theorem in 2004.

---

### Inferring time derivatives including cell growth rates using gaussian processes [^d99eaf3f]. Nature Communications (2016). Medium credibility.

Inferring the first and second time derivatives

To determine the time derivative of the data, we use that the derivative of a Gaussian process is another Gaussian process. We can therefore adapt standard techniques for Gaussian process to allow time derivatives to be sampled too.

Building on the work of Boyle, we let g (x) and h (x) be the first and second derivatives with respect to x of the latent function f (x). If f (x) is a Gaussian process then so are both g (x) and h (x). Writing ∂ 1 and ∂ 2 for the partial derivatives with respect to the first and second arguments of a bivariate function, we have

and that

as well as

following ref.

Consequently, the joint probability distribution for y and f *, g * and h * evaluated at points X * is again Gaussian (cf. equation (7)):

where we write K = K (X, X) and K ✱ = K (X *, X *) for clarity.

The covariance function is by definition symmetric: k (x i, x j) = k (x j, x i) from equation (1). Therefore, and so

for all positive integers k and. Consequently, the covariance matrix in equation (13) is also symmetric.

Conditioning on y now gives that the distribution P (f *, g *, h *| X, y, θ, X *) is Gaussian with mean

and covariance matrix

Equation (16) includes equation (9) and shows that

which gives the error in the estimate of the first derivative. Similarly,

is the error in estimating the second derivative.

Using an empirically estimated measurement noise

Although our derivation is given for a Gaussian process where the measurement errors in the data are independent and identically distributed with a Gaussian distribution of mean zero, the derivations are unchanged if the measurement noise has a different s.d. for each time point.

When the magnitude of the measurement noise appears to change with time, we first empirically estimate the relative magnitude of the measurement noise by the variance across all replicates at each time point. We then smooth this estimate over time (with a Gaussian filter with a width of 10% of the total time of the experiment, but the exact choice is not important) and replace the identity matrix, I, in equations (6), (15) and (16) by a diagonal matrix with the relative measurement noise on the diagonal in order to make predictions.

---

### The model muddle: in search of tumor growth laws [^7c1427d8]. Cancer Research (2013). Low credibility.

In this article, we will trace the historical development of tumor growth laws, which in a quantitative fashion describe the increase in tumor mass/volume over time. These models are usually formulated in terms of differential equations that relate the growth rate of the tumor to its current state and range from the simple one-parameter exponential growth model to more advanced models that contain a large number of parameters. Understanding the assumptions and consequences of such models is important, as they often underpin more complex models of tumor growth. The conclusion of this brief survey is that although much improvement has occurred over the last century, more effort and new models are required if we are to understand the intricacies of tumor growth.

---

### Quantum mechanics can reduce the complexity of classical models [^1a2c01aa]. Nature Communications (2012). Medium credibility.

Mathematical models are an essential component of quantitative science. They generate predictions about the future, based on information available in the present. In the spirit of simpler is better; should two models make identical predictions, the one that requires less input is preferred. Yet, for almost all stochastic processes, even the provably optimal classical models waste information. The amount of input information they demand exceeds the amount of predictive information they output. Here we show how to systematically construct quantum models that break this classical bound, and that the system of minimal entropy that simulates such processes must necessarily feature quantum dynamics. This indicates that many observed phenomena could be significantly simpler than classically possible should quantum effects be involved.

---

### An event-based architecture for solving constraint satisfaction problems [^f2e7014a]. Nature Communications (2015). Medium credibility.

Constraint satisfaction problems (CSPs) are a fundamental class of problems in computer science with wide applicability in areas such as channel coding, circuit optimizationand scheduling. Algorithms for solving CSPs are typically run on classical von Neumann computing platforms that were not explicitly designed for these types of problems. This paper addresses the question: how can we implement a more efficient computing substrate whose architecture and dynamics better reflect the distributed nature of CSPs?

Many dynamical systems that have been proposed for solving CSPs violate the 'physical implementability' condition. Non-physicality arises from the use of variables that can grow without bounds as the system is searching for solutions. On the other hand, there is a long, well-established tradition of studying physically realizable dynamical systems, for example, in the form of artificial neural networks, to solve CSPs or 'best-match problems'. Early attempts in this field used attractor networks, such as Hopfield networks, to solve NP hard (non-deterministic polynomial-time hard) problems such as the travelling salesman problem. These attractor networks, however, would often get stuck at locally optimal solutions. To overcome this problem, stochastic mechanisms were proposed, which require explicit sources of noise to force the network to continuously explore the solution space. While noise is an inextricable part of any physical system, dynamically controlling its power to balance 'exploratory' versus 'greedy' search, or to move the network from an exploratory phase to a greedy one according to an annealing schedule, is not a trivial operation and puts an additional overhead on the physical implementation.

---

### Abstract representations emerge naturally in neural networks trained to perform multiple tasks [^890a7077]. Nature Communications (2023). High credibility.

The multi-tasking model learns abstract representations from other kinds of nonlinear inputs

To understand the constraints on the multi-tasking model's ability to learn abstract representations from non-abstract input, we introduce both a new input model (Fig. 5 a–c) and a new kind of nonlinear task (Fig. 5 a, d, e). We control the length scale of correlations in both the input model and the tasks. Then, we quantify the classification and regression generalization performance as we vary both length scales simultaneously (Fig. 5 f, g).

Fig. 5
The multi-tasking model learns abstract structure from both random Gaussian process inputs and output tasks.

a (left) Schematic of the creation of a random Gaussian process input for one latent variable dimension (D = 1). Random Gaussian processes with a length scale = 1 are learned for the single latent variable shown on the left. Then, the responses produced by these random Gaussian processes are used as input to the multi-tasking model. The full random Gaussian process input has 500 random Gaussian process dimensions and D = 5 latent variables. (right) Schematic of the creation of two random Gaussian process tasks for the D = 1-dimensional latent variable shown on the left, showing both two example binary classification tasks and the random Gaussian process that is thresholded at zero to create each task. b Visualization of the input structure for random Gaussian process inputs of different length scales. c The embedding dimensionality (participation ratio) of the random Gaussian process for different length scales. Note that it is always less than the dimensionality of 200 achieved by the standard input. d Examples of random Gaussian process tasks for a variety of length scales. The multi-tasking model is trained to perform these tasks, as schematized in a. e The embedding dimensionality (participation ratio) of the binary output patterns required by task collections of different length scales. f Classifier generalization performance of a multi-tasking model trained to perform P = 15 classification tasks with D = 5-dimensional latent variables, shown for different conjunctions of task length scale (changing along the y axis) and input length scale (changing along the x axis). g Regression generalization performance shown as in f.

---

### Demonstrating multi-round subsystem quantum error correction using matching and maximum likelihood decoders [^a11e2ec7]. Nature Communications (2023). High credibility.

Fig. 5
Leakage analysis.

a Repeated measurement sequence for extracting leakage error during the measurement. The X π /2 pulse allows us to randomly sample leakage events fromorstates. b The leakage probability to thestate measured at Q F 14. The leakage and seepage rate is obtained by fitting the data with Eq. (15). c, d Qubit leakage in the system as a function of syndrome measurement rounds for Z − and X − basis logical states. Bar plots show theas computed from the gate and measurement leakage rates, obtained from randomized benchmarking (2Q gates) and from the sequence shown in a, respectively. Experimental results, where p accept is the acceptance probability calculated from the method outlined in Methods "Post-selection method", are shown as black symbols for comparison. The experimental results plotted here do not include initialization leakage. e Readout calibration data for Q F 12 (see Fig. 4 a). The qubit is prepared in its, andstates and measured. The collected statistics can be seen in as blue, red, and grey where the dot-dashed lines represent 3- σ for each distribution. f 3-state classification results for Q F 12 after qubit initialization, and g after the first X − syndrome measurement.

We extract the two-qubit gate leakage and seepage rate of thestate from simultaneous randomized benchmarking, with the simultaneity chosen to match the Z − and X − stabilizer sequences as illustrated in Fig. 1. Similarly, we extract the leakage/seepage rate from repeated measurement described in Fig. 5 a. In these estimations, we account for the number of gate operations and measurements for each syndrome/flag qubits as well as the code qubits measured at the end. For instance, a two round experiment for the logical Z − basis consists of an X − check for state preparation, two rounds of X − and Z − checks, and a final measurement of the code qubits. Each check consists of two-qubit gates and measurements. As a result, there are three sets of two-qubit gates and measurements on X − check qubits, two sets of two-qubit gates and measurements on Z − check qubits, and one measurement of the code qubits. The post-selection procedure discards the result if any of the qubit is leaked from the computational subspace. Therefore, we sum all the leakage probabilities to computefor each syndrome measurement round.

---

### Principles of analytic validation of immunohistochemical assays: guideline update [^cda8439c]. Archives of Pathology & Laboratory Medicine (2024). High credibility.

Supplemental Digital Content — AMSTAR (Assessing the Methodological Quality of Systematic Reviews) appraisal of methodological quality presents checklist items marked with "√" or "x": "Quality assessed & documented" is "√" and "x"; "Quality used appropriately for conclusion" is "√" and "x"; "Methods to combine used appropriately" is "√" and "√"; "Publication bias assessed" is "x" and "x"; and "COI (conflict of interest)" is "√" and "√". The AMSTAR SCORE /11 is 9 and 5.

---

### A general model for ontogenetic growth [^10660e44]. Nature (2001). Excellent credibility.

Several equations have been proposed to describe ontogenetic growth trajectories for organisms justified primarily on the goodness of fit rather than on any biological mechanism. Here, we derive a general quantitative model based on fundamental principles for the allocation of metabolic energy between maintenance of existing tissue and the production of new biomass. We thus predict the parameters governing growth curves from basic cellular properties and derive a single parameterless universal curve that describes the growth of many diverse species. The model provides the basis for deriving allometric relationships for growth rates and the timing of life history events.

---

### Emergent second law for non-equilibrium steady States [^d3105082]. Nature Communications (2022). High credibility.

In the CMOS memory the state space is two dimensional and therefore there is a single curl component f (x), or vorticity. In Fig. 5 we show the vorticity f (x) for the same parameters as in Fig. 1. We see that the model is genuinely non-equilibrium even at the coarse-grained level. However, we also see that the vorticity vanishes at the x and y axes, which explains why the bound in Fig. 4 matches the exact rate function. Finally, we study how the bound in terms of the coarse-grained entropy production rateperforms for the gray trajectory in Figs. 1 and 5, that starts in and goes through areas of non-vanishing vorticity, and thus genuinely out of equilibrium. Since in this case we do not have the exact rate function to compare with, we compare the bound with the estimation of the rate functionobtained from the exact steady state distribution. I est (x) is only defined for the discrete set of voltages v 1 and v 2 that are a multiple of the elementary voltage v e, and in the comparison we choose the closest values of v 1 and v 2 to the continuous deterministic trajectory x t, obtaining a stepwise function. The results are shown in Fig. 6. We see that even in this case our bound provides an accurate approximation of the probability distribution. This shows that the emergent second law in terms of the coarse-grained entropy production has the potential to offer a deterministic alternative to Gillespie simulations and spectral methods. As an example, we provide in the Supplementary Note 3 a full reconstruction of the steady state distribution of the CMOS memory.

---

### Pediatric Hodgkin lymphoma, version 3.2021 [^b007a021]. Journal of the National Comprehensive Cancer Network (2021). High credibility.

Pediatric Hodgkin lymphoma — stage I–II classical Hodgkin lymphoma with risk factors: Preferred initial therapy includes an ongoing clinical trial or ABVE-PC x 2 cycles (category 1) (AHOD0031) (not for stage IIBX), followed by response assessmentw. For adequate response (RER) z after ABVE-PC, give ABVE-PC x 2 cycles, then See Follow-up (PHL-8) or, if < complete response (CR), ISRTm, y to all sites. For inadequate response (SER) z, give ABVE-PC x 2 cycles, then End of therapy assessmentW. Note: All recommendations are category 2A unless otherwise indicated.

---

### 2021 ACC / AHA key data elements and definitions for heart failure: a report of the American college of cardiology / American Heart Association task force on clinical data standards (writing committee to develop clinical data standards for heart failure) [^c26855ed]. Circulation: Cardiovascular Quality and Outcomes (2021). High credibility.

Noncardiovascular history — COVID-19 exposure, previous infection, hospitalization, and date fields specify that exposure is "Exposure to someone who is confirmed or suspected to have COVID-19, and the exposed individual either tests negative or the test results are unknown", with permissible values Yes/No/Unknown; previous infection is "Patient had COVID-19 infection > 14 d ago and has recovered", with permissible values Yes/No/Unknown and notes that recovery "is usually defined after 2 negative RT-PCR tests"; hospitalization due to COVID-19 infection is recorded with permissible values Yes/No/Unknown; and timing is captured as "The date the COVID-19 diagnosis occurred" and "The date the COVID-19 hospitalization occurred", each using the permissible value format "Date, in mm/dd/yyyy".

---

### A general derivation and quantification of the third law of thermodynamics [^43738480]. Nature Communications (2017). Medium credibility.

The most accepted version of the third law of thermodynamics, the unattainability principle, states that any process cannot reach absolute zero temperature in a finite number of steps and within a finite time. Here, we provide a derivation of the principle that applies to arbitrary cooling processes, even those exploiting the laws of quantum mechanics or involving an infinite-dimensional reservoir. We quantify the resources needed to cool a system to any temperature, and translate these resources into the minimal time or number of steps, by considering the notion of a thermal machine that obeys similar restrictions to universal computers. We generally find that the obtainable temperature can scale as an inverse power of the cooling time. Our results also clarify the connection between two versions of the third law (the unattainability principle and the heat theorem), and place ultimate bounds on the speed at which information can be erased.

---

### Technical update on HIV-1 / 2 differentiation assays [^50eb7933]. CDC (2016). Medium credibility.

HIV-1/HIV-2 differentiation testing — alternatives when the FDA-approved antibody differentiation immunoassay cannot be used states that there may be circumstances under which a laboratory is unable to adopt the FDA-approved HIV-1/HIV-2 antibody differentiation immunoassay, and in this situation a laboratory has alternatives for that step in the algorithm, some of which could delay turnaround time for test results. Send specimens to another laboratory that offers the FDA-approved supplemental HIV antibody differentiation assay. Refer to CDC/APHL laboratory testing guidance section I, "Alternative Testing Sequences When Tests in the Recommended Algorithm Cannot be Used".

---

### Changes in semantic memory structure support successful problem-solving and analogical transfer [^5231072f]. Communications Psychology (2024). Medium credibility.

Introduction

In our daily life, we constantly deal with problems, ranging from the most mundane (e.g. what to cook for dinner given the ingredients at our disposal), to professional activities (e.g. how to reorganize our current plans to meet a new deadline), up to major societal challenges (e.g. how to find innovative solutions against global warming). How do we find new solutions to problems? While the ability to solve problems is a critical skill for adapting to new situations and innovating, the mechanisms underlying the problem-solving process remain largely unknown.

Among the new problems we face each day, some are well-defined (e.g. playing a jigsaw puzzle). The initial state (i.e. the number of independent pieces) and goal state (i.e. assembling the pieces so it looks like the picture model) are clear, and the solver can apply a set of operations (i.e. interlocking the pieces as a function of their shape) to reach the goal. However, for many of our problems (e.g. organizing work activities during the COVID-19 pandemic), the problem space is ambiguous. No heuristics or existing rules could be applied to transform the initial state into the goal state. Such "ill-defined" problemsthus require additional mental processes, which have been tightly linked to creative thinking –. Ill-defined problem-solving (or creative problem-solving) is often referred to as insight solving, where the solution comes to mind suddenly and effortlessly, with a "Eureka" phenomenon –. According to the Representational Change Theory, solving such problems involves restructuring the initial problem mental representational space, which presumably entails combining elements related to the problem in a new way. In theory, restructuring allows one to change perspective, reframe the problem, or escape its implicitly imposed constraints, leading to creative associations. For instance, consider the following problem: "A man walks into a bar and asks for a glass of water. The bartender points a shotgun at the man. The man says, 'Thank you', and walks out". The problem is ill-defined because the path to finding the solution is to be discovered, and the goal state is vague. Solving this problem first requires asking the right question: in which context would a shotgun and a glass of water help somebody? Rather than relying on obvious associations (e.g. a glass of water is related to thirst), solvers must fill the missing link between the relevant elements of the problem (a shotgun induces fear, and fear can be a remedy for hiccups, as can drinking a glass of water). Hence, restructuring the initial representation of a given problem would allow one to see this link and find its solution.

---

### A general derivation and quantification of the third law of thermodynamics [^314e2820]. Nature Communications (2017). Medium credibility.

Discussion

We hope the present work puts the third law on a footing more in line with those of the other laws of thermodynamics. These have already been long established, although they've recently been reformulated within the context of other resource theories. Namely, as described in ref. the first law (energy conservation), and unitarity (or microscopic reversibility) describe the class of operations which are allowed within thermodynamics. The zeroth law, is the fact that the only state which one can add to the theory without making it trivial, are the equivalence class of thermal states at temperature T. This allows the temperature to emerge naturally. The second law(s), tells us which state transformations are allowed under the class of operations. For macroscopic systems with short-range interactions, there is only one function, the entropy, which tells you whether you can go from one state to another, but in general there are many constraints. The third law quantifies how long it takes to cool a system. We propose to generalize it further: While the second laws tell us which thermodynamical transitions are possible, generalized third laws quantify the time of these transitions. In this context, it would be interesting to explore the time and resource costs of other thermodynamical transitions. It would also be interesting to explore the third law in more restricted physical settings, as well as to other resource theory frameworks, in particular, those discussed in ref.

It is worth noting that scaling we find, for example, the inverse polynomial of equation (1), is more benign than what one might have feared, and does not exclude obtaining lower temperatures with a modest increase of resources. However, it is also stronger than that envisioned when the third law was original formulated. Consider for example, the cooling protocol of Fig. 1 proposed by Nernst. It is unphysical, since it requires an infinite number of heat baths, each one at a lower and lower temperature, and with the final heat baths at close to zero temperature. However, it allows the temperature of the system to decrease exponentially in the number of steps, something which we are able to rule out when one doesn't have colder and colder reservoirs.

---

### Dextrose (dextrose 10%) [^cd9d252e]. FDA (2025). Medium credibility.

PHARMACOLOGY AND TOXICOLOGY

Glucose is an important nutrient component of the human body. Every 1 gram of glucose can produce 4 kilocalories of heat energy, so it is used to supplement heat.

---

### Norvir [^090e2dfb]. FDA (2020). Medium credibility.

15 REFERENCES

Sewester CS. Calculations. In: Drug Facts and Comparisons. St. Louis, MO: J.B. Lippincott Co; January, 1997:xix.

---

### Identifying domains of applicability of machine learning models for materials science [^72c4ba76]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ν that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by σ lin (x 1, x 2) ≡ −0.3 ≤ x 1 ≤ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: σ rbf ≡ −3.3 ≤ x 11 ≤ 3.1 ∧ x 2 ≤ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA σ poly (x 1, x 2) ≡ −3.5 ≤ x 2 ≤ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.