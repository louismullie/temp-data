# Medical Question & Answer

**Sample ID**: 3aa150f7-2d4b-f6a6-3118-bcca72a459d6
**Dataset Index**: 147476

---

## Question

Find the sum of the given polynomials in the given polynomial ring. f(x) = 4x - 5, g(x) = 2x² - 4x + 2 in Z₈[x].

Answer Choices:
A. 2x² + 5
B. 6x² + 4x + 6
C. 0
D. x² + 1

---

## Answer

> Let's see… What do we have here? The user is asking to find the sum of two polynomials in the ring Z₈[x]. Let's break this down step-by-step. First, I need to think about what Z₈[x] means and how addition is defined there. Then, I should add the polynomials term-by-term. Next, I will reduce each coefficient modulo 8. Finally, I will simplify and match the result to the given choices, double-checking for any arithmetic mistakes along the way.

> Let me first confirm the setting: Z₈[x] denotes the ring of polynomials with coefficients in the integers modulo 8, so all coefficients are reduced mod 8 after any operation. Addition is performed coefficient-wise, which means I add like terms and then reduce each coefficient modulo 8 to stay within the ring.

> Now, I will add the two polynomials term-by-term. f(x) = 4x − 5 and g(x) = 2x² − 4x + 2, so f(x) + g(x) = (4x − 5) + (2x² − 4x + 2). Combining like terms, I get 2x² + (4x − 4x) + (−5 + 2), which simplifies to 2x² + 0x − 3, or just 2x² − 3. Wait, let me verify the constant term: −5 + 2 is indeed −3, so that step is correct so far.

> Next, I should reduce coefficients modulo 8. The coefficient of x² is already 2, which is less than 8, so it remains 2. The coefficient of x is 0, so it stays 0. The constant term −3 needs to be converted to a representative in {0, 1, 2, …, 7}; −3 mod 8 is 5, because −3 + 8 = 5. So the polynomial becomes 2x² + 5. Hold on, I should double-check that −3 ≡ 5 mod 8; yes, 5 − (−3) = 8, which is divisible by 8, so the reduction is correct.

> I will now examine the answer choices. Option A is 2x² + 5, which matches exactly what I derived. Options B, C, and D do not match this result, so the correct choice is A. But wait, what if I misread the original polynomials or made a sign error? Let me reconsider: the original expressions are f(x) = 4x − 5 and g(x) = 2x² − 4x + 2, and my addition and modulo reduction are consistent with those inputs, so I should be confident in A.

---

The sum of f(x) = 4x − 5 and g(x) = 2x² − 4x + 2 in Z₈[x] is **2x² + 5**. Here's the step-by-step: add like terms to get 2x² + (4x − 4x) + (−5 + 2) = 2x² − 3, then reduce −3 modulo 8 to 5, yielding 2x² + 5. The correct choice is **A**.

---

## Step-by-step solution

### Step 1: add the polynomials

Add f(x) and g(x) term-by-term:

f(x) + g(x) = (4x − 5) + (2x² − 4x + 2)\
2x² + (4x − 4x) + (−5 + 2)\
2x² + 0x − 3\
2x² − 3

---

### Step 2: reduce coefficients modulo 8

In Z₈[x], coefficients are taken modulo 8. The coefficient −3 is congruent to 5 modulo 8, so:

2x² − 3 ≡ 2x² + 5 (mod 8)

---

### Step 3: match the result to the options

The simplified polynomial is **2x² + 5**, which corresponds to option A.

---

## Verification

To ensure correctness, substitute a value for x and check both sides modulo 8. For example, let x = 3:

- f(3) = 4(3) − 5 = 12 − 5 = 7 ≡ 7 (mod 8)
- g(3) = 2(3)² − 4(3) + 2 = 18 − 12 + 2 = 8 ≡ 0 (mod 8)
- f(3) + g(3) = 7 + 0 = 7 ≡ 7 (mod 8)

Now evaluate option A at x = 3:

2(3)² + 5 = 18 + 5 = 23 ≡ 7 (mod 8)

The results match, confirming that **option A is correct**.

---

## Conclusion

The sum of f(x) = 4x − 5 and g(x) = 2x² − 4x + 2 in Z₈[x] is **2x² + 5**, which is option A.

---

## References

### Digital display precision predictor: the prototype of a global biomarker model to guide treatments with targeted therapy and predict progression-free survival [^114kzW9f]. NPJ Precision Oncology (2021). Medium credibility.

In conclusion, DDPP is unique in two levels:
It empirically decides how many features (genes) to include in the predictors rather than stopping when the fit is no longer improved.
It empirically decides which basic parameter-free summation method best predicts the outcome.

5 Combined gene expression (parameter-free feature summation): in a previous publication, we explain how we derive F i, g from two color arrays. Briefly Equation (1) describe how feature i is calculated for gene g. where F i, g is used as the measurement for gene g in individual i, T i, g, and N i, g are the measured intensity of gene g in individual i correspondingly. In order to get a single value from multiple features (genes), we combined for each individual i all genes values (F i, g 1, F i, g 2… F i, gN) using different approaches: (1) mean:; (2) sum:; (3) median; (4) fold: and (5) fold_abs:
6 To define the optimal " n " genes investigated we interrogated the correlation between gene expression and the PFS: for each drug, a Pearson correlation test was performed between F g, the fold change multiplied by the intensity (of tumor and normal) of a single gene g (gene from the list of key genes of the drug) with the PFS for all the patients treated with the drug. The F g with the most significant correlated gene (by p value and correlation coefficient r) was driven to decision whether to continue with fold change multiplied by the intensity of the tumor or fold change multiplied by the intensity of the normal matched tissue. The key genes were then ranked based on the Pearson p value and correlation coefficient r such that the gene with the highest correlation between F g and the PFS was ranked first. Then, we added single genes by the following manner: the second most ranked gene was added to the first most ranked and the F g 1, g 2. This resulted with vector space which contains two vectors. Similarly, the third most ranked gene was added to the second highest ranked genes, and resulted with vector space with three vectors. The vector summation continued in a similar manner until we obtain n different combinations of genes, while n is the number of genes in the set and each combination denotes a vector space with n vectors in it. In order to get a single value for each vector space, we tested the five different basic mathematical operations on the vectors in each vector space mentioned above. Then, a Pearson correlation test was performed between the calculated transcriptomic value for each vector space with the PFS of the patients treated with the drug. The Pearson correlation results were ranked again by p value and correlation coefficient r. The number of genes in the set which was the most correlated with the PFS was indicated as the optimal " n " coordinates. In order to assess the likelihood of getting a significant correlator by " n " genes, we run an analysis with 100 K random " n " genes and tested how the transcriptomic value derived from a vector space which includes F g 1,… gn of these genes were correlated with the PFS. Significant results were considered by a threshold of absolute R value of 0.9 or above, and p value of 0.05 and below. Applying any of the mathematical operations listed above resulted with a single value that combines the transcriptomic expression of all targetable genes for each patient. These values were used in order to generate the linear regression model with the PFS observed.
7 Selecting the best correlators with PFS for each drug and computing a linear regression model to transform the best correlator into a predictor for a single drug. Supplemental Table 3 shows the log2 fold change (intensity in tumor versus intensity in normal) multiplied by the intensity of tumor of selected eight genes for patients treated with everolimus. The expression values were used to calculate DDPP values based on fold_abs, which is absolute value of the multiplication of all the gene's values in each patient. The DDPP values are shown in Supplemental Table 4. Then, we used the DDPP values and the real PFS values as shown in Supplemental Table 5 for Pearson correlation with 95% confidence interval analysis and linear regression model analysis (which provided the linear equation). The results of these analyzes are shown in Supplemental Fig. 3.
8 Linear regression equations developed to link gene expression with clinical outcome under specific treatments: Everolimus — Equation 1: Y = 1.499e−13 X + 3.134, where X = the absolute value of the fold of log2(fold change tumor versus normal) multiplied by log1.1 (Intensity_Tumor) of each value for each of the eight genes (AKT2, TSC1, FKB-12, TSC2, RPTOR, RHEB, PIK3CA, and PIK3CB) and Y = PFS in months. Axitinib — Equation 2: Y = 2.014e−02 X + 4.36, where X = the sum of log2(fold change tumor versus normal) multiplied by log1.1 (Intensity_Normal) of each value for each of the two genes (KIT-KITLG), and Y = PFS in months. Trametinib — Equation 3: Y = −6.872e−15 X + 7.745, where X = the fold of log2(fold change tumor versus normal) multiplied by log1.1 (Intensity_Tumor) of each values for each of the nine genes (ERK2, ARAF, CRAF, MEK1, MEK2, HRAS, ERK1, MAPK10, and KSR1), and Y = PFS in months. Afatinib — Equation 4: Y = −4.558e−02 X + 2.549, where X = the sum of log2(fold change tumor versus normal) multiplied by log1.1 (Intensity_Tumor) of each value for each of the two genes (NRG4 and NRG2), and Y = PFS in months. FGFR inhibitors — Equation 5: Y = −5.273e−02 X + 5.135, where X = the sum of log2(fold change tumor versus normal) multiplied by log1.1 (Intensity_Normal) of each value for each of the five genes (FGF10, FGF16, FGF5, FGF2, and FGF13), and Y = PFS in months. Anti-PD-1 — Equation 6: Y = 7.856e−10 X – 1.583, where X = the value of the fold of log2(fold change tumor versus normal) multiplied by log1.1 (Intensity_Normal) of each value for each of the six genes (TLR-4, PDL-2, PDL-1, CD16 (NK), CTLA-4, and CD28), and Y = PFS in months.

---

### Dominance vs epistasis: the biophysical origins and plasticity of genetic interactions within and between alleles [^115SLKER]. Nature Communications (2023). High credibility.

Model 1: protein folding

Phenotype is determined by the total concentration of folded protein. In this model, the protein of interest (X) expressed from each allele (α i, i ∈{1, 2} – one maternal and the other paternal copy respectively, and the alleles are allowed to be the wild type) has two configuration states: unfolded (X U, α i) and folded (X F, α i). The free energy difference between folded and unfolded protein states is ∆ G Folding, α i (kcal per mol). Mutations on each allele can affect folding energy (∆ G Folding), which is described as the sum of wild-type folding energy and the energy differences (mutations) ∆ G Folding, wt + ∆∆ G Folding, α i. Equilibrium between the two states follows Eq. (10).

In the above and following equations, R is the gas constant (R = 1.98 × 10 −3 kcal per mol), T is the absolute temperature for 37 °C (310.15 Kelvin) and the wild-type ∆ G Folding, wt is set to −2 kcal per mol unless stated otherwise.

The total concentration of the protein (X T) follows Eq. (11).

Expression levels from each allele were considered to be equal and therefore, [X T,1] = [X T,2] = 0.5 [X T] in all our models. Using Eqs. (10) and (11) with [X T] as a constant, we can calculate the functional molecule [X F, α i] as a function of energy terms and total protein concentration in the following way:

---

### Dominance vs epistasis: the biophysical origins and plasticity of genetic interactions within and between alleles [^116ie7jT]. Nature Communications (2023). High credibility.

Model 2: folding and ligand-binding

Phenotype is determined by the total concentration of protein-ligand complex in Model 2. In this model, one protein molecule binds to one ligand molecule and the total ligand concentration (L T) is the sum of free ligand (L) and those bound to the protein (equal to the protein-ligand complex concentration, Σ i = 1,2 (X L, α i)), which follows Eq. (19).

Compared to Model 1, there is an additional energy term – the free energy difference between the protein-ligand complex state and the unbound folded protein with free ligand state ∆ G Binding (kcal per mol) – which we define as binding energy. Mutations on each allele can now be described as those affecting protein folding energy (∆∆ G Folding) or binding energy (∆∆ G Binding).

There are three configuration states of the protein expressed from each allele: unfolded (X U, α i), folded (X F, α i) and protein-ligand complex (X L, α i). The equilibrium between [X U, α i] and [X F, α i] follows Eq. (10), and the equilibrium between [X F, α i] and [X L, α i] follows Eq. (20).

The total concentration of the protein (X T) follows Eq. (21), as shown below.

As stated earlier, [X T, α 1] = [X T, α 2] = 0.5 [X T]. By combining this information with Eq. (14), Eq. (19–21), we express the protein-ligand complex concentration from each allele as a function of energy terms, total protein concentration and the free ligand concentration as shown in Eq. (22):

---

### Heat-rechargeable computation in DNA logic circuits and neural networks [^115owKQp]. Nature (2025). Excellent credibility.

Fig. 5
Reusable logic gate.

a, Implementation of a two-input logic gate with signal restoration. A two-stranded gate G 3,4 sums the inputs together. A hairpin threshold Th 3,4 keeps the output off if the sum is below 0.8 (when both inputs are off) for logic OR and below 1.6 (when at most one input is on) for logic AND. A hairpin gate G 4,5 and fuel F 4 amplify and restore the signal to an ideal on state if the sum exceeds the threshold. Another hairpin gate (G 1,3 or G 2,3) and fuel (F 1 or F 2) are needed for each input (X 1 or X 2), upstream of the two-stranded summation gate; otherwise, the input inactivation would interfere with the reset of the summation gate as they occur at similar temperatures during cooling. b, Reactions and simulation of reset. A representative set of reactions are shown here, and the full set of reactions is listed in Supplementary Note 2.7. The reactions in black and grey indicate the desired and undesired reactions, respectively. The dark grey box highlights a strand displacement reaction that helps restore the threshold and summation gate. c, d, Simulations and fluorescence kinetics experiments of the OR (c) and AND (d) gates before and after rest. The DNA sequences are listed in Supplementary Table 8.

Source Data

To evaluate logic gate composability, we constructed a three-layer circuit that computes the first 16 elements of the infinite Fibonacci word (Fig. 6a), a binary sequence with rich combinatorial properties. Although the Fibonacci word can be generated iteratively, our circuit is feedforward and designed from a truth table, requiring inputs indicating the element index for each computation round. Composing arbitrary logic circuits naturally enforces the alternating hairpin and two-stranded gate rule (Fig. 6b). In a previous study, fan-out required extra gates for shared inputs. Here owing to input inactivation, every input connects to an extra first-layer gate, preserving the alternating rule when counting two-stranded input wastes as the 0th layer. The resulting seven-layer DNA circuit maintained a consistent performance across 16 rounds of computation, exhibiting reliable responses after 15 resets over 640 h (Fig. 6c).

---

### Improved machine learning algorithm for predicting ground state properties [^115TKUSN]. Nature Communications (2024). High credibility.

Introduction

Finding the ground state of a quantum many-body system is a fundamental problem with far-reaching consequences for physics, materials science, and chemistry. Many powerful methods – have been proposed, but classical computers still struggle to solve many general classes of the ground state problem. To extend the reach of classical computers, classical machine learning (ML) methods have recently been adapted to study this and related problems both empirically and theoretically –. A recent workproposes a polynomial-time classical ML algorithm that can efficiently predict ground state properties of gapped geometrically local Hamiltonians, after learning from data obtained by measuring other Hamiltonians in the same quantum phase of matter. Furthermore, shows that under a widely accepted conjecture, no polynomial-time classical algorithm can achieve the same performance guarantee. However, although the ML algorithm given inuses a polynomial amount of training data and computational time, the polynomial scalinghas a very large degree c. Here, denotes that f (x) is asymptotically upper bounded by g (x) up to constant factors with respect to the limit n → ∞. Moreover, when the prediction error ϵ is small, the amount of training data grows exponentially in 1/ ϵ, indicating that a very small prediction error cannot be achieved efficiently.

In this work, we present an improved ML algorithm for predicting ground state properties. We consider an m -dimensional vector x ∈ [−1, 1] m that parameterizes an n -qubit gapped geometrically local Hamiltonian given aswhere x is the concatenation of constant-dimensional vectorsparameterizing the few-body interaction. Let ρ (x) be the ground state of H (x) and O be a sum of geometrically local observables with ∥ O ∥ ∞ ≤ 1. We assume that the geometry of the n -qubit system is known, but we do not know howis parameterized or what the observable O is. The goal is to learn a function h * (x) that approximates the ground state propertyfrom a classical dataset, whererecords the ground state property for x ℓ ∈ [−1, 1] m sampled from an arbitrary unknown distribution. Here, means that y ℓ has additive error at most ϵ. If, the rigorous guarantees improves.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^116DSPos]. CDC (2011). Medium credibility.

Appendix G. Mathematical formula for sample size — sample size required to detect the difference of GM between two subpopulation groups defines a z test with null hypothesis GM1 = GM2 or GMmax/GMmin = 1 and alternative GM1 ≠ GM2 or GMmax / GMmin > 1, where GMmax = max(GM1, GM2) and GMmin = min(GM1, GM2). It considers power W (say 80% chance) to detect that one group's GM is at least k fold higher and gives the p-value as p = Prob(Z ≥ z) = 1 − φ−1(z) with Z ~ Normal (0,1). Given α (e.g., 0.05) and target ratio k, power is W = Prob (Z > φ−1(1 − α) − log10(k)/(S√(1/n1 + 1/n2))), where S is the expected standard deviation of log10(VL) in the population of interest; when n1 = n2 = n, W = Prob (Z > φ−1(1 − α) − log10(k)/(S√(2/n))). The required sample size is n = 2[(φ−1(1 − α) − φ−1(1 − W))S/log10(k)]², and tables are provided for W = 0.8 and W = 0.9 with α = 0.05.

---

### Quantifying unobserved protein-coding variants in human populations provides a roadmap for large-scale sequencing projects [^112aVx3J]. Nature Communications (2016). Medium credibility.

Methods

UnseenEst algorithm

UnseenEst uses the empirical SFS of a given class of variants in a cohort to estimate its frequency distribution in the population. The inputs into the algorithm are the number of alleles in the sample cohort, k, and the SFS, which is a set of counts, { F i }, where F i is the number of variants that are observed in exactly i out of the k alleles. A key challenge for the method is to accurately estimate the frequency distribution of variants that have empirical count of 0 (that is, they are not observed) in the cohort but are likely to have some small, non-zero frequency in the population.

More concretely, let X denote a discrete set of frequencies x in [0, 1] and let h (x) denote the fraction of all the variants with frequency x. UnseenEst estimates h (x) by finding the set of h (x) that jointly minimizes the value of the expression:

where bin(x, k, i) is the binomial probability of observing i heads in k independent flips of a coin with bias x. The intuition for minimizing this objective is as follows:is the number of variants that we expect to find in i out of k alleles in the cohort and F i is the empirical number of variants observed in i alleles. If h (x) is the true frequency distribution, then the expectation should be close to the empirical, which is why we want to find h (x) that minimizes the objective above. Given an estimate of the frequency distribution h (x), the expected number of unique variants in N alleles can be calculated by the formula.

The standard 3rd order jackknife estimator does not estimate the frequency distribution h (x). Instead, it directly estimates that the number of unique variants in N alleles is g 1 (N, k) F 1 +g 2 (N, k) F 2 +g 3 (N, k) F 3, where F 1, F 2, F 3 are the number of variants observed once, twice and three times in k alleles, and g 1, g 2, g 3 are specific functions of N and k derived from self-consistency requirements. Note that the confidence intervals of the jackknife in Supplementary Fig. 3 are very narrow because there is relatively little variation in the counts F 1, F 2, F 3, when the sample size is large.

---

### Bayesian tomography of high-dimensional on-chip biphoton frequency combs with randomized measurements [^115ZWVUa]. Nature Communications (2022). High credibility.

In addition to the parameters forming ρ, the scale factor K must also be suitably parameterized. Following ref. we find it convenient to write K = K 0 (1 + σ z), where K 0 and σ are hyperparameters defined separate of the inference process, and z is taken to follow a standard normal distribution, leading to a normal prior on K of mean K 0 and standard deviation K 0 σ. We take σ = 0.1 and K 0 equal to the sum of the counts in all d 2 bins for the first JSI measurement (r = 1), where the absence of modulation ensures that all initial photon flux remains in measured bins, i.e. This provides an effectively uniform prior, since a fractional deviation of 0.1 is much larger than the maximum amount of fractional uncertaintyexpected from statistical noise at our total count numbers; the use of a normal distribution simplifies the sampling process.

The total parameter set can therefore be expressed as the vector, with the prior distribution

We note that this parameterization entails a total of 4 d 4 + 1 independent real numbers (2 d 4 complex parameters for ρ, one real parameter for K) — noticeably higher than the minimum of d 4 − 1 required to uniquely describe a density matrix. Nevertheless, this ρ (y) parameterization is to our knowledge the only existing constructive method to produce Bures-distributed states, and is straightforward to implement given its reliance on independent normal parameters only.

Following Bayes' rule, the posterior distribution becomeswhereis a constant such that ∫ d x π (x) = 1. We have adopted this notation for Bayes' theorem — rather than the more traditional — to emphasize the functional dependencies on x, which are all that must be accounted for in the sampling algorithm below. From π (x), the Bayesian mean estimator f B of any quantity (scalar, vector, or matrix) expressible as a function of x can be estimated aswhere, in lieu of direct integration, S samples { x (1), …, x (S) } are obtained from the distribution π (x) through Markov chain Monte Carlo (MCMC) techniques, as described below.

---

### Using both qualitative and quantitative data in parameter identification for systems biology models [^1178TxuU]. Nature Communications (2018). Medium credibility.

Fitting with qualitative and quantitative data

In the following sections, we will demonstrate an approach by which we incorporate both qualitative and quantitative data into a parameter identification procedure. We minimize an objective function with contributions from both types of data:where x is the vector of the unknown model parameters. The term f quant (x) is a standard sum of squares over all quantitative data points j :For the term f qual (x), we construct a function that likewise takes a lower value as the qualitative data are better matched by model outputs. We express each qualitative data point as an inequality of the form g i (x) < 0. Given this constraint, we seek to minimize the value of C i · max(0, g i (x)), where C i is a problem-specific constant. In other words, if the constraint g i (x) < 0 is violated, we apply a penalty proportional to the magnitude of constraint violation. The final objective function to be minimized consists of the sum of the penalties arising from all of the individual constraints:In the constrained optimization literature, Eq. (3) is called a static penalty function. The squared difference, max(0, g i (x)) 2, is also sometimes used, but we chose Eq. (3) to avoid overpenalizing single constraints with a large degree of violation.

f tot (x) can be minimized using a standard optimization algorithm such as differential evolutionor scatter search.

Fitting a model of Raf inhibition

Next, we demonstrate usage of a combination of qualitative and quantitative data for parameter identification (which we will also refer to as finding a fit to the data) for a simple biological model. We use synthetic data to parameterize a model, originally described in Kholodenko et al. for the dimerization of the protein kinase Raf and the inhibition of Raf by a kinase inhibitor. This model has relevance for cancer treatment. We show that qualitative data can be used to improve confidence limits on the parameter values.

We consider the model shown in Fig. 2a. Raf (R) is able to dimerize, and each Raf monomer is able to bind an inhibitor (I). The model parameters consist of six equilibrium constants, denoted K 1 through K 6. Note that these six parameters are not independent. By detailed balance, K 4 = K 1 K 3 / K 2, and K 6 = K 4 K 5 / K 2 = , leaving four independent model parameters.

---

### Explainable artificial intelligence for mental health through transparency and interpretability for understandability [^113KHHaD]. NPJ Digital Medicine (2023). Medium credibility.

Definition (Transparency). The inputs x and feature space f (x) of a model should either
stand in close correspondence to how humans understand the same inputs or,
relationships between inputs (and their representation in features space) should afford an interpretation that is clinically meaningful

For example, if the inputs to a model are A g e and performance on some cognitive task, T e s t S c o r e, then the model is feature transparent if:
trivially, the feature space is identical to the inputs, f (x) ≡ x.
the feature space can be described e.g. as an explicit function of the inputs f (x) = A g e + T e s t S c o r e 2 — in this example, the function may represent feature engineering that includes human-expert domain knowledge relevant to the application,
the feature space can be represented to a human operator such that differences and similarities between exemplars (i.e. two different patients) preserves or affords a clinical meaning e.g. exemplars with similar T e s t S c o r e values aggregate together even if they differ in A g e. In this example, let f (x 1) represent an individual with a low T e s t S c o r e and younger A g e (a clinical group representing premature onset of a cognitive disorder) and f (x 2) represent an individual with a low T e s t S c o r e and higher A g e (representing another clinical group) — if f (⋅) is a non-trivial function, we need to provide a mechanism that exposes why x 1 and x 2 are represented differently/similarly under f (⋅) consistent with a human expert's differentiation of the two cases; an obvious method being distance-preserving mappings for unsupervised algorithms or conformity measures for e.g. supervised classification with deep learning.

---

### DNA synthesis for true random number generation [^1132UFJa]. Nature Communications (2020). High credibility.

Nucleotide-binding prevalence

For every position × along the sequence data, the relative presence of two nucleotides following each other (e.g. AT) is normalized by the relative presence of the single nucleotides (e.g. A and T) at the corresponding position. (e.g. for AT at position x: f x norm (AT) = ((f x (AT)/ f x (A))/(f x +1 (T)). This gives the general formula with nucleotides N 1 and N 2: f x norm (N 1 N 2) = ((f x (N 1 N 2)/ f x (N 1))/(f x +1 (N 2)).

NIST evaluation parameters

NIST evaluation tests were chosen such that for each test, 56 bit streams containing 1096 bits were tested (with the exception of the rank test, where 56 bit streams containing 100,000 bits each were tested). This variant of statistical analyses was chosen and explained by Rojas et al. The tests were applied with the standard parameters as given by the NIST statistical test suite, with the following parameters differing from set values: (1) frequency test, no parameter adjustments; (2) block frequency test, no parameter adjustments (block length, M = 128); (3) cumulative sums test, no parameter adjustments; (4) runs test, no parameter adjustments; (5) longest runs of ones test, no parameter adjustments; (6) rank test, no parameter adjustments; (7) discrete Fourier transform test, no parameter adjustments; (8) approximate entropy test parameter adjustment: block length, m = 5; 9) serial test no parameter adjustments (block length, m = 16). For each statistical test, the NIST software computes a P -value, which gives the probability that the sequence tested is more random than the sequence a perfect RNG would have produced. Thereby, a P -value of 1 indicates perfect randomness, whereas a P -value of 0 indicated complete non-randomness. More specifically, for a P -value ≥ 0.001: sequence can be considered random with a confidence of 99.9%, and for a P -value < 0.001 sequence can be considered non-random with a confidence of 99.9%.

---

### The causal pivot: a structural approach to genetic heterogeneity and variant discovery in complex diseases [^1142zN7J]. American Journal of Human Genetics (2025). Medium credibility.

Conditional analysis

Here, we present an overview of the mathematical derivation of the CP approach; more details are in the supplemental methods. In what follows, we refer to the variables PRS as X, RV as G, and disease outcome as Y. The CP derives from manipulation of the probabilistic factorization determined by the graphical model (Figures 1 A and 1E). The joint density under the model isIn the expression, represents the respective probability density functions for their arguments, and the bar symbol expresses conditioning on variables to the right-hand side of the bar. The parameter vector θ represents the unknown parameters governing the probabilistic structure of the system; for clarity, we emphasize that the parameter θ comprises the parameters θ R that determine the forward conditional relationship between Y and X, G — also known as the outcome model; the separate parameters θ XG determine the distributions of X and G, which in the simple model are exogenous and independent and can be represented separately as θ X and θ G.

The CP procedure follows from application of Bayes' rule to the probabilistic factorization implied by the graphical model. In notation, application of Bayes' rule and independence of X and G Equation 1 leads to

For G representing RV status, this expression states that the conditional probability for a sampled individual to bear an RV — denoted by G = 1 — or to not have an RV — denoted G = 0 — when X, Y are conditioned upon is given by the ratio of the forward conditional probability of their phenotype Y = y given PRS X = x and G = g divided by the average probability of Y integrating out G multiplied against the prior probability of G = g. This factorization is general and does not depend on the model specification of f(y | x, g) or f(x) or f(g); moreover, the factorizations in Equations 1 and 2 do not depend on a univariate restriction on X, G, or Y, nor does it depend on the binary character of G or the distributional character of X. The form holds under the model in Figure 1.

---

### The X factor: a robust and powerful approach to X-chromosome-inclusive whole-genome association studies [^112jh34A]. Genetic Epidemiology (2021). Medium credibility.

Abstract

The X‐chromosome is often excluded from genome‐wide association studies because of analytical challenges. Some of the problems, such as the random, skewed, or no X‐inactivation model uncertainty, have been investigated. Other considerations have received little to no attention, such as the value in considering nonadditive and gene–sex interaction effects, and the inferential consequence of choosing different baseline alleles (i.e. the reference vs. the alternative allele). Here we propose a unified and flexible regression‐based association test for X‐chromosomal variants. We provide theoretical justifications for its robustness in the presence of various model uncertainties, as well as for its improved power when compared with the existing approaches under certain scenarios. For completeness, we also revisit the autosomes and show that the proposed framework leads to a more robust approach than the standard method. Finally, we provide supporting evidence by revisiting several published association studies. Supporting Information for this article are available online.

---

### Correction [^111zWxo2]. Journal of Radiology Case Reports (2017). Low credibility.

[This corrects the article on p. 8 in vol. 10.].

---

### The X factor: a robust and powerful approach to X-chromosome-inclusive whole-genome association studies [^116iMkXs]. Genetic Epidemiology (2021). Medium credibility.

The X-chromosome is often excluded from genome-wide association studies because of analytical challenges. Some of the problems, such as the random, skewed, or no X-inactivation model uncertainty, have been investigated. Other considerations have received little to no attention, such as the value in considering nonadditive and gene-sex interaction effects, and the inferential consequence of choosing different baseline alleles (i.e., the reference vs. the alternative allele). Here we propose a unified and flexible regression-based association test for X-chromosomal variants. We provide theoretical justifications for its robustness in the presence of various model uncertainties, as well as for its improved power when compared with the existing approaches under certain scenarios. For completeness, we also revisit the autosomes and show that the proposed framework leads to a more robust approach than the standard method. Finally, we provide supporting evidence by revisiting several published association studies. Supporting Information for this article are available online.

---

### Natural statistics support a rational account of confidence biases [^114oZA1F]. Nature Communications (2023). High credibility.

MNIST

For the standard 10-choice version of MNIST, g c l a s s was trained with a cross-entropy loss over the 10 possible image classes. For the two-choice version of MNIST, g c l a s s was trained with a binary cross-entropy loss. The target was 0 if x belonged to class s1 and 1 if x belonged to class s2. The confidence output layer g c o n f was trained with a binary cross-entropy loss. The target was 1 if the classification output was correct and 0 if the classification output was incorrect. The classification and confidence losses were summed, and the entire architecture was trained through backpropagation. Training was performed for 5 epochs using the Adam optimizer, with a learning rate of 5 e − 4 and a batch size of 32. Note that for the two-choice version, these epochs were about 1/5 as long as they were for the standard version (since they only involved 2 out of the 10 possible image classes). All weights and biases were initialized using PyTorch defaults.

CIFAR-10

For the CIFAR-10 dataset, g c l a s s and g c o n f were trained with the same loss functions used for MNIST (either cross-entropy loss (10-choice) or binary cross-entropy loss (two-choice) for classification, binary cross-entropy loss for confidence), which were summed and used to train the entire architecture through backpropagation. Networks were trained for 164 epochs using stochastic gradient descent with weight decay of 1 e − 4, momentum of 0.9, and a batch size of 128. An initial learning rate of 0.1 was used, which was then set to 0.01 at training epoch 82, and 0.001 at training epoch 123. All weights in f were initialized using a Kaiming normal distribution, and all weights in the output layers g c l a s s and g c o n f were initialized using an Xavier normal distribution.

---

### Genetic association mapping leveraging gaussian processes [^114u5zkN]. Journal of Human Genetics (2024). Medium credibility.

GP regression and its use in genetic association mapping

Because the GP yielding f (x) has various useful properties inherited from the normal distribution, GP can be used to estimate a nonlinear function f (X) from output dataalong continuous factor X. The extended linear model y = f (X) + ε is referred to as the GP regression and widely used in the machine learning framework. This model can be used to map dynamic genetic associations for normalized gene expression or other common complex quantitative traits (e.g. human height) along the continuous factor x (e.g. cellular states or donor's age). Let us denote the genotype vectorand the kinship matrix R among N individuals, the mapping model, as proposed by us or others can be expressed as follows:whereare all GPs with similar covariance matrices, where ⊙ denotes element wise product between two vectors or matrices with the same dimensions, K = k (X, X) denotes the covariance matrix with a kernel function, and ε denotes the residuals. Intuitively, α models the average baseline change of y in relation to x, while β represents the dynamic genetic effect along x. The effect size is multiplied by the genotype vector g, indicating that the output y i varies between different genotype groups (g i ∈ {0, 1, 2}). In fact, the effect size β (x i) is additive to the baseline α (x i) at each x i, which is the same as the standard association mapping. Here statistical hypothesis testing is performed under the null hypothesis of δ g = 0, as the strength of genetic association is determined by δ g.

---

### X-f choice: reconstruction of undersampled dynamic MRI by data-driven alias rejection applied to contrast-enhanced angiography [^112Ac2rr]. Magnetic Resonance in Medicine (2006). Low credibility.

A technique for reconstructing dynamic undersampled MRI data, termed "x-f choice", was developed and applied to dynamic contrast-enhanced MR angiography (DCE-MRA). Regular undersampling in k-t space (a hybrid of k-space and time) creates aliasing in the conjugate x-f space that must be resolved. When regions in the object containing fast dynamic change are sparse, as in DCE-MRA, signal overlap caused by aliasing is often much less than the undersample factor would imply. x-f Choice reconstruction identifies overlapping signals using a model of the full non-aliased x-f space that is automatically generated from the undersampled data, and applies parallel imaging (PI) to separate them. No extra reference scans are required to generate either the model or the coil sensitivity maps. At each location in the reconstructed images, g-factor noise amplification is compared with predicted reconstruction errors to obtain an optimized solution. Acceleration factors greater than the number of receiver coils are possible, but are limited by the sparseness of the dynamic content and the signal-to-noise ratio (SNR) (in DCE-MRA the latter is dominant). Temporal fidelity was validated for up to a factor 10 speed-up using retrospectively undersampled data from a six-coil array. The method was tested on volunteers using fivefold prospective undersampling.

---

### Nonadditive gene expression is correlated with nonadditive phenotypic expression in interspecific triploid hybrids of willow (salix spp.) [^116uCbB2]. G3 (2022). Medium credibility.

Regulatory divergence classifications

To determine cis - and trans -effects on gene expression, separate binomial exact tests were performed using library-normalized read counts of diploid (P 2X) and tetraploid (P 4X) parent alleles in the parents (P 2X and P 4X) and the F 1 triploid progeny individual (H 2X and H 4X) from the P 2X × P 4X cross. For a 2-sided binomial test, the null hypothesis is that the expected counts are in the same proportions as the library sizes, or that the binomial probability for the first library is n 1 /(n 1 + n 2). To test the null of independence of rows (P 2X vs P 4X and H 2X vs H 4X) and columns (P 2X vs H 2X and P 4X vs H 4X), Fisher's exact test was performed on a 2 × 2 matrix comprised of P 2X and P 4X and H 2X and H 4X normalized read counts. For all tests, a fixed FDR was applied at a level of 0.005. Filtering parameters required ≥ 20 reads summed between the parents, and 2 alleles at a locus, such that each allele corresponds to either the diploid or tetraploid parent.

For each site, significant differences (FDR = 0.005) on the expression of parent alleles can occur either between the parents (P, binomial exact test), the hybrid (H, binomial exact test), or all (F, Fisher's exact test). Categories of regulatory functions considered cis -only, trans -only, cis + trans, cis × trans, compensatory were assigned following previously described methods. Conservation of expression was attributed to cases where no significant differences could be observed. Ambiguous cases were observed when only one of the 3 tests (P, H, or F, described above) were deemed significant. While ambiguous cases could somewhat be resolved by lowering the significance threshold (e.g. FDR = 0.05), approximately equal proportions of ambiguous assignments were observed across regulatory divergence classes and triploid individuals. However, parent-only (P)-ambiguous genes were more common than the other ambiguous cases, of which, F-ambiguous genes were the least frequent.

---

### Mitigating data bias and ensuring reliable evaluation of AI models with shortcut hull learning [^115tqCbd]. Nature Communications (2025). High credibility.

Theory for diagnosing data shortcuts with shortcut hull learning

For a Borel measurable function f, if, i.e. then. In practice, neural networks can be employed to learn the function f. Given a dataset, classification tasks typically divideinto a training setand a test set. If, it indicates that f has learned the data distribution. Assuming a neural network has m layers, with the first i layers denoted by g i, then f = g m. It can be posited that each layer of the neural network performs feature extraction, resulting in the relationship σ (X) ⊇ σ (g 1 (X)) ⊇ ⋯ ⊇ σ (g m (X)) by the Doob–Dynkin lemma. When interpreting neural networks, it is common to avoid considering thresholded output labels as output features due to potential significant information loss. Instead, we use the features from soft labels output or previous layers as task-correlated features learned by the neural networks, denoted as g (X), implying that σ (g (X)) ⊇ σ (f (X)). Multiple neural networks can be employed to learn the intrinsic features of the dataset, i.e. σ (f (X)) = ⋂ g ∣ σ (g (X))⊇ σ (f (X)) σ (g (X)). By equivalently rewriting Eq. (6) as, we obtainWe refer to Eq. (7) as the SH, which intuitively represents the minimal shortcut. As the number of models increases, it converges exponentially (see Supplementary Note 1 for proof). This approach of using multiple models to learn the SH is termed SHL. To map the abstract SHL in the probability space onto real-valued features that are both observable and operable, and to represent features from different models within the same space, we associate the neural network features with the input X, Consequently, Eq. (7) can be reformulated as follows:where, denotes a random subset index mapping of X, Forand ∀ ω ∈ Ω. A detailed derivation of Eq. (8) can be found in the Methods section.

---

### Spin dephasing under nonlinear gradients: implications for imaging and field mapping [^1171KZF1]. Magnetic Resonance in Medicine (2012). Low credibility.

This work examines the prototypical MR echo that would be expected for a voxel of spins evolving in a strong nonlinear field, specifically focusing on the quadratic z(2) - ½(x(2) + y(2)) field. Dephasing under nonlinear gradients is increasingly relevant given the growing interest in nonlinear imaging, and here, we report several notable differences from the linear case. Most notably, in addition to signal loss, intravoxel dephasing under gradients creating a wide and asymmetric frequency distribution across the voxel can cause skewed and nonlinear phase evolution. After presenting the qualitative and analytical origins of this difference, we experimentally demonstrate that neglecting these dynamics can lead to significant errors in sequences that assume phase evolution is proportional to voxel frequency, such as those used for field mapping. Finally, simplifying approximations to the signal equations are presented, which not only provide more intuitive forms of the exact expression but also result in simple rules to predict key features of the nonlinear evolution.

---

### Improved machine learning algorithm for predicting ground state properties [^116HxvWN]. Nature Communications (2024). High credibility.

The setting considered in this work is very similar to that in, but we assume the geometry of the n -qubit system to be known, which is necessary to overcome the sample complexity lower bound of N = n Ω(1/ ϵ) given in. Here, f (x) = Ω(g (x)) denotes that f (x) is asymptotically lower bounded by g (x) up to constant factors. One may compare the setting to that of finding ground states using adiabatic quantum computation –. To find the ground state propertyof H (x), this class of quantum algorithms requires the ground state ρ 0 of another Hamiltonian H 0 stored in quantum memory, explicit knowledge of a gapped path connecting H 0 and H (x), and an explicit description of O. In contrast, here we focus on ML algorithms that are entirely classical, have no access to quantum state data, and have no knowledge about the Hamiltonian H (x), the observable O, or the gapped paths between H (x) and other Hamiltonians.

The proposed ML algorithm uses a nonlinear feature map x ↦ ϕ (x) with a geometric inductive bias built into the mapping. At a high level, the high-dimensional vector ϕ (x) contains nonlinear functions for each geometrically local subset of coordinates in the m -dimensional vector x. Here, the geometry over coordinates of the vector x is defined using the geometry of the n -qubit system. The ML algorithm learns a function h * (x) = w * ⋅ ϕ (x) by training an ℓ 1 -regularized regression (LASSO) – in the feature space. An overview of the ML algorithm is shown in Fig. 1. We prove that given ϵ = Θ(1), Here, the notation f (x) = Θ(g (x)) denotes thatand f (x) = Ω(g (x)) both hold. Hence, f (x) is asymptotically equal to g (x) up to constant factors. the improved ML algorithm can use a dataset size ofto learn a function h * (x) with an average prediction error of at most ϵ, with high success probability.

---

### Titanium-based potassium-ion battery positive electrode with extraordinarily high redox potential [^115eHz9u]. Nature Communications (2020). High credibility.

KTiPO 4 F adopts a KTiOPO 4 -(KTP)-type structure built upon helical chains of corner-shared TiO 4 F 2 octahedra linked by PO 4 tetrahedra through vertexes to form a robust framework that encloses a 3D system of intersecting continuous spacious cavities accommodating K + ions (Fig. 1a, inset). The KTiPO 4 F unit cell is about 3.5% larger than that of KVPO 4 F due to a larger radius of Ti 3+ compared to V 3+ (0.67 vs. 0.64 Å, respectively, for coordination number, CN = 6).

Both K sites are disordered similar to KVPO 4 Fbeing split into two and three K subsites for the K1 and K2 positions, respectively (Supplementary Table 2), aligned along the "helical" migration pathway characteristic to the KTP-type structures. Such a distribution of K + ions within the voids with several energy minima anticipates their high mobility in the structure. Two symmetry inequivalent PO 4 tetrahedra are quite regular.

In the TiO 4 F 2 octahedra, fluorine atoms occupy cis - and trans -positions for the Ti1 and Ti2 atoms, respectively (Fig. 1d). Average Ti–X (X = O, F) bond lengths are 2.014 Å for the Ti1 site and 2.038 Å for the Ti2 site being both characteristic to a Ti 3+ environment. The 3+ oxidation state of titanium is also validated by calculations of bond valence sums for the titanium sites (3.10(5) and 2.95(5) for Ti1 and Ti2, respectively) as well as by the EELS spectroscopy at the Ti-L 2,3 edge (Fig. 1e), EPR measurements (Fig. 1f, g), and DFT + U calculations.

---

### Faecal occult blood loss accurately predicts future detection of colorectal cancer. A prognostic model [^113n4Uu7]. Gut (2023). Medium credibility.

Analysis

Statistical associations between predictions and outcomes were expressed as ORs and tested for significance using Pearson's χ 2 test for crude (unadjusted) ORs and Student's t-tests for multivariate-adjusted ORs (ie, those from the risk prediction models). In general, statistical associations and differences were considered significant below a 5% probability threshold.

Multivariate logistic regression was used to predict outcomes of the third round of FIT screening. For the main predictor (F-Hb concentration), we also considered log-transformed, squared, combined (summed), discrete terms (categories of 0, 0.1–2.5, 2.6–9.9, 10–19.9, …, 40–46.9 µg/g), and interactions with age and sex, to allow for non-linear relationships. The category 0.1–2.5 µg/g was included (combined with either 0 or 2.6–9.9) to examine whether concentrations below 2.6 µg/g (limit of detection) are predictive, despite a probability of > 5% of misclassification of those concentrations as 0 µg/g. The choice between different possible model specifications was made independently for AN and CRC as the primary outcome and was based on (1) statistical model specification tests, (2) the prognostic performance of the model and (3) the clinical face validity. More details on the model selection procedure are in the online supplemental appendix.

---

### A theoretical framework for controlling complex microbial communities [^116N7kjZ]. Nature Communications (2019). High credibility.

Structural accessibility characterizes the generic absence of autonomous elements

In general, it remains extremely difficult finding a pair { f, g } that models the controlled population dynamics of a microbial community. This fact might suggest it is impossible to predict if the controlled community has autonomous elements or not, making it impossible to identify its driver species. We now show that this seemingly unavoidable limitation can be solved using the topology of the controlled ecological network of the community.

Define the networkassociated with { f, g } as follows: (x j → x i) ∈ E f, g if x j appears in the right-hand side ofin Eq. (2) or x i (t +) in Eq. (3). Similarly, (u j → x i) ∈ B f, g if. Using this definition, we next describe the classof all possible controlled population dynamics that a controlled microbial community can have given we know its. Mathematically, contains all base models { f *, g * } such that, together with all deformations { f, g } of each of those base models. The base models characterize the simplest controlled population dynamics that the community can have, leading us to choose them as controlled GLV models with constant susceptibilities:for i = 1, …, N. The parameters, andrepresent the interaction matrix, the intrinsic growth rate vector, and the susceptibility matrix of the community, respectively. As the simplest population dynamics, the GLV model has been applied to microbial communities in lakes, soils, and human bodies.

---

### Procedure guideline for brain perfusion SPECT using (99m) Tc radiopharmaceuticals 3.0 [^113RtWch]. Journal of Nuclear Medicine Technology (2009). Medium credibility.

Brain perfusion SPECT processing — Filter all studies in 3 dimensions (x, y, and z), achieved either by 2-dimensionally prefiltering the projection data or by applying a 3-dimensional postprocessing filter to the reconstructed data. Low-pass (e.g. Butterworth) filters should generally be used, and resolution recovery or spatially varying filters should be used with caution, as they may produce artifacts.

---

### Mechanisms of leading edge protrusion in interstitial migration [^113h6xvh]. Nature Communications (2013). Medium credibility.

To determine the rate of F-actin assembly in 3D protrusions, we photobleached 7 μm × 2 μm rectangular regions located midway through the cell height, at the cell midline and spanning the whole leading edge length (red box, Fig. 3d, Supplementary Movie 5). We then monitored fluorescence recovery in a smaller region contained within the bleach region to minimize any potential contribution from F-actin network flows along the y axis (dashed white box, Fig. 3d). Because there was no retrograde flow relative to the substrate along the x axis and because there was no flow convergence at the midline, the initial rate of fluorescence increase θ A (x) at each location x reflected the instantaneous rate of incorporation of fluorescent G-actin monomers (Supplementary Methods). θ A (x) was maximal just behind the front membrane before decreasing with distance away from the leading edge (Fig. 3e, Supplementary Fig. S7C, Supplementary Methods). To interpret these data, we distinguished a protrusive regime and a turnover regime for G-actin incorporation. First, in regions of new protrusion, we expect G-actin incorporation to be high due to the high local density of free barbed-ends necessary to achieve forward protrusion of the membrane through polymerization together with the nucleation of new filaments. Second, in regions further away from the membrane, we expect free barbed-ends to exist in finite proportion relative to the local F-actin density, regulated by the processes that participate in the turnover of F-actin. On the basis of its localization within a region of new protrusion (Fig. 3d; greyed zone, Fig. 3e), we associated the maximum in θ A (x) occurring for 0.5 μm ≥ x ≥ −0.5 μm with the protrusive regime (Fig. 3e). Regions further away were situated within the bleached region (x ≤ −0.5 μm) and therefore we concluded that θ A (x) in these regions reflected the turnover regime (Fig. 3e). Within regions undergoing turnover, the G-actin monomer incorporation rate θ A (x) is the product of the local F-actin density and the G-actin monomer incorporation rate per unit mass of F-actin ω A (x) (or assembly rate constant, in s −1) that reflects, for example, the local density of free barbed-ends. To determine which of these two variables caused the spatial variations in θ A (x) within the bleached region (Fig. 3e), we determined ω A (x) by normalizing θ A (x) to the steady-state actin fluorescence intensity along the midline I SS (x) (Fig. 3f, Supplementary Methods). For regions in the turnover regime (x ≤ −0.5 μm), ω A (x) was constant signifying that the rate of G-actin monomer incorporation per unit mass of F-actin was constant, perhaps due to a constant density of free barbed-ends (5/5 cells examined, Fig. 3g, solid blue line with closed symbols), and therefore spatial variations in the monomer incorporation rate θ A (x) were wholly imputable to spatial variations in the steady-state F-actin density I SS (x).

---

### Geriatric emergency department guidelines [^116FHMGX]. Annals of Emergency Medicine (2014). Medium credibility.

Short Blessed Test scoring — Items with 'Errors (0–5)' are weighted x 4 for item 1, x 3 for items 2 and 3, and x 2 for items 4, 5, and 6, with a Sum Total calculated within (Range 0–28); interpretation thresholds are 0–4 Normal Cognition, 5–9 Questionable Impairment, and ≥ 10 Impairment consistent with dementia.

---

### Prostate cancer, version 3.2026, NCCN clinical practice guidelines in oncology [^114BrSSn]. Journal of the National Comprehensive Cancer Network (2025). High credibility.

NCCN Prostate Cancer — principles of radiation therapy lists external beam radiation therapy (EBRT) regimen examples with dose–fraction schedules in columns labeled Definitive RT, Post-Treatment RT, and Advanced Disease. Conventional fractionation examples include 1.8–2 Gy x 37–45 fx and 1.8–2 Gy x 30–39 fx; moderate hypofractionation options include 3 Gy x 20 fx (preferred)^a, 2.7 Gy x 26 fx, 2.5 Gy x 28 fx, and 2.63–2.75 Gy x 20 fx with 2.5 Gy x 25 fx; ultra hypofractionation/stereotactic body radiotherapy (SBRT) examples include 9.5 Gy x 4 fx, 7.25–8 Gy x 5 fx, 6 Gy x 6 fx, 6.1 Gy x 7 fx, 9–10 Gy x 3 fx, 12 Gy x 2 fx, and 16–24 Gy x 1 fx. EBRT boost techniques include an EBRT with simultaneous integrated boost (See footnote b) and an EBRT with sequential SBRT boost of Prostate: 1.8 Gy x 23–28 fx with Boost: 6 Gy x 3 fx or 9.5 Gy x 2 fx. Symbol definitions are provided (✓ Preferred; ✱ Acceptable based on clinical and medical need; Regimens shaded gray are not recommended), and the note states All recommendations are category 2A unless otherwise indicated.

---

### Single-shot self-supervised object detection in microscopy [^117NnBsr]. Nature Communications (2022). High credibility.

Methods

Theory of geometric self-distillation

This section shows theoretically that LodeSTAR locates objects correctly. First, we consider the case of an arbitrary, constant object with no noise. Then, we show the special case of a symmetrical object. Finally, we consider an imperfect estimator. We will describe the theory to find the position of the object, but the same arguments easily extend to other properties for which a symmetry is available (e.g. mass, size, orientation).

Let X be the set of possible images of an object, Y be the set of possible positions of the object in the image, f be the ground-truth function that returns the object position and h be a neural network, both mapping X → Y, and G be the Euclidean group consisting of translations, rotations and reflections, acting on both X and Y.

Given a single image x 0 ∈ X, we can define the subset, which are all the elements of X reachable by acting on x 0 with G. f is, by definition, equivariant to G on the subset.

Now, additionally assume that (Assumption 1), h is trained to be equivariant to G on the subset, i.e. h (g x) = g h (x) ∀ g ∈ G and. Next, we defined the error c as the error f (x 0) − h (x 0) for the input image x 0. Now, for any other, we know thatfor some. Let us factorinto, whereis the translational part of the transformation, andis the non-translational remainder. Finally, we define h as "consistent" if(i.e. the offset is independent of the translation and transforms correctly with). We can easily show this to hold by substitution:Note thatdisappears in Eq. (3) since it equals to adding and subtracting a constant vector. In Eq. (4), we use the fact thatis linear.

---

### Graphs [^1165hLQe]. Chest (2006). Low credibility.

Two rules of good graphs are presented and explicated.

---

### Inhibiting peptidylarginine deiminases (PAD1-4) by targeting a cadependent allosteric binding site [^117LKDA3]. Nature Communications (2025). High credibility.

Glutamate dehydrogenase (GDH) enzyme-coupled assay for PAD enzyme activity

The enzyme activity of each of the PAD isoforms was assayed by detection of the product, ammonia, via coupled enzyme detection. GDH catalyzes the conversion of ammonium, α-ketogluterate, NADH, and H + to NAD +, glutamate, and water, which is monitored spectrophotometrically at 340 nm. The PAD selectivity assay conditions were: 100 mM HESPES, pH 7.5, 50 mM NaCl, 0.25 mM CaCl 2, 1 mM substrate BAEE, 2 mM DTT, 0.01% Triton X-100, 40 μ/mL GDH (Sigma, G2626), 8.5 mM α-ketogluterate, and 0.2 mM NADH; the enzyme concentrations used were: 10 nM PAD1, 6 nM PAD2, 20 nM PAD3, or 4 nM PAD4. Serial dilutions of the compound were prepared in DMSO at 100x concentrations, then diluted into buffer to prepare a 4x stock solution immediately prior to assay. Reactions (40 μL) were prepared by combining 10 μL of [4x] BAEE, 10 μL of [4x] compound, and [2x] assay solution that contained all other reagents. Assays were performed in black/clear-bottom 384 well plates (Corning #3544), absorbance (340 nm) data were collected on an EnVision 2103 plate reader (PerkinElmer), and reactions were monitored at 140 s intervals for 50 plate repeats. An apparent molar absorptivity for NADH under the conditions of the assay was determined from a standard solution of NADH to be 4.13 mM -1. IC 50 values were determined via nonlinear regression by fitting the rate data to the equation: f(x) = a/(1 + (x/b)), where f(x) is the measured enzyme rate at inhibitor concentration, x; a is the maximum observed velocity at x = 0 (i.e. in the absence of inhibitor); and b is the IC 50. Enzyme inhibition kinetic data are graphically reported as % inhibition: 100*(1-(f(x)/a)).

---

### Secure and scalable gene expression quantification with pQuant [^116Xacsj]. Nature Communications (2025). High credibility.

Encoding indexed reference transcriptome with polynomials

Since in BFV the message space is polynomial, we need to encode the T gene in polynomials. We turn the g (m), which stores the k -mers of the genes into polynomialsfor 1 ≤ i ≤ N g and 0 ≤ m < N c. N c = ⌈∣ K ∣/ n ⌉ is the number of required ciphertexts. Recall that n − 1 is the degree of the polynomial from the ring dimension described above. Each polynomialis defined asHere, if m ≥ ∣ K ∣. This can be done in plaintext in the server or locally as reference transcriptome is publicly available information. This is a one time calculation whose results can be re-used by multiple clients.

Encoding the reads with polynomials and encrypting them

We first encode the read vector r, which has a length of ∣ K ∣, into N c number of polynomials r m (X), where each polynomial r m (X) is defined asHere, we define r m = 0 if m ≥ ∣ K ∣ for simplicity. The client then generates a set of ciphertexts ctxt m = Enc(r m (X)) for each m using their key. Please see Supplementary Methods 3.6 and Supplementary Algorithm 2 for details.

Inner product between a gene vector and reads vector

By definition, the sum of all multiplications of corresponding polynomialscontains the desired value 〈 g (i), r 〉 as its constant term modulo X n + 1. Hence, after homomorphically computing ctxtctxt k for each i, the client can download the encrypted results and obtain the desired output by decrypting ctxt (i) for each i. Please see Supplementary Methods 3.9 for details of inner product in HE and see Supplementary Methods 3.7 and Supplementary Algorithm 3 for details of the computations on the server.

---

### Hippocampal place cell sequences differ during correct and error trials in a spatial memory task [^114u8caB]. Nature Communications (2021). High credibility.

Fig. 2
Predictive firing in place cell ensembles developed with learning.

A Bayesian decoder (see Methods) was applied to ensemble spiking activity to estimate posterior probability distributions across sample-test trials. a–c The sum of posterior probability (P (x | n)) extending ahead of rats to the stop location (i.e. the sum of the maximum 5 posterior probabilities from 2 bins ahead of the rat's current location to the stop location) as rats ran toward the stop location is shown for each trial (Multiple linear regression: n = 224 trials, no significant interaction effect between trial type and trial number: t(436) = −0.5, p = 0.6; no significant effect of trial type: t (436) = 0.5, p = 0.6; significant effect of trial number: t (436) = 2.1, p = 0.04). Regression lines are shown for sample and test trials separately (correlation coefficient: r = 0.26, p = 6.4 × 10 −5 for sample and r = 0.19, p = 0.005 for test). Each dot indicates the sum for a lap of the indicated trial type. The sum is shown for all trials (a), correct trials (b), and error trials (c). d – f The maximum posterior probability (P(x | n)) estimates behind rats' current positions (i.e. the maximum 5 posterior probabilities from the beginning of the track to a rat's current position) was summed for each lap. In contrast to the results reported in (a – c), the sum of posterior probability behind a rat's current position did not increase across trials for all trials (d; multiple linear regression model, n = 224 trials, F(3,439) = 2.2, p = 0.08; no significant trial type by trial number interaction effect: t(436) = −0.2, p = 0.8; no significant effect of trial type: t(436) = −0.8, p = 0.4; no significant effect of trial number: t(436) = −0.3, p = 0.8), correct trials (e), and error trials (f). g – i The sum of the posterior probability (P(x | n)) estimates from the beginning of the trajectory to the stop location is also shown for each lap for all trials (g; repeated measures ANOVA: n = 224 trials, no significant main effect of trial number: F(7,378) = 1.6, p = 0.2), correct trials (h) and errors trials (i).

---

### Correction to2021; 2: e352-61 [^112skeU3]. The Lancet: Healthy Longevity (2021). High credibility.

[This corrects the article DOI: 10.1016/S2666-7568(21)00088-X.].

---

### LinkedSV for detection of mosaic structural variants from linked-read exome and genome sequencing data [^117EwtZ4]. Nature Communications (2019). High credibility.

If we regard (R (F i 1), L (F i 2)) as a point in a two-dimensional plane, according to Eq. (2), for almost all (98.01%) of the fragment pairs (F i 1, F i 2), ((R (F i 1), L (F i 2)) is restricted in a G × G square region with the point (b 1, b 2) being a vertex (Supplementary Fig. 14b).

We used a graph-based method to fast group the points into clusters and find square regions where the numbers of points were more than expected. First, every possible pair of endpoints (R (F 1), L (F 2)) meeting B (F 1) = B (F 2) formed a point in the two-dimensional plane. Each point indicated a pair of fragments that share the same barcode. For example, if ten fragments share the same barcode, pairs of endpoints will be generated. A point/pair of endpoints may or may not support an SV because there are two possible reasons for observing two fragments sharing the same barcode: (1) the two fragments originated from two different HMW DNA molecules but were dispersed into the same droplet partition and received the same barcode and (2) the two fragments originated from the same HMW DNA molecule but the reads were reconstructed into two fragments due to an SV. The points are sparsely distributed in the two-dimensional plane and it is highly unlikely to observe multiple points in a specific region. Next, a k – d tree (k = 2) was constructed, of which each node stores the (X, Y) coordinates of one point. A k – d tree is a binary tree that enable fast query of nearby nodes. Therefore, we could quickly find all pairs of points within a certain distance. Any two points (x 1, y 1) and (x 2, y 2) were grouped into one cluster if | x 1 − x 2 | < G and | y 1 − y 2 | < G. For each cluster, if the number of points in the cluster was more than a user-defined threshold (default: 5), it was considered as a potential region of enriched fragment endpoints. If the points in the cluster were not within a G × G square region, we used a G × G moving square to find a square region where the points are best enriched. Theoretically, the best enriched square region should contain 98.01% (0.99 × 0.99) of the points, according to Eq. (2). The predicted breakpoints were the X and Y coordinates of the right-bottom vertex of the square. The points in the square region were subjected to a statistical test describe below.

---

### What quantile regression does and doesn' T Do: a commentary on petscher and logan (2014) [^111sVEb9]. Child Development (2019). Medium credibility.

Petscher and Logan's (2014) description of quantile regression (QR) might mislead readers to believe it would estimate the relation between an outcome, y, and one or more predictors, x, at different quantiles of the unconditional distribution of y. However, QR models the conditional quantile function of y given x just as linear regression models the conditional mean function. This article's contribution is twofold: First, it discusses potential consequences of methodological misconceptions and formulations of Petscher and Logan's (2014) presentation by contrasting features of QR and linear regression. Second, it reinforces the importance of correct understanding of QR in empirical research by illustrating similarities and differences in various QR estimators and linear regression using simulated data.

---

### Deterministic and electrically tunable bright single-photon source [^11744Kwn]. Nature Communications (2014). Medium credibility.

Single-photon emission and Purcell effect

We now study the emission dynamics and photon statistics of the source. Figure 4a presents the autocorrelation function measured on the X line tuned into resonance with the cavity mode (U = −0.45 V) for different excitation powers. Figure 4b presents the autocorrelation function at zero delay g²(0) extracted from the measurements and corrected from the setup temporal resolution, as explained in the Methods section. A good single-photon purity with g²(0) < 0.2 is obtained up to saturation. From the experimental curves presented in Fig. 4a, we can also extract the deconvoluted rise time τ R of the autocorrelation function as a function of the excitation power. This rise time τ R is related to the X radiative lifetime τ X through+ g where g is the pumping rate. Figure 4d presentsas a function of power, showing the expected linear dependence. For g = 0 we find τ X = 0.82 ± 0.03 ns. Performing similar measurements for several reference QDs in the planar cavity for the same applied voltage, we find an average X lifetime of = 1.45 ± 0.1 ns. From this measurement, we deduce a Purcell factor of F P = −1 = 0.8 ± 0.08. This measured value is close to the expected Purcell factor = 1.1 calculated considering that Q = 1300 for the mode and that this connected pillar presents the same effective volume as an isolated pillar with diameter 4.1 μm, as estimated considering the fundamental mode energy.

---

### Non-linear relationships in clinical research [^117NccGZ]. Nephrology, Dialysis, Transplantation (2025). Medium credibility.

CONCLUSION

"Essentially all models are wrong, but some are useful". George Box reminds us that although statistical models are unable to perfectly capture the complexities of the real world, they still offer a convenient approximation, which can be useful to clinical practice. Despite the characteristic of all statistical models inherently being "wrong", in epidemiological research, we strive to produce models that are as truthful to reality as possible. As nonlinear relationships arguably occur more often than linear ones, appropriate methods are required to ensure that the modeled relationship follows the data at hand, and that the shape of the relationship is accurately represented by the model. Here we provide the strengths and limitations of the various tools capable of modeling non-linear relationships, including transformations, polynomials, splines and GAMs, whilst demonstrating how the reporting of results using these techniques can be surprisingly straightforward, despite their perceived complexity.

---

### Statistical practices: the seven deadly sins [^115kXAUM]. Child Neuropsychology (2003). Low credibility.

This paper discusses selected problems in applied statistical analysis: (a) over-reliance on null hypothesis statistical testing, (b) failing to perform a power analysis prior to conducting the study, (c) using asymptotic statistical approximations with small samples, (d) ignoring missing data, (e) failing to deal with the multiplicity problem when performing multiple statistical comparisons, (f) using stepwise procedures to select variables in regression analysis, and (g) failing to perform or report model diagnostics. Suggestions and guidelines to address these issues in manuscripts concerning child neuropsychological research are offered.

---

### Correction to2021; 2: e436-43 [^115J74kD]. The Lancet: Healthy Longevity (2021). High credibility.

[This corrects the article DOI: 10.1016/S2666-7568(21)00115-X.].

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^114ToLK7]. Kidney International (2024). High credibility.

CKD mineral–bone parameters by albuminuria category — figure context: The y axis represents the meta-analyzed absolute difference from the mean adjusted value at an eGFR of 80 ml/ min per 1.73 m² and albumin excretion < 30 mg/g (< 3 mg/mmol), with albuminuria categories defined as A1, albuminuria < 30 mg/g (< 3 mg/mmol); A2, albuminuria 30–300 mg/g (3–30 mg/mmol); and A3, > 300 mg/g (> 30 mg/mmol).

---

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^113N5852]. American Journal of Kidney Diseases (2006). Medium credibility.

Hemodialysis adequacy — minimum single‑pool Kt/V (spKt/V) targets per treatment to achieve a weekly standard Kt/V (stdKt/V) of approximately 2.0 are stratified by residual urea clearance (K_r): for 2x/wk dialysis with K_r < 2 mL/min/1.73 m2 it is Not recommended, whereas with K_r ≥ 2 mL/min/1.73 m2 it is 2.0*; for 3x/wk the minima are 1.2 (K_r < 2) and 0.9 (K_r ≥ 2); for 4x/wk 0.8 and 0.6; and for 6x/wk (short daily) 0.5 and 0.4. Table 13 states these sessional targets correspond to "a weekly stdKt/V value of 2.0" in patients "undergoing 2 to 6 treatments per week" with adjustment for "a weekly K_r of 2 mL/min"; a "urea clearance of 2 mL/min is approximately 20 L/wk" and with "V = 30 L, it represents about a 0.67 weekly Kt/V unit". It is noted that "the minimum values for spKt/V" do not account for outcome improvements when frequency "is increase to more than 3/wk". The Work Group "recommended targeting an spKt/V value that is about 15% higher than the recommended minimum targets", developed a scheme that "limited the downward adjustment in spKt/V for K_r to 2 mL/min", and stated that "Maintaining a minimum 'total Kt/V' value of 1.2… would allow reduction of the dialysis dose down to near zero".

---

### Recovery after stroke: not so proportional after all? [^116Q32Lh]. Brain (2019). Medium credibility.

Spurious r(X,Δ) are likely when σ Y /σ X is small

For any X and Y, it can be shown that:

A formal proof of Equation 1 is provided in the Supplementary material, Appendix A [proposition 4 and theorem 1; also see]; its consequence is that r(X,Δ) is a function of r(X, Y) and σ Y /σ X. To illustrate that function, we performed a series of simulations (Supplementary material, Appendix B) in which r(X, Y) and σ Y /σ X were varied independently. Figure 2 illustrates the results: a surface relating r(X,Δ) to r(X, Y) and σ Y /σ X. Figure 3 shows example recovery data at six points of interest on that surface.

Figure 2
The relationship between r(X, Y), r(X,Δ) and σ Y /σ X. Note that the x -axis is log-transformed to ensure symmetry around 1; when X and Y are equally variable, log(σ Y /σ X) = 0. Supplementary material, proposition 7 in Appendix A, provides a justification for unambiguously using a ratio of standard deviations in this figure, rather than σ Y and σ X as separate axes. The two major regimes of Equation 1 are also marked in red. In Regime 1, Y is more variable than X, so contributes more variance to Δ, and r(X,Δ) ≈ r(X, Y). In Regime 2, X is more variable than Y, so X contributes more variance to Δ, and r(X,Δ) ≈ r(X,−X) (i.e. −1). The transition between the two regimes, when the variability ratio is not dramatically skewed either way, also allows for spurious r(X,Δ). For the purposes of illustration, the figure also highlights six points of interest on the surface, marked A–F; examples of simulated recovery data corresponding to these points are provided in Fig. 3.

---

### Assessment and correction of macroscopic field variations in 2D spoiled gradient-echo sequences [^115iJZt1]. Magnetic Resonance in Medicine (2020). Medium credibility.

3.2 Phantom experiments

Thevalues estimated with the mono‐exponential model S 1 are plotted as a function of G z for α = 30° and α = 90° with positive and negative G slice polarity in Figure 3. The value ofincreases proportional to G z for α = 30° (Figure 3 A) with up to 8‐times highervalues for G z = 150 µT/m than for G z = 0 µT/m. For α = 30°, negligibly small differences between the polarity of G slice and the sign of G z were found, whereas for α = 90° (Figure 3 B), positive and negative G z yield differentvalues and a dependency on the polarity of G slice. Moreover, Figure 3 shows the normalized averaged signal decay for | G z | = 100 µT/m plotted with positive and negative G z, explaining the difference in estimatedvalues. For α = 90° with positive G slice and G z > 0 (blue line), the signal decays faster than for G z < 0 (red) and vice versa when switching G slice polarity.

---

### Clinical practice guideline for management of tinnitus: recommendations from the US VA / DOD clinical practice guideline work group [^111GiuXC]. JAMA Otolaryngology — Head & Neck Surgery (2025). High credibility.

Embase search strategy for KQ 2 specifies filter construction and combinations as follows: exclusions are combined as "#4 OR #5 OR #6"; inclusions limit records to "English language [english]/lim", "Publication year [2013–2023]/py", and "Entry date [(01-01-1900)/sd NOT [07-04-2023]/sd" combined as "#8 AND #9 AND #10"; study designs target "Systematic reviews and meta-analyses" and "Randomized controlled trials" combined as "#12 OR #13"; and the final application is "(#3 NOT #7) AND #11 AND #14". An example exclusion string uses "[animals]/lim NOT [humans]/lim".

---

### Predicted residual error sum of squares of mixed models: an application for genomic prediction [^116m1g4v]. G3 (2017). Low credibility.

The HAT method is a fast algorithm for the ordinary CV for a linear model because the regression analysis is only done once on the whole sample and then the estimated residual errors are modified afterward. Extension of the HAT method to mixed model has been made byin finding the optimal ridge factor in ridge regression. It is well known that ridge regression can be formulated as a mixed model problem with the variance ratio replaced by a given ridge factor.proposed a generalized cross-validation (GCV) method to find the optimal ridge factor so that the generalized residual error variance is minimized. These authors showed that the GCV-calculated residual error sum of squares is a rotation-invariant version of Allen's PRESS. The residual error variance obtained from the GCV method is equivalent to calculating the residual error variance by dividing the ERESS by an "effective" degree of freedom. Properties of the GCV method have been extensively studied by.applied GCV to wavelet thresholding. When performing genomic prediction, we prefer to see the actual predicted residual errors (errors in prediction of future individuals) obtained from the ordinary CV because the residual errors obtained via GCV may not be intuitive to most of us. The important gain from the GCV method of ridge regression analysis to genomic selection is the HAT matrix of the random model when the genomic variance is given.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^115o8v1f]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Δ Φ, the probability distribution at a point Φ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Φ ∈ {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters α and β, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since α and β are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Φ ∈ [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(α, β) and x 2 = 1 − x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Φ, it may seem natural to select α and β by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### Corrections [^116ozkd9]. The Lancet: Respiratory Medicine (2016). Medium credibility.

[This corrects the article DOI: 10.1016/S2213-2600(15)00283–0.].

---

### Input-output maps are strongly biased towards simple outputs [^11354s9E]. Nature Communications (2018). Medium credibility.

Many systems in nature can be described using discrete input-output maps. Without knowing details about a map, there may seem to be no a priori reason to expect that a randomly chosen input would be more likely to generate one output over another. Here, by extending fundamental results from algorithmic information theory, we show instead that for many real-world maps, the a priori probability P(x) that randomly sampled inputs generate a particular output x decays exponentially with the approximate Kolmogorov complexity [Formula: see text] of that output. These input-output maps are biased towards simplicity. We derive an upper bound P(x)≲[Formula: see text], which is tight for most inputs. The constants a and b, as well as many properties of P(x), can be predicted with minimal knowledge of the map. We explore this strong bias towards simple outputs in systems ranging from the folding of RNA secondary structures to systems of coupled ordinary differential equations to a stochastic financial trading model.

---

### Statistics review 7: correlation and regression [^112xk9rk]. Critical Care (2003). Low credibility.

The present review introduces methods of analyzing the relationship between two quantitative variables. The calculation and interpretation of the sample product moment correlation coefficient and the linear regression equation are discussed and illustrated. Common misuses of the techniques are considered. Tests and confidence intervals for the population parameters are described, and failures of the underlying assumptions are highlighted.

---

### Non-linear relationships in clinical research [^115S4XT5]. Nephrology, Dialysis, Transplantation (2025). Medium credibility.

True linear relationships are rare in clinical data. Despite this, linearity is often assumed during analyses, leading to potentially biased estimates and inaccurate conclusions. In this introductory paper, we aim to first describe-in a non-mathematical manner-how to identify non-linear relationships. Various methods are then discussed that can be applied to deal with non-linearity, including transformations, polynomials, splines and generalized additive models, along with their strengths and weaknesses. Finally, we illustrate the use of these methods with a practical example from nephrology, providing guidance on how to report the results from non-linear relationships.

---

### Role of X chromosome and dosage-compensation mechanisms in complex trait genetics [^113f98K1]. American Journal of Human Genetics (2025). Medium credibility.

Data and code availability

Male and female GWAS summary statistics are available at:and, respectively. Sex-combined GWAS summary statistics on height are available at. The code used in this study is available at.

---

### Correction to2021; 3: e744-47 [^117A4iNL]. The Lancet: Rheumatology (2022). High credibility.

[This corrects the article DOI: 10.1016/S2665-9913(21)00248–4.].

---

### The biomechanical origin of extreme wing allometry in hummingbirds [^114UfeTw]. Nature Communications (2017). Medium credibility.

The allometric version of the aerodynamic force equation (Eq. (1)) can thus be obtained by equating each term with body weight (omitting the constant of log 1/2). The slopes b are subscripted with the relevant term from the force equation (Eq. (1)), and for simplicity we drop the prime notation.

Allometric exponents are determined individually, allowing us to take advantage of partly overlapping data sets which may include observations of only some variables. In principle, separation of the problem into components could allow different statistical methods to be applied to each exponent, if warranted.

We can infer the statistical validity of the exponents as a group based on whether they correctly predict the relationship of force and body weight, b F (Supplementary Fig. 1). When the exponents do not sum to b F, some or all of them are likely biased. We cannot provide a hard 'rule' for violation of this constraint, but the magnitude of the difference can help place a minimum bound on the difference from a prediction (e.g. isometry) that can reasonably be considered an allometry. For instance, consider a scenario in which we find that the allometric exponent of wing area versus body mass is 0.57, and that the confidence (or credible) intervals exclude isometry (exponent 0.67). If, however, we also find that the sum of the exponents across the full model of force allometry (Σ b) equals 0.90, then at least one exponent, possibly wing area, is underestimated by a margin that could explain the discrepancy from isometry.

---

### Smooth 2D manifold extraction from 3D image stack [^117PJfMg]. Nature Communications (2017). Medium credibility.

where FFT denotes the fast Fourier transform. A three classes k-means is then performed on those spectral density profiles to roughly identify the three classes: the foreground z-profiles (class with the highest amount of lowest frequency components), the background z-profiles (class with the lowest amount of lowest frequency components) and the uncertain z-profiles (class with intermediate frequency components). This segmentation does not need to be precise as it is not definitive as such but helps driving the final index map towards the foreground. To offer a class-specific control on the relative weight between the regularization term and the data attachment term, a weight C (x, y) is assigned to each z-profile the following way:

The weight 0 is affected to the background class such that the z level for all positions of that class will be determined only by the local curvature and by extension by the local foreground. In the Supplementary Methods and in Supplementary Fig. 4, we show how the weights c F and c U can be set automatically. Intuitively, if the local curvature of the foreground is large in average and the noise level is low, then c F should remain high to preserve the foreground curvature. On the other hand, if the foreground lies on a flattish manifold and the noise level is high, this term should be lower to ensure convergence to a smooth z index map. Hence, an optimal value c F exists and depends both on the curvature of the foreground and the noise level. In the cost function (equation 1), decreasing the first term by a given Δ z for any location (x, y) translates to an increase of Δ σ and conversely. To scale those two terms on the whole image, we seek for a c = Δ σ /Δ z that would smooth the foreground yet preserving its highest local curvature (the distribution of such c on all the voxels of the foreground of an example image is showed in Supplementary Fig. 4b). Taking the maximum value of c would prevent any smoothing of the foreground and would be equivalent to consider that there is no noise. On the other hand, taking its average would ensure that the average curvature is maintained, however some regions of high curvature would be overly smoothed, causing partial loss of details. To choose the best compromise, we make the assumption that the noise follows a Bernoulli process, where a pixel of Z max (x, y) is either the right z -level value on the foreground, or a wrong random value uniformly distributed over [0, D] due to the fact a random voxel can be selected as the maximum intensity instead of the correct foreground level (with a higher probability when the signal-to-noise ratio is weak). Fortunately, the probability for the latter to happen can be obtained directly from the misclassification of the maximum intensity values in the foreground class as shown in Supplementary Fig. 4a. Following this idea, c F is chosen to be the value of the c distribution that match the probability of false positive of the maximum intensity distribution on the foreground profile. This is described in Supplementary Fig. 4c. Once c F has been computed, the rational is to choose c U between 0 and c F. Intuitively, if the signal to noise is rather low, then it means that a larger fraction of uncertain profiles in fact belong to the background and should be ignored, c U should then be closer to 0. On the opposite, if the signal to noise is rather high, c U should be close to c F. Thus, the correct ratio to apply can be obtained from the relative position of the means of the Z max distributions of foreground, background and uncertain classes as showed in Supplementary Fig. 4c.

---

### Rapid one-stepF-radiolabeling of biomolecules in aqueous media by organophosphine fluoride acceptors [^1161XGJC]. Nature Communications (2019). High credibility.

Fig. 2
Mechanism of H 2 O-resistant 18 F/ 19 F exchange interpreted by free energy parameters. Geometries are optimized at the B3LYP/6–31+G* level of theory, and single point calculations are performed at the CAM-B3LYP/6–311++G level of theory. Two substituent groups on the phosphorus center are simplified as two large spheres in the molecular structures for clarity. a Reaction pathways and free energy (kcal mol −1) profiles for the F/F isotope exchange process of five selected reactant systems. b Reaction pathways and free energy (kcal mol −1) profiles for the OH/F exchange process of the same five selected reactant systems

The calculated free energy profiles show that the pentacoordinate intermediates are unstable compared to the initial materials for both F/F isotope exchange and OH/F substitution. A comparison of the relative stabilities of TS1-X and TS2-X (X = 1–5) to those of TSO1-X and TSO2-X shows that the free energy barriers for the F/F exchange processes were distinctly lower than the corresponding values for the OH/F substitution. The differences in relative free energy values between TS1-X and TSO1-X for the 1–5 reaction systems were −3.6, −4.1, −5.1, −7.2, and −4.7 kcal mol −1, respectively, while those between TS2-X and TSO2-X for the five reaction systems were −5.9, −7.8, −7.1, −8.1, and −4.4 kcal mol −1, respectively. These results strongly indicate that the F/F isotope exchange is kinetically more favorable than the corresponding OH/F substitution for all of the reactant systems. According to the calculated relative free energies of the transition states, the predicted rates for F/F isotope exchange follow the order of 1 ≈ 2 > 5 > 4 > 3. In addition, the OH/F substitution processes exhibit endothermic free energy values (e.g. ∆G = 2.3–5.1 kcal mol −1), suggesting that these processes are neither spontaneous nor thermodynamically favorable. Overall, the DFT calculations demonstrate that the 18 F/ 19 F isotope exchange occurs automatically rather than the competing OH/F substitution in terms of both kinetics and thermodynamics (Table 1).

---

### Ab initio nonrigid X-ray nanotomography [^112yfVUh]. Nature Communications (2019). High credibility.

An advantage of the virtual geometry approach is that the optimization task in Eq. (3a) is formally identical to the classical straight-LoS tomography and thus any common algebraic method such as SIRT, SART, or CLGS can be used as long as changes in the sample density resulting from the deformation process can be neglected. This also means our approach can be easily combined with common regularization methods, such as total variation.

Because the curved geometry can generally differ for each projection, the NCT-based methods enable reconstruction of samples evolving faster than the acquisition time of a single sub-tomogram if the corresponding DVF evolution is available.

Estimation of deformation vector fields

Since the improved estimate of DVF Γ(x, t) affects both the reconstruction g (F) and also the partial reconstructions g (i) of each sub-tomogram, reconstruction of the volume g (F) and DVF Γ(x, t) is generally nonlinear and nonconvex. This means that the estimation of the optimal DVF leads to iterative joint optimization problem when the DVF and the volume are simultaneously reconstructed in order to satisfy all provided constraints.

The continuous time-evolving DVF Γ(x, t) is calculated from the discretized deformation vector fields Γ (i) (x), which describe an average deformation in the i th sub-tomogram with respect to the reference state. The discretized DVF Γ (i) (x) is estimated so that differences between the full reconstruction g (F) and the reconstructed sub-tomograms g (i) are minimized. Both g (i) and g (F) are already reconstructed using the previous estimate of the time-evolving DVF, Γ(x, t).

---

### Colloidal interactions and unusual crystallization versus de-mixing of elastic multipoles formed by gold mesoflowers [^115dyk7w]. Nature Communications (2020). High credibility.

Fig. 5
Orientation, effective repulsion from a wall and pairwise interactions of monopoles.

a, b Director structures around a mesoflower with a dominant elastic monopole. n (r) is shown by black rods and defect lines are depicted as red tubes. As a comparison, n (r) induced by a pure idealized monopole is shown as the top-right inset of a whereas the bottom-right insets in a, b show color-coded diagrams of n x on the interpolation sphere surrounding the mesoflower when viewed from different directions. The coordinate system is set so that n 0 || z. c, The xy perspective view of the n x and the color-coded scale of n x. d Elastic (red circles) and gravity (blue line) torques about the center of the particle core (Supplementary Figs. 5 and 6) versus θ. The gravity acts in the negative y -direction; g is the gravitational acceleration. The mesoflower is placed above a wall with fixed planar boundary conditions along z and distance from the wall h = 3.5 R 0. Balance of the two torques occurs at an equilibrium orientation θ eq, which is plotted in f versus Δ ρR 0 3 g. e, Reduced Landau-de-Gennes free energy F LdG versus h at θ = −40°. Black solid circles correspond to K 11 = K 33 = 2 K 22 = 7.8 pN and red solid squares to the one-constant approximation; we use T = 298 K and R 0 = 0.2 μm in the minimization. The inset in e shows the probability distributionof the particle-wall separation h for several values of R 0 indicated next to the curves. For the Landau-de Gennes free energy we use(parameters extracted from the black fitting curve in e), and E g is the particle gravitational energy ∝ h; average elastic constant, and T = 298 K. g F LdG versus separation distance r c between a pair of like (black solid circles) and opposite (blue solid triangles) monopoles at h = 20 R 0 θ 1 = θ 2 = 40° for like monopoles and θ 1 = 40°, θ 2 = −40° for opposite monopoles. Lines in e, g are fitting curves with the function f (x) ∝ 1/ x. Director structures around pairs of like-charged h, i and oppositely-charged j, k monopole particles at r c = 5 R 0 in h, j and r c = 10 R 0 in i, k.

---

### Skewed X-inactivation is common in the general female population [^112YdW7Y]. European Journal of Human Genetics (2019). Medium credibility.

Analysis of individual genes

To determine whether a gene escapes X-inactivation, we selected skewed individuals with a median paternal ratio across the entire X-chromosome (see previous paragraph) of ≤ 0.35 or ≥ 0.65. We used the following procedure for the analysis of skewing status of individual genes:

Let Mx = median (paternal ratio[i, k]), where i = 1, …, m, with m being the number of SNVs (covered by ≥ 10 reads) on the X-chromosome in a sample and Mg = median (paternal ratio[j, k]), where j = 1, …, n, with n being the number of SNVs in a sample that are mapped to the gene g and where k = 1, …, p, with p being the number of samples with SNVs in the gene g. Note that m and n differ per sample and that we analyze only the genes with p ≥ 5. Further, let Sx = | Mx − 0.5| and Sg = | Mg − 0.5| be the skew factors (the distance from 0.5, where 0.5 reflects balanced expression of the paternal and maternal alleles), then for each gene we have two possible situations:
X-chromosome and gene g agree distance direction(Mg > 0.5 and Mx > 0.5)|(Mg < 0.5 and Mx < 0.5) = > We perform the paired t test: t.test(Sx [k], Sg [k], alternative = "less").
X-chromosome and gene g disagree on distance direction (Mg < 0.5 and Mx > 0.5)|(Mg > 0.5 and Mx < 0.5) = > We perform the paired t test: t.test(Sx [k], − Sg [k], alternative = "less").

The null hypothesis in the test is that the median paternal ratio of the gene is not different from the overall median paternal ratio for that individual, consistent with absence of escapee behavior. The alternative hypothesis is that the median paternal ratio of the gene is closer to 0.5 than the overall median paternal ratio for that individual, consistent with escapee behavior.

---

### Summary benchmarks-full set – 2024 [^1169zFGq]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — GRADE evidence quality ratings are defined for forming recommendations for care as follows: "Good quality (GQ): Further research is very unlikely to change our confidence in the estimate of effect", "Moderate quality (MQ): Further research is likely to have an important impact on our confidence in the estimate of effect and may change the estimate", and "Insufficient quality (IQ): Further research is very likely to have an important impact on our confidence in the estimate of effect and is likely to change the estimate; any estimate of effect is very uncertain".

---

### Lost in translation: on the problem of data coding in penalized whole genome regression with interactions [^115tu6yE]. G3 (2019). Medium credibility.

Theoretical Results

The observations made in Example 1 are explained by the following proposition which has several interesting implications. More formal proofs of the statements made can be found in the Appendix.

Proposition 1. Let be the vector of marker values of individual i and let be a polynomial of total degree D in the marker data. Moreover, let be a translation of the marker coding and let us define a polynomial in the translated variables by. Then for any data, the sum of squared residuals (SSR) of both polynomials will be identical (each with the respective coding): Moreover, for any monomial m of highest total degree D, the corresponding coefficient of and of will be identical :The coefficients of highest total degreerepresent the additive effects in an addditive effect model, the interaction effects in a model including pair-wise interactions, the three way interactions in a polynomial model of total degree 3, and so on.

The content of Proposition 1 can be summarized the following way: Let us assume that we have dataand a polynomial f which is based on marker data. Moreover, we have the translated data, that is an alternative coding of the predictors. We define the alternative polynomialby the value of f at the corresponding point in the original coding:The left hand equation means that we define the alternative polynomialto beat. Then –by definition– the SSR of the fits are identical when each polynomial is used with its respective data coding. Moreover, both polynomials give –by definition– the same predictionsto each data point (in its respective coding). Since f is given, we have its coefficients (effects) and can thus use the first equality of Equation (10) to calculate the coefficients of. The coefficients of monomials of highest total degree are the same for f and. The latter statement needs a little more detailed consideration and we refer to the Appendix. We give an example.

---

### Guidelines and recommendations for performance of the fetal echocardiogram: an update from the American Society of Echocardiography [^115XEnaW]. Journal of the American Society of Echocardiography (2023). High credibility.

Published Z score equations for fetal cardiac measurements — spanning gestational age (GA) ranges and sample sizes — include linear, polynomial, and quantile regression models with independent variables such as femur length, biparietal diameter, menstrual age, or GA; examples include Schneider et al. 2005 (GA 15–39, 130) using a Z score with linear regression and "17 measures studied", Lee et al. 2010 (GA 20–40, 2,735) using a Z score with linear regression and "SD formula published, large sample size", Gu et al. 2018 (GA 17–39, 6,343) using a "q score" with "Quantile regression" and "Better account for nonlinear changes over time, large sample", and Vigneswaran et al. 2018 (GA 13–36, 7,945) using a Z score with "Polynomial regression" and "Large sample size".

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^116UmK71]. Annals of the American Thoracic Society (2023). High credibility.

Quantitative imaging (QI) — challenges and sources of error emphasize that advances in QI are crucially dependent on technology and computational power, with massive data sets run through "black box" algorithms and distilled into selected "biomarkers" to evaluate their ability to detect, stratify, and monitor lung disease. Numerous imaging modalities, manufacturers, platforms, and algorithms may not integrate smoothly, and emergent QI metrics vary among centers in the methods of acquisition, quantification, interpretation, and extrapolation; technological advances contribute to difficulty in standardization. Systematic errors may arise at each step of image acquisition, processing, and analysis, including 1) technical issues (e.g., depth of inspiration, acquisition protocol, signal-to-noise ratio, resolution), 2) nonrepresentative sampling of parts of interest (e.g., comparing regions, lobes, or airways), and 3) lack of a well-defined reference space, as well as 4) inaccurate landmark coregistration for evaluating paired images obtained at different lung volumes and 5) failure to consider basic physiology and/or biological or spatiotemporal heterogeneity.

---

### Local and long-distance organization of prefrontal cortex circuits in the marmoset brain [^113WrD2K]. Neuron (2023). Medium credibility.

Prediction of corticostriatal patch distribution patterns by polynomial regression model

To find regression models that can predict the projection coordinates from the injection coordinates, we tested the polynomial regression model with degree 2 and searched for the optimal fit. As predictor variables, we used the x, y, and z coordinates of injections in the STPT template space. As response variables, we used the x, y, or z coordinates for the average positions of the detected STPT-vox intervals that roughly correspond to AP = +13.5, +14.5, +15.5, +16.5, and +17.5 in the Paxinos atlas and visualized the corresponding lines in the caudate.

QUANTIFICATION AND STATISTICAL ANALYSIS

Statistical tests

We generally used 8, 6, 5, 2, 4, and 4 samples for dlPFCv, dlPFCd, dmPFC, FP. ACC, and OFC, respectively, for subregion comparisons. The injections that were positioned near the borders of these subregions were excluded in such cases. Values are reported as mean ± standard deviation (SD) throughout the manuscript. The correlation coefficients (r) in Figures 5G, 5H, S8B, and S11 refer to the Pearson correlation. Those in Figure 6H refer to Spearman's rank correlation. R 2 for the regression model refers to the coefficients of determination in Figures S4D, S4E, and S8C. The p-values were calculated using one-way ANOVA and Tukey's post hoc test for significant factors in the ANOVA for Figures 2E, 3C, 6D, and 6G.

---

### Using both qualitative and quantitative data in parameter identification for systems biology models [^117TFy8y]. Nature Communications (2018). Medium credibility.

Results

An illustration of the potential value of qualitative data

To demonstrate the potential value of qualitative data, we consider a simple case of solving for the coefficients of polynomial functions.

We consider two polynomial functions: y 1 = ax 2 − bx + c and y 2 = dx + e. Suppose we want to solve for the coefficients a, b, c, d, and e, which we will take to be positive. As the ground truth coefficients to be determined, we choose (a, b, c, d, e) = (0.5, 3, 5, 1, 1.5).

Suppose that a limited amount of quantitative information is available. Namely, it is known that the parabola y 1 contains the points (2, 1) and (8, 13), and the line y 2 contains the point (3.5,5). This is not enough information to solve for any of the coefficients because three points are required to specify a parabola, and two points are required to specify a line (Fig. 1a).

Fig. 1
A simple illustration using polynomial functions. We use qualitative and quantitative information to determine the unknown coefficients. a Visualization of the problem. We seek to find the coefficients of equations for a parabola and a line, with the ground truth shown (blue solid curves). Two points on the parabola and one point on the line are known (black dots). These three points are consistent with infinitely many possible solutions (e.g. orange dashed curves). Qualitative information (colored circles, x -axis) specifies whether the parabola is above (+) or below (−) the line. This information limits the possible values of intersection points x 1 and x 2 to the green shaded segments of the x -axis. b Bounds on coefficient values as a function of the number of qualitative points known. Shaded areas indicate the range of possible values of each coefficient

---

### Corrigendum: landscape of X chromosome inactivation across human tissues [^113iLzxP]. Nature (2018). Excellent credibility.

This corrects the article DOI: 10.1038/nature24265.

---

### A 128-channel receive array with enhanced signal-to-noise ratio performance for 10.5T brain imaging [^1145zU7A]. Magnetic Resonance in Medicine (2025). Medium credibility.

4.4 Parallel imaging

Parallel‐imaging performance at 10.5 T was exceptional compared with 7 T (Figure 7). However, it was only marginally better for the 128Rx array compared with the 80Rx array at 10.5 T for the orientations examined (Figure 7). As discussed, however, there are numerous differences between the 80Rx and 128Rx arrays that can account for this, including former dimensions and loop layout. The non‐overlapped loops in the 80Rx likely produce more distinct sensitivity profiles than fully overlapped loops in the 128Rx, resulting in better g‐factor performance. These differences notwithstanding, this finding is in close agreement with previously published results at 7 T showing a lack of major differences in g‐factor in going from 64Rx to 96Rx arrays.

---

### Multiple tipping points and optimal repairing in interacting networks [^112rr1pt]. Nature Communications (2016). Medium credibility.

The problem of optimal repairing

Knowing and understanding the phase diagram of interacting networks enable us to answer some fundamental and practical questions. A partially or completely collapsed system of n ≥ 2 interacting networks in which some of them are in the low activity state is a scenario common in medicine, for example, when diseases or traumas affect the human body and a few organs are simultaneously damaged and need to be treated, and the interaction between the organs is critical. It is also common in economics, when two or more coupled sectors of the economyexperience simultaneous problems, or when a few geographical clusters of countries experience economic difficulties. The practical question that arises is: what is the most efficient strategy to repair such a system? Many approaches are possible if resources are unlimited, but this is usually not the case and we would like to minimize the resources that we spend in the repairing process.

For simplicity, consider two interacting networks, both damaged (low activity). Is repairing both networks simultaneously the more efficient approach, or repairing them one after the other? What is the minimum amount of repair needed to make the system fully functional again? In other words, what is the minimum number of nodes we need to repair, to bring the system to the functional 11 ('up–up') state, and how do we allocate repairs between the two networks? An optimal repairing strategy is essential when resources needed for repairing are limited or very expensive, when the time to repair the system is limited, or when the damage is still progressing through the system, threatening further collapse, and a quick and efficient intervention is needed.

We show below that this problem is equivalent to finding the minimum Manhattan distance between the point in the phase diagram where the damaged system is currently situated and the recovery transition lines to the 11 region. The Manhattan distance between two points is defined as the sum of absolute horizontal and vertical components of the vector connecting the points, with defined vertical and horizontal directions. It is a driving distance between two points in a rectangular grid of streets and avenues. In our phase diagram, it is equal to. It turns out that two triple points of the phase diagram play a very important role in this fundamental problem. We find that these special points have a direct practical meaning and are not just a topological or thermodynamic curiosity.

---

### Testing linkage and gene X environment interaction: comparison of different affected sib-pair methods [^114CjC1E]. Genetic Epidemiology (2003). Low credibility.

The aim of this study was to compare, under different models of gene-environment (G x E) interaction, the power to detect linkage and G x E interaction of different tests using affected sib-pairs. Methods considered were: 1) the maximum likelihood lod-score (MLS), based on the distribution of parental alleles identical by descent (IBD) in affected sibs; 2) the sum of the MLS (sMLS) calculated in affected sib-pairs with 2, 1, or 0 sibs exposed; 3) the predivided sample test (PST), which compares the IBD distribution between affected sib-pairs with 2, 1, or 0 sibs exposed; 4) the triangle test statistic (TTS), which uses the IBD distribution among discordant affected sib-pairs (one exposed, one unexposed); and 5) the mean interaction test (MIT), based on the regression of the proportion of alleles shared IBD among affected sib-pairs on the exposure among sib-pairs. The MLS, sMLS, and MIT allow detection of linkage. However, the sMLS and MIT account for a possible G x E interaction without testing it. In contrast, the PST and the TTS allow detection of both linkage and G x E interaction. Results showed that when exposure cancels the effect of the gene, or changes the direction of this effect (i.e., the protective allele becomes the risk allele), the PST, sMLS, and MIT may provide, under some models, greater power to detect linkage than the MLS. Under models where exposure changes the direction of the effect of the gene, the TTS test may also be more powerful than the other tests accounting for G x E interaction. Under the other models, the MLS remains the most powerful test to detect linkage. However, only the PST and TTS allow the detection of G x E interaction.

---

### The optimal MR acquisition strategy for exponential decay constants estimation [^116oGtC8]. Magnetic Resonance Imaging (2008). Low credibility.

Estimating the relaxation constant of an exponentially decaying signal from experimental MR data is fundamental in diffusion tensor imaging, fractional anisotropy mapping, measurements of transverse relaxation rates and contrast agent uptake. The precision of such measurements depends on the choice of acquisition parameters made at the design stage of the experiments. In this report, chi(2) fitting of multipoint data is used to demonstrate that the most efficient acquisition strategy is a two-point scheme. We also conjecture that the smallest coefficient of variation of the decay constant achievable in any N-point experiment is 3.6 times larger than that in the image intensity obtained by averaging N acquisitions with minimal exponential weighting.

---

### Minimum electric-field gradient coil design: theoretical limits and practical guidelines [^111W84pB]. Magnetic Resonance in Medicine (2021). Medium credibility.

FIGURE 4
A, A region contours of constant Bz for Y gradient coil, with (red) and without (black) E max optimization, showing that gradient distortion is virtually identical despite the peak E‐field being reduced by a factor of 2.8 for the Y coil. B, Difference in the B y components of the gradient field with versus without E‐field optimization. Values are normalized to the gradient strength yielding units of length (mm) and show a highly uniform concomitant y‐directed field added to the optimized (asymmetric) solution. C, Wire pattern representing the difference between optimized (asymmetric) and nonoptimized (symmetric) solutions with same current scaling as Figure 3. Similar results are obtained for the X gradient coil

The key difference between the E‐field minimized and nonminimized solutions is the addition of a uniform concomitant B 0 field component in the x or y direction. Figure 4B shows a difference plot of the concomitant B y field, normalized to gradient strength (units of millimeters), which is responsible for the reduction in E‐field. Figure 4C shows the winding that would need to be added to the Y symmetric coil (Figure 3B) to obtain the winding pattern of the Y asymmetric coil (Figure 3D), and shows the fundamental difference between the two solutions. The winding in Figure 4C produces a uniform field transverse to the main B 0 field, resulting in a concomitant component with additional magnetic stored energy. Conceptually, this can be thought of as a second coil, which if activated, converts the symmetric coil into an asymmetric coil with reduced E‐field but otherwise identical distortion. The added concomitant field in Figure 4B is highly uniform with a 0.85% variation over the 26‐cm imaging region, resulting in minimal distortion of the desired gradient field; this concomitant field can be compensated with a phase/frequency offset correction by the scanner. Figure 4B shows that the average concomitant field of 88.5 mm within the imaging region can be directly compared with the z 0x defined by Meier et al, 29 in which a value of 127 mm was reported for an asymmetric head gradient. The z 0x for both the ESP and HG2 gradients are approximately 120 mm 30; these are both fully asymmetric designs (single eye per half coil with all return currents flowing above the head), similar to the original concept defined by Roemer. 12 The X coil of Figure 3 requires a smaller concomitant field (z 0y = 57.8 mm) than the Y coil, reflecting a decreased need to pull E‐field off the torso region due to the smaller extent of the body in the anterior–posterior direction.

---

### Cyclic quantum causal models [^1138inQL]. Nature Communications (2021). High credibility.

Definition 10 (Causal structure of a deterministic classical process) Given a deterministic process, the causal structure of the process is the directed graph with vertices X 1. X n and an arrow X i → X j, wheneverdepends onthrough the function f.

Definition 11 (Classical Markov condition — generalized) A processis called Markov for a directed graph G with classical split-nodes X 1, …, X n as its vertices iff it admits a factorization of the form, where P a (X i) denotes the set of parents of X i according to G.

The following is immediate.

Proposition 2 Every deterministic classical process is Markov for its causal structure.

In the case of general — i.e. not necessarily deterministic — classical processes, an account of their relationship to causal structure can be given that again mirrors the quantum case. Let us adopt the provisional approach that causal structure always inheres in deterministic reversible processes (where reversibility here may not be essential, but is assumed to provide a closer analogue to the quantum case in which unitarity is assumed). Then compatibility with a given directed graph can be defined in terms of extension to a reversible deterministic process with latent local noise variables.

Definition 12 (Reversible extendibility) A processis reversibly extendible iff there exists a reversible deterministic processwith an additional leaf node F and root node λ, such thatfor some P (λ out).

Definition 13 (Compatibility with a directed graph) A processis compatible with a directed graph G with nodes X 1. X n, iffis reversibly extendible to a deterministic process, with an additional leaf node F, root nodes λ i, and a product distribution, such that through f, depends neither onfor j ≠ i nor onfor X j ∉ P a (X i) (with P a (X i) referring to G).

With Proposition 2, the following analogue of Thm. 2 is straightforward.

Theorem 5 If a classical process is compatible with a directed graph G, then it is also Markov for G.

---

### Calculating T2 in images from a phased array receiver [^111vABko]. Magnetic Resonance in Medicine (2009). Low credibility.

Images reconstructed from multielement, phased array coils and presented as the square root of the sum of the squares of the signals received by the individual elements have a distribution of signal and noise that distorts the relationship between the image intensity and the underlying signal. The distortion is accentuated for long echo times for which the signal-to-noise ratio (SNR) may be low. When measuring T(2) or T(2)* this signal distortion leads to biased estimates of these parameters. We demonstrate this effect and its dependence on the image SNR and the number of elements in a phased array coil. We evaluated the effects of four techniques for calculating T(2) from data acquired in phased array coils (log transform, least squares, lookup table correction, and maximum likelihood [ML] estimation). The ML estimation gave the most accurate T(2) in the presence of this bias.

---

### Non-linear relationships in clinical research [^113d418D]. Nephrology, Dialysis, Transplantation (2025). Medium credibility.

Polynomial functions

In Fig. 4, we provide an example of a simple power transformation using the square function, which raises a variable to the power of 2 (x 2), providing a U-shaped curve. Higher powers of x can be also used to determine the shape of the function, such as the cubic function (x 3) which has a distinctive S-shape. These power terms form the core elements of so-called polynomial functions, which along with the coefficients included in the function, offer a flexible way to model various curves. By adjusting the coefficients and power functions, polynomials offer a wide variety of shapes. "Fractional" polynomial functions allow the power terms to be fractions instead of just whole numbers (i.e. x 1/2) [21]. A (fractional) polynomial function often provides sufficient flexibility to follow relatively simple non-linear curves, providing a simpler solution than the more advanced techniques we describe below. However, higher degree polynomials can be sensitive to "noise" in the data, and are not suited for fitting some of the more complex curves (e.g. sudden shifts, discontinuities, or logarithmic curves). They may also suffer from Runge's phenomenon, becoming unstable and oscillating at the edges of the data, and extrapolate poorly beyond the original range of the independent variable.

Regression splines

A powerful approach to dealing with non-linearity is provided by the family of spline functions. Instead of a single function defining the whole curve, such as polynomials or other transformations, splines are constructed by using a series of functions, each defining a different segment of the curve. As splines are constructed segment by segment — or piece by piece — they are often referred to as "piecewise" functions. Segments are connected to each other using so-called "knots", and the spline is restricted to join at these knots so there are no gaps in the curve. The simplest spline function is the linear spline function. This function assumes linearity within each segment, but the overall curve formed by the connected segments can be non-linear. For a small number of segments, linear spline models can be as easy to interpret as linear regression, as each segment can be represented by a single slope. Figure 3 provides an example of a simple linear spline with two knots (in green). These two knots divide the range of the independent variable into three segments, each with its own slope.

---

### Room temperature organic magnets derived from spfunctionalized graphene [^1177i72R]. Nature Communications (2017). Medium credibility.

Characterization techniques

The exact composition of the precursors (C 1 F 0.55, C 1 F 0.8 and C 1 F 1) and derived G(OH)F and partially fluorinated graphene (that is, C 18 F x, x = 2.5, 4, 6.3 and 11.5) samples was determined by XPS carried out with a PHI VersaProbe II (Physical Electronics) spectrometer using an Al K α source (15 kV, 50 W). The obtained data were evaluated with the MultiPak (Ulvac-PHI, Inc.) software package. The detection of residual metal content in the G(OH)F sample was performed by ICP-MS. The exact amount of the G(OH)F sample (10 mg) was immersed in a concentrated nitric acid (≥ 99.999% trace metals basis) and heated for 2 h at 100 °C. Afterwards, the mixture was transferred into 10 mL volumetric flask, diluted with water and the undissolved graphene was caught by a 200 nm Millipore filter. The obtained concentration of metals in the solution was recalculated to the amount of the tested sample (analogically, diluted nitric acid was used as a blank). FT-IR spectra were obtained using an iS5 FT-IR spectrometer (Thermo Nicolet) with a Smart Orbit ZnSe ATR technique (650–4,000 cm −1). Raman spectra were recorded on a DXR Raman microscope using the 532 nm excitation line of a diode laser. HRTEM images were obtained using an FEI TITAN 60–300 HRTEM microscope with an X-FEG type emission gun, operating at 300 kV. STEM–HAADF analyses for EDX mapping of elemental distributions on the G(OH)F sheets were performed with an FEI TITAN 60–300 HRTEM microscope operating at 80 kV. For HRTEM, STEM–HAADF and EDX experiments, an aqueous solution of G(OH)F with a concentration of 0.1 mg mL −1 was redispersed by ultrasonication for 5 min. A drop of the sonicated sample was then deposited on a carbon-coated copper grid and slowly dried at laboratory temperature for 24 h to reduce its content of adsorbed water. Atomic force microscopy images and appropriate height profiles were recorded in semi-contact mode (HA-NC tips, mica substrate) on an NTEGRA Aura instrument. Thermogravimetric analysis and evolved gas analysis were performed on a STA449 C Jupiter-Netzsch instrument with a heating rate of 1 °C min –1. The masses of released gases in the range of 12–60 m/z were determined with a QMS 403 Aolos mass spectrometer (Netzsch), starting at 100 °C to avoid overloading the spectrometer with adsorbed water. A superconducting quantum interference device magnetometer (MPMS XL-7 type, Quantum Design, USA) was employed for the magnetization measurements. The temperature dependence of the magnetization of tetramethylammonium hydroxide, C 1 F 1 precursor and seven final products (that is, C 18 (OH) 2 F 3, C 18 (OH) 1.5 F 6, C 18 (OH) 2.6 F 4.7, C 18 (OH) 2.4 F 7, C 18 F 2.5, C 18 F 4 and C 18 F 6.3) was recorded in the sweep mode over the temperature interval from 5 to 300 K under an external magnetic field of 10 kOe; the temperature evolution of magnetization of the C 18 (OH) 1.8 F 7.2 sample was monitored in the sweep mode over the temperature interval from 5 to 400 K and back under an external magnetic field of 10 kOe. The hysteresis loops of the C 18 (OH) 1.8 F 7.2 and C 18 F 4 samples were measured at a series of temperatures in the interval from 5 to 400 K and from 5 to 300 K, respectively, in external magnetic fields ranging from −50 to +50 kOe. The hysteresis loops of the C 18 (OH) 2 F 3, C 18 (OH) 1.5 F 6, C 18 (OH) 2.6 F 4.7, C 18 (OH) 2.4 F 7, C 18 F 2.5 and C 18 F 6.3 samples were measured at a temperature of 5 K and in external magnetic fields from −50 to +50 kOe. The magnetization values were corrected assuming the response of the sample holder, sample capsule and respective Pascal constants. EPR spectra were recorded on a JEOL JES-X-320 operating at X-band frequency (∼9.15 GHz), equipped with a variable temperature control ES 13060DVT5 apparatus, and were performed on the powder C 18 F 11.5 sample (∼2 mg loaded onto the EPR tube). The cavity Q quality factor was kept above 6,000, highly pure quartz tube was employed (Suprasil, Wilmad, < 0.5 OD) to load the sample powders. The g value accuracy was obtained by comparing the resonance signals of the C 18 F 11.5. sample with that of MnO standard (JEOL-internal standard). The experimental conditions for measuring EPR spectra were adjusted as follows: frequency = 9.15584 GHz, modulation frequency = 100 kHz, modulation amplitude = 0.8 mT, time constant = 30 ms, applied microwave power = 0.8 mW, sweep time = 480 s and phase = 0°s.

---

### Mapping child growth failure in Africa between 2000 and 2015 [^116WMN8K]. Nature (2018). Excellent credibility.

Geostatistical model

Binomial count data are modelled within a Bayesian hierarchical modelling framework using a logit link function and a spatially and temporally explicit hierarchical generalized linear regression model to fit prevalence of each of our indicators in five regions of Africa as defined in GBD('Northern', 'Western', 'Southern', 'Central', and 'Eastern'; see Extended Data Fig. 7). The GBD study design sought to create regions on the basis of two primary criteria: epidemiological homogeneity and geographic contiguity(see Extended Data Fig. 7). For each GBD region, we explicitly write the hierarchy that defines our Bayesian model as follows:

For each indicator and region, we modelled the number of children at cluster i, among a sample size, N i, who are subject to the indicator as binomial count data, C i. We have suppressed the notation, but the counts (C i), probabilities (p i), predictions from the three submodels (X i) and residual termsare all indexed at a space–time coordinate. The probabilities (p i) represent both the annual prevalence at the space–time location and the probability that an individual child will be afflicted with the risk factor given that they live at that particular location. The logit of annual prevalence (p i) of our indicators was modelled as a linear combination of the three sub-models (GAM, boosted regression trees and lasso regression), X i, a correlated spatiotemporal error term and an independent nugget effect. Coefficients (β) on the sub-models represent their respective predictive weighting in the mean logit link and are constrained to sum to 1. In order for this constraint to make any sense, we ensure that the predictions from the sub-models entered into INLA (integrated nested Laplace approximation)in the link space (logit) without having been centre-scaled. The joint error term accounts for residual spatiotemporal autocorrelation between individual data points that remains after accounting for the predictive effect of the sub-model covariates, and the nugget, which is an independent error term for each data point, representing irreducible error for that observation. The residuals are modelled as a three-dimensional Gaussian process in space–time centred at zero and with a covariance matrix constructed from a Kronecker product of spatial and temporal covariance kernels. The spatial covariance (Σ space) is modelled using an isotropic and stationary Matérn function, and temporal covariance (Σ time) as an annual autoregressive-order-1 function over the 16 years that are represented in the model. This approach leveraged the residual correlation structure of the data to more accurately predict prevalence estimates for locations with no data, while also propagating the dependence in the data through to uncertainty estimates. The posterior distributions were fitted using computationally efficient and accurate approximations in R INLAwith the stochastic partial differential equationsapproximation to the Gaussian process residuals. Pixel-level uncertainty intervals were generated from 1,000 draws (that is, statistically plausible candidate maps)created from the posterior-estimated distributions of modelled parameters. Additional detail on the geostatistical model and estimation process can be found in the Supplementary Methods.

---

### Practical guidance for the evaluation and management of drug hypersensitivity: specific drugs [^117ND6Xf]. The Journal of Allergy and Clinical Immunology: In Practice (2020). High credibility.

Paclitaxel desensitization — three-bag/12-step protocol: In the same dosing context (paclitaxel [135–175 mg/m²] infused every 3 wk over 3 h; example = 294 mg), the three-bag example lists Solution 1 as 250 mL at 0.0118 mg/mL with 9.38 mL infused (dose 0.111 mg), Solution 2 as 250 mL at 0.118 mg/mL with 18.75 mL infused (2.213 mg), and Solution 3 as 250 mL at 1.167 mg/mL with 250 mL infused (291.676 mg). Step examples include step 1 using solution 1 at 2.5 mL/h for 15 min with 0.625 mL infused (0.007 mg; cumulative 0.007 mg) and the final step 12 using solution 3 at 80 mL/h for 174.4 min with 232.5 mL infused (271.328 mg; cumulative 294.0 mg). Total time (h) = 5.67.

---

### Common errors in statistics and methods [^113hpBqT]. BMJ Paediatrics Open (2024). High credibility.

Model building, regression and choice of methods

Inappropriate sensitivity analyses (eg, exclusion of 'outliers', 'leave one out' analyses in meta-analysis)

'Sensitivity analysis' is used in a wide variety of ways in different fields; even within medicine, a quick Google search finds a bewildering set of examples. In general, sensitivity analyses should be conducted to investigate the robustness (Note that this is robustness in a statistical, rather than a medical sense. That is, resistance to outliers) of analysis results where assumptions may have been made within statistical methods, or in the presence of 'problems' within the analysis (eg, missing data). Like all statistical methods, sensitivity analyses should be prespecified and specific. Data-driven sensitivity analyses, such as 'leave one out' analyses in meta-analysis and exclusion of observed 'outliers' from datasets are generally not recommended as such analyses may result in exclusion of valid data and selective reporting and increase the risk of statistical type I error where multiple sensitivity analyses are conducted. Post hoc sensitivity analyses must be carefully justified and be sure to say what, exactly, you are testing.

Variable selection via stepwise, backward, forward, bivariate screening

Variable selection in regression models is part art and part science and a variety of methods can be used. But one thing is clear; the methods in this topic heading are generally not good! They result in p values that are too low, SEs that are too small and parameter estimates that are biased away from 0. Ideally, you would use expert knowledge to select variables, but if you must use an automated method, LASSO is not bad.

Overuse of linear regression

Linear regression is one of the most common statistical methods and it has many legitimate uses. But there are many tools that were either recently invented or that recently became practical because of increases in computer speed, where 'recent' may be 'last 50 years'. Some examples are multivariate adaptive regression splines, all sorts of regression trees and related methods, quantile regression, 'big' data (where 'big' keeps on changing), and permutation and randomisation tests. Authors should consider these advancements when deciding how to apply regression to their data.

---

### Perfect linear optics using silicon photonics [^113R5pHY]. Nature Communications (2024). High credibility.

In order to ease the understanding of the programming procedure, we indicatively illustrate in Fig. 2b the sum ofandexperimental values for the 4 constituent paths of the 1st Xbar column with r = [1,2,3,4]. These values were calculated via the HA model when a single node is driven in the rangeand the remaining 3 EAMs are driven at 0 V bias, revealing the divergence from the expected performance when using the standalone EAM metrics (baseline model). Figure 2c schematically captures the effectiveness of the developed HA programming model, by putting in juxtaposition the experimentally measured Y 1 output for the #256 linear transformation sets (black dashed line) with respect to the theoretically calculated Y 1 output when using the baseline programming model (gray curve) and the theoretically calculated Y 1 output when using the HA programming model (orange curve), revealing the excellent matching between the experimental curve and the curve calculated via the HA model. A quantitative representation of the model's accuracy in describing the underlying hardware parameters is presented in Fig. 2d, where the error of the baseline and HA programming model outputs with respect to the experimentally obtained response is plotted with the gray and orange histograms, respectively. Fitting a Gaussian distribution into the acquired error values results in mean and standard deviation values that equal −5.45 and 2.04, and ~2 * 10 −3 and 0.41 for the baseline and HA models respectively, revealing the lack of a biased-error parameter in our HA-model and a significantly lower standard deviation. The same procedure was followed for the remaining 3 Xbar columns and 12 constituent EAM-based nodes, revealing a small deviation of < 2% in the derived mean and standard deviation values.

---

### Work-relatedness [^113s8Kdx]. Journal of Occupational and Environmental Medicine (2018). Medium credibility.

Work-relatedness — Table 1 steps for evaluating epidemiological evidence list procedural actions and criteria: "Collect all epidemiological literature reported on that disorder", "Identify the design of each study", and "Assess each study's methods" including "Exposure assessment methods and potential biases", "Disease ascertainment methods and potential biases", "Absence of significant uncontrolled confounders; consideration of residual confounding", "Addressing of other potential biases", "Adequacy of biostatistical methods and analytical techniques", and "Ascertainment of statistical significance — degree to which chance may have produced those results". The table then directs to "Assess the studies using the Updated Hill's Criteria" including "Temporality", "Strength of association", "Dose–response", "Consistency", "Coherence", "Specificity", "Plausibility", "Reversibility", "Prevention/Elimination", "Experiment", and "Predictive Performance", and to make a "Conclusion regarding the degree to which such a causal association is/is not met".

---

### Some statistical implications of dose uncertainty in radiation dose-response analyses [^113gcFW1]. Radiation Research (2006). Low credibility.

Statistical dose-response analyses in radiation epidemiology can produce misleading results if they fail to account for radiation dose uncertainties. While dosimetries may differ substantially depending on the ways in which the subjects were exposed, the statistical problems typically involve a predominantly linear dose-response curve, multiple sources of uncertainty, and uncertainty magnitudes that are best characterized as proportional rather than additive. We discuss some basic statistical issues in this setting, including the bias and shape distortion induced by classical and Berkson uncertainties, the effect of uncertain dose-prediction model parameters on estimated dose-response curves, and some notes on statistical methods for dose-response estimation in the presence of radiation dose uncertainties.

---

### Ceftazidime (Tazicef) [^116yUW3s]. FDA (2024). Medium credibility.

The dosage of ceftazidime IV for treatment of bone and joint infections in adults is 2 g IV q8h

---

### The nature of the globular-to fibrous-actin transition [^112DBHM5]. Nature (2009). Excellent credibility.

Actin plays crucial parts in cell motility through a dynamic process driven by polymerization and depolymerization, that is, the globular (G) to fibrous (F) actin transition. Although our knowledge about the actin-based cellular functions and the molecules that regulate the G- to F-actin transition is growing, the structural aspects of the transition remain enigmatic. We created a model of F-actin using X-ray fibre diffraction intensities obtained from well oriented sols of rabbit skeletal muscle F-actin to 3.3 A in the radial direction and 5.6 A along the equator. Here we show that the G- to F-actin conformational transition is a simple relative rotation of the two major domains by about 20 degrees. As a result of the domain rotation, the actin molecule in the filament is flat. The flat form is essential for the formation of stable, helical F-actin. Our F-actin structure model provides the basis for understanding actin polymerization as well as its molecular interactions with actin-binding proteins.

---

### Solving the where problem and quantifying geometric variation in neuroanatomy using generative diffeomorphic mapping [^117UV2Yv]. Nature Communications (2025). High credibility.

The statistical interpretation allows us to accommodate images with non reference signals, such as missing tissue, tracer injection sites, or other anomalies. At each pixel, the identity of the signal type is modeled as missing data, and maximum likelihood estimators are computed using an Expectation Maximization algorithm, which alternates between the E step: compute posterior probability π i (x) that each pixel corresponds to the reference image rather than one of the non-reference types, and the M step: update parameters by solving a posterior weighted version of the above:As an EM algorithm, this approach is guaranteed to be monotonically increasing in likelihood. An example of posterior weights are shown in the right hand column of Fig. 2 b.

Our approach uses mixtures of Gaussians to model variability in data, to allow large outliers to be accommodated by additional components, even though the Gaussian distribution itself does not have long tails. The Gaussian model allows for closed form expression (in terms of matrix inverse) for contrast transformation parameters. Other groups have used long tailed distributions to model variability and outliers in a robust manner, most notably the exponential distribution for l1 optimization. Techniques such as iteratively reweighted least squares can be applied as in Reuter et al. which lead lead to a weighted least squares problem which is similar to ours.

Nonconvex optimization with low to high dimensional subgroups and resolutions

This registration problem is highly nonconvex, and allows for many local minima. To provide robustness in our solution, we solve a sequence of lower dimensional subproblems, initializing the next with the solution to the previous. (i) 2D slice to slice rigid alignment maximizing similarity to neighbors(ii) 3D affine only alignment, registration using the full model at (iii) low (200 μm), (iv) medium (100 μm), and (v) high (50 μm) resolution. Time varying velocity fields are discretized into 5 timesteps and integrated using the Semi Lagrangian method. For most subproblems, spatial transformation parameters are estimated by gradient descent, and intensity transformation parameters are updated by solving a weighted least squares solution at each iteration. For subproblems that include linear registration only, parameters are estimated using Reimannian gradient descent (discussed in ref.and similar to a second order Gauss–Newton optimization scheme).

---

### Clinical practice guideline for management of tinnitus: recommendations from the US VA / DOD clinical practice guideline work group [^112Y8MkL]. JAMA Otolaryngology — Head & Neck Surgery (2025). High credibility.

EMBASE search strategy — KQ 7 and KQ 8: The table specifies to "Combine inclusions (AND)", includes a set labeled "Systematic reviews and meta-analyses", defines randomized controlled trial terms (e.g., 'random sample'/de, 'randomization'/de, 'randomized controlled trial'/exp, 'phase 3':ti, ab, 'phase iii':ti, ab, random*:ti, ab, rct:ti, ab), notes "Combine study types (OR)", and applies all filters using "(#4 NOT #8) AND #12 AND #15".

---

### NCCN guidelines® insights: breast cancer, version 5.2025 [^114Wipi6]. Journal of the National Comprehensive Cancer Network (2025). High credibility.

NCCN Invasive Breast Cancer — HER2-negative paclitaxel plus carboplatin options include weekly paclitaxel + carboplatin (preoperative setting only) with paclitaxel 80 mg/m² days 1, 8, and 15 and carboplatin AUC 5 or 6 day 1, cycled every 21 days x 4 cycles, and weekly paclitaxel + weekly carboplatin with paclitaxel 80 mg/m² days 1, 8, and 15 and carboplatin AUC 1.5–2 days 1, 8, and 15, cycled every 28 days x 6 cycles.

---

### Stochastic representation of many-body quantum States [^113QiBGb]. Nature Communications (2023). High credibility.

Regression

As formulated here, regression is a fundamental supervised learning task at which NNs excel. Given an ansatzand a set of samplesrepresenting our prior knowledge of the true wavefunction at some points in space, we want to find the best value of ϑ. Perhaps the simplest approach is to minimize the sum of squared residuals:To express the ansatz itself, we use a NN. Our architecture is very minimal and has not been tuned for efficiency: the NNs we used consist of a sequence of dense layers withactivation functions, and finally a linear output layer. This is in some cases be followed by an optional layer enforcing analytically known boundary and/or cusp conditions by multiplying the output of the NN with a hand-chosen parameterized function of the coordinates, such as the asymptotic solution at large distances from the origin. Technical details are provided in the Methods section.

The top panel of Fig. 1 shows what this looks like for a single particle in 1D, where the process is easy to visualize. For an arbitrary initial state (top left panel) and the ground state (top right panel) of a 1D harmonic oscillator, a series of samples and the resulting NN-based regression curves are shown. In the insets of the panel below, an analogous visualization is shown for 2D cuts across 4D wavefunctions of two interacting fermions in a 2D harmonic potential.

Fig. 1
Propagation towards the ground state in a harmonic potential.

a Different steps in the propagation towards the ground state of a particle of mass m = 1 in a 1D harmonic oscillator with frequency ω = 1, with ℏ = 1. The green line is the function fitted by the neural network to a finite set of samples (black dots on the x axis) and their corresponding values (connected by a black line). Starting with an asymmetric guess (τ = 0), the function converges towards the correct solution (dotted orange line) at the center of the trap and acquires the right symmetry (τ = 3). b Extension of the upper system to two fermions in two spatial dimensions. The energy is estimated by Monte Carlo sampling with error bars showing the standard error, and converges to the ground state value of E 0 = 3.010 ± 0.007, which results in a relative error of 0.35% with respect to the exact value of. The inset shows a cut through the wavefunction. Source data are provided as a Source Data file.

---

### Erratum: X chromosome inactivation in the human placenta is patchy and distinct from adult tissues [^116yMvoa]. HGG Advances (2022). Medium credibility.

[This corrects the article DOI: 10.1016/j.xhgg.2022.100121.].

---

### SNMMI procedure standard / EANM practice guideline for brain [F] FDG PET imaging, version 2.0 [^111G73g2]. Journal of Nuclear Medicine (2025). High credibility.

Brain [18F]FDG PET image processing and display — reconstruction, resolution, and visualization — states that images are preferably reconstructed using (ordered subset) iterative reconstruction including use of time of flight (TOF) information, when available; current PET scanners allow matrix sizes as high as 400 x 400, but a suitable size is the 128 x 128 pixels, using a minimum zoom factor of 2; EANM Research GmbH (EARL) established limits of acceptability with an observed effective resolution of 5 to 6.5 mm FWHM, and depending on the PET system a final image resolution of 4–6 mm full width at half maximum (FWHM) typically yields adequate images; if movement artifacts are observed, reconstructing short frames (e.g., 5-min frames) can help; for display, spectrum or rainbow scales or gray scales with continuous progression are typical, and a standardized image display with upper/lower color scale thresholds is advocated.

---

### ABM clinical protocol # 27: breastfeeding an infant or young child with insulin-dependent diabetes [^116eDo3K]. Breastfeeding Medicine (2017). Medium credibility.

Table 1 — methods to estimate carbohydrate intake from breast milk present two approaches: using average daily volume for 7–12‑month infants with "Average breast milk volume in 24 hours @ 70 g/L of carbohydrate/number of feeds (for 7–12-month infants)" and formula "52 g lactose/number of feeds in 24 hours = x g carbohydrate per feed (estimate)", or a pre/post‑feed weight method with "Weight in grams = mL of milk intake × 7 g/100 mL = x g of carbohydrate consumption (estimated)".

---

### Deterministic and electrically tunable bright single-photon source [^112V1S4s]. Nature Communications (2014). Medium credibility.

Brightness and extraction efficiency

Finally, we evaluate the brightness as well as the extraction efficiency of the device. For these measurements, the source emission is collected using a microscope objective with a 0.4 numerical aperture, sufficient to collect the mode full radiation pattern. Figure 4f shows the photon count rate for the X line I X measured on the detector as a function of excitation power, reaching values as high as 4.08 ± 0.002 MHz at saturation. The detection efficiency of the whole setuphas been measured to be = 0.0084 ± 0.0008 in the present experimental conditions, leading to a photon rate collected in the first lens of I X / = 0.48 ± 0.05 GHz. Under continuous wave excitation, the photon rate is inversely proportional to the radiative lifetime of the transition. As a result, the source brightness, defined as the number of photons collected per excitation cycle of the QD is given by, where the square root allows correcting from multiphoton emission. As shown on the right scale of Fig. 4f, a source brightness of 37 ± 7% collected photon per cycle is demonstrated for the X line.

The brightness of the source is the product of the extraction efficiency with the occupation factor p X of the QD X state. In the present measurement, the QD is found to be either in the neutral exciton X state or in the charged exciton state CX. To determine the respective occupation factors p X and p CX, we keep the voltage constant and increase the temperature from 35–42 K to bring the CX in resonance into the cavity mode. Given the small temperature change, we assume that capture processes are mostly unchanged. g²(0) measurements allow measuring a radiative decay time of τ CX = 0.88 ± 0.16 ns (see Fig. 4e). The intensity of the CX line as a function of power (Fig. 4f) shows brightness at saturation of 17 ± 6%. From this we deduce p X = 0.69 and p CX = 0.31 and estimate the extraction efficiency to be around 53 ± 9% in a 0.4 numerical aperture. Knowing the cavity characteristics, the expected device extraction efficiency is given by. With Q ≈ Q 0 and = 1.1, the expected extraction efficiency is around 52%, very close to the experimental one.

---

### Experimental observation of flow fields around active Janus spheres [^114SJUnx]. Nature Communications (2019). High credibility.

Solution of the Stokes problem: stuck swimmer

The total flow field around a stuck Janus swimmer readswhere
u ph is the phoretic contribution (whether electrophoretic or diffusiophoretic),
u mono is the contribution from the force monopole that maintains the colloid steady in the laboratory reference of frame.

u ph is the solution of the Stokes problem (Eq. (1) and (2) from the main text) with the boundary conditionwherewhere P n are the Legendre polynomials. The contribution from the force monopole(f > 0) isor, in dyadic notations,

Using the known expression of, the total flow field then writeswith

The flow field at the surface of the colloid is

Imposingsuch that the swimmer does not move, we find, and

Solution of the Stokes problem: moving swimmer

Ignoring the contribution coming from the force monopole and solving the Stokes problem with the boundary conditionwhereis the swimming velocity of the colloid, we find

Fitting procedure

We start from the expression of the flow field in terms of the Legendre coefficients associated to the slip velocity (Eq. (15) for the case where the colloid is stuck and (17) for the case where it is freely moving). From the experimental data, we have actually four independent sets of data:… Projecting Eqs. (15) and (17) onto the unit vectorsand, we find the corresponding fit functions (we introduce for simplicity):

Each of these functions is used to fit the experimental data (for, with the constraint). The sums are truncated at order 5. Therefore there are five fit parameters. The values obtained from the fits (as well as the averages and standard deviations) are shown on the inset of Fig. 1a. We can then rebuild the slip velocity at the surface of the colloid, defined aswhere

The reconstructed v (θ) is shown on the main plot of Fig. 1a.

---

### Global strategy for the diagnosis, management, and prevention of chronic obstructive pulmonary disease (2025 report) [^117Dpbtm]. GOLD (2025). High credibility.

Spirometry — considerations in performing spirometry (Figure 2.4) state that forced spirometry is the most reproducible and objective measurement of airflow obstruction, and peak expiratory flow measurement alone cannot be reliably used as the only diagnostic test because of its weak specificity. Preparation includes that spirometers should produce hard copy or have a digital display of the expiratory curve to permit detection of technical errors or have an automatic prompt to identify an unsatisfactory test and the reason for it. During performance, the pause between inspiration and expiration should be less than one second, the recording should go on long enough for a volume plateau to be reached, which may take more than 15 seconds in severe disease, and both FVC and FEV1 should be the largest value obtained from any of three technically satisfactory curves and the FVC and FEV1 values in these three curves should vary by no more than 5% or 150 mL, whichever is greater; the FEV1/FVC ratio should be taken from the technically acceptable curve with the largest sum of FVC and FEV1. For bronchodilation testing, possible dosage protocols are 400 mcg short-acting beta2-agonist, 160 mcg short-acting anticholinergic, or the two combined; FEV1 should be measured 10–15 minutes after a short-acting beta2-agonist is given, or 30–45 minutes after a short-acting anticholinergic or a combination of both classes of drugs, and patients already on bronchodilator treatment, in whom spirometry is requested for monitoring purposes do not need to stop their regular treatment for spirometry. For evaluation, spirometry measurements are evaluated by comparison of the results with appropriate reference values based on age, height and sex, and the presence of a post-bronchodilator FEV1/FVC < 0.7 confirms the presence of non-fully reversible airflow obstruction.

---

### NCCN guidelines® insights: merkel cell carcinoma, version 1.2024 [^1134y7Ux]. Journal of the National Comprehensive Cancer Network (2024). High credibility.

Merkel cell carcinoma — palliative regimens include hypofractionationb, h schedules of 3 Gy x 10 fractions, 4 Gy x 5 fractions, and 8 Gy x 1–3 fractionse, f.