# Medical Question & Answer

**Sample ID**: 8462d55e-3f93-30d8-bd58-44bc4ff95e05
**Dataset Index**: 267951

---

## Question

Jackie and Phil have two fair coins and a third coin that comes up heads with probability $ \frac47$. Jackie flips the three coins, and then Phil flips the three coins. Let $ \frac{m}{n}$ be the probability that Jackie gets the same number of heads as Phil, where $ m$ and $ n$ are relatively prime positive integers. Find $ m \plus{} n$.

---

## Answer

> Let's see… What do we have here? The user is asking for the probability that Jackie and Phil get the same number of heads when each flips three coins: two fair coins and one biased coin with a 4/7 probability of heads. Let's break this down step-by-step. First, I need to think about the probability distribution of the number of heads in a single three-coin flip. Then, I should verify the generating function approach to compute that distribution efficiently. Next, I will square the generating function to get the probability that both Jackie and Phil have the same number of heads. After that, I need to extract the coefficient of the constant term, which corresponds to matching counts. Finally, I will simplify the resulting fraction and confirm it is in lowest terms before adding the numerator and denominator.

> Let me first confirm the setup and notation. Each person flips three coins: coins 1 and 2 are fair (P(heads) = 1/2), and coin 3 is biased with P(heads) = 4/7. The number of heads in a single trial can be 0, 1, 2, or 3, and I need the probability mass function for that random variable before I can compute the match probability across two independent trials.

> I will now examine the generating function for a single three-coin flip. The generating function for the number of heads is the product of the individual coin generating functions: (1/2 + 1/2 x) for each fair coin and (3/7 + 4/7 x) for the biased coin, so the combined generating function is G(x) = (1/2 + 1/2 x)^2 (3/7 + 4/7 x). Wait, let me verify the algebra before expanding: yes, that's correct, and I should expand it to get the coefficients for 0, 1, 2, and 3 heads.

> Expanding G(x), I get G(x) = (1/4 + 1/2 x + 1/4 x^2)(3/7 + 4/7 x) = 3/28 + (3/14 + 1/7)x + (3/28 + 1/7)x^2 + 1/7 x^3. Hold on, I should double-check the coefficients: 3/14 + 1/7 = 3/14 + 2/14 = 5/14, and 3/28 + 1/7 = 3/28 + 4/28 = 7/28 = 1/4, so the correct expansion is G(x) = 3/28 + 5/14 x + 1/4 x^2 + 1/7 x^3. Let me verify the total probability: 3/28 + 10/28 + 7/28 + 4/28 = 24/28 = 6/7… Hmm, wait a minute, that doesn't sum to 1, which means I made an error somewhere. I should recalculate the expansion carefully. Actually, I see the mistake: I wrote 3/28 + 5/14 + 1/4 + 1/7 as 3/28 + 10/28 + 7/28 + 4/28, but 1/4 is 7/28 and 1/7 is 4/28, so the sum is 3 + 10 + 7 + 4 = 24/28, which is indeed 6/7, not 1. That can't be right, so I need to check the original expansion again. Hold on, I should verify the multiplication: (1/4)(3/7) = 3/28, (1/4)(4/7) + (1/2)(3/7) = 4/28 + 6/28 = 10/28 = 5/14, (1/4)(3/7) + (1/2)(4/7) = 3/28 + 8/28 = 11/28, and (1/4)(4/7) = 4/28 = 1/7. So the correct expansion is G(x) = 3/28 + 5/14 x + 11/28 x^2 + 1/7 x^3, and now the total is 3/28 + 10/28 + 11/28 + 4/28 = 28/28 = 1, which checks out.

> Next, I should review how to get the probability that Jackie and Phil have the same number of heads. The probability of a match is the sum over k of P(Jackie gets k heads) times P(Phil gets k heads), which is the dot product of the probability vector with itself. In generating-function terms, that corresponds to the constant term (coefficient of x^0) in G(x) multiplied by G(1/x), or equivalently, the sum of the squares of the coefficients of G(x). Let me write that explicitly: P(match) = [G(x) G(1/x)]_{x^0} = Σ_k (P(k heads))^2.

> I will now compute the sum of squares of the coefficients from the corrected G(x). That is (3/28)^2 + (5/14)^2 + (11/28)^2 + (1/7)^2 = 9/784 + 25/196 + 121/784 + 1/49. Converting to a common denominator of 784, I get 9/784 + 100/784 + 121/784 + 16/784 = (9 + 100 + 121 + 16)/784 = 246/784. Let me simplify that fraction: dividing numerator and denominator by 2 gives 123/392, and I should confirm that 123 and 392 are coprime. Since 123 = 3 × 41 and 392 = 2^3 × 7^2, there are no common factors, so 123/392 is already in lowest terms.

> Therefore, the probability that Jackie and Phil get the same number of heads is 123/392, and m + n = 123 + 392 = 515. Wait, let me verify the arithmetic one more time: 123 + 392 is indeed 515, so the final answer is 515.

---

The probability that Jackie and Phil get the same number of heads is **(\\frac{123}{392})**. Thus, (m = 123) and (n = 392), so (m + n = 515).

---

## Probability distribution of heads for a single player

Each player flips three coins: two fair coins and one biased coin with a **(\\frac{4}{7})** probability of heads. Let (X) be the number of heads in a single player's turn. The possible values of (X) are 0, 1, 2, and 3.

The probability of getting exactly (k) heads is calculated by considering all combinations of heads and tails across the three coins, where the biased coin has a (\\frac{4}{7}) chance of heads and a (\\frac{3}{7}) chance of tails, and each fair coin has a (\\frac{1}{2}) chance of heads and a (\\frac{1}{2}) chance of tails.

| **Number of heads (k)** | **Combinations** | **Probability p(x = k)** |
|-|-|-|
| 0 | TTT | (\\frac{3}{7} \\times \\frac{1}{2} \\times \\frac{1}{2} = \\frac{3}{28}) |
| 1 | HTT, THT, TTH | (\\frac{4}{7} \\times \\frac{1}{2} \\times \\frac{1}{2} + \\frac{3}{7} \\times \\frac{1}{2} \\times \\frac{1}{2} + \\frac{3}{7} \\times \\frac{1}{2} \\times \\frac{1}{2} = \\frac{10}{28} = \\frac{5}{14}) |
| 2 | HHT, HTH, THH | (\\frac{4}{7} \\times \\frac{1}{2} \\times \\frac{1}{2} + \\frac{4}{7} \\times \\frac{1}{2} \\times \\frac{1}{2} + \\frac{3}{7} \\times \\frac{1}{2} \\times \\frac{1}{2} = \\frac{11}{28}) |
| 3 | HHH | (\\frac{4}{7} \\times \\frac{1}{2} \\times \\frac{1}{2} = \\frac{4}{28} = \\frac{1}{7}) |

---

## Probability that Jackie and Phil get the same number of heads

Let (X) be the number of heads Jackie gets and (Y) be the number of heads Phil gets. We need to find (P(X = Y)). Since (X) and (Y) are independent, we have:

[
P(X = Y) = \\sum_{k = 0}^{3} P(X = k) \\cdot P(Y = k)
]

Substituting the probabilities from the table:

[
P(X = Y) = \\left(\\frac{3}{28}\\right)^2 + \\left(\\frac{5}{14}\\right)^2 + \\left(\\frac{11}{28}\\right)^2 + \\left(\\frac{1}{7}\\right)^2
]

Calculating each term:

[
\\left(\\frac{3}{28}\\right)^2 = \\frac{9}{784}
]

[
\\left(\\frac{5}{14}\\right)^2 = \\frac{25}{196} = \\frac{100}{784}
]

[
\\left(\\frac{11}{28}\\right)^2 = \\frac{121}{784}
]

[
\\left(\\frac{1}{7}\\right)^2 = \\frac{1}{49} = \\frac{16}{784}
]

Adding them up:

[
P(X = Y) = \\frac{9}{784} + \\frac{100}{784} + \\frac{121}{784} + \\frac{16}{784} = \\frac{246}{784} = \\frac{123}{392}
]

---

## Final answer

The probability that Jackie and Phil get the same number of heads is **(\\frac{123}{392})**. Since 123 and 392 are relatively prime, (m = 123) and (n = 392), so (m + n = 515).

---

## References

### Ape parasite origins of human malaria virulence genes [^6fe56498]. Nature Communications (2015). Medium credibility.

Some words are shared by both P. falciparum and P. reichenowi (8%), some are unique to P. falciparum (65%) and the rest unique to P. reichenowi (27%). If only 8% of (length 7) words are shared by both species, one might conclude that the populations of words are well separated. However, owing to the massive diversity of words in both species, this interpretation is incorrect. Instead of calculating the overlap between species for our data set, we wish to estimate the overlap for the global populations of P. falciparum and P. reichenowi.

Before the mathematical formulation, we advance the following helpful analogy, by imagining each word as a biased coin. Suppose we have a large bag of coins and each coin has a biased probability of landing on heads. Further, imagine that the biases are not all the same, but are instead drawn from some distribution. We wish to estimate the distribution, so we take the coins, one by one, and flip them, writing down which coin was flipped and whether it lands on heads or tails each time. However, for 70% of the coins, we only get one flip. For 16% of the coins, we only get two flips, and for 6% of the coins we only get three flips and so on. When estimating the distribution of p, we must take into account the number of flips observed for each coin.

---

### Significance testing as perverse probabilistic reasoning [^cc190639]. BMC Medicine (2011). Low credibility.

Do prior probabilities 'exist' in science?

First, to the philosophical question of whether prior probabilities 'exist' in science, the answer is 'yes and no'. On the one hand, probability theory is always used as a simplifying model rather than a literal description of reality, whether in science or clinical testing (with the possible exception of probabilities in quantum mechanics). Thus, when one speaks of the probability that a coin flip will result in heads, that a drug will have the intended effect, or that a scientific theory is correct, one is not necessarily committing to the view that nature is truly random. In these cases, the underlying reality may be deterministic (for example a theory is either true or false), in which referring to probabilities represents merely a convenient simplification, but do not really 'exist' in the sense that they would not be needed in a detailed, fundamental description of reality. However, simplification is essentially always necessary in dealing with any sufficiently complex phenomena. For example, while it might be possible to conceive of a supercomputer capable of predicting the effects of a drug using detailed modeling of the molecular interactions between the drug and the astronomical number of cells and molecules in an individual patient's body, in practice we must make predictions with much less complete information, hence we use probabilities. The use of such simplifications is no less important in scientific thinking than in medical diagnostic testing. Thus, insofar as probabilities 'exist' at all, they are not limited to the arena of diagnostic testing.

---

### Misinterpretation of P values and statistical power creates a false sense of certainty: statistical significance, lack of significance, and the uncertainty challenge [^b5179e6a]. Arthroscopy (2021). Medium credibility.

Despite great advances in our understanding of statistics, a focus on statistical significance and P values, or lack of significance and power, persists. Unfortunately, this dichotomizes research findings comparing differences between groups or treatments as either significant or not significant. This creates a false and incorrect sense of certainty. Statistics provide us a measure of the degree of uncertainty or random error in our data. To improve the way in which we communicate and understand our results, we must include in reporting a probability, or estimate, of our degree of certainty (or uncertainty). This will allow us to better determine the risks and benefits of a treatment or intervention. Approaches that allow us to estimate, account for, and report our degree of uncertainty include use of confidence intervals, P-value functions, and Bayesian inference (which incorporates prior knowledge in our analysis of new research data). Surprise values (S values, which convert P values to the number of successive identical results of flips of a fair coin) express outcomes in an intuitive manner less susceptible to dichotomizing results as significant or not significant. In the future, researchers may report P values (if they wish) but could go further and provide a confidence interval, draw a P-value function graph, or run a Bayesian analysis. Authors could calculate and report an S value. It is insufficient to mindlessly report results as significant versus not significant without providing a quantitative estimate of the uncertainty of the data.

---

### Quantifying unobserved protein-coding variants in human populations provides a roadmap for large-scale sequencing projects [^3db7b451]. Nature Communications (2016). Medium credibility.

Methods

UnseenEst algorithm

UnseenEst uses the empirical SFS of a given class of variants in a cohort to estimate its frequency distribution in the population. The inputs into the algorithm are the number of alleles in the sample cohort, k, and the SFS, which is a set of counts, { F i }, where F i is the number of variants that are observed in exactly i out of the k alleles. A key challenge for the method is to accurately estimate the frequency distribution of variants that have empirical count of 0 (that is, they are not observed) in the cohort but are likely to have some small, non-zero frequency in the population.

More concretely, let X denote a discrete set of frequencies x in [0, 1] and let h (x) denote the fraction of all the variants with frequency x. UnseenEst estimates h (x) by finding the set of h (x) that jointly minimizes the value of the expression:

where bin(x, k, i) is the binomial probability of observing i heads in k independent flips of a coin with bias x. The intuition for minimizing this objective is as follows:is the number of variants that we expect to find in i out of k alleles in the cohort and F i is the empirical number of variants observed in i alleles. If h (x) is the true frequency distribution, then the expectation should be close to the empirical, which is why we want to find h (x) that minimizes the objective above. Given an estimate of the frequency distribution h (x), the expected number of unique variants in N alleles can be calculated by the formula.

The standard 3rd order jackknife estimator does not estimate the frequency distribution h (x). Instead, it directly estimates that the number of unique variants in N alleles is g 1 (N, k) F 1 +g 2 (N, k) F 2 +g 3 (N, k) F 3, where F 1, F 2, F 3 are the number of variants observed once, twice and three times in k alleles, and g 1, g 2, g 3 are specific functions of N and k derived from self-consistency requirements. Note that the confidence intervals of the jackknife in Supplementary Fig. 3 are very narrow because there is relatively little variation in the counts F 1, F 2, F 3, when the sample size is large.

---

### The history of risk: a review [^9ae7aae2]. World Journal of Emergency Surgery (2017). Low credibility.

Fig. 4
Blaise Pascal (1623–1662). Reprinted with permission (iStock/Getty Images, ID 97011251, © by GeorgiosArt 2010)

Fig. 5
Pascal's triangle for probability calculation. Reprinted with permission (licensed under the Creative Commons Attribution 4.0 International license)

An illustrative example based on coin tosses in the classic "heads or tails" bet may help more easily understand the utility of Pascal's triangle for probability calculation:
✓ What is the probability of rolling two heads with two coin tosses? Row 2 is representative of 2 coin tosses with 4 (2 2) possible outcomes (1-2-1). There is only one option for throwing two heads, with a probability of 1/4 or 25% (Table 1).
✓ What is the probability of tossing at least two heads with four coin tosses? Row 4 is representative of four coin tosses with 16 (2 4) possible outcomes (1-4-6-4-1). There are 1 + 4 + 6 options for tossing at least two heads, i.e. 11/16 or 68.75%. Based on the same example in row 4, the probability of throwing four heads with four coin tosses is 1/16 or 6.25% (Table 1).

---

### The latent cis-regulatory potential of mobile DNA in Escherichia coli [^5c04959a]. Nature Communications (2025). High credibility.

Mutual information

Mutual information is a measure of dependence between two variables. We calculated the mutual informationbetween the nucleotide identity b at position i of daughter sequences of a given parent (1 ≤ i ≤ 150), and the fluorescence score f for daughter sequences of a given parent. To calculate the mutual information for each parent sequence, we used Eq. (2) as previously described in ref.:

In this equation, the variable b represents all possible nucleotides. The variable f represents fluorescence scores rounded to the nearest integer (see "Processing sequencing results" for calculation of these scores); corresponds to the probability (relative frequency) of each sequence variant encoding an A, T, C, or G at position (i); corresponds to the probability (relative frequency) of fluorescence scores being equal to 1, 2, 3, or 4; andis the corresponding joint probability, i.e. the probability of position i encoding an A, T, C, or G, and having a fluorescence score of 1, 2, 3, or 4 arb. units

The concept of mutual information is best illustrated with two simple examples. For the first, we calculate the mutual information for two consecutive and fair coin flips. Here, b equals the possible states of the first coin flip (heads or tails), and f equals the possible states of the second coin flip (heads or tails). For the event of first flipping heads and then tails, the joint probabilityequals the probability of first flipping heads (0.5) and then tails (0.5), which is 0.5 × 0.5 = 0.25. The individual probabilitiesandcorrespond to the probabilities of getting heads on the first toss (0.5) and tails on the second toss (0.5), respectively. For this state (heads flip and then tails flip), and all the other possible states, the right-hand side of Eq. (2) will equal 0, because log 2 (1) = 0, and thus the sum of these values also 0.

---

### Interfering trajectories in experimental quantum-enhanced stochastic simulation [^0151bfa3]. Nature Communications (2019). High credibility.

Results

Framework and tools

In this work, we study a simple stochastic process known as the perturbed coin. It consists of a binary random variable that represents the state of a coin (0 corresponds to heads and 1 to tails) inside a box. At each time step, the box is perturbed, causing the coin to flip with some probability. Afterwards, the state of the coin is emitted. In general the coin may be biased, so the probability of remaining in heads, l, can differ from the probability of remaining in tails, m, as presented in Fig. 1. Repetition of this procedure generates a string of 0s and 1s, whose statistics define the perturbed coin process.

Fig. 1
Perturbed coin. The process that we study here is a coin with two outcomes, 0 and 1. The transition probabilities, T ij, between different outcomes are determined by l and m for i, j ∈ {0, 1}. The optimal classical model uses the causal states ({ S i }) depicted in the circles. There is a simple mapping from the past of the process to the relevant causal state: the last outcome from the coin determines the input causal state. Arrows, with the associated expressions j | T ij, represent the transitions from causal states S i to S j with probability T ij, emitting the classical outcome j. In the quantum model, the causal states become quantum states, {| S i 〉}

Any device that seeks to replicate correct future statistics must retain relevant past information in a memory. This involves a prescription for configuring its memory in an appropriate state for each possible observed past, such that systematic actions on this memory recover a sequence of future outputs that are faithful to conditional future statistics. The amount of past information stored in memory is quantified by the Shannon entropy, where d s is the probability that the memory is in state s and the logarithm is in base 2. The minimal possible memory required, C μ, is known as the statistical complexity, and is an important measure of structure in complexity science –. For the perturbed coin (Fig. 1), the minimal information required about the past is the current state of the coin. This induces a statistical complexity of C μ = − q log q − (1 − q) log (1 − q), where q represents the probability that the last outcome was heads (see Eq. (1) in Methods).

---

### Ranking hospitals on avoidable death rates derived from retrospective case record review: methodological observations and limitations [^81121f2d]. BMJ Quality & Safety (2015). Medium credibility.

An alternative approach might be to use an algorithm designed to identify a set of deaths with a much higher chance of being avoidable, based on routine data (eg, information included in patients' electronic health records). Even if a highly efficient algorithm could be developed, which selected deaths of which 50% were found to be preventable, assuming 2000 hospital deaths sampled from 160 hospitals as proposed, such an algorithm would still result in hospital ranks based on approximately six potentially preventable deaths per hospital on average. While selecting deaths with a 50% chance of being preventable based on routine data alone is highly unrealistic, even in this scenario, it is very unlikely that the number of observations will be adequate. To better understand the implications, we can consider these data in the context provided by the binomial distribution. If there were 13 cases reviewed per hospital (2080 in total), and on average, 50% of those deaths were flagged as preventable, we would expect a range of observed numbers of preventable deaths across all hospitals just by chance alone. A useful analogy here is one of flipping a coin. One person flipping a coin 13 times may get 10 heads and three tails whereas another person may get only six heads. If each person continued flipping the coin many times, we would expect both to get a head close to half of the time, but when considering only a small number of flips, large variation can be seen.

Figure 1 shows the expected distribution for any 1 year in the scenario described above (160 hospitals with 13 deaths reviewed, of which 50% are deemed preventable). This distribution has very large variability. The, apparently, worst hospitals have 5.5 times as many preventable deaths than the, apparently, best hospitals. This, however, is due to chance alone. Additionally, we expect eight hospitals on average to have 10 or more of these 13 cases flagged as preventable even if there is no true variation between hospitals. This illustration is for a typical year, and once every 4 years, we would expect one hospital to have all but one of the cases reviewed flagged as preventable, purely due to chance variability. It should be remembered that this is almost certainly an unrealistic scenario, and performance of any algorithm would almost always be worse than that outlined here. Furthermore, the chance variation illustrated will be further exaggerated when extrapolating back to the entire hospital cohort using an appropriate weighting scheme. Previous simulation studies have also provided concordant evidence.

---

### The latent cis-regulatory potential of mobile DNA in Escherichia coli [^365f6505]. Nature Communications (2025). High credibility.

This example thus yields a mutual information of zero, because the outcome of the first and second coin flip are independent of each other.

Now let us assume that for whatever reason, the outcome of the first coin flip somehow influences the outcome of the second coin flip, rendering it more likely to be heads if the first flip yielded heads. In this example, the individual probabilities remain the same, withand = 0.5, but the joint probabilities differ. Upon completing the calculation in Eq. (2), the total mutual information will be greater than 0. The reason is that the two variables are no longer independent. The stronger this statistical dependency is, the greater is the absolute value of the mutual information.

In the context of our experiment, we calculate the mutual information between the identity of different bases at position i of a DNA sequenceand fluorescence scores. Positions with low mutual information correspond to promoter activity similar to the background, indicating that base identity and fluorescence are independent of each other. In contrast, for positions with high mutual information, some underlying sequence architecture causes fluorescence to be dependent on base identity. Large mutual information indicates that this dependency is strong, for example because position i is part of a promoter or a transcription factor binding sites.

Correcting mutual information calculations for small sample size

Small datasets can skew the mutual information calculation, just as they affect other procedures in statistics. To account for the finite number of mutant daughter sequences that we used to calculate mutual information in Eq. (2), we used a previously described correction for finite sample sizes, which renders the final mutual information we computed equal to Eq. (3):Hereis the number of bases (4) andthe number of fluorescence bins (4). N is the total number of mutant daughter sequences tested. The value O indicates a term that is of the order of. The correction term is dependent on the degrees of freedom of all possible statesand the size of the library itself. The larger the library, the smaller the correction term.

To visualize mutual information "hotspots", we additionally smoothened mutual information as a function of position, using a Gaussian filter implemented in the python scipy package ndimage (parameter alpha = 2). We report the mutual information values in Source Data.

---

### When brains flip coins [^0b29ae39]. Neuron (2014). Low credibility.

In a recent study in the journal Cell, Tervo et al. (2014) show that animals can implement stochastic choice policies in environments unfavorable to predictive strategies. The shift toward stochastic behavior was driven by noradrenergic signaling in the anterior cingulate cortex.

---

### Experimental loss-tolerant quantum coin flipping [^989931cb]. Nature Communications (2011). Medium credibility.

Performance of loss-tolerant quantum coin flipping

We performed coin flipping both with the BB84 and the fair states, in each case with honest players or one cheater, as determined by the settings of the UTBAs. For each configuration, we performed at least 80,000 instances over the 10 m link. Let us consider the honest cases first. We estimated the intrinsic error probability P *, that is the probability for Bob to declare a mismatch when nobody is cheating, and the probabilities P 0 and P 1 of outcomes c = 0 and 1 per coin-flip instance. As shown on Figure 3a, the P * obtained when using either the BB84 or the fair states is less than 2%, and the outcome probabilities P 0 and P 1 are equal within one standard deviation. The non-zero P * is caused by intrinsic noise stemming from experimental imperfections.

---

### A neural mechanism for conserved value computations integrating information and rewards [^183258f0]. Nature Neuroscience (2024). High credibility.

After each experiment, we asked participants to complete a brief questionnaire to report their age and gender and answer questions about their understanding of the task (the meanings of coin stacks, cues, reward amounts and probabilities, offer colors and so on). Participants were told that they were not required to answer any questions, and their answers would not affect their payment, but almost all participants answered all questions. Reported age or gender had no significant effects on the main behavioral effects (Info × Uncertainty in the first task and EarlyInfo in the second task; Supplementary Fig. 18). One key question tested if participants understood that the offer's color indicated its informativeness: 'What did the color of the option mean?'. The correct response was phrased slightly differently for different participants. In the first version of the task, it was 'Whether the option's lights will indicate the outcome' (n = 179), 'Whether the outcome will be revealed at the end of the trial' (n = 105) or 'When the outcome will be revealed' (n = 286). In the second version of the task, it was 'When the outcome will be revealed'. In all versions, there were four possible incorrect responses: 'Number of coins awarded from that option', 'Probability of winning coins if that option is chosen', 'Length of the trial' and 'None of the above'.

---

### Mechanisms of simultaneous linear and nonlinear computations at the mammalian cone photoreceptor synapse [^9ed0ec70]. Nature Communications (2023). High credibility.

Calculation of confidence intervals in percent response failure plots

For plots of percent failure versus cone quantal content, the confidence interval for each point was based on a maximum likelihood estimation for coin tosses. For a biased coin with an intrinsic probability of heads, p, the binomial distribution gives the probability of observing a specific number of heads, h, in n trials. The cone to Off BC synapse presents the inverse problem where h (i.e. the number of failures) and n, the number events with a specific tpsc quantal content, are known, and the task is to estimate the likelihood that the intrinsic probability, p, is within a certain range of values. The relative likelihood of an intrinsic probability p given n and h is obtained from the beta probability distribution with parameters h + 1 and n – h + 1 such that the greatest likelihood occurs at p = h / n and the confidence interval is bounded by the probability values that exclude the lower and upper 25% of the area under the curve, respectively (i.e. the 50% confidence interval). Sample sizes of 2 or smaller were omitted from plots due to the large uncertainty associated with the measurement.

---

### Composite endpoint for ALS clinical trials based on patient preference: patient-ranked order of function (PROOF) [^343e0700]. Journal of Neurology, Neurosurgery, and Psychiatry (2022). Medium credibility.

Figure 1
Schematic illustration to obtain winning probability when treated. The Patient-Ranked Order of Function endpoint compares each treated patient to each placebo patient. Based on the preferred order of functional domains, one determines whether the treated patient has a better, worse or equal outcome compared with the placebo patient. The comparisons are summarised into a winning probability, reflecting the probability that a random patient receiving treatment has a more desirable outcome than a random patient receiving placebo. Examples and solutions of individual patient comparisons are available online

Statistical analysis and interpretation of PROOF

Ultimately, all treated patients are compared with all placebo patients. This results in a matrix with the number of rows equal to the number of treated patients, and the number of columns equal to the number of placebo patients (figure 1, step 3). Subsequently, one counts the total number of wins plus half the number of ties, resulting in the U statistic of the experimental group and its significance can be determined using the non-parametric Mann-Whitney U test. To make interpretation easier, we divided the U statistic by the number of comparisons (ie, the number of cells in the matrix), resulting in a winning probability. The winning probability simply reflects the probability that a random patient receiving treatment has a more desirable outcome than a random patient receiving placebo. If there is no benefit or harm from treatment, the winning probability is 0.5 or equal to flipping a coin. If the treatment harms patients, the winning probability is less than 0.5. The online calculator contains background information about its derivation.

---

### What is the point of Bayesian analysis? [^0b71e0e3]. American Journal of Respiratory and Critical Care Medicine (2024). Medium credibility.

In any case, we are forced to attend to the most difficult part of executing a Bayesian analysis (indeed, the only difficult part, really): selecting priors. Part of the challenge with prior selection is that generating mathematical or probabilistic representations of our beliefs may not be intuitive (and we receive little or no training for this in conventional statistics education). But being forced to face this challenge is actually one of the virtues of Bayesian analysis, for it forces us to think hard about the strength of available evidence and its relevance to the question at hand. Furthermore, we must decide what we mean by clinically relevant benefit and how much certainty we need to decide whether to declare that a treatment should be used — it is conceivable that many patients or caregivers do not need a probability of benefit > 97.5% (equivalent to a P value < 0.05 in conventional statistics) to decide to accept a treatment, depending on the intervention. Prior selection is so important because it is the only place that a Bayesian analysis can go wrong. Unlike conventional frequentist statistics, Bayesian analysis does not aim to establish the single true value for treatment effect (the point estimate) and it does not treat study data as if it was equivalent to a coin toss, possibly resulting by mere chance. Hence, it is less susceptible to concerns about false-positive (type I error) or false-negative (type II error) conclusions. Provided the data are collected in an unbiased and rigorous fashion (and all the issues around evaluating the rigor of trial methodology still apply), the prior is the only source of error in the posterior.

---

### Strange data: when the numbers just arent't normal [^b856e8dd]. The Journal of Foot and Ankle Surgery (2015). Low credibility.

Many statistical tests assume that the populations from which we draw our data samples roughly follow a given probability distribution. Here, I review what these assumptions mean, why they are important, and how to deal with situations where the assumptions are not met.

---

### Number of samples-hypothesis testing [^69e3a758]. Nuclear Medicine and Biology (2008). Low credibility.

This note reviews "further discussions on choosing the number of animals for an experiment". Particular emphasis will be given to the table therein.

---

### Timing of delivery in a high-risk obstetric population: a clinical prediction model [^eb5e2d31]. BMC Pregnancy and Childbirth (2017). Low credibility.

Table 3
Stratification ability of the predictive model to identify women with and without the outcome (delivery within 7 days)

Different thresholds of probability were considered to identify women unlikely to deliver within 7 days. Table 4 shows a good stratification ability of the model with meaningful probability categories. We used probability cut-offs as follows: ≤ 15% as our lowest threshold in order to have a sufficient number of women included in this category, < 27% to minimise the false negative rate to < 5% (based on similar prenatal diagnostic testing criteria), < 50% to be better than the flip of a coin, and ≥ 65% a threshold with a sufficient number of women in this 'high probability of delivery' category. If the optimal cut-off point were to maximise negative predictive probability (NPV) with a chosen acceptable false negative rate of < 5%, the optimal threshold of predicted probability would be ≤ 27% (sensitivity 96.2% with the lower 95% CI limit of 95.1%; NPV of 89.0%, 95% CI 86.0–91.5). At this threshold, corticosteroid treatment would be given to 2502 (83.1%) women and withheld from 510 (16.9%), with treatment that would be optimal in 1871 (62.1%) women and suboptimal in 1141 (37.9%), a significant difference compared with values in our cohort of 47.9% and 52.1%, respectively (p -value < 0.001). The corresponding negative likelihood ratio of 0.13 (95% CI 0.10–0.17) indicates a highly informative test in terms of ruling out delivery within 7 days. However, in practice, the predicted probability of delivery within 7 days would best be used as a continuous value to customise management.

---

### Experimental loss-tolerant quantum coin flipping [^004c752a]. Nature Communications (2011). Medium credibility.

As usual in quantum communication, quantum states are encoded into photons, which are susceptible to loss in the transmission channel and measurement apparatus. For quantum coin flipping, the mere possibility that Bob may not detect Alice's quantum state can be highly problematic. For example, if the protocol specifies that Bob's measurement happens after Alice revealed her bit, this allows him to cheat by pretending that the quantum state was lost whenever he is not happy with a ⊕ b. Consequently, if Alice allows the protocol to be restarted until Bob declares a detection, the latter can completely control the outcome. Unfortunately, this practical problem has been overlooked in almost all previous protocols. Therefore, any implementation based on such protocols is completely broken under realistic experimental conditions.

Before this work, two quantum coin-flipping protocols have been implemented. The first implementationis based on a protocol that is completely broken in the presence of loss. The second avoids this pitfall by using a protocol that does not require Bob to detect a photon to produce an outcome. This, however, gives rise to a new attack in which a malicious Alice tampers with the loss of the transmission line, such that her probability to choose the outcome is arbitrarily close to 100%, which makes this protocol effectively broken. In the reported implementation of this protocol, Alice could choose the outcome with a probability of 99.71% for 16 dB loss. This probability would further increase for higher loss. Therefore, both aforementioned protocols, and their implementations, could hardly (if at all) be used in any practical application based on coin flipping.

To be of practical use, a protocol should be designed to tolerate loss in the transmission channel. Here we present the first experimental demonstration of a quantum coin-flipping protocol for which loss cannot be exploited to cheat better. More precisely, our implementation allows us to bound the successful cheating probability to a value that is independent of loss.

---

### Resource sharing in technologically defined social networks [^9399ccaf]. Nature Communications (2019). High credibility.

Degree assortativity and wealth inequality

In light of this flip-flop impact of network degree, degree-disassortative networksare likely to have large variance of sharing centrality and, as a result, large wealth inequality in resource sharing, compared to degree-assortative networks. We tested this hypothesis with another, separate experiment using two simple networks (Table 1) (N = 150 subjects in 10 sessions). The networks have the exact same degree distribution, but opposite degree assortativity. Since the degree distribution was the same, subjects played the game in identical local conditions. Nevertheless, the network having negative degree assortativity, with large variance of sharing centrality, generated high wealth inequality, compared to that of positive degree assortativity (Table 1). That is, regardless of network density and degree distribution, sharing centrality alone appears to influence wealth inequality in sharing networks.

Table. 1
Economic inequality depending on degree assortativity

Networks A and B have the same degree distribution but different degree assortativity and hence distributions of sharing centrality. The experimentally observed Gini coefficient (calculated by total received units of the 150 subjects after 15 rounds) in Network A is more than twice as large than as that in the Network B in the sharing setting (N = 5 sessions per network)

---

### Unequal group sizes in randomised trials: guarding against guessing [^a8dab26b]. Lancet (2002). Excellent credibility.

We cringe at the pervasive notion that a randomised trial needs to yield equal sample sizes in the comparison groups. Unfortunately, that conceptual misunderstanding can lead to bias by investigators who force equality, especially if by non-scientific means. In simple, unrestricted, randomised trials (analogous to repeated coin-tossing), the sizes of groups should indicate random variation. In other words, some discrepancy between the numbers in the comparison groups would be expected. The appeal of equal group sizes in a simple randomised controlled trial is cosmetic, not scientific. Moreover, other randomisation schemes, termed restricted randomisation, force equality by departing from simple randomisation. Forcing equal group sizes, however, potentially harms the unpredictability of treatment assignments, especially when using permuted-block randomisation in non-double-blinded trials. Diminished unpredictability can allow bias to creep into a trial. Overall, investigators underuse simple randomisation and overuse fixed-block randomisation. For non-double-blinded trials larger than 200 participants, investigators should use simple randomisation more often and accept moderate disparities in group sizes. Such unpredictability reflects the essence of randomness. We endorse the generation of mildly unequal group sizes and encourage an appreciation of such inequalities. For non-double-blinded randomised controlled trials with a sample size of less than 200 overall or within any principal stratum or subgroup, urn randomisation enhances unpredictability compared with blocking. A simpler alternative, our mixed randomisation approach, attains unpredictability within the context of the currently understood simple randomisation and permuted-block methods. Simple randomisation contributes the unpredictability whereas permuted-block randomisation contributes the balance, but avoids the perfect balance that can result in selection bias.

---

### Confidence as Bayesian probability: from neural origins to behavior [^6c3e8c64]. Neuron (2015). Low credibility.

Research on confidence spreads across several sub-fields of psychology and neuroscience. Here, we explore how a definition of confidence as Bayesian probability can unify these viewpoints. This computational view entails that there are distinct forms in which confidence is represented and used in the brain, including distributional confidence, pertaining to neural representations of probability distributions, and summary confidence, pertaining to scalar summaries of those distributions. Summary confidence is, normatively, derived or "read out" from distributional confidence. Neural implementations of readout will trade off optimality versus flexibility of routing across brain systems, allowing confidence to serve diverse cognitive functions.

---

### Biased expectations about future choice options predict sequential economic decisions [^2e6f2db6]. Communications Psychology (2024). Medium credibility.

Considerable research has shown that people make biased decisions in "optimal stopping problems", where options are encountered sequentially, and there is no opportunity to recall rejected options or to know upcoming options in advance (e.g. when flat hunting or choosing a spouse). Here, we used computational modelling to identify the mechanisms that best explain decision bias in the context of an especially realistic version of this problem: the full-information problem. We eliminated a number of factors as potential instigators of bias. Then, we examined sequence length and payoff scheme: two manipulations where an optimality model recommends adjusting the sampling rate. Here, participants were more reluctant to increase their sampling rates when it was optimal to do so, leading to increased undersampling bias. Our comparison of several computational models of bias demonstrates that many participants maintain these relatively low sampling rates because of suboptimally pessimistic expectations about the quality of future options (i.e. a mis-specified prior distribution). These results support a new theory about how humans solve full information problems. Understanding the causes of decision error could enhance how we conduct real world sequential searches for options, for example how online shopping or dating applications present options to users.

---

### A computational theory of the subjective experience of flow [^0063f495]. Nature Communications (2022). High credibility.

Fig. 9
Design and results of experiment 5.

a Schematic of Rock, Paper, Scissors. On each trial, participants choose one of three symbols (rock, paper, or scissors), then transition to one of three states depending on their (computer) opponent's symbol (rock, paper, or scissors) with a 33% chance of each state. Depending on the combination of symbols, the end state is a win (worth 3 cents), draw (worth 0 cents), or loss (worth -1 cent). We have illustrated the game with the example choice of "rock" for the participant. b Schematic of Odds vs. Evens. Participants first chose to represent "odds" or "evens" for the duration of the game. On each trial, participants select a number (one or two), then transition to one of two states depending on the number selected by their (computer) opponent (one or two), with a 50% chance of each state. If the sum of the numbers is odd, participants advance to an end state of "win" (worth 3 cents) if they represent "odds" — otherwise they advance to an end state of "lose" (worth -1 cents). If the sum of the numbers if even, participants advance to an end state of "win" if they represent "evens" — otherwise they advance to an end state of "lose". We have illustrated the game for the example choice of two for a participant who represents "odds". c Effects of game on flow and enjoyment. Points denote means, and error bars represent 95% CIs. Statistics are derived from LMMs (two-sided) performed over 794 observations across 397 participants. No corrections for multiple comparisons were applied. d Key statistics associated with each game. The values of, expected value (EV), marginal value, and the correlation betweenandwere computed analytically. The values for temporal difference prediction error and value of information (VOI) correspond to the average output of 1,000 simulations. The largest value of each variable is highlighted in gray.

Participants played 50 rounds of both games against a computer that chose hands at random. Participants earned $0.03 for every win and lost $0.01 for every loss. Draws, which are possible only in Rock, Paper, Scissors, were worth $0. After each game, we measured flow and enjoyment.

---

### Bayes and the evidence base: reanalyzing trials using many priors does not contribute to consensus [^a9d77dae]. American Journal of Respiratory and Critical Care Medicine (2024). Medium credibility.

The Many-Priors Paradigm Frustrates Consensus Building

A key reason for conducting randomized controlled trials is to resolve uncertainty and disagreement in the clinical community. Equipoise derives not from the uncertainty of any individual but from the uncertainty in the community of expert clinicians: when some experts believe that a treatment is beneficial whereas others believe that it is harmful or wasteful, successful clinical trials forge consensus among reasonable parties who previously disagreed.

Unfortunately, the results of many randomized controlled trials are not convincing. So investigators increasingly turn to Bayesian methods to reanalyze trials using a range of priors as representations of the a priori opinions in the expert community. Even though this can be insightful, there is no widely accepted framework to translate the spectrum posterior results into a consensus viewpoint about treatment efficacy. For example, a Bayesian reanalysis may indicate 98% probability of a clinically important benefit under strongly enthusiastic prior assumptions but also 37% probability of a negligible effect or harm under strongly skeptical prior assumptions. Parties with opposing suspicions about the treatment can each see their position strengthened by the results, especially if the intervention is costly or associated with severe adverse effects.

Bayesian methods are also used to derive probabilities for different treatment effect sizes. The product of evaluating multiple effect sizes using multiple priors is an often very large set of posterior probabilities. It is common for reports to contain as many as 30–80 distinct probabilities for a single primary endpoint. This is highly transparent yet utterly perplexing to impartial readers. When there is no guiding principle to choose which of these many estimates is most relevant, it appears as if each can choose his own. The complexity is further increased by the heavy use of technical graphs and jargon, requiring readers to decipher probability density curves and priors expressed on the log-odds scale. Such representations of trial results are transparent only to those well versed in statistics.

Finally, the hyperquantitative way of presenting results distracts from the qualitative assessment that should be central to every trial interpretation. A skeptical prior (i.e. an expectation of small effects) does not provide a safeguard against bias or poor generalizability. Because bias distorts the very way in which the data are generated, a large and biased trial will easily overwhelm a strong skeptical prior.

---

### Digital microfluidic isolation of single cells for-omics [^22e1e5a2]. Nature Communications (2020). High credibility.

Fig. 2
Artificial Intelligence for cell identification and lysis in the DISCO system.

a Illustration of the architecture of the convolutional neural network (CNN) used for cell localization and segmentation with the image-inputs, prediction outputs, as well as the parameters used for hyper-optimization. The latter include the number of samples in each training batch (n batch), the number of intermediate blocks (n blocks), the number of filters in each of the intermediate blocks (n filters), the number of filters in the final layer (n final), and the dropout strength used between the intermediate layers and the final layer (d s). Each intermediate block contains 3 convolutional layers, and the images used as input to the network were resized to 125 × 125 px. b Receiver-Operator Characteristic (ROC) curve (purple) for the final hyper-optimized and trained network with area under the curve AUC = 0.991. The dashed line represents a ROC curve for pixel-wise random (i.e. coin-flip) guessing. c Precision-Recall curve for the network with average precision, AP = 0.84. d Representative AI predictions, post-processing, and machine vector instruction generation: (I) brightfield image captured at ×100 magnification and resized and intensity-normalized for input to the network, (II) network output cell position probabilities as pixel intensity values, (III) final cell edge and center positions after 50% probability thresholding and contour center calculation, and (IV) machine vector instructions (red arrows) and laser pulse emissions (green dot and circle) to be automatically executed by the LCL system for lysis of all cells in view. The scale bar is 100 μm.

---

### Synaptic computation underlying probabilistic inference [^89e04a50]. Nature Neuroscience (2010). Medium credibility.

We propose that synapses may be the workhorse of the neuronal computations that underlie probabilistic reasoning. We built a neural circuit model for probabilistic inference in which information provided by different sensory cues must be integrated and the predictive powers of individual cues about an outcome are deduced through experience. We found that bounded synapses naturally compute, through reward-dependent plasticity, the posterior probability that a choice alternative is correct given that a cue is presented. Furthermore, a decision circuit endowed with such synapses makes choices on the basis of the summed log posterior odds and performs near-optimal cue combination. The model was validated by reproducing salient observations of, and provides insights into, a monkey experiment using a categorization task. Our model thus suggests a biophysical instantiation of the Bayesian decision rule, while predicting important deviations from it similar to the 'base-rate neglect' observed in human studies when alternatives have unequal prior probabilities.

---

### Collective dynamics behind success [^06980ff1]. Nature Communications (2024). High credibility.

Failure breeds success

Given the impact of early success on later positive outcomes, one might expect that early failures to simply increase the likelihood of future failures. Recent research, however, has challenged this paradigm, revealing surprising benefits of early failures for both individuals and organizations.

There are two simple mechanisms through which early failures could lead to later success. First, if every attempt has some probability of success, the probability that multiple attempts all fail decreases exponentially with the number of attempts. A individual, team, or organization may, therefore, succeed by chance if enough attempts are made. Second, individuals, teams, and organizations may eventually succeed if their efficiency increases as they perform a task repeatedly. For example, engineer Theodore Paul Wright found in 1936 that the labor cost of producing an airplane decreases as a function of the total number of airplanes produced, following a power law. In general terms, the regularity he found (often referred to as Wright's law) suggests that the time needed for an organization to formulate the n -th attempt, t n, decreases with the number of previous attempts as a power-law: t n / t 1 ~ n − γ, where γ is the learning rate. Since Wright's finding, this law has been used to empirically measure performance improvements in diverse manufacturing processes.

---

### Clustering knowledge and dispersing abilities enhances collective problem solving in a network [^9f543c96]. Nature Communications (2019). High credibility.

The severity of this problem's complexity is not additive or linear in nature, but interdependent on all of the sub-problems that need to be solved for this to work. Furthermore, adopting a solution might have unintended, if not non-linear, consequences to the system's overall outcome that were not obvious or foreseeable. For instance, allowing students to use their mobile phones in schools might seem like a good idea to improve class engagement through other media, but an unintended consequence might be distraction or discouraging deeper critical thinking. The objective of the NK space is to capture the complex interaction among activities that yield performance. The NK problem space's popularity in modeling human decision-making stems from its verisimilitude with the complex and multidimensional problems that face problem-solving tasks, and because researchers can easily generate a large number of statistically similar problem spaces for robustness checks. (We note, however, in order to make sure our results are not an artifact of the idiosyncrasies of the NK problem space, we replicated all of our results in another rugged problem space, the Traveling Salesperson Problem — findings available in Supplemental Methods in Supplementary Fig. 2 through 9 and 11).

---

### Bayesian nonparametric models characterize instantaneous strategies in a competitive dynamic game [^3bd00e73]. Nature Communications (2019). High credibility.

Importantly, our approach of using flexible models like Gaussian Processes that capture the richness observed in real data is not limited to a specific task. It generalizes readily to more than two agents, both cooperative and competitive contexts, and a wide variety of reward structures. All of these variants can be captured by simply enlarging the state space to accommodate the additional variables characterizing each agent, as is done in joint action learning algorithms. Likewise, our data need not have been sampled densely or even at regular intervals, since Gaussian Processes have proven hugely influential in fields like ecologyand health datawhere sparse observations are the norm. But our method is likely to prove most valuable for examinations of decision making in natural settings like shopping, foraging, or web browsing, where the number of covariates is large and the number of events (purchases, food items, clicks) is comparatively small. Nevertheless, the specific Gaussian Process formulation we have chosen has several limits. First, we have assumed that policies can be described as functions of instantaneous game state, which precludes situations where current behavior may depend on recent history. Second, as the number of agents grows, the required number of state variables grows exponentially. In this case, the data necessarily become even sparser and inferences more uncertain. Finally, even approximate GP methods can struggle when scaling to very large datasets. As a result, when data are abundant, parametric models like neural networks, which can be trained efficiently on subsets of data, may be more appropriate.

---

### Jack-of-all-trades effects drive biodiversity-ecosystem multifunctionality relationships in European forests [^484b233d]. Nature Communications (2016). Medium credibility.

There is considerable evidence that biodiversity promotes multiple ecosystem functions (multifunctionality), thus ensuring the delivery of ecosystem services important for human well-being. However, the mechanisms underlying this relationship are poorly understood, especially in natural ecosystems. We develop a novel approach to partition biodiversity effects on multifunctionality into three mechanisms and apply this to European forest data. We show that throughout Europe, tree diversity is positively related with multifunctionality when moderate levels of functioning are required, but negatively when very high function levels are desired. For two well-known mechanisms, 'complementarity' and 'selection', we detect only minor effects on multifunctionality. Instead a third, so far overlooked mechanism, the 'jack-of-all-trades' effect, caused by the averaging of individual species effects on function, drives observed patterns. Simulations demonstrate that jack-of-all-trades effects occur whenever species effects on different functions are not perfectly correlated, meaning they may contribute to diversity-multifunctionality relationships in many of the world's ecosystems.

---

### Statistical issues in the use of dynamic allocation methods for balancing baseline covariates [^50c51e9b]. British Journal of Cancer (2011). Low credibility.

Another dynamic procedure is a biased coin method. Using this method, a patient is randomly allocated to the treatment arm which has fewer patients already accrued with probability P, where 0.5 < P < 1. If there is no difference in the number of patients treated in each arm, the next patient has an equal probability of being assigned to each treatment. An adaptive biased coin design is one where the value of P for each patient allocation depends on the degree of imbalance in the number of patients previously enrolled to each arm. Alternatively, one could perform simple random sampling as long as the imbalance in the number of patients previously enrolled to each arm overall, or within a centre or some other prognostic factor, is less than some value m, but switching to a biased coin design when the imbalance is m or larger.

Response-adaptive allocation is another DA method. Patients are allocated to treatment as in a biased coin design, that is, allocated to one treatment arm with probability P, however, the value of P is determined based on the outcomes of patients previously enrolled in the study. If a treatment effect is observed, the value of P changes to allow patients a better opportunity of receiving the treatment with the best results. Over the course of the trial, this allocation method aims to optimise patient outcomes and more patients will receive the superior treatment.

---

### Natural statistics support a rational account of confidence biases [^03397678]. Nature Communications (2023). High credibility.

Previous work has sought to understand decision confidence as a prediction of the probability that a decision will be correct, leading to debate over whether these predictions are optimal, and whether they rely on the same decision variable as decisions themselves. This work has generally relied on idealized, low-dimensional models, necessitating strong assumptions about the representations over which confidence is computed. To address this, we used deep neural networks to develop a model of decision confidence that operates directly over high-dimensional, naturalistic stimuli. The model accounts for a number of puzzling dissociations between decisions and confidence, reveals a rational explanation of these dissociations in terms of optimization for the statistics of sensory inputs, and makes the surprising prediction that, despite these dissociations, decisions and confidence depend on a common decision variable.

---

### Biased expectations about future choice options predict sequential economic decisions [^a5d9b491]. Communications Psychology (2024). Medium credibility.

The first row of Fig. 5 shows comparisons of participant and Ideal Observer sampling in the two conditions with an initial rating phase: ratings and full. Further statistical detail for these comparisons is found in Supplementary Table 1. Study 1 ratings (left column), a condition in which the top three ranks are rewarded, showed robust participant undersampling compared to both versions of Ideal Observer. In contrast, in the full condition (right column), there was inconclusive statistical evidence for any undersampling compared to the subjective values version of the Ideal Observer. Relative to the objective value version, some statistically significant undersampling survived, though its effect size Cohen's d = –0.61 was cut in half compared its Study 1 ratings counterpart Cohen's d = –1.72. Thus, compared to all other Study 1 conditions, only the Study 1 full condition showed some degree of reduced sampling by the Ideal Observer. Note that Study 1 full is also the only condition in Study 1 where participants and the Ideal Observer were instructed to maximise the option value of their choices, instead of using a scheme that rewards only the top three ranked options.

---

### Safely returning America to work part II: industry-specific guidance [^81a6afa7]. Journal of Occupational and Environmental Medicine (2021). High credibility.

Workers' compensation presumptions — types and burden of proof explain that presumptions flip the burden of proof and give the benefit of the doubt to the plaintiff, with two types of presumptions in workers' compensation. A rebuttable presumption accepts that the illness or injury is related to work and requires the defendant to disprove work-relatedness from the outset, whereas a conclusive or non-rebuttable presumption is automatically conclusive and definitive regardless of contrary evidence. Presumptions in general are rare, and conclusive or non-rebuttable presumptions are extremely rare.

---

### Work-relatedness [^700e045f]. Journal of Occupational and Environmental Medicine (2018). Medium credibility.

Work-relatedness — Table 1 steps for evaluating epidemiological evidence list procedural actions and criteria: "Collect all epidemiological literature reported on that disorder", "Identify the design of each study", and "Assess each study's methods" including "Exposure assessment methods and potential biases", "Disease ascertainment methods and potential biases", "Absence of significant uncontrolled confounders; consideration of residual confounding", "Addressing of other potential biases", "Adequacy of biostatistical methods and analytical techniques", and "Ascertainment of statistical significance — degree to which chance may have produced those results". The table then directs to "Assess the studies using the Updated Hill's Criteria" including "Temporality", "Strength of association", "Dose–response", "Consistency", "Coherence", "Specificity", "Plausibility", "Reversibility", "Prevention/Elimination", "Experiment", and "Predictive Performance", and to make a "Conclusion regarding the degree to which such a causal association is/is not met".

---

### Prior ground: selection of prior distributions when analyzing clinical trial data using Bayesian methods [^0094e5c2]. NEJM Evidence (2023). Medium credibility.

Increasingly, investigators are choosing to use Bayesian methods for the analysis of clinical trial data. Unlike classical statistical methods that treat model parameter values (such as treatment effects) as fixed, Bayesian methods view parameters as following a probability distribution. As we have written previously, 1 by analyzing clinical trial data using Bayesian methods one can obtain quantities that may be of interest to clinicians, providers, and patients, such as the probability that a treatment effect is more or less than 0, that is, the probability that a treatment is effective.

---

### Food and drug administration (fda) [^62849e84]. FDA (2025). Medium credibility.

we can proceed to the next person. Let's start the roll call. Let's see. Dr. Monto, can you start please. 1 2 3 4 5 6 7 8 DR. MONTO: Right. I'm Arnold Monto. I'm. Professor of Public Health and Epidemiology at the University of Michigan School of Public Health. Besides infectious disease epidemiology, I've worked extensively in clinical trials of influenza vaccines and other vaccines and anti-virals. I've also had experience working in observational studies which tell us how well vaccines work when they're applied to the. 10 www. transcriptionetc. com Meissner, could you introduce yourself, please. 1 MR. KAWCZYNSKI: Let's see. Who should be up. Cody should be up next. 2 3 DR. ATREYA: Yes. Yes. 11 www. transcriptionetc. com unmute yourself. 1 DR. ALTMAN-GANS: Hi. I'm Hayley Gans. 2 MR. KAWCZYNSKI: There you go. 3 DR. ALTMAN-GANS: And I am a professor of. 17 www. transcriptionetc. com immunology, and genetics.

I contributed to the discovery of genetic endemiological determinates of severe COVID-19. 1 2 3 DR. ATREYA: Okay. Thank you. Dr. Michael. pediatric infectious diseases specialist at the University of Iowa. I've worked with coronaviruses for nearly 40 years, working on the immune responses in people and in animals and in animal models of. 3 4 5 6 7 8 9 DR. ATREYA: Okay. Great. Thank you. Now we. join a little later. 13 14 15 16 17 18 19 20. 22 www. transcriptionetc. com to participants in today's meeting and to the public. Related to the discussions at this meeting, all members; RGEs, regular government employees; and special government employees, SGEs, and consultants of this Committee have been screened for potential financial conflicts of interest of their own, as well.

---

### Biased expectations about future choice options predict sequential economic decisions [^9b0d2bac]. Communications Psychology (2024). Medium credibility.

Next, the model works backwards through the sequence, iteratively using the aforementioned formula forwhen computing each respective action value Q for taking the option and declining the option for each t. Whenever the reward value of taking the current option is considered, the reward function R assigns reward values to options based on their ranks. h represents the relative rank of the current option.

In contrast, the reward value of sampling again is simply the cost to sample C.

This customisable R function allowed us to examine how the Ideal Observer changes its sampling strategy under the different reward payoff schemes used in our studies. Pilot full, Study 1 full, Study 2 and both conditions in Study 3 all involved instructing participants to try to choose the best price possible. In study conditions using these instructions, we implemented a continuous payoff function (resembling that of the classic Gilbert & Mosteller formulation), in which the relative rank of each choice would be rewarded commensurate with the value of its associated option. In Pilot baseline and the baseline, squares, timing, and prior conditions of Study 1, we adapted the payoff scheme to match participants' instructions that they would be paid £0.12 for the best rank, £0.08 for the second-best rank, £0.04 for the third best rank and £0 for any other ranks. Lastly, in the payoff condition of Study 1, we programmed the reward payoff function to match participants' reward of 5 stars for the best rank, 3 stars for the second-best rank, one star for the third-best rank and zero stars for any other ranks.

Another feature added to our implementation of the Ideal Observer, compared to the Gilbert & Mosteller base model, is the ability to update the model's generating distribution from its experience with new samples in a Bayesian fashion, instead of this generating distribution being specified in advance and then fixed throughout the paradigm. This Bayesian version of the optimality model treats option values as samples from a Gaussian distribution with a normal-inverse- χ2 prior. The prior distribution is initialised before experiencing any options with four parameters: the prior mean μ 0, the degrees of freedom of the prior mean κ, the prior variance σ 2 0 and the degrees of freedom of the prior variance ν. The μ 0 and σ 2 0 parameters of this prior distribution are then updated by the model following presentation of each newly sampled option value as each sequence progresses.

---

### Inference-based decisions in a hidden state foraging task: differential contributions of prefrontal cortical areas [^5950d297]. Neuron (2020). Medium credibility.

Introduction

In natural foraging behaviors, animals must continually choose between trying to exploit resources at their current location and leaving to explore another, potentially superior one, at the expense of a possibly costly travel period. Viewed from the perspective of optimal decision-making, the crucial question is when is it best to leave the current site for another one? According to the marginal value theorem, in order to maximize returns, an optimal forager ought to leave its current site when the immediate rate of reward drops below the average rate. However, this elegant solution to the foraging problem only applies in deterministic environments, in which both immediate and average reward rates are knowable to the agent. In a more realistic scenario — for example, where rewards are encountered probabilistically — the immediate reward rate is ill-defined and the marginal value theorem does not apply.

---

### Statistical inference for noisy nonlinear ecological dynamic systems [^b61f25b4]. Nature (2010). Excellent credibility.

Chaotic ecological dynamic systems defy conventional statistical analysis. Systems with near-chaotic dynamics are little better. Such systems are almost invariably driven by endogenous dynamic processes plus demographic and environmental process noise, and are only observable with error. Their sensitivity to history means that minute changes in the driving noise realization, or the system parameters, will cause drastic changes in the system trajectory. This sensitivity is inherited and amplified by the joint probability density of the observable data and the process noise, rendering it useless as the basis for obtaining measures of statistical fit. Because the joint density is the basis for the fit measures used by all conventional statistical methods, this is a major theoretical shortcoming. The inability to make well-founded statistical inferences about biological dynamic models in the chaotic and near-chaotic regimes, other than on an ad hoc basis, leaves dynamic theory without the methods of quantitative validation that are essential tools in the rest of biological science. Here I show that this impasse can be resolved in a simple and general manner, using a method that requires only the ability to simulate the observed data on a system from the dynamic model about which inferences are required. The raw data series are reduced to phase-insensitive summary statistics, quantifying local dynamic structure and the distribution of observations. Simulation is used to obtain the mean and the covariance matrix of the statistics, given model parameters, allowing the construction of a 'synthetic likelihood' that assesses model fit. This likelihood can be explored using a straightforward Markov chain Monte Carlo sampler, but one further post-processing step returns pure likelihood-based inference. I apply the method to establish the dynamic nature of the fluctuations in Nicholson's classic blowfly experiments.

---

### Work-relatedness [^d363de6d]. Journal of Occupational and Environmental Medicine (2018). Medium credibility.

Work-relatedness — evaluating the evidence explains that for epidemiologic surveillance a "highly sensitive, relatively nonspecific case definition is frequently employed", which "may increase the rate of screening yield for study, but generally produces a high rate of false-positives", and that if surveillance suggests an association, "more formal research can be done" with a research-study definition that is "much more rigid, traditionally implying 95% confidence that the purported causal relationship is not statistically spurious", with "The methodology for inferring a causal association" provided in Table 1.

---

### ESICM / ESCMID task force on practical management of invasive candidiasis in critically ill patients [^551f3d5b]. Intensive Care Medicine (2019). High credibility.

Regarding medical management for invasive candidiasis, more specifically with respect to step-down therapy, ESCMID/ESICM 2019 guidelines recommend to discontinue antifungal therapy in patients with suspected (but not proven) invasive candidiasis with negative blood cultures and/or other negative culture specimens taken from suspected infectious foci before starting antifungal therapy.

---

### Using the number needed to treat in clinical practice [^646ae153]. Archives of Physical Medicine and Rehabilitation (2004). Low credibility.

The number needed to treat (NNT) is gaining attention as a method of reporting the results of clinical trails with dichotomous outcome measures. The NNT is defined as the number of patients who would need to be treated, on average, with a specific intervention to prevent 1 additional bad outcome or to achieve 1 desirable outcome in a given time period. Because it reports outcomes in terms of patient numbers, it is extremely useful to clinicians for making decisions about the effort expended with a particular intervention to achieve a single positive outcome. This special communication describes the NNT statistic and its utility for choosing clinical interventions.

---

### Cutting edge or blunt instrument: how to decide if a stepped wedge design is right for you [^471f52f2]. BMJ Quality & Safety (2021). High credibility.

When might I consider doing a stepped wedge trial?

Research designs are shaped as much by practical constraints as by abstract schemes, and it is always a good idea to start with the constraints and work towards a design, rather than start with a design and try to fit it to constraints. These constraints will be unique to each research context, and box 1 lists some areas to think about. Still, there are some common features of settings where a stepped wedge trial might be considered as a possible design, and we now review these.

Box 1
Practical constraints on the design of a longitudinal cluster randomised trial

Are there limits on the time available to complete the evaluation, on the number of clusters, or on the number of participants (or the rate at which you can recruit participants) at each cluster? These constraints put limits on the overall scale of the evaluation, or force trade-offs between different design characteristics.
How will participants and their data be sampled in your study: as a series of cross-sectional surveys, as a continuous stream of incident cases, as a cohort followed over time, or some other way? Does the timescale divide into cycles, seasons or milestones that influence how you will sample participants and data?
Is there a limit on how many clusters can implement the intervention at the same time in the evaluation? If this is constrained by research resources (eg, if there are only enough trained research staff to implement the intervention one cluster at a time) then implementation must be staggered in some way.
If implementation is to be staggered, is there a minimum 'step length'? If the same team delivers the intervention in different clusters at different steps, then bear in mind it may take some time to get the intervention fully operational at a site, and the team will also need time to relocate from one cluster to the next.

---

### Psychotic-like reasoning styles in patients with borderline personality disorder? An experimental investigation of the jumping to conclusions bias [^94a955c4]. Clinical Psychology & Psychotherapy (2025). Medium credibility.

2.3.3 Scoring of the Fish Tasks

The main outcome was draws to decision. Draws to decision is the number of fish the participant had seen before making their first decision on a lake. If the participant did not decide after the tenth fish, the score was set to 11. Thus, the possible score ranged from 1 to 11. As done previously (Van Dael et al.), we calculated the frequency of decisions after the first draw (draws to decision ≤ 1) or after no more than two draws (draws to decision ≤ 2). Furthermore, we calculated the decision threshold, defined as the probability rating the participant gave for Lake A when they first decided that the fisherman was fishing from Lake A. Finally, the fourth and the ninth fish presented disconfirmatory evidence for lake A, which allowed us to calculate an overcorrection measure, as described by Garety et al. We subtracted the probability estimate after Fish 3 from the probability estimate after Fish 4 and averaged it with the same subtraction from the probability estimate after Fish 9 compared to the estimate after Fish 8.

2.3.4 Post‐Questionnaire About the Fish Task

After the task, participants filled out a paper‐and‐pencil questionnaire with the following questions: (1) Did you understand the task (responses: (a) yes, (b) no, (c) unsure); (2) What do you think was expected from you in the task (multiple responses possible: (a) determination, making a decision after as few fish as possible, (b) caution, meaning to wait for many fish before making a decision, (c) accuracy, meaning to make the right decision any way possible, (d) speed, meaning to give a fast response, (e) I cannot say).

---

### Clinical practice guideline: tympanostomy tubes in children (update) [^21ed39bb]. Otolaryngology — Head and Neck Surgery (2022). High credibility.

Regarding diagnostic investigations for otitis media with effusion, more specifically with respect to clinical assessment, AAO-HNS 2022 guidelines recommend to assess whether a pediatric patient with recurrent OME of any duration is at increased risk for speech, language, or learning problems from otitis media because of baseline sensory, physical, cognitive, or behavioral factors.

---

### Global strategy for asthma management and prevention [^da0a3408]. GINA (2024). High credibility.

Asthma exacerbation follow-up and referral — after an exacerbation, the written asthma action plan should be reviewed, and maintenance asthma treatment can generally be reduced to previous levels 2–4 weeks after the exacerbation unless the history suggests long-term poorly controlled asthma, in which case a step up in treatment may be indicated. Patients with more than 1–2 exacerbations per year despite Step 4–5 therapy (or Step 4 therapy in children 6–11 years), or with several emergency department visits, should be referred to a specialist center, if available, for assessment and strategies to reduce their risk of future exacerbations and their risk of exposure to OCS.

---

### Standards of care in diabetes – 2025 [^5c059b62]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for hypertension, more specifically with respect to patients with diabetes mellitus, lifestyle modifications, ADA 2025 guidelines recommend to advise lifestyle interventions in patients with BP > 120/80 mmHg, consisting of weight loss when indicated, a DASH-style eating pattern (including reducing sodium and increasing potassium intake), moderation of alcohol intake, smoking cessation, and increased physical activity.

---

### Degenerate boundaries for multiple-alternative decisions [^4c0563ef]. Nature Communications (2022). High credibility.

At the end of a decision episode, when a choice is made, the decision time and error are combined into a single reward. Here, we formulate reward as a linear combination of error and decision time weighted by their associated costs W i and c :This is a standard reward function used in a wide range of past work, for example, ref. Unequal error costs W i ≠ W j induce choice-dependent reward, where hypothesis-dependent error costs are relative to the "true" hypothesis and to each other. For tractability, we will assume all error costs are equal, with the expectation that similar results hold in the unequal cost case but that the analysis will be more complicated. We also consider a constant (time-independent) cost c per time step, assuming stationarity of evidence distributions and evidence accumulation in a free-response task. A challenging aspect of this framework is that reward is highly stochastic due to the random nature of evidence sampling. How then do we define optimality?

In this paper, we come from the view that humans and animals maximize expected reward. Then the optimum decision boundary maximizes the average reward for a given ratio of costs c / W. Monte Carlo simulations of decision trajectories of independent trials, using the formalism outlined above and the evidence inference method derived in the next section, yield reward values for a set of candidate decision boundaries. In general, we find a set of high-dimensional nonlinear, complex boundaries. We will show that these boundaries are consistent with a range of behavioral and phenomenological results along with testable neurophysiological predictions.

---

### Standards of care in diabetes – 2025 [^5eb43e21]. Diabetes Care (2025). High credibility.

Regarding follow-up and surveillance for diabetes mellitus type 1, more specifically with respect to continuous glucose monitoring, ADA 2025 guidelines recommend to implement early continuous glucose monitoring in adult patients with T1DM to improve glycemic outcomes and QoL and minimize hypoglycemia.

---

### Richard kryscio… [^19ef570f]. FDA (2025). Medium credibility.

7. Kryscio, RJ. The transition probabilities of the general stochastic epidemic model. Journal of Applied Probability, 12: 415–425, 1974.
8. Kryscio, RJ and Severo, NC. Computational and estimation procedures in multidimensional right-shift processes and applications. Advances in Applied Probability, 7: 349–382, 1975. Kryscio 3
16. Saunders, R, Kryscio, RJ and Funk, GM. Limiting results for arrays of binary random variables under sparseness conditions. Journal of Applied Probability, 16: 554–556, 1979.
17. Kryscio, RJ, Saunders, R and Funk, GM. Normal approximations for binary lattice systems. Journal of. Applied Probability, 17: 674–685, 1980.
18. Saunders, R, Kryscio, RJ and Funk, GM. Poisson limits for a hard-core clustering model. Stochastic Processes and Their Applications, 12: 97–106, 1982. objective assessment of prominence in renal transplant rejection. Radiology, 149: 787–791, 1983.
21. Kryscio, RJ and Saunders, R. On interpoint distances for planar Poisson cluster processes.

Journal of Applied Probability, 20: 513–528, 1983.
22. Griffith, W, Kryscio, RJ and Purdue, P. A stochastic model of particle shattering. Advances in Applied Probability, 16: 70–87, 1984.
23. Woodring, JH, Loh, FK and Kryscio, RJ. An evaluation of the plain radiographic manifestations of. Kryscio 8 91. Peterson, P. M. and Kryscio, R. J. Edgeworth approximations of null and alternative probabilities for quadratic epidemiologic clustering statistics. Proceedings of Section on Epidemiology, Joint Statistical Meetings, Chicago, 1996.
92. Noland, M. P. Kryscio, R. J. Hinkle, J. Riggs, R. Linville, L. H. Ford, V. Y., and Tucker, T. C.

---

### Chunking as the result of an efficiency computation trade-off [^6eeb4267]. Nature Communications (2016). Medium credibility.

To fit minimum-jerk models to data, for each trial, we inferred chunk boundaries from local minima of speed profiles (see above). Given these chunk boundaries, we fit a minimum-jerk trajectory for each chunk. We then assessed the goodness of fit of the model using the Pearson's correlation coefficient between speed trajectories of the model and the data.

Calculating the cost-effectiveness of learning paths

We contend that based on the known ways in which organisms improve efficiency over time, moving along the Pareto frontier in the efficiency–computation space produces the least cumulative cost of computation over the course of learning. To demonstrate this, we performed a simulation.

First, we parameterized the efficiency improvements with time. It is widely known that organisms improve efficiency exponential over time. Let us assume that this exponential improvement starts from zero efficiency and ends at unity efficiency from time zero to time unity following the form:

where 0 < C < ½ produces exponential improvements. Further, let us assume that the efficiency–computation space can be traversed using a similar exponential form, where complexity depends on efficiency as follows:

where C e defines whether the curve would follow along the Pareto frontier (0 < C e < ½) or away from it (C e > ½). We can define the cumulative complexity over time as

Assuming an exponential improvement in efficiency over time and using numerical integration, the cumulative complexity of learning is minimized when the efficiency–computation space is traversed close to the Pareto frontier (Fig. 4).

Efficiency changes for fixed computational complexity

To examine how efficiency improves when computational complexity does not change, we defined sets of degenerate chunk structures that have the same complexity (Fig. 6a). We selected the eight most frequently observed sets that covered > 70% of all analysed trials, for further analysis. For these eight sets, we fit a hierarchical exponential mixed effects model with the fixed effect capturing squared jerk as a function of repeats and the random effect capturing the differences across the eight degenerate sets. We then tested the fixed effect for statistical significance using a z -test on the exponential parameter b of the exponential model:

---

### Observation of spin-space quantum transport induced by an atomic quantum point contact [^ef5eaa3f]. Nature Communications (2021). High credibility.

We theoretically estimate the conductance from the numerical calculation of the transport dynamics, shown as a dashed line in Fig. 2c. In the calculation, we take into account the spatial inhomogeneity of the atom numbers among many tubes, and the total atom number difference Δ N tot is expressed aswhere i means the index of the tube, and Δ N i and μ i ± correspond to the atom number difference and the chemical potentials for thestates in the i th tube, respectively. As a result, we obtain G = 0.41/ h, which is consistent with the measured value (see Supplementary Fig. 3).

Figure 2e shows the impurity-fraction dependence of the spin-flip rate, revealing that the spin-flip rate is proportional to ρ imp = N imp (t = 0)/ N. This is consistent with our expectation that each impurity atom serves as a single-mode QPC and the overall transport current should be proportional to the number of impurity atoms. Based on this linear dependence, the single-impurity conductance is estimated as G 0 = 4.1 × 10 −2 / h. By increasing the sensitivity of our experiment, the low impurity-fraction limit of this measurement will reveal the conductance discretized in units of G 0, namely in the form of N imp G 0, expected from the Landauer-Büttiker formula.

---

### Colorectal cancer screening and prevention [^390b568a]. American Family Physician (2025). High credibility.

Regarding screening and diagnosis for colon cancer, more specifically with respect to indications for screening, high-risk individuals, family history, AAFP 2025 guidelines recommend to obtain CRC screening in patients with ≥ 1 first-degree relatives with CRC or adenomatous polyps, starting at 40 years of age or 10 years before the age of the youngest relative at the time of their diagnosis.

---

### Changes in semantic memory structure support successful problem-solving and analogical transfer [^5231072f]. Communications Psychology (2024). Medium credibility.

Introduction

In our daily life, we constantly deal with problems, ranging from the most mundane (e.g. what to cook for dinner given the ingredients at our disposal), to professional activities (e.g. how to reorganize our current plans to meet a new deadline), up to major societal challenges (e.g. how to find innovative solutions against global warming). How do we find new solutions to problems? While the ability to solve problems is a critical skill for adapting to new situations and innovating, the mechanisms underlying the problem-solving process remain largely unknown.

Among the new problems we face each day, some are well-defined (e.g. playing a jigsaw puzzle). The initial state (i.e. the number of independent pieces) and goal state (i.e. assembling the pieces so it looks like the picture model) are clear, and the solver can apply a set of operations (i.e. interlocking the pieces as a function of their shape) to reach the goal. However, for many of our problems (e.g. organizing work activities during the COVID-19 pandemic), the problem space is ambiguous. No heuristics or existing rules could be applied to transform the initial state into the goal state. Such "ill-defined" problemsthus require additional mental processes, which have been tightly linked to creative thinking –. Ill-defined problem-solving (or creative problem-solving) is often referred to as insight solving, where the solution comes to mind suddenly and effortlessly, with a "Eureka" phenomenon –. According to the Representational Change Theory, solving such problems involves restructuring the initial problem mental representational space, which presumably entails combining elements related to the problem in a new way. In theory, restructuring allows one to change perspective, reframe the problem, or escape its implicitly imposed constraints, leading to creative associations. For instance, consider the following problem: "A man walks into a bar and asks for a glass of water. The bartender points a shotgun at the man. The man says, 'Thank you', and walks out". The problem is ill-defined because the path to finding the solution is to be discovered, and the goal state is vague. Solving this problem first requires asking the right question: in which context would a shotgun and a glass of water help somebody? Rather than relying on obvious associations (e.g. a glass of water is related to thirst), solvers must fill the missing link between the relevant elements of the problem (a shotgun induces fear, and fear can be a remedy for hiccups, as can drinking a glass of water). Hence, restructuring the initial representation of a given problem would allow one to see this link and find its solution.

---

### Significance testing as perverse probabilistic reasoning [^7f917c9a]. BMC Medicine (2011). Low credibility.

Angle 3: P-values from ROC curves

To take a third angle, we consider an alternative definition for the P -value. The P -value is

the minimum false positive rate (Type I error rate) at which the NHSTP will reject the null hypothesis.

Though not obvious at first glance, this definition is mathematically equivalent to our previous definition of the P -value as the probability of a result at least as extreme as the one we observe. The effort required to see why this is the case affords additional insight into the nature of P -values.

Let us step back and consider the null hypothesis testing procedure from an abstract point of view. The NHSTP is one instance of threshold-decision procedure, that is, a procedure that chooses between two alternatives by comparing a test statistic computed from the data T (D) with a threshold γ (in the case of the NHSTP, the statistic is the P -value, and the threshold is the significance level α). The procedure declares one result when the test statistic is less than or equal to threshold, and the alternative result when the threshold is exceeded. Identifying one of the alternatives as 'positive' and the other as 'negative', in general any such threshold-based decision procedure must have a certain false positive and false negative rate, determined by the chosen threshold. More explicitly, let us denote the positive and negative alternatives as H 1 and H 0, respectively, and declare a positive result whenever T (D) ≤ γ, or a negative result whenever T (D) > γ. A false positive then occurs if T (D) ≤ γ when in fact H 0 is true, and the probability of this event is denoted FPR (γ) = Pr (T (D) ≤ γ|H 0). Similarly, a true positive result occurs if T (D) ≤ γ when H 1 is true, and the probability of this event is denoted TPR (γ) = Pr (T (D) ≤ γ|H 1). If we allow the threshold to vary, we can generate a curve of the false positive rate versus the false negative rate; such a curve is called a ROC curve. To make this discussion concrete, let us return to our coin flipping example. In that case, we set the 'positive' alternative to H 1 = 'the coin is biased' (that is Pr (Heads|H 1) ≠1/2), and set the negative alternative to H 0 = 'the coin is fair' (Pr (Heads|H 0) = 1/2). Setting the test statistic as before to T (D) = d = |1/2 - k / n |, we then have:

---

### Brief resolved unexplained events (formerly apparent life-threatening events) and evaluation of lower-risk infants [^7e69974f]. Pediatrics (2016). Medium credibility.

Guideline definitions for key action statements (Table 4) — a strong recommendation is defined as "A particular action is favored because anticipated benefits clearly exceed harms (or vice versa) and quality of evidence is excellent or unobtainable", with the implication "Clinicians should follow a strong recommendation unless a clear and compelling rationale for an alternative approach is present". A moderate recommendation is defined as "A particular action is favored because anticipated benefits clearly exceed harms (or vice versa) and the quality of evidence is good but not excellent (or is unobtainable)", implying "Clinicians would be prudent to follow a moderate recommendation but should remain alert to new information and sensitive to patient preferences". A weak recommendation (based on low-quality evidence) is defined as "A particular action is favored because anticipated benefits clearly exceed harms (or vice versa), but the quality of evidence is weak", with the implication "Clinicians would be prudent to follow a weak recommendation but should remain alert to new information and very sensitive to patient preferences". A weak recommendation (based on balance of benefits and harms) is defined as "Weak recommendation is provided when the aggregate database shows evidence of both benefit and harm that appear to be similar in magnitude for any available courses of action", implying "Clinicians should consider the options in their decision-making, but patient preference may have a substantial role".

---

### 2022 ESC guidelines for the management of patients with ventricular arrhythmias and the prevention of sudden cardiac death [^9a594a67]. European Heart Journal (2022). High credibility.

Regarding medical management for myocarditis, more specifically with respect to management of cardiac arrhythmias, catheter ablation, ESC 2022 guidelines recommend to consider performing catheter ablation as an alternative to ICD therapy, after discussion with the patient and provided that established endpoints have been reached, in patients with hemodynamically well-tolerated sustained monomorphic VT occurring in the chronic phase of myocarditis, preserved LV function, and a limited scar amenable to ablation.

---

### Standards of care in diabetes – 2025 [^27c0974d]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for diabetes mellitus type 1, more specifically with respect to pediatric patients (physical activity), ADA 2025 guidelines recommend to advise physical activity in all young patients with T1DM with the goal of 60 minutes of moderate- to vigorous-intensity aerobic activity daily, with vigorous muscle-strengthening and bone-strengthening activities at least 3 days per week.

---

### Pharmacological management of osteoporosis in postmenopausal women: an endocrine society guideline update [^8e1830e4]. The Journal of Clinical Endocrinology and Metabolism (2020). High credibility.

Regarding medical management for postmenopausal osteoporosis, more specifically with respect to antiresorptive and anabolic therapy, PTH analogs, ES 2020 guidelines recommend to initiate teriparatide or abaloparatide for up to 2 years to reduce the risk of vertebral and non-vertebral fractures in postmenopausal patients with osteoporosis at very high risk of fractures, such as severe or multiple vertebral fractures. (1, Moderate) Switch to an antiresorptive therapy to maintain bone density gains in postmenopausal patients with osteoporosis completing a course of teriparatide or abaloparatide.

---

### Biased expectations about future choice options predict sequential economic decisions [^85881870]. Communications Psychology (2024). Medium credibility.

Study 2

The two "full" conditions we investigated so far - Pilot full and the Study 1 full - showed that an optimal stopping task in which all choices are rewarded according to their value leads to (partially) reduced Ideal Observer sampling, compared to a raft of different conditions in which only the top three ranking choices were rewarded. Participants, in contrast, maintained relatively low and invariant sampling rates across all conditions. Consequently, the two full conditions (where there was not clear undersampling) created a situation where participants' and Ideal Observer sampling rates were close to each other, making a direct assessment of bias in this condition difficult to determine with high precision. Indeed, Pilot full and Study 1 full conflict in the biases they show relative to the objective and subjective values versions of the Ideal Observer. In Study 2 we obtained a higher quality estimate of participant sampling bias in the full condition by overcoming some limitations of our previous designs. We increased the target sample size from ~50 (in Pilot full and Study 1 full) to 151 in Study 2 full. Additionally, we generated a new set of sequence option values for every participant, whereas in Pilot full and Study 1 full, all participants engaged with sequences that were fixed in advance. These design elements also provide largest dataset for theoretical model fitting yet.

Participants appeared to sample about as many prices in Study 2 as in the previous studies reported herein (Fig. 2). Figure 6 and Supplementary Table 1 report Bayesian pairwise mean difference test results comparing participants' sampling to that of the two versions of the Ideal Observer (BF 01 > 3, at least moderate evidence for null model). Here, we see that the results resemble those of Study 1 full. Participants sampled statistically equivalently to the subjective values version of the Ideal Observer (this comparison was inconclusive in the lessor-powered Study 1 full) and undersampled compared to the objective values version (replicating Study 1 full). Model fitting in Study 2 unambiguously favoured both subjective and objective values versions of the Biased Prior model on both measures of mean BIC (Fig. 6, second row, and Table 6) and frequency of best-fitted participants (Fig. 6, third row).

---

### American College of Gastroenterology guidelines update: diagnosis and management of celiac disease [^2f907187]. The American Journal of Gastroenterology (2023). High credibility.

Celiac disease gluten challenge steps within the diagnostic algorithm include specific dosing and timing: The algorithm specifies 3 g gluten daily for 2 weeks, then 3 g gluten daily for up to 6 additional weeks, with repeat serology at end of challenge and repeat serology 2 to 6 weeks after end of challenge.

---

### Effective patient-physician communication [^a3fd44f2]. ACOG (2025). High credibility.

Best practices for communication in medical encounters — Enabling disease- and treatment- related behavior includes roles and responsibilities to Assess patient's interest in and capacity for self-management, Provide advice (information needs, coping skills, strategies for success), Agree on next steps, Assist patient to optimize autonomy and self-management of his or her problem, Arrange for needed support, and Advocate for, and assist patient with, health system; skills include Assess patient's readiness to change health behaviors and Elicit patient's goals, ideas, and decisions.

---

### A problem of persistence: still more questions than answers? [^1078e4fd]. Nature Reviews: Microbiology (2013). Medium credibility.

The current antibiotic resistance crisis has led to increased pressure to prioritize strategies to tackle the issue, with a strong focus being placed on the development of novel antimicrobials. However, one major obstacle that is often overlooked is persister cells, which are refractory to antibiotic treatment. Tackling persistence is a challenge because these cell types are extremely difficult to study and, consequently, little is known about their physiology and the factors that lead to their emergence. Here, four experts contemplate the main physiological features that define persistence and the implications of persistence for antibiotic treatment regimens, and consider what the study of bacterial persistence has taught us about the heterogeneity of bacterial populations.

---

### Standards of care in diabetes – 2025 [^1e25a6d5]. Diabetes Care (2025). High credibility.

Regarding nonpharmacologic interventions for diabetes mellitus type 1, more specifically with respect to physical activity, ADA 2025 guidelines recommend to advise most adult patients T1DM to engage in ≥ 150 minutes of moderate- to vigorous-intensity aerobic activity per week, spread over at least 3 days/week, with no more than 2 consecutive days without activity. Consider advising shorter durations (minimum 75 min/week) of vigorous-intensity or interval training for more physically fit patients.

---

### World guidelines for falls prevention and management for older adults: a global initiative [^ab6781a7]. Age and Ageing (2022). High credibility.

Regarding classification and risk stratification for falls in the elderly, more specifically with respect to risk prediction, general principles, WFG 2022 guidelines recommend to enquire about the goals and priorities, attitudes to activities, independence and risk, and willingness and capability as part of a multifactorial falls risk assessment in older adults, to inform decision-making on potential interventions.

---

### 2023 ESC guidelines for the management of cardiovascular disease in patients with diabetes [^24784e3d]. European Heart Journal (2023). High credibility.

Regarding medical management for diabetes mellitus type 2, more specifically with respect to prevention of CVD, glucose-lowering medications, ESC 2023 guidelines recommend to consider initiating metformin to reduce cardiovascular risk in patients with T2DM without ASCVD or severe target-organ damage at low, moderate,
high, or very high risk.

---

### Diagnosis and evaluation of prognosis of myelofibrosis: a British Society for Haematology guideline [^50b06971]. British Journal of Haematology (2024). High credibility.

Regarding diagnostic investigations for primary myelofibrosis, more specifically with respect to initial evaluation, BSH 2024 guidelines recommend to obtain an individualized risk assessment of thrombosis in all patients, particularly with the JAK2 V617F mutation.

---

### Standards of care in diabetes – 2025 [^14753c0e]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for chronic kidney disease, more specifically with respect to patients with diabetes mellitus (monitoring of renal function), ADA 2025 guidelines recommend to monitor urinary albumin (such as spot urinary albumin-to-creatinine ratio) and eGFR 1–4 times per year, depending on the stage of the disease, in patients with established diabetic kidney disease.

---

### 2025 ESC guidelines for the management of myocarditis and pericarditis [^230fe03b]. European Heart Journal (2025). High credibility.

Regarding specific circumstances for acute pericarditis, more specifically with respect to patients with post-cardiac injury syndromes, ESC 2025 guidelines recommend to initiate IL-1 antagonists in patients with refractory post-cardiotomy inflammatory syndrome to prevent recurrences and progression to constriction.

---

### 2025 ESC guidelines for the management of myocarditis and pericarditis [^b050a21b]. European Heart Journal (2025). High credibility.

Regarding nonpharmacologic interventions for myocarditis, more specifically with respect to physical activity restrictions, ESC 2025 guidelines recommend to restrict physical exercise until remission for at least 1 month in athletes and non-athletes after inflammatory myopericardial syndrome, using an individualized approach to accelerate recovery.

---

### Biased expectations about future choice options predict sequential economic decisions [^49921c9c]. Communications Psychology (2024). Medium credibility.

Here, we set the prior values of μ and σ 2 in two possible ways: objective value and subjective values versions. In some previous studies of optimal stopping for price decisions, the mean and variance of the generating distribution has been fixed in advance by the mean and variance of the distribution of objective prices. We implemented an objective values version of the Ideal Observer in this way for all the study conditions reported herein. This objective values procedure for the Ideal Observer assumes that the raw prices can be treated as a proxy for participants' subjective value of the prices, so an Ideal Observer that optimises only the raw prices when making decisions would therefore be an appropriate basis for comparison with participants. However, we also had direct access to participants' subjective values of options in some conditions (Pilot full, Study 1 full condition, Study 1 ratings condition, Study 2 and both sequence length conditions of Study 3), due to the presence of the initial rating phase, and so we could also build a subjective values version of the Ideal Observer. This second way of computing the Ideal Observer assumes that participants' subjective valuation of prices may not necessarily exactly equal the raw price values, especially in their scaling, which may be relevant to full information problems. We used each participants' individualised ratings (subjective valuations) of the prices as option values input to the subjective values version of the Ideal Observer, and we used the mean and variance of individual participants' ratings distributions when initialising the prior of the generating distribution of the Ideal Observer.

Because conditions with an initial rating phase had objective and subjective values versions of the Ideal Observer, with each version providing separate optimality estimates, we were able to test the hypotheses that the use of objective or subjective values when modelling (a) affects the strategy taken by the optimality model and (b) changes the assessment of participant bias. We ensured for both objective values and subjective values versions of the models that better options were always more positively-valued such that the models were always solving a maximisation problem. We further ensured that estimated parameters for both objective values and subjective values versions of the models would be on the same scales by reflecting the objective prices around their mean. Then we rescaled those values to span 1 (the highest/worst price) to 100 (the best price). These reflected and rescaled objective values were then used in objective values models when computing the prior generating distribution, and when inputting price values to the model as option values. Subjective values were already rated by participants on this same 1 to 100 scale.

---

### How can we improve the interpretation of systematic reviews? [^140240bd]. BMC Medicine (2011). Low credibility.

Abbreviations

AMSTAR: Assessment of Multiple Systematic Reviews; GRADE: Grading of Recommendations Assessment, Development, and Evaluation; PRISMA: Preferred Items for Systematic Reviews and Meta-analyses; SR: systematic review

---

### Standards of care in diabetes – 2025 [^cb4a5970]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for diabetes mellitus type 1, more specifically with respect to general principles, ADA 2025 guidelines recommend to obtain a complete medical evaluation at the initial visit and follow-up, as appropriate, to:

- confirm the diagnosis and classify diabetes

- ssess glycemic status and previous treatment

- evaluate for diabetes complications, potential comorbid conditions, and overall health status

- identify care partners and support system

- assess social determinants of health and structural barriers to optimal health and health care

- review risk factor management in the patient with diabetes

- begin engagement with the patient with diabetes in the formulation of a care management plan including initial goals of care

- develop a plan for continuing care.

---

### Standards of care in diabetes – 2025 [^9fec4de9]. Diabetes Care (2025). High credibility.

Regarding follow-up and surveillance for diabetes mellitus type 1, more specifically with respect to continuous glucose monitoring, ADA 2025 guidelines recommend to assess skin reactions, either due to irritation or allergy, and address to aid in successful use of devices.

---

### Standards of care in diabetes – 2025 [^9a2ec52c]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for hypoglycemia, more specifically with respect to elderly patients, ADA 2025 guidelines recommend to individualized goal levels for the treatment of hypertension in most older patients.

---

### On the concepts, methods, and use of "Probability of success" for drug development decision-making: a scoping review [^68d4465f]. Clinical Pharmacology and Therapeutics (2025). Medium credibility.

Spiegelhalter and Freedmanextended the concept of power to what they referred to as average power or predictive power. The first application of average power was in the context of sample size determination for a clinical trial; rather than defining the null hypothesis as no difference between the two treatment arms, the authors argued that it was more meaningful to define the null as the "minimum clinically worthwhile difference". Because the definition of the null hypothesis incorporated the typical definition of the alternative hypothesis in the power calculation, Spiegelhalter and Freedmanneeded to take a novel approach for defining the size of the treatment effect. Their solution was to incorporate a probability distribution of belief about the possible benefits of the new treatment into the power calculation and then to average over this distribution to determine the sample size. Making links to Bayesian methods, the authors recognized this probability distribution as a prior distribution, reflecting the independent external information available so far at the time of trial design.

Around the same time, Spiegelhalter et al.also highlighted the importance of considering the predictive power instead of the conditional power on a fixed treatment effect when evaluating interim results of an ongoing clinical trial. This approach has been used widely since then. Dallow and Finapointed out characteristics of predictive power when it is used for sample size re‐calculation at interim. Rufibach et al.investigated the concepts of PoS and predictive power, in the case of using Phase II data and/or interim data in calculation of PoS for time‐to‐event outcomes. The use of predictive power in interim analysis is out of scope for this review.

A decade after the publications by Spiegelhalter and Freedman, Spiegelhalter et al.and O'Hagan et al.also noted that the standard definition of power is dependent on assumptions of the unknown true value of the treatment effect and introduced the concept of assurance as the "unconditional probability that the trial will demonstrate a positive outcome". Assurance adopts the same methods as Spiegelhalter and Freedman'saverage power approach and incorporates uncertainty through the specification of a prior distribution for the unknown population parameters. Although the approaches of Spiegelhalter and Freedmanand O'Hagan et al.are essentially the same, O'Hagan et al. made a clear link to the use of assurance methods to support the decision to proceed to a phase III confirmatory trial based on the available information.

---

### Ocrelizumab (Ocrevus) [^0933df30]. FDA (2025). Medium credibility.

2.3	Recommended Dosage and Dose Administration

Administer OCREVUS under the close supervision of an experienced healthcare professional with access to appropriate medical support to manage severe reactions such as serious infusion reactions.

Initial dose: 300 mg intravenous infusion, followed two weeks later by a second 300 mg intravenous infusion.
Subsequent doses: single 600 mg intravenous infusion every 6 months.
Observe the patient for at least one hour after the completion of the infusion [see Warnings and Precautions (5.1)].

2.4	Delayed or Missed Doses

If a planned infusion of OCREVUS is missed, administer OCREVUS as soon as possible; do not wait until the next scheduled dose. Reset the dose schedule to administer the next sequential dose 6 months after the missed dose is administered. Doses of OCREVUS must be separated by at least 5 months [see Dosage and Administration (2.3)].

2.5	Dose Modifications Because of Infusion Reactions

Dose modifications in response to infusion reactions depends on the severity.

Life-threatening Infusion Reactions

Immediately stop and permanently discontinue OCREVUS if there are signs of a life-threatening or disabling infusion reaction [see Warnings and Precautions (5.1)]. Provide appropriate supportive treatment.

Severe Infusion Reactions

Immediately interrupt the infusion and administer appropriate supportive treatment, as necessary [see Warnings and Precautions (5.1)]. Restart the infusion only after all symptoms have resolved. When restarting, begin at half of the infusion rate at the time of onset of the infusion reaction. If this rate is tolerated, increase the rate as described in Table 1 [see Dosage and Administration (2.3)]. This change in rate will increase the total duration of the infusion but not the total dose.

Mild to Moderate Infusion Reactions

Reduce the infusion rate to half the rate at the onset of the infusion reaction and maintain the reduced rate for at least 30 minutes [see Warnings and Precautions (5.1)]. If this rate is tolerated, increase the rate as described in Table 1 [see Dosage and Administration (2.3)]. This change in rate will increase the total duration of the infusion but not the total dose.

---

### Consideration of disease penetrance in the selection of secondary findings gene-disease pairs: a policy statement of the American College of Medical Genetics and genomics (ACMG) [^32c522e2]. Genetics in Medicine (2024). High credibility.

Genetic terminology — definitions and evolving usage of penetrance and expressivity note that Genetics in Medicine defines penetrance as "the probability that an allele or alleles will have any phenotypic expression at all" and expressivity as "the severity of expression of that phenotype among individuals with the same disease-causing genotype", yet their understanding and use are described as inconsistent and sometimes interchangeable; historically, "Penetrance" and "expressivity" were originally coined by Oskar Vogt in 1926, and Ott later defined penetrance as "the conditional probability R(x) that an individual with a given genotype x expresses the phenotype x as measured within a pedigree; this definition remains the standard"; the page adds that rapid accumulation of sequencing data "has even further blurred these definitions", enabling genotype-first cohorts and calculation of "population-based" penetrance, while the same term is used for both gene-disease relationships and individual variants, leaving a tension between allele-level concepts in family data and gene-level properties in population data.

---

### Diabetes technology-continuous subcutaneous insulin infusion therapy and continuous glucose monitoring in adults: an endocrine society clinical practice guideline [^a3dbec73]. The Journal of Clinical Endocrinology and Metabolism (2016). Medium credibility.

Regarding follow-up and surveillance for diabetes mellitus type 1, more specifically with respect to continuous glucose monitoring, ES 2016 guidelines recommend to provide education, training, and ongoing support to help achieve and maintain individualized glycemic goals in adult patients with T1DM using continuous subcutaneous insulin infusion and continuous glucose monitoring.

---

### Standards of care in diabetes – 2025 [^2a4976a0]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for chronic kidney disease, more specifically with respect to patients with diabetes mellitus, renin-angiotensin system inhibitors, ADA 2025 guidelines recommend to monitor for increased serum creatinine and for increased serum potassium levels when ACEis, ARBs, or mineralocorticoid receptor antagonists are used, or for hypokalemia when diuretics are used at routine visits and 7–14 days after initiation or after a dose change.

---

### Venous thromboembolism: diagnosis and treatment [^bb2a0bb8]. American Family Physician (2025). High credibility.

Regarding classification and risk stratification for pulmonary embolism, more specifically with respect to risk stratification, AAFP 2025 guidelines recommend to use validated clinical prediction rules to estimate the pretest probability of VTE and guide further evaluation.

---

### World guidelines for falls prevention and management for older adults: a global initiative [^d9fd3aa2]. Age and Ageing (2022). High credibility.

Regarding classification and risk stratification for falls in the elderly, more specifically with respect to risk prediction, general principles, WFG 2022 guidelines recommend to enquire about the perceptions the older adult holds about falls, their causes, future risk, and how they can be prevented as part of a multifactorial falls risk assessment.

---

### Guidance for incorporating FDA processes into the ACC / AHA clinical practice guideline methodology: a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^d987ecf5]. Journal of the American College of Cardiology (2025). High credibility.

Initial combination therapy versus monotherapy — In adults with stage 1 hypertension (SBP 130–139 mm Hg and DBP 80–89 mm Hg), initiation of antihypertensive drug therapy with a single first-line antihypertensive drug is reasonable, with dose titration and sequential addition of other agents as needed to achieve BP control.

---

### Standards of care in diabetes – 2025 [^d89192ca]. Diabetes Care (2025). High credibility.

Regarding nonpharmacologic interventions for diabetes mellitus type 1, more specifically with respect to physical activity, ADA 2025 guidelines recommend to assess baseline physical activity and time spent in sedentary behavior (quiet sitting, lying, and leaning) in all patients with diabetes. Encourage increasing physical activities (such as walking, yoga, housework, gardening, swimming, and dancing) above baseline in patients not meeting activity guidelines.

---

### Standards of care in diabetes – 2025 [^39f7c162]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for diabetes mellitus type 1, more specifically with respect to screening for diabetic foot, ADA 2025 guidelines recommend to elicit a history of ulceration, amputation, Charcot foot, angioplasty or vascular surgery, cigarette smoking, retinopathy, and renal disease and assess current symptoms of neuropathy (pain, burning, numbness) and vascular disease (leg fatigue, claudication).

---

### Evaluating and understanding articles about treatment [^380d1821]. American Family Physician (2009). Low credibility.

Each year physicians must decide which of the thousands of newly published articles they will take time to read. To determine which articles are the most clinically useful, physicians should assess their relevance, validity, and clinical importance. Using these criteria can drastically decrease the number of articles physicians need to read.

---

### Promoting optimal development: screening for mental health, emotional, and behavioral problems: clinical report [^d13b5812]. Pediatrics (2025). High credibility.

Follow-up planning — once a decision on next steps has been reached in partnership with a family, it is important to ensure an appropriate follow-up plan, which will depend on the nature and severity of the problem, management plan implemented, and patient and family response to the plan.

---

### VA / DoD clinical practice guideline for management of bipolar disorder [^c1a5f23a]. DoD/VA (2023). High credibility.

Behavioral Family Therapy (BFT) — focus and duration: Behavioral Family Therapy is for families with greater needs, sessions are focused on family education, communication skills training, and problem-solving skills training, and BFT generally lasts 6–9 months and can be conducted in single-family or multi-family formats.

---

### Efficacy and safety of anakinra plus standard of care for patients with severe COVID-19: a randomized phase 2 / 3 clinical trial [^2357ce84]. JAMA Network Open (2023). High credibility.

Conclusions

The findings of this randomized clinical trial suggest that the use of anakinra together with SoC does not prevent the need for mechanical ventilation or reduce mortality risk compared with SoC alone in hospitalized patients with severe COVID-19 pneumonia and hyperinflammation. Although the primary and key secondary outcomes were not met, anakinra may have a role as an early treatment for patients with less-severe disease and inflammation.

---

### Global strategy for asthma management and prevention [^66b4d55d]. GINA (2024). High credibility.

Asthma strategy implementation — essential elements required to implement a health-related strategy enumerate "Steps in implementing an asthma strategy into a health system" that include "Develop a multidisciplinary working group", "Assess the current status of asthma care delivery, outcomes e.g., exacerbations, admissions, deaths, care gaps and current needs", "Select the material to be implemented, agree on main goals, identify key recommendations for diagnosis and treatment, and adapt them to the local context or environment", "In treatment recommendations, consider environmental issues (planetary health) in addition to patient health", "Identify barriers to, and facilitators of, implementation", and "Select an implementation framework and its component strategies". The plan continues with "Develop a step-by-step implementation plan:" to "Select target populations and evaluable outcomes, and specify data coding requirements (if relevant)", "Identify local resources to support implementation", "Set timelines", "Distribute tasks to members", "Evaluate outcomes", and "Continually review progress and results to determine if the strategy requires modification".

---

### Summary benchmarks-full set – 2024 [^6f68d3ad]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — use and ethics note states that "The PPPs are intended to serve as guides in patient care, with greatest emphasis on technical aspects", that "true medical excellence is achieved only when skills are applied in a such a manner that the patients' needs are the foremost consideration", and that "The Academy is available to assist members in resolving ethical dilemmas that arise in the course of practice".

---

### Selecting appropriate toys for young children in the digital era [^dd9f979e]. Pediatrics (2019). High credibility.

Toys and child development — general considerations states that "Toys are important in early child development in relation to their facilitation of cognitive development, language interactions, symbolic and pretend play, problem-solving, social interactions, and physical activity, with increasing importance as children move from infancy into toddlerhood".

---

### Small bowel adenocarcinoma, version 1.2020, NCCN clinical practice guidelines in oncology [^78641586]. Journal of the National Comprehensive Cancer Network (2019). High credibility.

Regarding medical management for small bowel adenocarcinoma - NCCN, more specifically with respect to systemic therapy (advanced or metastatic disease, regimens), NCCN 2019 guidelines recommend to offer the following regimen for 5-FU + radiation therapy:

- 5-FU 225 mg/m² IV over 24 hours (continuous infusion) daily on days 1–5 or 1–7 for 5 weeks with RT.

---

### NCCN guidelines® insights: gastrointestinal stromal tumors, version 2.2022 [^68aad06f]. Journal of the National Comprehensive Cancer Network (2022). High credibility.

Regarding diagnostic procedures for gastrointestinal stromal tumors - NCCN, more specifically with respect to mutation testing, NCCN 2022 guidelines recommend to consider germline KIT and platelet-derived growth factor receptor alpha testing for the following: patients with a family history of gastrointestinal stromal tumors and/or melanoma, patients with multifocal gastrointestinal stromal tumors, and/or patients with neurofibromatosis type 1 or succinate dehydrogenase-deficient gastrointestinal stromal tumors.

---

### NCCN guidelines® insights: bladder cancer, version 3.2024 [^503a1620]. Journal of the National Comprehensive Cancer Network (2024). High credibility.

Regarding medical management for bladder cancer - NCCN, more specifically with respect to systemic therapy for resectable disease (perioperative chemotherapy), NCCN 2024 guidelines recommend to do not substitute carboplatin for cisplatin in the perioperative bladder cancer setting. Consider administering a split-dose of cisplatin for patients with borderline renal function or minimal dysfunction (such as 35 mg/m² on days 1 and 2 or days 1 and 8) (category 2B). Recognize that while safer, the relative efficacy of the cisplatin-containing combination administered with such modifications remains undefined (category 2A).

---