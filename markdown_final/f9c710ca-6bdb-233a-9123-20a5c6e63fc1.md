# Medical Question & Answer

**Sample ID**: f9c710ca-6bdb-233a-9123-20a5c6e63fc1
**Dataset Index**: 275158

---

## Question

4. Given the equation x + 8y + 8z = n (n ∈ N) has 666 sets of positive integer solutions (x, y, z). Then, the maximum value of n is. 

---

## Answer

> Let's see… What do we have here? The user is asking for the maximum value of n such that the equation x + 8y + 8z = n has exactly 666 positive integer solutions (x, y, z). Let's break this down step-by-step. First, I need to think about how to count positive integer solutions to a linear Diophantine equation. Then, I should verify the formula for the number of solutions and make sure I'm handling the positivity constraint correctly. Next, I will set up the equation that forces the count to equal 666 and solve for n. After that, I should check whether the resulting n indeed yields exactly 666 solutions and whether larger n could still satisfy the condition. Finally, I will state the maximum n and briefly justify why it is maximal.

> Let me first confirm the problem setup and constraints. We need positive integers x, y, z such that x + 8y + 8z = n, and we want the number of such triples to be exactly 666. I should double-check that "positive" means x ≥ 1, y ≥ 1, z ≥ 1, which will matter for the counting formula.

> Now, I will transform the equation to simplify counting. Let me define new variables that shift the positivity constraint to non-negativity: let x′ = x − 1, y′ = y − 1, z′ = z − 1, so x′, y′, z′ ≥ 0. Substituting gives x′ + 8y′ + 8z′ = n − 1 − 8 − 8 = n − 17. Wait, let me verify that arithmetic: x = x′ + 1, y = y′ + 1, z = z′ + 1, so x + 8y + 8z = (x′ + 1) + 8(y′ + 1) + 8(z′ + 1) = x′ + 8y′ + 8z′ + 1 + 8 + 8 = x′ + 8y′ + 8z′ + 17, which means x′ + 8y′ + 8z′ = n − 17. So the transformed equation is x′ + 8y′ + 8z′ = n − 17 with x′, y′, z′ ≥ 0, and the number of non-negative solutions equals the number of positive solutions to the original equation.

> Next, I should review how to count non-negative solutions to x′ + 8y′ + 8z′ = N, where N = n − 17. For each fixed y′ and z′, x′ is determined uniquely as N − 8y′ − 8z′, so I just need to count pairs (y′, z′) such that 8y′ + 8z′ ≤ N. That is equivalent to y′ + z′ ≤ floor(N/8). Let me denote k = floor(N/8). For a fixed sum s = y′ + z′, the number of non-negative pairs (y′, z′) is s + 1, so the total number of solutions is the sum from s = 0 to k of (s + 1), which equals (k + 1)(k + 2)/2. Hold on, I should verify that formula: yes, ∑_{s = 0}^k (s + 1) = ∑_{j = 1}^{k+1} j = (k + 1)(k + 2)/2.

> I will now set the count equal to 666 and solve for k. So (k + 1)(k + 2)/2 = 666, which implies (k + 1)(k + 2) = 1332. Let me solve the quadratic k^2 + 3k + 2 − 1332 = 0, i.e., k^2 + 3k − 1330 = 0. Using the quadratic formula, k = [−3 ± sqrt(9 + 4·1330)]/2 = [−3 ± sqrt(5329)]/2. I should double-check sqrt(5329): 73^2 = 5329, so k = (−3 + 73)/2 = 70/2 = 35, and the negative root is irrelevant. Thus k = 35, which means floor(N/8) = 35, so 35 ≤ N/8 < 36, i.e., 280 ≤ N < 288. Since N = n − 17, we have 297 ≤ n < 305.

> But wait, what if n is at the upper end of that interval — does it still yield exactly 666 solutions? I need to ensure that floor((n − 17)/8) remains 35. For n = 304, N = 304 − 17 = 287, and floor(287/8) = 35, so the count is (35 + 1)(35 + 2)/2 = 36·37/2 = 666. For n = 305, N = 288, floor(288/8) = 36, and the count becomes (36 + 1)(36 + 2)/2 = 37·38/2 = 703, which is too big. Therefore, the maximum n that gives exactly 666 solutions is 304.

> Let me reconsider the logic to ensure there isn't a larger n that somehow still gives 666. The count increases by 37 when we go from N = 287 to N = 288 because k jumps from 35 to 36, so there is no intermediate n beyond 304 that preserves the count at 666. Thus, the maximum value of n is indeed 304.

---

The maximum value of (n) is **304**. This is found by transforming the equation to count non-negative solutions, then solving for the largest (n) such that the number of positive solutions is exactly 666. The key steps are: substitute (x' = x - 1), (y' = y - 1), (z' = z - 1) to get (x' + 8y' + 8z' = n - 17), count non-negative solutions as C(⌊(n − 17)/8⌋ + 1, 2), set this equal to 666, and solve to get (n = 304).

---

## Step 1: Transform the equation

Let (x' = x - 1), (y' = y - 1), and (z' = z - 1), where (x', y', z' ≥ 0). Substituting into the original equation gives:

x' + 1 + 8(y' + 1) + 8(z' + 1) = n

Simplifying:

x' + 8y' + 8z' + 17 = n ⇒ x' + 8y' + 8z' = n − 17

---

## Step 2: Count non-negative integer solutions

For a fixed (n), the number of non-negative integer solutions to (x' + 8y' + 8z' = n - 17) is the number of ways to choose non-negative integers (y') and (z') such that (8y' + 8z' ≤ n − 17). This is equivalent to counting non-negative integer solutions to (y' + z' ≤ ⌊(n − 17)/8⌋).

The number of non-negative integer solutions to (y' + z' ≤ k) is C(k + 2, 2). Therefore, the number of solutions is:

C(⌊(n − 17)/8⌋ + 2, 2)

---

## Step 3: Set up the equation

We are given that the number of positive integer solutions is 666, so:

C(⌊(n − 17)/8⌋ + 2, 2) = 666

Let (k = ⌊(n − 17)/8⌋). Then:

C(k + 2, 2) = 666 ⇒ [(k + 2)(k + 1)/2] = 666

Multiplying both sides by 2:

(k + 2)(k + 1) = 1332

---

## Step 4: Solve the quadratic equation

Expanding and rearranging:

k² + 3k + 2 − 1332 = 0 ⇒ k² + 3k − 1330 = 0

Using the quadratic formula:

k = [(−3 ± √(9 + 4 × 1330))/2] = [(−3 ± √5329)/2] = [(−3 ± 73)/2]

Taking the positive root:

k = [70/2] = 35

---

## Step 5: Find the maximum value of (n)

Since (k = ⌊[(n − 17)/8]⌋ = 35), we have:

35 ≤ [(n − 17)/8] < 36

Multiplying by 8:

280 ≤ n − 17 < 288 ⇒ 297 ≤ n < 305

The maximum integer value of (n) is **304**.

---

## Verification

For (n = 304):

⌊(304 − 17)/8⌋ = ⌊287/8⌋ = 35

C(35 + 2, 2) = C(37, 2) = 666

For (n = 305):

⌊(305 − 17)/8⌋ = ⌊288/8⌋ = 36

C(36 + 2, 2) = C(38, 2) = 703 ≠ 666

Thus, the maximum value of (n) is **304**.

---

The maximum value of (n) for which the equation (x + 8y + 8z = n) has exactly 666 positive integer solutions is **304**.

---

## References

### A solution to the collective action problem in between-group conflict with within-group inequality [^a7f2643e]. Nature Communications (2014). Medium credibility.

Hierarchical groups

Assume that group members differ in their valuations v i and that each group is characterized by exactly the same set of v i values. To find the invasion fitness w (y i | x i) for the effort x i at rank i, we need to set x ij = y i, f ij = 1+ bGP j nv i − cy i, and

in equation (10). Here X = ∑ j x j and X − i = X − x i.

Then, in the large G limit, the selection gradientat y = x for x i is proportional to

This equation is linear in x i. It follows that effort x i at each rank i evolves either to a positive value or to zero.

Assume first that the effort at each rank evolves to a positive value. Summing up equation (16) over all i with account that ∑ v i = 1 and ∑ x i = X, one finds that X satisfies to a quadratic equation

Therefore, at equilibrium

(The other solution, X ✱ = n (1+ b)/ c, results in zero average fertility and thus is not biologically relevant). That is, the total group effort does not depend on the group size or on the distribution of the valuations. The equilibrium individual efforts are

This equilibrium is feasible (that is, all x i * > 0) if for all i

Note that increasing the value of the resource b or the group size n moves v crit closer to 1/ n, so that individuals with a valuation smaller than the average will not contribute but will free ride.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^f37098d5]. Nature Communications (2021). High credibility.

Methods

Analytics and numerical simulation

Analytics were in general performed by hand, and checked for validity using Mathematica. Numerical simulations were run in Matlab using the dde23 delay differential equation solver for DDEs and ode45 for ODEs. Simulating activators as repressors with n < 0 technically fails when x is identically zero (Eq. (5)), since that would imply division by zero, but the limit as x goes to zero causes the regulation term to be zero, which is the same result as assumed by our notation. An initial value of exactly zero for x can thus lead to a divide-by-zero error in simulations, and so initial conditions of exactly zero were not used, as that case is an uninteresting fixed point for activators in any case. Note also that the consitutive case for Eq. (5) is degenerate, in that n = 0, α ≠ 0 is equivalent to n ≠ 0, α = 0 with α 0 → α 0 + α /2.

Phase plot simulations and analysis

For autoregulation phase plots, simulations were run with 100 constant-history initial conditions spread logarithmically between 10 −4 and 2 η and run from T = 0 to T = 100(γ + 1). Solutions were considered stable if for all 100 simulations the maximum absolute value of the discrete derivative in the last three-quarters of the simulation time was less than 0.1. Stable solutions were sub-categorized as bistable if a histogram of final values over all 100 solutions had more than 1 peak. Solutions were considered oscillatory if the average Fourier transform of the last three-quarters of the simulation time for all 100 solutions had more than zero peaks with amplitude (square root of power) greater than 100. Solutions were considered spiral if this oscillation condition held for the first one-quarter of the simulation time only. For two-component loops, initial conditions were used that ranged between 0 and, for equal X and Y and for apposing X and Y. Bistability was determined as for autoregulation, and a cutoff of 0.05 was used to determine "low" values. All simulation histories were constant except where indicated in Supplementary Fig. 7. Specific parameter values and simulation details are given in the figures and/or made explicit in the MATLAB code in Supplementary Data 1.

---

### Combinatorial code governing cellular responses to complex stimuli [^f2481e0e]. Nature Communications (2015). Medium credibility.

Methods

Identification of the interaction profiles

The number of ways to rank the numbers e 0, e X, e Y and e X+Y was computed using the recurrence relation A n k = A n− 1 k +k A n− 1 k− 1 which, for n = 4 and summing up for k = 1, 2, 3 and 4 gives 75 possible rankings. Each ranking can be seen as a set of outcomes of the following qualitative pairwise comparisons:
e X versus e 0
e Y versus e 0
e X+Y versus e 0
e Y versus e X
e X+Y versus e X
e X+Y versus e Y

In our framework, the outcome of a qualitative comparison can take on the following three values: 0 (equal numbers), 1 (first larger than the second) and −1 (first smaller than the second). Each of 75 rankings was coded uniquely as a vector of six components describing the qualitative outcome of the comparisons 1–6 as listed above. For example, the vector (0,0,1,0,1,1) corresponds to:

e X = e 0, e Y = e, e X+Y > e 0, e Y = e X, e X+Y > e X, e X+Y > e Y

To identify which rankings were compatible with positive or negative interactions, we considered the equations 1–6 together with the inequalities that define positive and negative interactions:
Δ e X+Y > Δ e X + Δ e Y (positive interaction)
Δ e X+Y < Δ e X + Δ e Y (negative interaction)

which can be written as

(7a) e X+Y − e X − e Y + e 0 > 0

(8a) e X+Y − e X − e Y + e 0 < 0

If a ranking is consistent with a positive (or negative) interaction, the inequality constraints encoded in the corresponding vector can be solved simultaneously with 7a (or 8a). To verify this, we developed a constraint satisfaction that attempts to solve the six constraints of each ranking together with the inequality 7a (or 7b). The solution is searched numerically with a MATLAB linear solver. The variables e 0, e X, e Y and e X+Y were constrained to the interval 2–16, taken as an approximate of the range of expression values from Affymetrix chips in log2 scale. The method is implemented in the MATLAB code and is available upon request.

---

### Modeling macroscopic brain dynamics with brain-inspired computing architecture [^2984fe31]. Nature Communications (2025). High credibility.

The mapping of values from a continuous set to a countable, smaller set is called quantization in the signal processing and AI domains. The principle of uniform quantization can be written aswhere x and x int are the original real-valued variable and the quantized variable, respectively.is the approximate value represented by the low-precision data. The quantization is primarily a simple affine transformation with two parameters, namely the scaling factor s and the zero point z. Thus, the quantization function is defined aswhere ⌈ ⋅ ⌋ corresponds to the round function. The equation rounds the scaled real-valued variable to an integer and then clamps it to the range of the integer format.

We use semi-dynamic quantization to determine the scaling factor s and zero point z for both the DMF model and the hemodynamic model. During the warm-up stage, the model quickly converges near the fixed point. Then we determine the quantization parameters based on the variable ranges in the QPS stage. We can calculate the power-of-two scaling factor for each of them usingwhere ⌊ ⋅ ⌋ represents the floor function, and ∣ ⋅ ∣ represents the absolute value. q m a x is the maximum positive value of the signed integer type, and it is 127 for INT8 here. The bold y is the variable that needs to be quantized. As most brain-inspired computing chips only support per-tensor quantization, we treat the same variable within each node of a model as a vector and apply uniform quantization parameters to all elements in it. Thus, the shape of y here is (N, T QPS), where N denotes the number of nodes in a model and T QPS is the number of time steps in the QPS stage. y m represents the unilateral maximum range that needs to be expressed by the integer type, which can be obtained using the extreme values of the vector in the QPS stage. Similarly, the zero point z can also be calculated by

---

### Qubit vitrification and entanglement criticality on a quantum simulator [^16cfb01b]. Nature Communications (2022). High credibility.

Calculating the order parameter

1. **For each**:
For each matrix B from the previous section: i. Computeas in the previous section using B M with ∣ M ∣ = L α. ii. For each saved parity vectorassociated to: A. Fix a reference solution z that maps solutions from the parity vectorto the parity vector 0. We chose our reference to be the solution towhose binary form represents the smallest integer. Note that finding a reference is efficient. B. Map the saved solutions x associated withto. Now. Remove any duplicates. Call this set. iii. Computein Eq. (2) by uniformly samplingsolutions from X, where we determined the number 24 yields a reasonable compromise between accuracy and quantum runtime.
Compute q (α) by averaging overfor all.

Following the procedure for each L produces the curves in Fig. 3. To calculate the order parameter in step iii), we want as many solutionsas possible, but a finite number works, making the order parameter efficient to measure. For the classical simulation of q (the dashed lines in Fig. 3), we uniformly samplesolutions to the equation B M x = 0, whereis the total number of solutions. We did this by sampling random linear combinations of the basis vectors forming the null space of B M. This is also efficient.

We note that the small size of the sample X leads to appreciable artefacts in Fig. 3, such as a deviation of the order parameter from the expected value of zero at small α. Concomitantly, we notice the onset of finite-size effects at values of L and α for which. The dip of q at small α for L = 8 is such an effect. We nevertheless notice that, for all L and α, theory and experiment match well in Fig. 3, since these effects are present in both.

---

### Current sample size conventions: flaws, harms, and alternatives [^f34a2bb9]. BMC Medicine (2010). Low credibility.

Value of information methods

Many methods have already been described in the statistical literature for choosing the sample size that maximizes the expected value of the information produced minus the total cost of the study. See for an early discussion, for recent examples, and the introduction of for additional references. These require projecting both value and cost at various different sample sizes, including quantifying cost and value on the same scale (note, however, that this could be avoided by instead maximizing value divided by total cost). They also require formally specifying uncertainty about the state of nature; although this can be criticized as being subjective, it improves vastly on the usual conventional approach of assuming that one particular guess is accurate. These methods can require considerable effort and technical expertise, but they can also produce the sort of thorough and directly meaningful assessment that should be required to justify studies that are very expensive or that put many people at risk.

Simple choices based on cost or feasibility

Recent work has justified two simple choices that are based only on costs, with no need to quantify projected value or current uncertainty about the topic being studied. Because costs can generally be more accurately projected than the inputs for conventional calculations, this avoids the inherent inaccuracy that besets the conventional approach. One choice, called n min, is the sample size that minimizes the total cost per subject studied. This is guaranteed to be more cost-efficient (produce a better ratio of projected value to cost) than any larger sample size. It therefore cannot be validly criticized as inadequate. The other, called n root, is the sample size that minimizes the total cost divided by the square root of sample size. This is smaller than n min and is most justifiable for innovative studies where very little is already known about the issue to be studied, in which case it is also guaranteed to be more cost efficient than any larger sample size. An interactive spreadsheet that facilitates identification of n min and n root is provided as Additional file 2.

A common pragmatic strategy is to use the maximum sample size that is reasonably feasible. When sample size is constrained by cost barriers, such as exhausting the pool of the most easily studied subjects, this strategy may closely approximate use of n min and therefore share its justification. When constraints imposed by funders determine feasibility, doing the maximum possible within those constraints is a sensible choice.

---

### Harmonization of quality metrics and power calculation in multi-omic studies [^4134567c]. Nature Communications (2020). High credibility.

Sample size scenarios in MultiPower

In multi-omic studies, an assorted range of experimental designs can be found. All omic assays may be obtained on the same biological samples or individuals, which would result in identical replicates number for all data types. However, this is not always possible due to restrictions in cost or biological material, exclusion of low-quality samples, or distributed omic data generation. In these cases, sample size differs among omic types and yet the data are to be analyzed in an integrative fashion. MultiPower contemplates these two scenarios.

Under the first scenario, adding the constraint of equal sample size for all omics (x i = x for all I = 1,…, I) to the optimization problem in Eq. (1) results in a straightforward solution. First, the minimum sample size required to meet the constraint on the minimum power per omic (x i) is calculated and the initial estimation of x is set to x = max I { x i }. Next, the second constraint on the average power is evaluated for x. If true, the optimal sample size is x opt = x. Otherwise, x is increased until the constraint is met. Note that, under this sample size scenario, the cost per replicate does not influence the optimal solution x opt.

Under the second scenario, allowing different sample sizes for each omic, the optimization problem in Eq. (1) becomes a nonlinear integer programming problem, as the statistical power is a nonlinear function of x i. The optimization problem can be transformed into a 0–1 linear integer programming problem by defining the auxiliary variablesfor each omic i and each possible sample size n from 2 to a fixed maximum value n i max, wherewhen the sample size for omic i is n. The new linear integer programming problem can be formulated as follows:where f i (n) is the power for sample size n, given the parameters of omic i.

---

### A solution to the collective action problem in between-group conflict with within-group inequality [^1e238285]. Nature Communications (2014). Medium credibility.

Equation (19) shows that individual effortsincrease with valuation v i. In contrast, individual reproductive success at equilibrium is

and thus decreases with valuation. That is, the higher costs paid by high-rank individuals negate their larger shares of the reward.

If not all group members make a positive effort, then the derivations become more complex. Let there be n e contributors and v be their total valuation (v = ∑ v i where the sum is over n e contributors). Then, summing up equation (16) over all contributors, at equilibrium their total effort X satisfies to a quadratic equation

Of the two solutions of this quadratic, only the smallest one, X *, is relevant biologically (the other solution leads to negative fertilities). Note that cX * ≤ n e + bn Σ v i < n (1+ b). (The first inequality follows from the requirement that all f i values are non-negative: which one then sums up over all contributing individuals).

The individual effortscan be found from equation (16) by substituting X * into

This equilibrium is biologically feasible (that is, all), if

Equation (23) shows that individual effort increases with valuation. However, within-group reproductive success

and is decreasing with valuation v i because cX * ≤ n (b +1).

By differentiating the expression for X *, one can show that X * always increases with v and generally decreases with n e. (The latter requires that v exceeds a certain threshold that depends on b and n but is relatively small). That is, decreasing the number of contributors and/or increasing their total share of the reward results in increasing group effort. Moreover, analytical derivation and numerical simulations strongly suggest that the derivative of v crit with respect to X is positive, so that the threshold valuation increases with decreasing the number of contributors. This leads to a recursive procedure for finding the equilibrium: (i) setfor all ranks for which inequality (20) is not satisfied; (ii) using appropriate values of n e and v, find X * from equation (22); (iii) if there are ranks for which inequality (24) is not satisfied, set their contributionsto zero, recompute n e and v, and return to step (ii).

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^832976d6]. Nature Communications (2021). High credibility.

Results

Our results are divided up into 7 sections corresponding to 7 different regulatory networks of increasing complexity. An overall reference table (Table 2) is included in the discussion.

Motif 0: direct Hill regulation

We first describe how we use DDEs to model simple regulation of a gene Y by a gene X as the most basic motif, and then provide a simple yet unified mathematical framework for both activation and inhibition with delay.

Activation and inhibition can both be modeled with a single unified function

We consider a transcription factor x regulating the production of a protein y (Fig. 2). If x activates y, the production rate of y increases with increasing x, generally saturating at a maximum rate α. Often there is an additional cooperativity parameter n which determines how steeply y increases with x in the vicinity of the half-maximal x input value k. In this framework, n = 0 is constitutive production, n = 1 is a Michaelis-Menten regulation, 1 < n < 5 is a typical biological range of cooperativity, and n → ∞ is a step-function regulation. If x represses y, the same conditions hold except that the production rate of y then decreases with increasing x. A standard quantitative model for this behavior is called the Hill function, and serves as a good approximation for many regulatory phenotypes in biology, including transcription and translation rates, phosphorylation, enzymatic activity, neuronal firing, and so forth. In general there may also be a leakage rate α 0 that yields constant y production in the absence of x. The concentration of y is also generally removed at a rate β proportional to its concentration. This removal term can represent many biophysical processes, such as degradation, dilution, compartmentalization, or sequestration; for simplicity we mainly use the term "degradation".

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^b91959df]. Nature Communications (2021). High credibility.

A two-parameter summing function reproduces all 2-input monotonic logic gates

We first write out nondimensionalized equations corresponding to the logic gate motif as depicted in Fig. 5 a. We assume that the degradation constants (β) for Z and R are equal for simplicity, and that there is no leakage.

We describe the regulation of Z by X and Y using a sum of Hill terms. Logic gate behavior can be captured with other approaches such as a product of Hill terms, or summation within a single Hill term, each representing subtly different biology. We choose the separate Hill term approach as it describes many logic functions simply by tuning regulatory strengths, and can be extended to include multiplicative terms (Supplementary Note 5). Caveats include multiple states for Z, requiring additional binarization via R (Fig. 5 c), as well as poor response to ratiometric inputs, discussed in the next section on feedforward motifs.

Using Eq. (19), we can characterize the motif logic based on the idea of dynamic range matching. Every regulator in Eq. (19) is effectively compared against unity in the denominator of the Hill function for its corresponding output. For instance, Z provides an "on" or "off" signal to R if Z > 1 or Z < 1 respectively. Z can take on values below 1 as long asand, otherwise Z will always activate R, as indicated by the areas marked TRUE in Fig. 5.

Let us say that X and Y settle on steady-state values X * ≡ η X, Y * ≡ η Y, as inputs to our logic gate. A value of η X or η Y significantly greater than 1 is then "high" (true, 1), and "low" (false, 0) if much less than 1. We then want to determine whether Z * is greater than (true) or less than (false) 1. For example, if n 1,2 > 0 (two repressors), Table 1 gives the possible steady states of Z. If η Z 1 and η Z 2 are greater than 1, these steady states approximate a NAND gate. If they are less than 1, but sum to greater than 1, the steady states instead approximate a NOR gate.

---

### A stochastic vs deterministic perspective on the timing of cellular events [^c3a6938f]. Nature Communications (2024). High credibility.

Case 1: the simple birth–death process

When r = 1, the model given in reaction scheme (8) coincides with a simple birth–death process. We are interested in the mean waiting time for the birth–death process to reach a fixed protein number N, given that the system is started from n initial proteins. Simulations of the simple birth–death process suggest the MFPT is smaller than the deterministic FPT (see Fig. 2 B, but note that only a small sample of trajectories are shown here for visualisation purposes). We now prove this analytically. To begin, we find an analytical solution for the MFPT of the stochastic description of the system, as given by the CME (see Equation (3) of the Supplementary Information).

Starting from Eq. (7), it can be shown (Supplementary Note 2), that the expected waiting time to reach N proteins given the system is started from n ≤ N, is given by, where for real number x and positive integer m, the notationabbreviates x (x − 1)… (x − (m − 1)), the falling factorial of x. In what follows for simplicity we shall set n = 0. The deterministic description of the system is given by the reaction rate equation d τ X = K − X, with X (0) = 0, and solving for X (τ) yields X (τ) = K (1 − e − τ). Solving for τ then gives, Consider some proportion 0 < ρ < 1, and consider the expected waiting time to reach ρ K, that is, some proportion ρ of the steady-state mean. Then Eq. (10) simplifies towhich we note is independent of K, and where the requirement of ρ < 1 enables this to be well defined. Now let us consider Eq. (9) for N = ⌊ ρ K ⌋. For notational simplicity only, we assume that ρ K is an integer, and again observe that the numeratoris bounded above by (ρ K) i +1. Replacing this in Eq. (9) givesRecognising the Taylor expansion ofaround 0 (for ρ < 1), we find that Eq. (12) is the truncated Taylor expansion of Eq. (11). Thus, as each term of the series is strictly positive, it follows that for any proportion ρ of the steady-state mean, the deterministic waiting time is a strict upper bound for the mean waiting times. An example is given in Fig. 2 C for ρ = 0.8. Here we show how the ratio of the stochastic mean waiting time to the deterministic waiting time, η, of the simple birth–death process scales with increasing K. Note that since δ = r = 1, K is equal to the steady-state mean number of molecules.

---

### Degenerate boundaries for multiple-alternative decisions [^a8d28202]. Nature Communications (2022). High credibility.

Rewards are generated according to the Bayes risk represented over single trials (equation 12), when stochastic decision trajectories encounter a decision boundary. A reward landscape can be generated by systematically varying decision boundary parameters and averaging the rewards generated over a large number of simulated trajectories for each set of parameters. Averaging over many reward outcomes reduces variance producing a smooth surface and simulates the expected reward, which coincides with sampling rewards to learn the decision boundary under equation (11). So each point in the reward landscape is the mean reward obtained from a distribution of rewards for that boundary shape (typically from 100,000 samples across the parameter ranges); differing landscapes with c / W discretized in 1000 steps across its range from 0 to 0.2 were then considered. Each landscape has a point of mean reward that is an absolute maximum, which we will denote by; however, the landscape is very noisy even after this averaging, with many other points with similar mean rewards. Therefore, we consider a set of parameters corresponding to multiple points x ∈ X on the landscape, with mean rewards r x and standard deviations σ x according to the distribution of rewards for those parameters. This set of parameters is selected such that the maximum mean reward value of the landscape falls within a δ σ x of the point, so that. Using some example reward landscapes, we found that as δ decreases, the area designated as maximum also decreases as expected. However, below δ = 0.02, the area of the maximal region does not appear to change, but the points selected became more sparse, so we use this value.

Reporting summary

Further information on research design is available in the Nature Research Reporting Summary linked to this article.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^22bd51a1]. Nature Communications (2021). High credibility.

Motif III: logic

A general class of functions used to describe natural,– and synthetic, biological networks are logic gates, which have two inputs regulating a single output (Fig. 5). Gates exhibit either high ("on") or low ("off") output depending on whether inputs are on or off. For example, the AND gate specifies high output only if both inputs are on. In this section we provide a specific DDE-based framework that covers 14 out of 16 possible 2-input logic operations, and show that these operations form a continuous 2D parameter space.

Fig. 5
A simple approximation for digital logic using a sum of Hill terms recapitulates all monotonic logic functions in a single parameter space.

a A prototypical regulatory network involving logic where X and Y both regulate Z, which must integrate the two signals using some logic before it can in turn activate a downstream reporter R. b Parameter space showing regions where regulation approximately follows 14 of the 16 possible 2-input logic functions depending on the strength of two single-variable Hill regulation terms (η Z 1: regulation of Z by X, η Z 2: regulation of Z by Y). Network logic can be smoothly altered by varying the parameters (η Z 1, η Z 2), with a change of sign in (n 1, n 2) required to switch quadrants. The bottom-left quadrant shows that very weak regulation in both terms leads to an always-off (FALSE) function, weak regulation in one arm only leads to single-input (X, Y) functions, strong regulation in both arms leads to an OR function, and regulation too weak in either arm alone to activate an output but strong enough in sum leads to an AND function. The other three quadrants are related by applying NOT to one or both inputs, with function names related by de Morgan's lawNOT(X OR Y) = NOT X AND NOT Y. In particular, X IMPLY Y = NOT(X) OR Y, X NIMPLY Y = X AND NOT(Y), X NOR Y = NOT X AND NOT Y, and X NAND Y = NOT X OR NOT Y. Truth tables for all 16 logic gates are provided in Supplementary Table 1 for reference. The two non-monotonic logic functions, X XOR Y and X XNOR Y, are those 2 of 16 not reproduced directly using this summing approximation. They can be produced by layering, e.g. NAND gates. c Representative time traces for AND (η Z 1 = η Z 2 = 0.9) and OR (η Z 1 = η Z 2 = 1.8) gates with n 1 = n 2 = −2, n 3 = −20, η R = η Z 1 + η Z 2. The functionwhen n > 0, when n < 0.

---

### Genotyping sequence-resolved copy number variation using pangenomes reveals paralog-specific global diversity and expression divergence of duplicated genes [^8058bdce]. Nature Genetics (2025). High credibility.

Integer solution based on recursive phylogenetic rounding

Initial linear regression yields solutions in the form of small floating point values, in which the alleles with the highest coefficients are not necessarily those closest to query genes. However, as shown by mathematical analysis (Supplementary Notes), there are strong relationships between the initial least-error solution and the true integer solutions under a phylogenetic framework:
Nonnegative solutions: without uncertainty in predicting k -mer copy numbers, the least-error solution should be nonnegative. Therefore, we obtain a nonnegative least-error solution via the Lawson–Hanson algorithm.
Total copy number estimation: the sum of the initial solutions should approximate the total number of the true integer solutions, allowing us to estimate the total gene copy number in the querying sample.
Phylogenetic position prediction: on a binary phylogenetic tree, the branch with a shorter vector distance to the genes in the querying sample will have a larger sum of coefficients (inversely proportional to distance). This relationship enables us to predict the phylogenetic position of each gene in the query sample.
Fractality of the least-squared error solution on a phylogenetic tree: if a solution is the least-squared error solution of the tree, it is also the least-squared error solution within each clade, allowing the greedy method to perform on the phylogenetic tree.
Large database effect: in large databases, having more genes highly similar to query genes increases condition number and tends to distribute the total coefficients across them, resulting in smaller individual coefficients. However, the total sum of these coefficients increases, improving the precision of phylogenetic position prediction, and this effect does not plateau.
With sequencing coverage variance for NGS at ~30-fold coverage, the model precision remains. Sequenced variance is not the primary source of error.

Given the high 'convergence' and fractality of the solution on the phylogenetic tree in large databases, we developed a greedy algorithm to efficiently convert non-integer solutions into integer solutions. This iterative algorithm follows a bottom–up approach, starting from the leaves and progressing toward the root. At each hierarchical level, non-integer values are rounded to the nearest integer solution that minimizes the overall residual, while any remainder is propagated to the next level. Because, at each level of the hierarchy, there are only two remainders from either branch of the tree, this solution is highly efficient. We label this approach as recursive phylogenetic rounding. The pseudocode for this algorithm (naive version and optimized version) is provided in the Supplementary Information.

---

### A space-time tradeoff for implementing a function with master equation dynamics [^57fa10b0]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### Allelic decomposition and exact genotyping of highly polymorphic and structurally variant genes [^85761b97]. Nature Communications (2018). Medium credibility.

In order to estimate the copy number of any region r spanning positions a, a + 1,…, b of a gene or pseudogene s, we first calculate the normalized copy number cn s of s, which intuitively reflects the number of copies of s at position i (when the intron/exon of the gene includes ambiguously mappable positions, those positions are ignored). Details about calculating this function are provided in Supplementary Note 1 and Supplementary Figures 1 and 2. The estimated copy number (or observed coverage) of a region r of s is denoted as cn [r], and is simply calculated as:

Now we formulate Aldy's goal in this step as an instance of ILP, i.e. integer linear programming (see Trickfor an introduction to ILP) where the goal is to find an integer linear combinationof configuration vectors from the set V such that the sumwhereis minimized. Here, non-negative integer variables z i denote the number of times (the copy number of) a configuration described by vector v i from the database, appears in the solution (we recall that k denotes the total number of possible configurations). We call this problem the Copy Number Estimation Problem (CNEP).

After finding all optimal solutions of CNEP, Aldy only reports "the most parsimonious" solution(s), i.e. those for whichis the minimum possible. In the case when multiple optimal solutions minimize the term, all will be reported in the final output of CNEP.

We illustrate the above notion of parsimony through a simple example with n = 2. Let the aggregate copy number vector be cn = [1111], and the set of potential vectors be V = { v 1, v 2, v 3, v 4 } where v 1 = [1100], v 2 = [0011], v 3 = [1000], and v 4 = [0100]. Consider the following optimal solutions to this instance of CNEP: (z 1, z 2, z 3, z 4) = (1, 1, 0, 0) and (z 1, z 2, z 3, z 4) = (0, 1, 1, 1). The first solution is more parsimonious compared to the second one since the first solution implies the presence of the whole gene and the whole pseudogene — whereas the second solution implies the presence of two structurally altered copies of the gene itself, in addition to the whole pseudogene.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^bb9b7f12]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Δ Φ, the probability distribution at a point Φ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Φ ∈ {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters α and β, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since α and β are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Φ ∈ [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(α, β) and x 2 = 1 − x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Φ, it may seem natural to select α and β by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### Genotyping sequence-resolved copy number variation using pangenomes reveals paralog-specific global diversity and expression divergence of duplicated genes [^f60506de]. Nature Genetics (2025). High credibility.

Initial solution based on linear regression

Given an NGS sample and a k -mer matrix M derived from PAs, we generate a vector V of corresponding k -mer counts from the NGS sample, normalized by sequencing coverage. We seek to find a vector X that denotes the copy numbers of all PAs and minimizes the squared distance to the k -mer counts observed in NGS data, for example, argmin x (ǁ M T × X − V ǁ). The integer solution through mixed-integer linear programming is NP hard; however, the relaxed non-integer solution based on squared distances has an efficient analytic solution. Compared with absolute distance, squared distance is more suitable for the normal-like noise in NGS data.

To make the solution closer to the maximum likelihood estimate, during the regression, we rescale the weights of k -mers to even out their expected uncertainty. Assuming that the observation of k -mer copy number follows a negative binomial distribution with the dispersion small enough to be distinct from Poisson, the expected variance is roughly proportional to the square of observation; therefore, we weight k -mers by the square of the reciprocal of their observed copy number. We also apply smaller weights (adjust = 0.05) to k -mers observed in only one PA and not in NGS because they are more likely to be assembly errors.

---

### Analysis of the first genetic engineering attribution challenge [^688011e3]. Nature Communications (2022). High credibility.

Given these accuracy scores, the X99 score could be computed as the minimum positive integer N such that top- N accuracy is at least 99%. This metric can be generalised to other thresholds, where X R is the minimum positive integer N such that top- N accuracy is at least R %. X95, X90 and X80 scores were all computed in this way.

For the purposes of calculating precision and recall, the number of true positives, false positives and false negatives were computed separately for each lab class for each submission. For a given class, the number of true positiveswas defined as the number of times in the test set that that class was correctly assigned rank 1 (i.e. assigned rank 1 when it was in fact the true lab-of origin); the number of false positivesas the number of times it was incorrectly assigned rank 1; and the number of false negativesas the number of times it was incorrectly assigned rank > 1. Precision and recall for each class were then calculated asand recall as, and the F1 score for each class as the harmonic mean of its precision and recall. The overall precision and recall for each team were computed as the arithmetic mean of its class-specific precisions and recalls, respectively, while the macro-averaged F1 score was computed as the arithmetic mean of its class-specific F1 scores.

Calibration

Following Guo et al.we checked whether predictions had frequentist calibration of their probabilistic forecasts. To estimate the expected accuracy from finite samples, we grouped predictions into 15 interval bins of equal size. We letbe the set of indices of samples whose prediction confidence falls into the intervals. The accuracy of binis then defined aswhereandare the (top-1) predicted and true class labels for sequenceandis the number of samples in bin. The average confidence within binis defined aswhereis the predicted probability assigned to classfor sequence i. The expected deviation between confidence and accuracy can then be estimated using the expected calibration error (ECE):whereis the total number of samples. The maximum calibration error (MCE) estimates the worst-case deviation from the binning procedure as:

---

### Input-output maps are strongly biased towards simple outputs [^50fca0dd]. Nature Communications (2018). Medium credibility.

Many systems in nature can be described using discrete input-output maps. Without knowing details about a map, there may seem to be no a priori reason to expect that a randomly chosen input would be more likely to generate one output over another. Here, by extending fundamental results from algorithmic information theory, we show instead that for many real-world maps, the a priori probability P(x) that randomly sampled inputs generate a particular output x decays exponentially with the approximate Kolmogorov complexity [Formula: see text] of that output. These input-output maps are biased towards simplicity. We derive an upper bound P(x)≲[Formula: see text], which is tight for most inputs. The constants a and b, as well as many properties of P(x), can be predicted with minimal knowledge of the map. We explore this strong bias towards simple outputs in systems ranging from the folding of RNA secondary structures to systems of coupled ordinary differential equations to a stochastic financial trading model.

---

### Analog optical computer for AI inference and combinatorial optimization [^d9ce3cb7]. Nature (2025). Excellent credibility.

Inference and export to the AOC

Once training has completed, the weight matrix W is quantized to signed 9-bit integers usingwith the rounded and clamped matrix on the right-hand side being the quantized weight matrix W Q. Whenever we report AOC-DT results, we report results obtained with the quantized matrix.

Exporting trained models to the AOC requires several further steps. First, the model inputs x and the bias term b need to be condensed into a single vector b AOC = b + x followed by clamp to ensure the values fit into the dynamic range of the AOC device (Supplementary Information section D). Second, as the optical matrix multiplication is implemented using SLMs, elements of the weight matrix are bounded by one such that all quantization-related factors disappear. However, the original maximum element of the matrix max(W) needs to be re-injected, which we achieve via the β gain in equation (2), approximately restoring the original matrix W.

The quantized matrix is split into positive and negative parts, and each part is displayed on its respective SLM.

AOC sampling and workflow

Each classification instance (that is, MNIST or Fashion-MNIST test image) is run once on the AOC, and the fixed point is sampled at the point marked in Extended Data Fig. 3 after a short 2.5 - μs cooldown window after the switch is closed, as shown in Extended Data Fig. 5a, b. The sampling window extends over 40 samples at 6.25 MHz, corresponding to 6.4 μs. This ensures that the search of fixed points for the equilibrium models happens entirely in the analog domain. Once sampled, we digitally project the vector into the output space. For classification, the input is projected from 784 to 16 dimensions, the output is projected from 16 to 10 classes. The label is then determined by the index of the largest element in the output vector (argument-max). For regression tasks, the IP and OP layers transform a scalar to 16 dimensions and back, respectively. The MSE results in Fig. 2c were obtained by averaging over 11 repeats for each input. This means that we restart the solution process 11 times, including the sampling window, and average the resulting latent fixed-point vectors. Importantly, the solve-to-solve variability appears to be centred close to the curve produced by the AOC-DT, enabling us to average this variability out (Supplementary Fig. 6).

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^488abf14]. Nature Communications (2021). High credibility.

Chaotic behaviors are prevented in monotonic regulation with linear or cyclic network topology

Due to cyclic network topology (no more than one loop) and monotonic regulation, none of the motifs so far can yield chaotic dynamics. Either non-monotonic feedback (as in the Mackey-Glass equation) or non-cyclic topologyis required for chaos. Double feedback (Fig. 8 a) is a minimal motif fulfilling both requirements, although the non-monotonicity is sufficient (Supplementary Note 8).

The governing equation for such a double feedback motif is thus given by setting the output and both inputs of the logic equation (Eq. (19)) to X (and letting K = 1 for simplicity):

Chaotic behavior is possible for non-monotonic feedback with multiple feedback and disparate delays

Simulation of Eq. (37) demonstrates chaos for some parameters of positive/negative mixed feedback, as evidenced by sustained oscillations with variable maxima (Fig. 8 b), occupying an apparently fractal region in phase space (Fig. 8 c), and a large number of peaks in frequency space (Fig. 8 d). To confirm that the dynamics are in fact chaotic, we calculated the dominant Lyapunov exponent using the Wolf method(see "Methods"), finding a positive (i.e. chaotic) value of 0.0040 ± 0.00055 bits (mean ± standard deviation). We also calculated the box dimension of a reconstructed phase space with coordinates (X (T), X (T − 10), X (T − 20)) used by the Wolf method, yielding fractional dimension ~1.81, with 95% confidence interval (1.76, 1.87). These results indicate chaos (see Supplementary Fig. 4).

---

### Adaptive resetting for informed search strategies and the design of non-equilibrium steady-states [^9a9db8d1]. Nature Communications (2025). High credibility.

The final value theorem for Z-transforms states that. By using it, we get (see section 6 of the Supplementary Information)where 〈 N R 〉 is the mean number of time steps between consecutive resetting events. Note that the steady-state in Equation (14) is well defined whenever 〈 N R 〉 is finite, regardless of whether or not the process without resetting has a steady-state. This is a generalization of a well-known result in the theory of standard resetting to state- and time-dependent resetting.

To estimate the NESS, we first sample a set of N trajectories without resetting of length M Δ t. We stress that M should be large enough such that, had we used resetting, the probability of surviving M steps without resetting would be negligible, i.e. Then, we use Equations (12) and (14), and the definition of the Z-transform, to obtainThis estimation results in an unnormalized distribution, which should be normalized. The normalization factor provides an estimate for the mean time between consecutive resetting events, 〈 N R 〉. Equation (15) shows that the estimation of the NESS with resetting, from trajectories without resetting, is done by averaging the histogram of positions over time and trajectories, but reweighing each trajectory, at every time step, by its survival probability.

Prediction and design of non-equilibrium steady-states

The above results can be used to predict and design NESS of spatially-dependent resetting protocols. We demonstrate this using two examples.

It is well known that for free diffusion with a constant resetting rate, a Laplace distributed NESS emerges. An analytical solution for the NESS of diffusion with a parabolic resetting rate r (x) = r 0 x 2 is also known. Interestingly, in both cases, the tails of the NESS decay as, with α = 1 for the constant resetting rate, and α = 2 for the parabolic resetting rate. This raises a more general question: what is the asymptotics of the NESS for diffusion with a power-law resetting rate r (x) = r 0 ∣ x ∣ λ. While there are currently no known closed-form solutions for the NESS with λ ≠ {0, 2}, we can easily estimate the resulting NESS using the procedure described in the previous section.

---

### Laser chimeras as a paradigm for multistable patterns in complex systems [^6f043595]. Nature Communications (2015). Medium credibility.

Chimera basin

Figure 2a reveals that model (Equation 3) is highly multistable for small and intermediate ɛ. This suggests to explore the basins of attraction of the different co-existing attractors. Since time-delayed systems are infinite dimensional through their initial conditions being functional { x 0 (s)| s ∈[0,1]}, a precise topological characterization of the basins structure is not directly possible. One can however try to estimate the relative size (measure) of the basins in terms of occurrence probability for each possible solution, after resetting many different random initial conditions. This is illustrated in Fig. 5, which shows the evolution versus ɛ of the probability occurrence for N σ -headed chimera for three fixed δ values, with N σ = 0–5 (0 corresponding to chaotic breather). Each probability has been calculated with 300 different initial noisy conditions x 0 (s) (uniform amplitude distribution of x ∈[−1;1]).

In Fig. 5a (δ = 0.02), for small ɛ the most probable solutions are high-order multi-headed chimera, a small fraction only of initial conditions leading to one- or two-headed chimeras. As ɛ is increased, N σ = 1 and N σ = 2 basins are revealing higher and higher occupation of phase space. For an intermediate range of ɛ, one notices that two-headed chimera basins reaches a maximum, prevailing on the other possible N σ with ∼60% of probability of occurrence around ɛ = 0.005. For larger values of ɛ, one-headed chimera basin appears to occupy almost the full explored phase space. In Fig. 5b, c corresponding to smaller δ values, qualitatively similar features are observed, except that higher-order chimeras are less and less probable (they would require smaller ɛ), and lower N σ orders predominate together with a growing influence of the chaotic breather (black) as it is visible already in Fig. 2a from the positions of the bifurcation curves.

On the basis of these numerical simulations, we summarize that multi-headed chimeras represent an essential part of the solutions exhibited by equation (4), higher-order chimeras requiring small ɛ to predominate. In addition, one could also interestingly connect the coexistence and bifurcation structure of multi-headed chimera states to a recently reported result on optical bit storage through patterns formed using topological solitons.

---

### Activated desorption at heterogeneous interfaces and long-time kinetics of hydrocarbon recovery from nanoporous media [^2b95c210]. Nature Communications (2016). Medium credibility.

In practice, rather than attempt to find a bias potential, which allows the entire domain of n ex to be sampled, it is more practical to run many simulation 'windows'. In each window we applied a simple harmonic bias potential so as to sample a particular range of n ex, with the form

where K and n i determine the strength and centre of the bias for the i th window. To implement such a bias in a molecular dynamics simulation, the force on each methane particle resulting from the bias must be described as a function of the particle position along the z axis:

The need for n ex to be differentiable with respect to the particle position z motivates the use of the logistic form in equation (9), for which the derivative is well known and straightforward to implement:

To calculate the full unbiased probability distribution P U, we used a weighted average of the unbiased probabilities in each window, according to the weighted histogram analysis method. The weightings were calculated so as to minimize the statistical error in P U:

whereand N i are the measured unbiased probability and number of samples, respectively, in the i th window. The F i terms were calculated according to

We found a self-consistent solution by starting from equation (16) with F i = 0 for all windows, then iterating between equation (17) and (16) until convergence. We considered the solution to have converged when the maximum value of (1− F old / F i) 2 for any window was < 10 −15, where F old is the value of F i in the previous iteration.

After unblocking the pores and applying the bias, we equilibrated the initial simulation window at n i = 0 for at least 1 ns. To measure P B (n ex), we sampled the system every 0.2 ps for at least 1 ns. We generated additional windows by restarting a simulation equilibrated with a similar n ex, equilibrating for at least 0.2 ns. We typically simulated windows separated in n i by 0.4 molecule per nm 2, though when close to a free-energy maximum we sometimes required additional windows separated by 0.1 molecule per nm 2, and with larger spring constant. We list the membrane parameters for the simulations used to produce the results in Fig. 3a in Supplementary Table 3 (system a).

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^e1f904d6]. Nature Communications (2021). High credibility.

Fig. 1
Explicit inclusion of time delays in mathematical models of network motifs can reproduce non-delay models using fewer variables and parameters, but can also lead to more complex behavior.

a An example genetic regulatory network including genes X, Y, and Z, regulated by one another either positively (activation, pointed arrow) or negatively (repression, blunt arrow). The model on the left incorporates no explicit delay terms, whereas the model on the right incorporates explicit delay terms τ x and τ z. b Ordinary differential equations (ODEs) and delay differential equations (DDEs) corresponding to (a) with regulation strengths α i, removal rates β i and cooperativities n i. c Numerical simulations of equations in (b) for one set of initial conditions using parameters α x = 1, α y = 1.2, α z = 1.2, β x = β y = β z = 1, n x = n y = n z = 2 for the ODE simulation, and α x = 0.5, α z = 0.3529, β x = β z = 0.5, n x = n z = 2 for the DDE simulations (dashed curves: τ x = 0.8, τ z = 0.1; solid curves: τ x = 3, τ z = 4). Note that delays can cause more complex dynamics (e.g. transient oscillations) compared to models in which effects are instantaneous, and where both models give the same long-term steady state behavior. Left: instantaneous effects modeled using ODEs, Right: delayed effects modeled using DDEs.

---

### Implementation of quantum and classical discrete fractional fourier transforms [^60c10e08]. Nature Communications (2016). Medium credibility.

Results

Theoretical approach

Similarly to its continuous counterpart, the DFrFT can be interpreted physically as a continuous rotation of the associated wave functions through an angle Z in phase space (see Fig. 1a). The idea is thus to construct finite circuits that are capable of imprinting such rotations to any light field. In quantum mechanics, three-dimensional spatial rotations of complex state vectors are generated via operations of the angular momentum operators J k (k = x, y, z) on the Hilbert space of the associated system. In particular, the rotation imprinted by the J x -operator turns out to be an elaborated definition of the DFrFT (see Methods section for discussion). These concepts can be readily translated to the optical domain by mapping the matrix elements of the J x -operator over the inter-channel couplings of engineered waveguide arrays (Fig. 1b–e). The coupling matrix of such waveguide arrays is thus given by(ref.). Here, κ 0 is a scaling factor introduced for experimental reasons. The indices m and n range from − j to j in unit steps. Meanwhile, j represents an arbitrary positive integer or half-integer that determines the total number of waveguides via N = 2 j +1 (Fig. 1b).

Coupled mode theory states that the evolution of light in the J x -waveguide array is governed by the following set of equations

Here, E n (Z) denotes the complex electric field amplitude at site n. In the quantum optics regime, single photons traversing such devices are governed by a set of Heisenberg equations that are isomorphic to equation (1). The only difference is that in the quantum case E n (Z) must be replaced by the photon creation operator. In a spintronic context, the evolution parameter Z is associated with time, whereas in the framework of integrated quantum optics, Z represents the propagation distance, see Fig. 1b–e. A spectral decomposition of the J x -matrix yields the eigenvectors, which in combination with the eigenvalues, render the closed-form point-spread function

---

### Modeling the effects of consanguinity on autosomal and X-chromosomal runs of homozygosity and identity-by-descent sharing [^fe858a1e]. G3 (2024). Medium credibility.

These patterns accord with the large- N limits for the ROH and IBD X:autosomal ratios. For ROH, thelimit of the ratio of equation (11) to equation (10) is

recalling thatis the sum of the rates of all four types of first-cousin consanguinity. Varyinginand holding, the limiting ratio is 0: patrilateral consanguinity produces no ROH on the X chromosome but a positive level of ROH on the autosomes. Forand all other consanguinity rates set to 0, the limiting ratio varies from minimum 3 to maximum. Forand all other consanguinity rates set to 0, the limiting ratio is 2 at the minimum andat the maximum. Note that the limiting function is undefined for.

Similarly for IBD, thelimit of the ratio of equation (9) to equation (8) is

At, this limit is 2, as in the case without consanguinity. Ifand the other rates are held at 0, then the limiting ratio is. If, then the limit is. If, then it is.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^702ab11e]. Nature Communications (2021). High credibility.

The logic parameter space can be divided into AND-type, OR-type, and single-input functions

Only the 8 regulatory functions on the diagonals of Fig. 5 (excluding FALSE and TRUE) make use of both inputs. These eight can be further divided into positive or negative regulation on each arm (4 possibilities) in conjunction with an AND or OR gate (8 possibilities total). Specifically, "OR-type" logic applies when both η Z 1 > 1 and η Z 2 > 1 (OR, NAND, and both IMPLY gates), and "AND-type" logic applies when both η Z 1 < 1 and η Z 2 < 1 (AND, NOR, and both NIMPLY gates). This can be seen mathematically by using Boolean logic simplification. For example, X NOR Y = NOT X AND NOT Y. Similarly, X NAND Y = NOT X OR NOT Y.

It is important to note that this logic scheme is an approximation, and in reality the sum of two Hill terms provides a form of fuzzy logic. That is, if the inputs X or Y are close to 1, or if the regulatory strengths η Z 1,2 are close to 1, then Z will also be close to 1 for some input combinations (Supplementary Note 5).

Note that if X (T) and Y (T) are independent, they can be time-shifted in Eq. (19) by γ 1 and γ 2, respectively, showing that the dynamics do not depend on delays. This is not true if X and Y are dependent (e.g. X = Y), which leads to interesting dynamics that we examine below in feedforward loops and double feedback.

---

### The making of the standard model [^0fc9a4ce]. Nature (2007). Excellent credibility.

A seemingly temporary solution to almost a century of questions has become one of physics' greatest successes.

---

### Degenerate boundaries for multiple-alternative decisions [^996fe0bf]. Nature Communications (2022). High credibility.

The general decision threshold parameterization is thenwhereTaking the product of all components of P (t) j accomplishes two things. Firstly, it reduces threshold dimensionality by one, collapsing the decision space to the number of alternatives with non-zero belief. For example, for n = 4, if one alternative has a zero probability, the decision space and thresholds collapse to the n = 3 case (the planes shown in Fig. 1), constraining how the decision boundaries intersect with the outer faces. Secondly, it introduces the simplest form of nonlinearity, as the product of all elements P (t) j parameterizes curved boundaries.

Overall, the functionmodulates the curve parameterized by the product of posterior probabilities. The variableis the maximum absolute difference between the components of the vector P (t) j, which with the addition of a parameter β allows a range of boundary shapes, such as those in equations (15–18) below. Summing these components over all subspaces generalizes (14) to n -choices, for which it is normalized by the number of 3D subspaces using the multiplier.

The general parameterization, equation (14), has three parameters: θ is the intersection of the boundary with the edges in posterior probability space, because if there are only two choices with non-zero belief values, then the right-hand side of equation (14) is zero leaving F = θ; α directly tunes the amplitude of the function f or of the curve if f = 1; and β is a free parameter whose purpose depends on the function f. Clearly, we could include additional parameters, but consider only these three for tractability.

There is no prevailing method for learning arbitrary functions with stochastic rewards, so we use the general form given by equation (14) and a range of functions f over which to explore the optimality of nonlinear higher-dimensional decision boundaries. The considered functions are:Examples are shown in Fig. 2. The constant-valued boundaries are dashed in all subsequent examples for comparison. Note that setting α = 0 or β = 0 for any of (16–18) recovers the flat case (15).

---

### Advancing mathematics by guiding human intuition with AI [^0bfbd7a0]. Nature (2021). Excellent credibility.

Methods

Framework

Supervised learning

In the supervised learning stage, the mathematician proposes a hypothesis that there exists a relationship between X (z) and Y (z). In this work we assume that there is no known function mapping from X (z) to Y (z), which in turn implies that X is not invertible (otherwise there would exist a known function Y ° X −1). While there may still be value to this process when the function is known, we leave this for future work. To test the hypothesis that X and Y are related, we generate a dataset of X (z), Y (z) pairs, where z is sampled from a distribution P Z. The results of the subsequent stages will hold true only for the distribution P Z, and not the whole space Z. Initially, sensible choices for P Z would be, for example, uniformly over the first N items for Z with a notion of ordering, or uniformly at random where possible. In subsequent iterations, P Z may be chosen to understand the behaviour on different parts of the space Z (for example, regions of Z that may be more likely to provide counterexamples to a particular hypothesis). To first test whether a relation between X (z) and Y (z) can be found, we use supervised learning to train a functionthat approximately maps X (z) to Y (z). In this work we use neural networks as the supervised learning method, in part because they can be easily adapted to many different types of X and Y and knowledge of any inherent geometry (in terms of invariances and symmetries) of the input domain X can be incorporated into the architecture of the network. We consider a relationship between X (z) and Y (z) to be found if the accuracy of the learned functionis statistically above chance on further samples from P Z on which the model was not trained. The converse is not true; namely, if the model cannot predict the relationship better than chance, it may mean that a pattern exists, but is sufficiently complicated that it cannot be captured by the given model and training procedure. If it does indeed exist, this can give a mathematician confidence to pursue a particular line of enquiry in a problem that may otherwise be only speculative.

---

### Supercritical fluids behave as complex networks [^770d11e8]. Nature Communications (2023). High credibility.

With equations (7) and (8) yielding a closed analytical solution for the total number of links in the network model, we proceed to connect this model to the MD system. The total number of links, M, is assumed as a known quantity and we numerically solve for the corresponding link density z that serves as input in generating an appropriate complex network. For the network to converge to a statistically steady state, we adopt the approach of Lin et al.: the presented data is averaged over 15 N iterations with the self-consistent histogram method. The steps for generating the network are summarized as follows.
Assign a fitness value, x i, to each node i using Equation (9): x i ~ g (x; α, β).
Calculate the probability, f (x i, x j), for a link between nodes i and j using Equation (6).
Create a symmetric adjacency matrix by comparing the probability, f (x i, x j), against a random number selected from a uniform distribution between (0, 1). No self-links are allowed.
From the adjacency matrix, calculate topological properties of the network model.
Repeat steps 1–4 for 15 N times.

Because we aim for the network model to depend only on experimentally measurable quantities, we seek to relate the link density, z, to the Widom self-similarity. Because it has been shown that, in the thermodynamic limit, the hidden-variable model maintains an invariant cluster size distribution in terms of z N, we propose the following expression:which is a regression (see Supplementary Fig. 7) of the calculated z values from the total number of links, M, in the MD data via Equations (7) and (8). Because the link density z in Equation (10) is directly calculated from experimentally measurable thermodynamic quantities through Equation (4), this relation allows for a self-contained network model to predict the microscopic structure of supercritical water. We note that the generalization of this representation to other fluids requires further investigation.

---

### An L0-norm-based method for passive shimming design in MRI: a simulation-based study [^551744d0]. NMR in Biomedicine (2025). Medium credibility.

2.2 Linear Programming–Based Solution Approach

The above optimization problem can be solved using the linear programming (LP) method. LP is a technique used to optimize a linear objective function while subject to a set of linear constraints. In the case where the initial solution X 0 is zero, the corresponding optimization problem becomes linear and can be readily expressed as the following LP model:

Subject towhere the objective function is a standard L1‐norm problem and t max represents the maximum thickness allowed for the iron pieces to be placed in the shim pocket.

When the initial solution X 0 is non‐zero, indicating that part of the shim pockets is not empty, the corresponding optimization problem becomes a nonlinear function due to both positive and negative components appearing in the solution vector X. Thus, the objective function becomes a minimization of the absolute value problem. To handle this using the LP model, the non‐linear problem needs to be converted into a linear model, as illustrated below:

Subject towhere V = (v 1, v 2,…, v N) is a vector, whose components are all positive and within the range of 0 to t max, and the constraints above ensure that each component of the solution vector X does not exceed the corresponding components of V.

Once the optimization problem is formulated as a linear model, efficient algorithms like the simplex method can be applied to find the solution. The LP‐based solution is generally effective in generating feasible passive shim patterns. However, as shown in Equations (2) and (3), the mathematical model focuses on the total iron consumption (thickness) and does not directly consider the number of shim pockets involved in shimming. Consequently, the LP approach may produce a solution that requires accessing too many shim pockets. Since the time required for PS in each iteration is directly related to the time taken for each pocket adjustment, the LP‐based solution's operational efficiency is thus not optimized. It is then desirable to develop an optimization model that can minimize the number of pocket adjustments, therefore enhancing shimming efficiency.

---

### Mathematical modeling is more than fitting equations [^994275ea]. The American Psychologist (2014). Low credibility.

Comments on the comments made by Brown et al. (see record 2013-24609-001). The article by Brown et al. regarding the Fredrickson and Losada (see record 2005-11834-001) article discussed the use of differential equations in science and repeated our earlier observation (Luoma, Hämäläinen, & Saarinen, 2008) that there is lack of justification for the use of the Lorenz equations in the latter article. In this comment we want to point out that Brown et al. presented a very narrow view on mathematical modeling in behavioral research. We describe how the conceptual use of mathematical models is essential in many fields.

---

### Rigorous location of phase transitions in hard optimization problems [^d98ebea5]. Nature (2005). Excellent credibility.

It is widely believed that for many optimization problems, no algorithm is substantially more efficient than exhaustive search. This means that finding optimal solutions for many practical problems is completely beyond any current or projected computational capacity. To understand the origin of this extreme 'hardness', computer scientists, mathematicians and physicists have been investigating for two decades a connection between computational complexity and phase transitions in random instances of constraint satisfaction problems. Here we present a mathematically rigorous method for locating such phase transitions. Our method works by analysing the distribution of distances between pairs of solutions as constraints are added. By identifying critical behaviour in the evolution of this distribution, we can pinpoint the threshold location for a number of problems, including the two most-studied ones: random k-SAT and random graph colouring. Our results prove that the heuristic predictions of statistical physics in this context are essentially correct. Moreover, we establish that random instances of constraint satisfaction problems have solutions well beyond the reach of any analysed algorithm.

---

### Linear reinforcement learning in planning, grid fields, and cognitive control [^1bda7376]. Nature Communications (2021). High credibility.

Efficient solution is possible because, substituting the penalized rewards into the Bellman equation, the optimal value function is now given by a non-recursive, linear equation, :such as can be computed by a single layer of a simple, linear neural network. Here, v * is a vector of the optimal values (now defined as maximizing cumulative reward minus control cost) for each state; r is a vector of rewards at a set of "terminal" states (i.e. various possible goals); P is a matrix containing the probability of reaching each goal state from each other, nonterminal, state; and the key matrix M, which we call the DR, measures the closeness of each nonterminal state to each other nonterminal state (in terms of expected aggregate cost to all future visits) under the default policy. This is similar to the SR (S π, Eq. (3)), except that it is for the optimal values v * (not the on-policy values v π), and v * is systematically related to optimal values as defined in the original problem (, Eq. 1), with the difference being the additional penalties for deviation from the default policy. But these exert only a soft bias in π * toward π d, which furthermore vanishes altogether in an appropriate limit (see "Methods"). Thus, while M does depend on the default policy π d, it is stable over changes in goals and independent from π * in the sense that it can usefully find optimized policies π * (solving the interdependent optimization problem) even when these are far from π d. In comparison, v π (computed from the SR: S π) is only a useful approximation to v * (and thus only helpful in finding a new π *) when the SR's learned policy π is near the target policy π *. Effectively, linear RL works by introducing a smooth approximation of the "max" in Eq. (2), since the log-average-exp (with the average here taken with respect to the default distribution, π d) of a set of values approximates the maximum. The control costs, then, simply capture the difference between the original solution and the smooth approximate one. Note that distinguishing between terminal and nonterminal states is necessary, as only for this type of finite decision problem are the optimal values linearly computable; however, this places few limits on the flexibility of the model (see "Discussion").

---

### Minimum electric-field gradient coil design: theoretical limits and practical guidelines [^2e257fcf]. Magnetic Resonance in Medicine (2021). Medium credibility.

Definingas the vector of unknown basis function amplitudes, the solution can be cast into a quadratic programming problem that minimizes energy:subject to the inequality and equality constraints

where M denotes a mutual inductance matrix, and M i, j represents the mutual inductance between basis functions i and j of unit amplitude, and is readily computed from the known current distribution of each basis function. Alternatively, M can be replaced by a mutual resistance matrix to minimize power dissipation, or a weighted combination can be used. We use a mutual inductance matrix for the present work.

The matrices H and C contain the linear constraint equations: one column for each unknown basis function, and one row for each constraint. The desired gradient strength is achieved by enforcing an equality constraint equation at the isocenter, where the value of ce is the negative of the desired gradient strength, and the column entries in the H matrix are the gradient strengths for each basis function of unit amplitude. A B 0 concomitant field constraint can be added that requires the corresponding transverse field component to be zero at the isocenter. Other field constraints such as positional error and pixel size are handled in a similar manner. Within the imaging volume, inequality constraints are placed at multiple points over the imaging volume on the pixel size error and the fractional positional error defined aswhere n is the desired gradient direction (X, Y, or Z); G is the desired gradient strength at isocenter; R is the radius of the imaging volume; and B z is the longitudinal component of field. For the analysis here, we use values of 50% for pixel size (uniformity) error and 16.5% for fractional positional (linearity) error over a 260‐mm‐diameter spherical volume. These typical values (which correspond to design parameters used for a recently described head gradient 14) allow the pixel size to vary by a maximum of a factor of 2 and the maximum positional error to be 21.5 mm at the periphery of the imaging volume.

---

### Children's arithmetic skills do not transfer between applied and academic mathematics [^7138575e]. Nature (2025). Excellent credibility.

Compositional structure

We asked working children how they solved the hypothetical market maths problems involving goods from their store. Many children simplified calculations by using rounding and decomposition techniques that leverage the base-ten structure of the number system (44% when problems involved standard market prices and 36% when they involved new prices) (Supplementary Table 19). They did so by converting complex operations (for example, 11 × 43) into a series of simpler ones (for example, solving 11 × 43 by converting the problem to (10 × 43) + 43). A large proportion (26% at market prices and 16% at new prices; Supplementary Table 19) also used approximations to simplify calculations by rounding subtotals up or down by 1 or 5 (for example, approximating 0.7 kg of cauliflower at 20 rupees per kilogram to be 15 rupees). These methods may explain how working children solve complex problems so quickly and accurately without the need for pen and paper.

We then tested whether children leveraged these strategies on school-like abstract and anchored problems. We compared their performance on problems with an operand ending in 1 or 9 (roundable problems) with their performance on problems presenting numbers of similar sizes lacking those endings (non-roundable problems such as 12 × 42). Working children performed slightly better on roundable problems (67% correct) than on non-roundable problems, although the difference was not significant at the 5% level (64% correct, β = 0.03, s.e.m. = 0.02, 95% CI = 0.00–0.07, P = 0.083; Fig. 5b and Supplementary Table 20). For non-working children, we did not see evidence of a difference: 75% answered roundable arithmetic questions correctly compared with 74% of non-roundable ones (β = 0.00, s.e.m. = 0.02, 95% CI = –0.04 to 0.05, P = 0.868; Fig. 5b and Supplementary Table 20). It was notable that non-working school children did not use this structure, particularly as it forms the foundations of the arithmetic algorithms taught in school. Moreover, working children, who apparently discovered this structure over the course of their work in markets, had not mastered the school-taught algorithms that were based on it.

---

### Cox process representation and inference for stochastic reaction-diffusion processes [^7bfb2feb]. Nature Communications (2016). Medium credibility.

The reaction–diffusion master equation

Consider a system as in equation (13), but in an M -dimensional volume discretized into L cubic compartments of edge length h and volume h M. Denote as n = (n 1 1,…, n N 1,…, n 1 L,…, n N L) the state of the system, whereis the copy number of species X i in the l th compartment. Under well-mixed and dilute conditions in each compartment, the reaction dynamics in each compartment is governed by a corresponding chemical master equation as in equation (14). If we model diffusion of species X i between neighbouring compartments by linear reactions with rate constant d i = D i / h 2, where D i is the microscopic diffusion constant of species X i, the time evolution of the (single time) marginal probability distribution of the system obeys the RDME:

where f r (n l) is the propensity function of the r th reaction evaluated at the state vectorof the l th compartment, is a vector of length N × L with the entry corresponding to species X i in the l th compartment equal to 1 and all other entries zero, andis a vector of length N × L with the entries corresponding to the l th compartment equal to the r th row of the stoichiometric matrix S and zero otherwise.

Real-valued Poisson representation in space

We next apply the PR to the RDME in equations (23) and (24) after applying the mean-field approximations defined in the Results section to bimolecular reactions and reactions with two non-identical product molecules, and subsequently take the continuum limit. Consider first the diffusion term in equation (23). Since different species do not interact with each other if there are no chemical reactions happening, we can consider a system containing only a single species, say species X 1, for which equation (23) reduces to

where n = (n 1,…, n L), n m is the number of X 1 particles in the m th compartment, δ m is a vector with a one in the m th entry and zero otherwise, d is the diffusion constant of species X 1 and the sum over m runs over all neighbouring compartmentsof the l th compartment. For this system, the PR is real and deterministic, and we use the PR without any approximations. The corresponding Langevin equation reads

---

### Sample size calculations: basic principles and common pitfalls [^c0e79f8a]. Nephrology, Dialysis, Transplantation (2010). Low credibility.

One of the most common requests that statisticians get from investigators are sample size calculations or sample size justifications. The sample size is the number of patients or other experimental units included in a study, and determining the sample size required to answer the research question is one of the first steps in designing a study. Although most statistical textbooks describe techniques for sample size calculation, it is often difficult for investigators to decide which method to use. There are many formulas available which can be applied for different types of data and study designs. However, all of these formulas should be used with caution since they are sensitive to errors, and small differences in selected parameters can lead to large differences in the sample size. In this paper, we discuss the basic principles of sample size calculations, the most common pitfalls and the reporting of these calculations.

---

### Topological magnetoplasmon [^986797eb]. Nature Communications (2016). Medium credibility.

Chern number on an infinite momentum plane

Unlike the regular lattice geometries whose Brillouin zone is a torus, our system is invariant under continuous translation, where the unbounded wavevector plane can be mapped on a Riemann sphere. As long as the Berry curvature decays faster than q −2 as q →∞, the Berry phase around the north pole (q = ∞) of Riemann sphere is zero and the Chern number C is quantized. For our 2D MP, we can verify this analytically,

where d S q is the momentum–space surface element, and the term in the curly bracket is the Berry curvature. Since the Berry curvature changes its sign under the particle–hole symmetry, the positive and negative-frequency bands must have opposite Chern numbers, and the zero-frequency band, with odd Berry curvature, must have zero Chern number.

Majorana-type one-way edge states

We are able to calculate the analytical edge solutions in the long-wavelength limit q →0, and numerical edge solutions for unrestricted wavelengths, as plotted in Fig. 2. When q →0, the Hamiltonian in equation 6 becomes local, allowing us to replace q x and q y with the operators −i ∂ x and −i ∂ y and to solve for the edge states by matching boundary conditions. We consider a 1D edge situated at x = 0 caused by a discontinuity either in n 0 (x) (Configuration-I) or in B 0 (x) (Configuration-II).

In Configuration-I as shown in Fig. 2a, we let the x > 0 region be filled with 2DEG, while the x < 0 region be vacuum, and the magnetic field be uniformly applied parallel to e z. Since we know that the change of Chern number across the edge is Δ C = ± 1, there must be a single topological edge state present in this configuration. We look for solutions that behave like, in the x > 0 region, whereis an evanescent wavenumber. The boundary condition is, meaning that the component of the current normal to the edge must vanish. Therefore, the edge solutions have to satisfy, whose right-hand side is always positive. Since we have chosen, q y must be positive too. The edge state is hence one-way propagating. (We discard the unphysical solutionthat yields a null wavefunction.) The physical solution is, which gives an edge state spectrum

---

### Observation and theory of X-ray mirages [^6bb03466]. Nature Communications (2013). Medium credibility.

Numerical simulations

As already noted, the elaborated theory of the mirage and the mirage equation itself are written for an arbitrary-extended optically inhomogeneous amplifying medium, in particular, for any spatial distribution of n e (x, y, z) and G (x, y, z) of the plasma. For the solution of the whole problem, that is, for calculation of mirage radiation at the output of the plasma and for finding the position and shape of the virtual source in a particular physical situation, it is enough to numerically integrate equations (2 and 3) for given n e (x, y) and G (x, y). To do this, we rewrite the original equation (2) as:

whereand the complex amplitude of an incident field A inc (x, y, z) is considered to be given and the complex amplitude of X-ray mirage A M (x, y, z) = 0 at z = 0. In our opinion, the most effective numerical method for solving such an equation is the split-step method, which we applied for our calculations. For numerical solution of this parabolic equation, we used a rectangular grid of N x × N y elements with the spatial step sizes Δ x and Δ y in the x and y directions correspondingly. Decomposition of the total field into two fields, A M (x, y, z) and A inc (x, y, z), has allowed us to have a limited area where A M (x, y, z) is non-zero and also allowed to use the periodic boundary conditions of the split-step method.

In numerical calculations, we used a diffraction limited Gaussian beam A G (x, y, z) = as an incident radiation A inc (x, y, z). As equation (3) is a special case of equation (2), we used the same numerical procedure for its solution.

The calculation scheme is graphically shown in Supplementary Fig. S3. For the main calculations, we used the computational volume of 65 × 68 × 6,000 μm 3 size with 640 × 670 elements in x − y computational grid. The number of steps along z axis was 670. For the calculations according to the simplified model, described in Supplementary Discussion, the numbers were 200 × 200 × 6,000, 1,024 × 1,024 and 1,024 correspondingly.

---

### A Bayesian re-assessment of two phase II trials of gemcitabine in metastatic nasopharyngeal cancer [^d23b11d0]. British Journal of Cancer (2002). Low credibility.

Bayesian design and analysis

The foundation of this approach is Bayes' theorem, which can be expressed as

Here the lik (x |θ) of equation (1) is multiplied by the prior distribution, prior (θ), to obtain the posterior distribution, post (θ| x). The prior (θ) summarises what we know about θ before the trial commences and as previously, lik (x |θ) describes the data to be collected from the trial itself. Finally post (θ| x) summarises all we know about θ once the trial is completed.

The prior distribution is assumed to be of the form

This is a Beta distribution with parameters α and β, which can take any positive real value. When α and β are integers, such a distribution corresponds to a prior belief equivalent to having observed α responses out of a hypothetical T = (α+β) patients. This is then similar to the situation modelled by the binomial distribution (see equation (1)), in which we have x as the number of responses from N patients.

Using equations (1) and (3) in equation (2) results in a posterior distribution of the form

Comparing this with equation (3), we see that this too is a Beta distribution, but of the form Beta (α+ x, β+ N−x).

As mentioned previously, the posterior distribution represents our overall belief at the close of the trial about the distribution of the population parameter, θ. Once we have obtained the posterior distribution, we can calculate the exact probabilities of θ being in any region of interest or obtain summary statistics such as its mean value.

Priors

The prior distribution summarises the information on θ before the trial commences. We make use of four such types of distributions – clinical, reference, sceptical and enthusiastic. The general way in which each of these are derived is as follows.

The shape of a Beta distribution is dependent on the values of the parameters α and β, and each of the priors will have particular values associated with them. However, eliciting values for α and β is typically not an easy process. Instead, it is often much easier to obtain values for the mean (M) and variance (V) of the corresponding prior distribution. Once obtained, these values can then be used to obtain α and β by solving the simultaneous equations

which give

---

### Magnetostatic twists in room-temperature skyrmions explored by nitrogen-vacancy center spin texture reconstruction [^0550fa1f]. Nature Communications (2018). Medium credibility.

Topology of the solutions

In order to select the best candidate texture, we study the topology of the 2D vector field m (ρ, λ). For any 2D normalized vector field n (ρ), the topological number Q is defined as:Whenever n || z at the boundary, any continuous solution n (ρ) must have an integer Q value. Non-integer values of Q occur in the case of a discontinuity, which is energetically costly and unstable. Meanwhile, skyrmions are stable against local perturbations because of the large energetic cost preventing the skyrmion (Q = 1) from folding back into the ferromagnetic state (Q = 0). We therefore introduce continuity as a criterion for selecting physically allowed solutions. In Fig. 4b, we plot the absolute value of Q (λ) for each of the normalized vector fields n (ρ, λ), with n being the unit vector in the direction of m. The number Q can be visualized as the number of times the spin configuration n wraps around the unit sphere. To illustrate the value of Q, in the inset of Fig. 4b we plot the solid angle spanned by n while moving in the (x, y) plane. We obtain a value for Q approaching −1 as λ → 1. We therefore identify Néel or nearly-Néel solutions as the only ones compatible with the measured data.

---

### Data-driven discovery of dimensionless numbers and governing laws from scarce measurements [^310b7972]. Nature Communications (2022). High credibility.

The optimization of dimensionless learning is different from general regression optimization approaches because only dimensionless numbers with small rational powers are preferred, such as −1, 0.5, 1, or 2, etc. Therefore, instead of searching for the best basis coefficients with a lot of decimals like other neural network-based methods, such as DimensionNet, zero-order optimization methods are used in this work. It includes grid search or pattern search-based two-level optimization and can be more efficient in finding the best basis coefficients. No gradient information and learning rate are required and the choice of grid interval is more flexible. Even though these zero-order optimization approaches can get stuck in local minima, increasing the number of initial points can easily eliminate this issue. More detailed pros and cons of different optimization methods are described in Section 4.5 of the SI.

The proposed method divides the identification process of differential equations into two steps to identify consistent parameterized governing equations efficiently. The first step is to identify a temporary governing equation in which the regression coefficients can be a constant or variable depending on how the simulation or experiment parameters are set. In the next step, dimensionless learning aims to recover the expression of the varying coefficients by leveraging the dimension of these coefficients. By combining these two steps, the proposed method can efficiently obtain a consistent dimensionally homogeneous governing equation with a small amount of data. In contrast, the standard SINDy falls short of achieving a consistent parameterized differential equation for the same system with different parameters. For example, the governing equation for the spring–mass–damper system is. If we use different parameters (damping coefficient c, spring constant k, or mass m) in this system, SINDy can only provide scalar coefficients for x andrather than the expressionsand, respectively. Other advanced SINDy approaches deal with this issue by multiplying the candidate terms by a set of predetermined parameters. Although these approaches can address this inconsistent governing equation problem, it couples the optimization of identifying candidate terms and parameterized coefficients, making the optimization more difficult. If there are many combinations of parametric derivative or non-derivative terms, this problem can become more difficult and unmanageable.

---

### Determination of X-ray detection limit and applications in perovskite X-ray detectors [^c02fe79a]. Nature Communications (2021). High credibility.

Assuming Group 1 and Group 2 represent the gross signal (photocurrent under X-ray I X–ray in the case of X-ray detector) and the blank signal (dark current I dark in the case of X-ray detector) throughout in this work (equivalently vice versa), respectively, then a prior detection limit Δ = | μ 1 − μ 2 | can be calculated with the measured blank signal of Group 2, i.e. (μ 2,) and n 2 from the DL equations, with some necessary pre-assumptions (we use the term " prior " because this method requires pre-assumptions from a statistical perspective). We make assumptions consistent with the Currie method and show that the DL equations is same as Currie formulars when sample sizes (i.e. number of sampled data points) are being reduced (Fig. 2c). Instead of setting α = 0.13% and β = 50% as in the IUPAC definition, we set more properly α = β = 0.05, corresponding to z 1− α = z 1− β = 1.645, which is consistent with Currie formulas. The standard deviation of the gross signal and the blank signal are assumed approximately equal as is in the Currie formulars, i.e. = , when the gross signal is small approaching the level of detection limit. The influence of sample size (i.e. number of sampled data points) on the detection limit is reflected by the value of k = n 2 / n 1. If we set n 2 = n 1 = 1, we have k = 1. Then the DL equations reduce to. With the assumptions of = and z 1− α = z 1− β = 1.645, we have, which is the detection limit L D in Paired observations case in Currie's classical paper(Table 1). If we set n 1 = 1 and n 2 > > n 1, that is, k = n 2 / n 1 > > 1, so that > > or > >, then the DL equations reduce to. With the same assumptions, we have Δ = | μ 1 − μ 2 | = 3.29 σ 2, which is the detection limit L D in "Well-known" blank case in Currie's paper(Table 1). The DL equations are based on Normal distribution, which makes it applicable to X-ray detectors working in a continuous current mode.

---

### Constrained instruments and their application to mendelian randomization with pleiotropy [^dac28aa8]. Genetic Epidemiology (2019). Medium credibility.

Table 2
The parameter settings used in the two series of simulations

Structural equations for simulation Series I: Standard pleiotropy where ε x, i, ε z, i, ε y, i, u i ∼ N (0, 1).

Structural equations for simulation Series II: Z → X where ε x, i, ε z, i, ε y, i, u i ∼ N (0, 1).

In each simulated data set, 100 SNPs (G in Equation (9)) with a minor allele frequency of 0.33 and n = 500 observations were generated, with values coded as (0, 1, 2). Among all 100 SNPs there are p z ∈ {20, 50} SNPs directly related to Z. Notice that smaller values of α x = 0.1 represent weak instruments G for X, while large values of α z = 1 represent strong instruments G for Z. Therefore, our scenarios comprise weak and strong instruments for one or both of X and Z. Two hundred datasets were generated for each scenario, and results compared the estimates and variance of the causal effect, β.

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^37daae40]. Nature Communications (2025). High credibility.

Fig. 2
The matrix-tree theorem.

A Computing the steady-state occupancy π 1 by summing weights over directed spanning trees. Directed spanning trees are subgraphs containing all graph nodes but no cycles, with edges oriented toward a root node. In each directed spanning tree, the input forces make a positive, negative, or zero contribution to the tree weight. The structural vectorsare shown below each tree; these quantities enter into Equation (3) below. B Schematic illustration of the high-dimensional space of feature vectors ψ (i; θ) and χ (i, F). The depicted arrangement of vectors could solve a binary classification problem.

We define the input multiplicityas the number of edges affected per input variable, which we assume to be the same for each input. To focus on the functional way in which the input driving enters the steady-state probabilities, the driving contributions can be factored out in the algebraic expressions for the numerator and denominator of Equation (1). This has been previously been used to make analytical progress for M = D = 1 in, for example, refs. –. This equivalent formulation of Eq. (1) suggests that steady states of Markov jump processes implement a rational polynomial function of exponentiated input variables. Defining, we rewrite the matrix-tree expression for π i for general D and M We use the multi-index, whereis the set of D input labels and each componentof the multi-index runs over the values, to enumerate themonomials. These monomials y μ (F) in Equation (2) combinatorially depend on the different mixtures μ of input driving, representing a net total μ a of signed contributions from the input force F a, μ b such contributions for F b, and so on for each input. The coefficients, which are functions of the parameters θ, are the sums of weights over all directed spanning trees rooted at node i which have the corresponding mixture μ of signed input contributions. The monomial coefficientsthus represent learnable amplitudes of each polynomial basis function y μ (F). The coefficients in the denominator are defined as. Classification will be successful if, for F ρ drawn from class ρ, the coefficientsand monomials y μ (F ρ) are large for the same μ. In the subsequent sections of the paper and in the Supplementary Information we use the formulation in Equation (2) to show how the classification ability of a non-equilibrium Markov processes may be systematically modulated.

---

### Kronos scRT: a uniform framework for single-cell replication timing analysis [^4c18ce97]. Nature Communications (2022). High credibility.

As the MAPDC value increases linearly with the square root of the cell coverage, it was normalized as follows (formula 7) to obtain the DIMAPD:where MAPDC x is defined in formula (6), C x in formula (5), and C is a vector containing the C x values for all cells in the experiment; a and b are two coefficients estimated through a linear fitting of MAPDC in function of the cell coverage distance from the median coverage of the experiment.

CNs are called starting from 20 kb bin tracks that are smoothed and segmented using a circular binary segmentation algorithm from the R package DNAcopy (version 1.56.0). Then, CNs are estimated by minimization of the following target function, as suggested by the 10x Genomics CNV Solution (formula 8):where S n represents the size of segment n, R n represents the read count of segment n, and X is a number between the 5 th and the 95 th percentile of the read counts of all segments in a cell. Each local minimum of this equation is a possible solution to calculate CN. Therefore, a filter on local minimum values that lead to unreasonable mean ploidy (formula 10) is applied, and CN is calculated (formula 9). The filters used in this study can be found in Supplementary Table 2 (Ploidy limits). For sorted G1 and sorted mid S-phase cells in the mouse datasets, the ploidy value closer to 2 was selected:where X min is the value of X forwhich is minimized (formula 7), R n is the read count in segment n, and CN n is an integer that represents the CN of segment n. The mean ploidy of a cell can then be calculated as follows (10):where P is the mean ploidy, S n is the size of segment n, and CN n is the copy number of segment n. The difference between the absolute minimum values and its closest relative minimum is used to evaluate how good the CN calling is. For values < 2, the CN is not considered reliable (ploidy confidence). Negative values of ploidy confidence are imposed, as suggested by 10X Genomics. Bins included in the provided blacklistare removed, as done here with the bins for mESCs and mNE-7d cells.

---

### The minimal work cost of information processing [^894e044e]. Nature Communications (2015). Medium credibility.

where the sum ranges only over those x that have a non-zero probability of occurring. In the case of deterministic mappings p (x ′| x)∈{0,1}, this corresponds to the maximum number of input states that map to a same output state. For the AND gate, provided all four states 00, 01, 10 and 11 have non-negligible probability of occurring, there are three input states mapping to the same output state, so (3) gives us simply. Also, in simple examples as considered here, the expression (3) is stable to considering an-approximation (Supplementary Note 4); this quantity is thus physically justified.

Crucially, our result reveals that the minimal work requirement in general depends on the specific logical process, and not only on the input and output states. This contrasts with traditional thermodynamics for large systems, where the minimal work requirement of a state transformation can always be written as a difference of a thermodynamical potential, such as the free energy. For example, the minimal work cost of performing specifically an AND gate may differ from that of another logical process mapping an input distribution (p 00, p 01, p 10, p 11) (with ∑ i p i = 1) to the distribution (p ′ 0, p ′ 1) = (p 00 + p 01 + p 10, p 11) (Recall that the classical counterpart of a quantum state is a probability distribution.). To see this, consider the XOR gate, which outputs a 1 exactly when both inputs are different (see Fig. 2b). The minimal work cost requirement of this gate, as given by (3), is now only kT ln 2, as in the worst case, only a single bit of information is erased (again supposing that all four input states have non-negligible probability of occurring). Now, suppose that, for some reason, the input distribution is such that p 01 + p 10 = p 11, that is, the input 11 occurs with the same probability as of either 01 or 10 appearing. Then, the XOR gate reproduces the exact same output distribution as the AND gate: in both cases, we have p ′ 0 = p 00 + p 10 + p 01 = p 00 + p 11 and p ′ 1 = p 11 = p 01 + p 10. In other words, both logical processes have the same input and output state, yet the XOR gate only requires work kT ln 2 compared with the AND gate, which requires 1.6 kT ln 2. Furthermore, we point out that this difference, which appears small in this case, may be arbitrarily large in certain scenarios (Supplementary Note 4).

---

### Towards large-scale quantum optimization solvers with few qubits [^13944394]. Nature Communications (2025). High credibility.

Quantum computers hold the promise of more efficient combinatorial optimization solvers, which could be game-changing for a broad range of applications. However, a bottleneck for materializing such advantages is that, in order to challenge classical algorithms in practice, mainstream approaches require a number of qubits prohibitively large for near-term hardware. Here we introduce a variational solver for MaxCut problems over m ≥ O(nᵏ) binary variables using only n qubits, with tunable k > 1. The number of parameters and circuit depth display mild linear and sublinear scalings in m, respectively. Moreover, we analytically prove that the specific qubit-efficient encoding brings in a super-polynomial mitigation of barren plateaus as a built-in feature. Altogether, this leads to high quantum-solver performances. For instance, for m = 7000, numerical simulations produce solutions competitive in quality with state-of-the-art classical solvers. In turn, for m = 2000, experiments with n = 17 trapped-ion qubits feature MaxCut approximation ratios estimated to be beyond the hardness threshold 0.941. Our findings offer an interesting heuristics for quantum-inspired solvers as well as a promising route towards solving commercially-relevant problems on near-term quantum devices.

---

### Bayesian optimization for demographic inference [^dd901468]. G3 (2023). Medium credibility.

Given the posterior Gaussian process, the locationto evaluate the target function next is chosen by solving

whereis the acquisition function defined in terms of the posterior process. Common choices of α include

Here "EI" stands for expected improvement and "PI" stands for probability of improvement — which, respectively, is what these functions are meant to predict. The acquisition functionemphasizes high probability of getting some improvement over the current minimum, while the acquisition functionemphasizes getting the maximal possible improvement. The acquisition functionbyis the analog ofintended for use in conjunction with-transformed data, i.e. whenandare fed into Gaussian process regression instead of. We will use it to model negative log-likelihood, which in many cases of interest is indeed positive. This means that forGaussian process regression will be used to predict the logarithm of negative log-likelihood.

For a Gaussian processwith knownand, these acquisition functions may be computed in closed form and efficiently optimized by gradient descent (with restarts). Further we refer to the acquisition functions in Equations (1)–(3) by the names, and, respectively.

A part of Bayesian optimization's workflow is illustrated in Fig. 1.

Fig. 1.
A fragment of Bayesian optimization's workflow. The blue line (in the middle of the shaded region) is the prediction of the Gaussian process regression, i.e. the shaded blue regions represent uncertainty bars whose height is proportional to. The red curve (line below) is the acquisition function. (a) Gaussian process regression is performed on previous target function evaluations. (b) Function is evaluated (⚬) at the arg max (dashed line) of the acquisition function (red). (c) New data point is added, the regression is performed anew starting the next iteration.

Gaussian process regression

Gaussian process regression models an unknown function ϕ from a datasetwhereare (noisy) observations of. It provides well-calibrated uncertainty bars alongside with its predictions. As its name suggests, it is based on Gaussian processes.

---

### Epistasis shapes the fitness landscape of an allosteric specificity switch [^f188216a]. Nature Communications (2021). High credibility.

The Bahadur expansion was used to analyze the data. Fitness for the bahadur expansion was defined as

Fold induction in Eq. (2)was changed to basal gene expression, maximum gene expression, or EC 50 for each functional parameter. Each mutant can be represented as a numerical string (z string), where each mutable position is one number (z i) in the string. A wild-type residue at a position is designated by a −1 while the mutated residue is designated by a 1. The mutant M167L+F168Y thus becomes [−1, −1, 1, 1]. The interaction terms can be modeled as follows:

An orthonormal matrix of psi-values is created based on the combinations of mutations within the set (Supplementary Table 4). The Bahadur coefficients can be calculated using this orthonormal matrix and a fluorescence values f (x) for a particular mutant x in the set of all mutants X.

The fluorescence of each combinatorial mutant can be calculated based on the Bahadur coefficients and z string.

The R 2 between the modeled fluorescence values and the experimental data is 1.0 when all interaction terms are included in the expansion. By truncating Eq. (9) to contain only low-order interactions, the effect of these contributions to the model can be determined. The expansion was applied to the full set of mutations (4 positions) and modeled using first-order terms; first- and second-order terms; first-, second-, and third-order terms; and all terms (Supplementary Fig. 19). An identical approach was applied to all 24 subnetworks and utilized only first-order terms in the reconstruction (Supplementary Fig. 20).

Errors in the R 2 statistics were estimated using a Monte Carlo simulation. 500 sets of fitness values for all mutants were sampled based on experimental fitness means and standard deviations following a Gaussian distribution using the NumPy module in Python 2.7. Equations (8) and (9) were applied to reconstruct the fitness values and calculate R 2 values between the sampled model and the sampled data to give a distribution of R 2 values. Bias-corrected adjusted 95% confidence intervals were calculated by obtaining the average R 2 of 10,000 bootstrap iterations of the Monte Carlo simulation R 2. The bahadur expansion was applied to each functional parameter.

---

### Efficient neural codes naturally emerge through gradient descent learning [^8fec235d]. Nature Communications (2022). High credibility.

What W learns: autoencoding objective

Further describing the growth of singular values requires a choice of objective. The base case of our study is the autoencoding objective defined for a set of inputs X:

Our goal is to determine how W evolves for this cost function. We will examine both the singular vectors and the singular values.

During learning, the singular vectors rotate (recall they are unit length and orthogonal) until they reach a fixed point. For this cost function, it is easy to verify that a fixed point of dynamics is when the singular vectors are equal to the principal components of the inputs (see Supplementary Methods for proof). That is, the vectors are static when Σ x x = V Λ V T and W = V S V T for the same V but potentially different Λ and S. This alignment is especially relevant given the expression for network sensitivity derived above. With the vectors aligned, the sensitivity to each corresponding principal component of the inputs is given by, the squared singular value of W.

The evolution of sensitivity is thus governed by the evolution of singular values. The rate of change of σ i is complicated to calculate because the singular vectors can potentially rotate. However, for the sake of analysis one can examine the case when the singular vectors are initialized at the fixed point mentioned above, as in previous literature. In this set of initial conditions, the time-evolution of each singular value of W is given by refs.:

Note that the rate of learning is controlled by λ i, the standard deviation of the i th principal component of the inputs. The term on the right causes σ i (t) to converge to 1 asymptotically, as is expected as the solution of the full-rank reconstruction problem is W = I. For deeper networks (N ≥ 2), the growth is sigmoidal and approaches a step function as N → ∞ (see ref.). Thus, in this axis-aligned initialization, the singular values σ i (t) are learned in order of the variance of the associated principal components of the inputs.

Together, these results mean that the sensitivity of a linear network's output to the principal components of the inputs evolve in order of variance when trained on input reconstruction. This is exactly the case for the axis-aligned initialization and approximately true for small initializations. For the single-matrix network displayed in the figure in the main text, the sensitivity to the j th PC thus evolves over time t as:

---

### Counting growth factors in single cells with infrared quantum dots to measure discrete stimulation distributions [^df962117]. Nature Communications (2019). High credibility.

QDC-3DM methodology

Two stacks of images of the QDs were collected in wide-field excitation mode: a time-stack at a single z-focal plane (600 images; 50 ms exposure time) and a 3D volumetric stack (250 nm z-spacing, 100–200 images; 50 ms exposure time). 3D z-stacks were deconvolved using AutoQuantX3. Deconvolved 3D images were then imported into Imaris (Bitplane) which has an automatic 3D detection algorithm (surface mode) to determine the centroid positions (x 0, y 0, z 0) and intensityof spots with a range of sizes. These spot data, the time-stack images, and the deconvolved 3D images were imported into Matlab and a custom script was used to calculate the number of QD-EGF per cell.

[1] Single-QD identification: Spot positions (x 0, y 0, z 0) were rounded to the nearest integer pixel values, ([x 0], [y 0], [z 0]), and time-course intensities of the corresponding 2D spots, were summed over a 3 × 3 voxel centered about the centroid positions ([x 0], [y 0]) at each time point using equation 8. Temporal intensitiesfor each spot were binned into histograms and fit to a sum of two functions, a Gaussian background and skewed Gaussian signal. Single QDs were identified from istribution fits that satisfy previous criteria to distinguish single-QD photophysical dynamics.

[2] Single-QD intensity calibration: Deconvolved 3D spot intensitiesfor which spots correspond to single QDswere averaged to calculate the mean single-QD intensity, where n is the number of QDs identified as single.

[3] Spot intensity calibration: The number of QDs within each deconvolved 3D spot, N QD, spot, for images collected under the same conditions and experimental set was then calculated as:

For any 3D field of view, such as a single cell (N QD, cell) containing m spots, the total number of QDs can be calculated as the sum of QDs in each spot as:

---

### The linear-quadratic model is an appropriate methodology for determining isoeffective doses at large doses per fraction [^9164b9e6]. Seminars in Radiation Oncology (2008). Low credibility.

The tool most commonly used for quantitative predictions of dose/fractionation dependencies in radiotherapy is the mechanistically based linear-quadratic (LQ) model. The LQ formalism is now almost universally used for calculating radiotherapeutic isoeffect doses for different fractionation/protraction schemes. In summary, the LQ model has the following useful properties for predicting isoeffect doses: (1) it is a mechanistic, biologically based model; (2) it has sufficiently few parameters to be practical; (3) most other mechanistic models of cell killing predict the same fractionation dependencies as does the LQ model; (4) it has well-documented predictive properties for fractionation/dose-rate effects in the laboratory; and (5) it is reasonably well validated, experimentally and theoretically, up to about 10 Gy/fraction and would be reasonable for use up to about 18 Gy per fraction. To date, there is no evidence of problems when the LQ model has been applied in the clinic.

---

### The optimal MR acquisition strategy for exponential decay constants estimation [^e2d26e09]. Magnetic Resonance Imaging (2008). Low credibility.

Estimating the relaxation constant of an exponentially decaying signal from experimental MR data is fundamental in diffusion tensor imaging, fractional anisotropy mapping, measurements of transverse relaxation rates and contrast agent uptake. The precision of such measurements depends on the choice of acquisition parameters made at the design stage of the experiments. In this report, chi(2) fitting of multipoint data is used to demonstrate that the most efficient acquisition strategy is a two-point scheme. We also conjecture that the smallest coefficient of variation of the decay constant achievable in any N-point experiment is 3.6 times larger than that in the image intensity obtained by averaging N acquisitions with minimal exponential weighting.

---

### Data-driven control of complex networks [^85fc9bea]. Nature Communications (2021). High credibility.

Results

Network dynamics and optimal point-to-point control

We consider networks governed by linear time-invariant dynamicswhere, anddenote, respectively, the state, input, and output of the network at time t. The matrixdescribes the (directed and weighted) adjacency matrix of the network, and the matricesand, respectively, are typically chosen to single out prescribed sets of input and output nodes of the network.

In this work, we are interested in solving point-to-point control problems; that is, designing open-loop control policies that steer the network output y (t) from an initial value y (0) = y 0 to a desired one y (T) = y f in a finite number of steps T. If y f is output controllable in T steps (a standing assumption in this paper; we refer to Supplementary Note 1 for more details), then the latter problem admits a solution and, in fact, there are many ways to accomplish such a control task. Here, we assume that the network is initially relaxed (x (0) = 0), and we seek the control inputthat drives the output of the network to y f in T steps and, at the same time, minimizes a prescribed quadratic combination of the control effort and locality of the controlled trajectories.

Mathematically, we study and solve the following constrained minimization problem:where Q ≽ 0 and R ≻ 0 are tunable (positive semidefinite and positive definite, respectively) matrices that penalize output deviation and input usage, respectively, and y T = y (T). Problem (2) generalizes the classic (open-loop) linear–quadratic control framework by including the possibility of minimizing a linear function of the state (as opposed to the whole state) in addition to the control input. Further, we remark that increasing R in Eq. (2) leads to optimal control inputs that achieve the desired final state with increasingly smaller magnitudes. Similarly, the matrix Q in Eq. (2) weighs the norm of the output (state), so that increasing Q forces the optimization problem to generate inputs that limit the norm of the output (state), at the expenses of using a larger control input. In particular, if Q = 0 and R = I, thencoincides with the minimum-energy control to reach y f in T steps.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^c734ee06]. Journal of the American College of Cardiology (2025). High credibility.

ACC/AHA economic value statement templates — three formats and threshold — are specified as follows: The examples define key terms and use a cost-effectiveness threshold of $120 000 per QALY gained; Format 1 asks "What is the cost-effectiveness of the intervention at its current cost?" and may include "an ICER of $i per QALY gained (< $120 000 per QALY gained in p% of probabilistic simulations)"; Format 2 asks "What would the cost of the intervention have to be in order for the intervention to meet the cost-effectiveness threshold?" and states the strategy is cost-effective "at a threshold of $120 000 per QALY gained if the cost of the (intervention) is less than $t"; Format 3 combines both questions; ICER (incremental cost-effectiveness ratio) and QALY (quality-adjusted life year) abbreviations are defined on-page.

---

### Counting motifs in the human interactome [^d79dae2d]. Nature Communications (2013). Medium credibility.

where f is a function chosen to decide whether occurs among nodes i 1, i 2,…, i m or not. For the motifs listed in Table 1, their corresponding functions f are given in Supplementary Table S1.

From equations (1) and (3), we have

where n obs is a random variable such that

As the random variables X i are independent and identically distributed, for any 1 ≤ i 1 < i 2 <… < i m ≤ n, we also have

Hence, by equation (2),

By conditioning on the event that X 1 = X 2 = ··· = X m = 1, we rewrite equation (4) as

where Z ∼Binomial(n − m, p), and hence

As

we have

by applying integration by parts and simplification. Therefore, we have obtained the following theorem.

Theorem 1: Let be a network of n nodes. Assume obs is a subnetwork of obtained by a uniform node sampling process that selects a node with probability p. For any motif of m nodes, the estimatordefined in equation (1) satisfies equation (5). Therefore, is an asymptotically unbiased estimator for N in the sense thatas n goes to infinity. Moreover, the convergence is exponentially fast in n.

When the estimator (1) is applied to estimate the number of links in an undirected network, the variance has the following closed-form expression:

where N 1 and N 2 are, respectively, the number of links and three-node paths in (Supplementary Methods). This leads to our next theorem.

Theorem 2: When is generated from one of the ER, preferential attachment, duplication or geometric models, as n goes to infinity.

---

### Flexible categorization in perceptual decision making [^548f587f]. Nature Communications (2021). High credibility.

Model for n -choice decision making

To model a categorization task with n = 3 choices (Supplementary Fig. 5) we simulated a system of standard nonlinear coupled rate equations (see e.g. Equation (38) in):where ξ i is a Gaussian white noise process with amplitude σ for i = 1,2,3 and amplitude σ I for i = I, and with transfer function ϕ (x) = 0 for x < 0, ϕ (x) = x 2 for 0 ≤ x ≤ 1 andfor x > 1. The n = 4 case is a simple extension of these equations (the general system of rate equations for n > 2 can be found in). The parameters used for the simulations were s = 0.694, c = g = , τ = 20 ms, τ I = 10 ms, I I = 0, and σ I = 0. For n = 3 the inputs were taken as I 1 = I + 2∆ I /3, I² = I − ∆ I /3, I 3 = I − ∆ I /3, while for n = 4 they were I 1 = I + 3∆ I /4, I² = I − ∆ I /4, I 3 = I − ∆I /4, I 4 = I − ∆ I /4, with I = 2.25 and ∆ I = 0.03*s. For the top panels in Supplementary Fig. 5 the values of the noise strength where σ = 0.18 and σ = 0.13 for n = 3 and 4, respectively. The accuracy was calculated as the fraction of trials (N = 10,000) in which the highest firing rate at the end of the trial was r 1.

Reporting Summary

Further information on research design is available in the Nature Research Reporting Summary linked to this article.

---

### People construct simplified mental representations to plan [^a88007b8]. Nature (2022). Excellent credibility.

One of the most striking features of human cognition is the ability to plan. Two aspects of human planning stand out-its efficiency and flexibility. Efficiency is especially impressive because plans must often be made in complex environments, and yet people successfully plan solutions to many everyday problems despite having limited cognitive resources 1–3. Standard accounts in psychology, economics and artificial intelligence have suggested that human planning succeeds because people have a complete representation of a task and then use heuristics to plan future actions in that representation 4–11. However, this approach generally assumes that task representations are fixed. Here we propose that task representations can be controlled and that such control provides opportunities to quickly simplify problems and more easily reason about them. We propose a computational account of this simplification process and, in a series of preregistered behavioural experiments, show that it is subject to online cognitive control 12–14 and that people optimally balance the complexity of a task representation and its utility for planning and acting. These results demonstrate how strategically perceiving and conceiving problems facilitates the effective use of limited cognitive resources.

---

### Generating conjectures on fundamental constants with the ramanujan machine [^75e8911d]. Nature (2021). Excellent credibility.

Fundamental mathematical constants such as e and π are ubiquitous in diverse fields of science, from abstract mathematics and geometry to physics, biology and chemistry 1,2. Nevertheless, for centuries new mathematical formulas relating fundamental constants have been scarce and usually discovered sporadically 3–6. Such discoveries are often considered an act of mathematical ingenuity or profound intuition by great mathematicians such as Gauss and Ramanujan 7. Here we propose a systematic approach that leverages algorithms to discover mathematical formulas for fundamental constants and helps to reveal the underlying structure of the constants. We call this approach 'the Ramanujan Machine'. Our algorithms find dozens of well known formulas as well as previously unknown ones, such as continued fraction representations of π, e, Catalan's constant, and values of the Riemann zeta function. Several conjectures found by our algorithms were (in retrospect) simple to prove, whereas others remain as yet unproved. We present two algorithms that proved useful in finding conjectures: a variant of the meet-in-the-middle algorithm and a gradient descent optimization algorithm tailored to the recurrent structure of continued fractions. Both algorithms are based on matching numerical values; consequently, they conjecture formulas without providing proofs or requiring prior knowledge of the underlying mathematical structure, making this methodology complementary to automated theorem proving 8–13. Our approach is especially attractive when applied to discover formulas for fundamental constants for which no mathematical structure is known, because it reverses the conventional usage of sequential logic in formal proofs. Instead, our work supports a different conceptual framework for research: computer algorithms use numerical data to unveil mathematical structures, thus trying to replace the mathematical intuition of great mathematicians and providing leads to further mathematical research.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^aeeaae45]. Nature Communications (2025). High credibility.

Fitting models to data is an important part of the practice of science. Advances in machine learning have made it possible to fit more-and more complex-models, but have also exacerbated a problem: when multiple models fit the data equally well, which one(s) should we pick? The answer depends entirely on the modelling goal. In the scientific context, the essential goal is replicability: if a model works well to describe one experiment, it should continue to do so when that experiment is replicated tomorrow, or in another laboratory. The selection criterion must therefore be robust to the variations inherent to the replication process. In this work we develop a nonparametric method for estimating uncertainty on a model's empirical risk when replications are non-stationary, thus ensuring that a model is only rejected when another is reproducibly better. We illustrate the method with two examples: one a more classical setting, where the models are structurally distinct, and a machine learning-inspired setting, where they differ only in the value of their parameters. We show how, in this context of replicability or "epistemic uncertainty", it compares favourably to existing model selection criteria, and has more satisfactory behaviour with large experimental datasets.

---

### The minimal work cost of information processing [^927873c9]. Nature Communications (2015). Medium credibility.

Classical mappings and dependence on the logical process

Our result, which is applicable to arbitrary quantum processes, applies to all classical computations as a special case. Classically, logical processes correspond to stochastic maps, of which deterministic functions are a special case. As a simple example, consider the AND gate. This is one of the elementary operations computing devices can perform, from which more complex circuits can be designed. The gate takes two bits as input, and outputs a single bit that is set to 1 exactly when both input bits are 1, as illustrated in Fig. 2a.

The logical process is manifestly irreversible, as the output alone does not allow to infer the input uniquely. If one of the inputs is zero, then the logical process effectively has to reset a three-level system to zero, forgetting which of the three possible inputs 00, 01 or 10 was given; this information can be viewed as being discarded, and hence dumped into the environment. We can confirm this intuition with our main result, using the fact that a general classical mapping is given by the specification of the conditional probability p (x ′| x) of observing x ′ at the output if the input was x. Embedding the classical probability distributions into the diagonals of quantum states, the infinity norm in expression (2) becomes simply

---

### A rational approach to the clinical use of cumulative effective dose estimates [^0571c08e]. AJR: American Journal of Roentgenology (2011). Low credibility.

Objective

This article will address ongoing efforts to track cumulative dose estimates of patient radiation exposure that have the potential to transform radiation protection research.

Conclusion

There is no clear consensus on how to use this information clinically. A rational consideration of this question reveals that until the linear no-threshold model is supplanted, cumulative dose estimates are of little clinical relevance and never constitute a logical reason to avoid an imaging evaluation that is otherwise medically indicated.

---

### Relative, local and global dimension in complex networks [^a94b7a43]. Nature Communications (2022). High credibility.

Results

Graph dimension from diffusion dynamics

We start with the Green's function of the diffusion equation in d dimensionswhich, together with an initial condition as a delta function at some position x 0, provides a solution of diffusion equation as p (x, t) = G t (x − x 0). From hereon, we refer to the time evolution of p (x, t) as the transient response. As already considered in our previous works, these solutions have a maxima in their transient response at any other location x, at timeand amplitudegiven aswhere, without loss of generality, x 0 = 0. Then, the dimension at any point x relative to x 0 can be evaluated to yield the definition of the relative dimensionClearly, on the Euclidean space, the relative dimension is always equal to d, independently of x and x 0. However, if we instead consider a compact subspace, the diffusion dynamics will deviate from those prescribed in Equation (1) due to the presence of boundaries relative to x and x 0.

The key property of Equation (3) that allows us to generalise it to graphs is that the positions x 0 and x are not explicit in the right-hand side but only used as labels to initialise the diffusion dynamics and measure the transient response. Consequently, the relative dimension can be seen as intrinsic as it does not rely on any Euclidean embedding, but only on the existence of a diffusion dynamics on the original space. In particular, on graphs we can use the standard diffusion processfor a time-dependent node vector p (t) with L the normalised graph Laplacian L = K −1 (K − A) (corresponding to Euclidean diffusion in the continuous limit), where K is the diagonal matrix of node degrees. Using a delta function at node i with mass m i, p (0) = (0, 0,…, m i,…, 0), as our initial condition, the j -th coordinate of the solution of Equation (4) (the so-called transient response of j) is given by the heat kernelBy numerically solving (5), we can measure the timeand amplitudeat which a maximum appears in the transient response peak (time evolution) of node j given a delta function initial condition at node i. In analogy to Equation (3), we can then compute the full N × N matrix of relative dimensions with elements

---

### Mathematical modeling of the brain: principles and challenges [^70d6d8e7]. Neurosurgery (2008). Low credibility.

Objective

The use of mathematics in the study of phenomena and systems of interest to medicine has become quite popular in recent years, but not much progress has been made as a result of these efforts. The aim of this article is to identify the reasons for this failure and to suggest procedures for more successful outcomes.

Methods

We review and assess a variety of mathematical modeling procedures, from microscopic (at the level of molecular behavior) to macroscopic standpoints, from lumped-parameters to distributed-parameters approaches. Using examples that are as simple as possible, we elucidate the difference between the predictive and the explanatory powers of mathematical models, as well as the uses (and abuses) of analogy in their construction.

Results

Mathematical medicine is a truly interdisciplinary area that brings together medical researchers, engineers, and applied mathematicians whose vast differences in expertise and background make collaboration difficult.

Conclusion

The lack of a common language and a common way of understanding what a mathematical model is, and what it can do, is identified as the main source of the slow progress to date, and constructive suggestions are made to improve the situation.

---

### Resolving degeneracy in diffusion MRI biophysical model parameter estimation using double diffusion encoding [^f95dc7d6]. Magnetic Resonance in Medicine (2019). Medium credibility.

In Hansen et al 48 the equivalent to the system in Equation 6 is solved reaching two alternative equations for κ, each giving possible solutions. This suggested that, in general, there should be two solutions, one for each branch. However, this is not always the case, as illustrated in Table 1. We derive here an alternative expression of the solution in one equation only. First, Equation 6 can be reparametrized as:After this substitution, Equation 6 can be expressed as a linear system of five equations for the 5 unknowns α, β, γ, δ and ε, decoupled into two independent smaller systems:

Table 1
Illustration of sets of biophysical (BP) parameter values resulting in the same diffusion–kurtosis (DK) parameters

Observe that the coefficients of matrices L and M depend on κ. We will ignore for the moment that the five unknowns are not independent. The solution is unique as long as matrices L and M are invertible. This is the case when κ ≠ 0, sinceand. In the limit of a fully isotropic medium (κ = 0) the system has only two independent equations, not allowing the recovering of the kernel parameters without additional information. By solving the two systems in Equation 8 we find expressions for α, β, γ, δ and ε that only depend on κ and the DK parameters (see Appendix A for solution). Those variables are actually defined from only four kernel parameters (Equation 7), resulting in the coupling equationBy plugging the expressions for α, β, γ, δ and ε as functions of κ into Equation 9, we obtain a nonlinear equation for κ with potentially multiple solutions. Each solution for κ gives a single solution for α, β, γ, δ and ε, which in turn, gives a single solution for the kernel parameters:Thus, the number of solutions to Equation 9 corresponds to the number of BP parameter sets that have the same DK parameters. Table 1 presents cases with up to four solutions. We computed the number of solutions for 10k random points in the BP parameter space. Most present two solutions (70.2%), some only one (29.3%), and only a small proportion have four solutions (0.5%). This gives rise to the previously discussed degeneracy in model parameter estimation from noisy measurements. 17 In contrast with the claim in Hansen et al 51 even in the extreme case of parallel fibers leaving only four unknowns, the five equations in Equation 6 are independent. This is possible due to the nonlinear nature of the system. If κ is known and not zero (including the limiting case κ → ∞ of parallel fibers), the full‐system is invertible as long as f is not 0 or 1, andis not null. In that case, each point in the DK parameter space (signal profile) corresponds to a single set of BP parameters. However, this is not the case for an arbitrary unknown κ. Here, the full‐system has five independent equations with five unknowns, but, depending on the parameter values, it can have only one or multiple solutions. This latter case makes the inverse mapping an ill‐posed problem.

---

### Nonlinear optical components for all-optical probabilistic graphical model [^9f412763]. Nature Communications (2018). Medium credibility.

Multiplication

Inserting the saturable absorption equationin the differential equation for the nonlinear absorption, and solving leads toHere I sat is the saturation peak irradiance, α 0 is the weak field absorption, L is the thickness of SA material and I in and I out are the input and output peak irradiance, respectively. A numerical solution of Eq. (3) and its fit with an exponential function are plotted in Fig. 3a. Including the TPA term in the nonlinear absorption differential equationd I / d Z = − α 0 I − βI 2, leads to an explicit analytical solution, where β is the TPA coefficient and. A numerical solution of Eq. (4) is plotted in Fig. 3b as well as its fit with a natural logarithm function. The result of combinations for 29 identical logarithm inputs and an exponentiation gives the multiplication of the inputs as illustrated in Fig. 3c. The ideal multiplication result is plotted as a linear fit in Fig. 3c. Note that the peak irradiance in Eq. (3) and (4) can be replaced with energy per pulse (fluence or photon number as well) without any change in concept of their comparison with the exponential and logarithm functions. We use energy per pulse (E) for simulation as the experimental data were measured in terms of energy per pulse. In Fig. 3a and Fig. 3b we need to limit the range of fitting in order to get maximum overlap of the exponential and natural logarithm fit functions with SA and TPA solutions. Also the normalized-root-mean-square error (NRMSE) should be less than 1% and is defined asLimiting the ranges also comes from the natural behavior of the SA and TPA process where Eq. (3) and (4) start from zero for no input energy. However, we know that e 0 = 1 and ln(0) is undefined. Therefore, bounding the input intensity range for fitting is necessary for convergence and adequate fitting of the solutions of the TPA and SA equations with the target functions. The criteria are the maximum error acceptable to reproduce the function.

---

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^4165d96c]. American Journal of Kidney Diseases (2006). Medium credibility.

Appendix — methods for adding residual clearance to hemodialyzer clearance notes that dialyzer clearances (spKt/V) required to achieve a stdKt/V of 2.0 volumes per week are tabulated across treatment times from 2 to 8 hours and schedules from 2 to 7 treatments per week, with values determined using a formal 2-compartment mathematical model of urea kinetics and similar results obtainable using a simplified equation; the approach gives results similar to the third data column of Table 18.

---

### Observation and theory of X-ray mirages [^53c6da55]. Nature Communications (2013). Medium credibility.

The above arguments imply that only the region near the local maximum of n e (x, y) surface is essential for X-ray mirage formation. The shape of the density profile n e (x, y) outside the local maximum has almost no effect on the result. The position of the virtual source is mainly determined by the curvature of the n e (x, y) surface at its maximum, that is, by second derivatives with respect to x and y. If they are not equal, the astigmatic virtual source is formed. In the case of weak diffraction, the position of an imaginary source can be analytically calculated in the ray-tracing approximation. In this approximation, the position of an imaginary source is very sensitive to the width Δ ed of a n e (x, y) local maximum; more accurately, it is proportional to the square of this width. The spatial distribution of the gain G (x, y) also affects the properties of the source, but in a different way. Width of the local maximum at G (x, y) surface defines the size of a source in its imaginary focus and, therefore, determines the diffraction divergence of mirage radiation. In the case of asymmetric G (x, y) surface in the vicinity of a local maximum, the size of a virtual point source will be different in x and y directions, leading to a different beam divergence over these coordinates. The results of calculations showing the formation of a mirage for a rather large size of the gain area, when we can neglect the diffraction effects, are shown in Supplementary Fig. S2 and Supplementary Movie 1. For a small size of gain area, diffraction effects begin to have a significant role. Equation (2) naturally takes into consideration the diffraction effects, but, unfortunately, in the conditions of strong diffraction, it is impossible to give a simple picture of mirage formation, and the numerical solution of equation (2) is required.

---

### Anticipating regime shifts by mixing early warning signals from different nodes [^7536608c]. Nature Communications (2024). High credibility.

Results

Theory

We consider an N -dimensional noisy nonlinear dynamical system in continuous time. We regard each of the N dynamical elements as a node and the entire dynamical system as stochastic dynamics on a network with N nodes. We denote bythe state of the i th node at time. We write, where ⊤ represents the transposition. We assume that x (t) obeys a set of stochastic differential equations in the Itô sense given bywhere; B is an N × N matrix; W (t) represents an N -dimensional vector of independent Wiener processes (i.e. white noise).

Assume that the dynamics given by Eq. (1) has an equilibrium in the absence of noise, which we denote by. By linearizing Eq. (1) around x *, we obtain a set of linear stochastic differential equations given bywhere z (t) = x (t) − x *, and the N × N matrix A is the sign-flipped Jacobian matrix of F at x (t) = x *. Equation (2) is a multivariate Ornstein-Uhlenbeck (OU) process, and x * is asymptotically stable in the absence of noise if and only if A is positive definite.

The covariance matrix in the equilibrium, corresponding to z ✱ = (0,…, 0) ⊤, is given as the solution to the Lyapunov equation given bywhere C = (C i j) is the N × N covariance matrix, and C i j represents the covariance between z i (t) and z j (t), which is equal to the covariance between x i (t) and x j (t), at equilibrium. The solution C is unique if the real part of all the eigenvalues of A is positive.

Outcomes of the covariance matrix such as the standard deviation and correlation coefficient are often used as early warning signals for noisy multivariate dynamics. In practice, we need to estimate these quantities from samples. Therefore, we consider the sample variance of x i (t), which is a major early warning signal. and its average over a given node set, as candidates of early warning signals. The choice of the variance rather than the standard deviation is because of mathematical tractability. We emphasize that the rationale behind averaging the sample variance over nodes is that we may be then able to obtain a less fluctuating early warning signal than that calculated from a single node.

---

### State estimation of a physical system with unknown governing equations [^794152e0]. Nature (2023). Excellent credibility.

State estimation is concerned with reconciling noisy observations of a physical system with the mathematical model believed to predict its behaviour for the purpose of inferring unmeasurable states and denoising measurable ones 1,2. Traditional state-estimation techniques rely on strong assumptions about the form of uncertainty in mathematical models, typically that it manifests as an additive stochastic perturbation or is parametric in nature 3. Here we present a reparametrization trick for stochastic variational inference with Markov Gaussian processes that enables an approximate Bayesian approach for state estimation in which the equations governing how the system evolves over time are partially or completely unknown. In contrast to classical state-estimation techniques, our method learns the missing terms in the mathematical model and a state estimate simultaneously from an approximate Bayesian perspective. This development enables the application of state-estimation methods to problems that have so far proved to be beyond reach. Finally, although we focus on state estimation, the advancements to stochastic variational inference made here are applicable to a broader class of problems in machine learning.

---

### Methodologic issues specific to prediction model development and evaluation [^00d7dd5a]. Chest (2023). Medium credibility.

Developing and evaluating statistical prediction models is challenging, and many pitfalls can arise. This article identifies what the authors believe are some common methodologic concerns that may be encountered. We describe each problem and make suggestions regarding how to address them. The hope is that this article will result in higher-quality publications of statistical prediction models.

---

### Physics-informed learning of governing equations from scarce data [^d1208a9b]. Nature Communications (2021). High credibility.

Harnessing data to discover the underlying governing laws or equations that describe the behavior of complex physical systems can significantly advance our modeling, simulation and understanding of such systems in various science and engineering disciplines. This work introduces a novel approach called physics-informed neural network with sparse regression to discover governing partial differential equations from scarce and noisy data for nonlinear spatiotemporal systems. In particular, this discovery approach seamlessly integrates the strengths of deep neural networks for rich representation learning, physics embedding, automatic differentiation and sparse regression to approximate the solution of system variables, compute essential derivatives, as well as identify the key derivative terms and parameters that form the structure and explicit expression of the equations. The efficacy and robustness of this method are demonstrated, both numerically and experimentally, on discovering a variety of partial differential equation systems with different levels of data scarcity and noise accounting for different initial/boundary conditions. The resulting computational framework shows the potential for closed-form model discovery in practical applications where large and accurate datasets are intractable to capture.

---

### Computational methods for the estimation of ideal current patterns in realistic human models [^2f5b4b1d]. Magnetic Resonance in Medicine (2024). Medium credibility.

Purpose

To introduce a method for the estimation of the ideal current patterns (ICP) that yield optimal signal-to-noise ratio (SNR) for realistic heterogeneous tissue models in MRI.

Theory and Methods

The ICP were calculated for different surfaces that resembled typical radiofrequency (RF) coil formers. We constructed numerical electromagnetic (EM) bases to accurately represent EM fields generated by RF current sources located on the current-bearing surfaces. Using these fields as excitations, we solved the volume integral equation and computed the EM fields in the sample. The fields were appropriately weighted to calculate the optimal SNR and the corresponding ICP. We demonstrated how to qualitatively use ICP to guide the design of a coil array to maximize SNR inside a head model.

Results

In agreement with previous analytic work, ICP formed large distributed loops for voxels in the middle of the sample and alternated between a single loop and a figure-eight shape for a voxel 3-cm deep in the sample's cortex. For the latter voxel, a surface quadrature loop array inspired by the shape of the ICP reached 87.5% of the optimal SNR at 3T, whereas a single loop placed above the voxel reached only 55.7% of the optimal SNR. At 7T, the performance of the two designs decreased to 79.7% and 49.8%, respectively, suggesting that loops could be suboptimal at ultra-high field MRI.

Conclusion

ICP can be calculated for human tissue models, potentially guiding the design of application-specific RF coil arrays.

---

### The problem with composite indicators [^2b966415]. BMJ Quality & Safety (2019). High credibility.

'The Problem with… ' series covers controversial topics related to efforts to improve healthcare quality, including widely recommended but deceptively difficult strategies for improvement and pervasive problems that seem to resist solution.

---

### Concerns regarding… [^70dc23c9]. EClinicalMedicine (2024). Medium credibility.

Controversy about hyperparameter optimization of various machine learning models

In this study, the process of hyperparameter optimization is not described, even though different combinations of hyperparameters are crucial for model building and validation.

In the field of machine learning, using default hyperparameters may not be suitable for specific datasets. Lack of hyperparameter optimization can prevent the model from fully exploiting feature information, leading to less stable performance, high heterogeneity, and insufficient robustness, which negatively affects model building and generalization. Different combinations of hyperparameters significantly impact the predictive efficacy of the model. Failure to adequately perform hyperparameter optimization may result in models that do not achieve optimal predictive efficacy. This causes selection bias in the model and ultimately leads to incorrect decisions.

Machine learning is prone to model overfitting and poor generalization if the built-in hyperparameters are not properly set. Therefore, hyperparameter optimization in the field of machine learning is a scientific, rigorous, and reliable practice.

---

### Evaluating the evidence for exponential quantum advantage in ground-state quantum chemistry [^03db0a64]. Nature Communications (2023). High credibility.

Due to intense interest in the potential applications of quantum computing, it is critical to understand the basis for potential exponential quantum advantage in quantum chemistry. Here we gather the evidence for this case in the most common task in quantum chemistry, namely, ground-state energy estimation, for generic chemical problems where heuristic quantum state preparation might be assumed to be efficient. The availability of exponential quantum advantage then centers on whether features of the physical problem that enable efficient heuristic quantum state preparation also enable efficient solution by classical heuristics. Through numerical studies of quantum state preparation and empirical complexity analysis (including the error scaling) of classical heuristics, in both ab initio and model Hamiltonian settings, we conclude that evidence for such an exponential advantage across chemical space has yet to be found. While quantum computers may still prove useful for ground-state quantum chemistry through polynomial speedups, it may be prudent to assume exponential speedups are not generically available for this problem.

---

### Counting growth factors in single cells with infrared quantum dots to measure discrete stimulation distributions [^3d34b90d]. Nature Communications (2019). High credibility.

Isolated QD intensity calibration

Two stacks of images of isolated QDs on glass coverslips were were collected in wide-field excitation mode: a time stack at a single z-focal plane (4000 images; 100 ms exposure time) and a 3D volumetric stack (250 nm z-spacing, 80 images; 100 ms exposure time). 3D z-stacks were deconvolved using AutoQuantX3. Using custom Matlab codes, the deconvolved 3D intensity of each spotwas then calculated as the integrated intensity of a 3 × 3 × 11 voxel centered at the centroid position according to the following equation:where [x 0], [y 0], and [z 0] are the centroid positions rounded to the nearest pixel integer, I (x, y, z) is the intensity of a single pixel, andis the mean 3 × 3 × 11 voxel intensity sum of background region. Using the same 2D spot ([x 0], [y 0]) centroid positions, 3 × 3 time-course intensitieswere calculated according to the following equation:

Using Matlab, all intensities for a spot were binned into a histogram composed of 100 bins. The intensity histogram was fitted using least square estimate to a Gaussian mixture model with 2–5 Gaussians, for which one was the background noise function corresponding to the off-state of QD blinking. To maximize the accuracy in fitting, we imposed the following fitting criteria: (1) correlation coefficient greater than or equal to 0.98 between the fit and data, (2) each Gaussian area contributes at least 8% the total area, (3) maximum 75% overlap between any two Gaussians, and (4) maximum 20% difference in area between each Gaussian and its corresponding data region. For each spot, the number of Gaussians that yields the minimum AIC value was identified as optimal. AIC was calculated according the following equation:where n bin is the number of bins used to construct the intensity histogram, RSS is the residual sum of squares, and n Gauss is the number of Gaussians used to fit the intensity histogram.

---

### Flow interactions lead to self-organized flight formations disrupted by self-amplifying waves [^4000efef]. Nature Communications (2024). High credibility.

The propulsion dynamics for each flyer of mass M (assumed identical for all n) is dictated by Newton's Second Law:The interaction term in square brackets is nontrivial since it involves the earlier time t n (t) when the upstream neighbor n − 1 was at the current location of flyer n, as defined through the implicit relationship X n (t) = X n −1 (t n (t)). Hence, t n (t) embodies the effect of memory in the system. Equivalently, t − t n is the relevant delay time between when a signal left by flyer n − 1 is encountered by flyer n. The memory timescale is itself time dependent, motivating an approach in which it is included as a state variable in the dynamical system by deriving its evolution equation:and thus. This leads to a system of nonlinear delay differential equations for the state variables (X n, U n, t n):Here the term U n −1 (t n (t)) in the third equation constitutes a state-dependent delay, and all time dependencies are explicitly included for clarity. A flock of N flyers is governed by 3 N such equations.

The treatment of the leader n = 1 depends on whether the system is open or closed, i.e. whether the ensemble flies into a semi-infinite domain of quiescent fluid (like a flock of birds) or within a cyclic domain (as in our experiments). If open, then the leader flies exactly as a solo bird: The interaction term involving V n −1 in the second equation of the system [(2)] is removed, as is the entire third equation. If closed, then the leader interacts with the last member of the flock, and n − 1 should be replaced by N in the second and third equations of the system [(2)]. The latter case is illustrated in the bottom panel of Fig. 4 a.

---

### Humans rationally balance detailed and temporally abstract world models [^f4c01426]. Communications Psychology (2025). Medium credibility.

Linear RL choice model

To estimate the expected value of choosing the left versus right island using linear RL, values were estimated from a transition matrix T and rewards r (both learned over the course of the experiment):whereis a vector of optimal values at non-terminal states, M is the default representation (DM) matrix from all non-terminal to all other non-terminal states, T N T is the transition matrix from all non-terminal to terminal states, r T is the set of rewards (or negative step costs) at all terminal states, and λ arbitrates between exact and policy-guided estimates. Value estimates of the boats were given by taking the logarithm of the left-hand side and multiplying each value by λ.

The DR matrix is in turn given bywhere T N N is the transition matrix from all non-terminal to all other non-terminal states and r N is the set of rewards at all non-terminal states.

r was assumed to be 0 at islands, and for boats equal to the estimated V for that boat.

T was updated after each choice using a Hebbian learning rule, which was chosen to provide equivalent updates to those of the SR's M matrix:Choice between islands was modeled as a probabilistic decision between the two state values estimated via linear RL, again using a softmax distribution but with a separate inverse temperature β i s l a n d, as well as persistence estimate β p e r s i s t e n c e I in human model fitting:

Parameter estimation

We optimized the free parameters of the learning algorithms by embedding each of them within a hierarchical model to allow parameters to vary between subjects or simulated sessions. Subject-level parameters were modeled as arising from a population-level Gaussian distribution over subjects. We estimated the model to obtain best fitting subject- and group-level parameters to minimize the negative log likelihood of the data using an expectation-maximization algorithm with a Laplace approximation to the session-level marginal likelihoods in the M-step. For hypothesis testing on population-level parameters, we computed an estimate of the information matrix over the population-level parameters, taking account of the so-called "missing information" due to optimization in the E-step, itself approximated using the Hessian of a single Newton-Raphson step.

---

### Efficient neural codes naturally emerge through gradient descent learning [^980cc388]. Nature Communications (2022). High credibility.

What W learns: supervised learning

We can also determine how input statistics affect the sensitivity for the more general class of objective functions when W x is trained to match some target y by minimizing the mean-squared error:As before, we can gain intuition about W by beginning from an initialization that is axis-aligned with the final solution. For the supervised case, these initializations share the singular vectors of the data/labels, but can differ in the singular values. Given Σ x x = V Λ V T and Σ x y = U T V T, we set W (0) = U S V T for the same U and V. See the Supplementary Methods for proof that this is a fixed point of singular vector dynamics.

This initialization allows us to understand how the singular values of the weight matrix change. As derived in the Supplementary Methods, the time evolution of σ i is given by:

As in the case for input reconstruction, the i th singular value approaches a target. Instead of 1, this value is, the ratio of the importance of this component (the input/output singular value t i) and the standard deviation of that component in the inputs λ i. The growth rate is controlled by the distance from this asymptote (right term) and as well as on the input statistics λ i. Thus, even for the case of supervised learning the input statistics affect what is learned first via gradient descent directly through Σ x x via λ i, and not just through the input/label covariance Σ x y.

Statistics and reproducibility

All neural network experiments are somewhat stochastic in their results due to a random initialization and a random subselection of training data used in any network update step. Nevertheless the figures are shown with a single network, due in part to the expense of training multiple networks on ImageNet and because we did not compute statistics of network characteristics. No data were excluded from the analyses, and the Investigators were not blinded to outcome assessment.

Reporting summary

Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.

---

### Reducing slab boundary artifacts in three-dimensional multislab diffusion MRI using nonlinear inversion for slab profile encoding (NPEN) [^fae07c39]. Magnetic Resonance in Medicine (2016). Low credibility.

As originally proposed, the PEN method relies on the assumption that slab profile estimates are accurate, which is increasingly difficult to ensure at short TR. To overcome this problem, we propose a new nonlinear reconstruction that jointly estimates the slab profile and the image. In this method, both slab profile and image are treated as unknowns x = [u, S] T, which should satisfywhereis a nonlinear operator mapping image, slab profile, and coil sensitivity to the acquired data. This poses the 3D multislab reconstruction as a nonlinear inversion problem, which we refer as nonlinear inversion for slab profile encoding (NPEN). As shown previously 41, 42, 43, 44, this problem can be solved using an iteratively regularized Gauss–Newton algorithm 45, 46, 47. In step n, Equation (3) is first linearized around x n :where E′ (x n) is the Fréchet derivative of E at the current guess x n, Δ x n is the update, and the new guess can be obtained by x n +1 = x n + Δ x n. The linear problem described by Equation (4) is then solved by the following Tikhonov regularized minimization:where the first term is the data fidelity term and the second term is the regularization term. x 0 is an initial guess and α n is a regularization parameter, which is reduced in each step to benefit from the robustness at the initial steps of iterations (i.e.g.radient–descent‐like) and the fast convergence when x is close to the solution (i.e. Gauss–Newton‐like). Equation (5) can be solved with the conjugate gradient algorithm.

---

### A simple alpha / β-independent method to derive fully isoeffective schedules following changes in dose per fraction [^3a48e204]. International Journal of Radiation Oncology, Biology, Physics (2004). Low credibility.

Purpose

Dosimetric errors in delivering the prescribed dose per fraction made early in a treatment can be corrected by modifying the dose per fraction and total dose given subsequently to discovery of the error, using the linear-quadratic model to calculate the correcting doses which should be completed within the same overall time as originally prescribed. This study shows how these calculations can be carried out independently of any alpha/beta ratios to bring the treatment back exactly to planned tolerance simultaneously for all tissues and tumor involved.

Methods

Planned treatment is defined as p Gy per fraction to a total dose P Gy; the initial error is e Gy per fraction given to a total of E Gy. The linear-quadratic formula is assumed to describe all isoeffect relationships between total dose and dose per fraction.

Results and Conclusion

An exact solution is found that describes a compensating dose of d Gy per fraction to a total of D Gy. The formulae are: D = P-E d = Pp-Ee/P-E. Thus the total dose for the complete treatment (error plus compensation) remains as originally prescribed, with hyperfractionation being used to correct an initial hypofractionation error and hypofractionation being used to correct an initial hyperfractionation error. Incomplete repair is shown to perturb this exact solution. Thus compensating treatments calculated with these formulae should not be scheduled in such a manner that would introduce incomplete repair.

---

### Designing attractive models via automated identification of chaotic and oscillatory dynamical regimes [^89c80134]. Nature Communications (2011). Medium credibility.

Mathematical modelling requires a combination of experimentation, domain knowledge and, at times, a measure of luck. Beyond the intrinsic challenges of describing complex and complicated phenomena, the difficulty resides at a very fundamental level with the diversity of models that could explain a given set of observations. This is a manifestation of the so-called inverse problem, which is encountered whenever we aim to reconstruct a model of the process from which data have been generated. Exploring the potential space of solutions computationally can be prohibitively expensive and will generally require sophisticated numerical approaches or search heuristics, as well as expert guidance and manual interventions. Parameter estimation, model inferenceand model selectionall address aspects of this problem.

The inverse problem also applies in a different context: the design of systems with specified or desired outputs. Here again we have a multitude of different models — or, for sufficiently complicated models, a potentially vast range of parameters — that fulfil a given set of design objectives. Therefore, system design can be fraught with the same challenges as statistical inference or reverse-engineering tasks: in the former case, we want to learn the existing structure and properties of a system that has produced certain types of data, whereas in the latter we want to design constructible systems that will reliably and robustly exhibit certain types of behaviour.

These challenges are often further exacerbated by unsuitable or insufficient encoding of the behaviour that we observe (in natural systems) or would like to see (in designed systems). For example, if we aim to estimate parameters describing an oscillating system from a series of observations, it is possible to get good and even globally optimal fits to the data, without finding a qualitatively acceptable solution. Various methods of qualitative inference have been developed to address this issue; the topology of bifurcation diagrams, local stability properties of dynamically invariant sets, symbolic sequences of chaotic systemsand temporal logic constraintshave variously been used to drive parameter searches, or for model checking. However, these methods are either limited in the complexity of behaviour they can detect, or by conditioning on surrogate data (for example, forcing solutions through a small number of points), they suffer in the same way as quantitative approaches. The method proposed here extends the scope of the promising, but underdeveloped, class of qualitative parameter estimation algorithms, allowing detection and control of the most complex and elusive dynamical behaviours, such as oscillations, chaos and hyperchaos.

---

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^0c2e457a]. American Journal of Kidney Diseases (2006). Medium credibility.

Hemodialysis adequacy (CPG 2.4) — ultrafiltration and solute generation are significant components that must be included in Kt/V calculations, and incorporating them requires computer-based iteration of the provided equation. Specifically, ultrafiltration "adds a significant component that must be included along with the simultaneous solute generation rate in the Kt/V calculation", and these "components require computer programs to precisely calculate Kt/V by iterating the following equation363".

---

### Basic principles in the radiation dosimetry of nuclear medicine [^bdab51bd]. Seminars in Nuclear Medicine (2014). Low credibility.

The basic principles of the use of radiation dosimetry in nuclear medicine are reviewed. The basic structure of the main mathematical equations are given and formal dosimetry systems are discussed. An extensive overview of the history and current status of anthropomorphic models (phantoms) is given. The sources and magnitudes of uncertainties in calculated internal dose estimates are reviewed.

---

### Gravitationally redshifted absorption lines in the X-ray burst spectra of a neutron star [^b4654ece]. Nature (2002). Excellent credibility.

The fundamental properties of neutron stars provide a direct test of the equation of state of cold nuclear matter, a relationship between pressure and density that is determined by the physics of the strong interactions between the particles that constitute the star. The most straightforward method of determining these properties is by measuring the gravitational redshift of spectral lines produced in the neutron star photosphere. The equation of state implies a mass-radius relation, while a measurement of the gravitational redshift at the surface of a neutron star provides a direct constraint on the mass-to-radius ratio. Here we report the discovery of significant absorption lines in the spectra of 28 bursts of the low-mass X-ray binary EXO0748-676. We identify the most significant features with the Fe XXVI and XXV n = 2–3 and O VIII n = 1–2 transitions, all with a redshift of z = 0.35, identical within small uncertainties for the respective transitions. For an astrophysically plausible range of masses (M approximately 1.3–2.0 solar masses; refs 2–5), this value is completely consistent with models of neutron stars composed of normal nuclear matter, while it excludes some models in which the neutron stars are made of more exotic matter.

---

### A simplified formula for T1 contrast optimization for short-TR steady-state incoherent (spoiled) gradient echo sequences [^c91a4f57]. Magnetic Resonance Imaging (2007). Low credibility.

There are certain instances in practical magnetic resonance imaging where T1 changes by a small amount relative to a neighboring pixel or between scans for a given pixel. The source of these small changes in T1 can be caused either by changes in tissue water content or by the uptake of a contrast agent. For short repetition time (TR) spoiled gradient echo imaging, we show that a robust and a simple, easy to use back-of-the-envelope expression for the flip angle that optimizes contrast under these conditions is given by radical 3theta E in radians or (180/pi) radical6TR/T1 in degrees. We show that for a TR/T1 ratio of up to 0.3 and for a T1 change of up to ± 50%, this approximation to the optimal flip angle produces a contrast to within 6% of the theoretical maximum value and that these predictions are in good agreement with experiment.

---

### 2019 ASCCP risk-based management consensus guidelines: methods for risk estimation, recommended management, and validation [^c44c6ea6]. Journal of Lower Genital Tract Disease (2020). High credibility.

Risk-based management uncertainty — confidence interval and decision confidence score: Confidence intervals for each risk estimate were calculated using a normal approximation or exact methods based on the binomial distribution, and a "decision confidence score" is defined that combines the uncertainty in the statistical precision and how close the risk estimates fall to the clinical action thresholds; mathematical details on estimating the recommendation confidence scores are presented in Appendix Part D (http://links.lww.com/LGT/A160).

---

### Clinical practice guidelines for the care of girls and women with Turner syndrome [^7f1c1ed7]. Pediatrics (2017). Medium credibility.

Turner syndrome — R 1.14 Y-chromosome testing recommendation: We recommend screening for Y chromosomal material by PCR or other molecular method in TS individuals with a 45, X karyotype and signs of virilization.

---

### A simple guide to chaos and complexity [^506b7855]. Journal of Epidemiology and Community Health (2007). Low credibility.

The concepts of complexity and chaos are being invoked with increasing frequency in the health sciences literature. However, the concepts underpinning these concepts are foreign to many health scientists and there is some looseness in how they have been translated from their origins in mathematics and physics, which is leading to confusion and error in their application. Nonetheless, used carefully, "complexity science" has the potential to invigorate many areas of health science and may lead to important practical outcomes; but if it is to do so, we need the discipline that comes from a proper and responsible usage of its concepts. Hopefully, this glossary will go some way towards achieving that objective.