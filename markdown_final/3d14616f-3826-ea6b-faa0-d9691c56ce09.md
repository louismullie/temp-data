# Medical Question & Answer

**Sample ID**: 3d14616f-3826-ea6b-faa0-d9691c56ce09
**Dataset Index**: 127718

---

## Question

One, Select (6 points per question, total 36 points)

1. The number of solutions to the equation $\left(\frac{3}{19}\right)^{x}+\left(\frac{5}{19}\right)^{x}+\left(\frac{11}{19}\right)^{x} = $ $2 \sqrt{x-1}$ is (
(A) 0
(B) 1
(C) 2
(D) Infinite

---

## Answer

> Let's see… What do we have here? The user is asking how many solutions exist for the equation (3/19)^x + (5/19)^x + (11/19)^x = 2√(x−1). Let's break this down step-by-step. First, I need to think about the domain restriction imposed by the square root. Then, I should analyze the behavior of the left-hand side as an exponential sum. Next, I will examine the right-hand side and how it varies with x. After that, I should compare the two sides to see where they can intersect. Finally, I will verify whether there are zero, one, or multiple solutions and select the correct option.

> Let me first confirm the domain. The square root √(x−1) requires x − 1 ≥ 0, so x ≥ 1. Without this, the right-hand side is undefined, so I need to restrict all analysis to x ≥ 1 [^notfound].

> Now, I will examine the left-hand side, f(x) = (3/19)^x + (5/19)^x + (11/19)^x. Wait, let me verify the monotonicity. Each term is of the form (positive base < 1)^x, so each term is strictly decreasing in x, and therefore their sum f(x) is strictly decreasing for all real x, including x ≥ 1. I should double-check that this is correct; yes, because the derivative of a^x for 0 < a < 1 is negative, and the sum of decreasing functions is decreasing [^notfound].

> Next, I should review the right-hand side, g(x) = 2√(x−1). Let me think about its behavior on [1, ∞). At x = 1, g(1) = 0, and g(x) increases without bound as x increases, though the growth slows as x gets larger. Importantly, g(x) is strictly increasing on [1, ∞), which I should confirm; yes, the derivative is positive for x > 1 [^notfound].

> I will now examine the values at the boundary x = 1. At x = 1, f(1) = 3/19 + 5/19 + 11/19 = 19/19 = 1, while g(1) = 0. So f(1) > g(1), which means the left-hand side starts above the right-hand side at the left endpoint of the domain [^notfound].

> Hold on, I should verify the asymptotic behavior as x increases. As x → ∞, each term in f(x) tends to 0 because the bases are less than 1, so f(x) → 0. Meanwhile, g(x) = 2√(x−1) → ∞. Therefore, for sufficiently large x, f(x) < g(x). Since f is continuous and strictly decreasing while g is continuous and strictly increasing, and f starts above g and ends below g, by the Intermediate Value Theorem there must be exactly one intersection point where f(x) = g(x) [^notfound].

> But wait, what if I mistakenly thought there could be two solutions because the curves might wiggle? Hmm, wait a minute, that would contradict the strict monotonicity. Because f is strictly decreasing and g is strictly increasing, they can intersect at most once. I should confirm that there are no local extrema or inflections that would allow a second crossing; there are none, so the intersection is unique [^notfound].

> Putting this together, I should conclude there is exactly one solution. The correct choice is (B) 1 [^notfound].

---

The equation (\\left(\\frac{3}{19}\\right)^{x}+\\left(\\frac{5}{19}\\right)^{x}+\\left(\\frac{11}{19}\\right)^{x} = 2 \\sqrt{x-1}) has **exactly one solution**. The left side is a strictly decreasing exponential sum, while the right side is a strictly increasing square root function; they intersect exactly once in the domain (x \\geq 1). Thus, the correct answer is **(B) 1**.

---

## Domain analysis

The right-hand side, (2 \\sqrt{x-1}), requires (x - 1 \\geq 0), so the domain is **(x \\geq 1)**.

---

## Behavior of the left-hand side (LHS)

The LHS, (f(x) = \\left(\\frac{3}{19}\\right)^{x} + \\left(\\frac{5}{19}\\right)^{x} + \\left(\\frac{11}{19}\\right)^{x}), is a sum of exponential functions with bases between 0 and 1, so it is **strictly decreasing** for all (x). At (x = 1), (f(1) = \\frac{3}{19} + \\frac{5}{19} + \\frac{11}{19} = 1). As (x \\to \\infty), (f(x) \\to 0).

---

## Behavior of the right-hand side (RHS)

The RHS, (g(x) = 2 \\sqrt{x-1}), is **strictly increasing** for (x \\geq 1). At (x = 1), (g(1) = 0). As (x \\to \\infty), (g(x) \\to \\infty).

---

## Intermediate value theorem application

At (x = 1), (f(1) = 1 > g(1) = 0). As (x \\to \\infty), (f(x) \\to 0 < g(x) \\to \\infty). Since (f(x)) is continuous and strictly decreasing, and (g(x)) is continuous and strictly increasing, the **intermediate value theorem** guarantees exactly one intersection point.

---

## Conclusion

There is **exactly one solution** to the equation, so the correct answer is **(B) 1**.

---

## References

### Nonlinear delay differential equations and their application to modeling biological network motifs [^e2a04562]. Nature Communications (2021). High credibility.

With DDEs, multiple steps ("cascades") within a network can be rigorously simplified into a single step with delay (see below), an approach which has been explored in a variety of biological contexts. This makes interpretation of the phenomenology simpler than with ODEs and reduces the number of equations and parameters in the model. This idea can be visualized by depicting regulatory networks as directed graphs, with nodes representing biological species, and pointed vs. blunt arrows indicating activation vs. repression, respectively (Fig. 1). DDE models allow a single arrow to faithfully capture many biochemical steps, expanding the available dynamics in a model with a reduced set of equations and parameters. DDE regulatory models have in fact been used widely to model a range of biological phenomena such as development, and hematopoiesis. For instance, a 1-variable ODE such as Eq. (1) can only produce exponential growth or decay. A corresponding DDE, such as Eq. (2), can also oscillate (with amplitude approaching zero or infinity) and, if nonlinear, lead to stable oscillations, bistability and chaos.

A key challenge in using DDEs is their mathematical complexity relative to ODEs. For example, while Eq. (1) is 1-dimensional because one initial condition (x at t = 0) determines all future behavior, Eq. (2) is infinite-dimensional, because x must be specified at all times − τ ≤ t ≤ 0 to predict a unique solution. Concretely, the solution to Eq. (1) can be written as a single exponential, while Eq. (2) generally must be specified as a sum of an infinite number of exponential terms to satisfy initial conditions.

Despite the challenges, much progress has been made in analytical understanding of DDEs, and numerical methods exist for simulation. We thus see an opportunity to use DDEs to recapitulate dynamics found in ODE solutions of network motif behavior with fewer genes and thus fewer modeling parameters and equations (Fig. 1), a type of "modeling simplicity."

---

### A space-time tradeoff for implementing a function with master equation dynamics [^57fa10b0]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### Linear mapping approximation of gene regulatory networks with stochastic dynamics [^fc741a40]. Nature Communications (2018). Medium credibility.

To summarize, the LMA procedure to find the approximate time-dependent probability distribution of protein numbers at time t in a general nonlinear GRN involves the following steps: (i) find the linear GRN by replacing any reversible promoter-protein reaction in the nonlinear GRN by a reversible pseudo first-order reaction between promoter states with stochastic rates; (ii) write the closed-set of moment equations for the linear GRN with the stochastic rates replaced by their means, solve for the moments at time t and use the latter to obtain the approximate value of the rate parameter/s at time t characterizing the pseudo first-order reaction/s in the linear GRN; (iii) calculate the time-average of these parameters over the time interval [0, t]; (iv) obtain the time-dependent probability distribution solution of the CME of the linear GRN assuming the rate parameter/s characterizing the pseudo first-order reactions are time-independent constants; (v) the approximate time-dependent probability distribution of the nonlinear GRN at time t is then given by replacing the "constant" rate parameter/s characterizing the pseudo first-order reactions solution in step (iv) by the time-averaged parameters calculated in step (iii).

Steps (i) to (iii) can always be performed but steps (iv) and (v) require the existence of a closed-form solution for the linear GRN and this is the major limitation of the method. When such a solution exists, then for a nonlinear GRN with N protein–promoter binding reactions, the approximate time-dependent probability distribution given by the LMA is a closed-form distribution with N effective parameters to be determined numerically. In practice, this leads to a considerable computational advantage over purely numerical methods such as the SSAand the Finite State Projection method(see Supplementary Note 3 for details) simply because the closed-form distribution is composed of well-known functions that can be evaluated by standard symbolic packages in fractions of a second.

If one is only interested to find an approximate steady-state probability distribution of protein numbers then the procedure is considerably simpler. Step (i) is as before. Step (ii) is the same but now the moments are found in steady-state. The final approximate solution is then obtained by substituting the effective rate parameters found in Step (ii) in the steady-state probability distribution solution of the CME of the linear GRN. In many cases, these steps can be done analytically and hence the output is an approximate solution in closed-form.

---

### An analytic approximation of the feasible space of metabolic networks [^e63ca097]. Nature Communications (2017). Medium credibility.

Assuming a steady-state condition within a cell, metabolic fluxes satisfy an underdetermined linear system of stoichiometric equations. Characterizing the space of fluxes that satisfy such equations along with given bounds (and possibly additional relevant constraints) is considered of utmost importance for the understanding of cellular metabolism. Extreme values for each individual flux can be computed with linear programming (as flux balance analysis), and their marginal distributions can be approximately computed with Monte Carlo sampling. Here we present an approximate analytic method for the latter task based on expectation propagation equations that does not involve sampling and can achieve much better predictions than other existing analytic methods. The method is iterative, and its computation time is dominated by one matrix inversion per iteration. With respect to sampling, we show through extensive simulation that it has some advantages including computation time, and the ability to efficiently fix empirically estimated distributions of fluxes.

---

### Anomalous frozen evanescent phonons [^d42644f1]. Nature Communications (2024). High credibility.

Decomposition of displacement field

For the simplified finite-length mass-and-spring model in Fig. 5b, we denote the axial (-direction) displacement of each mass bywith. We first focus on the masses in the bulk, i.e. Each of these masses is connected to two immediate neighbors and two-th nearest neighbors on both sides. Thus, the force-balance equation for the massreads:

Apparently, this set of equations always supports the linear non-Bloch solution

andare constants. Additionally, multiple frozen evanescent phonons of the following form are possible

Here, the complex wavenumbersatisfies the following condition

Equation (18) has infinitely many solutions for. Here, we only need to considerwith its real part inside of the first Brillouin zone, as other solutions are simply shifted by an integer multiple of, which does not influence the displacement solution Eq. (17).

For the considered parameter settings in Fig. 5c, we have the following solutions for:

The displacement field in Eq. (17) forwith a positive imaginary part represents a frozen evanescent phonon that exponentially decays to the right, while the one with a negative imaginary part stands for a frozen evanescent phonon exponentially decaying to the left.

The general displacement field of the finite-length system in Fig. 5b is a linear combination of the above solutions, i.e.

The displacement field Eq. (22) automatically ensures the force-balance equation Eq. (15) for the masses in bulk, i.e. The unknown complex coefficients, and, can be determined from the boundary conditions applied to the mass-and-spring chain.

For the single-site loading as in Fig. 5c, the left most mass and the right most mass have prescribed displacements,… In addition, masses, and massesmust be in force-balance, too. For the two-sites loading condition in Fig. 5d, the massis similarly prescribed with displacement. Other conditions are the same as the single-site loading conditions. It can be easily verified that the number of the boundary conditions, exactly matches the number of the unknown coefficients. Thus, the unknown coefficients can be determined. Afterward, the total displacement field Eq. (22), as well as individual parts, including the linear part and the frozen evanescent phonons, can be obtained. The analytical formula for the unknown coefficients is very lengthy and thus omitted here.

---

### Adherence to public institutions that foster cooperation [^193d8bdf]. Nature Communications (2021). High credibility.

In summary, for a population following the Stern Judging norm, the equilibrium frequencies of good individuals in each strategic type from the perspective of an arbitrary observer satisfy the following equations:An analogous derivation for Simple Standing yieldsLikewise, for Scoring we haveFinally, for Shunning we haveThese equations correspond to the expressions under private assessment with complete empathy from prior studies, with the exception that g = ∑ i f i g i, the proportion of the population with a good reputation in the eyes of an arbitrary observer, is replaced here by G = ∑ i f i G i, the proportion of the population with a good institutional reputation.

For each social norm, the system of equations above is closed once we specify how G depends on g X, g Y, g Z and on the size and strictness of the institution. We consider two limiting cases: very strict institutions that broadcast an individual as good only if all members agree she is good (q > (Q − 1)/ Q) and very tolerant institutions that broadcast an individual as good provided at least one member views her as good (q < 1/ Q). In these two respective cases we haveFor completeness' sake, we provide the expression for arbitrary q and Q :The resulting system of equations for g X, g Y, and g Z can be solved by radicals when Q ≤ 2. More generally, a unique feasible solution exists for any Q, and it can be computed numerically by iterating the above system of equations after choosing any initial value∈ (0, 1) 3. Successive iteratesremain in [0, 1] 3 and form a contraction: the equations for g X, g Y, g Z are each convex combinations of elements in (0, 1), provided e 2 > 0 and e 1 > 0. Solving this system determines the equilibrium frequency of good individuals of each strategic type from the eyes of an arbitrary observer (g i) and therefore also yields the institutional broadcast (G i, from Eq. (7), and G = ∑ i f i G i). Substituting these expressions into the replicator equation (Eqs. (1) and (2)) provides the dynamics of strategy frequencies f i, allowing us to compute selection gradients, strategic equilibria, and basins of attraction, as shown in Fig. 2.

---

### Minimum epistasis interpolation for sequence-function relationships [^02f80925]. Nature Communications (2020). High credibility.

Thus, to generalize our solution for the two-locus bi-allelic case with one missing genotype to larger sequence spaces and arbitrary geometric arrangements of the missing data, we want to find the value of f that matches our observed phenotypes where available, but otherwise minimizes. To do this, we note thatis non-negative, since the ϵ 2 for each face is non-negative, and that the formula for the ϵ 2 of each face is a second-degree polynomial and thus so is. As a result, our constrained minimization problem is in fact a positive semi-definite quadratic minimization problem with an equality constraint, a form of problem that has an analytical solutionbased on solving a single set of coupled linear equations (see "Methods").

In particular, if we write the set of known genotypes as B and the minimum epistasis interpolation solution as, we first assignfor i ∈ B to satisfy the constraint that our solution is equal to the observed phenotypic value when available. Then, for each i ∉ B, the minimum epistasis reconstruction of the sequence–function relationship is given by settingto the solution of the following α l − ∣ B ∣ equations (one equation for each i ∉ B, see "Methods"):where the values of c (i, j) depend only on the Hamming distance between genotypes i and j and are given by:

The above calculation comes down to solving a set of α l − ∣ B ∣ linear equations, and hence scales cubically with the number of unobserved genotypes, α l − ∣ B ∣. While this approach can readily be applied to moderately sized sequence spaces (e.g. α l less than a million), the exponential dependence of the number of possible genotypes on the sequence length l makes this straight-forward approach impractical for longer sequences. Nonetheless, we can show that the minimization problem can in fact be kernelized to remove this exponential dependence on l, so that ultimately the computational complexity scales linearly in l and cubically in the number of observed genotypes ∣ B ∣ (see Supplementary Methods Proposition 2). Moreover, these equations have a unique solution if and only if the least squares fit of the corresponding nonepistatic model has a unique solution (see Supplementary Methods Proposition 1).

---

### Multiple tipping points and optimal repairing in interacting networks [^d95be1ee]. Nature Communications (2016). Medium credibility.

Despite the seeming complexity of equations (1) and (2), it is noteworthy that there are only two unknown variables, a A and a B, and that all other parameters are fixed. These two equations define two curves in the (a A, a B) plane.

Figure 1a shows a graphical representation of the curves for a random regularnetwork (in which all the nodes have the same degree) with degree of k = 16 and threshold m = 8, for the symmetric parameter values, r A = r B = 0.60 and r d = 0.15. The size of each network is N = 2 × 10 4. The blue curve is a graphical representation of equation (1) and the brown curve is defined by equation (2). The curves, similar to two 'ropes', create a 'knot' that can have up to nine intersections, representing mathematical solutions of the system of equations. However, not all of these solutions represent observable and stable physical states. To see that, observe one of the curves in Fig. 1a, for example, the blue curve described by equation (1). If we increase damage done to network B (that is, we increase a B) and keep everything else constant, some damage will undoubtedly spread to network A. Thus, we expect that when a B is increased, a A must also increase (it would be very unusual if one network improves its activity as a result of damaging the other network). We conclude that the parts of the blue and brown curve that produce physical solutions are only those where a A and a B increase together or decrease together along the curve. This elimination leaves only four states in Fig. 1a that are stable (green circles), whereas the other five states are unstable (red crosses), for this particular choice of parameters. In simulated finite networks, when the network system evolves according to the rules of the model, at t = 0 we have a freedom to set initial conditions for the activities. Systems initially prepared to have a pair of values (a A, a B) corresponding to an unstable solution of equations (1) and (2) will be disturbed by a small fluctuation of a A or a B, owing to the system dynamics, and the values of a A or a B will rapidly change until one of the stable states is reached. Systems that are initially prepared to have values of a A or a B corresponding to a stable solution will fluctuate around these values, until perhaps a large finite fluctuation occurs and the system 'jumps' to another stable state. In general, for any choice of parameters, we have between one and four stable (physical) states. Figure 1b shows the scenario for the same network system when, r A = r B = 0.60 and r d = 0.15. In this case we have two stable states and one unstable state.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^488abf14]. Nature Communications (2021). High credibility.

Chaotic behaviors are prevented in monotonic regulation with linear or cyclic network topology

Due to cyclic network topology (no more than one loop) and monotonic regulation, none of the motifs so far can yield chaotic dynamics. Either non-monotonic feedback (as in the Mackey-Glass equation) or non-cyclic topologyis required for chaos. Double feedback (Fig. 8 a) is a minimal motif fulfilling both requirements, although the non-monotonicity is sufficient (Supplementary Note 8).

The governing equation for such a double feedback motif is thus given by setting the output and both inputs of the logic equation (Eq. (19)) to X (and letting K = 1 for simplicity):

Chaotic behavior is possible for non-monotonic feedback with multiple feedback and disparate delays

Simulation of Eq. (37) demonstrates chaos for some parameters of positive/negative mixed feedback, as evidenced by sustained oscillations with variable maxima (Fig. 8 b), occupying an apparently fractal region in phase space (Fig. 8 c), and a large number of peaks in frequency space (Fig. 8 d). To confirm that the dynamics are in fact chaotic, we calculated the dominant Lyapunov exponent using the Wolf method(see "Methods"), finding a positive (i.e. chaotic) value of 0.0040 ± 0.00055 bits (mean ± standard deviation). We also calculated the box dimension of a reconstructed phase space with coordinates (X (T), X (T − 10), X (T − 20)) used by the Wolf method, yielding fractional dimension ~1.81, with 95% confidence interval (1.76, 1.87). These results indicate chaos (see Supplementary Fig. 4).

---

### Predicting the genomic resolution of bulk segregant analysis [^442eb2f2]. G3 (2022). Medium credibility.

As long as, we can again model the creation of new ancestry breakpoints by a Poisson process along the chromosome. The distance D to the closest ancestry breakpoint downstream of the QTL captured in the sample will then be an exponential random variable with expectation value:

This result provides an analytic solution for the expected mapping resolution of a BSA experiment with an interbreeding population of effective size N e. However, its calculation requires iterative evaluation of Equation (2), and we are not aware of any closed-form solution for this recursion. Even though all elements of x (i) can be easily calculated with the help of a computer, this may not be particularly helpful in allowing us to understand how individual parameters are expected to affect the mapping resolution. To address this issue, we will make use of a previously suggested deterministic approximation for x (i), which can be obtained by mapping the recursion to a differential equation:

We will further replace the summation in Equation (3) by an integral over the t generations of the experiment, yielding:

Note that this integration assumes that recombination events along the genealogy create ancestry breakpoints with a uniform probability of 1/2 in every generation (not just from the F 2 onward). This assumption is obviously incorrect for individuals in the F 1, where every recombination event will generate a new ancestry breakpoint. However, by extending our integration back to the F 0, where recombination events never generate new ancestry breakpoints, we effectively compensate for this effect, at least as long as. This yields an expected mapping resolution of:

In the following, we will refer to Equation (4) as the "recursion" solution, while the approximation presented in Equation (7) will be referred to as the "integration" solution.

---

### Resolving degeneracy in diffusion MRI biophysical model parameter estimation using double diffusion encoding [^f95dc7d6]. Magnetic Resonance in Medicine (2019). Medium credibility.

In Hansen et al 48 the equivalent to the system in Equation 6 is solved reaching two alternative equations for κ, each giving possible solutions. This suggested that, in general, there should be two solutions, one for each branch. However, this is not always the case, as illustrated in Table 1. We derive here an alternative expression of the solution in one equation only. First, Equation 6 can be reparametrized as:After this substitution, Equation 6 can be expressed as a linear system of five equations for the 5 unknowns α, β, γ, δ and ε, decoupled into two independent smaller systems:

Table 1
Illustration of sets of biophysical (BP) parameter values resulting in the same diffusion–kurtosis (DK) parameters

Observe that the coefficients of matrices L and M depend on κ. We will ignore for the moment that the five unknowns are not independent. The solution is unique as long as matrices L and M are invertible. This is the case when κ ≠ 0, sinceand. In the limit of a fully isotropic medium (κ = 0) the system has only two independent equations, not allowing the recovering of the kernel parameters without additional information. By solving the two systems in Equation 8 we find expressions for α, β, γ, δ and ε that only depend on κ and the DK parameters (see Appendix A for solution). Those variables are actually defined from only four kernel parameters (Equation 7), resulting in the coupling equationBy plugging the expressions for α, β, γ, δ and ε as functions of κ into Equation 9, we obtain a nonlinear equation for κ with potentially multiple solutions. Each solution for κ gives a single solution for α, β, γ, δ and ε, which in turn, gives a single solution for the kernel parameters:Thus, the number of solutions to Equation 9 corresponds to the number of BP parameter sets that have the same DK parameters. Table 1 presents cases with up to four solutions. We computed the number of solutions for 10k random points in the BP parameter space. Most present two solutions (70.2%), some only one (29.3%), and only a small proportion have four solutions (0.5%). This gives rise to the previously discussed degeneracy in model parameter estimation from noisy measurements. 17 In contrast with the claim in Hansen et al 51 even in the extreme case of parallel fibers leaving only four unknowns, the five equations in Equation 6 are independent. This is possible due to the nonlinear nature of the system. If κ is known and not zero (including the limiting case κ → ∞ of parallel fibers), the full‐system is invertible as long as f is not 0 or 1, andis not null. In that case, each point in the DK parameter space (signal profile) corresponds to a single set of BP parameters. However, this is not the case for an arbitrary unknown κ. Here, the full‐system has five independent equations with five unknowns, but, depending on the parameter values, it can have only one or multiple solutions. This latter case makes the inverse mapping an ill‐posed problem.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^759c36a3]. Nature Communications (2021). High credibility.

In this work, we thoroughly examine the most common network motifs, with explicit delays and present an approachable, step-by-step view of the mathematical analysis (applying established DDE methods,) in order to make such delay equations easy to use for biologists and others. We show the essential role of delays in autoregulation, feedforward loops, feedback loops, multiple feedback, and complex networks, as well as instances where delays may be ignored. A reference summary is provided at the end in Table 2. Finally, we discuss how these results can be applied to understanding fundamental design principles of various natural biological systems.

---

### The model muddle: in search of tumor growth laws [^7c1427d8]. Cancer Research (2013). Low credibility.

In this article, we will trace the historical development of tumor growth laws, which in a quantitative fashion describe the increase in tumor mass/volume over time. These models are usually formulated in terms of differential equations that relate the growth rate of the tumor to its current state and range from the simple one-parameter exponential growth model to more advanced models that contain a large number of parameters. Understanding the assumptions and consequences of such models is important, as they often underpin more complex models of tumor growth. The conclusion of this brief survey is that although much improvement has occurred over the last century, more effort and new models are required if we are to understand the intricacies of tumor growth.

---

### One-shot learning for solution operators of partial differential equations [^eee17a5e]. Nature Communications (2025). High credibility.

Learning and solving governing equations of a physical system, represented by partial differential equations (PDEs), from data is a central challenge in many areas of science and engineering. Traditional numerical methods can be computationally expensive for complex systems and require complete governing equations. Existing data-driven machine learning methods require large datasets to learn a surrogate solution operator, which could be impractical. Here, we propose a solution operator learning method that requires only one PDE solution, i.e., one-shot learning, along with suitable initial and boundary conditions. Leveraging the locality of derivatives, we define a local solution operator in small local domains, train it using a neural network, and use it to predict solutions of new input functions via mesh-based fixed-point iteration or meshfree neural-network based approaches. We test our method on various PDEs, complex geometries, and a practical spatial infection spread application, demonstrating its effectiveness and generalization capabilities.

---

### Inference of field reversed configuration topology and dynamics during alfvenic transients [^1bc4b0ee]. Nature Communications (2018). Medium credibility.

The term in the denominator p (D) is called the evidence (or marginal likelihood) and normalizes the volume of the posterior distribution to 1.

Given prior and likelihood, the most likely solution is the maximum a posteriori (MAP) estimate, the solution in the posterior with the highest probability.

In the particular case where the forward model is linear, the spatial variable X (r) can always be discretized on a fine grid of dimension k, and a matrixcan be used to relate the discretized variablewith a set of n measurements in

Assuming additive Gaussian measurement noise ε = N(0, Σ D) independent of X, the likelihood function can be modelled by an n -dimensional Gaussian distributionwhereis the data covariance matrix.

The prior distribution can also be approximated by a multivariate probability distribution overwhereis the prior covariance kernel andis the prior mean. It is usually convenient (but by no means necessary) to consider a zero mean μ X = 0 on the prior.

The posterior distribution can likewise be approximated by a k -dimensional Gaussian probability distribution.

Since all probability distributions are Gaussian, the posterior distribution can be obtained analytically, since Gaussian distributions are transformed into Gaussian distributions through linear operations. The posterior mean (MAP estimate) and covariance are given in this case by:

As the dimension k of the multivariate normal distribution is made increasingly large, the multivariate normal distribution approaches a continuous distribution, and at this limit a GP is obtained. In our case, the vector X becomes a continuous function X (r) of the spatial location. All possible solutions for X (r) can then be thought of as being generated by a stochastic process, described by the corresponding GP.

For a large number of situations in plasma physics, transport processes will work in the direction to reduce the spatial gradients of X (r). In other words, our prior belief about X (r) is that it must be a smooth function of r. The prior covariance kernel required for the inference can then be parameterized using the Squared exponential (SE) function, which is one of many options availableto model the spatial correlations between the values of a smooth profile variable at two points r and r ':with.

---

### NIS-teen questionnaire-Q4… [^41bc55ed]. ftp.cdc.gov (2025). Medium credibility.

SAMPLE_USE_CODE = 1, 7 AND ASK_FLU = 0 THEN FILL TIS_UNDER18 AND GO TO TIS_S1AQT ELSE IF AND. SAMPLE_USE_CODE = 1, 7 AND ASK_FLU = 1 AND LONG_FLU_FLAG = 1 THEN FILL TIS_UNDER18 AND GO TO LF_CP_SELECTION ELSE IF AND. How many people less than 18 years old live in this household. IF ONE OR MORE, ENTER OF CHILDREN _____ IF S_NUMB > TIS_UNDER18, THEN GO TO TIS_UNDER18_CONF IF TIS_UNDER18 = 0 AND SAMPLE_USE_CODE = 1, 4, 7, 8 THEN GO TO TIS_S1AQT. IF TIS_UNDER18 = 1–76 AND, THEN GO TO TIS_C2Q0A IF TIS_UNDER18 = 1–76 AND OR S_NUMB = 0, PR. TIS_MULTIAG ELSEIF ALL TIS_S3AGE_x = 77 and/or 99 AND SUM > 0, GO TO INSTRUCTION1 ELSE GO TO TIS_SELECTION_INSTRUCTIONS1 TIS_MULTIAGE.

Since you have more than one child who is, I need a way to refer to each of them during the interview. CONTINUE. 1 RECORD NAMES IN TIS_NAME_1 – TIS_NAME_9]. 7 TIS3CONF That would make this child years old; is that correct. YES. 1 IF, THEN GO.

---

### Towards an exact description of electronic wavefunctions in real solids [^712ba015]. Nature (2013). Excellent credibility.

The properties of all materials arise largely from the quantum mechanics of their constituent electrons under the influence of the electric field of the nuclei. The solution of the underlying many-electron Schrödinger equation is a 'non-polynomial hard' problem, owing to the complex interplay of kinetic energy, electron-electron repulsion and the Pauli exclusion principle. The dominant computational method for describing such systems has been density functional theory. Quantum-chemical methods — based on an explicit ansatz for the many-electron wavefunctions and, hence, potentially more accurate — have not been fully explored in the solid state owing to their computational complexity, which ranges from strongly exponential to high-order polynomial in system size. Here we report the application of an exact technique, full configuration interaction quantum Monte Carlo to a variety of real solids, providing reference many-electron energies that are used to rigorously benchmark the standard hierarchy of quantum-chemical techniques, up to the 'gold standard' coupled-cluster ansatz, including single, double and perturbative triple particle-hole excitation operators. We show the errors in cohesive energies predicted by this method to be small, indicating the potential of this computationally polynomial scaling technique to tackle current solid-state problems.

---

### Structure and inference in annotated networks [^535f823a]. Nature Communications (2016). Medium credibility.

Computationally, the most demanding part of the EM algorithm is calculating the sum in the denominator of equation (7), which has an exponentially large number of terms, making its direct evaluation intractable on all but the smallest of networks. Traditionally one gets around this problem by approximating the full distribution q (s) by Monte Carlo importance sampling. In our calculations, however, we instead use a recently proposed alternative method based on belief propagation, which is significantly faster, and fast enough in practice for applications to very large networks.

Final likelihood value

The EM algorithm always converges to a maximum of the likelihood but is not guaranteed to converge to the global maximum — it is possible for there to be one or more local maxima as well. To get around this problem we normally run the algorithm repeatedly with different random initial guesses for the parameters and from the results choose the one that finds the highest likelihood value. In the calculations presented in this paper we did at least 10 such 'random restarts' for each network. To determine which run has the highest final value of the likelihood we calculate the log-likelihood from the right-hand side of (6) using P (A | Θ, s) and P (s | Γ, x) as in equation (2), the final fitted values of the parameters Θ and Γ from the EM algorithm, and q (s) as in equation (7). (As we have said, the right-hand side of (6) becomes equal to the left, and hence equal to the true log-likelihood, when q (s) is given the value in equation (7).)

---

### State estimation of a physical system with unknown governing equations [^794152e0]. Nature (2023). Excellent credibility.

State estimation is concerned with reconciling noisy observations of a physical system with the mathematical model believed to predict its behaviour for the purpose of inferring unmeasurable states and denoising measurable ones 1,2. Traditional state-estimation techniques rely on strong assumptions about the form of uncertainty in mathematical models, typically that it manifests as an additive stochastic perturbation or is parametric in nature 3. Here we present a reparametrization trick for stochastic variational inference with Markov Gaussian processes that enables an approximate Bayesian approach for state estimation in which the equations governing how the system evolves over time are partially or completely unknown. In contrast to classical state-estimation techniques, our method learns the missing terms in the mathematical model and a state estimate simultaneously from an approximate Bayesian perspective. This development enables the application of state-estimation methods to problems that have so far proved to be beyond reach. Finally, although we focus on state estimation, the advancements to stochastic variational inference made here are applicable to a broader class of problems in machine learning.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^284636ad]. Nature Communications (2021). High credibility.

Biological regulatory systems, such as cell signaling networks, nervous systems and ecological webs, consist of complex dynamical interactions among many components. Network motif models focus on small sub-networks to provide quantitative insight into overall behavior. However, such models often overlook time delays either inherent to biological processes or associated with multi-step interactions. Here we systematically examine explicit-delay versions of the most common network motifs via delay differential equation (DDE) models, both analytically and numerically. We find many broadly applicable results, including parameter reduction versus canonical ordinary differential equation (ODE) models, analytical relations for converting between ODE and DDE models, criteria for when delays may be ignored, a complete phase space for autoregulation, universal behaviors of feedforward loops, a unified Hill-function logic framework, and conditions for oscillations and chaos. We conclude that explicit-delay modeling simplifies the phenomenology of many biological networks and may aid in discovering new functional motifs.

---

### A mechanism for reversible mesoscopic aggregation in liquid solutions [^f7cd174c]. Nature Communications (2019). High credibility.

Additional computational difficulties are caused by the presence of the non-linear termin Eq. (2). We have numerically solved the resulting non-linear differential equations for several realizations of parameters — to be discussed in due time — however the majority of the calculations were performed for a linearized version of Eq. (2) so that the interconversion between the two species is effectively a first order reaction:wherein each phase. Note that if one considers Eq. (7) as a linearized version of Eq. (2), a variable change 2 n 2 → n 2 is implied. Equations (7) can also be considered on their own merit: They can approximate a physical situation where species 1 converts into species 2 by binding a third species that is part of the buffer. If the transport of this third species is fast compared with the transport of species 1 and 2, then the above equations apply. This said, we will continue to call species 1 and 2 "the monomer" and "the dimer," respectively.

The linearity of the reaction terms in Eq. (7) renders the problem linear within an individual phase. The chemical potentials and concentrations can be presented as linear combinations of Yukawa-like terms r −1 e ± qr while the differential equation is thus reduced to an algebraic characteristic equation for the lengths q −1 that can be solved much more readily than the original non-linear differential Eq. (2). This circumstance allows one to readily explore broad ranges of parameters. Once a non-trivial solution of the 1st order case (7) is found, one may then attempt to confirm whether a similar solution exists in the more complicated, 2nd order case from Eq. (2). Throughout, we consider exclusively the spherically symmetric geometry; such solutions are expected to minimize the surface tension between the two phases during phase coexistence.

---

### Combined diffusion-relaxometry microstructure imaging: current status and future prospects [^652023dd]. Magnetic Resonance in Medicine (2021). Medium credibility.

Since the correlation spectrumis infinite dimensional and practical MRI experiments only acquire a finite number of measurements, we should not expect the ILT to have a unique solution. In MRI and NMR spectroscopy, these problems are usually resolved by choosing the "simplest" solution, ie, the unique solution that fulfills minimum‐norm least‐squares (MNLS) criteria. However, while it is straightforward to formulate the ILT within the framework of infinite dimensional Hilbert spaces and derive simple analytic expressions for the unique MNLS solution, the ILT solutions obtained in this manner are usually not very satisfying or useful. This occurs because, unlike the case for MRI and NMR spectroscopy where MNLS solutions are stable and interpretable, the presence of exponential decays in Equation (3) can make the inverse problem severely ill‐posed and highly unstable. As a result, additional assumptions must usually be imposed to get reasonable ILT solutions.

There are several constraints that have been proposed to help achieve robust and interpretable correlation spectra. A widespread approach, originating in some of the early papers on diffusion‐relaxometry, is to assume that the correlation spectrumshould be everywhere nonnegative. This assumption can be motivated by physics and is inherited from earlier work on 1D relaxometry. To numerically perform the inversion and estimate the spectrum, Equation (8) is discretized on grids of predefined ranges withandnonnegative components in the D anddimensions, respectively. The maximum and minimum grid values are chosen such that the solution is physically realistic by excluding negative fractions. The resulting matrix equation iswhere vectors f and d are discretized versions ofand d (x, b, TE), respectively, and the matrix A is a corresponding discretization of the integral equation from Equation (8). A isby, f has length, and d has length, the total number of MR‐encodings in the experiment. This leads to a nonnegative least‐squares inverse problem formulation that can be written asusing the standard 2‐norm. This constrained optimization problem does not have an analytic solution, but can still be solved iteratively using efficient optimization algorithms. Importantly, nonnegativity constraints have good theoretical characteristics, and existence of a unique nonnegative solution can sometimes be guaranteed in inverse problems that would otherwise have infinitely many solutions.

---

### The making of the standard model [^0fc9a4ce]. Nature (2007). Excellent credibility.

A seemingly temporary solution to almost a century of questions has become one of physics' greatest successes.

---

### Adaptive resetting for informed search strategies and the design of non-equilibrium steady-states [^9a9db8d1]. Nature Communications (2025). High credibility.

The final value theorem for Z-transforms states that. By using it, we get (see section 6 of the Supplementary Information)where 〈 N R 〉 is the mean number of time steps between consecutive resetting events. Note that the steady-state in Equation (14) is well defined whenever 〈 N R 〉 is finite, regardless of whether or not the process without resetting has a steady-state. This is a generalization of a well-known result in the theory of standard resetting to state- and time-dependent resetting.

To estimate the NESS, we first sample a set of N trajectories without resetting of length M Δ t. We stress that M should be large enough such that, had we used resetting, the probability of surviving M steps without resetting would be negligible, i.e. Then, we use Equations (12) and (14), and the definition of the Z-transform, to obtainThis estimation results in an unnormalized distribution, which should be normalized. The normalization factor provides an estimate for the mean time between consecutive resetting events, 〈 N R 〉. Equation (15) shows that the estimation of the NESS with resetting, from trajectories without resetting, is done by averaging the histogram of positions over time and trajectories, but reweighing each trajectory, at every time step, by its survival probability.

Prediction and design of non-equilibrium steady-states

The above results can be used to predict and design NESS of spatially-dependent resetting protocols. We demonstrate this using two examples.

It is well known that for free diffusion with a constant resetting rate, a Laplace distributed NESS emerges. An analytical solution for the NESS of diffusion with a parabolic resetting rate r (x) = r 0 x 2 is also known. Interestingly, in both cases, the tails of the NESS decay as, with α = 1 for the constant resetting rate, and α = 2 for the parabolic resetting rate. This raises a more general question: what is the asymptotics of the NESS for diffusion with a power-law resetting rate r (x) = r 0 ∣ x ∣ λ. While there are currently no known closed-form solutions for the NESS with λ ≠ {0, 2}, we can easily estimate the resulting NESS using the procedure described in the previous section.

---

### A general model for ontogenetic growth [^10660e44]. Nature (2001). Excellent credibility.

Several equations have been proposed to describe ontogenetic growth trajectories for organisms justified primarily on the goodness of fit rather than on any biological mechanism. Here, we derive a general quantitative model based on fundamental principles for the allocation of metabolic energy between maintenance of existing tissue and the production of new biomass. We thus predict the parameters governing growth curves from basic cellular properties and derive a single parameterless universal curve that describes the growth of many diverse species. The model provides the basis for deriving allometric relationships for growth rates and the timing of life history events.

---

### Modeling and measurement of lead tip heating and resonant length for implanted, insulated wires [^f6c79829]. Magnetic Resonance in Medicine (2024). Medium credibility.

Although Equation (16) reduces to the simple exponential model when the reflection coefficient, Equation (15) does not.

Assuming a constant electric field, substituting Equations (16) into (3) yields an expression for the complex voltage at the lead tip.

In the following analysis, we will assume thatis real. The case of a complex reflection coefficient can be considered by retaining its imaginary part in Equation (17), or alternatively evaluating the line integral in Equation (3) numerically. Analogous to Equation (9), we can derive a closed form expression for lead tip temperature rise under the condition of a constant electric field:

The resonant length for the transmission line model can be found by the same procedure to derive Equations (10) and (11), but because of the complexity of the expressions it might be simpler to numerically maximize Equation (18), instead of constructing an analytical solution. An exception occurs for the physically interesting case of, corresponding to an electrical open circuit (see Discussion), under which Equation (18) reduces to manageable trigonometric and hyperbolic functions. Using the identitiesand, with, we can apply Equation (5), take the derivative ofwith respect to length, and set it equal to zero. After some algebra and applying further identities, we find the following condition for the resonant length

The first factor in Equation (19) can never be zero when, so we conclude that the resonant length is equal to the smallest, positive solution to the relatively simple transcendental equation:

Solving for the resonant length in the transmission line model withyieldswhereis tabulated in Table 1.

Like its counterpart from the simple exponential model Equation (13), the resonant length for the transmission line model is bounded byandwhen. This can be readily seen from Table 1, or directly from Equation (20) because bothand the productare never negative, implying that the argument of the tangent functionfor the smallest positive root of Equation (19) must lie in the second quadrant, that is, betweenand.

For the short wire case, Equations (17) or (18) can be expanded analogously to Equation (14). At least two limiting cases emerge, both of which, like Equation (14), display a quadratic dependence of temperature rise on length in the short wire case:

---

### Linear mapping approximation of gene regulatory networks with stochastic dynamics [^8fc66f3b]. Nature Communications (2018). Medium credibility.

Substituting Eq. (2) in Eq. (1) and solving the resultant coupled set of differential equations, we obtain a time-dependent solution for the moments. These solutions can then be substituted in Eq. (2) to obtain our estimate for the effective rate parameter in the linear (mapped) GRN:where f denotes function of. This function can generally be obtained by numerical solution of the aforementioned modified differential equations; in steady-state conditions an explicit formula can also be obtained:The last and remaining question is how can we use this parameter estimate to build the full time-dependent solution of the nonlinear GRN. We observe that the time-dependent probability distribution solution of the CME of the linear GRN with general time-dependentis likely impossible to obtain in closed-form. However it is possible to solve ifwere a constant independent of time (see "Methods" section); let this general probability distribution solution be denoted as. We shall then make the assumption that the time-dependent probability distribution solution of the CME of the linear GRN with general time-dependentgiven by Eq. (3) is well approximated bywhereis the time-average of Eq. (3):A rigorous theoretical justification of this assumption can be found in the Methods. Hence the linear mapping approximation (LMA) of the probability distribution of the nonlinear feedback loop at time t is given by. Note that the time-averaging assumption is only needed if one wants to calculate the distribution in finite time; in steady-state, there is no need of the assumption since thenis constant (and equal to Eq. (4)) and the steady-state probability distribution is directly given by.

---

### Mathematical modeling is more than fitting equations [^994275ea]. The American Psychologist (2014). Low credibility.

Comments on the comments made by Brown et al. (see record 2013-24609-001). The article by Brown et al. regarding the Fredrickson and Losada (see record 2005-11834-001) article discussed the use of differential equations in science and repeated our earlier observation (Luoma, Hämäläinen, & Saarinen, 2008) that there is lack of justification for the use of the Lorenz equations in the latter article. In this comment we want to point out that Brown et al. presented a very narrow view on mathematical modeling in behavioral research. We describe how the conceptual use of mathematical models is essential in many fields.

---

### Adaptive resetting for informed search strategies and the design of non-equilibrium steady-states [^cfcebb66]. Nature Communications (2025). High credibility.

Fig. 3
Search with environmental information.

a The mean first-passage time to the origin as a function of r 0 and b for one-dimensional diffusion with diffusion coefficient D = 1. Motion starts at L = 1 and is conducted under position-dependent resetting of the form given in Equation (8). The dashed and dotted lines indicate the optimal rate for diffusion with a constant resetting rate, r 0 = 2.5, and the optimal rate for a parabolic resetting, r 0 = 5.6 b 2, respectively. b First-passage time distributionsfor the three marked points in the phase space of (a), estimated from simulations without resetting (solid lines), and brute-force simulations with resetting (markers). For simulations with resetting, we plot the meanstandard deviation of ten independent repetitions with 10 4 trajectories each (errors are smaller than the marker size in almost all cases).

We emphasize that all the results of Fig. 3 a were obtained using a single set of trajectories with no resetting and simply re-evaluating Equations (4–7) for every value of r 0 and b, leading to a different MFPT through Equation (3). Previously, obtaining the above results would have required performing an ensemble of brute-force simulations at every value of the parameters r 0 and b, to map out the MFPT phase space. This approach would be computationally prohibitive in many cases. Moreover, the strength of our approach becomes even clearer when considering other forms of r (x) apart from Equation (8). Tackling these using brute-force simulations, analytical methods, or experiments, would require a complete reanalysis of the problem. On the other hand, using our method, we can use the same initial ensemble of trajectories to generate the MFPT phase space for any resetting protocol, with minimal added cost. In section 4 of the Supplementary Information, we demonstrate this for several other sigmoidal-shaped resetting protocols (Supplementary Fig. S2).

---

### Exact invariant solution reveals the origin of self-organized oblique turbulent-laminar stripes [^7f18b4c1]. Nature Communications (2019). High credibility.

Wall-bounded shear flows transitioning to turbulence may self-organize into alternating turbulent and laminar regions forming a stripe pattern with non-trivial oblique orientation. Different experiments and flow simulations identify oblique stripe patterns as the preferred solution of the well-known Navier-Stokes equations, but the origin of stripes and their oblique orientation remains unexplained. In concluding his lectures, Feynman highlights the unexplained stripe pattern hidden in the solution space of the Navier-Stokes equations as an example demonstrating the need for improved theoretical tools to analyze the fluid flow equations. Here we exploit dynamical systems methods and demonstrate the existence of an exact equilibrium solution of the fully nonlinear 3D Navier-Stokes equations that resembles oblique stripe patterns in plane Couette flow. The stripe equilibrium emerges from the well-studied Nagata equilibrium and exists only for a limited range of pattern angles. This suggests a mechanism selecting the non-trivial oblique orientation angle of turbulent-laminar stripes.

---

### Neurons exploit stochastic growth to rapidly and economically build dense dendritic arbors [^ab3baf69]. Nature Communications (2025). High credibility.

Steady-state solution of the three-state model

To determine the homogeneous steady-state solution, we begin by setting all time derivatives and spatial transport terms to zero in Eqs. (1–3). By expressingin terms ofandusing Eq. (3), we reduce the system to a two-dimensional matrix equation forand:

Here,… The coefficient matrix, denoted by, has two real eigenvalues of opposite signs, as indicated by its negative determinant. Sinceandmust asymptotically approach zero asto be physically meaningful, the positive eigenvalue should be discarded. Denoting the negative eigenvalue as, the solution takes the form asand. These solutions reveal that the growing and shrinking branches follow exponential length distributions with a characteristic mean length of. The vectoris an eigenvector corresponding to the eigenvalue. By combining the eigenequation with the boundary condition Eq. (6), we can derive explicit expressions forandin terms ofand:

Substituting the expressions forandderived above into the definition of total length densityand cancelingfrom both sides, we obtain a self-consistent cubic equation for:where, andare given by,… The cubic equation has a single positive root, corresponding to our desired value of. The dendrite length densitycan be further obtained from the following quadratic equation derived from(whereis the identity matrix):

The total dendrite number density is given by.

---

### Goldmann's equation and clinical measures of aqueous dynamics [^7600d7db]. Experimental Eye Research (2004). Low credibility.

Goldmann's equation has served for over 50 years as an adequate description of aqueous humor dynamics for clinical applications. Recent advances in therapeutics for glaucoma have made it necessary to revise the equation and to reinterpret the meanings of its parameters.

---

### A neurophysiological basis for aperiodic EEG and the background spectral trend [^5a1af79a]. Nature Communications (2024). High credibility.

This modified equation fit the EEG spectra at baseline conditions well, but following propofol infusion, the equation did not decay fast enough to capture the EEG spectra (Fig. S 7). This was seemingly because the original equations oversimplified the kinetics of inhibitory synapses. Notably, Eq. 1 is an analytical solution for exponentially decaying synaptic responses, whereas real synaptic responses are characterized by a rise time and decay time: exp (− t / τ 1) −exp (−t/ τ r). It follows that a more accurate synaptic response function for the power spectrum is given bywhere the first term is the analytical solution to the power spectrum of the difference of exponentials.is the risetime of inhibitory synaptic currents, which we fixed atms to keep the number of fitting parameters low. Notably, if we fixed bothms andms, physiologically plausible values, all our baseline data could be captured by simply changingand λ (Fig. S 5a). Moreover, fitting Eq. 6 provided consistent and physiologically plausible estimates forboth at baseline and following propofol infusion (Fig. 8e, f). Thus, Eq. 6 is biophysically motivated, physiologically interpretable, has only three parameters to fit, and captured our data well both at baseline and following the infusion of propofol (Fig. 8d; Fig. S 5a–c).

Reporting summary

Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.

---

### Inferring time derivatives including cell growth rates using gaussian processes [^952ce293]. Nature Communications (2016). Medium credibility.

Estimating the time derivatives of a signal is a common task in science. A well-known example is the growth rate of a population of cells, which is defined as the time derivative of the logarithm of the population sizeand is used extensively in both the life sciences and biotechnology.

A common approach to estimate such derivatives is to fit a mathematical equation that, say, describes cellular growth and so determine the maximum growth rate from the best-fit value of a parameter in the equation. Such parametric approaches rely, however, on the mathematical model being a suitable description of the underlying biological or physical process and, at least for cellular growth, it is common to find examples where the standard models are not appropriate.

The alternative is to use a non-parameteric method and thus estimate time derivatives directly from the data. Examples include taking numerical derivativesor using local polynomial or spline estimators. Although these approaches do not require knowledge of the underlying process, it can be difficult to determine the error in their estimationand to incorporate experimental replicates, which with wide access to high-throughput technologies, are now the norm.

Here we develop a methodology that uses Gaussian processes to infer both the first and second time derivatives from time-series data. One advantage of using Gaussian processes over parametric approaches is that we can fit a wider variety of data. Rather than assuming that a particular function characterizes the data (a particular mathematical equation), we instead make assumptions about the family of functions that can describe the data. An infinite number of functions exist in this family and the family can capture many more temporal trends in the data than any one equation. The advantages over existing non-parametric methods are that we can straightforwardly and systematically combine data from replicate experiments (by simply pooling all data sets) and predict errors both in the estimations of derivatives and in any summary statistics. A potential disadvantage because we use Gaussian processes is that we must assume that the measurement noise has a normal or log-normal distribution (as do many other methods), but we can relax this assumption if there are multiple experimental replicates.

---

### Modeling presynaptic inhibition by the amyloid precursor protein demonstrates one potential mechanism for preventing runaway synaptic modification in Alzheimer's disease [^acb8c430]. Alzheimer's & Dementia (2025). Medium credibility.

The instability of the equation can also be shown by solving Equation 3 as a first‐order differential equation for input patterns p and q to compute the change in synaptic strength over time. This yields a solution that grows linearly with presynaptic activity p and direct post‐synaptic activity q, but also grows exponentially with presynaptic activity p:

Note that this exponential growth concerns not only the growth in strength of individual synapses, but also the spread of undesired growth to synapses that should have no growth (orange lines weights in Figure 2L, Q), even if individual synapses are not allowed to grow beyond a certain maximum value. The exponential growth of synaptic weight in this equation demonstrates one major driving force that is proposed to underlie the pathology of early AD. This erroneous modification involves more than just excitotoxicity because it concerns increased synaptic modification at early stages rather than just increased neural activity. However, in some cases it could have the side effect of excitotoxicity depending on how synaptic growth influences activity among particular subsets of neurons.

The solution to Equation 3 can be made more stable by adding features that correspond to mechanisms that regulate biological synaptic transmission and synaptic modification. For example, as shown in Figure 3D, presynaptic inhibition of synaptic transmission during synaptic modification can prevent the exponential growth. This can provide stability in the equation by reducing the influence of prior weight W on synaptic modification. Abstract models of associative memory commonly avoid the instability using implementations of the equations that simply ignore retrieval during the encoding of new associations., Thus, stability of associative memory function could be obtained by ensuring that synaptic modification is combined with presynaptic inhibition of synaptic transmission during synaptic modification. Studies implementing optogenetic manipulations of LTP and long‐term depression (LTD) have demonstrated that the hippocampus can be programmed using simple Hebbian learning driven by LTP, resulting in the creation and ablation of synthetic memories in mice. It is necessary to provide biological mechanisms that ensure that the induction of these functional synaptic modifications does not lead to the instability innate in Hebbian learning. Physiologically, this could be obtained by secretion of sAPPα causing an encoding phase that includes both enhancement of LTP (encouraging modification for encoding of memories) and presynaptic inhibition of synaptic transmission via presynaptic GABA B R1a receptors, whereas less secretion of sAPPα would correspond to the retrieval phase.

---

### Quantum trajectory framework for general time-local master equations [^1a34e71a]. Nature Communications (2022). High credibility.

Master equations are one of the main avenues to study open quantum systems. When the master equation is of the Lindblad-Gorini-Kossakowski-Sudarshan form, its solution can be "unraveled in quantum trajectories" i.e., represented as an average over the realizations of a Markov process in the Hilbert space of the system. Quantum trajectories of this type are both an element of quantum measurement theory as well as a numerical tool for systems in large Hilbert spaces. We prove that general time-local and trace-preserving master equations also admit an unraveling in terms of a Markov process in the Hilbert space of the system. The crucial ingredient is to weigh averages by a probability pseudo-measure which we call the "influence martingale". The influence martingale satisfies a 1d stochastic differential equation enslaved to the ones governing the quantum trajectories. We thus extend the existing theory without increasing the computational complexity.

---

### Evaluating the evidence for exponential quantum advantage in ground-state quantum chemistry [^03db0a64]. Nature Communications (2023). High credibility.

Due to intense interest in the potential applications of quantum computing, it is critical to understand the basis for potential exponential quantum advantage in quantum chemistry. Here we gather the evidence for this case in the most common task in quantum chemistry, namely, ground-state energy estimation, for generic chemical problems where heuristic quantum state preparation might be assumed to be efficient. The availability of exponential quantum advantage then centers on whether features of the physical problem that enable efficient heuristic quantum state preparation also enable efficient solution by classical heuristics. Through numerical studies of quantum state preparation and empirical complexity analysis (including the error scaling) of classical heuristics, in both ab initio and model Hamiltonian settings, we conclude that evidence for such an exponential advantage across chemical space has yet to be found. While quantum computers may still prove useful for ground-state quantum chemistry through polynomial speedups, it may be prudent to assume exponential speedups are not generically available for this problem.

---

### Classification-based inference of dynamical models of gene regulatory networks [^dd8309ef]. G3 (2019). Medium credibility.

Glass networks:

In what follows, we show that one choice of the regulation-expression function permits a radical simplification of the gene circuit inference problem. If the regulation-expression function is chosen to be the Heaviside function, the resulting differential equations (Equation 1) are piece-wise linear and are referred to as Glass networks.

Using the state vectorto represent a point in the G -dimensional state space of the model and the vectorto represent therow of the genetic interconnectivity matrix, the Glass equations may be written asThe gene may said to be "ON" or "OFF" depending on whether the gene product is being synthesized or not respectively. Equation 3 implies thatThus the "gene g ON" and "gene g OFF" configurations are separated in state space by the hyperplane defined by the equation. We call this the switching hyperplane.is the normal to the switching hyperplane andis the perpendicular distance of any pointto the hyperplane. Furthermore, Equation 4 and Equation 5 imply that for Glass networks, the regulatory parameters, and, and the kinetic parameters, and, are separable. The former determine the switching hyperplane, while the latter determine the trajectories on either side of the hyperplane. Figure 1D, G shows examples of the switching hyperplanes and trajectory of a two-gene gene circuit (Figure 1A, B) having a stable spiral equilibrium solution.

---

### Solving the many-electron schrödinger equation with a transformer-based framework [^55e4e039]. Nature Communications (2025). High credibility.

Accurately solving the Schrödinger equation for intricate systems remains a prominent challenge in physical sciences. Here we present QiankunNet, a neural network quantum state (NNQS) framework that combines Transformer architectures with efficient autoregressive sampling to solve the many-electron Schrödinger equation. At its core is a Transformer-based wave function ansatz that captures complex quantum correlations through attention mechanisms, effectively learning the structure of many-body states. The quantum state sampling employs layer-wise Monte Carlo tree search (MCTS) that naturally enforces electron number conservation while exploring orbital configurations. The framework incorporates physics-informed initialization using truncated configuration interaction solutions, providing principled starting points for variational optimization. Our systematic benchmarks demonstrate QiankunNet's versatility across different chemical systems. For molecular systems up to 30 spin orbitals, we achieved correlation energies reaching 99.9% of the full configuration interaction (FCI) benchmark, setting a new standard for neural network quantum states. Most notably, in treating the Fenton reaction mechanism, a fundamental process in biological oxidative stress, QiankunNet successfully handled a large CAS(46e,26o) active space, enabling accurate description of the complex electronic structure evolution during Fe(II) to Fe(III) oxidation.

---

### Nonlinear wave propagation governed by a fractional derivative [^75a3e696]. Nature Communications (2025). High credibility.

Fractional Laplacians are nonlocal operators, and indeed this property enters Eq. (2) through the presence of the Hilbert transform. This leads to the discontinuity in the derivative of the spectrum, which in turn leads to the non-exponential asymptotic temporal behaviour of the solutions. The nonlocality makes it difficult to find analytic solutions of Eq. (4) since approaches that rely on local or asymptotic expansions cannot be used. Even though closed-form solutions to the Benjamin-Ono equation, which also involves a Hilbert transform and which is somewhat related to Eq. (4), are known, these solutions do not carry over to the present case.

In conclusion, we report the generation and full characterisation of nonlocal solitons that satisfy the Hilbert-NLS equation, a special case of the fractional NLS equation. These solitons have striking properties, summarised below, for which applications may be developed in future, for example in telecommunications or in laser physics.
The soliton spectrum has a discontinuous derivative, which reflects the discontinuous derivative of the dispersion relation Eq. (1). In turn, this discontinuous derivative leads to non-exponential asymptotic decay in time. This reflects the nonlocal properties of the Hilbert transform that enters the fractional Laplacian.
These solitons have a very small time-bandwidth product, much smaller than that of conventional solitons. This can be attributed to the compact spectrum, which is a consequence of the discontinuous derivative. Even though the algebraic decay implies a broad spectrum, this is not yet apparent at the relatively high intensities that determine the solitons' FWHM.
The soliton energy is independent of their peak power or width.

Our theoretical model is sufficient for all the cases considered here, even though it is based on a conservative Hilbert nonlinear Schrödinger equation. It is in very good agreement with both our experimental results and with numerical simulations based on an iterative map model, consistent with earlier studies of stationary solutions in similar systems with non-fractional dispersion relations.

While our experiments were carried out in an optics context, our results are universal and agnostic to the particular physical embodiment of the governing equation. As such, our results can act as a starting point for a new wave of experimental investigations of linear and nonlinear wave phenomena in media with fractional dispersion relations.

---

### Wave kinetics of random fibre lasers [^c4403925]. Nature Communications (2015). Medium credibility.

There are several principal differences between the wave kinetic equation (3) for the active cyclic system and a classical wave kinetic equation. First of all, as the energy pumping and dumping is non-homogenous over the evolution coordinate z within the cycle, the wave spectrum exhibits strong changes during the evolution within the cycle, so the local stationary statistically steady state does not exist. Formally that means one cannot equate the derivative over evolution coordinate z to zero and find in this way a stationary solution similar to classical wave kinetics.

The traditional wave kinetic equation for a conservative Hamiltonian system could be obtained from the local wave kinetic equation (3) in the limit of zero gain. In this case, the gain-related Lorentzian termsin the right-hand side of equation (3) turn into delta-functions, ensuring the energy conservation and giving the classical kinetic wave equation. However, the classical kinetic equation derived from NLSE is trivial in a sense that the right-hand side vanishes because of integrability of one-dimensional NLSE. In other words, in NLSE-based integrable system, the spectrum does not evolve at all. This is not the case for local wave kinetic equation (3). From the formal point of view, wave interaction is uniform over slow evolution timein classical wave kinetics. In active cyclic systems, however, these interactions are mediated by a non-homogenous gain (see Lorentzian factors due to gain in equation (3)), which results in the interaction being effective over the finite interval of the evolution coordinate only. Note that equation (3) does not conserve momentum, because the pumping and dumping are changed over the evolution coordinate z. We would like to also note that an interesting modification of the conventional wave turbulence kinetic equation was studied in the paper by Ascheri et al. where the integrability of the original NLSE model was broken by the transverse spatial inhomogeneity of the refraction index.

---

### Nonlinear optical components for all-optical probabilistic graphical model [^9f412763]. Nature Communications (2018). Medium credibility.

Multiplication

Inserting the saturable absorption equationin the differential equation for the nonlinear absorption, and solving leads toHere I sat is the saturation peak irradiance, α 0 is the weak field absorption, L is the thickness of SA material and I in and I out are the input and output peak irradiance, respectively. A numerical solution of Eq. (3) and its fit with an exponential function are plotted in Fig. 3a. Including the TPA term in the nonlinear absorption differential equationd I / d Z = − α 0 I − βI 2, leads to an explicit analytical solution, where β is the TPA coefficient and. A numerical solution of Eq. (4) is plotted in Fig. 3b as well as its fit with a natural logarithm function. The result of combinations for 29 identical logarithm inputs and an exponentiation gives the multiplication of the inputs as illustrated in Fig. 3c. The ideal multiplication result is plotted as a linear fit in Fig. 3c. Note that the peak irradiance in Eq. (3) and (4) can be replaced with energy per pulse (fluence or photon number as well) without any change in concept of their comparison with the exponential and logarithm functions. We use energy per pulse (E) for simulation as the experimental data were measured in terms of energy per pulse. In Fig. 3a and Fig. 3b we need to limit the range of fitting in order to get maximum overlap of the exponential and natural logarithm fit functions with SA and TPA solutions. Also the normalized-root-mean-square error (NRMSE) should be less than 1% and is defined asLimiting the ranges also comes from the natural behavior of the SA and TPA process where Eq. (3) and (4) start from zero for no input energy. However, we know that e 0 = 1 and ln(0) is undefined. Therefore, bounding the input intensity range for fitting is necessary for convergence and adequate fitting of the solutions of the TPA and SA equations with the target functions. The criteria are the maximum error acceptable to reproduce the function.

---

### Exact analytical solutions of the bloch equation for the hyperbolic-secant and chirp pulses [^02f2955f]. Magnetic Resonance in Medicine (2025). Medium credibility.

Purpose

To improve the accuracy and generality of analytical solutions of the Bloch equation for the hyperbolic-secant (HS1) and chirp pulses in order to facilitate application to truncated and composite pulses and use in quantitative methods.

Theory and Methods

Previous analytical solutions of the Bloch equation during an HS1 pulse driving function are refined and extended in this exact solution for arbitrary initial magnetization and pulse parameters including asymmetrical truncation. An unapproximated general solution during the chirp pulse is derived in a non-spinor formulation for the first time. The solution on the extended complex plane for the square pulse is included for completeness.

Results

The exact solutions for the HS1, chirp, and square pulses demonstrate high consistency with Runge-Kutta simulations for all included pulse and isochromat parameters. The HS1 solution is strictly more accurate than the most complete prior general solution. The analytical solution of the BIR-4 composite pulse constructed using asymmetrically truncated HS1 component pulses likewise agrees with simulation results.

Conclusion

The derived analytical solutions for the Bloch equation during an HS1 or chirp pulse are exact regardless of pulse parameters and initial magnetization and precisely conform with simulations enabling their use in quantitative MRI applications and setting a foundation for the analytical consideration of relaxation and pulses in multiply rotating frames.

---

### Wall mechanics and exocytosis define the shape of growth domains in fission yeast [^2c988911]. Nature Communications (2015). Medium credibility.

Here χ is the predicted, advected form of the fluorescence profile γ recorded in the experiments. Because we have measured the velocity of wall elements with Qdots, the only degrees of liberty are the two parameters α and k. They represent, respectively, a nonlinearity coefficient for the rate of deposition and the rate of incorporation of new wall material in the load-bearing matrix. According to the steady-state hypothesis, we will consider the steady-state solution of this equation, which means that the left-hand term is equal to zero. The steady-state advected distribution is:

Substituting the explicit form for the areal strain, the solution to this equation is:

where ξ is a dummy variable of integration. The time of transit to point τ (s) is given by the integral:

When k goes to infinity, the exponential approaches zero over the entire domain of integration except at ξ = s where it is equal to 1 as well as the functions that precede the exponential. Thus, for large k, the advected profile is close to the observed fluorescence profile. As k decreases with respect to the time of transit, the fluorescence becomes more 'smeared' laterally.

---

### Bacterial chemotaxis in a microfluidic T-maze reveals strong phenotypic heterogeneity in chemotactic sensitivity [^8146c42f]. Nature Communications (2019). High credibility.

In the absence of chemotaxis, when only the random motility of bacteria (with diffusivity D) is considered, the expected number of bacteria after junction n is computed as the solution to a diffusion equation (Methods), yielding B n (t) = , where B 0 is the initial concentration of bacteria and d n is the one-dimensional distance from the inlet to that junction (Supplementary Fig. 7). Chemotaxis can be incorporated into this solution by multiplying B n by a sorting index, which represents the ratio of the advection-diffusion solution and the diffusion solution (see Methods for derivation), calculated at the end of junction n, where y v is the length of the semibranch of each junction in the y -direction. Note that the predicted exponential increase of the sorting index S n with the junction number n (Eq. (1)) is in qualitative agreement with the increase of the sorting index observed in the experiments with M. adhaerens (compare Fig. 2b with Supplementary Fig. 6b). The exponential dependence of the ratio of the two populations on the junction number in Eq. (1) indicates that sorting could be further improved by increasing the number of junctions in the device.

---

### Unravelling the mechanism of neurotensin recognition by neurotensin receptor 1 [^28de6a8c]. Nature Communications (2023). High credibility.

Near-equilibrium relaxation of induced-fit binding

Solving the rate equations of the induced-fit binding model (see Fig. 4b) is complicated by the fact that the binding step is a second-order reaction, which leads to products of the time-dependent concentrations [R 1] and [L] of unbound receptors and unbound ligands in the equations. In the standard pseudo-first-order approximation, the rate equations are simplified by assuming that the total ligand concentration [L] 0 greatly exceeds the total receptor concentration [R] 0, so that the amount of ligand consumed during binding is negligible compared to the total amount of ligand. The concentration of the unbound ligand then can be taken to be constant, and the rate equations only contain terms that are linear in the time-dependent concentration of the receptor, which makes them solvable. In this solution, the time-dependent evolution of the concentrations is a double-exponential relaxation into equilibrium. A general solution of the rate equations that holds for all total receptor and ligand concentrations [R] 0 and [L] 0 can be achieved by expanding the rate equations around the equilibrium concentrations of the bound and unbound receptors and ligands. This expansion leads to a generally valid linearization of the rate equations and captures the final, double-exponential relaxation into equilibrium. The two rates of this double-exponential relaxation process are

Withand with the overall dissociation constantof the induced-fit binding model. This general result for the two rates of the final, double-exponential relaxation includes the result derived in the pseudo-first approximation as a special case in the limit of large total ligand concentrations [L] 0.

Near-equilibrium relaxation of conformational-selection binding

As in the case of the induced-fit binding model, a general solution of the conformational-selection binding model (see Fig. 4b), which holds for all total receptor concentrations [R] 0 and ligand concentrations [L] 0, can be achieved by expansion of the rate equations around the equilibrium concentrations of the bound and unbound receptors and ligands. In this general solution, the two rates of the final, double-exponential relaxation into equilibrium are

Withand δ as in Eq. (5), and with the overall dissociation constantof the conformational-selection binding model.

---

### Constructing minimal models for complex system dynamics [^7f7aae44]. Nature Communications (2015). Medium credibility.

One of the strengths of statistical physics is the ability to reduce macroscopic observations into microscopic models, offering a mechanistic description of a system's dynamics. This paradigm, rooted in Boltzmann's gas theory, has found applications from magnetic phenomena to subcellular processes and epidemic spreading. Yet, each of these advances were the result of decades of meticulous model building and validation, which are impossible to replicate in most complex biological, social or technological systems that lack accurate microscopic models. Here we develop a method to infer the microscopic dynamics of a complex system from observations of its response to external perturbations, allowing us to construct the most general class of nonlinear pairwise dynamics that are guaranteed to recover the observed behaviour. The result, which we test against both numerical and empirical data, is an effective dynamic model that can predict the system's behaviour and provide crucial insights into its inner workings.

---

### Statistical mechanics for metabolic networks during steady state growth [^8aeef127]. Nature Communications (2018). Medium credibility.

Competitive growth dynamics

The second possibility is that the Boltzmann distribution emerges from competitive growth dynamics. Since its historical origins in statistical physics, much research has been devoted to uncovering the dynamical roots of Boltzmann distributions, whose study highlighted important concepts and applications, ranging from ergodicity to fluctuation-response relations. The same questions naturally arise in the context of its application in metabolism. It has been shown that the maximum entropy distribution at a fixed average growth rate is recovered independently and justified dynamically as the steady state of logistic growth. Since the logistic growth is the standard model used to experimentally fit optical density curves, this link also provides a possible interpretation of the maximum entropy parameter β, as we discuss below.

Consider a population of initial size N 0 in a medium with carrying capacity N C and assume that the intrinsic growth rates of individuals, λ i, are sampled independently from a distribution q (λ), defined over the feasible polytope. In the simplest setting, upon neglecting growth state transitions, the number n i of cells with growth rate λ i will evolve in time according to

Then, with

Under a mean field approximation, the steady states of these dynamics are distributions with maximum entropy form at a fixed average growth rate, where the asymptotic optimization parameter, is given implicitly by the equation

Equation (13) can be viewed as a relationship between quantities that can be independently estimated for a specific experimental setup: the inoculum size (N 0) and carrying capacity (N C) on the one hand, as well as the typical value of β, via Eq. (8) or direct fitting of measured metabolic fluxes, on the other.

Taken together, the two mechanisms, active regulation and competitive growth dynamics, need not be exclusive, and can operate concurrently. A simple diagnostic that could provide insight into the relative importance of both mechanisms is to examine whether the relationship of Eq. (13) is satisfied. If it were, it would suggest that the Boltzmann distribution is dynamical in origin. If, on the other hand, the values of β inferred from fitting the maximum entropy model were higher than those derived from the N C / N 0 ratio and Eq. (13), additional active regulation may be at work. In the Results section and Supplementary Methods, we provide estimates of these quantities for the experiments under consideration.

---

### Radiolysis as a solution for accelerated ageing studies of electrolytes in lithium-ion batteries [^dd4ce252]. Nature Communications (2015). Medium credibility.

Similar equations can be written for R 2 –R 5 (Fig. 7). Let us point out that radicals arising from C–H bond cleavage may also be secondary radicals formed after H · atom abstraction from ethyl radicals, for example. The formation of the decomposition products arises then from different possible reaction mechanisms from the excited state: (i) direct production of small molecules such as CO, CO 2 and H 2… from the excited state of DEC. The high CO 2 yield measured, for example, cannot be explained by consecutive R 1 and R 2 bond cleavages, which are not very probable, but by a direct production from the excited state. The same explanation accounts for the high CO and CO 2 yields measured in irradiated DMC (Supplementary Table 2). The difference in the relative proportions of gases formed from DMC and DEC implies that the reaction pathways from their respective excited state are different; (ii) radical recombination: the formation of diethylether can for instance arise from the recombination of radicals issued from R 1 and R 2. This recombination explains also the lengthening and branching of the alkyl chain of DEC (red circles and green triangles in Fig. 4); (iii) proton abstraction of radicals from the solvent (DEC) molecules, which are the most abundant ones. Therefore, this process is more probable than the previous one. For instance, the ethyl radical can abstract an hydrogen atom from the DEC molecule, leading to the formation of ethane and to the DEC(-H) · radical. A similar reaction mechanism will explain the formation of H 2 from H · radicals.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^3332cb72]. Nature Communications (2021). High credibility.

Lyapunov exponents and box dimensions

For calculating dominant Lyapunov exponents, we used the Wolf methodwith recommended parameters from the MATLAB script "Wolf Lyapunov exponent estimation from a time series" (version 1.2.0.1) provided by the authors on the Mathworks FileExchange. In particular, we used an embedding dimension of 3 and a phase space reconstruction delay of 10 for a dataset with ~350 orbits of ~50–60 data points per orbit. The last 50 iterations of the algorithm were used to generate a mean and standard deviation of the estimated dominant Lyapunov exponent. A full set of parameters can be found in the included code. For calculating attractor dimensions, we used the box-counting method, whose code is also provided in Supplementary Data 1, on the reconstructed phase space (X (T), X (T − 10), X (T − 20)) used for calculating Lyapunov exponents. A linear regression was performed using MATLAB's fit function on half the linear domain to generate a mean and confidence interval (using MATLAB's confint) for the slope (i.e. box dimension) between numbers of boxes covering the attractor versus length of each box.

Reporting summary

Further information on research design is available in the Nature Research Reporting Summary linked to this article.

---

### Anomalous frozen evanescent phonons [^8f1504c8]. Nature Communications (2024). High credibility.

Evanescent Bloch waves are eigensolutions of spatially periodic problems for complex-valued wavenumbers at finite frequencies, corresponding to solutions that oscillate in time and space and that exponentially decay in space. Such evanescent waves are ubiquitous in optics, plasmonics, elasticity, and acoustics. In the limit of zero frequency, the wave "freezes" in time. We introduce frozen evanescent waves as the eigensolutions of the Bloch periodic problem at zero eigenfrequency. Elastic waves, i.e., phonons, in metamaterials serve as an example. We show that, in the complex plane, the Cauchy-Riemann equations for analytical functions connect the minima of the phonon band structure to frozen evanescent phonons. Their exponential decay length becomes unusually large if a minimum in the band structure tends to zero and thereby approaches a soft mode. This connection between unusual static and dynamic behaviors allows to engineer large characteristic decay lengths in static elasticity. For finite-size samples, the static solutions for given boundary conditions are linear combinations of frozen evanescent phonons, leading to interference effects. Theory and experiment are in excellent agreement. Anomalous behavior includes the violation of Saint Venant's principle, which means that large decay-length frozen evanescent phonons can potentially be applied in terms of remote mechanical sensing.

---

### Optimal free descriptions of many-body theories [^f77fa88f]. Nature Communications (2017). Medium credibility.

Methods

Optimization

The optimization to find σ andin (3) is performed by a Monte Carlo basin-hopping strategyusing the Nelder–Mead simplex algorithm for local minimization within basins of the cost function. This global strategy was selected to counteract an observed tendency for local methods to get trapped in local minima. The initial guess for this search is found as follows. The normalization constant E 0 is the lowest entanglement energy of the input entanglement spectrum { E }. We iteratively construct an approximate set of single-particle entanglement energies starting from an empty set. First, we take the lowest remaining level in the spectrum and subtract E 0 to produce a new single-particle level k. Then we remove the many-body levels, which are closest to the new combinatorial levels generated according to (2) by the additional single-particle level. This process is repeated until the input spectrum is exhausted. We can also introduce a truncation of the entanglement spectrum cutting off high entanglement energies, making the construction of the initial guess terminate faster. The minimization of D (σ, σ f) to identify the optimal free model for the Ising Hamiltonian (5) is calculated using a local Nelder–Mead method.

Finite-size scaling

We perform the finite-size scaling according to an ansatz (4). The parameters of the collapse were estimated using the method of ref. From the scaling ansatz (4) and for a trial set of scaling parameters g c, ν and ζ, the scaled values x L = (g − g c) L 1/ ν andare calculated from each unscaled data point. From this collection of scaled data points (x L, y L) across all L, we implicitly define a so-called master curve that best represents them. This curve y (x) is defined around a point x as the linear regression calculated by taking the scaled data points immediately left and right of x for each system size L. We characterize the deviation of the scaled points (x L, y L) from the master curve y (x L) using the χ 2 statistic. This measure is used as the cost function for an optimization problem over the scaling parameters g c, ν, ζ and θ, which can be solved using the same techniques as the previous problems.

---

### Dynamical patterns and nonreciprocal effective interactions in an active-passive mixture through exact hydrodynamic analysis [^d1582aea]. Nature Communications (2025). High credibility.

Our analysis relies on state-of-the-art mathematical techniques together with a simple underlying model. In particular, we assume that particles move on an underlying two-dimensional lattice with stochastic dynamical rules and that self-propulsion only occurs in the left and right directions. These idealized features enable a rigorous hydrodynamic limit,: when the lattice spacing tends to zero and the number of particles tends to infinity, the particle densities obey deterministic continuum equations, which we derive exactly. The resulting system differs from NRCH: it has some similar features but also reveals interesting new behavior, which we outline next.

In our model, active particles alone result in stationary, motility-induced phase separation (commonly referred to as motility-induced phase separation or MIPS), but adding an extra population of passive (diffusing) particles can induce patterns, including traveling clusters, where self-organized groups of active particles push their passive counterparts around the system, see also ref. The hydrodynamic equation reveals an unusual type of phase diagram where the spinodal curve for the liquid-vapor transition protrudes through the binodal, signaling the onset of pattern formation. In this region, we demonstrate the existence of asymmetric traveling patterns and patterns formed of counter-propagating (CP) clusters. For large systems, these patterns feature sharp interfaces that separate dense liquid and dilute vapor regions. By studying the hydrodynamic equation in large domains, we relate traveling interfaces to those of static MIPS clusters, providing a new link between the equilibrium-like physics of phase separation and the dynamical patterns characteristic of nonreciprocity.

Our study provides an explicit and important example of how nonreciprocal effective interactions can emerge at the macroscale from simple interactions at the microscale. Specifically, while the direct interactions among particles are reciprocal, resulting solely from volume exclusion, particle simulations and analyses of the hydrodynamic equation clearly demonstrate that the model exhibits the principles of nonreciprocal self-organization. Furthermore, its idealized features allow us to draw sharp conclusions about large length and time scales, which would be extremely challenging to achieve from numerical simulations of more complex models. Our analysis of the large-system limit demonstrates how concepts from equilibrium phases and their interfaces can be applied to pattern-forming states. These features also facilitate a fully nonlinear treatment of the pattern-forming (traveling) steady states, revealing much more intricate behavior than could be predicted by linear stability analysis of the homogeneous state.

---

### 2022 ACC expert consensus decision pathway on the role of nonstatin therapies for LDL-cholesterol lowering in the management of atherosclerotic cardiovascular disease risk: a report of the American college of cardiology solution set oversight committee [^047845bc]. Journal of the American College of Cardiology (2022). High credibility.

ACC 2022 expert consensus — LDL-C measurement methods, alternatives, and monitoring: The writing committee endorses use of the Friedewald equation, given that the majority of RCTs used this method and that it is the most widely available means in clinical practice, while acknowledging that there can be significant discrepancies in levels of directly measured versus calculated LDL-C within the same sample, especially at lower LDL-C levels. Newer methods should be considered by health systems and laboratories whenever possible, and the use of the Martin-Hopkins method provides a more accurate assessment of LDL-C in individuals with very low levels of LDL-C or with hypertriglyceridemia; a new method proposed by investigators at the National Heart, Lung, and Blood Institute may also be more precise, but additional validation is needed. If therapy is adjusted, the writing committee emphasizes the importance of monitoring the LDL-C response within 4 to 12 weeks, and the algorithms include use of non-HDL-C thresholds as an additional alternative for decision-making.

---

### Analytical solution of the bloch-mcConnell equations for steady-state CEST Z-spectra [^733bcd83]. Magnetic Resonance Imaging (2024). Medium credibility.

Purpose

To derive an analytic expression for the steady-state Chemical Exchange Saturation Transfer (CEST) Z-spectra of a two-pool proton-exchanging system, facilitating simulations and expedited fitting of steady-state Z-spectra.

Method

The analytical expression is derived by directly solving the set of Bloch-McConnell differential equations in matrix form for a two-pool exchanging system, determining water magnetization under steady-state saturation across the entire Z-spectrum. The analytic solution is compared and validated against the numerical solution of Bloch-McConnell equations under prolonged saturation. The study also explores the line shape of a CEST peak, interpolating under-sampled Z-spectra, and Z-spectral fitting in the presence of noise.

Results

The derived analytic solution accurately reproduces spectra obtained through numerical solutions. Direct fitting of simulated CEST spectra with the analytical solution yields the physical parameters of the exchanging system. The study shows that the analytical solution enables the reproduction of fully sampled spectra from sparsely sampled Z-spectra. Additionally, it confirms the approximation of the CEST spectrum of a single exchanging proton species with a Lorentzian function. Monte Carlo simulations reveal that the accuracy and precision of Z-spectral fittings for physical parameters are significantly influenced by data noise. The study also derives and discusses the analytical solution for three-pool Z-spectra.

Conclusion

The derived analytic solution for steady state Z-spectra can be utilized for simulations and Z-spectrum fitting, significantly reducing fitting times compared to numerical methods employed for fitting CEST Z-spectra.

---

### Bypassing the kohn-sham equations with machine learning [^2943b933]. Nature Communications (2017). Medium credibility.

Last year, at least 30,000 scientific papers used the Kohn-Sham scheme of density functional theory to solve electronic structure problems in a wide variety of scientific fields. Machine learning holds the promise of learning the energy functional via examples, bypassing the need to solve the Kohn-Sham equations. This should yield substantial savings in computer time, allowing larger systems and/or longer time-scales to be tackled, but attempts to machine-learn this functional have been limited by the need to find its derivative. The present work overcomes this difficulty by directly learning the density-potential and energy-density maps for test systems and various molecules. We perform the first molecular dynamics simulation with a machine-learned density functional on malonaldehyde and are able to capture the intramolecular proton transfer process. Learning density models now allows the construction of accurate density functionals for realistic molecular systems. Machine learning allows electronic structure calculations to access larger system sizes and, in dynamical simulations, longer time scales. Here, the authors perform such a simulation using a machine-learned density functional that avoids direct solution of the Kohn-Sham equations.

---

### Estimation in medical imaging without a gold standard [^82dda7fe]. Academic Radiology (2002). Low credibility.

Rationale and Objectives

In medical imaging, physicians often estimate a parameter of interest (eg, cardiac ejection fraction) for a patient to assist in establishing a diagnosis. Many different estimation methods may exist, but rarely can one be considered a gold standard. Therefore, evaluation and comparison of different estimation methods are difficult. The purpose of this study was to examine a method of evaluating different estimation methods without use of a gold standard.

Materials and Methods

This method is equivalent to fitting regression lines without the x axis. To use this method, multiple estimates of the clinical parameter of interest for each patient of a given population were needed. The authors assumed the statistical distribution for the true values of the clinical parameter of interest was a member of a given family of parameterized distributions. Furthermore, they assumed a statistical model relating the clinical parameter to the estimates of its value. Using these assumptions and observed data, they estimated the model parameters and the parameters characterizing the distribution of the clinical parameter.

Results

The authors applied the method to simulated cardiac ejection fraction data with varying numbers of patients, numbers of modalities, and levels of noise. They also tested the method on both linear and nonlinear models and characterized the performance of this method compared to that of conventional regression analysis by using x-axis information. Results indicate that the method follows trends similar to that of conventional regression analysis as patients and noise vary, although conventional regression analysis outperforms the method presented because it uses the gold standard which the authors assume is unavailable.

Conclusion

The method accurately estimates model parameters. These estimates can be used to rank the systems for a given estimation task.

---

### Ion-induced nucleation of pure biogenic particles [^d4badc27]. Nature (2016). Excellent credibility.

The HOM concentration in equation (4) is determined from its production and loss rates:where MT represents total monoterpenes. The IUPACreaction rate constants (in cm 3 per molecule per second) for oxidation of α-pinene by ozone and hydroxyl radicals are, respectively:where T (in K) is the temperature (the α-pinene+O 3 rate constant is updated on the IUPAC website at). The HOM yields in each ozone–monoterpene and hydroxyl–monoterpene reaction areand, respectively. The parameter k HOM is the HOM loss rate or, equivalently, the atmospheric condensation sink, CS (in s −1). The condensation sink is determined assuming the diffusion characteristics of a typical α-pinene oxidation product (see appendix A1 of ref.). Assuming steady-state in equation (5), the HOM concentration becomes:where the HOM yield from ozonolysis is, and from reaction with the hydroxyl radical is(Extended Data Fig. 2). The HOM yield from ozonolysis is determined from CLOUD measurements in the presence of a hydroxyl scavenger (0.1% H 2). The HOM yield from reaction with hydroxyl radicals is determined from CLOUD measurements in the absence of ozone, and where photolysed HONO provides the OH· source. Therefore, the experimental measurement of hydroxyl-initiated oxidation is made in the presence of NO x, as occurs in the atmosphere.

The small-ion concentration in equation (4) is calculated from the steady-state solution of the ion balance equation:where q (in cm −3 s −1) is the ion-pair production rate and α is the ion–ion recombination coefficient (in cm 3 s −1). The factor of 2 in equation (4) accounts for nucleation from positive and negative ions. For the CLOUD GCR data, q = 1.7 cm −3 s −1. Terrestrial radioactivity such as radon contributes additional ionization in the boundary layer over land masses. The ion loss rate, k i, is due to the condensation sink, CS, and ion-induced nucleation:where J iin /(2[n ±]) is given by equation (4) and the steady-state concentration of small ions is, from equation (8):From equations (8) and (9), J iin saturates at 2 q at high nucleation rates (see Fig. 3).

---

### The eighty five percent rule for optimal learning [^74499c58]. Nature Communications (2019). High credibility.

If the decision boundary is set to 0, such that the model chooses option A when h > 0, option B when h < 0 and randomly when h = 0, then the noise in the representation of the decision variable leads to errors with probabilitywhere F (x) is the cumulative density function of the standardized noise distribution, p (x) = p (x |0, 1), and β = 1/ σ quantifies the precision of the representation of Δ and the agent's skill at the task. As shown in Fig. 1b, this error rate decreases as the decision gets easier (Δ increases) and as the agent becomes more accomplished at the task (β increases).

The goal of learning is to tune the parameters ϕ such that the subjective decision variable, h, is a better reflection of the true decision variable, Δ. That is, the model should aim to adjust the parameters ϕ so as to decrease the magnitude of the noise σ or, equivalently, increase the precision β. One way to achieve this tuning is to adjust the parameters using gradient descent on the error rate, i.e. changing the parameters over time t according towhere η is the learning rate and ∇ ϕ ER is the derivative of the error rate with respect to parameters ϕ. This gradient can be written in terms of the precision, β, asNote here that only the first term on the right hand side of Eq. (5) depends on the difficulty Δ, while the second describes how the precision changes with ϕ. Note also that Δ itself, as the 'true' decision variable, is independent of ϕ. This means that the optimal difficulty for training, that maximizes the change in the parameters, ϕ, at this time point, is the value of the decision variable Δ * that maximizes ∂ ER/ ∂β. Of course, this analysis ignores the effect of changing ϕ on the form of the noise — instead assuming that it only changes the scale factor, β, an assumption that likely holds in the relatively simple cases we consider here, although whether it holds in more complex cases will be an important question for future work.

---

### Loss-proof self-accelerating beams and their use in non-paraxial manipulation of particles' trajectories [^21152f71]. Nature Communications (2014). Medium credibility.

Self-accelerating beams — shape-preserving bending beams — are attracting great interest, offering applications in many areas such as particle micromanipulation, microscopy, induction of plasma channels, surface plasmons, laser machining, nonlinear frequency conversion and electron beams. Most of these applications involve light-matter interactions, hence their propagation range is limited by absorption. We propose loss-proof accelerating beams that overcome linear and nonlinear losses. These beams, as analytic solutions of Maxwell's equations with losses, propagate in absorbing media while maintaining their peak intensity. While the power such beams carry decays during propagation, the peak intensity and the structure of their main lobe region are maintained over large distances. We use these beams for manipulation of particles in fluids, steering the particles to steeper angles than ever demonstrated. Such beams offer many additional applications, such as loss-proof self-bending plasmons. In transparent media these beams show exponential intensity growth, which facilitates other novel applications in micromanipulation and ignition of nonlinear processes.

---

### Maximum entropy technique and regularization functional for determining the pharmacokinetic parameters in DCE-MRI [^b8787de5]. Journal of Digital Imaging (2022). Medium credibility.

This paper aims to solve the arterial input function (AIF) determination in dynamic contrast-enhanced MRI (DCE-MRI), an important linear ill-posed inverse problem, using the maximum entropy technique (MET) and regularization functionals. In addition, estimating the pharmacokinetic parameters from a DCE-MR image investigations is an urgent need to obtain the precise information about the AIF-the concentration of the contrast agent on the left ventricular blood pool measured over time. For this reason, the main idea is to show how to find a unique solution of linear system of equations generally in the form of [Formula: see text] named an ill-conditioned linear system of equations after discretization of the integral equations, which appear in different tomographic image restoration and reconstruction issues. Here, a new algorithm is described to estimate an appropriate probability distribution function for AIF according to the MET and regularization functionals for the contrast agent concentration when applying Bayesian estimation approach to estimate two different pharmacokinetic parameters. Moreover, by using the proposed approach when analyzing simulated and real datasets of the breast tumors according to pharmacokinetic factors, it indicates that using Bayesian inference-that infer the uncertainties of the computed solutions, and specific knowledge of the noise and errors-combined with the regularization functional of the maximum entropy problem, improved the convergence behavior and led to more consistent morphological and functional statistics and results. Finally, in comparison to the proposed exponential distribution based on MET and Newton's method, or Weibull distribution via the MET and teaching-learning-based optimization (MET/TLBO) in the previous studies, the family of Gamma and Erlang distributions estimated by the new algorithm are more appropriate and robust AIFs.

---

### Diet optimization: modeling iron and zinc absorption by nonlinear programming and piecewise linear approximation using national health and nutrition examination survey [^0955e08f]. The American Journal of Clinical Nutrition (2025). Medium credibility.

Background

The accuracy of the calculation of absorbable nonheme iron and zinc content in diet-model-generated menu plans can be improved by using nonlinear absorption equations. The resulting diet models cannot be solved with standard linear programming software.

Objective

The aim of this study is to evaluate the effectiveness of nonlinear programming (NLP) and piecewise linear approximation (PLA) for solving diet models with nonlinear equations for nonheme iron and zinc absorption.

Methods

A mixed-integer and a continuous diet model were developed to optimize absorbable iron and zinc intake, using different absorption equations available from literature. Model input data were obtained from NHANES. Various diet plans were then generated applying both NLP and PLA techniques. Evaluation criteria included solution quality and computational efficiency.

Results

For the mixed-integer diet model, PLA found accurate solutions within minutes, outperforming NLP in consistency and solution quality. NLP frequently hit the 1-hour time limit and did not always find the best observed solution. In the worst cases, NLP either found no solution or the deviation was as large as 2.1 mg for absorbable iron. For absorbable zinc, the maximum deviation was only 0.2 mg. For the continuous diet model, NLP and PLA performed equally well in most cases.

Conclusion

This study provides practical examples for researchers who seek to improve the accuracy of their diet models through the implementation of nonheme iron and zinc absorption equations using either NLP or PLA.

---

### Non-additivity of molecule-surface van der waals potentials from force measurements [^0dec3a03]. Nature Communications (2014). Medium credibility.

Weighted least-squares regression

We use a weighted least-squares regression, minimizing the quantity

when fitting the N data points within the fit interval j = 1,… N. Choosing the right weights w j is a non-trivial task.

In the case of a linear fit, the goodness of fit (gof) is best described by the reduced χ 2, defined as

where x i is the i th measured point andthe corresponding value of the fitted curve. Here the respective weighting function is the inverse of the variance σ 2 that is a measure of the statistical noise in the measured curve. While with this definition one obtains χ 2 = 1 for a perfect fit, there is no general rule of how close a reduced χ 2 should be to 1 for a good fit.

For a linear fit, the reduced χ 2 as defined above is a suitable gof criterion, because according to this definition, each data point of a linear data set has the same chance of contributing to the overall χ 2 value. However, in our case, we do not perform a linear fit. Rather, we fit a force law that is proportional to z −5 and moreover our experimental noise is practically constant over the whole fit interval. To gauge the quality of our fit to experimental data following this force law, we must ensure that the entire measured curve contributes to our gof criterion. Otherwise, experimental information would be lost, because any part of the measured data curve that does not contribute significantly to the gof criterion has no influence on the outcome of the fit. For this reason, we use the gof criterion in equation (8) with the weighting function

---

### Recovery after stroke: not so proportional after all? [^d3259e7a]. Brain (2019). Medium credibility.

Spurious r(X,Δ) are likely when σ Y /σ X is small

For any X and Y, it can be shown that:

A formal proof of Equation 1 is provided in the Supplementary material, Appendix A [proposition 4 and theorem 1; also see]; its consequence is that r(X,Δ) is a function of r(X, Y) and σ Y /σ X. To illustrate that function, we performed a series of simulations (Supplementary material, Appendix B) in which r(X, Y) and σ Y /σ X were varied independently. Figure 2 illustrates the results: a surface relating r(X,Δ) to r(X, Y) and σ Y /σ X. Figure 3 shows example recovery data at six points of interest on that surface.

Figure 2
The relationship between r(X, Y), r(X,Δ) and σ Y /σ X. Note that the x -axis is log-transformed to ensure symmetry around 1; when X and Y are equally variable, log(σ Y /σ X) = 0. Supplementary material, proposition 7 in Appendix A, provides a justification for unambiguously using a ratio of standard deviations in this figure, rather than σ Y and σ X as separate axes. The two major regimes of Equation 1 are also marked in red. In Regime 1, Y is more variable than X, so contributes more variance to Δ, and r(X,Δ) ≈ r(X, Y). In Regime 2, X is more variable than Y, so X contributes more variance to Δ, and r(X,Δ) ≈ r(X,−X) (i.e. −1). The transition between the two regimes, when the variability ratio is not dramatically skewed either way, also allows for spurious r(X,Δ). For the purposes of illustration, the figure also highlights six points of interest on the surface, marked A–F; examples of simulated recovery data corresponding to these points are provided in Fig. 3.

---

### Exact, time-dependent analytical equations for spiral trajectories and matching gradient and density-correction waveforms [^7b336103]. Magnetic Resonance in Medicine (2026). Medium credibility.

Purpose

To analytically define a spiral waveform and trajectory that match the constraints of gradient frequency, slew rate, and amplitude.

Theory and Methods

Piecewise analytical solutions for gradient waveforms under the desired constraints are derived using the circle of an involute rather than an Archimedean spiral. Also given are the analytical equations for the time-dependent k-space trajectory and sampling density compensation weights, and analytical expressions for the time dependence of data acquisition in k-space. Open-source software implementing all these equations is shared. Performance is measured against numerically derived solutions to an Archimedean spiral. Scanner implementation is illustrated.

Results

The performance of the proposed equations is very similar to that of numerically derived solutions, but this method is much easier to implement and analyze.

Conclusion

The proposed method, WHIRLED PEAS (Winding Hybrid Interleaved Radial Lines Encoding Described by Piecewise Exact Analytical Solution), is an easy-to-implement solution for spiral MRI that performs comparable to optimal numerical designs.

---

### Spin dephasing under nonlinear gradients: implications for imaging and field mapping [^eaf2803c]. Magnetic Resonance in Medicine (2012). Low credibility.

This work examines the prototypical MR echo that would be expected for a voxel of spins evolving in a strong nonlinear field, specifically focusing on the quadratic z(2) - ½(x(2) + y(2)) field. Dephasing under nonlinear gradients is increasingly relevant given the growing interest in nonlinear imaging, and here, we report several notable differences from the linear case. Most notably, in addition to signal loss, intravoxel dephasing under gradients creating a wide and asymmetric frequency distribution across the voxel can cause skewed and nonlinear phase evolution. After presenting the qualitative and analytical origins of this difference, we experimentally demonstrate that neglecting these dynamics can lead to significant errors in sequences that assume phase evolution is proportional to voxel frequency, such as those used for field mapping. Finally, simplifying approximations to the signal equations are presented, which not only provide more intuitive forms of the exact expression but also result in simple rules to predict key features of the nonlinear evolution.

---

### Numerical solutions to the time-dependent bloch equations revisited [^9253948c]. Magnetic Resonance Imaging (2011). Low credibility.

The purpose of this study was to demonstrate a simple and fast method for solving the time-dependent Bloch equations. First, the time-dependent Bloch equations were reduced to a homogeneous linear differential equation, and then a simple equation was derived to solve it using a matrix operation. The validity of this method was investigated by comparing with the analytical solutions in the case of constant radiofrequency irradiation. There was a good agreement between them, indicating the validity of this method. As a further example, this method was applied to the time-dependent Bloch equations in the two-pool exchange model for chemical exchange saturation transfer (CEST) or amide proton transfer (APT) magnetic resonance imaging (MRI), and the Z-spectra and asymmetry spectra were calculated from their solutions. They were also calculated using the fourth/fifth-order Runge-Kutta-Fehlberg (RKF) method for comparison. There was also a good agreement between them, and this method was much faster than the RKF method. In conclusion, this method will be useful for analyzing the complex CEST or APT contrast mechanism and/or investigating the optimal conditions for CEST or APT MRI.

---

### Nonequilibrium thermodynamics of the asymmetric sherrington-kirkpatrick model [^159b8625]. Nature Communications (2023). High credibility.

Taken together, our results indicate that the behavior of entropy production peaking at a critical point is more general than the simple mean-field, homogeneous models, therefore a non-smooth change of the steady-state entropy production (or entropy dissipated to an external reservoir) can be a useful indicator of a number of nonequilibrium phase transitions. At the same time, our results demonstrate that an increase in entropy production does not necessarily mean that the system is approaching a phase transition point. Instead, combining the order parameters, entropy rate, and entropy production yields a more precise picture of the complex systems and their phase transitions.

Typically, solutions of the symmetric (equilibrium) SK model involve the replica trick to calculate the configurational average of the logarithm of the partition function. This method introduces an integer number of replicas of a system for averaging the disorder and then recovers the solution using a continuous number of replicas in the zero limit under the replica symmetry or replica-symmetry breaking ansatz. This treatment forces researchers to check the validity of solutions before reaching correct solutions. As an alternative to the replica methods, the path integral methods have been widely used in analyzing the symmetric SK model. However, for partially- or fully-symmetric SK models, the path integral method does not give a definite analytical solution but needs to be computed with Monte Carlo approaches. Fortunately, the path integral method derives an exact analytical solution for the case of the fully asymmetric nonequilibrium SK model, which we extended to cover synchronous and asynchronous updates, and theoretically underpinned their nonequilibrium properties by deriving the exact solution of the steady-state entropy production and entropy rates of the system.

---

### Modeling the effect of hyperoxia on the spin-lattice relaxation rate R1 of tissues [^7c4335b6]. Magnetic Resonance in Medicine (2022). Medium credibility.

Estimating Oxygen Consumption Ratefromand R t

Using the Krogh‐Erlang solution (Equation 9) also requires knowledge of the maximum tissue oxygen consumption rate, M 0. Once R t is found, we can derive M 0 from the OEF using the following logic and assumptions. It is important to note that in order for the assumptions of the following equations to be valid, M 0 must be calculated in the normoxic state (i.e. the patient is breathing air). Under this assumption, the SO 2 along the capillary can be calculated using the linear equation:Using Equation (6), which relates mean SO 2 to the OEF, we can create a linear equation of SO 2 along z, the mean SO 2 in the capillary (SO 2 cap, mean) will be equal to the linear Equation (14) at 0.5 L. Therefore, M 0 can be related to OEF using the following steps:which can be rearranged to calculate M 0 :Finally, with M 0 and R t estimated from the OEF and blood volume, the tissue PO 2 (into‐tissue axis) can be calculated. Since the PO 2 along the capillary (P cap) on air and oxygen breathing is known from Step 1, the PO 2 throughout the Krogh cylinder can be calculated for both normoxic and hyperoxic situations (shown in Supporting Information Figure S4).

2.4 Step 3. Calculatingin the three compartments

---

### Dynamical patterns and nonreciprocal effective interactions in an active-passive mixture through exact hydrodynamic analysis [^6dd1c504]. Nature Communications (2025). High credibility.

The formation of dynamical patterns is one of the most striking features of nonequilibrium physical systems. Recent work has shown that such patterns arise generically from forces that violate Newton's third law, known as nonreciprocal interactions. These nonequilibrium phenomena are challenging for modern theories. Here, we introduce a model mixture of active (self-propelled) and passive (diffusive) particles amenable to exact mathematical analysis. We exploit state-of-the-art methods to derive exact hydrodynamic equations for the particle densities, which reveal effective nonreciprocal couplings between the active and passive species. We study the resulting collective behavior, including the linear stability of homogeneous states and phase coexistence in large systems. This reveals a novel phase diagram with the spinodal associated with active phase separation protruding through the associated binodal, heralding the emergence of dynamical steady states. We analyze these states in the thermodynamic limit of large system size, showing, for example, that sharp interfaces may travel at finite velocities, but traveling phase-separated states are forbidden. The model's mathematical tractability enables precise new conclusions beyond those available by numerical simulation of particle models or field theories.

---

### Wave kinetics of random fibre lasers [^e864f6e9]. Nature Communications (2015). Medium credibility.

Results

Wave kinetic equation for fibre lasers

Kinetic theory describes an average (macroscopic) probabilistic evolution of complex system. Starting from basic microscopic dynamic equations governing the interaction of elementary constituents (for example, particles or waves), the kinetic description effectively reduces a large number of degrees of freedom in the original non-linear system by implying some assumptions about statistics of fluctuations. In the context of a wave system, the kinetic equation describes an evolution of quantities that are averages over times exceeding wave periods. In this case, waves with different frequencies acquire different phases enabling treating them as approximately independent at averaging. Nowadays, the wave kinetic approach (known in some contexts as wave turbulence) is used in a range of physical applications varying from Bose–Einstein condensate to astrophysics.

In traditional wave kinetics, initial (arbitrary) wave spectrum F ω evolves in a gradual way through cascades of numerous weak non-linear interactions towards a statistical steady state, as illustrated by Fig. 1a. The evolution from initial to asymptotically stationary spectrum is governed by the wave kinetic equation. The long-time asymptotic state is steady both globally and locally meaning that statistical properties do not change at any arbitrary time shift. This is achieved when the external pumping of energy into the system and dumping of energy out of the system are time independent.

In fibre laser, radiation undergoes strong periodic (cyclic) changes along the cavity due to combined action of gain, loss, dispersion and non-linearity, superimposed on a slow evolution from one round trip to another. The global statistical equilibrium still may exist in terms of the Poincare mappingas a state that is statistically reproduced after each round trip. However, contrary to the case of systems homogenous over evolution variable, a local statistically steady state does not exist, meaning that the wave spectrum is changed substantially at any arbitrary time shift. As a result, two scales over evolution coordinate do exist in laser systems — a fast evolution within each round trip, and slow evolution from cycle to cycle. This results in non-uniform spiral-like evolution that differs from a monotonic relaxation in classical wave kinetics, as schematically depicted in Fig. 1b. It is important that the amplitudes of waves could be sufficiently increased or decreased during the fast evolution within each round trip depending on either the pumping or dumping being dominant at particular moment in a laser cavity. So the classical wave kinetic approach cannot be directly applied to the typical fibre lasers as the strong local (quasi periodic) dynamics of the wave spectrum must be taken into account.

---

### Disordered metasurface enabled single-shot full-stokes polarization imaging leveraging weak dichroism [^93cd31c6]. Nature Communications (2023). High credibility.

To compare the performance of our metasurface array with other traditional ordered full-Stokes micro-polarizer array designs exhibiting strong dichroism, we conduct a theoretical analysis of these imaging systems. According to the compressed sensing theory –, the imaging process can be rewritten in the standard form of an underdetermined system of linear equations: whereis the representation of input signalin basis. If the input signalis k -sparse on the basis, i.e. there is only k non-zero elements in vector, can be exactly recovered usingminimizationwhen the number of measurements m satisfies: Here, whererepresents the i -th row of, represents the j -th column of. c is a known positive constant. With the sparsity prior, the solution space of the equations becomes much smaller than that in the raw spatial domain and only depends on a few number of unknown parameters in the sparse transform domain. This premise radically changes the ill-posed problem, making the search for solutions feasible. Based upon the theory, the smaller, the fewer sampling points are required, allowing the signal with sparsity prior to be reconstructed at a low sampling rate. Therefore, we define the sampling efficiency as the coherenceto compare the performance of DoFP systems.

Specifically, we compare the sampling efficiency of our disordered metasurface array with three types of conventional ordered polarization filter arrangements (Fig. 3), namely the octahedral arrangement, tetrahedral arrangement, and the arrangement described in. The sparse representationis derived from the recently released full-Stokes polarization datasetutilizing the principal component analysis (PCA) method. As shown in Fig. 3d, the sampling efficiency of our disordered metasurface array is comparable to that of conventional ordered sampling schemes. This indicates the potential of the proposed disordered sampling scheme for achieving high-performance polarization measurements or polarization imaging. Compared with ordered arrangements, the disorder property of the proposed random sampling strategy could be preserved after the sparse transformation, and thus leading to a larger number of unique sampling points in the sparse transform domain, boosting the solution of the equation with higher numerical stability. Additional details about the sampling efficiency analysis are provided in the Supplemental Material S2.

---

### A model for optimizing normal tissue complication probability in the spinal cord using a generalized incomplete repair scheme [^8683b9ed]. Radiation Research (2001). Low credibility.

The purpose of this study was to determine the treatment protocol, in terms of dose fractions and interfraction intervals, which minimizes normal tissue complication probability in the spinal cord for a given total treatment dose and treatment time. We generalize the concept of incomplete repair in the linear-quadratic model, allowing for arbitrary dose fractions and interfraction intervals. This is incorporated into a previously presented model of normal tissue complication probability for the spinal cord. Equations are derived for both mono-exponential and bi-exponential repair schemes, regarding each dose fraction and interfraction interval as an independent parameter, subject to the constraints of fixed total treatment dose and treatment time. When the interfraction intervals are fixed and equal, an exact analytical solution is found. The general problem is nonlinear and is solved numerically using simulated annealing. For constant interfraction intervals and varying dose fractions, we find that optimal normal tissue complication probability is obtained by two large and equal doses at the start and conclusion of the treatment, with the rest of the doses equal to one another and smaller than the two dose spikes. A similar result is obtained for bi-exponential repair. For the general case where the interfraction intervals are discrete and also vary, the pattern of two large dose spikes is maintained, while the interfraction intervals oscillate between the smallest two values. As the minimum interfraction interval is reduced, the normal tissue complication probability decreases, indicating that the global minimum is achieved in the continuum limit, where the dose delivered by the "middle" fractions is given continuously at a low dose rate. Furthermore, for bi-exponential repair, it is seen that as the slow component of repair becomes increasingly dominant as the magnitude of the dose spikes decreases. Continuous low-dose-rate irradiation with dose spikes at the start and end of treatment yields the lowest normal tissue complication probability in the spinal cord, given a fixed total dose and total treatment time, for both mono-exponential and bi-exponential repair. The magnitudes of the dose spikes can be calculated analytically, and are in close agreement with the numerical results.

---

### The eighty five percent rule for optimal learning [^70a73b15]. Nature Communications (2019). High credibility.

The predicted reward E [r] was computed according to Eq. (20). In line with Law and Gold (Supplementary Fig. 2 in ref.), the proportionality constant B was computed using logistic regression on the accuracy and absolute value of the decision variable, | h |, from last L trials, where L = min(300, t).

In addition to the weight update rule (Eq. (21)), weights were normalized after each update to keep the sum of the squared weights, a constant (= 0.02). While this normalization has only a small overall effect (see Supplementary Material in ref.), we replicate this weight normalization here for consistency with the original model.

To initialize the network, the first 50 trials of the simulation had a fixed coherence COH = 0.9. After this initialization period, the coherence was adjusted according to the difference between the target accuracy, A target, and actual accuracy in the last L trials, A L, where L = min(300, t). Specifically, the coherence on trial t was set aswhere Γ t was adjusted according toand dΓ was 0.1.

To estimate the post-training precision parameter, β, we simulated behavior of the trained network on a set of 20 logarithmically spaced coherences between 10 −3 and 1. Behavior at each coherence was simulated 100 times and learning was disabled during this testing phase. The precision parameter, β, was estimated using logistic regression between accuracy on each trial (0 or 1) and coherence; i.e.

---

### How many days was that? We' re still not sure, but we' re asking the question better! [^b41c7375]. Medicine and Science in Sports and Exercise (2008). Low credibility.

Unreliable measures limit the ability to detect relationships with other variables. Day-to-day variability in measurement is a source of unreliability. Studies vary substantially in numbers of days needed to reliably assess physical activity. The required numbers of days has probably been underestimated due to violations of the assumption of compound symmetry in using the intraclass correlation. Collecting many days of data become unfeasible in real-world situations. The current dilemma could be solved by adopting distribution correction techniques from nutrition or gaining more information on the measurement model with generalizability studies. This would partition the variance into sources of error that could be minimized. More precise estimates of numbers of days to reliably assess physical activity will likely vary by purpose of the study, type of instrument, and characteristics of the sample. This work remains to be done.

---

### Computational methods for the estimation of ideal current patterns in realistic human models [^2f5b4b1d]. Magnetic Resonance in Medicine (2024). Medium credibility.

Purpose

To introduce a method for the estimation of the ideal current patterns (ICP) that yield optimal signal-to-noise ratio (SNR) for realistic heterogeneous tissue models in MRI.

Theory and Methods

The ICP were calculated for different surfaces that resembled typical radiofrequency (RF) coil formers. We constructed numerical electromagnetic (EM) bases to accurately represent EM fields generated by RF current sources located on the current-bearing surfaces. Using these fields as excitations, we solved the volume integral equation and computed the EM fields in the sample. The fields were appropriately weighted to calculate the optimal SNR and the corresponding ICP. We demonstrated how to qualitatively use ICP to guide the design of a coil array to maximize SNR inside a head model.

Results

In agreement with previous analytic work, ICP formed large distributed loops for voxels in the middle of the sample and alternated between a single loop and a figure-eight shape for a voxel 3-cm deep in the sample's cortex. For the latter voxel, a surface quadrature loop array inspired by the shape of the ICP reached < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:mn > 87 < /mml:mn > < mml:mo >. < /mml:mo > < mml:mn > 5 < /mml:mn > < mml:mo > % < /mml:mo > < /mml:mrow > < mml:annotation > $$ 87.5\% $$ < /mml:annotation > < /mml:semantics > < /mml:math > of the optimal SNR at 3T, whereas a single loop placed above the voxel reached only < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:mn > 55 < /mml:mn > < mml:mo >. < /mml:mo > < mml:mn > 7 < /mml:mn > < mml:mo > % < /mml:mo > < /mml:mrow > < mml:annotation > $$ 55.7\% $$ < /mml:annotation > < /mml:semantics > < /mml:math > of the optimal SNR. At 7T, the performance of the two designs decreased to < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:mn > 79 < /mml:mn > < mml:mo >. < /mml:mo > < mml:mn > 7 < /mml:mn > < mml:mo > % < /mml:mo > < /mml:mrow > < mml:annotation > $$ 79.7\% $$ < /mml:annotation > < /mml:semantics > < /mml:math > and < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:mn > 49 < /mml:mn > < mml:mo >. < /mml:mo > < mml:mn > 8 < /mml:mn > < mml:mo > % < /mml:mo > < /mml:mrow > < mml:annotation > $$ 49.8\% $$ < /mml:annotation > < /mml:semantics > < /mml:math >, respectively, suggesting that loops could be suboptimal at ultra-high field MRI.

Conclusion

ICP can be calculated for human tissue models, potentially guiding the design of application-specific RF coil arrays.

---

### A simplified empirical model to estimate oxygen relaxivity at different magnetic fields [^3d321fe0]. NMR in Biomedicine (2022). Medium credibility.

2 METHODS

2.1 Model theory

T 1 (measured in ms), and its inverse, R 1 (typically reported in s −1), have both been used in the literature when reporting the relaxivity effect of oxygen. R 1 is linearly dependent on the concentration of paramagnetic particles, in this case dissolved molecular oxygen in the solution, with the following equation:where R 1Ox is the relaxation rate in the solution with oxygen added, R 1,0 is the relaxation rate in the solution without oxygen, C is the concentration or partial pressure of oxygen, and r 1Ox is the relaxivity of oxygen in that solution (whose units depend on the oxygen measurement used in the constant C) (shown in Figure 1). Since the partial pressure of oxygen (pO 2) is a common measurement in biomedicine and clinical applications, in this manuscript, we report C as pO 2 in mmHg and r 1Ox in s −1 /mmHg. Conversion factors to other common units such as kPa, Torr, mmol/L, mg/L, and mL/L can be found in Supplementary Table S1.

FIGURE 1
The relationship between T 1 and pO 2 (A) and R 1 and pO 2 (B) in a solution, with the initial T 1 of 3000 ms. The values are calculated using a range of r 1Ox (relaxivity) reported in the literature at 1.5 T, in units s −1 /mmHg oxygen

Changes in both T 1 and R 1 have been used to report changes in pO 2 in the past. However, although an increase in oxygen could still qualitatively roughly be inferred from a shortening of T 1, it is important to note that the linear relationship exists with 1/ T 1 (R 1) — not T 1 — and therefore the change in T 1 caused by oxygen will be dependent on the original T 1 (shown in Figure 2). Therefore, for a quantitative inference of pO 2 change it is necessary to discuss changes with respect to R 1.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^345386c9]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Selecting and reporting reference values — procedural guidance states that PFT laboratories must select appropriate reference values generated from high-quality data from a large sample of healthy asymptomatic individuals who have never smoked or had other respiratory illness or significant exposures, reports should identify the source of reference values, any change in selected equations should be noted and prior percent predicted values recalculated if possible, pediatric–adult equation discontinuity is discouraged, and extrapolation beyond the age range should not be done during growth, increases uncertainty in the elderly, and must be noted in technician comments.

---

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^4165d96c]. American Journal of Kidney Diseases (2006). Medium credibility.

Appendix — methods for adding residual clearance to hemodialyzer clearance notes that dialyzer clearances (spKt/V) required to achieve a stdKt/V of 2.0 volumes per week are tabulated across treatment times from 2 to 8 hours and schedules from 2 to 7 treatments per week, with values determined using a formal 2-compartment mathematical model of urea kinetics and similar results obtainable using a simplified equation; the approach gives results similar to the third data column of Table 18.

---

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^334d7bb3]. American Journal of Kidney Diseases (2006). Medium credibility.

Equilibrated Kt/V (eKt/V) — adjustment for postdialysis rebound and benchmarking — states that allowance for solute disequilibrium can be made by adjusting spKt/V for the rebound in urea concentration at the end of dialysis, yielding eKt/V with a time-dependent factor; the Daugirdas "rate equation" was fit to rebound blood urea nitrogen measured 30 or 60 minutes after dialysis. The guideline notes that many would like to convert the dose benchmark from spKt/V to eKt/V for hemodialysis, that for peritoneal dialysis eKt/V and spKt/V are identical, and that after debate the KDOQI HD Work Group unanimously decided to disallow shortened dialysis for treatments 3 times per week, while clarifying that use of eKt/V as a benchmark does not prohibit ultrashort dialysis provided the clearance can be increased.

---

### Wave kinetics of random fibre lasers [^619367b7]. Nature Communications (2015). Medium credibility.

Traditional wave kinetics describes the slow evolution of systems with many degrees of freedom to equilibrium via numerous weak non-linear interactions and fails for very important class of dissipative (active) optical systems with cyclic gain and losses, such as lasers with non-linear intracavity dynamics. Here we introduce a conceptually new class of cyclic wave systems, characterized by non-uniform double-scale dynamics with strong periodic changes of the energy spectrum and slow evolution from cycle to cycle to a statistically steady state. Taking a practically important example — random fibre laser — we show that a model describing such a system is close to integrable non-linear Schrödinger equation and needs a new formalism of wave kinetics, developed here. We derive a non-linear kinetic theory of the laser spectrum, generalizing the seminal linear model of Schawlow and Townes. Experimental results agree with our theory. The work has implications for describing kinetics of cyclical systems beyond photonics.

---

### Practical guide for interpreting and reporting cardiac PET measurements of myocardial blood flow: an information statement from the American Society of Nuclear Cardiology, and the Society of Nuclear Medicine and Molecular Imaging [^461b4a68]. Journal of Nuclear Cardiology (2021). High credibility.

Know your software (retention and compartment models) — compartment and retention model approaches for myocardial blood flow (MBF) are described as follows: The compartment model approach for calculating MBF requires a series of dynamic images that record tracer kinetics into and washout from the myocardium and uses a non-linear fit of the model to the acquired data to solve for the regional MBF, tracer washout, and partial volume artifacts. The retention model approach assumes tracer retention is determined by the blood pool concentration and irreversible extraction into the myocardium; the amount of radiotracer retained in the myocardium is corrected for or normalized to the amount of radiotracer delivered via arterial blood ("the arterial input function") and thus the integral of the arterial radiotracer concentration (assumed to be derived from the initial two-minute images), is corrected for the flow-dependent or K1-specific extraction fraction, and by assuming the tracer does not significantly washout determines blood flow based on tracer uptake; this model does not consider washout, assumes a partial volume correction factor, and its shortcoming can be somewhat mitigated by using a short dynamic scan acquisition time.

---

### Mathematical modeling / problem solving in global oxygen transport [^d22878cf]. The Journal of Surgical Research (2009). Low credibility.

A simplified approach to mathematical modeling/problem solving in global oxygen transport is presented. In addition to standard oxygen transport formulae, it uses the S-Factor and a mathematical relationship relating SvO(2) to the ratio DO(2)/VO(2). This method allows the determination or specification of SvO(2), PvO(2), P(50), and systemic shunting in the context of this simplified approach. Heretofore this has not been possible. With this approach, essentially all clinical problems in global oxygen transport can be dealt with. This is illustrated by the broad scope of the five problems presented.

---

### Theoretical models of nonlinear effects in two-component cooperative supramolecular copolymerizations [^0960167e]. Nature Communications (2011). Medium credibility.

The understanding of multi-component mixtures of self-assembling molecules under thermodynamic equilibrium can only be advanced by a combined experimental and theoretical approach. In such systems, small differences in association energy between the various components can be significantly amplified at the supramolecular level via intricate nonlinear effects. Here we report a theoretical investigation of two-component, self-assembling systems in order to rationalize chiral amplification in cooperative supramolecular copolymerizations. Unlike previous models based on theories developed for covalent polymers, the models presented here take into account the equilibrium between the monomer pool and supramolecular polymers, and the cooperative growth of the latter. Using two distinct methodologies, that is, solving mass-balance equations and stochastic simulation, we show that monomer exchange accounts for numerous unexplained observations in chiral amplification in supramolecular copolymerization. In analogy with asymmetric catalysis, amplification of chirality in supramolecular polymers results in an asymmetric depletion of the enantiomerically related monomer pool.

---

### The XZZX surface code [^8f6ac9c8]. Nature Communications (2021). High credibility.

This structured noise model thus leads to two distinct regimes, depending on which failure process is dominant. In the first regime where, we expect that the logical failure rate will decay like. We find this behaviour with systems of a finite size and at high bias where error rates are near to threshold. We evaluate logical failure rates using numerical simulations to demonstrate the behavior that characterises this regime; see Fig. 6 (a). Our data show good agreement with the scaling ansatz. In contrast, our data are not well described by a scaling.

Fig. 6
Sub-threshold scaling of the logical failure rate with the XZZX code.

a Logical failure rateat high bias near to threshold plotted as a function of code distance d. We use a lattice with coprime dimensions d × (d + 1) for d ∈ {7, 9, 11, 13, 15} at bias η = 300, assuming ideal measurements. The data were collected usingiterations of Monte-Carlo (MC) samples for each physical rate sampled and for each lattice dimension used. The physical error rates used are, from the bottom to the top curves in the main plot, p = 0.19, 0.20, 0.21, 0.22 and 0.23. Error bars represent one standard deviation for the Monte-Carlo simulations. The solid lines are a fit of the data to, consistent with Eq. (2), and the dashed lines a fit to, consistent with Eq. (3) where we would expect, see Methods. The data fit the former very well; for the latter, the gradients of the best fit dashed lines, as shown on the inset plot as a function of, give a linear slope of 0.61(3). Because this slope exceeds the value of 0.5, we conclude that the sub-threshold scaling is not consistent with. b Logical failure ratesat modest bias far below threshold plotted as a function of the physical error rate p. The data (markers) were collected at bias η = 3 and coprime d × (d + 1) code dimensions of d ∈ {5, 7, 9, 11, 13, 15} assuming ideal measurements. Data is collected using the Metropolis algorithm and splitting method presented in refs. The solid lines represent the prediction of Eq. (3). The data show very good agreement with the single parameter fitting for all system sizes as p tends to zero.

---

### High-order michaelis-menten equations allow inference of hidden kinetic parameters in enzyme catalysis [^6a200ef0]. Nature Communications (2025). High credibility.

The waiting times in Eqs. (2) and (3) have a clear physical meaning, which allows for an insightful interpretation of the underlying turnover kinetics. In Fig. 1b, we give two possible turnover paths for example. In the first path, binding is directly followed by catalysis and product formation. In the second path, binding is followed by unbinding which restarts the turnover cycle. This is followed by a second binding event leading to catalysis and product formation.

We note that single-molecule experiments are designed to track product formation events, but in most cases cannot follow state changes within the catalytic trajectory. Thus, the statistics ofis hidden and the challenge is to infer them from the observed statistics of the turnover times. Figure 2 illustrates this concept showing the waiting time histograms associated with the probability density functions (PDFs) of the observable turnover timeand the hidden timesand. Three different examples are considered: (1) a classical single-molecule Michaelis-Menten model, where all kinetic processes are Markovian (characterized by exponential PDFs, blue bars); and (2) two non-Markovian kinetic schemes, where binding times are exponentially distributed but catalysis times follow a Gamma-distribution. The non-Markovian schemes considered differ in the distribution of unbinding times, which is assumed to be exponential in one of them (orange bars) and Gamma-distributed in the other (green bars). Our main goal is to develop an approach for extracting information about the hidden PDFs from single-molecule measurements of the turnover time. Next, we develop the theory and practical tools that allow us to do just that.

---

### Emergent second law for non-equilibrium steady States [^4c03a36d]. Nature Communications (2022). High credibility.

Discussion

For systems accepting a description in terms of Markov jump processes, our results unveil a fundamental connection between the deterministic dynamics that emerges in a macroscopic limit and the non-equilibrium fluctuations at steady state. This is given by an inequality that can be interpreted as an emergent second law. In fact, it is a tighter version of the usual second law, that is saturated in the linear response regime. As shown in the Supplementary Note 4, the emergent second law can be alternatively understood as a generalized fluctuation-dissipation relation. The practical value of our result lies in the fact that the probability of non-equilibrium fluctuations is hard to evaluate, while the deterministic dynamics is directly given by standard methods. The corresponding linear response theory, working at the level of the rate function, was shown into be highly accurate in some model systems, with a regime of validity beyond that of usual linear response theories. Our result can also be employed in combination with numerical or experimental approaches: once normal or moderately rare fluctuations have been sampled and characterized, Eq. (3) can be used to bound the probability of very rare fluctuations, that otherwise would require extremely long simulation times. In addition, the refinement of our result by a coarse-graining procedure leads to novel numerical techniques to compute non-equilibrium distributions, that only rely on the deterministic dynamics and thus offer an alternative to stochastic simulations or spectral methods.

As a final comment, we note that the extensivity assumptions defining the macroscopic limit we have considered are a very natural extension of the usual notion of extensivity in equilibrium thermodynamics. The only additional requirements, besides the extensivity of the free energy associated to each state, are the extensivity of the jump rates between different states, and the intensivity of the work contributions. These natural requirements lead to an extensive non-equilibrium self-information, whose dominant contribution is constrained by our emergent second law, and that at equilibrium reduces to the regular extensive free energy.

---

### Best (but oft-forgotten) practices: checking assumptions concerning regression residuals [^f2ebb771]. The American Journal of Clinical Nutrition (2015). Low credibility.

The residuals of a least squares regression model are defined as the observations minus the modeled values. For least squares regression to produce valid CIs and P values, the residuals must be independent, be normally distributed, and have a constant variance. If these assumptions are not satisfied, estimates can be biased and power can be reduced. However, there are ways to assess these assumptions and steps one can take if the assumptions are violated. Here, we discuss both assessment and appropriate responses to violation of assumptions.

---

### Effects of changing population or density on urban carbon dioxide emissions [^49635916]. Nature Communications (2019). High credibility.

To account for the multicollinearity problem, we have fitted Eqs. (3) and (5) by using the ridge regression approach. This method solves the matrix inversion problem by adding a constant λ to the diagonal elements of X T X, so that the ridge estimator for the linear coefficients is a = (X T X + λ I) −1 X T y, where I is the identity matrix. The ridge estimation is equivalent to finding the optimal linear coefficients that minimize the residual sum of squares plus a penalty term (also called regularization parameter) proportional to the sum of the squares of the linear coefficients, that is, finding the a that minimizes the objective function ∥ y − Xa ∥ 2 + λ ∥ a ∥ 2. The optimal value of λ is usually unknown in practice and needs to be estimated from data. To do so, we have used the approach of searching for the value of λ that minimizes the mean squared error (MSE) in a leave-one-out cross validation strategy. In this approach, we estimate a (for a given λ) using all data except for one point that is used for calculating the squared error. This process is repeated until every data point is used exactly once for estimating the squared error, and then we calculate the value of the MSE for a given λ. The optimal value of λ = λ * is the one that minimizes the average value of the MSE estimated with the leave-one-out cross validation method. We have also standardized all predictors before searching for the optimal value λ *. This is a common practice when dealing with regularization methods and ensures that the penalty term is uniformly applied to the predictors, that is, the normalization makes the scale of the predictors comparable and prevents variables with distinct ranges from having uneven penalization.

---

### Survival probability of stochastic processes beyond persistence exponents [^12e77d9f]. Nature Communications (2019). High credibility.

For many stochastic processes, the probability [Formula: see text] of not-having reached a target in unbounded space up to time [Formula: see text] follows a slow algebraic decay at long times, [Formula: see text]. This is typically the case of symmetric compact (i.e. recurrent) random walks. While the persistence exponent [Formula: see text] has been studied at length, the prefactor [Formula: see text], which is quantitatively essential, remains poorly characterized, especially for non-Markovian processes. Here we derive explicit expressions for [Formula: see text] for a compact random walk in unbounded space by establishing an analytic relation with the mean first-passage time of the same random walk in a large confining volume. Our analytical results for [Formula: see text] are in good agreement with numerical simulations, even for strongly correlated processes such as Fractional Brownian Motion, and thus provide a refined understanding of the statistics of longest first-passage events in unbounded space.

---

### Comprehensive framework for accurate diffusion MRI parameter estimation [^f6dbc8d3]. Magnetic Resonance in Medicine (2013). Low credibility.

During the last decade, many approaches have been proposed for improving the estimation of diffusion measures. These techniques have already shown an increase in accuracy based on theoretical considerations, such as incorporating prior knowledge of the data distribution. The increased accuracy of diffusion metric estimators is typically observed in well-defined simulations, where the assumptions regarding properties of the data distribution are known to be valid. In practice, however, correcting for subject motion and geometric eddy current deformations alters the data distribution tremendously such that it can no longer be expressed in a closed form. The image processing steps that precede the model fitting will render several assumptions on the data distribution invalid, potentially nullifying the benefit of applying more advanced diffusion estimators. In this work, we present a generic diffusion model fitting framework that considers some statistics of diffusion MRI data. A central role in the framework is played by the conditional least squares estimator. We demonstrate that the accuracy of that particular estimator can generally be preserved, regardless the applied preprocessing steps, if the noise parameter is known a priori. To fulfill that condition, we also propose an approach for the estimation of spatially varying noise levels.

---

### A simplified formula for T1 contrast optimization for short-TR steady-state incoherent (spoiled) gradient echo sequences [^c91a4f57]. Magnetic Resonance Imaging (2007). Low credibility.

There are certain instances in practical magnetic resonance imaging where T1 changes by a small amount relative to a neighboring pixel or between scans for a given pixel. The source of these small changes in T1 can be caused either by changes in tissue water content or by the uptake of a contrast agent. For short repetition time (TR) spoiled gradient echo imaging, we show that a robust and a simple, easy to use back-of-the-envelope expression for the flip angle that optimizes contrast under these conditions is given by radical 3theta E in radians or (180/pi) radical6TR/T1 in degrees. We show that for a TR/T1 ratio of up to 0.3 and for a T1 change of up to ± 50%, this approximation to the optimal flip angle produces a contrast to within 6% of the theoretical maximum value and that these predictions are in good agreement with experiment.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^d8d04d7a]. Kidney International (2024). High credibility.

KDIGO 2024 CKD — regulatory perspective on pharmacokinetics notes recognition by major regulatory agencies that any contemporary, widely accepted, and clinically applicable estimating GFR equation is considered reasonable to assess GFR in pharmacokinetic studies.

---

### Fractional response analysis reveals logarithmic cytokine responses in cellular populations [^0d1364aa]. Nature Communications (2021). High credibility.

Calculation of typical fractions

The fractions of cells stimulated with dose i that have responses typical to dose j, v ij, can be easily calculated from data regardless of the number of doses and the type of experimental measurements. We have thatCalculation of typical fractions, v ij, with the above formula requires the possibility to examine the condition P (y | x j) > P (y | x k) for any experimentally observed response, y. The distributions P (y | x j) can be reconstructed from data using a variety of probability density estimators. The use of the available estimators, however, might be problematic for multivariate responses. We, therefore, propose a more convenient strategy. We replace the condition P (y | x j) > P (y | x k) with an equivalent condition that is computationally much simpler to evaluate. Precisely, we propose to use the Bayes formulaIf we set the equiprobable prior distribution, i.e. P (x j) = 1 /m, we have that P (y | x j) is proportional to P (x j | y) and the condition P (y | x j) > P (y | x k) is equivalent toThe above strategy allows avoiding estimation of the response distributions, P (y | x j), from data. For continuous and multivariate variable y the estimation of P (x j | y) is generally simpler than estimation of P (y | x j). Precisely, an estimatorof the distribution P (x j | y) can be built using a variety of Bayesian statistical learning methods. For simplicity and efficiency, here we propose to use logistic regression, which is known to work well in a range of applications. In principle, however, other classifiers could also be considered. The logistic regression estimators of P (x j | Y = y) arise from a simplifying assumption that log-ratio of probabilities, P (x j | Y = y) and P (x m | Y = y) is linear. Precisely, The above formulation allows fitting the logistic regression equations to experimental data, i.e. finding values of the parameters, α j and β j that best represent the data. The fitted logistic regression model allows assigning cellular responses to typical doses based on conditions given by Eq. 8. Formally, the fractions v ij defined by Eq. 6 are calculated aswhere n i is the number of cells measured for the dose x i, y l i denotes response of the l- th cell, andis equal 1 iffor anyand 0 otherwise.

---

### On the noise in "augmented T1-weighted steady state magnetic resonance imaging" [^5e296aaa]. NMR in Biomedicine (2023). Medium credibility.

Recently, Ye and colleagues proposed a method for "augmented T1-weighted imaging" (aT 1 W). The key operation is a complex division of gradient-echo (GRE) images obtained with different flip angles. Ye and colleagues provide an equation for the standard deviation of the obtained aT 1 W signal. Here, we show that this equation leads to wrong values of the standard deviation of such an aT 1 W signal. This is demonstrated by Monte Carlo simulations. The derivation of the equation provided by Ye and colleagues is shown to be erroneous. The error consists of a wrong handling of random variables and their standard deviations and of the wrong assumption of correlated noise in independently acquired GRE images. Instead, the probability distribution obtained with the aT 1 W-method should have been carefully analyzed, perhaps on the basis of previous literature on ratio distributions and their normal approximations.

---