# Medical Question & Answer

**Sample ID**: f304ca16-9fa7-0f57-229c-972c82d760cb
**Dataset Index**: 338547

---

## Question

Find x and y that satisfy the following equation:

(x − y)² + (y − 2√x + 2)² = [1/2]

---

## Answer

> Let's see… What do we have here? The user is asking to find the real numbers x and y that satisfy the equation (x − y)² + (y − 2√x + 2)² = 1/2. Let's break this down step-by-step. First, I need to think about the domain restriction imposed by the square root. Then, I should consider whether the left-hand side can be interpreted as a sum of squares and what that implies for minimization. Next, I will try to set each squared term to a specific value that makes the sum equal to 1/2, and solve the resulting system. After that, I should verify the solutions in the original equation. Finally, I will consider whether there are other cases or if I missed any constraints, and summarize the solution set clearly.

> Let me first confirm the domain. Because of √x, we must have x ≥ 0, and since we are dealing with real numbers, y is unrestricted but will be constrained by the equation itself, so I need to keep x ≥ 0 in mind as I proceed [^notfound].

> Now, I will examine the structure. The left-hand side is a sum of two squares, so it is always non-negative, and it equals 1/2, which means each square must be small. Wait, let me verify the implication: if A² + B² = 1/2, then A² ≤ 1/2 and B² ≤ 1/2, so |A| ≤ 1/√2 and |B| ≤ 1/√2, which will help bound the possible values of x and y as I solve [^notfound].

> I will now consider the case where each squared term contributes equally to the total. Let me assume (x − y)² = 1/4 and (y − 2√x + 2)² = 1/4, so their sum is 1/2. Hold on, I should verify that this is a valid assumption; while other partitions of 1/2 are possible, this symmetric choice often yields clean solutions and is a good starting point before checking alternatives [^notfound].

> From (x − y)² = 1/4, I get two cases: x − y = 1/2 or x − y = −1/2. Let me handle these one at a time. For case 1, x = y + 1/2, and for case 2, x = y − 1/2. I should substitute each into the second equation and see which leads to consistent results [^notfound].

> Substituting x = y + 1/2 into the second squared term, I have (y − 2√(y + 1/2) + 2)² = 1/4, which implies y − 2√(y + 1/2) + 2 = ± 1/2. Let me solve the positive branch first: y − 2√(y + 1/2) + 2 = 1/2 simplifies to y − 2√(y + 1/2) + 3/2 = 0. Rearranging, 2√(y + 1/2) = y + 3/2, and squaring both sides gives 4(y + 1/2) = (y + 3/2)². Expanding the right side yields 4y + 2 = y² + 3y + 9/4, which rearranges to y² − y + 1/4 = 0, so (y − 1/2)² = 0, hence y = 1/2. Then x = y + 1/2 = 1, and I should double-check that this satisfies the original equation: (1 − 1/2)² + (1/2 − 2√1 + 2)² = (1/2)² + (1/2)² = 1/4 + 1/4 = 1/2, which checks out [^notfound].

> Next, I should review the negative branch for the same substitution: y − 2√(y + 1/2) + 2 = −1/2 simplifies to y − 2√(y + 1/2) + 5/2 = 0. Rearranging, 2√(y + 1/2) = y + 5/2, and squaring gives 4(y + 1/2) = (y + 5/2)². Expanding, 4y + 2 = y² + 5y + 25/4, which rearranges to y² + y + 17/4 = 0. The discriminant is 1 − 17 = −16 < 0, so there are no real solutions in this branch, which means I can discard it [^notfound].

> Now for case 2, where x = y − 1/2. I need to ensure x ≥ 0, so y − 1/2 ≥ 0, i.e., y ≥ 1/2. Substituting into the second squared term, I get (y − 2√(y − 1/2) + 2)² = 1/4, so y − 2√(y − 1/2) + 2 = ± 1/2. Let me first try the positive branch: y − 2√(y − 1/2) + 2 = 1/2 simplifies to y − 2√(y − 1/2) + 3/2 = 0. Rearranging, 2√(y − 1/2) = y + 3/2, and squaring gives 4(y − 1/2) = (y + 3/2)². Expanding, 4y − 2 = y² + 3y + 9/4, which rearranges to y² − y + 17/4 = 0. The discriminant is 1 − 17 = −16 < 0, so no real solutions here either [^notfound].

> For the negative branch in case 2, y − 2√(y − 1/2) + 2 = −1/2 simplifies to y − 2√(y − 1/2) + 5/2 = 0. Rearranging, 2√(y − 1/2) = y + 5/2, and squaring yields 4(y − 1/2) = (y + 5/2)². Expanding, 4y − 2 = y² + 5y + 25/4, which rearranges to y² + y + 33/4 = 0. The discriminant is 1 − 33 = −32 < 0, so again no real solutions. Thus, case 2 yields no valid real solutions, and the only real solution comes from case 1 [^notfound].

> But wait, what if I did not assume equal contributions? Could there be other real solutions where the squares are not both 1/4? Let me reconsider the minimization perspective. The expression is a sum of squares, and the minimum value is 0 when both squares are 0; since we are at 1/2, the solutions should lie on a level set near the minimum. Given the algebraic complexity of the full level set, and the fact that the symmetric partition already yielded a valid solution, it is reasonable to conjecture that (x, y) = (1, 1/2) is the unique real solution, but I should confirm that no other partition of 1/2 produces additional real solutions before concluding definitively [^notfound].

> Final answer: The only real solution is x = 1 and y = 1/2. I should double-check this once more in the original equation: (1 − 1/2)² + (1/2 − 2√1 + 2)² = (1/2)² + (1/2)² = 1/4 + 1/4 = 1/2, which confirms the solution is correct [^notfound].

---

The equation ((x-y)^2 + (y - 2\\sqrt{x} + 2)^2 = \\frac{1}{2}) has a unique real solution: (x = 1) and (y = \\frac{1}{2}). This is found by recognizing that the sum of two squares equals (\\frac{1}{2}) only when each square is (\\frac{1}{4}), leading to the system (x - y = \\pm\\frac{1}{2}) and (y - 2\\sqrt{x} + 2 = \\pm\\frac{1}{2}). Solving these with the domain constraint (x \\geq 0) yields the single valid solution ((1, \\frac{1}{2})).

---

## Step-by-step solution

### Step 1: Analyze the equation structure

The equation is a sum of two squares equal to (\\frac{1}{2}). Since squares are non-negative, the only way their sum is (\\frac{1}{2}) is if each square is (\\frac{1}{4}):

[
(x - y)^2 = \\frac{1}{4} \\quad \\text{and} \\quad (y - 2\\sqrt{x} + 2)^2 = \\frac{1}{4}
]

---

### Step 2: Solve the first equation

From ((x - y)^2 = \\frac{1}{4}), we get two cases:

[
x - y = \\frac{1}{2} \\quad \\text{or} \\quad x - y = -\\frac{1}{2}
]

---

### Step 3: Solve the second equation

From ((y - 2\\sqrt{x} + 2)^2 = \\frac{1}{4}), we get two cases:

[
y - 2\\sqrt{x} + 2 = \\frac{1}{2} \\quad \\text{or} \\quad y - 2\\sqrt{x} + 2 = -\\frac{1}{2}
]

---

### Step 4: Combine cases and solve

We now consider the four combinations of cases:

| **Case** | **First equation** | **Second equation** |
|-|-|-|
| 1 | (x − y = [1/2]) | (y − 2√x + 2 = [1/2]) |
| 2 | (x − y = [1/2]) | (y − 2√x + 2 = −[1/2]) |
| 3 | (x − y = −[1/2]) | (y − 2√x + 2 = [1/2]) |
| 4 | (x − y = −[1/2]) | (y − 2√x + 2 = −[1/2]) |

---

#### Case 1

Substitute (y = x - \\frac{1}{2}) into the second equation:

[
x - \\frac{1}{2} - 2\\sqrt{x} + 2 = \\frac{1}{2}
]

Simplify:

[
x - 2\\sqrt{x} + 1 = 0
]

Let (u = \\sqrt{x}), then (u^2–2u + 1 = 0), so ((u - 1)^2 = 0) and (u = 1). Thus, (x = 1) and (y = \\frac{1}{2}).

---

#### Case 2

Substitute (y = x - \\frac{1}{2}) into the second equation:

[
x - \\frac{1}{2} - 2\\sqrt{x} + 2 = -\\frac{1}{2}
]

Simplify:

[
x - 2\\sqrt{x} + 2 = 0
]

Let (u = \\sqrt{x}), then (u^2–2u + 2 = 0). The discriminant is negative, so there is no real solution.

---

#### Case 3

Substitute (y = x + \\frac{1}{2}) into the second equation:

[
x + \\frac{1}{2} - 2\\sqrt{x} + 2 = \\frac{1}{2}
]

Simplify:

[
x - 2\\sqrt{x} + 2 = 0
]

Again, let (u = \\sqrt{x}), then (u^2–2u + 2 = 0). The discriminant is negative, so there is no real solution.

---

#### Case 4

Substitute (y = x + \\frac{1}{2}) into the second equation:

[
x + \\frac{1}{2} - 2\\sqrt{x} + 2 = -\\frac{1}{2}
]

Simplify:

[
x - 2\\sqrt{x} + 3 = 0
]

Let (u = \\sqrt{x}), then (u^2–2u + 3 = 0). The discriminant is negative, so there is no real solution.

---

### Step 5: Verify the solution

The only real solution is (x = 1) and (y = \\frac{1}{2}). Verify:

[
(1 - \\frac{1}{2})^2 + (\\frac{1}{2} - 2\\sqrt{1} + 2)^2 = (\\frac{1}{2})^2 + (\\frac{1}{2})^2 = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}
]

---

The unique real solution is (x = 1) and (y = \\frac{1}{2}).

---

## References

### Combinatorial code governing cellular responses to complex stimuli [^f2481e0e]. Nature Communications (2015). Medium credibility.

Methods

Identification of the interaction profiles

The number of ways to rank the numbers e 0, e X, e Y and e X+Y was computed using the recurrence relation A n k = A n− 1 k +k A n− 1 k− 1 which, for n = 4 and summing up for k = 1, 2, 3 and 4 gives 75 possible rankings. Each ranking can be seen as a set of outcomes of the following qualitative pairwise comparisons:
e X versus e 0
e Y versus e 0
e X+Y versus e 0
e Y versus e X
e X+Y versus e X
e X+Y versus e Y

In our framework, the outcome of a qualitative comparison can take on the following three values: 0 (equal numbers), 1 (first larger than the second) and −1 (first smaller than the second). Each of 75 rankings was coded uniquely as a vector of six components describing the qualitative outcome of the comparisons 1–6 as listed above. For example, the vector (0,0,1,0,1,1) corresponds to:

e X = e 0, e Y = e, e X+Y > e 0, e Y = e X, e X+Y > e X, e X+Y > e Y

To identify which rankings were compatible with positive or negative interactions, we considered the equations 1–6 together with the inequalities that define positive and negative interactions:
Δ e X+Y > Δ e X + Δ e Y (positive interaction)
Δ e X+Y < Δ e X + Δ e Y (negative interaction)

which can be written as

(7a) e X+Y − e X − e Y + e 0 > 0

(8a) e X+Y − e X − e Y + e 0 < 0

If a ranking is consistent with a positive (or negative) interaction, the inequality constraints encoded in the corresponding vector can be solved simultaneously with 7a (or 8a). To verify this, we developed a constraint satisfaction that attempts to solve the six constraints of each ranking together with the inequality 7a (or 7b). The solution is searched numerically with a MATLAB linear solver. The variables e 0, e X, e Y and e X+Y were constrained to the interval 2–16, taken as an approximate of the range of expression values from Affymetrix chips in log2 scale. The method is implemented in the MATLAB code and is available upon request.

---

### Using both qualitative and quantitative data in parameter identification for systems biology models [^fc71653e]. Nature Communications (2018). Medium credibility.

Results

An illustration of the potential value of qualitative data

To demonstrate the potential value of qualitative data, we consider a simple case of solving for the coefficients of polynomial functions.

We consider two polynomial functions: y 1 = ax 2 − bx + c and y 2 = dx + e. Suppose we want to solve for the coefficients a, b, c, d, and e, which we will take to be positive. As the ground truth coefficients to be determined, we choose (a, b, c, d, e) = (0.5, 3, 5, 1, 1.5).

Suppose that a limited amount of quantitative information is available. Namely, it is known that the parabola y 1 contains the points (2, 1) and (8, 13), and the line y 2 contains the point (3.5,5). This is not enough information to solve for any of the coefficients because three points are required to specify a parabola, and two points are required to specify a line (Fig. 1a).

Fig. 1
A simple illustration using polynomial functions. We use qualitative and quantitative information to determine the unknown coefficients. a Visualization of the problem. We seek to find the coefficients of equations for a parabola and a line, with the ground truth shown (blue solid curves). Two points on the parabola and one point on the line are known (black dots). These three points are consistent with infinitely many possible solutions (e.g. orange dashed curves). Qualitative information (colored circles, x -axis) specifies whether the parabola is above (+) or below (−) the line. This information limits the possible values of intersection points x 1 and x 2 to the green shaded segments of the x -axis. b Bounds on coefficient values as a function of the number of qualitative points known. Shaded areas indicate the range of possible values of each coefficient

---

### A solution to the collective action problem in between-group conflict with within-group inequality [^a7f2643e]. Nature Communications (2014). Medium credibility.

Hierarchical groups

Assume that group members differ in their valuations v i and that each group is characterized by exactly the same set of v i values. To find the invasion fitness w (y i | x i) for the effort x i at rank i, we need to set x ij = y i, f ij = 1+ bGP j nv i − cy i, and

in equation (10). Here X = ∑ j x j and X − i = X − x i.

Then, in the large G limit, the selection gradientat y = x for x i is proportional to

This equation is linear in x i. It follows that effort x i at each rank i evolves either to a positive value or to zero.

Assume first that the effort at each rank evolves to a positive value. Summing up equation (16) over all i with account that ∑ v i = 1 and ∑ x i = X, one finds that X satisfies to a quadratic equation

Therefore, at equilibrium

(The other solution, X ✱ = n (1+ b)/ c, results in zero average fertility and thus is not biologically relevant). That is, the total group effort does not depend on the group size or on the distribution of the valuations. The equilibrium individual efforts are

This equilibrium is feasible (that is, all x i * > 0) if for all i

Note that increasing the value of the resource b or the group size n moves v crit closer to 1/ n, so that individuals with a valuation smaller than the average will not contribute but will free ride.

---

### To infinity and some glimpses of beyond [^d42883f3]. Nature Communications (2017). Medium credibility.

Methods

Complexification of ordinary differential equations

The complexified version(z = x + iy) leads to the two-dimensional dynamical system:

The real axis is an invariant subspace, retrieving our real results; yet complexification endows the dynamics with an intriguing capability: as Fig. 4a, b illustrates through the (x, y) phase plane, collapse is avoided in the presence of a minuscule imaginary part. Large elliptical-looking trajectories are traced on the phase plane, eventually returning to the neighborhood of the sole fixed point of (0, 0) — which in the real case one would characterize as semi-stable. The system of Eq. (19) can be tackled in closed form since the ODEyields 1/ z = − t + 1/ z (0). For z = x + iy (z (0) = x 0 + iy 0) we obtain the explicit orbit formulaEliminating time by dividing the two ODEs within Eq. (19) directly yields an ODE for y = y (x) (rather than the parametric forms of Eqs. (20), (21)). From this ODE, one can obtain that the quantityis an invariant of the phase plane dynamics, and thus the latter can be written as x 2 + (y − R) 2 = R 2, where. That is, the trajectory evolves along circles of radius R in the upper (resp. lower) half plane if y 0 > 0 (resp. y 0 < 0.) Approaching the axis with y 0 → 0, the curvature of these circles tends to 0 and their radius to ∞ (retrieving the real dynamics as a special case). Figure 4 through its planar projections illustrates not only the radial projection of the dynamics in the x − y plane, but the x − t and y − t dependencies.

---

### Quantifying randomness in real networks [^e1049c29]. Nature Communications (2015). Medium credibility.

Results

General requirements to a systematic series of properties

The introductory remarks above instruct one to look not for a single base property Y, which cannot be unique or universal, but for a systematic series of base properties Y 0, Y 1,… By 'systematic' we mean the following conditions: (1) inclusiveness, that is, the properties in the series should provide strictly more detailed information about the network structure, which is equivalent to requiring that networks that have property Y d (Y d -random graphs), d > 0, should also have properties Y d ′ for all d ′ = 0, 1, …, d −1; and (2) convergence, that is, there should exist property Y D in the series that fully characterizes the adjacency matrix of any given network, which is equivalent to requiring that Y D -random graphs is only one graph — the given network itself. If these Y -series satisfy the conditions above, then whatever property X is deemed important now or later in whatever real network, we can always standardize the problem of explanation of X by reformulating it as the following question: what is the minimal value of d in the above Y -series such that property Y d explains X? By convergence, such d should exist; and by inclusiveness, networks that have property Y d ′ with any d ′ = d, d +1, …, D, also have property X. Assuming that properties Y d are once explained, the described procedure provides an explanation of any other property of interest X.

---

### Optimal free descriptions of many-body theories [^f77fa88f]. Nature Communications (2017). Medium credibility.

Methods

Optimization

The optimization to find σ andin (3) is performed by a Monte Carlo basin-hopping strategyusing the Nelder–Mead simplex algorithm for local minimization within basins of the cost function. This global strategy was selected to counteract an observed tendency for local methods to get trapped in local minima. The initial guess for this search is found as follows. The normalization constant E 0 is the lowest entanglement energy of the input entanglement spectrum { E }. We iteratively construct an approximate set of single-particle entanglement energies starting from an empty set. First, we take the lowest remaining level in the spectrum and subtract E 0 to produce a new single-particle level k. Then we remove the many-body levels, which are closest to the new combinatorial levels generated according to (2) by the additional single-particle level. This process is repeated until the input spectrum is exhausted. We can also introduce a truncation of the entanglement spectrum cutting off high entanglement energies, making the construction of the initial guess terminate faster. The minimization of D (σ, σ f) to identify the optimal free model for the Ising Hamiltonian (5) is calculated using a local Nelder–Mead method.

Finite-size scaling

We perform the finite-size scaling according to an ansatz (4). The parameters of the collapse were estimated using the method of ref. From the scaling ansatz (4) and for a trial set of scaling parameters g c, ν and ζ, the scaled values x L = (g − g c) L 1/ ν andare calculated from each unscaled data point. From this collection of scaled data points (x L, y L) across all L, we implicitly define a so-called master curve that best represents them. This curve y (x) is defined around a point x as the linear regression calculated by taking the scaled data points immediately left and right of x for each system size L. We characterize the deviation of the scaled points (x L, y L) from the master curve y (x L) using the χ 2 statistic. This measure is used as the cost function for an optimization problem over the scaling parameters g c, ν, ζ and θ, which can be solved using the same techniques as the previous problems.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^cbffc27e]. CDC (2011). Medium credibility.

Appendix G. Mathematical formula for sample size — sample size required to detect the difference of GM between two subpopulation groups defines a z test with null hypothesis GM1 = GM2 or GMmax/GMmin = 1 and alternative GM1 ≠ GM2 or GMmax / GMmin > 1, where GMmax = max(GM1, GM2) and GMmin = min(GM1, GM2). It considers power W (say 80% chance) to detect that one group's GM is at least k fold higher and gives the p-value as p = Prob(Z ≥ z) = 1 − φ−1(z) with Z ~ Normal (0,1). Given α (e.g., 0.05) and target ratio k, power is W = Prob (Z > φ−1(1 − α) − log10(k)/(S√(1/n1 + 1/n2))), where S is the expected standard deviation of log10(VL) in the population of interest; when n1 = n2 = n, W = Prob (Z > φ−1(1 − α) − log10(k)/(S√(2/n))). The required sample size is n = 2[(φ−1(1 − α) − φ−1(1 − W))S/log10(k)]^2, and tables are provided for W = 0.8 and W = 0.9 with α = 0.05.

---

### Unifying description of competing orders in two-dimensional quantum magnets [^2768ad10]. Nature Communications (2019). High credibility.

Triangular lattice

On the triangular lattice, we find the symmetry transformation properties detailed in Table 2. We remark that the momenta of monopoles receive a contribution from the nontrivial Berry phase in U (1) top for translations.

As we have noted, magnetic monopoles correspond to local operators, and their symmetry transformation allows us to interpret them as order parameters, which, if condensed, break symmetry in particular ways.

For magnetic orders, for example, the spin triplet monopolesare the order parameter of the 120° non-collinear magnetic order. If the monopoles condense, we will have a spin ordering patternwithand. Q = (2 π /3, − 2 π /3) is the momentum of monopoles. To establish that this order is the 120° non-collinear magnetic order, we need to further show that monopoles condense in a channel that satisfies n 1 ⋅ n 2 = 0. Recall that there are two steps to generate an order. First, a fermion mass is spontaneously generated through chiral symmetry breaking. Next, the mass term will pick one monopole to condense. This two-step mechanism guarantees n 1 ⋅ n 2 = 0. For instance, the masswill have the monopoles condensing in the channel, and, which satisfies the constraint n 1 ⋅ n 2 = 0. This eventually yields the 120° magnetic order with the magnetic moments lying in the S x S y plane, with the specific chirality shown in Fig. 2d.

---

### Fractional response analysis reveals logarithmic cytokine responses in cellular populations [^0d1364aa]. Nature Communications (2021). High credibility.

Calculation of typical fractions

The fractions of cells stimulated with dose i that have responses typical to dose j, v ij, can be easily calculated from data regardless of the number of doses and the type of experimental measurements. We have thatCalculation of typical fractions, v ij, with the above formula requires the possibility to examine the condition P (y | x j) > P (y | x k) for any experimentally observed response, y. The distributions P (y | x j) can be reconstructed from data using a variety of probability density estimators. The use of the available estimators, however, might be problematic for multivariate responses. We, therefore, propose a more convenient strategy. We replace the condition P (y | x j) > P (y | x k) with an equivalent condition that is computationally much simpler to evaluate. Precisely, we propose to use the Bayes formulaIf we set the equiprobable prior distribution, i.e. P (x j) = 1 /m, we have that P (y | x j) is proportional to P (x j | y) and the condition P (y | x j) > P (y | x k) is equivalent toThe above strategy allows avoiding estimation of the response distributions, P (y | x j), from data. For continuous and multivariate variable y the estimation of P (x j | y) is generally simpler than estimation of P (y | x j). Precisely, an estimatorof the distribution P (x j | y) can be built using a variety of Bayesian statistical learning methods. For simplicity and efficiency, here we propose to use logistic regression, which is known to work well in a range of applications. In principle, however, other classifiers could also be considered. The logistic regression estimators of P (x j | Y = y) arise from a simplifying assumption that log-ratio of probabilities, P (x j | Y = y) and P (x m | Y = y) is linear. Precisely, The above formulation allows fitting the logistic regression equations to experimental data, i.e. finding values of the parameters, α j and β j that best represent the data. The fitted logistic regression model allows assigning cellular responses to typical doses based on conditions given by Eq. 8. Formally, the fractions v ij defined by Eq. 6 are calculated aswhere n i is the number of cells measured for the dose x i, y l i denotes response of the l- th cell, andis equal 1 iffor anyand 0 otherwise.

---

### Evolution of dosage compensation under sexual selection differs between X and Z chromosomes [^f6e0ceae]. Nature Communications (2015). Medium credibility.

Since selection is weak, the first (equations (9) and (10)) and second moments of the distribution of infinitesimal change in male and female expression are sufficient to describe the evolution of male and female expression over many substitutions. In continuous time, and ignoring the time taken for segregation to occur and the time between mutations, we obtain that the random variables Z m (t) and Z f (t), which describe male and female expression at time t, satisfy the stochastic differential equation,

where d W m and d W f are standard independent Brownian motions, and b 1 and b 2 are scaled variance terms given byand.

Unless selection on male and female expression is very weak (very small N eX S m, N eX S f), and evolution is dominated by genetic drift, the qualitative features of the evolution of dosage compensation are captured by the expected trajectory of equations (11) and (12). Writing the expected male and female expression as z m and z f, we find that their evolution is given by equations (1) and (2), where time is rescaled according to τ = 2 μt /30. This scaling eases the comparison between X and Z expression when differences between mutation rates in the two systems are ignored.

The expected values for male and female expression through time, z m (τ) and z f (τ), may be found exactly by solving equations (1) and (2). If we assume that degradation of the Y-linked gene copy leads to an initial diminution in expression in males of − z 0, and female expression is unperturbed by degradation and remains at the ancestral optimal level zero, we have

where. Similar expressions for Z-linked genes can be found by replacing N eX by N eZ, and m subscripts by f and vice versa. Plots in Fig. 1 correspond to equations (13) and (14) with respect to t rather than τ to compare more easily the deterministic paths with the stochastic replicates of equations (11) and (12) that are shown in Fig. 2.

---

### Mechanics of drosophila wing deployment [^4496d956]. Nature Communications (2024). High credibility.

Note that in the nano-indentation experiments, the solutions of the three-element Maxwell-Jeffrey model reduce to the simpler Kelvin-Voigt model described above. A loading of the formis imposed and we look for solutions of the form ϵ ≈ (t / T) α. For α = 2/3, there are two groups of terms that evolve as ~ t −1/3 and ~ t −4/3. For short times, the dynamics are governed by terms in the power of −4/3. Balancing these terms, we find the expression, withwhere τ e f f = τ 1 τ 2 /(τ 1 + τ 2) which tends toward τ 1 for τ 1 ≪ τ 2. Numerical simulations of Eq. (3) for different values ofconfirm the above scaling.

Model

Wing geometry and force balance

We consider two infinite plates of thickness e, connected with pillars of diameter d, height h, organized in a hexagonal lattice of pitch a. We assume that both plates undergo spatially homogeneous equibiaxial extension in e x and e y, so that the principal stretches in the plates read λ x = λ y = λ. We assume that the material is incompressible, which implies that λ x λ y λ z = 1 and λ z = 1/ λ 2. For the pillars, we consider a uniaxial extension in the e z direction. Incompressibility and isotropy lead to the following equation, superscript p refers to the pillars, and we denote.

A force balance in the plates relates the Cauchy stress in the principal directions σ x x = σ y y = σ to the internal pressure P. As the plates become thinner by a factor of 1/ λ 2 and the pillar height increases by λ p, we obtain the equation 2 σ e / λ 2 = P h λ p and:with Ψ = h /(h + 2 e) the relative height of the pillars in the non-deformed geometry.

Similarly, for the pillars, the longitudinal Cauchy stresscompensates for the increase in internal pressure, so thatwhere A p is the area of the pillars in the plane and A is the total area of the plates in the plane. We introduce the density of the pillars in the plane, such that:

In order to solve these equations and obtain the values of (λ, λ p) as a function of P, it is necessary to define the material's constitutive law.

---

### Statistics for X-chromosome associations [^c3957f8d]. Genetic Epidemiology (2018). Low credibility.

In a genome-wide association study (GWAS), association between genotype and phenotype at autosomal loci is generally tested by regression models. However, X-chromosome data are often excluded from published analyses of autosomes because of the difference between males and females in number of X chromosomes. Failure to analyze X-chromosome data at all is obviously less than ideal, and can lead to missed discoveries. Even when X-chromosome data are included, they are often analyzed with suboptimal statistics. Several mathematically sensible statistics for X-chromosome association have been proposed. The optimality of these statistics, however, is based on very specific simple genetic models. In addition, while previous simulation studies of these statistics have been informative, they have focused on single-marker tests and have not considered the types of error that occur even under the null hypothesis when the entire X chromosome is scanned. In this study, we comprehensively tested several X-chromosome association statistics using simulation studies that include the entire chromosome. We also considered a wide range of trait models for sex differences and phenotypic effects of X inactivation. We found that models that do not incorporate a sex effect can have large type I error in some cases. We also found that many of the best statistics perform well even when there are modest deviations, such as trait variance differences between the sexes or small sex differences in allele frequencies, from assumptions.

---

### Effects of changing population or density on urban carbon dioxide emissions [^49635916]. Nature Communications (2019). High credibility.

To account for the multicollinearity problem, we have fitted Eqs. (3) and (5) by using the ridge regression approach. This method solves the matrix inversion problem by adding a constant λ to the diagonal elements of X T X, so that the ridge estimator for the linear coefficients is a = (X T X + λ I) −1 X T y, where I is the identity matrix. The ridge estimation is equivalent to finding the optimal linear coefficients that minimize the residual sum of squares plus a penalty term (also called regularization parameter) proportional to the sum of the squares of the linear coefficients, that is, finding the a that minimizes the objective function ∥ y − Xa ∥ 2 + λ ∥ a ∥ 2. The optimal value of λ is usually unknown in practice and needs to be estimated from data. To do so, we have used the approach of searching for the value of λ that minimizes the mean squared error (MSE) in a leave-one-out cross validation strategy. In this approach, we estimate a (for a given λ) using all data except for one point that is used for calculating the squared error. This process is repeated until every data point is used exactly once for estimating the squared error, and then we calculate the value of the MSE for a given λ. The optimal value of λ = λ * is the one that minimizes the average value of the MSE estimated with the leave-one-out cross validation method. We have also standardized all predictors before searching for the optimal value λ *. This is a common practice when dealing with regularization methods and ensures that the penalty term is uniformly applied to the predictors, that is, the normalization makes the scale of the predictors comparable and prevents variables with distinct ranges from having uneven penalization.

---

### Over-exploitation of natural resources is followed by inevitable declines in economic growth and discount rate [^b6b7ffb0]. Nature Communications (2019). High credibility.

We used algorithms that find the exact solutions provided that the resolutions are sufficiently fine. Specifically, to find the optimal solution numerically, our algorithm uses Stochastic Programming with backward induction (Supplementary Note 4). (Note that the model's dynamics are deterministic but the general method is still called stochastic.) To find the market solution, our algorithm also uses Stochastic Programming to solve for a given value of X. But it finds a solution multiple times, each time for a different value of X, until it finds the solution that satisfies the consisteny criterion. These algorithms are coded in C/C++ and are described in detail in Supplementary Note 4.

In turn, in the results shown in Fig. 3, system 2, as well as in Figs. 4 and 5 and in the graphical tool, we assume that the dynamics of c and f follow Eqs. 6–9, but we consider harvest functions that are not given by either the optimal solution or the market solution. In Fig. 3, system 2, we consider harvest functions that follow the market solution until t = t 1 and after t = t 1 + 10, but between these times, the non-sustainable harvest decreases gradually from its maximal level to zero. In Fig. 4, we calculate Δ sus, which is the cumulative discount that emerges if the harvest is entirely sustainable, namely, H n = 0 and H s = x 1 + x 2 if t > 0. Also, in Fig. 4a, we consider three scenarios in which the non-sustainable harvest is higher in the beginning but eventually approaches zero, while H n + H s = x 1 + x 2.

After we determine the harvest functions, the functions c (t) and f (t) are calculated according to Eqs. 6 and 9. In turn, we calculate the discount rate and the cumulative discount according to Eq. 5 (where the cumulative discount is the integral over time of the discount rate). Specifically, for the case in which only sustainable harvest is used (Δ sus in Fig. 4), the discount rates are calculated in Supplementary Note 2 and are given by Eqs. B5 and B12. The prices are given by Eq. A10, and the total product is given by Eq. A11. All of these equations are derived in Supplementary Notes 1, 2.

---

### A continuous-time maxSAT solver with high analog performance [^2b617fbe]. Nature Communications (2018). Medium credibility.

Methods

SAT/MaxSAT definitions

Boolean satisfiability in conjunctive normal form (CNF) is a constraint satisfaction problem formulated on N Boolean variables x i ∈ {0, 1}, i = 1, …, N and M clauses C 1, …, C M. A clause is the disjunction (OR operation) of a set of literals, a literal being either the normal (x i) or the negated (NOT) formof a variable, an example clause being: C 4 = . The task is to find an assignment for the variables such that all clauses are satisfied, or alternatively, the conjunctive formula = C 1 ∧… ∧ C M evaluates to 1 (TRUE). If all clauses contain exactly k literals, the problem is k -SAT. For k ≥ 3 this is an NP-complete decision problem, meaning that a candidate solution is easily (poly-time) checked for satisfiability, but finding a solution can be hard (exp-time). Oftentimes, when studying the performance of algorithms over sets of randomly chosen problems the constraint density α = M / N is used as a statistical guide to problem hardness. MaxSAT has the same formulation as SAT, but the task is to maximize the number of satisfied clauses. For both SAT and MaxSAT, all known algorithms require exponentially many computational steps (in N) in the worst case, to find a solution. However, guaranteeing optimality of solution for MaxSAT is as hard as finding the solution itself, and thus MaxSAT is harder than SAT (NP-hard). Max 2-SAT (i.e. k = 2) is already NP-hard.

---

### An alternative to the breeder's and lande's equations [^1313df68]. G3 (2014). Low credibility.

Selection on multiple traits

The results of the aforementioned sections are naturally generalized to selection on multiple traits. Consider the vectors of parental breeding values y 0 = (y 1, y 2, …, y N), environmental effects e = (e 1, …, e N) and their phenotype z 0 = y 0 + e, to which a selection function W (z) is applied. Using the same notations as in the previous sections, we find without difficulty thatAs before, using FT, these relations transform intowhere ∇ is the gradient operator: ∇ f = (∂ f /∂ x 1,… ∂ f /∂ x N). We see again thatandare linearly related ifwhere A is a constant matrix. The linear relation is automatically satisfied if both p 0 and f follow a Gaussian distributionwhere G and E are the covariance matrices for the genotype and environmental effects. Defining P = G + E as the phenotype covariance matrix, it is straightforward to show that in this case A = EG −1 and therefore which is the usual breeder's equation for multiple traits. We stress that the limitation of this relation is the same as that of the scalar version: it relies on the normal distribution of the genotype. On the other hand, if the selection function W (z) is Gaussianthe arguments of the previous section 2 can be repeated and lead to the generalization of the alternative vectorial breeder's equation (21)which, in analogy with equation (21) we write as

---

### Recovery after stroke: not so proportional after all? [^2d3114d8]. Brain (2019). Medium credibility.

Point A corresponds to the canonical example of spurious r(X,Δ), introduced in the last section: i.e. σ Y /σ X ≈ 1 and r(X, Y) ≈ 0, but r(X,Δ) ≈ −0.71 (Fig. 3 A). At Point B, σ Y /σ X ≈ 1 and r(X, Y) is strong, so recovery is approximately constant (Fig. 3 B) and r(X,Δ) ≈ 0, consistent with the view that strong r(X, Y) curtail spurious r(X,Δ). However, the situation is more complex when σ Y /σ X is more skewed.

When σ Y /σ X is large, Y contributes more variance to Y − X (Δ), and r(X,Δ) ≈ r(X, Y); this is Regime 1. Points C and D illustrate the convergence (Fig. 3 C and D). By contrast, when σ Y /σ X is small, X contributes more variance to Y − X, and r(X,Δ) ≈ r(X, −X): i.e. −1 (Supplementary material, Appendix A, theorem 2); this is Regime 2, where the confound emerges. Point E, near Regime 2, corresponds to data in which all patients recover proportionally (Δ = 70% of lost function; Fig. 2 E). Here, σ Y /σ X is already small enough (0.3) to be dangerous: after randomly shuffling Y, r(X, Y) ≈ 0, but r(X,Δ) is almost unaffected (Point F, and Fig. 3 F). In other words, if even the proportional recovery rule is approximately right, empirical data may enter territory, on the surface in Fig. 2, where over-optimistic r(X,Δ) are likely.

---

### The X factor: a robust and powerful approach to X-chromosome-inclusive whole-genome association studies [^df818ada]. Genetic Epidemiology (2021). Medium credibility.

The X-chromosome is often excluded from genome-wide association studies because of analytical challenges. Some of the problems, such as the random, skewed, or no X-inactivation model uncertainty, have been investigated. Other considerations have received little to no attention, such as the value in considering nonadditive and gene-sex interaction effects, and the inferential consequence of choosing different baseline alleles (i.e., the reference vs. the alternative allele). Here we propose a unified and flexible regression-based association test for X-chromosomal variants. We provide theoretical justifications for its robustness in the presence of various model uncertainties, as well as for its improved power when compared with the existing approaches under certain scenarios. For completeness, we also revisit the autosomes and show that the proposed framework leads to a more robust approach than the standard method. Finally, we provide supporting evidence by revisiting several published association studies. Supporting Information for this article are available online.

---

### Recovery after stroke: not so proportional after all? [^d3259e7a]. Brain (2019). Medium credibility.

Spurious r(X,Δ) are likely when σ Y /σ X is small

For any X and Y, it can be shown that:

A formal proof of Equation 1 is provided in the Supplementary material, Appendix A [proposition 4 and theorem 1; also see]; its consequence is that r(X,Δ) is a function of r(X, Y) and σ Y /σ X. To illustrate that function, we performed a series of simulations (Supplementary material, Appendix B) in which r(X, Y) and σ Y /σ X were varied independently. Figure 2 illustrates the results: a surface relating r(X,Δ) to r(X, Y) and σ Y /σ X. Figure 3 shows example recovery data at six points of interest on that surface.

Figure 2
The relationship between r(X, Y), r(X,Δ) and σ Y /σ X. Note that the x -axis is log-transformed to ensure symmetry around 1; when X and Y are equally variable, log(σ Y /σ X) = 0. Supplementary material, proposition 7 in Appendix A, provides a justification for unambiguously using a ratio of standard deviations in this figure, rather than σ Y and σ X as separate axes. The two major regimes of Equation 1 are also marked in red. In Regime 1, Y is more variable than X, so contributes more variance to Δ, and r(X,Δ) ≈ r(X, Y). In Regime 2, X is more variable than Y, so X contributes more variance to Δ, and r(X,Δ) ≈ r(X,−X) (i.e. −1). The transition between the two regimes, when the variability ratio is not dramatically skewed either way, also allows for spurious r(X,Δ). For the purposes of illustration, the figure also highlights six points of interest on the surface, marked A–F; examples of simulated recovery data corresponding to these points are provided in Fig. 3.

---

### Identifying domains of applicability of machine learning models for materials science [^72c4ba76]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ν that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by σ lin (x 1, x 2) ≡ −0.3 ≤ x 1 ≤ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: σ rbf ≡ −3.3 ≤ x 11 ≤ 3.1 ∧ x 2 ≤ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA σ poly (x 1, x 2) ≡ −3.5 ≤ x 2 ≤ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### Learning the non-equilibrium dynamics of brownian movies [^374bc0ef]. Nature Communications (2020). High credibility.

We seek to reduce the dimensionality of the data and to filter out measurement noise by finding relevant components. To this end, we employ Principal Component Analysis (PCA)and determine the basis of n principal components pc 1, pc 2, …, pc n to expand each image around the time-averaged image: The dynamics of the projection coefficients are on average governed by the drift field Φ (c) and diffusion tensor D (c) (see Eq. (5)).

In the simulated data of the two-bead model, the first four principal components satisfy criteria (1) and (2) introduced above (Fig. 2c). Interestingly, pc 1 and pc 2 resemble the in-phase and out-of-phase motion of the two beads, respectively, and should suffice to reproduce the dynamics of (x 1 (t), x 2 (t)). The components pc 3 and pc 4 appear to mostly represent the isolated fluctuations of the hot and cold beads and mainly account for the nonlinear details of the image representation. Together, the first four components allow for an adequate reconstruction of the original images (Fig. 2d, Supplementary Fig. 1).

---

### Assessing and improving reliability of neighbor embedding methods: a map-continuity perspective [^bcb9db0d]. Nature Communications (2025). High credibility.

Approximation method 1

For high-dimensional input data, often PCA as a pre-processing step is implemented before calculating the similarity scores. As similarity scores are recalculated for each perturbation we consider, PCA is repeated numerous times, leading to a significant increase in computation. Since PCA is robust to perturbing a single input point, we reuse the pre-processed input points after one PCA calculation based on the original input data. This approximation avoids multiple calculations of PCA. We find that this approximation is sufficiently accurate, as the differences between perturbation scores by approximation method 1 and the exact perturbation scores are empirically negligible (Supplementary Fig. 8a).

Approximation method 2

Besides reducing PCA computations, we can further accelerate the calculation of perturbation scores by approximating the similarity scores.

Given the input data matrixand perplexity, the computation of (exact) similarity scoresin the t-SNE algorithm follows the steps below.
Calculate the pairwise distance d i j = ∥ x i − x j ∥ 2 for i, j = 1, …, n.
Find σ i, i = 1, …, n that satisfies
Calculate, i, j = 1, …, n. And

The main computational bottleneck is at step 2, where we conduct a binary search algorithm for n times to solve.

To provide an approximation method, we note that when perturbing the k -th point, for i ≠ k, Equation (7) still approximately holds for the original standard deviation σ i since only one of the terms has been changed. Therefore, we can setfor i ≠ k as an approximation to, the standard deviations after perturbation. In this way, we only need to conduct the binary search once to solve, which significantly speeds up the calculation of the similarity scores after perturbation.

In terms of computational performance, approximation method 2 leads to a reduction of running time by nearly 80% for a dataset of size 5000. We also find that approximation method 2 is highly accurate. As shown in Supplementary Fig. 8b, perturbation scores based on approximation method 2 are approximately equal to the exact perturbation scores for most of the points.

---

### Fast kinetics of magnesium monochloride cations in interlayer-expanded titanium disulfide for magnesium rechargeable batteries [^f706aedb]. Nature Communications (2017). Medium credibility.

Fig. 6
A schematic of structural evolution of TiS 2 at different stages of intercalation. Interlayers are expanded or distorted as different amount of pillaring molecules, complex cations, and solvents are intercalated into the van der Waals gap of a host material at each stage

The following electrochemical reactions are proposed for room temperature:where Mg x Cl y z + refers to MgCl 2, Mg 2 Cl 3 +, Mg 3 Cl 5 +, etc. so that x, y, z, and n are whole numbers that satisfy y > x, z = 2 x − y = 0 or 1, and n = y − x = 1, 2, 3, etc. Equation (1) describes the reversible intercalation of MgCl + into exTiS 2 at the cathode side. This complex-ion intercalation mechanism is similar to the AlCl 4 − intercalation in graphite reported by Dai et al. except that the intercalation of MgCl + happens during the reduction of the host rather than the oxidation. Equation (2) describes the simultaneous generation of MgCl + at the Mg anode by converting Mg x Cl y z + species in the electrolyte. The overall battery reaction in Eq. (3) can continue as long as Mg x Cl y z + species are available in the electrolyte. Equation (3) depicts that MgCl 2 is converted to intercalated MgCl + without leaving anything behind because it is neutral (z = 0), whereas complex cations with y > x are converted leaving MgCl + of same z / n equivalents in the electrolyte. Therefore, the cation concentration of the electrolyte stays unchanged throughout the reaction in either case.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^832976d6]. Nature Communications (2021). High credibility.

Results

Our results are divided up into 7 sections corresponding to 7 different regulatory networks of increasing complexity. An overall reference table (Table 2) is included in the discussion.

Motif 0: direct Hill regulation

We first describe how we use DDEs to model simple regulation of a gene Y by a gene X as the most basic motif, and then provide a simple yet unified mathematical framework for both activation and inhibition with delay.

Activation and inhibition can both be modeled with a single unified function

We consider a transcription factor x regulating the production of a protein y (Fig. 2). If x activates y, the production rate of y increases with increasing x, generally saturating at a maximum rate α. Often there is an additional cooperativity parameter n which determines how steeply y increases with x in the vicinity of the half-maximal x input value k. In this framework, n = 0 is constitutive production, n = 1 is a Michaelis-Menten regulation, 1 < n < 5 is a typical biological range of cooperativity, and n → ∞ is a step-function regulation. If x represses y, the same conditions hold except that the production rate of y then decreases with increasing x. A standard quantitative model for this behavior is called the Hill function, and serves as a good approximation for many regulatory phenotypes in biology, including transcription and translation rates, phosphorylation, enzymatic activity, neuronal firing, and so forth. In general there may also be a leakage rate α 0 that yields constant y production in the absence of x. The concentration of y is also generally removed at a rate β proportional to its concentration. This removal term can represent many biophysical processes, such as degradation, dilution, compartmentalization, or sequestration; for simplicity we mainly use the term "degradation".

---

### Principles of analytic validation of immunohistochemical assays: guideline update [^02cc989d]. Archives of Pathology & Laboratory Medicine (2024). High credibility.

Supplemental Table 4 — AMSTAR risk of bias assessment of included systematic reviews — lists two reviews (Girolami et al, 2022 and Voidazan et al, 2022) with the following item ratings: "A priori design √ √"; "Duplicate study selection & data extraction √ √"; "Comprehensive lit search performed √ x"; "Grey lit used √ x"; "List included & excluded studies x x"; and "Characteristics of included studies provided √ √".

---

### Proximity of iron pnictide superconductors to a quantum tricritical point [^c83a8130]. Nature Communications (2011). Medium credibility.

Ab-initio Landau parameters

From the statistical mechanics point of view, the LDA is a mean-field theory. Thus in the second-order region of the phase diagram, at the LDA level, one should, and indeed does, find classical critical exponents. The order parameter as a function of a non-thermal parameter vanishes as | x–x α | β with β LDA = 1/2 far from the tricritical point and β LDA = 1/4 at the tricritical point. The coefficient of the quadratic term corresponds, irrespective of the phase under consideration, to the inverse of the susceptibility of the non-magnetic phase at momentum (π, 0) or (0, π). Thus, a single parameter α appears in equation (1). As a consistency check, we allowed for different values of α in the fits and found that indeed α converges to practically the same values as a function of doping except for x ~0 and 0.5 where higher order terms in the expansion become important (Fig. 3a). The vanishing of α at a critical doping x α determines the limit of stability of the paramagnet coming from large x. For the experimental lattice constants, we find α ~0.26(x–x α) eV/ μ B 2 with x α = 0.27.

---

### Chiral phonons in quartz probed by X-rays [^06cbf98d]. Nature (2023). Excellent credibility.

Source data

Here, are the phonon eigenvectors of each of the n atoms in the unit cell (normalized such that), andandare eigenvectors corresponding to pure right- and left-handed rotations. The phonon angular momentum (L) is then given by L = ħ S. We also report the mode effective charges (Fig. 5b) as a metric of the strength of the interaction between the mode and light, calculated following the method of ref.

When we match the calculated and measured modes, we find that those with strong dichroic contrast are those calculated to have a large chirality. The peak with the largest contrast is at approximately 50 meV for all the reciprocal points we measured (Q 1 in Fig. 4c and Q 2 = (−0.29, 0.14, 0.32) and Q 3 = (−0.25, 0.25, 0.32) in Supplementary Fig. 2), suggesting that a mode that has large phonon circular polarization and energy around 50 meV dominates the contrast. The mode at the energy of approximately 47.6 meV at Q 1, which we refer to as mode X, matches the conditions (Supplementary Fig. 4 and Supplementary Table 1, which tabulates the energy and phonon circular polarization of all phonon modes at the measured Q points). Figure 5c and Supplementary Video 1 visualize mode X at Q 1 and show that it involves a circular motion of the atoms. Importantly, the mode satisfies the symmetry requirement for a chiral phonon mode.

For non-magnetic quartz, the RIXS spectra at the O K edge are mainly sensitive to the O 2 p orbital states. This means that phonon modes that significantly affect, for example, the orientation of the 2 p orbital states will create large scattering contrast in RIXS and will also be strongly X-ray polarization dependent. Figure 5d and Supplementary Video 2 visualize the evolution of the local charge quadrupoles at the O site when the chiral phonon mode is excited (Fig. 5c or Supplementary Video 1). These charge quadrupoles reflect the time evolution of the O 2 p orbitals, which shows that the dichroic RIXS signal is due to an evolution of the chiral stacking of the O 2 p orbital moments in the chiral phonon excitation as described in equations (1) and (2).

---

### What quantile regression does and doesn' T Do: a commentary on petscher and logan (2014) [^220ddc10]. Child Development (2019). Medium credibility.

Petscher and Logan's (2014) description of quantile regression (QR) might mislead readers to believe it would estimate the relation between an outcome, y, and one or more predictors, x, at different quantiles of the unconditional distribution of y. However, QR models the conditional quantile function of y given x just as linear regression models the conditional mean function. This article's contribution is twofold: First, it discusses potential consequences of methodological misconceptions and formulations of Petscher and Logan's (2014) presentation by contrasting features of QR and linear regression. Second, it reinforces the importance of correct understanding of QR in empirical research by illustrating similarities and differences in various QR estimators and linear regression using simulated data.

---

### How dieting might make some fatter: modeling weight cycling toward obesity from a perspective of body composition autoregulation [^32b99e3d]. International Journal of Obesity (2020). Medium credibility.

Two exponential fits are shown: a generalized linear model (GLM) with 95% confidence intervals (solid line), and a linear model (LM), with R 2 values (dotted line). The four figures correspond to the four methods presented in the main text for computing the value FAT END at the final time END at which the subjects would have completely recovered their initial FFM. The two red squares correspond to the US Army Ranger data points presented in the section on 'Applications of the Model'.

Their exact values depend on the method used for computing the values FAT END when the FFM has been completely recovered and on the type of statistical regression used. As explained previously, we consider four methods for computing FAT END. In Fig. 4, we show the fits obtained from a generalized linear model (GLM) with 95% confidence intervals (solid line) and from a linear model (LM), with R 2 values (dotted line). Following the performance of diagnostics tests to analyze the residuals, it is found that the GLM satisfies the major assumptions of regression analysis better than the LM, especially concerning the normality assumption of the error terms. Nevertheless, as shown in Fig. 4, both models give curves that are close to each other. In simple terms, the main difference between the above LM and GLM approaches to estimate the best parameter values for the constants a and b is the way the error term is handled. In the LM method, we write y = a × e bx × ε, where ε is a random error variable that follows a log-normal distribution with parameters μ = 0 and σ 2. Taking the logarithm on both sides of the equality gives, which corresponds to the linear model, where, x ′ = x, β 1 = b andis a random error variable that follows a normal distribution with mean μ = 0 and variance σ 2. Note that in the LM method, the error term ε is multiplied with the exponential function y = a × e bx. In the GLM method, the error term is instead added to the exponential function. Formally, we write y = a ×e bx + ε, where ε is a random error variable that follows a normal distribution with mean 0 and variance σ 2. As a consequence, the GLM admits the possibility of the value y = 0 (which is excluded in LM) and assumes that the variance V (y) is constant, while the LM method assumes that V (y) varies with x. Finally, note that the LM method admits exact analytical solutions for the model parameters a and b, while the GLM method requires numerical optimization algorithms to find the best values (the maximum likelihood estimates) for a and b.

---

### Spin dephasing under nonlinear gradients: implications for imaging and field mapping [^eaf2803c]. Magnetic Resonance in Medicine (2012). Low credibility.

This work examines the prototypical MR echo that would be expected for a voxel of spins evolving in a strong nonlinear field, specifically focusing on the quadratic z(2) - ½(x(2) + y(2)) field. Dephasing under nonlinear gradients is increasingly relevant given the growing interest in nonlinear imaging, and here, we report several notable differences from the linear case. Most notably, in addition to signal loss, intravoxel dephasing under gradients creating a wide and asymmetric frequency distribution across the voxel can cause skewed and nonlinear phase evolution. After presenting the qualitative and analytical origins of this difference, we experimentally demonstrate that neglecting these dynamics can lead to significant errors in sequences that assume phase evolution is proportional to voxel frequency, such as those used for field mapping. Finally, simplifying approximations to the signal equations are presented, which not only provide more intuitive forms of the exact expression but also result in simple rules to predict key features of the nonlinear evolution.

---

### Principles of analytic validation of immunohistochemical assays: guideline update [^cda8439c]. Archives of Pathology & Laboratory Medicine (2024). High credibility.

Supplemental Digital Content — AMSTAR (Assessing the Methodological Quality of Systematic Reviews) appraisal of methodological quality presents checklist items marked with "√" or "x": "Quality assessed & documented" is "√" and "x"; "Quality used appropriately for conclusion" is "√" and "x"; "Methods to combine used appropriately" is "√" and "√"; "Publication bias assessed" is "x" and "x"; and "COI (conflict of interest)" is "√" and "√". The AMSTAR SCORE /11 is 9 and 5.

---

### Estimating the success of re-identifications in incomplete datasets using generative models [^9c6329d0]. Nature Communications (2019). High credibility.

Theoretical and empirical population uniqueness

For n individuals x (1), x (2), …, x (n) drawn from X, the uniqueness Ξ X is the expected percentage of unique individuals. It can be estimated either (i) by computing the mean of individual uniqueness or (ii) by sampling a synthetic population of n individuals from the copula distribution. In the former case, we havewhere T x = [∃! i, x (i) = x] equals one if there exists a single individual i such as x (i) = x and zero otherwise. T x follows a binomial distribution B (p (x), n). Thereforeand

This requires iterating over all combinations of attributes, whose number grows exponentially as the number of attributes increases, and quickly becomes computationally intractable. The second method is therefore often more tractable and we use it to estimate population uniqueness in the paper.

For cumulative marginal distributions F 1, F 2, …, F d and copula correlation matrix Σ, the algorithm 1 (Supplementary Methods) samples n individuals from q (⋅|Σ,Ψ) using the latent copula distribution. From the n generated records (y (1), y (2), …, y (n)), we compute the empirical uniqueness

Individual likelihood of uniqueness and correctness

The probability distributioncan be computed by integrating over the latent copula density. Note that the marginal distributions X 1 to X d are discrete, causing the inversestoto have plateaus. When estimating p (x), we integrate over the latent copula distribution inside the hypercube:with ϕ Σ the density of a zero-mean multivariate normal (MVN) of correlation matrix Σ. Several methods have been proposed in the literature to estimate MVN rectangle probabilities. Genz and Bretz, proposed a randomized quasi Monte Carlo method which we use to estimate the discrete copula density.

The likelihood ξ x for an individual's record x to be unique in a population of n individuals can be derived from p X (X = x):

Similarly, the likelihoodfor an individual's record x to be correctly matched in a population of n individuals can be derived from. With, the number of potential false positives in the population, we have:

Note that, since records are independent, T follows a binomial distribution B (n − 1, p (x)).

We substitute the expression for ξ x in the last formula and obtain:

---

### Universality of quantum phase transitions in the integer and fractional quantum hall regimes [^c603f914]. Nature Communications (2024). High credibility.

Measurement of localization exponent γ

We now focus on the localized regime, far away from E c, marked by the black rectangle in Fig. 4 a. Given the presence of strong interactions, it is reasonable to assume that transport in this localized part of the energy spectrum proceeds through Efros–Shklovskii (ES) type hopping mechanism. The localization exponent γ determines the T dependence of longitudinal conductance G x x, :withThe pre-factor G 0 ∝ 1/ T and δ ν = (ν − ν c). Figure 4 b shows plots of log(T G x x) versus T −1/2 at different values of δ ν; the dotted lines are linear fits to the data. The linearity of the data at low- T is consistent with transport by the ES hopping mechanism in the FQH regime (Eq. (4)). At high- T (in the region marked in Fig. 4 b by a dotted ellipse), the values of G x x become relatively large, and the plots deviate from a straight line. In passing, we note that as we move progressively closer to the center of the plateau in R x y, where the value of G x x ≈ 0 at low- T, the linearity of the plots persists to higher temperatures. Fitting T 0 (estimated from Eqn (4)) and ∣ δ ν C F ∣ to Eq. (5), we find the estimated γ to lie in the range 2.3−2.6 (Fig. 4 c) for FQH plateau-to-plateau transitions, very close to the predicted range of γ = 2.3−2.5. The fact that the exponent controlling the divergence of the localization length at criticality is almost identical for both FQH and IQH states points to an effective model of localization that is universal across the different statistics of the quasiparticles in these QH phases. Furthermore, from κ = 1/ z γ ≈ 0.41 ± 0.03 and γ ≈ 2.3, we get z ≈ 1, as expected for a strongly interacting system.

---

### Automatic network structure discovery of physics informed neural networks via knowledge distillation [^ee0c8851]. Nature Communications (2025). High credibility.

Similarly, we construct a low-frequency solution to the PDEs (27) with the source term f = 0, ensuring the permutation property of the solution. The permutation property can be simplified using the permutation equivariant groupas ∀ h ∈ H e :The constructed solution is. The low-rank parameter matrix extracted for this low-frequency function is:

Similarly, the relationship between the two-dimensional input x = (x 1, x 2) and the node outputs u a, u b of the final hidden layer can be obtained:From the expressions of both, it follows that:Since the expression of the final output u (x 1, x 2) is:this result contains the symmetry defined in (30).

In order to match the network structure results with the characteristics of the high-frequency solution, by setting the sign of the values in b 1 to be the same, we can obtain:from which we have:thus, the final expression:which satisfies the symmetry of the high-frequency solution.

Moreover, the structure can be adjusted by sign to satisfy other forms of symmetry. For example, after (36), adjusting the sign of the weights in the last hidden layer can yield:that is, the Poisson equation result can be adjusted by sign to satisfy the central symmetry u r (x 1, x 2) = u r (− x 1, − x 2).

The numerical results are shown in Fig. 4 c. The iteration step size used in this problem is 1e−3, which is reduced to 0.2 times the original value at steps 5e3 and 1.5e4. The Ψ -NN re-constructed NN outperforms both PINN and PINN-post models in terms of speed and accuracy. Their L2 errors are summarized in Table 1. All three models exhibit peak errors along the line x 2 = − x 1 + 1, but Ψ -NN maintains a lower overall error, particularly at the boundaries. In contrast, the other models in the control group show large gradient errors at local boundaries, highlighting Ψ -NN's superior performance in high-frequency fitting.

---

### Experimental demonstration of quantum advantage for NP verification with limited information [^776b5b5c]. Nature Communications (2021). High credibility.

Box 1 Protocol for NP verification

Input: Instance of the NP-complete problem and all its relevant parameters: N, δ, etc. after the reduction to a 2-out-of-4 SAT;

Goal: Verification of the solution;

1 Merlin and Arthur jointly perform a pre-calibration of the optical setup, finding the values of the visibility ν N and the transmittivity η;

2 Arthur computes the minimum value of the mean photon number number μ N in order to satisfy the quantum advantage conditions 1–3 and communicates it to Merlin in order to tune the amplitude of his pulses; he also computes the threshold T for accepting a proof;

3 Arthur sends a signal to Merlin to trigger the protocol;

4 Merlin encodes his proof in the phases of the pulses which are then sent to Arthur;

5 Arthur interferes Merlin's pulses with his own and assigns a value x k each time he registers a measurement in the k th pulse:

5.1 x k = 0 for a click in detector D 0 and no click in D 1;

5.2 x k = 1 for a click in detector D 1 and no click in D 0;

5.3 x k is randomly assigned if both detectors click.

6 For all the measured bits that form a clause, Arthur checks the satisfiability;

7 If the number of satisfied clauses is greater than T, Arthur accepts the proof, otherwise he rejects.

Experimental results

We now have all the ingredients to describe the experimental implementation of our verification test and the assessment of the quantum advantage for this task. As we defined previously, we need to satisfy three conditions to show quantum advantage. We need the verification procedure to take time linear in N, to have completeness and soundness such thatand, and that the number of clicks Arthur registers is much smaller than the input size N.

First, as we will see, in our experiment we use indeed a train of coherent pulses of size N and some simple classical post-processing of the measurement results, so our test satisfies condition 1. In fact, the real time to run the verification procedure for N between 5000 and 14,000 was a fraction of a second for the quantum part, a few seconds for the classical post-processing and a couple of minutes for the calibration procedure for each run.

---

### Approximations to the expectations and variances of ratios of tree properties under the coalescent [^8355e038]. G3 (2022). Medium credibility.

Properties of gene genealogies such as tree height (H), total branch length (L), total lengths of external (E) and internal (I) branches, mean length of basal branches (B), and the underlying coalescence times (T) can be used to study population-genetic processes and to develop statistical tests of population-genetic models. Uses of tree features in statistical tests often rely on predictions that depend on pairwise relationships among such features. For genealogies under the coalescent, we provide exact expressions for Taylor approximations to expected values and variances of ratios Xn/Yn, for all 15 pairs among the variables {Hn, Ln, En, In, Bn, Tk}, considering n leaves and 2 ≤ k ≤ n. For expected values of the ratios, the approximations match closely with empirical simulation-based values. The approximations to the variances are not as accurate, but they generally match simulations in their trends as n increases. Although En has expectation 2 and Hn has expectation 2 in the limit as n→∞, the approximation to the limiting expectation for En/Hn is not 1, instead equaling π2/3–2≈1.28987. The new approximations augment fundamental results in coalescent theory on the shapes of genealogical trees.

---

### Perspective on the phase diagram of cuprate high-temperature superconductors [^64c85a82]. Nature Communications (2016). Medium credibility.

Phase diagram of the cuprates based on NMR

In Fig. 2 we plot T c as a function of n d and 2 n p for all cuprates for which we could find both, Cu and O quadrupole splittings in the literature (see Supplementary Tables 1 and 2) with n d and n p calculated from equations (1) and (2). All materials appear in four separate groups, marked by colour: (1) La 2− x Sr x CuO 4; (2) YBa 2 Cu 3 O 6+ y, and other cuprates of that structure, for example, (Ca x La 1− x)(Ba 1.75− x La 0.25+ x)Cu 3 O 6+ y as well as YBa 2 Cu 4 O 8; (3) Bi, Tl and Hg based families; and finally, (4) the two electron-doped systems Pr 2− x Ce x CuO 4 and Nd 2− x Ce x CuO 4. The parent line, that is, the line that is given by n d +2 n p = 1 (bold dashed line) separates hole doped and electron-doped systems. Note that the lines parallel to the parent line are given by n d +2 n p = 1+ x, and represent constant hole (x = +0.1, +0.2) or electron (x = −0.1, −0.2) doping. While there may be material-specific uncertainties, for example, La 2 CuO 4 is not located exactly on the parent line, our straightforward analysis uncovers simple systematic trends concerning all cuprates, and we discuss some salient features now.

---

### Polarized blazar X-rays imply particle acceleration in shocks [^db1f8f0c]. Nature (2022). Excellent credibility.

Table 1
Summary of model properties

First, we find an increasing Π towards higher frequencies. Second, we do not find significant variability during the 2–3-day-long IXPE observations, and finally, we find a rough alignment of ψ with the jet axis from radio to X-rays. Therefore, a shock-accelerated, energy-stratified electron population model satisfies all our multiwavelength polarization observations.

a Slow variability, a few days to a week; moderate variability, days; high variability, less than 1 day.

b There is a slight dependence on the slope of the emission spectrum.

The first IXPE observation of Mrk 501 took place during the period 8–10 March 2022 (100 ks, MJD 59646–59648) and was accompanied by observations across the electromagnetic spectrum from multiple observatories (Methods). IXPE measured a polarization degree of Π X = 10 ± 2% and an electric vector position angle ψ X = 134 ± 5 ο (measured east of north) over the X-ray energy range of 2–8 keV. Contemporaneous millimetre-radio and optical observations (Extended Data Table 2) measured the degree of polarization Π R = 1.5 ± 0.5% along a radio polarization angle ψ R = 152 ± 10 ο and Π O = 4 ± 1% along an optical polarization angle ψ O = 119 ± 9 ο, respectively. A second IXPE observation took place during the period 26–28 March 2022 (86 ks, MJD 59664–59667) yielding Π X = 11 ± 2% along ψ X = 115 ± 4 ο. Simultaneously to the second observation, the optical polarization was measured as Π O = 5 ± 1% along ψ O = 117 ± 3 ο (Extended Data Table 3). The two observed ψ X are consistent within 3 σ. The radio and optical ψ also lie within 3 σ from each other and ψ X. Moreover, the position angle of the jet of Mrk 501 has been determined through Very Long Baseline Array imaging at 43 GHz to be 120 ± 12 ο. This would suggest that, in both cases, radio-to-X-ray ψ is aligned with the jet axis within uncertainties (Fig. 1). We do not find evidence of polarization variability during either IXPE observation. Compared with the archival multiwavelength observations, we find the flux and polarization of Mrk 501 for both observations to be within one standard deviation of the median of the long-term light curves (Fig. 2). Blazars such as Mrk 501 are known to reach X-ray fluxes during outbursts as much as an order of magnitude higher. For the first IXPE observation the measured X-ray flux indicates an average activity state, whereas during the second observation we find evidence of a slightly elevated X-ray flux state. Compared with the historical maximum X-ray flux, during our observations Mrk 501 was a factor of three and a factor of two fainter, respectively.

---

### Inferring time derivatives including cell growth rates using gaussian processes [^d99eaf3f]. Nature Communications (2016). Medium credibility.

Inferring the first and second time derivatives

To determine the time derivative of the data, we use that the derivative of a Gaussian process is another Gaussian process. We can therefore adapt standard techniques for Gaussian process to allow time derivatives to be sampled too.

Building on the work of Boyle, we let g (x) and h (x) be the first and second derivatives with respect to x of the latent function f (x). If f (x) is a Gaussian process then so are both g (x) and h (x). Writing ∂ 1 and ∂ 2 for the partial derivatives with respect to the first and second arguments of a bivariate function, we have

and that

as well as

following ref.

Consequently, the joint probability distribution for y and f *, g * and h * evaluated at points X * is again Gaussian (cf. equation (7)):

where we write K = K (X, X) and K ✱ = K (X *, X *) for clarity.

The covariance function is by definition symmetric: k (x i, x j) = k (x j, x i) from equation (1). Therefore, and so

for all positive integers k and. Consequently, the covariance matrix in equation (13) is also symmetric.

Conditioning on y now gives that the distribution P (f *, g *, h *| X, y, θ, X *) is Gaussian with mean

and covariance matrix

Equation (16) includes equation (9) and shows that

which gives the error in the estimate of the first derivative. Similarly,

is the error in estimating the second derivative.

Using an empirically estimated measurement noise

Although our derivation is given for a Gaussian process where the measurement errors in the data are independent and identically distributed with a Gaussian distribution of mean zero, the derivations are unchanged if the measurement noise has a different s.d. for each time point.

When the magnitude of the measurement noise appears to change with time, we first empirically estimate the relative magnitude of the measurement noise by the variance across all replicates at each time point. We then smooth this estimate over time (with a Gaussian filter with a width of 10% of the total time of the experiment, but the exact choice is not important) and replace the identity matrix, I, in equations (6), (15) and (16) by a diagonal matrix with the relative measurement noise on the diagonal in order to make predictions.

---

### Size limits the sensitivity of kinetic schemes [^71d1065f]. Nature Communications (2023). High credibility.

A different measure of sensitivity — the amplification of a fold-change in the input — provides a solution to this problem. Suppose that for some value x 0 of the input parameter x, scaling by a factor a scales the output by b, so f (a x 0) = b f (x 0). Then the quotientcan be thought of as a discrete approximation of the derivative defining the logarithmic sensitivity. And if f (x) is differentiable everywhere, then by the mean value theorem, there must be a value x * of x for whichThis means that careful measurement of any two points on the input-output curve (x versus f (x)) witnesses the (local, infinitesimal) logarithmic sensitivity somewhere. Importantly — unlike in the case of fitting to a Hill function — if error in the measurements is very low, then they are also telling us the derivative for some value of x very accurately.

Equation (25) leads us to another common definition of the effective Hill coefficient:where S 0.9 and S 0.1 are the values of the input variable (in our case, x) required to get 90% and 10% (respectively) of the maximum value of the output variable (in our case, f (x)). Note that (26) is like (25) with a = S 0.9 / S 0.1 and b = 9. It implies that somewhere between S 0.1 and S 0.9 there is a logarithmic sensitivity of H eff /2.

There is yet another common definition, specific to models of binding. Suppose x is the concentration of a ligand and 〈 n b 〉(x) is the expected number of sites bound by a ligand out of a total of n possible binding sites. It is common then, to takeor to report, as the Hill coefficient, the slope of a line fitted to x versusdata on a log-log plot.

---

### Esotropia and exotropia preferred practice pattern ® [^38cbc533]. Ophthalmology (2023). High credibility.

Esotropia ICD-10 CM codes — entities and laterality rules are specified as follows: Nonaccommodative (unspecified) H50.00; Accommodative H50.43; Alternating H50.05; Alternating with A pattern H50.06; Alternating with V pattern H50.07; Alternating with X or Y pattern (with other noncomitancies) H50.08; Monocular H50.01–; Monocular with A pattern H50.02–; Monocular with V pattern H50.03–; Monocular with X or Y pattern (with other noncomitancies) H50.04–; Intermittent, alternating H50.32; Intermittent, monocular H50.31–; Unspecified H50.00. CM = Clinical Modification used in the United States; = 1, right eye; 2, left eye. For bilateral sites, the final character of the codes indicates laterality. Esotropia and exotropia do not have bilateral codes. Therefore, if the condition is bilateral, assign separate codes for both the left and right side. When the diagnosis code specifies laterality, regardless of which digit it is found in (i.e., 4th digit, 5th digit, or 6th digit), most often you will find: Right is 1 Left is 2.

---

### Detection of haplotype-dependent allele-specific DNA methylation in WGBS data [^a60b696c]. Nature Communications (2020). High credibility.

JSD and uncertainty coefficient

Differences between two PDMs p 1 (x) and p 2 (x) are quantified by means of the JSD D (p 1, p 2), where, This quantity is a normalized metric since it takes values between 0 and 1, is symmetric, and satisfies the triangle inequality. Moreover, it achieves its minimum value of 0 if and only if the two PDMs p 1 (x) and p 2 (x) are identical, whereas it takes its maximum value of 1 when the PDMs do not overlap with each other. In addition, the amount of information that the allele of origin A ϵ {1, 2} conveys about the random methylation state X is measured by means of the uncertainty coefficient Q (X; A), given bywhereis the mutual information between the random methylation state X and the allele of origin A, h (X) is the NME of X without knowing its allele of origin, and N is the number of CpG sites. This quantity takes values between 0 and 1, with larger values indicating that the allele of origin conveys more information about the random methylation state.

Since we are considering diploid organisms, we can take the probability of finding one of the two homologous alleles of a given haplotype in a biological sample to be equal to the probability of finding the other allele; i.e. we can set Pr[A = 1] = Pr[A = 2] = 1/2. In this case, it can be shownthat I (X; A) = D 2 (p 1, p 2), where D (p 1, p 2) is the JSD between the PDMs p 1 (x) = Pr[X = x ∣ A = 1] and p 2 (x) = Pr[X = x ∣ A = 2] of the two homologous alleles. This implies thatwhich shows that, in addition to being proportional to the square JSD between the two PDMs p 1 (x) and p 2 (x) associated with the homologous alleles of a haplotype, the test statistic T PDM used by the CPEL method provides a measure of association between the random methylation state and the allele of origin by means of the uncertainty coefficient. Efficient techniques for evaluating the uncertainty coefficient and, therefore, T PDM, within homozygous and heterozygous haplotypes are discussed in Supplementary Methods, Section 9.

---

### In vivo cell biological screening identifies an endocytic capture mechanism for T-tubule formation [^cf24e3b1]. Nature Communications (2020). High credibility.

Application of the mathematical model to 48 hpf, 5 dpf 10 dpf and adult zebrafish

In order to calculate the surface areas of the T-tubule and sarcolemma at 48 hpf, 5 dpf and 10 dpf, values for fibre length (L) were 90.23 µm, 107.77 µm and 158.83 µm respectively, and were calculated using the regression equation for somite width (y = 0.2 x 3 − 2.6 x 2 + 17 x + 65; Fig. 3f) for values of x = 2, x = 5 and x = 10. Values for maximum fibre diameter (D) were 15.64 µm, 19.23 µm and 26.72 µm respectively (directly measured cross-sectional areas of 192.04 µm 2, 290.42 µm 2 and 560.86 µm 2) taken from the empirically derived data shown in Fig. 3a–c. Note that these calculations were based on maximum fibre size (rather than the mean size or animal age) since new fibres are continuously added to the myotome and existing fibres grow at different rates (see cellularity profile histograms in Supplementary Fig. 3a–c). To apply this model to the largest fibres found in adult zebrafish we took approximate measurements from the zebrafish atlas images at. The largest fibres we found in this data set were approximately 580 µm in length and 40 µm in diameter. Applying values of L = 580 µm and D = 40 µm, gave values of 113083: 75398 µm 2; (T-system: sarcolemma) which corresponds to approximately 1.5x. The graphs in Fig. 3h, i show the consequences of scaling fibre length and diameter up to the maximal values measured from 1 year old fish of L = 580 µm and D = 40 µm (cross-sectional area = 1256.64 µm 2).

In order to find the point at which the surface area of the sarcolemma and the surface area of the T-tubules are equal (the intercept, shown as a dotted line in Fig. 3h, i), we equated the formulae as follows:Solving for L :Solving for D (beginning from Eq. (58)):Applying the quadratic formula gives

---

### Mitigating data bias and ensuring reliable evaluation of AI models with shortcut hull learning [^bebb8ce0]. Nature Communications (2025). High credibility.

Morgan's Canon of data: hierarchical principle for data shortcuts

In comparative psychology, Morgan's Canon posits that if an animal's behavior can be adequately explained by lower-level psychological processes, then it should not be attributed to higher-level processes. Similarly, we can derive Morgan's Canon of data fields. There are two scenarios where shortcuts might occur:
For data providers, the measurable spacerepresents all possible events within a given research task. In this context, remains consistent across different probability spaces, while the ID sample setdiffers. Consider two probability spacesand, with corresponding ID sample setsand, where. Let random variables (X 1, Y 1) and (X 2, Y 2) be defined on these two probability spaces, respectively, and satisfy ∀ ω ∈ Ω, X 1 (ω) = X 2 (ω). If Y 1 (ω) = Y 2 (ω) holds for, then it must also hold for. That is, forand, it holds that. This implies that reducing themay introduce shortcuts. Therefore, to mitigate such shortcuts, data providers must appropriately control the probability measure, which amounts to carefully selectingto ensure that Eq. (10) is not satisfied.
For data users, by contrast, the ID sample setremains fixed across different probability spaces. However, when applying the same data to different tasks, the OOD sample setchanges, implying a change in the sample space Ω. Suppose two probability spacesandshare the same ID sample set, withand. Let (X 1, Y 1) and (X 2, Y 2) be random variables defined on these two respective probability spaces such that ∀ ω ∈ Ω 2, X 1 (ω) = X 2 (ω) and. It follows that σ (X 2) ⊆ σ (X 1), and thus, forand, it holds that. This suggests that enlarging Ω may also introduce shortcuts. Therefore, to avoid these artifacts, data users must apply the dataset within an appropriately defined task — i.e. choose Ω carefully — to prevent Eq. (6) from being satisfied. In practice, the choice of Ω influences the function J in Eq. (12).

---

### A graph homomorphism approach for unraveling histories of metastatic cancers and viral outbreaks under evolutionary constraints [^a0605574]. Nature Communications (2025). High credibility.

Lemma 2

(v, X) ∈ H α if and only if one of the following conditions hold:
α is not labeled and there exist w ∈ X such thatand.
α is labeled, ∣ X ∣ = k, and there exist a permutation (v 1, …, v k) of elements of X such that v ~ v 1, …, v k, and.

Lemma 2 can be straightforwardly used to construct tokens of α from tokens of its children, if α is unlabeled. When α is labeled, then finding a permutation (v 1, …, v k) required by Lemma 2 is more complicated and can be achieved using the approach described next.

Suppose thatis a collection of sets. A vector (x 1, …, x k) is termed a transversal ofif x i ∈ S i and all x i are distinct. Let now. Then the vector (v 1, …, v k) satisfies the condition 2) of Lemma 2 if and only if it is a transversal of.

Given this, the set H α can be obtained by generating all transversals of. This can be done by reducing the problem to finding all maximal matchings in a bipartite graph (a graph consisting of two disjoint vertex sets with edges only between them; a matching is a set of its non-overlapping edges, see Supplementary Note 4 for specific details. The full method is described in Supplementary Algorithm 3.

---

### Counting motifs in the human interactome [^d79dae2d]. Nature Communications (2013). Medium credibility.

where f is a function chosen to decide whether occurs among nodes i 1, i 2, …, i m or not. For the motifs listed in Table 1, their corresponding functions f are given in Supplementary Table S1.

From equations (1) and (3), we have

where n obs is a random variable such that

As the random variables X i are independent and identically distributed, for any 1 ≤ i 1 < i 2 <… < i m ≤ n, we also have

Hence, by equation (2),

By conditioning on the event that X 1 = X 2 = ··· = X m = 1, we rewrite equation (4) as

where Z ∼Binomial(n − m, p), and hence

As

we have

by applying integration by parts and simplification. Therefore, we have obtained the following theorem.

Theorem 1: Let be a network of n nodes. Assume obs is a subnetwork of obtained by a uniform node sampling process that selects a node with probability p. For any motif of m nodes, the estimatordefined in equation (1) satisfies equation (5). Therefore, is an asymptotically unbiased estimator for N in the sense thatas n goes to infinity. Moreover, the convergence is exponentially fast in n.

When the estimator (1) is applied to estimate the number of links in an undirected network, the variance has the following closed-form expression:

where N 1 and N 2 are, respectively, the number of links and three-node paths in (Supplementary Methods). This leads to our next theorem.

Theorem 2: When is generated from one of the ER, preferential attachment, duplication or geometric models, as n goes to infinity.

---

### Article 5. An introduction to estimation – 2: from z to T [^fb82b806]. Emergency Medicine Journal (2001). Low credibility.

Provided the sample size is large enough (that is, n greater than 100), the z statistic can be used to determine the confidence interval estimation of the population mean even when the sigma is not known. In these cases the estimation of the standard error of the mean is used. The z statistic is also valid when determining the population's proportion based upon a large sample. However, when dealing with smaller samples, the z statistic is replaced by the t statistic. This makes it possible to estimate, in a population with an unknown standard deviation: The probability of getting a sample mean greater than or equal to a particular value The value of a sample mean with a particular probability of occurring The probability of getting a sample mean between two particular values The confidence interval for the estimation of the population mean can also be determined using the t statistic.

---

### Improved security bound for the round-robin-differential-phase-shift quantum key distribution [^68502abf]. Nature Communications (2018). Medium credibility.

Theorem. For the RRDPS protocol with L-pulse packet, where each packet contains N photons (L ⩾ N + 1), Eve's information can be bounded bywhere, non-negative real parameters x i satisfying. Moreover, if the error rate of raw key bits is E, these parameters x i (i ∈ {1, 2. N + 1}) must satisfy the constraintand,

Corollary. If the photon-number L ≤ N − 2, I AE < 1 always holds.

Based on this theorem, the upper bound of I AE is generalized to find the maximum value of a given function under a constraint defined by E. If we ignore this constraint, we obtainwithout monitoring signal disturbance. Alternatively, if we retain this constraint, a tighter estimation may be achieved. It is remarkable that searching such a maximum value can be effective and concise through a numerical method, since its function is convex. A different, less tight bound improvement was recently reported in ref.

Potential improvements made by our theory

For a QKD protocol, the mutual information between Alice and Bob is given by I AB = 1 − h 2 (E). Thus, there is a maximum value E max of error rate E, which satisfies, when E = E max. Obviously, ifholds, I AB will be no larger than, and no secret key bits can be generated. Thus, E max is the maximum value of tolerable error rate of a QKD protocol. We first compare E max of RRDPS between the original method and our new formulae. In Table 1, we list the results for the cases in which a single-photon source is equipped. One can see that our formulae can increase E max, especially when L is small. It is remarkable to note that for the case L = 3, with our formulae E max can be up to 5%, while the original RRDPS protocol cannot generate secure key bits at all. More importantly, the notable difference between columns two and three in Table 1 implies that our theory leads to increased E max compared to the original RRDPS even when signal disturbance monitoring is still turned off.

---

### Vacuum laser acceleration of super-ponderomotive electrons using relativistic transparency injection [^d98e7d9f]. Nature Communications (2022). High credibility.

Test-particle simulation and Poincare-map analysis

The simulation solves the electron dynamics by directly integrating the relativistic equations of motion: where the vectors denote the three Cartesian components and the quantities are normalized as. A perfect light reflector is defined at x = 0 to mimic the foil before the onset of RT. As a result, the laser fields in front of the foil (x < 0) include both the incident laserand the reflected laser, wheredefine the temporal and spatial profiles, H is the Heaviside step function, H 2 = 1 – H, ω 2 is the angular frequency of the reflected laser normalized to that of the incident laser, and the initial phases satisfy. The pulse duration τ and spot radius σ follow that of the PIC simulation. The parameter t d introduces a time delay for the incident laser to arrive at the foil, thus t d < 0 means the laser has impinged on the foil for a period of |t d |. To obtain the Poincare map shown in Fig. 6c, we transform the independent variable of the above equations of motion from t to ξ = ξ 1 + ξ 2 where ξ 1 = t − x + ϕ 1 and ξ 2 = ω 2 (t + x) + ϕ 2 are the phase of the incident and reflected laser, respectively. The resulting equations of motion become, and are solved only at periodic ξ = 2Nπ where.

---

### Hand size influences optimal grip span in women but not in men [^17a9d839]. The Journal of Hand Surgery (2002). Low credibility.

This study investigates which position (grip span) on the standard grip dynamometer results in maximum grip strength. Our null hypotheses included (1) no optimal grip span exists for measuring grip strength and (2) optimal grip span is unrelated to hand size. We also intended to derive a simple mathematical algorithm to adapt grip span to hand size. Seventy healthy subjects (40 women/30 men; mean age, 40 years; range; 20–80 years) free of upper-limb lesions were evaluated. Each hand was randomly tested on 10 occasions using 5 different grip spans. Our findings showed that (1) optimal grip span was identified for both genders and (2) hand size and optimal grip span correlated in women but not in men. When measuring handgrip strength in women, hand size must be taken into consideration. We provide a mathematical equation (y = x/5 + 1.5 cm) to adapt optimal grip span (y) to hand size (x) in women. In adult men, optimal grip span can be set at a fixed value (5.5 cm).

---

### Revealing the range of equally likely estimates in the admixture model [^4446a58e]. G3 (2025). Medium credibility.

Many ancestry inference tools, including structure and admixture, rely on the admixture model to infer both, allele frequencies p and individual admixture proportions q for a collection of individuals relative to a set of hypothetical ancestral populations. We show that under realistic conditions the likelihood in the admixture model is typically flat in some direction around a maximum likelihood estimate (q^, p^). In particular, the maximum likelihood estimator is non-unique and there is a complete spectrum of possible estimates. Common inference tools typically identify only a few points within this spectrum. We provide an algorithm which computes the set of equally likely (q~, p~), when starting from (q^, p^). It is analytic for K = 2 ancestral populations and numeric for K > 2. We apply our algorithm to data from the 1000 genomes project, and show that inter-European estimators of q can come with a large set of equally likely possibilities. In general, markers with large allele frequency differences between populations in combination with individuals with concentrated admixture proportions lead to small areas with a flat likelihood. Our findings imply that care must be taken when interpreting results from STRUCTURE and ADMIXTURE if populations are not separated well enough.

---

### Determination of X-ray detection limit and applications in perovskite X-ray detectors [^2d9b8b03]. Nature Communications (2021). High credibility.

In the dark current (I dark & s) method, no measurement of photocurrent under X-ray I X–ray is needed as our pre-assumptions set the standard deviation σ 1 of I X–ray equals to the dark current's standard deviation σ 2, and fix the values of n 1 and n 2. Practically, I X–ray can be measured to obtain its standard deviation σ 1 (i.e.), sample size n 1 (i.e.), and mean value (i.e.). In combination with the measured dark current's standard deviation σ 2 (i.e.), sample size n 2 (i.e.), and mean value (i.e.), we can perform a posterior check of the detectability (we use term " posterior " because this examination is performed after the measurement of I X–ray and I dark). Specifically, with n 1, n 2, determined by measurement, the detection limitcan be calculated from the DL equations. Besides, a difference of the measured two mean valuescan also be obtained. We compare the calculated detection limit Δ with the measured mean value difference. If, the photocurrent under X-ray I X–ray is detected with the preset false positive probability α and the false negative probability β satisfied, otherwise the preset false positive and false negative probability cannot be satisfied. Following the principles above, we can successively lower the X-ray dose rate used to generate the I X–ray until I X–ray cannot be detected against the dark current. The lowest X-ray dose rate that generates the smallest detectable I X–ray is taken as the dose rate detection limit. Although this method needs the measurement of I dark and I X–ray, we simplify the name by calling it the X-ray photocurrent method (I dark & I X–ray method).

---

### The minimal work cost of information processing [^894e044e]. Nature Communications (2015). Medium credibility.

where the sum ranges only over those x that have a non-zero probability of occurring. In the case of deterministic mappings p (x ′| x)∈{0,1}, this corresponds to the maximum number of input states that map to a same output state. For the AND gate, provided all four states 00, 01, 10 and 11 have non-negligible probability of occurring, there are three input states mapping to the same output state, so (3) gives us simply. Also, in simple examples as considered here, the expression (3) is stable to considering an-approximation (Supplementary Note 4); this quantity is thus physically justified.

Crucially, our result reveals that the minimal work requirement in general depends on the specific logical process, and not only on the input and output states. This contrasts with traditional thermodynamics for large systems, where the minimal work requirement of a state transformation can always be written as a difference of a thermodynamical potential, such as the free energy. For example, the minimal work cost of performing specifically an AND gate may differ from that of another logical process mapping an input distribution (p 00, p 01, p 10, p 11) (with ∑ i p i = 1) to the distribution (p ′ 0, p ′ 1) = (p 00 + p 01 + p 10, p 11) (Recall that the classical counterpart of a quantum state is a probability distribution.). To see this, consider the XOR gate, which outputs a 1 exactly when both inputs are different (see Fig. 2b). The minimal work cost requirement of this gate, as given by (3), is now only kT ln 2, as in the worst case, only a single bit of information is erased (again supposing that all four input states have non-negligible probability of occurring). Now, suppose that, for some reason, the input distribution is such that p 01 + p 10 = p 11, that is, the input 11 occurs with the same probability as of either 01 or 10 appearing. Then, the XOR gate reproduces the exact same output distribution as the AND gate: in both cases, we have p ′ 0 = p 00 + p 10 + p 01 = p 00 + p 11 and p ′ 1 = p 11 = p 01 + p 10. In other words, both logical processes have the same input and output state, yet the XOR gate only requires work kT ln 2 compared with the AND gate, which requires 1.6 kT ln 2. Furthermore, we point out that this difference, which appears small in this case, may be arbitrarily large in certain scenarios (Supplementary Note 4).

---

### Bacterial motility can govern the dynamics of antibiotic resistance evolution [^c771c187]. Nature Communications (2023). High credibility.

Analytical and numerical techniques

In addition to computer simulations, we developed a combination of analytical and numerical techniques, which are used to compute the evolutionary time, ecological time and the adaptation rate. In contrast to simulations, these techniques are based on closed-form solutions of master equations, numerical methods for solving algebraic equations (such as Newton-Raphson method) and solving ODEs (such as Runge-Kutta method). To illustrate these techniques, we show how to compute the expected waiting times at a resistance state R and the adaptation rate a R in the original staircase model.
Find the wild-type profile N x at a resistance state R. Use Newton-Raphson method to solve the algebraic equations for the non-trivial fixed point of the mean-field theorywhereis the indicator function.
Compute the evolutionary time. A closed-form solution exists and follows from a modification of the theory in Hermsen et al. We consider three independent adaptation paths along which wild-type can produce a first mutant in the overlap region x = R + 1. On the position-genotype lattice (x, g), they are the division path D: (R, R) → (R, R + 1) → (R + 1, R + 1) (mutation at x = R followed by migration to the right); the overlap path O: (R + 1, R) → (R + 1, R + 1) (mutation at x = R + 1); and the no-division path N: (R + 2, R) → (R + 2, R + 1) → (R + 1, R + 1) (mutation at x = R + 2 followed by migration to the left). As the original theoryassumes low motility, it neglects the no-division path since there are no wild-type cells to mutate at position x = R + 2 in this regime (SI Theorem 3). Moreover, we assume that the flux of mutants between neighbouring compartments in the division region x ≤ R and the no-divison region x ≥ R + 2 is equilibrated, so that we can ignore the net probability flux of mutants that enter or exit the compartments x = R − 1, R, R + 1 and also ignore longer paths such as (R − 1, R) → (R − 1, R + 1) → (R, R + 1) → (R + 1, R + 1) (mutation at x = R − 1 followed by double migration to the right). This is a reasonable assumption as the flux of mutant-producing wild-type is equilibrated far from the population front. The computation ofhas the following structure. The waiting times till a mutant is produced in the overlap region along path i ∈ { D, O, N } are denoted by T i (Fig. 2 a). The probability that the waiting will be longer than t isand the probability density function of T i is given by F i (t) = −d S i /d t. Notice thatand that T i are independent. Thus. The probability density function ofis The expected timeis found by numerical integration. Therefore, the problem is solved by finding the functions S i (t) and differentiating them to produce F i (t). According to Hermsen et al. these functions are: For path D: whereandwhere 〈 n (t)〉 D is the average mutant population in compartment x = R at time t For path O: For path N: whereandwhere 〈 n (t)〉 N is the average mutant population in compartment x = R + 2 at time t Finally, if there is a nonnegligible chance that the first mutant in the overlap region dies before division, the evolutionary timemust be corrected to, where q is the probability that the first mutant in the overlap region divides before its death (Supplementary Note 8).
Compute the ecological time. The competition between wild-type and mutants can be described well by the mean-field equations:with initial conditions at time t = 0:In these equations w x and m x stand for the number of wild-type and mutant cells in spatial compartment x, respectively. The system is simulated from the end state of the previous stochastic process when a first mutant appears in the overlap region. This system can be simulated efficiently by a Runge-Kutta (RK4) method. The timeis reached when w R +1 < m R +1 for the first time, i.e. the mutants outcompete the wild-type in the overlap region.
Find the adaptation rate. The adaptation rate is found from

---

### Do CIs give you confidence? [^0c65ec35]. Chest (2012). Low credibility.

This article describes the conceptual basis for the P value and the CI. We show that both are derived from the same underlying concepts and provide useful, but similar information.

---

### Quantifying dissipation using fluctuating currents [^2479585e]. Nature Communications (2019). High credibility.

Suppose, however, that it is not simple to measure the heat flux. Rather, we imagine directly observing the bead positions as a function of time. Those measurements are sufficient to extract the entropy production rate, but to do so we must go beyond the thermodynamics and explicitly consider the system's dynamics, an approach known as stochastic thermodynamics. The starting point is to mathematically describe the bead-spring dynamics with a coupled overdamped Langevin equation, where x = (x 1, x 2) T is the vector consisting of each bead's displacement from its equilibrium position, ξ = (ξ 1, ξ 2) T is a vector of independent Gaussian white noises, andThe matrix A captures deterministic forces acting on the beads due to the springs, while F describes the random forces imparted by the medium. The strength of these random forces depends on the temperature and the Boltzmann constant k B, consistent with the fluctuation-dissipation theorem.

It is useful to cast the Langevin equation as a corresponding Fokker-Planck equation for the probability of observing the system in configuration x at time t, ρ (x, t):with D = FF T /2. Though we are modeling a two-particle system, it can be helpful to think of the entire system as being a single point diffusing through x space with diffusion tensor D and with deterministic force γ Ax. The second equality in Eq. (3) defines the probability current j (x, t). These probability currents (and their fluctuations) will play a central role in our strategies for inferring the rate of entropy production.

Due to its analytic and experimental tractability, this bead-spring system and related variants have been extensively studied as models for nonequilibrium dynamics –. In particular, the steady-state properties are well-known. Correlations between the position of bead i at time 0 and that of bead j at time t are given by C ij (t) = 〈 x i (0) x j (t)〉. The expectation value is taken over realizations of the Gaussian noise to giveThe steady-state density and current are expressed simply asin terms of the long-time limit of the correlation matrix

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^e2a04562]. Nature Communications (2021). High credibility.

With DDEs, multiple steps ("cascades") within a network can be rigorously simplified into a single step with delay (see below), an approach which has been explored in a variety of biological contexts. This makes interpretation of the phenomenology simpler than with ODEs and reduces the number of equations and parameters in the model. This idea can be visualized by depicting regulatory networks as directed graphs, with nodes representing biological species, and pointed vs. blunt arrows indicating activation vs. repression, respectively (Fig. 1). DDE models allow a single arrow to faithfully capture many biochemical steps, expanding the available dynamics in a model with a reduced set of equations and parameters. DDE regulatory models have in fact been used widely to model a range of biological phenomena such as development, and hematopoiesis. For instance, a 1-variable ODE such as Eq. (1) can only produce exponential growth or decay. A corresponding DDE, such as Eq. (2), can also oscillate (with amplitude approaching zero or infinity) and, if nonlinear, lead to stable oscillations, bistability and chaos.

A key challenge in using DDEs is their mathematical complexity relative to ODEs. For example, while Eq. (1) is 1-dimensional because one initial condition (x at t = 0) determines all future behavior, Eq. (2) is infinite-dimensional, because x must be specified at all times − τ ≤ t ≤ 0 to predict a unique solution. Concretely, the solution to Eq. (1) can be written as a single exponential, while Eq. (2) generally must be specified as a sum of an infinite number of exponential terms to satisfy initial conditions.

Despite the challenges, much progress has been made in analytical understanding of DDEs, and numerical methods exist for simulation. We thus see an opportunity to use DDEs to recapitulate dynamics found in ODE solutions of network motif behavior with fewer genes and thus fewer modeling parameters and equations (Fig. 1), a type of "modeling simplicity".

---

### Long-term memory induced correction to arrhenius law [^495130a6]. Nature Communications (2024). High credibility.

Fig. 4
Check of the approximations of the theory.

a Check of the stationary covariance approximation (i.e.): comparison between ψ π (t) = Var(x π (t)) measured in numerical simulations (symbols) and ψ (t) (dashed line: H = 3/8, full line H = 1/4). b Check of Eq. (9): comparison between the value m π (t) in simulations (symbols) and x 0 ϕ (t) (full line: x 0 = l /2 for H = 3/8; dashed line x 0 = l for H = 1/4). Note that m π (t) ≃ x 0 ϕ (t) is expected at large times only. c Check of the short-time scaling regime for H = 3/8. d Check of the long-time scaling regime (11) for H = 3/8. In a, c, d, the initial position is drawn from an equilibrium distribution, corresponding to our predictions for x 0 = 0. When present, error bars represent 68% confidence intervals.

General results

This approach first shows unambiguously that the mean FPT is finite. Indeed, we show in SI (Section B) that the solution to Eq. (8) satisfies at long timeswhich can be checked directly in numerical simulations, see figure 4b. This scaling, together with Eq. (6), shows that the mean FPT is finite. This contradicts the results obtained with the generalized Fokker–Planck equationor with the Wilemski-Fixman approximation. The latter amounts to assuming that the process is at all times in an equilibrium state, and would thus yield m π (t) ≃ L ϕ (t), leading to an infinite mean FPT when α < 1 (as noted earlier in a similar, but out of equilibrium, situation). Beyond this proof of finiteness, our approach yields a quantitative determination of 〈 T 〉 by solving numerically the integral equation (8) for m π (t) and next using Eq. (6); this shows quantitative agreement with numerical simulations in Fig. 3.

---

### Experimental demonstration of quantum advantage for one-way communication complexity surpassing best-known classical protocol [^24e17a73]. Nature Communications (2019). High credibility.

From the above equation, we see that D 0 clicks only if x k ⊕ x l = 0, and D 1 clicks otherwise. Now if Bob gets clicks at multiple time slots, he picks arbitrarily one of these time slots and outputs the pair 〈(k, l) ∈ σ i, b = x k ⊕ x l 〉 depending on which detector clicked. The only way he can output an incorrect parity value is if he does not observe any click during the entire run of the protocol, which happens with probability p 0 = exp(−| α | 2), in which case he outputs a random choice. Thus his error probability is.

In a practical setting, and following the same model for experimental imperfections as for Sampling Matching, the incoming state becomes, and the output state is now written as,

From the above equation, we see that the probability that there is a click in the correct detector is, while the probability that the wrong detector clicks is,

Let us now consider the cases where Bob can output an incorrect parity value outcome. (i) He does not observe any single click over the entire run of the experiment. The probability of this happening is p ¬1 = (1 − p 1) n /2, where p 1 = p c (1 − p w) + p w (1 − p c) is the probability of observing a single click in one time slot. In this case, he outputs a random parity value. (ii) Bob observes at least one single click within all time slots. He then randomly chooses any one of those to output the parity value. The probability that he outputs the wrong parity value is. From these two cases, we find that Bob's error probability is

---

### Adaptation is maintained by the parliament of genes [^0dcf551f]. Nature Communications (2019). High credibility.

Invasion of a mutant trait distorter

We ask when a mutant trait distorter (D 2) will invade against a resident trait distorter (D 1) that is unsuppressed and at fixation (k ≠). We write recursions detailing the generational frequency changes in the six possible gametes, D 0 / S 0, D 0 / S 1, D 1 / S 0, D 1 / S 1, D 2 / S 0, D 2 / S 1, with current generation frequencies denoted, respectively by x 00, x 01, x 10, x 11, x 20, x 21, and next-generation frequencies denoted with an appended dash (′):whereis the average fitness of individuals in the current generation, and equals the sum of the right-hand side of the system of equations. The mutant trait distorter can invade when the equilibrium given by x 00 ✱ = 0, x 01 ✱ = 0, x 10 ✱ = 1, x 11 ✱ = 0, x 20 ✱ = 0, x 21 ✱ = 0 is unstable, which occurs when the leading eigenvalue of the Jacobian stability matrix for this equilibrium is > 1. Testing for stability in this way, we find that, if the mutant trait distorter is weaker than the resident, it can never invade. If the mutant trait distorter is stronger than the resident, it invades from rarity when Δ t (1 − c trait) > Δ c trait, where Δ t = t − t (k), Δ c trait = c trait − c trait (k).

---

### Estimation of the risk of a qualitative phenotype: dependence on population risk [^4a647c65]. Journal of Human Genetics (2017). Low credibility.

Individual disease risk estimated based on the data from single or multiple genetic loci is generally calculated using the genotypes of a subject, frequencies of alleles of interest, odds ratios and the average population risk. However, it is often difficult to estimate accurately the average population risk, and therefore it is often expressed as an interval. To better estimate the risk of a subject with given genotypes, we built R scripts using the R environment and constructed graphs to examine the change in the estimated risk as well as the relative risk according to the change of the average population risk. In most cases, the graph of the relative risk did not cross the line of y = 1, thereby indicating that the order of the relative risk for given genotypes and the population average risk does not change when the average risk increases or decreases. In rare cases, however, the graph of the relative risk crossed the line of y = 1, thereby indicating that the order of the relative risk for given genotypes and the population average risk does change owing to the change in the population risk. We propose that the relative risk should be estimated for not only the point average population risk but also for an interval of the average population risk. Moreover, when the graph crosses the line of y = 1 within the interval, this information should be reported to the consumer.

---

### Holimap: an accurate and efficient method for solving stochastic gene network dynamics [^ccf3e72b]. Nature Communications (2024). High credibility.

Methods

Determining the effective parameter for the LMA

For the linear network in Fig. 2 b, the evolution of moments is governed byInserting Eq. (2) into Eq. (5) gives a closed set of moment equations, from which the values of g 0, μ 1,1, and μ 1,0 can be computed approximately. Finally, using these values, the effective parametercan be obtained from Eq. (2). The remaining steps for the LMA are the same as for the 2-HM.

Determining the effective parameters for the 4-HM

For the autoregulatory circuit, it follows from Eq. (1) thatFor the linear network in Fig. 2 d, the evolution of moments is governed byFrom these equations, it is easy to show thatMatching Eqs. (6) and (8), we find thatandshould satisfy the following system of linear equations:Matching the first and third identities in Eqs. (1) and (7), it is clear thatandshould satisfy the following system of linear equations:wherehas been determined by solving Eq. (9). Compared with Eq. (4), Eq. (10) has an additional term. This is because ρ b remains unchanged for the 2-HM but is changed for the 4-HM.

Finally, inserting Eqs. (9) and (10) into Eq. (7) gives a system of closed moment equations and hence the values of all zeroth, first, and second-order moments can be approximately calculated. Substituting these moments into Eqs. (9) and (10), one can finally solve for the four effective parameters, andof the linear network. The 4-HM predicts the protein distribution of the nonlinear network by solving the CME of the linear one in Fig. 2 d with the values of the effective parameters found above.

---

### An analytical theory of balanced cellular growth [^98f39440]. Nature Communications (2020). High credibility.

Without dependent reactants, A is a square matrix with a unique inverse A −1, and x ≡ [P, a] T is the corresponding vector of independent concentrations. Multiplying both sides of the mass balance constraint (1) by A −1, we obtain (Theorem 5)The right-hand side of the mass balance constraint (1) quantifies how much of each component x i needs to be produced to offset the dilution that would otherwise occur through the exponential volume increase at rate μ.quantifies the proportion of flux v j invested into offsetting the dilution of component i, and we thus name A −1 the investment (or dilution) matrix; see Supplementary Fig. 1 for examples. In contrast to the mass-normalized stoichiometric matrix A, which describes local mass balances, A −1 describes the structural allocation of reaction fluxes into offsetting the dilution of all downstream cellular components, carrying global, systems-level information.

From the kinetic equation (Eq. (3)), p j = v j / k j (a), and inserting v j from the investment equation (Eq. (4)) giveswhere ∑ i sums over the total protein and individual reactant concentrations (Theorem 6). Substituting these expressions into the total protein sum (Eq. (2)) and solving for μ results in the growth equation (Theorem 7)

As detailed in "Methods" (Theorems 5–7), a corresponding result also holds for BGSs with dependent reactants. Thus, for any active matrix A with full column rank (in particular for all active matrices of EGSs) and for any corresponding concentration vector x, there are unique and explicit mathematical solutions for the fluxes v, individual protein concentrations p, and growth rate μ. If μ (Eq. (6)) and all individual protein concentrations p j (Eq. (5)) are positive, the cellular state is a BGS; otherwise, no balanced growth is possible at these concentrations.

---

### Emergent second law for non-equilibrium steady States [^eeee434e]. Nature Communications (2022). High credibility.

The Gibbs distribution universally characterizes states of thermal equilibrium. In order to extend the Gibbs distribution to non-equilibrium steady states, one must relate the self-information [Formula: see text] of microstate x to measurable physical quantities. This is a central problem in non-equilibrium statistical physics. By considering open systems described by stochastic dynamics which become deterministic in the macroscopic limit, we show that changes [Formula: see text] in steady state self-information along deterministic trajectories can be bounded by the macroscopic entropy production Σ. This bound takes the form of an emergent second law [Formula: see text], which contains the usual second law Σ ≥ 0 as a corollary, and is saturated in the linear regime close to equilibrium. We thus obtain a tighter version of the second law of thermodynamics that provides a link between the deterministic relaxation of a system and the non-equilibrium fluctuations at steady state. In addition to its fundamental value, our result leads to novel methods for computing non-equilibrium distributions, providing a deterministic alternative to Gillespie simulations or spectral methods.

---

### Abstract representations of events arise from mental errors in learning and memory [^748e9b84]. Nature Communications (2020). High credibility.

Estimating parameters and making quantitative predictions

Given an observed sequence of nodes x 1, …, x t −1, and given an inverse temperature β, our model predicts the anticipation, or expectation, of the subsequent node x t to be. In order to quantitatively describe the reactions of an individual subject, we must relate the expectations a (t) to predictions about a person's reaction timesand then calculate the model parameters that best fit the reactions of an individual subject. The simplest possible prediction is given by the linear relation, where the intercept r 0 represents a person's reaction time with zero anticipation and the slope r 1 quantifies the strength with which a person's reaction times depend on their internal expectations.

In total, our predictionscontain three parameters (β, r 0, and r 1), which must be estimated from the reaction time data for each subject. Before estimating these parameters, however, we first regress out the dependencies of each subject's reaction times on the button combinations, trial number, and recency using a mixed effects model of the form 'RT~log(Trial)*Stage+Target+Recency+(1+log(Trial)*Stage+Recency|ID)', where all variables were defined in the previous section. Then, to estimate the model parameters that best describe an individual's reactions, we minimize the RMS prediction error with respect to each subject's observed reaction times, where T is the number of trials. We note that, given a choice for the inverse temperature β, the linear parameters r 0 and r 1 can be calculated analytically using standard linear regression techniques. Thus, the problem of estimating the model parameters can be restated as a one-dimensional minimization problem; that is, minimizing RMSE with respect to the inverse temperature β. To find the global minimum, we began by calculating RMSE along 100 logarithmically spaced values for β between 10 −4 and 10. Then, starting at the minimum value of this search, we performed gradient descent until the gradient fell below an absolute value of 10 −6. For a derivation of the gradient of the RMSE with respect to the inverse temperature β, we point the reader to the Supplementary Discussion. Finally, in addition to the gradient descent procedure described above, for each subject we also manually checked the RMSE associated with the two limits β → 0 and β → ∞. The resulting model parameters are shown in Fig. 4 a, b for random walk sequences and Fig. 4 g, h for Hamiltonian walk sequences.

---

### Derivation of a mathematical expression for predicting the time to cardiac events in patients with heart failure: a retrospective clinical study [^107f84b8]. Hypertension Research (2013). Low credibility.

The prognoses for patients with certain diseases are estimated by averaging the results of clinical trials. To investigate the possibility of deriving a mathematical formula for the estimation of prognosis, we formulated the equation τ = f(x1, …, xp), where x1, …, xp are clinical features and τ represents the clinical outcome for heart failure (HF). We attempted to determine the function to mathematically formulate the relationship between clinical features and outcomes for these patients. We followed 151 patients (mean age: 68.6 ± 14.6 years; men: 61.6%) who were consecutively hospitalized and discharged as a result of acute decompensated HF (ADHF) between May 2006 and December 2009. The mathematical analysis was performed through a probabilistic modeling of the relational data by assuming a Poisson process for rehospitalization owing to HF and by linearly approximating the relationship between the clinical factors and the mean elapsed time to rehospitalization. The former assumption was validated by a statistical test of the data, and the contribution of each parameter was assessed based on the coefficients of the linear relation. Using a regularization method to analyze 402 clinical parameters, we identified 252 factors that substantially influenced the elapsed time until rehospitalization. With the probability model based on the Poisson process, the actual (X; 388 ± 377 days) and estimated (Y; 398 ± 381 days) elapsed times to rehospitalization were tightly correlated (Y = 1.0076X+6.5531, R(2) = 0.9879, P < 0.0001). We established a mathematical formula that closely predicts the clinical outcomes of patients who are hospitalized with ADHF and discharged after appropriate treatment.

---

### Trend algorithms… [^b736a419]. seer.cancer.gov (2025). Medium credibility.

\}{Initial \textbf{ } Rate} }\right) \times {\Large10 }\). \ } \) \ } \). y = mx + b \ \) Because the methods used in the calculation of PC and APC are not directly related, it is possible that the signs of the PC and the APC may disagree. See Calculating APC with Weighting for the formulas used in calculating Weighted APCs.

\\right)}
- 1 }\right) \times {\Large100 }\) Where \\) is the inverse of the. t distribution function evaluated at \ and with \ degrees of freedom. Note: In SEERStat, you specify the p-value as opposed to the confidence interval. The confidence interval = x 100. Therefore, a 95% confidence interval is specified by p * = . 05.

---

### Casein [^fd0595f4]. FDA (2009). Low credibility.

Volume desired x Concentration desired = Volume needed x Concentration available.

Example 1: If a 1:10 w/v extract is available and it is desired to use a 1:1,000 w/v extract substitute as follows:

Vd x Cd = Vn x Ca

10ml x 0.001 = Vn x 0.1

0.1 mL = Vn

Using a sterile technique, remove 0.10 mL of extract from the 1:10 vial and place it into a vial containing 9.90 mL of sterile diluent. The resulting ratio will be a 10 mL vial of 1:1,000 w/v.

Example 2: If a 10,000 pnu/ml extract is available and it is desired to use a 100 pnu/ml extract substitute as follows:

10ml x 100 = Vn x 10,000

0.1 mL = Vn

Using a sterile technique, remove 0.10 mL of extract from the 10,000 pnu/ml vial and place it into a vial containing 9.90 mL of sterile diluent. The resulting concentration will be a 10 mL vial of 100 pnu/ml.

Example 3: If a 10,000 AU/ml or BAU/ml extract is available and it is desired to use a 100 AU/ml or BAU/ml extract substitute as follows: Vd x Cd = Vn x Ca

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^dee6cacc]. CDC (2011). Medium credibility.

Community viral load sample size calculations — sample size to detect differences in geometric mean (GM) viral load depends on power and standard deviation (S). The table specifies "α = 0.05 and W = 0.8", with S columns 1 to 1.5; for k = 3 the minimum sample sizes by S are 54, 66, 78, 92, 106, and 122, and "1.2 is the standard deviation of national VL data", so at S = 1.2 the needed sample size is 78.

---

### Discovery of long-range inhibitory signaling to ensure single axon formation [^6b48407b]. Nature Communications (2017). Medium credibility.

Mathematical model for the diffusion of Rho-kinase along the neurites

The dynamics of the spatial distribution of Rho-kinase along a neurite were described as follows:with boundary conditions of C (x = 0, t) = C s (t) and ∂ C /∂ x| x = L = 0, where t, x, L, C, C s, D and k indicate time, the space coordinate along a neurite, the length of the neurite, the concentration of Rho-kinase along the neurite, the concentration of Rho-kinase in the cell body, the diffusion rate, and the inactivation and degradation rates, respectively. The time-dependent concentration of Rho-kinase in the soma is represented by C s (t) = C o P (t), where C o and P (t) indicate the Rho-kinase concentrations in the cell body during photoactivation and the time course of photoactivation, in which the binary variables 0 and 1 represent without and with photoactivation, respectively.

Notice that this model did not include active transport of Rho-kinase from the cell body to neurite tips, in contrast to previous models. According to our previous theoretical work, actively transported molecules are more accumulated in longer neurites. If Rho-kinase is so, growth of longer neurites is more inhibited by active Rho-kinase. This is inconsistent with our experimental finding that growth of shorter neurites is more inhibited by photoactivation of Rho-kinase (Fig. 3g, h). Moreover, in our imaging, we did not observe active transport of Rho-kinase. Thus, we did not take account for active transport of Rho-kinase.

The steady state distribution of Rho-kinase along a neurite was described by ∂ C /∂ x 2 = (k/D) C, which was calculated as follows: C (x) = A exp(−(k / D) 1/2 x) + B exp((k / D) 1/2 x), where A and B represent constants. The general solution satisfying the boundary conditions was as follows:

Next, we calculated the amount of Rho-kinase at the neurite tip according to C (L), which is described in equation (1) in the text.

---

### World year of physics: a direct test of E = mc2 [^4a44afeb]. Nature (2005). Excellent credibility.

One of the most striking predictions of Einstein's special theory of relativity is also perhaps the best known formula in all of science: E = mc(2). If this equation were found to be even slightly incorrect, the impact would be enormous — given the degree to which special relativity is woven into the theoretical fabric of modern physics and into everyday applications such as global positioning systems. Here we test this mass-energy relationship directly by combining very accurate measurements of atomic-mass difference, Delta(m), and of gamma-ray wavelengths to determine E, the nuclear binding energy, for isotopes of silicon and sulphur. Einstein's relationship is separately confirmed in two tests, which yield a combined result of 1-Delta(mc2)/E = (-1.4 ± 4.4)x10(-7), indicating that it holds to a level of at least 0.00004%. To our knowledge, this is the most precise direct test of the famous equation yet described.

---

### Using both qualitative and quantitative data in parameter identification for systems biology models [^b7b4171c]. Nature Communications (2018). Medium credibility.

An experiment yielding quantitative data would be to measure a binding curve: measure the free inhibitor concentration I versus receptor occupancy(Fig. 2b). Here we defineas the fraction of Raf molecules bound to inhibitor, = (RI + RIR + 2RIRI)/ R tot. A curve similar to this has been measured for EGF and the EGF receptor.

We consider a second experiment that yields only qualitative data, in which we measure free inhibitor concentration I versus the overall Raf activity. We assume that the active Raf species are the uninhibited dimers RR and RIR.

We define A = RR + RIR as a quantity proportional to Raf activity, but suppose that the experiment can only detect whether the activity is higher (+), lower (−), or within error (0) of the basal value of A with no inhibitor (Fig. 2c). Note that a small concentration of inhibitor unintuitively causes an increase in Raf activity by increasing the concentration of the species RIR. This behavior is observed for many Raf inhibitors and poses a challenge for therapeutic applications.

We combine the quantitative and qualitative datasets by the method introduced in the previous section. Here, we are solving for the parameters x = (K 3, K 5).

For the quantitative term of the objective function f quant (x) (Eq. (2)), we directly use the synthetic data in Fig. 2b. For the qualitative term f qual (x) (Eq. (3)), we note that each data point in Fig. 2c can be expressed as an equality or inequality constraint on the value of A at the particular value of I given x = (K 3, K 5), which we will write as A (I, x). Namely, for a "−" data point, we have A (I, x) < A (0), and an analogous constraint holds for a "+" point. For a "0" point, we have, whereis the tolerance (we usedin this example). Note that A (0), the value of A with no inhibitor, is a known constant computed from K 1. Each constraint is then converted to the form g i (x) < 0 for use in Eq. (3) (e.g. for a "−" point at inhibitor concentration I i, we have A (I i, x) − A (0) < 0).

---

### Determination of X-ray detection limit and applications in perovskite X-ray detectors [^c02fe79a]. Nature Communications (2021). High credibility.

Assuming Group 1 and Group 2 represent the gross signal (photocurrent under X-ray I X–ray in the case of X-ray detector) and the blank signal (dark current I dark in the case of X-ray detector) throughout in this work (equivalently vice versa), respectively, then a prior detection limit Δ = | μ 1 − μ 2 | can be calculated with the measured blank signal of Group 2, i.e. (μ 2,) and n 2 from the DL equations, with some necessary pre-assumptions (we use the term " prior " because this method requires pre-assumptions from a statistical perspective). We make assumptions consistent with the Currie method and show that the DL equations is same as Currie formulars when sample sizes (i.e. number of sampled data points) are being reduced (Fig. 2c). Instead of setting α = 0.13% and β = 50% as in the IUPAC definition, we set more properly α = β = 0.05, corresponding to z 1− α = z 1− β = 1.645, which is consistent with Currie formulas. The standard deviation of the gross signal and the blank signal are assumed approximately equal as is in the Currie formulars, i.e. = , when the gross signal is small approaching the level of detection limit. The influence of sample size (i.e. number of sampled data points) on the detection limit is reflected by the value of k = n 2 / n 1. If we set n 2 = n 1 = 1, we have k = 1. Then the DL equations reduce to. With the assumptions of = and z 1− α = z 1− β = 1.645, we have, which is the detection limit L D in Paired observations case in Currie's classical paper(Table 1). If we set n 1 = 1 and n 2 > > n 1, that is, k = n 2 / n 1 > > 1, so that > > or > >, then the DL equations reduce to. With the same assumptions, we have Δ = | μ 1 − μ 2 | = 3.29 σ 2, which is the detection limit L D in "Well-known" blank case in Currie's paper(Table 1). The DL equations are based on Normal distribution, which makes it applicable to X-ray detectors working in a continuous current mode.

---

### KDOQI clinical practice guidelines and clinical practice recommendations for diabetes and chronic kidney disease [^e7d0d7b5]. American Journal of Kidney Diseases (2007). Medium credibility.

Results — associations are summarized by both direction and strength, with magnitude and both clinical and statistical significance accounted for, and both univariate and multivariate associations presented when appropriate; metrics used include prevalence, relative effects (relative risk [RR], odds ratio [OR], hazard ratio [HR], or net change — change from baseline in the intervention group minus the change in the control group), correlation (r or r^2), and test accuracy (sensitivity, specificity, and positive and negative predictive value); the choice of metric often was limited by the reported data and for some studies only the statistical significance was reported.

---

### Dynamic memristor-based reservoir computing for high-efficiency temporal signal processing [^ccd060e9]. Nature Communications (2021). High credibility.

Time-series prediction

In addition to the classification of temporal signals in the above two demonstrations, we also perform another benchmark task to demonstrate the prediction of temporal signals. Hénon map has been established as a typical discrete-time dynamic system with chaotic behavior. It describes a nonlinear 2-D mapping that transforms a point (x (n), y (n)) on the plane into a new point (x (n + 1), y (n + 1)), defined as follows:where w (n) is a Gaussian noise with a mean value of 0 and a standard deviation of 0.05. The task is to predict the system position at time step n + 1, given the values up to time step n. The system can be described as an equation containing only x if we combine Eqs. (3) and (2), so the input of the task is x (n) and the target output is x (n + 1). Using these equations, we generate the Hénon map dataset with a sequence length of 2000, in which the first 1000 data points is used for training and the rest is used for testing. To execute this task in our memristor-based parallel RC system, the input time series x (n) is linearly mapped to the voltage range of [V min, V max]. The mask process is similar to the one used in the previous waveform classification task. During each time interval τ, the pre-processed signal is multiplied by a special mask with a length of M to generate the input voltage sequence with a time step δ (δ = 120 μs). An N -parallel RC system is realized by using different mask sequences. The training and testing processes are similar to the previous tasks and the only difference is that a bias is added to the output layer to neutralize the influence of input signal offset on the output. Both bias and weights are trained with linear regression. After finding the suitable parameters, our RC system can achieve excellent performance on the time-series prediction. For example, Fig. 4a shows the predicted time series versus the ideal target during the testing process for the first 200 time steps, where a very low NRMSE of 0.046 is achieved by the dynamic memristor-based RC system. Here the parameters are set to be M = 4, N = 25, V max = 2.5 V, and V min = −0.8 V. In order to show the predicted results more intuitively, Fig. 4b is a 2-D display of the Hénon map in Fig. 4a, which demonstrates that the strange attractor of the Hénon map can be well reconstructed.

---

### Thinking outside the curve, part II: modeling fetal-infant mortality [^27ea1e69]. BMC Pregnancy and Childbirth (2010). Low credibility.

a. Simulation study to calibrate confidence intervals

We simulated 25 overlapping data sets of size 50,000, the degree of overlap consistent with a population of 200,000, based on the specifications in Table 2. The mixture density and the risk functions were chosen to mimic the patterns actually observed for white singletons born to heavily-smoking mothers; see panel b of Figure One from the previous paper and Figure 1d of the present paper. For each of various C between 2.0 and 5.0, we used Equation (6) to form confidence intervals for mortality risks at selected birthweights, namely r 1 (μ 1 - σ 1), r 1 (μ 1), r 1 (μ 1 + σ 1), r 2 (μ 2 - σ 2), r 2 (μ 2), r 2 (μ 2 + σ 2), r 3 (μ 3 - σ 3), r 3 (μ 3), r 3 (μ 3 + σ 3), r 4 (μ 4 - σ 4), r 4 (μ 4), and r 4 (μ 4 + σ 4). Above, μ j and σ j denote the mean and standard deviation of the birthweights in component j (1 ≤ j ≤ 4). This was repeated nine more times, and we tabulated how many of the 120 = 12 × 10 confidence intervals contained their targets. Confidence intervals were also formed using Equation (5) for comparative purposes. The above steps were repeated with overlapping data sets consistent with a population of 1,000,000 and with nonoverlapping data sets consistent with an effectively infinite population.

Table 2
Mixture Model and Mortality Functions for Simulation Study

The probability density for the mixture model used in our simulation study is specified, as are the mortality risk functions associated with the mixture model components. Above, z is defined as (x - 3000)/1000, where x is birthweight in grams.

The results are summarized in Table 3. With an effectively infinite population, only 75.0% of the confidence intervals formed using Equation (5) contained their targets at C = 5.0. On the other hand, the confidence intervals formed using Equation (6) contained their targets 95.0% of the time at C = 4.0. The latter finding provided the rationale for taking C 0 = 4.0 when constructing confidence intervals for mortality risks in our examples with real data.

---

### Estimation in medical imaging without a gold standard [^82dda7fe]. Academic Radiology (2002). Low credibility.

Rationale and Objectives

In medical imaging, physicians often estimate a parameter of interest (eg, cardiac ejection fraction) for a patient to assist in establishing a diagnosis. Many different estimation methods may exist, but rarely can one be considered a gold standard. Therefore, evaluation and comparison of different estimation methods are difficult. The purpose of this study was to examine a method of evaluating different estimation methods without use of a gold standard.

Materials and Methods

This method is equivalent to fitting regression lines without the x axis. To use this method, multiple estimates of the clinical parameter of interest for each patient of a given population were needed. The authors assumed the statistical distribution for the true values of the clinical parameter of interest was a member of a given family of parameterized distributions. Furthermore, they assumed a statistical model relating the clinical parameter to the estimates of its value. Using these assumptions and observed data, they estimated the model parameters and the parameters characterizing the distribution of the clinical parameter.

Results

The authors applied the method to simulated cardiac ejection fraction data with varying numbers of patients, numbers of modalities, and levels of noise. They also tested the method on both linear and nonlinear models and characterized the performance of this method compared to that of conventional regression analysis by using x-axis information. Results indicate that the method follows trends similar to that of conventional regression analysis as patients and noise vary, although conventional regression analysis outperforms the method presented because it uses the gold standard which the authors assume is unavailable.

Conclusion

The method accurately estimates model parameters. These estimates can be used to rank the systems for a given estimation task.

---

### A wind environment and lorentz factors of tens explain gamma-ray bursts X-ray plateau [^e312631a]. Nature Communications (2022). High credibility.

Closure relations and determination of the electron power-law indices

The theoretical model used herein imposes a relation between the spectral and the temporal slopes. These so-called closure relations are unique to each class and each observational band (optical and X-rays). Therefore, they can be used to assess the validity of our model. For each GRB, the X-ray spectral slopes (β p for the plateau phase, β A1 for the self-similar phase, β A2 for the spectral slope after the jet break) were obtained from the online Swift repository for the same time range as the temporal slopes, and optical spectral slopes were retrieved from the literature (the relevant references are given in Supplementary Table 1–3). We used the spectral and temporal slopes in each phase (plateau and self-similar) independently to further confront our theory to the data. Specifically, we checked that the closure relations relevant for regimes (C, E, F) for each band, each phase and each class are satisfied independently. The results are presented in Supplementary Figs. 15 and 16 for the plateau phase and the self-similar phase respectively. From these figures, it is clear that all the data––spectral and temporal, both in X-ray and optical - are consistent with the theoretical closure relations of the model.

Furthermore, from the closure relation, we deduced the power-law index p of the accelerated electrons by using 8 independent measurements, namely the temporal and spectral indices of both the optical and X-ray data during both the plateau and self-similar phases. This was done for both regions E and F. As we demonstrate in Fig. 2, we find that the power-law index of the accelerated electrons does not change in between the dynamical phases. We find that for all GRBs in our sample, the power-law index, p is in the range 1.8 ≲ p ≲ 2.5. For those bursts whose X-ray light curve is identified as being in region E, the electron power-law index is narrowly clustered around p ≃ 2, while a larger spread is found for other GRBs. In 3 cases, namely GRBs 060614, 060729 and 110213A, we find that the values derived from the optical spectral slopes are inconsistent with the other measurements (see caption of Table 1), in which case we use 6 out of 8 independent measurements in determining the region and the power-law index. The results are summarized in Table 1.

---

### Correction to: how can clinicians, specialty societies and others evaluate and improve the quality of apps for patient use? [^82bc7a71]. BMC Medicine (2019). Medium credibility.

Since the publication of this article [1] it has come to my attention that it contains an error in which the y-axis in Fig. 1 was inverted, thus incorrectly displaying a weak negative correlation rather than a weak positive one. This error was introduced as the order of the data on which Fig. 2 was based [2] was misread. The corrected version of Fig. 2 can be seen below, in which a weak positive correlation is now displayed. This does not change the general point, that app users and app stores appear to take little notice of the source of information on which apps are based. I apologise to readers for this error.

---

### Thinking outside the curve, part II: modeling fetal-infant mortality [^8321a00b]. BMC Pregnancy and Childbirth (2010). Low credibility.

c. Estimating odds ratios

To estimate an odds ratio comparing components in the same population, such as

odds of mortality at 1000 g in component 2 (white heavy smoking population) divided by

odds of mortality at 1000 g in component 1 (white heavy smoking population),

we apply Equation (6) with the following modifications. Instead of identifying θ with logit{ r j (x 0) }, we take θ = logit{ r j 1 (x 0) } - logit{ r j 2 (x 0) }, where 1 ≤ j 1, j 2 ≤ k. Then exp{ θ } equals the mortality odds in component j 1 at birthweight x 0 divided by the mortality odds in component j 2. Hence, exp{} is an estimate of the odds ratio, and

is a confidence interval.

To estimate an odds ratio comparing populations on the same component, such as

odds of mortality at 2500 g in component 3 (white heavy smoking population) divided by

odds of mortality at 2500 g in component 3 (white general population),

we define θ 1 = logit{ r j (x 0)} for the first population and θ 2 = logit{ r j (x 0)} for the second population. Then exp{ θ 1 - θ 2 } equals the mortality odds in component j of the first population at birthweight x 0 divided by the mortality odds in component j of the second population. Hence, is an estimate of the odds ratio, and

is a confidence interval. Subscripts 1 and 2 in Equation (9) identify the populations to which the "meta-sample" means, standard deviations, and bias adjustments pertain.

3. Further illustrations

---

### Inferring time derivatives including cell growth rates using gaussian processes [^b5444dae]. Nature Communications (2016). Medium credibility.

Methods

Using a Gaussian process to fit time-series data

In the following, we will denote a Gaussian distribution with mean μ and covariance matrix Σ as(μ, Σ) and use the notation of Rasmussen and Williamsas much as possible.

Prior probability

For n data points y i at inputs x i (each x i is a time for a growth curve), we denote the underlying latent function as f (x). We define a covariance matrix k (x, x ′), which has an explicit dependence on hyperparameters θ, and obeys

where the expectations are taken over the distribution of latent functions (samples of f (x)).

We interpret equation (1) as giving the prior probability distribution of the latent functions f (X), where were we use X to denote the inputs x i, such that

where K (X, X) is the n × n matrix with components k (x i, x j). With f denoting [f (x 1). f (x n)], this prior probability can be written as

noting the dependence of k (x, x ′; θ) on the hyperparameters θ.

---

### Modeling the effects of consanguinity on autosomal and X-chromosomal runs of homozygosity and identity-by-descent sharing [^622f6f15]. G3 (2024). Medium credibility.

Runs of homozygosity (ROH) and identity-by-descent (IBD) sharing can be studied in diploid coalescent models by noting that ROH and IBD-sharing at a genomic site are predicted to be inversely related to coalescence times-which in turn can be mathematically obtained in terms of parameters describing consanguinity rates. Comparing autosomal and X-chromosomal coalescent models, we consider ROH and IBD-sharing in relation to consanguinity that proceeds via multiple forms of first-cousin mating. We predict that across populations with different levels of consanguinity, (1) in a manner that is qualitatively parallel to the increase of autosomal IBD-sharing with autosomal ROH, X-chromosomal IBD-sharing increases with X-chromosomal ROH, owing to the dependence of both quantities on consanguinity levels; (2) even in the absence of consanguinity, X-chromosomal ROH and IBD-sharing levels exceed corresponding values for the autosomes, owing to the smaller population size and lower coalescence time for the X chromosome than for autosomes; (3) with matrilateral consanguinity, the relative increase in ROH and IBD-sharing on the X chromosome compared to the autosomes is greater than in the absence of consanguinity. Examining genome-wide SNPs in human populations for which consanguinity levels have been estimated, we find that autosomal and X-chromosomal ROH and IBD-sharing levels generally accord with the predictions. We find that each 1% increase in autosomal ROH is associated with an increase of 2.1% in X-chromosomal ROH, and each 1% increase in autosomal IBD-sharing is associated with an increase of 1.6% in X-chromosomal IBD-sharing. For each calculation, particularly for ROH, the estimate is reasonably close to the increase of 2% predicted by the population-size difference between autosomes and X chromosomes. The results support the utility of coalescent models for understanding patterns of genomic sharing and their dependence on sex-biased processes.

---

### A general model for ontogenetic growth [^10660e44]. Nature (2001). Excellent credibility.

Several equations have been proposed to describe ontogenetic growth trajectories for organisms justified primarily on the goodness of fit rather than on any biological mechanism. Here, we derive a general quantitative model based on fundamental principles for the allocation of metabolic energy between maintenance of existing tissue and the production of new biomass. We thus predict the parameters governing growth curves from basic cellular properties and derive a single parameterless universal curve that describes the growth of many diverse species. The model provides the basis for deriving allometric relationships for growth rates and the timing of life history events.

---

### Guidelines for handling decedents contaminated with radioactive materials [^ac3719a8]. CDC (2007). Medium credibility.

Funeral home procedures for radioactively contaminated decedents — if radioactive contamination is suspected and the medical examiner cannot certify that he has complied with CDC's guidelines, the funeral director should consider rejecting the decedent. If this is not feasible, find a radiation technician who can conduct a survey of the body and provide dose rate information, and do not attempt to embalm or work near a decedent with an unknown dose rate. Recommend to the family that they bury the decedent immediately and conduct a memorial service without a viewing; if this is not acceptable for emotional, cultural, or religious reasons then embalming is required, and the funeral director will have to estimate the exposures to the person performing the embalming and to the family members. Medical examiners should have provided a tag indicating the dose rate from the decedent and the distance from which it was measured, and because dose rate decreases with the square of the distance, encourage family members and friends to minimize their time next to the decedent. For estimation, sample values are provided: dose rate at 1 foot = 10 millirem / hour; dose rate at 3 feet = 10 x (1/3) 2 or 1.1 millirem / hour; and a person standing 3 feet from the decedent for ½ hour would receive 0.5 millirem. Avoid the use of adjectives like "safe" or "dangerous", and compare the estimated dose to a familiar source like chest radiographs, airline flights, or dental radiographs.

---

### Thinking outside the curve, part I: modeling birthweight distribution [^49eb3a50]. BMC Pregnancy and Childbirth (2010). Low credibility.

Results

1. Pragmatics for mixture modeling

a. Finite normal mixture models

Many phenomena cannot be accurately described via a normal distribution. When no other commonly used probability distribution seems appropriate, a finite normal mixture model is often reasonable. We now briefly describe the model. Readers interested in theoretical developments may consult references and works cited therein.

Let f (x;μ,σ) denote the probability density for the normal distribution with mean μ and standard deviation σ. A finite normal mixture model with k components has probability density

A common way to interpret Equation (1) is to imagine that the full population consists of k subpopulations. The proportion of individuals in the full population belonging to subpopulation j is p j. In subpopulation j, measurements are normally distributed with mean μ j and standard deviation σ j.

The mixture components may or may not represent subpopulations with obvious biological definitions outside the statistical model. For example, in a 2-component normal mixture describing birthweights for white singletons in the United States, there is not an obvious biological characterization for the two components: we may say that the component with the smaller mean reflects compromised pregnancies, but we cannot immediately attribute the compromised pregnancies to a specific biological mechanism.

Ideally, modeling with finite normal mixtures may lead to discoveries of subpopulations with biological definitions that were not immediately obvious, although the mixture components themselves may still only be approximations to such subpopulations.

---

### Appropriate systemic therapy dosing for obese adult patients with cancer: ASCO guideline update [^2ddf0964]. Journal of Clinical Oncology (2021). High credibility.

Regarding specific circumstances for obesity, more specifically with respect to patients with cancer, ASCO 2021 guidelines recommend to calculate body surface area using any of the standard formulae. Insufficient evidence to support one formula for calculating body surface area over another.

---

### A simple alpha / β-independent method to derive fully isoeffective schedules following changes in dose per fraction [^3a48e204]. International Journal of Radiation Oncology, Biology, Physics (2004). Low credibility.

Purpose

Dosimetric errors in delivering the prescribed dose per fraction made early in a treatment can be corrected by modifying the dose per fraction and total dose given subsequently to discovery of the error, using the linear-quadratic model to calculate the correcting doses which should be completed within the same overall time as originally prescribed. This study shows how these calculations can be carried out independently of any alpha/beta ratios to bring the treatment back exactly to planned tolerance simultaneously for all tissues and tumor involved.

Methods

Planned treatment is defined as p Gy per fraction to a total dose P Gy; the initial error is e Gy per fraction given to a total of E Gy. The linear-quadratic formula is assumed to describe all isoeffect relationships between total dose and dose per fraction.

Results and Conclusion

An exact solution is found that describes a compensating dose of d Gy per fraction to a total of D Gy. The formulae are: D = P-E d = Pp-Ee/P-E. Thus the total dose for the complete treatment (error plus compensation) remains as originally prescribed, with hyperfractionation being used to correct an initial hypofractionation error and hypofractionation being used to correct an initial hyperfractionation error. Incomplete repair is shown to perturb this exact solution. Thus compensating treatments calculated with these formulae should not be scheduled in such a manner that would introduce incomplete repair.

---

### Thinking outside the curve, part II: modeling fetal-infant mortality [^3a0b6eae]. BMC Pregnancy and Childbirth (2010). Low credibility.

A confidence interval for r j (x 0) is obtained by applying the inverse logit transformation to the confidence interval for θ. The above computations can be performed simultaneously at a series of birthweights. Connecting the resulting series of upper confidence limits produces an upper confidence bound for the risk function in component j, while connecting the resulting series of lower confidence limits produces a lower confidence bound.

As when constructing confidence intervals for p j, μ j, and σ j (1 ≤ j ≤ k), we can accommodate overlap in the N rep samples by choosing the value of C in Equation (6) according to the fraction of the underlying population that each of the N rep samples constitutes. Let C 0 denote the value of C that would be chosen if this fraction were negligibly small, and let C φ denote the value that would be chosen if this fraction were equal to φ, a positive number less than 1. In the previous paper, we established the relationship

---

### 2019 ASCCP risk-based management consensus guidelines: methods for risk estimation, recommended management, and validation [^c44c6ea6]. Journal of Lower Genital Tract Disease (2020). High credibility.

Risk-based management uncertainty — confidence interval and decision confidence score: Confidence intervals for each risk estimate were calculated using a normal approximation or exact methods based on the binomial distribution, and a "decision confidence score" is defined that combines the uncertainty in the statistical precision and how close the risk estimates fall to the clinical action thresholds; mathematical details on estimating the recommendation confidence scores are presented in Appendix Part D (http://links.lww.com/LGT/A160).

---

### KDIGO 2022 clinical practice guideline for diabetes management in chronic kidney disease [^bfaebbd4]. Kidney International (2022). High credibility.

Figure 5 — assessment of kidney function in glomerular disease — uses ml/min per 1.73 m^2 units and notes that the correction coefficient for race in GFR estimating equations is controversial and discussions are ongoing, referring readers to the KDIGO CKD guideline for more information.

---

### Aggregation-fragmentation and individual dynamics of active clusters [^a9e1f18c]. Nature Communications (2018). Medium credibility.

We now seek a microscopic understanding of the CSD, relying on aggregation–fragmentation kinetic models. This framework has a long history dating back to Smoluchowski and a range of applications from polymer chemistry to Saturn's rings. Though more general formulations are possible, we restrict from the outset to aggregation or fragmentation events that are binary:

As illustrated in Fig. 2a (left), two clusters of size N and M merge with rate A N, M C N C M, while the reverse fragmentation process occurs with rate F N, M C N + M. At this stage, the complexity of the problem is apparent. On one hand, obtaining the steady-state distribution from the full hierarchy of kinetic equations is a formidable task. On the other hand, if N max is the maximal cluster size, the number of rates that needs to be determined is on the order of.

Considerable simplification occurs by introducing the 'monomer approximation': not all binary events are considered, but only those involving a monomer. As illustrated in Fig. 2a (right), clusters then grow or decrease in size through the exchange of individual colloids. Our justification for such an approximation is based on the transition matrix, computed from experimental data and shown in Fig. 2c. The element of coordinates (N 0, N 1) is the probability that a cluster of initial size N 0 has size N 1 after one time step τ s. Looking at off-diagonal terms, one can see that the kinetics is dominated by ± 1 and ± 2 events. Remembering that dimers do no exist, doublets additions or subtraction actually correspond to a pair of monomeric events. As a consequence, the monomer events typically contribute to 90% of events, and account for the vast majority of aggregation–fragmentation events. Discarding entirely the non-monomeric processes, the kinetic equations then reduce towith A N and F N the aggregation and fragmentation rates for a cluster of size N. The CSD is governed by the master equationImposing a condition of detailed balance, which requires the equality of forward and backward rates in Eq. (2), F N +1 C N +1 = A N C 1 C N, yields the exact solution for the steady statewith C 1 fixed by normalization. This simple formula applies whatever the N -dependence of A N and F N.

---

### Efficient probabilistic inference in generic neural networks trained with non-probabilistic feedback [^c609312c]. Nature Communications (2017). Medium credibility.

The hidden units in both feedforward and recurrent networks were ReLUs. In feedforward networks, the hidden unit responses are described by the equation: r hid = [W in r in + b] + and in recurrent networks by the equation: r hid, t +1 = [W in r in, t +1 + W rec r hid, t + b] +, where W in and W rec are the input and recurrent weights, respectively, and + denotes elementwise rectification. For tasks with continuous output variables, the network output corresponds to a linear combination of the hidden unit responses: y = w Τ r hid + b, and in tasks with categorical variables, the network output was given by a linear combination of the hidden unit responses passed through a sigmoid nonlinearity: y = σ (w Τ r hid + b).

In the recurrent EI networks, all connections were constrained to satisfy Dale's law as in ref. The recurrent units are all rate-based neurons. The ratio of excitatory to inhibitory neurons in both input and recurrent populations was 4:1. Inputs were presented over 10 time steps, but the total input information was equated to the total input information in the feedforward networks. The network's estimate was obtained from its output at the final, i.e. 10th time step. Other details are the same as in the corresponding simulations in the feedforward case.

In random networks, input-to-hidden layer weights were sampled from a normal distribution with zero mean and standard deviation, where d is the number of input neurons and n is the number of hidden neurons; biases of the hidden units were sampled from a normal distribution with zero mean and standard deviation of 0.1.

In the main simulations, the networks had 200 hidden units. In cue combination, modular cue combination, coordinate transformation, Kalman filtering, binary categorization, and causal inference tasks, there were 50 input neurons per input population. To make our results comparable to earlier results, we used 20 input neurons per input population in the visual search task and 10 input neurons per input population in the stimulus demixing task.

---

### Modelling pathological spread through the structural connectome in the frontotemporal dementia clinical spectrum [^f6521431]. Brain (2025). Medium credibility.

The spread of pathology from an affected brain region (R 2) to an unaffected one (R 1) is given by:

where x 1,2 is the pathology concentration in region R 1,2; c 1,2 is the connectivity between R 1 and R 2; andis the diffusivity constant (the higher the value, the higher is the speed of pathology progression).

Pathology from all brain regions is combined into a vector x (t) = { x i (t)} w, and Equation (1) becomes:

where x (t) = { x i (t)}: represents the amount of diffusion of pathology at node i and time point t starting from an initial distribution at time t = 0(x (0)); L is the graph Laplacian matrix; and t are the time points (in arbitrary units).

From matrix algebra, Equation (1) is satisfied by:

where x (0) is a vector with 1 at the index corresponding seed brain regions where it is thought that the pathology begins to spread, 0 at all other brain regions. Four disease epicentres were identified from the peaks of atrophy of each FTD variant: left temporal pole (svPPA), left insula (bvFTD), right temporal pole (sbvFTD) and left supplementary motor area (nfvPPA), as previously shown.

The graph Laplacian represents the discretization of the Laplacian operator and indicates how a graph differs at one vertex from its values at nearby vertices. It was implemented as follows:

where I is the identity matrix; D is the diagonal matrix whose diagonal entries contain the degree of each node; and C is the averaged connectome of healthy subjects.

Given that brain regions are not the same size, L in Equation (3) is the normalized version of the graph Laplacian operator. It is a symmetric matrix, and its eigenvectors are orthonormal. The solution of Equation (1) was implemented mathematically in Matlab by the eigenvalue decomposition:

where N are the brain regions; λ are the eigenvalues of matrix L; and u are the eigenvectors of matrix L.

Brain Net Viewer was used to create diffusion maps.

---

### Global gain modulation generates time-dependent urgency during perceptual choice in humans [^e225fb4d]. Nature Communications (2016). Medium credibility.

Analysing empirical conditional accuracy functions

Single-trial logistic regression was used to estimate mean accuracy as a function of RT (the CAF), for each task condition and subject. To account for both the dominant decreasing portion of the observed CAFs and an initial increasing portion due to a small percentage of inaccurate premature responses, we constructed an algorithm that minimizes the combined sum of squared errors of piece-wise logistic regressions of accuracy (1 = correct, 0 = error) onto RT, splitting trials before and after a temporal inflection point α such that

Here β 0 is accuracy at α, β 1 is the slope of the CAF before α, and β 2 is the slope of the CAF after α. β 1 was constrained to be ≥ 0 to reflect the fact that the left segment of the piece-wise fit should only account for the initial increasing portion of the CAF. This model was fit using Nelder-Mead simplex minimization to estimate the β 0, β 1 and β 2 parameters while conducting an exhaustive search of possible α values (step-size = 10 ms ending at 1 s). Whichever piece-wise segment is fit first determines β 0 and thus constrains the fit of the remaining segment; therefore, the algorithm was run twice (left segment fit first and right segment fit first) for each α to find the true minimum. In fits to FR trials from task 1, all RTs longer than 5 s were excluded.

To estimate accuracy at the time of the deadline (Fig. 1c), we used the piece-wise regression fits for each subject to calculate accuracy when RT = 1.4 s. The low number of trials immediately preceding the deadline prohibits a precise characterization of the shape of the empirical CAF at this time point. However, the above single-trial regression approach allows for a reasonable approximation by exploiting consistencies in the temporal evolution of the CAF. The appropriateness of this approach relies on any change in decision policy being gradual rather than abrupt, which appeared to be the case in our data given the smooth right tails of the DL RT distributions (Fig. 1b) and the shapes of the fitted urgency signals (Fig. 3b).

---

### A national model for developing, implementing, and evaluating evidence-based guidelines for prehospital care [^508304b2]. Academic Emergency Medicine (2012). Medium credibility.

Respiratory distress — prehospital ventilator initial settings specify: tidal volume 6–8 mL/kg ideal body weight with the directive to "Use 6–8 mL/kg as a starting point" and that "Patients with known acidosis should start with 9 mL/kg"; respiratory rate 12–14 (or 8–12) breaths/min with "Adjust for target minute ventilation based on EtCO2"; inspiratory time 1 second with "Adjust 0.7–1.2 seconds to maintain desired I:E ratio… of 1:2"; positive end-expiratory pressure (PEEP) 5 cmH2O; fraction of inspired oxygen (FiO2) 60% with "Titrate to achieve target O2 saturation (94–98%)"; and sensitivity -2 cmH2O. Additional setup includes "Set the heat moisture exchange (HME) at circuit Y" and "Plateau pressure (PPlat) goal is less than 30 cmH2O".

---

### A space-time tradeoff for implementing a function with master equation dynamics [^57fa10b0]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.