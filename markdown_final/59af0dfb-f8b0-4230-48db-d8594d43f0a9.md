# Medical Question & Answer

**Sample ID**: 59af0dfb-f8b0-4230-48db-d8594d43f0a9
**Dataset Index**: 28620

---

## Question

40. Two different natural numbers a, b, their least common multiple is 64, then their greatest common divisor has
possible situations.

---

## Answer

> Let's see… What do we have here? The user is asking how many different possible values the greatest common divisor (gcd) can take for two distinct natural numbers whose least common multiple (lcm) is 64. Let's break this down step-by-step. First, I need to think about the fundamental relationship between gcd and lcm. Then, I should verify the prime factorization of 64 and what that implies for the exponents in a and b. Next, I will enumerate the valid pairs (a, b) with lcm(a, b) = 64 and a ≠ b, and extract the gcds. Finally, I will count the distinct gcd values and confirm there are no duplicates or edge cases I missed.

> Let me first confirm the core identity: for any two positive integers, gcd(a, b) × lcm(a, b) = a × b. Since lcm(a, b) = 64, this means gcd(a, b) must be a divisor of 64, so the possible gcds are among the divisors of 64, namely 1, 2, 4, 8, 16, 32, 64. Wait, let me verify that I'm not overcounting; I should ensure that each of these divisors is actually achievable with distinct a and b whose lcm is 64, rather than assume all divisors are possible just because they divide 64.

> Next, I should review the prime factorization constraint. Since 64 = 2⁶, both a and b must be powers of 2, say a = 2ˣ and b = 2ʸ with x, y non-negative integers. Then lcm(a, b) = 2^max(x, y) = 2⁶, so max(x, y) = 6. Hold on, I should verify that this is correct; yes, lcm of two powers of 2 is simply the larger exponent, so max(x, y) = 6 is necessary and sufficient for lcm = 64.

> I will now examine the gcd in terms of exponents. gcd(a, b) = 2^min(x, y), so the possible gcd values correspond to min(x, y) ranging from 0 up to 6. But wait, what if x = y? Then a = b, which violates the "different natural numbers" condition. So I need to ensure x ≠ y while still satisfying max(x, y) = 6, which means min(x, y) can be 0, 1, 2, 3, 4, 5, but not 6, because if min(x, y) = 6 then x = y = 6 and a = b = 64. Let me reconsider: I initially thought min(x, y) could be 6, but that would force a = b, so min(x, y) must be strictly less than 6, i.e., 0 through 5, yielding gcds 2⁰ through 2⁵, which are 1, 2, 4, 8, 16, 32 respectively.

> Hold on, let's not jump to conclusions; I should double-check that each of these gcds is indeed achievable with distinct a and b. For gcd = 2ᵏ where k ∈ {0, 1, 2, 3, 4, 5}, I can choose x = 6 and y = k, so that max(x, y) = 6 and min(x, y) = k, giving a = 2⁶ = 64 and b = 2ᵏ, with a ≠ b because k < 6. This constructs valid pairs for each target gcd, confirming that 1, 2, 4, 8, 16, and 32 are all attainable. I should confirm there are no other gcds possible; since min(x, y) cannot be 6 without making a = b, no other gcd values are feasible, so the set is complete.

> Let me count and state the final answer clearly. The distinct possible gcd values are 1, 2, 4, 8, 16, and 32, which totals 6 possible situations. I should double-check that I haven't overlooked any edge cases or duplicates; given the exponent constraints and the requirement that a ≠ b, the enumeration is exhaustive and each gcd is unique, so the answer is 6.

---

The greatest common divisor (gcd) of two different natural numbers (a) and (b) with least common multiple (lcm) 64 can be any divisor of 64 except 64 itself, because if (gcd(a, b) = 64), then (a = b = 64), which violates the condition that (a) and (b) are different. Thus, the possible gcd values are the divisors of 64 less than 64: 1, 2, 4, 8, 16, and 32. Therefore, there are **6 possible situations** for the gcd.

---

## Mathematical relationship between gcd and lcm

For any two natural numbers (a) and (b), the following relationship holds:

gcd(a, b) × lcm(a, b) = a × b

Given that (lcm(a, b) = 64), we have:

gcd(a, b) × 64 = a × b

This implies that (gcd(a, b)) must be a divisor of 64, since (gcd(a, b)) divides both (a) and (b), and therefore divides their product (a × b).

---

## Divisors of 64

The divisors of 64 are:

1, 2, 4, 8, 16, 32, 64

---

## Possible values of gcd(a, b)

Since (gcd(a, b)) must be a divisor of 64, the possible values are 1, 2, 4, 8, 16, 32, and 64. However, we need to ensure that (a) and (b) are different natural numbers. Let's examine each case:

- **Gcd(a, b) = 1**: This is possible. For example, (a = 1) and (b = 64).
- **Gcd(a, b) = 2**: This is possible. For example, (a = 2) and (b = 64).
- **Gcd(a, b) = 4**: This is possible. For example, (a = 4) and (b = 64).
- **Gcd(a, b) = 8**: This is possible. For example, (a = 8) and (b = 64).
- **Gcd(a, b) = 16**: This is possible. For example, (a = 16) and (b = 64).
- **Gcd(a, b) = 32**: This is possible. For example, (a = 32) and (b = 64).
- **Gcd(a, b) = 64**: This is not possible, because if (gcd(a, b) = 64), then both (a) and (b) must be multiples of 64. Given that (lcm(a, b) = 64), the only possibility is (a = b = 64), which violates the condition that (a) and (b) are different.

---

## Conclusion

The possible values of (gcd(a, b)) are 1, 2, 4, 8, 16, and 32. Therefore, there are **6 possible situations** for the greatest common divisor of (a) and (b).

---

## References

### How much does government's short-term response matter for explaining cross-country variation in COVID-19 infection outcomes? A regression-based relative importance analysis of 84 countries [^116h9yz6]. BMJ Public Health (2024). High credibility.

To measure the speed of government response, we use the number of days since a country recorded its first confirmed case until its policy value reached 39, the average value of the Stringency Index, to define the government's reaction speed. All the countries in our sample have reached a policy value of 39 at some point, which took 20 days on average and varies between 1 and 56 days.

Natural conditions: We include a set of continental indicators to capture the inherited natural characteristics of a country.

Socioeconomic conditions: The most recent data available on per capita gross domestic product (GDP) expressed in constant 2011 international dollars (purchasing power parity), government health expenditure as a proportion of GDP and median age are taken from the World Bank and United Nations Population Division.

Cultural conditions: To examine the role of culture, we employ the widely used individualism-collectivism index by Hofstede. It measures how people in a society see themselves as an autonomous entity or integrated into groups on a 0–100 scale, with larger numbers being more individualistic. Hofstede quantifies nations' characters on each value dimension that shapes the ideology and behaviour of the citizens. Among the six dimensions, the individualism-collectivism dimension is the one with the greatest predicted power and it best reflects the cross-national difference in behaviours of the government and the people facing a pandemic. The mean value in our sample is 37.64 table 1.

We are aware that plenty of other factors are used in the analysis of the risk factors of COVID-19 at the micro and macro levels. For instance, studies have reported that wider COVID-19 spread and higher mortality can be associated with having multiple comorbidities. Nonetheless, given the limited number of countries in our sample and for interpretation convenience, we include only the most representative indicators to capture a general picture of each country's natural, cultural and socioeconomic characteristics.

---

### On the philosophy of diagnosis: is doing more good than harm better than "primum non nocere"? [^117HxmXg]. Emergency Medicine Journal (2009). Low credibility.

Diagnosis is arguably the cornerstone of medicine. Without at least some form of diagnosis the practice of medicine would not be possible. This narrative review explores common philosophical assumptions and challenges the notion that a certain diagnosis can ever be made. The idealistic concept of "primum non nocere" is discussed, and whether the utilitarian goal of achieving "the greatest happiness for the greatest number" is a feasible or preferable alternative is considered. It is concluded that utilitarianism is inescapably intertwined with modern medical practice. Suggestions are presented to further the understanding of diagnostic medicine by embracing its principles.

---

### Multiple choice answers: what to do when you have too many questions [^113WWL7o]. The Journal of Foot and Ankle Surgery (2015). Low credibility.

Carrying out too many statistical tests in a single study throws results into doubt, for reasons statistical and ethical. I discuss why this is the case and briefly mention ways to handle the problem.

---

### Universal and uniquely human factors in spontaneous number perception [^113NkXT5]. Nature Communications (2017). Medium credibility.

Some researchers have suggested that a combination of non-numeric dimensions is used to quantify sets of objects, instead of representing number directly. To test this we implemented a regression analysis, which included the combination of variables suggested (convex hull, density, cumulative surface area and average diameter) and also included number (equation (12)) on our results from Experiment 1. Again group was entered as a random effects term. If number is represented via these alternative dimensions then we should find no significant effect of number when this collection of variables is included in the model. This is not what we found. Instead, number was a significant predictor over and above all other measures (β number = 1.05, P < 0.001, n = 4,785). This persistent effect of number even when other variables are controlled shows that the effect of number cannot be accounted for by alternative variables. Additionally, the effect of number was significantly greater than any of the variables tested (z -tests: Number versus Convex Hull: z = 7.39, P < 0.001, n = 4,785; Number versus Density: z = 2.79, P < 0.01, n = 4,785; Number versus Area: z = 2.15, P < 0.05, n = 4,785; Number versus Dot Diameter: z = 5.51, P < 0.001, n = 4,785). The results suggest that numerical information is perceived directly, rather than indirectly through the representation of other quantitative dimensions.

---

### Vernier templating and synthesis of a 12-porphyrin nano-ring [^115obohA]. Nature (2011). Excellent credibility.

Templates are widely used to arrange molecular components so they can be covalently linked into complex molecules that are not readily accessible by classical synthetic methods. Nature uses sophisticated templates such as the ribosome, whereas chemists use simple ions or small molecules. But as we tackle the synthesis of larger targets, we require larger templates-which themselves become synthetically challenging. Here we show that Vernier complexes can solve this problem: if the number of binding sites on the template, n(T), is not a multiple of the number of binding sites on the molecular building blocks, n(B), then small templates can direct the assembly of relatively large Vernier complexes where the number of binding sites in the product, n(P), is the lowest common multiple of n(B) and n(T) (refs 8, 9). We illustrate the value of this concept for the covalent synthesis of challenging targets by using a simple six-site template to direct the synthesis of a 12-porphyrin nano-ring with a diameter of 4.7nm, thus establishing Vernier templating as a powerful new strategy for the synthesis of large monodisperse macromolecules.

---

### The snm procedure guideline for general imaging 6.0 [^113YaMf1]. SNMMI (2010). Medium credibility.

SNM Procedure Guideline for General Imaging — matrix size and pixel depth in nuclear medicine acquisitions notes that matrix size is almost always a power of 2 with typical values 64 x 64, 128 x 128, 256 x 256 and 512 x 512, and that non-square matrix sizes also exist for whole-body studies. Each pixel can be represented with a single byte (pixel values ranging from 0 to 255 counts) or with 16 bit words (pixel values ranging up to a maximum of 32k or 64k). Overflow occurs when the number of counts recorded at some given position exceeds the maximum number of counts, and overflow is more likely to occur when using a byte matrix.

---

### Joint EANM / SNMMI guideline on radiomics in nuclear medicine: jointly supported by the EANM physics committee and the SNMMI physics, instrumentation and data sciences council [^117GnSoR]. European Journal of Nuclear Medicine and Molecular Imaging (2023). High credibility.

Computing features — radiomics in nuclear medicine specifies that PET voxel values should be converted into SUV values prior to feature computation, with cross-referencing to clinically certified viewing software, and that if physical voxel spacing differs across measurements, voxel intensities should be resampled to a grid with a common (ideally isotropic) voxel spacing using interpolation, preferably with a higher-order interpolator, such as a cubic spline; textural features should be computed in 3D unless there is a clear reason not to; textural features can be computed with either fixed bin number (FBN) or fixed bin size (FBS), with no consensus on superiority and, for comparison purposes, it may be useful to systematically implement and report both; for FBN, the recommended number of bins should lie between 4 and 64 bins; for FBS, the lower bound should generally be placed at 0.0 SUV, typical bin sizes lead to forming between 8 and 64 bins in the ROI, and exceedingly small bin sizes, e.g., 0.01 SUV, should be avoided; typically, texture matrices should be computed with default parameters as listed in the IBSI reference document and caution should be taken with respect to software that may not use the same default settings.

---

### Number needed to treat (or harm) [^116LRjHP]. World Journal of Surgery (2005). Low credibility.

The effect of a treatment versus controls may be expressed in relative or absolute terms. For rational decision-making, absolute measures are more meaningful. The number needed to treat, the reciprocal of the absolute risk reduction, is a powerful estimate of the effect of a treatment. It is particularly useful because it takes into account the underlying risk (what would happen without the intervention?). The number needed to treat tells us not only whether a treatment works but how well it works. Thus, it informs health care professionals about the effort needed to achieve a particular outcome. A number needed to treat should be accompanied by information about the experimental intervention, the control intervention against which the experimental intervention has been tested, the length of the observation period, the underlying risk of the study population, and an exact definition of the endpoint. A 95% confidence interval around the point estimate should be calculated. An isolated number needed to treat is rarely appropriate to summarize the usefulness of an intervention; multiple numbers needed to treat for benefit and harm are more helpful. Absolute risk reduction and number needed to treat should become standard summary estimates in randomized controlled trials.

---

### American Clinical Neurophysiology Society technical standards for electrical stimulation with intracranial electrodes for functional brain mapping and seizure induction [^115TfVJ4]. Journal of Clinical Neurophysiology (2025). High credibility.

ACNS intracranial electrode specifications — subdural electrodes list Total Contact Diameter (mm) 2.0, 3.0, 4.0*, 4.5, 6.0 and Exposed Contact Diameter (mm) 1.5, 1.8, 2.3*, with Inter-contact Distance (mm) 5, 10*, 15; Number of Contacts include Strips: 2, 4, 6, 8, 10, 12, 14, 16 and Grids: 8 to 256 (common: 8, 16, 64). Stereotactic depth electrodes are detailed with Diameter (mm) 0.8*, 0.86*, 1.12, 1.26; Contact length (mm) 1.32, 2.0, 2.29*, 2.41*, 2.5, 5.0; Inter-contact Distance (mm) 2.2, 3.0, 3.5, 3.97, 4.0, 4.43, 5.0, 7.0, 8.0, 10.0; Number of Contacts 4, 5, 6, 8, 10, 12, 14, 15, 16, 18; Other Configurations 3 sets of 5 or 6 contacts, inter-contact distance 3.5 mm, with 7 or 11 mm between sets; and Recording Lengths (mm) 17 to 92. Values marked with an asterisk are noted as used relatively commonly in clinical practice.

---

### The three numbers you need to know about healthcare: the 60-30-10 challenge [^116VSahp]. BMC Medicine (2020). Medium credibility.

Background

Healthcare represents a paradox. While change is everywhere, performance has flatlined: 60% of care on average is in line with evidence- or consensus-based guidelines, 30% is some form of waste or of low value, and 10% is harm. The 60-30-10 Challenge has persisted for three decades.

Main Body

Current top-down or chain-logic strategies to address this problem, based essentially on linear models of change and relying on policies, hierarchies, and standardisation, have proven insufficient. Instead, we need to marry ideas drawn from complexity science and continuous improvement with proposals for creating a deep learning health system. This dynamic learning model has the potential to assemble relevant information including patients' histories, and clinical, patient, laboratory, and cost data for improved decision-making in real time, or close to real time. If we get it right, the learning health system will contribute to care being more evidence-based and less wasteful and harmful. It will need a purpose-designed digital backbone and infrastructure, apply artificial intelligence to support diagnosis and treatment options, harness genomic and other new data types, and create informed discussions of options between patients, families, and clinicians. While there will be many variants of the model, learning health systems will need to spread, and be encouraged to do so, principally through diffusion of innovation models and local adaptations.

Conclusion

Deep learning systems can enable us to better exploit expanding health datasets including traditional and newer forms of big and smaller-scale data, e.g. genomics and cost information, and incorporate patient preferences into decision-making. As we envisage it, a deep learning system will support healthcare's desire to continually improve, and make gains on the 60-30-10 dimensions. All modern health systems are awash with data, but it is only recently that we have been able to bring this together, operationalised, and turned into useful information by which to make more intelligent, timely decisions than in the past.

---

### Spontaneous perception of numerosity in humans [^111zpgYz]. Nature Communications (2016). Medium credibility.

Discussion

In this study we investigated what cues human subjects spontaneously use when judging Numerosity, Density and Area, by varying simultaneously area and density over a wide 2D space. The results show that at low to moderate densities, sensitivity is far higher to numerosity than to density or area. When subjects did not know what aspects of the stimuli were changing (odd-one-out task), they spontaneously based decisions on numerosity. Even when explicitly asked to base their decision of density, observers used numerosity rather than density for discriminations. Area discriminations were also strongly influenced by numerosity. These data speak against the idea that numerosity is extracted from area and density, as sensitivity to numerosity was lower than to either area or density. And when asked to judge density, subjects used numerosity as a primary cue. At high densities the situation changes: density and area seem to be sensed more directly, and numerosity thresholds are consistent with being calculated via density and area. This agrees with previous work showing that texture mechanisms can come into play in numerosity judgments of dense stimuli, and also shows that the technique can, under these circumstances, reveal that numerosity can derive from density and area.

Our data speak clearly to the important point in the dispute as to whether number and density share common resources. Previous evidence has suggested that number judgments can be influenced by geometrical attributes: for instance, there is a mild positive influence of area(that is, larger patches appear more numerous). The current data agree with those observations as the plane for numerosity judgments is slightly slanted below 45 degrees (average 38°) indicating that number estimation is mildly influenced by area. It has been argued that this influence parallels that of density, which is also strongly influenced by area. Our data confirm these previous observations but reveal the bigger picture, which is quite different: the influence of area on density in fact results from the density choice plane following the number axis.

Our data also confirm that density judgments are particularly noisy at low numerosities and improve at higher numerosities, tending towards a square root law. Area judgments are also performed rather poorly at low numerosities. As both area and density judgments are performed poorly, they would not make a very useful basis set from which to calculate numerosity, at least at low-mid densities; on the contrary, it seems that information about number is used to aid area and density discriminations.

---

### The neural correlates of retrieval and procedural strategies in mental arithmetic: a functional neuroimaging meta-analysis [^112kGaVC]. Human Brain Mapping (2023). Medium credibility.

For a contrast to be included in the retrieval problems map, the experimental condition had to fit the following criteria, and simultaneously, the control condition could not fit the following criteria. For example, for an easy > hard problems contrast to be included, easy, but not hard problems, must fit the following criteria:
The problem had two or fewer operands.
Neither operand was a negative number.
The participant reported using a retrieval strategy.
For addition problems (augend + addend = sum), the augend and the addend were both 10 or lower.
For subtraction problems (minuend – subtrahend = difference), the minuend and subtrahend were both single‐digit numbers.
For multiplication problems (multiplicand x multiplier = product), the multiplicand and the multipliers were both 11 or less.
For division problems (dividend/divisor = quotient), both the dividend and the divisors were 11 or less or the resulting quotient was a whole number

The procedural problems map included the following contrast types:
Arithmetic > control task
Procedural > retrieval
Hard problems > easy problems
Arithmetic task > basic number processing task
Problem size effect
Contrasting operation types, A > B

For a contrast to be included in the procedural problems map, the experimental condition had to fit the following criteria, and simultaneously, the control condition could not fit the following criteria. For example, for a hard > easy problems contrast to be included, hard, but not easy problems, must fit the following criteria:
Contrasts that had more than two operands (e.g. 4 + 5–6) were included.
At least one operand was a negative number.
The participant reported using a procedural strategy.
For addition problems (augend + addend = sum), at least one of the augend or addend was 11 or higher.
For subtraction problems (minuend – subtrahend = difference), at least one of the minuend or subtrahend was a double‐digit number.
For multiplication problems (multiplicand x multiplier = product), at least one of the multiplicand or the multipliers were 12 or greater.
For division problems (dividend/divisor = quotient) at least one of the dividend or divisors were 12 or greater or the resulting quotient was not a whole number.

---

### The productivity-biodiversity relationship varies across diversity dimensions [^111dfhL4]. Nature Communications (2019). High credibility.

Introduction

Biological diversity changes dramatically along the gradient of ecosystem productivity. This is particularly visible in plant communities that transform from marginal grasslands high above the tree line to semi-natural meadows and pastures and finally to lush broad-leaved forests. Understanding how ecological processes, such as environmental filtering, competitive exclusion, or evolutionary context affect these transformations remains an unresolved challenge in ecology (see ref.for a review). Here, we follow a recent call to employ approaches that embrace more complexityand show that simultaneously and systematically exploring productivity–biodiversity relationships across several biodiversity dimensions may provide deeper insight than focusing on one biodiversity dimension alone.

Two approaches have commonly been used to study the relationship between productivity and biodiversity. First, the shape and strength of productivity–biodiversity relationships have been studied intensively in natural and semi-natural systems based on observations. Observational studies mostly identified unimodal productivity–biodiversity relationships, but alternative relationships were also found (e.g. ref.). Biodiversity of the typically large numbers of species considered in such studies has commonly been approximated by species richness. Second, manipulation experiments have been conducted to test to which extent biodiversity promotes productivity. Such experiments identified positive associations between several biodiversity dimensions and productivity –. However, these findings may not be directly comparable to observation-based assessments since their typically local extent leads to comparably narrow productivity ranges covered, and because biodiversity change in experiments is induced by artificial community modifications. Observational studies, in essence, may thus offer a less distorted picture of productivity–biodiversity relationships in natural communities, but so far have been limited by the rather crude assumption that biodiversity equals species numbers.

Species richness ignores how communities are structured by both species dominance and similarity. Biodiversity estimates accounting for species dominance assign a higher weight to species with higher coverage or higher abundance in a community. With the same species richness, dominance-corrected biodiversity is higher for a community where abundances are more evenly distributed in comparison to a community dominated by a few common species. Additionally, species are not equal in terms of their functions and ecological strategies, and thus the similarity between species should be considered, with higher biodiversity in communities where species are more dissimilar. Ecologically relevant similarity information may be obtained directly, by measuring functional traits, or indirectly, by comparing the positions of species on a phylogenetic tree. Biodiversity dimensions considering similarity information can be used to test ecological theories that link species similarity directly to their performance in abiotic (e.g. environmental filtering) and biotic environments (e.g. competitive exclusion).

---

### Spectral descriptors for bulk metallic glasses based on the thermodynamics of competing crystalline phases [^112bwYka]. Nature Communications (2016). Medium credibility.

Results

Using databases for materials discovery

Carrying out electronic structure ab initio calculations for the infinite number of available states for a given alloy system is obviously impossible, especially when no lattice model can be built, as in the case of BMGs. Therefore, we adopt the agnostic approach of exploring structural prototypes mostly observed in nature for these types of systems. The method, shown to be capable of reasonably sampling the phase space and predicting novel compounds, is expected to estimate the thermodynamic density of states of an alloy system. We use the binary alloy data available in the AFLOW set of repositoriesto count the number of different structural phases in a given formation enthalpy range as a function of the composition. These data were obtained utilizing the VASPcode within the AFLOW computational materials design framework, at the density functional theory level of approximation. The binary alloy systems are fully relaxed in accordance with the AFLOW standard settings, which uses the GGA-PBEexchange correlation, PAW potentials, at least 6,000 k -points per reciprocal atom and a plane wave cutoff at least 1.4 times the largest value recommended for the VASP potentials of the constituents. The multiple different crystalline phases for each particular stoichiometry are built from the AFLOW library of common prototypes.

---

### Measurements of the gravitational constant using two independent methods [^115Geb4G]. Nature (2018). Excellent credibility.

The Newtonian gravitational constant, G, is one of the most fundamental constants of nature, but we still do not have an accurate value for it. Despite two centuries of experimental effort, the value of G remains the least precisely known of the fundamental constants. A discrepancy of up to 0.05 per cent in recent determinations of G suggests that there may be undiscovered systematic errors in the various existing methods. One way to resolve this issue is to measure G using a number of methods that are unlikely to involve the same systematic effects. Here we report two independent determinations of G using torsion pendulum experiments with the time-of-swing method and the angular-acceleration-feedback method. We obtain G values of 6.674184×10–11 and 6.674484×10–11 cubic metres per kilogram per second squared, with relative standard uncertainties of 11.64 and 11.61 parts per million, respectively. These values have the smallest uncertainties reported until now, and both agree with the latest recommended value within two standard deviations.

---

### Multiple routes to mammalian diversity [^111dqBjN]. Nature (2011). Excellent credibility.

The radiation of the mammals provides a 165-million-year test case for evolutionary theories of how species occupy and then fill ecological niches. It is widely assumed that species often diverge rapidly early in their evolution, and that this is followed by a longer, drawn-out period of slower evolutionary fine-tuning as natural selection fits organisms into an increasingly occupied niche space. But recent studies have hinted that the process may not be so simple. Here we apply statistical methods that automatically detect temporal shifts in the rate of evolution through time to a comprehensive mammalian phylogeny and data set of body sizes of 3,185 extant species. Unexpectedly, the majority of mammal species, including two of the most speciose orders (Rodentia and Chiroptera), have no history of substantial and sustained increases in the rates of evolution. Instead, a subset of the mammals has experienced an explosive increase (between 10- and 52-fold) in the rate of evolution along the single branch leading to the common ancestor of their monophyletic group (for example Chiroptera), followed by a quick return to lower or background levels. The remaining species are a taxonomically diverse assemblage showing a significant, sustained increase or decrease in their rates of evolution. These results necessarily decouple morphological diversification from speciation and suggest that the processes that give rise to the morphological diversity of a class of animals are far more free to vary than previously considered. Niches do not seem to fill up, and diversity seems to arise whenever, wherever and at whatever rate it is advantageous.

---

### Degenerate boundaries for multiple-alternative decisions [^114bQhG1]. Nature Communications (2022). High credibility.

Integration-to-threshold models of two-choice perceptual decision making have guided our understanding of human and animal behavior and neural processing. Although such models seem to extend naturally to multiple-choice decision making, consensus on a normative framework has yet to emerge, and hence the implications of threshold characteristics for multiple choices have only been partially explored. Here we consider sequential Bayesian inference and a conceptualisation of decision making as a particle diffusing in n-dimensions. We show by simulation that, within a parameterised subset of time-independent boundaries, the optimal decision boundaries comprise a degenerate family of nonlinear structures that jointly depend on the state of multiple accumulators and speed-accuracy trade-offs. This degeneracy is contrary to current 2-choice results where there is a single optimal threshold. Such boundaries support both stationary and collapsing thresholds as optimal strategies for decision-making, both of which result from stationary representations of nonlinear boundaries. Our findings point towards a normative theory of multiple-choice decision making, provide a characterisation of optimal decision thresholds under this framework, and inform the debate between stationary and dynamic decision boundaries for optimal decision making.

---

### Using the number needed to treat in clinical practice [^113ZsfNi]. Archives of Physical Medicine and Rehabilitation (2004). Low credibility.

The number needed to treat (NNT) is gaining attention as a method of reporting the results of clinical trails with dichotomous outcome measures. The NNT is defined as the number of patients who would need to be treated, on average, with a specific intervention to prevent 1 additional bad outcome or to achieve 1 desirable outcome in a given time period. Because it reports outcomes in terms of patient numbers, it is extremely useful to clinicians for making decisions about the effort expended with a particular intervention to achieve a single positive outcome. This special communication describes the NNT statistic and its utility for choosing clinical interventions.

---

### Worldwide divergence of values [^117KACtH]. Nature Communications (2024). High credibility.

Computing value distinctiveness across timepoints allowed us to estimate which countries have relatively unique values at any given time. In our main text, Fig. 3 summarizes each country's value distinctiveness score in the first and last wave that it was included in the WVS. Supplementary Fig. 2 visualizes value distinctiveness for each country in each WVS wave, and Supplementary Table 5 summarizes value distinctiveness for each country in each wave.

In addition to estimating the value distinctiveness of individual countries at specific timepoints, we also analyzed general trends in value distinctiveness over time. Our reasoning was that, if countries are diverging in their values, the average value distinctiveness coefficient would be increasing. This would indicate that countries are "spreading out" around the global medians of values. One limitation of this analysis is that the global midpoint is not truly "global" — it is only the average of the countries sampled by the WVS at a particular point in time. If the WVS has become systematically more diverse in its sampling, then this could artificially create value divergence via a trend in sample heterogeneity. This is why we repeated all of our analyses for subsets of countries that had participated in 2, 3, 4, and 5 WVS waves. It is also why we conducted the decade-over-decade analysis in which we replicated the finding when looking across a subset of 33 countries which provided data in the 1990s, 2000s, and 2010s — the three decades with the greatest WVS coverage. Supplementary Table 6 summarizes value distinctiveness scores for each decade and for each country within the 33-country sample.

---

### The suboptimality of perceptual decision making with multiple alternatives [^111j9QZg]. Nature Communications (2020). High credibility.

It is becoming widely appreciated that human perceptual decision making is suboptimal but the nature and origins of this suboptimality remain poorly understood. Most past research has employed tasks with two stimulus categories, but such designs cannot fully capture the limitations inherent in naturalistic perceptual decisions where choices are rarely between only two alternatives. We conduct four experiments with tasks involving multiple alternatives and use computational modeling to determine the decision-level representation on which the perceptual decisions are based. The results from all four experiments point to the existence of robust suboptimality such that most of the information in the sensory representation is lost during the transformation to a decision-level representation. These results reveal severe limits in the quality of decision-level representations for multiple alternatives and have strong implications about perceptual decision making in naturalistic settings.

---

### Natural selection 150 years on… [^117AJzk7]. Nature (2009). Excellent credibility.

The theory of evolution by natural selection has prospered in its first 150 years and provides a consistent account of species as highly adapted and rare survivors in the struggle for existence. It now faces the challenge of finding order in the evolution of complex systems, including human society.

---

### A faster and more accurate algorithm for calculating population genetics statistics requiring sums of stirling numbers of the first kind [^115kWKNC]. G3 (2020). Medium credibility.

Figure 3
(A) Comparison of relative error of the estimator from and the single term asymptotic estimator in (35). Relative error for each is calculated against the arbitrary precision implementation described in. In total, 10,000 calculations were performed with n randomly sampled from a uniform distribution between 50 and 500; m between 2 and n; and θ between 1 and 50. A solid diagonal line is drawn at. Dotted lines are drawn at a relative error of 0.001. Numbers within each quadrant defined by the dotted lines indicate the number of points in each quadrant. The red dot indicates the one case where the relative error wasand the error of (35) was greater than the estimator from. (B) Comparison of mollified error (34) as a function of m. For this plot, we fixed(solid lines) or 500 (dotted lines) and(as indicated by different line colors).

The fewer calculations led to a clear improvement in calculation speed (median 54.6x faster; Figure 4). The speedup also depends on the parameter choices; in general, the speed advantage is greater when the hybrid calculator requires many calculations (namely, when m is small relative to n, as the hybrid calculator performs the sum in (2)) (Figure 4B).

Figure 4
(A) Comparison of run times between the hybrid algorithm from and the single term asymptotic estimator in (35). 100 iterations were run, each with 10,000 calculations; the time elapsed for each set of 10,000 calculations was recorded and plotted here. The same set of parameters were used for each algorithm. The order of running the algorithms was alternated with each iteration. The dark horizontal line indicates the median, the box indicates the first and third quartiles, the whiskers are drawn at 1.5x the interquartile range, and outliers are represented by open circles. The median for the hybrid algorithm is 62.64 s; the median for the asymptotic algorithm is 1.17 s. (B) Detailed benchmarking for(open violins) or 500 (gray violins),… Fold speedup (ratio of the time taken for the hybrid calculator to that taken for the aysmptotic estimator) is plotted on the y-axis. Each dot represents one set of parameters; the violin plots summarize the density of points on the y-axis. Times were calculated for 100 iterations of each estimator for the same parameter values.

---

### Thermodynamics of quantum systems with multiple conserved quantities [^114kLtUs]. Nature Communications (2016). Medium credibility.

Recently, there has been much progress in understanding the thermodynamics of quantum systems, even for small individual systems. Most of this work has focused on the standard case where energy is the only conserved quantity. Here we consider a generalization of this work to deal with multiple conserved quantities. Each conserved quantity, which, importantly, need not commute with the rest, can be extracted and stored in its own battery. Unlike the standard case, in which the amount of extractable energy is constrained, here there is no limit on how much of any individual conserved quantity can be extracted. However, other conserved quantities must be supplied, and the second law constrains the combination of extractable quantities and the trade-offs between them. We present explicit protocols that allow us to perform arbitrarily good trade-offs and extract arbitrarily good combinations of conserved quantities from individual quantum systems.

---

### American association for bronchology and interventional pulmonology (AABIP) evidence-based guidelines on bronchoscopic diagnosis and staging of lung cancer [^116sw8Vs]. Journal of Bronchology & Interventional Pulmonology (2025). High credibility.

AABIP guidelines — panel B meta-analysis of multiple-tool versus single-tool approaches shows common effect model OR 1.63 [1.33; 2.00] and random effects model OR 1.64 [1.33; 2.02], with prediction interval [1.20; 2.24]. Heterogeneity is I² = 0.0%, τ² = 0.0031, p = 0.6847. The axis is labeled "Favours Single" and "Favours Multiple" (Odds Ratio [95% CI]).

---

### Degenerate boundaries for multiple-alternative decisions [^115KHdNH]. Nature Communications (2022). High credibility.

Rewards are generated according to the Bayes risk represented over single trials (equation 12), when stochastic decision trajectories encounter a decision boundary. A reward landscape can be generated by systematically varying decision boundary parameters and averaging the rewards generated over a large number of simulated trajectories for each set of parameters. Averaging over many reward outcomes reduces variance producing a smooth surface and simulates the expected reward, which coincides with sampling rewards to learn the decision boundary under equation (11). So each point in the reward landscape is the mean reward obtained from a distribution of rewards for that boundary shape (typically from 100,000 samples across the parameter ranges); differing landscapes with c / W discretized in 1000 steps across its range from 0 to 0.2 were then considered. Each landscape has a point of mean reward that is an absolute maximum, which we will denote by; however, the landscape is very noisy even after this averaging, with many other points with similar mean rewards. Therefore, we consider a set of parameters corresponding to multiple points x ∈ X on the landscape, with mean rewards r x and standard deviations σ x according to the distribution of rewards for those parameters. This set of parameters is selected such that the maximum mean reward value of the landscape falls within a δ σ x of the point, so that. Using some example reward landscapes, we found that as δ decreases, the area designated as maximum also decreases as expected. However, below δ = 0.02, the area of the maximal region does not appear to change, but the points selected became more sparse, so we use this value.

Reporting summary

Further information on research design is available in the Nature Research Reporting Summary linked to this article.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^116hTmko]. CDC (2011). Medium credibility.

Community viral load sample size calculations — sample size to detect differences in geometric mean (GM) viral load depends on power and standard deviation (S). The table specifies "α = 0.05 and W = 0.8", with S columns 1 to 1.5; for k = 3 the minimum sample sizes by S are 54, 66, 78, 92, 106, and 122, and "1.2 is the standard deviation of national VL data", so at S = 1.2 the needed sample size is 78.

---

### The language of crisis: spatiotemporal effects of COVID-19 pandemic dynamics on health crisis communications by political leaders [^111UojWg]. NPJ Digital Medicine (2022). Medium credibility.

Data analysis

For each state, for each word, we tabulated total word counts for each speech. We included only words spoken by 20 or more governors, and which represented at least 0.02% of all words spoken across all speeches in at least one of the 50 US states.

Words that met the above criteria were grouped by two independent raters into semantic categories. Words with multiple common meanings, or that could not be neatly categorized into a single semantic category, were excluded from the semantic grouping analysis on a case-by-case basis.

In addition to grouping words by semantic category, we also grouped words by part of speech using the part of speech tagger from the Natural Language Toolkit library of Python version 3.6.2. We also calculated measures of linguistic complexity, including average word length (number of letters) per speech, and average word syllable count per speech.

We systematically analyzed the associations of each of these linguistic features (semantic categories, parts of speech, word length and syllable count) with COVID-19 case rates per 100,000 persons. We analyzed these associations over space — across each of the 50 US states for the entire duration of the study period, as well as over time — across each week of the study period for a given single US state. For the spatial analyses, we normalized word counts as a percentage of all words spoken by that governor across all speeches in that state. For the temporal analyses, we normalized word counts as a percentage of all words spoken by that governor across all included speeches in that state during that given week. To ensure sufficient sample size, the temporal analyses were conducted only in states for which a total of at least 50,000 words from speeches were available.

For the spatial analyses, we calculated confidence intervals using the formula for finding the confidence interval around a single Spearman's Rho value: where r is the estimate of the correlation and n is the sample size. For the temporal analyses, in which multiple Spearman's Rho values were available (one from each included state), we used the standard formula for a distribution of values. As a visual aid, we fit polynomial curves to the temporal plots using the numpy.polyfit function of Python version 3.6.2, and plotted them alongside the original data points.

Reporting summary

Further information on research design is available in the Nature Research Reporting Summary linked to this article.

---

### Dynamics of ranking [^113GmTqL]. Nature Communications (2022). High credibility.

Virtually anything can be and is ranked; people, institutions, countries, words, genes. Rankings reduce complex systems to ordered lists, reflecting the ability of their elements to perform relevant functions, and are being used from socioeconomic policy to knowledge extraction. A century of research has found regularities when temporal rank data is aggregated. Far less is known, however, about how rankings change in time. Here we explore the dynamics of 30 rankings in natural, social, economic, and infrastructural systems, comprising millions of elements and timescales from minutes to centuries. We find that the flux of new elements determines the stability of a ranking: for high flux only the top of the list is stable, otherwise top and bottom are equally stable. We show that two basic mechanisms - displacement and replacement of elements - capture empirical ranking dynamics. The model uncovers two regimes of behavior; fast and large rank changes, or slow diffusion. Our results indicate that the balance between robustness and adaptability in ranked systems might be governed by simple random processes irrespective of system details.

---

### Physical activity and exercise during pregnancy and the postpartum period: ACOG committee opinion, number 804 [^1148pFPr]. Obstetrics and Gynecology (2020). High credibility.

Recommended weight limits for lifting at work during pregnancy — an on-page algorithm and stepwise method describe how to determine a recommended weight limit (RWL). The algorithm asks, "At work do you perform lifting tasks more than once every 5 minutes?" and, if yes, "Do you lift more than three times per minute?", then queries "How many hours per day do you spend lifting at work?" with categories "Less than 1 continuous hour" or "1 or more hours per day", corresponding to infrequent, repetitive short duration, or repetitive long duration lifting panels. Steps instruct to "select the one graphic (A, B, or C) that best describes the lifting frequency or frequency/duration pattern", then "When less than 20 weeks pregnant, select the image on the left of the graphic; when pregnant for 20 weeks or more, select the image on the right", to underline all relevant numerical values along the lift path and circle the lowest, and that "The number circled in step 3 is the RWL (in pounds)" with a proviso that the short-duration category can include multiple hours per day provided each continuous bout is less than 1 hour and followed by at least 1 hour of nonlifting before resuming; the figure presents "Provisional recommended weight limits for lifting at work during pregnancy".

---

### Understanding the relationship between plasmodium falciparum growth rate and multiplicity of infection [^112dhcKw]. The Journal of Infectious Diseases (2015). Low credibility.

Natural infections with Plasmodium falciparum are often composed of multiple concurrent genetically distinct parasite clones. Such multiclonal infections are more common in areas of high transmission, and the frequency of multiclonal infection also varies with age. A number of studies have suggested that multiclonal infection predicts the risk of subsequent clinical malaria. The multiplicity of infection is determined by the rate of new infections, the number of clones inoculated at each mosquito bite, and the duration of infections. Here, we used a mathematical modeling approach to understand how variation in the growth rate of blood-stage parasites affects the observed multiplicity of infection (MOI), as well as the relationship between the MOI and the risk of subsequent malaria. We then analyzed data from a study of multiclonal infection and malaria in an malaria-endemic area in Tanzania and show that the proportion of multiclonal infections varies with age and that the observed relationship between multiclonal infection and subsequent clinical events can be explained by a reduction in blood-stage parasite growth with age in this population.

---

### In search of a drosophila core cellular network with single-cell transcriptome data [^113WeFZH]. G3 (2022). Medium credibility.

Coexpression networks in fly brain cells are highly context-dependent

If a core cellular network exists, we expect its edges to be present in all cell types. The distribution of edge commonality was right skewed, with more than 75% of the edges specific to fewer than 5 cell clusters and only 0.4% of edges common to more than 30 cell clusters (Fig. 2a). The largest observed commonality was 64, observed for only 2 edges. Given that 67 cell clusters were analyzed, no edges were found in every cluster. As a complement to the observed edge commonality distribution, we also plotted the gene commonality distribution, where gene commonality indicates the number of cell clusters in which a given gene shared an edge with at least one other gene. The gene commonality distribution showed that most genes had one or more edges in the majority of cell clusters, and 613 genes had at least one edge in all 67 cell clusters (Fig. 2a). Thus, commonly expressed genes were frequently coexpressed with other genes, though the specific coexpression partners vary among different cell clusters.

Fig. 2.
Cell cluster-specific coexpression networks share many more edges than random networks. a) Edge and gene commonality distributions. The commonality of an edge indicates the number of cell clusters in which that edge is detected (top). The commonality of a gene refers to the number of cell clusters in which that gene is coexpressed with at least one gene (bottom). The y -axis shows the frequency of genes or edges in the corresponding commonality score group. The numbers on top of each bar indicate the number of edges or genes in that commonality group. b) The observed edge commonality distribution (yellow) compared with the null expectation derived from network randomization (gray). Network randomization was performed 100 times for each cell cluster individually with network size (number of nodes and edges) and degree distribution (the number of coexpressed gene partners per gene) fixed. c) Yellow points and violin plot shows the adjusted P -value distributions of 100 sampled edges in each edge commonality group using a rank aggregation method. Edge commonality groups were selected to cover the full range of edge commonality scores. Each point represents a sampled edge, with black points indicating median values. The black curve line shows the Bonferroni-corrected combined P -value (Fisher's method) above each commonality group, values corresponding to the right axis.

---

### Degenerate boundaries for multiple-alternative decisions [^1155wMtM]. Nature Communications (2022). High credibility.

These dynamic decision thresholds have a range of temporal structures for each cost ratio c / W, separating naturally into three categories: increasing (Fig. 8, top row), collapsing (middle row), and static thresholds (bottom row). These categories appear to correlate with the shape of the decision boundary: increasing thresholds with convex boundaries (e.g. Fig. 7 a, dark blue curve), decreasing thresholds with concave boundaries (e.g. Fig. 7 a, red curve), and static thresholds with flat boundaries (e.g. Fig. 7 a, yellow line). This correlation seems to originate in an increase in decision time as the belief moves away from equality between choices (Fig. 7 a–c, shading). All things being equal, decisions that terminate with higher beliefs tend to be more accurate, whereas decisions terminating with a low belief of the choice tend to be less accurate. Therefore, experimental observation of implicitly dynamic decision boundaries (including increasing thresholds) could be due to time-dependent accuracy plots of data from individual subjects.

We emphasize that while these implicit threshold dynamics look like temporal dependence, they are, in fact due to the spatial structure of the decision boundary. Flat decision boundaries can therefore be interpreted as a special case where the boundary on the belief in one choice to cross its threshold is independent of the beliefs of the other choices; however, even then, the mean decision time and mean error have a spatial structure along these boundaries (Fig. 7).

Comparison to current models, interpretations, and predictions

There are few normative approaches to modeling multiple-choice decision-making, with the recent study of ref.the state-of-the-art in using an evidence accumulation vector accumulated in a race model with a boundary in n -dimensional space. Using dynamic programming, they find optimal decision boundaries for free-response, mixed-difficulty trials are nonlinear and collapse over time. Clearly, their n DRM is closely linked to the model presented here, but there are key differences: the models have a different evidence structure, trial structure, and optimization process, which leads to diverging perspectives on the nature of multiple-choice decision boundaries. In the following, we describe how these perspectives can be reconciled, and in doing so gain a broader understanding of normalization and inference in multiple-choice decision-making.

---

### Life after the screen: making sense of many P-values [^114n2V5v]. Genetic Epidemiology (2001). Low credibility.

A multiple analytic approach may be useful for analyzing complex traits since different methods extract both similar and distinct, but complementary pieces of information from genome screen data on extended pedigrees. We examined the usefulness of combining p-values both across methods and across adjacent markers, taking into account the observed correlation structure among these p-values. To this end, we employed the recently proposed truncated product method [Zaykin et al. Genet Epidemiol, in press]. It appears that this approach is helpful for visualizing priority regions for follow-up analysis and reducing the number of false-positive linkage signals.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^115Xt6bN]. CDC (2011). Medium credibility.

Table 1b — Minimum sample size for detecting geometric mean (GM) ratio k with α = 0.05 and W = 0.9 — presents required sample sizes by standard deviation (S) strata and defines parameters; α is significance level, W is power, GM is geometric mean, and S is standard deviation, noting that 1.2 is the standard deviation of national viral load (VL) data. Each jurisdiction will need to assess the standard deviation of their local VL data and then determine the appropriate sample size needed to assess VL, and if the sample size is inadequate to meet the recommended case inclusion criterion, an alternate method may need to be used, such as combining multiple years of data. For k = 3, sample sizes across S = 1, 1.1, 1.2, 1.3, 1.4, 1.5 are 75, 91, 108, 127, 147, 169, and Table 1b uses power = 90%; jurisdictions may also explore differences in means of viral loads, including categorical differences in the proportion with undetectable or very low VL.

---

### Common variable immunodeficiency: diagnosis, management, and treatment [^116LJ6MF]. Immunology and Allergy Clinics of North America (2015). Low credibility.

Common variable immunodeficiency (CVID) refers to a grouping of antibody deficiencies that lack a more specific genetic or phenotypic classification. It is the immunodeficiency classification with the greatest number of constituents, likely because of the numerous ways in which antibody production can be impaired and the frequency in which antibody production becomes impaired in human beings. CVID comprises a heterogeneous group of rare diseases. Consequently, CVID presents a significant challenge for researchers and clinicians. Despite these difficulties, both our understanding of and ability to manage this grouping of complex immune diseases has advanced significantly over the past 60 years.

---

### Degenerate boundaries for multiple-alternative decisions [^112whsRx]. Nature Communications (2022). High credibility.

At the end of a decision episode, when a choice is made, the decision time and error are combined into a single reward. Here, we formulate reward as a linear combination of error and decision time weighted by their associated costs W i and c :This is a standard reward function used in a wide range of past work, for example, ref. Unequal error costs W i ≠ W j induce choice-dependent reward, where hypothesis-dependent error costs are relative to the "true" hypothesis and to each other. For tractability, we will assume all error costs are equal, with the expectation that similar results hold in the unequal cost case but that the analysis will be more complicated. We also consider a constant (time-independent) cost c per time step, assuming stationarity of evidence distributions and evidence accumulation in a free-response task. A challenging aspect of this framework is that reward is highly stochastic due to the random nature of evidence sampling. How then do we define optimality?

In this paper, we come from the view that humans and animals maximize expected reward. Then the optimum decision boundary maximizes the average reward for a given ratio of costs c / W. Monte Carlo simulations of decision trajectories of independent trials, using the formalism outlined above and the evidence inference method derived in the next section, yield reward values for a set of candidate decision boundaries. In general, we find a set of high-dimensional nonlinear, complex boundaries. We will show that these boundaries are consistent with a range of behavioral and phenomenological results along with testable neurophysiological predictions.

---

### A Bayesian model for assessing the frequency of multiple mating in nature [^1179Cxzw]. The Journal of Heredity (2003). Low credibility.

Many breeding systems have multiple mating, in which males or females mate with multiple partners. With the advent of molecular markers, it is now possible to detect multiple mating in nature. However, no model yet exists to effectively assess the frequency of multiple mating (f(mm)) — the proportion of broods with at least two males (or females) genetically contributing — from limited genetic data. We present a single-sex model based on Bayes' rule that incorporates the numbers of loci, alleles, offspring, and genetic parents. Two genetic criteria for calculating f(mm) are considered: the proportion of broods with three or more paternal (or maternal) alleles at any one locus and the total number of haplotypes observed in each brood. The former criterion provides the most precise estimates of f(mm). The model enables the calculation of confidence intervals and allows mutations (or typing errors) to be incorporated into the calculation. Failure to account for mutations can result in overestimates of f(mm). The model can also utilize other biological data, such as behavioral observations during mating, thereby increasing the accuracy of the calculation as compared to previous models. For example, when two sires contribute equally to multiply mated broods, only three loci with five equally common alleles are required to provide estimates of f(mm) with high precision. We demonstrate the model with an example addressing the frequency of multiple paternity in small versus large clutches of the endangered Kemp's Ridley sea turtle (Lepidochelys kempi) and show that females that lay large clutches are more likely to have multiply mated.

---

### A problem of persistence: still more questions than answers? [^111RRQix]. Nature Reviews: Microbiology (2013). Medium credibility.

The current antibiotic resistance crisis has led to increased pressure to prioritize strategies to tackle the issue, with a strong focus being placed on the development of novel antimicrobials. However, one major obstacle that is often overlooked is persister cells, which are refractory to antibiotic treatment. Tackling persistence is a challenge because these cell types are extremely difficult to study and, consequently, little is known about their physiology and the factors that lead to their emergence. Here, four experts contemplate the main physiological features that define persistence and the implications of persistence for antibiotic treatment regimens, and consider what the study of bacterial persistence has taught us about the heterogeneity of bacterial populations.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^112Gttbo]. CDC (2011). Medium credibility.

Table 2 — types of viral load (VL) analyses and measures — describes mean-based metrics that average per-case results. One metric is calculated by "Determine the most recent VL for each case, total the values of those VLs, and divide by the number of cases with a VL" and indicates "The average number of virus particles in a population; can be influenced by large outliers; useful for comparisons between subpopulations (e.g., disparities)". A log-transformed variant uses "Log transform the most recent VL for each case, average the log transformed values, total the average values, and divide by the number of cases with a VL" and "The log transformation will provide a stable estimate by reducing the influence of outliers; tightens the association of trend between decline in CVL and new diagnoses". Another metric based on the per-case mean is computed by "Determine the mean VLs for each case, total the values of those mean VLs, and divide by the number of cases with a VL" and "Provides an average of the average number of virus particles per case; this may reflect a greater influence of those individuals with multiple measurements and those who were started on ART and trended towards suppression within 1 year". For starred metrics, " The denominator includes all viral load results, including undetectable viral load".

---

### Optimizing natural fertility: a committee opinion [^114WYnNb]. Fertility and Sterility (2013). Medium credibility.

Age-related fertility — summary statement: The chances of pregnancy and live birth for women significantly decrease after age 35, as the risks of aneuploidy and miscarriage increase.

---

### As-3 [^117FFFuc]. FDA (2025). Medium credibility.

Storage and handling

Store at room temperature (25 °C/77 °F). Avoid excessive heat.

Protect from freezing

---

### A probabilistic model for the MRMC method, part 2: validation and applications [^115WqH1X]. Academic Radiology (2006). Low credibility.

Rationale and Objectives

We have previously described a probabilistic model for the multiple-reader, multiple-case paradigm for receiver operating characteristic analysis. When the figure of merit is the Wilcoxon statistic, this model returns a seven-term expansion for the variance of this statistic as a function of the numbers of cases and readers. This probabilistic model also provides expressions for the coefficients in the seven-term expansion in terms of expectations over the internal noise, readers, and cases. Finally, this probabilistic model sets bounds on both the overall variance of the Wilcoxon statistic and the individual coefficients.

Materials and Methods

In this article, we will first validate the probabilistic model by comparing variances determined by direct computation of the expansion coefficients to empirical estimates of the variance using independent sampling. Validation of the probabilistic model will enable us to use the direct estimates of the expansion coefficients as a gold standard to compare other coefficient-estimation techniques. Next, we develop a coefficient-estimation technique that employs bootstrapping to estimate the Wilcoxon statistic variance for different numbers of readers and cases. We then employ constrained, least-squares fitting techniques to estimate the expansion coefficients. The constraints used in this fitting are derived directly from the probabilistic model.

Results

Using two different simulation studies, we show that the novel (and practical) bootstrapping/fitting technique returns estimates of the coefficients that are consistent with the gold standard.

Conclusion

The results presented also serve to validate the seven-term expansion for the variance of the Wilcoxon statistic.

---

### Casein [^117U7k7m]. FDA (2009). Low credibility.

Volume desired x Concentration desired = Volume needed x Concentration available.

Example 1: If a 1:10 w/v extract is available and it is desired to use a 1:1,000 w/v extract substitute as follows:

Vd x Cd = Vn x Ca

10ml x 0.001 = Vn x 0.1

0.1 mL = Vn

Using a sterile technique, remove 0.10 mL of extract from the 1:10 vial and place it into a vial containing 9.90 mL of sterile diluent. The resulting ratio will be a 10 mL vial of 1:1,000 w/v.

Example 2: If a 10,000 pnu/ml extract is available and it is desired to use a 100 pnu/ml extract substitute as follows:

10ml x 100 = Vn x 10,000

0.1 mL = Vn

Using a sterile technique, remove 0.10 mL of extract from the 10,000 pnu/ml vial and place it into a vial containing 9.90 mL of sterile diluent. The resulting concentration will be a 10 mL vial of 100 pnu/ml.

Example 3: If a 10,000 AU/ml or BAU/ml extract is available and it is desired to use a 100 AU/ml or BAU/ml extract substitute as follows: Vd x Cd = Vn x Ca

---

### Optimizing natural fertility: a committee opinion [^111FA5eG]. Fertility and Sterility (2013). Medium credibility.

Natural fertility — intercourse frequency: Cycle fecundity was similar with intercourse that occurred daily, every other day, and even every 3 days in the fertile window, but was lowest when intercourse occurred only once in the fertile window. Couples should be informed that reproductive efficiency increases with the frequency of intercourse and is highest when intercourse occurs every 1 to 2 days during the fertile window. Intercourse more frequently than every 1 to 2 days is not associated with lower fecundity, and couples should not be advised to limit the frequency of intercourse when trying to achieve pregnancy.

---

### Worldwide divergence of values [^1135SyoG]. Nature Communications (2024). High credibility.

In this work, we develop a general method to test whether social values have diverged or converged across the 76 countries that have provided multiple waves of WVS data. Instead of focusing on the mean levels of particular values over time, as in most prior studies, we create metrics that explicitly measure variation in values (see Table 1). These measures are meant to represent the same outcome — convergence vs. divergence of values across world nations. However, they vary in their level of analysis. The first measure, "value variation", focuses on divergence at the level of the WVS item whereas the second measure, "value distinctiveness", focuses on divergence at the country level. Table 1 describes both measures and defines a third measure called "within-country heterogeneity", which we use in secondary analyses to represent the homogeneity vs. heterogeneity of values within a nation. We define all three measures here so that readers can distinguish between them in our presentation of results.

Tables 1
Descriptions of key measures

Our main analyses test whether value variation and value distinctiveness are increasing or decreasing over time. Increases indicate value divergence, and decreases indicate value convergence. We try to test this relationship as rigorously as possible. For example, we include several robustness checks to ensure that the results are not driven by the way we normalize the items or the sampling strategy of the WVS. In our main text, we complement our primary analyses by analyzing different subsets of countries and by exploring geopolitical variables that can explain country-level clustering based on value similarity. Supplemental analyses show that results replicate with different approaches to normalizing means for items that have different response scales and across demographically weighted and unweighted country-level scores. Our materials and methods summarize our analysis procedure, and the Supplementary Methods have an "extended" materials and methods section with more detail.

Here we use this approach to show that values have diverged across national cultures over time. This value divergence mainly characterizes a growing gap between high-income Western countries and the rest of the world on emancipative values. We also find that worldwide value divergence has been accompanied by value convergence among countries in the same region.

---

### Allergy diagnostic testing: an updated practice parameter [^114ZPfFG]. Annals of Allergy, Asthma & Immunology (2008). Medium credibility.

Multiallergen screening tests — utility and limitations: Tests coupling multiple allergens to a single solid-phase substrate have been developed; if the multiple allergen test result is positive, there is a high probability that the patient is allergic to at least 1 of the allergens included, and additional tests that use individual allergens can then determine other sensitivities. In general, these multiallergen screening tests have shown acceptable diagnostic sensitivity and specificity when compared with skin tests, but their clinical value depends on the selection of patients. In a symptomatic self-selected population, a positive result significantly increases the probability of allergy, whereas in an unselected population there would be an unacceptable number of false-positive and false-negative results. By itself, a positive multiple allergen test result does not provide sufficient information to make a specific clinical diagnosis or to initiate therapy, and a negative result does not exclude clinical sensitivity because commercially-available multiallergen screening tests only screen for approximately 15 aeroallergens. Recommendations concerning the number of specific IgE tests for confirmation of suspected clinical sensitivity correspond to those discussed for prick/puncture tests in Summary Statement 43.

---

### Optimizing natural fertility: a committee opinion [^113YkxDz]. Fertility and Sterility (2013). Medium credibility.

Clinical pregnancy probability — cycle day and age: The probability of clinical pregnancy increases from 3.2% on cycle day 8 to 9.4% on cycle day 12 and decreases to less than 2% on cycle day 21, and the likelihood of success decreases with increasing age.

---

### Beyond pairwise mechanisms of species coexistence in complex communities [^111q8U4o]. Nature (2017). Excellent credibility.

The tremendous diversity of species in ecological communities has motivated a century of research into the mechanisms that maintain biodiversity. However, much of this work examines the coexistence of just pairs of competitors. This approach ignores those mechanisms of coexistence that emerge only in diverse competitive networks. Despite the potential for these mechanisms to create conditions under which the loss of one competitor triggers the loss of others, we lack the knowledge needed to judge their importance for coexistence in nature. Progress requires borrowing insight from the study of multitrophic interaction networks, and coupling empirical data to models of competition.

---

### Multiple tipping points and optimal repairing in interacting networks [^116G3JrX]. Nature Communications (2016). Medium credibility.

We study the international CDS system during the period between June 2005, the earliest date when CDSs traded for all countries, and February 2014. We apply the network model to it as follows. We represent each country with one node that can have two states: active or failed. As the raw CDS values are continuous by nature and our model uses binary node states (up or down), we perform a trend mapping procedure to form a binary signal (0 or 1) for each country. In particular, for each time t, we consider the interval [t −252, t] of 252 business days (the usual number of business days in a year). If the CDS value of a country has a net increase during that period, we consider the node of the country to be active at t (state = 1). If it does not, it is inactive (state = 0). Having individual binary signals for each country, we can calculate the average activity 0 ≤ z (t) ≤ 1 for both EU and Latin American networks. The resulting time series for EU and LA activities are shown in Fig. 6b. First, we note that the two geographical networks spend most of time having either a significantly high activity or significantly low activity (that is, there is an indication for two well-defined single-network states). We confirm this by measuring the frequency distribution of network activities (Fig. 7a, b), which exhibit a strong bimodality in z. The CDS network system in Fig. 6b shows rapid transitions between the high and low activity states, much similar to the artificial network system in Fig. 6a. Figure 7c shows the calculated correlations between binary signals of pairs of individual nodes. The correlation matrix reveals two strongly correlated blocks, which we identify as Latin American block (numbers 1–8) and EU block (numbers 9–25).

---

### Integrating multiple sources of ecological data to unveil macroscale species abundance [^117K7By7]. Nature Communications (2020). High credibility.

Implications for biodiversity conservation

Red listing of threatened species is a process of prioritisation that is a fundamental step of conservation planning. Modern red lists, including the IUCN Red Listand a number of national red lists, are based on quantitative assessments of rates of population decline, extinction risk, area of species range, and population abundance. Nonetheless, such assessments are difficult to conduct for every species; as a result, red lists are largely incomplete, with several potentially threatened species left unevaluated. Evaluation of macroscale species abundance is likely to mitigate this limitation in the process of red listing of species by facilitating the assessment of spatial distribution of abundance for a broad range of species.

We summarised the abundance and area of occupancy of species with respect to their category in the national red list of vascular plants in Japan, which essentially adheres to the IUCN Red List criteria but comprises modified categories. Details of the categories in this list are described in the "Methods" section (subsection, National red list categories of species in Japan). We obtained the regional abundance of each species by aggregating the abundance estimates over all grid cells. In addition, for each species, we obtained the area of occupancy within the region by summing the area of natural forest over all grid cells weighted by the estimated posterior probability of their presence.

We observe that the categories of the national plant red list adequately reflect the magnitude of abundance and area of occupancy of species in the country (Fig. 5). Nevertheless, the result indicated that the NC category (Not Classified; species that are not classified to the at-risk categories) contains a number of species with a limited number of individuals and area of occupancy. Most of these species may not be classified in threatened categories immediately because the IUCN Red List criteria based on range size and population size require additional conditions, such as population decline and habitat fragmentation; species with < 20 km² area of occupancy can be classified, however, as Vulnerable (VU) according to the IUCN Red List D2 criterion. The result thus implies that there are a number of species for which potential threats of extinction may have been overlooked.

Fig. 5
Abundance and area of occupancy of woody plant species in natural forests in Japan, with relation to their national red list categories.

NC not classified; NT near threatened; VU vulnerable; EN endangered; CR critically endangered. Error bars in the scatter plot indicate standard errors. Box plots: middle line, median; box, first and third quartiles; whiskers, minima and maxima.

---

### Why population attributable fractions can sum to more than one [^112rStC1]. American Journal of Preventive Medicine (2004). Low credibility.

Background

Population attributable fractions (PAFs) are useful for estimating the proportion of disease cases that could be prevented if risk factors were reduced or eliminated. For diseases with multiple risk factors, PAFs of individual risk factors can sum to more than 1, a result suggesting the impossible situation in which more than 100% of cases are preventable.

Methods

A hypothetical example in which risk factors for a disease were eliminated in different sequences was analyzed to show why PAFs can sum to more than 1.

Results

PAF estimates assume each risk factor is the first to be eliminated, thereby describing mutually exclusive scenarios that are illogical to sum, except under special circumstances. PAFs can sum to more than 1 because some individuals with more than one risk factor can have disease prevented in more than one way, and the prevented cases of these individuals could be counted more than once. Upper and lower limits of sequential attributable fractions (SAFs) can be calculated to describe the maximum and minimum proportions of the original number of disease cases that would be prevented if a particular risk factor were eliminated.

Conclusions

Improved descriptions of the assumptions that underlie the PAF calculations, use of SAF limits, or multivariable PAFs would help avoid unrealistic estimates of the disease burden that would be prevented after resources are expended to reduce or eliminate multiple risk factors.

---

### A practical guide for understanding confidence intervals and P values [^115vcziB]. Otolaryngology — Head and Neck Surgery (2009). Low credibility.

The 95 percent confidence interval about the mean demarcates the range of values in which the mean would fall if many samples from the universal parent population were taken. In other words, if the same observation, experiment, or trial were done over and over with a different sample of subjects, but with the same characteristics as the original sample, 95 percent of the means from those repeated measures would fall within this range. This gives a measure of how confident we are in the original mean. It tells us not only whether the results are statistically significant because the CI falls totally on one side or the other of the no difference marker (0 if continuous variables; 1 if proportions), but also the actual values so that we might determine if the data seem clinically important. In contrast, the P value tells us only whether the results are statistically significant, without translating that information into values relative to the variable that was measured. Consequently, the CI is a better choice to describe the results of observations, experiments, or trials.

---

### 30 × 30 biodiversity gains rely on national coordination [^117TtDb8]. Nature Communications (2023). High credibility.

Biodiversity trade-offs

To assess the biodiversity trade-offs associated with different conservation priorities, we used a modified SPI to measure the percentage of taxa considered adequately protected. SPI works by setting species-specific protection targets, based on how common or rare a species is across the landscape. Since we used probabilistic species distributions, we consider a species range to be the sum of probabilities in all cells across a landscape. The top 10% most common species, with large ranges, require at least 10% of their range inside protected areas to be considered adequately protected. The top 10% rarest species, with small ranges, require 100% of their range inside protected areas to be considered adequately protected. For the 80% of species between these thresholds, we used a log-linear model to set species-specific goals. Species that met or surpassed their conservation goals were considered "protected", while species that did not meet their goals were left "unprotected" by conservation scenarios.

We calculated biodiversity trade-offs as the relative difference between the percentage of species considered "protected" under the National prioritization scenario and the number of species considered "protected" under other scenarios. For example, if the national prioritization protected 10 species, and a different conservation priority scenario only protected five species, the biodiversity trade-off would be (5/10) × 100 = 50%. Or in other words, the different conservation priority protects 50% of the biodiversity compared to the optimum national prioritization. We calculated biodiversity trade-offs for all biodiversity (all species) as well as for birds, mammals, amphibians & reptiles, plants, butterflies, COSEWIC species at-risk, and IUCN species at-risk separately. In addition, we also quantified functional and phylogenetic biodiversity trade-offs by calculating the total functional and phylogenetic contributions of "protected" species as a percentage of total Canadian functional and phylogenetic diversity (the sum of all species contributions).

We also calculated trade-offs using weighted endemism which represents the "percentage" of biodiversity in protected cells. Because SPI and weighted endemism trade-offs were highly correlated, we chose to report SPI in the main text (Fig. 2b) and include both in the supplementary information (Supplementary Data 1).

Spatial commitments

To highlight the uneven challenge posed when prioritizing biodiversity, we calculated the total amount of each Province & Territory and Ecozone in the top 30% of the national prioritization scenario (Fig. 3).

---

### Three diverse motives for information sharing [^114spYgT]. Communications Psychology (2024). Medium credibility.

Information-seeking task

Following instructions, participants answered six comprehension questions before the first block and one question before the second block. Participants who responded incorrectly twice on at least one question were not permitted to go on to complete the task. After reading the instructions, participants completed two example trials before starting the actual task.

The task is illustrated in Fig. 1a. Recipients were told they owned 100 stocks in a financial market we created. On each trial recipients were presented with an algorithm's prediction of the value of their stocks and the algorithm's average prediction accuracy (the algorithms could be different on each trial). These cues were presented for 5 s. Predictions regarding the stocks' value ranged from −400 to −500 and from +400 to +500. A positive stock value meant recipients were earning money, a negative value meant they were losing money. We use the word 'valence' to indicate both the direction of the change in stock's value (positive or negative) and the magnitude of such change. The algorithm's prediction accuracy ranged from 0 to 99%, high numbers suggest the algorithm is often correct and vice versa. The algorithm's accuracy represents the probability of the revealed stocks' value being the real value. When the prediction is incorrect, the stocks' value is uniformly distributed over the possible values. Stocks' value and uncertainty level were randomly sampled for each trial. Recipients then indicated whether they wanted to open an envelope containing information about the true value of their stocks (information-seeking decision). They did so using a seven-point Likert scale ranging from 0, "Not at all", to 6, "Very much". We adopted a 7-point Likert scale rather than a binary response scale to increase the sensitivity of our measure. Recipients were told that the closer their answer was to "Very much", the more likely we were to open the envelope and reveal the value of their stocks, and vice versa (similar to other studies. If they selected 0, 1, 2, 3, 4, 5, or 6, then the information was delivered with a probability of 5%, 20%, 35%, 50%, 65%, 80%, and 95%, respectively. This probabilistic approach, which is adapted from past studies, is reflective of real life where actions are often not deterministically related to outcomes (e.g. one may ask a question but not receive an answer and may need to ask again if they really want to know). Recipients were not aware of these exact mathematical conversions as we did not want them to focus their attention on exact mathematical calculations to avoid distraction. Next, either the value of their stocks (information) was presented on screen for 4 s or hidden ('XX' was shown).

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^113SXd3E]. CDC (2011). Medium credibility.

Table 2 — additional community viral load (VL) measures — includes a cumulative measure defined as "Sum of VLs for all cases" that reflects "The number of virus particles in the population who have a VL; affected by the number of cases; reflects both prevalence of HIV cases and burden of viremia". A suppression proportion is defined by "The numerator is the number of cases whose most recent VL was suppressed; the denominator is the number of cases with a VL" and interpreted as "Maximal Virologic Suppression, or the percent virologically suppressed reflects ART uptake, adherence, and effectiveness in clinical settings; a quality of HIV care measure; in jurisdictions offering universal treatment, an important marker of uptake of universal ART guidelines". Calculation timing guidance notes "Time needs to be defined for each calculation, usually a 12-month period. Pooling over a few years can increase VL completeness but is harder to interpret". The abbreviation is specified as "ART, antiretroviral treatment".

---

### Cutting edge or blunt instrument: how to decide if a stepped wedge design is right for you [^112pSeFT]. BMJ Quality & Safety (2021). High credibility.

When might I consider doing a stepped wedge trial?

Research designs are shaped as much by practical constraints as by abstract schemes, and it is always a good idea to start with the constraints and work towards a design, rather than start with a design and try to fit it to constraints. These constraints will be unique to each research context, and box 1 lists some areas to think about. Still, there are some common features of settings where a stepped wedge trial might be considered as a possible design, and we now review these.

Box 1
Practical constraints on the design of a longitudinal cluster randomised trial

Are there limits on the time available to complete the evaluation, on the number of clusters, or on the number of participants (or the rate at which you can recruit participants) at each cluster? These constraints put limits on the overall scale of the evaluation, or force trade-offs between different design characteristics.
How will participants and their data be sampled in your study: as a series of cross-sectional surveys, as a continuous stream of incident cases, as a cohort followed over time, or some other way? Does the timescale divide into cycles, seasons or milestones that influence how you will sample participants and data?
Is there a limit on how many clusters can implement the intervention at the same time in the evaluation? If this is constrained by research resources (eg, if there are only enough trained research staff to implement the intervention one cluster at a time) then implementation must be staggered in some way.
If implementation is to be staggered, is there a minimum 'step length'? If the same team delivers the intervention in different clusters at different steps, then bear in mind it may take some time to get the intervention fully operational at a site, and the team will also need time to relocate from one cluster to the next.

---

### Why look at an SPC? [^112Qxdir]. Drug and Therapeutics Bulletin (2009). Low credibility.

Of the various sources of information about medicines used by healthcare professionals, the summary of product characteristics (SPC; sometimes abbreviated as SmPC) has a low profile, despite being a definitive document. This may be because many professionals do not know what an SPC is or what it contains. Here, we describe how SPCs are compiled, their significance in clinical practice, and where to find them.

---

### Editorial [^112Jyv6k]. Journal of Psychiatric Research (2024). Medium credibility.

N/A.

---

### Massive yet grossly underestimated global costs of invasive insects [^114DmDog]. Nature Communications (2016). Medium credibility.

Removing potential double counts

We made every effort to eliminate redundant amounts from the monetary values we used to estimate cost sums. First, we removed values that were obvious re-estimates of older values (with the more recent estimates tending to be more reproducible than older ones; for example, Supplementary Data 1, column E). We further separated costs into 'extrapolation' versus 'actual estimate' categories (columns G and H in Supplementary Data 1, respectively). Further removing those estimates already deemed irreproducible (column F), column I indicates with absolute certainty which estimates should be retained to avoid any potential case of double counting (that is, species with reproducible estimates that do not include both extrapolated and actual estimates).

The sum of estimates in column I ($22,629,029,314) versus our sum of the total costs (US$25,166,603,981) reported in the main text is only 10.1%, which suggest that even in the unlikely case of double counting, the bias is minimal, and well within the margin of error expected for a sum of median cost rates across the globe. It is essential to note that even if a species includes both extrapolations and actual values, it does not necessarily equate to double counting because often the different estimates apply to different regions of the insect's distribution or different economic components of their costs. However, this does not exclude the possibility of double counting within the irreproducible category, simply because we cannot verify how the estimates were derived to check for instances of potential double counting.

---

### 2023 ESC guidelines for the management of endocarditis [^115Jn81W]. European Heart Journal (2023). High credibility.

Regarding follow-up and surveillance for infective endocarditis, more specifically with respect to imaging follow-up, ESC 2023 guidelines recommend to consider obtaining TTE and/or TEE to detect new silent complications during follow-up of uncomplicated IE. Decide on the timing of repeat TTE and/or TEE based on initial findings, type of microorganism, and initial response to therapy.

---

### Investigating suspected cancer clusters and responding to community concerns: guidelines from CDC and the council of state and territorial epidemiologists [^114nvBwd]. MMWR: Recommendations and Reports (2013). Medium credibility.

Cancer cluster investigation — Step 2 decision to close or proceed to Step 3 is based on multiple factors, and to interpret the standardized incidence ratio (SIR) the health agency must assess whether there are enough cases and population for statistical stability and, in general, that the population size of a typical census tract is the smallest denominator that will allow reliable results; if the numerator allows stability, agencies should consider whether the 95% CI exclude 1.0, along with environmental contaminants, population changes, and other information beyond the SIR to estimate whether cancers represent an excess and share a common etiology. A SIR of limited magnitude that is not statistically significant with lack of known contaminant association and no increasing trend justifies closing at Step 2, whereas a statistically significant SIR of great magnitude with an increasing trend and a known contaminant argues for continuing to Step 3; example thresholds indicate that an SIR of < 2.0 with CIs surrounding or overlapping 1.0 and < 10 cases might justify closing, while an SIR of > 4 with CIs not overlapping 1.0 and ≥ 10 etiologically linked cases should encourage advancing to Step 3.

---

### Methodology for ACOEM's occupational medicine practice guidelines-2025 revision [^116RctGV]. ACOEM (2025). High credibility.

ACOEM evidence-based recommendations — classification and evidence synthesis methods are specified as follows: Strongly Recommended (A) requires at least 2 high-quality studies, Moderately Recommended (B) requires at least 1 high-quality and/or multiple moderate quality, and Recommended (C) requires at least one moderate quality study; additional categories include Recommended, Insufficient Evidence (I); No Recommendation, Insufficient Evidence (I); Not Recommended, Insufficient Evidence (I); Not Recommended (C); Moderately Not Recommended (B); and Strongly Not Recommended (A). Insufficient Evidence is used to clearly denote expert consensus recommendations and, when evidence is either absent or substantially conflicting, to demarcate consensus-based recommendations. Final recommendation summaries include the numbers and strength of studies in a clearly defined "Rationale for Recommendation" section. When combined analyses are sensible, tests for homogeneity (I²) and/or random effects models are used and reported; many pooled analyses are drawn from Cochrane reviews provided they reasonably support summary estimates. However, pooled summary measures may be inappropriate for topics with population heterogeneity, different outcome measures, or substantially varying time intervals. After the systematic review, discrepancies with prior quality systematic reviews are addressed first in the Guideline development group and then in panel deliberations.

---

### Standards of care in diabetes – 2025 [^113RQNuE]. Diabetes Care (2025). High credibility.

Regarding follow-up and surveillance for diabetes mellitus type 1, more specifically with respect to continuous glucose monitoring, ADA 2025 guidelines recommend to implement early continuous glucose monitoring in adult patients with T1DM to improve glycemic outcomes and QoL and minimize hypoglycemia.

---

### 2023 ESC guidelines for the management of endocarditis [^1146qUtx]. European Heart Journal (2023). High credibility.

Regarding follow-up and surveillance for infective endocarditis, more specifically with respect to imaging follow-up, ESC 2023 guidelines recommend to obtain TTE and/or TEE at completion of antibiotic therapy for evaluation of cardiac and valve morphology and function in patients with IE not undergone heart valve surgery.

---

### 2023 ESC guidelines for the management of endocarditis [^116UivVi]. European Heart Journal (2023). High credibility.

Regarding diagnostic investigations for infective endocarditis, more specifically with respect to TEE, ESC 2023 guidelines recommend to obtain TEE in all patients with clinical suspicion of IE and a negative or nondiagnostic TTE.

---

### 2023 ESC guidelines for the management of endocarditis [^113y121A]. European Heart Journal (2023). High credibility.

Regarding follow-up and surveillance for infective endocarditis, more specifically with respect to imaging follow-up, ESC 2023 guidelines recommend to obtain TEE in stable patients before switching from IV to oral antibiotic therapy.

---

### Drawing statistical conclusions from experiments with multiple quantitative measurements per subject [^113tLqcE]. Radiotherapy and Oncology (2020). Medium credibility.

In experiments with multiple quantitative measurements per subject, for example measurements on multiple lesions per patient, the additional measurements on the same patient provide limited additional information. Treating these measurements as independent observations will produce biased estimators for standard deviations and confidence intervals, and increases the risk of false positives in statistical tests. The problem can be remedied in a simple way by first taking the average of all observations of each specific patient, and then doing all further calculations only on the list of these patient means. A more sophisticated statistical modeling of the experiment, for example in a linear mixed model, is only required if (i) there is a large imbalance in the number of observations per patient or (ii) there is a specific interest in actually identifying the various sources of variation in the experiment.

---

### A multi-step model of Parkinson's disease pathogenesis [^116QBXeE]. Movement Disorders (2021). Medium credibility.

Parkinson's disease (PD) is a common neurodegenerative disorder across the world. Its burgeoning prevalence and associated societal impact makes developing effective preventative therapies a global health issue, but incomplete understanding of PD pathogenesis limits this goal. Over time, many mechanisms for disease development have been posited, including genetic, inflammatory, infectious, and other environmental exposures. Most current models of pathogenesis acknowledge the likelihood that, in most people, the development of the pathological changes that define PD is the final product of a complex interaction between many of these potential etiological factors. Although these observations suggest that PD might develop as a multistep process, empirical evidence for such an overarching framework is lacking, as are specific details such as the number of pathogenic steps required to cause the disease.

The application of techniques developed to understand another common disorder — cancer — to the field of neurodegeneration shows promise for elucidating disease mechanisms. In particular, a strong body of research demonstrates how epidemiological data can be used to infer the underlying pattern of cancer development, and whether a multistep process is likely to be present. Specifically, Armitage and Doll demonstrated that if the onset of a disease requires multiple prior steps, each with a relatively low probability of occurring per year, then a power relationship will exist between the incidence of the disease and age (Box 1 and see Webster 2019 for a full derivation)Furthermore, the exponent in this power relationship (ie, the slope of the log–log line) is one less than the total number of steps required to cause the disease. This insight has significantly enhanced the understanding of cancer biology. Furthermore, the potential utility of this approach for understanding the pathogenesis of neurodegenerative conditions has been demonstrated in amyotrophic lateral sclerosis (ALS), where the incidence rate was found to increase with age in a manner predicted by a multistage process requiring six events for disease development. Additionally, the contribution of single monogenic "causes" of ALS reduces the required number of steps to between two and five depending on the mutation.

---

### 'Relation, association, attribution… '-the multiple faces of death in critical care medicine [^114TNwCD]. Critical Care (2009). Low credibility.

Mortality is one of the most important quality markers in critical care, and there have been many epidemiological studies trying to identify risk factors to better understand the mechanisms leading to death in this complex disease. One of the major problems is that there are multiple factors contributing to fatal outcome of septic patients, and it is difficult to distinguish between those that are independent from the acute disease (comorbidities and 'risk factors') and those that are directly involved in the pathomechanisms of sepsis, thus leading to the 'sepsis-attributable' mortality. In this short commentary, some examples of different approaches of how to analyze data on mortality are presented.

---

### The diverse club [^1156Bjs6]. Nature Communications (2017). Medium credibility.

Generative evolutionary model

The model starts with a graph of 100 nodes that are randomly connected, with 5% of all possible edges (n = 247). Binary edges were used. No initial community structure is imposed. For each iteration, Q and E are calculated. Edges are then chosen for removal that, when removed, lead to a maximal increase in Q and E. To achieve this, the change in Q and E following the removal of each edge is calculated. These two vectors are then rank ordered separately. The minimum of the ranks that would have been assigned to all the tied values is assigned to each value (i.e. "competition" ranking). Next, the two vectors are z-scored and then weighted according to the Q -ratio parameter. For example, if the Q -ratio is 0.75, the rank ordered vector of Q changes is multiplied by 0.75, and the rank ordered vector of shortest path changes is multiplied by 0.25. The sum of the two vectors is then calculated, giving a weighted sum of the two objectives, jointly maximizing Q and E for each edge.

These edges are removed and then randomly placed back in the graph, maintaining a constant density of edges (0.05). At each iteration, 5% of edges (13) are removed from the graph and then placed randomly back into the network. This process jointly maximizes Q and E. This procedure is repeated for 150 iterations, resulting in 1950 edges being shuffled. At this point, the generative model stops. To ensure the model is stable after 150 iterations, we plotted the mean and 95% confidence intervals of Q and E at each iteration. At around 80 iterations, both Q and E remain stable (Supplementary Fig. 67). Next, using the mean value across model runs (n = 1000), for each iteration, we calculated the absolute percentage difference between Q or E at that iteration with Q or E in each of the previous 40 iterations. We then take the mean absolute percentage change over those 40 iterations. We then plotted these values for each iteration (Supplementary Fig. 67). Finally, using those change values, we calculated the mean absolute change over the last 30 iterations, asserting that it was less than 1% for both Q (0.3%) and E (0.9%). For a null model, we also ran the generative process, except we randomly selected the edges, removed them, and then randomly placed them in the network.

---

### Age-related macular degeneration preferred practice pattern ® [^114Zjx71]. Ophthalmology (2025). High credibility.

Regarding screening and diagnosis for age-related macular degeneration, more specifically with respect to natural history (intermediate age-related macular degeneration), AAO 2025 guidelines recommend to recognizr that following approximate risks for progression of age-related macular degeneration

- table: | Number of Risk Factors | 5-Year Risk of Progression to Advanced AMD: Patients Without Reticular Pseudodrusen | 5-Year Risk of Progression to Advanced AMD: Patients With Reticular Pseudodrusen | ∅ |- - - - | - - - - - - - - - - - - - - | - - - - - - - - - - - - - -| ∅ |0 | 0.3% | 3% | ∅ |1 | 4% | 8% | ∅ |2 | 12% | 29% | ∅ |3 | 27% | 59% | ∅ |4 | 50% | 72% |

- based on AREDS 2024 updated simplified severity scale; Risk factors included one or more large drusen (≥ 125 µm in diameter) and the presence of pigmentary changes.

---

### Computational mechanisms of distributed value representations and mixed learning strategies [^11319LaH]. Nature Communications (2021). High credibility.

Learning appropriate representations of the reward environment is challenging in the real world where there are many options, each with multiple attributes or features. Despite existence of alternative solutions for this challenge, neural mechanisms underlying emergence and adoption of value representations and learning strategies remain unknown. To address this, we measure learning and choice during a multi-dimensional probabilistic learning task in humans and trained recurrent neural networks (RNNs) to capture our experimental observations. We find that human participants estimate stimulus-outcome associations by learning and combining estimates of reward probabilities associated with the informative feature followed by those of informative conjunctions. Through analyzing representations, connectivity, and lesioning of the RNNs, we demonstrate this mixed learning strategy relies on a distributed neural code and opponency between excitatory and inhibitory neurons through value-dependent disinhibition. Together, our results suggest computational and neural mechanisms underlying emergence of complex learning strategies in naturalistic settings.

---

### Clustering knowledge and dispersing abilities enhances collective problem solving in a network [^1143zBgC]. Nature Communications (2019). High credibility.

Here, however, we find quite different results: intermixing hampers systemic performance. Furthermore, there is a tradeoff in systemic performance over time, as seen in Fig. 6b. Networks with some form of intermixing perform better in the short-run. However, the network with the minimal possible intermixing performs worse in the short-run but best in the long-run. In other words, this suggests an "all or nothing" trade-off over time: the configuration with the least amount of intermixing possible (minimal) performs worse in the short-run, but better in the long-run, while any gradation of increased intermixing yields the same rank-ordering in performance as their counterpart networks in the diversity of ability simulations.

Agents in the intermixed network setups explore for solutions early on in the simulation, which is a double-edged sword. In the short-run, with more intermixing, agents quickly turn to exploitation, as they merely take solutions that are marginally better relative to other setups but are still mediocre in absolute terms. Said differently, agents in setups with at least some intermixing quickly coalesce to whatever the few agents that did explore the problem space found, which are often not the best solutions possible. So, exploring the problem space using worse solutions often leads to modest gains. However, the agents in setups with minimal intermixing are more commonly exploring, rather than exploiting, because their neighbors' initial solutions are less optimal than their counterparts in intermixed networks. This is because there is minimal exposure to diversity in these minimal setups, which inadvertently produces poor solutions in the short-run but allows for more exploration to find better solutions in the long-run. In other words, by finding better solutions through exploration, agents more often uncover pathways to better solutions earlier on. The end result is that the long-run performance of setups with minimal intermixing is best.

---

### The suboptimality of perceptual decision making with multiple alternatives [^114mYXcL]. Nature Communications (2020). High credibility.

Each trial began with a 500-ms fixation, followed by a 500-ms stimulus presentation. The stimuli were then masked for 100 ms with a 7 × 7 grid of ellipsoid-shaped images consisting of uniformly distributed noise pixels. Each ellipsoid had a width of 0.54° and height of 0.95°, ensuring that it entirely covered each symbol. After the offset of the mask, subjects indicated the dominant symbol in the display without time pressure. No confidence ratings were obtained. The experiment had two conditions equivalent to the first two conditions in Experiment 1. In the first condition, subjects had to choose the dominant symbol among all six alternatives (six-alternative condition). In the second condition, subjects had to choose between two alternatives that were not announced in advance: the correct dominant symbol and a randomly selected nondominant symbol (two-alternative condition).

To obtain clear individual-level results, we collected data from each subject over the course of three different days. On each day, subjects completed 5 runs, each consisting of 4 blocks of 50 trials (for a total of 3000 trials per subject). We note that this very large number of trials makes it unlikely that any of our results in this or the subsequent experiments (which featured the same total number of trials) are due to insufficient training. The six- and two-alternative condition blocks were presented alternately, so that there were two blocks of each condition in a run. Subjects were given 15-s breaks between blocks and untimed breaks between runs. Before the start of the main experiment, subjects were given a short training on each day of the experiment.

Experiment 3

Experiment 3 used the same stimuli as in Experiment 2. Similar to Experiment 2, we presented a 500-ms fixation, a 500-ms stimulus, a 100-ms mask, and finally a response screen. Experiment 3 consisted of a single condition — subjects always chose the dominant symbol among all six alternatives. However, on 40% of trials in which subjects gave a wrong answer, they were asked to provide a second answer by choosing among the remaining five symbols. Subjects could take as much time as they wanted for both responses. Subjects again completed 3000 trials over the course of three different days in a manner equivalent to Experiment 2.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^1138YogM]. CDC (2011). Medium credibility.

Community viral load categorical measurement — additional measures and distribution guidance include examining the proportion of HIV-infected persons that meet a standardized definition of in-care, e.g., at least two lab results obtained within a calendar year that are at least 60 days apart. Categorical measures might be more informative than means, the actual distribution of VL results in surveillance populations is highly skewed with a large proportion of the population having undetectable or very low values (e.g., < 200 copies/mL), and jurisdictions should examine their data to determine the local viral load distribution and the best measures to describe it, including outlier cutoff values; such distributions can also be stratified by CD4+ T-lymphocyte count to reflect stage of illness and the U.S. HIV Treatment Guidelines recommendations on initiation of antiretroviral therapy.

---

### Quantifying microbial interactions: concepts, caveats, and applications [^1175h5W3]. Current Opinion in Microbiology (2024). Medium credibility.

Microbial communities are fundamental to every ecosystem on Earth and hold great potential for biotechnological applications. However, their complex nature hampers our ability to study and understand them. A common strategy to tackle this complexity is to abstract the community into a network of interactions between its members - a phenomenological description that captures the overall effects of various chemical and physical mechanisms that underpin these relationships. This approach has proven useful for numerous applications in microbial ecology, including predicting community dynamics and stability and understanding community assembly and evolution. However, care is required in quantifying and interpreting interactions. Here, we clarify the concept of an interaction and discuss when interaction measurements are useful despite their context-dependent nature. Furthermore, we categorize different approaches for quantifying interactions, highlighting the research objectives each approach is best suited for.

---

### Multiple patient samples of an analyte improve detection of changes in clinical status [^111D1c3R]. Archives of Pathology & Laboratory Medicine (2010). Low credibility.

Context

When comparing results over time, biologic variation must be statistically incorporated into the evaluation of laboratory results to identify a physiologic change. Traditional methods compare the difference in 2 values with the standard deviation (SD) of the biologic variation to indicate whether a "true" physiologic change has occurred.

Objective

To develop methodology to reduce the effect of biologic variation on the difference necessary to detect changes in clinical status in the presence of biologic variation.

Design

The standard test for change compares the difference between 2 points with the 95% confidence limit, given as ± 1.96 x square root of 2 x SD. We examined the effect of multiple data pairs on the confidence limit.

Results

Increasing the number of data pairs using the formula 1.96 x square root of 2 x SD, where n = number of data pairs, significantly reduces the difference between values necessary to achieve a 95% confidence limit.

Conclusions

Evaluating multiple paired sets of patient data rather than a single pair results in a substantial decrease in the difference between values necessary to achieve a given confidence interval, thereby improving the sensitivity of the evaluation. A practice of using multiple patient samples results in enhanced power to detect true changes in patient physiology. Such a testing protocol is warranted when small changes in the analyte precede serious clinical events or when the SD of the biologic variation is large.

---

### Quantifying randomness in real networks [^115SnyyU]. Nature Communications (2015). Medium credibility.

The general philosophy outlined above is applicable to undirected and directed networks, and it is shared by different approaches, including motifs, graphletsand similar constructions, albeit they violate the inclusiveness condition as we show below. Yet one can still define many different Y -series satisfying both conditions above. Some further criteria are needed to focus on a particular one. One approach is to use degree-based tailored random graphs as null models for both undirectedand directednetworks. The criteria that we use to select a particular Y -series in this study are simplicity and the importance of subgraph- and degree-based statistics in networks. Indeed, in the network representation of a system, subgraphs, their frequency and convergence are the most natural and basic building blocks of the system, among other things forming the basis of the rigorous theory of graph family limits known as graphons, while the degree is the most natural and basic property of individual nodes in the network. Combining the subgraph- and degree-based characteristics leads to dk-series.

---

### American Geriatrics Society identifies five things that healthcare providers and patients should question [^1121tSZa]. Journal of the American Geriatrics Society (2013). Medium credibility.

U.S. older-adult demographics and multimorbidity — With the eldest of the nation's 77 million baby boomers already 65 and the youngest reaching that milestone in 2019, older adults will make up a growing share for the next 4 decades; now roughly 40 million, U.S. residents age 65 and older will reach an estimated 78.9 million in 2050, and currently 80% of adults age 65 and older have at least one chronic health condition. The evidence base supporting the use of many common tests and treatments for older adults is described as inadequate, in part because older adults are significantly underrepresented in clinical trials.

---

### Multiple anthropogenic pressures eliminate the effects of soil microbial diversity on ecosystem functions in experimental microcosms [^114wTZDC]. Nature Communications (2022). High credibility.

Specifically, there were more saprophytic fungal species in module 4. The decrease in the relative abundance of these fungi could cause the reduction in soil functions related to decomposition, because fungi are a significant driver of enzyme activity. Moreover, the relative abundance of some rare soil bacteria, e.g. Chloroflexia, Gemmatimonadetes, Longimicrobia, Myxococcia, Planctomycetes, was correlated with the reduction of soil functions along with an increasing number of GCFs, probably because GCFs are likely more detrimental for rare microbial species than common species. Our results are consistent with previous studies showing that specific microbial taxa are important for soil functions. The presence of productive species is a major determinant of the effect of plant diversity on productivity, known as the selection effect. The changes in selection effects can affect the overall biodiversity effects. Analogously, functionally important species – in our case module 4, the structuring variations of which were a main component of the whole microbial community, could be critical for a biodiversity effect. Therefore, the increasing number of GCFs could reduce the impact of soil biodiversity on soil functions by reducing the relative abundance of module 4.

In summary, our study supports the claim that soil biodiversity is essential for maintaining soil functions when faced with none or only a few GCFs. Importantly, we find that the co-occurrence of multiple GCFs can eliminate the positive effects of soil microbial diversity, probably via detrimental effects on fungal abundance and the relative abundance of a microbial ecological cluster. In addition, the reduction in soil functions with the co-occurrence of multiple GCFs could come from a decrease in soil biodiversity effects, microbial abundance and the relative abundance of a coexisting microbial cluster (Fig. 4). Our study highlights the importance of working towards reducing the number of GCFs to achieve agroecosystem sustainability. Furthermore, because multiple GCFs are increasingly threatening ecosystems worldwide, there is an urgent need to evaluate the consequences for biodiversity-ecosystem function relationships in different ecosystems. A better understanding of the role of multiple GCFs is crucial for ecosystem management given the challenges imposed by global change.

---

### 2023 ESC guidelines for the management of endocarditis [^113NNKaY]. European Heart Journal (2023). High credibility.

Regarding diagnostic investigations for infective endocarditis, more specifically with respect to TEE, ESC 2023 guidelines recommend to obtain TEE in patients with suspected IE even in cases with positive TTE, except in isolated right-sided native valve IE with good quality TTE examination and unequivocal echocardiographic findings.

---

### 2023 ESC guidelines for the management of endocarditis [^117BAwBA]. European Heart Journal (2023). High credibility.

Regarding follow-up and surveillance for infective endocarditis, more specifically with respect to imaging follow-up, ESC 2023 guidelines recommend to obtain repeat TTE and/or TEE as soon as a new complication of IE is suspected (new murmur, embolism, persisting fever and bacteremia, HF, abscess, AV block).

---

### Improved measures for evolutionary conservation that exploit taxonomy distances [^114rkPVM]. Nature Communications (2019). High credibility.

Optimizing LIST hierarchical structure

We made the following two assumptions:

First, we assumed that, once compensated for alignment depth, each module's scores can be loosely considered probabilities after being rescaled to the range [0+ C, 1− C] and then weighted:where min and max are the minimum and maximum scores for each module observed in the optimization set 2. C = 0.2 was used to prevent extreme values from dominating the final outcome. A weight (ω ∈ [0.1,1]) is used to account for the relative prediction accuracy of each module, and the weighted scores were calculated as:Weights ω were learned using a grid search on optimization set 2 to maximize LIST's AUC, such that modules with higher accuracy are assigned higher ω values. Low ω values produce weighted scores that reflect high uncertainty, i.e. probabilities near 50%, whereas weighted scores resulting from higher ω values have more impact on the final score that is generated by combining weighted scores using Bayes rule.

Second, the output scores that are generated when combining weighted scores using Bayes rule are likely to be skewed away from the center because the different input scores are not completely independent. Thus, in order to use it as an input probability to the next hierarchical level, these scores are redistributed to fit a normal distribution centered at Bayes rule identity element 0.5, N (μ = 0.5, σ² = 0.01) and bounded by the range [0+ C, 1− C] (Supplementary Note 5, Supplementary Figs. 9 and 10).

We redistribute LIST's output scores to fit a uniform distribution (i.e. rank score), which, we believe, makes the interpretation of these scores simpler. We learned the redistribution function from optimization set 2. The final ROC curves representing the performances of each of LIST three sub-modules are shown in Supplementary Fig. 11.

---

### American Geriatrics Society identifies five things that healthcare providers and patients should question [^115ocwXz]. Journal of the American Geriatrics Society (2013). Medium credibility.

American Geriatrics Society Choosing Wisely — context for older adult care emphasizes that Choosing Wisely is designed to engage patients, healthcare professionals, and family caregivers in discussions about the safety and appropriateness of medical tests, medications, and procedures, and that these discussions should examine whether the tests and procedures are evidence-based, whether any risks they pose might overshadow their potential benefits, whether they are redundant, and whether they are truly necessary. Demographically, adults turning 65 can expect to live, on average, another 19 years, yet data indicate that more than half have three or more chronic diseases and, further complicating care for the more than 50% of older adults with multimorbidity, current clinical practice guidelines tend to focus on the treatment of individual disorders and may not be applicable to individuals with multiple disorders. Clinically, older adults may respond differently to medications and other interventions than younger individuals, and because older people — particularly those with multiple conditions — are underrepresented in clinical trials, judging the appropriateness of diagnostic and treatment approaches for aging adults can be difficult. As a stewardship rationale, as much as 30% of healthcare spending in the United States may be unnecessary, and with the eldest of the United States' 77 million "baby boomers" already 65, addressing inappropriate treatment for older adults is imperative.

---

### 2023 ESC guidelines for the management of endocarditis [^115vkBsp]. European Heart Journal (2023). High credibility.

Regarding specific circumstances for infective endocarditis, more specifically with respect to patients with device-related IE, device removal, ESC 2023 guidelines recommend to consider performing complete cardiac implanted electronic device extraction in patients with valvular IE, even without definite lead involvement, taking into account the identified pathogen and requirement for valve surgery.

---

### 2023 ESC guidelines for the management of endocarditis [^112APjzX]. European Heart Journal (2023). High credibility.

Regarding screening and diagnosis for infective endocarditis, more specifically with respect to indication for testing, ESC 2023 guidelines recommend to obtain TTE/TEE to rule out IE in patients with spondylodiscitis and/or septic arthritis with positive blood cultures for typical IE microorganisms.

---

### 2023 ESC guidelines for the management of endocarditis [^113bggPD]. European Heart Journal (2023). High credibility.

Regarding specific circumstances for infective endocarditis, more specifically with respect to patients with device-related IE, device removal, ESC 2023 guidelines recommend to perform complete system extraction without delay in patients with definite cardiac implanted electronic device-related IE under initial empirical antibiotic therapy.

---

### Dimensionless learning based on information [^113YLNkv]. Nature Communications (2025). High credibility.

We have shown that IT- π offers a complete set of dimensionless learning tools, including ranking inputs by predictability, identifying distinct physical regimes, uncovering self-similar variables, and extracting characteristic scales and dimensionless parameters. IT- π is also sensitive to the norm used to quantify errors and the optimal set of dimensionless variables may vary depending on the error metric of interest (e.g. prediction of ordinary versus rare events). Although some of these features are available through other methods, none encompass them all. Even in cases where alternative methods apply, IT- π distinguishes itself by being grounded in a theorem rather than relying on heuristic reasoning. This makes IT- π independent of specific modeling assumptions.

In additional to its model-free nature, IT- π offers unique capabilities that other methods do not, such as establishing bounds on the irreducible error and evaluating model efficiency. The former allows us to precisely determine the actual number of relevant dimensionless variables, l *, which is typically overestimated by the Buckingham- π theorem. Moreover, IT- π quantifies the degree of dynamic similarity achievable with the optimal variables, rather than providing merely a binary yes-or-no answer as classical dimensional analysis does. This feature can be decisive in designing laboratory experiments for extrapolation to real-world applications. For example, consider predicting the heat flux over a rough surface as discussed in the application above. According to Buckingham- π, seven dimensionless variables would be required. If three different values must be measured to capture the scaling behavior of each variable, that would entail ~3⁷ = 2187 experiments. In contrast, IT- π determined that only two dimensionless variables are necessary to achieve a dynamic similarity of 92% (i.e. an 8% normalized irreducible error). This entails a significantly reduced effort of only 3² = 9 experiments. The same reasoning applies to the construction of predictive modeling: models with fewer inputs require orders of magnitude less training data compared to those with high-dimensional inputs. In the previous example, this factor would be of the order of 1000.

---

### Division of filing review: best practices for ANDAs and… [^111V43fF]. FDA (2025). Medium credibility.

2 Learning Objectives
- Refuse-to-Receive Statistics
- Common Major Deficiencies
- Common Minor Deficiencies
- ANDA Submission Considerations
- Communication with DFR PM
- Controlled Correspondence
- References/Contact www. fda. gov. 8 Q1/Q2 Same to the RLD
- Ophthalmic or Otic drug products must contain the same inactive ingredients and in the same concentration as the RLD pursuant 21 CFR 314. 94
- Exception excipients that can differ from the RLD – Preservative – Buffer – Substance to adjust tonicity – Thickening agent www. fda. gov. 9 Q1/Q2 Same to the RLD
- **In ophthalmic drug products, applicant may not change the following**: – A buffer or substance to adjust tonicity for the purpose of claiming a therapeutic advantage over or difference from the RLD
- Deviation in Q1/Q2 with respect to exception excipients should be accompanied with an appropriate in vivo BE study
- Refer to 21 CFR 314.

94 for more details www. fda. gov. 10 Dissolution Studies
- Product-Specific Guidance recommended dissolution studies – Refer to Dissolution Methods database for completeness
- Half-tablet dissolution studies for functionally scored or modified-release products, where applicable
- Comparative dissolution studies should include a minimum of 12 dosage-units for each strength of test product and RLD www. fda. gov. 19 Module 3
- Module 3.
2. P. 1: – Provide proposed formulation in all units including %w/w, %v/v, and %w/v, as applicable – Provide mg/dose for oral suspensions and oral solutions – Provide grade, purity, hydration state for excipients, as applicable www. fda. gov. 27 Requests for Q1/Q2 Formulation Assessment
- Q1/Q2 the same as the RLD – 314.

94
- Submit no more than three formulations in a single controlled correspondence
- Submit separate Q1/Q2 formulation assessment for drug product with multiple.

---

### An official multi-society statement: the role of clinical research results in the practice of critical care medicine [^117KucQe]. American Journal of Respiratory and Critical Care Medicine (2012). Medium credibility.

Negotiating between various kinds of medical knowledge — no set hierarchy of knowledge to guide clinical decision making in all situations is possible, and a ranking based on study design or quality measures cannot be directly applied to clinical decisions; clinicians must continue to rely on sound clinical judgment to negotiate between potentially conflicting facts and reasons, and explicitness is to be greatly valued, as clinicians should be able to identify and articulate the sources and kinds of knowledge invoked; the initial step is identifying pertinent medical knowledge, including clinical research, pathophysiologic understanding, and clinical experience, and since no kind of knowledge is always superior, simply relying upon a randomized controlled trial or meta-analysis will be insufficient; clinicians are obligated to consider information and knowledge that might suggest action that would run counter to that suggested by clinical research, and a clinician ought to be able to concisely outline and justify the process of clinical reasoning for scrutiny and revision.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^111AVYvw]. CDC (2011). Medium credibility.

Guidance on community viral load — standardizing handling of undetectable viral load (VL) results and limits of detection states that "use of half LLD should be adequate", and specifies, "when a viral load assay with LLD < 50 copies/mL is used, undetectable viral load results should be replaced with 25 copies/mL for analysis". For calendar-time trend analyses with changing assays, it may be necessary "to truncate the lower limit of detection and possibly the upper limit of detection", with an example from "2005–2009" where assays shifted from "a LLD < 400 copies/mL" to "LLD of 75 copies/mL by 2009"; in that scenario, "undetectable, 400 copies/mL or less" are collapsed and assigned "200 (half LLD)" "for each year, 2005 through 2009", and analyses use "this common LLD of 400". For a single-year analysis in 2009, it "would be appropriate to use 75 copies/mL as the LLD and use 37.5 as the value for analysis when results of ≤ 75 copies/mL or undetectable were reported". The document advises that "the LLD or range of VL results used in analysis should be stated" and notes that "Sites should assess their data and determine if an upper limit of detection is needed" and that "use of an upper limit of detection may be prudent".

---

### Roflumilast (Zoryve) [^111TASQF]. FDA (2024). Medium credibility.

The dosage of roflumilast TOP for treatment of plaque psoriasis in both children (in patients 6–12 years) is 1 application(s) TOP daily (0.3% cream)

---

### Γ δ T cells mediate immunotherapy response in HLA class I-deficient cancers [^113PidJ8]. Cancer Discovery (2023). Medium credibility.

γδ T cells contribute to the efficacy of immunotherapy in the absence of antigen presentation.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^113hCx66]. CDC (2011). Medium credibility.

Community viral load — selection of viral load (VL) results for analysis states 'Use the most recent VL result per person for analysis — this would be for specimens obtained in 2009 that are the closest to December 31', and adds that if there is no VL specimen for a case in 2009 the case would be excluded and classified as 'In care, no VL' or 'Diagnosed but not in care'. It also notes jurisdictions may average VL results for each individual and then take the mean of those averages to compute the Community VL, In-Care VL, or Monitored VL, and that for other analyses it may be appropriate to use the 'highest, lowest, or most recent' VL result. The page explains that the examination of a single most recent VL is the least restrictive definition of being in care, whereas other measures require at least two lab results within a specified timeframe of each other.

---

### 2023 ESC guidelines for the management of endocarditis [^111bueur]. European Heart Journal (2023). High credibility.

Regarding diagnostic investigations for infective endocarditis, more specifically with respect to TEE, ESC 2023 guidelines recommend to obtain TEE in patients with clinically suspected IE and a prosthetic heart valve or an intracardiac device.

---

### Standards of care in diabetes – 2025 [^1177SxqY]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for diabetic nephropathy, more specifically with respect to pediatric patients, ADA 2025 guidelines recommend to determine the eGFR at the time of diagnosis and annually thereafter.