# Medical Question & Answer

**Sample ID**: ad0c7c9c-4d1c-6ce6-cbc7-12001e24588d
**Dataset Index**: 175012

---

## Question

14. The roots of the equation x² − 2 x − a² − a = 0 are (αₐ,
βₐ)(a = 1, 2, ..., 2011).
Find ∑_{a = 1}^{2011}([1/αₐ] + [1/βₐ]).

---

## Answer

> Let's see… What do we have here? The user is asking how to find the sum of 1/α_a + 1/β_a for a sequence of quadratic equations indexed by a from 1 to 2011, where each equation is x² − 2x − a² − a = 0 with roots α_a and β_a. Let's break this down step-by-step. First, I need to think about expressing 1/α_a + 1/β_a in terms of the elementary symmetric sums of the roots. Then, I should verify the Vieta's formulas for the given quadratic. Next, I will substitute and simplify the expression for each a. After that, I need to set up the summation and check whether it telescopes or requires a different approach. Finally, I will compute the total and double-check the arithmetic.

> Let me first confirm the key identity: for any quadratic with roots α and β, the sum of reciprocals is (α + β)/(αβ), which follows directly from combining the fractions and recognizing the numerator and denominator as the sum and product of roots, respectively [^notfound].

> Now, I should apply Vieta's formulas to the specific quadratic x² − 2x − a² − a = 0. The sum of roots α_a + β_a equals 2, and the product α_aβ_a equals −(a² + a), so I can substitute these into the identity to get 1/α_a + 1/β_a = 2 / (−(a² + a)) = −2/(a² + a) for each a [^notfound].

> Wait, let me verify the simplification of the denominator: a² + a factors as a(a + 1), so the expression becomes −2/[a(a + 1)], which is a standard partial-fraction candidate. I should confirm that this form will be useful for summation, and yes, it sets up a telescoping structure if I split it appropriately [^notfound].

> I will now examine the partial-fraction decomposition: −2/[a(a + 1)] = −2[1/a − 1/(a + 1)]. Hold on, I should verify the signs and coefficients; distributing the −2 gives −2/a + 2/(a + 1), which matches the intended decomposition and is ready for telescoping when summed over a [^notfound].

> Next, I should review the telescoping sum. The total sum from a = 1 to 2011 is Σ_{a = 1}^{2011} −2[1/a − 1/(a + 1)] = −2[(1/1 − 1/2) + (1/2 − 1/3) +… + (1/2011 − 1/2012)]. Most terms cancel, leaving −2[1 − 1/2012] = −2[2011/2012] = −4022/2012. Hmm, wait a minute, I initially thought the denominator would be 2011, but that was a mistake; the surviving term is 1/2012, so the correct simplification is −2 × 2011/2012 = −4022/2012, which reduces to −2011/1006 [^notfound].

> Let me double-check the arithmetic: −2 × 2011/2012 = −4022/2012, and dividing numerator and denominator by 2 yields −2011/1006. I should confirm that 2011 and 1006 share no common factors other than 1, which they do not, so the fraction is in lowest terms [^notfound].

> Final answer: The sum is −2011/1006. I need to ensure the sign is correct; since the product α_aβ_a is negative and the sum is positive, the reciprocals sum to a negative value, which aligns with our result, so I am confident in the answer [^notfound].

---

The sum ∑_{a = 1}^{2011}([1/αₐ] + [1/βₐ]) equals −[2011/1006]. This result is obtained by using Vieta's formulas to express [1/αₐ] + [1/βₐ] = [(αₐ + βₐ)/(αₐβₐ)] = [2/−(a² + a)] = −[2/(a(a + 1))], then summing the telescoping series −2 ∑_{a = 1}^{2011}([1/a] − [1/(a + 1)]) = −2(1 − [1/2012]) = −[2011/1006].

---

## Step 1: Express the sum in terms of the roots

We need to find ∑_{a = 1}^{2011}([1/αₐ] + [1/βₐ]). Using the identity [1/αₐ] + [1/βₐ] = [(αₐ + βₐ)/(αₐβₐ)], we can rewrite the sum as:

∑_{a = 1}^{2011}([1/αₐ] + [1/βₐ]) = ∑_{a = 1}^{2011}[(αₐ + βₐ)/(αₐβₐ)]

---

## Step 2: Apply Vieta's formulas

For the quadratic equation (x² − 2x − a² − a = 0), Vieta's formulas give:

- **Sum of roots**: (αₐ + βₐ = 2)
- **Product of roots**: (αₐβₐ = −(a² + a) = −a(a + 1))

Substituting these into the expression, we get:

[(αₐ + βₐ)/(αₐβₐ)] = [2/−a(a + 1)] = −[2/(a(a + 1))]

---

## Step 3: Simplify the sum

The sum now becomes:

∑_{a = 1}^{2011} −[2/(a(a + 1))] = −2 ∑_{a = 1}^{2011} [1/(a(a + 1))]

We can use partial fraction decomposition to simplify [1/(a(a + 1))]:

[1/(a(a + 1))] = [1/a] − [1/(a + 1)]

Thus, the sum becomes:

−2 ∑_{a = 1}^{2011}([1/a] − [1/(a + 1)])

---

## Step 4: Evaluate the telescoping series

The series ∑_{a = 1}^{2011}([1/a] − [1/(a + 1)]) is a telescoping series. Writing out the first few terms and the last few terms, we see:

([1/1] − [1/2]) + ([1/2] − [1/3]) + ⋯ + ([1/2011] − [1/2012])

Most terms cancel, leaving:

1 − [1/2012] = [2011/2012]

Therefore, the sum is:

−2 × [2011/2012] = −[4022/2012] = −[2011/1006]

---

## Final answer

∑_{a = 1}^{2011}([1/αₐ] + [1/βₐ]) = −[2011/1006]

---

## References

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^bb9b7f12]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Δ Φ, the probability distribution at a point Φ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Φ ∈ {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters α and β, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since α and β are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Φ ∈ [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(α, β) and x 2 = 1 − x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Φ, it may seem natural to select α and β by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^37daae40]. Nature Communications (2025). High credibility.

Fig. 2
The matrix-tree theorem.

A Computing the steady-state occupancy π 1 by summing weights over directed spanning trees. Directed spanning trees are subgraphs containing all graph nodes but no cycles, with edges oriented toward a root node. In each directed spanning tree, the input forces make a positive, negative, or zero contribution to the tree weight. The structural vectorsare shown below each tree; these quantities enter into Equation (3) below. B Schematic illustration of the high-dimensional space of feature vectors ψ (i; θ) and χ (i, F). The depicted arrangement of vectors could solve a binary classification problem.

We define the input multiplicityas the number of edges affected per input variable, which we assume to be the same for each input. To focus on the functional way in which the input driving enters the steady-state probabilities, the driving contributions can be factored out in the algebraic expressions for the numerator and denominator of Equation (1). This has been previously been used to make analytical progress for M = D = 1 in, for example, refs. –. This equivalent formulation of Eq. (1) suggests that steady states of Markov jump processes implement a rational polynomial function of exponentiated input variables. Defining, we rewrite the matrix-tree expression for π i for general D and M We use the multi-index, whereis the set of D input labels and each componentof the multi-index runs over the values, to enumerate themonomials. These monomials y μ (F) in Equation (2) combinatorially depend on the different mixtures μ of input driving, representing a net total μ a of signed contributions from the input force F a, μ b such contributions for F b, and so on for each input. The coefficients, which are functions of the parameters θ, are the sums of weights over all directed spanning trees rooted at node i which have the corresponding mixture μ of signed input contributions. The monomial coefficientsthus represent learnable amplitudes of each polynomial basis function y μ (F). The coefficients in the denominator are defined as. Classification will be successful if, for F ρ drawn from class ρ, the coefficientsand monomials y μ (F ρ) are large for the same μ. In the subsequent sections of the paper and in the Supplementary Information we use the formulation in Equation (2) to show how the classification ability of a non-equilibrium Markov processes may be systematically modulated.

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^f050c80c]. Nature Communications (2025). High credibility.

Defining the general beta and gamma distribution

One limitation of the method so far is that the beta and gamma distributions are constrained to a given fixed range of values. The gamma distribution is constrained to the positive real numbers, with a zero lower bound. Meanwhile, the beta distribution is constrained to the real numbers within the interval [0, 1]. These constraints limit the variety of variables that can be modelled by the gamma and beta distributions. For this reason, here we define general variants of both distributions such that variables with any lower or upper bound can be modelled and δ can be retrieved. Rather than modifying the pdfs and cdfs directly to allow for any lower or upper bound, we defined methods for indexing the functions and for calculating the variance and mean for the distributions from observations of a variable (V) with any lower (L) or upper (U) bound.

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^73c951cd]. Nature Communications (2025). High credibility.

Methods

In these methods, our objective was to demonstrate that δ is a mean-independent measure of statistical dispersion for bounded continuous variables, that other commonly used measures of heterogeneity (of which statistical dispersion is a subset) are dependent on and, thus, inherently biased by the mean, and that this bias has, in past work, resulted in spurious observed heterogeneity relationships with other variables.

Mean-heterogeneity relationships for the standard beta and gamma distribution

Here we introduce the gamma and beta distributions for modelling lower- and double-bounded variables, respectively. For both distributions, the most frequently used heterogeneity measures have analytical derivations based solely on the parameters of the distribution, which we can use to observe mean-heterogeneity relationships without being subject to feature relationships or chance. Additionally, both distributions can be evaluated in terms of a scale/dispersion parameter that influences the spread of observations in the distribution. A parameter is interpreted as a scale/dispersion parameter when, for a constant mean, increases in the parameter always correspond to increases in the variance. We can, therefore, fix the dispersion parameter, modify the mean, and observe the mean-heterogeneity relationships. We, then, derive a method of retrieving the dispersion parameter of the gamma and beta distribution, which we denote δ L and δ 2, respectively (with the subscript referring to the type of bounds, see Equations (2) and (3)).

---

### Stochastic representation of many-body quantum States [^5e3c8f08]. Nature Communications (2023). High credibility.

Regression

As formulated here, regression is a fundamental supervised learning task at which NNs excel. Given an ansatzand a set of samplesrepresenting our prior knowledge of the true wavefunction at some points in space, we want to find the best value of ϑ. Perhaps the simplest approach is to minimize the sum of squared residuals:To express the ansatz itself, we use a NN. Our architecture is very minimal and has not been tuned for efficiency: the NNs we used consist of a sequence of dense layers withactivation functions, and finally a linear output layer. This is in some cases be followed by an optional layer enforcing analytically known boundary and/or cusp conditions by multiplying the output of the NN with a hand-chosen parameterized function of the coordinates, such as the asymptotic solution at large distances from the origin. Technical details are provided in the Methods section.

The top panel of Fig. 1 shows what this looks like for a single particle in 1D, where the process is easy to visualize. For an arbitrary initial state (top left panel) and the ground state (top right panel) of a 1D harmonic oscillator, a series of samples and the resulting NN-based regression curves are shown. In the insets of the panel below, an analogous visualization is shown for 2D cuts across 4D wavefunctions of two interacting fermions in a 2D harmonic potential.

Fig. 1
Propagation towards the ground state in a harmonic potential.

a Different steps in the propagation towards the ground state of a particle of mass m = 1 in a 1D harmonic oscillator with frequency ω = 1, with ℏ = 1. The green line is the function fitted by the neural network to a finite set of samples (black dots on the x axis) and their corresponding values (connected by a black line). Starting with an asymmetric guess (τ = 0), the function converges towards the correct solution (dotted orange line) at the center of the trap and acquires the right symmetry (τ = 3). b Extension of the upper system to two fermions in two spatial dimensions. The energy is estimated by Monte Carlo sampling with error bars showing the standard error, and converges to the ground state value of E 0 = 3.010 ± 0.007, which results in a relative error of 0.35% with respect to the exact value of. The inset shows a cut through the wavefunction. Source data are provided as a Source Data file.

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^d027f21a]. Nature Communications (2025). High credibility.

Understanding the effect of heterogeneity is fundamental to numerous fields. In community ecology, classical theory postulates that habitat heterogeneity determines niche dimensionality and drives biodiversity. However, disparate heterogeneity-diversity relationships have been empirically observed, generating increasingly complex theoretical developments. Here we show that spurious heterogeneity-diversity relationships and subsequent theories arise as artifacts of heterogeneity measures that are mean-biased for bounded continuous variables. To solve this, we derive an alternative mean-independent measure of heterogeneity for beta and gamma distributed variables that disentangles statistical dispersion from mean. Using the mean-independent measure of heterogeneity, true monotonic positive heterogeneity-diversity relationships, consistent with classical theory, are revealed in data previously presented as evidence for both hump-shaped heterogeneity-diversity relationships and theories of an area-heterogeneity trade-off for biodiversity. This work sheds light on the source of conflicting results that have hindered understanding of heterogeneity relationships in broader ecology and numerous other fields. The mean-independent measure of heterogeneity is provided as a solution, essential for understanding true mean-independent heterogeneity relationships in wider research.

---

### Subsampling scaling [^8e2e9a81]. Nature Communications (2017). Medium credibility.

As each event is observed independently, the probability of X sub = s is the sum over probabilities of observing clusters of X = s + k events, where k denotes the missed events and s the sampled ones (binomial sampling):

This equation holds for any discrete P (s) defined on, the set of non-negative integers. To infer P (s) from P sub (s), we develop in the following a novel 'subsampling scaling' that allows to parcel out the changes in P (s) originating from spatial subsampling. A correct scaling ansatz collapses the P sub (s) for any sampling probability p.

In the following, we focus on subsampling from two specific families of distributions that are of particular importance in the context of neuroscience, namely exponential distributions P (s) = C λ e − λs with λ > 0, and power laws P (s) = C γ s − γ with γ > 1. These two families are known to show different behaviours under subsampling:
For exponential distributions, P (s) and P sub (s) belong to the same class of distributions, only their parameters change under subsampling. Notably, this result generalizes to positive and negative binomial distributions, which include Poisson distributions.
Power-laws or scale-free distributions, despite their name, are not invariant under subsampling. Namely, if P (s) follows a power-law distribution, then P sub (s) is not a power law but only approaching it in the limit of large cluster size (s →∞).

---

### Input-output maps are strongly biased towards simple outputs [^50fca0dd]. Nature Communications (2018). Medium credibility.

Many systems in nature can be described using discrete input-output maps. Without knowing details about a map, there may seem to be no a priori reason to expect that a randomly chosen input would be more likely to generate one output over another. Here, by extending fundamental results from algorithmic information theory, we show instead that for many real-world maps, the a priori probability P(x) that randomly sampled inputs generate a particular output x decays exponentially with the approximate Kolmogorov complexity [Formula: see text] of that output. These input-output maps are biased towards simplicity. We derive an upper bound P(x)≲[Formula: see text], which is tight for most inputs. The constants a and b, as well as many properties of P(x), can be predicted with minimal knowledge of the map. We explore this strong bias towards simple outputs in systems ranging from the folding of RNA secondary structures to systems of coupled ordinary differential equations to a stochastic financial trading model.

---

### Calculating T2 in images from a phased array receiver [^23db2d0c]. Magnetic Resonance in Medicine (2009). Low credibility.

Images reconstructed from multielement, phased array coils and presented as the square root of the sum of the squares of the signals received by the individual elements have a distribution of signal and noise that distorts the relationship between the image intensity and the underlying signal. The distortion is accentuated for long echo times for which the signal-to-noise ratio (SNR) may be low. When measuring T(2) or T(2)* this signal distortion leads to biased estimates of these parameters. We demonstrate this effect and its dependence on the image SNR and the number of elements in a phased array coil. We evaluated the effects of four techniques for calculating T(2) from data acquired in phased array coils (log transform, least squares, lookup table correction, and maximum likelihood [ML] estimation). The ML estimation gave the most accurate T(2) in the presence of this bias.

---

### Influence of noise correlation in multiple-coil statistical models with sum of squares reconstruction [^47d0e721]. Magnetic Resonance in Medicine (2012). Low credibility.

Noise in the composite magnitude signal from multiple-coil systems is usually assumed to follow a noncentral χ distribution when sum of squares is used to combine images sensed at different coils. However, this is true only if the variance of noise is the same for all coils, and no correlation exists between them. We show how correlations may be obviated from this model if effective values are considered. This implies a reduced effective number of coils and an increased effective variance of noise. In addition, the effective variance of noise becomes signal-dependent.

---

### Fundamental limits to learning closed-form mathematical models from data [^5954e9c9]. Nature Communications (2023). High credibility.

Fig. 1
Probabilistic model selection makes quasi-optimal predictions about unobserved data.

We select two models m *, whose expressions are shown at the top of each column. a, b From each model, we generate synthetic datasets D with N points (shown, N = 100) and different levels of noise s ϵ (shown, s ϵ = 1). Here and throughout the article, the values of the independent variables x 1 and x 2 are generated uniformly at random in [− 2, 2]. Vertical lines show the observation error ϵ i for each point in D. For a model not drawn from the prior and data generated differently, see Supplementary Fig. S1. c, d For each dataset D (with dataset sizes N ∈ {25, 50, 100, 200, 400}), we sample models from p (m ∣ D) using the Bayesian machine scientist, select the MDL model (maximum p (m ∣ D)) among those sampled, and use this model to make predictions on a test dataset, generated exactly as D. We show the prediction root mean squared error (RMSE) of the MDL model onas a function of N and s ϵ. For comparison, we also show the predictions from an artificial neural network (ANN, dotted lines; Methods). Since s ϵ is the irreducible error, predictions on the diagonal RMSE = s ϵ are optimal. e, f We plot the prediction RMSE scaled by the irreducible error s ϵ; optimal predictions satisfy RMSE/ s ϵ = 1 (dashed line).

---

### Parametric matrix models [^16855d0e]. Nature Communications (2025). High credibility.

The output functions of neural networks do not correspond to the solutions of any known underlying equations. Rather, they are nested function evaluations of linear combinations of activation functions. Many commonly-used activation functions have different functional forms for x < 0 and x ≥ 0 and therefore are not amenable to analytic continuation. Most of the other commonly-used activation functions involve logarithmic or rational functions of exponentials, resulting in a more complicated analytic structure.

Let us consider a PMM that uses Hermitian primary matrices with matrix elements that are polynomials of the input features up to degree D. Suppose now that we vary one input feature, c l, while keeping fixed all other input features. The output functions of c l can be continued into the complex plane, with only a finite number of exceptional points where the functions are not analytic. These exceptional points are branch points where two or more eigenvectors coincide. A necessary condition for such an exceptional point is that the characteristic polynomial of one of the primary matrices, has a repeated root. This in turn corresponds to the discriminant of the characteristic polynomial equaling zero. If the primary matrix has n × n entries, the discriminant is a polynomial function of c l with degree n (n − 1) D. We therefore have a count of n (n − 1) D branch points as a complex function of c l for each primary matrix. If a branch point comes close to the real axis, then we have a sharp avoided level crossing and the character of the output function changes abruptly near the branch point. Our count of n (n − 1) D branch points for each primary matrix gives a characterization of the Riemann surface complexity for the functions that can be expressed using a PMM.

---

### Improved machine learning algorithm for predicting ground state properties [^0c6418ab]. Nature Communications (2024). High credibility.

Theorem 1

(Sample and computational complexity). Givenand a training data setof sizewhere x ℓ is sampled from an unknown distributionandfor any observable O with eigenvalues between −1 and 1 that can be written as a sum of geometrically local observables. With a proper choice of the efficiently computable hyperparameters δ 1, δ 2, and B, the learned function h * (x) = w * ⋅ ϕ (x) satisfieswith probability at least 1 − δ. The training and prediction time of the classical ML model are bounded by.

The output y ℓ in the training data can be obtained by measuringfor the same observable O multiple times and averaging the outcomes. Alternatively, we can use the classical shadow formalism –, that performs randomized Pauli measurements on ρ (x ℓ) to predictfor a wide range of observables O. We can also combine Theorem 1 and the classical shadow formalism to use our ML algorithm to predict ground state representations, as seen in the following corollary. This allows one to predict ground state propertiesfor a large number of observables O rather than just a single one. We present the proof of Corollary 1 in Supplementary Section 3B.

Corollary 1

Givenand a training data setof sizewhere x ℓ is sampled from an unknown distributionand σ T (ρ (x ℓ)) is the classical shadow representation of the ground state ρ (x ℓ) using T randomized Pauli measurements. For, then the proposed ML algorithm can learn a ground state representationthat achievesfor any observable O with eigenvalues between −1 and 1 that can be written as a sum of geometrically local observables with probability at least 1 − δ.

We can also show that the problem of estimating ground state properties for the class of parameterized Hamiltoniansconsidered in this work is hard for non-ML algorithms that cannot learn from data, assuming the widely believed conjecture that NP-complete problems cannot be solved in randomized polynomial time. This is a manifestation of the computational power of data studied in. The proof of Proposition 1 inconstructs a parameterized Hamiltonian H (x) that belongs to the family of parameterized Hamiltonians considered in this work and hence establishes the following.

---

### Validation of monte carlo estimates of three-class ideal observer operating points for normal data [^26b6e5d9]. Academic Radiology (2013). Low credibility.

Rationale and Objectives

Traditional two-class receiver operating characteristic (ROC) analysis is inadequate for the complete evaluation of observer performance in tasks with more than two classes.

Materials and Methods

Here, a Monte Carlo estimation method for operating point coordinates on a three-class ROC surface is developed and compared with analytically calculated coordinates in two special cases: (1) univariate and (2) restricted bivariate trinormal underlying data.

Results

In both cases, the statistical estimates were found to be good in the sense that the analytical values lay within the 95% confidence interval of the estimated values about 95% of the time.

Conclusions

The statistical estimation method should be key in the development of a pragmatic performance metric for evaluation of observers in classification tasks with three or more classes.

---

### Bacterial motility can govern the dynamics of antibiotic resistance evolution [^c771c187]. Nature Communications (2023). High credibility.

Analytical and numerical techniques

In addition to computer simulations, we developed a combination of analytical and numerical techniques, which are used to compute the evolutionary time, ecological time and the adaptation rate. In contrast to simulations, these techniques are based on closed-form solutions of master equations, numerical methods for solving algebraic equations (such as Newton-Raphson method) and solving ODEs (such as Runge-Kutta method). To illustrate these techniques, we show how to compute the expected waiting times at a resistance state R and the adaptation rate a R in the original staircase model.
Find the wild-type profile N x at a resistance state R. Use Newton-Raphson method to solve the algebraic equations for the non-trivial fixed point of the mean-field theorywhereis the indicator function.
Compute the evolutionary time. A closed-form solution exists and follows from a modification of the theory in Hermsen et al. We consider three independent adaptation paths along which wild-type can produce a first mutant in the overlap region x = R + 1. On the position-genotype lattice (x, g), they are the division path D: (R, R) → (R, R + 1) → (R + 1, R + 1) (mutation at x = R followed by migration to the right); the overlap path O: (R + 1, R) → (R + 1, R + 1) (mutation at x = R + 1); and the no-division path N: (R + 2, R) → (R + 2, R + 1) → (R + 1, R + 1) (mutation at x = R + 2 followed by migration to the left). As the original theoryassumes low motility, it neglects the no-division path since there are no wild-type cells to mutate at position x = R + 2 in this regime (SI Theorem 3). Moreover, we assume that the flux of mutants between neighbouring compartments in the division region x ≤ R and the no-divison region x ≥ R + 2 is equilibrated, so that we can ignore the net probability flux of mutants that enter or exit the compartments x = R − 1, R, R + 1 and also ignore longer paths such as (R − 1, R) → (R − 1, R + 1) → (R, R + 1) → (R + 1, R + 1) (mutation at x = R − 1 followed by double migration to the right). This is a reasonable assumption as the flux of mutant-producing wild-type is equilibrated far from the population front. The computation ofhas the following structure. The waiting times till a mutant is produced in the overlap region along path i ∈ { D, O, N } are denoted by T i (Fig. 2 a). The probability that the waiting will be longer than t isand the probability density function of T i is given by F i (t) = −d S i /d t. Notice thatand that T i are independent. Thus. The probability density function ofis The expected timeis found by numerical integration. Therefore, the problem is solved by finding the functions S i (t) and differentiating them to produce F i (t). According to Hermsen et al. these functions are: For path D: whereandwhere 〈 n (t)〉 D is the average mutant population in compartment x = R at time t For path O: For path N: whereandwhere 〈 n (t)〉 N is the average mutant population in compartment x = R + 2 at time t Finally, if there is a nonnegligible chance that the first mutant in the overlap region dies before division, the evolutionary timemust be corrected to, where q is the probability that the first mutant in the overlap region divides before its death (Supplementary Note 8).
Compute the ecological time. The competition between wild-type and mutants can be described well by the mean-field equations:with initial conditions at time t = 0:In these equations w x and m x stand for the number of wild-type and mutant cells in spatial compartment x, respectively. The system is simulated from the end state of the previous stochastic process when a first mutant appears in the overlap region. This system can be simulated efficiently by a Runge-Kutta (RK4) method. The timeis reached when w R +1 < m R +1 for the first time, i.e. the mutants outcompete the wild-type in the overlap region.
Find the adaptation rate. The adaptation rate is found from

---

### Recovery after stroke: not so proportional after all? [^d3259e7a]. Brain (2019). Medium credibility.

Spurious r(X,Δ) are likely when σ Y /σ X is small

For any X and Y, it can be shown that:

A formal proof of Equation 1 is provided in the Supplementary material, Appendix A [proposition 4 and theorem 1; also see]; its consequence is that r(X,Δ) is a function of r(X, Y) and σ Y /σ X. To illustrate that function, we performed a series of simulations (Supplementary material, Appendix B) in which r(X, Y) and σ Y /σ X were varied independently. Figure 2 illustrates the results: a surface relating r(X,Δ) to r(X, Y) and σ Y /σ X. Figure 3 shows example recovery data at six points of interest on that surface.

Figure 2
The relationship between r(X, Y), r(X,Δ) and σ Y /σ X. Note that the x -axis is log-transformed to ensure symmetry around 1; when X and Y are equally variable, log(σ Y /σ X) = 0. Supplementary material, proposition 7 in Appendix A, provides a justification for unambiguously using a ratio of standard deviations in this figure, rather than σ Y and σ X as separate axes. The two major regimes of Equation 1 are also marked in red. In Regime 1, Y is more variable than X, so contributes more variance to Δ, and r(X,Δ) ≈ r(X, Y). In Regime 2, X is more variable than Y, so X contributes more variance to Δ, and r(X,Δ) ≈ r(X,−X) (i.e. −1). The transition between the two regimes, when the variability ratio is not dramatically skewed either way, also allows for spurious r(X,Δ). For the purposes of illustration, the figure also highlights six points of interest on the surface, marked A–F; examples of simulated recovery data corresponding to these points are provided in Fig. 3.

---

### Physical limits to sensing material properties [^c3e3f73f]. Nature Communications (2020). High credibility.

All materials respond heterogeneously at small scales, which limits what a sensor can learn. Although previous studies have characterized measurement noise arising from thermal fluctuations, the limits imposed by structural heterogeneity have remained unclear. In this paper, we find that the least fractional uncertainty with which a sensor can determine a material constant λ 0 of an elastic medium is approximately [Formula: see text] for a ≫ d ≫ ξ, [Formula: see text], and D > 1, where a is the size of the sensor, d is its spatial resolution, ξ is the correlation length of fluctuations in λ 0, Δ λ is the local variability of λ 0, and D is the dimension of the medium. Our results reveal how one can construct devices capable of sensing near these limits, e.g. for medical diagnostics. We use our theoretical framework to estimate the limits of mechanosensing in a biopolymer network, a sensory process involved in cellular behavior, medical diagnostics, and material fabrication.

---

### Detecting the ultra low dimensionality of real networks [^baa84bdd]. Nature Communications (2022). High credibility.

Methods

Estimation of hidden degrees of real networks

Given a real network, our goal is to generate networks with themodel but preserving the degree distribution of the real network as much as possible. In themodel, hidden degrees are given fixed values but nodes' degrees are random variables that depend on the hidden degrees. Therefore, to reproduce the degree distribution, we have to find the sequence of hidden degrees that better reproduces the sequence of observed degrees. To do so, we generalize the method into arbitrary dimensions. Given a set of parameters β and D, for each observed degree class k in a real network, we infer the corresponding hidden degree κ so that the ensemble average degree of themodel of a node with hidden degree κ is equal to its observed degree, that is.

After this procedure, the degree distribution of synthetic networks generated by themodel with the inferred sequence of hidden degrees is very similar to the one from the real network. Specifically,
Initially set κ i = k i ∀ i = 1, N, where k i is the observed degree of node i in the real network.
Compute the expected degree for each node i according to themodel as whereand.
Correct hidden degrees: Letbe the maximal deviation between actual degrees and expected degrees. If ϵ m a x > ϵ, the set of hidden degrees needs to be corrected. Then, for every class of degree k i, we set, where u is a random variable drawn from U (0, 1). The random variable u prevents the process from getting trapped in a local minimum. Next, go to step 2 to compute the expected degrees corresponding to the new set of hidden degrees. Otherwise, if ϵ m a x ≤ ϵ, hidden degrees have been correctly inferred for the current global parameters.

Following this algorithm, we can generate surrogates of a given network G with different D and β values without modifying the degree distribution. The tolerance value of ϵ used in this work is ϵ = 1.

---

### An additive gaussian process regression model for interpretable non-parametric analysis of longitudinal data [^d2023ff9]. Nature Communications (2019). High credibility.

Prior specifications

Before the actual GP regression, we standardise the target variable and all continuous covariates such that the mean is zero and the standard deviation is one. This helps in defining generally applicable priors for the kernel parameters. After the GP regression, the predictions are transformed back to the original scale. We visualise the results in the original scale after centering the data by subtracting the mean.

We define a priorfor the kernel parameters as follows. For continuous covariates without interactions, we use the log normal prior (μ = 0 and σ 2 = (log(1) − log(0.1)) 2 /4) for the length-scales (and) and the square root student- t prior (μ = 0, σ 2 = 1, and ν = 20) for the magnitude parameters (and). This length-scale prior penalises small length-scales such that smoothness less than 0.1 has very small probability and the mode is approximately at 0.3. For continuous covariates with interactions, the prior for the magnitude parameters is the same as for without interactions and the half truncated student- t prior (μ = 0, σ 2 = 1, ν = 4) is used for the length-scale, which allows smaller length-scales.

Scaled inverse chi-squared prior (σ 2 = 0.01 and ν = 1) is used for the noise variance parameter. The period parameter γ of the periodic kernel is predefined by the user. Square root student- t prior (μ = 0, σ 2 = 1, and ν = 4) is used for the magnitude parameterof all co kernels. Supplementary Fig. 6 visualises all the above-described priors with their default hyperparameter values.

---

### UnidecNMR: automatic peak detection for NMR spectra in 1–4 dimensions [^6d76f76f]. Nature Communications (2025). High credibility.

Results

Theory

The kernel of the algorithm was originally developed to analyse mass spectrometry data, a problem that shares many features with NMR data analysis. The method relies on the assumption that a spectrum of intensities, I, can be reasonably expressed as a convolution of a peak shape function, g, and an array of delta functions or sources, f, each of which spans the same set of spectral frequencies, i. The algorithm aims to perform the deconvolution that removes the peak shape function from the data, providing a user with a list of sources, f, that dictates both the peak positions and intensities. The kernel iterates (t) on the intensities of each spectral element:

When the back-calculated spectrum, (f∗g), has the same intensity as the data I at frequency i, then the ratio is equal to 1, and the algorithm has converged. The action of the algorithm is to suppress sources that are adjacent to the true 'centre' of a resonance. A user supplies a noise threshold and a peak shape function g (Fig. 1a). In our implementation, the initial intensities are set to the initial intensities of the raw data found at each position, and then iteratively adjusted according to Eq. (1) until convergence, where the final values in f provide the central locations and intensities of the picked peaks. To determine convergence, the changes made in intensities are assessed in each step, and when these fall below a user-specified threshold, or when the calculation exceeds a pre-specified number of maximum iterations, the calculation stops. The selected peak shape g should be a reasonable match for the average resonance in a spectrum, although the final results are reasonably tolerant to this parameter being mis-set (Supplementary Fig. 1a), as expected in the case of experimental data where there is a wide range of peak shapes. The algorithm can work on data of arbitrary dimensionality, which renders it highly amenable to NMR analysis.

---

### Improving local prevalence estimates of SARS-CoV-2 infections using a causal debiasing framework [^f0a11415]. Nature Microbiology (2022). High credibility.

Global and national surveillance of SARS-CoV-2 epidemiology is mostly based on targeted schemes focused on testing individuals with symptoms. These tested groups are often unrepresentative of the wider population and exhibit test positivity rates that are biased upwards compared with the true population prevalence. Such data are routinely used to infer infection prevalence and the effective reproduction number, R t, which affects public health policy. Here, we describe a causal framework that provides debiased fine-scale spatiotemporal estimates by combining targeted test counts with data from a randomized surveillance study in the United Kingdom called REACT. Our probabilistic model includes a bias parameter that captures the increased probability of an infected individual being tested, relative to a non-infected individual, and transforms observed test counts to debiased estimates of the true underlying local prevalence and R t. We validated our approach on held-out REACT data over a 7-month period. Furthermore, our local estimates of R t are indicative of 1-week- and 2-week-ahead changes in SARS-CoV-2-positive case numbers. We also observed increases in estimated local prevalence and R t that reflect the spread of the Alpha and Delta variants. Our results illustrate how randomized surveys can augment targeted testing to improve statistical accuracy in monitoring the spread of emerging and ongoing infectious disease.

---

### Clinicians are right not to like cohen's κ [^a5f2e987]. BMJ (2013). Excellent credibility.

Clinicians are interested in observer variation in terms of the probability of other raters (interobserver) or themselves (intraobserver) obtaining the same answer. Cohen's κ is commonly used in the medical literature to express such agreement in categorical outcomes. The value of Cohen's κ, however, is not sufficiently informative because it is a relative measure, while the clinician's question of observer variation calls for an absolute measure. Using an example in which the observed agreement and κ lead to different conclusions, we illustrate that percentage agreement is an absolute measure (a measure of agreement) and that κ is a relative measure (a measure of reliability). For the data to be useful for clinicians, measures of agreement should be used. The proportion of specific agreement, expressing the agreement separately for the positive and the negative ratings, is the most appropriate measure for conveying the relevant information in a 2 × 2 table and is most informative for clinicians.

---

### Universal scaling in real dimension [^d1d47cd6]. Nature Communications (2024). High credibility.

In order to extract the correlation length exponent from the scaling of 〈 R 2 (N)〉, one needs to identify a suitable region of N where one can reasonably apply the relation in Eq. (3). This universal region can be identified by observing the collapse of different curves 〈 R 2 (N)〉 for different system sizes L. This analysis is reported in Fig. 4, for both σ = 1.8 and σ = 1.1. In the upper panel the collapse in rather evident in a wider region of N, while in the lower panel the collapse region is harder to determine. The universal window is visually identified as the region where the three curves overlap and exhibit a linear behavior. For all σ ≳ 1.5 the universal window roughly corresponds to the region where the disagreement between the three curves remains below 5%. The same analysis has been repeated for all values of σ resulting in very extended and well-defined fitting regions for all σ ≳ 1.5, similarly to what is reported in the upper panel in Fig. 4. For 1.5 ≳ σ ≳ 1.1 the collapse region is less extended but still pronounced enough to obtain reliable ν estimates, at least up to d s ≃ 3.5, see the lower panel in Fig. 4. Finally, in the region σ ≤ 1 (corresponding to d s ≥ 4) it is not possible to identify a clear universal region and, accordingly, the ν estimates become less precise, see the Supplementary Note II. As a consequence, the agreement between our data and the theoretical line is poorer in this region, see Fig. 3.

Fig. 4
Logarithmic derivative of the average spatial extent of the random walk.

The plot displays the logarithmic derivative of the average spatial extent of the random walk as a function of the inverse logarithm of the number of steps N. Data shown for σ = 1.8 and σ = 1.1, corresponding to ρ = 3.8 (top) and ρ = 3.1 (bottom), for different lattice sizes: L × L = 64 × 64 (blue), 128 × 128 (yellow), 256 × 256 (green). The collapse region is well established for larger values of ρ and corresponds to a regime where scaling corrections and finite sizes effects are negligible, so that one can define a ' ', as bracketed by the vertical red dashed lines, and extrapolate from there the critical exponent.

---

### What are the implications of alternative alpha thresholds for hypothesis testing in orthopaedics? [^fb5050c0]. Clinical Orthopaedics and Related Research (2019). Medium credibility.

Background

Clinical research in orthopaedics typically reports the presence of an association after rejecting a null hypothesis of no association using an alpha threshold of 0.05 at which to evaluate a calculated p value. This arbitrary value is a factor that results in the current difficulties reproducing research findings. A proposal is gaining attention to lower the alpha threshold to 0.005. However, it is currently unknown how alpha thresholds are used in orthopaedics and the distribution of p values reported.

Questions/Purposes

We sought to describe the use of alpha thresholds in two orthopaedic journals by asking (1) How frequently are alpha threshold values reported? (2) How frequently are power calculations reported? (3) How frequently are p values between 0.005 and 0.05 reported for the main hypothesis? (4) Are p values less than 0.005 associated with study characteristics such as design and reporting power calculations?

Methods

The 100 most recent original clinical research articles from two leading orthopaedic journals at the time of this proposal were reviewed. For studies without a specified primary hypothesis, a main hypothesis was selected that was most consistent with the title and abstract. The p value for the main hypothesis and lowest p value for each study were recorded. Study characteristics including details of alpha thresholds, beta, and p values were recorded. Associations between study characteristics and p values were described. Of the 200 articles (100 from each journal), 23 were randomized controlled trials, 141 were cohort studies or case series (defined as a study in which authors had access to original data collected for the study purpose), 31 were database studies, and five were classified as other.

Results

An alpha threshold was reported in 166 articles (83%) with all but two reporting a value 0.05. Forty-two articles (21%) reported performing a power calculation. The p value for the main hypothesis was less than 0.005 for 88 articles (44%), between 0.05 and 0.005 for 67 (34%), and greater than 0.05 for 29 (15%). The smallest p value was between 0.05 and 0.005 for 39 articles (20%), less than 0.005 for 143 (72%), and either not provided or greater than 0.05 for 18 (9%). Although 50% (65 of 130) cohort and database papers had a main hypothesis p value less than 0.005, only 26% (6 of 23) randomized controlled trials did. Only 36% (15 of 42) articles reporting a power calculation had a p value less than 0.005 compared with 51% (73 of 142) that did not report one.

Conclusions

Although a lower alpha threshold may theoretically increase the reproducibility of research findings across orthopaedics, this would preferentially select findings from lower-quality studies or increase the burden on higher quality ones. A more-nuanced approach could be to consider alpha thresholds specific to study characteristics. For example, randomized controlled trials with a prespecified primary hypothesis may still be best evaluated at 0.05 while database studies with an abundance of statistical tests may be best evaluated at a threshold even below 0.005.

Clinical Relevance

Surgeons and scientists in orthopaedics should understand that the default alpha threshold of 0.05 represents an arbitrary value that could be lowered to help reduce type-I errors; however, it must also be appreciated that such a change could increase type-II errors, increase resource utilization, and preferentially select findings from lower-quality studies.

---

### Identifying domains of applicability of machine learning models for materials science [^72c4ba76]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ν that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by σ lin (x 1, x 2) ≡ −0.3 ≤ x 1 ≤ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: σ rbf ≡ −3.3 ≤ x 11 ≤ 3.1 ∧ x 2 ≤ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA σ poly (x 1, x 2) ≡ −3.5 ≤ x 2 ≤ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### Creating a library of generalized fourier sampling patterns for irregular 2D regions of support [^ad8490e3]. Magnetic Resonance in Medicine (2001). Low credibility.

Multiple-region MRI (mrMRI) represents a generalization of the Shannon sampling theorem to permit sparse k-space sampling whenever the scanned object or its high-contrast edges are confined to multiple known regions. Use of an optimal mrMRI sampling pattern produces an image with root-mean-squared (RMS) noise over the supporting regions equal to the RMS noise in a conventional Fourier image with the same total area of support. Analytical solutions for such sampling patterns have been described previously for all arrangements of two or three (noncollinear) supporting regions. This work describes a robust numerical method for creating a library of optimal and near-optimal mrMRI sampling patterns for more complicated geometries. The average noise amplification over all sampling patterns in the demonstration library was only 4%, with 30% of the sampling patterns resulting in no noise amplification whatsoever.

---

### Mitigating the impact of flip angle and orientation dependence in single compartment R2* estimates via 2-pool modeling [^f34ed303]. Magnetic Resonance in Medicine (2023). Medium credibility.

Purpose

The effective transverse relaxation rate (< mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:msubsup > < mml:mi > R < /mml:mi > < mml:mn > 2 < /mml:mn > < mml:mo > * < /mml:mo > < /mml:msubsup > < /mml:mrow > < mml:annotation > $$ {\mathrm{R}}_2^{\ast } $$ < /mml:annotation > < /mml:semantics > < /mml:math >) is influenced by biological features that make it a useful means of probing brain microstructure. However, confounding factors such as dependence on flip angle (α) and fiber orientation with respect to the main field (< mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:mi > θ < /mml:mi > < /mml:mrow > < mml:annotation > $$ \uptheta $$ < /mml:annotation > < /mml:semantics > < /mml:math >) complicate interpretation. The α- and < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:mi > θ < /mml:mi > < /mml:mrow > < mml:annotation > $$ \uptheta $$ < /mml:annotation > < /mml:semantics > < /mml:math > -dependence stem from the existence of multiple sub-voxel micro-environments (e.g., myelin and non-myelin water compartments). Ordinarily, it is challenging to quantify these sub-compartments; therefore, neuroscientific studies commonly make the simplifying assumption of a mono-exponential decay obtaining a single < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:msubsup > < mml:mi > R < /mml:mi > < mml:mn > 2 < /mml:mn > < mml:mo > * < /mml:mo > < /mml:msubsup > < /mml:mrow > < mml:annotation > $$ {\mathrm{R}}_2^{\ast } $$ < /mml:annotation > < /mml:semantics > < /mml:math > estimate per voxel. In this work, we investigated how the multi-compartment nature of tissue microstructure affects single compartment < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:msubsup > < mml:mi > R < /mml:mi > < mml:mn > 2 < /mml:mn > < mml:mo > * < /mml:mo > < /mml:msubsup > < /mml:mrow > < mml:annotation > $$ {\mathrm{R}}_2^{\ast } $$ < /mml:annotation > < /mml:semantics > < /mml:math > estimates.

Methods

We used 2-pool (myelin and non-myelin water) simulations to characterize the bias in single compartment < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:msubsup > < mml:mi > R < /mml:mi > < mml:mn > 2 < /mml:mn > < mml:mo > * < /mml:mo > < /mml:msubsup > < /mml:mrow > < mml:annotation > $$ {\mathrm{R}}_2^{\ast } $$ < /mml:annotation > < /mml:semantics > < /mml:math > estimates. Based on our numeric observations, we introduced a linear model that partitions < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:msubsup > < mml:mi > R < /mml:mi > < mml:mn > 2 < /mml:mn > < mml:mo > * < /mml:mo > < /mml:msubsup > < /mml:mrow > < mml:annotation > $$ {\mathrm{R}}_2^{\ast } $$ < /mml:annotation > < /mml:semantics > < /mml:math > into α-dependent and α-independent components and validated this in vivo at 7T. We investigated the dependence of both components on the sub-compartment properties and assessed their robustness, orientation dependence, and reproducibility empirically.

Results

 < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:msubsup > < mml:mi > R < /mml:mi > < mml:mn > 2 < /mml:mn > < mml:mo > * < /mml:mo > < /mml:msubsup > < /mml:mrow > < mml:annotation > $$ {\mathrm{R}}_2^{\ast } $$ < /mml:annotation > < /mml:semantics > < /mml:math > increased with myelin water fraction and residency time leading to a linear dependence on α. We observed excellent agreement between our numeric and empirical results. Furthermore, the α-independent component of the proposed linear model was robust to the choice of α and reduced dependence on fiber orientation, although it suffered from marginally higher noise sensitivity.

Conclusion

We have demonstrated and validated a simple approach that mitigates flip angle and orientation biases in single-compartment < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:msubsup > < mml:mi > R < /mml:mi > < mml:mn > 2 < /mml:mn > < mml:mo > * < /mml:mo > < /mml:msubsup > < /mml:mrow > < mml:annotation > $$ {\mathrm{R}}_2^{\ast } $$ < /mml:annotation > < /mml:semantics > < /mml:math > estimates.

---

### Strigolactones are chemoattractants for host tropism in orobanchaceae parasitic plants [^1412d9d1]. Nature Communications (2022). High credibility.

ImageJ (version 1.52q), Microsoft Excel and Adobe Illustrator software were employed to quantify the images. In all cases, WT roots were positioned on the left and d10 was on the right. First, images derived from 1- or 2-day(s) post infection were superimposed onto their corresponding 0-day images using Adobe Illustrator. Next, using ImageJ, the position of the P. japonicum root tip was defined as y = 0 and the position of the left (WT) host at horizontal axis with the P. japonicum root tip was defined as x = 0 in 0-day samples. Thus, each rice root and the P. japonicum root tip were assigned a unique x and y value: for instance, WT rice (x = 0 µm, y = 0 µm), P. japonicum (x = 904.8 µm, y = 0 µm) and d10 rice (x = 1979.6 µm, y = 0 µm) at 0 day. The xy positions of the P. japonicum root tip at 1- and 2-day(s) post infection relative to 0 day were then measured. Raw xy data generated from all samples as described above were subjected to centring and scaling processes before generating the graphs shown in the figures (For details refer to Source Data).

---

### An exact form for the magnetic field density of States for a dipole [^2c861285]. Magnetic Resonance Imaging (2001). Low credibility.

We present an analytical form for the density of states for a magnetic dipole in the center of a spherical voxel. This analytic form is then used to evaluate the signal decay as a function of echo time for different volume fractions and susceptibilities. The decay can be considered exponential only in a limited interval of time. Otherwise, it has a quadratic dependence on time for short echo times and an oscillatory decaying behavior for long echo times.

---

### Correction [^26cbbc41]. Journal of Radiology Case Reports (2017). Low credibility.

[This corrects the article on p. 8 in vol. 10.].

---

### Esotropia and exotropia preferred practice pattern ® [^38cbc533]. Ophthalmology (2023). High credibility.

Esotropia ICD-10 CM codes — entities and laterality rules are specified as follows: Nonaccommodative (unspecified) H50.00; Accommodative H50.43; Alternating H50.05; Alternating with A pattern H50.06; Alternating with V pattern H50.07; Alternating with X or Y pattern (with other noncomitancies) H50.08; Monocular H50.01–; Monocular with A pattern H50.02–; Monocular with V pattern H50.03–; Monocular with X or Y pattern (with other noncomitancies) H50.04–; Intermittent, alternating H50.32; Intermittent, monocular H50.31–; Unspecified H50.00. CM = Clinical Modification used in the United States; = 1, right eye; 2, left eye. For bilateral sites, the final character of the codes indicates laterality. Esotropia and exotropia do not have bilateral codes. Therefore, if the condition is bilateral, assign separate codes for both the left and right side. When the diagnosis code specifies laterality, regardless of which digit it is found in (i.e., 4th digit, 5th digit, or 6th digit), most often you will find: Right is 1 Left is 2.

---

### On the noise in "augmented T1-weighted steady state magnetic resonance imaging" [^5e296aaa]. NMR in Biomedicine (2023). Medium credibility.

Recently, Ye and colleagues proposed a method for "augmented T1-weighted imaging" (aT 1 W). The key operation is a complex division of gradient-echo (GRE) images obtained with different flip angles. Ye and colleagues provide an equation for the standard deviation of the obtained aT 1 W signal. Here, we show that this equation leads to wrong values of the standard deviation of such an aT 1 W signal. This is demonstrated by Monte Carlo simulations. The derivation of the equation provided by Ye and colleagues is shown to be erroneous. The error consists of a wrong handling of random variables and their standard deviations and of the wrong assumption of correlated noise in independently acquired GRE images. Instead, the probability distribution obtained with the aT 1 W-method should have been carefully analyzed, perhaps on the basis of previous literature on ratio distributions and their normal approximations.

---

### Spin dephasing under nonlinear gradients: implications for imaging and field mapping [^eaf2803c]. Magnetic Resonance in Medicine (2012). Low credibility.

This work examines the prototypical MR echo that would be expected for a voxel of spins evolving in a strong nonlinear field, specifically focusing on the quadratic z(2) - ½(x(2) + y(2)) field. Dephasing under nonlinear gradients is increasingly relevant given the growing interest in nonlinear imaging, and here, we report several notable differences from the linear case. Most notably, in addition to signal loss, intravoxel dephasing under gradients creating a wide and asymmetric frequency distribution across the voxel can cause skewed and nonlinear phase evolution. After presenting the qualitative and analytical origins of this difference, we experimentally demonstrate that neglecting these dynamics can lead to significant errors in sequences that assume phase evolution is proportional to voxel frequency, such as those used for field mapping. Finally, simplifying approximations to the signal equations are presented, which not only provide more intuitive forms of the exact expression but also result in simple rules to predict key features of the nonlinear evolution.

---

### Xenon (xenon, Xe-133) [^382e5366]. FDA (2024). Medium credibility.

PHYSICAL CHARACTERISTICS

Xenon Xe 133 decays by beta and gamma emissions with a physical half-life of 5.245 days. Photons that are useful for detection and imaging studies as well as the principal beta emission are listed in Table 2.

Table 2. Principal Radiation Emission Data

---

### Nonlinear wave propagation governed by a fractional derivative [^db5b4cd7]. Nature Communications (2025). High credibility.

The idea of fractional derivatives has a long history that dates back centuries. Apart from their intriguing mathematical properties, fractional derivatives have been studied widely in physics, for example in quantum mechanics and generally in systems with nonlocal temporal or spatial interactions. However, systematic experiments have been rare because the physical implementation is challenging. Here we report the observation and full characterization of a family of temporal solitons that are governed by a fractional nonlinear wave equation. We demonstrate that these solitons have non-exponential tails, reflecting their nonlocal character, and have a very small time-bandwidth product.

---

### Input-output maps are strongly biased towards simple outputs [^659947d0]. Nature Communications (2018). Medium credibility.

Random matrix map with bias but not simplicity bias

Finally, we provide an example of a map that exhibits strong bias that is not simplicity bias. We define the matrix map by having binary input vectors p of length n that map to binary output vectors x of same length though x i = Θ ((M ⋅ p) i), where M is a matrix and the Heaviside thresholding function Θ (y) = 0 if y < 0 and Θ (y) = 1 if y ≥ 0. This last nonlinear step, which resembles the thresholding function in simple neural networks, is important since linear maps do not show simplicity bias. In Fig. 1e, we illustrate this map for a matrix made with entries randomly chosen to be −1 or +1. The rank plot (Supplementary Fig. 17) shows strong bias, but in marked contrast to the other maps, the probability P (x) does not correlate with the complexity of the outputs. Simplicity bias does not occur because the n × n independent matrix elements mean that the mapping's complexity grows rapidly with increasing n so that the map violates our limited complexity condition (1). Intuitively: the pattern of 1s and 0s in output x is strongly determined by the particular details of the map M, and so does not correlate with the complexity of the output.

To explore in more detail how simplicity bias develops or disappears with limited complexity condition (1), we also constructed a circulant matrix where we can systematically vary the complexity of the map. It starts with a row of p positive 1 s and q − 1 s, randomly placed. The next row has the same sequence, but permuted by one. This permutation process is repeated to create a square n × n matrix. Thus the map is completely specified by defining the first row together with the procedure to fill out the rest of the matrix. In Fig. 2, we plot the ratio of the mean complexity sampled over outputs divided by the mean complexity sampled over all inputs, as a function of the complexity of the first row that that defines the matrix. If the ratiois significantly larger than one, then the map shows simplicity bias. Simplicity bias only occurs whenis very small, i.e. for relatively simple maps that respect condition (1). The output of one such simple matrix maps is shown in Fig. 1f. In Supplementary Note 12, we investigate these trends in more detail as a function of matrix type, size n, and also investigate the role of matrix rank and sparsity.

---

### Sustainable irrigation based on co-regulation of soil water supply and atmospheric evaporative demand [^9677f16d]. Nature Communications (2021). High credibility.

Physical processes

Ecosys uses a multilayered soil-root-canopy system to get hourly two-stage convergent solutions for crop carbon assimilation, water uptake, and energy fluxes –. The first stage focuses on the convergence of canopy temperature for the first-order closure of canopy energy balance in Equation 3, including net radiation R n, latent heat flux LE (including from evaporation, LE v in Equation 4a, and transpiration, LE c in Equation 4 b), sensible heat flux H, and soil heat flux G. Canopy latent heat is controlled by aerodynamic resistance (r a), and canopy stomatal resistance (r c). Canopy stomatal resistance is postulated by two controlling mechanisms: canopy photosynthesis (leaf level driven by rates of carboxylation vs. diffusion, Equation 5) and canopy turgor potential (ψ t, canopy level constrained by water status, Equation 6). The second stage focuses on the convergence of canopy water potential (ψ c) and thereby r c (= 1/ g s) at which transpiration (E c) based on canopy energy balance (left term in Equation 7) equals root water uptake from multiple soil layers (right term in Equation 7). Root water uptake is calculated using water potential differences between soil (ψ s) and root (ψ r), and the soil and root hydraulic resistances (Ω s and Ω r) in each rooted soil layer.where r c (min) is the minimum r c at ψ c = 0 MPa; r c (max) is canopy cuticular resistance to vapor flux; C b is [CO 2] in canopy air; C i ΄ is [CO 2] in canopy leaves at ψ c = 0 MPa; V c ΄ is the potential canopy CO 2 fixation rate at ψ c = 0 MPa; ψ π is canopy osmotic potential; e a is atmospheric vapor density at air temperature (T a) and ambient humidity; e c is canopy vapor density at canopy temperature (T c) and ψ c; Ω s, r, l is radial resistance to water transport from soil to surface of roots or mycorrhizae; Ω r, r, l is radial resistance to water transport from the surface to axis of roots or mycorrhizae; Ω a, r, l, x is axial resistance to water transport along axes of primary (x = 1) or secondary (x = 2) roots or mycorrhizae; l is soil or canopy layer; r is root or mycorrhizae; and X c is canopy capacitance.

---

### Matrix factorization from non-linear projections: application in estimating T2 maps from few echoes [^91a2e8bd]. Magnetic Resonance Imaging (2015). Low credibility.

This work addresses the problem of estimating T2 maps from very few (two) echoes. Existing multi-parametric non-linear curve fitting techniques require a large number (16 or 32) of echoes to estimate T2 values. We show that our method yields very accurate and robust results from only two echoes, where as the curve-fitting techniques require about 16 echoes to achieve the same level of accuracy. We model T2 maps as a rank-deficient matrix. Since the relationship between T2 values and intensity values/K-space samples is not linear, estimating the T2 values requires recovering a low-rank matrix from non-linear projections. We solve this as a non-linear matrix factorization problem. Since the said problem has not been solved before, we propose a simple algorithm for the same.

---

### Identifying domains of applicability of machine learning models for materials science [^196a563d]. Nature Communications (2020). High credibility.

Fig. 1
Domains of applicability of three 2d-models of a noisy third-degree polynomial.

Three different models, linear (top), radial basis function (rbf, center), and polynomial (poly, bottom), are shown approximating the same distribution of two independent features x 1 ~ N (0, 2) and x 2 ~ N (0, 2), and the target property, where N (μ, ϵ 2) denotes a normal distribution with mean μ and standard deviation ϵ. Test points are plotted in 3d plots against the prediction surface of the models (color corresponds to absolute error) where the DA is highlighted in gray. The distributions of individual errors for the DA (gray) and globally (black) are shown in the 2d plots of each panel with the mean error (solid) and the 95th percentile (95 perc./dashed) marked by vertical lines. Note that the global error distribution of the linear model has a considerably long tail, which is capped in the image.

---

### Inferring and validating mechanistic models of neural microcircuits based on spike-train data [^fedd3cc1]. Nature Communications (2019). High credibility.

Modeling input perturbations

In Results section "Inference of input perturbations" we consider input perturbations of the form μ (t) = μ 0 + Jμ 1 (t), where μ 1 (t) is described by the superposition of alpha functions with time constant τ, triggered at times, with Heaviside step function H. The alpha functions are normalized such that their maximum value is 1 when considered in isolation. As an alternative we also considered delayed delta pulses instead of alpha kernelswhere d denotes the time delay. The perturbation onset (trigger) times were generated by randomly sampling successive separation intervalsfrom a Gaussian distribution with 200 ms mean and 50 ms standard deviation.

In Fig. 3 and Supplementary Fig. 3, we quantified the sensitivity to detect weak input perturbations using our estimation methods (1a and 2) in comparison with a detection method based on the generated data only. For a given parametrization N r spike trains were simulated using different realizations of neuronal input noise and perturbation onset times. Detection sensitivity was quantified by comparing the inferred perturbation strengths from data generated with (J ≠ 0) and without (J = 0) perturbations.

For the I&F-based methods it was calculated by the fraction of N r = 50 estimates of J for true J > 0 (J < 0) that exceeded the 95th percentile (fell below the 5th percentile) of estimates without perturbation. The model-free reference method was based on CCGs between the spike trains and perturbation times (in other words, spike density curves aligned to perturbation onset times). For each realization one such curve was calculated by the differences between spike times and the perturbation onset times using a Gaussian kernel with 3 ms standard deviation. Detection sensitivity was assessed by the fraction of N r = 300 CCGs for which a significant peak (for J > 0) or trough (for J < 0) appeared in the interval [0, 100 ms]. Significance was achieved for true J > 0 (J < 0) if the curve maximum (minimum) exceeded the 95th percentile (fell below the 5th percentile) of maxima (minima) in that interval without perturbation.

---

### Defining the time-limited trial for patients with critical illness: an official American Thoracic Society workshop report [^cf1ee11e]. Annals of the American Thoracic Society (2024). High credibility.

Time-limited trial in critical care — essential elements and phased process were derived through a Delphi process: investigators "identified 18 potential steps" and found first-round consensus that "11 of these were essential elements"; during the second round, they "identified seven additional essential elements". The committee then "combined 4 related items into two steps (to reach 16 essential elements)" and "organized steps into four phases of care: consider, plan, support, and reassess". Additional steps "may be helpful in some cases, but not necessary for all trials", and involvement of other disciplines is "highly dependent on a patient's specific situation and on the available hospital resources".

---

### A framework for evaluating clinical artificial intelligence systems without ground-truth annotations [^f3b9d969]. Nature Communications (2024). High credibility.

Evaluate classifier

After training g ϕ, we evaluated it on a held-out set of data comprising data points with ground-truth labels (from both class 0 and class 1) (Fig. 1 b, Step 5). The intuition here is that a classifier which can successfully distinguish between these two classes, by performing well on the held-out set of data, is indicative that the training data and the corresponding ground-truth labels are relatively reliable. Since the data points from class 1 are known to be correct (due to our use of ground-truth labels), then a highly-performing classifier would suggest that the class 0 pseudo-labels of the remaining data points are likely to be correct. In short, this step quantifies how plausible it is that the sampled unlabelled data points belong to class 0.

As presented, this approach determines how plausible it is that the sampled set of data points in some probability interval, s ∈ (s 1, s 2], belongs to class 0. It is entirely possible, however, that a fraction of these sampled data points belong to the opposite class (e.g. class 1). We refer to this mixture of data points from each class as class contamination. We hypothesised (and indeed showed) that the degree of this class contamination increases as the probability output, s, by an AI system steers away from the extremes (s ≈ 0 and s ≈ 1). To quantify the extent of this contamination, however, we also had to determine how plausible it was that the sampled set of data points belong to class 1, as we outline next.

---

### Guidance for the Heart Rhythm Society pertaining to interactions with industry endorsed by the Heart Rhythm Society on April 26, 2011 [^8d7c1ab5]. Heart Rhythm (2011). Medium credibility.

Heart Rhythm Society research grant awards — through its peer review committees, HRS is the sole administrator of its research funding programs, and industry sponsors may not initiate or designate research projects to be reviewed by an HRS committee; HRS expects and encourages dissemination of results via annual progress reports, journal articles, and scientific meeting presentations; industry sponsors may not scrutinize, manage, direct, or control any research projects funded through HRS and are not entitled to intellectual property rights from such research; HRS is committed to transparency and discloses all industry sponsored research support through HRS's website.

---

### Is alpha asymmetry a byproduct or cause of spatial attention? New evidence alpha neurofeedback controls measures of spatial attention [^73e10066]. Neuron (2020). Medium credibility.

Cued spatial attention differentially modulates alpha power in attended relative to non-attended brain representations, termed the alpha asymmetry. Yet a causal role for alpha in attention is debated. In this issue of Neuron, Bagherzadeh et al., (2019) utilize neurofeedback to train alpha asymmetry and causally impact measures of spatial attention.

---

### Investigating suspected cancer clusters and responding to community concerns: guidelines from CDC and the council of state and territorial epidemiologists [^945666c0]. MMWR: Recommendations and Reports (2013). Medium credibility.

Cancer cluster investigation — Step 2 decision to close or proceed to Step 3 is based on multiple factors, and to interpret the standardized incidence ratio (SIR) the health agency must assess whether there are enough cases and population for statistical stability and, in general, that the population size of a typical census tract is the smallest denominator that will allow reliable results; if the numerator allows stability, agencies should consider whether the 95% CI exclude 1.0, along with environmental contaminants, population changes, and other information beyond the SIR to estimate whether cancers represent an excess and share a common etiology. A SIR of limited magnitude that is not statistically significant with lack of known contaminant association and no increasing trend justifies closing at Step 2, whereas a statistically significant SIR of great magnitude with an increasing trend and a known contaminant argues for continuing to Step 3; example thresholds indicate that an SIR of < 2.0 with CIs surrounding or overlapping 1.0 and < 10 cases might justify closing, while an SIR of > 4 with CIs not overlapping 1.0 and ≥ 10 etiologically linked cases should encourage advancing to Step 3.

---

### Hierarchical predictive information is channeled by asymmetric oscillatory activity [^58501fb8]. Neuron (2018). Low credibility.

Predictive coding and neural oscillations are two descriptive levels of brain functioning whose overlap is not yet understood. Chao et al. (2018) now show that hierarchical predictive coding is instantiated by asymmetric information channeling in the γ and α/β oscillatory ranges.

---

### Daily steps and all-cause mortality: a meta-analysis of 15 international cohorts [^f043ba51]. The Lancet: Public Health (2022). High credibility.

Background

Although 10000 steps per day is widely promoted to have health benefits, there is little evidence to support this recommendation. We aimed to determine the association between number of steps per day and stepping rate with all-cause mortality.

Methods

In this meta-analysis, we identified studies investigating the effect of daily step count on all-cause mortality in adults (aged ≥ 18 years), via a previously published systematic review and expert knowledge of the field. We asked participating study investigators to process their participant-level data following a standardised protocol. The primary outcome was all-cause mortality collected from death certificates and country registries. We analysed the dose-response association of steps per day and stepping rate with all-cause mortality. We did Cox proportional hazards regression analyses using study-specific quartiles of steps per day and calculated hazard ratios (HRs) with inverse-variance weighted random effects models.

Findings

We identified 15 studies, of which seven were published and eight were unpublished, with study start dates between 1999 and 2018. The total sample included 47471 adults, among whom there were 3013 deaths (10·1 per 1000 participant-years) over a median follow-up of 7·1 years ([IQR 4·3–9·9]; total sum of follow-up across studies was 297837 person-years). Quartile median steps per day were 3553 for quartile 1, 5801 for quartile 2, 7842 for quartile 3, and 10901 for quartile 4. Compared with the lowest quartile, the adjusted HR for all-cause mortality was 0·60 (95% CI 0·51–0·71) for quartile 2, 0·55 (0·49–0·62) for quartile 3, and 0·47 (0·39–0·57) for quartile 4. Restricted cubic splines showed progressively decreasing risk of mortality among adults aged 60 years and older with increasing number of steps per day until 6000–8000 steps per day and among adults younger than 60 years until 8000–10000 steps per day. Adjusting for number of steps per day, comparing quartile 1 with quartile 4, the association between higher stepping rates and mortality was attenuated but remained significant for a peak of 30 min (HR 0·67 [95% CI 0·56–0·83]) and a peak of 60 min (0·67 [0·50–0·90]), but not significant for time (min per day) spent walking at 40 steps per min or faster (1·12 [0·96–1·32]) and 100 steps per min or faster (0·86 [0·58–1·28]).

Interpretation

Taking more steps per day was associated with a progressively lower risk of all-cause mortality, up to a level that varied by age. The findings from this meta-analysis can be used to inform step guidelines for public health promotion of physical activity.

Funding

US Centers for Disease Control and Prevention.

---

### Standardization of spirometry 2019 update. An official American Thoracic Society and European Respiratory Society technical statement [^df365e99]. American Journal of Respiratory and Critical Care Medicine (2019). High credibility.

Back-extrapolated volume (BEV) on the volume–time curve — Time 0 is found by drawing a line with a slope equal to peak flow through the point of peak flow on the volume–time curve and setting Time 0 to the point where this line intersects the time axis. The BEV is equal to the volume of gas exhaled before Time 0, which, in these two examples from the same patient, is 0.136 L for the left panel (acceptable) and 0.248 L for the right panel (unacceptable). For this patient, the BEV limit is 5% FVC = 0.225 L.

---

### Guidance for the Heart Rhythm Society pertaining to interactions with industry endorsed by the Heart Rhythm Society on April 26, 2011 [^f1dedb5c]. Heart Rhythm (2011). Medium credibility.

Heart Rhythm Society financial relationship disclosure categories — greater transparency is required and the value of financial relationships is now categorized as $0; $1–$10,000; $10,001–$25,000; $25,001–$50,000; $50,001–$100,000; and greater than $100,000, whereas previously categories were none; modest ($1–$10,000); or significant (more than $10,000).

---

### Estimation of non-additive genetic variance in human complex traits from a large sample of unrelated individuals [^5a9b3a74]. American Journal of Human Genetics (2021). Medium credibility.

Non-additive genetic variance for complex traits is traditionally estimated from data on relatives. It is notoriously difficult to estimate without bias in non-laboratory species, including humans, because of possible confounding with environmental covariance among relatives. In principle, non-additive variance attributable to common DNA variants can be estimated from a random sample of unrelated individuals with genome-wide SNP data. Here, we jointly estimate the proportion of variance explained by additive (h SNP 2), dominance (δ SNP 2) and additive-by-additive (η SNP 2) genetic variance in a single analysis model. We first show by simulations that our model leads to unbiased estimates and provide a new theory to predict standard errors estimated using either least-squares or maximum likelihood. We then apply the model to 70 complex traits using 254,679 unrelated individuals from the UK Biobank and 1.1 M genotyped and imputed SNPs. We found strong evidence for additive variance (average across traits h¯ SNP 2 = 0.208). In contrast, the average estimate of δ¯ SNP 2 across traits was 0.001, implying negligible dominance variance at causal variants tagged by common SNPs. The average epistatic variance η¯ SNP 2 across the traits was 0.055, not significantly different from zero because of the large sampling variance. Our results provide new evidence that genetic variance for complex traits is predominantly additive and that sample sizes of many millions of unrelated individuals are needed to estimate epistatic variance with sufficient precision.

---

### Diagnosis and classification of diabetes: standards of care in diabetes – 2025 [^568cafd9]. Diabetes Care (2025). High credibility.

Type 1 diabetes natural history and progression risk indicate that stage 1, defined by the presence of two or more autoantibodies and normoglycemia, carries a 5-year risk of developing symptomatic type 1 diabetes of ~44%. In stage 2, risk is ~60% by 2 years and ~75% within 5 years of developing a clinical diagnosis of type 1 diabetes. A consensus group provides expert recommendations on what should be monitored and how often these factors should be monitored in individuals with presymptomatic type 1 diabetes.

---

### Quantifying population contact patterns in the United States during the COVID-19 pandemic [^6aaddb62]. Nature Communications (2021). High credibility.

Given μ i, we defineto be the expected number of contacts for respondent i. Then we model the reported number of contacts for respondent i, y i, aswhere ϕ ∈ [1, ∞) is a shape parameter that is inversely related to overdispersion; that is, the higher ϕ is estimated to be, the more similar y i 's distribution is to a Poisson distribution with rate parameter λ i.

In our data, observations from Wave 0 are censored above 10, because the survey instrument allowed respondents to report up to "10 or more" contacts. Waves 1 and up allowed respondents to enter any number of contacts, but in this analysis we top-coded contacts at 29, following previous studies of contact data. Reports that are topcoded or censored in any of the waves are treated as right-censored in the model. We adopt a Bayesian approach to fitting the model. For all of the regression coefficients β, we assume flat priors. For the intercept and the shape parameter, we assume very weak priors. Specifically, we assume a priori that the intercept α is distributed with mean 0 and a large variance by using pr(α) ~ Student- t (3, 0, 10); and we assume a priori that the shape parameter ϕ, is distributed with mean 1 and a very large variance by using pr(ϕ) ~ Gamma(0.01, 0.01). We did not collect data from Philadelphia in Wave 0, so the coefficient corresponding to Philadelphia in Wave 0 is constrained to be exactly 0 to allow estimation to proceed. Supplementary Table 1 shows summary statistics for the predictors used in our model.

---

### Technical update on HIV-1 / 2 differentiation assays [^50eb7933]. CDC (2016). Medium credibility.

HIV-1/HIV-2 differentiation testing — alternatives when the FDA-approved antibody differentiation immunoassay cannot be used states that there may be circumstances under which a laboratory is unable to adopt the FDA-approved HIV-1/HIV-2 antibody differentiation immunoassay, and in this situation a laboratory has alternatives for that step in the algorithm, some of which could delay turnaround time for test results. Send specimens to another laboratory that offers the FDA-approved supplemental HIV antibody differentiation assay. Refer to CDC/APHL laboratory testing guidance section I, "Alternative Testing Sequences When Tests in the Recommended Algorithm Cannot be Used".

---

### Three-dimensional wave breaking [^10c24ed3]. Nature (2024). Excellent credibility.

3D 'global steepness'

We now introduce a measure akin to the global steepness that accounts for the effects of directional spreading. For directionally spread (3D) linear waves, the surface slope has two orthogonal components:andThe directions in which waves propagate affect how they contribute to the total surface slope. When calculating S in equation (4), which corresponds to the maximum possible 2D linear slope (M = 1 and θ = 0), the phase argument in equation (5) is ignored. A similar approach may be used to calculate a 3D equivalent of S :For broadbanded spectra, the slope maxima of each component are not necessarily colocated in space. As a result, the 3D global slope S 3D can be quite different from the maximum achievable slope for a linear focused wave group. Thus, we did not use S 3D in 'Results'. To calculate the linear maximum achievable slope, we searched for the time and position at which the absolute slope was a maximum for a focused wave group based on each directional distribution. Extended Data Fig. 2 demonstrates how two focused wave groups with the same global steepness S can have different waveforms and local slopes (Extended Data Fig. 2b, c). The wave group in Extended Data Fig. 2a is unidirectional, whereas the wave group in Extended Data Fig. 2d is an axisymmetric standing wave.

Local steepness

Local measures of steepness, unlike the global steepness, implicitly capture the effects that nonlinear evolution and directionality may have on the waveform of steep waves. As a result, such measures may provide better descriptions of the free surface elevation at breaking onset for any given wave. The local steepness has various forms for which either a locally measured wave amplitude a or a height H (Extended Data Fig. 2) are non-dimensionalized using a chosen length scale, which is most commonly the wavelength λ (a k and k H /2, where k = 2π/ λ). The choice of length scale and the manner by which it is calculated (in space or time) have significant implications for the resulting measure of steepness. In highly spread conditions, even if highly resolved spatial measurements of surface elevation are available, the wavelength and other length scales become ill-defined. Even in 2D, where some of the aforementioned issues are resolved, the local steepness is not always a robust indicator of breaking onset. As a result, we have not used local steepness parameters herein.

---

### Clinical practice recommendations for the diagnosis and management of X-linked hypophosphataemia [^f243533b]. Nature Reviews: Nephrology (2025). High credibility.

Regarding diagnostic investigations for X-linked hypophosphatemia, more specifically with respect to laboratory tests, XLH-EG 2025 guidelines recommend to obtain a further work-up after diagnosis to investigate the presence and severity of common and rare complications of the disease.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^d6d8fcbc]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Spirometry reporting specifies that numerical values are given for the FEV1, the FVC, and the FEV1/FVC ratio; the latter should be reported as a decimal fraction and the space for percent predicted value left blank, and if bronchodilators are given the LLN column need not be repeated with absolute and percent change given only for FEV1 and FVC. Other numerical values such as the forced inspiratory flow at 75% of FVC (FEF75%) and FEF25–75% are not recommended for routine use. Graph requirements include that for the volume–time curve the volume scale should be at least 10 mm/L, the time scale at least 20 mm/s, and 1 second prior to the start of expiration should be displayed; on the flow–volume plot the flow display should be at least 5 l/min/L/s, and the ratio of flow to volume should be 2 L/s to 1 L, and linear and log scales where values are plotted as z-scores relative to the predicted value (z = 0) give an intuitive sense of severity.

---

### Cutting edge or blunt instrument: how to decide if a stepped wedge design is right for you [^471f52f2]. BMJ Quality & Safety (2021). High credibility.

When might I consider doing a stepped wedge trial?

Research designs are shaped as much by practical constraints as by abstract schemes, and it is always a good idea to start with the constraints and work towards a design, rather than start with a design and try to fit it to constraints. These constraints will be unique to each research context, and box 1 lists some areas to think about. Still, there are some common features of settings where a stepped wedge trial might be considered as a possible design, and we now review these.

Box 1
Practical constraints on the design of a longitudinal cluster randomised trial

Are there limits on the time available to complete the evaluation, on the number of clusters, or on the number of participants (or the rate at which you can recruit participants) at each cluster? These constraints put limits on the overall scale of the evaluation, or force trade-offs between different design characteristics.
How will participants and their data be sampled in your study: as a series of cross-sectional surveys, as a continuous stream of incident cases, as a cohort followed over time, or some other way? Does the timescale divide into cycles, seasons or milestones that influence how you will sample participants and data?
Is there a limit on how many clusters can implement the intervention at the same time in the evaluation? If this is constrained by research resources (eg, if there are only enough trained research staff to implement the intervention one cluster at a time) then implementation must be staggered in some way.
If implementation is to be staggered, is there a minimum 'step length'? If the same team delivers the intervention in different clusters at different steps, then bear in mind it may take some time to get the intervention fully operational at a site, and the team will also need time to relocate from one cluster to the next.

---

### Reconciling time and prediction error theories of associative learning [^48b97bba]. Nature Communications (2025). High credibility.

A Bayesian framework for estimating interval distributions

We present our generative model and discuss algorithms for inference. In this model, there are C possible stimuli generated by a point process. Our goal is to predict when the rewarding stimulus, indexed by r, will appear next, based on the times at which stimuli have appeared in the agent's history. We assume that the rewarding stimulus r has a single cause, which we index as c. This causal stimulus c can be any one of the C stimuli, including r itself. The interval between r and its cause c is drawn probabilistically from a distribution described further below. We denote ε i as the logarithm of the ratio between the prior probability that stimulus i causes r and the prior probability that stimulus r causes itself. Clearly, ε r = 0.

We denote the posterior probability that stimulus i is the causal stimulus as π i. Following our terminology in the main text, we call π i the association of stimulus i to the reward r. After observing data, π i is given according to Bayes' rule aswhereis the log-likelihood of observing the data given that i is the cause, relative to the log-likelihood of observing the data given that r is the cause.

The reward r appears after cause c with probability p, where p is drawn from a Beta prior B (p; a, b) with hyperparameters a and b. If r does appear after c, the interval t is drawn from a distribution ρ c. To sample the interval t ~ ρ c (t), we first sample an index μ from a Dirichlet-multinomial distribution. Specifically, μ (ranging from 1 to K) is drawn from a multinomial distribution with class probabilities q = (q 1, q 2,…, q K). The probabilities q are in turn drawn from a Dirichlet prior D (α), where α = (α 1, α 2,…, α K). Given the sampled index μ, we then draw t ~ ϕ μ (t), where ϕ μ (t) is a normalized probability density function defined below.

---

### CLINICAL PRACTICE GUIDELINE METHODOLOGY: methodology [^1ca4ad94]. Obstetrics and Gynecology (2021). High credibility.

ACOG clinical practice guideline development — topic selection, timeline, team composition, and grading indicate that each committee selects 2–3 new topics annually and the development process typically spans 24–36 months from initial topic selection to final publication. The process is led by a document writing team composed of committee members and subject matter experts, and Two committee members are selected to serve on the document writing team with the subject matter expert to develop the outline, synthesize evidence, propose recommendations, and author the manuscript. ACOG's guideline development process incorporates a modified GRADE framework, with recommendations classified by quality of evidence and strength of recommendation, and ungraded Good Practice Statements may be incorporated when evidence is extremely limited or nonexistent.

---

### Phase 0 / microdosing approaches: time for mainstream application in drug development? [^98d5620b]. Nature Reviews: Drug Discovery (2020). High credibility.

Phase 0 approaches - which include microdosing - evaluate subtherapeutic exposures of new drugs in first-in-human studies known as exploratory clinical trials. Recent progress extends phase 0 benefits beyond assessment of pharmacokinetics to include understanding of mechanism of action and pharmacodynamics. Phase 0 approaches have the potential to improve preclinical candidate selection and enable safer, cheaper, quicker and more informed developmental decisions. Here, we discuss phase 0 methods and applications, highlight their advantages over traditional strategies and address concerns related to extrapolation and developmental timelines. Although challenges remain, we propose that phase 0 approaches be at least considered for application in most drug development scenarios.

---

### Recommendations for noninvasive evaluation of native valvular regurgitation: a report from the American Society of Echocardiography developed in collaboration with the Society for Cardiovascular Magnetic Resonance [^f59ef754]. Journal of the American Society of Echocardiography (2017). Medium credibility.

Left ventricular volume calculation by short-axis summation — example and method show that sequential short-axis slices from the mitral annular plane (base) to the apex are summed using volume = area × thickness to compute LV end-diastolic volume (LVEDV) and LV end-systolic volume (LVESV); in the example, LV EDV = 41 mL + 41 mL + 38 mL + 34 mL + 28 mL + 19 mL + 5 mL = 206 mL and LV ESV = 0 mL + 30 mL + 27 mL + 24 mL + 18 mL + 3 mL + 0 mL = 102 ml; this methodology requires no geometric assumptions.

---

### Emergence of collective oscillations in massive human crowds [^de803956]. Nature (2025). Excellent credibility.

Simulation parameters

Without loss of generality, we can set γ = 1 and α = 1, which amounts to defining our units of length and time. The model has then six control parameters, which are β, γ p, k, η, σ and σ p. To integrate the equations of motion, we set δ t = 0.001, so that the damping coefficients in equations (1) and (2) verify k / γ ≳ 20 δ t and 1/ γ p ≳ 100 δ t for the typical values of γ p and k that we consider. This value of δ t also ensures that the angular frequency Ω ⋆ always remains in our simulation window. The number of steps n is set so that the system explores exhaustively its phase space. Finally, the number of steps n st = 1.5 × 10 5 is set so that the system reaches the limit cycle when β > β c.

The n run = 100 simulations are initialized on one of the limit cycles in the absence of noise when β > β c, namely, with φ drawn uniformly in the range, and, wherewith equal probability for the sign of Ω ⋆. Otherwise, the simulations are initialized from u = 0 and p = 0.

For Fig. 3a–c, we used k = 0.027, γ = 1.00, γ p = 18.00, β / β c = 1.10, η = 0.45, σ = 0.00 and σ p = 2.00. For Fig. 3f, we used γ = 1.00, γ p = 18.00, η = 0.45, σ = 2.00 and σ p = 2.00.

---

### Parallel excitation in the human brain at 9.4 T counteracting k-space errors with RF pulse design [^2e4a34c0]. Magnetic Resonance in Medicine (2010). Low credibility.

Multidimensional spatially selective radiofrequency (RF) pulses have been proposed as a method to mitigate transmit B1 inhomogeneity in MR experiments. These RF pulses, however, have been considered impractical for many years because they typically require very long RF pulse durations. The recent development of parallel excitation techniques makes it possible to design multidimensional RF pulses that are short enough for use in actual experiments. However, hardware and experimental imperfections can still severely alter the excitation patterns obtained with these accelerated pulses. In this note, we report at 9.4 T on a human eight-channel transmit system, substantial improvements in two-dimensional excitation pattern accuracy obtained when measuring k-space trajectories prior to parallel transmit RF pulse design (acceleration x4). Excitation patterns based on numerical simulations closely reproducing the experimental conditions were in good agreement with the experimental results.

---

### Assessing the predictive value of peak alpha frequency for the sensitivity to pain [^61a7397a]. Pain (2025). Medium credibility.

2.7.1. Statistical power

To obtain an estimate of statistical power for our primary analyses, we performed Bayes factor design analyses (BFDA)using the R package "BFDA"for n = 159 participants with default priors and 10,000 simulations. To address the statistical power of question 1, a BFDA for a Bayesian linear correlation was performed. For a negative correlation of medium size R = −0.3 and BF 10 boundaries of 1/3 and 3, BFDA revealed a statistical power of 93.6% with a false-positive rate of 0.9%. Using more conservative BF 10 boundaries of 1/10 and 10, statistical power was 85.8% with 0.2% false positives. To estimate statistical power for question 2, a BFDA was calculated for a Bayesian 1-sided 1-sample t test. For a medium effect size of Cohen d = 0.5 and BF 10 boundaries of 1/3 and 3, BFDA revealed a statistical power of 100% with a false-positive rate of 0.8%. For more conservative BF 10 boundaries of 1/10 and 10, statistical power remained at 100% with 0.2% false positives. Thus, the study was well-powered to detect medium effect sizes.

---

### X-linked hypophosphatemia management in adults: an international working group clinical practice guideline [^86938644]. The Journal of Clinical Endocrinology and Metabolism (2025). High credibility.

Regarding screening and diagnosis for X-linked hypophosphatemia, more specifically with respect to differential diagnosis, XLH-IWG 2025 guidelines recommend to obtain an evaluation of other possible causes of hypophosphatemia considering the differential.

---

### Alpha-1-antitrypsin deficiency targeted testing and augmentation therapy: a Canadian Thoracic Society meta-analysis and clinical practice guideline [^9f56ec6e]. Chest (2025). High credibility.

Regarding diagnostic investigations for alpha-1 antitrypsin deficiency, more specifically with respect to AAT levels, CTS 2025 guidelines recommend to consider obtaining initial measurement of serum α-1 antitrypsin levels and genetic testing in patients with high clinical suspicion of α-1 antitrypsin deficiency.

---

### Clinical practice recommendations for the diagnosis and management of X-linked hypophosphataemia [^bbe49141]. Nature Reviews: Nephrology (2025). High credibility.

Regarding screening and diagnosis for X-linked hypophosphatemia, more specifically with respect to diagnosis, XLH-EG 2025 guidelines recommend to evaluate any at-risk family member of a patient with X-linked hypophosphatemia for the condition, taking into account that sons and fathers of male patients affected by X-linked hypophosphatemia are not at risk.

---

### Colorectal cancer screening and prevention [^390b568a]. American Family Physician (2025). High credibility.

Regarding screening and diagnosis for colon cancer, more specifically with respect to indications for screening, high-risk individuals, family history, AAFP 2025 guidelines recommend to obtain CRC screening in patients with ≥ 1 first-degree relatives with CRC or adenomatous polyps, starting at 40 years of age or 10 years before the age of the youngest relative at the time of their diagnosis.

---

### 2022 AHA / ACC key data elements and definitions for cardiovascular and noncardiovascular complications of COVID-19: a report of the American college of cardiology / American Heart Association task force on clinical data standards [^fbac12da]. Journal of the American College of Cardiology (2022). High credibility.

COVID-19 symptom status — this data element captures the "Presence or absence of symptoms attributable to acute COVID-19", with permissible values "Symptomatic", "Asymptomatic", and "Unknown", and the note "See list of possible symptoms in Appendix 6".

---

### Standards of care in diabetes – 2025 [^31be65a6]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for hypoglycemia, more specifically with respect to screening for cognitive impairment, ADA 2025 guidelines recommend to simplify diabetes treatment plans as much as possible for patients with cognitive impairment and tailor to minimize the risk of hypoglycemia.

---

### Computational methods for the estimation of ideal current patterns in realistic human models [^2f5b4b1d]. Magnetic Resonance in Medicine (2024). Medium credibility.

Purpose

To introduce a method for the estimation of the ideal current patterns (ICP) that yield optimal signal-to-noise ratio (SNR) for realistic heterogeneous tissue models in MRI.

Theory and Methods

The ICP were calculated for different surfaces that resembled typical radiofrequency (RF) coil formers. We constructed numerical electromagnetic (EM) bases to accurately represent EM fields generated by RF current sources located on the current-bearing surfaces. Using these fields as excitations, we solved the volume integral equation and computed the EM fields in the sample. The fields were appropriately weighted to calculate the optimal SNR and the corresponding ICP. We demonstrated how to qualitatively use ICP to guide the design of a coil array to maximize SNR inside a head model.

Results

In agreement with previous analytic work, ICP formed large distributed loops for voxels in the middle of the sample and alternated between a single loop and a figure-eight shape for a voxel 3-cm deep in the sample's cortex. For the latter voxel, a surface quadrature loop array inspired by the shape of the ICP reached < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:mn > 87 < /mml:mn > < mml:mo >. < /mml:mo > < mml:mn > 5 < /mml:mn > < mml:mo > % < /mml:mo > < /mml:mrow > < mml:annotation > $$ 87.5\% $$ < /mml:annotation > < /mml:semantics > < /mml:math > of the optimal SNR at 3T, whereas a single loop placed above the voxel reached only < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:mn > 55 < /mml:mn > < mml:mo >. < /mml:mo > < mml:mn > 7 < /mml:mn > < mml:mo > % < /mml:mo > < /mml:mrow > < mml:annotation > $$ 55.7\% $$ < /mml:annotation > < /mml:semantics > < /mml:math > of the optimal SNR. At 7T, the performance of the two designs decreased to < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:mn > 79 < /mml:mn > < mml:mo >. < /mml:mo > < mml:mn > 7 < /mml:mn > < mml:mo > % < /mml:mo > < /mml:mrow > < mml:annotation > $$ 79.7\% $$ < /mml:annotation > < /mml:semantics > < /mml:math > and < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:mn > 49 < /mml:mn > < mml:mo >. < /mml:mo > < mml:mn > 8 < /mml:mn > < mml:mo > % < /mml:mo > < /mml:mrow > < mml:annotation > $$ 49.8\% $$ < /mml:annotation > < /mml:semantics > < /mml:math >, respectively, suggesting that loops could be suboptimal at ultra-high field MRI.

Conclusion

ICP can be calculated for human tissue models, potentially guiding the design of application-specific RF coil arrays.

---

### Initial conditions combine with sensory evidence to induce decision-related dynamics in premotor cortex [^12099a34]. Nature Communications (2023). High credibility.

Whereis the mean RT, R T k is the RT for the k t h trial, andis the RT predicted for the k t h trial.

For assessing if the R 2 values were significant, we computed a shuffled distribution (500 shuffles) where we shuffled the trials to remove the relationship between the RTs and spiking activity. We then assessed if the per bin R 2 values were significantly different from the 99th percentile of the shuffled distribution R 2 values.

Logistic Regression to decode choice: For decoding choice and previous outcome on a bin-by-bin basis, we used a regularized logistic regression approach. Decoders were trained with an equal number of trials for the opposing outcomes (i.e. left vs. right reaches; previous correct vs. previous error trials). The logistic regression approach assumes that the log odds in favor of one event (e.g. left) vs. right reach is given by the following equations: β 0 is the intercept of the model, β j is the model coefficient for the j t h neuron in the current bin, X j is the spiking activity of the j t h neuron of the current bin. The following equation is used to produce the outputs of the system: if p (Left∣ X) < 0.5 then – 1 and if p (Left∣ X) > 0.5 then 1.

---

### Step-by-step state-selective tracking of fragmentation dynamics of water dications by momentum imaging [^9ec320e6]. Nature Communications (2022). High credibility.

The double photoionization of a molecule by one photon ejects two electrons and typically creates an unstable dication. Observing the subsequent fragmentation products in coincidence can reveal a surprisingly detailed picture of the dynamics. Determining the time evolution and quantum mechanical states involved leads to deeper understanding of molecular dynamics. Here in a combined experimental and theoretical study, we unambiguously separate the sequential breakup via D + +OD + intermediates, from other processes leading to the same D + +D + +O final products of double ionization of water by a single photon. Moreover, we experimentally identify, separate, and follow step by step, two pathways involving the b 1 Σ + and a 1 Δ electronic states of the intermediate OD + ion. Our classical trajectory calculations on the relevant potential energy surfaces reproduce well the measured data and, combined with the experiment, enable the determination of the internal energy and angular momentum distribution of the OD + intermediate.

---

### 2022 AHA / ACC key data elements and definitions for cardiovascular and noncardiovascular complications of COVID-19: a report of the American college of cardiology / American Heart Association task force on clinical data standards [^b97f729b]. Journal of the American College of Cardiology (2022). High credibility.

COVID-19 data standards — SARS-CoV-2 variant captures the variant lineage of SARS-CoV-2 and provides permissible values for variant/lineage reporting, including Alpha (B.1.1.7 and Q lineages), Beta (B.1.351 and descendent lineages), Delta (B.1.617.2 and AY lineages), Gamma (P.1 and descendent lineages), Epsilon (B.1.427 and B.1.429), Eta (B.1.525), Iota (B.1.526), Kappa (B.1.617.1), B.1.617.3, Mu (B.1.621, B.1.621.1), Zeta (P.2), Omicron (B.1.1.529 and BA lineages), and the options Other, specify and Unknown, mapped to Centers for Disease Control and Prevention variant classifications and definitions.

---

### Standards of care in diabetes – 2025 [^cb4a5970]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for diabetes mellitus type 1, more specifically with respect to general principles, ADA 2025 guidelines recommend to obtain a complete medical evaluation at the initial visit and follow-up, as appropriate, to:

- confirm the diagnosis and classify diabetes

- ssess glycemic status and previous treatment

- evaluate for diabetes complications, potential comorbid conditions, and overall health status

- identify care partners and support system

- assess social determinants of health and structural barriers to optimal health and health care

- review risk factor management in the patient with diabetes

- begin engagement with the patient with diabetes in the formulation of a care management plan including initial goals of care

- develop a plan for continuing care.

---

### X-linked hypophosphatemia management in adults: an international working group clinical practice guideline [^3685d9cc]. The Journal of Clinical Endocrinology and Metabolism (2025). High credibility.

Regarding screening and diagnosis for X-linked hypophosphatemia, more specifically with respect to diagnosis, XLH-IWG 2025 guidelines recommend to diagnose X-linked hypophosphatemia in the presence of chronic hypophosphatemia when other conditions causing renal phosphate wasting are absent. Support the diagnosis by an X-linked inheritance pattern.

---

### Colorectal cancer screening and prevention [^c270f773]. American Family Physician (2025). High credibility.

Regarding screening and diagnosis for colon cancer, more specifically with respect to indications for screening, general population, aged 76–85 years, AAFP 2025 guidelines recommend to consider obtaining screening for CRC in adults aged 76–85 years at average risk based on overall health status, prior screening history, and patient preferences.

---

### Distributed spirals: a new class of three-dimensional k-space trajectories [^5f354c75]. Magnetic Resonance in Medicine (2013). Low credibility.

This work presents a new class of three-dimensional spiral based-trajectories for sampling magnetic resonance data. The distributed spirals trajectory efficiently traverses a cylinder or sphere or intermediate shape in k-space. The trajectory is shown to be nearly as efficient as a conventional stack of spirals trajectory in terms of scan time and signal-to-noise ratio, while reducing coherent aliasing in all three spatial directions and reducing Gibbs ringing due to the nature of collecting data from a sphere in k-space. The trajectory uses a single two-dimensional spiral waveform with the addition of a single orthogonal waveform which is scaled with each repetition, making it relatively easy to implement. Blurring from off-resonance only occurs in two dimensions due to the temporal nature of the sampling.

---

### Diagnosis, management and treatment of the Alport syndrome-2024 guideline on behalf of ERKNet, ERA and ESPN [^45eede36]. Nephrology, Dialysis, Transplantation (2025). High credibility.

Regarding diagnostic investigations for Alport syndrome, more specifically with respect to hearing assessment, ERA/ERN ERKNet/ESPN 2025 guidelines recommend to consider obtaining a hearing evaluation at diagnosis or upon reaching adulthood in female patients with X-linked alport syndrome, and then every 5 years in the absence of hearing loss symptoms.

---

### Trend algorithms… [^b736a419]. seer.cancer.gov (2025). Medium credibility.

\}{Initial \textbf{ } Rate} }\right) \times {\Large10 }\). \ } \) \ } \). Annual Percent Change The Annual Percent Change is calculated by fitting a least squares regression line to the natural logarithm of the rates, using the calendar year as a regressor variable. n = number of years r = rates y = Ln x = calendar year.

y = mx + b \ \) Because the methods used in the calculation of PC and APC are not directly related, it is possible that the signs of the PC and the APC may disagree. See Calculating APC with Weighting for the formulas used in calculating Weighted APCs. Confidence Intervals for APC The endpoints of a × 100% confidence interval are calculated as: \\right)}
- 1 }\right) \times {\Large100 }\). \\right)}
- 1 }\right) \times {\Large100 }\) Where \\) is the inverse of the.

---

### ASTCT consensus recommendations on testing and treatment of patients with donor-specific anti-HLA antibodies [^f8366cca]. Transplantation and Cellular Therapy (2024). High credibility.

Regarding therapeutic procedures for aplastic anemia, more specifically with respect to hemopoietic stem cell transplantation (management of donor-specific anti-HLA antibodies, testing), ASTCT 2024 guidelines recommend to recognize that solid-phase immunoassays have limited capability to detect very high levels of donor-specific antibodies due to interferences with the binding site of fluorescence-conjugated antibodies on HLA antibodies. Consider diluting and/or adding EDTA to the recipient serum to reverse this falsely lower anti-HLA antibody detection/identification.

---

### Norvir [^cd1699c3]. FDA (2020). Medium credibility.

0.25 0.8 mL (62.5 mg) 0.9 mL (75 mg) 1.1 mL (87.5 mg) 1.25 mL (100 mg)

0.50 1.6 mL (125 mg) 1.9 mL (150 mg) 2.2 mL (175 mg) 2.5 mL (200 mg)

0.75 2.3 mL (187.5 mg) 2.8 mL (225 mg) 3.3 mL (262.5 mg) 3.75 mL (300 mg)

1.00 3.1 mL (250 mg) 3.75 mL (300 mg) 4.4 mL (350 mg) 5 mL (400 mg)

1.25 3.9 mL (312.5 mg) 4.7 mL (375 mg) 5.5 mL (437.5 mg) 6.25 mL (500 mg)

1.50 4.7 mL (375 mg) 5.6 mL (450 mg) 6.6 mL (525 mg) 7.5 mL (600 mg)

Body surface area (BSA) can be calculated as follows1:

[norvir equation]

---

### Correlated electrons in delta-plutonium within a dynamical mean-field picture [^e42fbc2a]. Nature (2001). Excellent credibility.

Given the practical importance of metallic plutonium, there is considerable interest in understanding its fundamental properties. Plutonium undergoes a 25 per cent increase in volume when transformed from its alpha-phase (which is stable below 400 K) to the delta-phase (stable at around 600 K), an effect that is crucial for issues of long-term storage and disposal. It has long been suspected that this unique property is a consequence of the special location of plutonium in the periodic table, on the border between the light and heavy actinides-here, electron wave-particle duality (or itinerant versus localized behaviour) is important. This situation has resisted previous theoretical treatment. Here we report an electronic structure method, based on dynamical mean-field theory, that enables interpolation between the band-like and atomic-like behaviour of the electron. Our approach enables us to study the phase diagram of plutonium, by providing access to the energetics and one-electron spectra of strongly correlated systems. We explain the origin of the volume expansion between the alpha- and delta-phases, predict the existence of a strong quasiparticle peak near the Fermi level and give a new viewpoint on the physics of plutonium, in which the alpha- and delta-phases are on opposite sides of the interaction-driven localization-delocalization transition.

---

### Linear mapping approximation of gene regulatory networks with stochastic dynamics [^03602e5e]. Nature Communications (2018). Medium credibility.

Fig. 6
LMA for the feedback loop with explicit modeling of both mRNA transcription and protein translation. a Illustrates the nonlinear feedback loop. b Shows the LMA and SSA predictions for steady-state mRNA distribution (See Supplementary Note 7), showing that the LMA is able to capture the change in modality induced by a change in ρ u. The rest of the parameters are: ρ b = 25, σ b = 0.004, σ u = 0.25, ρ = 3, γ = 2 and d = 1. c Compares LMA and SSA predictions for the first to fourth moments (about zero) of both mRNA and protein numbers over a wide range of the mRNA-protein degradation ratio γ / d. The result indicates that the accuracy of the LMA is independent of time-scale separation assumptions. The parameters used are: ρ u = 5, ρ b = 0, ρ = 100, σ u = 1, σ b = 1.5 × 10 −5. The nine pairs of (γ, d) are: (0.02, 2), (0.02, 0.6), (0.05, 0.5), (0.05, 0.15), (0.2, 0.2), (0.15, 0.05), (0.2, 0.02), (0.15, 0.005) and (0.2, 0.002), constituting a logarithmic span from −2 to 2. The ratio γ / d varies over a range consistent with mammalian gene expression

---

### Standards of care in diabetes – 2025 [^5eb43e21]. Diabetes Care (2025). High credibility.

Regarding follow-up and surveillance for diabetes mellitus type 1, more specifically with respect to continuous glucose monitoring, ADA 2025 guidelines recommend to implement early continuous glucose monitoring in adult patients with T1DM to improve glycemic outcomes and QoL and minimize hypoglycemia.

---

### X-linked hypophosphatemia management in adults: an international working group clinical practice guideline [^a0150f85]. The Journal of Clinical Endocrinology and Metabolism (2025). High credibility.

Regarding diagnostic investigations for X-linked hypophosphatemia, more specifically with respect to laboratory tests, XLH-IWG 2025 guidelines recommend to consider obtaining the following tests in the initial assessment for X-linked hypophosphatemia:

- serum phosphorus

- serum calcium corrected for albumin

- ALP

- 25-hydroxyvitamin D

- kidney function (creatinine, eGFR)

- tubular maximum for phosphate corrected for GFR

- PTH

- 24-hour urine calcium and creatinine (or spot urine calcium and creatinine)

- 24-hour urine phosphorus and creatinine.

---

### Hourly step recommendations to achieve daily goals for working and older adults [^588d8957]. Communications Medicine (2024). Medium credibility.

Figure 3 shows the predicted mean step counts and 95% CI for the rest of the day, given the step counts accumulated from 6 p.m. to 11 p.m. adjusting for age group, BMI group and sex. Overall, there was little difference in the conditional predicted mean step counts for the rest of the days between the weekdays and the weekends. On weekdays, participants who accumulated 6000 steps by 6 p.m. had a 0.692 probability of achieving 10,000 steps (all the goals) by the end of the day (refer to the Goal 3 diagonal line in Fig. 3, Table 3, and Supplementary Table S2). Similarly, participants who had accumulated 7000 steps by 7 p.m. 8000 steps by 8 p.m. 8500 steps by 9 p.m. and 9500 steps by 10 p.m. achieved 10,000 mean steps by the end of the day with a probability of 0.910, 0.982, 0.700, and 1, respectively. On weekends, participants who had accumulated 6500 steps by 6 p.m. 7000 steps by 7 p.m. 8000 steps by 8 p.m. 8500 steps by 9 p.m. and 9500 steps by 10 p.m. achieved 10,000 mean steps by the end of the day with probability 0.852, 0.583, 0.947, 0.717 and 1, respectively. The step counts required to achieve 5000 and 7500 steps by the end of the day can be found in Supplementary Tables S3 and S4, respectively.

---

### Clinical practice recommendations for the diagnosis and management of X-linked hypophosphataemia [^3fd0d663]. Nature Reviews: Nephrology (2025). High credibility.

Regarding diagnostic investigations for X-linked hypophosphatemia, more specifically with respect to genetic testing, XLH-EG 2025 guidelines recommend to consider using a family history of X-linked inheritance and non-suppressed plasma levels of intact fibroblast growth factor 23 in association with hypophosphatemia to support the diagnosis of X-linked hypophosphatemia if molecular genetic analysis is not available.

---

### Arrays of individually controlled ions suitable for two-dimensional quantum simulations [^ee7134ad]. Nature Communications (2016). Medium credibility.

Results

Trap arrays and control potentials

Our surface ion trap chip is fabricated in similar manner to that described in ref.and consists of two equilateral triangular trap arrays with side length of ≃40 and ≃80 μm, respectively (Fig. 1a, b), both with a distance of ≃40 μm between the ions and the nearest electrode surface. The shapes of radio-frequency (RF) electrodes of the arrays are optimized by a linear-programming algorithm that yields electrode shapes with low fragmentation, and requires only a single RF-voltage source for operation. To design different and even non-periodic arrays for dedicated trap distances, we can apply the same algorithm to yield globally optimal electrode shapes. Resulting electrode shapes may look significantly different, but will have comparable complexity, spatial extent and the same number of control electrodes per trap site. Therefore, we expect that different arrays will not require different fabrication techniques (Methods). The two arrays are spaced by ≃5 mm on the chip, and only one of them is operated at a given time. Although we achieve similar results in both arrays, the following discussion is focussed on the 80 μm array.

Three-dimensional confinement of 25 Mg + ions is provided by a potential φ RF oscillating at Ω RF from a single RF electrode driven at Ω RF /(2 π) = 48.3 MHz with an approximate peak voltage U RF = 20 V. Setting the origin of the coordinate system at the centre of the array and in the surface plane of the chip, the RF potential features three distinct trap sites at T0 ≃(−46,0,37) μm,… Owing to the electrode symmetry under rotations of ± 2 π /3 around the z -axis, it is often sufficient to consider T0 only, as all our findings apply to T1 and T2 after an appropriate rotation. Further, the RF potential exhibits another trap site at ≃(0,0,81) μm (above the centre of the array); this 'ancillary' trap is used for loading as well as for re-capturing ions that escaped from the other trap sites. We approximate the RF confinement at position r by a pseudopotential, cp. ref. where Q denotes the charge and m the mass of the ion, and E RF (r) is the field amplitude produced by the electrode. Calculations of trapping potentials are based on ref.and utilizing the software package. Equipotential lines of φ ps are shown in Fig. 1c–e.

---

### Diagnosis, management and treatment of the Alport syndrome-2024 guideline on behalf of ERKNet, ERA and ESPN [^c59a118f]. Nephrology, Dialysis, Transplantation (2025). High credibility.

Regarding diagnostic procedures for Alport syndrome, more specifically with respect to kidney biopsy, ERA/ERN ERKNet/ESPN 2025 guidelines recommend to avoid performing a kidney biopsy in patients with a genetic diagnosis of X-linked Alport syndrome or autosomal recessive Alport syndrome.

---

### Long-term single-cell imaging and simulations of microtubules reveal principles behind wall patterning during proto-xylem development [^286dc2ed]. Nature Communications (2021). High credibility.

Fig. 4
Fidelity and speed of microtubule band formation depends on KATANIN.

a – c Maximum projection of dual-labelled mCh-TUA5 and GFP-KTN seedlings in non-induced cells (b) and during PX formation (b, c). KTN1-GFP localizes majorly to forming microtubule bands during early (b) and mid (b) stages of PX formation. Scale bars = 10 µm. d Maximum projections of mCh-TUA5 VND7 during PX formation in wild-type (left) and in ktn1-2 (right). Arrowheads in c and d point toward regions with incomplete band separation. Scale bar = 5 µm. e Fraction of cells classified into early, mid and late stages of PX formation for induced mCh-TUA5 VND7 (left) and ktn1-2 VND7 (right). Dots depict fractions of early and mid-stages of nine seedlings for each genotype. f Relative progression of VND7 (left) and ktn1-2 VND7 (right). Microtubule arrays in VND7 are significantly more progressed (2.1 ± 0.7, mean ± SD, 174 cells from 9 seedlings) compared to the ktn1-2 VND7 (1.7 ± 0.6, 127, 9, p = 0.0042, Welch's unpaired, two-sided t -test). Dots depict VND7 progression for nine seedlings for each genotype. g Maximum projection of stained secondary walls of induced wild type (left) and ktn1-2 (right) 48 h after induction. Scale bar = 10 µm. h – k Basic Fuchsin-stained wild-type (h) and ktn1-2 (i-j) xylem files in 6-day-old roots. In ktn1-2, an altered arrangement of PX (i) and MX (j) vessels is observed, PX vessel thicknesses vary (compare vertical lines in h and i) and cell wall bands are wider and more disordered (k, right) as compared to wild type (k, left). Band shapes are indicated by yellow lines. Scale bars = 10 µm. l Band separation is fastest for initially co-aligned transverse arrays (cyan and blue) and slowest for initially longitudinal arrays (red). Simulations assuming default parallel nucleation and no KTN activity (r x = 0 s −1). Lines and margins represent medians ± 16% and 84% percentiles.

---

### Standards of care in diabetes – 2025 [^9fec4de9]. Diabetes Care (2025). High credibility.

Regarding follow-up and surveillance for diabetes mellitus type 1, more specifically with respect to continuous glucose monitoring, ADA 2025 guidelines recommend to assess skin reactions, either due to irritation or allergy, and address to aid in successful use of devices.

---

### X-linked hypophosphatemia management in adults: an international working group clinical practice guideline [^7fc1b864]. The Journal of Clinical Endocrinology and Metabolism (2025). High credibility.

Regarding medical management for X-linked hypophosphatemia, more specifically with respect to management of tertiary hyperparathyroidism, surgery, XLH-IWG 2025 guidelines recommend to consider performing a subtotal parathyroidectomy if hyperparathyroidism and hypercalcemia remain uncontrolled.

---

### Hourly step recommendations to achieve daily goals for working and older adults [^9509c890]. Communications Medicine (2024). Medium credibility.

Table 3
Step counts (based on increments of 500 steps) to accumulate in the evenings to achieve 10,000 steps by the end of the day

Details on how these recommendations were derived are provided in the Supplementary Method.

*Same as the "Overall" group.

The step counts required in the evenings to achieve 10,000 steps among participants aged 17–59 were similar but were generally more than those required for participants aged 60 and above (Table 3). There were also differences in the step counts required in the evenings to achieve 10,000 steps among participants with BMI less than 27.5 kg/m 2 on weekdays.

Across different demographics, participants aged between 30 and 39 who had accumulated fewer than 7500 steps between 6 p.m. and 10 p.m. were associated with lower mean step counts for the rest of the day compared to participants aged 60 and above (Supplementary Fig. S6). Males who had accumulated fewer than 5000 steps between 6 p.m. and 10 p.m. were associated with lower mean step counts for the rest of the day compared to females on weekdays, and no sex differences among those who accumulated 5000 steps or more. Participants with normal BMI were associated with lower mean step counts for the rest of the day on weekdays, but they could also have higher mean step counts on weekends compared to participants with obese BMI. The mean step difference between participants with normal BMI and participants with obese BMI decreased with time, with little difference at 10 p.m.

---

### Primary open-angle glaucoma preferred practice pattern ® [^424d245d]. Ophthalmology (2021). High credibility.

Primary open-angle glaucoma — ICD-10 CM classifications and coding rules include the following: open-angle glaucoma is coded as H40.10X-; primary open-angle glaucoma uses H40.111-, H40.112-, or H40.113-; low-tension glaucoma uses H40.121-, H40.122-, or H40.123-; residual stage of open-angle glaucoma uses H40.151, H40.152, or H40.153; and glaucomatous atrophy of the optic disc uses H47.231, H47.232, or H47.233. The footnote defines " = 0, stage unspecified; 1, mild stage; 2, moderate stage; 3, severe stage; 4, indeterminate stage". Additional ICD-10 CM instructions state that certain categories require a "7th character" and, if needed, "a placeholder X must be used" to ensure it is in the seventh position, that for bilateral sites "the final character of the codes in the ICD-10 CM indicates laterality" and if no bilateral code exists "separate codes for both the left and right side should be assigned", and that laterality mapping is standardized as "Right is always 1", "Left is always 2", and "Bilateral is always 3".

---

### The European guideline on management of major bleeding and coagulopathy following trauma: sixth edition [^70b9f807]. Critical Care (2023). High credibility.

Regarding medical management for traumatic hemorrhage, more specifically with respect to management of antithrombotics (factor Xa inhibitors), ABC-T 2023 guidelines recommend to consider calibrating the measurement of anti-Xa activity for the specific agent. Consider using LMWH-calibrated anti-Xa assays as a reliable alternative.