# Medical Question & Answer

**Sample ID**: 67dfbcbd-3e13-3e46-bd2a-499c1a25fe1b
**Dataset Index**: 177527

---

## Question

4. A sequence of integers a₁, a₂, a₃, … is defined by
a₁ = k,
aₙ₊₁ = aₙ + 8n for all integers n ≥ 1.

Find all values of k such that every term in the sequence is a square.

---

## Answer

> Let's see… What do we have here? The user is asking which starting value (k) makes every term of the sequence defined by (a_1 = k) and (a_{n+1} = a_n + 8n) a perfect square. Let's break this down step-by-step. First, I need to think about deriving a closed-form expression for (a_n). Then, I should verify the algebraic identity that lets me rewrite the expression as a square. Next, I will examine the condition that forces the expression to be a perfect square for all (n), and I need to check whether that condition can be satisfied. Finally, I will test small values of (k) to confirm the pattern and state the conclusion clearly.

> Let me first confirm the recurrence and derive the general term. The recurrence (a_{n+1} = a_n + 8n) with (a_1 = k) is a nonhomogeneous linear recurrence, and I can unroll it to get (a_n = k + 8 \\sum_{i = 1}^{n-1} i = k + 8 \\cdot \\frac{(n-1)n}{2} = k + 4n(n-1)), so (a_n = 4n^2–4n + k). Wait, let me verify the arithmetic: the sum of the first (n-1) integers is (\\frac{(n-1)n}{2}), and multiplying by 8 gives (4n(n-1)), so yes, (a_n = 4n^2–4n + k) is correct [^notfound].

> Next, I should review whether this quadratic can be rewritten as a square. I will try to complete the square: (4n^2–4n + k = (2n - 1)^2 + (k - 1)). Hold on, I should verify that expansion: ((2n - 1)^2 = 4n^2–4n + 1), so (4n^2–4n + k = (2n - 1)^2 + (k - 1)), which confirms the identity. This means (a_n) is a perfect square if and only if ((2n - 1)^2 + (k - 1)) is a perfect square for all (n \\geq 1) [^notfound].

> Now, I need to ensure the condition is interpreted correctly. For (a_n) to be a square for every (n), the expression ((2n - 1)^2 + (k - 1)) must itself be a perfect square for every positive integer (n). Let me think about what that implies: for each (n), there must exist an integer (m) such that (m^2 = (2n - 1)^2 + (k - 1)), which rearranges to (m^2 - (2n - 1)^2 = k - 1). This is a difference of squares, so ((m - (2n - 1))(m + (2n - 1)) = k - 1) [^notfound].

> I will now examine whether this equation can hold for all (n). The left-hand side is a product of two integers whose difference is (2(2n - 1)), which grows without bound as (n) increases. But the right-hand side, (k - 1), is a fixed constant. Hmm, wait a minute, this is a contradiction: a growing product cannot equal a constant for all (n). Therefore, the only way this can work is if the constant (k - 1) is zero, forcing both factors to be zero and making the equation trivially satisfied for all (n) [^notfound].

> Let me verify the implication. If (k - 1 = 0), then (k = 1), and the expression becomes (a_n = (2n - 1)^2), which is indeed a perfect square for every (n). I should double-check a few terms: for (n = 1), (a_1 = 1 = 1^2); for (n = 2), (a_2 = 1 + 8 = 9 = 3^2); for (n = 3), (a_3 = 9 + 16 = 25 = 5^2); for (n = 4), (a_4 = 25 + 24 = 49 = 7^2). The pattern of odd squares holds, confirming that (k = 1) works [^notfound].

> But wait, what if I assumed (k - 1 \\neq 0)? Could there still be a solution for all (n)? I should confirm that no other (k) can satisfy the condition. If (k - 1 \\neq 0), then for sufficiently large (n), the factor (m + (2n - 1)) will exceed (|k - 1|), and since the product equals (k - 1), the other factor (m - (2n - 1)) must be a nonzero integer with magnitude less than 1, which is impossible. Hence, the only viable case is (k - 1 = 0), i.e., (k = 1) [^notfound].

> Conclusion: I need to ensure the final statement is precise. The only value of (k) for which every term (a_n) is a perfect square is (k = 1), in which case (a_n = (2n - 1)^2) for all (n \\geq 1) [^notfound].

---

The only value of (k) for which every term in the sequence is a perfect square is **(k = 1)**. This is because the closed-form expression for (aₙ) is (aₙ = 4n²–4n + k), and for (aₙ) to be a perfect square for all (n), (k) must be 1, making (aₙ = (2n − 1)²).

---

## Derivation of the general term

Given the recurrence relation:

aₙ₊₁ = aₙ + 8n with a₁ = k,

we can derive the general term (a_n) by telescoping:

aₙ = a₁ + ∑ from i=1 to n−1 8i = k + 8 × [(n−1)n/2] = k + 4n(n−1) = 4n²–4n + k.

Thus, the general term is:

aₙ = 4n²–4n + k.

---

## Condition for (a_n) to be a perfect square

For (aₙ) to be a perfect square for all (n), the expression (4n²–4n + k) must be a perfect square for all (n). We can rewrite this as:

aₙ = (2n − 1)² + (k − 1).

For (aₙ) to be a perfect square for all (n), the term ((k − 1)) must be zero, because otherwise ((2n − 1)² + (k − 1)) would not be a perfect square for all (n). Therefore, we must have:

k − 1 = 0 ⇒ k = 1.

---

## Verification

If (k = 1), then:

aₙ = 4n²–4n + 1 = (2n − 1)²,

which is a perfect square for all (n). For example:

- (a₁ = (2 × 1−1)² = 1² = 1),
- (a₂ = (2 × 2−1)² = 3² = 9),
- (a₃ = (2 × 3−1)² = 5² = 25),

and so on, confirming that every term is a perfect square when (k = 1).

---

## Conclusion

The only value of (k) for which every term in the sequence is a perfect square is **(k = 1)**.

---

## References

### Clustering knowledge and dispersing abilities enhances collective problem solving in a network [^60e1f0f9]. Nature Communications (2019). High credibility.

The NK problem space has been a standard formalization used to model complex problems – (We explain many of the design and analytic decisions in the "Methods" section, but offer a conceptual overview here.) An NK space is an N -dimensional space, which are each associated with one of several potential scores. What determines the exact score for any particular point in this space depends on a fixed tuning parameter K that sets up the NK space as either more rugged (higher values of K) or smoother (lower values of K). In an NK space with a low value of K, agents with different search strategies (i.e. say some search strategy A and B) can readily find the best solution (global optimum) with ease. However, in an NK space with a high value of K, agents meander about local optima that are less profitable in terms of score than the easily achievable low- K global optimum space. So much so, agents with narrow search capabilities can get stuck in local valleys. Figure 1 offers a stylized characterization of a comparison of two NK spaces, one with a low K, and one with a high K.

Fig. 1
NK space. Hypothetical plot of two NK problem spaces with varying levels of K

Every point in an NK space is defined by an address, represented by a sequence of length N of 1s and 0s. Agents in a simulation are given this address and manipulate it in various ways iteratively and myopically (e.g. changing one of the random 1s in the address to a 0 or vice-versa, for instance) to jump to a new point in the space that might have higher score. NK spaces that are very simple allow agents to quickly jump to points with higher scores, and those that are very complex are typically very difficult for agents for find paths that lead to the global optimum. Hence, tuning K allows us to create a challenging but searchable space to explore. The scores for each NK space are normalized such that the global optimum is assigned a score of 1 and the worst possible solution is assigned a score of 0.

---

### Shadow estimation of gate-set properties from random sequences [^e8ac337a]. Nature Communications (2023). High credibility.

Dynamical shadow norm

The versatility of Clifford UIRS in practice of course crucially depends on the sample efficiency of the estimation. From the above, it is not clear that k A (m) can be efficiently estimated for arbitrary A. Demanding that k A (1) = 1 in the limit of perfect state preparation, measurement, and gates, the normalization factor α in Eq. (2) is α = 2 n + 1, leading to a single-shot estimator taking values exponentially large in n. Building upon the machinery of the dynamic shadow norm and Theorem 1, we can still provide guarantees for efficiently estimatable probe operators and investigate the limits of Clifford UIRS. As a first step, we assume A to be a restriction of a unitary channel U to the traceless subspace, i.e. A = P ad U P ad. In this case, the dynamic shadow norm can in fact be bounded by a small constant independent of the sequence length.

Theorem 2

(Clifford UIRS unitary norm bound). For the n -q ubit Clifford UIRS protocol, U a unitary channel, and A = P ad U P ad, it holds that

Theorem 2 is noteworthy for several reasons. First, it does not depend on the number of qubits n. Therefore, the estimation of k U (m) is efficient even on a quantum system consisting of many qubits. Second, the shadow-norm bound does not depend on the sequence length m, enabling relative accuracy estimation of the decay rate in certain regimes. We note that the constant 10 is probably sub-optimal. The derivation of this theorem can be found in Supplementary Note 6.

As the main consequence of Theorem 2 together with Eq. (10), we find that it is possible to sample-efficiently estimate exponentially many relative fidelities with respect to unitary channels to additive precision from the same gate-set shadows obtained by multi-qubit Clifford UIRS.

Next, we consider a general probe super-operator A restricted to the traceless subspace. Note that A does not need to be a quantum channel. In the following, we show that the dynamical shadow norm can be controlled in terms of the unitarityof A, For instance, u (A) ≤ 1 if A is a quantum channel with equality if A is indeed unitary. We prove the following theorem.

---

### Ecological memory preserves phage resistance mechanisms in bacteria [^4496059d]. Nature Communications (2021). High credibility.

Methods

Phage-receptor binding and infection rate

In the quasi-steady-state approximation of Eq. (1), r b is taken to be at steady-state, By substituting r f = r − r b and P f = P − r b for free receptor and free phage concentrations, and solving the quadratic equation for r b we obtainknown as the Morrison equation, whereis the Michaelis constant. We expand in the small parameterto the first order, noting that we always have ϵ < 1, Using r = n r A above, we obtain the total phage absorption ratewhich yields the per cell infection rate given in Eq. (2).

The value of K m can be determined from experiments using E. coli and λ phage. In the Berg-Purcell limit of a perfectly absorbing host cell (n r → ∞), phage arrive at the cell surface at rate(Ref.). For finite n r, the rate is given by, where n c is a constant, which yields the per-receptor rate constantin Eq. (1). Measured values ofare on the order of 10 −11 − 10 −10 cm 3 s −1, whileand α are ~10 −3 s −1 and n c ~ 10 2 (Ref.). From these, we find K m /(n r + n c) ~ 10 7 − 10 8 cm −3. This estimate shows that most prior experiments performed with E. coli and λ (e.g. Refs.,) are consistent with the high binding affinity limit, n r A, P ≳ K m. The low binding affinity limit, n r A, P ≪ K m, yields a Lotka-Volterra interaction term A ⋅ P in the population dynamics equations (3), and may be appropriate in other systems. We assume high binding affinity (K m → 0) in the main text, and analyze the Lotka-Volterra interaction in Supplementary Note 1. We have ϵ ≪ 1 for K m ≫ n r A, P, or for K m ≪ n r A, P if either P /(n r A) ≪ 1 or P /(n r A) ≫ 1, i.e. in the limits of low or high MOI per receptor. We note that because the relevant MOI is per receptor, most experimental conditions have P /(n r A) ≪ 1 and the lowest order term in Eq. (9) is sufficient.

---

### Complex systems and the technology of variability analysis [^e26f6d93]. Critical Care (2004). Low credibility.

F(n) = v (1/N S k = 1 N [y(k) 2 - y n (k) 2])

By performing this analysis for all values of n, it is possible to calculate the relationship between F(n) and n. Scaling or fractal correlation is present if the data is linear on a graph of log F(n) versus log(n). The slope of the graph has been termed a, the scaling exponent, which will vary from 0.5 (white noise or uncorrelated random data) to 1.5 (Brownian noise or integrated white noise or random walk). When a = 1, behaviour corresponds to the 1/f noise. As a increases above 1 to 1.5, behaviour is no longer determined by a power law. Because the linear relationship between log F(n) and log(n) appears to have two distinct linear segments, one for small (n < 11) and large n (n > 11), the slopes of both lines are calculated separately and termed a 1 and a 2, respectively; repeatedly, a 1 has proven superior to a 2 in terms of prognostic ability.

Approximate entropy

ApEn is a measure of 'irregularity'; smaller values indicate a greater chance that a set of data will be followed by similar data (regularity), and a greater value for ApEn signifies a lesser chance of similar data being repeated (irregularity). To calculate ApEn of a series of data, the data series is evaluated for patterns that recur. This is performed by evaluating data sequences or runs of length m, and determining the likelihood that other runs of length m are similar, within a tolerance r. Thus, two parameters, m and r, must be fixed to calculate ApEn. Increased regularity is associated with illness.

The following is a description of the calculation of ApEn. Given any sequence of data points u(i) from i = 1 to N, it is possible to define vector sequences x(i), which consist of length m and are made up of consecutive u(i), specifically defined by the following:

---

### K-mer analysis of long-read alignment pileups for structural variant genotyping [^ec73b2ac]. Nature Communications (2025). High credibility.

Methods

Kanpig algorithm

K-mer vectors

Variant sequences are encoded as k-mer vectors. Let n be the set of nucleotides n = {A, G, C, T} and the function E be the two-bit encoding of each element of n such that A = 0; G = 1; C = 2; T = 3. Let S be a string of nucleotides with length l such that S = n 0, n 1, n 2,… n l-1. A k-mer K of size k is a substring of S beginning at 0-based position i and defined as K ki (S) = n i, n i+1,.n i+k-1. K-mer vectors (V) hold the counts of all k-mers inside of a string of nucleotides. Each possible k-mer combination of n can map to a unique index j of a conceptual array V with size | n | k. The index j of K k0 can be calculated using a left bitshift operation (< <) and the formula:

As an example, j of the first 3-mer for the sequence "CAGT" is:

V j is then incremented. Next, for every n from S k.l-1, we alter j via

This operation updates j by first masking (m) the two leftmost bits, left-shifting the remaining bits twice, and adding the current nucleotide's encoding before again incrementing V j. For the example sequence "CAGT", the second 3-mer's j is 7 (binary 000111). Sequences shorter than k cannot be vectorized and therefore return a vector of all zeros.

---

### Incipient ferroelectricity of water molecules confined to nano-channels of beryl [^141987d5]. Nature Communications (2016). Medium credibility.

Within the crystal, water molecules can form doublets, triplets and so on, disposed in adjacent cages separated by the bottlenecks inside the channels. We can estimate the concentrations of such agglomerations as follows. Let the cluster of k molecules have a length k, the total number of water molecules be M and that of cages N; 1 ≤ k ≤ M. Combinatorial consideration allows us to derive the following expression for the mathematical expectation n k of number of clusters that contain k sequential water molecules and that are enclosed at both ends either by lone empty sites or by a sequence thereof

This precise expression allows simplification to a formula that is more convenient for practical use. Assuming that M, N, M − k and N − k are large and using the Stirling expressionwe get

Let us use the relative fraction m k = k · n k M −1 of water molecules residing in a clusters of length k. With a total concentration of water molecules C w = MN −1 we obtain

This formula is self-consistent since it fulfils the condition. The results of the corresponding calculations are presented in Fig. 8 by solid lines. As an independent check, we have performed numerical computer simulations to find out how M water molecules can be distributed among N empty cavities (M < N), at different values of M. We assume an array of N = 10 5 elements, each of which is assigned a value 0. Using the standard random number generator, an integer number i ∈[1, N] was generated and the i -th element from those not equal to zero was given a value 1. After each such step the values of N and M deceased by 1. The process was repeated as long as the remaining value of M was positive. At the end, the frequencies of occurrence of clusters of varying lengths were determined. The results of such modelling are indicated by dots in Fig. 8. Clearly, both used analyses provide the same results.

---

### Mechanistic insights into transcription factor cooperativity and its impact on protein-phenotype interactions [^2b738877]. Nature Communications (2020). High credibility.

Tiled k -mer tables

There is an exponential decrease in the counts recovered per k -mer and the value of k, which prevents the calculation of robust k -mer tables and robust relative affinity estimates for high k values. To overcome this, we trimmed nucleotides from both flanking regions of each consensus sequence in the list derived from the CAP-SELEX data. We thereby obtain tiled k -mer tables with sufficient counts for further analyses. To avoid lower complexity of DNA sequences, tiled k -mer tables with a length lower than ten are not considered for further analyses. Our trimming approach was benchmarked through a comparison between the effect of shorter k -mers on the final performance metrics (see Trim-and-summarize coefficient of determination section and Supplementary Figs. 1 a-b and 8). To avoid relative affinity estimates with low support, thresholds of counts per k -mer are defined. In this work, k -mers derived from CAP-SELEX data are discarded if the number of counts supporting those are lower than 20 counts.

Regression models to model k -mer relative affinities

To relate binding affinities with sequence and/or shape features, we used L2-regularized multiple linear regression (L2-MLR), with the following formula:where y is the vector of relative affinities for each k -mer in the k -mer table, X (i = 1,…, n) represent a concatenated set of features that encode their respective DNA sequences, β i (i = 1,…, n) represent the regression coefficients, and β 0 represents the intercept. To prevent overfitting, L2-regularization employs an additional penalty term on the coefficients in the loss function L (β), i.e. coefficients are obtained by minimizingwith λ set to 1.

For regression models based on DNA sequence features, the baseline models are named 1mer and are defined by mononucleotide representations of each k -mer. At any k -mer position i, four features 4 i to 4 i + j with j < 4 are defined based on the nucleotide identity of k i:

---

### Theory and rationale of interpretable all-in-one pattern discovery and disentanglement system [^64945457]. NPJ Digital Medicine (2023). Medium credibility.

Definitions, equations and algorithmic steps

The input dataset, R, is an M×N matrix, where M is the number of entities (i.e. records), A m = 1,… M, each of which is assigned with a unique Entity ID (EID) i, and N is the number of attributes, A 1,…, A N. As for a mixed-mode dataset containing numerical or categorical values, we discretize the numerical values into quantized intervals with equal frequency such that each attribute would assume a finite set of values treated as categorical values denoted as, where AV j is the j th value in A i. For each record, the categorical and discretized values are represented as A i = AV j and referred to as discrete attribute values (AVs).

The first step is constructing association matrix with adjusted standardized residuals. Treating an AV pair as a compound event co-occurring on the same entities in the dataset, we intend to find out whether an AV-association is statistically significant. We adopt a well-established statistical measure, known as adjusted statistical residual (SR)to represent the statistical weight of the event as defined in Definition 1 for an AV pair, and generalize it for high order AV-association referred to as a pattern if the association is statistically significant.

Definition 1. statistical residual (SR)

The statistical residual is a statistical measure, referred to as the adjusted standardized residual, to account for the deviation of the observed frequency of occurrences of a compound event (such as an AV pair) from the expected frequency if the co-occurrence of the event is random (i.e. with statistically independent AVs, Definition 2). For an AV pair (AV 1 ↔ AV 2), it is written as SR (AV 1 ↔ AV 2) in Eq. (1)where Occ (AV 1) and Occ (AV 2) are the frequency of occurrences of the AV pair where Occ (AV 1 ↔ AV 2) is the total number of co-occurrences of the AV pairs (AV 1 and AV 2) in an entity; Exp (AV 1 ↔ AV 2) is the expected frequency if the AVs in the pair are independent; and T is the total number of records. By converting the frequency of all the distinct AV pairs in the frequency association table obtained from data into SR though the Address Table, we obtain a square SR-Matrix (Definition 3).

---

### Tomographic near-eye displays [^b9fe89a2]. Nature Communications (2019). High credibility.

Least-squares problem for occlusion blending

To formulate the least-squares problem for occlusion blending, we suppose a tomographic display that supports m focal planes with n × n resolution. The backlight image sequence consists of m binary images, each of which has the resolution of n × n. The RGB image can be considered as three grayscale images with n × n resolution. Accordingly, m focal plane imagesare given by the multiplication of the backlight image sequence and RGB image as follows:whereis the Hadamard product of matrices. Then, the derivation of the least-squares problem for optimization is very similar to that of previous studies.

In this study, we apply a multi-view-based optimization algorithm, that has merits in terms of its computation memory and speed. We note that this approach shows similar performance to that of retinal optimization, if we set the eye box size as the pupil diameter. The optimization target is a set of perspective view images within the pupil. In summary, we should solve the following least-squares problem:where P (k, j) denotes the projection matrix between the j th layer and the k th perspective view image. Note that the optimization parameters are b k and D.

The least-squares problem has specific constraints, due to the characteristics of the FSAB: b k should be binary (0 or 1) and D should be bounded between 0 and 1. The binary constraint makes the least-squares problem NP-hard. We decided to find the optimized parameters by solving the relaxation of the problem with the carefully designed initial condition. The initial condition is established based on the optical structure of the tomographic displays. For details on the solver, initial condition, convergence graph, and optimal solution, see Supplementary Note 2.

---

### A stochastic vs deterministic perspective on the timing of cellular events [^c3a6938f]. Nature Communications (2024). High credibility.

Case 1: the simple birth–death process

When r = 1, the model given in reaction scheme (8) coincides with a simple birth–death process. We are interested in the mean waiting time for the birth–death process to reach a fixed protein number N, given that the system is started from n initial proteins. Simulations of the simple birth–death process suggest the MFPT is smaller than the deterministic FPT (see Fig. 2 B, but note that only a small sample of trajectories are shown here for visualisation purposes). We now prove this analytically. To begin, we find an analytical solution for the MFPT of the stochastic description of the system, as given by the CME (see Equation (3) of the Supplementary Information).

Starting from Eq. (7), it can be shown (Supplementary Note 2), that the expected waiting time to reach N proteins given the system is started from n ≤ N, is given by, where for real number x and positive integer m, the notationabbreviates x (x − 1)… (x − (m − 1)), the falling factorial of x. In what follows for simplicity we shall set n = 0. The deterministic description of the system is given by the reaction rate equation d τ X = K − X, with X (0) = 0, and solving for X (τ) yields X (τ) = K (1 − e − τ). Solving for τ then gives, Consider some proportion 0 < ρ < 1, and consider the expected waiting time to reach ρ K, that is, some proportion ρ of the steady-state mean. Then Eq. (10) simplifies towhich we note is independent of K, and where the requirement of ρ < 1 enables this to be well defined. Now let us consider Eq. (9) for N = ⌊ ρ K ⌋. For notational simplicity only, we assume that ρ K is an integer, and again observe that the numeratoris bounded above by (ρ K) i +1. Replacing this in Eq. (9) givesRecognising the Taylor expansion ofaround 0 (for ρ < 1), we find that Eq. (12) is the truncated Taylor expansion of Eq. (11). Thus, as each term of the series is strictly positive, it follows that for any proportion ρ of the steady-state mean, the deterministic waiting time is a strict upper bound for the mean waiting times. An example is given in Fig. 2 C for ρ = 0.8. Here we show how the ratio of the stochastic mean waiting time to the deterministic waiting time, η, of the simple birth–death process scales with increasing K. Note that since δ = r = 1, K is equal to the steady-state mean number of molecules.

---

### The value of understanding feedbacks from ecosystem functions to species for managing ecosystems [^06e34c93]. Nature Communications (2019). High credibility.

Management actions

We assume that managers can protect one species at each time step, with the same management cost for each species (we run further analysis with increased management cost as the trophic level of the species increases, Supplementary Table 1). We define a strategy δ as a function that prescribes which species to protect for a given ecosystem state (defined by the set of species extant in the ecosystem). The strategy is applied in the initial ecosystem state, at the next time-step some species become extinct while other remain extant. The strategy then is applied to this new ecosystem state, and so on. The sequential application of the strategy defines a sequence of species to protect.

An optimal strategy is defined as a strategy that yields the maximum level of the expected outcome (see 'Value of a strategy, δ' in the Supplementary Methods). Here, we consider two definitions of the outcome: the discounted sum of expected number of extant species across all possible states in the system, and the discounted sum of expected amount of ecosystem service provided by the system (measured in US dollars).

---

### Sequence-dependent material properties of biomolecular condensates and their relation to dilute phase conformations [^f63346a1]. Nature Communications (2024). High credibility.

The degree of charge segregation in the sequences was quantified using the sequence charge decoration (SCD) parameter. Smaller values of SCD correspond to a more charge-segregated sequence. To compare different polyampholyte compositions and lengths, we defined a normalized SCD (nSCD) parameter that is scaled by the maximum and minimum SCD values achievable for each of the E-K, LAF1, and DDX4 sequence compositions. The values of this parameter lie in the interval 0 ≤ ≤ 1, with the lower and upper bounds corresponding to a perfectly alternating sequence and diblock sequence, respectively. The sequences and their nSCD value are shown in Fig. 1a. Note that the LAF1 sequence with = 0.010 and the DDX4 sequence with = 0.021 are the wild-type (WT) sequences. We characterized the diffusion coefficient, viscosity, and surface tension of the condensates by simulating the protein chains in bulk systems and phase coexistence systems (Fig. 1b), respectively, using the HPS model (see "Methods" section for model and simulation details). All results for the E-K and DDX4 sequence variants were obtained at a fixed temperature of T = 300 K. The results of LAF1 sequence variants were obtained at T = 280 K as the critical temperature of its WT obtained from our coarse-grained (CG) model(Supplementary Fig. 1) is.

Fig. 1
Model and natural IDP sequences, and setup for computing their condensate material properties.

a E-K (chain length, N = 50), LAF1 (N = 168), and DDX4 (N = 236) sequences with their normalized SCD (nSCD) parameter. The LAF1 sequence with = 0.010 and the DDX4 sequence with = 0.021 are the wild-type sequences. The negatively and positively charged residues are shown as red and blue beads, respectively. The uncharged residue positions in LAF1 and DDX4 are shown as gray line segments connecting the charged residues. b Simulation snapshots of one of the E-K sequences in a cubic simulation box for characterizing its dense phase dynamical and rheological properties, and in a slab geometry for characterizing its interfacial property between the dense and dilute phases. c Dense phase concentration ρ as a function of nSCD for all sequences. Error bars in (c) are standard deviations about the mean. The mean values are obtained as an average over n = 2 independent constant pressure P = 0 atm simulations. Source data are provided as a Source Data file.

---

### Accurately clustering biological sequences in linear time by relatedness sorting [^7d900ea6]. Nature Communications (2024). High credibility.

Methods

Measures of similarity between sequences

Clusterize permits several parameterizations of similarity. Most clustering programs define similarity as the number of matching positions divided by the length of the shortest sequence in each pair. By default, Clusterize defines similarity as the number of matching positions divided by the length of the overlapping region between pairs of sequences, because this definition works well for partial-length sequences. However, this definition is susceptible to deriving estimates of similarity from very small overlapping regions, which is why a coverage of at least 50% of positions is required by default. Clusterize allows the user to control whether gaps (i.e. insertions or deletions) are ignored, counted as mismatches or, by default, treated as a single mismatch per run of consecutive gaps. The latter definition represents an event-based model of distance, wherein a gapped region is considered a single event, analogous to a single substitution event. In practice, most definitions of distance yield comparable accuracies (i.e. AMI and NMI) at the same number of clusters, although the same number of clusters will be obtained at different similarity thresholds.

Since pairwise alignment is time-consuming, estimates of similarity are initially based on k-mer matching between pairs of sequences in phases 2 and 3. First, each sequence is converted into 32 bit integer vectors representing every overlapping k-mer. Second, these integers are ordered using radix sorting, and the k-mers' original positions are also stored as integers. These two integer vectors per sequence typically consume the majority of memory used by Clusterize. Third, the sorted integer vectors are matched between two sequences, and a vector is constructed mapping matching positions in one sequence to the other. Fourth, contiguous blocks of matching positions are recorded. Dynamic programming is used to determine the largest collinear set of blocks shared between the two sequences, defined as the maximum length set of matching k-mer regions (Fig. 1M). Fifth, these anchor regions are used to compute k-mer similarity or accelerate pairwise alignment (Fig. 1N). Sequences between anchor regions are aligned using the PFASUM50 substitution matrixfor amino acids or a scoring scheme of matches = 3, transitions = 0, and transversions = −3 for nucleotides. Standard affine gap penalties are added with gap opening = −10, gap extension = −2, and terminal gaps having zero cost during alignment.

---

### Defining HIV-1 transmission clusters based on sequence data [^19e802ab]. AIDS (2017). Low credibility.

Understanding HIV-1 transmission dynamics is relevant to both screening and intervention strategies of HIV-1 infection. Commonly, HIV-1 transmission chains are determined based on sequence similarity assessed either directly from a sequence alignment or by inferring a phylogenetic tree. This review is aimed at both nonexperts interested in understanding and interpreting studies of HIV-1 transmission, and experts interested in finding the most appropriate cluster definition for a specific dataset and research question. We start by introducing the concepts and methodologies of how HIV-1 transmission clusters usually have been defined. We then present the results of a systematic review of 105 HIV-1 molecular epidemiology studies summarizing the most common methods and definitions in the literature. Finally, we offer our perspectives on how HIV-1 transmission clusters can be defined and provide some guidance based on examples from real life datasets.

---

### Robust enzyme discovery and engineering with deep learning using cataPro [^7857fbc8]. Nature Communications (2025). High credibility.

Evaluation metrics

In this work, three evaluation metrics were primarily utilized, namely Pearson correlation coefficient (PCC), Spearman correlation coefficient (SCC), and root mean square error (RMSE):Here, X i and Y i are predicted and experimental log 10 k c a t or log 10 K m, or log 10 (k c a t / K m).andrepresent the mean predicted value and the mean experimental value, respectively. d i is the difference in ranking between the predicted and experimental value for a particular sample.

When evaluating the models' ability to identify the better-performing mutant between any two mutants for a given reaction, we first generate the all pairwise combinations (order-independent) of mutants for the reaction. For a reaction with N mutants, the total number of pairwise combinations is M t o t a l = N (N -1)/2. For each pair, if the model accurately identifies which of the two mutants performs better, the prediction is considered successful. By evaluating all pairwise combinations within the reaction and counting the number of successful predictions (M t r u e), the accuracy is defined as follows:

Calculating PSSM based on multiple sequence alignment

PSSM is a widely used in bioinformatics for protein sequence alignment. It is derived from multiple sequence alignments and provides a statistical representation of the conservation and variation of amino acids at each position in a protein sequence. The PSSM score at a given position indicates the likelihood of finding a particular amino acid at that position based on evolutionary information from aligned sequences. Higher positive scores indicate a higher degree of conservation, while negative scores suggest variability. PSSM scores are useful for identifying conserved regions and potential functional sites. In this work, we first applied Jackhmmerto search for homologous sequences of SsCSO from UniRef90 (April 2024), generating a multiple sequence alignment, based on which a PSSM score matrix was generated using the PSSM calculation tool in TGT_Package.

---

### Flexible and cost-effective genomic surveillance of P. falciparum malaria with targeted nanopore sequencing [^ecce9fd9]. Nature Communications (2024). High credibility.

Statistical model for hrp2/3 deletion detection

Data and notation

We first describe the data and notation used by our model (Table 2). Amplicon sequencing data is represented as a two-dimensional matrix of positive integers, X, which holds read counts after quality filtering, demultiplexing and mapping. The matrix X has rows representing samples, which are indexed by i ∈ {1, 2. n }, and columns representing target genes, which are indexed by j ∈ {1, 2. m }; each element, x i j, represents the number of reads from sample i that mapped to the target gene j. We define a corresponding binary matrix, C, where each element c i j indicates that the given target gene j is either present (c i j = 1) or deleted (c i j = 0) in sample i. We define a vector a of size n such that a i ∈ [0, 1], which represents the relative abundance of each sample in the sequencing library. Finally, we define two scalar parameters: a read misclassification rate, ϵ ∈ [0, 1], which represents the rate at which reads derived from sample k contribute to another sample's read counts (i.e. to x i ≠ k, j), whether by contamination or incorrect sample assignment during demultiplexing; and a read count dispersion term, which is given a precise mathematical definition below.

Table 2
Notation for hrp2/3 deletion detection model

---

### Large-scale DNA-based phenotypic recording and deep learning enable highly accurate sequence-function mapping [^645a0d4e]. Nature Communications (2020). High credibility.

Optimization of sampling time points

Sampling times were optimized using the high-quality kinetic profiles obtained from the proof-of-concept RBS library (see previous section for definition of high-quality profiles). To avoid biases towards the initial sampling schedule, we first represented the profile p of each RBS by an approximation with a logistic functionimputed at 5 min intervals, which was fixed as the minimal time difference between two samples (Supplementary Fig. 25a). In cases where logistic approximation was not possible (i.e. failed parameter optimization), an exponential decay function was used. Formally, RBS i is represented by its approximation. Afterwards, optimal sampling times were greedily selected while fixing the first and last sample at 0 and 720 min after induction. The goal of the optimization was to find a set S of optimal sampling times (initially S = (0, 720)) which allows to reconstructsuch that a linear approximation using time points in S is as close toas possible. Given the set of possible sampling times T = {0, 5, 10,…, 720} and the subset I of RBS profiles on which the sampling times should be inferred, the greedy optimization finds the next optimal sampling time point s * from T as follows:wherecorresponds to the linear approximation ofusing only sampling times in S (Supplementary Fig. 25b). In other words, s * is the time point that (i) is not part of the sampling schedule S yet and (ii) results in the smallest cumulative reconstruction error over all RBS profiles in I. Subsequently, S is augmented by s *, and Eq. (1) is evaluated to find the next optimal s *, until S contains the desired number of sampling times. Finally, the quality of the optimal sampling schedule for every RBS i is evaluated by computing the approximation error r i between the observed profile p i, and its linear interpolation at the optimal time points S, termed l i, S (Supplementary Fig. 25c):This optimization was performed for different sets of RBS profiles I: we first sorted RBSs by their observed strength (i.e. difference in the fraction of flipped discriminators between first and last sample) and optimized sampling for the top 5%, 10%, 25%, 50%, and 100%, respectively. This strategy was chosen to compensate for the strong bias towards weak RBSs in the initial library. Afterwards, we computed the cumulative approximation error on the entire library (top 100%) for the sampling schedules optimized on the different subsets. We found that for seven or more samples the difference in approximation error between subsets became indistinguishable and hence chose the sampling times inferred on the top 10% of profiles for the following experiments.

---

### Scalable watermarking for identifying large language model outputs [^9ade55d4]. Nature (2024). Excellent credibility.

Repeated context masking

To generate a full response, we could simply apply Algorithm 2 on every decoding step, using the sliding-window random seed generator ('Random seed generator' section) to generate the random seed r t for each step. However, it is possible that the same window of context, and thus the same random seed might occur more than once (particularly if the sliding-window size H is small or the response is long). It has been shown that in this scenario, the watermark can introduce a repeated bias that affects the quality of the text, for example, causing repeating loops. One way to avoid this problem is to apply repeated context masking, which prevents the watermark from being applied on step t if the context window (x t − H,…, x t −1) has been used to watermark previously.

We present the method in Algorithm 3, which we call K -sequence repeated context masking. The integer parameter K ≥ 1 controls for how long context windows are held in the history. In the simplest case of K = 1, we only hold the context history for the duration of generating a single response. For larger integers K > 1, we check against a history of contexts used in the last K responses. In the extreme case, we could set K = ∞ and retain the context history indefinitely. In Supplementary Information section G.2, we show that applying K -sequence repeated context masking achieves K -sequence non-distortion, an important property for quality preservation. In Supplementary Information section G.3, we discuss the trade-offs of smaller and larger K. For most of our experiments we use K = 1.

Algorithm 3

Generating watermarked responses with sliding-window random seed generation and K -sequence repeated context masking.

Require: LLM p LM (⋅∣⋅), context window size H, pseudorandom hash function h, watermarking key, sampling algorithm, integer K ≥ 1, stream of prompts (x 1, x 2,…).

1: for i ≥ 1 do

2:

3: t ≔ n where n is the length of

4: while do

5: t ≔ t + 1

6: if then

7: Sample

8: else

9:

10: Sample

11:

12: end if

13: end while

14: return Response

15: end for

---

### PGSE, OGSE, and sensitivity to axon diameter in diffusion MRI: insight from a simulation study [^7ca4a558]. Magnetic Resonance in Medicine (2016). Low credibility.

Implications for Practical Applications

In practical applications, e.g. for a realistic tissue model with fiber dispersion or unknown orientation and realistic SNR, the choice of N impacts significantly the range of axon diameters that can be identified and distinguished from one another. For a fixed TE, sequences with N > 1 increase the detectable difference in the signal, and distinguish between axon diameters that N = 1 cannot. For example, for, 300 mT/m scanner's lower limit for SNR = 10 is approximately 4.3 µm for N = 1, and 3.1 µm for N = 4. In another example, Figure 7 middle row shows that forand G = 300 mT/m, N = 4 signal can with 1 µm resolution access axon diameters which are 1 µm lower than with N = 1. Hence, going from N = 1 to the best choice of N increases the overlap of the range of axon diameter sensitivity with naturally occurring axon diameters, which can potentially allow more accurate axon diameter imaging of in vivo human brain. These advantages are especially prominent on stronger gradient strengths.

It is also worth noting that moving from the traditional PGSE sequences to the OGSE sequences discussed here is simple and offers additional practical benefits. They have only one additional parameter, the number of lobes N, therefore, are easy to implement and run on standard clinical and preclinical scanners. Low‐frequency OGSE (N > 1) can also significantly reduce the gradient heating and deal better with eddy currents than PGSE sequences.

Model Considerations

In this work, we choose a prevailing simple model of white matter 11, 12. More complex models increase the difficulty of locating most sensitive sequence parameters and could potentially influence some of the results presented here, e.g. membranes with permeable walls could create an increase in signal loss for sequences with large b ‐value, hence OGSE sequences with N > 1 would have further advantage over PGSE sequences. Another simplification of our model is the representation of axons as a collection of cylinders with one diameter. It is known from histology 44 that in the brain, white matter axons have a diameter distribution which is often modelled as a Gamma function. Nevertheless, an understanding of which sequence is most sensitive to each particular axon diameter is very informative as it provides a deeper understanding of the relationship between individual restrictions and sequence parameters, and is a first step toward understanding sensitivity to the combinations of different diameters.

---

### Natural language generation for electronic health records [^b39cedec]. NPJ Digital Medicine (2018). Low credibility.

Table 4
Discrete variables in our dataset

The first column shows the variable names, the second column a description of their unique original values, and the third column a bracketed set indicating the range of those values after being recoded during preprocessing

To preprocess the chief complaints, we begin by removing records containing words with a frequency of less than 10 (n = 240,343); this yields a vocabulary V of 23,091, and is primarily a way to reduce the size of the word embedding matrix in our Long Short Term Memory (LSTM) network, reducing the model's computational complexity, but is also serves to remove infrequent abbreviations and misspellings. Similarly, we also remove records with chief complaints longer than 18 words (n = 490,958), the 95th percentile for the corpus in terms of length, which prevents the LSTM from having to keep track of especially long-range dependencies in the training sequences. Following these 2 steps, we convert all the text to lowercase, and we append special start-of-sequence and end-of-sequence strings to each chief complaint to facilitate text generation during inference. Finally, we vectorize the text, convert each sentence to a sequence of vocabulary indices, and pad sequences shorter than 18 words with zeros (these are masked by the embedding layer of our LSTM).

After preprocessing, we are left with ~4.8 million record-sentence pairs, where each record is a 403-dimensional binary vector R, and each chief complaint is a 18-dimensional integer vector S. Using a random 75–25 split, we divide these into 3.6 million training pairs and 1.2 million validation pairs. To reduce the time needed to evaluate different sampling procedures during inference, we save only 50,000 of the validation pairs for final testing.

Modeling

---

### Understanding the neural code of stress to control anhedonia [^4f0a5755]. Nature (2025). Excellent credibility.

Contribution of facial and limb movements to neural activity in the BLA

We investigated whether the facial and limb features contributed to BLA neural activity during the pre-task period. We fitted facial and limb features to neural activity (firing rate) using linear regression in each mouse separately. We binned neural and facial and limb feature data using a 1 s time window (total of T = 360 bins) and defined our model as Y = AX T + β, in which Y is an N × T matrix with the firing rate of N recorded neurons, A is an N × K matrix with the regression coefficients of K = 6 facial and limb features, and X is a T × K matrix with the K facial and limb features values. β is the intercept (a constant). Before fitting, the data were centred to zero. We used the linear least square error as a loss function and added an L2-norm regularization term to prevent overfitting. We tried a range of values for the L2-norm regularization term, ranging from 0 (equivalent to ordinary least squares) to 10 3, with no significant difference in the final coefficient of determination (R 2) estimate.

We did not find a positive R 2 from any of the linear models, suggesting that using facial and limb features that we recorded, we could not predict BLA neural activity better than chance. In other words, these facial and limb features are unlikely to contribute significantly to BLA neural activity, and consequently, any group differences that we observed.

---

### Effective use of tables and figures in abstracts, presentations, and papers [^9d553efc]. Respiratory Care (2004). Low credibility.

In some situations, tables, graphs, and figures can present certain types of information (including complicated relationships and sequences of events) more clearly and in less space than the same information would require in sentence form. However, do not use tables, graphs, and figures for small amounts of data that could be conveyed clearly and succinctly in a sentence. Also, do not reiterate in sentences the data that are shown in a table, graph, or figure: the point of creating a table or graph or figure is to eliminate that type of sentence from your manuscript. In building a data table you must balance the necessity that the table be complete with the equally important necessity that it not be too complex. Sometimes it is helpful to break a large table into several smaller ones to allow the reader to identify important information easily, but, conversely, it is a common mistake of novice authors to split up into several tables data that belong in one table. In almost all cases, only one table or graph or figure should be included in an abstract, and then only if it can convey essential information in less space and in a more easily interpretable way than the sentence form. For a poster, in almost all instances you should use only one typeface and one font in a table, graph, or figure. In general, do not use bold, italics, or color unless you are presenting a great deal of data and you need to highlight certain data values and you are certain that using bold, italics, or color will improve readability, which is rare. Do not include identical information in a table and a graph/figure. In reporting a clinical trial you will need to include a patient flow chart that identifies the number of patients initially screened for the study, the number of patients who were excluded (and why) after initial screening or in the final analysis, and how many patients entered, exited early, and completed each arm of the study. A treatment protocol should also be described with a flow chart. In preparing a graph the most common error is to include a line that suggests an unsubstantiated extrapolation between or beyond the data points. In selecting the graph's axes, avoid truncating, enlarging, or compressing the axes in ways that might make the graph confusing or misleading. To prepare clear, accurate, easily interpretable tables, graphs, and figures, rely on the rules described in authoritative guides such as the Council of Science Editors' Scientific Style and Format and the American Medical Association's Manual of Style.

---

### Initial diagnostic workup of acute leukemia: guideline from the college of American pathologists and the American Society of Hematology [^fd37409d]. Archives of Pathology & Laboratory Medicine (2017). Medium credibility.

Supplemental Digital Content — acute leukemia initial diagnostic workup evidence table catalogs individual studies with year, descriptors such as "Single cohort", numeric entries like "N (104:323)", "N (72:579)", and "N (200:2653)", column values including "Y", "N", or "NR", funding noted as "Non-industry" or "NR", and study ratings labeled "Low", "Low-moderate", or "Moderate". Examples include Metzeler K et al (2011) listed with "N (104:323)", "Y", "Y", "Non-industry", "Low"; Montesinos P et al (2011) with "N (72:579)", "Y", "Y", "Non-industry", "Low"; Kayser S et al (2011) with "N (200:2653)", "Y", "NR", "Non-industry", "Low-moderate"; Barbaric D et al (2007) with "N (1702:189)", "Y", "NR", "Non-industry", "Low-moderate"; and Mrózek K et al (2008) with "N (4991:934)", "N", "NR", "Non-industry", "Moderate". Additional rows explicitly show "Single cohort" entries paired with terms such as "Not applicable (N/A)" or "N/A", alongside "Non-industry", "NR", and ratings including "Low-moderate" and "Moderate".

---

### Influence maximization in boolean networks [^8db2e9bf]. Nature Communications (2022). High credibility.

Fig. 1
Accuracy of the individual-based mean-field approximation.

a We evaluate the mean squared error e (t), as defined in Eq. (5), of the prediction made under the individual-based mean-field approximation (IBMFA) with respect to the ground-truth configuration of the system at stage t of the dynamics. The systems under observation are random Boolean networks (RBNs) with N = 100 nodes and fixed degree k. Initial configurations are sampled from Eq. (3), where the probability of node i to be active equals s i (t = 0) = 1/2 for all i = 1,…, N. We consider 100 RBNs for each value of k. For each of them, ground-truth trajectories used in the computation of the IBMFA error are estimated by relying on R = 100 independent simulations. We then display the average error over the 100 RBNs using full curves of different colors depending on the degree k of the underlying RBN. IBMFA errors are compared to their corresponding baseline values b (t), i.e. the variance of the sampled trajectories used to estimate the average trajectory of the network [see Eq. (6)], which are shown as dashed curves in the plot. b Same as in panel a, but for selected biological signaling networks: the Drosophila melanogaster segment polarity network (drosophila), the T cell large granular lymphocyte leukemia network (leukemia), and the estrogen receptor breast cancer network (breast cancer). Initial configurations are sampled from Eq. (1), where the probability of node i to be active equals to s i (t = 0) = 1/2 for all i = 1,…, N.

---

### Technical considerations for functional sequencing assays [^c96ed1e4]. Nature Immunology (2012). Medium credibility.

Next-generation sequencing technologies need careful design of experiments and evaluation of results to meet field requirements. Here we discuss technical considerations for these high-throughput assays, together with criteria to assess the quality of the results and the necessary validation.

---

### 2023 U.S. department of veterans affairs and U.S. department of defense clinical practice guideline for the management of headache [^7d673313]. Annals of Internal Medicine (2024). High credibility.

Algorithm — this clinical practice guideline (CPG) algorithm is designed to facilitate understanding of the clinical pathway and decision-making process used in managing patients with Headache, and the format represents a simplified flow that helps foster efficient decision making by providers; it includes steps of care in an ordered sequence, decisions to be considered, decision criteria recommended, and actions to be taken; the algorithm is a step-by-step decision tree, with standardized symbols to display each step and arrows connecting numbered boxes indicating the order in which the steps should be followed; shape meanings are specified: rounded rectangles represent a clinical state or condition, hexagons represent a decision point in the process of care formulated as a question that can be answered "Yes" or "No", rectangles represent an action in the process of care, and ovals represent a link to another section within the algorithm; Sidebars 1–7 provide more detailed information, and Appendix K contains alternative text descriptions of the algorithms.

---

### Not all sensors are created equal: a framework for evaluating human performance measurement technologies [^c010268e]. NPJ Digital Medicine (2019). Medium credibility.

Results

The evaluation framework follows a three-step process: (1) Requirements Definition, (2) Device Search and (3) Device Evaluation (Fig. 1). Each step of this framework is supported by relevant templates, which guide the user through the process and to allow for the clear documentation of the rationale for their choice. In this regard, the user is defined as the person/group responsible for selecting the device for deployment in the specified application. Though it is recommended that the framework be employed in a systematic manner, the steps could be applied in sequence or users could elect to apply isolated elements of the framework if constrained by resources and time. For example, there may be situations where one or more devices of interest have already been identified as part of an ad-hoc process. In this case, the user could complete step one (Requirements Definition), skip step two (Device Search) and proceed to step three (Device Evaluation) to determine which of the pre-selected devices is most fit for purpose and/or worthy of field testing, according to their specific requirements. On the other hand, some users may not have the resources or time to enable completion of a formal field testing phase, and therefore this could limit the application of the framework to a desk-based evaluation of identified devices.

Fig. 1
Evaluation framework's three-step process

---

### Jumper enables discontinuous transcript assembly in coronaviruses [^968df243]. Nature Communications (2021). High credibility.

Results

Discontinuous Transcript Assembly problem

To formulate the DISCONTINUOUS TRANSCRIPT ASSEMBLY problem, we define discontinuous transcripts as sequences of segments whose order matches the reference genome. More formally, we have the following definition.

Definition 1

Given a reference genome, a discontinuous transcript T is a sequence v 1,…, v ∣ T ∣ of segments where (i) each segment corresponds to a contiguous region in the reference genome, (ii) segment v i precedes segment v i + 1 in the reference genome for all i ∈ {1,…, ∣ T ∣ − 1}, (iii) segment v 1 contains the 5ʹ end of the reference genome and (iv) segment v ∣ T ∣ contains the 3ʹ end of the reference genome.

While the genomic transcript T 0 matches the reference genome, subgenomic transcripts contain jumps and correspond to subgenomic RNAs (sgRNAs). Discontinuous transcriptsoccur in abundances c = [c i] where c i ≥ 0 is the relative abundance of transcript T i such that. In this work, we focus on coronavirus sequencing samples obtained using Illumina sequencing, where reads originate from the reference genome of length L of about 10–30 Kbp and have a fixed length ℓ ranging from 100 to 400 bp. We refer to Supplementary Note A.3 for a discussion on why transcript assembly remains relevant for such samples in light of the availability long-read sequencing samples. For ease of exposition, we describe the formulation in the context of single-end reads, but in practice we use the paired-end information if it is available. We refer to Supplementary Note B.5 for details on the paired-end formulation.

---

### Olivar: towards automated variant aware primer design for multiplex tiled amplicon sequencing of pathogens [^350565b3]. Nature Communications (2024). High credibility.

Generation of a set of PDRs

Similar to forward primer (fP) and reverse primer (rP), there are also forward PDR (fPDR) and reverse PDR (rPDR). Here forward means closer to the 5' end (left end) of the targeted sequence. There are three restrictions for a valid PDR set given a targeted sequence:
PDRs should not overlap with each other (this guarantees that primers will not overlap).
All amplicons generated from a PDR pair should satisfy a range of desired amplicon length, as defined by the user. An amplicon is defined as a pair of fP and rP, and its length is defined from the start of fP to the end of rP.
The targeted sequence are fully covered with inserts. An insert is the region between a PDR pair.

Suppose a valid PDR set consists of N PDR pairs and each PDR pair is generated sequentially. For the k th PDR pair (k ≥ 1), the fPDR is selected from region C 2k-1, and the coordinates of the fPDR is [p 2 k −1, p 2 k −1 + l − 1]. The rPDR is selected from region C 2k, and the coordinates of the rPDR is [p 2 k, p 2 k + l − 1].is minimum amplicon length andis maximum amplicon length.

Generation of a PDR set starts with the first PDR pair (k = 1). Starting from the left end of the risk array and the targeted sequence, the fPDR of the first PDR pair is selected from region C 1 (Supplementary Fig. 12), Here C 1 starts at 1 for simplicity, and region size of 3 l gives enough room for selecting the first PDR. There are 2 l + 1 possible PDRs within C 1, and one of them is selected as the first fPDR, with start coordinate p 1, based on the method described in the section above (Generation of one PDR). The first rPDR is selected from region C 2 (Supplementary Fig. 13), and based on the amplicon length restriction, The start coordinate of the rPDR is selected as p 2.

---

### Fast and sensitive mapping of nanopore sequencing reads with graphMap [^f4e7db26]. Nature Communications (2016). Medium credibility.

Refining alignments using L 1 linear regression. The alignments obtained using LCSk tend to be largely accurate but since its definition lacks constraints on the distance between substrings, the alignments obtained may include outlier matches and incorrect estimation of overall alignment length (Fig. 1e). These outliers are caused by repeats or sequencing errors, but they still satisfy the monotony condition. Similar to the observation presented for region selection, the LCSk list of anchors should ideally be collinear in the 2D query-target coordinate space, with a slope of 45°. All deviations from this line are caused by indel errors, and can be viewed as noise. The filtering of outlier anchors begins by fitting a 2D line with a 45° slope in the query-target space under the least absolute deviation criteria (L 1). Next, a subset of anchors which are located withinfrom either side of the L 1 line is selected, where e is the expected error rate (by default, conservatively set to 45%), T is the target (read) length and the factoris used to convert the distance from target coordinate space to a distance perpendicular to the L 1 line. A confidence intervalis calculated, where d i is the distance from a selected anchor i to the L 1 line (the constant 3 was chosen to mimic a 3σ rule). LCSk is then repeated once again but only on the anchors which are located within the distance ± c from the L 1 line to compensate for possible gaps caused by anchor filtering (Fig. 1e). The use of L 1 filtering was observed to improve the precision of alignment start and end coordinates for many reads, though the overall impact on performance was less significant in comparison to the LCSk stage (Supplementary Data 3).

After filtering, five empirically derived scores that describe the quality of the region are calculated. They include: the number of exact kmers covered by the anchors n kmers, the s.d. σ of anchors around the L 1 line, the length of the query sequence which matched the target (distance from the first to the last anchor) m len, the number of bases covered by anchors (includes only exact matching bases) n cb and the read length. The last four scores are normalized to the range [0,1] with the following equations (4, 5, 6, 7):

where Q is the length of the reference sequence (query in our previous definition). The overall quality of the alignment in a region is then calculated as the product of the normalized scores:

---

### Principles of analytic validation of immunohistochemical assays: guideline update [^6a4f0275]. Archives of Pathology & Laboratory Medicine (2024). High credibility.

Immunohistochemical assay analytic validation — evidence by outcome shows, for sensitivity/specificity (Sen/Spec), "39 CS, 18 NRS, 2 observational descriptive, 2 diagnostic studies, 2 SR/MA and 1 other study design" and the graded fields read "Serious Not serious Not serious Not serious None Critical Moderate Moderate". For positive predictive value/negative predictive value (PPV/NPV), the table lists "28 CS, 12 NRS, 1 observational descriptive" with the same subsequent sequence "Serious Not serious Not serious Not serious None Critical Moderate Moderate". For concordance, the evidence base comprises "19 CS, 9 NRS, 1 observational descriptive, 1 randomly selected series of patients, 1 diagnostic study and 3 other study designs" and the same sequence follows. For diagnostic accuracy (Dx Acc), the evidence base shows "8 CS, 4 NRS, 1 observational descriptive and 1 other study design" alongside the same sequence. Abbreviation definitions for study types and outcomes are provided on-page (e.g. PPV, NPV, Sen, Spec, Dx Acc, SR/MA, CS, NRS).

---

### The estimands framework: a primer on the ICH E9 (R1) addendum [^18187c8a]. BMJ (2024). Excellent credibility.

Intercurrent events

Intercurrent events are post-baseline events (or post-randomisation events in randomised trials) that affect either the interpretation or existence of outcome data (fig 1, box 5). These generally fall into two distinct categories: treatment-modifying events and truncating events. Other types of intercurrent events can also be defined, but their use is less frequent and we do not consider them here.

Fig 1
Different strategies regarding intercurrent events. In this example, a randomised trial compares intervention with control to understand how outcomes differ at month 2. However, one participant stops treatment before month 2 (ie, an intercurrent event). The figure shows what happens to this participant under each intercurrent event strategy. Under a composite strategy, investigators have decided to assign a score of 0 to any participant who experienced an intercurrent event. Under a while-on-treatment strategy, because the participant experienced an intercurrent event before month 2, their month 1 score of 3 is used in place of their month 2 score. Under a hypothetical strategy, the participant's outcome that would have occurred had they continued treatment at month 2 is used (here, it is a value of 9); but in practice, this value will not be known and so must be estimated. M = month

Box 5
Intercurrent events, protocol deviations, and missing data

The definition of an intercurrent event is broad, encompassing several distinct concepts (eg, treatment-modifying events, truncating events). Owing to some overlap with other common concepts, understanding what is (and what is not) an intercurrent event can be challenging. We summarise below how intercurrent events differ from protocol deviations and missing data.

Treatment-modifying events affect receipt of the assigned treatment. In the example asthma study described in table 1, early discontinuation of dupilumab and use of rescue treatment are treatment-modifying intercurrent events. Other examples might be if patients received the wrong dose of dupilumab, or if patients in the placebo arm received dupilumab instead. These events affect the interpretation of outcome data because outcomes from participants who experienced the intercurrent event might provide different information about treatment than outcomes from participants who did not experience the intercurrent event.

---

### What is a clinical pathway? Development of a definition to inform the debate [^ee2b0721]. BMC Medicine (2010). Low credibility.

Method

We undertook a four-stage process aiming to develop evidence-informed and practical criteria to define a clinical pathway. The four stages were: (1) identify publications exploring the scope and definition of clinical pathways (or similar terms); (2) synthesise previously suggested components and derive draft criteria for testing; (3) pilot test the level of agreement between review authors when applying criteria to identified studies; and (4) modify criteria to maximise agreement between review authors.

---

### Guidelines for assessment of and intervention with persons with disabilities [^fcb7bc45]. The American Psychologist (2012). Medium credibility.

Disability awareness, training, accessibility, and diversity — Guideline 1 states that psychologists strive to learn about various disability paradigms and models and their implications for service provision, noting that legal definitions determine eligibility and parameters for services and benefits, and that the ADA defines disability as a "physical or mental impairment that substantially limits a major life activity, or a record of such an impairment, or being regarded as having such impairment", with the clarification, "This holds even with the use of equipment designed to mitigate the disability", and that this definition is inclusive of episodic disabilities or chronic illnesses when these affect one or more major life activities. Models described include the moral model, which views disability as "an embodiment of evil, a punishment for a family member's or ancestor's transgression, a divine gift, a test of faith and opportunity to overcome a challenge", and scientific/medical traditions such as the biomedical model that "views disability as a medical problem that deviates from the norm" and the medical model described as "a linear sequence, that is, pathology to disease to disability", which "emphasizes finding a cure and relieving or eliminating symptoms caused by impairment". The guidance underscores practice implications, stating it is important for psychologists to understand the influence of their own and clients' paradigms for "establishing and maintaining a therapeutic relationship and weighing clinical decisions", to consider "compensatory applications of these interventions in individualized ways that support the specific client and their needs", and it highlights a primary weakness of the medical model as "its omission of social determinants of health", with many advocates arguing that it "devalues individuals as "patients".

---

### Changes in the distribution of fitness effects and adaptive mutational spectra following a single first step towards adaptation [^76625280]. Nature Communications (2021). High credibility.

The theoretical expectations on our detection threshold were set as in ref. The fitness detection threshold was determined by our definition of neutrality. We treated low abundance lineages as neutrals and used the decay of those lineages over time to infer population mean fitness. The resolution of our study cannot detect adaptive lineages with a fitness < 2% due to clonal inference and the lineage size limit (see Supplementary Fig. 36 in ref.). Certain mutations with fitness < 2% can establish in the population without being detected. However, such mutations never reach large sizes and will not dominate or have much of an impact on population dynamics. In particular, during the 1st step evolutions, population mean fitness reached 0.02 at generation 90. Due to clonal interference, mutations with s = 0.02 have to arise before generation 40 (90–1/s) in order to establish. To be detected, such mutations have to arise 60 generations before the start of the evolution experiment (from equation 77 in ref.;, wherecells/lineage represents the lineage size). Thus, mutations of such effect that occur immediately at generation 0, are nearly impossible to be detected unless this group of mutations has a very large mutational target size or a very high mutation rate. This is illustrated in detail in the Supplementary material in ref. section 11.1, page 54.

Barcode determination of isolated clones

To identify the barcodes of the isolated clones in 96-well plates, we employed a 2- (column, row) or 3- (column, row, plate) dimensional pooling strategy, inspired by ref. Briefly, we arranged 20 plates per batch into a 4 columns × 5 rows plate matrix and constructed 48 column pools from clones out of 40 wells each and 40 row pools from clones out of 48 wells each. For the second batch we included semi-redundant half-plate pools (40 pools from clones out of 48 wells each) to increase the successful barcode recovery rate. We pooled our samples after cell growth and prepared barcode libraries for Illumina sequencing. Barcodes were recovered for each well at a rate of ~90%, which was somewhat dependent on the barcode diversity of the sampled timepoint (identical barcodes in multiple wells makes it more challenging algorithmically to match barcodes to wells).

High-throughput fitness measurements and analysis

---

### Rise and fall of subclones from diagnosis to relapse in pediatric B-acute lymphoblastic leukaemia [^f8336336]. Nature Communications (2015). Medium credibility.

whereand λ k is the mixing parameter.

In turn, the joint probability distribution of data {(X i, N i)} can be written as

where X = { X i, i = 1,2,…, n }, N = { N i, i = 1,2,…, n } and Z = { Z ik, i = 1,2,…, n; k = 1,2,…, K }.

The likelihood of model parametersandgiven X and Z is defined as

The above likelihood is maximized using expectation maximization algorithm with Akaike information criterion (AIC) AIC = 2 k −2ln(L), where k = (n +2)−1.

For shared SNVs present in both D and R, the above binomial mixture model was modified to a multiplicative binomial mixture model as

whereand λ k is mixing parameter.

The joint probability distribution of datais written as

The likelihood of model parametersandgiven X, N and Z is defined as

where X, N and Z are defined similarly as above.

The above likelihood is maximized using an expectation maximization algorithm with the AIC = 2 k –2ln(L), where k = (n +3) K –1.

Each mutational cluster inferred from the binomial mixture models was labelled by genes involved in a significantly mutated pathway (Fig. 1) or a somatic variant with the highest coverage if there is no mutation in the selected pathway.

---

### Asynchronous locking in metamaterials of fluids of light and sound [^7c9acbc4]. Nature Communications (2023). High credibility.

Fig. 4
Polaromechanics: asynchronous locking of weakly coupled traps.

a, c Display the excitation power dependence of the polariton emission for an array of square traps of 1.6 μm lateral size separated by 3.2 μm-etched regions, and for two different spot positions (indicated at the bottom-right of each figure). The detuning between the ground states of the two neighbor traps is presented in b and d. In these panels the more pumped trap defines the zero of energy, and the detuning is given in units ofGHz, the fundamental cavity phonon mode. A histogram on the frequency of occurrence of the different detunings, obtained from an extensive series of experiments in arrays of 1, 1.3, and 1.6 μm square traps separated by 2, 2.6, and 3.2 μm barriers, respectively, is presented in e. f Shows the dressed detuning, obtained numerically using the model described in the Supplementary Note 8 B, with two phonons of frequenciesGHz andGHz, as a function of the bare detuning Δ ω and the reservoir-polariton interaction U R n 0. Regions where frequency locking occurs (at) are shown in red (for even values of n) and in green (odd values of n). The blue line highlights the behavior of the detuning for a given value of the interaction(to be compared with b and d). g Presents a histogram of dressed detuning obtained using a self-consistent model for the phonons (see text and the Supplementary Note 8 C). Note the emergence of peaks at integer values of n, as in the experimental histogram in e.

---

### International guideline development for the determination of death [^7577ef1b]. Intensive Care Medicine (2014). Low credibility.

Introduction and Methods

This report summarizes the results of the first phase in the development of international guidelines for death determination, focusing on the biology of death and the dying process, developed by an invitational forum of international content experts and representatives of a number of professional societies.

Results and Conclusions

Precise terminology was developed in order to improve clarity in death discussion and debate. Critical events in the physiological sequences leading to cessation of neurological and/or circulatory function were constructed. It was agreed that death determination is primarily clinical and recommendations for preconditions, confounding factors, minimum clinical standards and additional testing were made. A single operational definition of human death was developed: 'the permanent loss of capacity for consciousness and all brainstem functions, as a consequence of permanent cessation of circulation or catastrophic brain injury'. In order to complete the project, in the next phase, a broader group of international stakeholders will develop clinical practice guidelines, based on comprehensive reviews and grading of the existing evidence.

---

### Genome-wide association study of childhood B-cell acute lymphoblastic leukemia reveals novel African ancestry-specific susceptibility loci [^32b70cf3]. Nature Communications (2025). High credibility.

Functional annotation of genetic credible sets and colocalization

Consistent with research practices described in previously published ALL GWAS meta-analyses, we conducted preliminary fine mapping and broadly annotated the potential molecular consequences of putative risk variants, with the goal of providing comprehensive evidence identifying the most reasonable gene targets for each genetic variant of interest. We constructed 99% credible intervals, i.e. sets of variants at each locus with a 99% posterior probability of containing the causal variant, for each distinct B-ALL risk signal using a Bayesian approachto define genomic regions with 99% credible interval coverage and permit broader functional annotation of novel B-ALL risk loci. With meta-analysis summary statistics, we calculated approximate Bayes factors given bywhere β k and σ k are the per allele log(OR) and standard error of variant k, respectively, andusing a Gaussian prior N (0,2 2). We then computed the posterior probability that the k th variant is causal given by. The 99% credible set was then constructed by ordering variants by their posterior probabilities from highest to lowest and including variants until the cumulative posterior probability was 0.99.

---

### NCCN guidelines® insights: bladder cancer, version 3.2024 [^8ca23d01]. Journal of the National Comprehensive Cancer Network (2024). High credibility.

Regarding follow-up and surveillance for bladder cancer - NCCN, more specifically with respect to follow-up (post-bladder sparing), NCCN 2024 guidelines recommend to consider obtaining less frequent surveillance (eg, cystoscopy every 6 months, extended to annually over time) in patients who are not eligible for aggressive therapy.

---

### Standards and guidelines for validating next-generation sequencing bioinformatics pipelines: a joint recommendation of the Association for Molecular Pathology and the college of American pathologists [^101e0942]. The Journal of Molecular Diagnostics (2018). Medium credibility.

Minimum number of variants and calculation — For in-scope variants, they fall into one of three categories: SNVs, indels, and other; the last category includes variants that cannot be classified as SNVs or indels, such as vertically complex variants that are ≤ 21 bp in contiguous size or variants that require special algorithmic approaches; for each category in scope, the minimum number of variants to include in the validation sample set can be calculated as n = ln(1 – CI)/ln(P), where n is the number of variants to be included, CI is the confidence level of detection, and P is the probability of detection.

---

### A framework for the development and interpretation of different sepsis definitions and clinical criteria [^a87135b7]. Critical Care Medicine (2016). Low credibility.

Although sepsis was described more than 2,000 years ago, and clinicians still struggle to define it, there is no "gold standard", and multiple competing approaches and terms exist. Challenges include the ever-changing knowledge base that informs our understanding of sepsis, competing views on which aspects of any potential definition are most important, and the tendency of most potential criteria to be distributed in at-risk populations in such a way as to hinder separation into discrete sets of patients. We propose that the development and evaluation of any definition or diagnostic criteria should follow four steps: 1) define the epistemologic underpinning, 2) agree on all relevant terms used to frame the exercise, 3) state the intended purpose for any proposed set of criteria, and 4) adopt a scientific approach to inform on their usefulness with regard to the intended purpose. Usefulness can be measured across six domains: 1) reliability (stability of criteria during retesting, between raters, over time, and across settings), 2) content validity (similar to face validity), 3) construct validity (whether criteria measure what they purport to measure), 4) criterion validity (how new criteria fare compared to standards), 5) measurement burden (cost, safety, and complexity), and 6) timeliness (whether criteria are available concurrent with care decisions). The relative importance of these domains of usefulness depends on the intended purpose, of which there are four broad categories: 1) clinical care, 2) research, 3) surveillance, and 4) quality improvement and audit. This proposed methodologic framework is intended to aid understanding of the strengths and weaknesses of different approaches, provide a mechanism for explaining differences in epidemiologic estimates generated by different approaches, and guide the development of future definitions and diagnostic criteria.

---

### Evaluation after a first seizure in adults [^b94f7748]. American Family Physician (2022). High credibility.

Regarding diagnostic investigations for first seizure in adults, more specifically with respect to initial assessment, AAFP 2022 guidelines recommend to evaluate for provoking factors after a first seizure, such as inflammatory, infectious, structural, toxic, or metabolic causes.

---

### Standards of care in diabetes – 2025 [^9a2ec52c]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for hypoglycemia, more specifically with respect to elderly patients, ADA 2025 guidelines recommend to individualized goal levels for the treatment of hypertension in most older patients.

---

### Randomised phase 1 clinical trials in oncology [^dc10bfe5]. British Journal of Cancer (2021). Medium credibility.

The aims of Phase 1 trials in oncology have broadened considerably from simply demonstrating that the agent/regimen of interest is well tolerated in a relatively heterogeneous patient population to addressing multiple objectives under the heading of early-phase trials and, if possible, obtaining reliable evidence regarding clinical activity to lead to drug approvals via the Accelerated Approval approach or Breakthrough Therapy designation in cases where the tumours are rare, prognosis is poor or where there might be an unmet therapeutic need. Constructing a Phase 1 design that can address multiple objectives within the context of a single trial is not simple. Randomisation can play an important role, but carrying out such randomisation according to the principles of equipoise is a significant challenge in the Phase 1 setting. If the emerging data are not sufficient to definitively address the aims early on, then a proper design can reduce biases, enhance interpretability, and maximise information so that the Phase 1 data can be more compelling. This article outlines objectives and design considerations that need to be adhered to in order to respect ethical and scientific principles required for research in human subjects in early phase clinical trials.

---

### VA / DoD clinical practice guideline for management of posttraumatic stress disorder and acute stress disorder [^a105391d]. DoD/VA (2023). High credibility.

Algorithm — VA/DoD Clinical Practice Guideline for Management of Posttraumatic Stress Disorder — states that this clinical practice guideline's algorithm is designed to facilitate understanding of the clinical pathway and decision-making process used in managing patients with PTSD and represents a simplified flow that helps foster efficient decision making by providers; it includes steps of care in an ordered sequence, decisions to be considered, decision criteria recommended, and actions to be taken. The algorithm is a step-by-step decision tree in which standardized symbols display each step and arrows connect the numbered boxes indicating the order in which the steps should be followed; sidebars 1–11 provide more detailed information to assist in defining and interpreting elements in the boxes. The legend specifies that rounded rectangles represent a clinical state or condition, hexagons represent a decision point in the process of care formulated as a question that can be answered Yes or No, rectangles represent an action in the process of care, and ovals represent a link to another section within the algorithm, and Appendix H contains alternative text descriptions of the algorithms.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^b6486558]. CDC (2011). Medium credibility.

Reducing HIV-Related Disparities and Health Inequities — community viral load actions include Step 1.1 "Ensure that high-risk groups have access to regular viral load and CD4 tests" and Step 2.2 "Ensure that all high prevalence localities are able to collect data necessary to calculate community viral load, measure the viral load in specific communities, and reduce the viral load in those communities where HIV incidence is high".

---

### Theory and rationale of interpretable all-in-one pattern discovery and disentanglement system [^7a7dd585]. NPJ Digital Medicine (2023). Medium credibility.

Methods

Introduction

Before we present the notations, definitions, and the equations of PDD, we first give a high-level conceptual view of the methodology. The following items (labeled with the header M) represent the methodologies we developed in support of the PDD paradigm. We link them to the definitions, algorithmic steps, and experimental evidence. Specifically, we, via M1, use statistical residual (SR) (Definition 1), SR-Matrix and AV-Vector (Step 1 and Definition 2), Principal Component Analysis (Step 2) to obtain a small set of Statistically Significant Association Disentangled Spaces DS (Step 2, Step 3, and Definition 6).

M2, obtain from each DS AV clusters (Step 3 Definition 8) and statistically significant attribute association patterns (Step 5, Definition 10) and pattern clusters (Step 5) without class labels given, or the need for feature engineering and machine training (Step 6 and Step 7) — achieving AV clustering, pattern discovery, pattern clustering and entity clustering in an All-in-One System.

M3, use the intersection of AV Entity ID lists (Definition 3, Fig. 1c, d) of AVs in an AV Group (Definition 7) from the Entity ID Address Table (created at the outset in Step 1, Definition 3) to obtain the frequency count of AV Groups for Frequency-Matrix construction (Step 1) and pattern discovery (Step 5, Fig. 1d) — shifting the search of association or discriminative information or patterns from entities employed in the existing ML models to a direct simple table-lookup and intersecting computation process for pattern discovery and class association — a great complexity reduction of the search process in ML.

M4, use dataset with class labels to exploit the ground truth, and dataset without class label to minimize label bias to obtain auto-error-correcting class associations (Step5, Step 6 Step 7 and Definition 10) and entity clusters (Step 5, Step 6, Step 7 and Definition 11) — an unique way of combining the strength of supervised and unsupervised methods in auto-correcting class association and entity clustering;

---

### Standards of care in diabetes – 2025 [^31be65a6]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for hypoglycemia, more specifically with respect to screening for cognitive impairment, ADA 2025 guidelines recommend to simplify diabetes treatment plans as much as possible for patients with cognitive impairment and tailor to minimize the risk of hypoglycemia.

---

### CONSORT-DEFINE explanation and elaboration: recommendations for enhancing reporting quality and impact of early phase dose-finding clinical trials [^44f95af1]. EClinicalMedicine (2025). Medium credibility.

Example 1

This example is from Figure 1 a in Zhang et al. used under CC BY 4.0, cropped from original (Fig. 1 a).

Fig. 1
(a) Item 3a.2, trial design schema to show the flow of major transition points, Example 1 — obtained from Figure 1a in Zhang et al.used under CC BY 4.0, cropped from original; (b) Item 3a.2, trial design schema to show the flow of major transition points, Example 2 — obtained from Online Resource 1 in Lee et al.used under CC BY 4.0; (c) Item 3a.3, statistical methods or rationale underpinning the trial design, Example 2 — obtained from Supplementary Figure S1 in the online Supplementary Appendix of Hutchings et al.reprinted from The Lancet with permission from Elsevier.

Example 2

This example is from Online Resource 1 in Lee et al. used under CC BY 4.0 (Fig. 1 b).

Explanation

Planning dosing strategies in EPDF trials can be complex depending on the research context, adaptive trial design features, and methods considered. EPDF trials are increasingly designed to seamlessly address multiple objectives that span multiple transition points of clinical research (e.g. dose escalation to [multiple] expansion cohort(s), phase I to II, single ascending dose to multiple ascending dose). The increasing complexity of trial design and dosing strategies can be challenging for readers to comprehend.

Hence, authors are encouraged to provide a graphical representation of the overall schema of the proposed trial, when possible, to show the timing of major reviews and decision points, highlighting any overlap between study cohorts and parts to help the reader interpret the logical stages of the process (see Example 2). When a trial consists of different parts, the trial design schema should show the timing and criteria of major transition or progression points (e.g. dose escalation to expansion, dose escalation to dose optimisation (particularly as outlined in projects like the FDA Optimus project, which focuses on selected tolerable doses to ensure they are active and tolerable), phase I to phase II, single ascending dose to multiple ascending dose, stage 1 to stage 2 with a formal interim analysis for futility/activity, monotherapy to combination regimen, or exploration of an alternative administration schedule or route).

Item 3a.3 [new] Statistical methods or rationale underpinning the trial design

---

### Next-generation sequencing for infectious disease diagnosis and management: a report of the Association for Molecular Pathology [^98572262]. The Journal of Molecular Diagnostics (2015). Medium credibility.

Bioinformatics filtering and causality considerations for NGS-based viral discovery note that "This filtering step is crucial because viral sequences may comprise < 1% of the initial aligned reads", and that "demonstrating the presence of a virus in a patient with disease does not automatically imply pathogenicity". The page states that proving causation has "depended on fulfilling Koch's postulates: a putative etiologic agent is found in affected hosts but not healthy controls, it is propagated in culture, and it can reproduce the disease when a healthy host is inoculated", yet "many viruses cannot be cultured", prompting revised guidelines that "eliminate the requirement for microorganism isolation" while increasing rigor in linking microorganism to disease.

---

### Predicting of elderly population structure and density by a novel Grey fractional-order model with theta residual optimization: a case study of shanghai city, China [^9ff08664]. BMC Geriatrics (2023). Medium credibility.

Methods

In this chapter, the definition of traditional fractional-order grey model is introduced, and then a Grey Theta fractional-order grey model (GT-FGM) based on residual optimization is established. Furthermore, the detailed modeling process and hyperparameter optimization method are given, and the innovation and improvement of the model are discussed.

Basic model theory

The GM(1,1) model is the basic model of grey prediction theory and is widely used. The model builds a model through systematic behavioral data sequences, and can effectively predict and simulate data in the case of small samples. The GM(1,1) model is based on the grey system theory, through the continuous processing of discrete data, the differential equation is used to replace the difference equation. The new accumulated time sequence is used to replace the original time sequence, and then the differential equation is established. The fractional-order GM(1,1) model (FGM(1,1)) converts the first-order cumulative generation in the traditional GM(1,1) model into fractional-order cumulative generation, and uses the fractional order to effectively weaken the randomness of the original data sequence. It can improve the prediction accuracy of the model and reduce the disturbance of the model solution.

The modeling process is as follows:

Step 1: Suppose the sequence of the original data is, its corresponding r-order accumulation sequence is, where stipulating.

For the background value generation of, a sequence close to mean generation is, where,

Step 2: The grey differential equation of r-order cumulative GM(1,1) model is

Its corresponding whitening differential equation can be expressed aswhereis the development coefficient andis the grey effect. The time response function can be obtained by solving the above differential equation

Step 3: Based on the principle of minimum sum of squares of errors, can be calculated by the least square method aswhere,

Step 4: Substitutingandinto the equation, the time response function of the original sequence can be obtained aswhereis the fitting value at the time of, and the sequence is

Step 5: The sequenceis subjected to-order subtractive reduction, then the fitting sequence of the original data iswhere,

The fitting value of the original data sequence can be obtained as, and the predicted value is.

---

### Defining the time-limited trial for patients with critical illness: an official American Thoracic Society workshop report [^cf1ee11e]. Annals of the American Thoracic Society (2024). High credibility.

Time-limited trial in critical care — essential elements and phased process were derived through a Delphi process: investigators "identified 18 potential steps" and found first-round consensus that "11 of these were essential elements"; during the second round, they "identified seven additional essential elements". The committee then "combined 4 related items into two steps (to reach 16 essential elements)" and "organized steps into four phases of care: consider, plan, support, and reassess". Additional steps "may be helpful in some cases, but not necessary for all trials", and involvement of other disciplines is "highly dependent on a patient's specific situation and on the available hospital resources".

---

### Reciprocal learning between military and civilian surgeons: past and future paths for medical innovation [^213acb9b]. Annals of Surgery (2021). Medium credibility.

DCS can be understood as a 3-phase procedure (Fig. 1). The first step emphasizes rapid control of hemorrhage and contamination with abbreviated procedures; the second step focuses on life support and restoring physiology; the third step, once the patient is stable, completes the sequence with definitive surgical repairs. Promising results from the use of DCS in civilian surgical centers later led military surgeons to embrace it as a new paradigm of care. Military surgeons were placed at civilian trauma care centers to learn the new technique, and DCS was then successfully implemented during the conflicts in Iraq and Afghanistan. Additional surgical techniques to abbreviate the time to control bleeding and restore perfusion to vital structures came forth, and were widely applied by military surgeons and other specialties. To control blood vessels and re-establish perfusion, temporary vascular shunts had been used in civilian injuries. Subsequently, use of temporary vascular shunts for war injuries allowed for efficient patient resuscitation whereas maintaining perfusion of the extremities. In the decades before DCS, during WWII, arterial injuries were treated by immediate ligation and had a 49% amputation rate. In contrast, the DCS technique temporized limb perfusion allowing interval limb revascularizations and led to limb salvage rates of > 90%. More broadly, the concept of DCS was useful in the surgical team's ability to handle mass casualty in settings with limited resources.

FIGURE 1
The original 3-phase damage control proposed by the surgical team at the University of Pennsylvania. The surgeons advocated for a systematic approach designed to disrupt the lethal cascade of events leading to the death by exsanguination. Source: Rotondo MF, and Zonies DH. The damage control sequence and underlying logic. Surgical Clinics of North America, 1.997000000000000e+03

Many other innovations have resulted from military-civilian partnerships, such as breakthroughs in burn trauma care at the U.S. Army Institute of Surgical Research, and most recently, bystander and prehospital tourniquet use. Some of these initiatives are summarized in Table 1.

TABLE 1
Selected Examples of Military-Civilian Exchanges that Lead to Innovations in Patient Care, Advances of Scientific Understanding and Policy Change

---

### VA / DoD clinical practice guideline for the management of type 2 diabetes mellitus [^8da629fa]. VA/DoD (2023). High credibility.

Algorithm — This clinical practice guideline's algorithm is designed to facilitate understanding of the clinical pathway and decision-making process used in managing patients with T2DM. It represents a simplified flow of the management of patients with T2DM and helps foster efficient decision making by providers, and it includes steps of care in an ordered sequence, decisions to be considered, decision criteria recommended, and actions to be taken. The algorithm is a step-by-step decision tree; standardized symbols display each step, arrows connect the numbered boxes indicating the order in which the steps should be followed, and sidebars 1–8 provide more detailed information to assist in defining and interpreting elements in the boxes.

---

### Colorectal cancer screening and prevention [^c270f773]. American Family Physician (2025). High credibility.

Regarding screening and diagnosis for colon cancer, more specifically with respect to indications for screening, general population, aged 76–85 years, AAFP 2025 guidelines recommend to consider obtaining screening for CRC in adults aged 76–85 years at average risk based on overall health status, prior screening history, and patient preferences.

---

### Standards and guidelines for validating next-generation sequencing bioinformatics pipelines: a joint recommendation of the Association for Molecular Pathology and the college of American pathologists [^c00ae56b]. The Journal of Molecular Diagnostics (2018). Medium credibility.

Variant calling — definition and inputs: Variant calling is the process of accurately identifying the differences or variations between the sample and the reference genome sequence, and the typical input is a set of aligned reads in BAM or another similar format traversed by the variant caller to identify sequence variants; variant calling is a heterogeneous collection of algorithmic strategies based on the types of sequence variants, such as single-nucleotide variants (SNVs) and small insertions and deletions.

---

### 2017 ACC / AHA / HRS guideline for the evaluation and management of patients with syncope: a report of the American college of cardiology / American Heart Association task force on clinical practice guidelines and the Heart Rhythm Society [^2b979233]. Heart Rhythm (2017). Medium credibility.

Regarding diagnostic investigations for syncope, more specifically with respect to initial ECG, ACC/AHA/HRS 2017 guidelines recommend to obtain a resting 12-lead ECG as part of the initial evaluation of patients with syncope.

---

### SeqSharp: a general approach for improving cycle-sequencing that facilitates a robust one-step combined amplification and sequencing method [^fb8fd97e]. The Journal of Molecular Diagnostics (2010). Low credibility.

Sequencing a specific DNA element within a genome or a complex mixture of DNA by the Sanger sequencing method generally involves PCR-mediated amplification of target DNA with forward and reverse primers, followed by a sequencing reaction directed from a single primer. To minimize the contribution of fluorescent signal due to the extension products originating from primers carried over from the amplification step, an intermediate step is routinely incorporated to remove the excess primers before proceeding to the sequencing reaction. We have developed a method called SeqSharp that removes noise in the sequencing data by enzymatically removing chain termination products originating from one or both of the amplification primers. This method substantially improves the quality of sequence information even without an intermediate primer removal step. Importantly, we show that SeqSharp significantly improves the sequence quality from a combined (one-step) amplification/sequencing protocol and provides a more robust method that, unlike previously described one-step sequencing methods, yields high quality sequence data from a single reaction by using equimolar primer concentrations. One-step SeqSharp is generally applicable and produced excellent sequence data from bacterial, fungal, and human DNA.

---

### Are there circumstances in which phase 2 study results should be practice-changing? [^293b9755]. Hematology: American Society of Hematology. Education Program (2007). Low credibility.

New pharmaceuticals, innovative combinations of approved agents, and novel treatment modalities have resulted in a marked increase in the need for clinical trials. Evidence for treatment efficacy is best derived from large phase 3 randomized, controlled clinical trials. However, phase 3 investigations are lengthy and expensive, and consume patient resources. Furthermore, some diseases and treatment indications are rare, and adequate numbers of patients for a definitive phase 3 trial do not exist. Consequently, it is imperative for clinicians to understand phase 2 trial design, since their interpretation is required to apply the findings in clinical practice appropriately. The complexity of phase 2 studies is explored, including unique designs, possible use of randomization, and other key elements necessary for interpretation of phase 2 trials. Specific examples and application of these concepts are discussed in this review.

---

### Evaluation after a first seizure in adults [^59ccbaac]. American Family Physician (2022). High credibility.

Regarding diagnostic investigations for first seizure in adults, more specifically with respect to initial assessment, AAFP 2022 guidelines recommend to assess for history of medication or substance use that can provoke seizures through normal use, withdrawal, overdose, drug-drug interaction, or impaired metabolism due to comorbidities.

---

### Screening for speech and language delay and disorders in children: US preventive services task force recommendation statement [^cffd6e3b]. JAMA (2024). Excellent credibility.

Definitions of delay and disorder in speech and language: Speech or language delay refers to development of speech and language in the correct sequence but at a slower rate than expected, and performance on a standardized assessment that falls at least 1 standard deviation below the mean is often considered a delay. Speech or language disorders refer to speech or language ability that is qualitatively different from typical development; speech disorders are defined by difficulty with forming specific sounds or words correctly (articulation) or making words or sentences flow smoothly (fluency), while language disorders are characterized by difficulty understanding (receptive language) and/or using (expressive language) language relative to a child's peers.

---

### KDIGO 2021 clinical practice guideline for the management of glomerular diseases [^a2767dfa]. Kidney International (2021). High credibility.

KDIGO methods — GRADE system for grading quality of evidence outlines a stepwise process with "Step 1: Starting grade for quality of evidence based on study design", "Step 2: reduce grade", and "Step 3: raise grade", followed by "Final grade for quality of evidence and definition" that provides the standard GRADE definitions: "High — Further research is unlikely to change confidence in the estimate of the effect", "Moderate — Further research is likely to have an important impact on confidence in the estimate, and may change the estimate", "Low — Further research is very likely to have an important impact on confidence in the estimate, and may change the estimate", and "Very Low — Any estimate of effect is very uncertain".

---

### Otoplasty: sequencing the operation for improved results [^e72499ab]. Plastic and Reconstructive Surgery (2005). Low credibility.

Learning Objectives

After studying this article, the participant should be able to: 1. Understand the anatomy and embryology of the external ear. 2. Understand the anatomic causes of the prominent ear. 3. Understand the operative maneuvers used to shape the external ear. 4. Be able to sequence the otoplasty for consistent results. 5. Understand the possible complications of the otoplasty procedure. Correction of prominent ears is a common plastic surgical procedure. Proper execution of the surgical techniques is dependent on the surgeon's understanding of the surgical procedure. This understanding is best founded on an understanding of the historical bases for the operative steps and the execution of these operative steps in a logical fashion. This article describes the concept of sequencing the operation of otoplasty to produce predictable results combining the technical contributions from many authors. The historical, embryological, and anatomic bases for the operation are also discussed. Finally, the authors' preferred techniques are presented. Sequencing the steps in the preoperative assessment, preoperative planning, patient management, operative technique, and postoperative care will produce reproducible results for the attentive surgeon. Careful attention to the details of the operation of otoplasty will avoid many postoperative problems.

---

### Use of endocrine therapy for breast cancer risk reduction: ASCO clinical practice guideline update [^ec006ee8]. Journal of Clinical Oncology (2019). High credibility.

Regarding preventative measures for breast cancer, more specifically with respect to chemoprophylaxis, ASCO 2019 guidelines recommend to evaluate patients for baseline fracture risk and measure bone mineral density prior to initiating an aromatase inhibitor. Recognize that multiple studies have reported an increased rate of bone loss in women treated with aromatase inhibitors. Use anastrozole with caution in postmenopausal women with moderate bone mineral density loss, and if it is used, consider the use of bone-protective agents such as bisphosphonates and RANK ligand inhibitors. Encourage all patients receiving aromatase inhibitors to exercise regularly and take adequate calcium and vitamin D supplements. Recognize that a history of osteoporosis and/or severe bone loss is a relative contraindication for the use of anastrozole. In the International Breast Intervention Study-II, women with severe osteoporosis (T score < -4 or more than two vertebral fractures) were excluded. Recognize that other endocrine preventive therapies that do not reduce bone density, such as tamoxifen or raloxifene, are also available for this group of women.

---

### Best practices research [^e22572bf]. Family Medicine (2003). Low credibility.

"Best practices research", described in this paper, refers to a systematic process used to identify, describe, combine, and disseminate effective and efficient clinical and/or management strategies developed and refined by practicing clinicians. It involves five steps: development of a conceptual model or series of steps, definition of "best" based on values and standards, identification and evaluation of potentially effective methods for each component or step, combination of most-effective methods, and testing of combined methods. The chronological development of this process is described with case examples, and the methodological steps are discussed.

---

### 2021 ACC / AHA key data elements and definitions for heart failure: a report of the American college of cardiology / American Heart Association task force on clinical data standards (writing committee to develop clinical data standards for heart failure) [^b111bcc2]. Circulation: Cardiovascular Quality and Outcomes (2021). High credibility.

Cardiovascular history — angina class permissible values and definitions include: Class 0 asymptomatic; Class I where ordinary physical activity such as walking or climbing stairs does not cause angina and angina occurs with strenuous, rapid, or prolonged exertion; Class II with slight limitation of ordinary activity in which angina occurs on walking or climbing stairs rapidly and includes thresholds of walking > 2 blocks on the level and climbing > 1 flight of ordinary stairs at a normal pace and in normal conditions; Class III with marked limitation of ordinary physical activity in which angina occurs on walking 1 to 2 blocks on the level and climbing 1 flight of stairs in normal conditions and at a normal pace; Class IV with inability to perform any physical activity without discomfort, where angina symptoms may be present at rest; and Unknown indicating a proper value is applicable but not known.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^f02e2b98]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Potential sources of error affecting next-generation sequencing assays designed for formalin-fixed, paraffin-embedded tissue — Table 1 lists steps with assay design considerations and quality assessment during validation, including DNA yield with Optimize extraction and Measure yield, DNA purity and integrity with Optimize DNA library preparation and Monitor DNA library preparation, Contamination with Change blades during tissue dissection and No template control, Stochastic bias with Increase input, multiple displacement amplification, single-molecule barcoding and Sensitivity control, Amplification errors with High-fidelity polymerase, duplex reads and Confirm all positives with orthogonal method, Capture bias with Optimize enrichment, long-range PCR and Define minimum coverage, back-fill with orthogonal method, and Primer bias and allele dropout with Assess causes of false negatives, design overlapping regions and Bioinformatically flag homozygosity of rare variants; the table notes This is not a comprehensive list.

---

### Balancing false positives and false negatives for the detection of differential expression in malignancies [^83737245]. British Journal of Cancer (2004). Low credibility.

Data sets

We applied the methodology described above on microarray data originating from two sources that contain measurements from two or three classes of acute leukaemia.

The first data set studies the expression profiles of bone marrow or peripheral blood of 72 patients with acute lymphoblastic (ALL; Condition 1; 47 patients) or myeloid leukaemia (AML; Condition 2; 25 patients) using an Affymetrix chip. In the original publication the patients are divided into training and a test set, but this distinction is not important here. Although the separation between the two conditions is more pronounced than in most other cases, this data set can still be considered as a benchmark (paper cited over 1090 times). The data contains N = 7129 genes and can be downloaded from.

The second data set also contains several microarray experiments obtained from patients with ALL or AML and from a third class or condition containing ALLs with an MLL translocation (called MLL leukaemia). Armstrong et al discovered that MLL leukaemias have a distinct expression pattern and can be considered as a separate disease distinguishable from ALL and AML. The data set they used is publicly available and can be downloaded from. It contains expression profiles for 12 582 genes measured using Affymetrix technology. In total, 24 ALL patients, 28 AML patients and 20 MLL patients are available. This resulted in a data set containing 72 patients.

---

### Evidence-based guideline: management of an unprovoked first seizure in adults: report of the guideline development subcommittee of the American Academy of Neurology and the American Epilepsy Society [^ff9ee1a2]. Neurology (2015). Medium credibility.

Regarding patient education for first seizure in adults, more specifically with respect to general counseling, AAN/AES 2015 guidelines recommend to inform patients presenting with an unprovoked first seizure that the likelihood of a recurrent seizure is highest within the first 2 years after the initial seizure, ranging from 21% to 45%.

---

### Large-scale DNA-based phenotypic recording and deep learning enable highly accurate sequence-function mapping [^1f3d1625]. Nature Communications (2020). High credibility.

RBS library design

Initial efforts for training a convolutional neural network (CNN)based on the proof-of-concept data set resulted in a systematic underestimation of RBS strength, in particular for strong RBSs. This is likely due to the library being skewed towards weak sequences as a result of the full randomization of the 17 bases upstream of the Bxb1 start codon (Supplementary Fig. 7). To overcome this, three libraries (High1-3) presumably enriched in moderate-to-strong RBSs were designed in silico based on the proof-of-concept data set and added to a fully randomized library (N 17). Libraries High1 and High2 were designed using position probability matrices (PPMs), 2D matrices in which each element represents the proportion of times a nucleotide occurs at a given position in the sequence. To this end, RBSs from the proof-of-concept data set were grouped into 10 linearly distributed bins according to a proxy for the normalized integral of their flipping profile (IFP trz, contained in [0, 1]), for each of which a PPM was computed. The IFP trz was computed using the trapezoidal rule on the flipping profiles. Degenerate RBS sequences for High1 and High2 were designed with the goal to obtain PPMs that most closely resemble (minimal mean-squared error) the PPMs of the highest and second highest bin, respectively. Library High3 was designed using a genetic algorithm on the basis of predictions from an initial CNN trained on the proof-of-concept data set. The RBS sequences from the three highest IFP trz bins were randomly mutated for 200 iterations (1–2 mutations per sequence and iteration). Only sequences for which the predicted IFP trz was increased due to the mutations were propagated to the next iteration. At the end of this process, we calculated the PPM of the resulting pool of sequences with high predicted IFP trz, randomly selected 20,000 sequences from this PPM, and computed the predicted IFP trz distribution for this sub-sample. Finally, the degenerate RBS sequence of High3 was obtained by greedily minimizing the Kolmogorov–Smirnov distance between the predicted IFP trz distribution of the sub-sample and the predicted IFP trz distribution for the respective degenerate candidate RBS sequence. For further details regarding the computation of the IFP trz, the CNN and the genetic algorithm please refer to the ML Annex.

---

### Standards of care in diabetes – 2025 [^5c059b62]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for hypertension, more specifically with respect to patients with diabetes mellitus, lifestyle modifications, ADA 2025 guidelines recommend to advise lifestyle interventions in patients with BP > 120/80 mmHg, consisting of weight loss when indicated, a DASH-style eating pattern (including reducing sodium and increasing potassium intake), moderation of alcohol intake, smoking cessation, and increased physical activity.

---

### 2025 ACC / AHA / ACEP / NAEMSP / SCAI guideline for the management of patients with acute coronary syndromes: a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^ffad25f5]. Circulation (2025). High credibility.

Regarding diagnostic investigations for ST-elevation myocardial infarction, more specifically with respect to ECG, ACC/ACEP/AHA/NAEMSP/SCAI 2025 guidelines recommend to obtain and interpret a 12-lead ECG within 10 minutes of first medical contact in patients with suspected ACS to identify patients with STEMI and guide management.

---

### Tuberous sclerosis complex surveillance and management: recommendations of the 2012 international tuberous sclerosis complex consensus conference [^3a86d3fd]. Pediatric Neurology (2013). Medium credibility.

Regarding follow-up and surveillance for tuberous sclerosis complex, more specifically with respect to follow-up of cerebral involvement, TSCi 2013 guidelines recommend to obtain routine EEG in patients with known or suspected seizure activity. Determine the frequency of routine EEG by clinical need rather than specific defined intervals. Consider obtaining a prolonged video EEG ≥ 24 hours when seizure occurrence is unclear or when unexplained sleep, behavioral changes, or other alteration in cognitive or neurological function is present.

---

### Acute lymphoblastic leukemia, version 2.2024, NCCN clinical practice guidelines in oncology [^a53cc048]. Journal of the National Comprehensive Cancer Network (2024). High credibility.

Regarding screening and diagnosis for acute lymphoblastic leukemia - NCCN, more specifically with respect to diagnosis, NCCN 2024 guidelines recommend to base criteria for classification of mixed phenotype acute leukemia on the WHO 2022 and ICC 2022 criteria. Recognize that in acute lymphoblastic leukemia, myeloid-associated antigens such as CD13 and CD33 may be expressed, and the presence of these myeloid markers does not exclude the diagnosis of acute lymphoblastic leukemia, nor is it associated with adverse prognosis.

---

### It's ALL in the diagnosis [^8dd31135]. Cancer Cell (2002). Low credibility.

The molecular diagnosis of human cancer will hasten the development of treatments tailored to the abnormalities present in each patient's tumor cells. Recent gene expression profiling studies of pediatric acute lymphoblastic leukemia (ALL) suggest that the molecular diagnosis of these diseases is right around the corner.

---

### IgE autoreactivity in bullous pemphigoid: eosinophils and mast cells as major targets of pathogenic immune reactants [^d5c9c4be]. The British Journal of Dermatology (2017). Low credibility.

To uncover the dominant epitopes for both IgG and IgE and gain a general understanding of how these specific immunoglobulins relate to each other, ELISAs were performed using short peptide sequences of the NC16A and C‐terminal regions of BP180. The results obtained showed that, in patients with BP, both IgG and IgE α‐BP180 antibodies recognized a particular stretch of NC16A in an overlapping fashion (Fig. 1 c). Such a BP‐specific motif was not seen in the sera of HC, one‐third of which also contained low levels of α‐BP180 antibodies of the IgG and IgE classes. As shown in Figure 1 c, the antibodies from HC seem to recognize different portions of the molecule at random, without any apparent pattern or overlap between the two immunoglobulin classes. Although these results are strongly suggestive of a competition phenomenon between IgG and IgE from patients with BP, we were not able to definitively demonstrate via ELISA that preincubation with purified BP IgG leads to a statistically significant reduction of the specific IgE signal (Fig. S3; see Supporting Information). However, it should be noted that the α‐BP180 IgE signal tended to be lower when the sera incubation was preceded by a blocking step with α‐BP180 IgG vs. nonspecific BP IgG.

---

### Curriculum development: a how to primer [^55349fb6]. Family Medicine and Community Health (2019). Medium credibility.

Step 1. Needs assessment/statement

The needs assessment helps us answer 'Why'? In the case of curriculum development, the answer may be quite broad and should point to the distinction between the current teaching strategy surrounding a learning need and what should be changed about it. At the start, it is wise to consider whose needs are the priority. This may start with a learner's needs (either attitudinal and knowledge-based needs, readiness to learn or timing), but likely extends to the patients and communities for whom the learner will be caring. When justifying time or funding, an articulation of how this curriculum might meet regulatory or board requirements can be useful.

The mechanics of a needs assessment includes readily available information and the collection of new information. The acquisition of new information can be structured (survey or medical knowledge assessment), semi-structured (series of discussions or a call to action based on sentinel event), research/data driven (data on learners' performance or clinical quality data) or based on regulatory requirements.

A very basic example (see table 1), experienced by one of the authors was the identification of a gap in knowledge leading to the development of a newly structured educational activity.

Once the needs assessment is finalised, and the needs have been articulated, a rationale statement should be agreed on. This rationale statement is 1–3 lines that articulates the fundamental findings from the needs assessment to guide the development of the curriculum. The rationale statement can then be used to keep the curriculum on task. It is intended to be modified only if there is a serious oversight in the development. In this way, the needs assessment and rationale statement can truly render a solid foundation for the curriculum in development.

Step 2. Determining and prioritising content

This is the first step in beginning to articulate what is going to be included, a general description of the content, along with a prioritisation of that content. In the example of the communication skills curriculum referenced above, the content was determined both by working backwards from the milestone goals and also from reviewing what experts in the field have identified. In some cases, there will not be expert knowledge or milestones to work from, and in these cases, original research might be needed, such as surveys of experts in the field, or analysis of conversation around a difficult topic needing to be addressed. An example of this last strategy can be seen in a recent publication on addressing the topic of racism in medical education (see table 1).

---

### Cyclosporine (Verkazia) [^382ebe7e]. FDA (2025). Medium credibility.

Using Verkazia

Follow the instructions (Step 1 to Step 12) each time you use Verkazia. If there is anything that you do not understand, ask your healthcare provider.

Step 1. Wash your hands.

Step 2. Open the aluminum pouch, which contains 5 single-dose vials.

Step 3. Take 1 vial from the aluminum pouch. Leave the remaining vials in the pouch.

Step 4. Gently shake the vial.

Step 5. Twist off the cap (see Figure A).

Figure A

Step 6. Pull down your lower eyelid (see Figure B).

Figure B

Step 7. Tilt your head back and look up.

Step 8. Gently squeeze 1 drop of the medicine onto your eye. Make sure that the tip of the vial does not touch your eye. If a drop misses your eye, try again.

Step 9. Blink a few times so that the medicine spreads across your eye.

Step 10. After using Verkazia, press the corner of your eye closest to your nose and gently close the eyelid for 2 minutes (see Figure C).

Figure C

Step 11. If you need to use drops in both eyes, repeat Steps 6 to 10 for your other eye. There is enough medicine in one single-dose vial to use in both eyes.

Step 12. Throw away the vial with any remaining medicine after use.

Storing Verkazia

Store Verkazia at room temperature between 68°F to 77°F (20°C to 25°C).
Do not freeze.
After opening the pouch, keep vials in the pouch to protect from light and evaporation.

Throwing away Verkazia

Any open vial with any medicine left over should be thrown out right after use.

Talk with your healthcare provider if you want more information about Verkazia.

Distributed by:

Santen Incorporated

6401 Hollis St. Suite 125

Emeryville, CA 94608

This Instructions for Use has been approved by the U.S. Food and Drug Administration.

Approved: June 2021

---

### Enhanced risk stratification for children and young adults with B-cell acute lymphoblastic leukemia: a children's oncology group report [^c767dccd]. Leukemia (2024). Medium credibility.

Variable selection methods

Choice of predictor variables is a crucial step when building a clinical prediction model. Investigators typically must reduce a larger set of candidates to a final set of predictor variables used for final model estimation. The methods of predictor selection can be classified into two categories: (1) reduction before modeling and (2) reduction while modeling. Method (1) implies that the predictors are selected based on domain expertise prior to studying the relationship between the outcome and candidate predictors in the data to be used for model building. This method of predictor selection is generally preferred, as it best preserves the statistical properties of later model estimation and hypothesis testing. Method (2) implies that knowledge of the relationship between the outcome and candidate variables in the data is used to select predictors. Examples of method (2) include univariable screening and stepwise selection (forward, backward, and combined). Though occasionally justifiable, the disadvantages of stepwise selection are well documented and include unstable selection; misleading bias in regression coefficients, standard errors, and p-values; and poorer predictions relative to a full model. Univariable screening inherits the same disadvantages as forward stepwise selection but tends to have poorer performance due to neglect of marginally "insignificant" variables. Therefore, in this work, predictor variables (described in detail below) were selected for inclusion in the model a priori based on clinical expertise.

---

### Talquetamab (Talvey) [^c4a4fe0b]. FDA (2024). Medium credibility.

5.1 Cytokine Release Syndrome (CRS)

TALVEY can cause cytokine release syndrome, including life-threatening or fatal reactions [see Adverse Reactions (6.1)].

In the clinical trial, CRS occurred in 76% of patients who received TALVEY at the recommended dosages, with Grade 1 CRS occurring in 57% of patients, Grade 2 in 17%, and Grade 3 in 1.5%. Recurrent CRS occurred in 30% of patients. Most events occurred following step-up dose 1 (29%) or step-up dose 2 (44%) at the recommended dosages. CRS occurred in 33% of patients with step-up dose 3 in the biweekly dosing schedule (N = 153). CRS occurred in 30% of patients with the first 0.4 mg/kg treatment dose and in 12% of patients treated with the first 0.8 mg/kg treatment dose. The CRS rate for both dosing schedules combined was less than 3% for each of the remaining doses in Cycle 1 and less than 3% cumulatively from Cycle 2 onward. The median time to onset of CRS was 27 (range: 0.1 to 167) hours from the last dose, and the median duration was 17 (range: 0 to 622) hours. Clinical signs and symptoms of CRS include but are not limited to pyrexia, hypotension, chills, hypoxia, headache, and tachycardia. Potentially life-threatening complications of CRS may include cardiac dysfunction, acute respiratory distress syndrome, neurologic toxicity, renal and/or hepatic failure, and disseminated intravascular coagulation (DIC).

Initiate TALVEY therapy with step-up dosing and administer pre-treatment medications (corticosteroids, antihistamine, and antipyretics) prior to each dose of TALVEY in the step-up dosing schedule to reduce the risk of CRS. Monitor patients following administration accordingly. In patients who experience CRS, pre-treatment medications should be administered prior to the next TALVEY dose [see Dosage and Administration (2.2, 2.3)].

Counsel patients to seek medical attention should signs or symptoms of CRS occur. At the first sign of CRS, immediately evaluate patient for hospitalization and institute treatment with supportive care based on severity and consider further management per current practice guidelines. Withhold TALVEY until CRS resolves or permanently discontinue based on severity [see Dosage and Administration (2.5)].

TALVEY is available only through a restricted program under a REMS [see Warnings and Precautions (5.3)].

---

### Evaluation after a first seizure in adults [^92d54188]. American Family Physician (2022). High credibility.

Regarding diagnostic investigations for first seizure in adults, more specifically with respect to initial assessment, AAFP 2022 guidelines recommend to look for findings most predictive of a seizure, such as tongue biting, head turning or twisting, limb jerking, and urinary incontinence when evaluating a patient for possible seizures.

---

### High-throughput sequencing in respiratory, critical care, and sleep medicine research. An official American Thoracic Society workshop report [^0e23fd97]. Annals of the American Thoracic Society (2019). High credibility.

Box 1 — definitions of sequencing study terms: Batch effect refers to library construction and sequencing done in batches (e.g., 96-well plate) that introduce technical variation; complex trait is a non-Mendelian phenotype influenced by multiple genes and environmental factors; deconvolution methods estimate cell type proportions and/or identify responsible cell type(s) in heterogeneous tissues; the exome is the portion of the human genome (approximately 1%) encoding proteins and whole-exome sequencing (WES) targets these sequences; an expression quantitative trait locus (eQTL) affects gene expression and can be cis or trans when distant (> 1 Mb away); next-generation sequencing (NGS) determines millions or even billions of nucleotides up to a whole genome in 1 day; a single-nucleotide polymorphism (SNP) is a single base pair change prevalent (> 1%) and the most common type of genetic variation; sequencing coverage/depth for DNA can be averaged across the genome (e.g., 30×) and for RNA depth is the total number of sequencing reads; the RNA integrity number (RIN) quantifies RNA degradation; and variant calling identifies genetic variants in an individual exome or genome sequence.

---

### Opportunities and challenges associated with clinical diagnostic genome sequencing: a report of the Association for Molecular Pathology [^cb811e9e]. The Journal of Molecular Diagnostics (2012). Medium credibility.

Bioinformatics requirements — NGS computational workflow and outputs — are outlined: Only three major computations are performed with NGS data: i) data assembly within base calling at the level of individual reads, ii) alignment of the assembled sequence to a reference sequence, and iii) and variant calling. Considering these steps in greater detail, they can be delineated as follows: i) image processing, ii) base calling and quality scoring, iii) purity filtering, iv) quality control checks, v) alignment to a reference sequence or de novo assembly, vi) SNP-calling or application-specific steps (eg, chromatin immunoprecipitation or RNA sequencing), vii) packaging, and viii) archiving. Typically, analysis begins with a large set of tiled fluorescence or luminescence images of a flow-cell surface recorded after each iterative round of sequencing; the next step, base calling, converts images to sequence reads by localizing clusters or beads and using parameters such as intensity, background, and noise via platform-specific algorithms to generate read sequences and per-base quality scores. As a result of base calling, FASTQ files are generated and are usually platform specific and contain sequencing read data that combine both the sequence and an associated per base quality score; the sequence reads are then aligned to a known reference sequence or assembled de novo, with the appropriate alignment method depending on the sequencing platform, data type, and computational resources. Limitations and error properties are noted: one limitation of short-reads alignment and assembly is the inability to uniquely align much of a read set; error rates for individual NGS reads are higher than for Sanger sequencing, whereas overall accuracy is achieved by sequencing a given region numerous times via the massively parallel process.

---

### Defining preconception: exploring the concept of a preconception population [^7844fbdd]. BMC Pregnancy and Childbirth (2020). Medium credibility.

Stage 2: concept analysis

The second aim of this study, to generate a working definition of the preconception population and associated attributes, was achieved via concept analysis. A concept analysis provides guidance in understanding a concept of interest that is vaguely defined or poorly understood. It involves the dissection of a concept into simpler elements to promote clarity and understanding and can help elucidate its meaning or definition. Our concept analysis of the term "preconception population" was guided by the eight-step method described by Walker and Avant, which included selecting a concept, determining the aims of the analysis, identifying possible uses of the concept, determining the defining attributes, identifying model cases, identifying additional cases, identifying antecedents and consequences, and defining empirical referents. Descriptions of these steps are outlined in Table 1.

Table 1
Walker and Avant's eight steps of concept analysis with applied methodology

---

### Inherited susceptibility to pediatric acute lymphoblastic leukemia [^9d8292a5]. Nature Genetics (2009). Medium credibility.

Although genome-wide analyses have identified somatic alterations contributing to the pathogenesis of pediatric acute lymphoblastic leukemia (ALL), few studies have identified germline variants conferring risk of this disease. Two reports now provide the first genome-wide glimpse into the role of inherited alleles in ALL pathogenesis.

---

### What is a clinical pathway? Development of a definition to inform the debate [^1f6562b0]. BMC Medicine (2010). Low credibility.

Application of criteria

The weighted criteria were applied to 260 full text articles. Two review authors independently screened articles to assess which studies met the criteria for a clinical pathway and the Cochrane methodological quality criteria. The Cochrane methodological criteria relates to minimum quality standards for quantitative studies using randomised controlled trial, controlled clinical trial, controlled before and after and interrupted time series study designs. Unresolved disagreements on inclusion were referred to a third review author. In all, 63 papers were excluded as they did not meet the criteria defining a clinical pathway. Only two studies needed to be referred to a third review author because of disagreement on a study meeting the minimum definition criteria.

A total of 27 studies were included on both definition and methodological criteria in the final review. In all, 14 studies termed their intervention a clinical pathway whilst 8 others referred to their intervention as a 'protocol'. No other term was used more than once. The remaining terms used for what we included as a clinical pathway were 'care model', 'care map', 'multidisciplinary care', 'evidence-based care' and 'guideline'.

---

### Event analysis: using transcript events to improve estimates of abundance in RNA-seq data [^e7e4a4b1]. G3 (2018). Low credibility.

Exonic regions:

The second step of event analysis is to identify all the exonic regions. In higher eukaryotes, exons may overlap with one another (e.g. due to alternative donors, alternative acceptors, alternative transcription initiation/termination sites, etc.). The exonic region may be quantified without regard to differences in donor/acceptor sites to avoid double counting reads. In regions where differences are small (less than 10 bp) there is no meaningful loss of information in this approach. However, where exonic regions are comprised of overlapping exons that differ measurably, the 5′ and 3′ positions of exons within the exonic region are used to separate the region into exon fragments. EA annotates each exon fragment within an exonic region to indicate whether it is exclusive to a single exon or is shared among sets of exons (Figure 1B) and is further annotated to transcripts.

Annotating events:

EA annotates junctions based on whether they exclude exons (exon-skipping splice junctions) or if they use alternative 5′ and/or 3′ splice sites (alternative donor junctions, alternative acceptor junctions). Exon-skipping junctions are defined as splice junctions that exclude ("skip") one or more exons that are situated in the reference annotation between the donor and acceptor site of the junction (Figure 1, e.g. junction between exons E and I; Table 1). Where there are multiple possible donor and/or acceptor sites within a group of overlapping exons, all donors/acceptors are classified as "alternative". This follows a similar convention to other splicing definitions, such as those used by AStalavista. Junction definitions are not mutually exclusive: a junction can utilize both alternative donor and alternative acceptor sites and also exclude one or more intermediary exons (see examples in Figure 1, Table 1). The unannotated, putative novel donor/novel acceptor sites may reflect unprocessed transcript and intron retention events. To accommodate this uncertainty EA annotates these as border junctions. The likelihood of truly novel donor/acceptor sites is evaluated in a separate step based on the experimental data that includes mapping information from the adjoining exons/intron region. (Figure 2).

---

### 2023 ESC guidelines for the management of cardiovascular disease in patients with diabetes [^24784e3d]. European Heart Journal (2023). High credibility.

Regarding medical management for diabetes mellitus type 2, more specifically with respect to prevention of CVD, glucose-lowering medications, ESC 2023 guidelines recommend to consider initiating metformin to reduce cardiovascular risk in patients with T2DM without ASCVD or severe target-organ damage at low, moderate,
high, or very high risk.

---

### MLL translocations specify a distinct gene expression profile that distinguishes a unique leukemia [^e4078172]. Nature Genetics (2002). Medium credibility.

Acute lymphoblastic leukemias carrying a chromosomal translocation involving the mixed-lineage leukemia gene (MLL, ALL1, HRX) have a particularly poor prognosis. Here we show that they have a characteristic, highly distinct gene expression profile that is consistent with an early hematopoietic progenitor expressing select multilineage markers and individual HOX genes. Clustering algorithms reveal that lymphoblastic leukemias with MLL translocations can clearly be separated from conventional acute lymphoblastic and acute myelogenous leukemias. We propose that they constitute a distinct disease, denoted here as MLL, and show that the differences in gene expression are robust enough to classify leukemias correctly as MLL, acute lymphoblastic leukemia or acute myelogenous leukemia. Establishing that MLL is a unique entity is critical, as it mandates the examination of selectively expressed genes for urgently needed molecular targets.

---

### Epilepsy in adults [^ef22bf7a]. Lancet (2019). Excellent credibility.

The following constitutes key background information on epilepsy:

- **Definition**: Epilepsy is a neurological disorder characterized by recurrent epileptic seizures or, a single unprovoked seizure with a high risk of recurrence.
- **Pathophysiology**: Epilepsy is due to an imbalance between excitation and inhibition activity in the brain due to genetic (benign familial neonatal epilepsy) or acquired factors (head trauma, prolonged febrile seizures).
- **Epidemiology**: The estimated prevalence of epilepsy in the US is 1.2%.
- **Risk factors**: Age (infants < 1 year and adults > 50 years), genetic abnormalities and mutations, geographic location, low-income countries (poor hygiene and basic sanitation, substandard health delivery system), infections (falciparum malaria, neurocysticercosis, onchocerciasis), head trauma, brain development malformations, cerebrovascular diseases, and tumors increase the risk of epilepsy.
- **Disease course**: The imbalance between excitatory and inhibitory activity in the neuronal network within the brain leads to epileptogenic networks causing generalized, focal, and unknown type of seizures resulting in focal, generalized, combined generalized and focal, or unknown type of epilepsy and epilepsy syndromes.
- **Prognosis and risk of recurrence**: Approximately 60–70% of patients with epilepsy can be managed with medication. The overall risk of recurrence after a single seizure varies between 27–71%; 80% of these recurrent seizures occur within 2 years of the initial seizure.

---

### Where do we stand in the treatment of relapsed acute lymphoblastic leukemia? [^319df2aa]. Hematology: American Society of Hematology. Education Program (2012). Low credibility.

Acute lymphoblastic leukemia (ALL) is the most common and one of the most treatable cancers in children. Although the majority of children with ALL are now cured, 10%-20% of patients are predicted to relapse and outcomes with salvage therapy have been disappointing, with approximately only one-third of children surviving long-term after disease recurrence. Several prognostic factors have been identified, with timing of recurrence relative to diagnosis and site of relapse emerging as the most important variables. Despite heterogeneity in the elements of salvage therapy that are delivered in trials conducted internationally, outcomes have been remarkably similar and have remained static. Because most intensive salvage regimens have reached the limit of tolerability, current strategies are focusing on identifying new agents tailored to the unique biology of relapsed disease and identifying methods to develop these agents efficiently for clinical use. Recently, high-resolution genomic analyses of matched pairs of diagnostic and relapse bone marrow samples are emerging as a promising tool for identifying pathways that impart chemoresistance.

---

### Acute lymphoblastic leukemia, version 2.2024, NCCN clinical practice guidelines in oncology [^b0575bfd]. Journal of the National Comprehensive Cancer Network (2024). High credibility.

Regarding follow-up and surveillance for acute lymphoblastic leukemia - NCCN, more specifically with respect to follow-up, NCCN 2024 guidelines recommend to obtain periodic BCR::ABL1 transcript-specific quantification in patients with Philadelphia chromosome-positive acute lymphoblastic leukemia.

---

### Diagnosis, management and treatment of the Alport syndrome-2024 guideline on behalf of ERKNet, ERA and ESPN [^45eede36]. Nephrology, Dialysis, Transplantation (2025). High credibility.

Regarding diagnostic investigations for Alport syndrome, more specifically with respect to hearing assessment, ERA/ERN ERKNet/ESPN 2025 guidelines recommend to consider obtaining a hearing evaluation at diagnosis or upon reaching adulthood in female patients with X-linked alport syndrome, and then every 5 years in the absence of hearing loss symptoms.

---

### Evaluation after a first seizure in adults [^32e3209b]. American Family Physician (2022). Medium credibility.

The following constitutes key background information on first seizure in adults:

- **Definition**: A first seizure in adults refers to a single, spontaneous episode of abnormal neuronal activity resulting in transient neurologic dysfunction. It may be classified as either provoked (acute symptomatic) or unprovoked, depending on the presence of an identifiable acute cause.
- **Pathophysiology**: Seizures arise from abnormal, synchronous electrical discharges within the cerebral cortex, reflecting an imbalance between excitatory and inhibitory neuronal activity. Mechanistically, seizures involve recurrent excitatory synaptic activity and loss of coordinated signaling between neuronal populations. This cortical disorganization leads to transient neurologic dysfunction. In prolonged seizures, normal homeostatic mechanisms may become impaired, exacerbating metabolic stress, altering cerebral perfusion, and increasing the risk of neuronal injury. The pathophysiology varies depending on whether the seizure is provoked (due to acute metabolic, infectious, or structural insults) or unprovoked, which may involve chronic or idiopathic mechanisms including genetic predisposition or remote cerebral injury.
- **Epidemiology**: The incidence of a first seizure in adults ranges from 23 to 61 cases per 100,000 person-years, with the risk increasing significantly after age 55 and peaking in individuals older than 75. The lifetime risk of experiencing a seizure is up to 10%, while only about 3% of adults are ultimately diagnosed with epilepsy. The rate of first seizures is higher in low- and middle-income countries, which account for around 80% of the global epilepsy burden.
- **Risk factors**: Approximately 40% of first seizures in adults are provoked by identifiable acute factors. Common causes include metabolic disturbances (hypoglycemia, hyponatremia, hepatic or renal failure), infections (meningitis, encephalitis), structural brain abnormalities (stroke, traumatic brain injury, tumors), and toxic exposures (alcohol withdrawal, drug intoxication). Inflammatory conditions such as SLE and autoimmune encephalitis may also contribute. Other reversible factors include prescribed medications, sleep deprivation, and substance misuse. Identifying a provoking factor is critical for prognosis and treatment planning, as provoked seizures have a lower risk of recurrence once the underlying cause is resolved. Unprovoked seizures occur without an acute precipitating factor and are more common in younger adults. They are classified as either idiopathic, with no identifiable cause, or secondary to preexisting or remote CNS injury. Known risk factors include prior traumatic brain injury, congenital neurological conditions such as cerebral palsy, and a history of CNS infections. These underlying abnormalities contribute to a higher risk of seizure recurrence and the potential diagnosis of epilepsy.
- **Disease course**: The clinical presentation of a first seizure depends on the site of onset and the pattern of spread. Generalized seizures often begin without warning, while focal seizures may be preceded by an aura, such as a rising epigastric sensation, déjà vu, or unusual tastes or smells. The absence of an aura does not exclude focal onset, especially in seizures originating from the frontal lobe. Convulsive seizures typically begin with a tonic phase of limb stiffening, followed by a clonic phase of rhythmic jerking. These events last approximately 1–3 minutes and may involve open eyes, apnea, and cyanosis. Postictal recovery is often marked by confusion, fatigue, and signs such as lateral tongue biting. Psychogenic nonepileptic seizures, also known as pseudoseizures, are seizure-like symptoms that are not associated with abnormal brain electrical activity.
- **Prognosis and risk of recurrence**: Adults experiencing a first unprovoked seizure have a 21% to 45% risk of recurrence within the first 2 years, with approximately one-third experiencing a second seizure within the first year. The risk is highest in the initial months following the first event. Factors associated with increased recurrence include a prior brain insult (such as stroke or trauma), epileptiform abnormalities on EEG, structural brain abnormalities on imaging, and a seizure occurring during sleep. Initiating antiseizure medication immediately after the first seizure reduces the risk of recurrence by approximately 35% over the first 2 years. However, early treatment does not improve long-term remission rates beyond 3 years compared to delayed treatment.

---