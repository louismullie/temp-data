# Medical Question & Answer

**Sample ID**: e1b078fa-15d5-0821-1d34-714af3421627
**Dataset Index**: 374284

---

## Question

9.30 Let α ∈ ([π/4], [π/2]), then the order of (cos α)^(cos α),(sin α)^(cos α),(cos α)^(sin α) is
(A) (cos α)^(cos α) < (sin α)^(cos α) < (cos α)^(sin α).
(B) (cos α)^(cos α) < (cos α)^(sin α) < (sin α)^(cos α).
(C) (sin α)^(cos α) < (cos α)^(cos α) < (cos α)^(sin α).
(D) (cos α)^(sin α) < (cos α)^(cos α) < (sin α)^(cos α).
(China High School Mathematics Competition, 1990)

---

## Answer

> Let's see… What do we have here? The user is asking about ordering three expressions involving powers of sine and cosine for angles between π/4 and π/2. Let's break this down step-by-step. First, I need to think about the interval and what it implies for the relative sizes of sine and cosine. Then, I should verify how the base and exponent interact in expressions like (cos α)^cos α versus (cos α)^sin α. Next, I will compare pairs systematically, starting with (cos α)^cos α versus (sin α)^cos α, then (cos α)^cos α versus (cos α)^sin α, and finally (sin α)^cos α versus (cos α)^sin α. Finally, I will synthesize the overall order and match it to the correct option, making sure I double-check any assumptions about monotonicity and inequalities along the way.

> Let me first confirm the interval and the basic inequalities it entails. For α ∈ (π/4, π/2), we have sin α > cos α > 0, and both sin α and cos α lie in (0, 1), which means any positive power less than 1 will increase the value of a base in (0, 1). Wait, let me verify that last point: for 0 < b < 1 and 0 < p < 1, we indeed have b^p > b, so raising to a smaller positive exponent increases the value when the base is less than 1. I should keep that in mind as I compare terms with different exponents.

> Now, I will compare (cos α)^cos α with (sin α)^cos α. Since the exponent cos α is the same and positive, and sin α > cos α, the inequality of bases directly implies (sin α)^cos α > (cos α)^cos α. Hold on, I should verify that I'm not overlooking any subtlety with the exponent; because the exponent is positive and identical, the comparison reduces to the bases, so yes, (sin α)^cos α is larger than (cos α)^cos α.

> Next, I should compare (cos α)^cos α with (cos α)^sin α. Here the base is the same and less than 1, and the exponents satisfy cos α < sin α, so by the property that smaller positive exponents increase values in (0, 1), we have (cos α)^sin α < (cos α)^cos α. Wait, let me double-check the direction: for 0 < b < 1, if p < q then b^p > b^q, so indeed (cos α)^sin α is smaller than (cos α)^cos α.

> I will now examine (sin α)^cos α versus (cos α)^sin α. Let me think about a concrete example to sanity-check the general reasoning. Take α = π/3, so sin α = √3/2 ≈ 0.866 and cos α = 1/2 = 0.5. Then (sin α)^cos α ≈ 0.866^0.5 ≈ 0.93, and (cos α)^sin α ≈ 0.5^0.866 ≈ 0.54, which shows (sin α)^cos α > (cos α)^sin α. I should confirm that this pattern holds across the interval; given sin α > cos α and both bases are in (0, 1), the larger base raised to the smaller exponent tends to dominate, reinforcing that (sin α)^cos α > (cos α)^sin α.

> Synthesizing these pairwise results, I get the chain (cos α)^sin α < (cos α)^cos α < (sin α)^cos α. Let me reconsider the ordering to ensure no misstep: from step 2, (cos α)^cos α < (sin α)^cos α; from step 3, (cos α)^sin α < (cos α)^cos α; and from step 4, (cos α)^sin α < (sin α)^cos α, which is consistent with the chain. Therefore, the correct order is (cos α)^sin α < (cos α)^cos α < (sin α)^cos α, which corresponds to option D.

---

The correct order is **(cos α)^sin α < (cos α)^cos α < (sin α)^cos α**, which corresponds to option D. This follows because: (1) sin α > cos α for α ∈ (π/4, π/2), so (sin α)^cos α > (cos α)^cos α; (2) since 0 < cos α < 1, raising to a smaller exponent increases the value, so (cos α)^cos α > (cos α)^sin α. Thus, the final ordering is (cos α)^sin α < (cos α)^cos α < (sin α)^cos α.

---

## Step-by-step analysis

### Step 1: establish the range of α and basic inequalities

Given α ∈ (π/4, π/2), we have:

- sin α > cos α > 0
- Both sin α and cos α are in (0, 1)

---

### Step 2: compare (cos α)^cos α and (sin α)^cos α

Since the exponent cos α is the same and positive, and sin α > cos α, we have:

(cos α)^cos α < (sin α)^cos α

---

### Step 3: compare (cos α)^cos α and (cos α)^sin α

Since 0 < cos α < 1 and cos α < sin α, raising to a smaller exponent yields a larger value:

(cos α)^sin α < (cos α)^cos α

---

### Step 4: combine the inequalities

From Steps 2 and 3, we obtain:

(cos α)^sin α < (cos α)^cos α < (sin α)^cos α

---

## Verification with a specific example

Let α = π/3 (60°), so cos α = 1/2 and sin α = √3/2 ≈ 0.866. Then:

- (cos α)^sin α = (1/2)^(√3/2) ≈ 0.54
- (cos α)^cos α = (1/2)^(1/2) ≈ 0.707
- (sin α)^cos α = (√3/2)^(1/2) ≈ 0.93

This confirms the order: **0.54 < 0.707 < 0.93**, matching option D.

---

## Conclusion

The correct order is **(cos α)^sin α < (cos α)^cos α < (sin α)^cos α**, which corresponds to option D.

---

## References

### Power-law dynamics in neuronal and behavioral data introduce spurious correlations [^a4a9ae3c]. Human Brain Mapping (2015). Low credibility.

Relating behavioral and neuroimaging measures is essential to understanding human brain function. Often, this is achieved by computing a correlation between behavioral measures, e.g., reaction times, and neurophysiological recordings, e.g., prestimulus EEG alpha-power, on a single-trial-basis. This approach treats individual trials as independent measurements and ignores the fact that data are acquired in a temporal order. It has already been shown that behavioral measures as well as neurophysiological recordings display power-law dynamics, which implies that trials are not in fact independent. Critically, computing the correlation coefficient between two measures exhibiting long-range temporal dependencies may introduce spurious correlations, thus leading to erroneous conclusions about the relationship between brain activity and behavioral measures. Here, we address data-analytic pitfalls which may arise when long-range temporal dependencies in neural as well as behavioral measures are ignored. We quantify the influence of temporal dependencies of neural and behavioral measures on the observed correlations through simulations. Results are further supported in analysis of real EEG data recorded in a simple reaction time task, where the aim is to predict the latency of responses on the basis of prestimulus alpha oscillations. We show that it is possible to "predict" reaction times from one subject on the basis of EEG activity recorded in another subject simply owing to the fact that both measures display power-law dynamics. The same is true when correlating EEG activity obtained from different subjects. A surrogate-data procedure is described which correctly tests for the presence of correlation while controlling for the effect of power-law dynamics.

---

### Correction to: how can clinicians, specialty societies and others evaluate and improve the quality of apps for patient use? [^82bc7a71]. BMC Medicine (2019). Medium credibility.

Since the publication of this article [1] it has come to my attention that it contains an error in which the y-axis in Fig. 1 was inverted, thus incorrectly displaying a weak negative correlation rather than a weak positive one. This error was introduced as the order of the data on which Fig. 2 was based [2] was misread. The corrected version of Fig. 2 can be seen below, in which a weak positive correlation is now displayed. This does not change the general point, that app users and app stores appear to take little notice of the source of information on which apps are based. I apologise to readers for this error.

---

### Fluctuations in instantaneous frequency predict alpha amplitude during visual perception [^0f13b3bd]. Nature Communications (2017). Medium credibility.

Instantaneous frequency

Instantaneous frequency is defined as the first derivative in time of the phase of the EEG signal, or the change in phase per unit time as time approaches zero (see ref.for review). For each subject and EOI, artifact free epochs were bandpass filtered at ± 2.5 Hz around peak alpha using a 3rd order Butterworth filter (again, bandpass filtering done on 6000 ms epochs surrounding target onset to attenuate edge artifacts in the peri-stimulus window). We then applied a Hilbert transform to the filtered data from each epoch to obtain the amplitude and phase of the EEG response at each point in time on each trial. The phase angle was unwrapped to be cumulative so that there were no discontinuities at –pi and pi. We then calculated instantaneous frequency by approximating the derivative of these unwrapped phase angles. To yield an estimate of frequency in Hz at time t and trial k, we then normalized this approximate derivative by the sampling rate (sr). Because computing numerical derivatives of discretely sampled timeseries can produce sharp discontinuities, we attenuate the influence of these outliers by low-pass filtering our estimates of the derivative of the phase angle. More formally, we estimated the instantaneous frequency on trial k and time t by fitting a line of the following form to the unwrapped phase data in temporal window of 88 data samples centered on time t: Wherecorresponds to the estimated unwrapped phase, parameterized by scalar, or an estimated slope (change in phase angle ϕ), vector x, the time axis, and scalar, the y intercept. The window size of 172 ms for x, corresponding to 88 data samples at sr = 512 Hz, was selected because it was the smallest window that kept the average instantaneous frequency fluctuations on single trials within the 5 Hz wide bandpass range (Supplementary Figure 1). Given this fit, we defined instantaneous frequency at time t and trial k :Wherecorresponds to an estimate of the instantaneous frequency at time t on trial k. The regression lines were estimated using a least squares fitting algorithm to the unwrapped phase data and the fits were generally quite good (R 2 = 0.995 ± 0.016, mean ± SD). We also evaluated our results by estimating instantaneous frequency by simply subtracting sequential points along the timeourse of the unwrapped phase:

---

### The neural dynamics of hierarchical Bayesian causal inference in multisensory perception [^3f4d4ca3]. Nature Communications (2019). High credibility.

Second, we fitted a more constrained regression model with one single sine and cosine at F = 10 Hz that uses the Φ dect averaged over trials within a particular decile = dec at a time point = t to predict p common, dec, t. Hence, this model assumes that the modulation of p common, dec, t by alpha phase for each time point (i.e. a column in Fig. 6b) evolves slowly over time according to a 10 Hz alpha oscillatory rhythm: p common, dec, t = causal prior estimated over trials in a particular decile dec for time t, Φ dect = across-trials average phase in a particular decile dec at time = t; C = constant

The statistical significance of this model was assessed for the time window of −280 up to −80 ms encompassing the significant cluster (i.e. to include two alpha cycles; Fig. 6a) in each participant with an F test on the residual sum of squares against a reduced model that included only the constant C as a regressor (i.e. df1 = 2; df2 = 107; see Fig. 6b). Next, we assessed whether the phase angle of the alpha oscillation (i.e. Φ Participant = angle(β cos + i β sin) from equation (9) was consistent across participants and hence deviated significantly from a circular uniform distribution using a Raleigh test. The distribution of phase angles over participants was not significantly different from uniformity, which can be explained by participant-specific cortical folding leading to differences in the orientation of the underlying neural sources. We therefore identified the peak in predicted p common, dec at t = −160 ms in each participant (based on Equation (8)), computed the difference in deciles between the participant's peak decile and the group peak decile (Supplementary Fig. 5B) and then circularly shifted the predicted and observed p common, dec in each participant by this difference across all time points. As a consequence, the adjusted participant's peak is aligned with the predicted group peak p common, dec at t = −160 ms and Φ = −0.29 π = 52° (Supplementary Fig. 5A, B). Then we averaged the observed and predicted (cf. Equation (8)) p common across participants for illustrational purposes (Fig. 6c, d).

---

### The neural dynamics of hierarchical Bayesian causal inference in multisensory perception [^af07db09]. Nature Communications (2019). High credibility.

Second, based on previous research implicating prestimulus alpha phase in temporal binding, we investigated whether the circular mean phase (i.e. averaged across trials within a decile) of alpha oscillations (8–12 Hz) was correlated with the BCI parameters (i.e. p common or σ V) over alpha phase deciles using linear-circular correlation. To enable an unbiased group-level statistic, we first randomized the assignment between mean circular phase and BCI parameters across the deciles (5000 randomizations) within each participant. Next, we computed the percentile of a participant's true circular-linear correlation in relation to this participant's null-distribution of circular-linear correlations. At the group level, we then tested whether the across-participants' mean percentile was significantly greater than 50% (i.e. the mean percentile under the null hypothesis) using a randomization test (i.e. sign flip of deviation of percentile from 50% in 5000 randomizations; test statistic: one-sided t -tests) and cluster-based correction for multiple comparisons (Fig. 6a; cluster-level statistic: sum of the t values in a cluster; cluster threshold t = 2).

To characterize the modulation of p common by alpha phase, we first fitted a sine and cosine to p common, dec over alpha phase deciles Φ dec at 10 Hz individually to each participant's data, separately for each time point (Equation (8)). Thus, we computed the average phase Φ dec at 10 Hz across trials for each decile at a particular time point. We then used this average phase Φ dec in each decile at 10 Hz to predict p common, dec for this particular time point over deciles based on a sinusoidal model:with p common, dec = causal prior estimated based on trials in a particular decile; Φ dec = across-trials average phase in a particular decile; C = constant

Crucially, this regression model (i.e. Equation (8)) estimates β sin and β cos independently for each time point. Thus, Equation (8) characterizes the relationship between alpha phase and p common over deciles for a particular time point, so that the phase of this modulation can in principle vary across time (Fig. 6d).

---

### Flexible multitask computation in recurrent networks utilizes shared dynamical motifs [^e0205812]. Nature Neuroscience (2024). High credibility.

In total, there were

N in = 1(fixation) + 2(modalities) × 2(A sin θ and A cos θ) + 15(rule) = 20 input units.

The network projected the state, to an output ring, which contained two units (sin ϕ and cos ϕ) to encode response direction on a circle. In addition, the network projectedto a fixation output unit, which should be at the high activity value of 1 before the response and at 0 once a response is generated.

In total, there were

N out = 1(fixation) + 1(modality) × 2(sin ϕ and cos ϕ) = 3 output units.

Tasks and performances

Inputs and outputs for an example trial on each task are shown in Extended Data Fig. 1. Fixation input was 1 for the duration of the trial until the response period, when fixation input changed to 0. Reaction timed tasks never received a 'go' cue; therefore, the fixation input was always at 1, and the network was required to break fixation to respond as soon as the relevant stimulus arrived. Target fixation output activity was high before the response period and low during the response period for all tasks. If the activity of the fixation output prematurely fell below 0.5, the network was considered to have erroneously broken fixation, and the trial was incorrect.

The response direction of the network was read out in a two-dimensional vector (sin ϕ and cos ϕ). The decoded response direction was considered correct if it was within π/10 of the target direction.

---

### The neural dynamics of hierarchical Bayesian causal inference in multisensory perception [^c83d8ce5]. Nature Communications (2019). High credibility.

EEG – The relation of prior stimulus history, prestimulus alpha power and the causal prior

Previous research has shown that prior stimulus history influences observers' binding tendency –. For instance, prior congruent audiovisual speech stimuli increased observers' tendency to bind incongruent audiovisual signals into illusionary McGurk percepts. Hence, we investigated whether the numeric disparity of previous flash-beep stimuli (going back in history to five trials prior to stimulus onset) influenced observers' causal prior on the current trial. Indeed, as shown in Fig. 7a, a 2 (numeric disparity: small vs. large) × 5 (stimulus order: 1, 2, 3, 4, 5 trials back) repeated-measures ANOVA revealed a significant main effect of numeric disparity (F 1,21 = 6.260, p = 0.021, partial η 2 = 0.230) and a significant interaction between numeric disparity and stimulus order (F 2.6,54.9 = 4.060, p = 0.015, partial η 2 = 0.162; Greenhouse-Geisser corrected df). Post-hoc tests for the effect of numeric disparity separately for specific stimulus order showed that the effect of numeric disparity was most pronounced for the first- and second-order previous stimulus (first order: t 22 = 3.731, p = 0.001, Cohen's d = 0.778; marginally significant second order: t 22 = 2.042, p = 0.053, Cohen's d = 0.426; two-sided paired t -tests) and tapered off with stimulus order.

---

### Numerical working memory alters α-beta oscillations and connectivity in the parietal cortices [^41e3c9e0]. Human Brain Mapping (2020). Medium credibility.

FIGURE 1
Numerical working memory manipulation task. Each trial began with an empty 4 × 1 grid with a blue fixation cross in the center presented for 1 s, followed by a 4 × 1 grid of single numerical digits (1–9) presented for 1.2 s (encoding). The digits then disappeared for 2.5 s (maintenance), and the underlying color of the grid became darker or lighter. The color indicated whether the participant was to maintain the original sequence, or rearrange the sequence into a numerically ascending order. After the 2.5 s, a 4 × 1 grid with a single probe digit was presented in any of the four boxes for 1.6 s (retrieval), and participants were tasked with responding (by button press) if the digit was in the correct box in the grid (right index finger) or not (right middle finger). It is important to note that the meaning of the light or dark color change was pseudorandomized between participants

---

### The backtracking survey propagation algorithm for solving random K-SAT problems [^dcfc43b2]. Nature Communications (2016). Medium credibility.

Results

Probability of finding a SAT assignment

The standard way to study the performance of a solving algorithm is to measure the fraction of instances it can solve as a function of α. We show in Fig. 1 such a fraction for BSP run with three values of the r parameter (r = 0,0.5 and 0.9) on random 4-SAT problems of two different sizes (N = 5,000 and N = 50,000). The probability of finding a solution increases both with r and N, but an extrapolation to the large N limit of these data is unlikely to provide a reliable estimation of the algorithmic threshold α a BSP.

In each plot having α on the abscissa, the right end of the plot coincides with the best estimate of α s, in order to provide an immediate indication of how close to the SAT-UNSAT threshold the algorithm can work.

Order parameter and algorithmic threshold

In order to obtain a reliable estimate of α a BSP we look for an order parameter vanishing at α a BSP and having very little finite size effects. We identify this order parameter with the quantity Σ res / N res, where Σ res and N res are respectively the complexity (for example, log of number of clusters) and the number of unassigned variables in the residual formula. As explained in Methods, BSP assigns and re-assigns variables, thus modifying the formula, until the formula simplifies enough that the SP fixed point has only null messages: the residual formula is defined as the last formula with non-null SP fixed point messages. We have experimentally observed that the BSP algorithm (as the SID one) can simplify the formula enough to reach the trivial SP fixed point only if the complexity Σ remains strictly positive during the whole decimation process. In other words, on every run where Σ becomes very close to zero or negative, SP stops converging or a contradiction is found. This may happen either because the original problem was unsatisfiable or because the algorithm made some wrong assignments incompatible with the few available solutions. Thanks to the above observation we have that Σ res ≥ 0 and thus a null value for the mean residual complexity signals that the BSP algorithm is not able to find any solution, and thus provides a valid estimate for the algorithmic threshold α a BSP. From the statistical physics solution to random K -SAT problems we expect Σ res to vanish linearly in α.

---

### The snm procedure guideline for general imaging 6.0 [^58130ebe]. SNMMI (2010). Medium credibility.

General imaging — preprocessing for single-photon emission computed tomography (SPECT) studies notes that 'Prefiltering of the projection data is appropriate in many SPECT studies because smoothing in the axial direction may be included' and advises that 'When necessary, guidance on the choice of filter and filter parameters (cutoff and order) should be sought from experts'. It further states that 'Correction for patient motion may reduce reconstruction artifacts' and that 'The highest quality study will be obtained by minimizing patient motion'.

---

### Designing attractive models via automated identification of chaotic and oscillatory dynamical regimes [^344b5db2]. Nature Communications (2011). Medium credibility.

With [X] i denoting the i th column of the matrix X, the UKF algorithm for parameter estimation is given by,

Initialize:

For each time point k = t 1. t N:

Prediction step:

Update step:

where

Various schemes for sigma-point selection exist including those for minimal set size, higher than third-order accuracy and (as defined and used in this study) guaranteed positive-definiteness of the parameter covariance matrices, which is necessary for the square roots obtained by Cholesky decomposition when calculating the sigma points. The scaled sigma-point scheme thus proceeds as,

where

and parameters κ, α and β may be chosen to control the positive definiteness of covariance matrices, spread of the sigma-points, and error in the kurtosis, respectively. Our choices for the UKF parameter and noise covariances are discussed in the Supplementary Information.

To apply to the UKF for qualitative inference, we amend the dynamical state-space model to,

where L (·) maps parameters to the encoding of the dynamical behaviour (here a numerical routine to calculate the Lyapunov spectrum), λ target is a constant target vector of LEs, y 0 denotes the initial conditions, and f is the dynamical system under investigation (with unknown parameter vector θ, considered as a hidden state of the system and not subject to temporal dynamics). To see how equations (9) and (10) fit the state-space model format for UKF parameter estimation, it is helpful to consider the time series (λ target, λ target, λ target,.) as the 'observed' data from which we learn the parameters of the nonlinear mapping L (·). Our use of the UKF is characterized by a repeated comparison of the simulated dynamics for each sigma point to the same (as specified) desired dynamical behaviour. In this respect, we use the UKF as a smoother; there is no temporal ordering of the data supplied to the filter because all information about the observed (target) dynamics is given at each iteration. From an optimization viewpoint, the filter aims to minimize the prediction-error function,

thus moving the parameters towards a set for which the system exhibits the desired dynamical regime.

---

### The neural dynamics of hierarchical Bayesian causal inference in multisensory perception [^0ec3eac4]. Nature Communications (2019). High credibility.

Fig. 6
Effect of prestimulus alpha phase on the BCI model's causal prior (p common) in occipital electrodes. a Time-frequency t -value map (n = 23) for the circular-linear correlation between p common and the phase deciles of alpha oscillations for 8, 10 and 12 Hz and averaged across occipital electrodes. Significant clusters (p < 0.05; one-sided cluster-based corrected randomization t 22 test) are demarcated by a solid line. b Left: Across participants' mean predicted p common (colour coded) as a function of time (x-axis) and 10 Hz alpha phase bin (y-axis, alpha phase indicated as fraction of π). The predicted p common is based on the constrained sinusoidal model which predicts p common for a particular decile and time by a single sine and cosine of the phase at 10 Hz averaged across trials in this particular decile and time bin (see Equation (9)). Hence, this constrained sinusoidal model assumes that the modulation of p common by alpha phase evolves at 10 Hz over time. Right: Individual model fits are significant (p < 0.05; individual F 2,107 tests, dashed lines) in 20 of 23 participants, but the sinusoidal model's phase angles (i.e. Φ Participant = angle(z) = angle(β cos + i β sin)) do not deviate significantly from a circular uniform distribution (z 22 = 2.486, p = 0.082, Raleigh test) across participants. c, d Across-participants' mean observed (c) and predicted (d) p common is shown coded in color as a function of time (x-axis) and 10 Hz alpha phase bin (y-axis, alpha phase indicated as fraction of π). The phases of the observed and predicted p common were aligned across participants (at the peak of the non-aligned effect with alpha phase = −0.29 π at−160 ms; see methods section and Supplementary Fig. 5B) before averaging across participants. The predicted p common (d) is based on the sinusoidal models that predicted p common by alpha phase across the deciles independently for each time point (i.e. time-specific alpha phase models) and participant (i.e. Equation (8)). Source data are provided as a Source Data file

---

### Stability of synchronization in simplicial complexes [^14660083]. Nature Communications (2021). High credibility.

Fig. 1
Synchronization in simplicial complexes of Rössler oscillators.

Contour plots of the time averaged (over an observation time T = 500) synchronization error E (see "Methods" for definition and the vertical bars of each panel for the color code) in the plane (σ 1, σ 2) for some examples of simplicial complexes (whose sketches are reported in the top left of each panel). Simulations refer to coupled Rössler oscillators (x = (x, y, z) T and f = (− y − z, x + a y, b + z (x − c)) T) with parameters fixed in the chaotic regime (a = b = 0.2, c = 9). In a – d, while in (e). As for the other coupling function, one hasin (d) andin all other panels. The blue continuous lines are the theoretical predictions of the synchronization thresholds obtained from Eq. (3). a, b, and c are examples of class III problems, whereas panels d and e are examples of class II problems.

Far from being limited to the case of D = 2, our approach can be extended straightforwardly to simplicial complexes of any order D. Each term on the right hand side of Eq. (1) can, indeed, be manipulated following exactly the same three conceptual steps described in the "Methods". Once again, one is entitled to select the eigenvector set which diagonalizes, to introduce the new variables η = (V −1 ⊗ I m) δ x. Following the very same steps which led us to write Eq. (3), one then obtainswhere JG (d) = J 1 g (d) (x s,…, x s) + J 2 g (d) (x s,…, x s) +… + J d g (d) (x s,…, x s) and the coefficientsresult from transformingwith the matrix that diagonalizes. As a result, one has conceptually the same reduction of the problem to a single, uncoupled, nonlinear system, plus a system of N − 1 coupled linear equations, from which the maximum Lyapunov exponent Λ max = can be extracted and monitored (for each simplicial complex) in the D -dimensional hyper-space of the coupling strength parameters.

---

### Modelling sequences and temporal networks with dynamic community structures [^7129d625]. Nature Communications (2017). Medium credibility.

Bayesian Markov chains with communities

As described in the main text, a Bayesian formulation of the Markov model consists in specifying prior probabilities for the model parameters, and integrating over them. In doing so, we convert the problem from one of parametric inference where the model parameters need to be specified before inference, to a nonparametric one where no parameters need to be specified before inference. In this way, the approach possesses intrinsic regularisation, where the order of the model can be inferred from data alone, without overfitting.

To accomplish this, we rewrite the model likelihood, using eqs. (1) and (5), asand observe the normalisation constraints,… Since this is just a product of multinomials, we can choose conjugate Dirichlet priors probability densitiesand, which allows us to exactly compute the integrated likelihood, whereand. We recover the Bayesian version of the common Markov chain formulation (see ref.) if we put each memory and token in their own groups. This remains a parametric distribution, since we need to specify the hyperparameters. However, in the absence of prior information it is more appropriate to make a noninformative choice that encodes our a priori lack of knowledge or preference towards any particular model, which amounts to choosing α x = β rs = 1, making the prior distributions flat. If we substitute these values in eq. (19), and re-arrange the terms, we can show that it can be written as the following combination of conditional likelihoods, wherewithbeing the multiset coefficient, that counts the number of m -combinations with repetitions from a set of size n. The expression above has the following combinatorial interpretation: P ({ x t }| b, { e rs }, { k x }) corresponds to the likelihood of a microcanonical modelwhere a random sequence { x t } is produced with exactly e rs total transitions between groups r and s, and with each token x occurring exactly k x times. In order to see this, consider a chain where there are only e rs transitions in total between token group r and memory group s, and each token x occurs exactly k x times. For the first transition in the chain, from a memory x 0 in group s to a token x 1 in group r, we have the probabilityNow, for the second transition from memory x 1 in group t to a token x 2 in group u, we have the probabilityProceeding recursively, the final likelihood for the entire chain iswhich is identical to eq. (21).

---

### The expected behaviour of random fields in high dimensions: contradictions in the results of bansal and peterson [^657f9d59]. Magnetic Resonance Imaging (2022). Medium credibility.

Bansal and Peterson (2018) found that in simple stationary Gaussian simulations Random Field Theory incorrectly estimates the number of clusters of a Gaussian field that lie above a threshold. Their results contradict the existing literature and appear to have arisen due to errors in their code. Using reproducible code we demonstrate that in their simulations Random Field Theory correctly predicts the expected number of clusters and therefore that many of their results are invalid.

---

### People are more error-prone after committing an error [^ad89f139]. Nature Communications (2024). High credibility.

To address these shortcomings, we examined post-error effects using a forced-response paradigm which controls response time and treats processing time as an independent variable. In this paradigm (Fig. 1A), participants are cued to respond at the same time on each trial while the onset of the target stimulus is uniformly varied in a 4-alternative forced choice stimulus–response task (Fig. 1B). This allows us to examine post-error effects on processing per se by controlling the time of response initiation to query the state of cognitive processing as time unfolds. We use these data to fit a model that makes minimal assumptions about the cognitive processes leading to a response: (1) each stimulus leads to the preparation of the appropriate response with some mean latency and normally distributed trial-to-trial variability; (2) if stimulus-based response preparation is not yet complete, participants will guess randomly; and (3) action selection following perceptual processing is not perfect and "slips of action" sometimes occur. These assumptions lead directly to four free parameters in this model: α, the likelihood of a correct response given no response has been prepared (i.e.g.uessing); μ, the average speed of cognitive processing underlying a correct response; σ, the standard deviation (i.e. trial-to-trial variability) of the speed of cognitive processing; and β, the probability a correct response will be produced when this cognitive processing is complete (i.e. the "efficacy" of stimulus-based action selection). 1 − β is the probability that an action slip occurs even if it is very likely that enough time has elapsed for stimulus-based action selection to occur. (Note that the model is agnostic as to whether action slips are due to problems at the time of response initiation or during response preparation itself.) When combined, these parameters can be used to predict accuracy when the amount of time given for stimulus–response processing is known (Fig. 1C; see Methods for complete modeling details). In combination with controlling the time of response initiation, this modeling framework allows us to distinguish among several distinct ways in which cognitive processing might be affected following an error. Following an error, stimulus–response processing might be slower (Fig. 2A), more variable (Fig. 2B), or simply less effective at producing the correct response resulting in more frequent slips of action (Fig. 2C). Each of these possibilities would map onto a quantitative change in just one of the parameters of our model. As seen in the bottom row of Fig. 2, simulated data from our model shows that each parameter uniquely controls a certain aspect of a participant's speed-accuracy tradeoff function (e.g. slower processing coinciding with a change in μ would shift the psychometric function to the right without affecting its slope or asymptote).

---

### Statistics review 7: correlation and regression [^4cb8dd2d]. Critical Care (2003). Low credibility.

The present review introduces methods of analyzing the relationship between two quantitative variables. The calculation and interpretation of the sample product moment correlation coefficient and the linear regression equation are discussed and illustrated. Common misuses of the techniques are considered. Tests and confidence intervals for the population parameters are described, and failures of the underlying assumptions are highlighted.

---

### Reconciling time and prediction error theories of associative learning [^4b7d07a0]. Nature Communications (2025). High credibility.

We now specify the key features of timescale invariance required to recapitulate the behavioral phenomena of interest. Informally, we would like our model to capture the notion that the animal builds a histogram based on past stimulus-reward intervals. The histogram's bins are spaced uniformly on a logarithmic scale, and thus the width of a histogram bin is proportional to the bin's location. Formally,
Each class μ is associated with a timescale τ μ. The timescales τ μ are spaced uniformly on a logarithmic scale; that is, τ μ +1 = (1 + k) τ μ, where k ≪ 1. The smallest and largest timescales are thus τ 1 and τ K = (1+ k) K −1 τ 1, respectively. We assume that the support of the distribution spans many orders of magnitude, i.e. τ K / τ 1 ≫ 1, which implies K ≫ 1 if k ≪ 1.
The emission probabilities ϕ μ (t) for all μ have the form, where ϕ (x) is a density function that normalizes to one. Thus, the functions ϕ μ tile the interval axis from τ 1 to τ K, and the width of ϕ determines how much smoothing is applied when inferring the probability density from finite data. A reasonable choice is to require that ϕ μ has width proportional to the difference in adjacent timescales, Δ τ μ = k τ μ. Two possible choices for ϕ are: (a) a uniform distribution where ϕ μ (t) = 1/(k τ μ) when τ μ ≤ t ≤ τ μ +1 and zero otherwise; and (b) a Gamma distributionfor.

Inference

Exact inference in this model is challenging due to the "assignment" problem. For instance, consider a scenario where the causal stimulus c (cue) appears thrice at times t 1 < t 2 < t 3, and the target stimulus r (reward) appears thrice afterward at times s 1 < s 2 < s 3, with t 3 < s 1. Exact inference would involve iterating over the six possible assignments of the three causal cues with the three rewards. Unless the separation between successive rewards is significantly larger than the cue-reward interval, the number of possible assignments increases exponentially with the number of presentations of c and r in the worst case scenario. We outline a method for performing approximate inference in Supplementary Note 2, although our analysis of the standard conditioning protocol later allows for exact inference due to its trial structure.

---

### Evaluation of ocular residual astigmatism in eyes with myopia and myopic astigmatism and its interaction with other forms of astigmatism [^3442e2b7]. Clinical Ophthalmology (2022). Medium credibility.

Step 3: Conversion of Astigmatism from Polar to Vector Form

Different forms of astigmatism measured for each eye (RA, ACA, PCA, TNP) are converted from their polar notation of cylinder and axis to the corresponding vector notation of J 0 and J 45 using the following equations by Thibos and Hornerand corresponding Microsoft Excel functions as demonstrated by Millerwhere J 0 is the component of astigmatism that can be represented by the power of a Jackson's cross cylinder at 0°/180°, J 45 is the component of astigmatism that can be represented by the power of a Jackson's cross cylinder at 45°, C is the polar cylinder of astigmatism, and α is the polar axis of astigmatism.

J0 = -(C+0.00001)/2*COS(2*α*PI/180)

J45 = -(C+0.00001)/2*SIN(2*α*PI/180)

As Miller explains, these equations include conversion from degrees to radians, as this is the form accepted by Microsoft Excel for transcendental functions, as well as a very small number to negate the need to test for 0 or special handling.

Step 4: Calculation of ORA in Vector Notation

ORA is calculated for each eye in vector notation by subtracting the corresponding J 0 and J 45 of ACA from RA. In order to evaluate whether accounting for the posterior cornea affects ORA, we also calculated ORA by subtracting TNP from RA.

Step 5: Conversion of ORA from Vector to Clinical Form

ORA is then converted from vector notation to polar notation using the following Microsoft Excel functionbased on the equations by Thibos and Horner.

C = 2* SQRT(J0^2+J45^2)

α = 0.5*(ATAN2(J0, J45)*180/PI)+90

Step 7: Statistical Analysis

Data analysis was performed using the software SPSS (Statistical Package for the Social Sciences) version 26. Statistical tests were then performed including the Kolmogorov–Smirnov test of normality, the Wilcoxon signed-rank test to compare non-parametric paired sample data, and the Spearman's rank correlation coefficient for the direction and strength of correlation between non-normally distributed variables. Statistical significance in this study is defined as a p-value < 0.05, and was considered highly statistically significant if P ≤ 0.001.

---

### Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks [^be171ab3]. Nature Communications (2021). High credibility.

With this setup, we calculate the generalization error of kernel regression for any kernel and data distribution to be (Methods and Supplementary Note 2):We note that the generalization error is the sum of a σ -independent term and a σ -dependent term, the latter of which fully captures the effect of noise on generalization error.

Formally, this equation describes the typical behavior of kernel regression in a thermodynamic limit that involves taking P to infinity. In this limit, variations in kernel regression's performance due to the differences in how the training set is formed, which is assumed to be a stochastic process, become negligible. The precise nature of the limit depends on the kernel and the data distribution. In this work, we consider two different analytically solvable cases and identify natural scalings of N and D with P, which in turn govern how the kernel eigenvalues η ρ scale inversely with P. We further give the infinite- P limits of Eq. (4) explicitly for these cases. In practice, however, we find that our generalization error formula describes average learning curves very well for finite P for even as low as a few samples. We observe that the variance in learning curves due to stochastic sampling of the training set is significant for low P, but decays with increasing P as expected.

We will demonstrate various generalization phenomena that arises from Eq. (4) through simulations and analytical study. One immediate observation is the spectral bias: faster rates of convergence of the error along eigenfunctions corresponding to higher eigenvalues in the noise-free (σ 2 = 0) limit. The generalization error can be decomposed into a sum of modal errors, where each normalized mode errorrepresents the contribution of the mode error due to estimation of the coefficient for eigenfunction ψ ρ (Methods). The normalized mode errors are ordered according to their eigenvalues for all P (Methods)which implies that modes ρ with large eigenvalues η ρ are learned more rapidly as P increases than modes with small eigenvalues.

An important implication of this result is that target functions acting on the same data distribution with higher cumulative power distributions C (ρ), defined as the proportion of target power in the first ρ modesfor all ρ ≥ 1 will have lower generalization error normalized by total target power, E g (P)/ E g (0), for all P (Methods). Therefore, C (ρ) provides a measure of the compatibility between the kernel and the target, which we name task-model alignment.

---

### The singularity response reveals entrainment properties of the plant circadian clock [^32652250]. Nature Communications (2021). High credibility.

Here, the phase variable ϕ in g (ϕ) is defined as the phase at the start of the pulse. By solving Eq. (8), we can obtain the cellular-PRCs g (ϕ) for various parameter values of, α, and Δ t (Supplementary Methods).

Next, from the derived g (ϕ), we define the SR quantities R ′ and Θ′ as follows:

R ′ and Θ′ are determined from the integral on the right-hand side of Eq. (10). This right-hand side shows that cellular oscillators are uniformly distributed over all phases (from 0 to 2π rad) before stimulation (i.e. singularity state), and also show that each oscillator undergoes a phase change of g (ϕ) by the stimulation. R ′ and Θ′ represent the order parameter (synchronization index), and the mean phase of the population of cellular oscillators after the stimulation. R ′ is independent of α because, if α changes, only the mean phase changes in the phase distribution after a stimulus (detailed in Supplementary Methods). Thus, R ′ is parameterized only by(and Δ t) as shown in Supplementary Fig. 2a. Because R ′ increases from 0 to 1 monotonously with, is uniquely determined by R ′. α is also uniquely determined by Θ′, which ranges from 0 to 2π rad (Supplementary Fig. 2b). Using the estimated values forand α, the PRC g (ϕ) can be recovered by Eqs. (8) and (9).

In practice, given an experimental data for SR with R ′ and Θ′, the parametersand α were inversely estimated step-by-step. First, by minimizing a square error between the experimental R ′ and the theoretical R ′ by generalized reduced gradient method, the parameterwas estimated. Next, the parameter α was estimated to fit the theoretical Θ′ to the experimental Θ′. The functional relationships between R ′-to-and Θ′-to- α indicate that they are in a one-to-one correspondence, making the estimation procedure straightforward without any local minima (Supplementary Fig. 2). Using these results, all experimental PRCs were estimated successfully, as shown in Fig. 2 and Supplementary Fig. 4.

---

### Confidence controls perceptual evidence accumulation [^e733d95b]. Nature Communications (2020). High credibility.

Computational modelling

A computational model was defined based on. The model takes the Bayesian optimal accumulation of sensory evidence in this task, and disrupts this process with several sources parametrically defined suboptimalities. The Bayesian optimal observer is assumed to know the category means, and the concentration, κ = 0.5, and takes the evidence in favour of category ψ (ψ = 1 or ψ = 2) based on the orientation presented on a specific trial, for a specific sample n, as the probability of the orientation θ n given each category:Where I 0 (·) is the modified Bessel function of order 0. The optimal observer then chooses the category ψ with the greatest posterior probability over all samples for that trial, T (T varies from trial to trial), given a uniform category prior,:

This is achieved by accumulating the log probabilities of each category, given each orientation presented in the sequence:

Given the evidence for each category is perfectly anti-correlated over the stimulus orientations, the evidence from each sample can be summarised as:and the optimal observer sums this evidence over all samples in the trial (T):

Such that the Bayesian optimal decision is 1 if z > 0 and 2 if z ≤ 0.

This optimal decision-making was disrupted by several sources of suboptimality in order to account for each observer's behaviour. First, variability is added to the evidence accumulation process, such that independent and identically distributed (i.i.d) noise, ε n, is added to each evidence sample. The noise is Gaussian distributed with zero mean, and the degree of variability parameterised by σ, the standard deviation:

---

### ASPEN safe practices for enteral nutrition therapy [formula: see text] [^3565c3fc]. JPEN: Journal of Parenteral and Enteral Nutrition (2017). Medium credibility.

ASPEN Safe Practices — measuring and ensuring accurate enteral nutrition (EN) volume infused provides recommendations and supporting rationale. Recommendations state: "Do not rely on pump rate and volume settings alone to determining the amount of feeding infused", "Document the volume of EN and other fluid administered and investigate when suboptimal nutrition and fluid seems to have been delivered", "Monitor nutrition and fluid trends, including any gaps in delivery", and "Implement methods to ensure that adequate nutrition is being administered for patients who continue EN after they transition from acute care to another setting". Ordering should be tailored to accuracy: "Consider volume-based feeding schedules where a specific volume is to be infused in a 24-hour period" and "Use an easily measurable volume, such as one or two 1-liter containers/d or 2 cartons (cups) of feeding per EN 'meal', in orders for EN in the home care setting". Oversight is reinforced: "Institute systems to embed accountability and oversight for accurate delivery of nutrition intake, including methods of ordering and documenting actual intake. Have policies and procedures to determine whether systems are suboptimal or break down, and use system improvement methods to address problems", and "Encourage use of electronic connectivity between the enteral pump and the intake portion of the EHR to document EN volume infused". Rationale notes that "Volume-based ordering has been recommended over rate-based ordering for more accurate EN delivery", that unresolved discrepancies can leave "why 100 mL of EN remains after an overnight infusion", that specificity can improve accountability such as "2 cartons/cans/cups of feeding 3 times per day or one 1000-mL container per night", and that pump performance contributes to underdelivery since "Deficits of 0.5%-21% have been observed".

---

### KDOQI US commentary on the KDIGO 2024 clinical practice guideline for the evaluation and management of CKD [^cbe080e9]. American Journal of Kidney Diseases (2025). High credibility.

Evaluation of albuminuria — initial testing, confirmation, and interpretation: Use the following measurements for initial testing of albuminuria (in descending order of preference), and in all cases, a first void in the morning midstream urine sample is preferred. If measuring urine protein, use urine protein-to-creatinine ratio (PCR), reagent strip urinalysis for total protein with automated reading, or reagent strip urinalysis for total protein with manual reading. Use more accurate methods when albuminuria is detected using less accurate methods, confirm reagent strip positive albuminuria and/or proteinuria by quantitative laboratory measurement and express as a ratio to urine creatinine wherever possible, and confirm ACR ≥ 30 mg/g (≥ 3 mg/mmol) on a random untimed urine with a subsequent first morning void in the morning midstream urine sample. Understand factors that may affect interpretation of measurements of urine albumin and urine creatinine and order confirmatory tests as indicated.

---

### Learning shapes cortical dynamics to enhance integration of relevant sensory input [^5f42e6ef]. Neuron (2023). Medium credibility.

Non-normal dynamics (Figure S1)

We derived expressions relating linear Fisher Information to the dynamics of an arbitrary normal or non-normal network (subject to the same approximations described above). These expressions had a simple and interpretable form in three special cases: two-dimensional networks, normal networks, and non-normal networks with strong functionally-feedforward dynamics. Related findings have been presented previously.

To illustrate our analytical findings for the two-dimensional case, we constructed networks with modes m 1 = [cos θ 1; sin θ 1], m 2 = [cos θ 2; sin θ 2]. Figure S1A was constructed using the same procedure as for Figure 2, but this time with τ 1 = 10, τ 2 = 5. For Figure S1B we chose input with isotropic covariance Σ η = I 2 (where I N is the N x N identity matrix) and Δ g = g (s 2) - g (s 1) = [1; 0]. These inputs were chosen in order to demonstrate the influence of non-normality as clearly as possible. We set τ 1 = 10, τ 2 = 1,5,7.5,9 and varied θ 1, θ 2 from – π /2 to π /2 for each value. For each network (defined by the parameters θ 1, θ 2, τ 1, τ 2 using the procedure described for Figure 2), the Fisher Information of the stationary state network responsewas computed by substituting the long-run solution for the mean Δ r = – A –1 Δ g and the numerical solution to the Lyapunov equation for Σ (described above). We normalized this linear Fisher Information by the maximum achievable SNR in any normal network with the same time constants by defining. For each network, we computed the information-limiting correlations as ρ ILC = Δ r T ΣΔ r /(Δ r T Δ r Trace(Σ)). For each choice of τ 2, we computed the Pearson correlation between the Fisher information and the information-limiting correlations corr(𝓘 F, ρ ILC), where the correlation was computed over a set of networks spanning the range of θ 1, θ 2 ∈ [- π /2, π /2). We computed this correlation for various settings of Σ η = [v 1, v 2] [λ 1, 0; 0, λ 2] [v 1, v 2] T, by varying the angle of its principal eigenvector v 1 from Δ g and the ratio of its two eigenvalues λ 2 / λ 1 with λ 1 = 1 and λ 2 ∈ [0, 1].

---

### Toroidal topology of population activity in grid cells [^832ecbb4]. Nature (2022). Excellent credibility.

Idealized torus models

To compare the results of both the original and simulated grid cell networks with point clouds where the topology is known, a priori, to be toroidal, points were sampled from a square and a hexagonal torus. First, a 50 × 50 (angle) mesh grid (θ 1, θ 2) was created in the square [0,2π)×[0,2π) and slight Gaussian noise (ϵ = 0.1⋅ N (0,1)) was added to each angle. The square torus was then constructed via the 4D Clifford torus parametrization: (cos(θ 1), sin(θ 1), cos(θ 2), sin(θ 2)). The hexagonal torus was constructed using the 6D embedding: (cos(θ 1), sin(θ 1), cos(a 1 θ 1 + θ 2), sin(a 1 θ 1 + θ 2), cos(a 2 θ 1 + θ 2), sin(a 2 θ 1 + θ 2)), where a 1 = 1/√3 and a 2 = −1/√3.

Histology and recording locations

Rats were given an overdose of sodium pentobarbital and were perfused intracardially with saline followed by 4% formaldehyde. The extracted brains were stored in formaldehyde and a cryostat was used to cut 30-µm sagittal sections, which were then Nissl-stained with cresyl violet. The probe shank traces were identified in photomicrographs, and a map of the probe shank was aligned to the histology by using two reference points that had known locations in both reference frames: (1) the tip of the probe shank; and (2) the intersection of the shank with the brain surface. In all cases, the shank traces were near-parallel to the cutting plane, therefore it was deemed sufficient to perform a flat 2D alignment in a single section where most of the shank trace was visible. The aligned shank map was then used to calculate the anatomical locations of individual electrodes (Extended Data Fig. 1).

---

### Perceived age discrimination in the second half of life: an examination of age, period, and cohort effects [^72f4c81c]. Innovation in Aging (2023). Medium credibility.

Analysis

First, we explored the data by estimating the age-standardized incidence (by using the proportion of age-specific population over all the waves as the standard structure) to inspect the first trend of incidence of perceived age discrimination. Next, we relied on a set of APC models to determine the separate effects of aging (age) factors, contextual (period) factors, and generational (cohort) factors in perceived age discrimination. APC models represent a descriptive device that can partially determine the effects separately. Due to the identification problem (a perfect collinearity between the three dimensions of age, period, and cohort), we had to resort into a particular strategy to successfully model any outcome variable because of the potential infinite solutions. It is important to note that there is a variety of ways in which APC models can be conducted. No particular method is perfect, and all methods have certain limitations. We followed the approach suggested by, which considers age, period, and cohort as continuous dimensions and could be an extension of the method of constrained dimensions in order to have estimable functions.

To avoid perfect collinearity, this method constrains (one or two) dimensions (generally period or cohort), by making the average trend effect equal to 0 and presenting risk ratios compared to the average trend. These effects, known as nonlinear, second-order, or curve effects, are fully identifiable and independent of the chosen parameterization. This method also assumes that the linear drift (the linear change) is attributed to the nonconstrained dimension and presents risk ratios to a selected reference category. This assumption, mathematically arbitrary, is largely driven by theory. Age is expressed in age-specific rates, depending on a reference period or cohort, generally by the choice of the user. Because we do not have a priori any theoretical insights telling us how the linear effects should behave in this case, we opted to constrain both period and cohort dimensions, getting second-order effects that are fully identifiable, only focusing on identifying nonlinear effects that are fully identifiable, and getting cross-sectional age effects (based on the reference period, 2007; e.g. 2006–2008 data collection, combined). These sets of flexible APC models are available in the Epi Package in R free software.

---

### Significance, errors, power, and sample size: the blocking and tackling of statistics [^3bcc4e96]. Anesthesia and Analgesia (2018). Low credibility.

Inferential statistics relies heavily on the central limit theorem and the related law of large numbers. According to the central limit theorem, regardless of the distribution of the source population, a sample estimate of that population will have a normal distribution, but only if the sample is large enough. The related law of large numbers holds that the central limit theorem is valid as random samples become large enough, usually defined as an n ≥ 30. In research-related hypothesis testing, the term "statistically significant" is used to describe when an observed difference or association has met a certain threshold. This significance threshold or cut-point is denoted as alpha (α) and is typically set at .05. When the observed P value is less than α, one rejects the null hypothesis (Ho) and accepts the alternative. Clinical significance is even more important than statistical significance, so treatment effect estimates and confidence intervals should be regularly reported. A type I error occurs when the Ho of no difference or no association is rejected, when in fact the Ho is true. A type II error occurs when the Ho is not rejected, when in fact there is a true population effect. Power is the probability of detecting a true difference, effect, or association if it truly exists. Sample size justification and power analysis are key elements of a study design. Ethical concerns arise when studies are poorly planned or underpowered. When calculating sample size for comparing groups, 4 quantities are needed: α, type II error, the difference or effect of interest, and the estimated variability of the outcome variable. Sample size increases for increasing variability and power, and for decreasing α and decreasing difference to detect. Sample size for a given relative reduction in proportions depends heavily on the proportion in the control group itself, and increases as the proportion decreases. Sample size for single-group studies estimating an unknown parameter is based on the desired precision of the estimate. Interim analyses assessing for efficacy and/or futility are great tools to save time and money, as well as allow science to progress faster, but are only 1 component considered when a decision to stop or continue a trial is made.

---

### Experimental noise cutoff boosts inferability of transcriptional networks in large-scale gene-deletion studies [^2ddd1736]. Nature Communications (2018). Medium credibility.

PRC

We aim at inferring direct interactions between N observable molecular components, such as transcripts or proteins, by measuring their concentrations. We define byan N dimensional vector that represents the logarithm of these concentrations, which is the natural scale where experimental data are reported. We assume that the available data set has been generated from P perturbation experiments, which may also include experimental replicates. We further assume that the molecular targets of the perturbations are known, as it is the case for gene knockout or knockdown experiments. The elements of the interaction matrixdefine the strengths of the directed interactions among the molecular components, for example, A ij quantifies the direct impact of component j on component i. Given the available experimental data, our aim is to correctly classify the off-diagonal elements of A as zero or non-zero to obtain the structural organisation of the interaction network. We assume that the observed component abundance on log-scale, y obs, differs from the true value, y, by additive measurement noise, which is characterised by zero mean, and variance, with I N the N dimensional identity matrix. We assume that the observed data can by described to sufficient accuracy by a linear stationary modelwith A negative definite to ensure stability. Equations of this type typically arise from linear expansion of a non-linear model around a reference state, y ref. Linear models are usually preferred for network inference a on larger scale, as the amount of data often limit model complexity and the fact that linear models can give surprisingly good resuits for non-linear cases. The perturbation vector u reflects perturbations that persist long enough to propagate through the network, such as mutations that affect gene activity. Here, u is defined such that for u = 0 the system approaches the reference state y = y ref. Note that the reference state, y ref is not necessarily the unperturbed state but could be also defined as the average over perturbed and unperturbed states. We assume that the perturbation forces are sampled from a standard normal distribution, with meanand covariance matrix. The identity matrix is a consequence of the fact that we can absorb the associated standard deviations of the perturbative forces, u, in the matrix. We introduce normal distributed perturbations for mathematical convenience, as this implies that also y is normal distributed and the resulting maximum likelihood approach is analytically solvable. In general, only the positions of the non-zero elements of B are known from the experimental setup but their actual values are unknown. Using a linear model that operates on log-scale of physical quantities implies that only perturbations can be modelled that act multiplicatively on molecular concentrations. Fortunately, most enzymatic reactions typically fall into this class, such as sequestration and inhibition by other components and also knockout and knockdown experiments can be described on multiplicative level. From Eq. (1) we can derive a relation between the interaction matrix A and the covariance matrix of observed component abundancesWe exploit Eq. (2) to infer directed networks from correlation data. Here, we assume that component abundances are obtained from averaging over a large number of cells. In this case, fast fluctuating perturbations that arise from thermal noise and can be observed only on single-cell level average out. To infer the interaction matrix, A, we start with singular value decomposition of the matrix product A −1 B with U and V orthogonal matrices and Σ a diagonal matrix containing the singular values. The negative definite matrix A has full rank and hence is invertible. In the following, we show that it is possible to infer the strength of a directed link between a sender node j and a receiver node i, if all direct perturbations on receiver node i are removed from the data set and if a significant partial correlation between i and j exists. Removing the perturbation data for node i implies that the matrix B has at least one zero entry. As a consequence, N 0 ≥ 1 singular values are zero–as in general not all nodes are perturbed–and the corresponding rows of U span the left nullspace of A −1 B. In the absence of fast fluctuating perturbations, γ = 0, we can rewrite the covariance matrix as

---

### IMATINIB tablet… [^2df2d1f3]. FDA (DailyMed) (2025). Medium credibility.

DOSAGE FORMS AND STRENGTHS Tablets: 100 mg and 400 mg CONTRAINDICATIONS None. 2 DOSAGE AND ADMINISTRATION 2. 1 Drug Administration
2. 2 Adult Patients With Ph+ CML CP, AP, or BC 2. 3 Pediatric Patients With Ph+ CML CP
2. 4 Adult Patients With Ph+ ALL 2. 5 Pediatric Patients With Ph+ ALL. 3 DOSAGE FORMS AND STRENGTHS 4 CONTRAINDICATIONS.

1. 2 Ph+ CML in Blast Crisis, Accelerated Phase or Chronic Phase After Interferon-alpha Therapy Patients with Philadelphia chromosome positive chronic myeloid leukemia in blast crisis, accelerated phase, or in chronic phase after failure of interferon-alpha therapy.
2. 3 Adult Patients With Ph+ Acute Lymphoblastic Leukemia Adult patients with relapsed or refractory Philadelphia chromosome positive acute lymphoblastic leukemia.

1.

4 Pediatric Patients With Ph+ Acute Lymphoblastic Leukemia Pediatric patients with newly diagnosed Philadelphia chromosome positive acute lymphoblastic leukemia in combination with chemotherapy.

1. 7 Hypereosinophilic Syndrome and/or Chronic Eosinophilic Leukemia Adult patients with hypereosinophilic syndrome and/or chronic eosinophilic leukemia who have the FIP1L1-PDGFRα fusion kinase and for patients with HES and/or CEL who are FIP1L1-PDGFRα fusion kinase negative or unknown.
2. 7 Adult Patients With ASM Determine D816V c-Kit mutation status prior to initiating treatment. Ph+ ALL ANC less than 0. 5 x 109/L and/or platelets less than 10 x 109/L 1.
2. 3.
4. Check if cytopenia is related to leukemia If cytopenia is unrelated to leukemia, reduce dose of imatinib mesylate tablets to 400 mg.

---

### How do you design randomised trials for smaller populations? A framework [^9ac3f0db]. BMC Medicine (2016). Low credibility.

Exploring less common approaches to reducing sample size

We now consider some less standard approaches to bringing the sample size requirements closer to the numbers it is feasible to recruit in a reasonable time frame.

Step 3: Relaxing α by a small amount, beyond traditional values

The much-criticised 5% significance level is used widely in much applied scientific research, but is an arbitrary figure. It is extremely rare for clinical trials to use any other level. It may be argued that this convention has been adopted as a compromise between erroneously concluding a new treatment is more efficacious and undertaking a trial of an achievable size and length. Settings where traditionally sized trials are not possible may be just the area where researchers start to break this convention, for good reason.

In considering the type I error, it is critical to consider the question: 'What are the consequences of erroneously deciding to use a new treatment routinely if it is truly not better?'

Taking the societal perspective as before, we might consider the probability of making a type I error, thus erroneously burdening patients with treatments that do not improve outcomes, or even worsen them, while potentially imposing unnecessary toxicity.

First, for conditions where there are only enough patients available to run one modestly sized randomised trial in a reasonable time frame, research progress will be relatively slow, and making a type I error may be less of a concern than a type II error. In contrast, making several type I errors in a common disease could lead in practice to patients taking several ineffective treatments; for a disease area where only one trial can run at any given time, the overall burden on patients is potentially taking one ineffective treatment that does not work.

Thus, if we take the societal perspective with the trials in Table 1 then, if each trial was analysed with α = 0.05 and we see (hypothetically) 40% positive results, then the expected number of false positive trials is given in the final column. We also assumed 10% and 70% positive results, with qualitatively similar conclusions.

---

### Geriatric emergency department guidelines [^a38d649e]. Annals of Emergency Medicine (2014). Medium credibility.

Geriatric emergency department guidelines — cognitive screening instructions state: "Now I would like to ask you some questions to check your memory and concentration. Some of them may be easy and some of them may be hard". Items include "1) What year is it now?" and "2) What month is this?" with scoring "(0) (1)". The patient repeats "John Brown, 42 Market Street, Chicago" with "Trials to learning ______ (if unable to do in 3 trials = C)". For time, "Without looking at your watch or the clock, tell me what time it is", with "(within 1-hour) ______ Correct Incorrect (0) (1)" and "Actual time: ________". "4) Count aloud backwards from 20 to 1" is scored "0 1 2 Errors", with the note "If subject starts counting forward or forgets the task, repeat instructions and score one error", and the sequence "20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1". "5) Say the months of the year in reverse order", showing "D N O S A JL JN MY AP MR F J" and "0 1 2 Errors", with the rule "If the tester needs to prompt with the last name of the month of the year, one error should be scored". "6) Repeat the name and address you were asked to remember" shows "(John Brown, 42 Market Street, Chicago) 0 1 2 3 4 5 Errors".

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^bb9b7f12]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Δ Φ, the probability distribution at a point Φ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Φ ∈ {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters α and β, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since α and β are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Φ ∈ [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(α, β) and x 2 = 1 − x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Φ, it may seem natural to select α and β by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^73c951cd]. Nature Communications (2025). High credibility.

Methods

In these methods, our objective was to demonstrate that δ is a mean-independent measure of statistical dispersion for bounded continuous variables, that other commonly used measures of heterogeneity (of which statistical dispersion is a subset) are dependent on and, thus, inherently biased by the mean, and that this bias has, in past work, resulted in spurious observed heterogeneity relationships with other variables.

Mean-heterogeneity relationships for the standard beta and gamma distribution

Here we introduce the gamma and beta distributions for modelling lower- and double-bounded variables, respectively. For both distributions, the most frequently used heterogeneity measures have analytical derivations based solely on the parameters of the distribution, which we can use to observe mean-heterogeneity relationships without being subject to feature relationships or chance. Additionally, both distributions can be evaluated in terms of a scale/dispersion parameter that influences the spread of observations in the distribution. A parameter is interpreted as a scale/dispersion parameter when, for a constant mean, increases in the parameter always correspond to increases in the variance. We can, therefore, fix the dispersion parameter, modify the mean, and observe the mean-heterogeneity relationships. We, then, derive a method of retrieving the dispersion parameter of the gamma and beta distribution, which we denote δ L and δ 2, respectively (with the subscript referring to the type of bounds, see Equations (2) and (3)).

---

### On the noise in "augmented T1-weighted steady state magnetic resonance imaging" [^5e296aaa]. NMR in Biomedicine (2023). Medium credibility.

Recently, Ye and colleagues proposed a method for "augmented T1-weighted imaging" (aT 1 W). The key operation is a complex division of gradient-echo (GRE) images obtained with different flip angles. Ye and colleagues provide an equation for the standard deviation of the obtained aT 1 W signal. Here, we show that this equation leads to wrong values of the standard deviation of such an aT 1 W signal. This is demonstrated by Monte Carlo simulations. The derivation of the equation provided by Ye and colleagues is shown to be erroneous. The error consists of a wrong handling of random variables and their standard deviations and of the wrong assumption of correlated noise in independently acquired GRE images. Instead, the probability distribution obtained with the aT 1 W-method should have been carefully analyzed, perhaps on the basis of previous literature on ratio distributions and their normal approximations.

---

### Spontaneous variability in gamma dynamics described by a damped harmonic oscillator driven by noise [^233d16a3]. Nature Communications (2022). High credibility.

Fig. 9
A damped harmonic oscillator reproduces gamma-cycle amplitude and duration cross- and auto-correlations in the LFP.

a Correlation between the amplitude of a gamma half-cycle and the duration of 10 gamma half-cycles before and after it. These correlation coefficients were computed for synthetic signals generated from AR(2) processes with corresponding eigenvalue magnitudes ranging from 0.9 (dark blue) to 0.999 (bright yellow) in steps of approximately 0.01. The error regions show ± 2 SEM based on a bootstrap procedure. b Scatter plot of the eigenvalue magnitudes displayed in Fig. 8c and the instantaneous correlation between gamma half-cycle amplitude and duration from the corresponding LFP data. The regression fit (black line) was computed with the least-squares method. c The correlation between the amplitude of a given gamma half-cycle and the 10 gamma half-cycles before and after it (i.e. the autocorrelation) for synthetic signals generated from AR(2) processes. The error regions show ± 2 SEM based on a bootstrap procedure. d The autocorrelation for LFP data from monkey T during presentation of a full-screen drifting grating. The gray lines and gray-shaded areas depict the means and 99.9% confidence regions, after randomizing the order of duration values across trials. e Same as b, but now showing the correlation between the amplitude of a given gamma half- cycle and the amplitude of its preceding and succeeding half-cycle, pooling data points from multiple datasets, conditions, and channels. f – h Same as c – f but for gamma full-cycle durations. Source data are provided in the Source Data file.

---

### A framework for evaluating clinical artificial intelligence systems without ground-truth annotations [^f3b9d969]. Nature Communications (2024). High credibility.

Evaluate classifier

After training g ϕ, we evaluated it on a held-out set of data comprising data points with ground-truth labels (from both class 0 and class 1) (Fig. 1 b, Step 5). The intuition here is that a classifier which can successfully distinguish between these two classes, by performing well on the held-out set of data, is indicative that the training data and the corresponding ground-truth labels are relatively reliable. Since the data points from class 1 are known to be correct (due to our use of ground-truth labels), then a highly-performing classifier would suggest that the class 0 pseudo-labels of the remaining data points are likely to be correct. In short, this step quantifies how plausible it is that the sampled unlabelled data points belong to class 0.

As presented, this approach determines how plausible it is that the sampled set of data points in some probability interval, s ∈ (s 1, s 2], belongs to class 0. It is entirely possible, however, that a fraction of these sampled data points belong to the opposite class (e.g. class 1). We refer to this mixture of data points from each class as class contamination. We hypothesised (and indeed showed) that the degree of this class contamination increases as the probability output, s, by an AI system steers away from the extremes (s ≈ 0 and s ≈ 1). To quantify the extent of this contamination, however, we also had to determine how plausible it was that the sampled set of data points belong to class 1, as we outline next.

---

### Microbial production of megadalton titin yields fibers with advantageous mechanical properties [^5b459267]. Nature Communications (2021). High credibility.

To estimate the degree of orientation of the crystallites along the fiber axis, we obtained two azimuthal 1D profiles from the 2D diffraction image (Fig. 2j, k). Figure 2j shows the plot of the diffraction intensity integration as a function of azimuthal angle within the radial range of the equatorial (120) peak, and Fig. 2k is the corresponding plot within the radial range of the equatorial (200) peak. After deconvolution of the 1D profiles, we applied the calculated FWHMs of the crystalline peaks and amorphous components to Herman's orientation function (8) f crystal = (3 < cos 2 φ > −1)/2 (Supplementary Tables 2, 3) –. Here φ is the angle between the c-axis and the fiber axis and < cos 2 φ > is obtained based on the equation (9) < cos 2 φ ≥ 1 − 0.8 < sin 2 (0.4 × FWMH (200)) > − 1.2 < sin 2 (0.4 × FWMH (120)) >. The parameter f crystal is 0 for no preferred orientation and 1 if all crystallites are perfectly aligned –.

---

### Cortical evidence accumulation for visual perception occurs irrespective of reports [^fc78b121]. Nature Communications (2025). High credibility.

We estimated a latent variable for each trial by applying the set of weights from the decoder at each point in time. The threshold obtained in the training procedure did not provide the best categorization of classes, as it was focused on maximizing distances instead. We thus estimated individualized detection thresholds. per participant by taking the threshold value of the receiver operating characteristic curve that maximized the rate of true positives (hit trials correctly classified as such) to false positives (miss trials incorrectly classified as hit). In evidence accumulation analyses, hit trials for which the estimated latent variable crossed the threshold were selected for a Spearman's rank-order correlation between the timing of this threshold crossing and reaction times. In analyses on false alarms and trials in which participants reported seeing more than one face (Fig. 5), we averaged by condition the number of times that the latent variable crossed the detection threshold computed from hit and miss trials in experiment 2. Statistical significance was also calculated using permutation tests.

Computational model

The model instantiated an evidence accumulation process with a leakage factor, and included five free parameters: a drift rate with mean Γ and standard deviation s, a flat decision boundary θ, a leakage factor λ, and a non-decision time ndt. A white Gaussian noise process W (t) with zero mean and unit variance was added to the process to account for stochasticity (Eq. 1). Since neuronal responses such as high-gamma activity are positive, the evidence accumulation EA (t) process was bound to zero for biological plausibility.

The accumulation process started at the beginning of each trial with a zero drift rate. On stimulus onset S, following a non-decision time, d (t) increased to a level (Γ + sZ) log (I + 1), for 600 ms (stimulus duration) before returning to zero at stimulus offset (Eq. 2). I represents the stimulus intensity and Z a zero-mean and unit-variance Gaussian noise varying on a trial-by-trial basis.

---

### Validation of monte carlo estimates of three-class ideal observer operating points for normal data [^26b6e5d9]. Academic Radiology (2013). Low credibility.

Rationale and Objectives

Traditional two-class receiver operating characteristic (ROC) analysis is inadequate for the complete evaluation of observer performance in tasks with more than two classes.

Materials and Methods

Here, a Monte Carlo estimation method for operating point coordinates on a three-class ROC surface is developed and compared with analytically calculated coordinates in two special cases: (1) univariate and (2) restricted bivariate trinormal underlying data.

Results

In both cases, the statistical estimates were found to be good in the sense that the analytical values lay within the 95% confidence interval of the estimated values about 95% of the time.

Conclusions

The statistical estimation method should be key in the development of a pragmatic performance metric for evaluation of observers in classification tasks with three or more classes.

---

### What are the implications of alternative alpha thresholds for hypothesis testing in orthopaedics? [^fb5050c0]. Clinical Orthopaedics and Related Research (2019). Medium credibility.

Background

Clinical research in orthopaedics typically reports the presence of an association after rejecting a null hypothesis of no association using an alpha threshold of 0.05 at which to evaluate a calculated p value. This arbitrary value is a factor that results in the current difficulties reproducing research findings. A proposal is gaining attention to lower the alpha threshold to 0.005. However, it is currently unknown how alpha thresholds are used in orthopaedics and the distribution of p values reported.

Questions/Purposes

We sought to describe the use of alpha thresholds in two orthopaedic journals by asking (1) How frequently are alpha threshold values reported? (2) How frequently are power calculations reported? (3) How frequently are p values between 0.005 and 0.05 reported for the main hypothesis? (4) Are p values less than 0.005 associated with study characteristics such as design and reporting power calculations?

Methods

The 100 most recent original clinical research articles from two leading orthopaedic journals at the time of this proposal were reviewed. For studies without a specified primary hypothesis, a main hypothesis was selected that was most consistent with the title and abstract. The p value for the main hypothesis and lowest p value for each study were recorded. Study characteristics including details of alpha thresholds, beta, and p values were recorded. Associations between study characteristics and p values were described. Of the 200 articles (100 from each journal), 23 were randomized controlled trials, 141 were cohort studies or case series (defined as a study in which authors had access to original data collected for the study purpose), 31 were database studies, and five were classified as other.

Results

An alpha threshold was reported in 166 articles (83%) with all but two reporting a value 0.05. Forty-two articles (21%) reported performing a power calculation. The p value for the main hypothesis was less than 0.005 for 88 articles (44%), between 0.05 and 0.005 for 67 (34%), and greater than 0.05 for 29 (15%). The smallest p value was between 0.05 and 0.005 for 39 articles (20%), less than 0.005 for 143 (72%), and either not provided or greater than 0.05 for 18 (9%). Although 50% (65 of 130) cohort and database papers had a main hypothesis p value less than 0.005, only 26% (6 of 23) randomized controlled trials did. Only 36% (15 of 42) articles reporting a power calculation had a p value less than 0.005 compared with 51% (73 of 142) that did not report one.

Conclusions

Although a lower alpha threshold may theoretically increase the reproducibility of research findings across orthopaedics, this would preferentially select findings from lower-quality studies or increase the burden on higher quality ones. A more-nuanced approach could be to consider alpha thresholds specific to study characteristics. For example, randomized controlled trials with a prespecified primary hypothesis may still be best evaluated at 0.05 while database studies with an abundance of statistical tests may be best evaluated at a threshold even below 0.005.

Clinical Relevance

Surgeons and scientists in orthopaedics should understand that the default alpha threshold of 0.05 represents an arbitrary value that could be lowered to help reduce type-I errors; however, it must also be appreciated that such a change could increase type-II errors, increase resource utilization, and preferentially select findings from lower-quality studies.

---

### FDG PET / CT: EANM procedure guidelines for tumour imaging: version 2.0 [^d5dba515]. European Journal of Nuclear Medicine and Molecular Imaging (2015). Medium credibility.

FDG PET/CT — Strategy 4 (diagnostic CT scan) specifies the acquisition order and contrast timing: CT topogram; a deep‑inspiration thoracic CT scan with a 20 s delay from the beginning of contrast agent infusion (this CT scan is used neither for attenuation correction nor for PET/CT image fusion); a whole‑body diagnostic CT scan (with shallow breathing) with a 45 s delay after the thoracic CT scan if performed, or with a 60 s delay after the beginning of contrast agent infusion if the thoracic CT scan was not performed; then PET acquisition.

---

### Three-dimensional atomic scale electron density reconstruction of octahedral tilt epitaxy in functional perovskites [^d6c04c5c]. Nature Communications (2018). Medium credibility.

Symmetry analysis of the SHG polarimetry was performed using an analytical model described below. Fundamental field is written as (E ω cos(φ), E ω sin(φ),0) under the laboratory coordinates (X, Y, Z), and incident onto sample at an angle θ. Sample orientation can be described by β, with β = 0° for O1 and β = 90° for O2. Considering refraction and transmission at sample surface, the fundamental field E ′ ω, i inside the sample can be expressed asWhere sin(θ ′) = sin(θ)/ n, n is refractive index, and t p = 2cos(θ)/[n cos(θ)+cos(θ′)] and t s = 2cos(θ)/[cos(θ) + n cos(θ′)]are Fresnel coefficients. The SHG fieldgenerated inside the sample can be calculated by, is nonlinear SHG coefficients, or under Voigt notation,.matrix for m point group symmetry is:

To simplify the analysis, we ignored the dispersion effect, i.e. The transmitted SHG field in the laboratory coordinates is:Where. SHG intensity from sample isand, where α is a constant. In above equations. α andcan be further eliminated by defining effective SHG matrices as. For CaTiO 3 on NdGaO 3 and DyScO 3, single domain state of CaTiO 3 films is observed. Explicitly, we have following equations for orientations O1 and O2:

For CaTiO 3 on LSAT, four equivalent domains represented by different β values are considered. Under the phase uncorrelated approximation, we have following equations:Where w 1, w 2, w 3, are the area fraction of three of the four domain variants in the probed area. The fits reveal these factors to be ~1/4 each as suggested by COBRA results.

Code availability

The computer codes that support the findings of this study are available from the corresponding author upon reasonable request.

---

### Natural statistics support a rational account of confidence biases [^5353bac3]. Nature Communications (2023). High credibility.

Latent ideal observer

The previous results show that our model captures a number of established behavioral dissociations between confidence and decision accuracy. How can these dissociations be explained? One possibility is that, despite being extensively optimized to estimate confidence by predicting its own probability of being correct, the model nevertheless converged on a suboptimal heuristic strategy. An alternative possibility is that these effects reflect a strategy that is optimal given the actual distribution of the data for which the model was optimized, which may violate the assumptions underlying the presumed optimality of the BE rule. We found strong evidence to support this latter interpretation.

To answer this question, we first sought to quantitatively characterize the distribution of the model's training data. Because it is not tractable to perform ideal observer analysis directly on the model's high-dimensional inputs, we instead used unsupervised deep learning techniques to extract the low-dimensional, latent space underlying those inputs. Specifically, we used a denoising variational autoencoder (VAE; Fig. 4 a), which was trained to map a high-dimensional input x to a low-dimensional embedding z (consisting of just two dimensions), such that a denoised version of x can be decoded from z. Figure 4 b depicts a summary of the low-dimensional latent distributions extracted by the VAE. These distributions had two important properties. First, these distributions had an elliptical shape, as quantified by the ratio of the variance along the major and minor axes (σ t a r g e t / σ n o n t a r g e t = 2.44 ± 0.04 over 100 trained VAEs). Second, the distributions underlying classes s1 and s2 fell along non-parallel axes (θ s 1, s 2 = 51. 4° ± 1.7). Under these conditions, the optimal approach for estimating confidence follows a more complex function than either the BE or RCE rules, as visualized in Fig. 4 b. We then constructed an ideal observer that computed confidence according to p (correct∣ z), using the distribution of the training data in the low-dimensional space extracted by the VAE, and we evaluated this function according to the distribution of the test data in this space. This ideal observer model robustly captured both versions of the PE bias (Fig. 4 c, d), as well the dissociation between type-1 and type-2 sensitivity (Fig. 4 e), thus replicating the same dissociations displayed by our performance-optimized neural network model. A more comprehensive analysis is presented in Supplementary Figs. S4 and S5, showing that the emergence of these biases depends on sensory evidence distributions that are both asymmetrical (σ t a r g e t / σ n o n t a r g e t > 1), and non-parallel (as quantified by 0° < θ s 1, s 2 < 180°).

---

### Health care provider knowledge regarding alpha-gal syndrome-United States, March-May 2022 [^ffab8d3a]. MMWR: Morbidity and Mortality Weekly Report (2023). Medium credibility.

TABLE 1
Survey questions and responses by health care providers regarding their practice characteristics and knowledge about alpha-gal syndrome (N = 1,500) — Spring DocStyles survey, * United States, March–May 2022

Abbreviations: AGS = alpha-gal syndrome; PCR = polymerase chain reaction; sIgE = alpha-gal–specific serum IgE antibody.

* Administered by Porter Novelli.

† Evaluated together to generate a composite knowledge score.

§ Correct response.

Among all respondents who were aware of AGS, 416 (48%) reported that they did not know the correct diagnostic tests to order. One third of respondents (285; 33%) correctly reported that patients develop AGS after a tick bite, and approximately one third (272; 32%) reported not knowing how it was acquired. More than one half of the respondents (502; 58%) correctly identified topics on which to counsel AGS patients, such as tick bite prevention, eliminating red meat from their diet, exercising caution when receiving new medications and vaccines, and recognizing and managing anaphylaxis. Overall, 64% and 66% of respondents indicated that guidelines for the diagnosis and management of AGS, respectively, would be helpful clinical resources.

Among the 865 survey respondents who had heard of AGS, only 42 (5%; 95% CI = 3.1%–5.9%) correctly answered all three questions related to etiology, testing, and patient counseling (Table 2). Knowledge scores were higher among pediatricians, 12.3% of whom correctly answered all three questions, than among internists (4.2%), family practitioners (3.7%), PAs (2.6%), and NPs (0%). Knowledge scores were similar across U.S. Census Bureau regions (p = 0.44), and number of years in practice was not significantly associated with provider knowledge scores. There was an inverse relationship in knowledge scores and the number of AGS cases that HCPs reported they had diagnosed and managed (Table 2).

TABLE 2
Knowledge about alpha-gal syndrome among health care providers, overall and by region and provider characteristics (N = 865) — Spring DocStyles survey, * United States, March–May 2022

Abbreviations: FP = family practitioner; NP = nurse practitioner; PA = physician assistant; Ref = referent group.

* Administered by Porter Novelli.

†

---

### Estimating the success of re-identifications in incomplete datasets using generative models [^cfb6f216]. Nature Communications (2019). High credibility.

Results

Using Gaussian copulas to model uniqueness

We consider a dataset, released by an organization, and containing a sample ofindividuals extracted at random from a population of n individuals, e.g. the US population. Each row x (i) is an individual record, containing d nominal or ordinal attributes (e.g. demographic variables, survey responses) taking values in a discrete sample space. We consider the rows x (i) to be independent and identically distributed, drawn from the probability distribution X with, abbreviated p (x).

Our model quantifies, for any individual x, the likelihood ξ x for this record to be unique in the complete population and therefore always successfully re-identified when matched. From ξ x, we derive the likelihood κ x for x to be correctly re-identified when matched, which we call correctness. If Doe's record x (d) is unique in, he will always be correctly re-identified (and). However, if two other people share the same attribute (not unique,), Doe would still have one chance out of three to have been successfully re-identified. We modelas:and κ x as:with proofs in "Methods".

We model the joint distribution of X 1, X 2,… X d using a latent Gaussian copula. Copulas have been used to study a wide range of dependence structures in finance, geology, and biomedicineand allow us to model the density of X by specifying separately the marginal distributions, easy to infer from limited samples, and the dependency structure. For a large sample spaceand a small numberof available records, Gaussian copulas provide a good approximation of the density using only d (d − 1)/2 parameters for the dependency structure and no hyperparameter.

The density of a Gaussian copula C Σ is expressed as:with a covariance matrix Σ, u ∈ [0, 1] d, and Φ the cumulative distribution function (CDF) of a standard univariate normal distribution.

---

### Optimal preoperative assessment of the geriatric surgical patient: a best practices guideline from the American college of surgeons national surgical quality improvement program and the American Geriatrics Society [^1e2b20f8]. Journal of the American College of Surgeons (2012). Medium credibility.

Cognitive assessment: Mini-Cog provides stepwise instructions, scoring, and interpretation for word recall and a clock draw. Patients are instructed, 'Give the patient 3 tries to repeat the words. If unable after 3 tries, go to next item', and, for the clock task, 'If subject has not finished clock drawing in 3 minutes, discontinue and ask for recall items', with the clock set to '11:10 (10 past 11)'. A normal clock requires 'All numbers 1–12, each only once, are present in the correct order and direction (clockwise) inside the circle' and 'Two hands are present, one pointing to 11 and one pointing to 2'; 'ANY CLOCK MISSING ANY OF THESE ELEMENTS IS SCORED ABNORMAL. REFUSAL TO DRAW A CLOCK IS SCORED ABNORMAL'. Scoring is '3 item recall (0–3 points): 1 point for each correct word' plus 'Clock draw (0 or 2 points): 0 points for abnormal clock 2 points for normal clock'. Interpretation thresholds are 'Total Score of 0, 1, or 2 suggests possible impairment' versus 'Total Score of 3, 4, or 5 suggests no impairment', and if impairment is present, 'consider a referral to a primary care physician, geriatrician, or mental health specialist'.

---

### Standardized competencies for parenteral nutrition prescribing: the American Society for Parenteral and Enteral Nutrition model [^93a9f5c3]. Nutrition in Clinical Practice (2015). Medium credibility.

American Society for Parenteral and Enteral Nutrition (A.S.P.E.N.) recommendations — standardized parenteral nutrition (PN) prescribing state that healthcare organizations shall use a standardized process for PN management that includes clinicians with expertise in nutrition support; the primary healthcare team, in collaboration with nutrition support professionals, shall evaluate, clearly define, and accurately document the patient's medical problem(s) and indication(s) for PN and shall specify and document the therapeutic goal(s) of PN therapy; PN shall be prescribed using a standardized PN order format and review process applicable to patients of every age and disease state; institutions shall create a home parenteral nutrition (HPN) order template/format that provides a safe plan for multiple days of therapy, with the HPN prescription written to reflect trends in laboratory values and previous PN days, and an institutional daily PN order format should not be used as an HPN prescription; and the most appropriate nutrition modality should be prescribed, with healthcare organizations determining appropriate PN formulation(s) or delivery methods and developing criteria for each formulation used.

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^37daae40]. Nature Communications (2025). High credibility.

Fig. 2
The matrix-tree theorem.

A Computing the steady-state occupancy π 1 by summing weights over directed spanning trees. Directed spanning trees are subgraphs containing all graph nodes but no cycles, with edges oriented toward a root node. In each directed spanning tree, the input forces make a positive, negative, or zero contribution to the tree weight. The structural vectorsare shown below each tree; these quantities enter into Equation (3) below. B Schematic illustration of the high-dimensional space of feature vectors ψ (i; θ) and χ (i, F). The depicted arrangement of vectors could solve a binary classification problem.

We define the input multiplicityas the number of edges affected per input variable, which we assume to be the same for each input. To focus on the functional way in which the input driving enters the steady-state probabilities, the driving contributions can be factored out in the algebraic expressions for the numerator and denominator of Equation (1). This has been previously been used to make analytical progress for M = D = 1 in, for example, refs. –. This equivalent formulation of Eq. (1) suggests that steady states of Markov jump processes implement a rational polynomial function of exponentiated input variables. Defining, we rewrite the matrix-tree expression for π i for general D and M We use the multi-index, whereis the set of D input labels and each componentof the multi-index runs over the values, to enumerate themonomials. These monomials y μ (F) in Equation (2) combinatorially depend on the different mixtures μ of input driving, representing a net total μ a of signed contributions from the input force F a, μ b such contributions for F b, and so on for each input. The coefficients, which are functions of the parameters θ, are the sums of weights over all directed spanning trees rooted at node i which have the corresponding mixture μ of signed input contributions. The monomial coefficientsthus represent learnable amplitudes of each polynomial basis function y μ (F). The coefficients in the denominator are defined as. Classification will be successful if, for F ρ drawn from class ρ, the coefficientsand monomials y μ (F ρ) are large for the same μ. In the subsequent sections of the paper and in the Supplementary Information we use the formulation in Equation (2) to show how the classification ability of a non-equilibrium Markov processes may be systematically modulated.

---

### BUPIVACAINE HYDROCHLORIDE injection, solution… [^3c04db8b]. FDA (DailyMed) (2025). Medium credibility.

- **Chondrolysis with Intra-Articular Infusion**: Intra-articular infusions of local anesthetics including Bupivacaine Hydrochloride Injection following arthroscopic and other surgical procedures is an unapproved use, and there have been post-marketing reports of chondrolysis in patients receiving such infusions. 2 DOSAGE AND ADMINISTRATION 2. 1 Important Dosage and Administration Information
2. 2 Recommended Concentrations and Dosages of Bupivacaine Hydrochloride Injection/Bupivacaine Hydrochloride and Epinephrine Injection 2. 3 Use in Epidural Anesthesia
2. 4 Test Dose for Caudal and Lumbar Epidural Blocks 2. 5 Use in Dentistry
2. 6 Use in Ophthalmic Surgery. 3 DOSAGE FORMS AND STRENGTHS 4 CONTRAINDICATIONS.
2. 2 Recommended Concentrations and Dosages of Bupivacaine Hydrochloride Injection/Bupivacaine Hydrochloride and Epinephrine Injection. Table 2.

Bupivacaine Hydrochloride Injection/Bupivacaine Hydrochloride and Epinephrine Injection Concentration vs. Motor Function.
- *
- With continuous techniques, repeat doses increase the degree of motor block. The first repeat dose of 0. 5% may produce complete motor block. Intercostal nerve block with 0. 25% also may produce complete motor block for intra-thoracic and upper intra-abdominal surgery.
12. 5–175 moderate to complete 5–90 12. 5–225 Retrobulbar block
0. 75% 2–4 15–30 complete Sympathetic block.
37. 5–75 moderate Lumbar epidural block
0. 75% ‡ 10–20 75–150 complete 0. 5% † 10–20 50–100 moderate to complete
0. 25%. with epinephrine 1. 8–3. 6 per site 9–18 per site
2. 3 Use in Epidural Anesthesia.

---

### Appropriate systemic therapy dosing for obese adult patients with cancer: ASCO guideline update [^2ddf0964]. Journal of Clinical Oncology (2021). High credibility.

Regarding specific circumstances for obesity, more specifically with respect to patients with cancer, ASCO 2021 guidelines recommend to calculate body surface area using any of the standard formulae. Insufficient evidence to support one formula for calculating body surface area over another.

---

### Label: DUPIXENT-dupilumab injection, solution… [^8c348468]. FDA (DailyMed) (2025). Medium credibility.

-5914-02, 0024-5914-20, 0024-5915-00, 0024-5915-01, 0024-5915-02, 0024-5915-20, 0024-5918-00, 0024-. Prurigo Nodularis for the treatment of adult patients with prurigo nodularis. Chronic Obstructive Pulmonary Disease as an add-on maintenance treatment of adult patients with inadequately controlled chronic obstructive pulmonary disease and an eosinophilic phenotype. Dosage in Pediatric Patients 6 Years of Age and Older: Body Weight Initial Loading Dose Subsequent Dosage
- - Q2W – every 2 weeks; Q4W – every 4 weeks 15 to less than 30 kg 600 mg 300 mg Q4W 30 to less than 60 kg 400 mg 200 mg Q2W 60 kg or more 600 mg 300 mg Q2W. Prurigo Nodularis:

- Recommended dosage for adult patients is an initial dose of 600 mg, followed by 300 mg given every 2 weeks. Chronic Obstructive Pulmonary Disease:

- Recommended dosage for adult patients is 300 mg given every 2 weeks. 3 DOSAGE FORMS AND STRENGTHS 4 CONTRAINDICATIONS. 5 to less than 15 kg 200 mg every 4 weeks 15 to less than 30 kg 300 mg every 4 weeks Dosage in Pediatric Patients 6 Years of Age and Older. The recommended dosage of DUPIXENT for pediatric patients 6 years of age and older is specified in Table 2. Table 3: Dosage of DUPIXENT in Adult and Pediatric Patients 12 Years and Older with Asthma Initial Loading Dose Subsequent Dosage 400 mg 200 mg every 2 weeks Or 600 mg 300.

---

### Fluctuations in instantaneous frequency predict alpha amplitude during visual perception [^11121a73]. Nature Communications (2017). Medium credibility.

Correlations with observed alpha amplitude

We correlated PaA timecourses — computed by passing instantaneous frequency through the amplitude look-up-table — with empirically observed amplitude timecourses on a timepoint-by-timepoint basis. Note that these correlations emphasize the similarity of the timecourses as opposed to matching the exact scaling of the PaA with respect to the scale of the empirically observed data. Indeed, differences in the overall magnitude of PaA and the observed amplitude vary because (a) wavelet transforms were used to estimate the look-up-tables while Hilbert transforms on bandpassed data were used to generate empirical estimates of alpha amplitude and (b) stable amplitude look-up-tables result from averaging many trials, and thus PaA reflect these average magnitudes as opposed to the single trial magnitudes for observed amplitude. We used wavelets to generate the look-up-tables so that we could increase the frequency resolution of our look-up-tables (i.e. smaller step sizes along the x -axis in Fig. 1b). In Experiment 1, we computed correlations over 2000 ms epochs centered on target presentation. In Experiment 2, we computed correlations over 4000 ms epochs locked to an attentional cue that occurred 0.5 s into each trial.

Phase locking and phase bifurcation index

To make contact with previous papers, we also examined the relationship between alpha phase and behavioral performance. We first computed the intertrial phase locking index (PLI) by applying Hilbert transforms to data bandpassed around peak alpha as described previously. PLI was estimated from the complex values obtained from this Hilbert transform at time t over trials 1 to k using the formula:Whereis the same complex representation of the data as outlined in the amplitude section above. This value ranges from 0 to 1 (no phase locking to perfect phase locking at any phase). As stimulus onset was unpredictable, pre-stimulus alpha phase should be randomly distributed over all trials. Thus, we computed a phase bifurcation index from PLI to assess whether any observed phase locking occurred at the same or opposite phases between accuracy conditions (as described in). Bifurcation was computed over correct and incorrect trials at time t:

Note that this value ranges from 1 (perfect phase locking in both conditions at opposite phases, leading to PLI all = 0) to −1 (perfect phase locking in only one condition). Values close to zero indicate random phase distributions for correct and incorrect trials (Supplementary Figure 4).

---

### Excitation of coupled spin-orbit dynamics in cobalt oxide by femtosecond laser pulses [^453906ec]. Nature Communications (2017). Medium credibility.

When a material has high symmetry, such as cubic symmetry, the parameters G 1, G 2, G 3 and G 4 are zero and the amplitude of the magnetic oscillation induced by the ICME gives sin 2 θ cos ψ dependence in Eq. (2). This implies that its sign changes when the pump azimuthal angle θ is changed from 45° to 135° for linearly polarized light (i.e. ψ = 0°), and that it vanishes for circularly polarized light (ψ = ± 90°). Until very recently, magnetic excitations with only this particular polarization dependence were considered to originate from the ICME. However, the magnon modes experimentally observed in the TG never exhibit standard sin 2 θ cos ψ dependence. In a crystal with a reduced symmetry like in CoO, other G -related terms, G 1 – G 4, can also appear in Eq. (2). These terms may lead to magnetic oscillations induced by the ICME that are independent of helicity (ψ) and proportional to cos 2 θ or (1 − cos 2 θ). It is noteworthy that the (1 − cos 2 θ) contribution from ICME does not vanish even for circularly polarized light with cos 2 θ = 0 and the amplitude for circular polarization should be equal to that for linear polarization with θ = 45° and 135°, as mentioned above. This relationship enables other mechanisms to be excluded, e.g. thermal excitation. This helicity-independent excitation of the magnon modes by circularly polarized light is a non-standard manifestation of the ICME and is a characteristic of materials with low symmetry.

---

### Relational integration demands are tracked by temporally delayed neural representations in alpha and beta rhythms within higher-order cortical networks [^1c34a120]. Human Brain Mapping (2025). Medium credibility.

Relational reasoning is the ability to infer and understand the relations between multiple elements. In humans, this ability supports higher cognitive functions and is linked to fluid intelligence. Relational complexity (RC) is a cognitive framework that offers a generalisable method for classifying the complexity of reasoning problems. To date, increased RC has been linked to static patterns of brain activity supported by the frontoparietal system, but limited work has assessed the multivariate spatiotemporal dynamics that code for RC. To address this, we conducted representational similarity analysis in two independent neuroimaging datasets (Dataset 1 fMRI, n = 40; Dataset 2 EEG, n = 45), where brain activity was recorded while participants completed a visuospatial reasoning task that included different levels of RC (Latin Square Task). Our findings revealed that spatially, RC representations were widespread, peaking in brain networks associated with higher-order cognition (frontoparietal, dorsal-attention, and cingulo-opercular). Temporally, RC was represented in the 2.5–4.1s post-stimuli window and emerged in the alpha and beta frequency range. Finally, multimodal fusion analysis demonstrated that shared variability within EEG-fMRI signals within higher-order cortical networks were better explained by the theorized RC model, relative to a model of cognitive effort (CE). Altogether, the results further our understanding of the neural representations supporting relational processing, highlight the spatially distributed coding of RC and CE across cortical networks, and emphasize the importance of late-stage, frequency-specific neural dynamics in resolving RC.

---

### Clinicians are right not to like cohen's κ [^a5f2e987]. BMJ (2013). Excellent credibility.

Clinicians are interested in observer variation in terms of the probability of other raters (interobserver) or themselves (intraobserver) obtaining the same answer. Cohen's κ is commonly used in the medical literature to express such agreement in categorical outcomes. The value of Cohen's κ, however, is not sufficiently informative because it is a relative measure, while the clinician's question of observer variation calls for an absolute measure. Using an example in which the observed agreement and κ lead to different conclusions, we illustrate that percentage agreement is an absolute measure (a measure of agreement) and that κ is a relative measure (a measure of reliability). For the data to be useful for clinicians, measures of agreement should be used. The proportion of specific agreement, expressing the agreement separately for the positive and the negative ratings, is the most appropriate measure for conveying the relevant information in a 2 × 2 table and is most informative for clinicians.

---

### Improving early clinical trial phase identification of promising therapeutics [^fe4b6c1f]. Neurology (2015). Low credibility.

This review addresses decision-making underlying the frequent failure to confirm early-phase positive trial results and how to prioritize which early agents to transition to late phase. While unexpected toxicity is sometimes responsible for late-phase failures, lack of efficacy is also frequently found. In stroke as in other conditions, early trials often demonstrate imbalances in factors influencing outcome. Other issues complicate early trial analysis, including unequally distributed noise inherent in outcome measures and variations in natural history among studies. We contend that statistical approaches to correct for imbalances and noise, while likely valid for homogeneous conditions, appear unable to accommodate disease complexity and have failed to correctly identify effective agents. While blinding and randomization are important to reduce selection bias, these methods appear insufficient to insure valid conclusions. We found potential sources of analytical errors in nearly 90% of a sample of early stroke trials. To address these issues, we recommend changes in early-phase analysis and reporting: (1) restrict use of statistical correction to studies where the underlying assumptions are validated, (2) select dichotomous over continuous outcomes for small samples, (3) consider pooled samples to model natural history to detect early therapeutic signals and increase the likelihood of replication in larger samples, (4) report subgroup baseline conditions, (5) consider post hoc methods to restrict analysis to subjects with an appropriate match, and (6) increase the strength of effect threshold given these cumulative sources of noise and potential errors. More attention to these issues should lead to better decision-making regarding selection of agents to proceed to pivotal trials.

---

### Smaller sample sizes for phase II trials based on exact tests with actual error rates by trading-off their nominal levels of significance and power [^cbb77402]. British Journal of Cancer (2012). Low credibility.

Background

Sample sizes for single-stage phase II clinical trials in the literature are often based on exact (binomial) tests with levels of significance (alpha (α) < 5% and power > 80%). This is because there is not always a sample size where α and power are exactly equal to 5% and 80%, respectively. Consequently, the opportunity to trade-off small amounts of α and power for savings in sample sizes may be lost.

Methods

Sample-size tables are presented for single-stage phase II trials based on exact tests with actual levels of significance and power. Trade-off in small amounts of α and power allows the researcher to select from several possible designs with potentially smaller sample sizes compared with existing approaches. We provide SAS macro coding and an R function, which for a given treatment difference, allow researchers to examine all possible sample sizes for specified differences are provided.

Results

In a single-arm study with P(0) (standard treatment) = 10% and P(1) (new treatment) = 20%, and specified α = 5% and power = 80%, the A'Hern approach yields n = 78 (exact α = 4.53%, power = 80.81%). However, by relaxing α to 5.67% and power to 77.7%, a sample size of 65 can be used (a saving of 13 patients).

Interpretation

The approach we describe is especially useful for trials in rare disorders, or for proof-of-concept studies, where it is important to minimise the trial duration and financial costs, particularly in single-arm cancer trials commonly associated with expensive treatment options.

---

### Inverting polar domains via electrical pulsing in metallic germanium telluride [^d57fb101]. Nature Communications (2017). Medium credibility.

The following aspects of equation (1) enable us to qualitatively predict the changes in domain fractions from the virgin to programmed state: (i) The domain whose polarization is perpendicular to SHG polarizer, contributes only to the coefficient of sin2 (φ − θ) (C θ) of the total intensity (that is, variants of β, γ and δ for θ = 0 o, 60 o and 120 o respectively), and the rest of the domains contribute to coefficients of all the three terms. (ii) Coefficient of sin2 (φ − θ) or C θ produces a periodicity of π /2 to produce four lobes in polarimetry plots for φ varying from −180 to 180 o, while coefficients of cos 2 (φ − θ) and sin 2 (φ − θ) (A θ and B θ) produce two lobes. The change in phase of the fits at θ = 60 o (Fig. 5d), predominantly dominated by coefficients of sin 2 (φ − 60 o) and cos 2 (φ − 60 o) (two lobes) between virgin and programmed states suggests significant changes in domain fractions of either the variants of β or δ. Figure 5e shows no significant differences between the two states at θ = 120 o, both fit to a two lobed function, except for a stronger signal in virgin state. This indicates that neither the variants of β nor γ change significantly upon programming, and in conjunction with the polarimetry plots for θ = 60 o (Fig. 5d), a significant change in variants of δ may be predicted. This is also consistent with the θ = 0 o (Fig. 5c) polarimetry plot, where changes in variants of δ enhance the contribution of coefficients of sin 2 (φ) and cos 2 (φ) (responsible for two lobes) in comparison with the coefficient of sin(2 φ) (responsible for four lobes) post programming.

---

### Measures of interrater agreement [^0810a709]. Journal of Thoracic Oncology (2011). Low credibility.

Kappa statistics is used for the assessment of agreement between two or more raters when the measurement scale is categorical. In this short summary, we discuss and interpret the key features of the kappa statistics, the impact of prevalence on the kappa statistics, and its utility in clinical research. We also introduce the weighted kappa when the outcome is ordinal and the intraclass correlation to assess agreement in an event the data are measured on a continuous scale.

---

### Topological one-way fiber of second chern number [^2faec267]. Nature Communications (2018). Medium credibility.

Results

Single Weyl dipole

Our starting point is a photonic crystal containing two Weyl points, which were found in the double gyroid (DG) made of magnetic materials. The DG is a minimal surface that can be approximated by the iso-surface of a single triply periodic function: f (x, y, z) = sin(2 πx / a)sin(4 πy / a)cos(2 πz / a) + sin(2 πy / a)sin(4 πz / a)cos(2 πx / a) + sin(2 πz / a)sin(4 πx / a)cos(2 πy / a). This definition, although having a different form, yields almost identical geometry and band structure to those of the DG defined in ref.by two separate trigonometric functions (one for each gyroid). In Fig. 2a, two cubic cells of the DG are shown, where f (x, y, z) > f 0 = 0.4 is filled with gyroelectric material of dielectric constantand unity magnetic permeability. The rest of the volume is air. In this structure, there exists only two Weyl points (a single "Weyl dipole") separated by about half of the Brillouin zone along z direction, as plotted in Fig. 2b. This means that an infinitesimal supercell modulation of the crystal (in z with a period of 2a) can superimpose the two Weyl points on top of each other to form a 3D Dirac point between four bands(Fig. 2b), which opens a gap under a finite modulation/coupling strength (Fig. 2d). The fact that a bandgap does not close under small perturbations ensures the robustness of this approach: certain mismatch between the Weyl-point separation and the wavevector of the modulation can be tolerated.

Fig. 2
3D Chern crystal from magnetic Weyl crystals. a Two cubic unit cells of the DG photonic crystal magnetized along z. b The band structure of a cubic cell shows two Weyl points, which fold into one 3D Dirac point in the Brillouin zone of the supercell. c The DG photonic crystal whose volume fraction (blue–red colored) is periodically modulated along z. d The band structure of the 3D Chern crystal whose topological gap frequencies are highlighted in green. λ 0 is the vacuum wavelength

---

### A simple alpha / β-independent method to derive fully isoeffective schedules following changes in dose per fraction [^3a48e204]. International Journal of Radiation Oncology, Biology, Physics (2004). Low credibility.

Purpose

Dosimetric errors in delivering the prescribed dose per fraction made early in a treatment can be corrected by modifying the dose per fraction and total dose given subsequently to discovery of the error, using the linear-quadratic model to calculate the correcting doses which should be completed within the same overall time as originally prescribed. This study shows how these calculations can be carried out independently of any alpha/beta ratios to bring the treatment back exactly to planned tolerance simultaneously for all tissues and tumor involved.

Methods

Planned treatment is defined as p Gy per fraction to a total dose P Gy; the initial error is e Gy per fraction given to a total of E Gy. The linear-quadratic formula is assumed to describe all isoeffect relationships between total dose and dose per fraction.

Results and Conclusion

An exact solution is found that describes a compensating dose of d Gy per fraction to a total of D Gy. The formulae are: D = P-E d = Pp-Ee/P-E. Thus the total dose for the complete treatment (error plus compensation) remains as originally prescribed, with hyperfractionation being used to correct an initial hypofractionation error and hypofractionation being used to correct an initial hyperfractionation error. Incomplete repair is shown to perturb this exact solution. Thus compensating treatments calculated with these formulae should not be scheduled in such a manner that would introduce incomplete repair.

---

### Adaptive hierarchical origami-based metastructures [^6c003a44]. Nature Communications (2024). High credibility.

The selected two reconfiguration processes exhibit only local and stepwise transition kinematics. From shape M 7 to M 13 (Fig. 4d, i and iii), only the folds of links #2 and #4 (Fig. 4d, i) are involved with sequential rotations (Fig. 4d, ii) and the remaining folds of link #1, link #3 and the level-2 keep unchanged (Fig. 4d, iv). For kinematic details of the reconfigured links #2 and #4 shown in Fig. 4d, iv, during the initial process ① → ②, we only need to linearly change the folds angle γ m4,6 (m = 2 or 4) from 180° to γ 0 (Here we set γ 0 as 150° while it ranges from 90° to 180°; see more details in Supplementary Fig. 17) and meanwhile linearly increase γ m2,8 from 0° to γ 0. Then, in the following process ② → ③, we can maintain folds angles γ m2,4,6,8 as γ 0 while both linearly decreasing γ m1,5 from 180° to sin -1 [(sin γ 0) 2 /(1 + (cos γ 0) 2] (≈ 109.5° for γ 0 = 150°) and augmenting γ m3,7 from 0° to 180°−sin −1 [(sin γ 0) 2 /(1 + (cos γ 0) 2] (≈ 70.5° for γ 0 = 150°). Lastly, for ③ → ④ → ⑤, we can simultaneously transfigure links #2 and #4 as 8R-looped rigid linkage with kinematics as γ m1,5 = sin −1 [(sin γ m2) 2 /(1 + (cos γ m2) 2], γ m3,7 = 180°−sin −1 [(sin γ m2) 2 /(1 + (cos γ m2) 2] (γ m2 reducing from γ 0 to 90°) while γ m2 = γ m4 = γ m6 = γ m8 (see Supplementary Note 6) to reach shape M 13. Moreover, as illustrated in Fig. 4e, vi that displays sequential and local kinematic features, we note that the reconfiguration kinematics from shape M 15 to M 21 by bypassing shape M 25 (Fig. 4e, i–v) are much simpler with only linear angle relationships.

---

### An official multi-society statement: the role of clinical research results in the practice of critical care medicine [^48106aec]. American Journal of Respiratory and Critical Care Medicine (2012). Medium credibility.

Critical care medicine — incorporation of clinical research results in practice is framed within a debate about the relative value of research for guiding clinical decision making alongside other forms of medical knowledge, and professional organizations offer a conceptual framework rather than a statement for or against evidence-based medicine. No single form of medical knowledge always takes precedence, and no hierarchy of knowledge can be uniformly applied to clinical decisions. The aim is not to resolve current controversies but to provide clinicians with a conceptual background and framework for evaluating discordant information and conflicting claims when caring for the critically ill. Bedside care of critically ill patients requires real-time analysis and rapid decisions, and the vulnerability of these patients, their general inability to participate directly in medical decision making, and the high risk of poor outcome amplify the clinician's responsibility. Use of clinical research in critical care is complicated by complex syndromic classifications, lack of reliable syndrome definitions, heterogeneity and co-morbidities, and a relative paucity of large, randomized trials to inform practice.

---

### Long-term trends and seasonality of omphalocele during 1996–2010 in China: a retrospective analysis based on the hospital-based birth defects surveillance system [^f6143bd8]. BMC Pregnancy and Childbirth (2015). Low credibility.

To facilitate parameter estimation of the model, we transformed ψ cos(2 πωt j − θ) into a linear form. We set ψ cos(θ) = γ 1, ψ sin(θ) = γ 2; therefore, ψ cos(2 πωt j − θ) = γ 1 cos(2 πωt j) + γ 2 sin(2 πωt j). The parameters ψ and θ can be estimated by formula F1:We set cos(2 kπωt j) = c k, sin(2 kπωt j) = s k. Therefore, model M1 is equal to model M2, which was used for the final analysis in our study. We used three models (k set to 0, 1, and 2, respectively) to estimate the long-term trends and seasonal fluctuations of omphalocele nationwide, in the northern region, southern region, urban areas, and rural areas. The likelihood ratio test statistic G 2 was used to explore the significant seasonal fluctuations.

All statistical analyses in this study were performed using SAS 9.3 software (SAS Institute Inc. Cary, NC, USA). The statistical significance level for α was set at 0.05.

---

### Subnational variations in the quality of household survey data in sub-saharan Africa [^642599de]. Nature Communications (2025). High credibility.

Geostatistical model, model-fitting and prediction

To predict the data quality indicators at 5 × 5 km resolution and the district level, we fitted a Bayesian geostatistical model with a binomial likelihood. Using similar notation as before, the first level of the model can be expressed as Eq. (2):where p (s i) is the underlying true proportion of individuals possessing the attribute being modelled, e.g. incomplete age, at location s i. We model p (s i) using the logistic regression model expressed as Eq. (3):where x (s i) is a vector of covariate data associated with location s i, β are the corresponding regression coefficients, ϵ (s i) is an independent and identically distributed (iid) Gaussian random effect with variance σ ϵ used to model non-spatial residual variation, and ω (s i) is a Gaussian spatial random effect used to capture residual spatial correlation in the model; i.e.is assumed to follow the Matérn covariance functiongiven bywhere·denotes the distance between cluster locations s i and s j, σ is the marginal variance of the spatial process, κ is a scaling parameter related to the range– the distance at which spatial correlation is close to 0.1, andis the modified Bessel function of the second kind and order ν > 0. Further, for identifiability reasons, we set ν = 1. We fit model (1) for each region (Supplementary Fig. 16). A Bayesian approach is adopted for the analysis, which is implemented using the integrated nested Laplace approximation – stochastic partial differential equation (INLA-SPDE) approach. To complete the model specification, we place a non-informativeprior on the regression coefficients, β. A penalized complexity (PC) prior introduced in Simpson et al. 2017 is set on σ ϵ such that p (σ ϵ > 3) = 0.01. Similarly, following Fuglstad et al. 2018, a joint PC prior is placed on the covariance parameters of the spatial random effect, ω. These are: p (r < r 0) = 0.01 and p (σ > 3) = 0.01, with r 0 chosen to be 5% of the size of the region. The SPDE approach involves a triangulation of the spatial domain to approximate ω. A spherical mesh was constructed for this approximation using the boundary points and the data locations falling within each region. The maximum triangle edge length is set to 15 km in the inner mesh and 100 km (50 km in Southern Africa) in the outer mesh, with an offset of 500 km in each case. With the spherical mesh, the distance matrix needed to fit the Matérn covariance function is calculated as the great circle distance along the surface of the Earth. To implement this in practice, the data locations and boundary points for each region are first converted to three-dimensional coordinates on the unit sphere. These are then used to construct a mesh on a spherical manifold, on which the SPDE is defined. From each of the fitted models, we generate 1000 samples from the posterior distributions of the parameters of the model, as well as from the posterior predictive distributions of the indicators for each of the prediction locations, i.e. the 5 × 5-km grid cells. The latter are then used to calculate the district level estimates as population-weighted averages taken over all the grid cells falling within each district.

---

### Lineage dynamics of murine pancreatic development at single-cell resolution [^6c0b0b4f]. Nature Communications (2018). Medium credibility.

To validate our pseudotime results, we performed ISH for markers that defined each branch of the trajectory. First, we confirmed the expression of Peg10 and Gng12 within the Fev Hi population (Fig. 7d, e, indigo and teal gradient arrows), validating the expression of these genes in a stage before hormone acquisition. We also validated the enrichment of Peg10 and Gng12 in alpha and beta cells, respectively (Fig. 7f, g, solid indigo and teal arrows). First, 95.8% of beta cells expressed Gng12 (n = 46 cells, 6 pancreata), while 30.5% expressed Peg10 (n = 71 cells, 7 pancreata) (Fig. 7f and Supplementary Fig. 9a). Additionally, 100% of alpha cells expressed Peg10 (n = 31 cells, 6 pancreata), while only 5.4% expressed Gng12 (n = 32 cells, 4 pancreata) (Fig. 7g and Supplementary Fig. 9b). The lineage relationships generated by pseudotime ordering, combined with the validation in vivo, lead us to hypothesize that the Fev+/Peg10+ cells are fated toward an alpha cell identity and Fev+/Gng12+ cells toward a beta cell identity (Fig. 7h). These results suggest that lineage allocation of endocrine progenitors toward alpha or beta cell fates may occur after the onset of Fev expression.

---

### Identification of signal bias in the variable flip angle method by linear display of the algebraic ernst equation [^866dc6c7]. Magnetic Resonance in Medicine (2011). Low credibility.

Noise Propagation with Correlated Errors

The image noise is scaled individually along abscissa (σ S τ) and ordinate (σ S /τ). The errors are equal for τ = 1 or α = 53°. At smaller flip angle (τ → 0), the y -error is enhanced and the x -error is diminished, and vice versa for τ → ∞ (α → π).

In a function F of two variables, x and y, the statistical errors (given by the variances var x and var y) are propagated by the respective partial derivatives ∂ F /∂ x. If the errors in x and y are not independent (i.e. correlated), their covariance has to be taken into account by using the general law of Gaussian error propagation.

Here, F is the residual F (x, y) = y – (A – x /2ρ 1) with x = S × τ and y = S /τ. Thus, the covariance is identical to the image noise variance σ S 2 and independent of τ.

This is in stark contrast to the common linearization of Eq. 2, where the errors and their covariance increase strongly for small flip angles by 1/sin α or 1/tan α. Equation 16 yields

The expression in brackets denotes a τ-dependent scaling factor of the image noise. It can be cancelled by imposing suitable weights w (τ) onto the square residues

in the linear least squares objective function. These weights are maximal at τ E and decrease towards the intercepts with the x - and y - axes.

Short TR

If R 1 TR ≪ 1 then ρ 1 can be approximated and Eq. 9 arranged as

Under this condition, Sτ /2TR can be used as abscissa to incorporate varying values of TR into the regression. Estimates of T 1 are obtained directly as the (negative) slope

This is the generalization of the formula for the dual angle experiment.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^dee6cacc]. CDC (2011). Medium credibility.

Community viral load sample size calculations — sample size to detect differences in geometric mean (GM) viral load depends on power and standard deviation (S). The table specifies "α = 0.05 and W = 0.8", with S columns 1 to 1.5; for k = 3 the minimum sample sizes by S are 54, 66, 78, 92, 106, and 122, and "1.2 is the standard deviation of national VL data", so at S = 1.2 the needed sample size is 78.

---

### AHA / ACCF / HRS recommendations for the standardization and interpretation of the electrocardiogram: part IV: the ST segment, T and U waves, and the QT interval: a scientific statement from the American Heart Association electrocardiography and arrhythmias committee, council on clinical cardiology; the American college of cardiology foundation; and the Heart Rhythm Society: endorsed by the international society for computerized electrocardiology [^2c6851b8]. Circulation (2009). Medium credibility.

QT correction for rate — preferred methods and reporting thresholds: It is recommended that linear regression functions rather than the Bazett's formula be used for QT-rate correction and that the method used for rate correction be identified in ECG analysis reports, and rate correction of the QT interval should not be attempted when RR interval variability is large or when identification of the end of the T wave is unreliable; Bazett's formula leaves a strong positive residual correlation (r = 0.32) and Fridericia's formula leaves a negative correlation (r = −0.26 to −0.32) with heart rate, and the adjusted QT values may be substantially in error, particularly when the heart rate is high; FDA guidelines for industry recommend that 3 severity levels for rate-corrected QT be reported when considering possible QT-prolonging effects of drugs: longer than 350 ms, longer than 480 ms, and longer than 500 ms.

---

### Influence of noise correlation in multiple-coil statistical models with sum of squares reconstruction [^47d0e721]. Magnetic Resonance in Medicine (2012). Low credibility.

Noise in the composite magnitude signal from multiple-coil systems is usually assumed to follow a noncentral χ distribution when sum of squares is used to combine images sensed at different coils. However, this is true only if the variance of noise is the same for all coils, and no correlation exists between them. We show how correlations may be obviated from this model if effective values are considered. This implies a reduced effective number of coils and an increased effective variance of noise. In addition, the effective variance of noise becomes signal-dependent.

---

### Artificial two-dimensional polar metal at room temperature [^12805d82]. Nature Communications (2018). Medium credibility.

SHG measurement

SHG measurements are performed in a far-field transmission geometry using an 800 nm fundamental laser beam generated by a Spectra-Physics Solstice Ace Ti:Sapphire femtosecond laser system (< 100 fs, 1 kHz). The schematic of experimental setup is shown in Fig. 2a, where a linear polarized fundamental 800 nm beam with polarization direction φ is incident on the sample at a tilted angle θ, the transmission SHG field is first spectrally filtered, then decomposed into s-/p-polarized components (E 2 ω, s / E 2 ω, p) and finally detected by a photon multiplier tube. The in-plane orientation of the sample is controlled by angle β. Systematic polarimetry is done by scanning polarization direction φ with fixed θ and β.

Theoretical modeling of SHG data is described below. The fundamental field with polarization direction φ inside the sample can be written aswhere sin(θ ′) = sin(θ)/ n, n is the refractive index, t p = 2 cos(θ)/(n cos(θ) + cos(θ ′)) and t s = 2 cos(θ)/(cos(θ) + n cos(θ ′)) are Fresnel coefficients, E ω is the magnitude of electric component in air. Under Voigt notation, the SHG field generated through nonlinear optical process can be expressed as, where normalized SHG d matrix for 4mm and mm 2 symmetry is

In practice, all the coefficients in d tensor are normalized to d 15. By ignoring the index dispersion, that is, n = n ω ≈ n 2 ω. The transmitted SHG field iswhere = 2 n cos(θ ′)/(n cos(θ) + cos(θ ′)), = 2 n cos(θ ′)/(cos(θ) + n cos(θ ′)), the subscripts s/p denote s/p components of corresponding field variables. Finally, we have below equations for SHG measured intensity:where α is scaling factor.

---

### Qualitative and quantitative evaluation of blob-based time-of-flight PET image reconstruction in hybrid brain PET / MR imaging [^42a591ab]. Molecular Imaging and Biology (2015). Low credibility.

Image Reconstruction

All images were reconstructed using a list-mode time-of-flight ordered subsets expectation maximization (TOF-OSEM) algorithm following 3-class segmentation-based MRI-guided attenuation correction with the exception of the Hoffman phantom data in which TOF information was not used. Scatter correction was performed using a TOF-dependent single scatter simulation technique. Images were reconstructed on a blob grid and subsequently sampled on a 288 × 288 × 90 image grid with a voxel size of 2 × 2 × 2 mm.

The image quality phantom scans were reconstructed using the combination of parameters displayed in Table 1. To cover all possible combinations, representative parameters were derived from the first (u 1 = 6.988), second (u 2 = 10.417), and third (u 3 = 13.698) zero crossings. This resulted in 15 different parameter combinations, all satisfying Eq. 2. In all cases, the blob sampling distance was set at 2.0375. The data were reconstructed with various iterations (1, 2, 4, 8, and 12) using 32 subsets. For the Hoffman 3D brain phantom and the clinical studies, seven representative parameter combinations with 1, 4, and 12 iterations were used for reconstruction. These combinations are marked with a star in Table 1. The clinical studies were also reconstructed with the current protocol used in clinical routine, namely two iterations, an alpha value of 6.3716, a radius of 2.8, and blob spacing of 2.0375.

Table 1
Chosen radius parameters for given alpha values, used for the phantom studies satisfying conditions of Eq. 2. The blobs spacing (d) is set at 2.0375 (relative units)

* combination used for Hoffmann brain phantom and clinical studies

---

### Experimental noise cutoff boosts inferability of transcriptional networks in large-scale gene-deletion studies [^361afac2]. Nature Communications (2018). Medium credibility.

Assuming that the observed node activities follow a multivariate normal distribution, we can find estimates for the unknown orthogonal matrix U, the singular values Σ, and the observational noise σ by maximising the log-likelihood functionunder the constraint, with U k the k -th column vector of U and δ lk the Kronecker delta. It fact, it suffices to constrain the norm of the vectors, as the corresponding maximum likelihood solution leads to an eigenvalue problem with U k as eigenvectors, which can always be made orthogonal. We can therefore define the likelihood function byHere, anddenote maximum likelihood estimates of the covariance matrix. From this definition of y ref follows that the initially introduced perturbation vector, u, must satisfy. We further defined with λ k a Lagrange multiplier and denoted by tr(.) the trace of a matrix. In the following calculations, we substitute S by the unbiased sample covariance matrix, S → P (P − 1) −1 S. Note that V must disappear in the likelihood function as the covariance matrix of u is invariant under any orthogonal transformation u → V T u.

The maximum of the log-likelihood function is determined by the conditions, and, which results inshowing that maximum likelihood estimates of, andare determined by the sample covariance matrix S. If N 0 > 1 and the full-rank sample covariance matrix is significantly different from a block-diagonal form — e.g. the network is not separable in subnetworks–the orientations of the corresponding N 0 eigenvectors are determined by sampling noise in the space orthogonal to remaining N − N 0 eigenvectors. In case that we have less data points than nodes in the network — e.g. the number of perturbed nodes times their replicates is smaller than the network size — some of the N 0 smallest eigenvalues become exactly zero and as a consequence the noise level, is underestimated. Although a maximum likelihood solution exists in this case, it is necessary to regularise the covariance matrix, witha regularisation parameter, as a correct estimate of the noise level is essential for statistical analysis. Note that the derivation of the maximum likelihood solution is mathematically equivalent to the derivation of principle component analysis from a probabilistic perspective.

---

### Practitioner's guide to latent class analysis: methodological considerations and common pitfalls [^65e648f3]. Critical Care Medicine (2021). Medium credibility.

Latent class analysis is a probabilistic modeling algorithm that allows clustering of data and statistical inference. There has been a recent upsurge in the application of latent class analysis in the fields of critical care, respiratory medicine, and beyond. In this review, we present a brief overview of the principles behind latent class analysis. Furthermore, in a stepwise manner, we outline the key processes necessary to perform latent class analysis including some of the challenges and pitfalls faced at each of these steps. The review provides a one-stop shop for investigators seeking to apply latent class analysis to their data.

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^f4bdcdc4]. Nature Communications (2025). High credibility.

Derivation of δ L from the standard gamma distribution

Due to its flexibility, the standard gamma distribution has been used as a model for many zero-bounded variables in a variety of fields. The gamma distribution is commonly defined with parameters for shape (k) and dispersion (δ L) where the pdf, indexed by x, iswith x, k, δ L > 0. The product of two exponential functions, describes all distribution shapes modelled by the pdf, whilenormalises the area below the pdf to be equal to one.

The mean and variance of the gamma distribution are, respectively, andwhere μ > 0 and σ 2 > 0. It is clear from this formulation alone that the variance changes linearly with the mean while δ L is fixed and that the rate of this change is scaled by the dispersion of the distribution (Equation (22); see Fig. 2 e). It is also clear that the standard deviation (σ) has a square root relationship with the mean when the dispersion is fixed.

The solution to derive a mean-independent measure of dispersion for approximately gamma distributed variables is to simply retrieve the distribution's dispersion parameter, δ L. Recall that the variance had linear dependence on the mean (Equation (22)), which can be removed with the reciprocal of μ. As such, the mean-independent measure of dispersion, δ L, can be derived from the mean and variance withproving the mean independence of δ L for gamma distributed variables, where σ 2 and μ can be estimated from a sample.

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^d027f21a]. Nature Communications (2025). High credibility.

Understanding the effect of heterogeneity is fundamental to numerous fields. In community ecology, classical theory postulates that habitat heterogeneity determines niche dimensionality and drives biodiversity. However, disparate heterogeneity-diversity relationships have been empirically observed, generating increasingly complex theoretical developments. Here we show that spurious heterogeneity-diversity relationships and subsequent theories arise as artifacts of heterogeneity measures that are mean-biased for bounded continuous variables. To solve this, we derive an alternative mean-independent measure of heterogeneity for beta and gamma distributed variables that disentangles statistical dispersion from mean. Using the mean-independent measure of heterogeneity, true monotonic positive heterogeneity-diversity relationships, consistent with classical theory, are revealed in data previously presented as evidence for both hump-shaped heterogeneity-diversity relationships and theories of an area-heterogeneity trade-off for biodiversity. This work sheds light on the source of conflicting results that have hindered understanding of heterogeneity relationships in broader ecology and numerous other fields. The mean-independent measure of heterogeneity is provided as a solution, essential for understanding true mean-independent heterogeneity relationships in wider research.

---

### The snm procedure guideline for general imaging 6.0 [^e005ca59]. SNMMI (2010). Medium credibility.

General imaging — iterative reconstruction of single-photon emission computed tomography (SPECT) states that 'With the increasing power and memory of computer hardware and with development of the more efficient OSEM algorithm, iterative reconstruction of SPECT studies is now common in the clinical environment'. It adds that 'This methodology makes it possible to incorporate correction for many physical effects such as non-uniform attenuation correction, scatter reduction or removal, variation of spatial resolution with distance, etc'.

---

### Trapping of drops by wetting defects [^1cd66ff7]. Nature Communications (2014). Medium credibility.

Calibration of dissipation coefficient

To determine the damping coefficient of the sliding drops on the homogeneous surface, we measured the steady sliding velocity v 0 away from the trap as a function of inclination angle. Figure 6 shows the expected linear relation between v 0 and sin α. Owing to the finite residual hysteresis, the curves intercept the abscissa at a finite value sin α 0. The balance between gravity, viscous friction and pinning is

we can determine both λ and F p from the these calibration measurements. The pinning forces are related to contact angle hysteresis by F p = 2 R γ(cos θ r –cos θ a), where θ r and θ a are the receding and the advancing contact angle, respectively. From this expression, we find sin α 0 = 2 R γ(cos θ r –cos θ a)/ mg, leading to a critical sliding angle α 0 = 1… 3°, decreasing with increasing drop size, in agreement with the experimental results (Fig. 6).

From the slope of the calibration curves in Fig. 6, we can extract the damping coefficients λ w = (4.5 ± 0.2) × 10 −4, (5.9 ± 0.1) × 10 −4, (5.8 ± 0.5) × 10 −4 kg s −1 for the water drops of 20, 40 and 60 μl, and λ wg = (1.22 ± 0.03) × 10 −2 kg s −1 and (1.32 ± 0.03) × 10 −2 kg s −1 for the 40 and 60 μl water–glycerol drops. This weak size dependence is attributed to the different scaling of driving, viscous and pinning forces with the drop size. For the purpose of our analysis, this minor variation can be neglected. Dividing the values of λ by the drop width 2 R, we find values for the contact line friction coefficient ξ w ≈100 mPa·s and ξ wg ≈5 Pa·s. The value for water is somewhat larger than the one extracted in earlier experiments with sliding drops on dry substrates. The additional contribution may arise from the viscous dissipation of the oil film on the substrate as described by Smith et al.

---

### ACOEM practice guidelines: elbow disorders [^51466a24]. Journal of Occupational and Environmental Medicine (2013). Medium credibility.

Summary of recommendations — basis and use: These recommendations are based on critically appraised higher quality research evidence and on expert consensus only when higher quality evidence was unavailable or inconsistent, and the more detailed indications, specific appropriate diagnoses, temporal sequencing, prior testing or treatment, and contraindications — which are elaborated in more detail for each test or treatment in the guideline — should be reviewed for recommendations in clinical practice or medical management; these recommendations are not simple "yes/no" criteria, and the evidence supporting them is in nearly all circumstances developed from typical patients, not unusual situations or exceptions.

---

### Endocrine lineage biases arise in temporally distinct endocrine progenitors during pancreatic morphogenesis [^72f59070]. Nature Communications (2018). Medium credibility.

Amotl2 functions in endothelial cell migration, canonical Wnt signaling repression, polarity loss, and Hippo pathway activation –. Amotl2, which exhibited similar expression patterns to Smarcd2 and Ngn3, showed enrichment of chromatin accessibility in e14.5 EPs compared to e16.5 EPs (Supplementary Fig. 17d-f). Further, as in murine pancreatic development, AMOTL2 significantly increased in hESC-derived PPs and EPs compared to hESCs (Supplementary Fig. 17g). This shift in the number of GCG/INS cells after AMOTL2 LOF suggests that endocrine fate may be primed in earlier progenitors. In support of this, we found that e14.5 BPs expressed alpha cell markers while e16.5 BPs had increased expression of beta cell markers, mirroring the subsequent temporal lineage biases (Fig. 8f).

Transcriptional maps as EPs become alpha or beta cells

Finally, we sought to determine the transcript dynamics as EPs differentiate into alpha or beta cells. We addressed this by two independent methods of pseudotemporal ordering, Monocle2, which orders single cells along a trajectory before grouping transcripts based off of expression patterns over pseudotime, and Wishbone, which acts as a branch predictor to resolve bifurcating trajectories. The use of both Monocle2 and Wishbone allowed the unbiased analysis of transcripts over pseudotime coupled with a secondary method to visualize transcript changes over time with higher branch fidelity. In Monocle2, we utilized a density peak clustering algorithm and differential expression analysis to determine the top 300 genes (lowest q -value) when EPlo cells bifurcate to alpha and beta cells and then analyzed these gene expression dynamics across differentiation (Fig. 9a). Genes that increase in expression as EPs differentiate into beta cells (in cluster 1) included Gjd2, Mlxipl, Scg2, Sytl4, and Ociad2, which have not yet been described in the embryonic pancreas (Fig. 9b).

---

### Management of lower urinary tract symptoms attributed to benign prostatic hyperplasia (BPH): AUA guideline amendment 2023 [^98f02ef9]. The Journal of Urology (2024). High credibility.

Anejaculation versus retrograde ejaculation (RE) with alpha 1a blockers — The EjD associated with selective alpha 1a blockers is correctly called "anejaculation", with tamsulosin producing significantly decreased ejaculate volume (-2.4 ± 0.17 mL) compared to alfuzosin (+0.3 ± 0.18 mL; p < 0.0001 versus tamsulosin) or placebo (+0.4 ± 0.18 mL; p < 0.0001 versus tamsulosin; p = nonsignificant versus alfuzosin). Despite volume differences, no significant differences were observed in post-ejaculate urine sperm concentrations among tamsulosin, alfuzosin, and placebo (1.6 ± 0.87, 1.3 ± 0.87 and 0.9 ± 0.88 million/mL), and these data demonstrate the phenomenon is anejaculation rather than RE.

---

### Advanced statistics: missing data in clinical research – part 1: an introduction and conceptual framework [^a1e7a700]. Academic Emergency Medicine (2007). Low credibility.

Missing data are commonly encountered in clinical research. Unfortunately, they are often neglected or not properly handled during analytic procedures, and this may substantially bias the results of the study, reduce study power, and lead to invalid conclusions. In this two-part series, the authors will introduce key concepts regarding missing data in clinical research, provide a conceptual framework for how to approach missing data in this setting, describe typical mechanisms and patterns of censoring of data and their relationships to specific methods of handling incomplete data, and describe in detail several simple and more complex methods of handling such data. In part 1, the authors will describe relatively simple approaches to handling missing data, including complete-case analysis, available-case analysis, and several forms of single imputation, including mean imputation, regression imputation, hot and cold deck imputation, last observation carried forward, and worst case analysis. In part 2, the authors will describe in detail multiple imputation, a more sophisticated and valid method for handling missing data.

---

### A continuous-time maxSAT solver with high analog performance [^87d3d16d]. Nature Communications (2018). Medium credibility.

Algorithm description

Here, we give a simple, nonoptimized variant of the algorithm (see flowchart in Supplementary Fig. 2). Better implementations can be devised, for example with better fitting routines, however the description below is easier to follow and works well. Given a SAT problem, we first determine the b parameter as described previously. Step 1: initially we set, Γ min, = and t max. Unless specified otherwise, in our simulations we used Γ min = 100, Γ max = 2 × 10 6, t max = 50. Step 2: to initialize our statistics, we run Γ min trajectories up to t max, each from a random initial condition. For every such trajectory ω we update the p (E, t) distributions as function of the energies of the orthants visited by ω. We record the lowest energy value found. Step 3: starting from Γ = Γ min + 1 and up to Γ max, we continue running trajectories in the same way and for each one of them check: (a) If, set, update p (E, t) and go to Step 4. (b) If Γ just reached, go to Step 4. (c) If Γ = Γ max, output "Maximum number of steps reached, increase Γ max ", output the lowest energy value found, the predictedand the quality of fit for, then halt. Step 4: using the p (E, t) distributions, estimate the escape rates κ (E) as described in the corresponding Methods section. Step 5: the κ (E) curve is extrapolated to the E − 1 value obtaining κ (E − 1) and then using this we predict(as described in another Methods section). Further extrapolating the κ (E) curve to κ = 0 we obtain(see the corresponding Methods section). Step 6: we check the consistency of the prediction defined here as saturation of the predicted values. We call it consistent, ifhas not changed during the last 5 predictions. If it is not consistent yet, we continue running new trajectories (Step 4). If the prediction is consistent, we check for the following halting conditions: (i) Ifthen we decide the global optimum has been found:and skip to Step 7. (ii) If the fitting is consistently predicting(usually it is very close,) we check the number of trajectories that has attained states with, i.e. = . If it is large enough (e.g. > 100), we decide to stop running new trajectories and setand go to Step 7. (iii) Ifthen we most probably have not found the global optimum yet and we go to Step 4. We added additional stopping conditions that can shorten the algorithm in case of easy problems, see Methods corresponding section, but these are not so relevant. Step 7: the algorithm ends and outputs, values, the Boolean variables corresponding to the optimal state found, along with the quality of fit.

---

### A continuous-time maxSAT solver with high analog performance [^b7d612ab]. Nature Communications (2018). Medium credibility.

Performance on random Max 3-SAT problems

We first test our algorithm and its prediction power on a large set (in total 4000) of random Max 3-SAT problems with N = 30, 50, 100 variables and constraint densities α = 8, 10. (In 3-SAT the SAT-UNSAT transition is around α ≃ 4.267). We compare our results with the true minimum values (E min) provided by the exact algorithm MaxSATZ. In Fig. 3, we compare the lowest energy found by the algorithm, the predicted minimumand the final decision by the algorithmwith the true optimum E min, by showing the distribution of their deviations from E min across many random problem instances. We use t max = 25 and at most Γ max = 150,000 runs, after which we stop the algorithm even if the prediction is not final. Thus, one expects that the performance of the algorithm decreases as N increases, (e.g. at N = 100), so that we would need to run more trajectories to obtain the same performance. Nevertheless, the results show that all three distributions have a large peak at 0. Most errors occur in the prediction phase, but many of these can be significantly reduced through simple decision rules (see Methods), because they occur most of the time at easy/small problems, where the statistics is insufficient (e.g. too few points since there are only few energy values). To show how the error in prediction depends on the hardness of problems, we studied the correlation between the errorand the hardness measure applicable to individual instances η = −ln κ /ln N (see ref.), see Fig. 3d (and Supplementary Fig. 4). Interestingly, larger errors occur mainly at the easiest problems with η < 2. Calculating the Pearson correlation coefficient betweenand η (excluding instances where the prediction is correct) we obtain a clear indication that often smaller η (thus for easier problems) generates larger errors. Positive errors are much smaller and shifted toward harder problems. Negative errors mean that the algorithm consistently predicts a slightly lower energy value than the optimum, which is good as this gives an increased assurance that we have found the optimum state. In Supplementary Fig. 4b, we show the correlation coefficients calculated separately for problems with different N and α.

---

### Hierarchical predictive information is channeled by asymmetric oscillatory activity [^58501fb8]. Neuron (2018). Low credibility.

Predictive coding and neural oscillations are two descriptive levels of brain functioning whose overlap is not yet understood. Chao et al. (2018) now show that hierarchical predictive coding is instantiated by asymmetric information channeling in the γ and α/β oscillatory ranges.

---

### Relational integration demands are tracked by temporally delayed neural representations in alpha and beta rhythms within higher-order cortical networks [^80c1b120]. Human Brain Mapping (2025). Medium credibility.

FIGURE 5
Correlation between participant accuracy (y ‐axis) and RC model coding strength (x ‐axis) with corresponding brain activation across complexity levels. (A) fMRI RC model coding strengths for the frontoparietal (left) and cingulo‐opercular (right) networks. (B) EEG frequency derived peak RC model coding strengths in the alpha (8–12 Hz, left) and beta (13–30 Hz, right) bands. r s represents Spearman's rank correlation coefficient; p values are uncorrected. (C) Left. Frontoparietal activation (β coefficients) across complexity levels. Right. Periodic peak beta power calculated across all EEG electrodes in each complexity condition during the 2.1–4.2 s time window.

In the EEG dataset, task accuracy also had a negative correlation with the temporal peak representation in the beta oscillatory band (r s = −0.33, p = 0.026), whereas no relationship was found in the alpha band (r s = 0.00, p = 0.983, Figure 5B). Contrary to RC, we did not find evidence to suggest that the spatial or temporal CE neural representations were linked to performance (Figure S3). Further exploration of sensor‐level activity during the RC coding window of the beta band (Figure 3B, 2.1–4.2 s) showed a decrease in peak beta power with RC: Binary (Mean = 0.437), Ternary (Mean = 0.380) and Quaternary (Mean = 0.388); F (2, 80) = 9.68, p < 0.001 (Figure 5C, right). Although the reported correlations do not survive appropriate correction for multiple comparisons, the reliable pattern of results across key cognitive control networks and modalities supports the genuine association between task accuracy and the strength of RC brain representations.

---

### Cystic fibrosis newborn screening: a systematic review-driven consensus guideline from the United States Cystic Fibrosis Foundation [^3d94bc8b]. International Journal of Neonatal Screening (2025). High credibility.

Cystic fibrosis newborn screening (NBS) process terms define the initial biomarker and genetic test types: immunoreactive trypsinogen (IRT) is a pancreatic enzyme precursor found in blood and is the first step in CF NBS in all US states; DNA panel testing evaluates only a pre-determined list of CFTR variants and panels may vary in size from 23 to > 300 variants; CFTR sequencing determines the presence, absence, and order of nucleotides and may include the entire gene or only coding and flanking regions with a limited number of intronic nucleotides near intron–exon junctions.

---

### A.S.P.E.N. clinical guidelines: parenteral nutrition ordering, order review, compounding, labeling, and dispensing [^909eee22]. JPEN: Journal of Parenteral and Enteral Nutrition (2014). Medium credibility.

A.S.P.E.N. evidence — Energy-dense TNA storage stability showed that the emulsion was "TNA stable for 28 d at 4°C followed by 2 d at 22°C" in a study to "Determine whether the emulsion in a more calorie-dense (0.9 non-protein kcal/mL) TNA remained stable for longer storage periods of 4 wk refrigerated + 2 d at room temperature". Observations included "Light microscopy: mean diameter of lipid particles < 3 μm through study" and "No significant change in pH, osmolality, or fatty acid profile over study period", with macronutrient content "Concentration of macronutrients in TNA: AAs (FreAmine III-B. Braun) 3.9%, dextrose 19.2%, IVFE (Soyacal, Alpha Therapeutic) 1.9%" and storage noted as "TNAs stored in EVA bags".

---

### Uncomputability of phase diagrams [^e853d9ad]. Nature Communications (2021). High credibility.

The phase diagram of a material is of central importance in describing the properties and behaviour of a condensed matter system. In this work, we prove that the task of determining the phase diagram of a many-body Hamiltonian is in general uncomputable, by explicitly constructing a continuous one-parameter family of Hamiltonians H(φ), where [Formula: see text], for which this is the case. The H(φ) are translationally-invariant, with nearest-neighbour couplings on a 2D spin lattice. As well as implying uncomputablity of phase diagrams, our result also proves that undecidability can hold for a set of positive measure of a Hamiltonian's parameter space, whereas previous results only implied undecidability on a zero measure set. This brings the spectral gap undecidability results a step closer to standard condensed matter problems, where one typically studies phase diagrams of many-body models as a function of one or more continuously varying real parameters, such as magnetic field strength or pressure.

---

### Effective noise estimation and filtering from correlated multiple-coil MR data [^2344c1fd]. Magnetic Resonance Imaging (2013). Low credibility.

Modern magnetic resonance (MR) imaging protocols based on multiple-coil acquisitions have carried on a new attention to noise and signal statistical modeling, as long as most of the existing techniques for data processing are model based. In particular, nonaccelerated multiple-coil and GeneRalized Autocalibrated Partially Parallel Acquisitions (GRAPPA) have brought noncentral-χ (nc-χ) statistics into stake as a suitable substitute for traditional Rician distributions. However, this model is only valid when the signals received by each coil are roughly uncorrelated. The recent literature on this topic suggests that this is often not the case, so nc-χ statistics are in principle not adequate. Fortunately, such model can be adapted through the definition of a set of effective parameters, namely, an effective noise power (greater than the actual power of thermal noise in the Radio Frequency receiver) and an effective number of coils (smaller than the actual number of RF receiving coils in the system). The implications of these artifacts in practical algorithms have not been discussed elsewhere. In the present paper, we aim to study their actual impact and suggest practical rules to cope with them. We define the main noise parameters in this context, introducing a new expression for the effective variance of noise which is of capital importance for the two image processing problems studied: first, we propose a new method to estimate the effective variance of noise from the composite magnitude signal of MR data when correlations are assumed. Second, we adapt several model-based image denoising techniques to the correlated case using the noise estimation techniques proposed. We show, through a number of experiments with synthetic, phantom, and in vivo data, that neglecting the correlated nature of noise in multiple-coil systems implies important errors even in the simplest cases. At the same time, the proper statistical characterization of noise through effective parameters drives to improved accuracy (both qualitatively and quantitatively) for both of the problems studied.

---

### Effect of alcohol label designs with different pictorial representations of alcohol content and health warnings on knowledge and understanding of low-risk drinking guidelines: a randomized controlled trial [^3b052135]. Addiction (2021). Medium credibility.

Statistical analysis

Sample size

A pre‐trial power calculation showed that 1000 participants in each arm was sufficient to identify an increase of 4.5–6.4% in the participants who correctly identified the LRDG as 14 units per week, with 80% power and an alpha level of between 0.2 and 5% (we adjusted alpha to account for multiple comparisons using a Hochberg step‐up procedure), assuming that 13% of participants in the baseline condition, who saw the existing industry‐standard labels, would correctly identify the LRDG as 14 units per week. We also recruited a further 500 participants (approximately 70 in each condition) for a pilot investigation, which included a warning concerning health risks alongside the label.

In order to test knowledge of the LRDG, we ran a logistic regression with whether or not the participant gave the correct answer as the dependent variable, controlling for demographic characteristics, AUDIT‐C and warning labels. In order to test understanding, we ran an ordinary least squares (OLS) regression for each of our distance measures (servings and containers), controlling for demographic characteristics, AUDIT‐C and warning labels. For our secondary measures we ran OLS regressions, controlling for demographic characteristics, AUDIT‐C and warning labels. Data were analysed in Stata version 14.2. The analysis plan was pre‐specified in an internal trial protocol (Supporting information, Appendix S1) but it was not pre‐registered on a publicly available platform, so the results could be considered exploratory. Post hoc, we ran exploratory OLS regressions of our understanding measures disaggregated into different types of alcohol and also with the measures converted into number of units. Upon the request of reviewers, we added a comparison of the proportion in each condition who over‐ versus underestimated the LRDG, given that they had answered incorrectly.

---

### Data-driven control of complex networks [^8532e913]. Nature Communications (2021). High credibility.

Data-driven fault recovery in power-grid networks

We address the problem of restoring the normal operation of a power-grid network after the occurrence of a fault that desynchronizes part of the grid. If not mitigated in a timely manner, such desynchronization instabilities may trigger cascading failures that can ultimately cause major blackouts and grid disruptions –. In our case study, we consider a line fault in the New England power-grid network comprising 39 nodes (29 load nodes and 10 generator nodes), as depicted in Fig. 5 a, and we compute an optimal point-to-point control from data to recover the correct operation of the grid. A similar problem is solved in ref.using a more sophisticated control strategy which requires knowledge of the network dynamics. As in refs. we assume that the phase δ i and the (angular) frequency ω i of each generator i obey the swing equation dynamics with the parameters given in ref.(except for generator 1 whose phase and frequency are fixed to a constant, cf. "Methods"). Initially, each generator operates at a locally stable steady-state condition determined by the power-flow equations. At time t = 2 s, a three-phase fault occurs in the transmission line connecting nodes 16 and 17. After 0.5 s, the fault is cleared; however, the generators have lost synchrony and deviate from their steady-state values (Fig. 5 b). To recover the normal behavior of the grid, 0.5 s after the clearance of the fault, we apply a short, optimal control input to the frequency of the generators to steer the state (phase and frequency) of the generators back to its steady-state value. The input is computed from data via Eq. (4) using N = 4000 input/state experiments collected by locally perturbing the state of the generators around its normal operation point using the real, nonlinear swing dynamics (see also "Methods"). We consider data sampled with period T s = 2.5 × 10 −4 s, and set the control horizon to T = 400 time samples (corresponding to 0.1 s), R = I, and Q = ε I with ε = 0.01 to enforce locality of the controlled trajectories. As shown in Fig. 5 c, the data-driven input drives the state of the generators to a point close enough to the starting synchronous solution (left, inset) so as to asymptotically recover the correct operation of the grid (right). We remark that, because of the nonlinearity of the dynamics, the data-driven control input is not able to exactly steer the state the network back to the original synchronous state, but to a point close to it. The latter point, however, falls within the basin of attraction of the synchronous solution. Thus, the control input is able to correctly steer the network to the desired synchronous state, although not in finite time. Notably, as previously discussed, the computation of the control input requires only pre-collected data, is numerically efficient, and optimal (for the linearized dynamics). More generally, this numerical study shows that the data-driven strategy in Eq. (4) could represent a simple, viable, and computationally efficient approach to control complex nonlinear networks around an operating point.

---

### Human confidence judgments reflect reliability-based hierarchical integration of contextual information [^36ff42e1]. Nature Communications (2019). High credibility.

Response distribution

We assume that the probability of obtaining the behavioral confidence report y t on trial t conditional on the data d t and the model parameters is a Gaussian distribution truncated to the interval from zero to one. The mean parameter of the normal distribution is set to the model prediction. The latter is denoted byto distinguish it from the response y of the participant which is formally represented by a draw from the response distribution to account for task-intrinsic behavioral variability beyond the variations captured by the model. The standard deviation (SD) parameter θ of the Gaussian is assumed to be constant and robustly estimated from the data (see Supplementary Methods). When analyzing the patterns of behavior produced by a fitted model (either a heuristic model or the optimal model with distortions), we computed the expected value of the mode under the truncated gaussian noise. Because of such truncated noise, the expected value is more centered than the noiseless model prediction:whereand

As our data might be contaminated by other processes such as lapses, we take precaution against far outlying responses. The response likelihood is calculated for all responses as

Additionally, to prevent isolated points from being assigned virtually zero probability we generally add a small probability ofto all. This corresponds to the probability of a point at four standard deviations from the standard normal distribution. For non-outlying points this alteration is considered negligible. To avoid singularity problems common to fitting mixture models, we constrained the SD parameter θ to be larger than 0.01 during fitting.

Inferential patterns for fitted block tendency

The probabilistic model assumes that the block tendency from which the trial-by-trial (airplane) proportions μ are drawn is given by one of two skewed Beta-distributions (see Methods). By convention a 'blue' (b = 1) context is characterized by the block tendency Betawhile the 'red' context (b = 0) is correspondingly denoted by Beta. The two distributions are symmetric with respect to the block aligned trial majorities, which immediately follows from the property of the Beta distribution: BetaBeta. A variation of the optimal inference routine (Eqs. (7–9)) is used that allows for different values of the parameters v 1, v 2 governing the block tendency with the restriction that v 1 ≥ v 2. In addition, the sigmoidal response mapping (Eq. (13)) is used to allow for nonlinear distortions of the output.

---

### An official American Thoracic Society workshop report: developing performance measures from clinical practice guidelines [^0802211b]. Annals of the American Thoracic Society (2014). Medium credibility.

Process-based performance measure development — When developing process-based measures, considerations include the purpose of measurement, the importance of the clinical problem, ease of measurement, variability in delivered care, opportunity for improvement, and potential unintended consequences; successful measures are described as important, scientifically sound, and feasible. After selecting a care process, steps include writing an indicator statement, specifying eligibility and the desired action in plain language, defining the denominator and numerator, and specifying how patient preferences are handled, with options such as removing refusals from the numerator, removing refusals from both the numerator and denominator, or adding refusals to both as a marker that the indicator was offered but refused.

---

### The XZZX surface code [^8f6ac9c8]. Nature Communications (2021). High credibility.

This structured noise model thus leads to two distinct regimes, depending on which failure process is dominant. In the first regime where, we expect that the logical failure rate will decay like. We find this behaviour with systems of a finite size and at high bias where error rates are near to threshold. We evaluate logical failure rates using numerical simulations to demonstrate the behavior that characterises this regime; see Fig. 6 (a). Our data show good agreement with the scaling ansatz. In contrast, our data are not well described by a scaling.

Fig. 6
Sub-threshold scaling of the logical failure rate with the XZZX code.

a Logical failure rateat high bias near to threshold plotted as a function of code distance d. We use a lattice with coprime dimensions d × (d + 1) for d ∈ {7, 9, 11, 13, 15} at bias η = 300, assuming ideal measurements. The data were collected usingiterations of Monte-Carlo (MC) samples for each physical rate sampled and for each lattice dimension used. The physical error rates used are, from the bottom to the top curves in the main plot, p = 0.19, 0.20, 0.21, 0.22 and 0.23. Error bars represent one standard deviation for the Monte-Carlo simulations. The solid lines are a fit of the data to, consistent with Eq. (2), and the dashed lines a fit to, consistent with Eq. (3) where we would expect, see Methods. The data fit the former very well; for the latter, the gradients of the best fit dashed lines, as shown on the inset plot as a function of, give a linear slope of 0.61(3). Because this slope exceeds the value of 0.5, we conclude that the sub-threshold scaling is not consistent with. b Logical failure ratesat modest bias far below threshold plotted as a function of the physical error rate p. The data (markers) were collected at bias η = 3 and coprime d × (d + 1) code dimensions of d ∈ {5, 7, 9, 11, 13, 15} assuming ideal measurements. Data is collected using the Metropolis algorithm and splitting method presented in refs. The solid lines represent the prediction of Eq. (3). The data show very good agreement with the single parameter fitting for all system sizes as p tends to zero.

---

### Do we need to improve the reporting of evidence in tendinopathy management? A critical appraisal of systematic reviews with recommendations on strength of evidence assessment [^9708e49f]. BMJ Open Sport & Exercise Medicine (2021). High credibility.

Pooling of results

Item 3: The principal summary measures for each included outcome measure need to be stated in the methods. For quantitative analyses, where outcome measures are continuous, a justification needs to be provided for the use of (raw) mean differences (MD) and not standardised MD (SMD) and vice versa (ie, identical or different outcome measure tools used across studies, respectively). Similarly, for dichotomous outcomes, the authors should state in the methods whether OR or relative risks (RR) were used.

Item 4: An SR without a meta-analysis should ideally have some quantitative summary measures (ie, MD, SMD, OR or RR with accompanying CI) to demonstrate the treatment effect of the assessed intervention over the comparator. Where results are pooled only based on direction of effect (ie, increased, decreased or unchanged), this needs to be stated in the methods with a justification. If pooling is not possible and results are only described narratively, the reason should be stated (usually substantial clinical heterogeneity); in such cases, we question whether the article should be identified as an SR.

Item 5: When meta-analyses are performed, the model used needs to be stated (ie, fixed effects or random effects) with a justification and the statistical heterogeneity assessment (usually χ 2 and/or I 2 test) and how it was used in data syntheses. Sensitivity and subgroup analyses should also be pre-defined in the methods.

Items 6 and 7: Results should only be pooled for similar follow-up time periods which should be prespecified in the methods. Traditionally these are short term, mid term and long term. The range of these should be defined by the authors in the methods based on the population, outcome measures and interventions. Equally, assessed outcome measures should be predefined (usually as part of PICOS).

Item 8: A statement needs to be included in the section explaining how the authors dealt with missing data, which are usually methodological details of the included studies needed for the RoB assessment and descriptive statistics for quantitative analyses. For both, ideally attempts should be made to contact the authors of the RCTs for retrieval of the missing data and this should be stated. If significant statistical data of included studies cannot be retrieved (ie, sample sizes and means) these studies should be excluded from quantitative analyses; if variability statistics (SD) are missing, the authors have the option to impute these using data from other RCTs and the imputation method used should be described.

---

### Designing accurate emulators for scientific processes using calibration-driven deep models [^5e03881e]. Nature Communications (2020). High credibility.

Predictive models that accurately emulate complex scientific processes can achieve speed-ups over numerical simulators or experiments and at the same time provide surrogates for improving the subsequent analysis. Consequently, there is a recent surge in utilizing modern machine learning methods to build data-driven emulators. In this work, we study an often overlooked, yet important, problem of choosing loss functions while designing such emulators. Popular choices such as the mean squared error or the mean absolute error are based on a symmetric noise assumption and can be unsuitable for heterogeneous data or asymmetric noise distributions. We propose Learn-by-Calibrating, a novel deep learning approach based on interval calibration for designing emulators that can effectively recover the inherent noise structure without any explicit priors. Using a large suite of use-cases, we demonstrate the efficacy of our approach in providing high-quality emulators, when compared to widely-adopted loss function choices, even in small-data regimes.

---