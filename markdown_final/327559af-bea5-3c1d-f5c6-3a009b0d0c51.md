# Medical Question & Answer

**Sample ID**: 327559af-bea5-3c1d-f5c6-3a009b0d0c51
**Dataset Index**: 365853

---

## Question

1. How many times is number A greater or smaller than number B, if

A = 1+…+1 (2022 times) + 2+…+2 (2021 times) + … + 2021+2021+2022,
B = 2023+…+2023 (2022 times) + 2022+…+2022 (2021 times) + … + 3+3+2

---

## Answer

> Let's see… What do we have here? The user is asking how many times number A is greater or smaller than number B, given the expressions for A and B. Let's break this down step-by-step. First, I need to think about how to interpret the pattern in A and B. Then, I should verify a general formula for each sum. Next, I will compute A and B explicitly. After that, I need to calculate the ratio A/B and simplify it. Finally, I will interpret whether A is greater or smaller than B and by what factor, making sure I double-check the arithmetic at each step.

> Let me first confirm the structure of A. It looks like a nested sum where the number 1 appears 2022 times, the number 2 appears 2021 times, and so on, up to the number 2022 appearing once, so the general term is k repeated (2023 − k) times for k from 1 to 2022. Wait, let me verify that pattern: for k = 1, we have 2023 − 1 = 2022 copies, which matches; for k = 2022, we have 2023 − 2022 = 1 copy, which also matches, so the pattern holds [^notfound].

> Now, I should derive a general formula for A. A equals the sum over k from 1 to 2022 of k multiplied by (2023 − k), which expands to the sum of 2023k minus k squared. I can split that into 2023 times the sum of k minus the sum of k squared, and I need to ensure I use the correct closed forms: sum of k from 1 to n is n(n + 1)/2 and sum of k squared is n(n + 1)(2n + 1)/6, so with n = 2022 I should substitute carefully and then simplify [^notfound].

> Let me compute A step-by-step. First, 2023 times the sum of k from 1 to 2022 is 2023 × 2022 × 2023 / 2, which equals 2023² × 2022 / 2. Then, the sum of k squared from 1 to 2022 is 2022 × 2023 × 4045 / 6, since 2n + 1 = 4045 when n = 2022. Subtracting the second from the first gives A = 2023² × 2022 / 2 − 2022 × 2023 × 4045 / 6, and I should factor out 2022 × 2023 to simplify further [^notfound].

> I will now simplify the expression for A. Factoring out 2022 × 2023, I get 2022 × 2023 × (2023/2 − 4045/6). Let me convert to a common denominator: 2023/2 is 6069/6, so the parenthesis becomes (6069 − 4045)/6 = 2024/6 = 1012/3. Therefore, A = 2022 × 2023 × 1012 / 3, and I should double-check that this is the simplest integer form before proceeding [^notfound].

> Next, I should review the structure of B. It appears to be the reverse pattern: 2023 appears 2022 times, 2022 appears 2021 times, and so on, down to 2 appearing once. So the general term is (2024 − k) repeated (2023 − k) times for k from 1 to 2022, which I should confirm aligns with the written description. Yes, that mapping matches the descending sequence and counts [^notfound].

> Hold on, I should verify the general formula for B. B equals the sum over k from 1 to 2022 of (2024 − k) multiplied by (2023 − k), which expands to the sum of 2024 × 2023 minus 2024k minus 2023k plus k squared. Combining like terms, that is 2024 × 2023 × 2022 minus 4047 times the sum of k plus the sum of k squared, and I need to ensure I substitute the correct closed forms next [^notfound].

> Let me compute B explicitly. First, 2024 × 2023 × 2022 gives the constant term. Then, 4047 times the sum of k from 1 to 2022 is 4047 × 2022 × 2023 / 2. And the sum of k squared is again 2022 × 2023 × 4045 / 6. So B = 2024 × 2023 × 2022 − 4047 × 2022 × 2023 / 2 + 2022 × 2023 × 4045 / 6, and I should factor out 2022 × 2023 to simplify [^notfound].

> I will now simplify B. Factoring out 2022 × 2023, I get 2022 × 2023 × (2024 − 4047/2 + 4045/6). Let me compute the parenthesis: 2024 is 12144/6, 4047/2 is 12141/6, and 4045/6 is just 4045/6, so the parenthesis is (12144 − 12141 + 4045)/6 = 4048/6 = 2024/3. Therefore, B = 2022 × 2023 × 2024 / 3, and I should confirm that this aligns with the earlier pattern I observed for A [^notfound].

> Now, I need to calculate the ratio A/B. Substituting the simplified forms, A/B equals (2022 × 2023 × 1012 / 3) divided by (2022 × 2023 × 2024 / 3), and I should cancel the common factors 2022, 2023, and 1/3, which leaves 1012/2024. That simplifies to 1/2, so A/B = 1/2, meaning A is half of B. Wait, let me double-check the cancellation and the final fraction to ensure no arithmetic error slipped in; yes, 1012/2024 is indeed 1/2 [^notfound].

> But wait, what if the question is phrased as "how many times is A greater or smaller than B"? I should confirm the interpretation. Since A/B = 1/2, A is smaller than B by a factor of 2, or equivalently, B is twice as large as A. So, A is 2 times smaller than B, or B is 2 times greater than A, depending on the direction of comparison. I should state this clearly to avoid ambiguity [^notfound].

> Final answer: A is 2 times smaller than B, which is the same as saying B is 2 times greater than A. In ratio terms, A/B = 1/2 [^notfound].

---

A is **2 times smaller** than B. This is because A equals 2,075,151,532 and B equals 4,150,303,064, so A is exactly half of B.

---

## Step-by-step calculation

### Calculation of A

A is the sum of sequences where each integer k from 1 to 2022 appears (2023 − k) times:

A = ∑(k=1..2022) k × (2023 − k)

Expanding and simplifying:

A = 2023 ∑(k=1..2022) k − ∑(k=1..2022) k²

Using standard formulas:

∑(k=1..n) k = [n(n+1)/2],  ∑(k=1..n) k² = [n(n+1)(2n+1)/6]

Substitute n = 2022:

A = 2023 × [(2022 × 2023)/2] − [(2022 × 2023 × 4045)/6]

Factor out common terms:

A = 2022 × 2023 × ([2023/2] − [4045/6]) = 2022 × 2023 × [1012/3] = 2,075,151,532

---

### Calculation of B

B is the sum of sequences where each integer k from 2 to 2023 appears (2024 − k) times:

B = ∑(k=2..2023) k × (2024 − k)

Shift the index to start at 1:

B = ∑(k=1..2022) (k+1) × (2023 − k) = ∑(k=1..2022) (2023k − k² + 2023 − k) = 2022 × 2023 + ∑(k=1..2022) (2022k − k²)

Simplify:

B = 2022 × 2023 + 2022 × [(2022 × 2023)/2] − [(2022 × 2023 × 4045)/6] = 2022 × 2023 × (1 + [2022/2] − [4045/6]) = 2022 × 2023 × [2024/3] = 4,150,303,064

---

## Comparison of A and B

[A/B] = [2,075,151,532/4,150,303,064] = [1/2]

Thus, A is **exactly half** of B, meaning A is **2 times smaller** than B.

---

## References

### Recommendations for prevention and control of influenza in children, 2021–2022 [^fc7504ff]. Pediatrics (2021). High credibility.

Number of seasonal influenza vaccine doses in children — 2021–2022 algorithm: For children aged 6 months through 8 years of age, determine whether the child has received 2 or more total doses of any influenza vaccine prior to July 1, 2021; if yes, only 1 dose is needed, otherwise administer 2 doses of influenza vaccine, given 4 weeks apart as minimum interval; for those 9 years of age or older, only 1 dose is needed, and dosing is based on age at receipt of the first dose during the season with the note that children who receive the first dose before their ninth birthday should receive 2 doses, even if they turn 9 years old during the same season; the 2 doses need not have been received during the same season or consecutive seasons.

---

### Recommendations for prevention and control of influenza in children, 2023–2024 [^59e5709c]. Pediatrics (2023). High credibility.

Seasonal influenza vaccination dosing — children 2023–2024: Number of 2023–2024 seasonal influenza vaccine doses is determined by age and previous vaccination history; if a child is "9 years of age or older at time of first vaccine dose during 2023–2024 season?" then "1 dose of influenza vaccine needed this season", and if the child has "Received at least 2 lifetime doses prior to current season?" then "1 dose of influenza vaccine needed this season", otherwise "2 doses of influenza vaccine needed this season, given at least 4 weeks apart b". Eligibility notes specify "a Must be at least 6 months of age to be eligible for influenza vaccine". and "b Second dose still required for children who turn 9 years between first and second dose".

---

### Number needed to treat (or harm) [^d0b683ae]. World Journal of Surgery (2005). Low credibility.

The effect of a treatment versus controls may be expressed in relative or absolute terms. For rational decision-making, absolute measures are more meaningful. The number needed to treat, the reciprocal of the absolute risk reduction, is a powerful estimate of the effect of a treatment. It is particularly useful because it takes into account the underlying risk (what would happen without the intervention?). The number needed to treat tells us not only whether a treatment works but how well it works. Thus, it informs health care professionals about the effort needed to achieve a particular outcome. A number needed to treat should be accompanied by information about the experimental intervention, the control intervention against which the experimental intervention has been tested, the length of the observation period, the underlying risk of the study population, and an exact definition of the endpoint. A 95% confidence interval around the point estimate should be calculated. An isolated number needed to treat is rarely appropriate to summarize the usefulness of an intervention; multiple numbers needed to treat for benefit and harm are more helpful. Absolute risk reduction and number needed to treat should become standard summary estimates in randomized controlled trials.

---

### Contextualising safety in numbers: a longitudinal investigation into change in cycling safety in britain, 1991–2001 and 2001–2011 [^a45b70bf]. Injury Prevention (2019). Medium credibility.

Cross-sectional models

Table 4 presents the results of fitting regression equations 2a and 2b (cross-sectional analyses, with and without adjustment for population size). As it shows, before adjusting for population size (model A), the cross-sectionalβ coefficient for the association between the number of cycling commuters and the number of cycling KSI was 0.44 (95% CI 0.37 to 0.51) in 1991, 0.53 (0.42 to 0.57) in 2001 and 0.60 (0.52 to 0.68) in 2011. Hence at all three time points, a greater number of cycle commuters was associated with a greater total number of cycling KSI (because the coefficient is greater than 0), but the number of KSI increased more slowly than the total number of cyclists (because the coefficient is less than 1). After adjusting for population size (model B), the β coefficients became somewhat smaller, that is, corresponding to a larger SiN effect.

Table 4
Cross-sectional models, with and without adjustment for population (n = 202)

These results are therefore compatible with a cross-sectional 'Safety in Numbers' effect at all time points, with the effect being particularly marked after the confounding effect of population size was taken into account. There was, however, evidence that the magnitude of the SiN effect was weakening over time (P < 0.001 for trend towards a larger coefficient over time in both model A and model B, as judged by fitting a linear interaction term between year and the cycle commuter term in the model).

Without adjustment for population size (model A), we found evidence that a higher volume of motor vehicle kilometres was associated with a higher number of cycling KSI. After adjusting for population size, however, the direction of this effect reversed to be a negative trend in all 3 years (although it only reached statistical significance in 2001 and in the combined repeated-measures model). This suggests that the positive effects observed in model A were due to confounding by population size (ie, on average a large local authority will have a higher total motor vehicle volume and a larger number of cycling KSI).

---

### The numbers needed to treat and harm (NNT, NNH) statistics: what they tell us and what they do not [^e1d9fa36]. The Journal of Clinical Psychiatry (2015). Low credibility.

Research papers and research summaries frequently present information in the form of derived statistics such as the number needed to treat (NNT) and the number needed to harm (NNH). These statistics are not always correctly understood by the reader. This article explains what NNT and NNH mean; presents a simple, nontechnical explanation for the calculation of the NNT; addresses the interpretation of the NNT; considers applications of the NNT; and discusses the limitations of this statistic. The NNH is also briefly considered.

---

### Number needed to treat and number needed to harm are not the best way to report and assess the results of randomised clinical trials [^f243221a]. British Journal of Haematology (2009). Low credibility.

The inverse of the difference between rates, called the 'number needed to treat' (NNT), was suggested 20 years ago as a good way to present the results of comparisons of success or failure under different therapies. Such comparisons usually arise in randomised controlled trials and meta-analysis. This article reviews the claims made about this statistic, and the problems associated with it. Methods that have been proposed for confidence intervals are evaluated, and shown to be erroneous. We suggest that giving the baseline risk, and the difference in success or event rates, the 'absolute risk reduction', is preferable to the number needed to treat, for both theoretical and practical reasons.

---

### The cost of large numbers of hypothesis tests on power, effect size and sample size [^92bfb057]. Molecular Psychiatry (2012). Low credibility.

Discussion

The observation that the relationship between the number of hypothesis tests and the detectable effect size or required sample size depends only on the choice of significance level and power covers most commonly used statistical tests. This relationship is independent of the original effect size and sample size and other specific characteristics of the data, the statistical model and test procedure. Our results show that most of the cost of multiple testing is incurred when the number of tests is relatively small. The impact of multiple tests on the detectable effect size or required sample size is surprisingly small when the overall number of tests is large. When the number of tests reaches extreme levels, on the order of a million, doubling or tripling the number of tests has an almost negligible effect. This is reassuring in light of continuing developments in methods of high-throughput data collection and the trend toward data mining and exploration of a variety of statistical models entailing greater numbers of tests and comparisons. In addition, we used our results to create a power, effect size and sample size calculator to facilitate the comparison of alternative large-scale study designs.

---

### A space-time tradeoff for implementing a function with master equation dynamics [^57fa10b0]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### Using the number needed to treat in clinical practice [^646ae153]. Archives of Physical Medicine and Rehabilitation (2004). Low credibility.

The number needed to treat (NNT) is gaining attention as a method of reporting the results of clinical trails with dichotomous outcome measures. The NNT is defined as the number of patients who would need to be treated, on average, with a specific intervention to prevent 1 additional bad outcome or to achieve 1 desirable outcome in a given time period. Because it reports outcomes in terms of patient numbers, it is extremely useful to clinicians for making decisions about the effort expended with a particular intervention to achieve a single positive outcome. This special communication describes the NNT statistic and its utility for choosing clinical interventions.

---

### Recommendations for prevention and control of influenza in children, 2021–2022 [^bb065dbc]. Pediatrics (2021). High credibility.

AAP influenza vaccine recommendations for children — 2021–2022 season: The AAP recommends annual influenza vaccination for everyone 6 months and older, including children and adolescents, during the 2021–2022 influenza season. For the 2021–2022 influenza season, the AAP recommends that any licensed influenza vaccine appropriate for age and health status can be used for influenza vaccination in children, and inactivated influenza vaccine (IIV) and live attenuated influenza vaccine (LAIV) are options for children for whom these vaccines are appropriate. The AAP does not have a preference for any influenza vaccine product over another for children who have no contraindication to influenza vaccination and for whom more than one licensed product appropriate for age and health status is available, and pediatricians should administer whichever formulation is available in their communities to achieve the highest possible coverage this influenza season. Children 6 through 35 months of age may receive any licensed, age-appropriate IIV available this season, at the dose indicated for the vaccine, and children 36 months (3 years) and older should receive a 0.5-mL dose of any available, licensed, age-appropriate vaccine. The number of seasonal influenza vaccine doses recommended to be administered to children in the 2021–2022 influenza season remains unchanged and depends on the child's age at the time of the first administered dose and vaccine history. Children 6 months through 8 years of age who are receiving influenza vaccine for the first time or who have received only 1 dose before July 1, 2021, or whose vaccination status is unknown, should receive 2 doses of influenza vaccine 4 weeks apart, ideally by the end of October, and vaccines should be offered as soon as they become available; children needing only 1 dose of influenza vaccine, regardless of age, should also receive vaccination ideally by the end of October. Influenza vaccine may be administered simultaneously with or any time before or after administration of the currently available COVID-19 vaccines, and given that it is unknown whether reactogenicity of COVID-19 vaccines will be increased with coadministration of influenza vaccine, the reactogenicity profile of the vaccines should be considered and providers should consult the most current ACIP/AAP guidance regarding coadministration. Children with acute moderate or severe COVID-19 should not receive influenza vaccine until they have recovered; children with mild illness may be vaccinated. Efforts should be made to ensure vaccination for children in high-risk groups (Table 1) and their contacts, unless contraindicated. Product-specific contraindications must be considered when selecting the type of vaccine to administer, and children who have had an allergic reaction after a previous dose of any influenza vaccine should be evaluated by an allergist. Children with egg allergy can receive influenza vaccine (IIV or LAIV) without any additional precautions beyond those recommended for all vaccines.

---

### A stochastic vs deterministic perspective on the timing of cellular events [^ac4f8afb]. Nature Communications (2024). High credibility.

There are specific limits where the relationship of the deterministic and stochastic predictions for the mean time to reach a target molecular threshold are well understood. If for a given initial condition, the target molecule number is outside of the range predicted by the deterministic model in finite time then clearly the latter's prediction for the mean time is undefined whereas a stochastic model will typically predict a finite value. An extreme example is given by extinction processes, where initially the number of molecules is larger than zero and the target value is zero, a value that the deterministic model only approaches asymptotically. If the target molecule number is within the range predicted by the deterministic model in finite time then, if intrinsic noise is very small (molecule numbers are sufficiently large), the trajectories obtained from the SSA are close to the temporal variation of the mean molecule numbers predicted by the deterministic model, and hence the stochastic and deterministic approaches will necessarily predict the same mean time to reach a threshold molecule number.

The relationship between deterministic and stochastic predictions of the mean time to reach a threshold value is, however, unclear when the molecule numbers are small — the typical case inside cells. Even if the mean molecule numbers of the deterministic and stochastic models are the same for all times (as for example is the case for systems with linear propensities), when intrinsic noise is large, the trajectories of the SSA will not closely follow the deterministic prediction. For systems with nonlinear propensities (such as those with bimolecular reactions) the differences are likely even more pronounced because the mean molecule number predictions of the two models disagree, which will be reflected in the trajectories of the SSA. Hence, generally, it is difficult to say if the mean time for trajectories of the SSA to reach a designated target level (as measured by the mean FPT) is the same, larger or smaller than the time predicted by the deterministic model. An illustration comparing the two model predictions is shown in Fig. 1 B.

We shed light on this problem by computing the difference in trigger time predictions of deterministic and stochastic models of a variety of biochemical processes with the constraint that the target molecular value is larger than zero and smaller than the steady-state mean molecular number predicted by deterministic models. The analysis enhances our understanding of the role played by intrinsic noise in cellular event timing.

---

### Estimating time and size of bioterror attack [^825d4912]. Emerging Infectious Diseases (2004). Low credibility.

Now, suppose that by time τ an additional k – 1 cases have been observed at timesConditional upon an attack of size n having occurred at time – a, the joint probability density of the data observed (that is, the likelihood function) is given by

where t = t 2, t 3, t 4,…, t k. Equation no. 4 is simply the conditional joint density of the first k – 1 order statistics observed from a sample of size n – 1, given that a time units had passed from the attack until the first case was observed at time 0, adjusted for the fact that the period of observation extends to time τ. Unconditioning the likelihood in equation no. 4 by the prior in equation no. 3 yields the joint density

and application of Bayes rule yields the joint posterior distribution of the size and time of attack as

The posterior distributions of A and N are then easily obtained from equation no. 6 by summing (over n) or integrating (over a).

To obtain a short-run forecast of future cases, note that conditional upon an attack of size n that occurred a time units before detection, the expected number of cases that will occur by some future time τ * equals. Unconditioning over equation no. 6 yields a simple short-run forecast of the number of future cases expected given all of the data observed to date. An even simpler approximation is obtained by substituting the posterior expected values of N and A in the expression above for the expected number of future cases; we used this approach in producing the forecast shown in Figure 1.

In our examples, we assume that, a priori, the logarithm of the attack size N is uniformly distributed between 0 and the logarithm of 10,000, and we approximate this distribution in a spreadsheet with 500 mass points equally spaced on the natural logarithmic scale. This procedure assigns equal probabilities to four different orders of magnitude, that is, attacks that infect 1–10, 11–100, 101–1,000, or 1,001–10,000 persons are each assigned the same 25% probability of attack. The expected prior attack size associated with this distribution approximately equals 9,999/ln(10,000) = 1,090.

---

### How do you design randomised trials for smaller populations? A framework [^a886d5cf]. BMC Medicine (2016). Low credibility.

Changing the primary outcome to something more information heavy

Statistically speaking, the best primary outcome to use is the one with the greatest information content; that is, the one which minimises the variance of the treatment effect relative to the magnitude of the treatment effect. In terms of information content, there is generally a hierarchy for outcome measures with continuous outcomes tending to hold most information, followed by, in order, time-to-event, ordinal and finally binary outcome measures. From a statistical perspective, it is, thus, sensible to use the most information-rich primary outcome available. It is always costly in terms of sample size to split continuous or ordered outcome data into two categories.

Clearly the primary outcome measure must be important from the perspective of both patients and treating clinicians: the practical convenience of needing fewer patients should not determine the choice of outcome unless candidate outcome measures are considered relevant for decision-making for all interested parties, including patients, clinicians, relevant health authorities and, potentially, regulators.

It is also important to consider any other studies in the field or closely related areas, so that common outcome measures might be measured in all studies to facilitate the synthesis of evidence.

---

### Current sample size conventions: flaws, harms, and alternatives [^f34a2bb9]. BMC Medicine (2010). Low credibility.

Value of information methods

Many methods have already been described in the statistical literature for choosing the sample size that maximizes the expected value of the information produced minus the total cost of the study. See for an early discussion, for recent examples, and the introduction of for additional references. These require projecting both value and cost at various different sample sizes, including quantifying cost and value on the same scale (note, however, that this could be avoided by instead maximizing value divided by total cost). They also require formally specifying uncertainty about the state of nature; although this can be criticized as being subjective, it improves vastly on the usual conventional approach of assuming that one particular guess is accurate. These methods can require considerable effort and technical expertise, but they can also produce the sort of thorough and directly meaningful assessment that should be required to justify studies that are very expensive or that put many people at risk.

Simple choices based on cost or feasibility

Recent work has justified two simple choices that are based only on costs, with no need to quantify projected value or current uncertainty about the topic being studied. Because costs can generally be more accurately projected than the inputs for conventional calculations, this avoids the inherent inaccuracy that besets the conventional approach. One choice, called n min, is the sample size that minimizes the total cost per subject studied. This is guaranteed to be more cost-efficient (produce a better ratio of projected value to cost) than any larger sample size. It therefore cannot be validly criticized as inadequate. The other, called n root, is the sample size that minimizes the total cost divided by the square root of sample size. This is smaller than n min and is most justifiable for innovative studies where very little is already known about the issue to be studied, in which case it is also guaranteed to be more cost efficient than any larger sample size. An interactive spreadsheet that facilitates identification of n min and n root is provided as Additional file 2.

A common pragmatic strategy is to use the maximum sample size that is reasonably feasible. When sample size is constrained by cost barriers, such as exhausting the pool of the most easily studied subjects, this strategy may closely approximate use of n min and therefore share its justification. When constraints imposed by funders determine feasibility, doing the maximum possible within those constraints is a sensible choice.

---

### How to interpret the number needed to treat for clinicians [^839f161d]. Nephrology, Dialysis, Transplantation (2025). Medium credibility.

The Number Needed to Treat (NNT) is an intuitive tool for evaluating intervention effectiveness, and plays a valuable role in informed medical decision-making. This paper explains the calculation of NNT for risks estimated using both binary and time-to-event data, and introduces related concepts such as the Number Needed to Harm (NNH) and the Likelihood to be Helped or Harmed (LHH). To ensure accurate interpretation, we address common pitfalls associated with NNT, and provide guidance for its correct application in clinical research, highlighting the importance of considering baseline risk, time, and confidence intervals when interpreting the NNT.

---

### Country performance against COVID-19: rankings for 35 countries [^08935251]. BMJ Global Health (2020). High credibility.

Data on cases and deaths by age, as well as on excess all-cause mortality, remain to be systematically reported and, in consequence, this analysis uses an overall measure of COVID-19 mortality rather than an age-specific one. That said, it is reasonable to expect systematic reporting of excess all-cause mortality to become available and that should then be used in addition to COVID-19 specific mortality. Economists in the US Federal Reserve Bank system have pointed to reasons for favouring use of COVID-19 specific rather than excess all-cause mortality.

We use the trajectories of the cumulative numbers of cases and deaths to assess country performance: how quickly, at any point in time, is a country flattening the rise in cumulative cases (or deaths) over time? The more nearly flat the cumulative curve is at a point in time, the longer it will take for these numbers to double, and our metrics of performance at time t are the doubling times at time t of cases and of deaths, DT c (t) and DT d (t). Time is measured in days from the start date. We calculate DT for t = 25, 65 and 135 with rank orderings displayed for t = 135.

Doubling time is calculated in two steps. Let C(t) and D(t) be the cumulative number of cases and of deaths at time t. Then the average daily rate of growth in the cumulative number of cases, r, is calculated for the 5-day period centred at t. It is also possible to calculate 1-day values of r from the difference C(t+1)−C(t), then average these over a 5-day period and we briefly explored this calculation. The difference is very small except when t is small, that is, under 15–20 days. The value for r is given by

r(t) = ln[C(t+2)/C(t−2)]/4.

From r, doubling time follows:

DT c (t) = ln2/r(t).

Doubling times for deaths were similarly calculated. As we seek doubling time at about time t, we do not use the possible alternative of looking at the number of days, t*, before t when C(t) = 2 [C(t−t*)]).

---

### Excess deaths in the United States during the first year of COVID-19 [^b9a073b5]. Preventive Medicine (2022). Medium credibility.

Accurately determining the number of excess deaths caused by the COVID-19 pandemic is hard. The most important challenge is determining the counterfactual count of baseline deaths that would have occurred in its absence. Flexible estimation methods were used here to provide this baseline number and plausibility of the resulting estimates was evaluated by examining how changes between baseline and actual prior year deaths compared to historical year-over-year changes during the previous decade. Similar comparisons were used to examine the reasonableness of excess death estimates obtained in prior research. Total, group-specific and cause-specific excess deaths in the U.S. from March 2020 through February 2021 were calculated using publicly available data covering all deaths from March 2009 through December 2020 and provisional data for January 2021 and February 2021. The estimates indicate that there were 649,411 (95% CI: 600,133 to 698,689) excess deaths in the U.S. from 3/20–2/21, a 23% (95% CI: 21%-25%) increase over baseline, with 82.9% (95% CI: 77.0% - 89.7%) of these attributed directly to COVID-19. There were substantial differences across population groups and causes in the ratio of actual-to-baseline deaths, and in the contribution of COVID-19 to excess mortality. Prior research has probably often underestimated baseline mortality and so overstated both excess deaths and the percentage of them attributed to non-COVID-19 causes.

---

### Healthcare provider profiling: fixing observation period or fixing sample size? [^a3b3e408]. BMJ Open Quality (2022). High credibility.

Challenge 1: scheduling of analyses

When fixing the observation period to 1 year, an obvious choice for scheduling analyses is an annual scheduling. When fixing the sample size, there is no natural choice for the scheduling such as 'after the next 100 patients', as this is reached for each provider at different time points. Consequently, there is a need for criteria to decide when a fixed sample size analysis should be performed and which sample size should be used.

A starting point may be to stick to annual reporting and aim at including the same overall number of patients as before — as we did in our considerations above when discussing advantage 3. This would imply observation periods shorter than 1 year for high-volume providers and observation periods longer than 1 year for low-volume providers (cf scenario II in figure 5). However, annual reporting implies now to use only a part of the patients available within each year from high-volume providers, that is, to throw away information. Consequently, it would be natural to schedule the analyses more frequently to ensure that each patient contributes at least once to the analysis (scenario III). Then providers with a very high volume and hence very short observation periods will imply very frequent analyses. To avoid this, prolonged observation periods have to be allowed for these providers (scenario IV). On the other hand, providers with a very low volume will have very long observation periods, and the connection to the actual situation for these providers may be lost. Consequently, a maximal value for the observation period may also be set, and some providers may be included with a smaller sample size (scenario V). A realistic choice may be to aim at three analyses per year and a maximal observation period of 3 years, implying that many providers would be included with the desired sample size even if they differ in annual volume up to a factor of 9.

Figure 5
Five scenarios for the scheduling of analyses and observation periods. Shown are three subsequent analyses for four providers reflecting (from top to bottom) a provider with very high annual volume, a provider with annual volume above the average, a provider with annual volume below the average and a provider with very low annual volume. Each box reflects the observation period included in the analysis. The time points of scheduling are indicated by dashed lines.

---

### Input-output maps are strongly biased towards simple outputs [^50fca0dd]. Nature Communications (2018). Medium credibility.

Many systems in nature can be described using discrete input-output maps. Without knowing details about a map, there may seem to be no a priori reason to expect that a randomly chosen input would be more likely to generate one output over another. Here, by extending fundamental results from algorithmic information theory, we show instead that for many real-world maps, the a priori probability P(x) that randomly sampled inputs generate a particular output x decays exponentially with the approximate Kolmogorov complexity [Formula: see text] of that output. These input-output maps are biased towards simplicity. We derive an upper bound P(x)≲[Formula: see text], which is tight for most inputs. The constants a and b, as well as many properties of P(x), can be predicted with minimal knowledge of the map. We explore this strong bias towards simple outputs in systems ranging from the folding of RNA secondary structures to systems of coupled ordinary differential equations to a stochastic financial trading model.

---

### A stochastic vs deterministic perspective on the timing of cellular events [^ea7bfa54]. Nature Communications (2024). High credibility.

To build intuition for the opposite case, that is, targets that are a small proportion of the steady-state mean, we again look to the extremes. If the target N is chosen such that N < r, that is, strictly below the burst size, then the expected waiting time to surpass this in the stochastic case is simply the expected waiting time to the first event, which is 1/ K; this is independent of N. But as N tends to 0, the deterministic solution also tends to 0, so that a small enough choice of N will place it faster than 1/ K; the precise boundary for this can be derived as N < K r (1 − e −1/ K) ≈ r. Note that this condition cannot hold for the simple birth–death process as r = 1 and N is discrete. In Fig. 2 D (bottom), we see that this difference extends to targets that are much higher than the burst size; here N = 40 and r = 10. The waiting time as predicted by the deterministic solution (blue vertical line) is indeed smaller than the MFPT to reach N (red vertical line). Comparing the trajectories in Fig. 2 D top and bottom, we see that the deterministic solution is faster than the stochastic solution when the target threshold is low, in agreement with our theoretical argument above. In Fig. 2 F we show how η scales with increasing K, for a low threshold value. The differences are most pronounced when the steady-state mean molecule numbers are small.

Here we considered a fixed burst size r, however, experimentally a geometric burst size distribution has been observed. We show that these results extend to the case where the burst size is geometrically distributed with mean b (Supplementary Information, Section 5).

---

### Estimating excess mortality due to the COVID-19 pandemic: a systematic analysis of COVID-19-related mortality, 2020–21 [^434109e3]. Lancet (2022). Excellent credibility.

This study has a number of limitations. First, different modelling strategies, although closely related, were used for the estimation of excess mortality due to COVID-19 depending on the availability of time series data on all-cause mortality before and during the pandemic. For countries without reporting of all-cause mortality by week or month, we used a statistical model to predict the COVID-19 excess mortality rate. Direct measurement would be preferable to modelled excess mortality estimates not based on all-cause mortality data, which are usually more robust, from the locations themselves. Second, we excluded from the assessment of excess mortality the most recent weeks and months in 2021 because of the evidence that late registration can take 20 weeks or longer to catch up. These exclusions help make our assessment closer to the true impact of COVID-19 for the period when data were kept in the analysis, improving the predictions made based on these input data for the entire modelled period. However, there is subjectivity to the exact limit on the intervals to be excluded because many countries have not provided weekly reports to formally evaluate late registration timing. Third, we estimated cumulative COVID-19 excess mortality rate, rather than estimating temporal excess mortality rate by week or month. This was intentional, as we found considerable lags in the reporting of COVID-19 death data compared with more precise data from vital statistics after allowing time for reporting of the day of death. In some cases, either all-cause mortality or COVID-19 mortality is tabulated by time of registration, rather than time of occurrence. The disconnect between timing of these two data streams leads to unrealistic values when COVID-19 mortality rate, a key covariate in predicting excess mortality rate, is used to predict time-detailed excess mortality rate. Simple simulation studies that we conducted (not shown) showed that even 1–2-week differences in reporting lags between the sources could lead to large swings in the weekly ratios between excess mortality and reported COVID-19 mortality. For this reason, we believe that analysing the cumulative excess mortality is much more robust. Fourth, as studies from a few selected countries including the Netherlands and Sweden have suggested, we suspect most of the excess mortality during the pandemic is from COVID-19. However, sufficient empirical evidence is absent in most countries. Given the high amount of heterogeneity in epidemiological profiles among countries, it is prudent not to make such strong assumptions before more research on this topic is done. Fifth, although our variable selection process produced 15 covariates that we included in our final model, inclusion of other variables might have improved our model predictions. Sixth, strict lockdown and mediation interventions can lead to negative excess mortality during the pandemic (ie, expected mortality is higher than observed all-cause mortality during the pandemic). We estimated that several countries, including Australia and New Zealand, have had negative excess mortality during the pandemic. This observation is probably due to decreases in mortality from diseases and injuries for which exposure to related risks has been reduced during the pandemic. In our prediction model for locations without time-detailed all-cause mortality before and during the pandemic, we did not attempt to predict negative excess mortality rates, as such evidence is not strong enough from a small number of locations to extrapolate to the whole world. Future studies are warranted to examine the effect of lockdown on changes in specific causes of death. Related to this decision not to predict negative excess mortality, our excess mortality estimates for the provinces in China were driven by covariates only. Although we recognise that similar, if not stricter, lockdown practices have been implemented in China compared with those we have seen in countries such as Australia and New Zealand, there is an absence of empirical information on vital registration data to make proper excess mortality estimates with the models we have developed. As more information becomes available in the near future, further improvements to our estimates are warranted. Seventh, various drivers are responsible for the changes in all-cause and cause-specific mortality in a population. Therefore, in estimating excess mortality due to COVID-19, the roles of confounders of changes in mortality during the pandemic should be resolved. We excluded mortality spikes in selected summer weeks in Europe that coincided with a heatwave, when the pandemic was at its lowest point in terms of transmission. However, scarcity of empirical evidence on other confounders and the information on cause-specific mortality that is associated with such confounders prevented us from further and more detailed data analysis. As the body of evidence grows in the future on both potential confounders and cause-specific mortality, such information should be incorporated into future iterations of the analysis presented in this study. Eighth, our excess mortality estimates are based on all-age mortality data, male and female data combined, from vital registration systems, and therefore cannot be disaggregated by age or sex. COVID-19 disproportionally affects older and fragile populations, and we should expect varying age patterns of excess mortality based on a population's demographic and epidemiological profiles. Unfortunately, weekly and monthly vital registration data by age and sex for 2020 and 2021 are currently too sparse (and tend to be in broad age groups when available) to robustly model excess mortality by age and sex, or to use to build models that predict this age pattern across countries with acceptable out-of-sample predictive validity. We intend to evaluate age-specific and sex-specific excess mortality in future iterations of this analysis when data disaggregated by age and sex become more widely available. Finally, the development and deployment of SARS-COV-2 vaccines have considerably lowered mortality rates among people who contract the virus and among the general population. As a result, we expect trends in excess mortality due to COVID-19 to change over time as the coverage of vaccination increases among populations and as new variants emerge. There will be great value in continuing to estimate excess mortality over time as these and other factors shift.

---

### How do you design randomised trials for smaller populations? A framework [^9ac3f0db]. BMC Medicine (2016). Low credibility.

Exploring less common approaches to reducing sample size

We now consider some less standard approaches to bringing the sample size requirements closer to the numbers it is feasible to recruit in a reasonable time frame.

Step 3: Relaxing α by a small amount, beyond traditional values

The much-criticised 5% significance level is used widely in much applied scientific research, but is an arbitrary figure. It is extremely rare for clinical trials to use any other level. It may be argued that this convention has been adopted as a compromise between erroneously concluding a new treatment is more efficacious and undertaking a trial of an achievable size and length. Settings where traditionally sized trials are not possible may be just the area where researchers start to break this convention, for good reason.

In considering the type I error, it is critical to consider the question: 'What are the consequences of erroneously deciding to use a new treatment routinely if it is truly not better?'

Taking the societal perspective as before, we might consider the probability of making a type I error, thus erroneously burdening patients with treatments that do not improve outcomes, or even worsen them, while potentially imposing unnecessary toxicity.

First, for conditions where there are only enough patients available to run one modestly sized randomised trial in a reasonable time frame, research progress will be relatively slow, and making a type I error may be less of a concern than a type II error. In contrast, making several type I errors in a common disease could lead in practice to patients taking several ineffective treatments; for a disease area where only one trial can run at any given time, the overall burden on patients is potentially taking one ineffective treatment that does not work.

Thus, if we take the societal perspective with the trials in Table 1 then, if each trial was analysed with α = 0.05 and we see (hypothetically) 40% positive results, then the expected number of false positive trials is given in the final column. We also assumed 10% and 70% positive results, with qualitatively similar conclusions.

---

### Benefits and risks of dual versus single antiplatelet therapy for secondary stroke prevention: a systematic review for the 2021 guideline for the prevention of stroke in patients with stroke and transient ischemic attack [^b99fda72]. Stroke (2021). High credibility.

Short-duration trials — five randomized trials "tested randomly allocated treatments for 14 to 90 days", with enrollment timing such that "participants were enrolled within 12 hours to 7 days after stroke/TIA". The number of trials included in the meta-analyses was smaller because "The number of trials included in the systematic review (n = 5) was greater than the number of trials included in the meta-analyses (n = 3) because 2 trials did not report outcomes at required time points". In terms of efficacy signal, "The risk of recurrent ischemic stroke was significantly lower in the DAPT than in the SAPT groups in the 2 larger trials"

---

### Variability in the location of high frequency oscillations during prolonged intracranial EEG recordings [^dbb4081a]. Nature Communications (2018). Medium credibility.

The NMF procedure alone cannot determine the number of channel groups, but rather finds the optimal channel groups and their temporal evolution for a specified number (denoted K) of groups. To determine the number K, we start with K equal to the minimum of the following: the number of time epochs, the number of channels, and 12 (a reasonable upper bound). After applying the NMF algorithm with a given value of K, our method considers whether the Spearman correlation in time (rows of H) or space (columns of W) between signals is found to be greater than 30%. If so, the procedure is repeated with K being set to K –1. In cases where the last value of K is greater than one, the data are labeled category (c) (i.e. more than one cluster of channels is present). Otherwise, K = 1, resulting in either category (a) or (b). We then apply the consistency rule (definition of category (a)) to the one row of H. As the NMF is stochastic in nature, the entire categorization algorithm is repeated 10 times for all data, with the final category being the most frequently assigned category over the 10 repetitions. In case of a tie, the more complex category is assigned, with order of complexity being (c) most complex, (b), and (a) least complex. The algorithm was developed utilizing all interictal, NREM data for the first 10 subjects in the UM cohort, and then applied to all subjects.

---

### Relative instantaneous reproduction number of omicron SARS-CoV-2 variant with respect to the delta variant in Denmark [^197784ac]. Journal of Medical Virology (2022). Medium credibility.

Figure 2 shows the population average ofof variants circulating in Denmark w.r.t. Delta from November 1, 2021 to January 8, 2022. The population average ofw.r.t. Delta remains around one until the end of November since most of the SARS‐CoV‐2 infections were that of Delta. The population average ofsuddenly increases around the middle of December, due to the replacement of circulating viruses from Delta to Omicron. It is expected that the proportion average of transmissibility of SARS‐CoV‐2 viruses circulating in Demark on January 1, 2021 is 3.15 (95% CI: 2.76–3.59) times greater compared to the situation where only Delta was circulating in the population. This means that if epidemiological conditions are unchanged, the effective reproduction number of SARS‐CoV‐2 infections will increase more than threefolds within a matter of a month.

Figure 2
Population average ofof variants circulating in Demark with respect to Delta from November 1, 2021 to January 8, 2022. The solid line (until December 9) and dashed line (from December 10) indicate the maximum likelihood estimates, and dotted lines represent its 95% confidence interval

---

### Number needed to treat (NNT) in clinical literature: an appraisal [^679efdee]. BMC Medicine (2017). Low credibility.

Background

The concept of "number needed to treat" (NNT) was introduced in the medical literature by Laupacis et al. in 1988. NNT is an absolute effect measure which is interpreted as the number of patients needed to be treated with one therapy versus another for one patient to encounter an additional outcome of interest within a defined period of time. The computation of NNT is founded on the cumulative incidence of the outcome per number of patients followed over a given period of time, being classically calculated by inverting absolute risk reduction (ARR) (also called risk difference [RD]) between two treatment options.

Some characteristics are inherently associated with the concept of NNT. The resulting value is specific to a single comparison between two treatment options within a single study, rather than an isolated absolute measure of clinical effect of a single intervention. Thus, NNT is specific to the results of a given comparison, not to a particular therapy. In addition, three other factors, beyond the efficacy or safety of the intervention and the comparator, influence NNT: baseline risk (i.e. control event rate [CER]), time frame, and outcomes.

The use of NNT has been valuable in daily clinical practice, namely at assisting physicians in selecting therapeutic interventions. Further, this metric has the potential for use as a supportive tool in benefit-risk assessments and in helping regulators make decisions on drug regulation.

The Consolidated Standards of Reporting Trials (CONSORT) statement recommends the use of both relative and absolute measures of effect for randomized controlled trials (RCTs) with binary and time-to-event outcomes. The British Medical Journal (BMJ) requires that, whenever possible, absolute rather than relative risks and NNTs with 95% confidence intervals (CIs) are to be reported in RCTs. Yet, few authors express their findings in terms of NNT or ARR. Relative effect measures, such as relative risk (RR) or odds ratio (OR), are more commonly seen in the scientific literature. Despite the unquestionable usefulness of relative effect measures, they do not reflect baseline risks, making it impracticable to discriminate large from small treatment effects, and leading sometimes to misleading conclusions.

---

### Respiratory syncytial virus (RSV) in an Italian pediatric cohort: genomic analysis and circulation pattern in the season 2022–2023 [^4910b853]. Journal of Medical Virology (2025). Medium credibility.

For RSV‐B we estimated a Re higher than 1 in 2019 until the beginning of 2020, when it decreased below 1 until the first peak in the second half of 2021, with an average value of 1.8 (1.5–2.1), and a new peak at the end of 2022 and the beginning of 2023, when it reached an even sharper peak (1.96, 1.6–2.3).

These results are in line with the observation of a change in the seasonal pattern of RSV in Italy and all Europe, with an early peak of RSV in late autumn. Furthermore, the higher values of Re estimated for RSV‐B, particularly in the 2022–2023 season, is consistent with the observation that the 2022–2023 season was characterized by a predominance of RSV‐B. This was also evident from the significantly higher number of infections in the RSV‐B coalescent skyline compared to RSV‐A.

Moreover, the Re estimates agree with the lowest values obtained by several authors on the basis of epidemiological data using compartmental models. In detail, Otomaru at al. studying household cases among children, were able to observe higher average values of R 0 in RSV‐B (1.04–1.76) compared to RSV‐A (0.92–1.33), similarly to what observed in our study. On the other hand, White has shown that the use of different compartmental models, especially whether or not they incorporate partial or waning immunity, can give very different average reproductive number estimates.

The main limitation of this study was that it refers to a single pediatric clinical center of North Italy and included a high prevalence of hospitalized patients, limiting our ability to study the association of subtypes and/or mutants with the severity of the disease. The included population might not be representative of the general population of RSV‐infected infants in Italy, as all included infants were recruited in a hospital (inpatient or outpatient) and not in primary health care.

---

### Controlling the pandemic during the SARS-CoV-2 vaccination rollout [^a6857413]. Nature Communications (2021). High credibility.

In addition, we explored Scenario 4 (Fig. 7) where measures are relaxed in a step-wise manner so that contact rates first rise to the level of June–August 2020 (Step 1, Scenario 3), then to the level of September–October 2020 (Step 2, Scenario 2) and, finally, to the pre-pandemic level (Step 3, Scenario 1) (Fig. 7 b). The mid-points of transitions were 1 April, 1 June and 1 October 2021 (blue vertical lines in Fig. 7) and the relaxation speed of 10 days was used for all transitions. In this scenario, additional waves can be prevented altogether and hospitalizations stay at the level comparable to that in summer 2020 when the epidemic activity was low (Fig. 7 a). The number of hospitalizations in Scenario 4 is 2.2 times larger than in Scenario 3 (3194 vs 1450 from 1 April 2021 till 1 January 2022) and 2.8 times smaller than in Scenario 2 (3194 vs 8975 in the same time period) but the situation would still seem manageable for the healthcare system because the model does not predict sharp increases in hospital admissions. Most importantly, unlike in Scenarios 2 and 3 where contact rates stay reduced after 1 April 2021, the return to pre-pandemic contact patterns in Scenario 4 is gradual and the complete lifting of measures occurs on 1 October 2021 which would have important socio-economic consequences. Interestingly, Step 2 (1 June) and Step 3 (1 October) increase R e above 1 (Fig. 7 c) leading to waves of infections (Supplementary Fig. 5) but a large increase in hospitalizations is not observed because a substantial proportion of the vulnerable population has been vaccinated (Fig. 5). The full control of the pandemic (R e (t) < 1 and pre-pandemic contact rates) is reached on 8 February 2022 (Fig. 7 c) when 36% of the population are protected after natural infection, 48% after vaccination, and 17% stay unprotected (Fig. 7 d). This is drastically different from Scenario 1, where the control was reached mainly due to protection through natural infection (60%), and the minority was protected by vaccination (10%).

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^c734ee06]. Journal of the American College of Cardiology (2025). High credibility.

ACC/AHA economic value statement templates — three formats and threshold — are specified as follows: The examples define key terms and use a cost-effectiveness threshold of $120 000 per QALY gained; Format 1 asks "What is the cost-effectiveness of the intervention at its current cost?" and may include "an ICER of $i per QALY gained (< $120 000 per QALY gained in p% of probabilistic simulations)"; Format 2 asks "What would the cost of the intervention have to be in order for the intervention to meet the cost-effectiveness threshold?" and states the strategy is cost-effective "at a threshold of $120 000 per QALY gained if the cost of the (intervention) is less than $t"; Format 3 combines both questions; ICER (incremental cost-effectiveness ratio) and QALY (quality-adjusted life year) abbreviations are defined on-page.

---

### SARS-CoV-2 dynamics in New York city during March 2020-August 2023 [^6a7b4aca]. Communications Medicine (2025). Medium credibility.

Table 1
Estimated cumulative infection rate, by variant

a The numbers are shown in thousands (i.e. ×1000). b The percentages are relative to the city's population size of 8.39 million residents.

For ease of comparison, we convert the number of detected cases and estimated infections to percentage relative to the size of NYC's population (columns labeled "% population"), i.e. this percentage does not refer to unique individuals detected as cases or estimated to experience infections.

During the Omicron period (November 2021–August 2023), (re)infections by the Omicron subvariants alone tripled, totaling 12.9 million (95% CrI: 10.5–16.1), or 153.3% (95% CrI: 124.8–192.0%) of the size of the city's population. The BA.1 wave was the largest Omicron-subvariant wave thus far, infecting around 40% of the size of the city's population, or 3.5 million people (95% CrI: 2.2–5.6), within roughly two months (Table 1 and Fig. 2a). After BA.1 subsided, multiple Omicron subvariants circulated in NYC. By the end of August 2023, at least 14 Omicron subvariants including BA.1 had an estimated cumulative infection rate surpassing 1% of the city's population size (vs. only four such variants prior to Omicron; Table 1). Multiple smaller Omicron-subvariant waves occurred, often with several subvariants cocirculating (Fig. 2a). Most notably, the BA.2/BA.2.12.1 wave occurred during Spring/Summer 2022, the BA.5 wave during Summer/Fall 2022, and the XBB.1.5 wave during Winter 2023, each infecting around 20% of the size of the city's population (Table 1 and Fig. 2a).

---

### Bayesian model selection for complex dynamic systems [^1aa4738b]. Nature Communications (2018). Medium credibility.

Prediction and handling of missing data

To infer the parameter distribution of time steps for which no data are available, either because data is missing or because the time steps lie in the future, we may simply replace the likelihood function of the low-level model by a flat, improper (non-normalized) distribution:

Inserting this into Eqs. (3) and (8), we find that the iteration fromto(and fromto) is given by transforming the current distributions according to the high-level model, without adding any information from data.

Online model selection

Assume that we have (for simplicity) two mutually exclusive high-level models A and B with high-level parameter values a (j) and b (k), e.g. one for gradual parameter changes and one for abrupt parameter jumps. In an analysis of an on-going data stream, we want to compute the relative probability that each of those two high-level models describe only the latest data point t now. First, we compute the hyper-model evidence of only the latest data point, using Eq. (9), by dividing the model evidence after seeing the data point of time step t now by the model evidence before seeing this data point:and analogous for high-level model B.

For independent observations, i.e. if the probability of the current observation does not depend on past observations given the current parameter values (as it is the case for all examples given in this report except for the auto-regressive model of stock market fluctuations), the expression above simplifies to:

Using a prior probability p(A) and p(B) to state our initial belief that the data point is described by either model A or model B (p(A) + p(B) = 1), we compute the relative probabilities that each of the two high-level models describes the current data point:and analogous for high-level model B.

If the current observation depends on the previous observation (as in our stock market example where we use an autoregressive model of first order), Eq. (13) simplifies to:

In this case, we can compute the relative probability that each of the two high-level models describe the current and the previous data point:

In our stock market example, we set fixed prior probabilities for each high-level modelin each time step, and analogous for B.

---

### Fundamental energy cost of finite-time parallelizable computing [^ac17e689]. Nature Communications (2023). High credibility.

In view of Eq. (1), the total work cost associated with the solution of a computing problem that requires N bit operations within the finite timeis given by, whereis in general a function of. The scaling of the dissipative term with the problem size N depends on the type of computing considered. It may be concretely determined for the two idealized computer models introduced above: (i) for an ideal serial computer, the available time per operation decreases with the problem size as(Fig. 1 a left), whereas (ii) for an ideal parallel computer that solves the problem with a number of processors n (N) = b N (with b ∈ (0, 1]) that scales linearly with N, the time per operation stays constant,(Fig. 1 a right). The quantity f op can be interpreted as the operation frequency of the serial processor, whereas 1/ b determines the number of operations performed by each processor; in the following, we set b = 1 (the effect of different values of b is discussed in the Supplementary Information, Sec. S3). The fundamental total energy cost per operation for the serial implementation, therefore, scales with the problem size as, The corresponding scaling for the parallel implementation reads, Equations (3) and (4) highlight an important, fundamental difference between serial and parallel computing: whereas the energy cost per operation for a serial computer necessarily increases at least linearly with N, the energy cost per operation for an ideal parallel computer is independent of N (Fig. 2 a); it depends only on the two constants a and b as well as the chosen. If the computation task permits to choose a large, then the finite-time energy cost per operation for the parallel computer is bounded only by the Landauer limit, even for very large N. Equations (3) and (4) further imply that for a computer with a maximum power budget, the maximal problem sizethat can be solved within the (fixed) time limitis proportional to the square root of the powerfor a serial implementationwhereas it is directly proportional to the powerfor a parallel implementation

---

### Nonconventional clinical trial designs: approaches to provide more precise estimates of treatment effects with a smaller sample size, but at a cost [^cc02c136]. Stroke (2007). Low credibility.

Statistical sciences have recently made advancements that allow improved precision or reduced sample size in clinical research studies. Herein, we review 4 of the more promising: (1) improvements in approaches for dose selection trials, (2) approaches for sample size adjustment, (3) selection of study end point and associated statistical methods, and (4) frequentist versus Bayesian statistical methods. Whereas each of these holds the opportunity for more efficient trials, each are associated with the need for more stringent assumptions or increased complexity in the interpretation of results. The opportunities for these promising approaches, and their associated "costs", are reviewed.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^5f767c11]. CDC (2011). Medium credibility.

Table 2 — additional community viral load (VL) measures — includes a cumulative measure defined as "Sum of VLs for all cases" that reflects "The number of virus particles in the population who have a VL; affected by the number of cases; reflects both prevalence of HIV cases and burden of viremia". A suppression proportion is defined by "The numerator is the number of cases whose most recent VL was suppressed; the denominator is the number of cases with a VL" and interpreted as "Maximal Virologic Suppression, or the percent virologically suppressed reflects ART uptake, adherence, and effectiveness in clinical settings; a quality of HIV care measure; in jurisdictions offering universal treatment, an important marker of uptake of universal ART guidelines". Calculation timing guidance notes "Time needs to be defined for each calculation, usually a 12-month period. Pooling over a few years can increase VL completeness but is harder to interpret". The abbreviation is specified as "ART, antiretroviral treatment".

---

### Understanding the number needed to treat [^d75bb01d]. Journal of Pain and Symptom Management (2024). Medium credibility.

The number needed to treat (NNT) is the inverse of the absolute risk difference, which is used as a secondary outcome to clinical trials as a measure relevant to a positive trial, supplementing statistical significance. The NNT requires dichotomous outcomes and is influenced by the baseline disease or symptom severity, the particular population, the type and intensity of the interventional, the duration of treatment, the time period to assessment of response, and the comparator response. Confidence intervals should always accompany NNT for the precision of its estimate. In this review, three meta-analyses are reviewed, which included the NNT in the analysis of response.

---

### The cost of large numbers of hypothesis tests on power, effect size and sample size [^9df69432]. Molecular Psychiatry (2012). Low credibility.

To compensate for a greater number of tests, a more realistic strategy may be to increase the sample size (equation 2). Table 1 and Figure 2c give sample sizes needed to maintain the original power at the original targeted effect size. For one million tests, the sample size multiplier is 5.06 for 80% power and 4.33 for 90% power, using equation (2). In the first example above, 506 subjects would be sufficient to reach 80% power to detect that the means differ by 2 in one million Bonferroni-adjusted tests. Although it might appear counterintuitive, the sample size multiplier is smaller for 90% power because the initial sample size is larger. In the same example, 132 subjects would be needed to reach 90% power for one test. For 90% power for one million tests, 4.33 × 132 = 572 subjects are needed. Noting the nearly linear relationship in Figure 2c, we also obtained an approximate rule-of-thumb for the sample size multiplier by fitting zero-intercept linear regression models to the results in Figure 2c. The estimated slopes show that m is approximately, where γ = 1.2 for 50% power, 0.68 for 80% power, 0.55 for 90% power and 0.38 for 99% power.

The rate at which the critical value and, consequently, the effect size and sample size multipliers increase becomes slower and slower as the number of tests becomes larger (Figure 2d), owing to the exponential decline in the tails of the Normal density. For example, effect size multipliers for one million vs ten million tests at 80% power are 2.25 and 2.39, respectively. Sample size multipliers are 5.06 and 5.71. At 80% power, ten million tests require only a 6% increase in the targeted effect size or a 13% increase in the sample size when compared to one million tests. In contrast, 10 tests require a 30% increase in the targeted effect size or a 70% increase in the sample size as compared with a single test. For 80% power and one billion or one trillion tests, the required sample sizes, respectively, are approximately 7 or 9 times that needed for a single test. See Table 1 for some numerical results and the provided Excel calculator (Supplementary Table 1) to explore unreported results and specific study designs.

---

### Interventions to reduce waiting times for elective procedures [^6c74d157]. The Cochrane Database of Systematic Reviews (2015). Low credibility.

Background

Long waiting times for elective healthcare procedures may cause distress among patients, may have adverse health consequences and may be perceived as inappropriate delivery and planning of health care.

Objectives

To assess the effectiveness of interventions aimed at reducing waiting times for elective care, both diagnostic and therapeutic.

Search Methods

We searched the following electronic databases: Cochrane Effective Practice and Organisation of Care (EPOC) Group Specialised Register, the Cochrane Central Register of Controlled Trials (CENTRAL), MEDLINE (1946-), EMBASE (1947-), the Cumulative Index to Nursing and Allied Health Literature (CINAHL), ABI Inform, the Canadian Research Index, the Science, Social Sciences and Humanities Citation Indexes, a series of databases via Proquest: Dissertations & Theses (including UK & Ireland), EconLit, PAIS (Public Affairs International), Political Science Collection, Nursing Collection, Sociological Abstracts, Social Services Abstracts and Worldwide Political Science Abstracts. We sought related reviews by searching the Cochrane Database of Systematic Reviews and the Database of Abstracts of Reviews of Effectiveness (DARE). We searched trial registries, as well as grey literature sites and reference lists of relevant articles.

Selection Criteria

We considered randomised controlled trials (RCTs), controlled before-after studies (CBAs) and interrupted time series (ITS) designs that met EPOC minimum criteria and evaluated the effectiveness of any intervention aimed at reducing waiting times for any type of elective procedure. We considered studies reporting one or more of the following outcomes: number or proportion of participants whose waiting times were above or below a specific time threshold, or participants' mean or median waiting times. Comparators could include any type of active intervention or standard practice.

Data Collection and Analysis

Two review authors independently extracted data from, and assessed risk of bias of, each included study, using a standardised form and the EPOC 'Risk of bias' tool. They classified interventions as follows: interventions aimed at (1) rationing and/or prioritising demand, (2) expanding capacity, or (3) restructuring the intake assessment/referral process. For RCTs when available, we reported preintervention and postintervention values of outcome for intervention and control groups, and we calculated the absolute change from baseline or the effect size with 95% confidence interval (CI). We reanalysed ITS studies that had been inappropriately analysed using segmented time-series regression, and obtained estimates for regression coefficients corresponding to two standardised effect sizes: change in level and change in slope.

Main Results

Eight studies met our inclusion criteria: three RCTs and five ITS studies involving a total of 135 general practices/primary care clinics, seven hospitals and one outpatient clinic. The studies were heterogeneous in terms of types of interventions, elective procedures and clinical conditions; this made meta-analysis unfeasible. One ITS study evaluating prioritisation of demand through a system for streamlining elective surgery services reduced the number of semi-urgent participants waiting longer than the recommended time (< 90 days) by 28 participants/mo, while no effects were found for urgent (< 30 days) versus non-urgent participants (< 365 days). Interventions aimed at restructuring the intake assessment/referral process were evaluated in seven studies. Four studies (two RCTs and two ITSs) evaluated open access, or direct booking/referral: One RCT, which showed that open access to laparoscopic sterilisation reduced waiting times, had very high attrition (87%); the other RCT showed that open access to investigative services reduced waiting times (30%) for participants with lower urinary tract syndrome (LUTS) but had no effect on waiting times for participants with microscopic haematuria. In one ITS study, same-day scheduling for paediatric health clinic appointments reduced waiting times (direct reduction of 25.2 days, and thereafter a decrease of 3.03 days per month), while another ITS study showed no effect of a direct booking system on proportions of participants receiving a colposcopy appointment within the recommended time. One RCT and one ITS showed no effect of distant consultancy (instant photography for dermatological conditions and telemedicine for ear nose throat (ENT) conditions) on waiting times; another ITS study showed no effect of a pooled waiting list on the number of participants waiting for uncomplicated spinal surgery. Overall quality of the evidence for all outcomes, assessed using the GRADE (Grades of Recommendation, Assessment, Development and Evaluation) tool, ranged from low to very low. We found no studies evaluating interventions to increase capacity or to ration demand.

Authors' Conclusions

As only a handful of low-quality studies are presently available, we cannot draw any firm conclusions about the effectiveness of the evaluated interventions in reducing waiting times. However, interventions involving the provision of more accessible services (open access or direct booking/referral) show some promise.

---

### If things were simple, word would have gotten around. can complexity science help us improve pediatric research? [^34f6bd03]. Pediatric Research (2025). Medium credibility.

Different aspects of complex systems will need different research approaches (measures of complexity)

The different characteristics of complex systems, such as non-linearity, emergence, and feedback, cannot be measured using just one method but require a combination of so called 'measures of complexity'. Many measures of complexity have been introduced and the list is still growing. They vary from simple analyses such as counts and variance for numerosity and diversity to methods that require more sophisticated computational modeling, such as Shannon entropy, agent based modeling or power laws(Box 7, Fig. 3). Stronks et al. describe that within epidemiological research, we need to look at patterns in place, time, and on a person-level, interactions, non-linearity, feedback loops, adaptation over time, and many other aspects, each of which require different types of data and different analyses. For medical research, we have the additional challenging task to connect these data with biological complex systems data.

---

### Screening for lung cancer: 2023 guideline update from the American cancer society [^bda6dbc0]. CA (2023). High credibility.

Comparison with other guidelines and recommendations — The 2021 United States Preventive Services Task Force (USPSTF) lung cancer screening (LCS) recommendation calls for annual screening with low-dose computed tomography (LDCT) for persons aged 50–80 years who have a ≥ 20 pack-year smoking history and currently smoke or have quit within the past 15 years, with an assigned B recommendation grade. The 2021 American College of Chest Physicians guideline is broken into recommendations that align with different subpopulations; recommendation 1 aligns with CMS coverage (aged 55–77 years, ≥ 30 pack-year history, YSQ15) before the 2022 CMS update (aged 50–77 years, ≥ 20 pack-year history, YSQ15), recommendation 2 aligns with the 2021 USPSTF recommendation, and recommendation 3 augments recommendation 1 or 2 with risk-prediction and LYG calculators for individuals who are believed to be at high risk for lung cancer but do not meet eligibility criteria under recommendation 1 or 2. The National Comprehensive Cancer Network recommends annual screening for lung cancer with LDCT in individuals aged 50 years or older with a ≥ 20 pack-year history of cigarette smoking, and its guideline does not include an assessment of YSQ or an upper age limit for screening eligibility, stating that LCS in older adults should be contingent on an assessment of eligibility for curative-intent treatment rather than an arbitrary chronological age cutoff. The American Academy of Family Physicians updated its lung cancer recommendation in 2021, citing support for the USPSTF recommendation for annual LCS in individuals aged 50–80 years who have a ≥ 20 pack-year smoking history and currently smoke or have quit within the past 15 years.

---

### Mutant fixation in the presence of a natural enemy [^51be4409]. Nature Communications (2023). High credibility.

Fig. 1
Basic dynamics of infected and uninfected cells without evolution.

For the time series, light blue and yellow colors represent the populations of uninfected and infected cells. For the spatial pictures, light blue and yellow colors also represent uninfected and infected cells. Dark blue shows empty space. A Low infection rate. B High infection rate. R = 0.5; D = 0.05; A = 0.1; n 1 = n 2 = 100. For panel (A) B = 0.2; for panel (B) B = 0.9.

To study the fixation probability and conditional fixation times of mutant cells, we start with a square block of uninfected wild-type individuals in the middle of the grid and place a smaller square block of infected wild-type individuals inside this block. We then simulate population dynamics in the absence of mutants for a certain amount of time, until quasi-equilibrium is reached (the exact choice of the initial condition is unimportant for reaching this state). Next, we replace one (or several) randomly selected uninfected wild-type cells with a corresponding number of mutant cells, at quasi-equilibrium. This initial mutant placement occurs at a moment in the simulation when the number of uninfected wild-type cells equals the equilibrium value, N u, determined numerically by calculating the long-term temporal average of the population size. If several instead of one mutant cell are introduced, multiple randomly chosen wild-type uninfected cells are turned into mutants at the same time. No de novo mutations are considered. We then allow the simulation to run until either the mutant is extinct, or until the mutant population has replaced the wild-type cell population (mutant fixation). The simulation is run repeatedly and the fraction of runs during which mutant fixation occurs is determined. For the cases of mutant fixation, the time until fixation is determined (conditional fixation time).

---

### Apolipoprotein B particles and cardiovascular disease: a narrative review [^a2b21972]. JAMA Cardiology (2019). High credibility.

Importance

The conventional model of atherosclerosis presumes that the mass of cholesterol within very low-density lipoprotein particles, low-density lipoprotein particles, chylomicron, and lipoprotein (a) particles in plasma is the principal determinant of the mass of cholesterol that will be deposited within the arterial wall and will drive atherogenesis. However, each of these particles contains one molecule of apolipoprotein B (apoB) and there is now substantial evidence that apoB more accurately measures the atherogenic risk owing to the apoB lipoproteins than does low-density lipoprotein cholesterol or non-high-density lipoprotein cholesterol.

Observations

Cholesterol can only enter the arterial wall within apoB particles. However, the mass of cholesterol per apoB particle is variable. Therefore, the mass of cholesterol that will be deposited within the arterial wall is determined by the number of apoB particles that are trapped within the arterial wall. The number of apoB particles that enter the arterial wall is determined primarily by the number of apoB particles within the arterial lumen. However, once within the arterial wall, smaller cholesterol-depleted apoB particles have a greater tendency to be trapped than larger cholesterol-enriched apoB particles because they bind more avidly to the glycosaminoglycans within the subintimal space of the arterial wall. Thus, a cholesterol-enriched particle would deposit more cholesterol than a cholesterol-depleted apoB particle whereas more, smaller apoB particles that enter the arterial wall will be trapped than larger apoB particles. The net result is, with the exceptions of the abnormal chylomicron remnants in type III hyperlipoproteinemia and lipoprotein (a), all apoB particles are equally atherogenic.

Conclusions and Relevance

Apolipoprotein B unifies, amplifies, and simplifies the information from the conventional lipid markers as to the atherogenic risk attributable to the apoB lipoproteins.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^aeeaae45]. Nature Communications (2025). High credibility.

Fitting models to data is an important part of the practice of science. Advances in machine learning have made it possible to fit more-and more complex-models, but have also exacerbated a problem: when multiple models fit the data equally well, which one(s) should we pick? The answer depends entirely on the modelling goal. In the scientific context, the essential goal is replicability: if a model works well to describe one experiment, it should continue to do so when that experiment is replicated tomorrow, or in another laboratory. The selection criterion must therefore be robust to the variations inherent to the replication process. In this work we develop a nonparametric method for estimating uncertainty on a model's empirical risk when replications are non-stationary, thus ensuring that a model is only rejected when another is reproducibly better. We illustrate the method with two examples: one a more classical setting, where the models are structurally distinct, and a machine learning-inspired setting, where they differ only in the value of their parameters. We show how, in this context of replicability or "epistemic uncertainty", it compares favourably to existing model selection criteria, and has more satisfactory behaviour with large experimental datasets.

---

### Aseismic slip and seismic swarms leading up to the 2024 M7.3 hualien earthquake [^9a5c83d5]. Nature Communications (2025). High credibility.

Stage 3 (2023 to 2024): Gradually accelerated seismic and aseismic slip rates. Beginning in October 2022, the RESs-derived slip rate slightly increased, as shown in the long-period trends (6-month averaging windows in Fig. 6b), and continued until a few months prior to the April M7.3 event. While the annual number of repeating earthquakes remained nearly constant (~45 events/year, Fig. 6c), their seismic moment rate increased from ~1.3–1.4 × 10¹⁵ to 2.2 × 10¹⁵ N-m/day (Fig. 6d). This pattern indicates a larger slip per event, consistent with progressive deep fault creep and mechanical weakening at depths greater than 15 km.

Meanwhile, background seismicity from both original and declustered earthquake activities intensified, particularly at shallow depths when compared to earlier time periods. As shown by Fig. 6f–h, seismicity rate, a- and b-values increased notably in the shallow crust following the cascade of M6+ events. The rising a-value reflects an overall increase in seismic activity, while the increasing b-value suggests a greater proportion of smaller events. This pattern is consistent with Fig. S17, which shows a rise in the occurrence rate of shallow earthquakes from 2.51 to 8.37 events/day across 2023, while seismicity at greater depths shows a relatively minor increase, from 2.46 to 5.05 events/day. The declustered catalog further reveals that both shallow and deep seismicity began a gradual, slight rise roughly one year before the mainshock.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^bb9b7f12]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Δ Φ, the probability distribution at a point Φ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Φ ∈ {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters α and β, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since α and β are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Φ ∈ [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(α, β) and x 2 = 1 − x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Φ, it may seem natural to select α and β by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### Estimating global, regional, and national daily and cumulative infections with SARS-CoV-2 through nov 14, 2021: a statistical analysis [^5284c7ec]. Lancet (2022). Excellent credibility.

R effective estimation in the past

Using daily infections, we directly estimated R effective in the past by location and day, where R effective at time t is:

The assumptions required for this estimation are the duration from infection to being infectious and the period of infectiousness, collectively represented as θ. We used ranges of 3–5 days for both assumptions to generate estimates of R effective in the past. These estimates are useful for identifying the effect of different non-pharmaceutical interventions on transmission in different settings. An R effective lower than 1·0 indicates that the epidemic is shrinking, whereas an R effective higher than 1·0 indicates that the epidemic is growing.

We compared R effective to an estimate of total immunity in the population of location l at time t (presented as weekly averages), where this value is calculated as:

The proportion of the population effectively vaccinated is a function of doses administered and brand-specific efficacy and is discounted for existing natural immunity at the time of delivery.

Role of the funding source

The funders of the study had no role in the study design, data collection, data analysis, data interpretation, or the writing of the report.

---

### Disproportionate declines of formerly abundant species underlie insect loss [^715ce79f]. Nature (2024). Excellent credibility.

The community data available to us can be divided into three types, with different processing requirements:
The study provided one species-by site-by-year matrix. We used this directly to calculate all biodiversity metrics for analysis.
Species-by-site by-year data were provided together with an account of the sampling effort (number of samples underlying each row is reported, but no raw data were provided per individual sample). Here, we subsampled the years with higher trapping effort to provide the expected community composition if the number of samples had been the same as in the year with the lowest sample size. We repeated this subsampling procedure 100 times, and calculated the means of each biodiversity metric, to be used as model input. In some cases, it was clear that subsampling to the lowest sample size would lead to a large loss of data, and in such cases, it was more economical to discard a year than lose a large amount of data. A practical example: if year 1 has 300 individuals in 6 traps and year 10 has 500 individuals in 4 traps, we subsampled 200 individuals (4/6 of 300) from year 1, so that each year has the number of individuals equivalent to 4 traps. We repeated this resampling 100 times to derive 100 hypothetical community compositions, and calculated the biodiversity metrics and population abundances for each iteration.
Exact data were provided per sampling event (number of individuals per species per trap/site per date), but the number of traps, dates or periods sampled is variable among years (owing to trap malfunctions or changes in sampling design or sampling frequency). Here, we first divided the years into monthly or bimonthly periods, and then decided whether we could maximize data use by including more months or more years (that is, whether to exclude periods that were not sampled each year, or exclude years that did not have samples in each period). The aim here was to maximize data use, so that we could attain the highest quality of data for a maximum number of years. For each period, we then subsampled the minimum number of samples observed in any year from the available samples, and added these together to form one yearly sample. This was repeated 100 times, and the biodiversity metrics were calculated each time. The mean values of these 100 iterations were used for model input. A practical example: ten pitfall traps per site were usually emptied weekly, but sometimes fortnightly, the trapping period changed from year-round at the start to May–September in later years, and sometimes samples were missing owing to flooding or trampling by livestock. Here, we first discarded all months other than May–September. Then we allocated five monthly periods and calculated the number of sampling days (sum of sampling period × number of traps) for each month. We then drew random samples (without replacement) from the available samples per month, until the total number of sampling days added up to the minimum number sampling days observed. We added the months up to form one yearly sample and calculated our metrics. This was repeated 100 times.

---

### 2018 ACC / HRS / NASCI / SCAI / SCCT expert consensus document on optimal use of ionizing radiation in cardiovascular imaging: best practices for safety and effectiveness [^c81f6684]. Catheterization and Cardiovascular Interventions (2018). Medium credibility.

Image noise and spatial resolution — x-ray CT and nuclear imaging depend on photon/count statistics: in CT, "the image noise in x-ray CT images is determined in part by the number of x-ray photons that reaches the scanner detectors", and "larger doses will yield images with less noise and, within limits, greater spatial resolution"; for CT tasks, "the spatial resolution required to assess myocardial contours… is less than that required to characterize coronary artery lesions". In nuclear scans, "the number of gamma ray counts… determines the image noise and, accordingly, its spatial resolution, which improves as the number of counts acquired increases", with counts set by "the amount of radioactivity administered… and the image acquisition time, with longer acquisition times acquiring a larger number of counts".

---

### February, 2024 SATISH IYENGAR department of statistics… [^e7cee0a6]. FDA (2025). Medium credibility.

00190–3. Iyengar S. On the exceedances of exchangeable random variables. Sankhya B, 83: 26–35, doi:

10. 1007/s13571-021-00252-3. Simsek B, Iyengar S. Computing tail areas for a high-dimensional Gaussian mixture. Applicable Analysis and Discrete Mathematics, 13, 871–882. Gaunt R, Iyengar S, Olde Dalhuis A, Simsek B. An asymptotic expansion for the normalizing constant of the Conway-Maxwell-Poisson distribution. Annals of the Institute of Statistical Mathematics, 71: 163–180. Simsek B, Iyengar S. On certain mathematical problems in two-photon laser scanning microscopy. Mathematical Modeling and Analysis, 22, 587–600. Saunders D, Tsui LK, Iyengar S. Lower tail dependence of hitting times of two-. teractions. J Computational Neuroscience, 32, 479–497. Perez-Rosello T, Baker JL, Ferrante M, Iyengar S, Ascoli GA, Barrionuevo G. Passive and active shaping of unitary responses from associational/commissural and perforant path. Iyengar S.

Evaluation of normal probabilities of symmetric regions. SIAM J Scientific and Statistical Computing, 9, 418–424. Iyengar S, Greenhouse JB. Selection models and the file-drawer problem. Statistical Science, 3, 109–135. Iyengar S. On a lower bound for the multivariate normal Mills' ratio. Annals of Prob- ability, 14, 1399–1403. Iyengar S. Hitting lines with two-dimensional Brownian motion. SIAM J Applied Mathematics, 45, 983–989. Protocol for a machine learning algorithm predicting depressive disorders using the T1w/T2w ratio. MethodsX, 8, 101595, doi:

10. 1016/j. mex.
2021. 101595. Diler RS, Merranko JA, Hafeman DM, Goldstein TR, Goldstein BI, Hower H, Gill MK.

---

### More efficient, smaller multicancer screening trials [^1109ffee]. Journal of the National Cancer Institute (2025). Medium credibility.

We considered testing using a χ 2 test of proportions, which has comparable sample size requirements with a log-rank test if the event rate is small, as expected for cancer-specific mortality in a screening trial. With 1:1 random assignment, denominators for targeted analysis are identical in both arms (number randomly assigned). Random assignment also ensures identical expected denominators for intended-effect analysis in both arms (expected number of individuals who test positive). It follows that most information about efficacy is from the shared numerator (number of events in individuals who test positive). Therefore, it may be sufficient to retrospectively test control individuals with an event (targeted analysis). We identified that design differences in power for a trial withindividuals per arm are driven by the following considerations (see Supplementary Material for more general formulae).

First, the traditional approach contrasts the number of events r j in each arm j = 0, 1 (control, screening) through. Events may be decomposed into test positive r pj or negative r nj, and (with rare events) variance is approximately. Using the assumption of no effect in individuals who test negative, the targeted test statistichas the same expectation as the traditional statistic but with a smaller variance — that is. The variance is reduced by; the more events in individuals who test negative, the greater the reduction in sample size. The factor that determines the benefit of targeted vs traditional designs is the ratio; the required sample size is multiplied by the proportion of events that are in individuals who also test positive.

Second, recall that the intended-effect analysis is restricted to individuals who test positive. The test statistic is the same as in the targeted analysis, but we can condition on the number of individuals who test positive in each arm. An estimate of the conditional variance of is. Therefore, relative sample size requirements depend on the factorsand; approximately, the sample size is multiplied by the proportion of test-positive individuals who do not have an event. Intended-effect analysis will offer a marked reduction in sample size over the targeted approach only if events are common in individuals who test positive (at least in the control arm).

Scenario analysis

To evaluate the potential role of the new methods in trial design, we applied the sample size formulae (Supplementary Material) to scenarios previously reported, and new scenarios for multicancer early detection trials. The designs were evaluated based on sample size and trial cost.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^11d967f5]. Journal of the American College of Cardiology (2025). High credibility.

Cost/value methodology — analytic time horizon: Economic value statements should be informed by CEAs that adopt an analytic horizon long enough to fully capture health effects and resource use, which is usually a lifetime horizon in cardiovascular CEAs, and economic evaluations of cardiovascular interventions should generally consider the lifetime horizon as the default with any alternatives carefully justified. Some interventions have delayed benefits or costs, and others may have safety concerns many years after the initial intervention; because randomized trials evaluating a novel therapy rarely follow trial subjects for more than 5 years, longer horizons require extrapolation about long-term effectiveness and safety. Interpretation should therefore pay careful attention to sensitivity and scenario analyses, and short analytic horizons may be acceptable for some questions but should be carefully vetted for bias.

---

### Respiratory syncytial virus (RSV) in an Italian pediatric cohort: genomic analysis and circulation pattern in the season 2022–2023 [^eaf83c77]. Journal of Medical Virology (2025). Medium credibility.

3.5 Estimates of the Effective Reproductive Number for RSV‐A and RSV‐B Italian Data Set

To estimate the population dynamics of the epidemics in Italy, we analyzed the Italian subsets including 89 RSV‐A genomes and 136 RSV‐B genomes collected in 2021–2022 and 2022–2023, by a Bayesian birth‐death skyline model.

The curves representing the dynamic of the effective reproduction number (R e) are reported in the Figure 4.

Figure 4
Birth‐death skyline plot of RSV‐A (A) and RSV‐B (B), in relation to time (x ‐axis) and the effective reproduction rate (R e) (y ‐axis).

The effective reproduction number of RSV‐A reached a value of 1.39 (95% HPD: 0.8–2.1) in 2021, then decreased < 1 and grew again between 2022 and 2023 reaching the value of 1.2 (95% HPD: 0.9–1.6) (Figure 4A). On the contrary, RSV‐B showed a multiphasic trend with a value > 1 already in the years preceding the pandemic (mean R e estimate = 1.43; 95% HPD: 0.99–1.9), followed by a decrease during 2020‐2021 period and an increase starting in the second half of 2021 (mean R e estimate = 1.79; 95% HPD: 1.48–2.07). Then, a further decrease in the first half of 2022, and a renewed (and seemingly more pronounced) growth between the second half of 2022 and the beginning of 2023 (R e = 1.96; 95% HPD: 1.58–2.27) (Figure 4B).

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^68e2e74a]. Journal of the American College of Cardiology (2025). High credibility.

Cost-effectiveness plane (Figure 1) — when a new intervention produces less health and lower costs compared with the comparator, it may be considered cost-effective if the incremental cost-effectiveness ratio (ICER) is greater than the cost-effectiveness threshold; however, this situation does not arise often because patients, clinicians, and health systems rarely accept worse health outcomes even if it reduces spending, and the figure includes labels "Intervention is cost-saving ("dominant") or always acceptable" and "Intervention is inferior ("dominated") or never acceptable".

---

### Guideline for disinfection and sterilization in healthcare facilities, 2008 [^b1a4872e]. CDC (2008). Medium credibility.

Number and location of microorganisms — greater bioburden and challenging device geometry increase time and difficulty of disinfection. All other conditions remaining constant, the larger the number of microbes, the more time a germicide needs to destroy all of them; under identical conditions it took 30 minutes to kill 10 B. atrophaeus spores but 3 hours to kill 100,000 Bacillus atrophaeus spores. Reducing microorganisms through meticulous cleaning increases the margin of safety and shortens exposure time, and aggregated or clumped cells are more difficult to inactivate than monodispersed cells. The location of microbes matters: medical instruments with multiple pieces must be disassembled and devices with crevices, joints, and channels are harder to disinfect; only surfaces directly contacting germicide are disinfected, so there must be no air pockets and equipment must be completely immersed for the entire exposure period.

---

### Strategies to preventinfections in acute-care hospitals: 2022 update [^08892976]. Infection Control and Hospital Epidemiology (2023). High credibility.

CDI surveillance scope, program expectations, and rate calculation: Conducting CDI surveillance to determine CDI rates provides a measure to determine the burden of CDI at a healthcare facility, and these data are also utilized to assess the efficacy of interventions to prevent CDI; when reported back to HCP and hospital administrators, CDI rates can be applied as a tool to improve adherence to CDI preventive measures. Surveillance can be performed on specific wards or units and/or an entire healthcare facility level, and infection prevention and control programs should have a system in place for reviewing results of positive C. difficile tests in patients included in their CDI surveillance plan to ensure accurate and complete case ascertainment. The healthcare facility-onset CDI case rate can be expressed as the number of CDI case patients per 10,000 patient days, with calculation defined as (number of case patients ÷ the number of inpatient days per reporting period) × 10,000 = rate per 10,000 inpatient days. Outbreaks and hyperendemic rates can occur at the ward level, where an outbreak can be defined as an increase in CDI in time and/or space believed to be greater than that expected by chance alone for a given healthcare facility or ward, and a hyperendemic rate can be defined as a persistently elevated CDI rate compared to past rates or compared to other similar healthcare facilities and/or wards.

---

### Let the question determine the methods: descriptive epidemiology done right [^f11b421f]. British Journal of Cancer (2020). Medium credibility.

What does it mean to control for confounding, and when do we actually need to do it? To answer this, we need a well-defined research question, driven by the goal of the study. For descriptive goals, we explain that confounding adjustment is often not just unnecessary but can be harmful.

---

### COVID-19 in the 47 countries of the WHO African region: a modelling analysis of past trends and future patterns [^0f4f98dc]. The Lancet: Global Health (2022). High credibility.

Background

COVID-19 has affected the African region in many ways. We aimed to generate robust information on the transmission dynamics of COVID-19 in this region since the beginning of the pandemic and throughout 2022.

Methods

For each of the 47 countries of the WHO African region, we consolidated COVID-19 data from reported infections and deaths (from WHO statistics); published literature on socioecological, biophysical, and public health interventions; and immunity status and variants of concern, to build a dynamic and comprehensive picture of COVID-19 burden. The model is consolidated through a partially observed Markov decision process, with a Fourier series to produce observed patterns over time based on the SEIRD (denoting susceptible, exposed, infected, recovered, and dead) modelling framework. The model was set up to run weekly, by country, from the date the first infection was reported in each country until Dec 31, 2021. New variants were introduced into the model based on sequenced data reported by countries. The models were then extrapolated until the end of 2022 and included three scenarios based on possible new variants with varying transmissibility, severity, or immunogenicity.

Findings

Between Jan 1, 2020, and Dec 31, 2021, our model estimates the number of SARS-CoV-2 infections in the African region to be 505·6 million (95% CI 476·0–536·2), inferring that only 1·4% (one in 71) of SARS-CoV-2 infections in the region were reported. Deaths are estimated at 439 500 (95% CI 344 374–574 785), with 35·3% (one in three) of these reported as COVID-19-related deaths. Although the number of infections were similar between 2020 and 2021, 81% of the deaths were in 2021. 52·3% (95% CI 43·5–95·2) of the region's population is estimated to have some SARS-CoV-2 immunity, given vaccination coverage of 14·7% as of Dec 31, 2021. By the end of 2022, we estimate that infections will remain high, at around 166·2 million (95% CI 157·5–174·9) infections, but deaths will substantially reduce to 22563 (14970–38831).

Interpretation

The African region is estimated to have had a similar number of COVID-19 infections to that of the rest of the world, but with fewer deaths. Our model suggests that the current approach to SARS-CoV-2 testing is missing most infections. These results are consistent with findings from representative seroprevalence studies. There is, therefore, a need for surveillance of hospitalisations, comorbidities, and the emergence of new variants of concern, and scale-up of representative seroprevalence studies, as core response strategies.

Funding

None.

---

### It's that time again [^2468ea4d]. Nature Neuroscience (2010). Medium credibility.

How do we estimate the duration of a temporal interval in a familiar context? A new study finds that it is appropriate, perhaps even advantageous, to tolerate a small bias in our estimate to reduce the overall temporal uncertainty.

---

### The cost of large numbers of hypothesis tests on power, effect size and sample size [^4b1c0c24]. Molecular Psychiatry (2012). Low credibility.

Results

For a fixed targeted effect size and fixed sample size, power decreases as the number of tests and corresponding critical value increase (Table 1, Figure 2a). If the power for a single test is 80%, the power is approximately 50% for 10; 10% for 1000; and 1% for 100 000 Bonferroni-adjusted tests. To avoid a drop in nominal power without increasing sample size, an investigator may target larger effect sizes (Table 1, Figure 2b) using equation (1). For one million tests, the effect size multiplier is 2.25 at 80% power and 2.08 at 90% power. Suppose it has been determined that a sample of 100 yields 80% power to detect a difference in group means when the mean in group 1 is 10 and the mean in group 2 is 12. The original sample size of 100 would also yield 80% power to detect a mean difference of 2.25 × 2 = 4.50, with Bonferroni adjustment for one million tests. The effect size multiplier works on the scale of the underlying Normally distributed test statistic. For example, effect size multiplication for an OR should be carried out after conversion to the natural log scale. Suppose 500 total cases and controls provide 90% power to detect an OR = 1.2, the effect size multiplier for 90% power and 1000 tests is 1.65. Since log e (1.2) = 0.18, the same sample of 500 yields 90% power in 1000 tests to detect a log e (OR) = 1.65 × 0.18 = 0.30 or, equivalently, an OR = exp(0.30) = 1.35.

---

### Cryptic transmission of SARS-CoV-2 and the first COVID-19 wave [^5e1cd115]. Nature (2021). Excellent credibility.

Onset of local transmission

The model's ensemble of realizations provides a statistical description of all the potential pandemic histories compatible with the initial evolution of the pandemic in China. Rather than describing a specific, causal chain of events, we can estimate possible time windows pertaining to the initial chains of transmission in different geographical regions. We define the onset of local transmission for a country or state as the earliest date when at least 10 new infections are generated per day. This number is chosen because at this threshold the likelihood of stochastic extinction is extremely small. As detailed in the Supplementary Information, further calibration on the US states and European countries suggests posterior values of R 0 ranging from 2.4 to 2.8. These values are consistent with many other (country-dependent) estimates –. At the same time, given the doubling time of the number of COVID-19 cases before the implementation of public health measures, any variation of a factor 2 around the 10 infections per day threshold corresponds to a small adjustment of 3−5 days to the presented timelines.

In Fig. 2, we show the posterior probability distribution, p (t), of the week, t, of the onset of local transmission for 15 US states (Fig. 2a) and European countries (Fig. 2b) (see Supplementary Information for all states and countries). We also calculate, for each country or state, the median date, T, that identifies the first week in which the cumulative distribution function is larger than 50%. Among the US states, CA and NY have the earliest dates, T, by the week of 19 January (CA) and 2 February (NY) 2020. In Europe, Italy, the UK, Germany and France are the first countries with T close to the end of January 2020. However, it is worth noting that each distribution, p (t), has a support spanning several weeks. In Italy, the 5th and 95th percentiles of the p (t) distribution are the week of 6 January and the week of 30 January 2020, respectively. These dates also suggest that it is not possible to rule out introductions and transmission events as early as December 2019, although the probability of this is very small.

---

### Number needed to treat (NNT) in clinical literature: an appraisal [^bb555026]. BMC Medicine (2017). Low credibility.

Background

The number needed to treat (NNT) is an absolute effect measure that has been used to assess beneficial and harmful effects of medical interventions. Several methods can be used to calculate NNTs, and they should be applied depending on the different study characteristics, such as the design and type of variable used to measure outcomes. Whether or not the most recommended methods have been applied to calculate NNTs in studies published in the medical literature is yet to be determined. The aim of this study is to assess whether the methods used to calculate NNTs in studies published in medical journals are in line with basic methodological recommendations.

Methods

The top 25 high-impact factor journals in the "General and/or Internal Medicine" category were screened to identify studies assessing pharmacological interventions and reporting NNTs. Studies were categorized according to their design and the type of variables. NNTs were assessed for completeness (baseline risk, time horizon, and confidence intervals [CIs]). The methods used for calculating NNTs in selected studies were compared to basic methodological recommendations published in the literature. Data were analyzed using descriptive statistics.

Results

The search returned 138 citations, of which 51 were selected. Most were meta-analyses (n = 23, 45.1%), followed by clinical trials (n = 17, 33.3%), cohort (n = 9, 17.6%), and case-control studies (n = 2, 3.9%). Binary variables were more common (n = 41, 80.4%) than time-to-event (n = 10, 19.6%) outcomes. Twenty-six studies (51.0%) reported only NNT to benefit (NNTB), 14 (27.5%) reported both NNTB and NNT to harm (NNTH), and 11 (21.6%) reported only NNTH. Baseline risk (n = 37, 72.5%), time horizon (n = 38, 74.5%), and CI (n = 32, 62.7%) for NNTs were not always reported. Basic methodological recommendations to calculate NNTs were not followed in 15 studies (29.4%). The proportion of studies applying non-recommended methods was particularly high for meta-analyses (n = 13, 56.5%).

Conclusions

A considerable proportion of studies, particularly meta-analyses, applied methods that are not in line with basic methodological recommendations. Despite their usefulness in assisting clinical decisions, NNTs are uninterpretable if incompletely reported, and they may be misleading if calculating methods are inadequate to study designs and variables under evaluation. Further research is needed to confirm the present findings.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^6517cbfd]. Annals of the American Thoracic Society (2023). High credibility.

Basic stereological measurements — first-order parameters and probes identify that "First-order parameters are volume (three-dimensional [3D]), surface (two-dimensional [2D]), length (one-dimensional [1D]), and number (zero-dimensional [0D])". Measurements "are performed using geometric test probes, such as points (0D), lines (1D), planes (2D), or volumes (3D)". These probes create countable events in the image, and "Raw counts provide ratios… that are multiplied by the reference space volume to obtain absolute measures for the lung or subcompartment".

---

### Cutting edge or blunt instrument: how to decide if a stepped wedge design is right for you [^471f52f2]. BMJ Quality & Safety (2021). High credibility.

When might I consider doing a stepped wedge trial?

Research designs are shaped as much by practical constraints as by abstract schemes, and it is always a good idea to start with the constraints and work towards a design, rather than start with a design and try to fit it to constraints. These constraints will be unique to each research context, and box 1 lists some areas to think about. Still, there are some common features of settings where a stepped wedge trial might be considered as a possible design, and we now review these.

Box 1
Practical constraints on the design of a longitudinal cluster randomised trial

Are there limits on the time available to complete the evaluation, on the number of clusters, or on the number of participants (or the rate at which you can recruit participants) at each cluster? These constraints put limits on the overall scale of the evaluation, or force trade-offs between different design characteristics.
How will participants and their data be sampled in your study: as a series of cross-sectional surveys, as a continuous stream of incident cases, as a cohort followed over time, or some other way? Does the timescale divide into cycles, seasons or milestones that influence how you will sample participants and data?
Is there a limit on how many clusters can implement the intervention at the same time in the evaluation? If this is constrained by research resources (eg, if there are only enough trained research staff to implement the intervention one cluster at a time) then implementation must be staggered in some way.
If implementation is to be staggered, is there a minimum 'step length'? If the same team delivers the intervention in different clusters at different steps, then bear in mind it may take some time to get the intervention fully operational at a site, and the team will also need time to relocate from one cluster to the next.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^dee6cacc]. CDC (2011). Medium credibility.

Community viral load sample size calculations — sample size to detect differences in geometric mean (GM) viral load depends on power and standard deviation (S). The table specifies "α = 0.05 and W = 0.8", with S columns 1 to 1.5; for k = 3 the minimum sample sizes by S are 54, 66, 78, 92, 106, and 122, and "1.2 is the standard deviation of national VL data", so at S = 1.2 the needed sample size is 78.

---

### Douglas s. diekema, M.D. M.p.h.-curriculum vitae… [^e2c947eb]. FDA (2025). Medium credibility.

Exercise/Discussion/Lecture/Module Development — State Use of Police Powers: Coercive Control of Pregnant Women 2018 BH 201A, Student Exercise/Discussion/Lecture/Module Development — Topics in Public Health: Isolation and Quarantine. 2019 BH 201A, Student Exercise/Discussion/Lecture/Module Development — Topics in Public Health: Ethical Issues in Vaccination 2022 BH 536: Lecture and Exercise on Children in Research 2023. Scientific and Medical Research Funding Working Group Targeted Clinical Development Awards Review Meeting Douglas Diekema
9. Grant Award: $337, 280 Parental permission and adolescent assent and decision making in clinical research PI: Christine Grady RN, PhD, Department of Bioethics, NIH Funded at. 04 FTE as co-investigator Funded period 7/1/09–12/31/10.
9. Diekema DS. Is Taller Really Better.: Growth Hormone Therapy in Short Children.

Perspectives in Biology and Medicine 1990; 34: 109–123. PMID: 2274399
10. Diekema DS. Accomplices. JAMA 1991; 265:

802. wilderness recreational deaths in western Washington state. Ann Emerg Med 1998; 32: 687–692. PMID: 9832665
22. Klein EJ, M Koenig, DS Diekema, W Winters. Discordant radiograph interpretation between emergency physicians and radiologists in a pediatric. Care Med 2000; 28: 1590–1594. PMID: 10834717
24. Diekema DS. Withdrawing and withholding life-sustaining treatment in children. West J Med 2000; 173: 411–412. PMID:
11112761. Academic Medicine 2004; 79: 265–271. PMID: 14985202
32. Diekema DS. Parental Refusals of Medical Treatment: The Harm Principle as Threshold for State Intervention. Theoretical Medicine and Bioethics 2004; 25: 243–264. PMID: 15637945.

---

### Temporal context calibrates interval timing [^27df0941]. Nature Neuroscience (2010). Medium credibility.

We use our sense of time to identify temporal relationships between events and to anticipate actions. The degree to which we can exploit temporal contingencies depends on the variability of our measurements of time. We asked humans to reproduce time intervals drawn from different underlying distributions. As expected, production times were more variable for longer intervals. However, production times exhibited a systematic regression toward the mean. Consequently, estimates for a sample interval differed depending on the distribution from which it was drawn. A performance-optimizing Bayesian model that takes the underlying distribution of samples into account provided an accurate description of subjects' performance, variability and bias. This finding suggests that the CNS incorporates knowledge about temporal uncertainty to adapt internal timing mechanisms to the temporal statistics of the environment.

---

### Voluntary risk mitigation behaviour can reduce impact of SARS-CoV-2: a real-time modelling study of the January 2022 omicron wave in england [^5b3bd4a7]. BMC Medicine (2023). Medium credibility.

Background

Predicting the likely size of future SARS-CoV-2 waves is necessary for public health planning. In England, voluntary "plan B" mitigation measures were introduced in December 2021 including increased home working and face coverings in shops but stopped short of restrictions on social contacts. The impact of voluntary risk mitigation behaviours on future SARS-CoV-2 burden is unknown.

Methods

We developed a rapid online survey of risk mitigation behaviours ahead of the winter 2021 festive period and deployed in two longitudinal cohort studies in the UK (Avon Longitudinal Study of Parents and Children (ALSPAC) and TwinsUK/COVID Symptom Study (CSS) Biobank) in December 2021. Using an individual-based, probabilistic model of COVID-19 transmission between social contacts with SARS-CoV-2 Omicron variant parameters and realistic vaccine coverage in England, we predicted the potential impact of the SARS-CoV-2 Omicron wave in England in terms of the effective reproduction number and cumulative infections, hospital admissions and deaths. Using survey results, we estimated in real-time the impact of voluntary risk mitigation behaviours on the Omicron wave in England, if implemented for the entire epidemic wave.

Results

Over 95% of survey respondents (N ALSPAC = 2686 and N Twins = 6155) reported some risk mitigation behaviours, with vaccination and using home testing kits reported most frequently. Less than half of those respondents reported that their behaviour was due to "plan B". We estimate that without risk mitigation behaviours, the Omicron variant is consistent with an effective reproduction number between 2.5 and 3.5. Due to the reduced vaccine effectiveness against infection with the Omicron variant, our modelled estimates suggest that between 55% and 60% of the English population could be infected during the current wave, translating into between 12,000 and 46,000 cumulative deaths, depending on assumptions about severity and vaccine effectiveness. The actual number of deaths was 15,208 (26 November 2021–1 March 2022). We estimate that voluntary risk reduction measures could reduce the effective reproduction number to between 1.8 and 2.2 and reduce the cumulative number of deaths by up to 24%.

Conclusions

Predicting future infection burden is affected by uncertainty in disease severity and vaccine effectiveness estimates. In addition to biological uncertainty, we show that voluntary measures substantially reduce the projected impact of the SARS-CoV-2 Omicron variant but that voluntary measures alone would be unlikely to completely control transmission.

---

### A comparison of phase II study strategies [^ebb760ae]. Clinical Cancer Research (2009). Low credibility.

The traditional oncology drug development paradigm of single arm phase II studies followed by a randomized phase III study has limitations for modern oncology drug development. Interpretation of single arm phase II study results is difficult when a new drug is used in combination with other agents or when progression-free survival is used as the endpoint rather than tumor shrinkage. Randomized phase II studies are more informative for these objectives but increase both the number of patients and time required to determine the value of a new experimental agent. In this article, we compare different phase II study strategies to determine the most efficient drug development path in terms of number of patients and length of time to conclusion of drug efficacy on overall survival.

---

### Importance of fractional b value for calculating apparent diffusion coefficient in DWI [^3dc34fd5]. AJR: American Journal of Roentgenology (2016). Low credibility.

Objective

The purpose of this study is to examine how the fractional b value affects the calculation of apparent diffusion coefficient (ADC) using DWI. The fractional b value is the point of intersection between the fast and slow components of biexponential decay in DWI.

Subjects and Methods

Human brains were imaged using multiple b values on echo-planar DWI. The ADCs of white matter, gray matter, and thalamus were calculated using the combination of b values by two-point and multipoint methods, and the characteristics of each ADC value were compared.

Results

When the two selected points for calculation were smaller than the fractional b value (b = 1700 s/mm 2), the ADC value was 0.0007–0.0008 mm 2 /s, but when the two points used for calculation were greater than the fractional b value, the ADC value was 0.0003–0.0004 mm 2 /s. When a range of b values was included in the fast and slow components by use of the multipoint method, the ADC value showed a statistically significant increase as the number of multiple b values increased.

Conclusion

The ADC value fluctuated when the b values used for calculation were higher than the fractional b value. Therefore, it is important to determine the fractional b value of the target tissue.

---

### Association of generic competition with price decreases in physician-administered drugs and estimated price decreases for biosimilar competition [^bdcf35db]. JAMA Network Open (2021). High credibility.

Importance

Price decreases of biologic and biosimilar products in Medicare Part B have been minimal, even with biosimilar competition. Medicare reimburses clinicians for biologics and biosimilars differently than for brand-name and generic drugs, which has generated greater price reductions.

Objective

To characterize the nature of price competition among brand-name and generic drugs under Medicare Part B and to estimate the cost savings to the program of subjecting biologic and biosimilar therapies to a similar price competition.

Design, Setting, and Participants

This cohort study analyzed all brand-name drugs and their approved generic versions as well as biologics and biosimilars that were reimbursed under Medicare Part B from quarter 1 of 2005 to quarter 2 of 2021. Two separate data sets were created: brand-name and generic drugs as well as biologics and biosimilars data sets. Brand-name products with generic versions that were introduced before 2005 were excluded, and so were vaccines.

Exposures

Number of generic and biosimilar competitors over time.

Main Outcomes and Measures

Price change as a percentage of the brand-name drug or biologic price in the quarter before generic or biosimilar competition. Price change was modeled using a linear, fixed-effects time series regression, with the number of generic or biosimilar competitors as the main covariate. Time was expressed as the number of quarters since the first generic or biosimilar competitor entered the market. Savings were estimated by projecting the regression model of brand-name and generic drug competition to observed biologic and biosimilar competition and by applying the estimated price reduction to actual Medicare spending for those products from 2015 to 2019.

Results

Of the 988 Healthcare Common Procedure Coding System codes identified, 50 (5.0%) met the inclusion criteria for the brand-name and generic drug data set and 28 (2.8%) met the criteria for the biologic and biosimilar data set. The first generic competitor was associated with reduced drug prices by 17.0%, the second competitor with a 39.5% decrease, the third competitor with a 52.5% decrease, and the fourth and more competitors with a 70.2% decrease (price decline was measured from brand-name drug price before the first generic competitor rather than from price established with fewer competitors). If biologics and biosimilars were subject to the same Medicare reimbursement framework as brand-name and generic drugs, Medicare spending on these products was estimated to have been 26.6% lower ($1.6 billion) from 2015 to 2019.

Conclusions and Relevance

This study found minimal uptake of biosimilars and limited price reductions for biologics and biosimilars under the current Medicare Part B reimbursement policy. Adopting the bundled biosimilar reimbursement structure for biologic and biosimilar therapies may be associated with substantial savings and encourage greater biosimilar market entry.

---

### Dynamic clade transitions and the influence of vaccination on the spatiotemporal circulation of SARS-CoV-2 variants [^dce8c574]. NPJ Vaccines (2024). Medium credibility.

Fig. 3
Prevalence and spread of SARS-CoV-2 infections in the 15th Regional Health District and Brazil.

A Incidence of COVID-19 per 100,000 inhabitants in the RHD XV region on the left y-axis together with estimated effective reproductive number (Reff) for all COVID-19 cases in the region, represented as a blue line on the right y-axis; (B) COVID-19 incidence per 100,000 inhabitants for all cases across Brazil and estimated effective reproductive number (Reff) over time (blue line) for all reported COVID-19 cases (right y-axis), including the percentage of vaccine coverage in the entire population during the same timeframe (left y-axis).

To further analyze the role of vaccination in Brazil relative to VOC dominance, we looked at the vaccine effectiveness for every 10% increase in overall vaccination uptake in terms of cases and death numbers, adjusting for the number of tests administered (Fig. 4A). We found that vaccine effectiveness in preventing cases during the period dominated by Delta and Omicron (1 August – 1 November 2021 and 1 December 2021−30 April 2022, respectively) were six and 11 times higher than when Gamma was the dominant variant (1 February−31 July 2021). Similarly, vaccine effectiveness in preventing deaths during Delta's dominance was higher than Gamma's. However, vaccination showed no statistical effect in preventing deaths during Omicron's dominance, which could be anticipated if the case fatality rate varied over time independent of vaccination. This variation is illustrated in Fig. 4B, where the deaths-to-case ratio decreased from 2.96% (95% CI: 2.77, 3.14) before December 2021 to 0.36% (95% CI: 0.31, 0.41) from December 2021 onwards.

Fig. 4
Vaccine effectiveness on the number of COVID-19 cases and deaths in Brazil.

A Effectiveness of accination per 10% increase in vaccination uptake in preventing new COVID-19 cases and deaths in Brazil. B Number of new deaths per new cases observed in 2021 and 2022 in Brazil. VE: Vaccine effectiveness.

---

### Triazolam [^59f91090]. FDA (2025). Medium credibility.

WARNING: RISKS FROM CONCOMITANT USE WITH OPIOIDS; ABUSE, MISUSE, AND ADDICTION; and DEPENDENCE AND WITHDRAWAL REACTIONS

Concomitant use of benzodiazepines and opioids may result in profound sedation, respiratory depression, coma, and death. Reserve concomitant prescribing of these drugs in patients for whom alternative treatment options are inadequate. Limit dosages and durations to the minimum required. Follow patients for signs and symptoms of respiratory depression and sedation [see Warnings and Precautions (5.1), Drug Interactions (7.1)].
The use of benzodiazepines, including triazolam, exposes users to risks of abuse, misuse, and addiction, which can lead to overdose or death. Abuse and misuse of benzodiazepines commonly involve concomitant use of other medications, alcohol, and/or illicit substances, which is associated with an increased frequency of serious adverse outcomes. Before prescribing triazolam and throughout treatment, assess each patient's risk for abuse, misuse, and addiction [see Warnings and Precautions (5.2)].
The continued use of benzodiazepines, including triazolam, may lead to clinically significant physical dependence. The risks of dependence and withdrawal increase with longer treatment duration and higher daily dose. Abrupt discontinuation or rapid dosage reduction of triazolam after continued use may precipitate acute withdrawal reactions, which can be life-threatening. To reduce the risk of withdrawal reactions, use a gradual taper to discontinue triazolam or reduce the dosage [see Dosage and Administration (2.3), Warnings and Precautions (5.3)].

WARNING: RISKS FROM CONCOMITANT USE WITH OPIOIDS; ABUSE, MISUSE, AND ADDICTION; and DEPENDENCE AND WITHDRAWAL REACTIONS

See full prescribing information for complete boxed warning.

Concomitant use of benzodiazepines and opioids may result in profound sedation, respiratory depression, coma, and death. Reserve concomitant prescribing of these drugs in patients for whom alternative treatment options are inadequate. Limit dosages and durations to the minimum required. Follow patients for signs and symptoms of respiratory depression and sedation (5.1, 7.1).
The use of benzodiazepines, including triazolam, exposes users to risks of abuse, misuse, and addiction, which can lead to overdose or death. Before prescribing triazolam and throughout treatment, assess each patient's risk for abuse, misuse, and addiction (5.2).
Abrupt discontinuation or rapid dosage reduction of triazolam after continued use may precipitate acute withdrawal reactions, which can be life-threatening. To reduce the risk of withdrawal reactions, use a gradual taper to discontinue triazolam or reduce the dosage (2.3, 5.3).

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^9d0e2219]. Journal of the American College of Cardiology (2025). High credibility.

Supporting text requirements for economic value statements specify that supporting text should include key elements: a summary of the high-quality CEAs used to generate the statement with rationale for which studies were included and details such as study population, intervention, comparator, clinical outcomes, sources of key effectiveness and safety parameters, perspective, analytic horizon, base-case treatment cost, and source of funding; point estimates and the proportion of simulations that were cost-effective at a specified threshold; and results of key sensitivity analyses, noting that a 1-way sensitivity analysis of intervention cost variation can be helpful, that additional information about sensitivity to intervention cost should be provided, and that inclusion of a figure similar to Figure 4 is encouraged if available.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^63fa1003]. Journal of the American College of Cardiology (2025). High credibility.

Cost-effectiveness thresholds for cardiovascular cost-effectiveness analyses (CEAs) — The 2014 ACC/AHA cost-value methodology statement provided thresholds to identify high-, intermediate-, and low-value care based on U.S. per capita gross domestic product, and the writing committee determined that an intervention with an ICER < $50 000 per QALY gained should be considered high value, $50 000 to < $150 000 (1–3× per capita GDP) per QALY gained should be considered intermediate value, and ≥ $150 000 per QALY gained should be considered low value. WHO-origin GDP benchmarks suggested an intervention that costs less than 3 times the national annual GDP per capita per disability-adjusted life year averted should be considered cost-effective and that costs less than 1 times should be considered highly cost-effective, but critics note they "set such a low bar for cost effectiveness that very few interventions with evidence of efficacy can be ruled out"; WHO experts now also advise against per capita GDP-based thresholds and suggest considering cost-effectiveness alongside other contextual information in a transparent multicomponent decision process. Reflecting these concerns, the writing committee reviewed alternative approaches to incorporating CEA results into guidelines, including relying on US-based studies for identifying a cost-effectiveness threshold or thresholds for cardiovascular CEAs, and Vanness et al used a health opportunity cost approach estimating that for every $1 000 000 increase in health care expenditures, 1869 persons become uninsured, resulting in 81 QALYs lost due to mortality and 15 QALYs lost due to morbidity, implying a cost-effectiveness threshold of $104 000 per QALY ($51 000 to $209 000 per QALY) in 2019 US dollars; in the probabilistic analysis, 40% of the simulations suggested the threshold was less than $100 000 per QALY.

---

### A general derivation and quantification of the third law of thermodynamics [^43738480]. Nature Communications (2017). Medium credibility.

The most accepted version of the third law of thermodynamics, the unattainability principle, states that any process cannot reach absolute zero temperature in a finite number of steps and within a finite time. Here, we provide a derivation of the principle that applies to arbitrary cooling processes, even those exploiting the laws of quantum mechanics or involving an infinite-dimensional reservoir. We quantify the resources needed to cool a system to any temperature, and translate these resources into the minimal time or number of steps, by considering the notion of a thermal machine that obeys similar restrictions to universal computers. We generally find that the obtainable temperature can scale as an inverse power of the cooling time. Our results also clarify the connection between two versions of the third law (the unattainability principle and the heat theorem), and place ultimate bounds on the speed at which information can be erased.

---

### Recommendations for prevention and control of influenza in children, 2022–2023 [^d86965b7]. Pediatrics (2022). High credibility.

Influenza-associated pediatric deaths by season — Figure 2 reports number of deaths as 2018–2019 Number of Deaths = 144, 2019–2020 Number of Deaths = 199, 2020–2021 Number of Deaths = 1, and 2021–2022 Number of Deaths = 33.

---

### Quantum mechanics can reduce the complexity of classical models [^1a2c01aa]. Nature Communications (2012). Medium credibility.

Mathematical models are an essential component of quantitative science. They generate predictions about the future, based on information available in the present. In the spirit of simpler is better; should two models make identical predictions, the one that requires less input is preferred. Yet, for almost all stochastic processes, even the provably optimal classical models waste information. The amount of input information they demand exceeds the amount of predictive information they output. Here we show how to systematically construct quantum models that break this classical bound, and that the system of minimal entropy that simulates such processes must necessarily feature quantum dynamics. This indicates that many observed phenomena could be significantly simpler than classically possible should quantum effects be involved.

---

### Guideline no. 428: management of dichorionic twin pregnancies [^a68f4727]. Journal of Obstetrics and Gynaecology Canada (2022). High credibility.

Regarding screening and diagnosis for multiple gestation, more specifically with respect to determination of gestational age, SOGC 2022 guidelines recommend to use the measurements of the larger twin to determine the gestational age of a twin pregnancy.

---

### SCAI publications committee manual of standard operating procedures: 2022 update [^194c72cc]. Journal of the Society for Cardiovascular Angiography & Interventions (2022). High credibility.

Remarks accompanying recommendations may include additional considerations such as "Justification or explanation for judgments", "Subgroup considerations", and "Implementation considerations (including strategies to address any concerns about acceptability or feasibility)".

---

### Preparing for a meta-analysis of rates: extracting effect sizes and standard errors from studies of count outcomes with person-time denominators [^413712ae]. Injury Prevention (2025). Medium credibility.

Background

Formulas for the extraction of continuous and binary effect sizes that are entered into a meta-analysis are readily available. Only some formulas for the extraction of count outcomes have been presented previously. The purpose of this methodological article is to present formulas for extracting effect sizes and their standard errors for studies of count outcomes with person-time denominators.

Methods

Formulas for the calculation of the number of events in a study and the corresponding person time in which these events occurred are presented. These formulas are then used to estimate the relevant effect sizes and standard errors of interest. These effect sizes are rates, rate ratios and rate differences for a two-group comparison and rate ratios and rate differences for a difference-in-difference design.

Results

Two studies from the field of suicide prevention are used to demonstrate the extraction of the information required to estimate effect sizes and standard errors. In the first example, the rate ratio for a two-group comparison was 0.957 (standard error of the log rate ratio, 0.035), and the rate difference was -0.56 per 100,000 person years (standard error 0.44). In the second example, the rate ratio for a difference-in-difference analysis was 0.975 (standard error of the log rate ratio 0.036) and the rate difference was -0.30 per 100,000 person years (standard error 0.42).

Conclusions

The application of these formulas enables the calculation of effect sizes that may not have been presented in the original study. This reduces the need to exclude otherwise eligible studies from a meta-analysis, potentially reducing one source of bias.

---

### Respiratory syncytial virus (RSV) in an Italian pediatric cohort: genomic analysis and circulation pattern in the season 2022–2023 [^e39b3da3]. Journal of Medical Virology (2025). Medium credibility.

Phylogenetic analysis showed that the Italian sequences were mainly distributed into mixed clusters, including both Italian and non‐Italian genomes, a half of which were present in both seasons, confirming that, similarly to what was already described for the 2021–2022 season, even the 2022–2023 RSV season was driven by multiple introductions of international clusters having tMRCA estimates dating back from 2015 to 2021 that probably continued to circulate throughout the Europe during the pandemic restriction period. Moreover, several of the Italian strains tended to group together, forming pure Italian clades within the larger international clusters, probably representing local chains of transmission. Interestingly, a few of them (three for RSV‐A and two for RSV‐B) included isolates sampled in both seasons, suggesting the existence of local strains persisting for more than one season.

To reconstruct the phylodynamic of the RSV‐A and RSV‐B in Italy we used two complementary Bayesian approaches: the coalescent and the birth‐death skyline. The first approach allowed to estimate the changes in effective number of infections and the second the changes in the effective reproductive number (Re). Interestingly, for both RSV‐A and RSV‐B the skyline plot showed a first peak in transmissions between 2017 and 2019 followed by a temporary reduction (a bottleneck) in 2021 coinciding with the widespread use of nonpharmaceutical control measures against COVID‐19.

In agreement with these results, the Re estimates showed fluctuating values in relation to the period intra or interepidemic. Particularly, for RSV‐A the Re value rose above 1 in the second half of 2021 (1.4, 0.8–2.1), dropped below 1 in 2022, and then climbed again (1.2, 0.9–1.6) at the end the same year and the beginning of 2023.

---

### Value of a scheduled duration quantified in terms of equivalent numbers of historical cases [^eac7e1f9]. Anesthesia and Analgesia (2013). Low credibility.

Background

Probabilistic estimates of case duration are important for several decisions on and soon before the day of surgery, including filling or preventing a hole in the operating room schedule, and comparing the durations of cases between operating rooms with and without use of specialized equipment to prevent resource conflicts. Bayesian methods use a weighted combination of the surgeon's estimated operating room time and historical data as a prediction for the median duration of the next case of the same combination. Process variability around that prediction (i.e., the coefficient of variation) is estimated using data from similar procedures. A Bayesian method relies on a parameter, τ, that specifies the equivalence between the scheduled estimate and the information contained in the median of a certain number of historical data.

Methods

Times from operating room entrance to exit ("case duration") were obtained for multiple procedures and surgeons at 3 U.S. academic hospitals. A new method for estimating the parameter τ was developed.

Results

(1) The method is reliable and has content, convergent, concurrent, and construct validity. (2) The magnitudes of the Somer's D correlations between scheduled and actual durations are small when stratified by procedure (0.05–0.14), but substantial when pooled among all cases and procedures (0.58–0.78). This pattern of correlations matches that when medians (or means) of historical durations are used. Thus, scheduled durations and historical data are essentially interchangeable for estimating the median duration of a future case. (3) Most cases (79%-88%) either have so few historical durations (0–2) that the Bayesian estimate is influenced principally by the scheduled duration, or so many historical durations (> 10) that the Bayesian estimate is influenced principally by the historical durations. Thus, the balance between the scheduled duration versus historical data has little influence on results for most cases. (4) Mean absolute predictive errors are insensitive to a wide range of values (e.g., 1–10) for the parameter. The implication is that τ does not routinely need to be calculated for a given hospital, but can be set to any reasonable value (e.g., 5).

Conclusions

Understanding performance of Bayesian methods for case duration is important because variability in durations has a large influence on appropriate management decisions the working day before and on the day of surgery. Both scheduled durations and historical data need to be used for these decisions. What matters is not the choice of τ but quantifying the variability using the Bayesian method and using it in managerial decisions.

---

### No evidence for decision fatigue using large-scale field data from healthcare [^557df0bf]. Communications Psychology (2025). Medium credibility.

a) Overlap AM-PM shift. Here we want as much separation as possible in time spent at work (long for the AM shift, short for the PM shift) but also as large a sample as possible. For example, choosing only those nurses who start very early would mean increased separation (longer time until the overlap period) but at the cost of getting a smaller sample, because those who start later but are still in the AM shift would be excluded. We used the following main rules: Consider only decisions taken between 13:00–17:00 on Mon–Fri for workers who either take their first call 6:00–9:00 and their last call after 13:00 (AM worker), or take their first call 13:00–18:00 and their last call after 19:00 (PM worker). Then also exclude workers who take < 12 calls because they are likely working on other tasks (e.g. admin) that we do not have data on. Further, we divide the relevant time window (13:00–17:00) in 30 min time bins (thus eight in total) and keep only those bins with > 100 observations for each category of workers (AM and PM shifts, respectively). Applied to the pilot data these rules resulted in a final sample size of n = 9730 calls to k = 166 nurses; applied to the target data (stage 2), using these rules resulted in a final sample size of n = 9648 calls to k = 143 nurses.

b) Intense call sequences before vs. after breaks. Here we want sufficiently long and intense sequences, meaning many calls with little time in-between for rest, but also as large a sample as possible. We used the following main rules: Consider only sequences where a nurse handled at least five calls before and five calls after a break 8–40 min, and there are at most five min between each call. The exact values used here are arbitrary (unless changed substantially), e.g. using 7 or 9 min instead of 8 min as lower cutoff for a break should not make a difference; the important part for us was to decide on and specify these values exactly and in advance, in order to credibly restrict analytic flexibility later on when analyzing the new data. These rules resulted in a final sample size of n = 5120 calls to k = 116 nurses in the pilot data; and n = 4940 calls to k = 77 nurses in the target (stage 2) data.

---

### Dissecting the mechanisms underlying the accumulation of mitochondrial DNA deletions in human skeletal muscle [^2a8de594]. Human Molecular Genetics (2014). Low credibility.

Figure 1.
Proposed mechanism for accumulation of smaller mtDNA molecules longitudinally through muscle fibres. Illustration of increased longitudinal accumulation of larger mtDNA deletions through muscle fibres, as predicted by the 'survival of the smallest' hypothesis. (A) In the presence of no mtDNA deletions, wild-type mtDNA copy number is maintained throughout a muscle fibre over time. (B) Following mtDNA deletion formation a replicative advantage over wild-type mtDNA, due to the smaller size of the deleted molecule, causes an increase in the mtDNA deletion level and a spread of the deleted species through the muscle fibre. (C) The larger the size of the mtDNA deletion, the smaller the remaining genome and the greater the replicative advantage over wild-type mtDNA. A larger mtDNA deletion size therefore leads to a faster accumulation of the deleted species over the same time span and a greater spread through a muscle fibre.

---

### Recommendations for prevention and control of influenza in children, 2021–2022 [^5fd5e384]. Pediatrics (2021). High credibility.

Timing of vaccination and duration of protection — Complete influenza vaccination by the end of October is recommended by the CDC and AAP. Children who need 2 doses of the vaccine should receive their first dose as soon as possible when the vaccine becomes available, to allow sufficient time for receipt of the second dose ≥ 4 weeks after the first, before the onset of the influenza season, and children who require only 1 dose should also ideally be vaccinated by the end of October. Recent data in adults suggest that the CDC now discourages vaccination in the summer months, particularly among older adults. A multiseason analysis from the US Flu VE Network found that VE declined by approximately 7% per month for influenza A (H3N2) and influenza B and by 6% to 11% per month for influenza A (H1N1)pdm09 in individuals ≥ 9 years and older; VE remained greater than 0 for at least 5 to 6 months after vaccination. In children older than 2 years, an odds ratio increased approximately 16% with each additional 28 days from vaccine administration, and another study demonstrated 54% to 67% protection from 0 to 180 days after vaccination.

---

### Guideline no. 440: management of monochorionic twin pregnancies [^bde8d331]. Journal of Obstetrics and Gynaecology Canada (2023). High credibility.

Regarding screening and diagnosis for multiple gestation, more specifically with respect to determination of gestational age, SOGC 2023 guidelines recommend to use the larger of the two crown-rump lengths to estimate gestational age in spontaneously conceived pregnancies.

---

### The legacy of "just the facts" [^fbe407dc]. Schizophrenia Research (2011). Low credibility.

This commentary to the capstone paper by Keshavan and colleagues reflects on the history of this project and the challenging nature of curating and establishing facts. While a number of approaches exist, none of them in-and-of-themselves are without drawbacks. A combination of techniques may provide the most satisfying results. We emphasize the importance of determining what facts are specific to the disorder - a particularly challenging endeavor - and the usefulness of various ways of quantifying these facts.

---

### Discrete-time survival analysis in the critically ill: a deep learning approach using heterogeneous data [^a2b62520]. NPJ Digital Medicine (2022). Medium credibility.

Within each domain, the collection of unique tokens constituted that domain's so-called vocabulary whose size is the number of unique tokens therein. Generally, embeddings serve to represent the original data in fewer dimensions to elicit latent information and relationships not necessarily discernible in the original data. Thus, finding the right embedding size is a trade-off: we seek an embedding that captures all the information we need in as few dimensions as possible. If the embedding becomes too small, however, the data are squeezed too much so pertinent, latent information is lost, hurting the overall model performance.

Because there is no obvious way to specify the best embedding size a priori, we estimated this with a hyperparameter α d (the embedding coefficient) determining heuristically, together with the vocabulary size V, the embedding size as D = 6α d V 1/4. So, for example with an embedding coefficient of 1, the embedding size would be 6 × 1 × 10,000 1/4 = 60 for a vocabulary of size 10,000 (166-fold reduction is dimensionality) and 6 × 1 × 1000 1/4 ≈ 34 for one of size 1000 (30-fold reduction).

Event temporality was operationalised as discretized timestamps: prior events were labelled as occurring < 1, 2–7, 8–30, 31–90, 91–365, or > 365 days before baseline whereas events after baseline were timestamped on an hourly basis (Fig. 5a). In this way, for example, within each domain embedding vectors of tokens observed < 1 day before baseline were concatenated, so were the embedding vectors of tokens observed in the first hour of ICU admission, and so forth. This data representation condensed the parameter space to optimize model robustness and allow the model to learn abstract, high-level relationships between data points with no manual feature engineering. The entity embedding weights were trained jointly with the rest of the model.

---

### Survival probability of stochastic processes beyond persistence exponents [^12e77d9f]. Nature Communications (2019). High credibility.

For many stochastic processes, the probability [Formula: see text] of not-having reached a target in unbounded space up to time [Formula: see text] follows a slow algebraic decay at long times, [Formula: see text]. This is typically the case of symmetric compact (i.e. recurrent) random walks. While the persistence exponent [Formula: see text] has been studied at length, the prefactor [Formula: see text], which is quantitatively essential, remains poorly characterized, especially for non-Markovian processes. Here we derive explicit expressions for [Formula: see text] for a compact random walk in unbounded space by establishing an analytic relation with the mean first-passage time of the same random walk in a large confining volume. Our analytical results for [Formula: see text] are in good agreement with numerical simulations, even for strongly correlated processes such as Fractional Brownian Motion, and thus provide a refined understanding of the statistics of longest first-passage events in unbounded space.

---

### Guideline no. 428: management of dichorionic twin pregnancies [^ee286411]. Journal of Obstetrics and Gynaecology Canada (2022). High credibility.

Regarding screening and diagnosis for multiple gestation, more specifically with respect to determination of gestational age, SOGC 2022 guidelines recommend to use the larger biometric estimate when using sonographic fetal biometry to date a twin pregnancy in the second trimester.

---

### A comparative analysis of statistical methods to estimate the reproduction number in emerging epidemics, with implications for the current coronavirus disease 2019 (COVID-19) pandemic [^529178b0]. Clinical Infectious Diseases (2021). Medium credibility.

In the current context of the SARS-CoV-2 pandemic, caution is required when using these methods for the estimation of R 0. As countries expand surveillance systems to better manage the pandemic, the assumption of constant reporting of cases implicit in these methods likely does not hold true until the capacity of the surveillance system has stabilized and a consistent case definition is applied. In these contexts, whilst testing capacity is growing, the use of hospital admission or death data, where available, may be preferable for inferring R 0 if the reporting of these data is believed to be constant in time. Delays in the time from infection to hospitalization and/or death, however, result in significant lags between when transmission events occur and when they can be quantified. Regardless of the metric used for estimation of R 0, we urge caution when interpreting short-term fluctuations in these estimates, as variable data quality and methodological accuracy may play a role in these trends.

---

### If things were simple, word would have gotten around. can complexity science help us improve pediatric research? [^616f4077]. Pediatric Research (2025). Medium credibility.

Don't we just need more data?

The most common argument against complexity science is that the unpredictability of complex systems results from a lack of knowledge. 'We just don't have enough data, but once we have more data, we will understand the system and eliminate unpredictability'. Complexity science states that although lack of data is clearly a problem in medicine, because of the way complex systems function, some degree of unpredictability will always remain. So why exactly won't collecting more data solve the problem of unpredictability in complex systems? We identify three main arguments.
The reference class problem. In clinical research, when everything is measured, every human being is unique and therefore different. To make accurate predictions, we would need to include reference groups in our studies for each of the relevant characteristics that can differ between people, and all the interactions and feedback loops. Evidently, there are too many variables and not enough data (and humans) in the world to solve this. This will mean that patients will always differ in some ways from patients in clinical studies, and therefore predictions will sometimes fail and treatments that are expected to benefit patients may not.
Changes over time. Humans and their environment change constantly. The moment we finish a study, the world has already moved on and our predictions or understandings – even if they were impeccable at the time the data were generated - may be out of date. In addition, intervening in a system will often change the system. We are always a step behind; data cannot predict the future.
Interconnecting systems. Complex systems are often part of other systems and consist of subsystems. Most of the time, we cannot investigate all relevant systems that are connected to our system of interest. We need to put boundaries and because complex systems are open systems, change can always occur across these boundaries.

Some may argue that artificial intelligence will allow us to reduce unpredictability. Though we expect that artificial intelligence can improve our understanding of complex systems substantially, because it can account for the many interactions and feedback loops that conventional analytic methods cannot, we maintain our claim that there will still be irreducible uncertainty, for the same reasons as mentioned above.

---

### Standards of care in diabetes – 2025 [^14753c0e]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for chronic kidney disease, more specifically with respect to patients with diabetes mellitus (monitoring of renal function), ADA 2025 guidelines recommend to monitor urinary albumin (such as spot urinary albumin-to-creatinine ratio) and eGFR 1–4 times per year, depending on the stage of the disease, in patients with established diabetic kidney disease.

---

### Incorporating economic evidence in clinical guidelines: a framework from the clinical guidelines committee of the American College of Physicians [^902eae12]. Annals of Internal Medicine (2025). High credibility.

Appendix figure 3 — sample summary of findings table provides a template for reporting economic evaluations, with columns for Outcome, Number of Studies (Reference), Population, Perspective, Incremental Cost (Range), Incremental Benefit (quality-adjusted life-year [QALY], patient-centered outcomes) (Range), Reported incremental cost-effectiveness ratio (ICER) per outcome (Range), Certainty of Body of Evidence, Adjusted ICER per U.S. Currency/Inflation, and Value Threshold†; the rows include Intervention A vs. Intervention B with placeholders ICER per QALY at X years and ICER per QALY at Y years, and footnotes clarify CGC = Clinical Guidelines Committee; ICER = incremental cost-effectiveness ratio; QALY = quality-adjusted life-year, note it was prepared with GRADEpro and modified by the authors, and that economic value is defined by the CGC (willingness-to-pay thresholds).

---

### Synopsis of the 2023 U.S. department of VA and U.S. DoD clinical practice guideline for the management of pregnancy [^8ddf4c93]. Military Medicine (2025). High credibility.

Anatomy (dating) ultrasound — antenatal ultrasonography is described as an integral part of routine antenatal care used to assess fetal growth and anatomy and to detect fetal anomalies, and it is recommended as an accurate method of determining gestational age, fetal number, viability, anatomic survey, placental location, amniotic fluid, and assessment of pelvic organs. The optimal timing of the complete fetal anatomy ultrasound is in the second trimester between 18 and 22 weeks' gestation, ultrasonography should be performed by technologists or providers who have undergone specific training and only when a valid medical indication for examination is present, and routine clinical scanning of every pregnant patient using real‑time B‑mode (2‑D) imaging is not contraindicated. The standard ultrasound evaluation includes fetal presentation, amniotic fluid volume, fetal cardiac activity, placental position, fetal biometry, and fetal number plus an anatomic survey.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^658be462]. Nature Communications (2025). High credibility.

Interpreting the realisationsas PPFs also imposes a third constraint, more subtle but equally important:
The processmust be non-accumulating.

A process which is accumulating would start at one end of the domain, say Φ = 0, and sequentially accumulate increments until it reaches the other end. Brownian motion over the interval [0, T] is an example of such a process. In contrast, consider the process of constructing a PPF for the data in Fig. 2: initially we have few data points and the PPF of their loss is very coarse. As the number of points increases, the PPF gets refined, but since loss values occur in no particular order, this happens simultaneously across the entire interval.

The accumulation of increments strongly influences the statistics of a process; most notably, the variance is usually larger further along the domain. This would not make sense for a PPF: ifis smaller than, that should be a consequence of δ EMD (Φ) being smaller than — not of Φ occurring "before".

This idea that a realisationis generated simultaneously across the interval led us to defineas a sequence of refinements: starting from an initial incrementfor the entire Φ interval [0, 1], we partition [0, 1] into n subintervals, and sample a set of n subincrements in such a way that they sum to. This type of distribution, where n random variables are drawn under the constraint of a fixed sum, is called a compositional distribution. Note that the constraint reduces the number of dimensions by one, so a pair of increments would be drawn from a 1-d compositional distribution. A typical 1-d example is the beta distribution for x 1 ∈ [0, 1], with x 2 = (1 − x 1) and α, β > 0:

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^675fa3aa]. Nature Communications (2025). High credibility.

Additionally, we adopted here the common convention used in machine learning of one-hot encoding to specify classification outcomes, but biologically, it may be more realistic to specify whole profiles of chemical concentrations as computational outputs. In previous workwe showed analytically that the ratiois independent of F i j for any k and, which can be shown to imply that the monotonicity constraint holds under any linear mapping. Thus, the expressivity limitations identified in this work should at least hold for output profiles that are arbitrary linear transformations of one-hot encoded outputs. By considering the encoding-agnostic objective of maximizing the mutual information between the output distribution π and the input distribution, we have provided preliminary evidence that the expressive capacity of the system increases with M, independent of specific encoding schemes.

Finally, an additional aspect of physics that deserves attention in the future is that chemical dynamics are inherently stochastic, and fluctuations about the steady-state mean are important, especially when copy numbers are small. Decision-making under fluctuations has often been treated using the framework of information theory. A general trend from this line of research is that maximizing information flow requires reducing fluctuations, which in turn requires greater expenditure of chemical free energy. On the other hand, some forms of chemical computation harness stochasticity to generatively model probability distributions. Combining insights from these works with our results on classification expressivity can help paint a full picture of how biochemical systems use fuzzy logic to make decisions.

---

### A computational method for cell type-specific expression quantitative trait loci mapping using bulk RNA-seq data [^87da5fb1]. Nature Communications (2023). High credibility.

Methods

Statistical models

Notations and the joint model of TReC and ASReC

Since our model is the same for any gene-SNP pair, we omit gene and SNP indices to simplify notations. We use i and q as indices for sample and cell type, respectively, where i = 1,…, n, q = 1,…, Q, and n and Q denotes sample size and the number of cell types, respectively. Let T i and N i be the total read count (TReC) and the allele-specific read count (ASReC) mapped in the i th sample. Each SNP of interest has two alleles, A and B. Each gene has two haplotypes that are arbitrarily defined as haplotype 1 and 2. Let N i − N i 2 and N i 2 denote the ASReC mapped to the first and second haplotypes of sample i, respectively.