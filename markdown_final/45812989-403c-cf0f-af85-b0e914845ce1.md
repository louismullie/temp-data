# Medical Question & Answer

**Sample ID**: 45812989-403c-cf0f-af85-b0e914845ce1
**Dataset Index**: 194581

---

## Question

On a $50$-question multiple choice math contest, students receive $4$ points for a correct answer, $0$ points for an answer left blank, and $-1$ point for an incorrect answer. Jesse's total score on the contest was $99$. What is the maximum number of questions that Jesse could have answered correctly?
$\textbf{(A)}\ 25 \qquad \textbf{(B)}\ 27 \qquad \textbf{(C)}\ 29 \qquad \textbf{(D)}\ 31 \qquad \textbf{(E)}\ 33$

---

## Answer

> Let's see… What do we have here? The user is asking for the maximum number of questions Jesse could have answered correctly on a 50-question multiple-choice contest with a specific scoring system, given that Jesse's total score was 99. Let's break this down step-by-step. First, I need to think about defining variables and setting up the score equation. Then, I should verify the constraints and the objective function. Next, I will isolate the variable of interest and derive an inequality for the maximum correct answers. After that, I need to check feasibility by ensuring the implied number of incorrect answers is non-negative and that the total questions add up to 50. Finally, I will compute the maximum and confirm it matches one of the provided options.

> Let me first define the variables clearly: let c be the number of correct answers, w be the number of wrong answers, and b be the number of blank answers. The total number of questions is 50, so c + w + b = 50, and the total score is 4c − w = 99, since correct answers earn 4 points and incorrect answers lose 1 point, with blanks earning 0 points [^notfound].

> Wait, let me verify the objective and constraints. I want to maximize c, subject to c + w + b = 50 and 4c − w = 99, with c, w, b all non-negative integers. To maximize c, I should minimize the penalty from wrong answers, which means I should try to make w as small as possible while still satisfying the equations and non-negativity constraints.

> I will now examine the score equation to express w in terms of c. From 4c − w = 99, I get w = 4c − 99. Substituting this into the total questions constraint gives c + (4c − 99) + b = 50, which simplifies to 5c − 99 + b = 50, and then b = 149 − 5c. Hold on, I should verify that b is non-negative, so 149 − 5c ≥ 0, which implies c ≤ 29.8. Since c must be an integer, c ≤ 29.

> Hmm, wait a minute, I initially thought c could be as high as 31, but that would make b negative, which is impossible. Let me reconsider: if c = 30, then b = 149 − 150 = −1, which is invalid, so c cannot be 30. If c = 29, then b = 149 − 145 = 4, which is valid, and w = 4(29) − 99 = 116 − 99 = 17, which is also valid. So the maximum feasible integer value for c is 29.

> I should double-check the totals to ensure consistency. With c = 29, w = 17, and b = 4, the sum is 29 + 17 + 4 = 50, which matches the total number of questions, and the score is 4(29) − 17 = 116 − 17 = 99, which matches the given total score. Therefore, the maximum number of correct answers is 29.

> The answer corresponds to option (C) 29. I need to ensure there isn't a higher option that still works; as shown, c = 30 leads to b = −1, which is impossible, so 29 is indeed the maximum.

---

The maximum number of questions Jesse could have answered correctly is **29**. This is achieved by answering 29 questions correctly, 17 incorrectly, and leaving 4 blank, yielding a total score of 99 points [^notfound].

---

## Step-by-step solution

Let (c) be the number of correct answers, (w) the number of wrong answers, and (b) the number of blank answers. We have the following equations:

- **Total questions**: (c + w + b = 50)
- **Total score**: (4c - w = 99)

We want to **maximize (c)**. From the score equation, (w = 4c - 99). Substitute this into the total questions equation:

[
c + (4c - 99) + b = 50
]

Simplify:

[
5c - 99 + b = 50
]

[
5c + b = 149
]

Since (b \\geq 0), we have:

[
5c \\leq 149
]

[
c \\leq 29.8
]

Because (c) must be an integer, the **maximum value of (c) is 29**. Verify by substituting (c = 29):

- (w = 4(29) - 99 = 116–99 = 17)
- (b = 50–29–17 = 4)

Check the score: (4(29) - 17 = 116–17 = 99), which matches the given total.

---

Therefore, the **maximum number of questions Jesse could have answered correctly is 29** [^notfound].

---

## References

### Multiple choice answers: what to do when you have too many questions [^622557ce]. The Journal of Foot and Ankle Surgery (2015). Low credibility.

Carrying out too many statistical tests in a single study throws results into doubt, for reasons statistical and ethical. I discuss why this is the case and briefly mention ways to handle the problem.

---

### Great question! the art and science of crafting high-quality multiple-choice questions [^b59efb11]. Journal of the American College of Radiology (2022). Medium credibility.

Assessment of medical knowledge is essential to determine the progress of an adult learner. Well-crafted multiple-choice questions are one proven method of testing a learner's understanding of a specific topic. The authors provide readers with rules that must be followed to create high-quality multiple-choice questions. Common question writing mistakes are also addressed to assist readers in improving their item-writing skills.

---

### Why questionnaire scores are not measures: a question-raising article [^21ae0208]. American Journal of Physical Medicine & Rehabilitation (2023). Medium credibility.

The "Sufficiency" Requirement and the Issue of Redundancy

The actual difficulty of each item is only roughly estimated with TTT. Could we better know this difficulty level, we might predict, given the total score achieved by a subject, which is the score expected in each item (see the ideal Guttmann pattern on Table 1 A) and vice versa. This property is called "sufficiency". Suppose two or more items share the same difficulty level. In that case, they indicate the same overall ability level: if their scores are summed anyway, as in the case of TTT, then an illusion of a greater ability is by fault considered.

Take the following simplistic example of a 6-item math test. A score of 1 is given to each correct answer; 0 is assigned to wrong responses.

A) 3 + 2 = ?; B) 5–4 = ?; C) 4*5 = ?; D) 9/3 = ?; E) 6/2 = ?; F) 4/1 = ?

It is clear that once the respondent can answer correctly to item D, the first of the three divisions, he/she will likely answer correctly also to items E and F, the other two divisions (all with 1-digit factors and an integer quotient). The respondent will get three score points for the same ability level ("ability to solve this kind of division"). His/her score will artificially jump from 3 to 6. This "jackpot" passes easily undetected in many questionnaires but makes it hazardous to compare changes within and between subjects. Rasch analysis solves this problem elegantly.

---

### How to write a high quality multiple choice question (MCQ): a guide for clinicians [^71dc64ba]. European Journal of Vascular and Endovascular Surgery (2017). Low credibility.

Despite the variety of assessment tools available, multiple choice questions (MCQs) still play an integral part in examinations at both a national and speciality board level. MCQs have a number of methodological advantages yet their strength is related to the quality of the question posed. Specifically, there has been a move towards the MCQ testing a taxonomically higher order concept of integration-interpretation and problem solving. This paper focuses on question development and the potential pitfalls to avoid.

---

### Why questionnaire scores are not measures: a question-raising article [^3669e2d5]. American Journal of Physical Medicine & Rehabilitation (2023). Medium credibility.

FIGURE 1
The raw scores from a 0–100 cumulative questionnaire (on the ordinate) are given as a function of the "true" linear measure (on the abscissa) of the variable tackled. Suppose scores are the number of correct responses (or, interchangeably here, the percent of correct answers). The true measure is given in linear logit units from RA. The increase in scores is roughly proportional to the rise in measure only in the "central" portion of the S-shaped (here, logistic) function (heavily hatched area). At the extremes (here, the "ceiling" of the scale, not the "floor", is highlighted), the same increase of 10 score points corresponds to a much higher increase in true measure (lightly hatched area). The scale's capacity to discriminate across subjects is lower the more the scores approach the ceiling of the questionnaire (reprinted with permission from Linacre, Fig. 1 in Wright and Linacre).

---

### Why questionnaire scores are not measures: a question-raising article [^e83922fd]. American Journal of Physical Medicine & Rehabilitation (2023). Medium credibility.

From Observations to a Scale

Cumulative questionnaires (or "scales") consist of a list of observable behaviors (i.e. the questionnaire's items) considered as the manifestations (effects) of a shared latent property. When the number of behaviors expressed by an individual is counted (such as when a questionnaire's total score is calculated, see hereinafter), three assumptions are implicitly accepted: (1) that all observations are caused by the same variable; (2) that all observations are caused by a single variable; and (3) that the larger the sum score, the more of the latent variable is there. Remember with this regard the oranges' example reported previously.

In the most straightforward "dichotomous" items, a 0/1 labelling (no/yes, pass/fail) is assigned to absence/presence of a given behavior (e.g. wrong/correct answer to an algebraic question in a knowledge of math scale; being dependent/independent in walking in a disability questionnaire, etc.). By "observed behavior", a "yes = 1" endorsement by the subject or, passing a given test, is intended. The symbol "1" is simply a label. "How many times" a "1" is observed, that is, how many times a subject manifests these behaviors across items (the cumulated, total scale score), provides the estimate of subjects' overall "ability". Conversely, "how many times" an item receives a 1/yes/pass label across subjects provides an estimate of item "difficulty" (the lower the item score across subjects, the "more difficult" the item).

Together with dichotomous items, "polytomous" items are also widespread (e.g. "with someone's help/with orthotic aid/with supervision/autonomous" = 0/1/2/3 or, "fully disagree/partially disagree/moderately agree/fully agree" = 0/1/2/3). If they are made of polytomous items, questionnaires are dubbed "rating scales". The concept of "counting" does not change. The total scale score comes from summing "how many times" a "1" was observed, plus "how many times" a "2" was observed, etc. The difference lies in the "weight" assigned to the counts. Observing the behavior labeled "2" counts twice as much, compared with observing the behavior scored 1, etc.

---

### Knowledge acquisition is governed by striatal prediction errors [^08a2e294]. Nature Communications (2018). Medium credibility.

During Test1, all trials had the potential to display the correct or false feedback — the latter being a plausible yet significantly different answer to the question. Just as there was only one possible correct feedback answer which could be displayed, each question was associated with one possible false-feedback answer. Naturally, this investigation necessitated that participants be naïve to the manipulation. Thus, we did not provide false feedback to any questions answered with a confidence above 70 — a level determined by pilot testing to satisfy this prerequisite. Participants who nevertheless became aware of the manipulation were excluded from the study. Using a pseudo-random number generator, questions to which answers were supplied with a confidence rating below 60 displayed the false feedback on 75% of trials, and on 60% of trials with a confidence rating between 60 and 70. The large proportion of false-feedback trials was necessary because the veracity of recall answers could only be determined post testing, so we could not provide false feedback solely following correct answers. Furthermore, there were fewer correct than incorrect answers (average 36%) and they tended to be expressed with higher confidence (average confidence for correct answers was 70.5, confidence was above 70 — the cutoff — in 56% of correct trials). In order to generate sufficient trials where participants were supplied with false feedback following correct answers of low and medium confidence, we therefore resorted to high proportions within these confidence ranges — the corollary being that a large proportion of incorrect answers was met with false feedback (Supplementary Table 3).

---

### The subject-dependent, cumulative, and recency association of aerobic fitness with academic performance in Taiwanese junior high school students [^c983e334]. BMC Pediatrics (2019). Medium credibility.

Measurements

Aerobic fitness was assessed by a 1600- and 800-m run test for boys and girls at the beginning of each academic year. This test is a standard measure of aerobic fitness used for Taiwanese adolescents, and has been shown to have a high criterion-related validity (r = 0.79). Students were instructed to give their best effort to run/walk the distance as fast as possible. The score on this test was the total time in seconds to cover the 1600- or 800-m distance, with shorter time indicating better performance.

Academic achievement was assessed by the BCTJH at the end of the final year. The BCTJH is a compulsory, nationwide examination given to all high-school-bound students in Taiwan. This measure is not only a standardized and validated measure of academic achievement, students are also highly motivated to do their best when taking the exam since the results determine their chances of being admitted to competitive high schools which, in turn, will affect their future educational and career path. This test consists of 6 subjects: language (Chinese), foreign language (English), math, social science, science, and an essay. The first five tests consist of computer-scored, multiple choice questions. Scale scores ranging from 1 to 80 points are determined for each test based on performance on the questions answer correctly. The essay, which was marked by trained MOE examiners, had a maximum score of 12 and required students to write down their thoughts on a given topic. Thus, the maximum obtainable score on the BCTJH was 412 points. The current study extracted data from all subjects except the essay due to its subjective nature and low discriminatory validity (only 6 scores were given, 2, 4, 6, 8, 10, and 12, with the vast majority of students being given a 6 or 8). In practice, even very small differences in exam performance often have significant implications. For example, a 2 to 4 points increase in total score might typically allow entry to a high school standing one or two places higher in the national rankings.

---

### APA guidelines for psychological assessment and evaluation [^42edaf3b]. APA (2020). High credibility.

Psychological assessment — standardized administration, scoring accuracy, and procedural/legal responsibilities — states that standardized administration is most important for validity, which may be compromised when adherence to standardization is not met; important points include a high level of knowledge of instructions, awareness of instructions on feedback, query, prompts, and response to questions, facility with supplemental material and audio/visual components, inclusion of behavioral observations, and compliance with equipment and placement of materials. Scoring of responses lends itself to error and is unforgiving, and examiners strive to be alert to miscalculation of base and ceiling, simple arithmetic errors, reversal and discontinuance mistakes, failure to record examinee comments in addition to scores, and imprecise timing on timed tests. Knowledge about procedural requirements, confidentiality of information, communication of results, and security are important, and psychologists are encouraged to understand the legal and ethical issues related to the release of test data; psychologists strive to be able to explain results and limitations to diverse audiences and to include in reports the purpose of the test and the setting in which it occurred.

---

### Opioids for chronic pain: a knowledge assessment of nonpain specialty providers [^5a12633d]. Journal of Pain Research (2016). Low credibility.

KnowPain-50

The KnowPain-50 survey comprised 50 questions. Eighteen of the 50 questions were related to opioid management and the remaining 32 questions were related to general knowledge about pain medicine. The focus of each opioid question was further categorized as either medicolegal (n = 7) or clinical (n = 11). All medicolegal questions contained the words or phrases "investigated", "illegal", "not lawful", "state and federal requirements", or "federal regulations". The clinical questions pertained to the clinical management of pain with opioids.

The survey was scored according to the approach described by Harris et al. Five questions used a multiple-choice single response format; correct single responses were awarded 5 points and incorrect single responses were awarded 0 points. One of the five multiple-choice questions was a clinical question about opioids ("By far the most common adverse side effect of opioid therapy is:"). The remaining 45 questions were answered using a Likert scale ranging from 1 to 6, where 1 = strongly agree; 2 = agree; 3 = somewhat agree; 4 = somewhat disagree; 5 = disagree; 6 = strongly disagree. The correct response for 25 of the Likert questions was the negative (strongly disagree, disagree, somewhat disagree), and the correct response for the remaining 20 Likert questions was the affirmative (strongly agree, agree, somewhat agree). All responses to the Likert questions were converted to a 0- to 5-point scale, where the "strongly agree" or "strongly disagree" correct responses were awarded 5 points, and all other responses were awarded a sequentially diminishing number of points. For example, for questions where the correct response was the negative, 5 points were awarded for the response "6 = strongly disagree", 4 points were awarded for the response "5 = disagree", 3 points were awarded for the response "3 = somewhat disagree", and so on until 0 points were awarded for the response "6 = strongly agree". Alternatively, for questions where the correct response was the affirmative, 5 points were awarded for the response "1 = strongly agree", 4 points were awarded for the response "5 = agree", 3 points were awarded for the response "3 = somewhat agree", and so on until 0 points were awarded for the response "6 = strongly disagree". The maximum possible points for the 50-item survey were 250.

---

### Knowledge, attitude, and practice of pre-diabetic older people regarding pre-diabetes [^c358efb7]. BMC Geriatrics (2024). Medium credibility.

The knowledge section includes 10 multiple-choice questions about etiology, diagnosis, consequences, and recommendations about pre-diabetes, with only one correct answer. The wrong answer is scored zero. For every correct answer in questions 1, 2, 7, 8, 9, and 10, 1 point is awarded. For every correct answer in questions 4 and 5, 2 points are awarded. For the correct answer to question 3, 3 points, and the correct answer to question 6, 4 points are awarded. The total score of the questionnaire is 17, and the participants' knowledge level is classified under three levels: less than 10 (weak), 10–13 (medium), and 14–17(good).

The attitude section measures the participants' attitudes on a 3-option Likert scale (strongly agree, neither agree nor disagree, and strongly disagree). The items of this section are focused on pre-diabetic people's attitudes towards correcting lifestyle and gaining beliefs and feelings about pre-diabetes. Every positive attitude is awarded 1 point, every negative attitude is scored − 1, and every neutral attitude is scored zero. The total score is 10. The scores less than zero indicate an extremely negative attitude, the scores between 0 and 2 indicate a negative attitude, the scores between 3 and 6 indicate a neutral attitude, 7 and 8 indicate a positive attitude, and scores 9 and 10 indicate an extremely positive attitude. The range of scores is between − 10 and + 10. Higher scores in the attitude section indicate a positive attitude toward diabetes prevention.

---

### Knowledge acquisition is governed by striatal prediction errors [^4ee77abd]. Nature Communications (2018). Medium credibility.

We calculated the overall updating effect (Fig. 2c) by summing the confidence level of each correct answer in Test2 (assigning 0 to those remaining incorrect) and dividing by the total number of incorrect answers for each unique Test1 PE value. In other words, an overall score of 100 would indicate that all incorrect Test1 trials for a given PE value were answered correctly and with a confidence of 100 in Test2. If only half the trials were answered correctly, and with an average confidence of 50, the overall updating value would be 25. In the recognition study, significance of the relationship between overall updating and PE (Fig. 2c), was determined by performing a one-sample t -test on the β values of the single-subject regressions to show that the parameter significantly differed from zero, as well as testing the significance of the group-level regression model.

In the recall study, certain trial types — which occurred very infrequently — were excluded from analyses. These were instances of incorrect Test1 answers in the false-feedback condition where (a) the incorrect answer matched the ensuing false feedback (leading to a false sense of having answered correctly), or (b) the answer provided in Test2 was correct with respect to the original text (i.e. provided the correct answer despite initially failing to do so and receiving false feedback). We also excluded rare instances where correct Test1 answers were stated with a confidence rating of zero (for the false-feedback analyses), and in the imaging analyses - hence there were no +100 PE events. In the recognition study, we excluded a small number of trials where for a given question in either Test1 or Test2, (a) no answer was selected within the time limit, or (b) a mistake was made in answer selection, as indicated by a zero-confidence rating where an answer was chosen (i.e. not skipped). In instances where these events occurred in Test2, we did not exclude the corresponding Test1 trial from the imaging analysis of PE correlative regions.

---

### Performance of large language models on a neurology board-style examination [^9a1e546a]. JAMA Network Open (2023). High credibility.

Level of Confidence

Both models consistently responded to multiple-choice questions using confident or highly confident language (100%, 400 of 400 questions evaluated by investigators). Self-assessment of confidence expressed by LLM 1 and LLM 2 in their answers showed a small difference between incorrect and correct responses (mean [SD] Likert score for LLM 1, 4.69 [0.46] vs 4.79 [0.41]; P < .001; for LLM 2, 4.77 [0.45] vs 4.93 [0.30]; P < .001). Incorrect LLM 2 and LLM 1 answers were all subjectively assessed by the models as expressing confidence or high confidence (Likert score 4 or 5 for LLM 2: 292 of 294 [99.3%]; for LLM 1: 650 of 650 [100%]) (eFigure 5 in Supplement 1). When prompted with the correct answer after an incorrect response, both models responded by apologizing and agreeing with the provided correct answer in all cases.

Reproducibility of Responses

Reproducibility analyses revealed that highly reproducible answers were more likely to be answered correctly than inconsistent answers by LLM 1 (66 of 88 [75.0%] vs 5 of 13 [38.5%]; P = 0.02), potentially indicating another marker of confidence of LLMs that might be leveraged to filter out invalid responses. The same observation was made with LLM 2, with 78 of 96 correct answers (81.3%) in those with high reproducibility vs 1 of 4 (25.0%) in answers with low reproducibility (P = 0.04).

---

### Knowledge acquisition is governed by striatal prediction errors [^d4185286]. Nature Communications (2018). Medium credibility.

FMRI participants performed Test1 in the scanner, in two runs (50 questions in each, lasting for 20–25 min). Listed below each question were four potential answers to choose from. The same 100 questions and potential answers were used for all participants (randomized in order). Participants selected their answer using a right-hand response box featuring four buttons, or skipped the question using a response box supplied to their left hand — as with the recall study, they were instructed not to make an outright guess if they had no conception of the answer. Where an answer was selected, the chosen option was highlighted in yellow for 0.5 s. If no response was registered within 50 s the trial proceeded. For the confidence rating, two of the right-hand buttons were used to move the red bar on the visual analog scale left or right (from an initial location of 50) in steps of 10. The confidence number corresponding to the bar's location was indicated directly above it (Fig. 1). To enter the rating and move to the next phase, the left-hand button was pressed. In the event of a mistake (incorrect button press), instructions were to enter zero for the confidence. The feedback phase displayed the question and potential answers, with the correct answer highlighted in green, and lasted for 4.5 s. There were no false-feedback answers supplied in the fMRI study. Intertrial intervals (black screen), as well as those separating question/response–confidence, and confidence–feedback phases were jittered, to aid in deconvolution of event-related neural responses (range and median: 3–7 s, 4 s; 1–3 s, 2 s; 3–10 s, 5 s, respectively).

---

### Lisdexamfetamine dimesylate (Vyvanse) [^96a4f544]. FDA (2025). Medium credibility.

Adults: Short-Term Treatment in ADHD

A double-blind, randomized, placebo-controlled, parallel-group study (Study 7) was conducted in adults ages 18 to 55 (N = 420) who met DSM-IV criteria for ADHD. In this study, patients were randomized to receive final doses of 30 mg, 50 mg, or 70 mg of VYVANSE or placebo for a total of four weeks of treatment. All patients receiving VYVANSE were initiated on 30 mg for the first week of treatment. Patients assigned to the 50 mg and 70 mg dose groups were titrated by 20 mg per week until they achieved their assigned dose. The primary efficacy outcome was change in Total Score from baseline to endpoint in investigator ratings on the ADHD Rating Scale (ADHD-RS). Endpoint was defined as the last post-randomization treatment week (i.e. Weeks 1 through 4) for which a valid score was obtained. All VYVANSE dose groups were superior to placebo in the primary efficacy outcome (Study 7 in Table 6).

The second study was a multi-center, randomized, double-blind, placebo-controlled, cross-over, modified analog classroom study (Study 8) of VYVANSE to simulate a workplace environment in 142 adults ages 18 to 55 who met DSM-IV-TR criteria for ADHD. There was a 4-week open-label, dose optimization phase with VYVANSE (30 mg/day, 50 mg/day, or 70 mg/day in the morning). Patients were then randomized to one of two treatment sequences: 1) VYVANSE (optimized dose) followed by placebo, each for one week, or 2) placebo followed by VYVANSE, each for one week. Efficacy assessments occurred at the end of each week, using the Permanent Product Measure of Performance (PERMP), a skill-adjusted math test that measures attention in ADHD. PERMP total score results from the sum of the number of math problems attempted plus the number of math problems answered correctly. VYVANSE treatment, compared to placebo, resulted in a statistically significant improvement in attention across all post-dose time points, as measured by average PERMP total scores over the course of one assessment day, as well as at each time point measured. The PERMP assessments were administered at pre-dose (-0.5 hours) and at 2, 4, 8, 10, 12, and 14 hours post-dose (Study 8 in Table 6, Figure 6).

---

### Does educator training or experience affect the quality of multiple-choice questions? [^dc1098bf]. Academic Radiology (2015). Low credibility.

Rationale and Objectives

Physicians receive little training on proper multiple-choice question (MCQ) writing methods. Well-constructed MCQs follow rules, which ensure that a question tests what it is intended to test. Questions that break these are described as "flawed". We examined whether the prevalence of flawed questions differed significantly between those with or without prior training in question writing and between those with different levels of educator experience.

Materials and Methods

We assessed 200 unedited MCQs from a question bank for our senior medical student radiology elective: an equal number of questions (50) were written by faculty with previous training in MCQ writing, other faculty, residents, and medical students. Questions were scored independently by two readers for the presence of 11 distinct flaws described in the literature.

Results

Questions written by faculty with MCQ writing training had significantly fewer errors: mean 0.4 errors per question compared to a mean of 1.5–1.7 errors per question for the other groups (P < .001). There were no significant differences in the total number of errors between the untrained faculty, residents, and students (P values .35-.91). Among trained faculty 17/50 questions (34%) were flawed, whereas other faculty wrote 38/50 (76%) flawed questions, residents 37/50 (74%), and students 44/50 (88%). Trained question writers' higher performance was mainly manifest in the reduced frequency of five specific errors.

Conclusions

Faculty with training in effective MCQ writing made fewer errors in MCQ construction. Educator experience alone had no effect on the frequency of flaws; faculty without dedicated training, residents, and students performed similarly.

---

### Cognitive and field testing of a new set of medication adherence self-report items for HIV care [^e2893e76]. AIDS and Behavior (2014). Low credibility.

We tested a series of questions that asked about taking medicine "exactly as the doctor prescribed". Subjects found this difficult for several different reasons. One problem is that it is not always a physician that does the prescribing. Another problem was that doctors differ in the extent to which they describe how medicines should be taken, with some giving little instruction, and others giving more. However, even if a physician did provide instructions, patients often could not remember the details of the instructions. We tested several variations to solve this problem, and patients most consistently understood the questions that used the phrase "the way you are supposed to take" your medicine.

We found that subjects usually had a consistent understanding of the concept of a "dose". However, there was inconsistent application of this concept when answering the actual survey questions, particularly in cases where there was twice a day dosing.

With regard to response options, both visual analogue scales and asking about percents worked relatively poorly. Most people have to do some math to respond to these questions, and they often make errors when they do. Moreover, there are some subjects who made a guess or an estimate without doing any math. In general, we found that participants were not consistently able to understand and apply fractions or percentages.

A scale that ranged from "very poor" to "perfect" did not work well. Some subjects answered by saying that "no one is perfect". Many refused to endorse "perfect" even when their adherence was 100% on other scales. There was also confusion about a scale we tested that asked "Overall, how do you feel about the way you took [medication name] in the last 7 days?" Response options ranged from delighted to terrible. This caused confusion between how they felt physically and how they took their medications. Also, "delighted" was not a term that many associated with medication taking.

In general, subjects were more comfortable and confident using adjectives and adverbs as response options than they were with quantitative assessments. One partial reason is that, as we noted above, detailed recall was far from perfect. The level of detail respondents could recall was more appropriate for verbal than quantitative answers. For those with less than perfect adherence, as previously noted, subjects tended to estimate rather than count or enumerate, and words seem to map onto this cognitive estimation process more accurately and more comfortably for most patients than numbers.

The three best performing items are shown in the Appendix.

---

### Survey research [^c44856f5]. Anesthesiology (2019). Medium credibility.

Surveys provide evidence on practice, attitudes, and knowledge. However, conducting good survey research is harder than it looks. The authors aim to provide guidance to both researchers and readers in conducting and interpreting survey research. Like all research, surveys should have clear research question(s) using the smallest possible number of high-quality, essential, survey questions (items) that will interest the target population. Both researchers and readers should put themselves in the position of the respondents. The survey questions should provide reproducible results (reliable), measure what they are supposed to measure (valid), and take less than 10 min to answer. Good survey research reports provide results with valid and reliable answers to the research question with an adequate response rate (at least 40%) and adequate precision (margin of error ideally 5% or less). Possible biases among those who did not respond (nonresponders) must be carefully analyzed and discussed. Quantitative results can be combined with qualitative results in mixed-methods research to provide greater insight.

---

### Why questionnaire scores are not measures: a question-raising article [^3d475008]. American Journal of Physical Medicine & Rehabilitation (2023). Medium credibility.

The Puzzle of Missing Values

Missing items are common in questionnaires, resulting from different reasons. Typical examples are made by subjects who may inadvertently skip over some questions or may not have enough time to complete the test. Subjects would prefer not to answer if higher penalties are assigned to wrong than missing answers. Sometimes, some items are reserved to some respondents, for example, those who attempted in the past a given activity or have other characteristics such as sex or language.

When missing answers are found, the easiest and most practiced solution would be to transform scores into the percentage of the total score given by the answered items. For example, if a subject misses two items and totals seven on a questionnaire of 10 dichotomous items, the subject's "total score" is 87.5% (7/8). Note that this procedure is equivalent to the common "mean substitution" procedure, assigning the missing items the average score observed in the other items. This method relies on the (strong and unlikely) assumption that questionnaires' items are all indicators of the same quantity of the latent variable of interest and thus exchangeable (i.e. they all share the same difficulty level).

Table 2 provides an example showing why this solution does not work. Suppose 10 items again, each scored 0 or 1, aligned from left to right for increasing difficulty. The "x" symbol flags a missing response. According to traditional psychometrics, subjects A and B are assigned the same score. Subject C, providing two missing answers, is assigned a lower score. However, it seems that subject A's missing response should be given a higher score estimate, given that the subject passed five more difficult items. In contrast, subject B failed three items easier than the missed one. It is doubtful that subject C deserves a lower score than subject B: he/she passed four items more difficult than the missed items.

TABLE 2
Estimation of missing values, based on average scores across answered items

Presumably, subjects A and C were careless in their answers or had not enough time to complete the test.

This example is intentionally simplistic. The treatment of missing data is a complex research field, faced by the whole field of statistics. As far as psychometric statistics is concerned, RA provides a satisfactory answer to this problem.

---

### Toward expert-level medical question answering with large language models [^51b00fa2]. Nature Medicine (2025). Excellent credibility.

Pairwise ranking evaluation of long-form answers

In addition to independent evaluation of each answer, pairwise ranking evaluations involved directly comparing alternative answers to a given question. Raters were presented with pairs of answers from different sources (for example, physician versus Med-PaLM 2) for a given question.

For each pair of answers, raters were asked to select the preferred answer or indicate a tie along the following axes (with exact instruction text in quotes):
Alignment with medical consensus: "Which answer better reflects the current consensus of the scientific and clinical community?"
Reading comprehension: "Which answer demonstrates better reading comprehension (indication the question has been understood)?"
Knowledge recall: "Which answer demonstrates better recall of knowledge (mention of a relevant and/or correct fact for answering the question)?"
Reasoning: "Which answer demonstrates better reasoning step(s) (correct rationale or manipulation of knowledge for answering the question)?"
Inclusion of irrelevant content: "Which answer contains more content that it shouldn't (either because it is inaccurate or irrelevant)?"
Omission of important information: "Which answer omits more important information?"
Potential for demographic bias: "Which answer provides information that is biased for any demographic groups? For example, is the answer applicable only to patients of a particular sex where patients of another sex might require different information?"
Possible harm extent: "Which answer has a greater severity/extent of possible harm (which answer could cause more severe harm)?"
Possible harm likelihood: "Which answer has a greater likelihood of possible harm (more likely to cause harm)?"

For three axes (reading comprehension, knowledge recall and reasoning), pairwise ranking evaluation differed from the long-form individual answer evaluation. Individual answer evaluation separately examines whether an answer contains both positive and negative evidence of performance on each axis, while pairwise ranking evaluation consolidates these two questions to assess overall quality. These evaluations were performed on the MultiMedQA 1066 and adversarial datasets. Raters were blinded as to the source of each answer, and the order in which answers were shown was randomized. Due to technical issues in the display of answers, raters were unable to review eight of 1,066 answers for the Med-PaLM 2 versus physician comparison, and 11 of 1,066 answers for the Med-PaLM 2 versus Med-PaLM comparison; these answers were excluded from analysis in Figs. 1 and 3 and Supplementary Tables 6 and 7.

---

### Questioning as a teaching tool [^c4f8062f]. Pediatrics (2015). Low credibility.

The Dreyfus and Bloom frameworks can help the great clinical teacher craft questions that are learner-centric and appropriately challenging. Employing strategies to ask the right questions in the right way can further add to the effectiveness of using questions as a valuable teaching, learning, and assessment tool.

---

### The neural basis of metacognitive monitoring during arithmetic in the developing brain [^0d463605]. Human Brain Mapping (2020). Medium credibility.

Second, we compared performance measures on the MC and color question. It should be noted that we did not compare the two conditions on the number of correct responses that were given within the time limit, because in the MC condition, determining an accuracy measure was not possible as there is no correct or incorrect response. Namely, the MC question that was asked is a question on what the child thinks about the correctness of his/her answer. As such, accuracy in the experimental condition comes down to MC accuracy. In the color condition, on the other hand, there is indeed one correct response (i.e. the color of the equal sign). As a result, a comparison of "accuracy" between the two conditions to investigate task difficulty was not possible. The response rate of both conditions, on the other hand, can be compared to give an indication of potential differences in task difficulty. Using independent samples t test, we compared whether or not participants were able to provide an answer to the MC or color question within the given time frame (i.e. 2,500 ms), independent of the accuracy of that answer (i.e. a score of 0 was given if participants failed to answer within the time limit; a score of 1 when they were able to answer within the time frame). The results indicated that there was no difference in response rate (M MC = 0.94, SD MC = 0.06; M C = 0.94, SD C = 0.05; t (98) = −0.58, p = .57, BF 10 = 0.25). The Bayes factor indicated evidence for the null hypothesis of no difference in response rate between the conditions, thus pointing to equivalence in task difficulty.

---

### Toward expert-level medical question answering with large language models [^48956d60]. Nature Medicine (2025). Excellent credibility.

Self-consistency

Self-consistency (SC) is a strategy introduced in ref.to improve performance on multiple-choice benchmarks by sampling multiple explanations and answers from the model. The final answer is the one with the majority (or plurality) vote. For a domain such as medicine with complex reasoning paths, there might be multiple potential routes to the correct answer. Marginalizing over the reasoning paths can lead to the most accurate answer. In this work, we performed SC with 11 samplings using CoT prompting, as in ref.

Ensemble refinement

Building on CoT and SC, we developed a simple prompting strategy that we refer to as ensemble refinement (ER). ER builds on other techniques that involve conditioning an LLM on its own generations before producing a final answer, including CoT prompting and self-refine.

ER involves a two-stage process: first, given a (few-shot) CoT prompt and a question, the model produces multiple possible generations stochastically via temperature sampling. In this case, each generation involves an explanation and an answer for a multiple-choice question. Then, the model is conditioned on the original prompt, question and the concatenated generations from the previous step, and is prompted to produce a refined explanation and answer. This can be interpreted as a generalization of SC, where the LLM is aggregating over answers from the first stage instead of a simple vote, enabling the LLM to take into account the strengths and weaknesses of the explanations it generated. To improve performance, we performed the second stage multiple times and finally took a plurality vote over these generated answers to determine the final answer. ER is depicted in Extended Data Fig. 1.

Unlike SC, ER may be used to aggregate answers beyond questions with a small set of possible answers (for example, multiple-choice questions). For example, ER can be used to produce improved long-form generations by having an LLM condition on multiple possible answers to generate a refined final answer. Given the resource cost of approaches requiring repeated samplings from a model, we apply ER only for multiple-choice evaluation in this work, with 11 samplings for the first stage and 33 samplings for the second stage.

---

### Research priorities for the intersection of alcohol and HIV / AIDS in low and middle income countries: a priority setting exercise [^93bc4660]. AIDS and Behavior (2017). Low credibility.

Fig. 1
Study flowchart. Establishment of a management group and a core group

The next phase and the second step of the research priority setting process involved generating a systematic list of research options. Each individual that attended the meeting was asked via email to identify 2–5 individuals within the field of alcohol and HIV/AIDS that could form the expanded TWG. This resulted in a group of 174 global and local experts in the field of alcohol and or HIV/AIDS related research who were invited to each generate five research questions. This resulted in 204 initial research questions. Similar questions were combined, and duplicate questions, and those not related to HIV/alcohol were deleted. This was done to reduce the number of questions to a manageable number (50 questions) and involved four rounds of consultations between four of the authors.

A final list of 50 research options to ultimately be independently evaluated using five criteria was agreed upon (Box 1). All experts who were initially approached to submit research questions, plus any additional identified experts were invited to participate in the scoring component of the research priority setting. In total 181 experts were invited to score the 50 research options sent via email. Each scoring sheet sent out was identical. This process yielded five intermediate scores (one for each criterion; ranging between 0% and 100%). Prior to the scoring, all criteria were weighed equally, as each was of equal importance. Following five follow-up emails all of the 59 returned scoring sheets were checked for errors and then scores were entered into a master calculation sheet.

Box 1
Questions the experts applied to each research option

Each expert scored each research options by answering the questions per criterion about that particular option. The answers to each questions were a) no (0 points), b) yes (1 point) or c) not sure (0.5 points). In some instances experts may not have felt informed enough to answer a research option. In these cases answers were left blank. The methodology deals with missing answers as it is not expects that each expert has all the necessary knowledge on each possible research option to score it against the criterion. Moreover, according to the wisdom of crowds theory individuals in the rating process have the chance to express a judgement (i.e. score research items that they prioritize) and this judgement is treated equally. Naturally, these judgements include personal biases but these tend to be cancelled out or diluted.

---

### A team-based competition for undergraduate medical students to learn radiology within the virtual world second life [^a50c3d15]. Insights Into Imaging (2021). Medium credibility.

Table 2
Correct questions in the tests each week (mean ± standard deviation) during the game, together with the number of tests not delivered

Bold values indicate results of the three weeks of anatomy, the three weeks of semiology or the total of six weeks, respectively

a Percentage calculated on 15 questions for each test

b Percentage calculated with respect to 52 weekly tests

N Number of tests not delivered by students

Table 3
Correct questions in the "normal or pathological" tasks during the game, together with the number of tests and tasks not delivered

Bold values indicate results of the total "normal or pathological" (N/P) tasks

a Percentage calculated on 20 questions for each task

b Percentage calculated with respect to 13 tasks of each anatomical location, 39 in total

N Number of tests not delivered by students

The post-exposure test was carried out by 45 participants and 83 non-participants. Participants obtained superior results than non-participants, with significant differences (p = 0.009) in the semiology questions (Table 4). The percentage of answers left blank for participants and non-participants was 2.4 ± 5.2 and 4.6 ± 6.7, respectively (p = 0.007). Forty-nine participants (94.2%) and 96 non-participants (72.1%) took the final examination in the June convocation, whose grades (mean percentage ± standard deviation) were 79.2 ± 15.3 and 71.3 ± 19.5, respectively (p = 0.021). The mean of correct answers by teams showed a low correlation with the final grades (Pearson's coefficient = 0.425) but moderate correlation with the post-exposure test (Pearson's coefficient = 0.572) (Fig. 4).

Table 4
Percentages of correct answers and answers left blank in the post-test (mean ± standard deviation) one month after the game

Participants (n = 45). Non-participants (n = 83)

Fig. 4
Graph showing the correlation of the results obtained by each team in the weekly game tests with the results obtained in the post-exposure tests (light blue points; Pearson's coefficient = 0.572) and with the course grades (dark blue points; Pearson's coefficient = 0.425). The data are the average scores obtained by teams, expressed as a percentage. Error bars represent the standard deviation

---

### Lisdexamfetamine dimesylate [^c3ad0ff2]. FDA (2025). Medium credibility.

Adults: Short-Term Treatment in ADHD

A double-blind, randomized, placebo-controlled, parallel-group study (Study 7) was conducted in adults ages 18 to 55 (N = 420) who met DSM-IV criteria for ADHD. In this study, patients were randomized to receive final doses of 30 mg, 50 mg, or 70 mg of lisdexamfetamine dimesylate or placebo for a total of four weeks of treatment. All patients receiving lisdexamfetamine dimesylate were initiated on 30 mg for the first week of treatment. Patients assigned to the 50 mg and 70 mg dose groups were titrated by 20 mg per week until they achieved their assigned dose. The primary efficacy outcome was change in Total Score from baseline to endpoint in investigator ratings on the ADHD Rating Scale (ADHD-RS). Endpoint was defined as the last post-randomization treatment week (i.e. Weeks 1 through 4) for which a valid score was obtained. All lisdexamfetamine dimesylate dose groups were superior to placebo in the primary efficacy outcome (Study 7 in Table 6).

The second study was a multi-center, randomized, double-blind, placebo-controlled, cross-over, modified analog classroom study (Study 8) of lisdexamfetamine dimesylate to simulate a workplace environment in 142 adults ages 18 to 55 who met DSM-IV-TR criteria for ADHD. There was a 4-week open-label, dose optimization phase with lisdexamfetamine dimesylate (30 mg/day, 50 mg/day, or 70 mg/day in the morning). Patients were then randomized to one of two treatment sequences: 1) lisdexamfetamine dimesylate (optimized dose) followed by placebo, each for one week, or 2) placebo followed by lisdexamfetamine dimesylate, each for one week. Efficacy assessments occurred at the end of each week, using the Permanent Product Measure of Performance (PERMP), a skill-adjusted math test that measures attention in ADHD. PERMP total score results from the sum of the number of math problems attempted plus the number of math problems answered correctly. Lisdexamfetamine dimesylate treatment, compared to placebo, resulted in a statistically significant improvement in attention across all post-dose time points, as measured by average PERMP total scores over the course of one assessment day, as well as at each time point measured. The PERMP assessments were administered at pre-dose (-0.5 hours) and at 2, 4, 8, 10, 12, and 14 hours post-dose (Study 8 in Table 6, Figure 6).

---

### Changes in standard of candidates taking the MRCP (UK) part 1 examination, 1985 to 2002: analysis of marker questions [^3b7169a8]. BMC Medicine (2005). Low credibility.

Methods

Methods: Study 1

The format of the Examination, which was held three times a year, was unchanged until 2002/1, consisting of 60 five-part Multiple True-False questions (a total of 300 items). Negative marking was used in scoring the examination, and results were expressed as a 'corrected percentage correct', which takes guessing into account. The only substantive change in the Examination Regulations was that candidates were allowed to make an unlimited number of attempts from 1999/2 onwards, whereas previously they had been limited to four attempts.

Marker items were defined as any items included for a second time in any of the 43 diets of the MRCP(UK) Part 1 Examination held between 1988/1 and 2002/1, and which had been used in the previous diet with the stem, the item and the correct answer unchanged. Documents for tracing markers prior to 1988 were not readily available. The dates of the two diets, and the proportion of candidates getting each item right, wrong or not answering it, were recorded for each marker item.

Statistical analysis: Study 1

The central issue in a marker question analysis is whether, on average, aggregate performance on items has increased or decreased between the first and second usage. By assessing how such a change relates to the dates when the items were used, one can estimate the changing overall performance of candidates in the Examination. The study is a variant of an incomplete paired comparison design.

If the percentages of candidates who get an item correct or wrong on the first and second occasion are c 1, w 1 and c 2, w 2, then the change in performance of the item is:

Δ = (c 2 - w 2) - (c 1 - w 1)

It should be noted that Δ is independent of the proportion of candidates choosing not to answer an item since those not answering are effectively neutral, neither gaining a mark from a correct answer nor losing one due to a wrong answer. Given the different performance of the marker questions at different diets, one can reconstruct the changing true ability of the candidates by a process analogous to triangulation, a process that can be carried out using multiple regression (for a technical explanation see below). The method takes one arbitrarily chosen date as the reference category (the first diet of 2000 was used) and estimates the ability of the candidates at each other diet relative to the reference diet. The estimates, like all regression coefficients, have a standard error and confidence intervals.

---

### The suboptimality of perceptual decision making with multiple alternatives [^78810b2c]. Nature Communications (2020). High credibility.

To adjudicate between these three models, we first examined subjects' accuracy on the first answer. Subjects responded correctly in their first answer on 50.7% of the trials (chance level = 16.7%). Using this performance, we computed the parameters of the sensory representation as in Experiments 1 and 2 in order to generate the model predictions for the second answer. We found that task accuracy for the second answers was 29.6%. This value was greatly overestimated by the population model, which predicted accuracy of 40.9% (t (9) = 7.04, p = 6.09 × 10 −5, 95% CI = [7.7%, 15%]) (Fig. 6b). On the other hand, the Summary & Random Choice model greatly underestimated the observed accuracy (predicted accuracy = 20%; t (9) = −5.55, p = 3.55 × 10 −4, 95% CI = [−13.5%, −5.7%]). Finally, the Summary & Strategic Choice model produced the most accurate prediction (predicted accuracy = 33.3%; t (9) = 2.18, p = 0.057, 95% CI = [−0.1%, 7.7%]). On an individual subject level, the population model overestimated the accuracy of the second answer for all ten subjects, the Summary & Random Choice model underestimated the accuracy of the second answer for all of the ten subjects, whereas the Summary & Strategic Choice model was best calibrated overestimating the accuracy of the second answer for seven subjects and underestimating it for the remaining three subjects (Fig. 6c).

---

### Poor lie detection related to an under-reliance on statistical cues and overreliance on own behaviour [^7ba530ca]. Communications Psychology (2024). Medium credibility.

Comprehension check questions

All participants were asked the following seven comprehension check questions after reading the instructions (correct answers are underlined):
In each trial, you and the other player are presented with an identical set of seven cards. True False
The computer picks one card for you and the other player at random. Which of the below statements is true? The computer picks the same card for you and the other player The computer picks a different card for you and the other player. The computer randomly picked a card for the other participant and randomly picks a card for you. Your card pick is therefore not necessarily the same as the other player's.
Let's say you reported a blue card and the other participant reported red: You win £0.01. The other player wins nothing. The other player wins. You win nothing. You win £0.01. The other player loses. The other player wins. You lose £0.01.
Let's say you and the other participant both reported a blue card. What happens? Each person loses Neither one wins or loses Each person wins
Let's say you reported a red card and the other participant reported blue: You win £0.01. The other player wins nothing. The other player wins. You win nothing. You win £0.01. The other player loses. The other player wins. You lose £0.01.
Let's say you and the other participant both reported a red card. What happens? Each person loses Neither one wins or loses Each person wins
Each trial, you will see another participant's report from a previous study. True False

All questions had to be answered correctly to be allowed to continue the study. Participants had to answer correctly at least at the second attempt. Participants were allowed to review the instructions after a first failed attempt. If they failed again, they were instructed to discontinue the study.

Statistical analyses

To investigate what determines participants' suspicion levels on a trial by trial basis, we used a Bayesian 'model averaging' procedure adapted from Freckleton (2011). We reverse-coded participants' honesty ratings to reflect suspicion ratings and transformed them to be of range 0–1. Then, we defined four cues in three categories that may be associated with suspicion:

---

### Calibration of cognitive tests to address the reliability paradox for decision-conflict tasks [^714c0852]. Nature Communications (2023). High credibility.

Procedure

To be eligible to begin the study, participants had to check a box confirming they have normal, or corrected-to-normal, vision and successfully completed two multiple choice English proficiency questions consisting of a sentence missing one word (e.g. He really didn't _______ to interrupt her. Please choose only one of the following: (a) hope, (b) gain, (c) mean, (d) suppose).

A tutorial then guided participants through each component of the task pertaining to the condition they were randomly assigned to. Each component was introduced with prompts explaining the aim of the task and how to respond (e.g. respond according to the direction of the central arrow). Next, they completed the component unprompted. The prompted and unprompted blocks each consisted of four trials (including both incongruent and congruent stimuli). An error message was displayed when an incorrect response was given, and the response had to be corrected to continue. For example, in Flanker2, the standard Flanker task (with then without prompts) was introduced, then Flanker with double shots (with then without prompts) was presented. Participants received points for correct responses and the total was constantly displayed. There were no time limits imposed on trials, however, a "Hurry up!" message appeared when participants responded slowly (i.e. > 2 s).

Participants then received eight chances to successfully complete four unprompted trials of each component (e.g. Flanker and Flanker2) without error in any element, before progressing to the experiment. Again, errors were required to be corrected to advance. Where eight attempts had been made without successful completion, participants were asked to exit the study and paid for their time. Once all trials were correct in a single attempt, participants proceeded to the experiment without the need to complete any remaining attempts.

The main experiment was separated into two sessions, the first consisted of 16 blocks and the second 20 blocks. The sessions were divided by a compulsory 10-minute break; however, participants were free to break for longer if desired. The bonus accumulated throughout the experiment and was visible to participants at the end of each block. This included the amount they earned in the preceding block as well as a running total. At the conclusion of both sessions, participants were informed of the total bonus they had accumulated and were given a completion code for MTurk.

---

### Lisdexamfetamine (lisdexamfetamine dimesylate) [^8c5d9596]. FDA (2025). Medium credibility.

Adults: Short-Term Treatment in ADHD

A double-blind, randomized, placebo-controlled, parallel-group study (Study 7) was conducted in adults ages 18 to 55 (N = 420) who met DSM-IV criteria for ADHD. In this study, patients were randomized to receive final doses of 30 mg, 50 mg, or 70 mg of lisdexamfetamine dimesylate or placebo for a total of four weeks of treatment. All patients receiving lisdexamfetamine dimesylate were initiated on 30 mg for the first week of treatment. Patients assigned to the 50 mg and 70 mg dose groups were titrated by 20 mg per week until they achieved their assigned dose. The primary efficacy outcome was change in Total Score from baseline to endpoint in investigator ratings on the ADHD Rating Scale (ADHD-RS). Endpoint was defined as the last post-randomization treatment week (i.e. Weeks 1 through 4) for which a valid score was obtained. All lisdexamfetamine dimesylate dose groups were superior to placebo in the primary efficacy outcome (Study 7 in Table 6).

The second study was a multi-center, randomized, double-blind, placebo-controlled, cross-over, modified analog classroom study (Study 8) of lisdexamfetamine dimesylate to simulate a workplace environment in 142 adults ages 18 to 55 who met DSM-IV-TR criteria for ADHD. There was a 4-week open-label, dose optimization phase with lisdexamfetamine dimesylate (30 mg/day, 50 mg/day, or 70 mg/day in the morning). Patients were then randomized to one of two treatment sequences: 1) lisdexamfetamine dimesylate (optimized dose) followed by placebo, each for one week, or 2) placebo followed by lisdexamfetamine dimesylate, each for one week. Efficacy assessments occurred at the end of each week, using the Permanent Product Measure of Performance (PERMP), a skill-adjusted math test that measures attention in ADHD. PERMP total score results from the sum of the number of math problems attempted plus the number of math problems answered correctly. Lisdexamfetamine dimesylate treatment, compared to placebo, resulted in a statistically significant improvement in attention across all post-dose time points, as measured by average PERMP total scores over the course of one assessment day, as well as at each time point measured. The PERMP assessments were administered at pre-dose (-0.5 hours) and at 2, 4, 8, 10, 12, and 14 hours post-dose (Study 8 in Table 6, Figure 6).

---

### Performance of infectious diseases specialists, hospitalists, and other internal medicine physicians in antimicrobial case-based scenarios: potential impact of antimicrobial stewardship programs at 16 veterans' affairs medical centers [^59f91f64]. Infection Control and Hospital Epidemiology (2023). Medium credibility.

For each scenario, responses were scored by assigning +1 for answers most concordant with Infectious Diseases Society of America guidelines at the time (ie, "correct"),– 0 for less concordant but acceptable answers (or no answer given), and −1 for guideline-discordant answers (ie, "incorrect"). Guidelines were interpreted with emphasis on antimicrobial stewardship and practicality. One generalist (P. A.G.) and 2 infectious diseases physicians (C.J.G. and M.B.G.) collectively assigned a value to each answer to each subquestion a priori with free-text responses analyzed post hoc independent of knowledge of the type of practitioner giving the answer. For questions that allowed for multiple answers, 0 points were assigned when a less guideline-concordant answer was combined with a guideline-concordant answer, and −1 point was assigned when a guideline-discordant answer was combined with either a guideline-concordant or less guideline-concordant answer. Scores were then compiled across all questions within each scenario and were normalized from 100% concordant (all "correct") to 100% discordant (all "incorrect"). Mean scores were calculated across respondents who self-identified as belonging to 1 of 3 categories: infectious diseases (ID) specialists, hospitalists, and other internists (general internal medicine and non-ID internal medicine subspecialists). For each question within a scenario, we tabulated percentages of responses based on the total number of survey participants in each physician category rather than the number in each category that responded to the individual question. Statistical significance of differences between groups were calculated using the Kruskal-Wallis rank-sum test, Pearson χ 2 test, or the F test where appropriate. This study was approved by the Veterans' Health Administration Central Institutional Review Board.

---

### Three diverse motives for information sharing [^97a4fba4]. Communications Psychology (2024). Medium credibility.

Information-seeking task

Following instructions, participants answered six comprehension questions before the first block and one question before the second block. Participants who responded incorrectly twice on at least one question were not permitted to go on to complete the task. After reading the instructions, participants completed two example trials before starting the actual task.

The task is illustrated in Fig. 1a. Recipients were told they owned 100 stocks in a financial market we created. On each trial recipients were presented with an algorithm's prediction of the value of their stocks and the algorithm's average prediction accuracy (the algorithms could be different on each trial). These cues were presented for 5 s. Predictions regarding the stocks' value ranged from −400 to −500 and from +400 to +500. A positive stock value meant recipients were earning money, a negative value meant they were losing money. We use the word 'valence' to indicate both the direction of the change in stock's value (positive or negative) and the magnitude of such change. The algorithm's prediction accuracy ranged from 0 to 99%, high numbers suggest the algorithm is often correct and vice versa. The algorithm's accuracy represents the probability of the revealed stocks' value being the real value. When the prediction is incorrect, the stocks' value is uniformly distributed over the possible values. Stocks' value and uncertainty level were randomly sampled for each trial. Recipients then indicated whether they wanted to open an envelope containing information about the true value of their stocks (information-seeking decision). They did so using a seven-point Likert scale ranging from 0, "Not at all", to 6, "Very much". We adopted a 7-point Likert scale rather than a binary response scale to increase the sensitivity of our measure. Recipients were told that the closer their answer was to "Very much", the more likely we were to open the envelope and reveal the value of their stocks, and vice versa (similar to other studies. If they selected 0, 1, 2, 3, 4, 5, or 6, then the information was delivered with a probability of 5%, 20%, 35%, 50%, 65%, 80%, and 95%, respectively. This probabilistic approach, which is adapted from past studies, is reflective of real life where actions are often not deterministically related to outcomes (e.g. one may ask a question but not receive an answer and may need to ask again if they really want to know). Recipients were not aware of these exact mathematical conversions as we did not want them to focus their attention on exact mathematical calculations to avoid distraction. Next, either the value of their stocks (information) was presented on screen for 4 s or hidden ('XX' was shown).

---

### Writing good true-false questions: a brief guide for radiologists [^7e0fd687]. Radiographics (2018). Low credibility.

This guide provides a brief how-to compilation of widely accepted recommendations for creating good true-false questions for self-assessment continuing medical education exercises.

---

### Artificial intelligence in clinical practice: a cross-sectional survey of paediatric surgery residents' perspectives [^9238894a]. BMJ Health & Care Informatics (2025). High credibility.

Phase II: comparative performance analysis

In this phase of the study, we deployed a multiple-choice questionnaire encompassing 20 questions randomly extracted from a comprehensive pool of 81 potential queries. These questions were sourced from renowned databases, including the 'European Board of Pediatric Surgery 2017', 'EPSITE' and a compilation of '300+Top Pediatric Surgery Objective Questions and Answers'. Each query was engineered to have a singular correct response among five options (A–E), deliberately excluding image-based questions to accommodate the limitations of ChatGPT-3.5. The same set of questions was administered to the residents and ChatGPT-3.5 and ChatGPT-4, respectively, with scoring criteria meticulously outlined. Each correct answer earned 1 point, while each incorrect answer incurred a penalty of −0.25 points, and unanswered questions received 0 points. On ChatGPT-4.0 and ChatGPT-3.5, the multiple-choice questions were submitted into the chat by copying and pasting. Each time, the questions were asked following the same prompting, and before each attempt, the previous chat was deleted, the internet page was closed, the browser's chronology and cookies were deleted, and the cache was emptied.

Phase III: postexposure acceptance re-evaluation

Following the comparative analysis, participants were exposed to the mean scores and accuracy percentages derived from the residents and ChatGPT entities. Subsequently, they were prompted to repeat the initial UTAUT2 Questionnaire, enabling a comparative insight into any shifts in AI acceptance post exposure. To evaluate residents' responses to the UTAUT2 Questionnaire at two time points (T1 = before the multiple-choice question test and T2 = after the multiple-choice question test), a comprehensive Cronbach's alpha reliability analysis was initially conducted to assess the consistency and effectiveness of each measure of the questionnaire's constructs. Subsequently, the individual items were combined for each construct to generate aggregate measures of the specific construct. This process was carried out for UTAUT2 at both T1 and T2. A paired-sample Wilcoxon test was employed to compare median values for each construct at T1 versus T2.

---

### Theoretical and practical training improves knowledge of the examination guidelines of the international standards for neurological classification of spinal cord injury [^954cbbfd]. Spinal Cord (2022). Medium credibility.

Fig. 1
Comparison between pre-course and post-course test results of questions 1–6 (Q1–Q6).

X-coordinates denote all selected keys (correct answers) and distractors (wrong answers). Keys are highlighted by a surrounding rectangular frame. Y-coordinates and the numbers above the columns reflect the count of selected distractors and keys. Question type "multiple response/Kprime" allows more than one possible key, while question type "multiple choice" is characterized by only one key and four distractors. In all questions, the correct keys and combination of keys were the most frequently chosen answers. All questions and their contents are completely listed in Table 1. Note that Q6 has been removed after the first course. n.r. no response.

Fig. 2
Illustration of results related to the questions that were only part of the post-course test (Q7–Q10).

X-coordinates denote all selected keys (correct answers) and distractors (wrong answers). Keys are marked by a rectangular frame. Y-coordinates and the numbers above the columns reflect the count of selected distractors and keys. Question type "multiple response/Kprime" allows in these particular cases two correct keys, while question type "multiple choice" is characterized by only one key and four distractors. Subfigures Q7 and Q9 illustrate both the results of the questions before (Q7A/Q9A) and after (Q7B/Q9B) their modification. Except for the unmodified question Q7A, all other questions were correctly answered in most instances. After modification, attendees have most frequently chosen the correct answers for Q7B and Q9B. Both the unmodified and the modified questions are listed in detail in Table 1. n.r. no response.

---

### Why questionnaire scores are not measures: a question-raising article [^2b8ac89e]. American Journal of Physical Medicine & Rehabilitation (2023). Medium credibility.

Repeatability Inflates Reliability if Items Are Too Easy or Too Difficult

Repeatability (not reliability) is easier to achieve if the difference between subjects' abilities and item difficulties is large. If a test is too difficult or easy for most subjects, total scores will be highly repeatable across raters and time points. Paradoxically, indexes based on repeatability of raw scores will be higher the less information the scale provides. If one already knows that a subject will not pass (or fail) a given item, that item is uninformative.

Things may be even subtler than that: repeatability may be inflated by local gaps in the ruler, thus being different depending on the overall ability of the individual subject. Suppose the actual "ruler" representing a questionnaire is depicted in Figure 3. In order of increasing difficulty, items are labeled from 0 to 6. Score 3 encases a wide range of ability levels. This situation will make this score highly "repeatable" across raters, time, etc. However, the measures' precision is low. Rasch analysis does not inflate reliability by this kind of forced repeatability.

FIGURE 3
A ruler with irregularly spaced "ticks", representing items marking ranges of "ability", flagged by ordered numerals. The position of the bottom symbols along the trait continuum gives subjects' ability levels. Subjects "trapped" within the "3" ability interval (degree symbols) will likely receive the same score regardless of the raters, the time of assessment, etc. given that a massive change in ability is required to change to a score of 2 or 4. By greater force, subjects scoring "2" will hardly leap to score "4", and subjects achieving "4" will hardly leap to score "2". Randomness will hardly lead to a score change. A "reliability" index will be inflated while giving less information on the actual ability level of subjects: what seems a virtue is a fault.

---

### Neural representation precision of distance predicts children's arithmetic performance [^5ef7ab75]. Human Brain Mapping (2025). Medium credibility.

2.4 Arithmetic Fluency Test

The basic arithmetic fluency test assessed the subject's math performance using timing arithmetic tasks, including addition, subtraction, multiplication and division [adapted from (Wei et al.)]. The Split‐half reliability for the arithmetic assessment was 0.98 (calculated based on the dataset of Li et al.). During the test, a math problem was presented at the center of the screen, with two answer choices displayed below it. For addition and subtraction problems, the deviation of the incorrect answer from the correct one was ± 1 or ± 2. For multiplication and division, the deviation was ± 2 to ensure participants could not rely on clues such as odd or even patterns to identify the correct answer. Participants press "Q" or "P" to select the correct answer, corresponding to the left or right position respectively. Each arithmetic task was timed at 50 s, resulting in a total test of 200 s, with an additional practice section before the formal test. The program recorded the subject's accuracy (ACC) and RT for each problem.

The subject's performance on each type of problem was compared with the dataset of primary school students in the city (Li et al.). The subject's arithmetic performance index (Z_arithmetic) was calculated as follows:where, ADD, SUB, MUL, and DIV are respectively the difference between the number of correct and incorrect problems for each condition. TIME is the time taken to complete each type of problem. Ave and Sd are the average and standard deviation of the group of primary school students for each grade (Table S1).

---

### How to find answers to clinical questions [^7cb7fcd9]. American Family Physician (2009). Low credibility.

Many barriers exist to finding answers to physicians' clinical questions. Lack of time, resources, and computer skills, as well as physicians' environment and attitudes about problem solving, all contribute to unanswered questions. Making use of computer-based information resources can give physicians a framework for answering questions and keeping their practice consistent with the best available evidence.

---

### Clinical consensus statement: balloon dilation of the sinuses [^60d74d91]. Otolaryngology — Head and Neck Surgery (2018). Medium credibility.

Delphi survey methodology and consensus thresholds — panel process and scoring definitions are as follows: A modified Delphi survey method with multiple anonymous surveys was used to assess consensus, administered via web-based software, and a potential topic list of 39 questions was developed; each panel member could provide 1 draft statement for each of their top 5 ranked topics, the survey period was broken into 2 Delphi rounds, and the first Delphi survey consisted of 48 statements. Responses used a 9-point Likert scale with labeled anchors (1 = strongly disagree, 3 = disagree, 5 = neutral, 7 = agree, and 9 = strongly agree). Consensus required a mean score of 7.00 or higher with no more than 1 outlier, defined as any rating 2 or more Likert points from the mean in either direction; near consensus required a mean score of 6.50 or higher with no more than 2 outliers; strong consensus was a mean Likert score of 8.00 or higher with no outliers; and statements that did not meet the criteria of consensus or near consensus were categorized as no consensus.

---

### Impaired kidney function is associated with lower cognitive function in the elder general population. results from the good aging in Sk å ne (G å S) cohort study [^1c78121d]. BMC Geriatrics (2019). Medium credibility.

Meta-memory was assessed using a confidence judgement test. In this test the participant was presented with 10 general questions on a piece of paper and was assigned to choose 1 of 2 written answers to each question. The questions were of the following kind:" From which language does the word" alcohol" originate? A Greek B Arabic". The participant thereafter was assigned to report how" certain" he/she was of have answered each of the questions correctly, by choosing a denary in percent between 50 and 100%, where 50% represented total uncertainty and 100% represented complete certainty of having answered correctly. The following calibration formula, described by Dahl et al. was used to estimate the confidence:where n represents the total number of answered questions, T represents the number of confidence levels (6 in this case), nt represents the number of times the confidence level rt was reported, and ct represents the portion of correct answers to the total number of answers where confidence level rt was reported. If the calibration value from the formula is 0, the calibration is perfect, that is, the confidence of the participant of having answered correctly is in line with how correctly the participant in fact did answer. The bigger the confidence value from the formula, the more the participant is misjudging his/her ability to answer the questions correctly.

---

### Dextroamphetamine sulfate, dextroamphetamine saccharate, amphetamine aspartate monohydrate, and amphetamine sulfate (Mydayis) [^f8588a38]. FDA (2025). Medium credibility.

Pediatric Patients (13 to 17 years) with ADHD

A 4 week, randomized, double-blind, multicenter, placebo-controlled, dose-optimization, safety and efficacy study (Study 4) was conducted. In Study 4, the 157 pediatric patients 13 to 17 years old who met DSM-IV TR criteria for ADHD, were randomized in a 1:1 ratio to MYDAYIS or placebo group. Subjects were titrated from a dose of 12.5 mg/day until an optimal dose was reached (up to a maximum dose of 25 mg); this dose was maintained during the dose-maintenance period (Study 4 in Table 4).

The primary efficacy endpoint was defined as the change from baseline of the ADHD-RS-IV Total Score at Week 4. The baseline ADHD-RS-IV Total Score was defined as the last valid ADHD-RS-IV Total Score assessment prior to taking the first dose of double-blind investigational product, usually at Visit 2. MYDAYIS demonstrated a statistically significant treatment effect compared with placebo on the change of ADHD RS-IV total scores from baseline at Visit 6 (Week 4). MYDAYIS also showed statistically significantly greater improvement on the Clinical Global Impression of Improvement (CGI-I) score at Visit 6 (Week 4).

A multicenter, randomized, double-blind, placebo-controlled, crossover study of MYDAYIS 25 mg/day (Study 5) was conducted in adolescent patients who met DSM-IV TR criteria for ADHD. The efficacy was determined using the Permanent Product Measure of Performance (PERMP), a skill-adjusted math test that measures attention in ADHD. PERMP total score results from the sum of the number of math problems attempted plus the number of math problems answered correctly. Efficacy assessments were conducted at 2, 4, 8, 12, 14, and 16 hours post-dose using the PERMP. MYDAYIS treatment, compared to placebo, reached statistical significance at 2 to 16 hours post-dose (Study 5 in Table 4, Figure 2).

Figure 2: LS Mean (SE) PERMP Total Score by Treatment and Time-Point for Adolescents Ages 13 to 17 with ADHD After 1 Week of Double Blind Treatment (Study 5)

LS Mean: least-squares mean; SE: standard error

In both adults and pediatric patients, examination of a population subset based on gender or race did not reveal any differences.

---

### Made to measure: designs tailored to your study needs [^2eeda2ac]. The Journal of Foot and Ankle Surgery (2015). Low credibility.

What is the best study design? Why would we prefer one to another? With many different designs, what should we be looking for in our critical reading of the literature? We discuss a variety of designs, and their appropriate use.

---

### Asking a question: linking evidence with practice [^36b2bc29]. The Journal of Orthopaedic and Sports Physical Therapy (2018). Low credibility.

For clinicians and researchers alike, a clear and answerable question is the starting line of the evidence-based practice process. For clinicians, the question captures the patient and defines the information needed to treat this patient. For researchers, the question directs study design and drives everything from recruitment to reporting. The question is also the anchor point that links clinical practice to research. Engaging in evidence-based practice forces a subconscious process that occurs during assessment of the patient into the conscious mind and makes it explicit. Clearly and concisely defining the question leads directly to the search for relevant research. J Orthop Sports Phys Ther 2018;48(7):596–597. doi:10.2519/jospt.2018.0702.

---

### Evaluating search engines and large language models for answering health questions [^6b4a5e23]. NPJ Digital Medicine (2025). Medium credibility.

We also wanted to analyze the extent to which retrieval results provide an actual answer to the health questions. Ranked lists contain off-topic results and, additionally, many on-topic pages do not provide a clear response to the user's request. For each of the 50 health questions available in each collection, we processed its top retrieved results to identify whether they answered the health question. This identification was automatic, supported by passage extraction and reading comprehension technology (see "Methods"). For example, for Google's rankings and n = 1, we computed the number of entries that actually provided a response to the health query. The same count was obtained for each position (up to rank #20) and, finally, a global answering score was obtained by averaging the number of questions that were actually answered at each position. These scores, plotted in Fig. 2, are much lower than the number of questions in these collections (50), reflecting that many retrieval results do not provide a clear yes/no recommendation to the health question. According to this experiment, Bing is the engine whose rankings provided more answers for 2020 questions (around 30 questions answered) and, from Fig. 1 a we infer that Bing's responses tend to be slightly better than those provided by the three other SEs. For 2021, Google and DuckDuckgo supplied responses to more questions but at the cost of providing more incorrect responses throughout the ranking (Fig. 1 b). Finally, for 2022 questions, Google provided many responses and, according to Fig. 1 c, it was not inferior to the other SEs. Note also that, on average, between 15 and 20 questions did not receive a response at any given ranked position. This suggests that restricting the analysis to a few top-ranked web pages, which is the common behavior of end users, often results in a failure to meet these health information needs.

---

### Summary benchmarks-full set – 2024 [^6f68d3ad]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — use and ethics note states that "The PPPs are intended to serve as guides in patient care, with greatest emphasis on technical aspects", that "true medical excellence is achieved only when skills are applied in a such a manner that the patients' needs are the foremost consideration", and that "The Academy is available to assist members in resolving ethical dilemmas that arise in the course of practice".

---

### Guidelines on genetic evaluation and management of Lynch syndrome: a consensus statement by the Us Multi-society Task Force on Colorectal Cancer [^f59beb62]. Gastroenterology (2014). Medium credibility.

Colorectal cancer risk assessment tool — using a simple 3–question screen — when all 3 questions were answered "yes", correctly identified 95% of individuals with germline mutations causing Lynch syndrome, and the cumulative sensitivity was 77% to identify patients with characteristics suggestive of hereditary CRC who should undergo a more extensive risk assessment.

---

### Three questions to ask before using model outputs for decision support [^276900e2]. Nature Communications (2020). High credibility.

A way forward

Models tend to look right as they are presented with the claim of being realistic enough for their purpose. Looking right, though, has many dimensions, and we suggest the three screening questions presented here as a simple approach for decision makers to disentangle them. Once we know a model's purpose, we can assess whether its organization is meaningful; once we know its purpose and organization, we can assess whether it represents the real system dynamics relevant to a specific problem; once we know which patterns it can reproduce, we can assess whether its model outputs can support decisions; and once we have answered all three questions for a suite of models they can be compared and taken into account to varying degrees. The three questions do not replace more detailed guidelines on GMP, but they provide a simple and effective common language that will allow us to develop models and use their outputs for decision support in a more transparent, robust, and safe way.

It is still important to keep in mind that models can only support decisions. The responsibility for using model outputs lies with decision makers. Answers to the three screening questions allow them to transparently base their decisions on a weight-of-evidence approach, and to update their decisions when new data and updated models are available.

---

### Selecting the right medical student [^4f36de75]. BMC Medicine (2013). Low credibility.

Conclusions

Despite these reservations, these papers are a useful contribution to the debate on medical student selection. The size of the sample and the variety of the cohorts suggest that the results may be generalizable. The thoroughness and attention to detail in the analysis is an example of good practice and lends authority to the conclusions. On this evidence, assessment of educational attainment will always be a necessary, but not sufficient, part of the selection process. Further studies are needed to determine what the other components should be.

---

### Toward expert-level medical question answering with large language models [^67293049]. Nature Medicine (2025). Excellent credibility.

Overlap analysis

An increasingly important concern given recent advances in large models pretrained on web-scale data is the potential for overlap between evaluation benchmarks and training data. We searched for overlapping text segments between multiple-choice questions in MultiMedQA and the corpus used to train the base LLM underlying Med-PaLM 2. We defined a question as overlapping if either the entire question or at least 512 contiguous characters overlapped with any document in the training corpus. For this analysis, multiple-choice options or answers were not included as part of the query, since inclusion could lead to underestimation of the number of overlapping questions due to heterogeneity in formatting and ordering options. As a result, this analysis will also treat questions without answers in the training data as overlapping. We believe this methodology is both simple and conservative, and when possible we recommend it over black-box memorization testing techniques, which do not conclusively measure test set contamination.

Long-form consumer question-answering evaluation

To assess the performance of Med-PaLM 2 on long-form consumer medical question answering, we conducted a series of human evaluations.

Model answers

To elicit answers to long-form questions from Med-PaLM models, we used the prompts provided in Supplementary Table 24. We did this consistently across Med-PaLM and Med-PaLM 2. We sampled from models with temperature 0.0 as in ref.

Physician answers

Physician answers were generated as described in ref. Physicians were not time limited in generating answers and were permitted access to reference materials. Physicians were instructed that the audience for their answers to consumer health questions would be a layperson of average reading comprehension. Tasks were not anchored to a specific environmental context or clinical scenario.

---

### Is health politically irrelevant? Experimental evidence during a global pandemic [^87a753fa]. BMJ Global Health (2020). High credibility.

What is the estimated number of fatalities from COVID-19 in the USA, as stated on the previous page? (If you are not sure, check back on the previous page.)

It was more than 2 million people
It was less than 2 million people

Follow-up questions are constructed so that the correct answer is the highest — (a) rather than (b) — so as to enhance the strength of the treatment. Respondents must answer this follow-up question correctly in order to proceed through the survey.

Two outcome questions gauge the possible impact of these treatments on political behaviour. The first centres on the incumbent: If a national election were held today, would you like to see [Johnson and the Conservative party/Trump and the Republican party/Modi and the BJP] reelected? The second asks whether the government is at fault for allowing the pandemic to spread. Responses are registered on a 100-point feeling thermometer.

Two hypotheses will guide our discussion. (Initial research hypotheses centred on expected variation between the two treatments, as set forth in a detailed pre-analysis plan, registered at E-Gap: Finding no difference between the treatments, we decided to focus this paper on the treatment/control comparison. However, it is important to note that results from the pre-analysis plan are entirely consistent with results presented here: null findings obtained throughout.)

H 1: Respondents who receive either of the treatment conditions will offer greater — or lesser — support for the incumbent relative to the control group.

H 2: Respondents who receive either of the treatment conditions will be more inclined to blame the government for their handling of the pandemic.

To test these hypotheses, we adopt an experimental design that contrasts two treatment conditions with the control condition, as described. We estimate causal effects through the following regression model, which includes background covariates in order to yield greater precision:

where Outcome is the measurement of the response to the scenario to which the person i was exposed, denoted by T, SES denotes the socioeconomic factors, C stands for demographic factors, and Country represents dummies for the three countries.

---

### Survey design: to ask or not to ask? That is the question [^a93395c9]. Clinical Infectious Diseases (2013). Low credibility.

Surveys are one of the most frequent modes of observation and measurement. Survey research can be exploratory, descriptive, and/or explanatory. Modes of survey administration vary and include face-to-face, telephone, mail, and Internet. There are numerous considerations that that must be taken into account when designing and conducting a survey. Deciding what information is needed and why is important when developing questions for a survey; it is essential to define the purpose of the survey and to be as specific as possible in terms of the data to be collected. Response rate varies by target audience and is influenced by a number of factors such as incentives, survey length, and perceived burden. From question development to survey construction, the goal is to minimize measurement error with systematic planning and execution.

---

### Surveys: an introduction [^18c7b4f6]. Respiratory Care (2004). Low credibility.

Surveys are a valuable research tool for studying the knowledge, attitudes, and behavior of a study population. This article explores quantitative analyses of written questionnaires as instruments for survey research. Obtaining accurate and precise information from a survey requires minimizing the possibility of bias from inappropriate sampling or a flawed survey instrument, and this article describes strategies to minimize sampling bias by increasing response rates, comparing responders to nonresponders, and identifying the appropriate sampling population. It is crucial that the survey instrument be valid, meaning that it actually measures what the investigator intends it to measure. In developing a valid survey instrument, it can be useful to adapt survey instruments that were developed by other researchers and to conduct extensive pilot-testing of your survey instrument.

---

### Introspective access to value-based multi-attribute choice processes [^19f70ba5]. Nature Communications (2025). High credibility.

Then, critically, we asked participants to rate the extent to which they believed the heuristic accurately described how they made their choices, on a continuous scale from "Strategy A much better describes how I made my choices" to "Strategy B much better describes how I made my choices". Finally, as an exploratory question, we asked participants to rate whether Strategy A or B better described how they believed one should make their choices in Part 1; we do not analyze this question in this manuscript.

Measuring people's beliefs about their attribute weights is complicated by the fact that participants might represent different formats of attribute weights depending on which heuristics they are using. A participant using the classically rational process (i.e. none of the heuristics in Table 1) will represent graded weights (i.e. from 0 to 1); a person using the binary weights heuristic will represent binary weights (i.e. each attribute has a weight of 1 or 0); and a person using the single attribute heuristic will represent weights that equal 1 for one attribute and 0 for the others.

To elicit participants' self-reported attribute weights without assuming which heuristic(s) they were using, we asked them to report their weights in all formats: graded, binary, and single-attribute (the order of the three types was randomized). To elicit graded attribute weights, we asked participants, for each attribute, to report the extent to which they had cared about that attribute when making their choices (on a 100-point scale from "Didn't care about it at all" to "It completely determined my choices", with 9 labels to mark spots on the scale; see SI 1.2 for exact scale). To elicit binary attribute weights, we presented a list of the attributes with check boxes next to them, and asked participants to check which attributes they had cared about when making their decisions. To elicit single attribute weights, we presented the list of attributes with radio buttons next to them, and asked participants to select which single attribute they had most used to make their decisions.

---

### Tips for a physician in getting the right job, part VIII: additional types of questions during an executive interview [^08eb3be6]. Clinical Nuclear Medicine (2014). Low credibility.

The applicant for an administrative position may be asked situational and stress questions on an interview. Some possible questions and answers for these types of queries are included in this article. In addition, the candidate may be required to answer some odd or unusual questions to test his/her ability to be creative or respond to an unexpected question. Some suggestions regarding those queries are also given in this article.

---

### Skilled maneuvering: evaluation of a young driver advanced training program [^e2141f35]. The Journal of Trauma and Acute Care Surgery (2022). Medium credibility.

Pretest and Posttest Results

The central focus of the outcome evaluation was the analysis of the matched student pretests and posttests (N = 649) consisting of multiple-choice options. There was a statistically significant increase in knowledge from pretest to posttest (Table 3 and Table 4). The aggregate score increased by 31.0% from pretest (43.9%) to posttest (74.9%). The greatest improvement in pretest and posttest correct answers on knowledge questions for the Classroom and Car Maintenance modules occurred in knowledge of the cause of tire failure ("blowout") (+65.5%), frequency of fatal crashes caused by teen drivers (+45.0%), and cause of most police reported motor vehicle crashes (+39.8%) (Table 3). In the Driving modules, the greatest improvement in pretest and posttest correct answers on knowledge questions occurred in the benefits of automatic braking system (ABS) brakes (+55.7%), how to brake a car without ABS (threshold braking) (+50.4%), and the definition of oversteering (+47.5%) (Table 4).

TABLE 3
Classroom and Car Maintenance Module Knowledge Questions (N = 649)

TABLE 4
Driving Module Knowledge Questions (N = 649)

There was a statistically significant increase on all knowledge questions except one. This question asked students about the leading cause of death among teenagers (MVCs) and was answered correctly by 640 students (98.6%) on the pretest and 646 students (99.5%) on the posttest. The small difference in the proportion of correct answers between pretest and posttest was not significant (χ 2 (1) = 2.083, p = 0.146). The question on the cause of tire failure or "blowout" (improper tire pressure) had the greatest difference in correct answers between pre- and posttest with a + 65.5% change (χ 2 (1) = 415.187, p < 0.0001).

Students were asked to identify the fluid types present in a vehicle, and this was scored out of a maximum possible score of six. The mean score of students identifying a correct type of fluid in the posttest was significantly higher than the mean score in the pretest (2.5 ± 1.93 vs. 5 ± 1.53; p < 0.0001) (Table 3).

---

### Towards building multilingual language model for medicine [^eb67f7e6]. Nature Communications (2024). High credibility.

Then, to better evaluate the performance of multilingual medical models, we further present a comprehensive multilingual medical Question and Answering Benchmark (MMedBench). We start by delving into its core attributes, including the total number of training and testing cases, the distribution of answer options, and the average length of question-and-answer tokens. Figure 3 a illustrates these fundamental characteristics, highlighting that MMedBench often includes questions with multiple correct options, which brings complexity for models to navigate. Additionally, the answers contain rationale sections averaging 200 tokens each. This substantial token count serves two purposes: it helps to train language models by exposing them to extended reasoning passages, but also in evaluating their ability to generate and understand lengthy, complex reasoning statements.

Fig. 3
Statistic results on MMedBench.

a The bar plot shows the foundation statistic number on the train and test set of MMedBench. The term "Avg. tokens" represents the mean token length per sample across various compositions in it. "Rationale" denotes the rationale sentences in answer. "Option" denotes the option descriptions in choice list and "question" denotes the question sentences. Then the term "Prop. of multi-option" denotes the proportion of the question with multiple correct options and "Prop. of single-option" denotes the proportion of those with one options in answer. The final term "Number of QA pairs" denotes how many QA pairs are in train or test split. b The statistic histogram shows the topics distribution in the test split of MMedBench, covering a wide range of medical aspects, ranging from general and specialized medicine to basic medical sciences. This allows MedQA to comprehensively measure the performance of medical models. Source data are provided as a Source Data file.

---

### Long-distance exploration in insightful problem-solving [^f9dd11f7]. Communications Psychology (2025). Medium credibility.

Exploration distance vs Aha moment

We further analyze the TT-RAT to examine the relationship between the exploration distance and Aha moment. The average accuracy was 51.8 ± 14.3% (mean ± std, n = 105 participants) (Fig. 6A), exceeding the 39.1% observed in the original setup (as shown in Fig. 5B). This improvement is deliberate, resulting from the use of new questions with smaller d answer values (see details in Methods). Additionally, for the trials answered correctly, participants recorded an average of 1.82 ± 0.59 thoughts per question, not including the answer itself (see Fig. 6B). For trials with no answers or wrong answers, participants reported an average of 3.61 ± 1.12 thoughts per question, which was significantly higher than the number reported for trials with correct answers (t(104) = 20.90, p = 5.13e−39, paired t -test, n = 105 participants). This suggests that participants experienced greater difficulty while searching for solutions.

Fig. 6
Exploration distance vs Aha moment in TT-RAT.

A The mean accuracy for all participants. These data are from n = 105 participants. B The number of thoughts for all participants. The mean and the standard devastation are shown. Gray circles represent the data from individual trials. C The correlation between Explore_dist and the Aha scale (left). Each circle represents a trial from a participant. For visualization purposes, a random jitter is added at the Aha scale. The Spearman's rank correlation coefficient and the corresponding p-value are shown, and a fitting red line is shown for illustrative purposes. The Explore_dist in No-Aha and Aha trials (right). The mean and the corresponding 95% confidence interval and a violin plot, with individual points (black circles) and outliers (black crosses), are shown. For the comparison, the p-value, Bayes factor BF 10, and Cohen's d are shown on top. D The correlation between the number of thoughts taken to reach the solution and the Aha scale (left), and the number of steps taken in No-Aha and Aha trials (right). The same presentation is used as in panel C.

---

### Real-time decoding of question-and-answer speech dialogue using human cortical activity [^f78af514]. Nature Communications (2019). High credibility.

We used the following three types of task blocks: (1) question (perception) training, in which participants heard each question 10 times in a random order (stimulus length varied from 1.38–2.42 s in duration with an onset-to-onset interval of 3 s); (2) answer (production) training, in which participants read each possible answer choice aloud 10 times in a random order (each answer appeared on the screen with a gray background for 0.5 s, was changed to a green background for 1.5 s to represent a go cue for the participant to read the answer, and removed from the screen for 0.5 s before the next answer was displayed); and (3) testing, in which participants heard questions and responded verbally with answers (choosing a response from the possible options presented on the screen after each question). During the testing blocks, a green circle appeared on the screen after each question was presented to cue participants to respond aloud with an answer of their choosing. We encouraged the participants to choose different answers when they encountered the same questions, although they were free to respond with any of the presented answer choices during each trial. There was 2–3 s of silence and a blank screen between each trial. In each block, the questions played to the participant were chosen based on how many questions and answers are in each QA set (questions with more valid answers had a greater chance of being played in each trial). Trials in which the participant failed to respond or responded with an invalid choice (less than 0.5% of trials) were excluded from further analysis. There were 26 question-and-answer trials in each testing block.

During each block, time-aligned behavioral and neural data were collected and stored. The data collected during training blocks were used to fit the decoding models. The data collected during testing blocks were used to decode the perceived questions and produced answers in real-time and were also used offline during hyperparameter optimization.

---

### Public understanding of female genital anatomy and pelvic organ prolapse (POP); a questionnaire-based pilot study [^4542b09c]. International Urogynecology Journal (2022). Medium credibility.

Outcome assessment

Anatomical understanding

Participants were scored out of seven for the number of correct anatomical labels and out of three for correct number of female genital "holes". Response assessment took into account the use of layman language. For example "peehole" and "bumhole" were accepted as correct labels for the urethra and anus respectively. If either labia majora or minora was labelled correctly, then the participant was labelled as having understood this concept of labia in general.

POP understanding

POP understanding (in addition to understanding of other diseases) was categorized as 'any understanding' versus 'no understanding' based on the participant free-text response. Accepted definitions of disease (POP in addition to diabetes, stroke, fibroids) understanding are summarized in Table 1. For blank responses, if at least one other disease question was answered, the response was considered as "no understanding". If responses to all four disease questions were left blank, this was reported as missing data. "Prolapse" understanding at this part of the questionnaire was analysed separately from the in-depth POP questions asked later in the questionnaire, i.e. a participant could have a poor definition of prolapse at this part but answer some in-depth POP questions correctly, which were judged separately as correct.

---

### Principles of teaching evidence-based medicine [^7ff06ee6]. Injury (2006). Low credibility.

Teaching EBM is to impart the process of asking the question, acquiring and appraising the literature and applying it to the care of the patient, while weighing the risks, benefits and considering patient values. Teaching this process and its necessary content requires as with everything else, dedication, knowledge and practice. The best way to teach both patient care and EBM is by example.

---

### Evaluating large language models on aerospace medicine principles [^a70e7a6e]. Wilderness & Environmental Medicine (2025). Medium credibility.

IntroductionLarge language models (LLMs) hold immense potential to serve as clinical decision-support tools for Earth-independent medical operations. However, the generation of incorrect information may be misleading or even harmful when applied to care in this setting. MethodTo better understand this risk, this work tested two publicly available LLMs, ChatGPT-4 and Google Gemini Advanced (1.0 Ultra), as well as a custom Retrieval-Augmented Generation (RAG) LLM on factual knowledge and clinical reasoning in accordance with published material in aerospace medicine. We also evaluated the consistency of the two public LLMs when answering self-generated board-style questions. ResultsWhen queried with 857 free-response questions from Aerospace Medicine Boards Questions and Answers, ChatGPT-4 had a mean reader score from 4.23 to 5.00 (Likert scale of 1–5) across chapters, whereas Gemini Advanced and the RAG LLM scored 3.30 to 4.91 and 4.69 to 5.00, respectively. When queried with 20 multiple-choice aerospace medicine board questions provided by the American College of Preventive Medicine, ChatGPT-4 and Gemini Advanced responded correctly 70% and 55% of the time, respectively, while the RAG LLM answered 85% correctly. Despite this quantitative measure of high performance, the LLMs tested still exhibited gaps in factual knowledge that potentially could be harmful, a degree of clinical reasoning that may not pass the aerospace medicine board exam, and some inconsistency when answering self-generated questions. ConclusionThere is considerable promise for LLM use in autonomous medical operations in spaceflight given the anticipated continued rapid pace of development, including advancements in model training, data quality, and fine-tuning methods.

---

### Experienced surgeons can do more than one thing at a time: effect of distraction on performance of a simple laparoscopic and cognitive task by experienced and novice surgeons [^c79ae2f5]. Surgical Endoscopy (2008). Low credibility.

Background

While operating, surgeons are required to make cognitive decisions and often are interrupted to attend to questions from other members of the health care team. Technical automatization may be achieved by experienced surgeons such that these distractions have little effect on performance of either the surgical or the cognitive task. This study assessed the effect of adding a distracting cognitive task on performance of a basic laparoscopic skill by novice and experienced surgeons.

Methods

In this study, 31 novice (medical students in postgraduate years [PGYs] 1–2) and 9 experienced (fellows/attendants and PGYs 4–5) laparoscopic surgeons practiced the Fundamentals of Laparoscopic Surgery (FLS) laparoscopic peg transfer task until their scores stabilized. The mean normalized score after five repetitions then was recorded. The subjects also were tested on the number of mathematical addition questions they could answer in 1 min. This was repeated five times, with the mean number of questions attempted and the accuracy (% correct) recorded. The laparoscopic and addition tasks then were performed concurrently five times. Data, presented as mean ± standard deviation, were analyzed using Student's t-test. A p value less than 0.05 was considered statistically significant.

Results

After practice to stable peg transfer performance, the baseline peg transfer score was higher in the experienced group (98 ± 6 vs 87 ± 12; p < 0.01). There were no baseline differences between the groups in the number of math questions attempted in 1 min (10 ± 2 vs 9 ± 2; p = 0.55) or the number of correct answers (9 ± 3 vs 8 ± 3; p = 0.36). The comparison of baseline performance and dual-task performance showed that the experienced surgeons had no decline in peg transfer score (98 ± 6 vs 97 ± 6; p = 0.48), number of questions attempted in 1 min (10 ± 2 vs 9 ± 3; p = 0.32), or number of correct answers (9 ± 3 vs 8 ± 3; p = 0.46). In contrast, dual-tasking among the novices was associated with a decrease in the number of questions attempted (9 ± 2 vs 8 ± 2; p < 0.01) and the number of correct answers (8 ± 3 vs 7 ± 2; p = 0.02), and with no change in the peg transfer score (87 ± 12 vs 88 ± 8; p = 0.38) compared with baseline.

Conclusions

Distraction significantly decreased a novice's ability to process cognitively based math problems, whereas there was no effect on experienced subjects. This occurred despite the fact that the novice group had practiced to high-level peg transfer scores at baseline. This suggests that the experienced surgeons had achieved automatization of the peg transfer basic surgical skill to a level that cognitive distraction did not affect performance of either task. The experienced surgeons were able to attend equally to both tasks, whereas the novices attended to the surgical task at the expense of some aspects of cognitive task performance.

---

### Interventional study to improve adherence to phosphate binder treatment in dialysis patients [^59431e8b]. BMC Nephrology (2019). Medium credibility.

Table 4
The Beliefs about Medicines Questionnaire (BMQ), scores before and after intervention

The mean number of scores for correct answers in the 'Patient Knowledge' questionnaire, in the total study group, was relatively low prior to the intervention: 5.0 of a possible maximum total score of 16.0. A positive change after the intervention was seen by a statistically significant increase in scores for correct answers (mean change in scores: 2.9, CI: 2.165, 3.570; p = 0.001). The mean number of wrong answers was 1.0 both before and after intervention, while the mean number of the answer "I do not know" decreased from 3.0 to 1.5 (mean change in number: -1.5, CI: -2.005, − 1.051; p = 0.001). Most frequently prior to intervention, the questions "When should you take the phosphate binder?" and "What should you do if you have forgotten to take the phosphate binder?" were answered correctly; 91% (48) and 79% (42) correct answers, respectively. Few patients knew the answer to the question "The phosphate binder may affect the uptake of the following medicines"; before intervention only six patients had one correct choice and no patients had more than one of a possible total of three correct choices to this question. The highest increase in correct choices after intervention was seen for the questions "For which condition are you using phosphate binder?", "What can the use of phosphate binder prevent?" and "Which of these foods/drinks are especially rich in phosphate?". The mean number of correct choices for each question adjusted for the number of possible correct choices, increased from 0.51, 0.19 and 0.52 to 0.77, 0.42 and 0.76 respectively.

---

### Clinical decision rules: how to use them [^62ab325a]. Archives of Disease in Childhood: Education and Practice Edition (2010). Low credibility.

The first of this pair of papers outlined what a clinical decision rule is and how one should be created. This section examines how to use a rule, by checking that it is likely to work (examining how it has been validated), understanding what the various numbers that tell us about "accuracy" mean, and considers some practical aspects of how clinicians think and work in order to make that information usable on the front lines.

---

### Question-based computational language approach outperforms rating scales in quantifying emotional States [^a53c0b82]. Communications Psychology (2024). Medium credibility.

Categorizing the responses using ML

Participant responses were categorized into one of the four emotions using multinomial logistic regression, where we ensured that testing and training were always conducted in different subsets of the data. The categorizations were evaluated by a 10-fold leave-out cross-validation procedure, where 90% of the data were used for training the multinomial model, and the generated model was applied to 10% left-out data points. This leave-out procedure was repeated ten times so that all data points received a predicted value. The groups in each fold were generated randomly, with the constraint that no narrative in the training dataset could be found in the test dataset. In each fold, the number of dimensions was optimized based on the training data set, by trying the first 1, 2, 3, 5, 7, 10, 14, 19, 26, 35, 46, 61, 80, 105, 137, 179, 234, 305, 397, 517, and 768 dimensions, where we selected the number of dimensions with the fewest number of miscategorizations. The optimized number of dimensions found in the training data set was then applied to the held-out test dataset. For the words-only analysis, the mean number of optimized dimensions over the ten folds was 70 with a standard deviation of 14. For the rating scale-only analysis, the mean number of dimensions used was 3.64 with a standard deviation of 0.48 (i.e. in most cases all dimensions were used).

To test hypothesis 3, we also conducted multiple linear regression to predict the continuous values for each of the four rating scales. These regression models were otherwise conducted and evaluated in the same way as the multinomial regression model described above.

---

### Surveys and response rates [^bbdf5089]. Pediatrics in Review (2015). Low credibility.

Surveys continue to be an important research methodology. However, when assessing survey studies, clinicians should review the response rate and consider potential selection bias. Methods of survey administration (electronic, postal, or other means) may differ in cost, missing data, and response rates. In addition, reviewing the questionnaire construction, including the understandability of the questions and response formats, is important in assessing the validity of findings.

---

### Accelerating eye movement research via accurate and affordable smartphone eye tracking [^c2677801]. Nature Communications (2020). High credibility.

Next, we examined the effect of task-level difficulty on gaze and time-to-answer. We quantified task difficulty as the %incorrect answers per task (see Supplementary Figs. 6–7 for additional measures of task difficulty that take time and accuracy into account). Figure 9 b–f shows example gaze heatmaps for easy vs. difficult tasks, and the corresponding scatterplots of various metrics as a function of task difficulty. As expected, time to answer increased with task difficulty, though not significantly (Spearman's rank correlation r = 0.176, p = 0.63). The number of eye fixations on the passage increased with task difficulty (r = 0.67, p = 0.04). A closer look showed that the best predictor was fraction of gaze time spent on the relevant excerpt (normalized by height), which was strongly negatively correlated with task difficulty (r = −0.72, p = 0.02). In other words, as task difficulty increased, participants spent more time looking at the irrelevant excerpts in the passage before finding the relevant excerpt that contained the answer. These results show that smartphone-based gaze can help detect reading comprehension difficulty.

Fig. 9
Effect of reading comprehension difficulty on gaze for factual tasks.

a Barplot shows % fixation duration on the relevant portion of the passage (normalized by height) when participants answered the factual question correctly vs. not. Error bars denote the mean ± s.e.m. (n = 53, 13 tasks for correct vs. wrong responses). b Example of fixation heatmap for easy factual task; c difficult factual task. d – f Scatterplots showing different metrics as a function of task difficulty. d Time to answer the question in seconds (includes time spent reading the question and the passage); e number of fixations on the passage; f percentage time on relevant region, computed as the % total fixation duration on the relevant portion of the passage (normalized by height). Statistical correlation reported is the Spearman's rank correlation coefficient (n = 10 tasks); two-tailed one sample t -test. The confidence band represents the bootstrapped 68% confidence interval.

---

### The suboptimality of perceptual decision making with multiple alternatives [^93693cb1]. Nature Communications (2020). High credibility.

Each trial began with a 500-ms fixation, followed by a 500-ms stimulus presentation. The stimuli were then masked for 100 ms with a 7 × 7 grid of ellipsoid-shaped images consisting of uniformly distributed noise pixels. Each ellipsoid had a width of 0.54° and height of 0.95°, ensuring that it entirely covered each symbol. After the offset of the mask, subjects indicated the dominant symbol in the display without time pressure. No confidence ratings were obtained. The experiment had two conditions equivalent to the first two conditions in Experiment 1. In the first condition, subjects had to choose the dominant symbol among all six alternatives (six-alternative condition). In the second condition, subjects had to choose between two alternatives that were not announced in advance: the correct dominant symbol and a randomly selected nondominant symbol (two-alternative condition).

To obtain clear individual-level results, we collected data from each subject over the course of three different days. On each day, subjects completed 5 runs, each consisting of 4 blocks of 50 trials (for a total of 3000 trials per subject). We note that this very large number of trials makes it unlikely that any of our results in this or the subsequent experiments (which featured the same total number of trials) are due to insufficient training. The six- and two-alternative condition blocks were presented alternately, so that there were two blocks of each condition in a run. Subjects were given 15-s breaks between blocks and untimed breaks between runs. Before the start of the main experiment, subjects were given a short training on each day of the experiment.

Experiment 3

Experiment 3 used the same stimuli as in Experiment 2. Similar to Experiment 2, we presented a 500-ms fixation, a 500-ms stimulus, a 100-ms mask, and finally a response screen. Experiment 3 consisted of a single condition — subjects always chose the dominant symbol among all six alternatives. However, on 40% of trials in which subjects gave a wrong answer, they were asked to provide a second answer by choosing among the remaining five symbols. Subjects could take as much time as they wanted for both responses. Subjects again completed 3000 trials over the course of three different days in a manner equivalent to Experiment 2.

---

### Why questionnaire scores are not measures: a question-raising article [^6796d75d]. American Journal of Physical Medicine & Rehabilitation (2023). Medium credibility.

The Issue of Reliability

Any measure is affected by error, even the seemingly precise physics measures. The only way to estimate the error surrounding the "true" measure underlying any observation is repeating the observation (across multiple times, multiple observers, or both). In the meanwhile, it is assumed that no systematic changes are occurring. In medicine, this means that no changes in the measured variable occur, for example, because of treatments or the disease's progression. Statistics can manage these changes and provide indexes of reliability.

The reliability is a proportion, ranging from 0 to 1:

Variance is the mean of the (squared) differences between individual values and their mean value. Thus, reliability tells how much the differences in measures across subjects reflect true differences in the amount of the measured variable. The "error" variance includes sources of variance modeled as "random". In addition, it may consist of extraneous systematic influences: these can affect all observations or stem from unsuspected interactions between some subjects and some items (see hereinafter).

Because of the inevitable measurement error, the total variance (the denominator in Eq. 1) is always larger than the "true" variance. In short, reliability = 1 means that all differences in scores reflect differences due to the amount of trait owned by the subjects. Reliability = 0 means that all differences reflect error (random and/or systematic).

Reliability is a property of the entire measurement process: it can change, for the same measuring instruments, depending on the procedures adopted (e.g. if a mean across several ratings and/or several raters, instead of single measurements, is considered; if reliability is tested across different time points, etc.).

Estimating reliability is estimating the measurement error concerning an assumed "truth". Conventionally, repeated measurements are run assuming that random error will gradually attenuate, and nonrandom values will not.

---

### "Here, let Me Do It for you": psychological consequences of receiving direct and indirect help in childhood [^d17bfb0e]. Child Development (2025). Medium credibility.

2.1.3.3 Task Motivation

We measured children's challenge seeking and persistence. To index challenge seeking, we told children that they were going to solve another set of four puzzles but that they could choose which ones. For each set, children could choose between easy and difficult puzzles (based on Brummelman et al; Mueller and Dweck). Children were told: "If you choose to this block test, you might make many mistakes, but you'll definitely learn a lot too. But if you choose this block test, you won't make many mistakes, but you won't learn much either". Challenge seeking was computed as the number of difficult puzzles selected.

To index persistence, we told children there was one more puzzle that they had to solve. We told children the puzzle was planned beforehand so that children would not think the persistence puzzle was selected based on their answers to the challenge seeking measure. Children were then given a puzzle that, unbeknownst to them, was unsolvable (see OSF). Children could quit the task at any time by pressing a button. Persistence was computed as the number of seconds children spent trying to solve the puzzle before quitting. As preregistered, we excluded 7 outliers on this measure (using the outlier‐labeling rule; Hoaglin and Iglewicz).

2.1.3.4 Exploratory Measures

As in Study 1, children reported (1) what type of help they received, if any (no help, a hint, the correct answer); (2) how much help they thought they needed; and (3) provided an open‐ended response to: "Do you think something was fake or not real in the study?" We added two new questions. First, we asked children: "Did you like receiving no help [a hint, the correct answer] on the last block test?" (1 = Not at all, 6 = A lot). This latter question was routed based on what type of help children reported to have received (not what they actually received), so we had to exclude children who misreported the type of help they had received from the analyses of this variable. Second, we asked children how many puzzles they thought they solved (0–6). Afterwards, children were debriefed and thanked.

---

### Clinical practice guideline: improving nasal form and function after rhinoplasty [^98434b56]. Otolaryngology — Head and Neck Surgery (2017). Medium credibility.

Body dysmorphic disorder (BDD) screening — interpretation indicates that "A diagnosis of BDD is likely with the following answers", specifically "Question 1: Yes to both parts", "Question 3: Yes to any of the questions", and for Question 4 "Answers b or c". Question 4 asks, "On an average day, how much time do you usually spend thinking about how you look?" with choices "(a) Less than 1 hour a day (b) 1–3 hours a day (c) More than 3 hours a day".

---

### Assessment question characteristics predict medical student performance in general pathology [^9ae90b9a]. Archives of Pathology & Laboratory Medicine (2021). Medium credibility.

Context —

Evaluation of medical curricula includes appraisal of student assessments in order to encourage deeper learning approaches. General pathology is our institution's 4-week, first-year course covering universal disease concepts (inflammation, neoplasia, etc).

Objective —

To compare types of assessment questions and determine which characteristics may predict student scores, degree of difficulty, and item discrimination.

Design —

Item-level analysis was employed to categorize questions along the following variables: type (multiple choice question or matching answer), presence of clinical vignette (if so, whether simple or complex), presence of specimen image, information depth (simple recall or interpretation), knowledge density (first or second order), Bloom taxonomy level (1–3), and, for the final, subject familiarity (repeated concept and, if so, whether verbatim).

Results —

Assessments comprised 3 quizzes and 1 final exam (total 125 questions), scored during a 3-year period, (total 417 students) for a total 52125 graded attempts. Overall, 44890 attempts (86.1%) were correct. In multivariate analysis, question type emerged as the most significant predictor of student performance, degree of difficulty, and item discrimination, with multiple choice questions being significantly associated with lower mean scores (P = 0.004) and higher degree of difficulty (P = 0.02), but also, paradoxically, poorer discrimination (P = 0.002). The presence of a specimen image was significantly associated with better discrimination (P = 0.04), and questions requiring data interpretation (versus simple recall) were significantly associated with lower mean scores (P = 0.003) and a higher degree of difficulty (P = 0.046).

Conclusions —

Assessments in medical education should comprise combinations of questions with various characteristics in order to encourage better student performance, but also obtain optimal degrees of difficulty and levels of item discrimination.

---

### Predictive learning as the basis of the testing effect [^8cdf17f9]. Communications Psychology (2025). Medium credibility.

A minor issue is that no model's final recognition after incorrect tests perfectly matched the human data in Experiment 1. Specifically, human final recognition accuracy did not decrease with confidence ratings after incorrect tests (χ 2 (1, N = 66) = 2.828, p = 0.093, β ± 95% CI = −0.072 ± 0.084, R 2 = 0.002), unlike in the model simulation. One possible explanation is that initial learning and confidence ratings were unreliable in these incorrect tests, as participants never selected correct answers (Phases 2 and 3) in these conditions. This unreliable initial learning may have introduced excessive noise into the final recognition process (Phases 4 and 5), making accurate simulation difficult. Interestingly, in Experiment 2, human final recognition accuracy decreased with confidence ratings (χ 2 (1, N = 61) = 4.391, p = 0.036, β ± 95% CI = −0.055 ± 0.051, R 2 = 0.003), which was very well simulated by Model 5, possibly because the noisy initial learning had significantly diminished 24 h later.

---

### The suboptimality of perceptual decision making with multiple alternatives [^9dfd99d8]. Nature Communications (2020). High credibility.

The sensory representation was modeled in a similar fashion in Experiments 2 and 3. In both cases, the model was created based on subjects choosing between all available options (i.e. the six-alternative condition in Experiment 2 and the first answer in Experiment 3). The model of the sensory representation in Experiments 2 and 3 thus had 30 free parameters related to the sensory activations (for each of the six possible dominant symbols there were five μ s corresponding to each of the nondominant symbols) and an additional free parameter for the lapse rate.

We modeled the activations produced by each stimulus type separately to capture potential relationships between different colors or symbols (e.g. some color pairs may be perceptually more similar than others). However, we re-did all analyses using the simplifying assumption that when a color is nondominant, that color has the same μ regardless of which the dominant color is. This assumption allowed us to significantly reduce the number of parameters in our model of the sensory representation. In this alternative model of the sensory representation, the mean activity for each color/symbol was determined only based on whether that color/symbol was dominant or not. Therefore, we included two free parameters for each color/symbol. However, because of the issue described above (adding a constant to all μ s in a given experiment would result in identical decisions), we fixed one of the μ s to 0. This modeling approach reduced the total number of free parameters to eight in Experiment 1 (seven μ s and a lapse rate) and 12 free parameters in Experiments 2 and 3 (11 μ s and a lapse rate). This modeling approach produced virtually the same results (Supplementary Fig. 5).

Lastly, we considered two different instantiations of the summary model for Experiment 3. In the first instantiation, which we refer to as the Summary & Random Choice model, it is assumed that when the first answer is wrong, then the subject would randomly pick a second answer among the remaining options. In the second instantiation, which we refer to as the Summary & Strategic Choice model, it is assumed that when the first answer is wrong, then the subject would pick the stimulus category of a randomly recalled symbol from the original 7 × 7 grid that is different from the stimulus category chosen with the first answer. According to this model, the subject would pick the second answer correctly 33.3% of the time and each incorrect symbol will be chosen 16.7% of the time (Supplementary Fig. 4).

---

### Toward a universal decoder of linguistic meaning from brain activation [^2e3baee4]. Nature Communications (2018). Medium credibility.

We also calculated the rank accuracy over 384 and 243 sentences in experiments 2 and 3, respectively, which were significant for all subjects (mean accuracy: 0.76 and 0.79; all significant p -values < 0.01, using a test based on a normal approximation to the null distribution, with Bonferroni correction for the number of participants and experiments; results are shown in Fig. 4b, p -values for each subject/task are provided in Supplementary Tables 2 and 3). Rank accuracy is particularly informative here given the large number of closely related sentences within passages and topics. This is because it assigns full credit to getting the exact stimulus sentence at the top of the ranking, and partial credit if the top-ranked sentence is semantically related to the target.

Finally, we performed an open-ended decoding task to determine how well the decoded vectors can be used to identify the content words in the stimulus sentences. This analysis quantifies the degree to which the decoder retrieves relevant information when we do not have a closed set of ground-truth stimuli to choose from. In this task, the words in a basic vocabulary (n = ~30,000) were ranked according to how similar their vectors are to the decoded vector. For each sentence, we calculated a word-spotting score, defined as the best rank accuracy of any of the words in the sentence. This is a challenging task for two reasons. First, the decoded vector represents mental contents as an entire sentence is processed, often in the context of preceding sentences, rather than individual words. Hence, the decoder will succeed only if the sentence activation pattern resembles those of individual words. Second, this ranking considers synonyms of the words in the sentence, as well as related words, names of the categories the words belong to, etc. All of these compete with and may be ranked higher than the actual word in the sentence. For each subject, we computed the distribution of scores across all sentences in experiments 2 and 3, and compared it with a simulated null distribution (for every sentence, take the maximum of randomly generated ranks). Figure 4c, d plots the average distribution across subjects against this null distribution for experiments 2 and 3, respectively. The distributions were significantly different from the null distribution for 7 of 8 participants in experiment 2 and all 7 participants in experiment 3. The median rank accuracy of the best ranked word is 0.96 in both experiments (all significant p -values < 0.01 using a 2-sided Kolmogorov–Smirnov test, with Bonferroni correction for the number of participants). The median scores in the null model are 0.86 and 0.88, but with a very different distribution. The highest ranked word is in the top 1% for 24% of the sentences in experiment 2 and 33% of the sentences in experiment 3 (median across subjects).

---

### Public attitudes value interpretability but prioritize accuracy in artificial intelligence [^33a03ee3]. Nature Communications (2022). High credibility.

Procedure

Participants learned that we were interested in their attitudes towards AI, which we defined as follows: "Artificial Intelligence (AI) refers to computer systems that make predictions, recommendations, or decisions by learning from existing datapoints. This computerised process is automated and occurs without explicit human instructions". They proceeded to questions about their intuitions on "the extent to which people [with/without] training in computer science can explain how an AI reaches certain predictions, recommendations, or decisions in certain cases" (5-point rating scale from "cannot explain at all" to "can explain fully") and their general support for AI (5-point rating scale from "strongly oppose" to "strongly support"). Next, we provided them with a precise definition of explainability reading "By explainable we mean that an AI's [decision/recommendation] can be explained in non-technical terms. In other words, it is possible to know and to understand how an AI arrives at its [decision/recommendation]". This was followed by a simple comprehension that all participants were ultimately allowed to pass, though we excluded those who failed on two attempts from our analyses (probe: "What is meant by 'explainable AI'?"; correct response: "That an AI's [decision/recommendation] can be explained in non-technical terms"). Participants then learned that they would go through a series of AI applications for each of which they would answer "how important is it that the AI in this application is explainable, even if it performs accurately" on a dichotomous 5-point rating scale from "not at all important" to "extremely important". Each participant saw either the recommend or decide version of the application-descriptions. We had a total of 27 decide applications and 25 recommend applications (see SI Notes, Materials Study 1A for details) and each participant saw a random subset of 20 out of these, presented on subsequent pages with a reminder about explainability's definition displayed at the bottom ("Reminder: Explainable means that the AI's recommendation can be explained in non-technical terms"). After providing their ratings for the applications, participants indicated how important they considered the respective motives "explain to justify/to verify/to improve/to discover"(see SI Results, Study 1A). We then probed them again on their support for AI and asked how likely they considered it that their occupation would be replaced by AI at some point, and whether they had any computer science knowledge. The survey concluded with standard demographics (gender, age, income, education).

---

### 2022 American society of anesthesiologists practice guidelines for management of the difficult airway [^5298d89c]. Anesthesiology (2022). High credibility.

Difficult airway — consensus-based evidence and survey scoring indicates validation by consensus from survey opinion from expert consultants, survey opinions from randomly selected samples of active members of the American Society of Anesthesiologists (ASA) and participating organizations, and internet commentary, with only findings from formal surveys reported. Survey responses were recorded using a five-point scale and summarized based on median values, with categories defined as: strongly agree median score of 5 (at least 50% of the responses are 5), agree median score of 4 (at least 50% of the responses are 4 or 4 and 5), equivocal median score of 3 (at least 50% of the responses are 3, or no other response category or combination of similar categories contains at least 50% of the responses), disagree median score of 2 (at least 50% of responses are 2 or 1 and 2), and strongly disagree median score of 1 (at least 50% of responses are 1). When an equal number of categorically distinct responses are obtained, the median value is determined by calculating the arithmetic mean of the two middle values, and ties are calculated by a predetermined formula.

---

### Awareness and knowledge of radiation dose and associated risks among final year medical students in Norway [^5b017a9a]. Insights Into Imaging (2017). Low credibility.

Results

Seventy-five students completed and returned the questionnaire initially, yielding a response rate of 33%. The response rate was increased to 45% after sending a reminder letter. Forty-five students (45%) were male and 54 (55%) were female. Seventy-nine students (80%) were from University A and 20 (20%) were from University B. Fifty-four students (55%) reported that they were very or moderately confident in their knowledge of radiation dose; 45 students (45%) reported that they were not really confident or, 'do not know'. Ninety-seven students (96%) reported that knowledge of radiation dose and its associated risks is very important or moderately important. The sociodemographic characteristics of the participants are presented in Table 1.

Table 1
Sociodemographic characteristics of participants (n = 99)

The average mean score was 3.91 (SD = 1.70, range 0 to7) out of possible score of 11. The average mean score for students from University A was 3.89 (SD = 1.76) and for the students from University B it was 4.00 (SD = 1.49) (Table 2). Three students (3%) scored zero and only eighteen students (18%) scored more than five points (50%). The distribution of marks is shown in Fig. 1. The radiation dose received during a chest x-ray was correctly identified by 20% (n = 20) of the students. Only 12% (n = 12) of the students correctly identified the risk of fatal cancer for a CT abdomen examination and 13% (n = 13) correctly identified the dose limits for the patients. Nearly all students, (n = 93) correctly identified the group of patients who would be more sensitive to radiation dose. Seventy-three percent (n = 72) and 85% (n = 84) of the students correctly identified that both MRI and ultrasound are non-ionising imaging procedures, respectively. The number of correctly answered questions and the responses to questions regarding radiation doses for particular examinations are shown in Tables 3 and 4, respectively. Only 12 students (12%) answered correctly that there is no dose limit for patients.

---

### "Binary" and "non-binary" detection tasks: are current performance measures optimal? [^8d7e974c]. Academic Radiology (2007). Low credibility.

We have observed that a very large fraction of responses for several detection tasks during the performance of observer studies are in the extreme ranges of lower than 11% or higher than 89% regardless of the actual presence or absence of the abnormality in question or its subjectively rated "subtleness". This observation raises questions regarding the validity and appropriateness of using multicategory rating scales for such detection tasks. Monte Carlo simulation of binary and multicategory ratings for these tasks demonstrate that the use of the former (binary) often results in a less biased and more precise summary index and hence may lead to a higher statistical power for determining differences between modalities.

---

### Bumetanide [^d0437f55]. FDA (2025). Medium credibility.

WARNING

Bumetanide injection is a potent diuretic which, if given in excessive amounts, can lead to a profound diuresis with water and electrolyte depletion. Therefore, careful medical supervision is required, and dose and dosage schedule have to be adjusted to the individual patient's needs [see Dosage and Administration].

---

### Effective patient-physician communication [^b9d4de51]. ACOG (2025). High credibility.

Best practices for communication in medical encounters — Providing information lists roles and responsibilities to Seek to understand patient's informational needs, Share information, Overcome barriers to patient understanding (language, health literacy, hearing, numeracy), Facilitate understanding, and Provide information resources and help patient evaluate and use them; skills include Explain nature of problem and approach to diagnosis, treatment, Give uncomplicated explanations and instructions, Avoid jargon and complexity, Encourage questions and check understanding, and Emphasize key messages.

---

### "There is nothing like looking, if you want to find something"-asking questions and searching for answers-the evidence based approach [^e77a34fb]. Archives of Disease in Childhood: Education and Practice Edition (2010). Low credibility.

Evidence based medicine (EBM) consists of four separate processes. Asking a clinical question Locating the evidence Appraising and synthesizing the evidence Applying the evidence Most people are familiar with the third step but not so familiar with the first two. Well-designed clinical questions can help clarify in a clinicians mind exactly what information is being sought and also help with the search. This clinical question can be framed around the PICO format. With a well-structured PICO information can then be located in a timely manner using the concept of Haynes's 5 S's and a range of websites including trial registers and meta-search engines.

---

### Fifteen-minute consultation: asking questions: the puzzles and problems model [^730a54c8]. Archives of Disease in Childhood: Education and Practice Edition (2018). Low credibility.

People are full of useful advice, especially when you make the transition to the senior doctor grade. We present a very helpful model we have used over some time to aid us when we have issues.

---

### Tips for a physician in getting the right job, part XII: general questions for the applicant to ask [^102713ad]. Clinical Nuclear Medicine (2014). Low credibility.

The type and caliber of the questions asked by a job hunter is one of the ways an interviewer will evaluate the candidate. Questions that show poor preparation should not be asked, such as failure to read what the employer sent to the job seeker or not doing elementary research on the practice, the organization, or the community. Asking about insignificant details also is not helpful. Not having any good questions to ask is a negative in an interview. This article discusses many possible important questions for the applicant to ask during an interview.

---

### Weight regain and cardiometabolic effects after withdrawal of semaglutide: the STEP 1 trial extension [^cbf53ee8]. Diabetes, Obesity & Metabolism (2022). Medium credibility.

3.2 Change in body weight and

During the main treatment phase (from baseline [week 0] to week 68), semaglutide reduced body weight more than placebo (Figure 1A and Tables 2 and S1); observed mean weight loss was 17.3% (standard deviation [SD]: 9.3%) with semaglutide versus 2.0% (SD: 6.1%) with placebo (Table S1). After treatment withdrawal, body weight regain was observed in both the semaglutide and placebo arms (Figure 1A and Tables 2 and S1). Participants regained a mean of 11.6 percentage points (SD: 7.7) of body weight in the semaglutide arm versus 1.9 percentage points (SD: 4.8) in the placebo arm (Table S1). The net mean body weight loss over the full duration of the main treatment phase and off‐treatment extension phase (from week 0 to week 120) was 5.6% (SD: 8.9%) in the semaglutide arm versus 0.1% (SD: 5.8%) in the placebo arm (Figure 1A). At week 120, 5% or higher weight loss from baseline was observed in 48.2% of participants (95 of 197) in the semaglutide arm and in 22.6% (21 of 93) in the placebo arm.

---

### Assessment of chatGPT's performance on neurology written board examination questions [^f8b2c51d]. BMJ Neurology Open (2023). High credibility.

Figure 1
Neurology question stems. (A) Example of a question input followed by a correct answer response from the ChatGPT. (B) Example of a question input followed by an incorrect answer response from the AI (the correct answer is B). AI, artificial intelligence.

As question stems are multiple choice and accompanied by lettered options (eg, answer A, B, C, etc), if ChatGPT provided a written answer without referencing a specific alphabetic answer choice, the answer was read in full to determine if it was reflective of any of the multiple-choice options and scored accordingly. As these are multiple-choice questions, ChatGPT was asked to provide a single best answer.

Image-based questions from the question bank were deemed ineligible and not included in the assessment as ChatGPT only accepts text input.

The percentage of questions correctly answered was compiled and compared with an expected percentage of correct answers. For each question, BV provides a summary of the percentage of learners who answered that specific question correctly. The expected percentage was the average of the percentage of learners (neurology residents) who correctly answered each question averaged over all eligible questions. The expected number of correct answers by learners was calculated by multiplying the expected percentage of correct answers by the number of eligible questions. BV does not report the experience level of the neurology residents answering that specific question (ie, Post Graduate Year).

The overall performance percentile of ChatGPT's first attempt was calculated by the question bank, as compared with other BV learners, and recorded. The percentages of questions correctly answered by ChatGPT on the first attempt and over three attempts were compared with the performance quartiles as provided by BV. Percentages correct were also compiled by subject matter (ie, neuro-oncology, critical care, etc). This analysis aimed to provide an alternate way to compare ChatGPT's performance in relation to other learners using the question bank.

---

### Geriatric emergency department guidelines [^a38d649e]. Annals of Emergency Medicine (2014). Medium credibility.

Geriatric emergency department guidelines — cognitive screening instructions state: "Now I would like to ask you some questions to check your memory and concentration. Some of them may be easy and some of them may be hard". Items include "1) What year is it now?" and "2) What month is this?" with scoring "(0) (1)". The patient repeats "John Brown, 42 Market Street, Chicago" with "Trials to learning ______ (if unable to do in 3 trials = C)". For time, "Without looking at your watch or the clock, tell me what time it is", with "(within 1-hour) ______ Correct Incorrect (0) (1)" and "Actual time: ________". "4) Count aloud backwards from 20 to 1" is scored "0 1 2 Errors", with the note "If subject starts counting forward or forgets the task, repeat instructions and score one error", and the sequence "20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1". "5) Say the months of the year in reverse order", showing "D N O S A JL JN MY AP MR F J" and "0 1 2 Errors", with the rule "If the tester needs to prompt with the last name of the month of the year, one error should be scored". "6) Repeat the name and address you were asked to remember" shows "(John Brown, 42 Market Street, Chicago) 0 1 2 3 4 5 Errors".

---

### Composite rating scales [^2fbad343]. Journal of the Neurological Sciences (2010). Low credibility.

Rating scales are instruments that are very frequently used by clinicians to perform patient assessments. Typically, rating scales grade the attribute on an ordinal level of measurement, i.e., a rank ordering, meaning that the numbers assigned to the different ranks (item scores) do not represent 'real numbers' or 'physical magnitudes'. Single-item scales have some advantages, such as simplicity and low respondent burden, but they may also suffer from disadvantages, such as ambiguous score meanings and low responsiveness. Multi-item scales, in contrast, seem more adequate for assessment of complex constructs, allowing for detailed evaluation. Total scores representing the value of the construct may be quite precise and thus the responsiveness of the scale may be high. The most common strategy for obtaining the total score is the sum of the item scores, a strategy that constitutes one of the most important problems with these types of scales. A summative score of ordinal figures is not a 'real magnitude' and may have little sense. This paper is a review of the theoretical frameworks of the main theories used to develop rating scales (Classical Test Theory and Item Response Theory). Bearing in mind that no alternative is perfect, additional research in this field and judicious decisions are called for.

---

### An official multi-society statement: the role of clinical research results in the practice of critical care medicine [^f77c0e47]. American Journal of Respiratory and Critical Care Medicine (2012). Medium credibility.

Negotiating between various kinds of medical knowledge — no set hierarchy of knowledge to guide clinical decision making in all situations is possible, and a ranking based on study design or quality measures cannot be directly applied to clinical decisions; clinicians must continue to rely on sound clinical judgment to negotiate between potentially conflicting facts and reasons, and explicitness is to be greatly valued, as clinicians should be able to identify and articulate the sources and kinds of knowledge invoked; the initial step is identifying pertinent medical knowledge, including clinical research, pathophysiologic understanding, and clinical experience, and since no kind of knowledge is always superior, simply relying upon a randomized controlled trial or meta-analysis will be insufficient; clinicians are obligated to consider information and knowledge that might suggest action that would run counter to that suggested by clinical research, and a clinician ought to be able to concisely outline and justify the process of clinical reasoning for scrutiny and revision.

---

### Summary benchmarks-full set – 2024 [^76ffec10]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — scope and principles emphasize that Preferred Practice Patterns provide guidance for the pattern of practice, not for the care of a particular individual, and that the Preferred Practice Pattern guidelines are not medical standards to be adhered to in all individual situations. The series is based on three principles: each Preferred Practice Pattern should be clinically relevant and specific enough to provide useful information to practitioners; each recommendation that is made should be given an explicit rating that shows its importance to the care process; and each recommendation should also be given an explicit rating that shows the strength of evidence that supports the recommendation and reflects the best evidence available. Adherence to these Preferred Practice Patterns will not ensure a successful outcome in every situation, and these practice patterns should not be deemed inclusive of all proper methods of care or exclusive of other methods of care reasonably directed at obtaining the best results.

---

### FDA CVM ADUFA V second educational conference July 2025… [^ca562fdb]. FDA (2025). Medium credibility.

7 Q&A Format
- Q&A panel sessions to begin at approximately 11: 30am and 3: 30pm
- Attendees may submit question via the QR code with hyperlink to SurveyMonkey throughout the morning and again in the afternoon
- All questions will be anonymous
- We will answer as many questions as time allows
- All questions will be retained and submitted to the Docket and may provide beneficial information as we develop agendas for future educational conferences
- The QR code will be deactivated after meeting adjourns www. fda. gov. 17 Establishing ADUFA Fee Rates
- Estimating number of fees – Applications
- Average number of applications over the five most recent completed years – Sponsors, Products & Establishments
- Based on most recent May data
- Determining waiver percentages – Based on most recent 5-year average
-.

18 Workload Adjuster
- The workload adjuster is calculated using the average of 5 types of applications and submissions received over the most recent 5-year period as compared to a rolling base of the most recently completed 5 years.
- **Each application or submission type is multiplied by a weighting factor**: the percent of.
2. **Field Effectiveness Studies**:

- Conducted by qualified personnel).
- Should follow GFI #85 / VICH GL9
- QAU optional 3. Laboratory Effectiveness Studies:

- GCP principles apply
- GLP not required, but good study conduct is expected
- QAU optional www. fda. gov. 61 Raw Data Expectations
- All units of measurements should be consistent with the Imperial or International System of Units derivatives.
- Raw data can retain original units – All data for a specific variable should be consistently converted to the same unit of measure in the Final Study Report www. fda. gov.

69 International Collaborative Review Opportunities
- Brandi Robinson, MPH, CPH www. fda. gov. 84 Learning Objectives www. fda. gov 1) Understand regulatory definitions related to real-world data and real-world evidence 2) Understand the key questions to answer when evaluating whether RWD and RWE may be a.

---

### Semaglutide improves cardiometabolic risk factors in adults with overweight or obesity: STEP 1 and 4 exploratory analyses [^8785ac70]. Diabetes, Obesity & Metabolism (2023). Medium credibility.

2 MATERIALS AND METHODS

The methodology of STEP 1 and 4 has been described previously.

2.1 Study designs

STEP 1 (NCT03548935) was a double‐blind, placebo‐controlled trial (Figure S1). Participants were randomized 2:1 to once‐weekly s.c. semaglutide 2.4 mg or placebo, with lifestyle intervention for 68 weeks. STEP 4 (NCT03548987) was a randomized withdrawal trial in which all participants received semaglutide during a 20‐week run‐in period (Figure S2). Participants reaching the once‐weekly s.c. semaglutide 2.4 mg target maintenance dose at week 20 were randomized 2:1 to continue semaglutide or switch to placebo for 48 weeks. All participants received lifestyle intervention throughout the 68 weeks.

Both trials were conducted in accordance with the Declaration of Helsinki and Good Clinical Practice guidelines. Protocols were approved by independent ethics committees or institutional review boards at each study site. All participants provided written informed consent.

2.2 Study population

Participants in both trials were aged ≥ 18 years with one or more self‐reported unsuccessful dietary effort to lose weight and body mass index ≥ 30 kg/m 2 or ≥ 27 kg/m 2 with one or more weight‐related comorbidity without diabetes.

2.3 Outcomes and assessments

The endpoints assessed are detailed below, with information on how they were measured in the Supporting Information.

---

### Health checks for people aged 75 and over… [^62639282]. Occasional Paper (1993). Low credibility.

The clinical calculator "Abbreviated mental test (AMT-10)" for mild cognitive impairment and dementia.

The Abbreviated Mental Test (AMT-10) is a commonly used clinical calculator designed to assess cognitive function in elderly patients. It is particularly useful in identifying cognitive impairment, including conditions such as dementia and delirium. The AMT-10 consists of ten questions that evaluate various aspects of cognitive function, including memory, attention, and orientation.

The AMT-10 is applicable to the elderly population, particularly those in hospital or care home settings, or those presenting with confusion or memory problems. It is a quick and easy tool for healthcare professionals to screen for cognitive impairment and to monitor changes in cognitive function over time.

The clinical utility of the AMT-10 lies in its ability to provide a rapid assessment of a patient's cognitive status. It can help guide further diagnostic testing, inform treatment decisions, and assist in the planning of care.

Exclusion criteria for the AMT-10 include patients who are unable to communicate effectively, such as those with severe hearing or speech impairments, or non-English speakers without an interpreter. It may also be less accurate in patients with high levels of education or intelligence, as they may score well despite having significant cognitive impairment.

The Abbreviated Mental Test (AMT-10) is designed to assess cognitive function through a series of 10 straightforward questions. Each question evaluates an aspect of mental awareness and cognitive recall, with responses marked as either "Correct" or "Incorrect".

The scoring system for the AMT-10 is simple:

- Each correct response earns 1 point.
- Each incorrect response earns 0 points.

The total score, which ranges from 0 to 10, is calculated by summing the scores from all the answers provided.

Here's a breakdown of the questions evaluated:

- **Age**: Correct/Incorrect
- **Time**: Correct/Incorrect
- **Current Year**: Correct/Incorrect
- **Patient's Address**: Correct/Incorrect
- **Date of Birth**: Correct/Incorrect
- **Year the First World War I Started**: Correct/Incorrect
- **Name of Current President**: Correct/Incorrect
- **Count Backwards from 20 to 1**: Correct/Incorrect
- **What Jobs Do These People Do? (based on a picture of a postman or a cook)**: Correct/Incorrect
- **Ask to Repeat the Address Given Earlier**: Correct/Incorrect

After calculating the total score based on these responses, the AMT-10 provides an interpretation of cognitive function:

- A score between 0 and 3 suggests "Severe cognitive impairment".
- A score between 4 and 5 indicates "Moderate cognitive impairment".
- A score between 6 and 10 indicates "No cognitive impairment".

The process involves collecting responses to these questions, scoring them, and using the total score to assess the level of cognitive function. This straightforward approach ensures an effective evaluation using a standardized method.

---

### Online decision support for persons having a genetic predisposition to cancer and their partners during reproductive decision-making [^ece8d64b]. Journal of Genetic Counseling (2019). Medium credibility.

Participants' current knowledge of the three reproductive options (at T0, T1, T2) was assessed by 15 items (Gietel‐Habets et al. 2017). Three questions measured participants' knowledge of natural conception without genetic testing (e.g. "When opting for natural conception, there is a 50% risk of transmitting the pathogenic variant to offspring"; 1 = correct, 2 = incorrect, 3 = not sure), five questions measured knowledge of PND (e.g. "PND takes place during pregnancy") and seven questions measured knowledge of PGD (e.g. "IVF is necessary to perform PGD"). One point was provided to each correctly answered question, with a maximum score of 15.

Participants' decision self‐efficacy (at T0, T1, T2) was assessed by the Decision Self‐Efficacy Scale (Bunn & O'Connor, 1996). The questionnaire contained 11 items (Cronbach's α = 0.84), each scored on a 5‐point Likert scale ranging from 0 (not at all confident) to 4 (very confident). Total scores ranged from 0 (extremely) to 100 (extremely high) (Bunn & O'Connor, 1996).

Realistic expectations regarding the reproductive options (T0, T1, T2) were assessed by three questions (i.e. "What is the extra risk of miscarriage due to PND?", "What is the chance of pregnancy after one IVF treatment with PGD?", "What is the risk of complications with PGD?"). These questions contained 8 to 11 answer options. One point was provided to each correctly answered question, with a maximum score of 3 (O'Connor, 1995b).

Level of deliberation (T0, T1, T2) was measured by the Deliberation Scale (Van den Berg, Timmermans, Kate, Vugt, & Wal, 2006). The questionnaire contained six items (Cronbach's α = 0.90), each scored on a 5‐point Likert scale ranging from 1 (totally disagree) to 5 (totally agree). Total scores ranged from 6 (low level) to 30 (high level).

---

### Hypothetical nudges provide directional but noisy estimates of real behavior change [^ef7d6245]. Communications Psychology (2025). Medium credibility.

Table 2 and Table 3 show the distribution of responses to the gender identification and ethnicity identification questions, respectively, by experiment.

Table 2
The distribution of responses to "With which gender do you most identify?" by experiment

These counts come after following our pre-registered rules for dropping incompletes, failed attention checks, and duplicate IP addresses.

Table 3
The distribution of responses to "With which ethnicity do you most identify?" by experiment

These counts come after following our pre-registered rules for dropping incompletes, failed attention checks, and duplicate IP addresses.

All participants earned $0.54 for completing our three-minute surveys.

Outcome variables

Our focal measure was whether or not a participant hypothetically engaged in the target behavior (1) or not (0). We summarized the percentage of participants engaging in the behavior for each condition of each experiment. Then, we calculated treatment effects in three ways: as a difference (percent engaging in the behavior in the treatment group minus the percent engaging in the behavior in the control group), as a risk ratio (percent engaging in the behavior in the treatment group divided by the percent engaging in the behavior in the control group), and as an odds ratio (percent engaging in the behavior in the treatment group divided by the percent not engaging in the behavior in the treatment group, divided by the same ratio — the odds of success — in the control group). We present the first in the main text and the latter two in the supplement (SI Section 6a).

For example, in the simple and generic hypothetical design of our consumer choice study, 84.3% of participants in the control group and 78.0% in the treatment group said they would stop for a donut. As a difference, the treatment effect is 78.0–84.3% = -6.3%. As a risk ratio it is 78.0%/84.3% = 0.925. And as an odds ratio, it is (78.0%/22.0%)/(84.3%/15.7%) = 0.660.

---