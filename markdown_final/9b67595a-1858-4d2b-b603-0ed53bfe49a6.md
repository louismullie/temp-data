# Medical Question & Answer

**Sample ID**: 9b67595a-1858-4d2b-b603-0ed53bfe49a6
**Dataset Index**: 83950

---

## Question

In an ordinary set of dominoes, there are 28 tiles. How many tiles would a set of dominoes contain if the values indicated on the tiles ranged not from 0 to 6, but from 0 to 12?

---

## Answer

> Let's see… What do we have here? The user is asking how many tiles would be in a domino set if the values on the tiles range from 0 to 12 instead of 0 to 6. Let's break this down step-by-step. First, I need to think about the combinatorics of a standard double-six set and verify the 28-tile count. Then, I should generalize the counting formula to a double-n set. Next, I will apply that formula to double-12. Finally, I will double-check the arithmetic and consider whether any special cases or conventions change the total, before stating the answer clearly.

> Let me first confirm the baseline. In a standard double-six set, each tile is an unordered pair of values from 0 to 6, including doubles like 0–0, 1–1, up to 6–6. The total number of unique tiles is the number of combinations with repetition of 7 items taken 2 at a time, which equals 7 choose 2 plus 7 (for the doubles), or more compactly, (7 + 1) choose 2, which is 28 tiles. Wait, let me verify that formula: for unordered pairs with repetition from a set of size n+1, the count is (n+1)(n+2)/2, so for n = 6 that is 7×8/2 = 28, which matches the ordinary set size [^notfound].

> Now, I need to generalize. For a double-n set, the values range from 0 to n, so there are n+1 distinct values. The number of unique unordered pairs with repetition is (n+1)(n+2)/2. Hold on, I should verify the logic: this is equivalent to choosing 2 items from n+1 with replacement, where order does not matter, which is indeed (n+1)(n+2)/2. For n = 6, that gives 7×8/2 = 28, confirming the formula works for the standard case [^notfound].

> Next, I should apply this to double-12. Here, n = 12, so the number of distinct values is 13 (0 through 12). Plugging into the formula, the total number of tiles is 13×14/2. Let me calculate that: 13×14 = 182, and 182 divided by 2 is 91. So, a double-12 set should contain 91 tiles. Hmm, wait a minute, I initially thought to say 13×12/2 = 78, but that would be incorrect because I mistakenly used 12 instead of 13 for the first factor; the correct first factor is n+1 = 13, not n = 12, so 13×14/2 = 91 is the right answer [^notfound].

> I should double-check that there are no conventions that would alter this count. Some specialty domino sets exclude the double-zero tile, but that is not the standard convention for ordinary sets; the standard double-six includes 0–0, so by analogy a double-12 set would include 0–0 through 12–12, and the combinatorial total remains 91. If someone were to exclude 0–0, the count would drop to 90, but that would be a nonstandard variant rather than the usual definition of a complete double-n set [^notfound].

> Final answer: A double-12 domino set contains 91 tiles.

---

A standard domino set with values 0–6 has 28 tiles, which is the number of unique pairs (including doubles) from 0 to 6. For values 0–12, the number of unique pairs is **91 tiles**, calculated as (13 × 14) / 2 = 91, where 13 is the count of values (0–12) and 14 is 13 + 1. This follows the combinatorial formula for combinations with repetition: **(n + 1)(n + 2) / 2**, where n is the highest value.

---

## Mathematical explanation

The problem is a classic example of **combinations with repetition**, where we select 2 values from a set of 13 values (0–12) and allow repetition (doubles). The formula for combinations with repetition is:

Number of combinations = [(n + r − 1)!/(r!(n − 1)!)]

Where:

- (n) is the number of distinct values (13 for 0–12)
- (r) is the number of values to choose (2 for a domino tile)

Plugging in the values:

Number of tiles = [(13 + 2 − 1)!/(2!(13 − 1)!)] = [14!/(2! × 12!)] = [(14 × 13)/2] = 91

---

## Verification with the standard domino set

For a standard domino set with values 0–6, the same formula applies:

Number of tiles = [(7 + 2 − 1)!/(2!(7 − 1)!)] = [8!/(2! × 6!)] = [(8 × 7)/2] = 28

This matches the known count of 28 tiles, confirming the formula's validity.

---

## Conclusion

A domino set with values ranging from 0 to 12 contains **91 tiles**. This is calculated using the combinations-with-repetition formula, which generalizes to any maximum value (n) as [(n + 1)(n + 2)/2].

---

## References

### An aperiodic chiral tiling by topological molecular self-assembly [^24d72d0a]. Nature Communications (2025). High credibility.

The Wang aperiodic tiling game employs sets of congruent square tiles, each possessing four numbered (or colored) edges. Wang tiles were introduced as a tool for studying decidability in mathematical logic but are also important in theoretical computer science. The question of whether tiling exclusively by translation with matching edge numbers or colors — commonly referred to as the domino problem — possesses a periodic solution, has been proven to be undecidable. However, recent research has demonstrated that a set of 11 Wang tiles is adequate for aperiodically tiling the plane. In Penrose tiling, a set of shapes — typically two different types of tiles — are arranged according to specific rules, leading to a structure that covers a plane without periodic repetition but still exhibits long-range order. Several thin film systems have been reported to exhibit non-repetitive patterns, such as quasicrystallinity –. When applied at the molecular level, the challenge lies in guiding molecules to form such intricate patterns naturally through self-assembly. Recent molecular random self-assemblies were based either on porous structures with directed polar intermolecular interactions –, or had different species involved.

The here reported aperiodic tiling evolves from a single molecule that, due to its chirality, transmits into aperiodic tiling of triangular arranged ensembles. At the very least, our STM observations lend support to this conjecture, as only patterns lacking translational invariance have been observed. One of the fundamental rules of Wang tiling is the prohibition of tile reflection, which essentially transmits into the single handedness requirement of a solution to the monotile problem. The original hat-shaped monotile ('einstein'), however, required that some of the tiles be reflected. This restriction persists here, as only a minority of opposite-handed molecules are present in all observed structures. While periodic solutions are conceivable (Supplementary Fig. 11), it remains unclear whether these consistencies enforce aperiodicity in the t[4]HB self-assembly on Ag(111). A mathematical proof demonstrating that subsets of the triangular tiles presented here inherently possess strictly aperiodic solutions is beyond the scope of this report.

---

### 3D replicon distributions arise from stochastic initiation and domino-like DNA replication progression [^7e1e70c4]. Nature Communications (2016). Medium credibility.

If the removal triggers a chromatin zone boundary crossing or a firing event (because a limiting factor has been freed), then the addition and removal of future collision events becomes necessary. To keep such operations efficient, ordered lists are maintained for barriers, potential origins, left-going forks and right-going forks. These lists are implemented using a special red-black tree that, in addition to standard red-black tree behaviour, allows indexed element access scaling O (ln N) with the number of elements N (all nodes keep track of the number of their children).

For instance, if it is determined that an origin has to be fired, a random origin is picked from the available origins and checked if it has been passively replicated by the active forks. If not, its relative firing probability (a value between 0 and 1) is determined by the maximum of the spontaneous and induced firing probabilities and a random number between 0 and 1 is drawn. Should the random number be lower than the probability, the origin is fired, otherwise the process is repeated. If the origins lies inside the inhibition distance of an active fork, its firing probability is set to zero. Firing of the origin means that two forks, one in each direction, will be created, which have to be inserted into the fork lists, and for which collision events have to be calculated.

Experimental data suggest that the total number of replicons is between 4,000 and 6,000. We consider a replicon to consist of two forks, meaning that the number of active replication forks is ∼12,000. Accordingly, in our model, the maximum value of the number of replication forks is set to L max = 12,000. With this value, the total genome replication time obtained in the computer simulation agrees with the empirically found S-phase duration. To model the increase of the limiting factor L (t) in the beginning of S-phase, we used the function

with τ = 15 min, as obtained from the dynamics of RFi numbers measured in live HeLa Kyoto cells in the beginning of S-phase. The function as well as the experimental data is shown in Fig. 2a. A model with a linearly increasing number of limiting factors as proposed beforewould not fit the data as well as the exponential relaxation used in our model.

---

### Diverse and robust molecular algorithms using reprogrammable DNA self-assembly [^38c5f011]. Nature (2019). Excellent credibility.

Molecular biology provides an inspiring proof-of-principle that chemical systems can store and process information to direct molecular activities such as the fabrication of complex structures from molecular components. To develop information-based chemistry as a technology for programming matter to function in ways not seen in biological systems, it is necessary to understand how molecular interactions can encode and execute algorithms. The self-assembly of relatively simple units into complex products 1 is particularly well suited for such investigations. Theory that combines mathematical tiling and statistical-mechanical models of molecular crystallization has shown that algorithmic behaviour can be embedded within molecular self-assembly processes 2,3, and this has been experimentally demonstrated using DNA nanotechnology 4 with up to 22 tile types 5–11. However, many information technologies exhibit a complexity threshold-such as the minimum transistor count needed for a general-purpose computer-beyond which the power of a reprogrammable system increases qualitatively, and it has been unclear whether the biophysics of DNA self-assembly allows that threshold to be exceeded. Here we report the design and experimental validation of a DNA tile set that contains 355 single-stranded tiles and can, through simple tile selection, be reprogrammed to implement a wide variety of 6-bit algorithms. We use this set to construct 21 circuits that execute algorithms including copying, sorting, recognizing palindromes and multiples of 3, random walking, obtaining an unbiased choice from a biased random source, electing a leader, simulating cellular automata, generating deterministic and randomized patterns, and counting to 63, with an overall per-tile error rate of less than 1 in 3,000. These findings suggest that molecular self-assembly could be a reliable algorithmic component within programmable chemical systems. The development of molecular machines that are reprogrammable-at a high level of abstraction and thus without requiring knowledge of the underlying physics-will establish a creative space in which molecular programmers can flourish.

---

### Integrating digital pathology and mathematical modelling to predict spatial biomarker dynamics in cancer immunotherapy [^fb4e40ba]. NPJ Digital Medicine (2022). Medium credibility.

Optimisation methodology

For the model optimisation, we selected 1000 sets of the four chosen parameters (IMpprol, IMpdeath, IMrwalk and IMpkill) from uniform or logarithmic distributions within realistic physiological ranges, as indicated in Table 4. Model simulations were performed using each parameter set for every tile, for every patient in the training set, and the simulations were repeated five times to allow averaging over stochastic runs. The end-time of the simulation was set according to the real time-point of the on-treatment biopsy sample for each patient. The spatial distribution of CD8 cells from observed biopsies was compared with that of the simulated biopsies via the SAM, providing a score between 0–1 for every parameter set, for every patient as summarised in the heatmap in Fig. 4. There are several circumstances in which there are exceptions to this method which are handled as follows:
In cases where the simulations ended before the prescribed end-time (usually due to the tumour being eradicated), the last simulated time point was used to calculate the SAM.
In cases where there are very few CD8 cells on either the simulated or observed biopsy sample, the RDFs are dominated by noise and the SAM is no longer considered representative. Whenever the majority of tiles from the observed on-treatment biopsy for any given patient contained less than ten CD8 cells, we excluded these samples from the SAM calculations. In the case that there were less than 10 CD8 cells in the majority of simulated tiles for any given parameter set, the SAM was not calculated and a NaN was recorded. In these cases, rather than comparing the spatial distribution of CD8 cells in collections of tiles, the number of CD8 cells per tile is compared instead. Specifically, the two-sample t -test was calculated to evaluate whether the total CD8 cells per tile in observed and simulated tiles could belong to the same underlying distribution, and therefore whether the parameter set should be accepted.

In order to identify parameter sets that generated realistic on-treatment biopsies across the population, we ranked the parameter sets by the mean SAM across all patients and accepted all parameter sets where the SAM was > 0.7. From the selected parameter sets, we excluded those that would not yield sufficient variability as measured by the VarSAM (explained above). The threshold for VarSAM was 0.3. The resulting final population parameter set contains 12 parameter combinations that we consider to produce the most realistic biological behaviour.

---

### Uncomputability of phase diagrams [^9c1212a3]. Nature Communications (2021). High credibility.

A set of Wang tiles — square, 2D tiles with coloured sides, with the rule that adjacent sides of neighbouring tiles in a tiling of the plane must have matching colours — can be mapped to a classical Hamiltonian: if tiles t i, t j cannot be placed next to each other, then we introduce a terminto the Hamiltonian. The overall Hamiltonian is the sum over all such local terms, such that its ground state represents a tiling satisfying the tiling rules (if such a tiling exists). If no such tiling exists, the ground state energy is ≥ 1.

Similarly, it is well known that there exist tile sets that encode the evolution of a classical TM, within a square grid: TM tape configurations are represented by rows, such that adjacent rows represent successive time steps of the TM (Fig. 2).

Fig. 2
The evolution of a classical TM can be represented by Wang tiles, where colours of adjacent tiles have to match, and arrowheads have to meet arrow tails of the appropriate kind.

Here, the evolution runs from the bottom of the square to the top, where it places a marker
- on the boundary as explained in section 'Classical tiling with quantum overlay'. In this image, the TM's evolution is contained in an individual square in the checkerboard grid shown in the figure below.

We combine both Wang tiles and the TM tiling ideas by constructing a tile set whose valid tilings have the following properties:
A tiling pattern that creates a square grid across the lattice Λ (much like a checkerboard). The side length of the grid squares can be varied (provided all grid squares are the same size) and still correspond to a valid tiling (depicted in Fig. 3).
Within each square of the grid, we use the tiles to encode a TM that first counts the size of the square it is contained in, and then outputs a marker
- on the top border of the square, where the placement of this marker is a function of the size of the square (depicted in Fig. 2).

---

### Bias in mobility datasets drives divergence in modeled outbreak dynamics [^a796bdb8]. Communications Medicine (2025). Medium credibility.

Meta Data for Good

In addition to CDR data, we analyzed mobility estimates from Meta's Data for Good program. Since Meta's Data for Good is free and openly available to response workers and researchers who request access, it has been used for mobility estimates in several modeling studies, especially during the SARS-CoV-2 pandemic –. While data from Meta is more accessible to researchers than CDR data, there are important differences in the populations captured by the different data sources. CDR users include people who have a cell phone with a SIM card, whereas Meta's Data For Good captures an open cohort of people who meet the following criteria: (1) have a smartphone, (2) have the Meta application on their phone, and (3) have shared their location history data by having location services enabled on their phone.

We used Meta's Movement Between Tiles and Population (Tile Level) datasetsfrom the time period March 21-September 30, 2020. User location is categorized by Bing Tiles. Population data are based on users' most frequently visited tile during 8-hour windows. The Movement Between Tiles data consist of movement vectors estimated based on changes in users' modal location in the current 8-hour window compared to their modal location in the preceding 8-hour window. Lastly, to help preserve anonymity, if the number of users is below 10 users for a given 8-hour window, the observation is dropped from the dataset.

We calculated district-level values by mapping the Bing Tiles' centroids to districts and summing across values that fell in each district. Daily estimates were calculated by taking an average across the three, 8-hour windows per day.

---

### Complex shapes self-assembled from single-stranded DNA tiles [^84f12b1b]. Nature (2012). Excellent credibility.

Programmed self-assembly of strands of nucleic acid has proved highly effective for creating a wide range of structures with desired shapes. A particularly successful implementation is DNA origami, in which a long scaffold strand is folded by hundreds of short auxiliary strands into a complex shape. Modular strategies are in principle simpler and more versatile and have been used to assemble DNA or RNA tiles into periodic and algorithmic two-dimensional lattices, extended ribbons and tubes, three-dimensional crystals, polyhedra and simple finite two-dimensional shapes. But creating finite yet complex shapes from a large number of uniquely addressable tiles remains challenging. Here we solve this problem with the simplest tile form, a 'single-stranded tile' (SST) that consists of a 42-base strand of DNA composed entirely of concatenated sticky ends and that binds to four local neighbours during self-assembly. Although ribbons and tubes with controlled circumferences have been created using the SST approach, we extend it to assemble complex two-dimensional shapes and tubes from hundreds (in some cases more than one thousand) distinct tiles. Our main design feature is a self-assembled rectangle that serves as a molecular canvas, with each of its constituent SST strands — folded into a 3 nm-by-7 nm tile and attached to four neighbouring tiles — acting as a pixel. A desired shape, drawn on the canvas, is then produced by one-pot annealing of all those strands that correspond to pixels covered by the target shape; the remaining strands are excluded. We implement the strategy with a master strand collection that corresponds to a 310-pixel canvas, and then use appropriate strand subsets to construct 107 distinct and complex two-dimensional shapes, thereby establishing SST assembly as a simple, modular and robust framework for constructing nanostructures with prescribed shapes from short synthetic DNA strands.

---

### Satellite mapping reveals extensive industrial activity at sea [^8c3e2c81]. Nature (2024). Excellent credibility.

First, we are classifying the environmental context in which the vessel in question operates. To do so, we constructed 11 gridded fields (rasters) with a resolution of 0.01° (approximately 1 km per pixel at the equator) and with global coverage. At every pixel, each raster contains contextual information on the following variables: (1) vessel density (based on SAR); (2) average vessel length (based on SAR); (3) bathymetry; (4) distance from port, (5) and (6) hours of non-fishing-vessel presence (from the AIS) for vessels less than 50 m and more than 50 m, respectively; (7) average surface temperature; (8) average current speed; (9) standard deviation of daily temperature; (10) standard deviation of daily current speed; and (11) average chlorophyll. For every detected vessel, we sampled 100 × 100-pixel tiles from these rasters, producing an 11-band image that we then classified with the ConvNet. Each detection is thus provided with context in an area just over 100 × 100 km. We obtained the fishing and non-fishing labels from AIS vessel identities.

Second, our predictions are produced with an ensemble of two models with no overlap in spatial coverage. To avoid leakage of spatial information between the training sets of the two models, and also to maximize spatial coverage, we divided the centre of the tiles into a 1° longitude and latitude grid. We then generated two independent labelled datasets, one containing the tiles from the 'even' and the other from the 'odd' latitude and longitude grid cells. This alternating 1° (the size of the tile) strategy ensures no spatial overlap between tiles across the two sets. We trained two independent models, one for 'even' tiles and another for 'odd' tiles, with each model 'seeing' a fraction of the ocean that the other model does not 'see'. The test set that we used to evaluate both models contains tiles from both 'even' and 'odd' grid cells, with a 0.5° buffer around all the test grid cells removed from all the neighbouring cells (used for training) to ensure spatial independence across all data partitions (no leakage). By averaging the predictions from these two models, we covered the full spatial extent of our detections with independent and complementary spatial information.

---

### Deepath-MSI: a clinic-ready deep learning model for microsatellite instability detection in colorectal cancer using whole-slide imaging [^481e370b]. NPJ Precision Oncology (2025). Medium credibility.

Diagnostic performance on real-world validation set

Initially, 2267 colorectal cancer cases were included. Of these, 2236 cases meet the minimum tumor tiles (n = 100) and enrolled in this study, achieving a successful rate of 98.6% (2236/2267) (Fig. 3A). Among the enrolled cases, 111 (5.0%) were classified as dMMR, while the remaining 2125 (95.0%) were classified as pMMR. Detailed clinicopathological features are presented in Table 2. The dMMR ratio showed a significant association with factors such as age, primary tumor site, tumor size, histological type and tumor differentiation.

Fig. 3
Clinical workflow.

Performance evaluation of the model in the real-world validation set. A Overview of the real-world validation set with dMMR/pMMR status. B Distribution of the number of tiles per slide for real-world validation set. C Distribution of MSI scores in dMMR and pMMR specimens. D AUROC for the real-world validation set. E The overall accuracy, sensitivity, specificity, PPV and NPV of the model in the real-world validation set.

Table 2
Clinicopathological features and performance of Deepath-MSI model in the real-world validation set

a A total of 1248 samples were derived from left- or right- hemicolon tumors, b Tumor size data were missing for 12 cases, c Tumor differentiation data were missing for 27 cases.

In the real-world validation set, the median number of tiles per sample was 681, with a range from 106 to 2924 (Fig. 3B). The distribution of MSI score in real-world validation set was shown in Fig. 3C. With a prespecified threshold of 0.4, the model achieved AUROC of 0.978 (Fig. 3D), along with a sensitivity of 94.6%, a specificity of 90.7%, a PPV of 34.7%, a NPV of 99.7% (Fig. 3E). Overall, 90.9% of specimens were accurately classified according to their true MSI status. Additionally, performance was evaluated across various clinicopathological subgroups. Notably, the model exhibited relatively diminished performance in specific subgroups, including those with tumors located in the ascending or transverse colon, right-sided colon cancer, tumors larger than 6 cm, tumors present with mucus, or poorly differentiated tumors (Table 2).

---

### Human mobility is well described by closed-form gravity-like models learned automatically from data [^beae4d97]. Nature Communications (2025). High credibility.

Information about municipalities and census tracts

We obtained shapefiles containing the geographical coordinates of the polygons delimiting census tracts and municipalities from. We also retrieved population data of each municipality using the GEO ID.

Finally, using a local copy ofwith, we retrieved information about Points of Interest (POI) in each census tract (see Supplemantary Information and supplementary Table S5). We selected 18 categories of OSM elements that represent geographical, demographic and socio-economic features of the different municipalities.

Construction of train and test datasets

To speed up the training process, we train the models with the same random sample of 1000 points of the training fold except for the Deep Gravity model for which we have to use a lager number of data points for training.

Economic classification of municipalities

We retrieve the median income per capita of each municipality and state as of 2020 from. Then, we classify each municipality with the label rich if the median income of the municipality is above the median income of the state, or poor if the median income per capita of the municipality is below the median income of the state.

Mobility data at small scales from Simini et al.

The code available for theprovides data at the level of census tracts for the New York State area. Mobility flows are obtained from the same dataset. In order to obtain the predictions of the Deep Gravity model for each individual trip and also the same dataset, we run the program and save, for each trip, the corresponding tile, the real value, the prediction and the variables. We store the train and test sets for the comparison with other methods.

---

### Predicting modular functions and neural coding of behavior from a synaptic wiring diagram [^cde9ce98]. Nature Neuroscience (2024). High credibility.

Table 1
Parameters used for image alignment

Stitching of tiles (montaging) within a single section was split into a linear translation step (premontage) and a nonlinear elastic step (elastic montage). In the premontage step, individual tiles were assembled to have 10% overlap between neighboring tiles, as specified during imaging and by fixing a single tile (anchoring) in place. They were then translated row by row and column by column according to the signal correspondence found between the overlaps. In the elastic montage step, the locations of the tiles were initialized from the translated locations found previously, and blockmatches were computed every 100 pixels on a regular triangular mesh (see Table 1 for the parameters used). Once the correspondences were found, outliers were filtered by checking the spatial distribution of the cross-correlogram (σ filter), height of the peak of the correlogram (Pearson r value), dynamic range of the source patch contrast, kurtosis of the source patch, local consensus (average of immediate neighbors) and global consensus (inside the section). After the errors had been corrected by filtering bad matches, the tiles were modeled as individual spring meshes attached via the matches, and the linear system was solved using conjugate gradient descent. The mean residual errors were in the range of 0.5–1.0 pixels after relaxation.

---

### An interpretable machine learning system for colorectal cancer diagnosis from pathology slides [^5a4291d0]. NPJ Precision Oncology (2024). Medium credibility.

Experimental setup

For our experimental setup, we divide our data into training and validation sets. Besides, we further evaluate the performance of the former in our test set composed of slides never seen by any of the methods presented or in the literature. Following the split of these three sets, we have 8587, 1009 and 900 stratified non-overlapping samples in the training, validation and test set, respectively.

In an attempt to also contribute to reproducible research, the training of all the versions of the proposed algorithm uses the deterministic constraints available on PyTorch. The usage of deterministic constraints implies a trade-off between performance, either in terms of algorithmic efficiency or on its predictive power, and the complement with reproducible research guidelines. As such, due to the current progress in the field, we have chosen to comply with the reproducible research guidelines.

All the trained backbone networks were ResNet-34with ImageNet weights. PyTorch was used to train these networks with the Adaptive Moment Estimation (Adam)optimiser, a learning rate of 6 × 10 −6 and a weight decay of 3 × 10 −4. The training batch size was set to 32 for both fully and weakly supervised training, while the test and inference batch size was 256. The performance of the model was verified on the validation set used for model selection in terms of the best accuracies and quadratic weighted kappa (QWK). The training was accelerated by an Nvidia Tesla V100 (32GB) GPU for 50 epochs of both weakly and fully supervised learning. In addition to the proposed methodology, we extended our experiments to include the aggregation approach proposed by Neto et al.on top of our best-performing method. This strategy does not consider spatial location or context of the tiles, instead it select the seven most severe tiles, concatenates the output of the last convolutional layer for each of those tiles and feeds it to a multi-layer perceptron.

The number of epochs for training the fully- and weakly-supervised models was selected as follows: the fully supervised model was evaluated at every ten epochs for its performance on a weakly supervised scenario (using the non annotated samples), when its performance was stable the training was stopped; for the weakly-supervised model, several experiments were conducted on smaller versions of the training set and validated on the validation set. For the latter, besides training for 50 epochs, the best weights (with respect to the performance on the validation set) were selected.

---

### Pattern recognition in the nucleation kinetics of non-equilibrium self-assembly [^788a8c74]. Nature (2024). Excellent credibility.

For pattern recognition experiments, we enhanced concentrations of tiles in the SHAM mix in accordance with each of the 18 training images (using the optimized θ) and annealed each of the 18 mixes with a 150 hour ramp from 48 to 45 °C. As verified by AFM imaging and real-time fluorescence quenching, we found that the 18 training images yielded correct nucleation, in the sense that there was more of the correct shape than any other shape and in all but five cases was highly (more than 80%) selective (Fig. 6).

Fig. 6
Pattern recognition results with a pre-existing multifarious system.

a – c, All images (three shown) are converted (a) through a single pixel-to-tile map θ to vectors of tile concentrations (b), which are shown mapped onto tile locations in each shape, with nucleation rate predictions (c). d, Normalized fluorescence over time in hours (one label per shape; other label configurations shown in Extended Data Fig. 9) during a 150 hour temperature ramp from 48 to 45 °C, and final AFM images. Scale bars, 100 nm. e, Summary of results for both fluorescence and AFM for all 36 images, and a uniform 60 nM tile concentration control sample. Above, colours of vertical lines indicate the target shape for each pattern, whereas triangular markings of each colour indicate the relative fraction of growth time (on right) or fraction of shapes counted in AFM images (on left) for the corresponding shape (solid markings indicate target shape). Dashed lines indicate samples with no significant quenching or observed shapes. Below, ternary plots summarize the same results, with proximity to triangle corners indicating relative fractions of growth time (right) or counted shapes (left) and circle size indicating overall growth time (right) or total number of shapes (left).

---

### Human tripartite cortical network model for temporal assessment of α-synuclein aggregation and propagation in Parkinson's disease [^df4368cb]. NPJ Parkinson's Disease (2024). Medium credibility.

The number of tiles obtained for different sample groups were prepared for quantification of α-s aggregation as follows: 3 dpt control, 6 regions n = 34 tiles; 3 dpt PFF treated, 6 regions n = 36 tiles; 6 dpt control, 5 regions n = 31 tiles; 6 dpt PFF treated, 6 regions n = 24 tiles; 13 dpt control, 5 regions n = 29 tiles; 13 dpt PFF treated, 6 regions n = 29 tiles, and 3 additional regions, n = 19, were also acquired for presynaptic protein quantification for 6 dpt PFF. Details for tile information for each sample group are given in Supplementary Table 3.

Prior to quantification, deconvolution was performed on confocal tile images with Huygens Essential v23.04 (Scientific Volume Imaging, The Netherlands,) with standard settings. 3D reconstruction and quantification were achieved with the Imaris v9.5 (Oxford Instruments,) surface detection tool on the images. α–s assemblies were detected and automatically reconstructed with the Surface tool. In summary, the foreground intensity levels were initially set to values greater than 5 (ranging from 0 to 255). Surfaces were reconstructed, and volumes outside the range of 1 µm³ to 100 µm³ were excluded from further analysis. The Intensity Mean, representing the average intensity within each voxel of a detected surface, was set to 22, and the Intensity Center, indicating the intensity of the voxel at the center of a detected surface, was set to 100. These parameter settings were chosen to emphasize serpentine-like aggregates with high fluorescence intensities. The number of aggregates and their total size per tile were calculated. The tiles that did not present any aggregation were shown as zero in the analysis.

---

### High-throughput whole-slide scanning to enable large-scale data repository building [^4aa48e30]. The Journal of Pathology (2022). Medium credibility.

High‐throughput whole‐slide scanning

High‐throughput slide scanning can be distinguished from conventional slide scanning by the presence of all three of the following factors:
Scan speed: the scanner must be able to scan slides quickly at the target magnification. Although manufacturers often cite the 15 × 15 scan speed (the time it takes to scan a 15 × 15 mm square region in the slide), there are several factors that are often not captured by this metric. For example, the geometry of the tissue on the slide can impact the time it takes to scan a slide, particularly for line scanners (as opposed to tile scanners, Figure 2). This effectively makes the scan area larger, leading to longer scan times. Also, other factors such as the time it takes for the scanner to load the slide onto the stage can vary considerably across scanners, and in many cases pre‐scan snapshots, focal plane capture, and file post‐processing can be a significant factor in the duration of a scan, which may not necessarily be accounted for in the manufacturer's specification. There are a multitude of other factors as well that can impact speed considerably, such as tile overlap, objective magnification, focal plane range and precision, tissue detection sensitivity, geometric parameters, and others. Therefore, the 'real world' speed of a scanner on slides likely to be encountered in practice should be assessed before predicting throughput. To demonstrate how this may vary across modern scanners, we measured the scan time from nine sample slides scanned on four different scanners (Table 1). The fastest scanner that we tested usually scanned slides in under 2 min, and for some slides, under 1 min. The two slowest scanners were 7.4× slower, on average, with particularly long scan times for large tissue sections.
Capacity: the scanner capacity must be large enough to enable batch scanning without the need for frequent human interaction. Many modern whole‐slide scanners can accommodate several hundred to a thousand slides per batch. The optimal capacity for a high‐throughput operation should be built around the anticipated volume for the project. For example, if only 400 slides are loaded by staff per day, a 1,000‐slide capacity scanner will likely have a lot of unused capacity. Conversely, a single 100‐slide scanner may also not be optimal, as the staff would have to unload/reload the scanner several times per day to accommodate their volume. Depending on budget, available space, and the needs of the project, several smaller‐capacity scanners may sometimes be preferable to a single large‐capacity scanner, as they can operate in parallel, therefore scanning more quickly (all else being equal) and enabling users to mitigate downtime by scanning on the remaining scanners in case of scanner failure. It may also provide a more flexible method for prioritizing urgent scans, although many manufacturers have built such functionality into their scanners already.
Automation: a high‐throughput operation fundamentally relies on the ability to batch scan multiple slides quickly without significant user intervention or excessive setup time. This requires a degree of automation that, at minimum, relies on accurate automatic tissue detection and focusing but also benefits from other factors as described in the following section.

---

### Deepath-MSI: a clinic-ready deep learning model for microsatellite instability detection in colorectal cancer using whole-slide imaging [^12086675]. NPJ Precision Oncology (2025). Medium credibility.

Performance of the Deepath-MSI model across cohorts

In this study, a Deepath-MSI model was employed to predict MSI status directly from digitized H&E slides of CRC tissue (Fig. 1C). Our findings indicate that MSI status can be accurately determined using histopathological images, with the model achieving an AUROC of 0.976 in test set (Table 1, Fig. 2A). The Deepath-MSI model displayed robust performance across various cohorts. Specifically, AUROC values ranged from 0.967 in the NBPC cohort to 0.988 in the SHGH cohort. Scanner variability was also assessed, with AUROC values ranging from 0.950 for the 3D Histech scanner to 0.991 for the KFBIO scanner. Population-based analyses further revealed that the model performed similarly in the Chinese population (AUROC 0.977) compared to the Western population (AUROC 0.968) (Figs. S1–2).

Fig. 2
Performance evaluation of the model in the test set.

A AUROC for the test set. B Distribution of MSI scores in MSI-H and MSS specimens. C Relationship between AUROC, the percentage of included specimens and the number of tiles per specimen. D Representative examples of MSI-H and MSS predictions.

Table 1
Performance of the Deepath-MSI model

To enhance clinical utility, we established an optimal MSI score threshold for the Deepath-MSI model by fixing the sensitivity at 95% across the test set. This yielded an MSI score threshold of 0.4, at which the model achieved a sensitivity of 95.0%, specificity of 91.7%, PPV of 69.0%, NPV of 99.0% and an overall accuracy of 92.2% (Table 1).

The distribution of MSI score in test set was shown in Fig. 2B. To ensure sufficient representation of tumor tissue and model reliability, we determined the minimum number of tumor tiles required for quality control. From the analysis of the test set, we established a requirement of at least 100 tiles, achieving an AUROC of 0.971 and covering 100% of samples (Fig. 2C, Table S3). This threshold corresponds to approximately 6.6 mm² of tumor area, ensuring stable predictive performance. Figure 2D represents representative examples of MSI-H and MSS predictions.

---

### Adversarial attacks and adversarial robustness in computational pathology [^fc6c4a9f]. Nature Communications (2022). High credibility.

Statistics

The main statistical endpoint was the patient-wise micro-averaged area under the receiver operating curve (AUROC). 95% confidence intervals were obtained by 1000-fold bootstrapping based on sampling with replacement. The test dataset remained the same for the experiments between different models. All experiments were repeated five times with different random seeds. We reported the mean AUROC with standard deviation (SD) and median AUROC with interquartile range (IQR =). Two-sided unpaired t -tests were used to compare sets of AUROCs between different deep learning models for the same experimental condition. No correction for multiple testing was applied. Furthermore, we calculated the attack success rate (ASR). The ASR quantified the effectiveness of an attack by calculating the degree of misclassification: if the model's prediction score for the perturbed image changes, the attack was deemed successful. The ASR was calculated for 450 randomly selected tiles per class from the AACHEN-RCC set.

Reporting summary

Further information on research design is available in the Nature Research Reporting Summary linked to this article.

---

### An analog-AI chip for energy-efficient speech recognition and transcription [^545f5354]. Nature (2023). Excellent credibility.

Inference was achieved in two steps (Extended Data Fig. 2c). During the integration phase, PWM pulses activated in each row for a time proportional to the desired input magnitude (unlike ref. these durations were not converted to analog voltages using DACs). V read was forced by a per-column operational amplifier, which biased the entire bit line. These pulses were buffered along the row to maintain pulse-width integrity. Although IR drops did occur along columns, the wide wires stopped them being critical to degradation of MAC accuracy, especially when compared with other more-important factors such as peripheral circuit linearity and saturation effects. Current was then mirrored into a per-column capacitor, which could be tuned by the LC by connecting up to 8 parallel metal-oxide-semiconductor capacitors, where each capacitor was 50 fF (we typically chose 250 fF). The choices of capacitor size and range of tunability were based on the available column area, the expected current in the array, the integration time and the mirror ratios achievable. The summation over an entire 512-row tile was performed fully in analog, without the need for partial summation in the digital domain. In the wide-input case involving two vertically neighbouring tiles (Fig. 1i), summation over 1,024 rows (or even 2,048 in the two 2-PCM-per-weight case) was still fully performed in the analog domain, without any intermediate digitization. For layers that used wide input, the read operation during closed-loop tuning used this combined configuration, allowing an individual weight to experience and correct for the same non-idealities that it would experience in the eventual inference MAC. This provided significant mitigation from additional MAC error induced by combining tiles. Depending on the sign of the input, the current could be steered to either charge or discharge the capacitor. After current integration, the tile was disconnected and the output duration was generated. During this step, a tunable ramp circuit, shared among all columns, set a linear voltage ramp that was compared with the voltage on the 512 peripheral capacitors (Extended Data Fig. 2d). For each column, the output voltage started high, and when the comparator switched, the output duration ended, determining the duration of that particular output pulse, which is similar to the approaches in refs. Finally, an AND port enabled or disabled the pulse output. With proper enable signal timings controlled from the LC, activation functions such as ReLU or hard sigmoid could be implemented on chip. The 512 durations were produced in parallel, exiting the tile on 512 individual wires. Area-efficient design choices (such as the use of a common ramp generator circuit shared across all the columns, the elimination of a conventional ADC and associated digital registers, as well as optimized full-custom layouts) enabled dedicated per-column circuitry at pitch, without the need for column multiplexers.

---

### A brownian quasi-crystal of pre-assembled colloidal penrose tiles [^4fab437d]. Nature (2018). Excellent credibility.

Penrose's pentagonal P2 quasi-crystal 1–4 is a beautiful, hierarchically organized multiscale structure in which kite- and dart-shaped tiles are arranged into local motifs, such as pentagonal stars, which are in turn arranged into various close-packed superstructural patterns that become increasingly complex at larger length scales. Although certain types of quasi-periodic structure have been observed in hard and soft matter, such structures are difficult to engineer, especially over large areas, because generating the necessary, highly specific interactions between constituent building blocks is challenging. Previously reported soft-matter quasi-crystals of dendrimers 5, triblock copolymers 6, nanoparticles 7 and polymeric micelles 8 have been limited to 12- or 18-fold symmetries. Because routes for self-assembling complex colloidal building blocks 9–11 into low-defect dynamic superstructures remain limited 12, alternative methods, such as using optical and directed assembly, are being explored 13,14. Holographic laser tweezers 15 and optical standing waves 16 have been used to hold microspheres in local quasi-crystalline arrangements, and magnetic microspheres of two different sizes have been assembled into local five-fold-symmetric quasi-crystalline arrangements in two dimensions 17. But a Penrose quasi-crystal of mobile colloidal tiles has hitherto not been fabricated over large areas. Here we report such a quasi-crystal in two dimensions, created using a highly parallelizable method of lithographic printing and subsequent release of pre-assembled kite- and dart-shaped tiles into a solution-dispersion containing a depletion agent. After release, the positions and orientations of the tiles within the quasi-crystal can fluctuate, and these tiles undergo random, Brownian motion in the monolayer owing to frequent collisions between neighbouring tiles, even after the system reaches equilibrium. Using optical microscopy, we study both the equilibrium fluctuations of the system at high tile densities and also the 'melting' of the pattern as the tile density is lowered. At high tile densities we find signatures of a five-fold pentatic liquid quasi-crystalline phase, analogous to a six-fold hexatic liquid crystal. Our fabrication approach is applicable to tiles of different sizes and shapes, and with different initial positions and orientations, enabling the creation of two-dimensional quasi-crystalline systems (and other systems that possess multiscale complexity at high tile densities) beyond those of current self- or directed-assembly methods 18–20. We anticipate that our approach for generating lithographically pre-assembled monolayers could be extended to create three-dimensional Brownian systems of fluctuating particles with custom-designed shapes through holographic lithography 21,22 or stereolithography 23.

---

### Validating whole slide imaging for diagnostic purposes in pathology: guideline from the college of American pathologists pathology and laboratory quality center [^ddcb0ec9]. Archives of Pathology & Laboratory Medicine (2013). Medium credibility.

Supplemental Table 12 — Evidence to Decision Summary of Recommendation 3 reports response distributions as follows: For "Is the problem a priority?", Yes was 70% (7/10) with 20% (2/10) Probably No, 10% (1/10) Probably Yes, and 0 for No, Varies, and Don't Know. For "How accurate is the test?", Very Inaccurate 0, Inaccurate 10% (1/10), Accurate 30% (3/10), Very Accurate 30% (3/10), Varies 20% (2/10), and Don't Know 10% (1/10). For "How substantial are the desirable anticipated effects?", Trivial 0, Small 0, Moderate 20% (2/10), Large 40% (4/10), Varies 20% (2/10), and Don't Know 20% (2/10). For "What is the overall certainty of the evidence of test accuracy?", Very Low 10% (1/10), Low 10% (1/10), Moderate 20% (2/10), High 40% (4/10), Very High 20% (2/10), and No Included Studies 0. For "What is the overall certainty of the evidence of effects of the management that is guided by the test results?", Very Low 10% (1/10), Low 20% (2/10), Moderate 30% (3/10), High 30% (3/10), Very High 10% (1/10), and No Included Studies 0. For "How certain is the link between test results and management decisions?", Very Low 10% (1/10), Low 20% (2/10), Moderate 10% (1/10), High 40% (4/10), Very High 10% (1/10), and No Included Studies 10% (1/10). For "What is the overall certainty of the evidence of effects of the test?", Very Low 10% (1/10), Low 0, Moderate 40% (4/10), High 40% (4/10), Very High 10% (1/10), and No Included Studies 0. For "What is the overall certainty of the evidence for any critical or important direct benefits, adverse effects or burden of the test?", Very Low 10% (1/10), Low 20% (2/10), Moderate 30% (3/10), High 40% (4/10), Very High 0, and No Included Studies 0.

---

### Direct gene expression profile prediction for uveal melanoma from digital cytopathology images via deep learning and salient image region identification [^653eefe6]. Ophthalmology Science (2023). Medium credibility.

Results

In total, our study included 89 whole-slide images from 82 patients and 121 388 unique ROIs (image tiles). Of the 82 patients, 32 were women, and the mean age was 62.2 years at the time of diagnosis. Our training set included 65 slides from 58 patients. Of the 58 tumors, there were 28 class 1 tumors, the mean thickness was 6.3 mm (range, 0.7–16.2 mm), and the mean largest basal diameter was 12.1 mm (range, 2.2–20 mm). Of the 58 tumors, 8, 38, and 12 were small, medium, and large tumors by the COMS standard, respectively. Our testing set included 24 slides from 24 patients (12 class 1 tumors; 12 class 2 tumors; 1 slide per patient). Of the 24 tumors, the mean thickness was 6.0 mm (range, 1.0–11.5 mm), and the mean largest basal diameter was 12.6 mm (range, 3.5–18 mm). Of the 24 tumors, 2, 15, and 7 were small, medium, and large tumors by the COMS standard, respectively. Our DL system for GEP status prediction achieved an area under the receiver operating characteristic curve of 0.944, an accuracy of 91.7%, a sensitivity of 91.7%, and a specificity of 91.7% on a patient-level analysis. The accuracy, sensitivity, and specificity numbers were identical by chance because there was 1 false-positive and 1 false-negative during testing.

---

### Uncovering 2-D toroidal representations in grid cell ensemble activity during 1-D behavior [^b6fb5f8b]. Nature Communications (2024). High credibility.

Rate maps and autocorrelograms

Spatial positions in the open field were binned into a 30 2 square grid, generating spatial rate maps. The mean neural activity in each bin was computed and spatially convolved with a Gaussian filter of width 2 bins (during which non-visited bins were assigned the mean value of the visited bins).

1D spatial autocorrelograms were computed for the linearized positions on the running wheel and the VR track, using 1 and 4 cm spatial bins, respectively. Autocorrelograms for each neuron were computed by finding the mean activity in each spatial bin and taking the dot product between this and a zero-padded copy of it, iteratively shifting the latter up to 300 bins.

Toroidal firing rate maps were calculated in the same way as the OF arena, first binning the toroidal surface into a square grid of 12° × 12° bins and computing the average activity in each position bin. To spatially smooth the toroidal rate map, the 60° angle of the toroidal axes was addressed by first shifting the bins horizontally by a length equal to half the bins' vertical position. To address boundary conditions, nine copies of the shifted rate map were then tiled and spatially smoothed as the OF rate maps. The middle tile was then extracted and shifted back. For visualizations, the resulting rate maps were given 15° shear angles both horizontally and vertically.

The angular tuning curves were computed using 60 bins and max-normalized.

For 3-D toroidal tuning, radial downsampling of the coordinates (ϵ = 0.5) was first used to obtain spherical bins, and each center point was colored by the mean value for each bin.

---

### Clinical decision rules: how to use them [^62ab325a]. Archives of Disease in Childhood: Education and Practice Edition (2010). Low credibility.

The first of this pair of papers outlined what a clinical decision rule is and how one should be created. This section examines how to use a rule, by checking that it is likely to work (examining how it has been validated), understanding what the various numbers that tell us about "accuracy" mean, and considers some practical aspects of how clinicians think and work in order to make that information usable on the front lines.

---

### American Clinical Neurophysiology Society technical standards for electrical stimulation with intracranial electrodes for functional brain mapping and seizure induction [^ae78ffdd]. Journal of Clinical Neurophysiology (2025). High credibility.

ACNS intracranial electrode specifications — subdural electrodes list Total Contact Diameter (mm) 2.0, 3.0, 4.0*, 4.5, 6.0 and Exposed Contact Diameter (mm) 1.5, 1.8, 2.3*, with Inter-contact Distance (mm) 5, 10*, 15; Number of Contacts include Strips: 2, 4, 6, 8, 10, 12, 14, 16 and Grids: 8 to 256 (common: 8, 16, 64). Stereotactic depth electrodes are detailed with Diameter (mm) 0.8*, 0.86*, 1.12, 1.26; Contact length (mm) 1.32, 2.0, 2.29*, 2.41*, 2.5, 5.0; Inter-contact Distance (mm) 2.2, 3.0, 3.5, 3.97, 4.0, 4.43, 5.0, 7.0, 8.0, 10.0; Number of Contacts 4, 5, 6, 8, 10, 12, 14, 15, 16, 18; Other Configurations 3 sets of 5 or 6 contacts, inter-contact distance 3.5 mm, with 7 or 11 mm between sets; and Recording Lengths (mm) 17 to 92. Values marked with an asterisk are noted as used relatively commonly in clinical practice.

---

### Therapy intensity level scale for traumatic brain injury: clinimetric assessment on neuro-monitored patients across 52 European intensive care units [^d11ccb47]. Journal of Neurotrauma (2024). Medium credibility.

FIG. 7.
Relationship between TIL and TIL (Basic). The numeric definition of each scale is listed in Table 1, and the calculation of daily (e.g. TIL 24), maximum (e.g. TIL max), and median (e.g. TIL median) scores are described in the "Methods" section. The 95% confidence intervals of information coverage were derived from bootstrapping with 1000 resamples of unique patients over 100 missing value imputations. (A) Distribution of corresponding TIL (Basic) 24 scores per each TIL 24 score. The values in each cell represent the percent of assessments at a given TIL 24 score (i.e. column) corresponding to a TIL (Basic) 24 score (i.e. row). The vertical, dark red lines represent cut-offs across which the median corresponding TIL (Basic) 24 score per TIL 24 score changes. (B) Distribution of corresponding TIL 24 scores per each TIL (Basic) 24 score. The width of violin plots is scaled for each TIL (Basic) 24 score, but the width of the points inside them demonstrates relative frequency across the TIL (Basic) 24 scores. The grey, shaded zones represent the range of TIL 24 scores with corresponding median TIL (Basic) 24 scores on the x -axis, as determined in panel (A). (C) The information of TIL 24, TIL max, and TIL median covered by TIL (Basic) 24, TIL (Basic) max, and TIL (Basic) median, respectively. Information coverage is defined as the mutual information of TIL 24 and TIL (Basic) 24 (or TIL max and TIL (Basic) max or TIL median and TIL (Basic) median) divided by the entropy of TIL 24 (or TIL max or TIL median). AUC, area under the receiver operating characteristic curve; ICU, intensive care unit; TIL, Therapy Intensity Level scale; TIL (Basic), condensed TIL scale.

---

### AGA clinical practice guideline on management of gastroparesis [^c9c3d497]. Gastroenterology (2025). High credibility.

Supplemental Figure 12 — Domperidone for early satiety and/or post-prandial fullness: With scales transformed to a 0 to 4 range, the Silvers 1998* study shows a mean difference (MD) of -0.41 [-0.73; -0.10], and when transformed to standardized mean difference (SMD) the result is -0.35 [-0.63; -0.08] with total 208. The plots display the direction labels "Favors domperidone" and "Favors placebo", and the figure notes that a "Common (or fixed) effect model was used when the number of studies was less than 5" and that " indicates that imputation was used".

---

### Therapy intensity level scale for traumatic brain injury: clinimetric assessment on neuro-monitored patients across 52 European intensive care units [^199c1400]. Journal of Neurotrauma (2024). Medium credibility.

Table 1.
Scoring Configurations for TIL and Alternative Scales

TIL 24, the daily TIL score based on the sum of the highest scores per item per calendar day,
TIL max, the maximum TIL 24 over the first week of a patient's ICU stay,
TIL median, the median TIL 24 over the first week of a patient's ICU stay.

We also calculated scores from four other therapeutic intensity scales to compare with TIL scores. The 21-point, unweighted TIL (uwTIL) scale replaces each sub-item score in TIL with its ascending rank index (i.e. 1, 2, 3,…) within each item (Table 1). The five-category TIL (Basic) was also developed by the CDE scheme for TBI and takes the maximum score, from zero (i.e. no ICP-related intervention) to four, amongst all included sub-items over the calendar day. We adapted the 38-point PILOTand 15-point TIL (1987) scaleswith minor adjustments to fit the items of TIL with a daily assessment frequency. PILOT also was shown to have high inter-rater and intra-rater reliability by Shore and colleagues. For the four alternative scales, daily (i.e. uwTIL 24, TIL (Basic) 24, PILOT 24, and TIL (1987) 24), maximum (i.e. uwTIL max, TIL (Basic) max, PILOT max, and TIL (1987) max), and median (i.e. uwTIL median, TIL (Basic) median, PILOT median, and TIL (1987) median) scores were calculated in the same way as TIL 24, TIL max, and TIL median, respectively.

---

### The snm procedure guideline for general imaging 6.0 [^638aaec4]. SNMMI (2010). Medium credibility.

SNM Procedure Guideline for General Imaging — matrix size and pixel depth in nuclear medicine acquisitions notes that matrix size is almost always a power of 2 with typical values 64 x 64, 128 x 128, 256 x 256 and 512 x 512, and that non-square matrix sizes also exist for whole-body studies. Each pixel can be represented with a single byte (pixel values ranging from 0 to 255 counts) or with 16 bit words (pixel values ranging up to a maximum of 32k or 64k). Overflow occurs when the number of counts recorded at some given position exceeds the maximum number of counts, and overflow is more likely to occur when using a byte matrix.

---

### Heat health risk assessment in philippine cities using remotely sensed data and social-ecological indicators [^60015fc3]. Nature Communications (2020). High credibility.

The relative weights were determined based on an analytic hierarchy process pairwise comparisonsurvey questionnaire administered to a number of experts (academics and researchers). In total, 26 experts were consulted (24 questionnaires were received in response; 12 of these were screened out) (Supplementary Tables 2 and 3); this number is within the range considered in other, related studies –. The concept of aggregating components and indicators and using relative weights was consistent with the IPCC's statement regarding the derivation of vulnerability indexes, namely, "a climate vulnerability index is typically derived by combining, with or without weighting, several indicators assumed to represent vulnerability" [p. 1775].

Before aggregation, the indicators' data values were first normalized to a common 0 to 1 value range to make them comparable to each other and enable aggregation at the indicator and component levels. Here, we used the min–max normalization method, a technique that performs a linear transformation of the original data. All the indicators used fell under either an interval or ratio scale of measurement (Supplementary Table 1); therefore, the same normalization method was applied to all indicators.

The risk index (RI) of city i was calculated on the basis of the derived indexes of the three components of risk (see below), by using a weighted arithmetic (additive) aggregation procedure (Eq. (1)),.whereis the value of city i for index j, is the relative weight of index j, and n is the number of indexes. Here, n = 3, referring to the three components of risk. The relative weights of the three components sum to 1 (Supplementary Table 2).

With the use of 12 sets of relative weights derived from the 12 experts' questionnaire responses (Supplementary Tables 2 and 3), the whole process produced 12 risk assessment results. The overall HHRI values of the cities were derived by taking the average of these 12 risk assessment results. The HHRI values were later categorized into a five-level qualitative scale: very high (0.80 to 1.00), high (0.60 to 0.80), moderate (0.40 to 0.60), low (0.20 to 0.40), and very low (0.00 to 0.20). The upper limit of each category level was inclusive.

---

### Pixel-level deep segmentation: artificial intelligence quantifies muscle on computed tomography for body morphometric analysis [^928835ff]. Journal of Digital Imaging (2017). Low credibility.

Image Conversion: HU to Grayscale

Medical images contain 12 to 16 bits per pixel, ranging from 4096 to 65,536 shades of gray per pixel. A digital CT image has a dynamic range of 4096 gray levels per pixel (12 bits per pixel), far beyond the limits of human perception. The human observer can distinguish many hundred shades of gray, and possibly as high as 700–900, but substantially less than the 4096 gray levels in a digital CT image. Displays used for diagnostic CT interpretation support at most 8 bits per pixel, corresponding to 256 gray levels per pixel. To compensate for these inherent physiologic and technical limitations, images displayed on computer monitors can be adjusted by changing the window level (WL) and window width (WW), followed by assigning values outside the window range to minimum (0) or maximum (2 BIT -1) value, as described in Fig. 5 a. The WL — the center of the window range — determines which HU values are converted into gray levels. The WW determines how many of HU values are assigned to each gray level, related to the slope of the linear transformation shown in Fig. 5 a. BIT, the available number of bits per pixel, determines how many shades of gray are available per pixel. The effects of the three configurations on image appearance are demonstrated with four examples images in Fig. 5 b. The optimal window setting configuration is dependent on the HUs of the region of interest (ROI) and the intrinsic image contrast and brightness. These settings are ultimately workarounds for the constraints of human perception. However, computer vision does not necessarily have these limitations.

Fig. 5
(a) The relationship between gray level and Hounsfield units (HU) determined by window level (WL), window width (WW), and bit depth per pixel (BIT). (b) The effect of different WL, WW, and BIT configurations on the same image

Most prior investigations have converted CT images to grayscale with the commonly used HU range for the target tissue or organ without studying the effect of window settings on the performance of their algorithms. While recent work has identified that image quality distortions limit the performance of neural networks in computer vision systems, the effect of window setting and bit resolution on image quality is often overlooked in medical imaging machine learning. Therefore, we evaluated the effects of window and BIT settings on segmentation performance by sweeping different combinations of window configurations and bit depth per pixel.

---

### Summary benchmarks-full set – 2024 [^5c7a7a31]. AAO (2024). High credibility.

Retina — recommended follow-up intervals for posterior vitreous detachment (PVD) and select retinal lesions are as follows: Asymptomatic PVD does not require visits except routine follow up unless symptoms develop; symptomatic PVD without retinal breaks and without high-risk features is followed at 4–6 weeks unless further symptoms develop; symptomatic PVD without a break but with vitreous or retinal hemorrhage is followed at 1–2 weeks depending on severity, and for vitreous hemorrhage, weekly until resolved with ultrasonography to check for retinal tears as needed; acute symptomatic horseshoe tears are followed 1–2 weeks after treatment, then 4–6 weeks, then 3–6 months, then annually; acute symptomatic operculated holes are followed 1–4 weeks, then 1–3 months, then 6–12 months, then annually; traumatic retinal breaks are followed 1–2 weeks after treatment, then 4–6 weeks, then 3–6 months, then annually; asymptomatic horseshoe tears are followed 1–4 weeks, then 2–4 months, then 6–12 months, then annually; asymptomatic operculated holes are followed 1–4 months, then 6–12 months, then annually; and asymptomatic dialyses are followed, if untreated, 1–4 weeks, then 3 months, then 6 months, then every 6 months, and if treated, 1–2 weeks after treatment, then 4–6 weeks, then 3–6 months, then annually.

---

### Standards and guidelines for validating next-generation sequencing bioinformatics pipelines: a joint recommendation of the Association for Molecular Pathology and the college of American pathologists [^101e0942]. The Journal of Molecular Diagnostics (2018). Medium credibility.

Minimum number of variants and calculation — For in-scope variants, they fall into one of three categories: SNVs, indels, and other; the last category includes variants that cannot be classified as SNVs or indels, such as vertically complex variants that are ≤ 21 bp in contiguous size or variants that require special algorithmic approaches; for each category in scope, the minimum number of variants to include in the validation sample set can be calculated as n = ln(1 – CI)/ln(P), where n is the number of variants to be included, CI is the confidence level of detection, and P is the probability of detection.

---

### Robust estimates of overall immune-repertoire diversity from high-throughput measurements on samples [^96ca99ba]. Nature Communications (2016). Medium credibility.

Sampling

We assume that each clone in the individual contributes cells to the sampled population according to a Poisson distribution. This will be true if (i) clones are well mixed in the blood or evenly distributed in the tissue being sampled, (ii) the parent population is sufficiently large that the Poisson estimate for the probability of, for example, a singleton contributing > 1 cell is negligible and (iii) no single clone is a large fraction (∼30% or more) of the parent population. In practice, condition (iii) is satisfied by counting large clones directly (see the 'Fitting').

Fitting

The largest clones may be represented by hundreds or even thousands of cells in a sample. For such large clones, sampling error is small: the relative size of the clone in the sample and in the individual will be about the same. As a result, clones that are large enough to have sufficiently small sampling error do not have to be fit by EM, and instead can simply be added to the MLE. We found that using a threshold of 30 cells, and therefore applying EM only to clones that contribute ≤ 30 cells to the sample and then adding larger clones back to the resulting MLE gives results that are indistinguishable from applying EM on the entire sample distribution, but with vast gains in speed. (Note that observing an absence of clones at a given size counts towards the number of observations used for calculating the Akaike information criterion.)

Scanning

In the standard EM algorithm, the exact sizes and frequencies of clones in the final MLE can vary depending on the sizes and frequencies used at the start of each iteration, reflecting different local maxima. To find global maxima, we developed a 'scanning' approach in which we applied EM to many starting clone sizes and frequencies (56 in our implementation), ranking results by maximum likelihood (after first adjusting likelihoods according to the number of ways to choose clones in each distribution; see Supplementary Methods). In each round, we perform an additional fit with starting clone sizes and frequencies at an average of the two top-ranked results. We then select the resulting best-ranked fit from the starting points. Runtime and (to some extent) accuracy correlate with the number of starting points.

---

### A rationally engineered decoder of transient intracellular signals [^2ff70d78]. Nature Communications (2021). High credibility.

Enumeration of the master network's sub-topologies yields a set of 4122 possible decoder topologies. Experimentally testing that many alternatives is impossible, although efficient methods for circuit construction exist. Also, existing computational design methods either help designing dynamic circuits for few topologies that can be enumerated and analyzed individually, or focus on logical circuits using well-characterized components. Because neither condition applied here, we extended our Bayesian circuit design methodto a rapid prototyping method, TopoDesign. Without requiring a catalog of specific biological parts, it explores possible combinations of hypothetical parts. TopoDesign accounts for uncertain knowledge about the parts and their biological variability, which allows capitalizing on existing biological parts that may be ill-characterized.

We define a design objective and a dynamic mathematical model for the set of possible topologies (Fig. 1c, Supplementary Fig. 1; Supplementary Methods). For the model, we used commonly applied specifications of processes and interactions, such as Hill functions for gene expression control; note that all model inferences and predictions are contingent on this formulation. Topological filtering, explores simultaneously model topologies and parameters to find functional topologies T i. For each T i, at least one parameter set is viable: it achieves the design objective. By efficient sampling, we also obtain the corresponding viable space V i, that is, the parameter space for correct circuit behavior. This allows us to define two metrics for topology robustness and feasibility (Fig. 1c). Our robustness metric is a quantitative version of the "Q-value" or "robustness score". It measures globally how much of the parameter space is viable, giving the theoretical probability of a circuit to achieve the design objective, without prior knowledge on parameters. However, with correlated parameters, a topology may not tolerate variation in individual parameters. To account for such dependencies between parts in practice, we therefore measure feasibility: the proportion of a parameter distribution that fits into the viable space (Fig. 1c). The definition of the metric is distinct fromand critical: it enables a systematic integration of experimental data, and thereby all iterations of computation and experiments. Without further information, we compute ideal feasibility by assuming optimal parameters with small variance (Fig. 1c; Supplementary Methods).

---

### Summary benchmarks-full set – 2024 [^0ecc241d]. AAO (2024). High credibility.

Idiopathic epiretinal membrane and vitreomacular traction — surgery and postoperative care state that vitrectomy surgery is often indicated in patients who are affected by a decrease in visual acuity, metamorphopsia, double vision, or difficulty using their eyes together, and that patients should be examined postoperatively day 1, and again 1 to 2 weeks following surgery, or sooner depending upon the development of new symptoms or new findings during early postoperative examination.

---

### A modified cued recall test for detecting prodromal AD in adults with down syndrome [^d3e7cd04]. Alzheimer's & Dementia (2022). Medium credibility.

2.2.1 Neuropsychological evaluations

For the mCRT, stimuli were two sets of 12 black‐and‐white line drawings of simple objects, each representing a distinct semantic category, adapted from the Snodgrass and Vanderwart picture set. Items were selected to be easily recognizable objects, although not the prototypical exemplar of their respective category.

During the learning phase, participants were shown three 8½ × 11 white, laminated cards, one at a time, with each illustrating four items, one item per quadrant (Figure 1).

FIGURE 1
Version A and Version B of the modified Cued Recall Test (mCRT)

Participants were asked to point to and name aloud each item (e.g. grapes) in response to the verbal presentation of the semantic category (e.g. "What is the piece of fruit?"). After the four items on the card were pointed to and named, the card was turned over and participants were asked to immediately recall them. For items not recalled, the card was shown for a second time, and participants were reminded of those items not recalled on the preceding trial. The card was then withdrawn, and participants were given another opportunity to recall. If a participant successfully named all four items, the next set of four items was presented and the process repeated. If a participant failed to recall all four items, they were shown the card a third time and asked to point to and name the items. This continued until either all four items were named or three learning trials per card were completed.

The testing phase immediately followed and consisted of three trials, each requiring recall of all 12 items. Initially participants were asked to recall as many items as possible. Approximately 1 minute was allowed for free recall, after which category cues were provided for missed items. One point was given for each item that was recalled correctly.

Following Devenny et al.the total number of items recalled across all three trials, combining free and cued recall, was selected as the primary measure of performance (range = 0–36). Additional measures included (1) the total number of correct responses during free recall only (range = 0–36); (2) the number of items correctly recalled on each of the three trials individually (range 0–12 items per trial); and (3) the number of extra‐list responses (intrusions, range from 0 upward).

---

### A practical guide for understanding confidence intervals and P values [^c0a7d4a0]. Otolaryngology — Head and Neck Surgery (2009). Low credibility.

The 95 percent confidence interval about the mean demarcates the range of values in which the mean would fall if many samples from the universal parent population were taken. In other words, if the same observation, experiment, or trial were done over and over with a different sample of subjects, but with the same characteristics as the original sample, 95 percent of the means from those repeated measures would fall within this range. This gives a measure of how confident we are in the original mean. It tells us not only whether the results are statistically significant because the CI falls totally on one side or the other of the no difference marker (0 if continuous variables; 1 if proportions), but also the actual values so that we might determine if the data seem clinically important. In contrast, the P value tells us only whether the results are statistically significant, without translating that information into values relative to the variable that was measured. Consequently, the CI is a better choice to describe the results of observations, experiments, or trials.

---

### Descriptive statistics: reporting the answers to the 5 basic questions of who, what, why, when, where, and a sixth, so what? [^85777ffe]. Anesthesia and Analgesia (2017). Low credibility.

Descriptive statistics are specific methods basically used to calculate, describe, and summarize collected research data in a logical, meaningful, and efficient way. Descriptive statistics are reported numerically in the manuscript text and/or in its tables, or graphically in its figures. This basic statistical tutorial discusses a series of fundamental concepts about descriptive statistics and their reporting. The mean, median, and mode are 3 measures of the center or central tendency of a set of data. In addition to a measure of its central tendency (mean, median, or mode), another important characteristic of a research data set is its variability or dispersion (ie, spread). In simplest terms, variability is how much the individual recorded scores or observed values differ from one another. The range, standard deviation, and interquartile range are 3 measures of variability or dispersion. The standard deviation is typically reported for a mean, and the interquartile range for a median. Testing for statistical significance, along with calculating the observed treatment effect (or the strength of the association between an exposure and an outcome), and generating a corresponding confidence interval are 3 tools commonly used by researchers (and their collaborating biostatistician or epidemiologist) to validly make inferences and more generalized conclusions from their collected data and descriptive statistics. A number of journals, including Anesthesia & Analgesia, strongly encourage or require the reporting of pertinent confidence intervals. A confidence interval can be calculated for virtually any variable or outcome measure in an experimental, quasi-experimental, or observational research study design. Generally speaking, in a clinical trial, the confidence interval is the range of values within which the true treatment effect in the population likely resides. In an observational study, the confidence interval is the range of values within which the true strength of the association between the exposure and the outcome (eg, the risk ratio or odds ratio) in the population likely resides. There are many possible ways to graphically display or illustrate different types of data. While there is often latitude as to the choice of format, ultimately, the simplest and most comprehensible format is preferred. Common examples include a histogram, bar chart, line chart or line graph, pie chart, scatterplot, and box-and-whisker plot. Valid and reliable descriptive statistics can answer basic yet important questions about a research data set, namely: "Who, What, Why, When, Where, How, How Much?"

---

### A space-time tradeoff for implementing a function with master equation dynamics [^57fa10b0]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### Statistical analysis of high-dimensional biomedical data: a gentle introduction to analytical goals, common approaches and challenges [^082c9871]. BMC Medicine (2023). Medium credibility.

Results produced by clustering algorithms are difficult to evaluate and often require subjective judgement. The validity of the results depends on the notion of a cluster, which varies between clustering algorithms, and this ambiguity carries through to estimation of the number of clusters (Table 14).

Table 14
Methods for estimation of the number of clusters: Scree plots, silhouette values

Fig. 13
Example of a scree plot, which involves plotting some measure of within-cluster variation (here the total within sum of squares) on the y -axis and the number of clusters assumed in applying the algorithm on the x -axis. Source:

Fig. 14
Silhouette values for observations that are grouped into four clusters. Observations are sorted along the x -axis by decreasing silhouette value, grouped by the four clusters. The silhouette values for the observations of the first two clusters have very low values, indicating two not well-separated clusters. Source:

Some clustering methods have been specifically developed to handle the typical large storage requirements and long run times for HDD settings. For example, CAST (Cluster Affinity Search Technique) is especially useful for large numbers of observations or variables. Iteratively, clusters are constructed as follows. Choose a randomly selected observation not already assigned to a cluster and assign it to a newly defined cluster. Then repeat the following two steps until the set of observations assigned to this new cluster no longer changes. Add unassigned observations with average similarity to the current cluster members above a predefined threshold, and remove observations with average similarity below this threshold.

Another method is subspace clustering, where first subsets of variables are identified (called subspaces) and clusters are determined by defining regions of values based only on these variables. Then, iteratively, lower-dimensional subspaces are combined to higher-dimensional ones. In biclustering (or two-way clustering), first introduced by Hartigan, simultaneously variables and observations are selected to generate clusters that do not depend on all variables at the same time. Again, heuristic and stable algorithms are required to find approximate solutions in acceptable time (see, e.g.).

---

### Artificial-intelligence-driven discovery of catalyst genes with application to COactivation on semiconductor oxides [^71cefbe0]. Nature Communications (2022). High credibility.

The details of SGD

The SGD was done with the RealKD code, modified to include quality functions described by Eqs. (1) and (2) in which the information gain was defined as:here p is the number of samples in a subgroup within the required adsorption energy range divided by the total number of samples in the subgroup. Since Shannon entropy is a symmetric parabola-like function around 0.5, we set here F (Z) = 0 for p ≤ 0.5. Also, x· ln(x) = 0 for x = 0. The search of subgroups is performed using a Monte-Carlo scheme adapted for these tasks.

The cutoff values x, y. used for setting propositions (feature-1 < x, feature-2 ≥ y, etc.) are obtained by k -means clustering, as implemented within RealKD. That is, for a desired number n = k − 1 of cutoff values a set of k representative values of a given feature and k groups (clusters) of the data points are determined that minimize the deviation of all the feature values from the representative values. Thus, each value of the feature in the dataset is assigned to a particular cluster, and the cutoffs are determined as the arithmetic mean between the closest feature values in neighboring clusters. The number k is a parameter, and different k -values can in principle result in different cutoff values. It is worth noting that, due to the stochastic Monte-Carlo sampling, the exact definitions of the subgroups may vary for consecutive runs of the SGD algorithm. We have tested k = 12, 14, and 16 and rerun the algorithm several times for each k. While the results indeed depend on the run and on the k value, the subgroups maximizing the quality function have largely or entirely overlapping populations, and selectors with the same or similar propositions. Here we report selectors that appear most often and have high population and quality function values.

---

### A faster and more accurate algorithm for calculating population genetics statistics requiring sums of stirling numbers of the first kind [^b9d5300d]. G3 (2020). Medium credibility.

Figure 3
(A) Comparison of relative error of the estimator from and the single term asymptotic estimator in (35). Relative error for each is calculated against the arbitrary precision implementation described in. In total, 10,000 calculations were performed with n randomly sampled from a uniform distribution between 50 and 500; m between 2 and n; and θ between 1 and 50. A solid diagonal line is drawn at. Dotted lines are drawn at a relative error of 0.001. Numbers within each quadrant defined by the dotted lines indicate the number of points in each quadrant. The red dot indicates the one case where the relative error wasand the error of (35) was greater than the estimator from. (B) Comparison of mollified error (34) as a function of m. For this plot, we fixed(solid lines) or 500 (dotted lines) and(as indicated by different line colors).

The fewer calculations led to a clear improvement in calculation speed (median 54.6x faster; Figure 4). The speedup also depends on the parameter choices; in general, the speed advantage is greater when the hybrid calculator requires many calculations (namely, when m is small relative to n, as the hybrid calculator performs the sum in (2)) (Figure 4B).

Figure 4
(A) Comparison of run times between the hybrid algorithm from and the single term asymptotic estimator in (35). 100 iterations were run, each with 10,000 calculations; the time elapsed for each set of 10,000 calculations was recorded and plotted here. The same set of parameters were used for each algorithm. The order of running the algorithms was alternated with each iteration. The dark horizontal line indicates the median, the box indicates the first and third quartiles, the whiskers are drawn at 1.5x the interquartile range, and outliers are represented by open circles. The median for the hybrid algorithm is 62.64 s; the median for the asymptotic algorithm is 1.17 s. (B) Detailed benchmarking for(open violins) or 500 (gray violins),… Fold speedup (ratio of the time taken for the hybrid calculator to that taken for the aysmptotic estimator) is plotted on the y-axis. Each dot represents one set of parameters; the violin plots summarize the density of points on the y-axis. Times were calculated for 100 iterations of each estimator for the same parameter values.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^22bd51a1]. Nature Communications (2021). High credibility.

Motif III: logic

A general class of functions used to describe natural,– and synthetic, biological networks are logic gates, which have two inputs regulating a single output (Fig. 5). Gates exhibit either high ("on") or low ("off") output depending on whether inputs are on or off. For example, the AND gate specifies high output only if both inputs are on. In this section we provide a specific DDE-based framework that covers 14 out of 16 possible 2-input logic operations, and show that these operations form a continuous 2D parameter space.

Fig. 5
A simple approximation for digital logic using a sum of Hill terms recapitulates all monotonic logic functions in a single parameter space.

a A prototypical regulatory network involving logic where X and Y both regulate Z, which must integrate the two signals using some logic before it can in turn activate a downstream reporter R. b Parameter space showing regions where regulation approximately follows 14 of the 16 possible 2-input logic functions depending on the strength of two single-variable Hill regulation terms (η Z 1: regulation of Z by X, η Z 2: regulation of Z by Y). Network logic can be smoothly altered by varying the parameters (η Z 1, η Z 2), with a change of sign in (n 1, n 2) required to switch quadrants. The bottom-left quadrant shows that very weak regulation in both terms leads to an always-off (FALSE) function, weak regulation in one arm only leads to single-input (X, Y) functions, strong regulation in both arms leads to an OR function, and regulation too weak in either arm alone to activate an output but strong enough in sum leads to an AND function. The other three quadrants are related by applying NOT to one or both inputs, with function names related by de Morgan's lawNOT(X OR Y) = NOT X AND NOT Y. In particular, X IMPLY Y = NOT(X) OR Y, X NIMPLY Y = X AND NOT(Y), X NOR Y = NOT X AND NOT Y, and X NAND Y = NOT X OR NOT Y. Truth tables for all 16 logic gates are provided in Supplementary Table 1 for reference. The two non-monotonic logic functions, X XOR Y and X XNOR Y, are those 2 of 16 not reproduced directly using this summing approximation. They can be produced by layering, e.g. NAND gates. c Representative time traces for AND (η Z 1 = η Z 2 = 0.9) and OR (η Z 1 = η Z 2 = 1.8) gates with n 1 = n 2 = −2, n 3 = −20, η R = η Z 1 + η Z 2. The functionwhen n > 0, when n < 0.

---

### Validating whole slide imaging for diagnostic purposes in pathology: guideline from the college of American pathologists pathology and laboratory quality center [^8c342dfb]. Archives of Pathology & Laboratory Medicine (2013). Medium credibility.

College of American Pathologists whole slide imaging (WSI) guideline — evidence-to-decision panel voting distributions are reported across certainty, values, balance of effects, and resource use. For "overall certainty of the evidence of test accuracy", votes were 0 0 30% (3/10) 60% (6/10) 10% (1/10) 0 across columns labeled Very Low, Low, Moderate, High, Very High, and No Included Studies. For "overall certainty of the evidence of effects of the management that is guided by the test results", votes were 0 20% (2/10) 20% (2/10) 60% (6/10) 0 0. For "How certain is the link between test results and management decisions?" the distribution was 0 10% (1/10) 40% (4/10) 20% (2/10) 30% (3/10) 0. For "overall certainty of the evidence of effects of the test", votes were 0 0 30% (3/10) 40% (4/10) 30% (3/10) 0, and for "overall certainty of the evidence for any critical or important direct benefits, adverse effects or burden of the test?" votes were 0 0 30% (3/10) 50% (5/10) 20% (2/10) 0. Regarding values, responses to "Is there important uncertainty about or variability in how much people value the main outcomes?" were 10% (1/10) 50% (5/10) 30% (3/10) 10% (1/10) across, respectively, Important uncertainty or variability, Possibly important uncertainty or variability, Probably no important uncertainty or variability, and No important uncertainty or variability. For "Does the balance between desirable and undesirable effects favor the intervention or the comparison?" ratings were 30% (3/10) 30% (3/10) 0 0 40% (4/10) 0 0 across Favors the comparison, Probably favors the comparison, Does not favor either the intervention or comparison, Probably favors the intervention, Favors the intervention, Varies, and Don't Know. For "How large are the resource requirements (costs)?", votes were 10% (1/10) 60% (6/10) 30% (3/10) 0 0 0 0 across Large costs, Moderate costs, Negligible costs and savings, Moderate savings, Large savings, Varies, and Don't Know.

---

### The three numbers you need to know about healthcare: the 60-30-10 challenge [^d6cb22ab]. BMC Medicine (2020). Medium credibility.

Background

Healthcare represents a paradox. While change is everywhere, performance has flatlined: 60% of care on average is in line with evidence- or consensus-based guidelines, 30% is some form of waste or of low value, and 10% is harm. The 60-30-10 Challenge has persisted for three decades.

Main Body

Current top-down or chain-logic strategies to address this problem, based essentially on linear models of change and relying on policies, hierarchies, and standardisation, have proven insufficient. Instead, we need to marry ideas drawn from complexity science and continuous improvement with proposals for creating a deep learning health system. This dynamic learning model has the potential to assemble relevant information including patients' histories, and clinical, patient, laboratory, and cost data for improved decision-making in real time, or close to real time. If we get it right, the learning health system will contribute to care being more evidence-based and less wasteful and harmful. It will need a purpose-designed digital backbone and infrastructure, apply artificial intelligence to support diagnosis and treatment options, harness genomic and other new data types, and create informed discussions of options between patients, families, and clinicians. While there will be many variants of the model, learning health systems will need to spread, and be encouraged to do so, principally through diffusion of innovation models and local adaptations.

Conclusion

Deep learning systems can enable us to better exploit expanding health datasets including traditional and newer forms of big and smaller-scale data, e.g. genomics and cost information, and incorporate patient preferences into decision-making. As we envisage it, a deep learning system will support healthcare's desire to continually improve, and make gains on the 60-30-10 dimensions. All modern health systems are awash with data, but it is only recently that we have been able to bring this together, operationalised, and turned into useful information by which to make more intelligent, timely decisions than in the past.

---

### Prognostic significance of tumor-infiltrating lymphocytes determined using linkNet on colorectal cancer pathology images | JCO precision oncology… [^5b751714]. ASCO (2023). Medium credibility.

23a combination of 3 × 3 and 5 × 5 convolution kernels, instead of the conventional single 3 × 3 convolutional kernels. We adopted the first block of the ResNet18 model as the initial block and the next four blocks as the encoder. Every encoder block contained two multiscale convolutional layers, followed by a BatchNorm2d layer, ReLu layer, and shortcut connections. The four encoder blocks were followed by four decoder blocks, each of which contained a 2 × 2 deconvolution layer and two 3 × 3 convolutional layers. Acknowledgment Data Supplement Authors retain all rights in any data supplements associated with their articles The ideas and opinions expressed in this Data Supplement do not necessarily reflect those of the American Society of Clinical Oncology. The mention of any product, service, or therapy in this Data Supplement should not be construed as an endorsement of the products mentioned.

It is the responsibility of the treating physician or other health care provider, relying on independent experience and knowledge of the patient, to determine drug dosages and the best treatment for the patient. Authors' Disclosures of Potential Conflicts of Interest Steven Xu Employment: Genmab Stock and Other Ownership Interests: Genmab. References J Clin Oncol32: 2959–2966, 2014 JCO Oncol Pract13: 301–303, 2017 N Engl J Med363: 1938–1948, 2010 Semin Cancer Biol52: 178–188, 2018. J Clin Oncol37: 559–569, 2019 J Pathol Clin Res8: 327–339, 2022 LinkNet: Exploiting Encoder Representations for Efficient Semantic Segmentation. Presented at the IEEE VCIP, 10–13 Dec. 2017, 2017. Information Published In Copyright History Published online: February 27, 2023 Permissions.

Authors Author Contributions Conception and design: Anran Liu, Xingyu Li, Hong Zhang, Steven Xu Financial support: Jitendra Jonnagaddala, Hong Zhang Administrative support: Hong Zhang Provision of study materials or patients: Xingyu Li, Jitendra Jonnagaddala.

---

### Summary benchmarks-full set – 2024 [^ac55ce29]. AAO (2024). High credibility.

Idiopathic macular hole management — stages 3 or 4: For stage 3 or 4, vitreoretinal surgery is performed usually within a few weeks to a few months of diagnosis, with postoperative follow-up at 1–2 days, then 1–2 weeks during which time strict face down positioning is advised; frequency and timing of subsequent postoperative visits varies depending on the outcome of surgery and the patient's clinical course.

---

### The hammam effect or how a warm ocean enhances large scale atmospheric predictability [^382ddf87]. Nature Communications (2019). High credibility.

Local dimension and persistence

To determine d, we exploit recent results from the application of extreme value theory to Poincaré recurrences in dynamical systems. This approach considers long trajectories of a system — in our case successions of daily SLP latitude–longitude maps — corresponding to a sequence of states on the attractor. For a given point ζ in phase space (e.g. a given SLP map), we compute the probability that the system returns within a ball of radiuscentred on the point ζ. The Freitas et al.theorem, modified by Lucarini et al. states that logarithmic returns:

yield a probability distribution such that:

where z = g (x (t)) and s is a high threshold associated to a quantile q of the series g (x (t)). Requiring that the orbit falls within a ball of radiusaround the point ζ is equivalent to asking that the series g (x (t)) is over the threshold s; therefore, the ball radiusis simply e − s(q). The resulting distribution is the exponential member of the Generalised Pareto Distribution family. The parameters μ and σ, namely the location and the scale parameter of the distribution, depend on the point ζ in phase space. μ (ζ) corresponds to the threshold s (q), while the local dimension d (ζ) can be obtained via the relation.

When x (t) contains all the variables of the system, the estimation of d based on extreme value theory has a number of advantages over traditional methods (e.g. the box counting algorithm). First, it does not require to estimate the volume of different sets in scale space: the selection of s (q) based on the quantile provides a selection of different scales s, which depends on the recurrence rate around the point ζ. Moreover, it does not require the a priori selection of the maximum embedding dimension as the observable g is always a univariate time-series.

The persistence of the state ζ is measured via the extremal index, an adimensional parameter, from which we extract. θ (ζ) is therefore the inverse of the average residence time of trajectories aroundand it has unit of a frequency (in this study 1/days). If ζ is a fixed point of the attractor. For a trajectory that leaves the neighbourhood of ζ at the next time iteration, θ = 1. To estimate θ, we adopt the Süveges estimator. For further details on the extremal index, see ref.

---

### Dolutegravir sodium and lamivudine (Dovato) [^c3bd8905]. FDA (2024). Medium credibility.

The dosage of dolutegravir sodium / lamivudine PO for treatment of human immunodeficiency virus type 1 infection in both children (in patients ≥ 12 years) is 50/300 mg PO daily

---

### Docusate sodium [^d673e47a]. FDA. Low credibility.

The dosage of docusate sodium PO for symptomatic relief of constipation in both children (in patients 2–12 years) (occasional) is 100 mg PO daily

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^d3e55ccc]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Read depth planning example — confidence versus tolerance interpretation: For example, to determine the probability of getting a minimum of 250 reads for a given region, if a validation set shows a mean depth of coverage 275 reads and a standard deviation (SD) of 50 reads, after running 100 samples the 95% lower confidence limit indicates confidence that the average depth of coverage would be > 266, whereas the 95% lower tolerance interval indicates that for any given sample one could only be confident of reliably getting a read depth of 179 or greater.

---

### Development of machine learning models to predict gestational diabetes risk in the first half of pregnancy [^8ff0fae6]. BMC Pregnancy and Childbirth (2023). Medium credibility.

Model performance

Table 5 shows the model type, number of input variables, whether or not DA was used, with "w/o DA" meaning that Data Augmentation is not used in this model, "DA LE", meaning Data Augmentation w/Limited Expert range, "DA EO", meaning Data Augmentation w/Expert original range, and the results of the following: Accuracy, Sensitivity, Specificity, Recall Macro, AUCROC, False Positives (FP), False Negatives (FN), and FP + FN. Table 5 show the top 4 models for each sensitivity level with the model that has the highest AUCROC in bold type, for models with up to 12 variables. All these metrics were computed for each model in the test set. As mentioned in the Methods section, the test set was only used to test the generalization capacity of the models. The test set was not used to train or to select the hyperparameters of the models. On Table 5 we show the results of models that reached a sensitivity above 0.9231 in the test set (model numbers 1 to 16), while model numbers 17 to 36 show the results of models with sensitivity above 0.7949 but below 0.9231 in the test set. Models with high sensitivity allow minimizing FN when screening patients. Sensitivity is important since the main goal is to prevent the serious consequences of GDM that may occur in mothers and babies even several years after pregnancy. Our method allows identifying many possible models for various levels of sensitivity and specificity. For example, model numbers 29–32 on Table 5 all have a high sensitivity of 0.82 and a specificity in the range 0.72–0.74, with accuracy between 0.73–0.75; AUCROC of 0.81; and Recall Macro between 0.77 and 0.78. A model could be selected from these ranges to have a good compromise between low numbers of FN and FP as is shown in the last column of Table 5.

---

### Principles of analytic validation of immunohistochemical assays: guideline update [^657277c4]. Archives of Pathology & Laboratory Medicine (2024). High credibility.

Principles of Analytic Validation of Immunohistochemical Assays: Guideline Update — supplemental digital content presents a table with multiple rows that display repeated per-column qualitative ratings using the labels moderate, low, and serious across entries.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^82094a02]. CDC (2011). Medium credibility.

Address and residency — For analyses that use address, the current address, if available, should be used, and patients with post office boxes or incomplete or inaccurate street addresses cannot be geocoded; a timeframe in which current address should be updated could not be agreed upon by the Workgroup, and analyses using residence or geographic analyses should specify when addresses were updated and the percentage of cases that could not be geocoded with a high level of confidence.

---

### Nearly 20 000 e-liquids and 250 unique flavour descriptions: an overview of the Dutch market based on information from manufacturers [^18b36688]. Tobacco Control (2021). Medium credibility.

Data analyses

The classified set of 19 266 products was analysed in R statistical software V.3.4.3. For each of the 16 main flavour categories, the following values were determined: the number of unique subcategories; the number of products (within each main and subcategory); the number of products with a secondary flavour description; the number of unique secondary flavour descriptions; and the average number of secondary flavour descriptions (within each main and subcategory).

Finally, the nicotine concentrations as declared by the industry were analysed. E-liquids with a package unit other than one and a volume other than 10 mL were excluded from this analysis (n = 2427), because declared nicotine values of these products could not be related to a unit. E-liquids with unusual nicotine values that could not be related to a common nicotine unit (n = 359) or without data on their nicotine concentration at all (n = 1343) were excluded. This resulted in a total set of 12 551 e-liquids for nicotine analysis. These e-liquids were divided into five groups according to their declared nicotine values, to represent the most common nicotine concentrations available on the Dutch market (0, 3, 6, 12 and 18 mg/mL). Most e-liquids contained a nicotine value that exactly matched these concentrations. Some values slightly deviated, therefore the following ranges were maintained: 0 mg/mL (n = 381), 3 mg/mL (range > 0 and < 4.5; n = 2836), 6 mg/mL (range ≥ 4.5 and < 9; n = 3702), 12 mg/mL (range ≥ 9 and < 15; n = 3229) and 18 mg/mL (range ≥ 15 and < 20; n = 2403).

---

### Docusate sodium [^06fbe802]. FDA. Low credibility.

The dosage of docusate sodium PO for symptomatic relief of constipation in both children (in patients ≥ 12 years) (occasional) is 100–300 mg PO daily, in 1–3 divided doses

---

### Limits on fundamental limits to computation [^98c96d2d]. Nature (2014). Excellent credibility.

An indispensable part of our personal and working lives, computing has also become essential to industries and governments. Steady improvements in computer hardware have been supported by periodic doubling of transistor densities in integrated circuits over the past fifty years. Such Moore scaling now requires ever-increasing efforts, stimulating research in alternative hardware and stirring controversy. To help evaluate emerging technologies and increase our understanding of integrated-circuit scaling, here I review fundamental limits to computation in the areas of manufacturing, energy, physical space, design and verification effort, and algorithms. To outline what is achievable in principle and in practice, I recapitulate how some limits were circumvented, and compare loose and tight limits. Engineering difficulties encountered by emerging technologies may indicate yet unknown limits.

---

### Itraconazole (Sporanox) [^7e8c0173]. FDA (2024). Medium credibility.

The dosage of itraconazole PO for treatment of onychomycosis in adults (toenails, with or without fingernail involvement, caused by dermatophytes, immunocompromised host) is 200 mg PO daily for 12 weeks

---

### Recovery of genomes from metagenomes via a dereplication, aggregation and scoring strategy [^9fab95f4]. Nature Microbiology (2018). Medium credibility.

Parameter estimation

To determine the optimal values for weighting factors b and c, and the score threshold t, we performed a grid search over a range of parameters. We applied DAS Tool with the range of parameters (b, c ∈ {0, 0.1, …,3}, t ∈ {0, 0.1, …,0.9}) on data from a synthetic microbial community that was constructed by mixing together DNA of 22 bacteria (including different species from the same genus) and 3 archaeaand evaluated the quality of the selected bins. Higher values of b and c resulted in higher average precision and recall of reconstructed bins (Supplementary Fig. 12a–d), but a lower total number of high-quality bins (Supplementary Fig. 12e, f). In contrast, a higher score threshold leads to higher average precision and recall of bins but lower number of total reported high-quality bins (Supplementary Fig. 12). We selected parameters that maximize the sum of the fraction of reconstructed high-quality bins, precision and recall. In general, the performance of DAS Tool was very robust to parameter variations on this relatively small data set of 25 genomes. Therefore, no unique optimum but a range of parameters (b, c ∈ {0.4,0.5,0.6}, t ∈ {0.3,0.4,0.5,0.6}) could be determined that maximize bin number, precision and recall. The analyses in this study were performed using b = 0.5, c = 0.5 and t = 0.5.

Assembly and mapping

The reads of the synthetic community and soil samples were quality filtered by SICKLE (version 1.21, default parameters) and assembled using IBDA_UD. All samples were assembled separately. Read mapping for all samples was done using Bowtie 2.

---

### Summary benchmarks-full set – 2024 [^6f68d3ad]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — use and ethics note states that "The PPPs are intended to serve as guides in patient care, with greatest emphasis on technical aspects", that "true medical excellence is achieved only when skills are applied in a such a manner that the patients' needs are the foremost consideration", and that "The Academy is available to assist members in resolving ethical dilemmas that arise in the course of practice".

---

### Healthy housing reference manual [^10031d72]. CDC (2006). Medium credibility.

Portable fire extinguishers — ratings, placement, and operation are outlined for home use. Fire extinguishers have a number rating for Type A fires in gallons of water: 1 means 1¼ gallons of water; 2 means 2½ gallons of water; 3 means 3¾ gallons of water, and for Type B and Type C fires the number represents square feet, for example 2 equals 2 square feet and 5 equals 5 square feet. For multi-class units, an extinguisher labeled 2A5B is good for Type A fires with a 2½-gallon equivalence and for Type B fires with a 5-square-foot equivalency. A good extinguisher to have in each residential kitchen is a 2A10BC fire extinguisher. An extinguisher should be placed in the kitchen and in the garage or workshop, and each extinguisher should be installed in plain view near an escape route and away from potential fire hazards such as heating appliances. PASS is a simple acronym to remind you how to operate most fire extinguishers — pull, aim, squeeze, and sweep. Pull the pin at the top of the cylinder. Aim the nozzle at the base of the fire. Squeeze or press the handle. Sweep the contents from side to side at the base of the fire until it goes out. Shut off the extinguisher and then watch carefully for any rekindling of the fire.

---

### Solu-medrol [^dffbb7bb]. FDA (2024). Medium credibility.

PRINCIPLE DISPLAY PANEL, SERIALIZED LABEL NDC 51662-1264-1

---

### Pseudarthrosis in primary fusions for adult idiopathic scoliosis: incidence, risk factors, and outcome analysis [^04b44d60]. Spine (2005). Low credibility.

Study Design

A retrospective study.

Objective

To analyze the incidence, characteristics, risk factors, and Scoliosis Research Society Instrument-24 (SRS-24) outcome scores of pseudarthrosis in adult idiopathic scoliosis primary fusions.

Summary Of Background Data

The healing of spinal fusion is complex and difficult to study in a clinical setting. There are no detailed reports on pseudarthrosis in primary fusion for adult idiopathic scoliosis since the introduction of "modern" segmental fixation techniques.

Methods

A retrospective chart and radiographic review of 96 patients (average age 42.2 years; range 18.2–62.9 years) with adult idiopathic scoliosis undergoing first time (primary) spinal instrumentation and fusion with a minimum 2-year follow-up (average 5.9 years; range 2–16.8 years) treated at a single institution between 1985 and 2001 were analyzed.

Results

Sixteen patients had pseudarthroses (17%). Fifty-nine percent of the pseudarthroses occurred between T9 and L1, and 81% presented with multiple levels involved (2–6 levels). The site of crosslinks or dominoes correlated with pseudarthrosis site in 69%. Pseudarthroses were detected radiologically at 32.4 months (range 12–67 months) postoperatively. Patient age at surgery more than 55 years significantly correlated with pseudarthrosis (P = 0.007). The number of fused levels more than 12 vertebrae is also significantly correlated with pseudarthrosis (P = 0.03). Smoking history and comorbidity did not increase the pseudarthrosis rate (P = 0.71 and 0.19, respectively). A larger preoperative Cobb angle (≥ 70 degrees) and a greater thoracic kyphosis (T5-T12 > 40 degrees) did not correlate with a higher pseudarthrosis rate (P = 0.76 and 0.73, respectively). Thoracolumbar kyphosis (T10-L2 ≥ 20 degrees) correlated with a significantly higher pseudarthrosis rate (P < 0.0001). Preoperative global sagittal and coronal imbalance did not increase the pseudarthrosis rate (P = 0.45 and 0.62, respectively). Patients with pseudarthrosis had lower SRS-24 scores than those without (P = 0.01). CONCLUSION.: The incidence of pseudarthrosis following adult idiopathic scoliosis primary fusion was 17%. The pseudarthrosis was most likely to occur at the thoracolumbar junction. Older patients (> 55 years), longer fusion (> 12 vertebrae), and those with thoracolumbar kyphosis (≥ 20 degrees) demonstrated increased risk for pseudarthrosis. Patients' outcomes as measured by the SRS-24 were "negatively" affected by the pseudarthrosis.

---

### Standards of care in diabetes – 2025 [^ef149bc3]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for diabetic nephropathy, more specifically with respect to pediatric patients, ADA 2025 guidelines recommend to determine the eGFR at the time of diagnosis and annually thereafter.

---

### Summary benchmarks-full set – 2024 [^53958861]. AAO (2024). High credibility.

Age-related macular degeneration (AMD) risk for progression and treatments by severity level — Table 5 lists: No drusen (AREDS category 1) with 5-year risk 0.5%, 10-year risk 1.5% and treatment option None; Early AMD (AREDS category 2) with 5-year risk 4%, 10-year risk 8% and treatment option None; Intermediate AMD (AREDS category 3) with 5-year risk 9%, 10-year risk 28% and treatment option AREDS2 antioxidant supplements; Advanced AMD – GA (AREDS category 4) with 5-year risk 45%, 10-year risk 71% and Consider intravitreal medication; Advanced AMD – MNV (AREDS category 4) with 5-year risk 45%, 10-year risk 71% and Anti-VEGF intravitreal injections may be recommended; and Extrafoveal Choroidal Neovascularization in AMD (AREDS category 4) with 5-year risk 45%, 10-year risk 71% and Anti-VEGF intravitreal injections or laser surgery may be recommended. Abbreviations are defined as AMD = Age-Related Macular Degeneration; Anti-VEGF = Anti-vascular endothelial growth factor; GA = geographic atrophy; MNV = macular neovascularization, and the risk of progression to advanced AMD is based on the simplified severity scale and based on the AREDS cohort aged 55 to 80 years of age; the AREDS design included participants ages 55–80 years old.

---

### Recommendations for tumor mutational burden assay validation and reporting: a joint consensus recommendation of the Association for Molecular Pathology, college of American pathologists, and Society for Immunotherapy of Cancer [^50750ea2]. The Journal of Molecular Diagnostics (2024). High credibility.

Tumor mutational burden (TMB) assay validation — orthogonal comparisons — was variably designed and reported. One-third of publications (n = 15) reported validation of TMB against an orthogonal standard, and most of these publications (n = 12) referenced whole-exome sequencing; when specified, approaches were in silico only in one-third of cases (n = 5), wet laboratory based in another third (n = 5), and both in some (n = 2). The number of cases included in orthogonal studies was highly variable, with studies using 1 to 50, 51 to 500, and > 500 cases; validation case sets were either focused on thoracic tumors specifically or included a broad distribution of solid tumor types. Most studies reported a correlation coefficient with values ranging from 0.62 to 0.99, with most > 0.85, and no studies commented on potential sources of bias that led to imperfect correlation with orthogonal standards.

---

### Vanzacaftor, tezacaftor, and deutivacaftor (Alyftrek) [^22f47176]. FDA (2025). Medium credibility.

The dosage of vanzacaftor / tezacaftor / deutivacaftor PO for treatment of cystic fibrosis in both children (in patients 6–12 years) (F508del or another responsive CFTR gene mutation) is 12/60/150 mg PO daily

---

### Vanzacaftor, tezacaftor, and deutivacaftor (Alyftrek) [^edebc2a1]. FDA (2025). Medium credibility.

The dosage of vanzacaftor / tezacaftor / deutivacaftor PO for treatment of cystic fibrosis in both children (in patients 6–12 years) (F508del or another responsive CFTR gene mutation) is 20/100/250 mg PO daily

---

### KDIGO 2017 clinical practice guideline update for the diagnosis, evaluation, prevention, and treatment of chronic kidney disease-mineral and bone Disorder (CKD-MBD) [^2abf2819]. Kidney International Supplements (2017). Medium credibility.

Regarding diagnostic investigations for chronic kidney disease-mineral and bone disorder, more specifically with respect to routine laboratory evaluation, KDIGO 2017 guidelines recommend to consider measuring 25-hydroxyvitamin D levels in patients with CKD stages 3-5D and repeat testing determined by baseline values and therapeutic interventions.

---

### 2025 ESC guidelines for the management of myocarditis and pericarditis [^670fb5cb]. European Heart Journal (2025). High credibility.

Regarding medical management for myocarditis, more specifically with respect to management of cardiac arrhythmias, antiarrhythmic drugs, ESC 2025 guidelines recommend to consider initiating β-blockers, with continuation for at least 6 months, in patients with acute myocarditis, particularly those with elevated troponin levels, to control symptoms and prevent arrhythmias.

---

### Cannabidiol (Epidiolex) [^17b9b14c]. FDA (2025). Medium credibility.

The dosage of cannabidiol PO for treatment of seizures in both children (in patients ≥ 1 year) with tuberous sclerosis complex is:

- **Start at**: 2.5 mg/kg PO BID
- **Maintenance**: 12.5 mg/kg PO BID

---

### KDIGO 2017 clinical practice guideline update for the diagnosis, evaluation, prevention, and treatment of chronic kidney disease-mineral and bone Disorder (CKD-MBD) [^d8e8a83a]. Kidney International Supplements (2017). Medium credibility.

Regarding diagnostic investigations for chronic kidney disease-mineral and bone disorder, more specifically with respect to routine laboratory evaluation, KDIGO 2017 guidelines recommend to consider measuring serum PTH or bone-specific ALP for the evaluation of bone disease in patients with CKD stages 3-5D as markedly high or low values predict underlying bone turnover.

---

### The snm procedure guideline for general imaging 6.0 [^0e7310d4]. SNMMI (2010). Medium credibility.

SNM Procedure Guideline for General Imaging 6.0 — field uniformity testing for scintillation cameras states that field uniformity should be tested each day a scintillation camera is used, using a solid- or liquid-filled uniform flood source to evaluate system (collimator on) uniformity; an acceptable alternative is to remove the collimator and produce an image using a distant (5 times the maximum detector dimension) point source, and the point source must be on the central axis. For planar imaging, images for small-field cameras must contain at least 1.25M counts, large-field circular, 2.5M counts and large rectangular field cameras, 4M counts, and all imaging parameters and the technologist's initials should be recorded.

---

### Standards of care in diabetes – 2025 [^ff82c0b4]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for obesity, more specifically with respect to patients with diabetes mellitus, lifestyle changes, ADA 2025 guidelines recommend to offer weight management as the primary goal of treatment along with glycemic management in patients with T2DM and overweight or obesity.

---

### Carp [^1feb27b6]. FDA (2009). Low credibility.

Therapeutic Use: Recommended dosage & range

Check the listed ingredients to verify that it matches the prescription ordered. When using a prescription set, verify the patient's name and the ingredients listed with the prescription order. Assess the patient's physical and emotional status prior to giving as injection. Do not give injections to patients who are in acute distress. Parenteral drug products should be inspected visually for particulate matter and discoloration prior to administration, whenever solution and container permit.

Dosage of allergenic extracts is a highly individualized matter and varies according to the degree of sensitivity of the patient, his clinical response and tolerance to the extract administered during the early phases of an injection regimen. The dosage must be reduced when transferring a patient from non-standardized or modified extract to standardized extract. Any evidence of a local or generalized reaction requires a reduction in dosage during the initial stages of immunotherapy as well as during maintenance therapy. After therapeutic injections patients should be observed for at least 20 minutes for reaction symptoms.

SUGGESTED DOSAGE SCHEDULE

The following schedule may act as a guide. This schedule has not been proven to be safe or effective. Sensitive patients may begin with smaller doses of weaker solutions and the dosage increments can be less.

Preparation Instructions:

All dilutions may be made using sterile buffered diluent. The calculation may be based on the following ratio:

---

### The minimal work cost of information processing [^927873c9]. Nature Communications (2015). Medium credibility.

Classical mappings and dependence on the logical process

Our result, which is applicable to arbitrary quantum processes, applies to all classical computations as a special case. Classically, logical processes correspond to stochastic maps, of which deterministic functions are a special case. As a simple example, consider the AND gate. This is one of the elementary operations computing devices can perform, from which more complex circuits can be designed. The gate takes two bits as input, and outputs a single bit that is set to 1 exactly when both input bits are 1, as illustrated in Fig. 2a.

The logical process is manifestly irreversible, as the output alone does not allow to infer the input uniquely. If one of the inputs is zero, then the logical process effectively has to reset a three-level system to zero, forgetting which of the three possible inputs 00, 01 or 10 was given; this information can be viewed as being discarded, and hence dumped into the environment. We can confirm this intuition with our main result, using the fact that a general classical mapping is given by the specification of the conditional probability p (x ′| x) of observing x ′ at the output if the input was x. Embedding the classical probability distributions into the diagonals of quantum states, the infinity norm in expression (2) becomes simply

---

### Healthy housing reference manual [^ca56453a]. CDC (2006). Medium credibility.

Household diurnal water-use pattern — The amount of water we use in our homes varies during the day, with lowest rate of use — 11:30 pm to 5:00 am; sharp rise/high use — 5:00 am to noon (peak hourly use from 7:00 am to 8:00 am); moderate use — noon to 5:00 pm (lull around 3:00 pm); and increasing evening use — 5:00 pm to 11:00 pm (second minor peak, 6:00 pm to 8:00 pm).

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^01148c3a]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Nonparametric tolerance intervals for non-normal distributions: The above estimate of the tolerance interval would only be applicable to a population that is normally distributed, but when the underlying population is often not normal (eg, when there is a natural boundary that the data cannot exceed (ie, 0% or 100%)) it is helpful to define tolerance intervals using nonparametric methods; the one-sided nonparametric tolerance interval can be determined by finding the value for k that satisfies the cumulative binomial equation, where CL is the confidence level (eg, 0.95), and by setting k = 0 (ie, 0 failures) the formula can be simplified.

---

### KDIGO 2017 clinical practice guideline update for the diagnosis, evaluation, prevention, and treatment of chronic kidney disease-mineral and bone Disorder (CKD-MBD) [^5c067e90]. Kidney International Supplements (2017). Medium credibility.

Regarding diagnostic investigations for chronic kidney disease, more specifically with respect to evaluation of mineral and bone disorders, KDIGO 2017 guidelines recommend to consider measuring serum PTH or bone-specific ALP for the evaluation of bone disease in patients with CKD stages 3-5D as markedly high or low values predict underlying bone turnover.

---

### What's new? [^fd479f89]. seer.cancer.gov (2025). Medium credibility.

The Login window would sometimes fall behind other windows.
13. The Arrange window functions Tiled, Horizontal, Vertical, and Cascade will arrange all windows on the monitor that the current window resides in.
14. When viewing a dictionary from the Select Database dialog, you should not be able to change the variables.
27. You could click on a group to edit it in the Merge Variable dialog, then get an error if you kept the same name.
28. Creating a Rate session with a custom database would cause a crash if there were invalid variables present.
29. On the Case Listing Table tab, the Both button at the bottom was only adding to one list.
8. When importing variables, the import would sometimes miss values in a variable.
9. **Note**: You can change the font and font size on the Result window by pressing the Font button in the toolbar.
10. Updated internal version to match SEER*Stat changes for version 8.
4.
0. Version 9.
0. **22**:
4.

The Edit Variable dialog was not scrolling the Values list to display the selected values.
5. On the Edit Variable dialog, the "Values " was not being updated properly.
6. On the Edit Variable dialog, using the arrow keys to change the group would not update the values.
7. On the Edit Variable dialog, you could get duplicate Group names.
7. Changing options on the matrix would result in a Print Preview being wrong.
8. Row numbers were not printing for Case Listings.
9. Formatted and Unformatted menu items on Case Listings were working incorrectly.
10. Selected cells were showing in the print.
11. Copying cells from a matrix might fail or include hidden columns. selected including headers.
19. The wrong row was selected when using keyboard menu selection on the Results Matrix. Version 9.
0. **16**:

1. The Case Selection Line dialog was not validating single values correctly.
2. The Case Selection Line dialog was not displaying ranges and a selection list together correctly.
3.

The Case Selection Line dialog was not displaying merged variable values correctly.
4. The Case Selection Line dialog was not displaying the value range correctly for large numbers.
5. The Case Selection Line dialog was not adding merged variables correctly when they evaluated to multiple lines.
6. Fixed an error when exporting variables.
7. Selecting a cell in a result matrix after highlighting rows and columns would cause an error.

---

### Principles of analytic validation of immunohistochemical assays: guideline update [^b92a92ee]. Archives of Pathology & Laboratory Medicine (2024). High credibility.

Supplemental Table 6 and Supplemental Table 7 — ROBINS-I risk of bias assessment of included randomized and non-randomized series of patients studies — report overall risk-of-bias ratings by study. In the randomized series table, overall risk of bias is serious for Gaule et al. Rimm et al., and Huang et al., moderate for Ilie et al., and low for Phillips et al. and Perez et al. In the non-randomized series table, overall risk of bias is low for Hernandez et al. and moderate for Munari et al. Domains assessed include Confounding, Patient Selection, Intervention classification, Deviations from intended interventions, Missing data, Measurement of outcomes, Selection of reported result, and Overall Risk of Bias.

---

### Healthy housing reference manual [^91ea5c0e]. CDC (2006). Medium credibility.

Clear Creek water contaminated with sewage — an investigation revealed 12 additional pipes that were also releasing untreated sewage, and some people in the area reportedly regard this creek as a source of drinking water.

---

### The basic reproduction number (R) of measles: a systematic review [^f3ec52f0]. The Lancet: Infectious Diseases (2017). Medium credibility.

The basic reproduction number, R nought (R 0), is defined as the average number of secondary cases of an infectious disease arising from a typical case in a totally susceptible population, and can be estimated in populations if pre-existing immunity can be accounted for in the calculation. R 0 determines the herd immunity threshold and therefore the immunisation coverage required to achieve elimination of an infectious disease. As R 0 increases, higher immunisation coverage is required to achieve herd immunity. In July, 2010, a panel of experts convened by WHO concluded that measles can and should be eradicated. Despite the existence of an effective vaccine, regions have had varying success in measles control, in part because measles is one of the most contagious infections. For measles, R 0 is often cited to be 12–18, which means that each person with measles would, on average, infect 12–18 other people in a totally susceptible population. We did a systematic review to find studies reporting rigorous estimates and determinants of measles R 0. Studies were included if they were a primary source of R 0, addressed pre-existing immunity, and accounted for pre-existing immunity in their calculation of R 0. A search of key databases was done in January, 2015, and repeated in November, 2016, and yielded 10883 unique citations. After screening for relevancy and quality, 18 studies met inclusion criteria, providing 58 R 0 estimates. We calculated median measles R 0 values stratified by key covariates. We found that R 0 estimates vary more than the often cited range of 12–18. Our results highlight the importance of countries calculating R 0 using locally derived data or, if this is not possible, using parameter estimates from similar settings. Additional data and agreed review methods are needed to strengthen the evidence base for measles elimination modelling.

---

### Standards of care in diabetes – 2025 [^de96358f]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for obesity, more specifically with respect to patients with diabetes mellitus, lifestyle changes, ADA 2025 guidelines recommend to offer nutrition, physical activity, and behavioral therapy in patients with T2DM and overweight or obesity to achieve both weight and health outcome goals.

---

### Healthy housing reference manual [^d1462def]. CDC (2006). Medium credibility.

How to reduce the mosquito population — source reduction measures for residents: The most efficient method of controlling mosquitoes is by reducing the availability of water suitable for larval and pupal growth; residents are advised to examine their home and neighborhood and take precautions recommended by the New Jersey Agricultural Experiment Station, including disposing of unwanted tin cans and tires, cleaning clogged roof gutters and draining flat roofs, turning over unused wading pools and other containers that tend to collect rainwater, and changing water in birdbaths, fountains, and troughs twice a week; additional measures include cleaning and chlorinating swimming pools, covering containers tightly with window screen or plastic when storing rainwater, flushing sump-pump pits weekly, and stocking ornamental pools with fish.

---

### Healthy housing reference manual [^d4c68057]. CDC (2006). Medium credibility.

Reducing heat loss and condensation — window energy efficiency metrics are defined. The energy efficiency of windows is measured in terms of their U-values (measure of the conductance of heat) or their R-values.

---

### KDIGO 2017 clinical practice guideline update for the diagnosis, evaluation, prevention, and treatment of chronic kidney disease-mineral and bone Disorder (CKD-MBD) [^89a3a1f4]. Kidney International Supplements (2017). Medium credibility.

Regarding diagnostic investigations for chronic kidney disease, more specifically with respect to evaluation of mineral and bone disorders, KDIGO 2017 guidelines recommend to consider measuring 25-hydroxyvitamin D levels in patients with CKD stages 3-5D and repeat testing determined by baseline values and therapeutic interventions.

---

### Trametinib (Mekinist) [^e6a09e21]. FDA (2025). Medium credibility.

The dosage of trametinib dimethyl sulfoxide PO for treatment of low-grade glioma in both children (in patients ≥ 1 year) with BRAF V600E mutation is 1 mg PO daily until disease recurrence or unacceptable toxicity

---

### Data-driven discovery of dimensionless numbers and governing laws from scarce measurements [^310b7972]. Nature Communications (2022). High credibility.

The optimization of dimensionless learning is different from general regression optimization approaches because only dimensionless numbers with small rational powers are preferred, such as −1, 0.5, 1, or 2, etc. Therefore, instead of searching for the best basis coefficients with a lot of decimals like other neural network-based methods, such as DimensionNet, zero-order optimization methods are used in this work. It includes grid search or pattern search-based two-level optimization and can be more efficient in finding the best basis coefficients. No gradient information and learning rate are required and the choice of grid interval is more flexible. Even though these zero-order optimization approaches can get stuck in local minima, increasing the number of initial points can easily eliminate this issue. More detailed pros and cons of different optimization methods are described in Section 4.5 of the SI.

The proposed method divides the identification process of differential equations into two steps to identify consistent parameterized governing equations efficiently. The first step is to identify a temporary governing equation in which the regression coefficients can be a constant or variable depending on how the simulation or experiment parameters are set. In the next step, dimensionless learning aims to recover the expression of the varying coefficients by leveraging the dimension of these coefficients. By combining these two steps, the proposed method can efficiently obtain a consistent dimensionally homogeneous governing equation with a small amount of data. In contrast, the standard SINDy falls short of achieving a consistent parameterized differential equation for the same system with different parameters. For example, the governing equation for the spring–mass–damper system is. If we use different parameters (damping coefficient c, spring constant k, or mass m) in this system, SINDy can only provide scalar coefficients for x andrather than the expressionsand, respectively. Other advanced SINDy approaches deal with this issue by multiplying the candidate terms by a set of predetermined parameters. Although these approaches can address this inconsistent governing equation problem, it couples the optimization of identifying candidate terms and parameterized coefficients, making the optimization more difficult. If there are many combinations of parametric derivative or non-derivative terms, this problem can become more difficult and unmanageable.

---

### Principles of analytic validation of immunohistochemical assays: guideline update [^c849f424]. Archives of Pathology & Laboratory Medicine (2024). High credibility.

Supplemental Table 5 — risk of bias assessment of included consecutive series of patients studies summarizes per-study ratings across specified domains, including classification of intervention, deviation from intended intervention, measurement of outcomes, selection of reported result, and an Overall Risk of Bias, using categorical judgments such as low, moderate, and serious as illustrated by rows with all low ratings, all moderate ratings, and entries with serious ratings in specific domains and overall.

---

### Direct inference of patlak parametric images in whole-body PET / CT imaging using convolutional neural networks [^4161b216]. European Journal of Nuclear Medicine and Molecular Imaging (2022). Medium credibility.

Data preparation and training

Prior to training, all PET images and parametric images were preprocessed/normalized. The input data sets were categorized into two groups: "with SUV" and "without SUV". In the "with SUV" group, the input data sets included SUV images alone for training the first model and added a pass each time for training other models starting from the last pass (i.e. SUV plus pass 13, SUV plus passes 13 and 12, SUV plus passes 13 to 11, SUV plus passes 13 to 10, and SUV plus passes 13–9 (6 input data sets)) to evaluate the impact of SUV images on the model's prediction. In the "without SUV" group, we followed the same trend but without SUV images (starting directly from the last pass (pass 13) and added one pass (12, 11, 10, 9…) each time to investigate the effect of the number of passes on the accuracy of the prediction model. The output data set involved reference K i -Patlak images generated using our in-house developed code. To generate Patlak parameters using our in-house developed code, an image-derived IF is required. The code uses a modified hybrid whole-body dynamic protocol that allows the estimation of micro-parameters in the initial bed targeting the blood pool in addition to calculating whole-body macro-parameter maps. In this new protocol, in order to avoid restricting the initial bed position to the heart region, it was proposed to choose the region for input function (IF) extraction based on the location of the suspected pathology and from the descending or ascending aorta. Therefore, in this work, depending on the location of the primary malignancy and therefore the site of the initial bed position, the IF can be extracted from the heart region or aorta. Overall, 11 independent deep learning models were developed. The input data sets including SUV and passes were converted to SUV units to reduce the dynamic range of image intensities. All images were normalized within the range [0–2]. Therefore, the maximum value of each data set was calculated and then the maximum values averaged over the 19 patients and the averaged number divided by 2. To maintain the quantitative value of PET and parametric images, fixed normalization factors were employed for all subjects and different types of data. A fixed normalization factor was determined for each type of image data. To normalize each type of images to the range [0–2], the average of the maximum values was selected as the normalization factor. No intensity cropping/clipping was performed on the normalized images. Hence, images of the different subjects may have different maximum values within the range of 0 to almost 2. Thereafter, SUV images and dynamic frames were scaled accordingly. Subsequently, all images were cropped to a matrix size of 168 × 168 voxels to reduce the computational cost through eliminating the irrelevant background pixels in the images.

---

### Selpercatinib (Retevmo) [^f264b069]. FDA (2024). Medium credibility.

The dosage of selpercatinib PO for treatment of solid tumors in both children (in patients 2–12 years) with RET gene fusion (locally advanced or metastatic, no satisfactory alternative therapy or progression on or after prior systemic therapy) is 120–320 mg PO daily, in 2–3 divided doses, until disease progression or unacceptable toxicity

---

### Standards of care in diabetes – 2025 [^ab126e75]. Diabetes Care (2025). High credibility.

Regarding preventative measures for hypoglycemia, more specifically with respect to choice of insulin, ADA 2025 guidelines recommend to prescribe continuous subcutaneous insulin infusion or multiple daily doses of prandial (injected or inhaled) and basal insulin for the treatment of most adult patients with T1DM.

---

### 2025 ESC guidelines for the management of myocarditis and pericarditis [^5aeff972]. European Heart Journal (2025). High credibility.

Regarding medical management for myocarditis, more specifically with respect to management of cardiac arrhythmias, catheter ablation, ESC 2025 guidelines recommend to consider performing catheter ablation in specialized centers for patients with recurrent symptomatic sustained monomorphic VT or ICD shocks post-myocarditis when antiarrhythmic drugs are ineffective, not tolerated, or not desired.

---

### Https://ftp.cdc.gov/pub/health_statistics/NCHS/lso… [^5e492643]. ftp.cdc.gov (2025). Medium credibility.

VALUE $P2L 94 = '1994'; VALUE $P3L 1 = 'Quarter 1' 2 = 'Quarter 2' 3 = 'Quarter 3' 4 = 'Quarter 4'; VALUE $P4L.; VALUE P18L 01 = 'White' 02 = 'Black' 03 = 'Indian ' 04 = 'Eskimo' 05 = 'Aleut' 06 = 'Chinese' 07 = 'Filipino' 08 = 'Hawaiian'.; VALUE P19L 1 = 'White' 2 = 'Black' 3 = 'Other'; VALUE P20L 1 = 'White' 2 = 'Non-white'; VALUE P21L. 27 = 'Unknown'; VALUE P32L 0 = 'Under $5, 000' 1 = '$5, 000-$6, 999' 2 = '$7, 000-$9, 999' 3 = '$10, 000-$14, 999'. '1' = '1 secondary family' '2' = '2 secondary family' '3' = '3 secondary family' '4' = '4 secondary family' '5' = '5 secondary family' '6' = '6 secondary family' '7' = '7 secondary family' '8' = '8 secondary family'. 4 = 'Something else' 5 = 'Unknown'; VALUE P40L 1 = 'Excellent' 2 = 'Very Good' 3 = 'Good' 4 = 'Fair' 5 = 'Poor' 6 = 'Unknown'; VALUE P41L. 6 = 'Self-employed' 7 = 'Without pay' 8 = 'Never worked' 9 = 'Unknown'; .; VALUE P48L 01 = 'Agriculture' 02 = 'Forestry and fisheries' 10 = 'Mining' 20 = 'Construction' 30 = 'Food and kindred products' 31 = 'Textile mill and finished textile pro. ' /ducts/.

---

### Healthy housing reference manual [^ef8347b5]. CDC (2006). Medium credibility.

Healthy housing — acid water corrosion (Table 8.6) uses "Colorimetric field titrametric tests for acidity, pH, and carbon dioxide (pH is determined at the titration end point)", with thresholds that "A pH below 6.5 causes corrosion" and "Carbon dioxide should be less than 10 mg/L Or less than 5 mg/L if alkalinity is less than 100 ppm"; corrective actions are that "Soda ash solution is fed into the well or suction line of a pump. May be fed along with chlorine solution. Limestone chips (calcite) neutralize the water by increasing its alkalinity and hardness".

---

### The snm procedure guideline for general imaging 6.0 [^32c9988d]. SNMMI (2010). Medium credibility.

General imaging processing and display components — The processing system is a computer comprised of one or more central processing units (CPUs), computer memory (Random Access Memory-RAM), backup storage such as hard disk drives, and peripherals such as removable disk drives. The display is an important component of a medical imaging system and may be specifically designed for this purpose. Monitors typically display color, are large enough to display several images simultaneously, and important parameters include the display matrix size, number of display levels, number of overlays, number of lookup, and display tables. Hardcopy devices are becoming less common; important parameters include print size, print resolution (number of points per unit of surface, number of gray levels or colors per point), and print time. Labeling guidance states that the monitor or hardcopy display should include patient name, a patient identifier such as a medical record number, date, and type of study so that the displayed data can be uniquely identified, and that other information such as the phase of the study, view and institution may facilitate interpretation, particularly when viewed out of the nuclear medicine department or at another institution. An archive is described as a storage device for saving old studies that is normally provided. Networking is common; computers involved in processing nuclear medicine data may be linked in the form of a network called a Local Area Network (LAN).

---

### Healthy housing reference manual [^decd17c7]. CDC (2006). Medium credibility.

Table 8.6. Analyzing and Correcting Water Quality Problems — water quality issues, measurements, and corrective actions: For turbidity (cloudy water), causes include silt, sediment, large numbers of microorganisms or organic material; measurement uses nephelometric turbidity units (NTUs) with "Less than 5 NTUs is best, > 10 not acceptable". Corrective actions are fine filtering with a sand filter or diatomaceous earth filter and, for ponds, coagulation and sedimentation. Blue stain on porcelain fixtures is due to corrosion of copper pipes and fixtures from low pH, hardness, and alkalinity; the Langelier index determines proper balance of pH, hardness, and alkalinity, and corrective action is to use methods mentioned above to adjust pH, hardness, and alkalinity. Lead contamination from leaching from lead service lines, solder, or brass or lead fittings has measurement at 15 parts per billion with corrective options to adjust pH, use filtration, and apply chemical treatment.