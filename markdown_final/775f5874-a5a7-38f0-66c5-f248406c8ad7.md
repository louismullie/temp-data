# Medical Question & Answer

**Sample ID**: 775f5874-a5a7-38f0-66c5-f248406c8ad7
**Dataset Index**: 10376

---

## Question

B3 Simplify the expression (x+2)² − (x−2)(x+2) − 4x² + [2/x] and accurately calculate the value of the expression for x = √2. Rationalize the result.

---

## Answer

> Let's see… What do we have here? The user is asking me to simplify the expression ((x+2)² − (x−2)(x+2) − 4x² + [2/x]) and then evaluate it at (x = √2), rationalizing the final result. Let's break this down step-by-step. First, I need to think about expanding and combining like terms in the polynomial portion. Then, I should verify the simplification and ensure no algebraic errors. Next, I will substitute (x = √2) and compute each term carefully. Finally, I need to rationalize the denominator and present the exact, simplified form.

> Let me first expand each term methodically. ((x+2)²) expands to (x² + 4x + 4), and ((x-2)(x+2)) is a difference of squares, so it simplifies to (x²−4). The term (−4x²) remains as is, and ([2/x]) stays unchanged for now. So the expression becomes ((x² + 4x + 4) − (x²−4) − 4x² + [2/x]).

> Wait, let me verify the subtraction step. Distributing the minus sign, I get (x² + 4x + 4 − x² + 4 − 4x² + [2/x]). Combining like terms, the (x²) and (−x²) cancel, leaving (4x + 4 + 4−4x² + [2/x]), which simplifies to (−4x² + 4x + 8 + [2/x]). Hold on, I should double-check that I didn't miss anything; yes, that's correct.

> Now, I will substitute (x = √2) into the simplified expression. Let me compute each term carefully: (−4x² = −4(√2)² = −4 × 2 = −8), (4x = 4√2), the constant term is (8), and ([2/x] = [2/√2]). So the expression evaluates to (−8 + 4√2 + 8 + [2/√2]).

> I need to rationalize the denominator for ([2/√2]). Multiplying numerator and denominator by (√2) gives ([2√2/2] = √2). Let me reconsider the entire sum: (−8 + 8) cancels, and (4√2 + √2 = 5√2). Therefore, the final rationalized result is (5√2).

---

The simplified expression is (−4x² + 4x + 8 + [2/x]).
For (x = √2), the value is (5√2).

---

## Step-by-step simplification

Start with the original expression:

(x+2)² − (x−2)(x+2) − 4x² + [2/x]

Expand (x+2)² and ((x-2)(x+2)):

(x² + 4x + 4) − (x²−4) − 4x² + [2/x]

Distribute the minus sign and combine like terms:

x² + 4x + 4 − x² + 4 − 4x² + [2/x] = −4x² + 4x + 8 + [2/x]

---

## Evaluation at (x = √2)

Substitute (x = √2) into the simplified expression:

−4(√2)² + 4√2 + 8 + [2/√2]

Simplify each term:

−4 × 2 + 4√2 + 8 + [2/√2] = −8 + 4√2 + 8 + √2

Combine like terms:

(−8 + 8) + (4√2 + √2) = 0 + 5√2 = 5√2

---

## Final answer

The simplified expression is (−4x² + 4x + 8 + [2/x]), and its value at (x = √2) is (5√2).

---

## References

### Cartography of genomic interactions enables deep analysis of single-cell expression data [^117ERYPq]. Nature Communications (2023). High credibility.

Efficient computation of the interaction matrix

Computation of the pairwise interaction matrix (eq. (3)) can be intensive because of the inversion of the covariance matrix. Interestingly, the formulation of the interaction strength between two genes (eq. (3)) is same as that of their partial correlation. An efficient way to compute the partial correlation matrix (and thus interaction matrix) is to 1) solve the two associated linear regression problems (shown below), 2) get the residuals from the regression problems, and 3) calculate the correlation between the residuals. Let us assume that X and Y are random variables taking real values (denoting expression levels of two genes), and let Z be the (n − 2)-dimensional vector-valued random variable (denoting expression levels of all other genes). Let us also assume that x i, y i and z i, i = 1, …, m denotes the independent and identically distributed m observations from some joint probability distribution of the random variables X, Y and Z. If we want to find the relationship between the random variables through regression, we have to find the regression coefficient vectorsandsuch thatwith 〈 w, v 〉 the scalar product between the vectors w and v. The residuals can then be computed asThe partial correlation between X and Y can then be expressed as:For independent 2-way (pairwise) interactions between X and Y (Z has no effect on the interaction of X and Y), eq. (16) can be simplified as

---

### Frequency spectra and the color of cellular noise [^115qPmJW]. Nature Communications (2022). High credibility.

RNA splicing network

The recently proposed RNA Splicing network (see Fig. 2 B) was used to model the concept of RNA velocity that can help in understanding cellular differentiation from single-cell RNA-sequencing data. Here a single gene-transcript can randomly switch between active (X 1) and inactive (X 2) states with different rates of transcription of unspliced mRNA (X 3). The splicing process converts these unspliced mRNAs into spliced mRNAs (X 4). Both spliced and unspliced mRNAs undergo first-order degradation. Applying formula (13) we can write the PSD of the dynamics of active gene count asNote that when the active gene count is X 1 ∈ {0, 1} the transcription rate is α off + (α on − α off) X 1. We can view transcription as a superposition of two reactions — a constitutive reaction with rate α off and reaction of the form (14) where the stimulant is the active gene X 1. Applying Theorem 2.1 we can decompose the PSD of the spliced mRNA count aswhereis given by (36).

Observe that for both gene expression and RNA splicing networks we can find an analytical expression for the PSD by directly applying formula (13) for the full network. However, using our PSD decomposition result we not only simplify the computation but also identify the contribution of the network mechanisms to the PSD.

For a specific parameterisation of these two networks, we compare the PSDs obtained analytically with those obtained by our Padé PSD method and the standard periodogram estimator for PSD that is based on discrete-sampling and DFT (see Box 1). The results are presented in Fig. 2 A, B and they show good agreement, despite the noisy nature of the DFT estimate. The analytical expressions for the PSD along with the PSD estimates produced by Padé PSD are given in Table 1. One can see that the PSD estimated by our method is quite "close" to the analytical PSD for the gene expression network. The same holds for the RNA splicing network (see the PSD plots in Fig. 2 B) even though it is not apparent from the expressions in Table 1.

Table 1
Expressions for PSDs estimated analytically and with the Padé PSD method.

To estimate the Padé approximant we use s = (1, 1.5, 2, ∞) and ρ = (2, 2, 2, 3) for the RNA splicing network and for all other networks we use s = (∞) and ρ = (4).

---

### Bayesian tomography of high-dimensional on-chip biphoton frequency combs with randomized measurements [^115ZWVUa]. Nature Communications (2022). High credibility.

In addition to the parameters forming ρ, the scale factor K must also be suitably parameterized. Following ref. we find it convenient to write K = K 0 (1 + σ z), where K 0 and σ are hyperparameters defined separate of the inference process, and z is taken to follow a standard normal distribution, leading to a normal prior on K of mean K 0 and standard deviation K 0 σ. We take σ = 0.1 and K 0 equal to the sum of the counts in all d 2 bins for the first JSI measurement (r = 1), where the absence of modulation ensures that all initial photon flux remains in measured bins, i.e. This provides an effectively uniform prior, since a fractional deviation of 0.1 is much larger than the maximum amount of fractional uncertaintyexpected from statistical noise at our total count numbers; the use of a normal distribution simplifies the sampling process.

The total parameter set can therefore be expressed as the vector, with the prior distribution

We note that this parameterization entails a total of 4 d 4 + 1 independent real numbers (2 d 4 complex parameters for ρ, one real parameter for K) — noticeably higher than the minimum of d 4 − 1 required to uniquely describe a density matrix. Nevertheless, this ρ (y) parameterization is to our knowledge the only existing constructive method to produce Bures-distributed states, and is straightforward to implement given its reliance on independent normal parameters only.

Following Bayes' rule, the posterior distribution becomeswhereis a constant such that ∫ d x π (x) = 1. We have adopted this notation for Bayes' theorem — rather than the more traditional — to emphasize the functional dependencies on x, which are all that must be accounted for in the sampling algorithm below. From π (x), the Bayesian mean estimator f B of any quantity (scalar, vector, or matrix) expressible as a function of x can be estimated aswhere, in lieu of direct integration, S samples { x (1), …, x (S) } are obtained from the distribution π (x) through Markov chain Monte Carlo (MCMC) techniques, as described below.

---

### Resolving discrepancies between chimeric and multiplicative measures of higher-order epistasis [^1143qc5W]. Nature Communications (2025). High credibility.

Equation (21) demonstrates that X = (X 1, X 2) follows an exponential family distribution, a wide class of distributions that includes many common distributions including normal distributions or Poisson distributions. In particular, using the terminology of exponential families, equation (21) shows that the sufficient statistics of X are X 1, X 2, and X 1 X 2, with corresponding canonical parameters β 1, β 2, and β 12. As a result, the distribution P (X) is uniquely defined by the expected values E [X 1], E [X 2], E [X 1 X 2] of the sufficient statistics, sometimes called the moments or the mean parameters of the distribution. Thus, we obtain a third parametrization of the distribution P (X) using the moments μ 0 = 1, μ 1 = E [X 1], μ 2 = E [X 2], μ 12 = E [X 1 X 2]. The elements of the vector μ = (1, μ 1, μ 2, μ 12) of moments are sometimes called the mean parameters of the distribution.

---

### Greater fuel efficiency is potentially preferable to reducing NOemissions for aviation's climate impacts [^117XKPkS]. Nature Communications (2021). High credibility.

Table 2
The recalculated aircraft NO x radiative forcing (RF) from Table 1 using a revised simplified expression for the RF of CH 4 as presented by Etminan et al.

Fig. 4
Net NO x radiative forcing by emission rate, updated CH 4 parameterisation.

Aviation net NO x radiative forcing (RF) (the sum of the short-term positive O 3 RF perturbation and the negative RF terms caused by a reduction in CH 4 lifetime, see Methods), by aviation NO x emission rate according to a range of background emission scenarios, utilising the updated simplified expression for the calculation of CH 4 forcing of Etminan et al. Aviation net NO x RF systematically decreases with increasing NO x emissions from aviation, showing a variation according to the background surface emissions, with high mitigation (RCP 2.6) having a smaller aviation net NO x RF than, lower mitigation scenarios (RCP 4.5, RCP 8.5) for the same aviation NO x emission. The pattern of behaviour is in contrast to Fig. 1 because the updated CH 4 forcing expression accounts for the short-wave forcing of CH 4, increasing CH 4 RF estimates by approximately 25%, which in the case of aviation net NO x impacts greatly increases the negative terms from the reduction in CH 4 lifetime from aviation NO x, tipping the net NO x term from being positive to negative with increasing aviation NO x emissions. Overall uncertainties are indicated by the grey shading, which is one standard deviation (68% confidence interval) from the ensemble of 20 NO x studies presented in Supplementary Fig. 1.

---

### Advances in mixed cell deconvolution enable quantification of cell types in spatial transcriptomic data [^1149E6Zf]. Nature Communications (2022). High credibility.

Box 2 The SpatialDecon algorithm

1. Run the log-normal deconvolution algorithm.

2. Choose δ as the expression level below which technical noise predominates. For GeoMx data normalized to have expected background = 1, we use δ = 0.5.

3. Define the residuals of the algorithm fit as R = log 2 (pmax(Y, δ)) – log 2 (pmax(B + X, δ)), where pmax(x, δ) is the function replacing all elements of x below δ with δ.

4. For all { i, j } with | R i, j | > 3, set Y i, j to NA. In simulated data, this threshold of 3 performed well, and deconvolution results were not sensitive to the choice of threshold (Supplementary Fig. 9).

5. Re-run the log-normal deconvolution algorithm using the updated Y matrix, obtaining estimatesand covariance matrices.

6. Calculate the standard error for eachwith sqrt.

7. Calculate the p-value for each β i, j with p = 2 (1−ɸ(t = , df = p – K – 1)), where ɸ is the cumulative distribution function of the standard normal distribution. (/is the square root of the Wald statistic, which is asymptotically normal.)

8. Calculate the 95% confidence interval for each β i, j with ± 1.96 1/2

Returnalong with the standard errors and p-values of its elements.

Estimating background

For GeoMx studies, each region's background level can be estimated by taking the mean of the negative control probes. These probes target sequences identified by the External RNA Controls Consortium (ERCC) as alien to the human genome.

---

### Automatic network structure discovery of physics informed neural networks via knowledge distillation [^1175vGDv]. Nature Communications (2025). High credibility.

Similarly, we construct a low-frequency solution to the PDEs (27) with the source term f = 0, ensuring the permutation property of the solution. The permutation property can be simplified using the permutation equivariant groupas ∀ h ∈ H e :The constructed solution is. The low-rank parameter matrix extracted for this low-frequency function is:

Similarly, the relationship between the two-dimensional input x = (x 1, x 2) and the node outputs u a, u b of the final hidden layer can be obtained:From the expressions of both, it follows that:Since the expression of the final output u (x 1, x 2) is:this result contains the symmetry defined in (30).

In order to match the network structure results with the characteristics of the high-frequency solution, by setting the sign of the values in b 1 to be the same, we can obtain:from which we have:thus, the final expression:which satisfies the symmetry of the high-frequency solution.

Moreover, the structure can be adjusted by sign to satisfy other forms of symmetry. For example, after (36), adjusting the sign of the weights in the last hidden layer can yield:that is, the Poisson equation result can be adjusted by sign to satisfy the central symmetry u r (x 1, x 2) = u r (− x 1, − x 2).

The numerical results are shown in Fig. 4 c. The iteration step size used in this problem is 1e−3, which is reduced to 0.2 times the original value at steps 5e3 and 1.5e4. The Ψ -NN re-constructed NN outperforms both PINN and PINN-post models in terms of speed and accuracy. Their L2 errors are summarized in Table 1. All three models exhibit peak errors along the line x 2 = − x 1 + 1, but Ψ -NN maintains a lower overall error, particularly at the boundaries. In contrast, the other models in the control group show large gradient errors at local boundaries, highlighting Ψ -NN's superior performance in high-frequency fitting.

---

### Spin dephasing under nonlinear gradients: implications for imaging and field mapping [^1171KZF1]. Magnetic Resonance in Medicine (2012). Low credibility.

This work examines the prototypical MR echo that would be expected for a voxel of spins evolving in a strong nonlinear field, specifically focusing on the quadratic z(2) - ½(x(2) + y(2)) field. Dephasing under nonlinear gradients is increasingly relevant given the growing interest in nonlinear imaging, and here, we report several notable differences from the linear case. Most notably, in addition to signal loss, intravoxel dephasing under gradients creating a wide and asymmetric frequency distribution across the voxel can cause skewed and nonlinear phase evolution. After presenting the qualitative and analytical origins of this difference, we experimentally demonstrate that neglecting these dynamics can lead to significant errors in sequences that assume phase evolution is proportional to voxel frequency, such as those used for field mapping. Finally, simplifying approximations to the signal equations are presented, which not only provide more intuitive forms of the exact expression but also result in simple rules to predict key features of the nonlinear evolution.

---

### Evolution of dosage compensation under sexual selection differs between X and Z chromosomes [^1158UjZ8]. Nature Communications (2015). Medium credibility.

The probability of fixation determines the expected change in expression over one particular substitution in each sex. If the mutant is lost, expression does not change, if the mutant is fixed, male and female expression become z m + δ m and z f +2 δ f, respectively (Table 2). To calculate the expected change in expression, we consider all possible mutational sizes and their frequencies,

where 3 N is the total number of genes in the population, μ is the mutation rate and f (δ m, δ f) is the probability density function for the effects of mutants on male and female expression. We assume that f (δ m, δ f) is a bivariate normal distribution with mean (0, 0) and covariance matrix

In this case, equations (6) and (7) read

and the second moments of the change in male and female expressions are approximately E[(Δ z m) 2 | z m, z f] = E[(Δ z f) 2 | z m, z f] = σ 2, and E[(Δ z m)(Δ z f)| z m, z f] = ρ σ 2. For simplicity, we set σ 2 = 0.1. Setting different values for σ 2 changes the impact of mutations, but since it does so with the same intensity in males and females, it does not change our results. However, increasing σ 2 increases the overall rate of evolution.

---

### Unbiased estimation of odds ratios: combining genomewide association scans with replication studies [^113Q77h1]. Genetic Epidemiology (2009). Low credibility.

Without loss of generality we assume that the event Q: X 1 ≥ X 2 ≥… ≥ X k has occurred, so that X i = X (i). Let. The pairand Z i = (σ 2, i /σ 1, i) X i)+(σ 1, i /σ 2, i)ϒ i are then sufficient and complete statistics for μ 1, …, μ k. The joint distribution of ϒ i and X 1, …, X k given Q, f (ϒ i, X | Q) is then transformed into f (X, Z i | Q) and. The joint densityis obtained from the integral

which enables the densityto be expressed as the ratio. This is greatly simplified due to numerous cancellations, in particular the selection probabilities which are analogous to those that feature in the denominator of, and cause problems for, formula (1). Using the Rao-Blackwell theorem formula (4), which is, is the uniformly minimum variance unbiased estimator for μ (i)′, conditional on Q (we call it the UMVCUE). This means that, given the ranking in stage 1, the estimator is unbiased for the corresponding effects and has minimum variance among all such unbiased estimators.

We now propose some modifications to this formula in order to apply the same estimation procedure to a genome scan followed by replication. Instead of the magnitude of the point estimates determining the rank order, it is more common in the genomewide setting to rank SNPs according to the statistical significance of their effects, and to restrict attention to those passing an initial P -value threshold p crit. For a one-sided Wald-type test, we can make this extension by conditioning instead on the event

This leads to slight changes in the proof since conditioning on Q * as opposed to Q changes the limits of integration for X i and Y i, with the result that W s, t in (4) becomes

For (8) to work generally we define X (k +1) /σ 1,(k +1) = Φ −1 (1 – p crit). This expression was noted in, though they did not allow for a P -value threshold. If SNPs that confer either an increased or decreased disease risk are of equal interest, as is usually the case, then the rank order of significance should be based on two-sided P -values. This now requires conditioning on the event

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^1112btau]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Nonparametric tolerance intervals for non-normal distributions: The above estimate of the tolerance interval would only be applicable to a population that is normally distributed, but when the underlying population is often not normal (eg, when there is a natural boundary that the data cannot exceed (ie, 0% or 100%)) it is helpful to define tolerance intervals using nonparametric methods; the one-sided nonparametric tolerance interval can be determined by finding the value for k that satisfies the cumulative binomial equation, where CL is the confidence level (eg, 0.95), and by setting k = 0 (ie, 0 failures) the formula can be simplified.

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^116Sk959]. Nature Communications (2025). High credibility.

Improving expressivity by increasing input multiplicity

Biologically, F a can be interpreted, for example, as depending on the chemostatted activity of an enzyme (see the Methods). In biochemical kinetics, it is common for some species to be involved in multiple reactions simultaneously, making it plausible for F a to drive multiple edges. We find that allowing for input multiplicity improves classification expressivity, and one way this happens is by lifting the monotonicity constraint. We assume for simplicity that each of the D input variables, whereis the set of input labels, affects the same numberof edges. Setting M > 1 lifts the monotonicity constraint because the condition for π k (F i j) to be a monotonic function is that all other edge parameters are held fixed; with M > 1 this is no longer true since several edge parameters change simultaneously as an input is varied.

To better understand the gain in the decision boundary's flexibility allowed by setting M > 1, in the Supplementary Information we analyze the steady-state representation in the rational polynomial form of the matrix-tree expression, Eq. (2). Considering the case D = 1 and identifying turning points as roots of ∂ π i /∂ F a, we show that the maximum number R of such roots obeyswhich is a direct measure of the classifier's expressivity; see Fig. 3 E for an illustration and the Supplementary Information for a numerical verification up to M = 4. A proof of the scaling 2 M − 1 for rational polynomials with non-negative coefficients can be found in ref. Thus, once M > 1, π i is no longer subject to the monotonicity constraint and behaves like a non-negative rational polynomial of degree up to 2 M. Input multiplicity thus allows the non-equilibrium biological process to be more expressive and draw out decision boundaries that can classify more complex data structures. Indeed, returning to the previously failed classification with M = 1 (Fig. 3 C), we see that setting M = 2 allows the same network to now learn a decision boundary which successfully encloses the data assigned to class 1 (Fig. 3 F). This implies that classifying a finite band of input signal levels (like a band-pass filter) requires setting M > 1 along the corresponding input dimension. A recent development in synthetic biology has in fact shown in a specific example that drug binding to receptor molecules via two distinct binding pathways can be used to design band-pass-like responses to the drug (Fig. 3 G).

---

### Evolution of dosage compensation under sexual selection differs between X and Z chromosomes [^116mYjqD]. Nature Communications (2015). Medium credibility.

The role of effective population size

The evolutionary dynamics at Z- and X-linked genes also depend on the chromosomes' effective population sizes N eZ and N eX in equations (1)–(4). Larger N e results in faster evolution of dosage compensation, due to the more reliable fixation of positively selected mutations and elimination of negatively selected mutations (Fig. 2). With effective population size inversely proportional to the stochasticity in replicate evolutionary paths, small values of N e not only slow down the evolution of dosage compensation but also result in more variable expression levels across replicate evolutionary trajectories. This effect is observed both during adaptation (Fig. 2), as well as at the evolutionary equilibrium (equation (15)).

Differences in ecology or mating systems (and hence N e) could underlie variation in the rate at which dosage compensation evolves. Under simplifying assumptions, the effective population sizes of the Z and X chromosomes in a population reproducing under harem polygamy are N eZ = 9 N f /(6+4 η) and N eX = 9 N f /(6+2 η), where N f is the number of successfully breeding females and η is the ratio of the number of successfully breeding females to successfully breeding males. As two-thirds of the Z chromosomes are transmitted by males each generation, compared with only one-third of X chromosomes, greater variance in male reproductive success (η > 1) leads to a more marked diminution of N eZ than N eX. This predicts slower and more stochastic evolution of dosage compensation on the Z than on the X (Fig. 2).

---

### Combinatorial code governing cellular responses to complex stimuli [^111GeLu4]. Nature Communications (2015). Medium credibility.

Formalization of complexity underlying signal integration

First, we theoretically enumerated the possible interactions between two abstract signals X and Y. We denoted by e 0, e X, e Y and e X+Y the log-normalized expression level of a multivariate output with i components measured, respectively, in the conditions 0 (control), signals X, Y and X+Y. Following log normalization, the fractional effects exerted by X, Y and X+Y on the j -th output component relative to the control condition correspond to the increments Δ e j X = e j X −e j 0, Δ e j Y = e j Y — e j 0, Δ e j X+Y = e j X+ Y — e j 0 (Fig. 1a), while the conditions of positive and negative interactions are translated, respectively, into the inequalities Δ e j X+Y > Δ e j X +Δ e j Y and Δ e j X+Y < Δ e j X +Δ e j Y. For each output component independently, we used recursion analysis to enumerate the combinations of k statistically different groups (rankings) being observed from n experimental conditions. The generalized solution to this problem (A n k = A n− 1 k +kA n− 1 k− 1) is illustrated as a modified Pascal's triangle (Fig. 1b). Our experimental design involves n = 4 conditions (0, X, Y and X+Y), and thus a number of groups varying from a minimum of k = 1 to a maximum of k = 4. For example, for n = 4, k = 1 group there is only one possible profile where the output is constant in all conditions (Fig. 1b). By increasing to k = 2, we add 14 non-redundant possible profiles corresponding to all combinations of the two output levels for the n = 4 conditions. Summing each contribution up to a maximum of k = 4, we obtained 75 possibilities (Fig. 1b). We then listed the profiles compatible with the inequalities Δ e j X+Y > Δ e j X +Δ e j Y (positive interactions) and Δ e j X+Y < Δ e j X +Δ e j Y (negative interactions). Five profiles were consistent with a lack of interaction (additivity) and were removed from subsequent analysis. Twenty-nine profiles were consistent with a positive, 29 with a negative and 12 were compatible with both a positive and a negative interaction. We then split each of these 12 'ambiguous' cases in two in order to account for both a positive and a negative realization. Altogether, we obtained 41 positive and 41 negative instances termed 'interaction profiles' (Fig. 1c). We developed a constraint satisfaction algorithm in order to formalize the mathematical structure of the different profiles (Supplementary Fig. 1).

---

### Greater fuel efficiency is potentially preferable to reducing NOemissions for aviation's climate impacts [^115nUTk1]. Nature Communications (2021). High credibility.

Discussion

The short-term O 3 RF is very sensitive to changes in all explored changes in surface precursor emissions (NO x, CO and NMVOC). The long-term CH 4 RF is mainly affected by the reduction in surface NO x emissions and it changes very little in the case of the reductions of surface CO and NMVOC alone (probably due to the decrease in OH consumption by CO). This is in agreement with responses from other sensitivity tests performed with UCI CTM by Holmes et al.with the difference that our short-term aviation O 3 RF is not as responsive to surface CO emissions as those modelled with the UCI CTM. In addition, after accounting for the long-term negative RFs that were not given in their 2011 paper (long-term O 3 and reductions in stratospheric water vapour, SWV), they also observe that the reduction in surface NO x emissions decreases the RF from aviation NO x, a 42% reduction in the aviation net NO x RF resulting from a halving of surface NO x emissions (C. D. Holmes, personal communication, October 19, 2018), which is in reasonable agreement with the sensitivity shown here. In general, to a great extent, the long-term CH 4 -mediated effects drive the response of aviation net NO x RF resulting from modified NO x emissions. Taking into account that these long-term RFs are fully parametrised as well as the fact that the CH 4 /O 3 ratio is very model specificmake the impact of surface NO x emissions on aircraft net NO x RF relatively more uncertain than the impact of other O 3 precursor emissions. For example, if the new CH 4 RF simplified expression that accounts for short-wave forcing is used, reduction in surface NO x emissions not only decreases the aviation net NO x RF but also changes its sign from positive to negative. Table 2 gives the recalculated aviation RF numbers from Table 1 using a simplified expression for RF of CH 4 as presented by Etminan et al. The improved understanding of CH 4 RF has a significant impact on aviation estimates as it increases the negative CH 4 RF from aviation NO x emissions by ~20%, which substantially reduces the aircraft net NO x RFs (Table 2). Moreover, the revision to the CH 4 term provides a perspective that as aviation NO x emissions are reduced an increase in the global aviation net NO x RF is shown, and vice versa (Fig. 4 and Table 2). This revised formulation of the CH 4 for RF does not contradict findings presented in this study, in terms of the sensitivity of responses, but turns out to be crucial for quantification of net NO x RFs and it provides a new perspective on the potential RF impact from future aviation NO x emissions. Other potential effects from NO x emissions include the direct enhancement of nitrate aerosol and indirect formation of sulfate aerosol (more efficient conversion of sulfur dioxide to sulfuric acid via increased OH). These aerosol effects are associated with large uncertainties and are addressed in only a few modelling studies, and were not considered here. However, the effects of NO x on aerosol abundances are expected to result in negative forcings, such that inclusion of these processes would increase the negative NO x -associated forcings and be consistent with the findings of this work that emphasizes the role of the negative forcings.

---

### One-way dependent clusters and stability of cluster synchronization in directed networks [^111hPTzE]. Nature Communications (2021). High credibility.

Each balanced partition corresponds to a cluster-synchronous solution where all the nodes in the same cluster follow the same time evolution, but nodes in different clusters do not. These solutions can be expressed in terms of the so-called invariant synchrony subspaces (ISS), i.e. the polydiagonal subspaces under all admissible vector fields.

The ISS Δ j corresponding to each partitionof Fig. 2 can be easily determined by inspection: Δ 1 is such that the state evolution of nodes 4 and 5 is the same, i.e. { x 4 = x 5 }; Δ 2 is such that { x 1 = x 2 and x 3 = x 4 }; Δ 6 is such that each node of the network has a different time evolution, and so forth. A number of algorithms are available to compute all the ISS for a generic network (see "Methods").

We remark that not all the balanced partitions compatible with a given topology can also be observed when taking into account the network dynamics (1), as some of these may be unstable. Thus the dynamics governing the network must be incorporated in the study of stability for the CS solution.

Stability

The possibility of observing a synchronized cluster in a given physical system depends on its stability, and hence on the particular network structure and dynamics of the nodes.

Once the Q equitable clusters corresponding to a balanced partition are found, the CS analysis can be focused on a simplified dynamical model (called quotient network) whose Q nodes correspond to each one of the clusters. In particular, we analyze the stability of clusters by linearizing Eq. (1) about a state corresponding to exact synchronization among all the nodes within each cluster. The Master Stability Function approach, which evaluates the stability of synchronized solutions through linearization, is based on two standard steps: (i) finding the variational equations of the network about the synchronized solutions and (ii) expressing these variational equations in a new system of coordinates, which decouples the perturbation dynamics along the transverse manifold from that along the synchronous manifold.

---

### Size limits the sensitivity of kinetic schemes [^113ujoyg]. Nature Communications (2023). High credibility.

A different measure of sensitivity — the amplification of a fold-change in the input — provides a solution to this problem. Suppose that for some value x 0 of the input parameter x, scaling by a factor a scales the output by b, so f (a x 0) = b f (x 0). Then the quotientcan be thought of as a discrete approximation of the derivative defining the logarithmic sensitivity. And if f (x) is differentiable everywhere, then by the mean value theorem, there must be a value x * of x for whichThis means that careful measurement of any two points on the input-output curve (x versus f (x)) witnesses the (local, infinitesimal) logarithmic sensitivity somewhere. Importantly — unlike in the case of fitting to a Hill function — if error in the measurements is very low, then they are also telling us the derivative for some value of x very accurately.

Equation (25) leads us to another common definition of the effective Hill coefficient:where S 0.9 and S 0.1 are the values of the input variable (in our case, x) required to get 90% and 10% (respectively) of the maximum value of the output variable (in our case, f (x)). Note that (26) is like (25) with a = S 0.9 / S 0.1 and b = 9. It implies that somewhere between S 0.1 and S 0.9 there is a logarithmic sensitivity of H eff /2.

There is yet another common definition, specific to models of binding. Suppose x is the concentration of a ligand and 〈 n b 〉(x) is the expected number of sites bound by a ligand out of a total of n possible binding sites. It is common then, to takeor to report, as the Hill coefficient, the slope of a line fitted to x versusdata on a log-log plot.

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^114C4gev]. Nature Communications (2025). High credibility.

To explain this difference, in the Supplementary Information we use the 1D rational polynomial form of the matrix-tree expression, Equation (3), to maximize ∂ π S /∂ F with respect to the learnable coefficientsand(treated for now as free and independent). The multi-index μ used in Equation (2) simplifies here to the single index m. We show thatwhere the derivative is evaluated at the location of the decision boundary F 0 and M R = M max − M min is the range in exponential powers of e F /2 among all directed spanning trees. This result shows that the sharpness of the classifier is fundamentally limited by the structure of the network. A tighter approximation to the bound can be obtained by replacing M R with, which is the range in exponential powers among only the directed spanning trees rooted on the output nodes. We further explain in the Supplementary Information that the directed spanning trees of the parallelly extended push-pull networks preventfrom scaling with n, whereas the spanning trees for serially extended networks allow, which enables increasingly sharp transitions as more edges are added. In serially extended networks, the structure allows this rangeto grow with the number of added edges n, leading to increasingly sharp transitions. In contrast, parallelly extended networks constrain all output-rooted spanning trees to use the same number of driven edges, keepingfixed and preventing sharper transitions.

Finally, even when the bound in Eq. (8) is large it may not be achieved in practice (see Supplementary Information for details). Saturating the bound requires that the coefficientsbe concentrated on trees with either the smallest or largest possible net input drive. However, in networks such as those with a ladder-like architecture, many spanning trees make intermediate contributions, and equality constraints among the functionsprevent the network from assigning large weights solely to the extremal trees. As shown in Fig. 4 B, overlapping spanning trees entangle the coefficients and reduce the effective degrees of freedom. This structural limitation suggests that sharp decision boundaries may be inherently inaccessible in densely interconnected biochemical networks. This finding resonates with, though is technically distinct from, recent results in refs.

---

### Gene regulatory network inference from sparsely sampled noisy data [^11632Qhn]. Nature Communications (2020). High credibility.

The GP framework can handle combinatorial effects, meaning nonlinearities that cannot be decomposed into f (x 1, x 2) = f 1 (x 1) + f 2 (x 2). This is an important property for modelling a chemical system — such as gene expression — where reactions can happen due to combined effects of reactant species. For example, the dynamics corresponding to a chemical reaction x 1 + x 2 → x 3 cannot be modelled by.

In a classical variable selection problem from input–output data η j = f (ξ j) + v j, for j = 1, …, N, where, the task is to find out which components of ξ does the function f depend on. In the GP framework, variable selection can be done by a technique known as "automatic relevance determination", based on estimating the hyperparameters of the covariance function of f. For BINGO, we have chosen the squared exponential covariance function for each component f i :The hyperparameters β i, j ≥ 0 are known as inverse length scales, sincecorresponds to a distance that has to be moved in the direction of the j th coordinate for the value of function f i to change considerably. If β i, j = 0, then f i is constant in the corresponding direction. The mean function for f i is m i (x) = b i − a i x i where a i and b i are regarded as nonnegative hyperparameters corresponding to mRNA degradation and basal transcription, respectively.

---

### Fractional response analysis reveals logarithmic cytokine responses in cellular populations [^112qhN4V]. Nature Communications (2021). High credibility.

Methods

Software implementation

The methodology to perform and visualize FRA is provided as a user-friendly R-package available for download at. The package contains an installation guide and a brief user manual.

Formal definition of the FRC

Consider a series of doses x 1, …, x i, …, x m and denote a single-cell response as y. Depending on the context, y, may be a number or a vector, e.g. the level of one or more measured signaling effectors. Suppose that responses to a given dose, x i, are represented as the probability distribution, The FRC is then formally defined aswhere integration takes place over, the set of all possible responses, y. The integral quantifies the area under the curve (or under surface for multivariate data), with respect to y, defined asFor the calculations shown in Fig. 2 the integration corresponds to the calculation of the area of the gray regions in c–e. As explained in Supplementary Note 1, the FRC defined as above is closely related Rényi min-information capacity.

Formal definition of typical fractions

Having the responses represented in terms of the probability distribution, Eq. 3, we can define which responses, y, are typical to any of the doses. Precisely, we define the response, y, to be typical for dose x j if it is most likely to arise for this dose, which writes asThe above condition allows assigning any response, y, to a dose for which it is typical. Therefore, for a given dose, x i, we can identify what fraction of cells stimulated with this dose exhibits responses typical to any dose, x j, for j from 1 to m. These fractions, denoted as v ij, can be practically computed as explained below.

---

### Unraveling the rate-limiting step of two-electron transfer electrochemical reduction of carbon dioxide [^114geXNL]. Nature Communications (2022). High credibility.

Results

The rate expressions with different reaction steps as the RLS

In order to discover the RLS, the Butler-Volmer equation was employed to describe the kinetic rate expression of two-electron transfer CO 2 ER. It describes how the electrical current passing through an electrode depends on the voltage difference between the electrode and the bulk electrolyte for simple unimolecular redox reactions, when both a cathodic and an anodic reaction proceeding on the same electrode are controlled by surface reactions rather than the mass transfer of electrolyte. For electroreduction reactions (Eq. 1, where O x and R ed represent oxidant and reductant, respectively), the Butler-Volmer equation is shown as Eq. 2.

In Eq. 2, j is the current density; η is the overpotential for the cathodic reaction; k f 0 is the standard forward rate constant; k b 0 is the standard backward rate constant; F is the Faraday constant; f = F/RT, where R is the ideal gas constant and T is absolute temperature; α is the transfer coefficient assumed to be equal to 0.5; n is the number of transferred electrons; a [R ed] and a [O x] are the concentrations of reductant and oxidant.

When the overpotential is sufficiently high, i.e. exp[–(1– α) f] < < exp(– αf), the backward reaction can be ignored. Even the high-performance CO 2 electrolysis catalysts have sufficient overpotentials to meet this condition. Therefore, Eq. 2 can be simplified to Eq. 3. At equilibrium conditions (j = 0), Eq. 2 can be simplified to Eq. 4.

By combining Eq. 4, the a [O x] in Eq. 3 can be represented by the concentration of reactants and K θ exp(– f) in the previous step (see the supplementary information for more details). Subsequently, the rate expression of the two-electron transfer CO 2 ER with a specific reaction step as the RLS can be derived (Tables 1 and 2, different labels are assigned to the corresponding RLSs according to the reaction processes). Whether the RLS is controlled by ET, PT, PCET, or D is also shown in the Tables. One thing should also be kept in mind is that all these expressions are based on assumptions of what might happen in the mechanism, which may not cover all possible kinetic cases at current cognitive levels.

---

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^113b8vCj]. American Journal of Kidney Diseases (2006). Medium credibility.

Methods for measuring and expressing the hemodialysis dose — simpler methods (CPG 2.4) present an empirical single-equation estimate of single-pool Kt/V (spKt/V): Kt/V = −ln(R − 0.008·t) + (4 − 3.5·R) ΔBW/BW, where R is the ratio of postdialysis blood urea nitrogen (BUN) to predialysis BUN, t is time on dialysis in hours, and BW is body weight; calculated Kt/V matches modeled Kt/V fairly closely when applied to dialysis given 3 times per week for 2.5 to 5 hours, but disadvantages include no measure of the net protein catabolic rate (PCR) and errors when applied to short, frequent, or prolonged dialysis, with additional simplified equations allowing calculation of normalized PCR (nPCR), also called normalized protein nitrogen appearance rate (nPNA).

---

### Leveraging data-driven self-consistency for high-fidelity gene expression recovery [^115cUYWD]. Nature Communications (2022). High credibility.

ROI selection and histogram equalization

We divide the expression matrix S is divided into N ≥ 4 ROIs, which we denote by S i, i = 1, 2, ⋯, N. Letanddenotes the number of matrix elements and possible expression values, in each ROI S i, respectively. Furthermore, we define the normalized histogram associated with the possible gene expression values in the ROI S i as followswhereis the number of elements in the ROI with the gene count n such that. The corresponding cumulative distribution function (CDF), is given by

Adaptive equalization of histogram

The goal of the histogram adaptive equalization is to find a transformation for the expression values such that the transformed CDF is approximated by the CDF of f (x), where f (x) can be f g (x), f r (x) or f e (x). As example, the CDF of an exponential distribution can be written aswhere λ denotes the decay parameter. SERM uses the transformationfor each ROI to map the expression values to their new values. In particular, consider transforming the expression counts bywhere the floor function ⌊ x ⌋ is the biggest integer not exceeding x. The mapis then applied to each expression value in the ROI S i.

The main intuition behind this transformation comes from the inverse sampling method. In particular, let X denotes continuous random variables whose CDF is strictly increasing on the possible value onwith densities p X and p Y, respectively. Consider the transformationThen, Y has the uniform distribution, i.e. To see this note that

Furthermore, using the inverse transform sampling technique, the random variablehas the exponential distribution, i.e. Z ~ F (t). Combining the transformations T 0 and T 1, we conclude thathas the desired exponential distribution. Now, the transformationdefined in Eq. (11) is an inverse sampling method for the discrete random variable similar to the continuous transformation T 1 ∘ T 0. Similar analytical equations to Eqs. (10)–(15) can be derived for Gaussian and Rayleigh distributions.

Sliding ROI

The ROI is slided throughout the whole expression matrix and gene imputation is performed at each ROI to obtain the resulting imputed gene expression matrix from SERM.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^114MTJhs]. Nature Communications (2021). High credibility.

Results

Our results are divided up into 7 sections corresponding to 7 different regulatory networks of increasing complexity. An overall reference table (Table 2) is included in the discussion.

Motif 0: direct Hill regulation

We first describe how we use DDEs to model simple regulation of a gene Y by a gene X as the most basic motif, and then provide a simple yet unified mathematical framework for both activation and inhibition with delay.

Activation and inhibition can both be modeled with a single unified function

We consider a transcription factor x regulating the production of a protein y (Fig. 2). If x activates y, the production rate of y increases with increasing x, generally saturating at a maximum rate α. Often there is an additional cooperativity parameter n which determines how steeply y increases with x in the vicinity of the half-maximal x input value k. In this framework, n = 0 is constitutive production, n = 1 is a Michaelis-Menten regulation, 1 < n < 5 is a typical biological range of cooperativity, and n → ∞ is a step-function regulation. If x represses y, the same conditions hold except that the production rate of y then decreases with increasing x. A standard quantitative model for this behavior is called the Hill function, and serves as a good approximation for many regulatory phenotypes in biology, including transcription and translation rates, phosphorylation, enzymatic activity, neuronal firing, and so forth. In general there may also be a leakage rate α 0 that yields constant y production in the absence of x. The concentration of y is also generally removed at a rate β proportional to its concentration. This removal term can represent many biophysical processes, such as degradation, dilution, compartmentalization, or sequestration; for simplicity we mainly use the term "degradation".

---

### StatCalc: statistical calculators… [^11151A5h]. CDC (2025). Medium credibility.

StatCalc: Statistical Calculators Visual Dashboard The StatCalc tools are also available in Visual Dashboard. The functionality is identical, however, the StatCalc tools appear as gadgets on the dashboard. For additional information about Visual Dashboard, refer to the Visual Dashboard section of the user guide. To add a StatCalc gadget to the dashboard, right-click on the canvas and select. Add StatCalc calculator. To create a 2 x 2 table, select Tables. The StatCalc 2 x 2 table appears on the canvas. Enter Exposure and Outcome data to calculate the statistical outcomes as described in the Tables section in StatCalc. All other StatCalc gadgets can be added to the Visual Dashboard with similar processes and perform the same functions in StatCalc as they do in Visual Dashboard.

---

### The minimal work cost of information processing [^114WZWVs]. Nature Communications (2015). Medium credibility.

where the sum ranges only over those x that have a non-zero probability of occurring. In the case of deterministic mappings p (x ′| x)∈{0,1}, this corresponds to the maximum number of input states that map to a same output state. For the AND gate, provided all four states 00, 01, 10 and 11 have non-negligible probability of occurring, there are three input states mapping to the same output state, so (3) gives us simply. Also, in simple examples as considered here, the expression (3) is stable to considering an-approximation (Supplementary Note 4); this quantity is thus physically justified.

Crucially, our result reveals that the minimal work requirement in general depends on the specific logical process, and not only on the input and output states. This contrasts with traditional thermodynamics for large systems, where the minimal work requirement of a state transformation can always be written as a difference of a thermodynamical potential, such as the free energy. For example, the minimal work cost of performing specifically an AND gate may differ from that of another logical process mapping an input distribution (p 00, p 01, p 10, p 11) (with ∑ i p i = 1) to the distribution (p ′ 0, p ′ 1) = (p 00 + p 01 + p 10, p 11) (Recall that the classical counterpart of a quantum state is a probability distribution.). To see this, consider the XOR gate, which outputs a 1 exactly when both inputs are different (see Fig. 2b). The minimal work cost requirement of this gate, as given by (3), is now only kT ln 2, as in the worst case, only a single bit of information is erased (again supposing that all four input states have non-negligible probability of occurring). Now, suppose that, for some reason, the input distribution is such that p 01 + p 10 = p 11, that is, the input 11 occurs with the same probability as of either 01 or 10 appearing. Then, the XOR gate reproduces the exact same output distribution as the AND gate: in both cases, we have p ′ 0 = p 00 + p 10 + p 01 = p 00 + p 11 and p ′ 1 = p 11 = p 01 + p 10. In other words, both logical processes have the same input and output state, yet the XOR gate only requires work kT ln 2 compared with the AND gate, which requires 1.6 kT ln 2. Furthermore, we point out that this difference, which appears small in this case, may be arbitrarily large in certain scenarios (Supplementary Note 4).

---

### Modelling sequences and temporal networks with dynamic community structures [^113tmetL]. Nature Communications (2017). Medium credibility.

Bayesian Markov chains with communities

As described in the main text, a Bayesian formulation of the Markov model consists in specifying prior probabilities for the model parameters, and integrating over them. In doing so, we convert the problem from one of parametric inference where the model parameters need to be specified before inference, to a nonparametric one where no parameters need to be specified before inference. In this way, the approach possesses intrinsic regularisation, where the order of the model can be inferred from data alone, without overfitting.

To accomplish this, we rewrite the model likelihood, using eqs. (1) and (5), asand observe the normalisation constraints,… Since this is just a product of multinomials, we can choose conjugate Dirichlet priors probability densitiesand, which allows us to exactly compute the integrated likelihood, whereand. We recover the Bayesian version of the common Markov chain formulation (see ref.) if we put each memory and token in their own groups. This remains a parametric distribution, since we need to specify the hyperparameters. However, in the absence of prior information it is more appropriate to make a noninformative choice that encodes our a priori lack of knowledge or preference towards any particular model, which amounts to choosing α x = β rs = 1, making the prior distributions flat. If we substitute these values in eq. (19), and re-arrange the terms, we can show that it can be written as the following combination of conditional likelihoods, wherewithbeing the multiset coefficient, that counts the number of m -combinations with repetitions from a set of size n. The expression above has the following combinatorial interpretation: P ({ x t }| b, { e rs }, { k x }) corresponds to the likelihood of a microcanonical modelwhere a random sequence { x t } is produced with exactly e rs total transitions between groups r and s, and with each token x occurring exactly k x times. In order to see this, consider a chain where there are only e rs transitions in total between token group r and memory group s, and each token x occurs exactly k x times. For the first transition in the chain, from a memory x 0 in group s to a token x 1 in group r, we have the probabilityNow, for the second transition from memory x 1 in group t to a token x 2 in group u, we have the probabilityProceeding recursively, the final likelihood for the entire chain iswhich is identical to eq. (21).

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^115o8v1f]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Δ Φ, the probability distribution at a point Φ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Φ ∈ {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters α and β, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since α and β are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Φ ∈ [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(α, β) and x 2 = 1 − x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Φ, it may seem natural to select α and β by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### Evaluation of the evenness score in next-generation sequencing [^111oWZQR]. Journal of Human Genetics (2016). Low credibility.

The evenness score (E) in next-generation sequencing (NGS) quantifies the homogeneity in coverage of the NGS targets. Here I clarify the mathematical description of E, which is 1 minus the integral from 0 to 1 over the cumulative distribution function F(x) of the normalized coverage x, where normalization means division by the mean, and derive a computationally more efficient formula; that is, 1 minus the integral from 0 to 1 over the probability density distribution f(x) times 1-x. An analogous formula for empirical coverage data is provided as well as fast R command line scripts. This new formula allows for a general comparison of E with the coefficient of variation (= standard deviation σ of normalized data) which is the conventional measure of the relative width of a distribution. For symmetrical distributions, including the Gaussian, E can be predicted closely as 1-σ(2)/2⩾E⩾1-σ/2 with σ ≤ 1 owing to normalization and symmetry. In case of the log-normal distribution as a typical representative of positively skewed biological data, the analysis yields E≈exp(-σ*/2) with σ*(2) = ln(σ(2)+1) up to large σ (≤ 3), and E≈1-F(exp(-1)) for very large σ (⩾2.5). In the latter kind of rather uneven coverage, E can provide direct information on the fraction of well-covered targets that is not immediately delivered by the normalized σ. Otherwise, E does not appear to have major advantages over σ or over a simple score exp(-σ) based on it. Actually, exp(-σ) exploits a much larger part of its range for the evaluation of realistic NGS outputs.

---

### Estimating the statistical significance of gene expression changes observed with oligonucleotide arrays [^115mZNvb]. Human Molecular Genetics (2002). Low credibility.

We present a simple method to assign approximate P-values to gene expression changes detected with Affymetrix oligonucleotide arrays and software. The method pools data for groups of genes and a small number of like-to-like comparisons in order to estimate the significance of changes observed for single genes in comparisons of experimental interest. Statistical significance levels are based on the observed variability in the fractional majority of probe pairs that indicate increasing or decreasing differential expression in comparisons of technical replicates. From this reference distribution or error model, we compute the expected frequency for fractional majorities in comparisons for N ≥ 2. These computed distributions are the source of P-value estimates for changes seen in the experimental comparisons. The method is intended to complement the Affymetrix software and to rationalize gene selection for experimental designs involving limited replication.

---

### Using both qualitative and quantitative data in parameter identification for systems biology models [^117TFy8y]. Nature Communications (2018). Medium credibility.

Results

An illustration of the potential value of qualitative data

To demonstrate the potential value of qualitative data, we consider a simple case of solving for the coefficients of polynomial functions.

We consider two polynomial functions: y 1 = ax 2 − bx + c and y 2 = dx + e. Suppose we want to solve for the coefficients a, b, c, d, and e, which we will take to be positive. As the ground truth coefficients to be determined, we choose (a, b, c, d, e) = (0.5, 3, 5, 1, 1.5).

Suppose that a limited amount of quantitative information is available. Namely, it is known that the parabola y 1 contains the points (2, 1) and (8, 13), and the line y 2 contains the point (3.5,5). This is not enough information to solve for any of the coefficients because three points are required to specify a parabola, and two points are required to specify a line (Fig. 1a).

Fig. 1
A simple illustration using polynomial functions. We use qualitative and quantitative information to determine the unknown coefficients. a Visualization of the problem. We seek to find the coefficients of equations for a parabola and a line, with the ground truth shown (blue solid curves). Two points on the parabola and one point on the line are known (black dots). These three points are consistent with infinitely many possible solutions (e.g. orange dashed curves). Qualitative information (colored circles, x -axis) specifies whether the parabola is above (+) or below (−) the line. This information limits the possible values of intersection points x 1 and x 2 to the green shaded segments of the x -axis. b Bounds on coefficient values as a function of the number of qualitative points known. Shaded areas indicate the range of possible values of each coefficient

---

### Approximations to the expectations and variances of ratios of tree properties under the coalescent [^114MiDuy]. G3 (2022). Medium credibility.

Properties of gene genealogies such as tree height (H), total branch length (L), total lengths of external (E) and internal (I) branches, mean length of basal branches (B), and the underlying coalescence times (T) can be used to study population-genetic processes and to develop statistical tests of population-genetic models. Uses of tree features in statistical tests often rely on predictions that depend on pairwise relationships among such features. For genealogies under the coalescent, we provide exact expressions for Taylor approximations to expected values and variances of ratios Xn/Yn, for all 15 pairs among the variables {Hn, Ln, En, In, Bn, Tk}, considering n leaves and 2 ≤ k ≤ n. For expected values of the ratios, the approximations match closely with empirical simulation-based values. The approximations to the variances are not as accurate, but they generally match simulations in their trends as n increases. Although En has expectation 2 and Hn has expectation 2 in the limit as n→∞, the approximation to the limiting expectation for En/Hn is not 1, instead equaling π2/3–2≈1.28987. The new approximations augment fundamental results in coalescent theory on the shapes of genealogical trees.

---

### A blueprint for a synthetic genetic feedback optimizer [^113GBDMY]. Nature Communications (2023). High credibility.

Comparator module

We next need to compare x with x d and y with y d to obtain the indicator signals x +, x −, y +, and y − (Fig. 1c). The comparator module provides these signals by combining three parts: the delay module from the previous section, bistable switches, and an oscillator, to periodically activate/deactivate regulatory interactions (Fig. 3a).

Fig. 3
The comparator module generates the indicator signals based on the actual and delayed signals for both the regulator and the reporter.

Simulation parameters and further details are provided in Supplementary Section 5. a The signal c alternates between two states (c = 0 and c = 1) with period τ, activating two different sets of regulatory interactions. b During phase 1, (x +, x −) tracks the reference (x, x d), whereas during phase 2, (x +, x −) converges to either of the stable fixed points based on the sign of x − x d. c The signals x + and x − switch between their ON and OFF states depending on whether x < x d or x > x d (phase 1 is depicted in gray). d Closed loop performance is largely unaffected by the value of α c,2.

For the sake of simplicity, here we assume that the output c of a genetic oscillator alternates between two values: c = 0 and c = 1 during phase 1 and phase 2, respectively. We rely on this periodic signal to switch the comparator with inputs x d and x and outputs x − and x + between two modes according towhere α c,1 and α c,2 are production rate constants (de-dimensionalized, see Supplementary Section 1.3), and ϵ c characterizes the timescale of the comparator dynamics.

During phase 1 (c = 0), the system behaves as the delay module in Fig. 2: the indicator signals x + and x − track the reference signals x and x d, respectively (Fig. 3b). Assuming that this tracking occurs on a timescale faster than that of x and x d, during phase 1 x and x d are effectively frozen and x + and x − converge to these values if α c,1 = 1 (Fig. 3c). Therefore, at the end of phase 1, the values of x and x d are stored in x + and x −, respectively.

---

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^112fS4S2]. American Journal of Kidney Diseases (2006). Medium credibility.

Methods for measuring and expressing the hemodialysis dose — Caution must be exercised when using Appendix methods to adjust dose for residual kidney function (Kr) values above 2 mL/min, and adequacy standards based on dialysis-session spKt/V do not include an adjustment for the continuous component of residual urea clearance. An alternative simplified method (using a table) for adjusting spKt/V in patients with Kr ≥ 2 mL/min/1.73 m2 can be found in CPR 4, Minimally Adequate Hemodialysis.

---

### Maximizing association statistics over genetic models [^111za28y]. Genetic Epidemiology (2008). Low credibility.

The assessment of the association between a candidate locus and a disease may require the assumption of an inheritance model. Most researchers select the additive model and test the association with the Cochran-Armitage trend test. This test assumes a dose-response effect with regard to the number of copies of the variant allele. However, if there is reason to expect dominance or recessiveness in the effect of the variant allele, the heterozygous genotype may be grouped with one of the two homozygous, depending on the inheritance model, and a simple test on the 2 x 2 table can be used to assess independence. When the underlying genetic model is unknown, association may be assessed using the max-statistic, which selects the largest test statistic from the dominant, recessive and additive models. The statistical significance of the max-statistic has been previously addressed using permutation or Monte Carlo simulation approaches. We aimed to provide simpler alternatives to the max-test to make it feasible in large-scale association studies. Our simulations show that this procedure has an effective number of tests of 2.2, which can be used to correct the significance level or P-values. We also derive the asymptotic distribution of max-statistic, which leads to a simple way to calculate the significance level and allows the derivation of a formula for power calculations in the design of studies that plan to use the max-statistic. A simulation study shows that the use of the max-statistic is a powerful approach that provides safeguard against model uncertainty.

---

### Pixel super-resolution with spatially entangled photons [^114hBx8p]. Nature Communications (2022). High credibility.

Analytical derivation of Γ k k and Γ k k +1

We consider an unidimensional object t (x) illuminated by photon pairs using a near-field illumination configuration. Photon pairs are characterised by a two-photon wavefunction Ψ t (x 1, x 2) in the object plane. JPD values Γ k l are measured using an array of pixels with pitch Δ and gap δ. Assuming that the imaging system is not limited by diffraction but only by the sensor pixel resolution, its point spread function can be approximate by a Dirac delta function and Γ k l can be formally written as:where we assumed unity magnification between object and camera planes. A graphical representation of this integral is shown in Fig. 5 c. For clarity, we only represented an array of three pixels. The bivariate function ∣ t (x 1) t (x 2)∣ 2 is represented in green and overlaps with an grid of squares of size Δ and spacing δ. Each square represents an integration area associated to a specific JPD value. For example, the central square corresponds to the integration area of Γ k k i.e. In addition, the bivariate function ∣Ψ t (x 1, x 2)∣ 2 is represented by two dashed black lines. These two lines delimit the most intense part of the function, which corresponds to a diagonal band of width σ using a double Gaussian model.

We seek to calculate the JPD values Γ k k and Γ k k +1. Graphically, these values are located at the intersection between the grid, the green area and the surface inside the dashed lines. They are represented in blue and red, respectively. For small widths σ < Δ, it is clear in Fig. 5 c that the blue and red integration areas are tightening around the positions (x k, x k) and (x k +1/2, x k +1/2) positions, which results in Γ k k ~ ∣ t (x k)∣ 4 and Γ k k +1 ~ ∣ t (x k +1/2)∣ 4. More formally, one can also apply a change of variable and perform a first-order Taylor expansion in Eq. 6 to reach the same results:whereand S 1 ≈ σ 2 /2. Full calculations are provided in section 2.5 of the supplementary document. Note also that the difference between integration areas S 1 ≠ S 2 makes the normalization step after calculating the JPD projection necessary (see Methods section Normalization).

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^112Romk3]. Nature Communications (2025). High credibility.

Fig. 2
The matrix-tree theorem.

A Computing the steady-state occupancy π 1 by summing weights over directed spanning trees. Directed spanning trees are subgraphs containing all graph nodes but no cycles, with edges oriented toward a root node. In each directed spanning tree, the input forces make a positive, negative, or zero contribution to the tree weight. The structural vectorsare shown below each tree; these quantities enter into Equation (3) below. B Schematic illustration of the high-dimensional space of feature vectors ψ (i; θ) and χ (i, F). The depicted arrangement of vectors could solve a binary classification problem.

We define the input multiplicityas the number of edges affected per input variable, which we assume to be the same for each input. To focus on the functional way in which the input driving enters the steady-state probabilities, the driving contributions can be factored out in the algebraic expressions for the numerator and denominator of Equation (1). This has been previously been used to make analytical progress for M = D = 1 in, for example, refs. –. This equivalent formulation of Eq. (1) suggests that steady states of Markov jump processes implement a rational polynomial function of exponentiated input variables. Defining, we rewrite the matrix-tree expression for π i for general D and M We use the multi-index, whereis the set of D input labels and each componentof the multi-index runs over the values, to enumerate themonomials. These monomials y μ (F) in Equation (2) combinatorially depend on the different mixtures μ of input driving, representing a net total μ a of signed contributions from the input force F a, μ b such contributions for F b, and so on for each input. The coefficients, which are functions of the parameters θ, are the sums of weights over all directed spanning trees rooted at node i which have the corresponding mixture μ of signed input contributions. The monomial coefficientsthus represent learnable amplitudes of each polynomial basis function y μ (F). The coefficients in the denominator are defined as. Classification will be successful if, for F ρ drawn from class ρ, the coefficientsand monomials y μ (F ρ) are large for the same μ. In the subsequent sections of the paper and in the Supplementary Information we use the formulation in Equation (2) to show how the classification ability of a non-equilibrium Markov processes may be systematically modulated.

---

### Estimating the additive genetic effect of the X chromosome [^113Vghqz]. Genetic Epidemiology (2005). Low credibility.

We propose a method for efficient estimation of the additive genetic effect of the X chromosome with explicit modeling of eutherian-type dosage compensation. The theoretical derivation of the variance-components model for X-linked loci is reviewed in detail. We develop a model of dosage compensation that allows for both incomplete and heterogeneous lyonization, the existence of which is suggested by recent expression studies. Modeling this relationship, especially in the limit cases of complete or absent compensation, allows estimation of the X effect as a single parameter for ease of comparison to other sources of variance. We present simulation studies to estimate the power and computational efficiency of our proposed method.

---

### A blueprint for a synthetic genetic feedback optimizer [^1178Ui7W]. Nature Communications (2023). High credibility.

Logic module

The final module in the optimizer (Fig. 1c) combines the indicator signals x +, x −, y +, and y − to generate u 1 and u 2 by approximating the control law in (4). This can be done by relying on standard logic gates, as illustrated in Fig. 4. For the sake of simplicity, here we assume that these logic gates operate on a faster timescale than any other module in Fig. 1c (Supplementary Section 1.4). Therefore, the behavior of the logic gates can be approximated by utilizing their quasi-steady state input-output mappings. In particular, considering the common mathematical model of AND (∧) and OR (∨) gates with input signals A and B (Supplementary Section 1.4), the outputs are given bywith Hill coefficient n and dissociation constants K ∧ and K ∨ for the AND and OR gates, respectively. The input-output mapping of the proposed circuit in Fig. 4 is thus given byand.

Fig. 4
The logic module combines the indicator signals to generate the control signals.

The dynamic range of the signals in the input, middle (between the AND and OR gates), and output layer of the logic module is denoted by ρ ∧, ρ ∨, and ρ u, respectively. While selecting the dissociation constants K ∧ and K ∨ in the geometric mean of the respective dynamic ranges may be an intuitive choice (yielding the dark gray lines), the performance of the optimizer displays significant robustness to deviations from this particular baseline choice when tracking the optimum value x * (blue). Colored circles correspond to different choices of α c,2 (affecting the input dynamic range ρ ∧), together with substantial perturbations in the dissociation constants compared to the above specified baseline choice. Simulation parameters and further details are provided in Supplementary Section 5.

---

### Combinatorial code governing cellular responses to complex stimuli [^117CCEA9]. Nature Communications (2015). Medium credibility.

Methods

Identification of the interaction profiles

The number of ways to rank the numbers e 0, e X, e Y and e X+Y was computed using the recurrence relation A n k = A n− 1 k +k A n− 1 k− 1 which, for n = 4 and summing up for k = 1, 2, 3 and 4 gives 75 possible rankings. Each ranking can be seen as a set of outcomes of the following qualitative pairwise comparisons:
e X versus e 0
e Y versus e 0
e X+Y versus e 0
e Y versus e X
e X+Y versus e X
e X+Y versus e Y

In our framework, the outcome of a qualitative comparison can take on the following three values: 0 (equal numbers), 1 (first larger than the second) and −1 (first smaller than the second). Each of 75 rankings was coded uniquely as a vector of six components describing the qualitative outcome of the comparisons 1–6 as listed above. For example, the vector (0,0,1,0,1,1) corresponds to:

e X = e 0, e Y = e, e X+Y > e 0, e Y = e X, e X+Y > e X, e X+Y > e Y

To identify which rankings were compatible with positive or negative interactions, we considered the equations 1–6 together with the inequalities that define positive and negative interactions:
Δ e X+Y > Δ e X + Δ e Y (positive interaction)
Δ e X+Y < Δ e X + Δ e Y (negative interaction)

which can be written as

(7a) e X+Y − e X − e Y + e 0 > 0

(8a) e X+Y − e X − e Y + e 0 < 0

If a ranking is consistent with a positive (or negative) interaction, the inequality constraints encoded in the corresponding vector can be solved simultaneously with 7a (or 8a). To verify this, we developed a constraint satisfaction that attempts to solve the six constraints of each ranking together with the inequality 7a (or 7b). The solution is searched numerically with a MATLAB linear solver. The variables e 0, e X, e Y and e X+Y were constrained to the interval 2–16, taken as an approximate of the range of expression values from Affymetrix chips in log2 scale. The method is implemented in the MATLAB code and is available upon request.

---

### Bulk tissue cell type deconvolution with multi-subject single-cell expression reference [^115RNHLX]. Nature Communications (2019). High credibility.

To illustrate this recursive tree-guided deconvolution procedure, we start with a simple case with four cell types and G genes. Let X 1, X 2, X 3, X 4 represent cell type-specific expression in the design matrix, obtained from scRNA-seq, and let Y be the gene expression vector in the bulk RNA-seq data. The relationship of bulk and single-cell data can be written aswhere the superscripts (1) and (2) indicate two sets of genes. Suppose the four cell types are grouped into two clusters, (X 1, X 2) and (X 3, X 4). The first set of genes are those showing small intra-cluster variance in gene expression, that is, and, whereas the second set of genes are the remaining genes.
Stage 1: Estimate cluster proportions π 1 = p 1 + p 2 and π 2 = p 3 + p 4,

The cluster proportions, and, are estimated by W-NNLS using intra-cluster homogenous genes.
Stage 2: Estimate cell type proportions (p 1, p 2, p 3, p 4),

The cell type proportions are estimated by W-NNLS using the remaining genes subject to the constraint that

Interconversion of different gene expression measures

MuSiC links bulk and single-cell gene expression by mRNA molecule counts. There are many measures of mRNA abundance, such as read counts, UMI counts, RPKM and TPM. As molecule counts are not observed in real studies, we approximate the molecule counts by read counts and estimate cell type proportions based on assumptions A1–A3. The interconversion between other gene expression measures and read count determines if MuSiC can utilize other measures as the input for deconvolution. One step in MuSiC estimation is the use of average library size as a proportional measure of average cell size for a given cell type, which is absent in normalized measurements of mRNA abundance such as RPKM and TPM. For RPKM, we would need the average library size for each cell type to be provided, or the average cell size for each cell type to be obtained from other sources. Cell type proportions cannot be estimated by MuSiC with TPM information alone. Below, we derive the relationships of various types of gene expression measures in detail.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^116DSPos]. CDC (2011). Medium credibility.

Appendix G. Mathematical formula for sample size — sample size required to detect the difference of GM between two subpopulation groups defines a z test with null hypothesis GM1 = GM2 or GMmax/GMmin = 1 and alternative GM1 ≠ GM2 or GMmax / GMmin > 1, where GMmax = max(GM1, GM2) and GMmin = min(GM1, GM2). It considers power W (say 80% chance) to detect that one group's GM is at least k fold higher and gives the p-value as p = Prob(Z ≥ z) = 1 − φ⁻¹(z) with Z ~ Normal (0,1). Given α (e.g., 0.05) and target ratio k, power is W = Prob (Z > φ⁻¹(1 − α) − log10(k)/(S√(1/n₁ + 1/n₂))), where S is the expected standard deviation of log10(VL) in the population of interest; when n1 = n2 = n, W = Prob (Z > φ⁻¹(1 − α) − log10(k)/(S√(2/n))). The required sample size is n = 2[(φ⁻¹(1 − α) − φ⁻¹(1 − W))S/log10(k)]², and tables are provided for W = 0.8 and W = 0.9 with α = 0.05.

---

### Approximations to the expectations and variances of ratios of tree properties under the coalescent [^1161DzoG]. G3 (2022). Medium credibility.

Exact expectations, variances, and covariances of tree properties

Expected values and variances of variables H n, L n, E n, I n, B n, and T k that are used in equations (3) and (4) are known, in many cases, from early studies in coalescent theory. We summarize these expectations and variances in Table 2.

The covariances compiled byappear in Table 3. In the case of pairs (E n, B n) and (I n, B n), the covariances are approximate, as described by.

Table 3.
Covariances of pairs of variables that summarize genealogical trees.

Evaluating the approximations

For each of 15 pairs of random variables, considering H n, L n, E n, I n, and B n as well as T k, we substitute expressions from Tables 2 and 3 into equations (3) and (4) to obtain approximate expectations and variances for ratios of pairs of variables. For each pair, we choose one variable for the numerator and the other for the denominator; approximate expectations and variances for the reciprocals can be obtained similarly. We present the approximations in Tables 4 and 5, and we plot them in Figs. 2–5.

Fig. 2.
Simulated and theoretical approximations of expectations of ratios of pairs of variables, plotted as functions of sample size n. Expressions for theoretical values are taken from Table 4.

Fig. 3.
Theoretical approximationsfor variables X in, plotted as functions of k for n = 10, n = 20, and n = 50. The expressions plotted are taken from Table 4.

Fig. 4.
Simulated and theoretical approximations of variances of ratios of pairs of variables, plotted as functions of sample size n. Expressions for theoretical values are taken from Table 5.

Fig. 5.
Theoretical approximationsfor variables X in, plotted as functions of k for n = 10, n = 20, and n = 50. The expressions plotted are taken from Table 5.

Table 4.
Approximations to expectations of ratios of pairs of variables.

Table 5.
Approximations to variances of ratios of pairs of variables.

For pairs (X n, Y n), we simulate the values ofandunder the coalescent model using ms, performing 100,000 replicate simulations for each tree size. We plot the simulated values alongside the approximate values from Tables 4 and 5 in Figs. 2 and 4.

---

### Frequency spectra and the color of cellular noise [^114nuyw1]. Nature Communications (2022). High credibility.

It is known from ref.that the combined closed-loop dynamics is ergodic and mean steady-state protein copy-number is μ / θ

As discussed in ref. this ergodicity is preserved under certain conditions when an extra negative feedback from protein X 2 to the production of mRNA X 1 is added. Letting z 1 and x 2 denote the copy-numbers of Z 1 and X 2, respectively, we add the extra feedback by changing the rate of the actuation reaction from k z 1 to (k z 1 + F b (x 2)) where F b is a monotonically decreasing feedback function which takes non-negative values. As in ref. we consider two types of feedback. Lettingto be the reference point, the first is Hill feedback of the formwhich is based on the actual output copy-number x 2, while the second is the proportional feedback that is essentially the linearisation of the Hill feedback at the reference pointOne can easily see that at the reference point, the values of this feedback functionand its derivative(equal to − k fb) are the same for both types of feedback. We can view k fb as the feedback gain parameter. The Hill feedback is biologically more realisable, while the proportional feedback captures the classical controller where the feedback strength depends linearly on the deviation of the output x 2 from the reference point, in the output range. In our analysis, we set the reference pointas the set-point μ / θ.

---

### A transition to stable one-dimensional swimming enhances E. coli motility through narrow channels [^116XSYMa]. Nature Communications (2020). High credibility.

In order to locate more precisely the threshold tunnel size for stable axial swimming we introduce a minimal model to describe position fluctuations along a transverse coordinate x. For axial swimming to be stable, a restoring speed component should appear on the cell center as it moves away from the axis (x = 0). For small displacements we may assume that this speed component is linear in x :where α is the slope ∂ v x /∂ x and η represents an effective white noise term 〈 η (t) η (0)〉 = A δ (t) incorporating random speed fluctuations due to both thermal and active fluctuations in flagellar dynamics. A positive α means stable axial swimming while a negative α leads to unstable axial swimming. In this situation we would expect to describe the x ~ 0 part of the histograms in Fig. 2 c with a Gaussian distribution. Figure 4 a confirms that the logarithm of the distribution of cell positions is well fitted by a quadratic law where the coefficient of the quadratic term can be assumed to be an indicator of the slope α. If we now plot α as a function of tunnel width we find a nice linear curve crossing zero for a channel size of 2.4 μm (Fig. 4 b). We evaluate α also numerically using a Rotne–Prager method (see Methods) to simulate a full cell swimming at different lateral displacements from the tunnel axis. Numerical values for α are reported on top of Fig. 4 b. Although experimental and numerical values for α can be only compared within an unknown multiplication factor, we find a remarkable agreement with experimental data with an α value that goes to zero for a tunnel width that is very close to the experimental value. In simulations we can easily check for the robustness of this result for different shapes of the tunnel cross-section. Repeating the calculations for a circular tunnel we find that α values can be scaled on top of each other by expressing tunnel widths as the square root of cross-sectional area (side length for square tunnels andradius for circular tunnels)

---

### Towards a fully automated algorithm driven platform for biosystems design [^1174Qzme]. Nature Communications (2019). High credibility.

We next sought to illustrate the optimization method with a similar 3-variable function with three inputs and one output to simulate a similar multi-dimensional optimization problem. It is noteworthy that Bayesian optimization has been used in numerous applications – and the purpose of this simulation is testing the algorithm on a simple but similar setting. The search perimeter was set to be 1–24 for each of the inputs and the maximum of the function was set to be 9 (y = f (x 1, x 2, x 3) | x i ɛ {1, 2, …, 24}, f max = 9). The Bayesian optimization algorithm was able to find the maximum value of this function by only evaluating 12 points out of all possible 24 3 = 13,824 points. These 12 evaluations were the result of 12 iterations of learning and testing, with each evaluation being followed by a learn step that produced the next point to evaluate. We then sought to compare this optimization strategy with baseline approach where randomly sampled points are evaluated and all of these points are used to train an Exterior Derivative Estimation (EDE)-based regression model described in previous publications. We found that although the EDE approach shows impressive predictive capability, especially given that all the data have been acquired at once and not through iterative sampling, even after sampling 192 random points, the maximum could not be found (Supplementary Table 1).

---

### Evolving copy number gains promote tumor expansion and bolster mutational diversification [^114BTNa4]. Nature Communications (2024). High credibility.

However, if cells with GD can acquire an additional beneficial post-GD gain (meaning GD alone is not enough to drive the final expansion), the situation drastically changes. To emphasize how this happens, let us denote cells with the beneficial post-GD gain by type 1 cells and consider the first type 1 cell that grows into an infinite number of descendants. We assume that the descendants of the first type 1 cell dominate the cell population when the sampling is performed (see Methods for details). The expected number of passenger post-GD gains carried by a type 1 cell at the moment of its introduction is proportional to the time of occurrence of the type 1 cell (the birth time of the tumor-initiating cell is set to be 0). In Methods we show that the distribution of the birth time of the first non-extinct type 1 cell, where σ 1 represents the birth time and Ω ∞ represents the event that the population does not go extinct, can be characterized as a function of the rate of beneficial gains u 1 and growth parameters of type 1 and type 0 cells, respectively (Lemma 1).is thuswhere we utilized the formula (see Section V.6 of) to calculate the expected birth time of the first non-extinct type 1 cell. This calculation involved utilizing the tail probability and multiplying the expected birth time by the passenger mutation rate u 0.

---

### Evolution of new regulatory functions on biophysically realistic fitness landscapes [^116pU5HH]. Nature Communications (2017). Medium credibility.

Methods

We consider mutation rates to be low enough that a beneficial mutation fixes before another beneficial mutation arises, allowing us to assume that the population is almost always captured by a single genotype. The probability that the population occupies a particular genotypic state, evolves according to a continuous-time discrete-space Markov chain. Transition rates between states are a product between the mutation rates between different genotypes and the fixation probabilities that depend on the fitness advantage a mutant has over the ancestral genotype, r xy = 2 Nμ xy Φ y → x, where N is the population size, μ xy is the mutation rate from genotype y to x, and Φ y → x is the probability of fixation of a single copy of x in a population of y. Our model only requires us to keep track of mismatches and not full sequences (i.e. the reduced-genotypes,), which significantly reduces the genotype space dimensionality. This framework allows for calculation of the steady state distribution of genotypes, or reduced-genotypes Eq. (3) and classification of genotypes into relevant macrostates.

To calculate the neutral distribution P 0 of the reduced-genotypes (distribution in the absence of selection), we enumerate the number of possible BS sequences j that have mismatch values (k 1 j, k 2 j) with respect to two TFs that match each other at M out of L consensus positions:The neutral distribution (up to proportionality constant) equalsWe iterate this calculation for various parameter combinations (Ns, ρ, f 1,2). For each, we determine the most probable macrostate at steady state (Supplementary Note 2) as illustrated in Fig. 3.

To determine evolutionary dynamics we numerically integratein time-steps corresponding to one generation t g :where R is the Markov chain transition matrix. Again, at every time-point we determine the most probable macrostate (Supplementary Note 2), as illustrated in Fig. 4.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^111tWAoz]. Nature Communications (2021). High credibility.

Motif III: logic

A general class of functions used to describe natural,– and synthetic, biological networks are logic gates, which have two inputs regulating a single output (Fig. 5). Gates exhibit either high ("on") or low ("off") output depending on whether inputs are on or off. For example, the AND gate specifies high output only if both inputs are on. In this section we provide a specific DDE-based framework that covers 14 out of 16 possible 2-input logic operations, and show that these operations form a continuous 2D parameter space.

Fig. 5
A simple approximation for digital logic using a sum of Hill terms recapitulates all monotonic logic functions in a single parameter space.

a A prototypical regulatory network involving logic where X and Y both regulate Z, which must integrate the two signals using some logic before it can in turn activate a downstream reporter R. b Parameter space showing regions where regulation approximately follows 14 of the 16 possible 2-input logic functions depending on the strength of two single-variable Hill regulation terms (η Z 1: regulation of Z by X, η Z 2: regulation of Z by Y). Network logic can be smoothly altered by varying the parameters (η Z 1, η Z 2), with a change of sign in (n 1, n 2) required to switch quadrants. The bottom-left quadrant shows that very weak regulation in both terms leads to an always-off (FALSE) function, weak regulation in one arm only leads to single-input (X, Y) functions, strong regulation in both arms leads to an OR function, and regulation too weak in either arm alone to activate an output but strong enough in sum leads to an AND function. The other three quadrants are related by applying NOT to one or both inputs, with function names related by de Morgan's lawNOT(X OR Y) = NOT X AND NOT Y. In particular, X IMPLY Y = NOT(X) OR Y, X NIMPLY Y = X AND NOT(Y), X NOR Y = NOT X AND NOT Y, and X NAND Y = NOT X OR NOT Y. Truth tables for all 16 logic gates are provided in Supplementary Table 1 for reference. The two non-monotonic logic functions, X XOR Y and X XNOR Y, are those 2 of 16 not reproduced directly using this summing approximation. They can be produced by layering, e.g. NAND gates. c Representative time traces for AND (η Z 1 = η Z 2 = 0.9) and OR (η Z 1 = η Z 2 = 1.8) gates with n 1 = n 2 = −2, n 3 = −20, η R = η Z 1 + η Z 2. The functionwhen n > 0, when n < 0.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^115G7opx]. Nature Communications (2021). High credibility.

Frequency response of feedforward motifs can be solved analytically and demonstrates low- and band-pass filtering capabilities

Biological regulatory networks often encode information as the change in frequency of an oscillating input, which has been suggested to be more robust to noise than encoding information in absolute concentration –. That feedforward loops filter short square pulses suggests more general frequency-filtering capabilities. We therefore analyze feedforward frequency response to sinusoidal input (Fig. 6 d–g). We show that the response follows the outline of a universal transfer function curve, independent of the logic or delay difference.

Instead of the step input analyzed above, here we consider a sinusoidal inputwhich oscillates between zero and 2 A (twice the amplitude) at a frequency f > 0. Taking the Fourier decomposition of each Hill-regulated term 1/[1 + X n (T)] and plugging into the governing Eq. (21) (Supplementary Note 6) provides the output Z (T) in terms of its magnitude I k and phase ϕ k as a function of frequency:whereare Fourier coefficients of Hill-regulated terms 1/[1 + X n (T)] at integer multiples k of the fundamental frequency f :for all f > 0. Note that the magnitudes are symmetric to interchange of the two regulation arms (η 1 ↔ η 2, n 1 ↔ n 2), while the phases are not.

---

### A multi-sample approach increases the accuracy of transcript assembly [^113886zp]. Nature Communications (2019). High credibility.

For completeness, we include a brief description of the dynamic programming optimization procedure. The algorithm considers all subpaths L, and recursively calculates the maximum number of constraints f (L) for substranscripts starting with subpath L: f (L) = max L ' { f (L ′) + c (L, L ′), if L ′ exists; c (L), if L ′does not exist }, where: (i) L ′ is a subpath immediately following L so that all constraints compatible with L end before or within L ′; (ii) c (L, L ′) is the number of constraints starting in and (partially) compatible with L and L ′, and compatible with the concatenated subpath L, L ′; and (iii) c (L) is the number of constraints covered by subtranscript L. To take into account the abundance levels in the optimization process, at each sweep of the graph the algorithm excludes subpaths that cover constraints with abundance below a fixed value x; hence, the dynamic programming algorithm will return the best transcript with abundance greater than x (x -abundance transcript). With this modification, at each graph sweep the selection process selects an x -abundance transcript, starting with x 0 = 0 (thus guaranteeing that such a transcript exists), and each selected transcript's abundance value used as lower bound for the selection process at the following step: 0 = x 0 < x 1 < x 2 < ⋯ x m, until no transcript can be found. The optimal transcript then is among those selected by the sweeps. More details, along with proof of correctness for the algorithm, can be found in Song et al.

Selecting a global set of transcripts. PsiCLASS selects a set of meta-annotations from the individual samples' sets of transcripts by voting. Each transcript is assigned a score equal to the transcript's average estimated abundance across the samples (i.e. every sample has one vote, weighted by the transcript's abundance level; default cutoff: 1.0). As different cutoff parameters values might work best for data with specific characteristics, the user can readily adjust or recalibrate the voting parameters postassembly, starting from the already computed sets of transcripts for the individual samples.

---

### Identifying domains of applicability of machine learning models for materials science [^113w9fho]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ν that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by σ lin (x 1, x 2) ≡ −0.3 ≤ x 1 ≤ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: σ rbf ≡ −3.3 ≤ x 11 ≤ 3.1 ∧ x 2 ≤ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA σ poly (x 1, x 2) ≡ −3.5 ≤ x 2 ≤ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### Complete deconvolution of cellular mixtures based on linearity of transcriptional signatures [^115zDw4z]. Nature Communications (2019). High credibility.

Singular value decomposition estimates number of cell types

One important aspect of all deconvolution methods is that they require knowledge of the number of pure cell types that make up the mixture. Fortunately, understanding the linear structure of the transcriptional space provides a direct way to infer the number of linearly independent components that contribute to variation in the dataset. In the idealized scenario, the matrix of the expression data X is the product of the matrices of pure cell type signatures W and corresponding proportions matrix H (Fig. 4d). Both H and W are matrices of rank N (number of pure cell types); accordingly, their product is a matrix of the same rank N. Therefore, if we can compute the effective rank of matrix X of mixed gene expression data, we can immediately infer the number of pure cell types in the mixture. In practice, there are two limitations: (1) gene expression matrix X is a non-square matrix, and traditional eigenvalue-based approaches are not applicable; (2) matrix X inevitably contains noise, and therefore the rank of X cannot always be defined precisely. These limitations can be circumvented to some extent by using Singular Value Decomposition (SVD) (Methods). For instance, Fig. 4e shows the cumulative variance explained by singular vectors obtained by SVD of the gene expression matrix of mixed samples from Fig. 4c, which immediately reveals that there are four major linearly independent components that define this gene expression matrix.

---

### An expectation-maximization algorithm for the analysis of allelic expression imbalance [^114HjFqo]. American Journal of Human Genetics (2006). Low credibility.

A significant proportion of the variation between individuals in gene expression levels is genetic, and it is likely that these differences correlate with phenotypic differences or with risk of disease. Cis-acting polymorphisms are important in determining interindividual differences in gene expression that lead to allelic expression imbalance, which is the unequal expression of homologous alleles in individuals heterozygous for such a polymorphism. This expression imbalance can be detected using a transcribed polymorphism, and, once it is established, the next step is to identify the polymorphisms that are responsible for or predictive of allelic expression levels. We present an expectation-maximization algorithm for such analyses, providing a formal statistical framework to test whether a candidate polymorphism is associated with allelic expression differences.

---

### Transient power-law behaviour following induction distinguishes between competing models of stochastic gene expression [^111GPGgB]. Nature Communications (2025). High credibility.

Inferring the number of inactive gene states is difficult from steady-state data

In Fig. 2 the steady-state mRNA count distributions of thousands of rate parameter sets for the N = 3, 4, 5 state models are compared with those of their effective telegraph models (computed using the previously detailed three-step procedure). Note that to ensure biological relevance, the rate parameter ranges for each model were chosen to comfortably cover the bounds of the telegraph model rate parameters estimated using a range of eukaryotic data in various published studies (see Table 1 of ref.for these estimates and see Supplementary Note 2 for further details on generating the rate parameter sets).

Fig. 2
The steady-state mRNA count distribution of the N -state model can often be well fit by an effective telegraph model.

Thousands of rate parameter sets of the N -state model (N = 3, 4, 5) were sampled in the range relevant for eukaryotic gene expression. For each parameter set the mapping in Eq. (2) was used to obtain the rate parameters of an effective telegraph model. A – C P-P plots comparing the cumulative distribution function (CDF) of the N -state and effective telegraph models. These are constructed from the 500 rate parameter sets with the largest Wasserstein distance (WD) values between the distributions of the two models. Deviations from the line y = x (red) indicate differences between the distributions. D – G Typical steady-state mRNA count distributions of the 5-state (solid blue lines) and effective telegraph models (histograms) for four classes of distributions (Shape I: Unimodal with Fano factor < 2; Shape II: Unimodal with Fano factor ≥ 2; Shape III: Bimodal with one peak at zero and another at a non-zero value; Shape IV: Bimodal with peaks at non-zero values. H Boxplots displaying summary statistics of the WD between the steady-state distribution of mRNA counts of the N -state and effective telegraph model. The horizontal line and the box show the median and the interquartile range, respectively; whiskers extend to the 10th and 90th percentiles; sample sizes given in Supplementary Table 2. I – K Bar charts displaying the percentage of each distribution shape sampled across the parameter space. The agreement between the N -state and its effective telegraph model becomes worse as N increases and is worst for Shape IV because this shape is qualitatively impossible to capture by the telegraph model. See Supplementary Note 2 for implementation details.

---

### Using both qualitative and quantitative data in parameter identification for systems biology models [^115L44gc]. Nature Communications (2018). Medium credibility.

We must choose the penalty constants C i in Eq. (3) based on our confidence in the qualitative data — a larger C i gives the qualitative data more weight compared to the quantitative data. We hand-selected a value of C i = 0.03 for all i, to give roughly equal contributions from the qualitative and quantitative datasets (i.e. such that the blue and orange curves in Fig. 2d, e can be plotted using the same scale). Note that giving equal contributions to both datasets is an arbitrary choice for illustration — with real experimental data, the modeler may choose to give unequal weights based on the relative importance/credibility of the datasets.

We minimized f tot (x) by differential evolution, and found best-fit parameters of K 3 = 5100 μM −1 and K 5 = 0.060 μM −1, which are reasonably close to the ground truth.

To evaluate the strengths of combining quantitative and qualitative data, we performed uncertainty quantification. We used a variant of the profile likelihood approach, a method that is well-established for quantitative fitting. One parameter of interest is held fixed, and the objective function is minimized by varying the remaining parameters. The resulting minimum is taken as the negative log likelihood of the fixed parameter value. The minimization is repeated for many possible fixed values of the parameter to produce a curve (Fig. 2d, e). Note that our objective function is not a likelihood in the rigorous sense, but has a similar interpretation in that a lower objective value indicates the parameter value is more consistent with the data.

We performed profile likelihood analysis using f quant (x), f qual (x), and f tot (x) in turn as the objective function, and considering two free parameters K 3 and K 5 (Fig. 2d, e). We found that each dataset individually provided bounds on possible parameter values, but the tightest bounds were obtained when we combined both datasets.

---

### Prior ground: selection of prior distributions when analyzing clinical trial data using Bayesian methods [^1111s1ku]. NEJM Evidence (2023). Medium credibility.

Increasingly, investigators are choosing to use Bayesian methods for the analysis of clinical trial data. Unlike classical statistical methods that treat model parameter values (such as treatment effects) as fixed, Bayesian methods view parameters as following a probability distribution. As we have written previously, 1 by analyzing clinical trial data using Bayesian methods one can obtain quantities that may be of interest to clinicians, providers, and patients, such as the probability that a treatment effect is more or less than 0, that is, the probability that a treatment is effective.

---

### A faster and more accurate algorithm for calculating population genetics statistics requiring sums of stirling numbers of the first kind [^1148V1ip]. G3 (2020). Medium credibility.

Ewen's sampling formula is a foundational theoretical result that connects probability and number theory with molecular genetics and molecular evolution; it was the analytical result required for testing the neutral theory of evolution, and has since been directly or indirectly utilized in a number of population genetics statistics. Ewen's sampling formula, in turn, is deeply connected to Stirling numbers of the first kind. Here, we explore the cumulative distribution function of these Stirling numbers, which enables a single direct estimate of the sum, using representations in terms of the incomplete beta function. This estimator enables an improved method for calculating an asymptotic estimate for one useful statistic, Fu's [Formula: see text] By reducing the calculation from a sum of terms involving Stirling numbers to a single estimate, we simultaneously improve accuracy and dramatically increase speed.

---

### Derivation of a mathematical expression for predicting the time to cardiac events in patients with heart failure: a retrospective clinical study [^111RTdmZ]. Hypertension Research (2013). Low credibility.

The prognoses for patients with certain diseases are estimated by averaging the results of clinical trials. To investigate the possibility of deriving a mathematical formula for the estimation of prognosis, we formulated the equation τ = f(x1, …, xp), where x1, …, xp are clinical features and τ represents the clinical outcome for heart failure (HF). We attempted to determine the function to mathematically formulate the relationship between clinical features and outcomes for these patients. We followed 151 patients (mean age: 68.6 ± 14.6 years; men: 61.6%) who were consecutively hospitalized and discharged as a result of acute decompensated HF (ADHF) between May 2006 and December 2009. The mathematical analysis was performed through a probabilistic modeling of the relational data by assuming a Poisson process for rehospitalization owing to HF and by linearly approximating the relationship between the clinical factors and the mean elapsed time to rehospitalization. The former assumption was validated by a statistical test of the data, and the contribution of each parameter was assessed based on the coefficients of the linear relation. Using a regularization method to analyze 402 clinical parameters, we identified 252 factors that substantially influenced the elapsed time until rehospitalization. With the probability model based on the Poisson process, the actual (X; 388 ± 377 days) and estimated (Y; 398 ± 381 days) elapsed times to rehospitalization were tightly correlated (Y = 1.0076X+6.5531, R(2) = 0.9879, P < 0.0001). We established a mathematical formula that closely predicts the clinical outcomes of patients who are hospitalized with ADHF and discharged after appropriate treatment.

---

### Unified theory for light-induced halide segregation in mixed halide perovskites [^113sGhkb]. Nature Communications (2021). High credibility.

Our finding that x ≈ 0 for the photosegregated I-rich phase (see the dashed green lines in Fig. 3) seems at odds with the experimental finding of Hoke et al. that x ≈ 0.2 when segregation is complete. An explanation for the latter finding was given by Ruth et al. In their kinetic Monte Carlo simulations of vacancy-mediated hopping of I and Br ions during the phase segregation process, Br ions get kinetically trapped in the I-rich nuclei, with a final concentration close to 0.2. Our theory applies to the onset of phase segregation and is therefore not incompatible with this result. There is recent experimental evidence from photoluminescence measurements that halide segregation commences with an almost I-pure phase, which gradually becomes less pure by the inclusion of Br. This is in line with our argument.

Threshold photocarrier densities

Figure 4 shows for the five compounds results for the threshold photocarrier density n t for halide segregation. This is the value of n at which the light-induced binodals are crossed for a given Br concentration x and temperature T (the full green lines in Fig. 3 f–t). The light-induced nucleated phase is almost 100% I-rich (see the dashed green lines in Fig. 3 f–t). From this fact, the following very accurate expression can be derived for n t (see 'Methods'):where Δ E g (x) ≡ E g (x) − E g (0). The prefactor in this expression is. Equation (4) predicts extremely low thresholds n t at room temperature. We note that n t is the threshold photocarrier density in the mixed phase or in the parent phase at the onset of phase separation. The photocarrier density in the nucleated phase is according to Eq. (1) much larger. For example, for MAPb(I 0.5 Br 0.5) 3 we have Δ E g (x = 0.5) ≈ 0.28 eV (see the green line in Fig. 2 b), so that the photocarrier density in the almost I-pure nucleated phase is at room temperature a factor of about 7 × 10 4 larger. This also means that, while the bimolecular recombination in the mixed or in the parent phase is negligible, this is definitely not the case in the nucleated phase.

---

### Cells change their sensitivity to an EGF morphogen gradient to control EGF-induced gene expression [^11516fsm]. Nature Communications (2015). Medium credibility.

We generated 5 × 10 4 random parameter combinations of the parameters θ, β, φ and. For each parameter the value was given by, with the exponentuniformly distributed in a particular interval, so that the parameter values span many orders of magnitudes (Supplementary Fig. 7). For each time point, we obtained the average VPC body length as the mean distance between nuclei of adjacent VPCs for P4.p-P8.p, averaged over all animals in the time point.

This yielded L C = {15,15,17,19,19,18}μm for each time point ordered by increasing time of induction. For each random parameter combination and for each time point, we then obtained a LIN-3 decay length and amount of activator A that formed the best fit to the measured spatial expression profiles in Fig. 4a, b. Specifically, we obtained the values for λ and α lag −2 that minimized the SSE between the experimental data and the spatial expression profile calculated for Model B3 using equation 3. In general, we found many parameter combinations that were able to reproduce the experimental data in Fig. 4a, b, all constrained to very specific regions in parameter space (Supplementary Fig. 7a). We subsequently selected the subset of these fits that also accurately reproduced the gene expression dynamics for the lin-3(++) and lin-1(0) mutants (Supplementary Fig. 7b, c). Specifically, for each parameter combination we also calculated the lag-2 expression dynamics in P6.p, that is, for x = 0, for both the lin-3(++) mutant (θ ′ = 10 θ) and the lin-1(0) mutant (λ 1, λ 2 = 0). To allow for comparison between the SSEs for different mutants and different time points, we normalized each of the different SSEs by the median SSE over all 5 × 10 4 parameter combinations. Finally, we calculated a single SSE for the fit to the lag-2 and apx-1 expression profiles by summing the normalized SSEs for each time point and ligand. This still yielded many good fits, but constrained the parameters to a substantially smaller region of parameter space (Supplementary Fig. 7a). To select the overall best fit, we selected the parameter combination that minimized the sum of the single SSE for the spatial expression profiles and the SSEs with respect to the lin-3(++) and the lin-1(0) mutants. This fit, which is very similar in quality to the best 1% of fits, accurately captures the observed spatial expression profiles and time dynamics of both lag-2 and apx-1 (Fig. 4a, b, f–i).

---

### Stone treatment index: a mathematical summary of the procedure for removal of stones from the urinary tract [^111Dc6aC]. Journal of Endourology (2007). Low credibility.

Background and Purpose

Numerous factors influence the treatment result and efforts for stone removal. To summarize the important factors, our aim was to formulate a general mathematical expression of the stone-removal procedure.

Materials and Methods

A mathematical expression (stone treatment index; STI) was developed for patients who became stone free (SF): STI(SF) = (Num(SF) x mean(sqrt [SA] x HI) x meanAge(R) x meanBMI(R) x (1 + meanNum(ANA)) divided by Num(SESSIONS) + Num(AUX) + Num(ANE) + Num(COMP). The variables included were the number of stone-free patients (Num(SF)), mean stone burden expressed as the product of the square root of the stone surface area (SA(1/2)) and the hardness index (HI), the mean age ratio (Age(R) = age/50), the mean body mass index (BMI) ratio (BMI(R) = BMI/25), the mean number of patients with anatomic abnormalities (Num(ANA)), the total number of treatment sessions (Num(SESSIONS)), the number of auxiliary procedures (Num(AUX)), the number of procedures requiring general or regional anesthesia (Num(ANE)), and the number of complications (Num(COMP)). A similar index was calculated for patients who were either stone free or had residual fragments ≤ 4 mm (STI(DIS)). For conclusions on efficacy, these indices were compared with optimal and total values of STI. The STI was calculated for 450 patients with renal and 374 patients with ureteral stones treated with shockwave lithotripsy as the primary procedure. In patients with a known BMI, this factor was used; otherwise, the BMI(R) was set to 1.

Results

When STI was compared with the efficiency quotient (EQ) for stones in different size intervals, STI was less sensitive to variations in stone burden. The quotients STI(SF)/STI(OPTIMAL) corresponded roughly to EQ but might be more informative because of the inclusion of factors for anesthesia and complications.

Conclusion

With or without related estimates, the STI might be useful for summarizing stone-removal procedures in groups or individual patients with urinary-tract stones. The STI might be helpful because it encompasses the factors of importance for conclusions about the treatment procedure, such as the hardness of the stone, the age and body size of the patient, and the presence of anatomic abnormalities likely to influence the result. The STI might be used advantageously for comparison of different equipment and treatment strategies.

---

### Casein [^117U7k7m]. FDA (2009). Low credibility.

Volume desired x Concentration desired = Volume needed x Concentration available.

Example 1: If a 1:10 w/v extract is available and it is desired to use a 1:1,000 w/v extract substitute as follows:

Vd x Cd = Vn x Ca

10ml x 0.001 = Vn x 0.1

0.1 mL = Vn

Using a sterile technique, remove 0.10 mL of extract from the 1:10 vial and place it into a vial containing 9.90 mL of sterile diluent. The resulting ratio will be a 10 mL vial of 1:1,000 w/v.

Example 2: If a 10,000 pnu/ml extract is available and it is desired to use a 100 pnu/ml extract substitute as follows:

10ml x 100 = Vn x 10,000

0.1 mL = Vn

Using a sterile technique, remove 0.10 mL of extract from the 10,000 pnu/ml vial and place it into a vial containing 9.90 mL of sterile diluent. The resulting concentration will be a 10 mL vial of 100 pnu/ml.

Example 3: If a 10,000 AU/ml or BAU/ml extract is available and it is desired to use a 100 AU/ml or BAU/ml extract substitute as follows: Vd x Cd = Vn x Ca

---

### Optimal marker gene selection for cell type discrimination in single cell analyses [^1134JLwy]. Nature Communications (2021). High credibility.

Synthetic multimodal gene expression

We next considered a synthetic model based on the intuition that cell labels may be defined by nonadditive gene interactions. We illustrate this through simulations in which gene expression is multimodal, and synthetic cell labels are defined in combinatorial ways, such as an exclusive-or (xor) relationship among a pair of genes (Fig. 1C-i–v, Table 1, and Supplementary Figs. 2–6).

Table 1
In this example, we consider three functional groups, A, B, C.

Across all groups, we consider a homoskedastic variance σI k, with σ = 0.5, where I 2 is the identity matrix and k is the number of genes determining the group. Group A is determined by two genes with multivariate normal gene expression with means [0, 0], [0, 1], [1, 0], and [1, 1], group B is determined by one gene with multivariate normal gene expression means [2] (mode B1) and [3] (mode B2). Similarly, group C is determined by two genes with multivariate normal gene expression means [0, 0], [2, 0], [0, 2], and [2, 2]. Cell labels are identified through functional group tuples (A i, B j, C k). In this example, we have 32 = 4 × 4 × 2 possible cell types. For instance, if we consider the cell types T 1 = (A 1, B 1, C 4) and T 2 = (A 2, B 2, C 3) it would suffice to use G 3 as a marker to distinguish both cell types. However, if we consider all possible 32 cell types, one needs all five genes to distinguish them. We use this functional groups example in Fig. 1C and Supplementary Figs. 3 and 4.

---

### Generating quantitative binding landscapes through fractional binding selections combined with deep sequencing and data normalization [^112fPGmD]. Nature Communications (2020). High credibility.

All available experimental data on ΔΔ G bind for the BPTI/BT complex was used to obtain the best normalization formula for converting enrichment values from four sorted populations into ΔΔ G bind values. To this end, we used a linear regression model function in Mathematica (Wolfram Research) with five parameters (Y = a X 1 + b X 2 + c X 3 + d X 4 + f). The parameters a, b, c, d, f were optimized using 29 experimental data points as values of Y and the set of X 1, X 2, X 3, X 4 values. The obtained normalization formula was used to calculate ΔΔ G bind values for all the remaining single BPTI mutants sampled in the NGS experiment, for which no ΔΔ G bind values were previously measured. To calculate the uncertainties in ΔΔG bind predictions we propagated the errors in enrichment values into the normalization formula (Eq. 1). The standard deviation of ΔΔ G bind predictions for each BPTI mutant was calculated according to the formula:where a, b, c, d are the coefficients in front of X 1, X 2, X 3, and X 4 in Eq. (1), respectively; ∂ X 1, ∂ X 2, ∂ X 3, ∂ X 4 are the standard deviations on these variable obtained from the bootstrapping analysis of the NGS data and ∂ a, ∂ b, ∂ c, ∂ d, ∂ f are the standard deviations of these coefficients obtained from the leave-one-out analysis. 95% confidence level was calculated assuming a normal distribution as CI = 1.96*σ.

BPTI mutant expression and purification

The BPTI WT sequence was cloned into a pPIC9K vector and desired mutation was introduced by the TPCR protocol. The mutants were expressed in P. pastoris (GS115 strain, ATCC ® 20864) and purified by nickel affinity chromatography, followed by size-exclusion chromatography, as described in previous work. The correct DNA sequence of each produced protein was confirmed by extracting the plasmidic DNA from P. pastoris and sequencing. Protein purity was validated by SDS-PAGE on a 20% polyacrylamide gel, and the mass was confirmed with mass spectrometry.

---

### Treatment of the X chromosome in mapping multiple quantitative trait loci [^113cR8vn]. G3 (2021). Medium credibility.

Statistical methods to map quantitative trait loci (QTL) often neglect the X chromosome and may focus exclusively on autosomal loci. But the X chromosome often requires special treatment: sex and cross-direction covariates may need to be included to avoid spurious evidence of linkage, and the X chromosome may require a separate significance threshold. In multiple-QTL analyses, including the consideration of epistatic interactions, the X chromosome also requires special care and consideration. We extend a penalized likelihood method for multiple-QTL model selection, to appropriately handle the X chromosome. We examine its performance in simulation and by application to a large eQTL data set. The method has been implemented in the package R/qtl.

---

### Gene-expression measurement: variance-modeling considerations for robust data analysis [^112ztjMs]. Nature Immunology (2012). Medium credibility.

System-wide measurements of gene expression by DNA microarray and, more recently, RNA-sequencing strategies have become de facto tools of modern biology and have led to deep understanding of biological mechanisms and pathways. However, analyses of the measurements have often ignored statistically robust methods that account for variance, resulting in misleading biological interpretations.

---

### Uncovering the mechanism for aggregation in repeat expanded RNA reveals a reentrant transition [^116FZKnU]. Nature Communications (2023). High credibility.

In order to fit experimental data on the prevalence of multiple nucleic acid strands binding to one another in vitro, nucleic acid models include a free energy penalty for multimerization. This leads to the term (m −1)Δ F in Eq. 2. This penalty is motivated by the enthalpic and entropic costs of nucleic acids binding, including ion effects and the translational and orientational entropies lost upon association –. This penalty scales linearly with the number of strands in a multimer, such that each additional strand added to a multimer carries the same penalty. See the "Methods" section and Supplementary Note 2 for further discussion.

The sum in Eq. 2 can be approximated by its dominant term (a saddlepoint approximation). There are three regimes to consider, corresponding to strong, intermediate, and weak binding, in which the sum in Eq. 2 is dominated by large, intermediate, and small values of N b, respectively (Fig. 1 c). The value ofthat dominates the sum is that which maximizes a combination of the bond energy F and configurational entropy g. For example, the strong binding regime is characterized by bond energy considerations overwhelming configurational entropy effects, while the intermediate binding regime is characterized by a degree of balance between the two.

The model is validated by comparing to exact computational enumeration and previously published results

To validate the analytical model, we constructed a dynamic programming-based computational model that exactly enumerates Z m in polynomial time (Supplementary Note 5.2). The analytical model described above makes three primary approximations compared to the computational model: (1) it assumes a constant entropy for all loops; (2) it considers only structures with a given number of bonds N b (with a single next-order correction term); (3) it uses an approximate form for g (n, m, N b) (see the "Methods" section). The computational model makes none of these approximations, considering all (non-pseudoknotted; see Supplementary Note 5.1) structures that can form and including a loop-length-dependent loop entropy term.

---

### Digital display precision predictor: the prototype of a global biomarker model to guide treatments with targeted therapy and predict progression-free survival [^114kzW9f]. NPJ Precision Oncology (2021). Medium credibility.

In conclusion, DDPP is unique in two levels:
It empirically decides how many features (genes) to include in the predictors rather than stopping when the fit is no longer improved.
It empirically decides which basic parameter-free summation method best predicts the outcome.

5 Combined gene expression (parameter-free feature summation): in a previous publication, we explain how we derive F i, g from two color arrays. Briefly Equation (1) describe how feature i is calculated for gene g. where F i, g is used as the measurement for gene g in individual i, T i, g, and N i, g are the measured intensity of gene g in individual i correspondingly. In order to get a single value from multiple features (genes), we combined for each individual i all genes values (F i, g 1, F i, g 2… F i, gN) using different approaches: (1) mean:; (2) sum:; (3) median; (4) fold: and (5) fold_abs:
6 To define the optimal " n " genes investigated we interrogated the correlation between gene expression and the PFS: for each drug, a Pearson correlation test was performed between F g, the fold change multiplied by the intensity (of tumor and normal) of a single gene g (gene from the list of key genes of the drug) with the PFS for all the patients treated with the drug. The F g with the most significant correlated gene (by p value and correlation coefficient r) was driven to decision whether to continue with fold change multiplied by the intensity of the tumor or fold change multiplied by the intensity of the normal matched tissue. The key genes were then ranked based on the Pearson p value and correlation coefficient r such that the gene with the highest correlation between F g and the PFS was ranked first. Then, we added single genes by the following manner: the second most ranked gene was added to the first most ranked and the F g 1, g 2. This resulted with vector space which contains two vectors. Similarly, the third most ranked gene was added to the second highest ranked genes, and resulted with vector space with three vectors. The vector summation continued in a similar manner until we obtain n different combinations of genes, while n is the number of genes in the set and each combination denotes a vector space with n vectors in it. In order to get a single value for each vector space, we tested the five different basic mathematical operations on the vectors in each vector space mentioned above. Then, a Pearson correlation test was performed between the calculated transcriptomic value for each vector space with the PFS of the patients treated with the drug. The Pearson correlation results were ranked again by p value and correlation coefficient r. The number of genes in the set which was the most correlated with the PFS was indicated as the optimal " n " coordinates. In order to assess the likelihood of getting a significant correlator by " n " genes, we run an analysis with 100 K random " n " genes and tested how the transcriptomic value derived from a vector space which includes F g 1,… gn of these genes were correlated with the PFS. Significant results were considered by a threshold of absolute R value of 0.9 or above, and p value of 0.05 and below. Applying any of the mathematical operations listed above resulted with a single value that combines the transcriptomic expression of all targetable genes for each patient. These values were used in order to generate the linear regression model with the PFS observed.
7 Selecting the best correlators with PFS for each drug and computing a linear regression model to transform the best correlator into a predictor for a single drug. Supplemental Table 3 shows the log2 fold change (intensity in tumor versus intensity in normal) multiplied by the intensity of tumor of selected eight genes for patients treated with everolimus. The expression values were used to calculate DDPP values based on fold_abs, which is absolute value of the multiplication of all the gene's values in each patient. The DDPP values are shown in Supplemental Table 4. Then, we used the DDPP values and the real PFS values as shown in Supplemental Table 5 for Pearson correlation with 95% confidence interval analysis and linear regression model analysis (which provided the linear equation). The results of these analyzes are shown in Supplemental Fig. 3.
8 Linear regression equations developed to link gene expression with clinical outcome under specific treatments: Everolimus — Equation 1: Y = 1.499e−13 X + 3.134, where X = the absolute value of the fold of log2(fold change tumor versus normal) multiplied by log1.1 (Intensity_Tumor) of each value for each of the eight genes (AKT2, TSC1, FKB-12, TSC2, RPTOR, RHEB, PIK3CA, and PIK3CB) and Y = PFS in months. Axitinib — Equation 2: Y = 2.014e−02 X + 4.36, where X = the sum of log2(fold change tumor versus normal) multiplied by log1.1 (Intensity_Normal) of each value for each of the two genes (KIT-KITLG), and Y = PFS in months. Trametinib — Equation 3: Y = −6.872e−15 X + 7.745, where X = the fold of log2(fold change tumor versus normal) multiplied by log1.1 (Intensity_Tumor) of each values for each of the nine genes (ERK2, ARAF, CRAF, MEK1, MEK2, HRAS, ERK1, MAPK10, and KSR1), and Y = PFS in months. Afatinib — Equation 4: Y = −4.558e−02 X + 2.549, where X = the sum of log2(fold change tumor versus normal) multiplied by log1.1 (Intensity_Tumor) of each value for each of the two genes (NRG4 and NRG2), and Y = PFS in months. FGFR inhibitors — Equation 5: Y = −5.273e−02 X + 5.135, where X = the sum of log2(fold change tumor versus normal) multiplied by log1.1 (Intensity_Normal) of each value for each of the five genes (FGF10, FGF16, FGF5, FGF2, and FGF13), and Y = PFS in months. Anti-PD-1 — Equation 6: Y = 7.856e−10 X – 1.583, where X = the value of the fold of log2(fold change tumor versus normal) multiplied by log1.1 (Intensity_Normal) of each value for each of the six genes (TLR-4, PDL-2, PDL-1, CD16 (NK), CTLA-4, and CD28), and Y = PFS in months.

---

### Protein sequence modelling with Bayesian flow networks [^112JfogJ]. Nature Communications (2025). High credibility.

Sampling

We explored a variety of alternative sampling methods that reduce the overall temperature of sample generation from ProtBFN. In the conventional discrete-data sampling method described in, restricting ourselves to the logit space, the discrete sample generation process moves from y (t) at time t to y (s) at time s according to the equation, where, is isotropic noise and α s = β (s) − β (t) is the change in accuracy. By the additivity of normally distributed variables and with y (0) = 0, the distribution of y (s) given preceding steps at times i = 0,… t is, or equivalently, where z ~ N (0, I) is a fixed isotropic noise. Sampling according to Equation (15) would yield an ODE similar to that described in. However, we observed that simply by replacing the summation of previous predictions with the most recent prediction, e.g.substantially reduced the perplexity of generated samples. Effectively, this method is equivalent to taking our most recent predictionand supposing that we had predicted it at every step 0… s. This method is further motivated under the assumption that the most recent prediction from the model is the 'best guess' for the centre of the input distribution; by reconstructing the input distribution at that point, we more closely match the flow distribution seen during sampling. We note the similarity of our method to the sampling method proposed in ref. where at each step, the input distribution is reconstructed from the most recent prediction. The main distinction between the two methods is that the sampler proposed in ref.resampled the underlying isotropic noise at each step (SDE-like), whereas our method uses a fixed isotropic noise throughout the process (ODE-like). Our overall sampling method is described in Box 2, which we refer to as Reconstructed ODE (R-ODE). As the choice of sampling method can dramatically change performance, we include an empirical comparison of various sampling methods in Supplementary Note 4.

Box 1 Continuous-time loss

1: Input: Φ, x

2: Output: L

3: t ~ U [0, 1]

4: β (t) ← β 1 t 2

5: α (t) ← 2 β 1 t

6:

7:

8:

9: return L ∞

---

### Standards of care in diabetes – 2025 [^112GkM3T]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for hypoglycemia, more specifically with respect to screening for cognitive impairment, ADA 2025 guidelines recommend to simplify diabetes treatment plans as much as possible for patients with cognitive impairment and tailor to minimize the risk of hypoglycemia.

---

### Clinical utility of polygenic risk scores for embryo selection: a points to consider statement of the American College of Medical Genetics and genomics (ACMG) [^1151TykV]. Genetics in Medicine (2024). High credibility.

PGT-P results communication — Figure 3 illustrates using 3 components (percentile rank, relative risk, and absolute risk): families (a) and (b) have embryos at rank extremes while family (c) shows a broader range; two families display different relative risk ranges, with family x values 0.9, 1.0, 1.1, 1.2 and family y values 0.6, 0.9, 1.1, 1.4; example absolute risk increases span 1.0% - 1.4% versus 10.0% - 14.0% for diseases differing by 10x population prevalence.

---

### Methane from microbial hydrogenolysis of sediment organic matter before the great oxidation event [^114cBA3y]. Nature Communications (2021). High credibility.

Reaction equation Eq. 8 is expanded to the scheme in Fig. 3a to quantify the five most abundant isotopologues in methane (three or more substitutions such as 13 CH 2 D 2 or 12 CHD 3 are ignored due to their low abundances). For the subscripts in Fig. 3a (m, i, and j in r amij or r bmij), the first digit (m = 0 or 1) is the number of 13 C atoms involved in the reaction, the second digit (i = 0, 1, or 2) is the number of deuterium atoms connected in the methylene or methyl group, and the third digit (j = 0 or 1) is the number of deuterium atoms in the hydrogen donor.

Clumped isotopic compositions of methylene and methane are defined as the following:

Note that the isotopic compositions here are expressed in decimals; they should be multiplied by 1000 to give per mil values.

The deuterium isotope ratio between the hydrogen donor (denoted with subscript B) and the methylene group (subscript A) is expressed as:

For each reaction step in Fig. 3a, the corresponding rate constants are denoted as k amij for step a or k bmij for step b. Kinetic fractionation factors α k amij = k amij / k a 000 and α ﻿kbmij = k bmij / k b 000 define KIEs. Note that a DKIE is often expressed as k H / k D, which is the reciprocal of the α k nomenclature here. A DKIE may be primary or secondary; a primary DKIE results in α k a 001 ≠ 1 and α k b 001 ≠ 1, and a secondary one results in α k a 010 ≠ 1 and α k b 010 ≠ 1. Kinetic clumped isotope fractionation factors γ amij = α k amij /(α k a 100 m α k a 010 i α k a 001 j) and γ bmij = α k bmij / (α k b 100 m α k b 010 i α k b 001 j) define the excessive KIE due to isotope clumping in steps a and b, respectively.

---

### Evolving copy number gains promote tumor expansion and bolster mutational diversification [^114XCX6G]. Nature Communications (2024). High credibility.

Multiple paths or ordering of gain events could lead to the same SCNA state. In Fig. 1 D, we graphically demonstrate the two possible histories of SCNA at 5:1. Our objective is to construct estimators of t 0 and t 3 using q and A. Denote by c the sum of the fraction of time multiplied by the number of copies in each stage. We haveBecause the probability of a mutation occurring in stage k is proportional to t k −1 (k + 1) (the fraction of the lifespan spent in stage k multiplied by the number of copies in stage k), we haveWe can write equation (4) in matrix form:When the history matrix A is invertible, we haveand thus the desired estimators can be obtained. Note that in Fig. 1 D, history A(a) induces an invertible history matrix, while the history matrix for history A(b) is singular. Therefore, the method introduced in ref.cannot be directly applied.

For single and double gains (N t: N b at 3:1, 2:0, 4:1 or 3:0), t 0 and t K are directly solved because matrix A is unique and invertible. For other SCNA states, Butte uses linear programming to obtain the upper bounds of timings across all possible history matrices for the corresponding CN configuration (Supplementary Fig. S28). Let s denote the vector of the column sum of matrix A. Let a denote the vector of possible allele states. Abusing the notation, we now interpret q as the vector with entry q j representing the probability of a randomly acquired SSNV having allele frequency for each possible allele state a j. Then the relation between A and t can be expressed asFrom equation (7), we havewhere 0 represents a vector with all 0's. Since t denotes the time fractions of different CN evolution stages, we havewhere 1 represents a vector with all 1's. Butte solves the following optimization problem by linear programming:where t K is the last element in vector t. The maximum value of t K gives us an upper bound of the lead time given A. For upper bounds of initiation times, we instead maximize t 0 which is the first element in t. To tolerate noise in the allele state distribution estimated from sequencing data, we add a slack variable on each capacity constraint, having a penalty cost of 100. The confidence intervals of the estimated upper bounds were calculated through bootstrapping the SSNV data.

---

### Echocardiography in the management of patients with left ventricular assist devices: recommendations from the American Society of Echocardiography [^117SsYSk]. Journal of the American Society of Echocardiography (2015). Medium credibility.

LVAD outflow graft Doppler calculation — direct Doppler of the distal outflow graft by transthoracic echocardiography (TTE) allows computation of left ventricular assist device (LVAD) output using "LVAD output = (outflow graft d/2)² × π × outflow graft TVI × HR". For irregular heart rate or variable stroke volumes, average 3–5 cycles and calculate average outflow graft TVI as TVI (throughout n cardiac cycles) / n; because of continuous flow the LVAD VTI includes systolic and diastolic areas, and in the example n = 2 (π = 3.14) with TVI1 = 21.2 cm, TVI2 = 23.6 cm, HR = 82 bpm, and d graft = 1.3 cm, yielding TVI avg = 22.4 cm, LVAD stroke volume = 29.7 ml, and LVAD cardiac output = 2,435 mL / min = 2.4 L/min; note that this type of imaging may be challenging to obtain in routine practice.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^114Jf4uH]. CDC (2011). Medium credibility.

Community viral load (VL) calculation — log transformation and mean: We recommend that calculation of Community VL and its related viral load Measures is performed after transformation of viral load results onto the logarithmic base 10 scale, followed by calculation of the mean, and we recommend calculation of geometric mean (GM) for viral load by averaging the log transformed values and transforming the average back to the original (linear) scale; using log base 10 is advantageous because a value of 2 on the log10 scale is 100 on the original scale, 3 corresponding to 1000, 4 corresponding 10000. Early VL work often used the median, but jurisdictions that have successfully engaged HIV-infected persons in care may have over 75% with undetectable VL; the median would be undetectable, so the mean was used, and the logarithmic transformation helps to normalize the distribution of viral load values and reduces the influence of outlying measurements for persons having extreme viremia.

---

### The influence of explicit local dynamics on range expansions driven by long-range dispersal [^11724niC]. G3 (2023). Medium credibility.

The evolution of initial neutral diversity in jump-driven range expansions was analyzed in, using a lattice model in which neutral variation was introduced in the starting population, and no new mutations appeared during the expansions. The existence of the time-doubling hierarchy, Eq. (1), was used to identify an effective population of homogeneous satellites whose evolution captures the balance between diversification and coarsening for a given jump kernel exponent. As with the behavior of the core radius growth, the amount of initial diversity preserved after a range expansion was shown to suffer different fates depending on the value of the kernel exponent relative to the spatial dimension. When, the diversifying influence of long jumps dominates; note the large number of satellites well separated from the core in Fig. 2g–i. The seeding of many satellites by long-range dispersal events from the core enables the population to preserve a finite amount of its initial heterozygosity at long times. By contrast, when, the local coarsening of diversity due to bottlenecks becomes more significant; note the small number of large monoclonal satellites in Fig. 2d–f. The heterozygosity decays inexorably toward zero as the range expansion progresses, albeit at a slow rate. As μ approaches d, the heterozygosity approaches a finite value but the convergence to this value becomes extremely slow and cannot be observed over practical simulation times. Notably, for, some diversity is also preserved at long times due to the formation of sectors in outward range expansions, as shown in Fig. 2a–c. Jump kernels of intermediate breadth therefore support lower neutral diversity than broader and narrower kernels.

---

### Estimating heritability of gene expression using parent-offspring regression with 2-channel microarrays [^111bAQ6o]. The Journal of Heredity (2008). Low credibility.

With the advent of microarrays, it is possible to look at the entire transcriptome of an organism as a suite of quantitative traits. An obvious question to now ask is: To what extent is gene expression heritable? In quantitative genetics, single parent-offspring regression is the most straightforward method in situations where the progenies are produced by cross-pollination to many male parents of unknown location. However, estimation of the heritability of gene expression with single parent-offspring regression has not yet been examined with 2-channel microarrays. Here we introduce 3 experimental designs: chain design, independent quartets design, and completely independent design. We then compare them with common reference design in respect to statistical power and bias of the estimates. In our simulations, we also incorporated a model of simple inheritance with one gene. The results of our simulations indicate the efficiency of the chain design over the alternative design considered.

---

### Revealing the range of equally likely estimates in the admixture model [^112kDtUM]. G3 (2025). Medium credibility.

Many ancestry inference tools, including structure and admixture, rely on the admixture model to infer both, allele frequencies p and individual admixture proportions q for a collection of individuals relative to a set of hypothetical ancestral populations. We show that under realistic conditions the likelihood in the admixture model is typically flat in some direction around a maximum likelihood estimate (q^, p^). In particular, the maximum likelihood estimator is non-unique and there is a complete spectrum of possible estimates. Common inference tools typically identify only a few points within this spectrum. We provide an algorithm which computes the set of equally likely (q~, p~), when starting from (q^, p^). It is analytic for K = 2 ancestral populations and numeric for K > 2. We apply our algorithm to data from the 1000 genomes project, and show that inter-European estimators of q can come with a large set of equally likely possibilities. In general, markers with large allele frequency differences between populations in combination with individuals with concentrated admixture proportions lead to small areas with a flat likelihood. Our findings imply that care must be taken when interpreting results from STRUCTURE and ADMIXTURE if populations are not separated well enough.

---

### Thermodynamics of quantum systems with multiple conserved quantities [^114kLtUs]. Nature Communications (2016). Medium credibility.

Recently, there has been much progress in understanding the thermodynamics of quantum systems, even for small individual systems. Most of this work has focused on the standard case where energy is the only conserved quantity. Here we consider a generalization of this work to deal with multiple conserved quantities. Each conserved quantity, which, importantly, need not commute with the rest, can be extracted and stored in its own battery. Unlike the standard case, in which the amount of extractable energy is constrained, here there is no limit on how much of any individual conserved quantity can be extracted. However, other conserved quantities must be supplied, and the second law constrains the combination of extractable quantities and the trade-offs between them. We present explicit protocols that allow us to perform arbitrarily good trade-offs and extract arbitrarily good combinations of conserved quantities from individual quantum systems.

---

### Minimum epistasis interpolation for sequence-function relationships [^114DxVSP]. Nature Communications (2020). High credibility.

Massively parallel phenotyping assays have provided unprecedented insight into how multiple mutations combine to determine biological function. While such assays can measure phenotypes for thousands to millions of genotypes in a single experiment, in practice these measurements are not exhaustive, so that there is a need for techniques to impute values for genotypes whose phenotypes have not been directly assayed. Here, we present an imputation method based on inferring the least epistatic possible sequence-function relationship compatible with the data. In particular, we infer the reconstruction where mutational effects change as little as possible across adjacent genetic backgrounds. The resulting models can capture complex higher-order genetic interactions near the data, but approach additivity where data is sparse or absent. We apply the method to high-throughput transcription factor binding assays and use it to explore a fitness landscape for protein G.

---

### The optimal MR acquisition strategy for exponential decay constants estimation [^116oGtC8]. Magnetic Resonance Imaging (2008). Low credibility.

Estimating the relaxation constant of an exponentially decaying signal from experimental MR data is fundamental in diffusion tensor imaging, fractional anisotropy mapping, measurements of transverse relaxation rates and contrast agent uptake. The precision of such measurements depends on the choice of acquisition parameters made at the design stage of the experiments. In this report, chi(2) fitting of multipoint data is used to demonstrate that the most efficient acquisition strategy is a two-point scheme. We also conjecture that the smallest coefficient of variation of the decay constant achievable in any N-point experiment is 3.6 times larger than that in the image intensity obtained by averaging N acquisitions with minimal exponential weighting.

---

### Evolving copy number gains promote tumor expansion and bolster mutational diversification [^112efJrc]. Nature Communications (2024). High credibility.

Mathematical modeling of late evolving gains

Consider two contrasting models based on multi-type branching processes with mutations. In both models, the tumor grows from a single tumor-initiating cell which just acquired GD. During the tumor's progression, cancer cells accumulate mutations (post-GD gains). In the base model (neutral model), all mutations are passenger mutations. Therefore, all cancer cells give birth at a rate of a 0 and die at a rate of b 0. The net growth rate is λ 0 = a 0 − b 0 > 0. Neutral mutations occur at rate u 0 per unit time throughout the lifetime of a cell, and each mutation is distinct following the infinite-sites model. Clonal post-GD gains are those acquired prior to the cell division (or the onset of expansion) leading to two surviving sublineages. Let this division event (denoted by an effective birth) occur at a rate of λ 0 (cf. page 10 of) conditioned on the non-extinction of the population. By the memoryless property of the exponential distribution, counting the number of post-GD gains prior to the first effective birth is analogous to counting the number of tails until the first head in a sequence of coin tosses, where the probability of a head (effective birth) isand the probability of a tail (neutral mutation) is. Therefore, the number of gains before the first effective birth follows a geometric distribution with parameterand mean. We then investigated the number of mutations which are shared by more than 90% of the total population (we refer to them as dominant mutations). Gunnarsson and his co-authorsderived exact expressions for the expected SFS of a cell population that evolves according to a branching process. We utilized their results on the skeleton subpopulation (see Appendix C of) — cells with an infinite line of descents which determines the high frequency spectrum — to express the expected number of dominant mutationswhen the tumor reaches a fixed size N as

---

### X-tile: a new bio-informatics tool for biomarker assessment and outcome-based cut-point optimization [^114Roo7b]. Clinical Cancer Research (2004). Low credibility.

The ability to parse tumors into subsets based on biomarker expression has many clinical applications; however, there is no global way to visualize the best cut-points for creating such divisions. We have developed a graphical method, the X-tile plot that illustrates the presence of substantial tumor subpopulations and shows the robustness of the relationship between a biomarker and outcome by construction of a two dimensional projection of every possible subpopulation. We validate X-tile plots by examining the expression of several established prognostic markers (human epidermal growth factor receptor-2, estrogen receptor, p53 expression, patient age, tumor size, and node number) in cohorts of breast cancer patients and show how X-tile plots of each marker predict population subsets rooted in the known biology of their expression.

---

### Evolving copy number gains promote tumor expansion and bolster mutational diversification [^116cg4N1]. Nature Communications (2024). High credibility.

Mathematical modeling in the context of multiple drivers

In our multi-type branching process model, we examine two driver mutations: mutation one and mutation two. We initiate with a single cell devoid of mutations, which possesses a birth rate of a 0 and a death rate of b 0. Mutation one, acquired at a rate of u 1, adds δ 1 to the birth rate (a 1 = a 0 + δ 1). Mutation two, acquired at a rate of u 2, adds δ 2 to the birth rate (a 2 = a 0 + δ 2). Cells with both mutations have a birth rate of a 3 = a 0 + δ 1 + δ 2. Death rate remain the same as b 0. We simulated tumor growth from a mutation-free cell until the first cell acquired both drivers without going extinct (100,000 for each parameter set). We calculated the fraction of cases where mutation one occurred first.

Reporting summary

Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.

---

### Application of transcriptional gene modules to analysis of' gene expression data [^116iBtJG]. G3 (2020). Medium credibility.

Quantification of module activity in gene expression data

To project a data vector, x, such as a set of gene-expression fold changes, onto a set of gene modules, we used the scalar projection method, in which a mixing vector, a, is calculated from the dot product of the data vector and the unit vectors comprising the module definitions, as shown in equation 2:The resulting mixing vector, a, provides an indication of the weight of each module definition vector in the projected data, x. Note that because x is a vector of gene fold changes, this analysis can be applied to data generated using any RNA profiling platform (e.g. RNAseq, microarray). Projection of a data matrix, X, which generates a mixing matrix, A, was carried out using the same procedure.

To calculate signed variance explained (SVE), we calculated the relative variance explained (VE) for each module from a as follows, where n is the total number of modules:We then multiplied these values, which are strictly positive, by -1 in each case where a i < 0 to obtain SVE.

Generation of the Enrichment matrix, E

First, ANN-based partitioning of the module definition matrix, S, was used to generate matrix S p. S p contains two gene sets per independent component (which we refer to as hemi-modules), for a total of 418. Using a matrix of known Boolean associations between genes and annotations, B, we calculated a hypergeometric probability for each annotation in each hemi-module. We used the frequency of genes bearing a particular annotation in the hemi-module, the frequency of such genes in the compendium, the number of genes in the hemi-module, and the number of genes not in the hemi-module as the q, m, k, and n input parameters, respectively, to the phyper function of the stats (v3.0.3) R package.

We used these p-values to populate a matrix, E, with a row for each hemi-module and a column for each annotation. For under-represented annotations, we entered the log(p-value) in the matrix, and for over-represented annotations we entered the –log(p-value).

---

### Accurate eye tracking from dense 3D surface reconstructions using single-shot deflectometry [^1179weDr]. Nature Communications (2025). High credibility.

Correspondence evaluation in classical phase measuring deflectometry (PMD)

In PMD, the phase values for correspondence evaluation are obtained by temporally shifting vertical and horizontal sinusoidal patterns on the display. Patterns with horizontal stripes (as shown in Fig. 2 e) encode the vertical display coordinate y D, while patterns with vertical stripes encode the horizontal display coordinate x D. The reflection of the pattern over the object's surface is imaged with the camera. The intensity in each camera pixel (x c, y c) can be expressed as

A (x c, y c) and B (x c, y c) are terms that contain information about the unknown reflectivity of the respective surface point, the bias illumination, and the unknown background illumination at this point. ϕ y (x c, y c) is the vertical phase value that is needed to establish correspondence. In classical PMD, ϕ y (x c, y c) is retrieved via temporal phase shifting, e.g. by sequentially changing Δ ϕ toand acquiring an image I k (k = 1, 2, 3, 4) for each of those phase shifts ("four-phaseshift-method") (see Fig. 2 e, f). Eventually, ϕ y (x c, y c) is calculated byFigure 2 g shows the retrieved vertical phase map ϕ y in the camera image, which uniquely encodes the corresponding vertical display coordinates (Fig. 2 h) after unwrapping. The horizontal phase map ϕ x and the horizontal display coordinates are retrieved in an analog fashion by phase-shifting vertical sinusoidal patterns. Eventually, the surface normal is calculated from the obtained correspondence between display pixels and camera pixels via ray tracing (see Fig. 2 a), where height-normal ambiguities are resolved by exploiting information from the second camera, as described in sec. "Common surface normal and shape reconstruction approach from stereo-deflectometry".

Multi-phase shift procedures like the discussed four-phaseshift method provide high-quality results but have one severe drawback for the purpose of this paper: Several sequentially captured camera images (8 in this example) are required to capture one single 3D model. During this sequence, the object is not allowed to move. As the human eye is in constant fast motion, this method is not feasible for our purpose in practice. For this reason, we utilize a procedure to facilitate high-quality deflectometry measurements in single-shot.

---

### Roflumilast (Zoryve) [^114frE3u]. FDA (2024). Medium credibility.

The dosage of roflumilast TOP for treatment of seborrheic dermatitis in both children (in patients ≥ 9 years) is 1 thin layer TOP daily (0.3% foam)

---

### Evolving copy number gains promote tumor expansion and bolster mutational diversification [^113fYBXG]. Nature Communications (2024). High credibility.

The observed SCNA of a genomic segment (with a configuration N t: N b different from 2: 1) is the result of a series of CN events. For an SCNA involving at least K gain events, the total time of somatic evolution can be divided into K + 1 stages. The segment begins with the 2: 1 setting in the first stage and keeps "climbing up" by duplicating one of its existing copies in each subsequent stage, respectively, until it arrives at the observed SCNA state in the last stage (Fig. 1 D). Accordingly, each stage is associated with certain time proportion (t k ≥ 0) and, the total time for the somatic evolution. SSNVs occurring at stage k on a segment copy that experiences duplication(s) in later stages will remain present on the duplicated copies with the allele state a ≥ 2/ N t. By contrast, SSNVs acquired at stage k on a copy without further duplications remain at the single allele state (a = 1/ N t). One can define a history matrix A with entry A j k representing the number of segment copies in stage k that produce the final allele state (i.e. frequency) a j = j / N t. It can be seen that the abundance of SSNVs at allele state a j depends on ∑ A j k t k. From the site frequency spectrum (SFS) of SSNVs in a region affected by SCNA, one can estimate the relative abundance of SSNVs at each allele state, and in turn, solve for each t k. There has been much effort to infer t 0, i.e. the timing of the first copy number event. These efforts focused on single gain (2: 0 and 3: 1) and at most double gains (3: 0 and 4: 1), where the history matrix A is invertible. By contrast, for other SCNA states, multiple possible trajectories can exist and the underlying linear system is underdetermined, i.e. there are more time stages (unknown variables) than the possible allele states (equations). We note that, however, regardless of the underlying history, multi-allele SSNVs (≥ 2/ N t) can only occur before the last stage (K) of CN evolution; once the genomic region arrives at the observed clonal SCNA state, all the copies (N t) would accumulate SSNVs at single allele state (1/ N t). Therefore, the longer the last stage of CN evolution (from the emergence of the SCNA to the onset of population expansion), the more overwhelmingly the single allele SSNVs dominate the SFS (Fig. 1 D). The monotonicity property enables the examination of the proportion of time of stage K.

---

### Robust estimates of overall immune-repertoire diversity from high-throughput measurements on samples [^111X3pAF]. Nature Communications (2016). Medium credibility.

Current methods tend to overestimate species richness when coverage is low, as small clones added to the estimate result in overfitting of the sample distribution — in the limit, as mentioned, leading to an estimate with infinite infinitesimal clones. Recon uses discrete clone sizes, which in the worst case ensures that estimates are bounded by the number of cells in the overall repertoire (clones cannot outnumber cells). Beyond that, Recon's use of both a noise threshold and the (corrected) Akaike information criterion provide tighter bounds, rejecting additional clones unless their expected contribution to the sample rises above sampling noise (by 3 standard deviations in our implementation) and outweighs the penalty of adding more parameters. The trade-off is that for each sample, there is a minimum clone size that Recon can detect: if ≤ 1, Recon's species-richness estimate will include clones represented by just a single cell in the overall repertoire, if there are any; if > 1, in principle there may be clones in the overall repertoire that are too small to detect. In this case, Recon can be used to calculate a strict upper bound, U, on species richness that includes clones that may be 'hiding' (Methods and Supplementary Methods). However, we note that even in this case, in practice, for a given sample, the smallest clones detected may still be the smallest clones there are (the case for our in silico repertoires; below).

Validation

We validated Recon on in silico repertoires that spanned nearly five orders of magnitude of overall diversity (300 to 10 million clones) and a wide range of clone-size distributions: from steep, that is, dominated by small clones, to flat exponentials; reciprocal–exponential distributions that derive from a generative model; and multiple bimodal distributions of small and large clones, 1,711 in all, with and without simulated experimental noise (Methods). These repertoires served as gold standards. We sampled a known number of cells from each, for coverage ranging from 0.01x to 10x, and used Recon to reconstruct overall repertoires from each sample. (Coverage is the number of cells in the sample divided by the number of clones in the overall repertoire.) We then compared the diversity of the reconstructed overall repertoire with the true overall diversity and sample diversity. We measured diversity by species richness, entropy, Simpson Index and BPI (Fig. 1b).

---

### The contributions of sex, genotype and age to transcriptional variance in drosophila melanogaster [^115MpYgj]. Nature Genetics (2001). Medium credibility.

Here we present a statistically rigorous approach to quantifying microarray expression data that allows the relative effects of multiple classes of treatment to be compared and incorporates analytical methods that are common to quantitative genetics. From the magnitude of gene effects and contributions of variance components, we find that gene expression in adult flies is affected most strongly by sex, less so by genotype and only weakly by age (for 1- and 6-wk flies); in addition, sex x genotype interactions may be present for as much as 10% of the Drosophila transcriptome. This interpretation is compromised to some extent by statistical issues relating to power and experimental design. Nevertheless, we show that changes in expression as small as 1.2-fold can be highly significant. Genotypic contributions to transcriptional variance may be of a similar magnitude to those relating to some quantitative phenotypes and should be considered when assessing the significance of experimental treatments.

---

### Linear mapping approximation of gene regulatory networks with stochastic dynamics [^11161Gc5]. Nature Communications (2018). Medium credibility.

Fig. 6
LMA for the feedback loop with explicit modeling of both mRNA transcription and protein translation. a Illustrates the nonlinear feedback loop. b Shows the LMA and SSA predictions for steady-state mRNA distribution (See Supplementary Note 7), showing that the LMA is able to capture the change in modality induced by a change in ρ u. The rest of the parameters are: ρ b = 25, σ b = 0.004, σ u = 0.25, ρ = 3, γ = 2 and d = 1. c Compares LMA and SSA predictions for the first to fourth moments (about zero) of both mRNA and protein numbers over a wide range of the mRNA-protein degradation ratio γ / d. The result indicates that the accuracy of the LMA is independent of time-scale separation assumptions. The parameters used are: ρ u = 5, ρ b = 0, ρ = 100, σ u = 1, σ b = 1.5 × 10 −5. The nine pairs of (γ, d) are: (0.02, 2), (0.02, 0.6), (0.05, 0.5), (0.05, 0.15), (0.2, 0.2), (0.15, 0.05), (0.2, 0.02), (0.15, 0.005) and (0.2, 0.002), constituting a logarithmic span from −2 to 2. The ratio γ / d varies over a range consistent with mammalian gene expression

---

### Clinical practice guidelines: pediatric amplification [^116UUDVt]. AAA (2018). Medium credibility.

Pediatric amplification — signal processing: Independent prescriptive formulas provide a starting point for target gains at multiple input levels.

---

### Optimality guarantees for crystal structure prediction [^1153oHaw]. Nature (2023). Excellent credibility.

Crystalline materials enable essential technologies, and their properties are determined by their structures. Crystal structure prediction can thus play a central part in the design of new functional materials 1,2. Researchers have developed efficient heuristics to identify structural minima on the potential energy surface 3–5. Although these methods can often access all configurations in principle, there is no guarantee that the lowest energy structure has been found. Here we show that the structure of a crystalline material can be predicted with energy guarantees by an algorithm that finds all the unknown atomic positions within a unit cell by combining combinatorial and continuous optimization. We encode the combinatorial task of finding the lowest energy periodic allocation of all atoms on a lattice as a mathematical optimization problem of integer programming 6,7, enabling guaranteed identification of the global optimum using well-developed algorithms. A single subsequent local minimization of the resulting atom allocations then reaches the correct structures of key inorganic materials directly, proving their energetic optimality under clear assumptions. This formulation of crystal structure prediction establishes a connection to the theory of algorithms and provides the absolute energetic status of observed or predicted materials. It provides the ground truth for heuristic or data-driven structure prediction methods and is uniquely suitable for quantum annealers 8–10, opening a path to overcome the combinatorial explosion of atomic configurations.

---

### Colorectal cancer screening and prevention [^112Ta6ws]. American Family Physician (2025). High credibility.

Regarding screening and diagnosis for colon cancer, more specifically with respect to indications for screening, high-risk individuals, family history, AAFP 2025 guidelines recommend to obtain CRC screening in patients with ≥ 1 first-degree relatives with CRC or adenomatous polyps, starting at 40 years of age or 10 years before the age of the youngest relative at the time of their diagnosis.

---

### Evolving copy number gains promote tumor expansion and bolster mutational diversification [^114ogAuy]. Nature Communications (2024). High credibility.

We first analyze the base model. Conditioned on the non-extinction of the population, we can obtain that the number of post-GD gains reaching fixation follows a geometric distribution with parameterand mean(see Methods). The mode of this distribution is at zero, similar to the cases where GD appears late and post-GD CN gains are rare. To tolerate the inclusion of subclonal but dominant SCNAs as the clonal variants, we further evaluated the dominant post-GD gains shared by the majority (≥ 90%) of cancer cells. Building on the results of, we derived the expected number of dominant post-GD gains in a tumor with size N aswhich is only slightly larger than the clonal ones. Assuming that u 0 and λ 0 are comparable (based on experimentally measured u 0 for SCNA around 0.2 and the cancer cell death rate not significantly approaching the birth rate,), would be no more than just a few. Moreover, numerical simulations show that the number of dominant post-GD gains continues to follow a geometrical-like distribution with the mode at zero (Supplementary Fig. S18). Thus, if post-GD gains do not provide growth benefits, GD would be one of the last events before the MRCA as few of post-GD gains can become dominant in the observed tumor.