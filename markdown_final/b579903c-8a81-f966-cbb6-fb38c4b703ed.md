# Medical Question & Answer

**Sample ID**: b579903c-8a81-f966-cbb6-fb38c4b703ed
**Dataset Index**: 347614

---

## Question

A program is expressed in a programming language. Which of the following is true of the program?

Answer Choices:
A. The program can also be expressed as binary code, but will be more easily understood by humans when expressed in a higher-level programming language.
B. The program can also be expressed as binary code, which will reduce the likelihood of errors.
C. The program cannot be expressed as binary code, because binary code can only be used to represent data.
D. Some parts of the program can be expressed as binary code, but operations must be expressed using a higher-level programming language.

---

## Answer

> Let's see… What do we have here? The user is asking which statement correctly describes the relationship between a program written in a programming language and its representation in binary code. Let's break this down step-by-step. First, I need to think about whether programs can be represented as binary code. Then, I should verify whether binary representations reduce errors or increase them. Next, I will consider whether binary is only for data or also for instructions. After that, I need to check whether operations must remain in a high-level language or can be expressed in binary. Finally, I will weigh the human readability trade-off and select the best option with supporting evidence.

> Let me first confirm the foundational point: programs written in high-level languages are routinely translated into binary machine code that the processor executes, so the claim that programs cannot be expressed as binary is incorrect. The Nature Communications paper on computational modeling explicitly shows a cascade from high-level task descriptions down to a low-level machine interface formalism, with intermediate binary representations of inputs and operations, which confirms that programs can indeed be expressed as binary code [^c4dc10b3].

> Wait, let me verify the error claim in option B. Binary code is harder for humans to read and debug than high-level source code, so expressing a program as binary does not reduce the likelihood of errors; in fact, it tends to obscure logic and make mistakes harder to spot, which is why we prefer higher-level languages for development and maintenance. This is reinforced by the broader principle that reproducible computational science requires accessible source code, underscoring that binary executables alone are insufficient for verification and debugging [^0f6e8786].

> I need to check whether binary code is only for data. That would be misleading; binary encodings are used both for data and for instructions. The same Nature Communications work shows binary representations of both operands and operations in a computational cascade, and in genomics, binary formats like BAM are standard for storing aligned sequence data, illustrating that binary is a general encoding medium rather than exclusively a data format [^c4dc10b3] [^f5f8973b].

> Hold on, let's not jump to conclusions about operations. Operations are not constrained to remain in a high-level language; they are compiled or assembled into binary instructions that the hardware executes. The hierarchical modeling example demonstrates functions at an intermediate level being processed stepwise through binary representations, which directly contradicts the idea that operations must stay in a high-level language [^c4dc10b3].

> Let me consider human readability. High-level programming languages are designed to be more intelligible to humans than raw binary, which is precisely why we write, review, and debug in languages like Python, C, or Julia before compilation or interpretation. This aligns with the reproducibility argument that source code should be available for scrutiny and with modern bioinformatics practices that emphasize accessible, maintainable codebases and toolkits [^0f6e8786] [^974500a4].

> Putting this together, option A is the only statement that correctly asserts that a program can be expressed as binary code and that it will be more easily understood by humans when expressed in a higher-level programming language, which matches both the computational hierarchy evidence and the software engineering principle favoring source code accessibility and readability [^c4dc10b3] [^0f6e8786].

> Therefore, the answer is A.

---

The correct answer is **A**. Programs written in a programming language can be translated into binary code (machine code) for execution, but binary is far less readable and harder to understand than higher-level languages, which are designed for human clarity and maintainability [^c4dc10b3]. Options B, C, and D are incorrect: binary does not reduce errors (it obscures logic), programs can be expressed in binary, and operations are routinely encoded in binary machine instructions, not restricted to high-level languages [^c4dc10b3].

---

## Relationship between programming languages and binary code

Programming languages serve as **intermediaries** between human-readable instructions and machine-executable binary code. High-level languages (e.g. Python, Java, C++) provide abstractions that simplify complex tasks, whereas binary code consists of 0s and 1s that the CPU executes directly. The process of converting high-level code to binary is called compilation or interpretation, depending on the language and execution model [^c4dc10b3].

---

## Expressing programs in binary code

Programs written in high-level languages can indeed be expressed as binary code. This is achieved through **compilation**, where a compiler translates the high-level source code into machine code (binary) that the computer's hardware can execute. For example, a simple Python program can be compiled into bytecode (an intermediate binary representation) and then interpreted or further compiled into native machine code. Similarly, C and C++ programs are directly compiled into binary executables [^1d725631].

---

## Human readability and understanding

While programs can be expressed in binary code, binary is **not designed for human readability**. Binary code consists of sequences of 0s and 1s that are difficult to interpret without specialized knowledge and tools. In contrast, high-level programming languages are designed to be human-readable, using meaningful variable names, control structures, and syntax that closely resemble natural language or mathematical notation. This readability facilitates understanding, debugging, and maintenance of software [^0f6e8786].

---

## Error likelihood in binary code

Binary code does not inherently reduce the likelihood of errors. In fact, working directly with binary code increases the risk of errors due to its complexity and lack of abstraction. High-level programming languages provide features such as type checking, error handling, and debugging tools that help developers identify and fix errors more efficiently. Therefore, the claim that binary code reduces errors is **incorrect** [^818dbec6].

---

## Limitations of binary code

Binary code is primarily used to represent data and instructions that the computer can execute. It is not designed to represent high-level abstractions or complex logic in a human-readable format. While binary code can represent any program, it lacks the expressive power and readability of high-level languages. This limitation is why high-level languages are preferred for software development, with binary code reserved for low-level system programming or performance-critical applications [^c4dc10b3].

---

## Conclusion

The correct answer is **A**. Programs expressed in programming languages can also be expressed as binary code, but they are more easily understood by humans when expressed in higher-level programming languages. Binary code is essential for machine execution but lacks the readability and abstraction necessary for effective human comprehension and software development [^c4dc10b3].

---

## References

### Toward a formal theory for computing machines made out of whatever physics offers [^c4dc10b3]. Nature Communications (2023). High credibility.

We begin with the algorithmic dissection of the multiplication task. Digital computations are modelled (or, equivalently, programmed) by breaking down a user's task description through a cascade of increasingly finer-granular formalisms down to a low-level machine interface formalism — think of compiling a program written in the task-level machine learning toolbox TensorFlowdown to machine-specific assembler code, passing through a series of programs written in languages of intermediate abstraction like Python or C/C++. In our arithmetics example we condense this to three modeling levels (Fig. 1 a–c). On the top level L (3), the user specifies task instances by typing the input string u (3) = 6 * 5, or 17 * 4 etc. In our graphics the structure of this input is rendered by the cells in the vertical state bars. The user knows that there is a mathematical function f (3) (the multiplication of integers) that is evaluated by the calculator, but the user is not further concerned about how, exactly, this computation is effected. This is taken care of by the designer of the calculator, who must hierarchically disassemble the input 6 * 5 down to a machine-interface level L (1).

On an intermediate modeling level L (2) one might encode the original input u (3) = 6 * 5 in a binary representation (for instance 6 → 1 1 0, 5 → 1 0 1, * → binarymult (leftmost green cell bars u (2) in b)). This new input encoding is then processed stepwise with functions f (2) (green arrows), possibly in parallel threads, through sequences of intermediate binary representations, until some binary string representation y (2) of the result is obtained, which then can be decoded into the top-level representation y (3) = 3 0. One of these functions f (2) could for instance be an addition operation, like binaryadd(1 1 0, 1 0 0 = 1 0 1 0 (inner grey rectangle)).

---

### OCMA: fast, memory-efficient factorization of prohibitively large relationship matrices [^1d725631]. G3 (2019). Medium credibility.

Software Availability

The source code and executable are freely available at GitHub: The repository provides OCMA for Windows (ocma-0.1.zip) and Linux/Unix systems (ocma-0.1.tar.gz). Upon decompression, one will see two folders: src and bin. The folder src contains all the source code, and the folder bin contains the compiled binary executable. Under the folder bin, two small matrices, SM10 and M3_4 for testing purpose are also provided.

The users can also build their own binary executable. The program is written in the C language, and it uses Intel MKL. Thus, in order to compile the source code, one needs to install C compiler, e.g. the Intel C/C++ Compilers, the GNU C Compiler or Visual Studio C Compiler. Additionally, one needs to install Intel MKL. When C compiler and Intel MKL are available in the system, one can enter the src folder and modify the makefile based on the C compiler and the Intel MKL paths; then type make or nmake to compile. After that, the ocma executable will be generated in the bin folder.

---

### Isqg: a binary framework forQuantitative genetics [^562302c9]. G3 (2019). Medium credibility.

The dna is the fundamental basis of genetic information, just as bits are for computers. Whenever computers are used to represent genetic data, the computational encoding must be efficient to allow the representation of processes driving the inheritance and variability. This is especially important across simulations in view of the increasing complexity and dimensions brought by genomics. This paper introduces a new binary representation of genetic information. Algorithms as bitwise operations that mimic the inheritance of a wide range of polymorphisms are also presented. Different kinds and mixtures of polymorphisms are discussed and exemplified. Proposed algorithms and data structures were implemented in C++ programming language and is available to end users in the R package "isqg" which is available at the R repository (cran). Supplementary data are available online.

---

### Plasma cell differentiation is controlled by multiple cell division-coupled epigenetic programs [^06210bf6]. Nature Communications (2018). Medium credibility.

The kinetic analysis of gene expression and accessibility described herein revealed a subset of primed genes that contained accessible promoters but were not transcribed in undivided B cells. In memory CD8 T cells, genes with primed promoters encoded important effector functions that would be required during reactivation. In naive B cells, primed promoters occurred at genes with cell cycle and metabolic functions but also the essential plasma cell transcription factor Prdm1. Therefore, similar to CD8 T cells, B cell genes that function in the next developmental phase (i.e. activation and differentiation) displayed a pre-existing chromatin accessibility programming that may reduce the number of molecular steps necessary for or allow for a more rapid induction. A subset of primed promoters was marked by the repressive post-translational histone modification H3K27me3 and inhibition of the enzyme responsible for deposition, EZH2, resulted in enhanced expression of primed genes. H3K27me3 has been shown to mark essential developmental gene promoters and has been suggested to allow precise temporal and synchronized control of transcription. EZH2 is highly upregulated in proliferating GC B cells – and in response to LPS in vivoand ex vivo. These data suggest that in proliferating B cells, as in the GC, EZH2 functions to repress differentiation. Moreover, inhibition of EZH2 catalytic activity resulted in early induction of Prdm1 expression ex vivo. Reducing the molecular steps necessary for induction of genes such as Prdm1 is likely an important mechanism to allow rapid differentiation and formation of plasma cells. As methods improve to analyze the histone code on small numbers of primary cells, it will be revealing to track changes in post-translational histone modifications by cell division, including H3K27me3, to precisely determine when these marks change during differentiation.

Together, these chromatin accessibility data provide a readout of the trans -acting factors and epigenetic mechanisms and define the chromatin centric reprogramming steps required for B cells to differentiation into plasma cells. The initiation of the plasma cell gene expression program involves an elaborate interaction of transcription factors and the opening of thousands of enhancers. Histone modifications add an important layer of complexity and provide potential temporal control mechanisms for gene expression kinetics. Defining the epigenetic landscape in a cell-division linked manner for other stimuli will provide insights into the common and unique programming of B cell fates and the control of humoral immunity.

---

### The case for open computer programs [^0f6e8786]. Nature (2012). Excellent credibility.

Scientific communication relies on evidence that cannot be entirely included in publications, but the rise of computational science has added a new layer of inaccessibility. Although it is now accepted that data should be made available on request, the current regulations regarding the availability of software are inconsistent. We argue that, with some exceptions, anything less than the release of source programs is intolerable for results that depend on computation. The vagaries of hardware, software and natural language will always ensure that exact reproducibility remains uncertain, but withholding code increases the chances that efforts to reproduce results will fail.

---

### Combinatorial code governing cellular responses to complex stimuli [^90daabf0]. Nature Communications (2015). Medium credibility.

In order to classify experimental gene expression profiles into one of the 82 theoretical profiles (and corresponding mode), we developed an original computational workflow consisting of the following steps (Fig. 3a): (1) data pre-processing, (2) selection of differentially expressed genes (one-way analysis of variance (ANOVA)), (3) identification of non-additive genes (two-way ANOVA model) and (4) classification of non-additive genes using vector-based approach or statistical learning. A false-discovery rate (FDR) of 0.05 was applied at both steps (2) and (3) for most subsequent analyses (see Methods section). The same framework can be used with a higher or lower FDR in order to further reduce false-negative or false-positive results, respectively.

---

### Apomorphine hydrochloride (Onapgo) [^517f1092]. FDA (2025). Medium credibility.

When programming the pump, a flashing value is one that you may edit. The table below describes the default pump settings and range of values to which the pump can be programmed. See the Prescribing Information for the recommended dosage regimen, including the recommended flow rate (i.e., 1 mg/hr to 6 mg/hr), infusion time (i.e., generally over 16 waking hours), and extra doses (i.e., no more than 3 per day).

Important Information: Do not disclose the pump's security codes or other information to the patient. It would allow them access to the programming functions.

If this is the first time the pump will be programmed, the existing values will reflect the default factory settings (Page 8).

For any subsequent programming, the values will reflect the previously programmed patient prescription.

The settings will need to be changed to match the time, date, and patient's prescription.

Symbols Used on the ONAPGO Pump

Note: The symbols displayed on the product label are reduced and simplified compared to the specifications in the reference standard CENELEC EN50419.

Limited Warranty

The pump is covered by a manufacturer's warranty for two (2) years. After this period, the manufacturer shall not be responsible for any repairs.

The manufacturer shall not be liable for damage to persons or property after a pump lifetime of two (2) years.

Contact & Support

For help and additional information on the pump or system, contact Customer Support at 1-833-3ONAPGO (1-833-366-2746).

Manufactured for:

MDD US Operations, LLC, a

subsidiary of Supernus Pharmaceuticals, Inc.

Rockville, MD 20850

MDD US Operations, LLC is the exclusive licensee and distributor of ONAPGO in the United States and Its territories.

©2025. ONAPGO is a registered trademark of BRITUSWIP LTD.

Version 8

This Instructions for Use has been approved by the U.S. Food and Drug Administration.

Revised: 06/2025

---

### Programming in the small [^e2ef3162]. Journal of Digital Imaging (2011). Low credibility.

Academic medical centers, in general, and radiation oncology research, in particular, rely heavily on custom software tools and applications. The code development is typically the responsibility of a single individual or at most a small team. Often these individuals are not professional programmers but physicists, students, and physicians. While they possess domain expertise and algorithm knowledge, they often are not fully aware of general "safe coding" practices — nor do they need the full complexity familiar in large commercial software projects to succeed. Rather, some simple guidelines we refer to as "programming in the small" can be used.

---

### Single cell multi-omics reveal intra-cell-line heterogeneity across human cancer cell lines [^49528ec3]. Nature Communications (2023). High credibility.

Additionally, we compared program clusters identified in our study with a recent study, where 198 cancer cell lines were analyzed by scRNA-seq, and 12 programs were determined. According to the similarity of signature genes, six out of twelve program clusters were identified in our study, including IFN response, stress response, EMT I, EMT II, G1/S and G2/M (Supplementary Table 3). This suggests that even with largely diverged transcriptome features, the heterogeneous transcriptional programs were shared among different cancer cell lines of different tissue origins. Moreover, due to the different spectrum of cancer cell lines that were profiled in two studies, some recurrent expression program clusters were only observed in our analysis, such as estrogen response (Program 6), which was enriched in breast cancer cell lines (Supplementary Fig. 2d).

Pan-cancer scATAC-seq of human cell lines

Although single-cell transcriptomics allows the detection of cellular diversity, it remains a challenge to interpret the epigenomic regulatory code based solely on the transcriptome data. By comparison, scATAC-seq provides a more direct characterization of the genome-wide activity of enhancers and promoters in heterogeneous cell populations. Therefore, we further used scATAC-seq to investigate the epigenomic heterogeneity within individual cell lines at single-cell resolution. To increase the throughput and reduce the cost of scATAC-seq experiments, at most six cell lines with distinct expression profiles were pooled for each scATAC-seq run and then computationally assigned to the corresponding cell line according to the accessibility features of the differentially expressed genes (Fig. 3a and Supplementary Fig. 3a).

---

### A geometric approach to characterize the functional identity of single cells [^29ff3e5f]. Nature Communications (2018). Medium credibility.

Finally, we construct matrixwhere each column is. Terms in this matrix are called residual expressions and help identify distinguishing marker genes for each archetype.

Those genes with high residual expression in each archetype are controlled through regulatory networks within the cell. To uncover these relationships, we identify TFs that are significantly associated with the expression of marker genes, which we will refer to as functionally active TFs. Functional activity of TFs is inferred directly from the expression of their TGs; thus, these TF activities can be controlled at different stages, ranging from transcriptional to post-translation regulations. To infer these activities, we first need to classify their TGs as either active or inactive in a given context (archetype). We partition genes according to their residual expression and declare top-ranked genes as active. We use the minimum hypergeometric (mHG) methodto find the optimal partition of genes and assign a p -value to it. The main step of this algorithm is similar to classic enrichment analysis: for a fixed size l, we use the hypergeometric p -value to assess the over-representation of TGs for a given TF among top- l markers for an archetype. Then, we compute the same statistic for all 1 ≤ l ≤ m, where m is the total number of genes. The mHG tail that is obtained, referred to as the mHG score, specifies the best cut, l (best), and all TGs that are ranked higher than l (best) among marker genes are selected as regulated targets for that TF. Finally, we use the obtained mHG score to assess the significance of the TF itself. This can be accomplished using a dynamic programming algorithm that assesses the probability of observing the same or more significant mHG score within the population of all binary vectors of size m with exactly r nonzeros, where r is the number of targets for the current TF. The set of all significant TFs, together with their TGs that fall above the cut that results in the mHG score, are used to construct the final TRN.

Code availability

All codes are available in C/C++ with R/Matlab interfaces from.

Data availability

The data that support the findings of this study are available from the corresponding author upon reasonable request.

---

### Standards and guidelines for validating next-generation sequencing bioinformatics pipelines: a joint recommendation of the Association for Molecular Pathology and the college of American pathologists [^bd876188]. The Journal of Molecular Diagnostics (2018). Medium credibility.

Next-generation sequencing (NGS) bioinformatics pipeline — scope and abbreviations: The figure illustrates a bioinformatics pipeline and its components that are typically used for processing NGS data, and states that the components of the pipeline that overlap with the gray shaded region are out of scope of this guideline; abbreviations defined include BAM (binary alignment map), HGVS (Human Genome Variation Society), indel (insertion/deletion), QC (quality control), SNV (single-nucleotide variant), and VCF (variant call format).

---

### Combined language and code emergent literacy intervention for at-risk preschool children: a systematic meta-analytic review [^910dacec]. Child Development (2025). Medium credibility.

1.4 Potential Benefits and Risks of Combined Intervention

Translating these theories and findings to emergent literacy intervention suggests that simultaneously targeting oral language and code subskills could be reciprocally beneficial. Lonigan et al. referred to these potential benefits as synergistic intervention effects. However, the few studies that have examined synergistic intervention effects between code and language skills in at‐risk children have found mixed results. Lonigan et al. and Gettinger and Stoiber explored the possibility of synergistic intervention effects by comparing combined emergent literacy intervention to domain‐specific code and language intervention conditions. They found that intervention effects were only observed for the code and language subskills targeted and that the combined intervention produced similar vocabulary and phonological awareness subskill effects to the interventions with a domain‐specific focus (Gettinger and Stoiber; Lonigan et al.). Aram found enhanced vocabulary effects when vocabulary intervention was combined with an alphabetic skills program. Interestingly, Gettinger and Stoiber and Aram found that alphabet knowledge performed better in domain‐specific intervention than in the combined condition. It is worth noting, however, that cumulative intervention time between the combined and domain‐specific interventions was held constant across all three studies. This meant that the combined interventions had at least half the language and code dosage of the domain‐specific interventions but managed to produce similar intervention effects for vocabulary and phonological awareness outcomes. This raises two important considerations. Firstly, code and language subskills require explicit focus to promote skill acquisition and are thereby both needed in a preventative emergent literacy intervention. Secondly, combined interventions will inherently reduce the dosage time for each subskill. As dosage is a critical moderator of intervention effectiveness (Frizelle et al.), the question remains: can potential synergistic benefits of multi‐target combined interventions overcome the reduction in specific subskill dosage?

Our systematic meta‐analytic review seeks to collate the outcomes of high‐quality combined code and oral language intervention studies to answer the following questions:
What are the characteristics of combined code and oral language emergent literacy interventions for preschool children at risk of literacy difficulties?
Are combined code and oral language preschool interventions effective for preschool children at risk of literacy difficulties?
Is there evidence of a synergistic relationship between code and language outcomes from a combined intervention?

---

### Combinatorial code governing cellular responses to complex stimuli [^f2481e0e]. Nature Communications (2015). Medium credibility.

Methods

Identification of the interaction profiles

The number of ways to rank the numbers e 0, e X, e Y and e X+Y was computed using the recurrence relation A n k = A n− 1 k +k A n− 1 k− 1 which, for n = 4 and summing up for k = 1, 2, 3 and 4 gives 75 possible rankings. Each ranking can be seen as a set of outcomes of the following qualitative pairwise comparisons:
e X versus e 0
e Y versus e 0
e X+Y versus e 0
e Y versus e X
e X+Y versus e X
e X+Y versus e Y

In our framework, the outcome of a qualitative comparison can take on the following three values: 0 (equal numbers), 1 (first larger than the second) and −1 (first smaller than the second). Each of 75 rankings was coded uniquely as a vector of six components describing the qualitative outcome of the comparisons 1–6 as listed above. For example, the vector (0,0,1,0,1,1) corresponds to:

e X = e 0, e Y = e, e X+Y > e 0, e Y = e X, e X+Y > e X, e X+Y > e Y

To identify which rankings were compatible with positive or negative interactions, we considered the equations 1–6 together with the inequalities that define positive and negative interactions:
Δ e X+Y > Δ e X + Δ e Y (positive interaction)
Δ e X+Y < Δ e X + Δ e Y (negative interaction)

which can be written as

(7a) e X+Y − e X − e Y + e 0 > 0

(8a) e X+Y − e X − e Y + e 0 < 0

If a ranking is consistent with a positive (or negative) interaction, the inequality constraints encoded in the corresponding vector can be solved simultaneously with 7a (or 8a). To verify this, we developed a constraint satisfaction that attempts to solve the six constraints of each ranking together with the inequality 7a (or 7b). The solution is searched numerically with a MATLAB linear solver. The variables e 0, e X, e Y and e X+Y were constrained to the interval 2–16, taken as an approximate of the range of expression values from Affymetrix chips in log2 scale. The method is implemented in the MATLAB code and is available upon request.

---

### Analysis of dynamic texture and spatial spectral descriptors of dynamic contrast-enhanced brain magnetic resonance images for studying small vessel disease [^bdea9a43]. Magnetic Resonance Imaging (2020). Medium credibility.

2.3.3 Local binary patterns descriptors

Local binary patterns (LBP) are another visual descriptor that quantify local signal variations. The original approximation characterises the way voxels relate to their neighbourhood using binary codes and summarises them for the entire region of interest using a histogram. The process is three-fold. First, the binary code for each voxel is computed by analysing the way its intensity relates to the ones of its neighbours: if the value is higher than the one of its neighbour, we assign a zero; and a one otherwise. For instance, if the voxel value is equal to 4 and the neighbouring intensities are 1 − 2 − 3 − 4 − 5 − 6 − 7 − 8, then the corresponding code would be 1 − 1 − 1 − 0 − 0 − 0 − 0 − 0. Second, the frequency of each one of these codes is totalised. Third, the resulting histogram is divided by its sum to express probability. The normalised histogram is used as a texture descriptor.

In this work, we considered two variants of the original approximation called uniform local binary patterns (ULBP)and local configuration patterns (LCP). The former maps the original set of codes (2 8 = 256 different binary patterns) to a subset of 59 codes to reduce the cardinality of the histogram and provide the descriptor with a simple rotation invariance. The latter combines the LBP with rotation invariance and another descriptor which quantifies local linear dependencies between a voxel and its neighbours. We calculated these descriptors for each region of interest for each patient using a radius equal to 1. For each time point and each region of interest, the number of ULBP and LCP features per patient were 59 and 81, in that order.

---

### Speeding up eQTL scans in the BXD population using GPUs [^c45cba51]. G3 (2021). Medium credibility.

Benefit of using Julia

In our explorations of matrix multiplication, Julia's speed is comparable to C/C++ (results not shown). However, the low learning curve, clean syntax, as well as support for GPU programming libraries such as CUDAnative reduce programming effort relative to C/C++. Compared with writing GPU functions in C/C++, writing in Julia is cleaner and easier because it requires much less boilerplate code. Below are some example code snippets. The first example shows how to call cuBLAS from Julia, and the second example shows how to write a custom kernel in Julia. To respect page limits, we will not show the corresponding C code. An example of using cuBLAS with C can be found online.

Comparison with tensorQTL

Table 2 shows the comparison of runtimes broken down by time taken for data transfer, core computation, and post processing. The main finding is that the data transfer and core computation take about the same time for tensorQTL and LiteQTL for both CPU and GPU. Both program timings indicate a speedup factor of 20 times for the core computation of the full matrix. For the filtered version, the GPU was about 56 times faster than CPU only for tensorQTL and about 18 times faster for LiteQTL.

Table 2
Timing comparison between tensorQTL and LiteQTL: Times are averaged over 10 runs and expressed in seconds

Depending on user-defined options, the output may need further processing after the core computation. This post processing is responsible for the main differences in elapsed time. Because it depends on user options, both programs exclude that time in calculating the GPU speedup. In our tests, the elapsed time for tensorQTL was longer than for LiteQTL.

---

### SimpleITK image-analysis notebooks: a collaborative environment for education and reproducible research [^974500a4]. Journal of Digital Imaging (2018). Low credibility.

Overview

SimpleITK, originally introduced in, is a simplified programming interface to the algorithms and data structures of the Insight Segmentation and Registration Toolkit. SimpleITK has specializations for multiple programming languages including C+ +, Python, R, Java, C#, Lua, Ruby, and TCL. The SimpleITK interfaces address the primary challenge associated with using ITK algorithms by directly allowing scientific domain experts to use the most common forms of these algorithms in any one of the programming languages that they are familiar with. SimpleITK enables users to use the skills they have rather than requiring a significant amount of effort to become proficient in C+ +.

Three characteristics of ITK that make it a phenomenal platform for efficiently implementing complex medical image analysis tools also require advanced software development experience. The first characteristic is that the toolkit is distributed in source code form, requiring the developer to compile ITK before they can use it. The second characteristic derives from ITK's extensive use of the C+ +template mechanism to provide data-type flexibility. Finally, ITK components are designed for use as part of a data analysis pipeline. ITK elements are connected sequentially to construct the pipeline with no computations performed during pipeline construction. This powerful pipeline architecture adds complexity that is a significant barrier to implementation for many ITK newcomers. Each of these power-enabling characteristics requires specialized understanding of advanced programming concepts.

SimpleITK shortens the time it takes to access the ITK toolkit algorithms by trading-off potential flexibility and performance for global ease of use. There are binary distributions for a variety of programming languages. It abandons the use of templates in favor of a small subset of a specific pixel, image, and filter types which correspond to the most widely used data-type combinations. And finally, it hides the use of the filter pipeline in favor of the more simple programming approach whereby the user provides input to a filter which immediately returns the output.

When introduced in, SimpleITK included a large number of components for image filtering and segmentation. Since then, the toolkit has been significantly extended to include over 280 ITK filters, with the primary addition being the incorporation of the ITK version 4 (ITKv4) registration framework. This addition completes the SimpleITK functionality, with comprehensive support for image filtering, segmentation, and registration. We next describe the new registration framework, starting with its two principal data elements, transformations, and images.

---

### Machine-guided design of cell-type-targeting cis-regulatory elements [^50af4284]. Nature (2024). Excellent credibility.

Cis-regulatory elements (CREs) control gene expression, orchestrating tissue identity, developmental timing and stimulus responses, which collectively define the thousands of unique cell types in the body 1–3. While there is great potential for strategically incorporating CREs in therapeutic or biotechnology applications that require tissue specificity, there is no guarantee that an optimal CRE for these intended purposes has arisen naturally. Here we present a platform to engineer and validate synthetic CREs capable of driving gene expression with programmed cell-type specificity. We take advantage of innovations in deep neural network modelling of CRE activity across three cell types, efficient in silico optimization and massively parallel reporter assays to design and empirically test thousands of CREs 4–8. Through large-scale in vitro validation, we show that synthetic sequences are more effective at driving cell-type-specific expression in three cell lines compared with natural sequences from the human genome and achieve specificity in analogous tissues when tested in vivo. Synthetic sequences exhibit distinct motif vocabulary associated with activity in the on-target cell type and a simultaneous reduction in the activity of off-target cells. Together, we provide a generalizable framework to prospectively engineer CREs from massively parallel reporter assay models and demonstrate the required literacy to write fit-for-purpose regulatory code.

---

### The future landscape of large language models in medicine [^95419c43]. Communications Medicine (2023). Medium credibility.

Box 1 Glossary of computational terms

Artificial Intelligence (AI) models: Computational systems designed to simulate human intelligence and perform tasks such as problem-solving, decision-making, and language processing.

Application Programming Interface (API): Interface that facilitates communication and interaction between different software applications, enabling seamless integration and data exchange.

Bidirectional Encoder Representations from Transformers (BERT): A specific natural language processing (NLP) model that utilizes a transformer-based neural network architecture. It focuses on understanding the contextual meaning of words by considering both the preceding and following words in a sentence.

Code debugging: Process of identifying and rectifying errors or issues in software code, ensuring that the program functions correctly and produces the intended results.

Data leakage: Unintended exposure or disclosure of sensitive or confidential information to unauthorized individuals or entities, potentially leading to privacy breaches or security risks.

Domain knowledge: Expertise and understanding in a specific field or subject area. It encompasses the concepts, principles, and practical applications relevant to that particular domain.

Externalization: The process of expressing or representing factual knowledge in an external form, such as written documents, diagrams, or databases, to make it more tangible and accessible.

Generative Pre-trained Transformer (GPT)-1: Generative Pre-trained Transformer (GPT)-1 is a large language model developed by OpenAI. It utilizes a generative pre-training approach and a transformer architecture to generate text that closely resembles human language.

Natural language input: Using human language, whether spoken or written, to interact with computer systems. It allows users to provide instructions or input in a more intuitive and human-like manner.

---

### SynLight: a bicistronic strategy for simultaneous active zone and cell labeling in the drosophila nervous system [^120ed145]. G3 (2023). Medium credibility.

In Drosophila, there is a multitude of binary expression system drivers (GAL4, lexA, and QF) available, enabling tissue-specific expression in nearly any desired nervous system region and cell type. However, there is considerable variability in the expression strength of binary expression system driver lines, such that not all driver lines can enable transgene expression equivalently. Some cell-type–specific GAL4 lines are weakly expressing, making it difficult to express multiple UAS transgenes simultaneously for concurrent labeling of multiple targets. As progressively more UAS transgenes are incorporated into an experimental design, the consistent translation of each transgene by a weak GAL4 is less likely, which can result in poor or even failed expression of each respective effector or label. To overcome this expression issue and enable expression of multiple neural markers via a single transgenic construct, we used a vector that contains the 2A peptide from the porcine teschovirus-1 (P2A) coding sequence to create a single transgenic construct that encodes both mCD8-GFP and Bruchpilot-Short-mStrawberry (Brp-Short-mStraw) from the same coding sequence. This new construct makes it possible to simultaneously express both neuronal labels from a single ORF rather than 2 independent ORFs (as with 2 independent UAS constructs) while concurrently reducing the genetic load on the system. Therefore, even weakly expressing GAL4s would be able to drive expression of both proteins with high signal fidelity as there would be fewer total transgenes being driven. As the independent constructs fluorescently label synapses as well as the neuronal membrane for visualization at the level of light microscopy, we have named the tool SynLight, a portmanteau of synapse and light. We designed versions of SynLight that can be driven via multiple binary expression systems, including the GAL4/ UAS and QF/ QUAS systems, making it possible to ask new questions about the development and organization of synapses throughout the nervous system with less concern over transgenic dilution or failed labeling. Here, we validate SynLight expression in multiple regions of the adult and larval nervous systems in Drosophila, including the olfactory, visual, and neuromuscular systems using both the GAL4 /UAS and QF /QUAS systems. We additionally demonstrate that SynLight expression does not affect normal neuronal morphology; active zone puncta number as measurements from SynLight expression are quantitatively indistinguishable from measurements via independent Brp-Short-mStraw or mCD8-GFP transgene expression as well as endogenous Bruchpilot antibody staining. Thus, SynLight labels presynapses and neurite membranes, facilitating their visualization with high resolution and permitting more complex experimental design with reliable quantitative measurements. When expressed in a cell-type–specific manner using existing GAL4 or QF promoter lines, SynLight has a wide applicability to a variety of Drosophila nervous system regions and is a versatile tool for studying synaptic development and organization.

---

### Clinical bioinformatician body of knowledge-bioinformatics and software core: a report of the Association for Molecular Pathology [^f5f8973b]. The Journal of Molecular Diagnostics (2025). High credibility.

Next-generation sequencing (NGS) file types — familiarity requirements for clinical bioinformaticians — are outlined as follows: FASTQ files are typically the input to NGS pipelines and consist of four lines per sequence read capturing read names, sequence, and quality scores; FASTA stores reference genome files and contains two lines per read for the read name and sequence; aligning reads to a reference generates SAM or BAM files, where SAM contains sequence alignment data and when compressed into binary form is represented as BAM and is more commonly used given the smaller file size, with processing such as deduplication potentially causing loss of unwanted sequencing reads; variant calling generates a VCF file that describes variants at specific positions in a reference genome, often requiring annotation to convert coordinates into interpretable information, and because VCF tags and headers vary by software the bioinformatician should be able to parse and process VCF files; BED stores genomic regions as coordinates with annotations to filter regions of interest and BAM, VCF, and GTF/GFF can be converted into BED; GFF and GTF describe genes and genomic features of DNA, RNA, and protein sequences; a clinical genomics bioinformatician is expected to know these file types and their use in a clinical bioinformatics pipeline.

---

### Diagnostic performance of host protein signatures as a triage test for active pulmonary TB [^4e7d1842]. Journal of Clinical Microbiology (2023). Medium credibility.

Statistical analysis

We evaluated the diagnostic performance of 12 host biomarkers for diagnosing active TB by means of the efficient use of the machine learning algorithms with the programming language python (Version 3.8) and the scipy library. The corresponding programming code and data have been made publicly available under references and, respectively.

The statistical analysis was first performed on each of the biomarkers individually, then on all possible biomarker combinations. We compared the goodness of fit between the combinations by the value of their negative log-likelihood of the fitted binomial model accounting for country and human immunodeficiency virus (HIV) effect. This estimator is a transformation of the maximum likelihood value yet preserving numerical stability. It allows a ranking of the different combinations, because the smaller the negative log-likelihood value the better is the model fit. For each fixed number of biomarkers, we further analyzed the top three performing biomarker combinations, ranked by their negative loglikelihood value, by using a variety of methodologically varying supervised machine learning algorithms: This includes logistic regression from a generalized linear model perspective, random forests representing an ensemble method, support vector machines representing a non-probabilistic binary linear classifier, and Naïve Bayes representing a probabilistic classifier.

The aim of this analysis was to investigate the potential of algorithms to discern between active TB disease and no TB disease, and to identify promising biomarker combinations for further research. As an exploratory analysis, the full data set was analyzed using fivefold cross-validation (alternating 80% of the data for training and 20% for testing) in order to make the best use of the limited data available. The algorithm produced a number between 0 and 1. A (shifting) percentage threshold determined whether an algorithmic outcome for an individual was denoted as having TB or no TB. Comparing the resulting classification with the reference standard in the data led to sensitivity [True positives/(True positives + False negatives)] and specificity [True negatives/(True negatives + False positives)] estimates. We did not aim at validating a certain algorithmic cutoff for one specific sensitivity and specificity value pair, which is why we did not set aside a separate part of the data set for this validation purpose.

---

### Transcriptional programming using engineered systems of transcription factors and genetic architectures [^a1b0a314]. Nature Communications (2019). High credibility.

The control of gene expression is an important tool for metabolic engineering, the design of synthetic gene networks, and protein manufacturing. The most successful approaches to date are based on modulating mRNA synthesis via an inducible coupling to transcriptional effectors. Here we present a biological programming structure that leverages a system of engineered transcription factors and complementary genetic architectures. We use a modular design strategy to create 27 non-natural and non-synonymous transcription factors using the lactose repressor topology as a guide. To direct systems of engineered transcription factors we employ parallel and series genetic (DNA) architectures and confer fundamental and combinatorial logical control over gene expression. Here we achieve AND, OR, NOT, and NOR logical controls in addition to two non-canonical half-AND operations. The basic logical operations and corresponding parallel and series genetic architectures represent the building blocks for subsequent combinatorial programs, which display both digital and analog performance.

---

### Designing phase II trials in cancer: a systematic review and guidance [^bbefce16]. British Journal of Cancer (2011). Low credibility.

Practical considerations

Finally, in developing a structured approach to selecting a phase II trial design, it became apparent that practical considerations such as the number of patients available and the need for specialist computer programming would aid selection between alternative designs. These elements are discussed for each design listed in the design library.

The manual is freely available to researchers upon request from the CTRU's website, and is compiled as follows. Section 1 lays out the contents of the manual, and section 2 provides an introduction to the background of its development and introduces users to its structure, focus and terminology used throughout. The thought process shown in Figure 4 is also introduced in this section. Section 3 provides detailed discussion of each of the key elements presented in the flow diagram, containing further detail on the background and reasoning behind each of the key elements, and information on the various design options available to researchers. Section 4 presents the library of phase II designs, which are grouped according to the various design elements selected from the flow diagram (e.g. Trial Aim: Treatment selection; Outcome of interest: activity; Outcome measure: Binary; Randomisation: including a control arm; Design category: Two stage). Each design is summarised briefly, with detail regarding the practical considerations of the design and any adaptations that may be incorporated. Finally, Sections 5 and 6 provide references and an index, respectively, to each of the phase II designs listed in Section 4.

As discussed, each of the elements presented in the flow diagram in Figure 4 are described in detail in the guidance manual, and are designed to facilitate interaction between the clinician and statistician throughout the design process. Incorporating the initial clinical information determined through point 1 (therapeutic considerations), researchers may make choices at each stage of the flow diagram (points 2 to 6) regarding the key elements of phase II design. This in turn narrows down the number of designs appropriate to the specific requirements of the trial, and users may incorporate practical issues into their final design choice. The guidance itself is not intended to identify a single trial design that researchers must use; rather, it acts as a resource for identifying designs appropriate to the requirements of each trial from which the researchers can make their choice. This allows the library of designs to be used more in the format of a dictionary as compared with being read from cover to cover.

---

### Pediatric application of coding and valuation systems [^991649b1]. Pediatrics (2019). High credibility.

Role of code sets and the RBRVS in alternative payment models — design and functions — includes Medicare's establishment of the Quality Payment Program under MACRA as evolution from fee-for-service toward value-based alternative payment models that are designed to reduce the rate of growth in health care expenditures by reducing wasteful care and encouraging efficiencies. Standardized code sets such as CPT and ICD have historically supported fee-for-service and will continue to play an instrumental role in alternative payment model design; ICD-10 was created to provide enhanced clinical detail essential to defining populations and assigning clinical risk in payment bundles. Hierarchical condition category (HCC) models illustrate the role of accurate diagnosis in allocating risk-based payment, and although CMS-HCC and Health and Human Services–HCC include stratification of infant and child, organizations may modify HCCs in proprietary ways not transparently disclosed to providers. Many required quality measures in alternative payment models include ICD-based identification of patient eligibility, and the AAP monitors and participates in pediatric-relevant quality metrics through membership with the National Quality Forum. CPT will continue to specify services for populations and episodes; many alternative payment models layer core fee-for-service, CPT-based processes on financial risk-based performance benchmarks; CPT assignment enables identification of the physician's role for assessment of clinical contribution and patient attribution; and leveraging CPT valuation and the RBRVS supports strategic contracting within alternative payment models.

---

### On the efficacy and mediation of a one-on-one HIV risk-reduction intervention for African American men who have sex with men: a randomized controlled trial [^620a7d57]. AIDS and Behavior (2015). Low credibility.

Assessments

The participants completed confidential questionnaires via audio computer-assisted self-interviewing (ACASI) technology, which provided both audio and video presentation of the questions and response options on a laptop computer. Sexual-risk behaviors, theoretical constructs, sociodemographic variables, and health-promotion behaviors and theoretical constructs were assessed pre-intervention and 6 and 12 months post-intervention. The theoretical constructs were also assessed immediately post-intervention. We pilot tested the paper version of the questionnaire with 217 men to ensure that the questions were clear and appropriate for the target population and then programmed it for ACASI and pilot tested it with 16 men to identify and correct any programming errors.

The primary outcome was consistent condom use, a binary variable reflecting whether the participant reported using a condom every time he had anal or vaginal intercourse in the past 90 days. It was based on a comparison of the sum of the reported anal and vaginal intercourse acts in the past 90 days and the sum of the reported condom-protected anal and vaginal intercourse acts in the past 90 days. Men who reported at least one intercourse act and whose number of reported protected acts equaled their number of acts were coded as practicing consistent condom use. Men who reported at least one intercourse act and whose reported number of protected acts was less than their number of acts were coded as not practicing consistent condom use. A widely used measure in HIV prevention trials, considerable evidence indicates that self-reported consistent condom use is associated with a reduced risk of STI, including HIV.

---

### Solu-medrol [^dffbb7bb]. FDA (2024). Medium credibility.

PRINCIPLE DISPLAY PANEL, SERIALIZED LABEL NDC 51662-1264-1

---

### Principles of financing the medical home for children [^e321edba]. Pediatrics (2020). High credibility.

Medical home financing — easing the administrative burden — emphasizes that payers using fee-for-service payments should consistently adhere to CPT definitions of services, because providers waste administrative resources encoding the same service differently for different payers, resultant errors in billing can reduce revenue, and variant coding enfeebles a practice's ability to conduct internal analyses of its services. Pediatricians choose from among specific quality improvement programs designed by the American Board of Pediatrics or one of its chartered entities for maintenance of certification (MOC) and also separately participate in specific programs designed by health care payers to qualify for incentive payments; synergy between these programs would ease administrative burden within the medical home and better promote community health, and payers and MOC program administrators can develop and share large amounts of data to increase program impact, while the efficiency of the medical home would benefit from reducing the multiplicity of efforts necessary to satisfy separate MOC and payer incentive rules.

---

### Buprenorphine [^0b99b496]. FDA (2025). Medium credibility.

Dispense with Medication Guide available at: https://www.sunpharma.com/usa/products

---

### Developing consensus for a more provider-friendly next-generation sequencing molecular biomarker report: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^13712ab7]. The Journal of Molecular Diagnostics (2025). High credibility.

Biomarker results table — design and content recommendations note that data tables are an important and preferred feature of many genomic reports, but although the conspicuous presence of a table is helpful for quickly locating results within a report, information density can hinder rapid interpretation; limiting the number of columns is helpful as it breaks different categories of results into separate tables, each variant should be listed in its own row and described by gene and c. nomenclature, additional information may include the coding transcript, coding impact (eg, nonsense, missense, and frameshift variants), and classification of pathogenicity or clinical relevance, and although these must be brief to be effective in tabular form, a narrative description or clinical interpretation in the context of the patient's illness may be included.

---

### Principles of analytic validation of immunohistochemical assays: guideline update [^58228240]. Archives of Pathology & Laboratory Medicine (2024). High credibility.

Principles of Analytic Validation of Immunohistochemical Assays — supplemental digital content presents a table of published studies with qualitative ratings across multiple columns, using categorical labels such as low, moderate, and serious for each study; example rows include Bisschop et al, Wang et al, Lo et al, and Acs et al demonstrating combinations of these labels.

---

### Auvi-Q [^e27034b4]. FDA (2025). Medium credibility.

NDC 51662-1661-2 LABEL

---

### Trypan blue (Membraneblue) [^414ab890]. FDA (2025). Medium credibility.

MembraneBlue 0.15% - Package label. Principal display panel

---

### Trypan blue (Visionblue) [^2d7220cc]. FDA (2023). Medium credibility.

VisionBlue 0.06% - Package label. Principal display panel

---

### Identification of transcriptional programs using dense vector representations defined by mutual information with geneVector [^2d7a3538]. Nature Communications (2023). High credibility.

Deciphering individual cell phenotypes from cell-specific transcriptional processes requires high dimensional single cell RNA sequencing. However, current dimensionality reduction methods aggregate sparse gene information across cells, without directly measuring the relationships that exist between genes. By performing dimensionality reduction with respect to gene co-expression, low-dimensional features can model these gene-specific relationships and leverage shared signal to overcome sparsity. We describe GeneVector, a scalable framework for dimensionality reduction implemented as a vector space model using mutual information between gene expression. Unlike other methods, including principal component analysis and variational autoencoders, GeneVector uses latent space arithmetic in a lower dimensional gene embedding to identify transcriptional programs and classify cell types. In this work, we show in four single cell RNA-seq datasets that GeneVector was able to capture phenotype-specific pathways, perform batch effect correction, interactively annotate cell types, and identify pathway variation with treatment over time.

---

### Software development skills for health data researchers [^818dbec6]. BMJ Health & Care Informatics (2022). High credibility.

Unit tests

When repetitive tasks are grouped together into functions, these functions can be more easily "tested" to check that their observed behaviour matches their expected behaviour. Performing these checks manually is tedious and error-prone for humans, so programming languages provide additional tools to automate this process. Central to these tools are "unit tests": pieces of code that systematically test a "unit" of code such as a function. They provide the function to be tested with a range of controlled inputs and allow the programmer to make assertions about the expected outputs, to verify that the function is performing as expected.

How to write a unit test

Tests are important: they allow you to change small parts of a complex analytic codebase confidently, with a safety net, knowing that many errors will be caught early. The programmer can run tests individually or in groups when writing code. There are also automatic integrations via platforms like GitHub or GitLab that run tests automatically each time new code is committed. It is a good idea to follow the "Arrange, Act, Assert" principle. Arrange a suitable curated input for the function to be tested on: for example, if the function transforms data, then recreate a much smaller version of that dataset where the correct function output has been pre-calculated. Then Act by passing this pre-prepared test dataset to the function that is being tested, and record the answer. Lastly, Assert: compare the output you got from the tested function with your earlier calculation. This could be done manually or preferably via code to assert that these two outputs match each other.

Working collaboratively

Software engineers and health data researchers usually work in teams and need to collaborate effectively. Software engineers are well-versed in using tools such as GitHub for collaborative working, and these tools have a low barrier to entry for health-data researchers. In this section, we will introduce GitHub and how it can facilitate best practices of version control, and code review, within a team.

---

### Natural language generation for electronic health records [^b39cedec]. NPJ Digital Medicine (2018). Low credibility.

Table 4
Discrete variables in our dataset

The first column shows the variable names, the second column a description of their unique original values, and the third column a bracketed set indicating the range of those values after being recoded during preprocessing

To preprocess the chief complaints, we begin by removing records containing words with a frequency of less than 10 (n = 240,343); this yields a vocabulary V of 23,091, and is primarily a way to reduce the size of the word embedding matrix in our Long Short Term Memory (LSTM) network, reducing the model's computational complexity, but is also serves to remove infrequent abbreviations and misspellings. Similarly, we also remove records with chief complaints longer than 18 words (n = 490,958), the 95th percentile for the corpus in terms of length, which prevents the LSTM from having to keep track of especially long-range dependencies in the training sequences. Following these 2 steps, we convert all the text to lowercase, and we append special start-of-sequence and end-of-sequence strings to each chief complaint to facilitate text generation during inference. Finally, we vectorize the text, convert each sentence to a sequence of vocabulary indices, and pad sequences shorter than 18 words with zeros (these are masked by the embedding layer of our LSTM).

After preprocessing, we are left with ~4.8 million record-sentence pairs, where each record is a 403-dimensional binary vector R, and each chief complaint is a 18-dimensional integer vector S. Using a random 75–25 split, we divide these into 3.6 million training pairs and 1.2 million validation pairs. To reduce the time needed to evaluate different sampling procedures during inference, we save only 50,000 of the validation pairs for final testing.

Modeling

---

### Language models and protocol standardization guidelines for accelerating synthesis planning in heterogeneous catalysis [^7685960f]. Nature Communications (2023). High credibility.

The action terms were then used to manually annotate a randomized subset of 127 synthesis paragraphs (approximately 25% of the total paragraphs compiled) comprising 936 sentences. The annotation was performed on the dedicated software, as shown in Fig. 3b. (see "Methods" for details). Using this set of annotated paragraphs, and combining it with previously annotated set for organic synthesis, we fine-tuned a pretrained transformer modelto devise our ACE (sAC transformEr) model, which translates full-length unstructured sentences from entire paragraphs into a structured, machine-readable sequence of information (see "Methods" for details and Supplementary Note 3). We evaluated the fidelity of our model based on metrics such as the Levenshtein similarity and BLEU (Bilingual Evaluation Understudy) scores. With an overall Levenshtein similarity of 0.66, our ACE model can capture and extract approximately 66% of information from synthesis protocols into correct action sequences, while a BLEU score of 52 attests to the high-quality translation of synthesis sentences from natural language into machine-readable formats (Supplementary Note 4)

To maximize the potential of this language model for automated extraction of synthetic protocols, we wanted to make it easily accessible to a broad audience and, more specifically, experimental researchers. For this purpose, we developed a web application and open-sourced it at synthesis_protocol_extraction. Synthetic and computational researchers can easily access this tool, requiring no prior programming or coding experience.

Accelerating literature analysis and synthesis insights

The ACE model serves the dual purpose of synthetic protocol text mining and collective analysis of this literature data to reveal interesting patterns. As such, this model serves as an invaluable tool to catalysis practitioners as it expedites literature review practice, which is often routine, tedious, and time-intensive. We estimate the time spent on one single paper and for collecting details on the metal speciation, composition, synthetic route, and reaction types by a SAC researcher to be approximately 30 min without any help and under 1 min with our ACE model. Scaling this effort to 1000 publications would cumulatively result in a minimum of 500 man-hours in an ideal case scenario, while text mining these publications by the ACE model would take a mere 6–8 h (a set of 300 papers were text mined in less than 2 h for quantification) offering over 50-fold reduction in the time invested for literature analysis- and thus demonstrating the true worth of language models.

---

### An equity-focused assessment of evidence-based parenting intervention research [^481ebedc]. Clinical Child and Family Psychology Review (2024). Medium credibility.

Results

Descriptive Results

Table 1 provides descriptive information about each program and service included in the review. On average, there were 4.8 papers per program (range: 1–12). The programs represented a mix of prevention (n = 5) and treatment (n = 5). One program, Familias Unidas, was developed specifically for the Latinx population, and Healthy Families America had multiple sites that served different racial/ethnic populations; the rest did not specify a racial or ethnic target population. Nine of the ten programs were available in languages other than English, with Spanish being available for all nine and additional languages available in four. Five of the reviewed programs met the "well-supported" FFPSA evidentiary threshold (Brief Strategic Family Therapy, Familias Unidas, Family Check-Up, Healthy Families America, and Nurse Family Partnerships), while three met "supported" (Child First, Multidimensional Family Therapy, Promoting First Relationships) and two met "promising" (Generation PMTO-Individual and Video Interaction Project). Only one program (Healthy Families America) had more than ten studies that could have been reviewed by the Clearinghouse. When this happens, the Clearinghouse starts with the first ten studies and continues to review studies to ensure the highest possible rating. Studies not included in the rating are still assessed for risk of harm but are not included in the evidence base for the program rating. Thus, for this program, we only reviewed the ten studies that were included for the Clearinghouse rating determination.

Racial Equity Strategies Observed in Studies and Programs

Table 2 displays the frequencies at which the racial equity codes were observed across papers and programs. In the planning/study development domain, the most frequently observed code was "research questions/s were directly related to racial equity", which was observed in 27.1% of the papers and 60% of the programs coded. In this domain, the least frequently observed code was any inclusion of community-based or youth-based participatory research (CBPR/YBPR) or community inclusion methods, which only occurred within two (4.1%) of papers and two programs (20%), Healthy Families America and Familias Unidas.

Table 2
Extent of racial equity strategies used by paper and by program

Most codes can be interpreted positively (the presence of this feature indicates consideration to racial equity). Codes with an asterisk (*) are to be interpreted negatively (presence of this feature is not consistent with best practices for racial equity)

---

### Open and reusable deep learning for pathology with WSInfer and quPath [^093e0a39]. NPJ Precision Oncology (2024). Medium credibility.

Fig. 1
WSInfer workflow.

The WSInfer ecosystem streamlines the deployment of trained deep neural networks on whole slide images through three steps. a In Step 1, users begin by selecting their WSIs and specifying the platform for model inference along with the choice of a pretrained model. If employing the WSInfer Python Runtime, the dataset is expected to be a directory containing WSI files. Alternatively, when using the WSInfer QuPath extension, the image currently open in QuPath serves as the input. QuPath users also have the option to designate a region of interest for model inference. The pretrained model can be selected from the WSInfer Model Zoo or users can provide their own model in TorchScript format. b In Step 2, WSInfer performs a series of processing steps, including the computation of patch coordinates at the patch size and spacing prescribed by the model. Image patches are loaded directly from the WSI and used as input to the patch classification model. The model outputs are aggregated and saved to CSV and GeoJSON files. c In Step 3, model outputs can be visualized and analyzed in QuPath or other software. This example shows breast tumor patch classification on a slide from TCGA.

Table 1
Models currently available in the WSInfer Model Zoo.

WSInfer provides an open-source, cross-platform, and cross-language ecosystem to make deep learning methods uniquely accessible and intuitive for a wide range of digital pathology stakeholders. The core inference runtime is developed in Python, making it readily accessible for data scientists and deep learning specialists working in digital pathology — for whom Python is typically the programming language of choice. By also providing an extension in the widely adopted QuPath software, we aim to greatly broaden access. We anticipate that making the application of models more streamlined in this way will encourage more pathologists to try the methods on new data. This should, in turn, make it easier to identify strengths and weaknesses, and thereby accelerate the critical feedback loop necessary to develop robust and generalizable algorithms that might improve diagnostic, prognostic, and predictive capabilities.

---

### Pediatric application of coding and valuation systems [^6e657ea5]. Pediatrics (2019). High credibility.

CMS and MPFS process and scope are outlined: when RVUs are designated for the target CPT service, the RUC reports its RVU recommendations to the CMS, which then has final authority to either accept, reject, or modify the RUC recommendations, and the CMS announces its final valuation assignments through its annually published MPFS. The MPFS lists CMS-accepted RVU values and details such as whether Medicare designates the CPT code as an active (covered) service, its procedural global package, and the applicability of other descriptive modifiers, with many other payers, including Medicaid programs and commercial plans, using MPFS information in modeling fee schedules. In the 2018 MPFS, the CMS expressed interest in eventually modifying evaluation and management (E/M) documentation guidelines to reduce regulatory burden and place greater emphasis on medical decision-making, and the AAP provided extensive comments highlighting the uniqueness of pediatric care in documenting E/M visits; global packages represent 0-, 10-, or 90-day postsurgical periods and specify the bundled services.

---

### Exenatide [^fd888dbb]. FDA (2025). Medium credibility.

c. When it is time for your next routine dose, go to Section 3, Step 1, and repeat Steps 1 to 6.

Section 4

COMMONLY ASKED QUESTIONS

1.000000000000000e+00 Do I need to do the One-Time-Only New Pen Setup before every dose?

No. The One-Time-Only New Pen Setup is done only once, just before each new pen is used for the first time.
The purpose of the setup is to make sure that your Exenatide Injection Pen is ready to use for the next 30 days.
If you repeat the One-Time-Only New Pen Setup before each routine dose, you will not have enough Exenatide Injection for 30 days. The small amount of Exenatide Injection used in the new pen setup will not affect the 30-day supply of Exenatide Injection.

2.000000000000000e+00 Why are there air bubbles in the cartridge?

A small air bubble is normal. It will not harm you or affect your dose.
If the pen is stored with a needle attached, air bubbles may form in the cartridge. Do not store the pen with the needle attached.

3.000000000000000e+00 What should I do if Exenatide Injection does not come out of the needle tip after four tries during One-Time-Only New Pen Setup?

---

### Identification of transcriptional programs using dense vector representations defined by mutual information with geneVector [^91eb85b8]. Nature Communications (2023). High credibility.

Next, we benchmarked GeneVector's cell type assignment performance with respect to four input types: raw read counts, log-transformed raw counts, normalized total counts per cell, and normalized total counts per cell with log-transformation. We evaluated each approach on the original TICA dataset, in addition to a series of datasets for which we artificially generated a library size batch effect by down sampling reads in subsets of cells (Supplementary Fig. 1A–E). Performance was evaluated by calculating cell type assignment accuracy and by ranking gene pairs by cosine similarity for annotated co-expressed and mutually exclusive gene pairs using CellTypist (Methods: Coexpressed and Mutually Exclusive Markers). Evaluating gene pair rankings on repeat trained models of the full TICA dataset, we find that all four preprocessing types show a similar ranking of co-expressed gene pairs (Supplementary Fig. 1A, left). Interestingly, raw counts produce significantly lower rankings for mutually exclusive gene pairs (Supplementary Fig. 1A, right). When introducing an artificial batch effect, we find that raw counts generate significantly lower rankings for mutually exclusive marker pairs (Supplementary Fig. 2B). Next, we compared the cell type prediction accuracy between normalization procedures using the same gene markers used to perform cell type prediction in the main results section on both the full and subset datasets with artificial batch effect. Raw counts significantly outperformed all other preprocessing inputs by a large margin, with both normalized and log-normalized showing very poor performance (Supplementary Fig. 2C, D). Our results suggest that normalization increases association between pairs of genes with mutually exclusive expression, resulting in negative downstream performance of cell type prediction. Cell typing appears to be sensitive to mutual exclusivity of cell type markers, and normalization produces slight increases in cosine similarity between gene vectors of mutually exclusive genes resulting in poor cell typing accuracy.

After learning the gene embedding, GeneVector allows rapid testing of different marker genes and phenotypes in exploratory analysis settings. Increased performance in classification is important given the large variation of markers used to define the same phenotypes across different studies. Cell type prediction can be recomputed interactively within a Jupyter notebook within twenty seconds for even large datasets on most machines (Supplementary Fig. 2). An additional advantage of having a probability is the ability to map genes from known pathways to a continuous value in each cell. In both phenotype and pathway, demonstration of continuous gradients across cells provides a measure of change and activation that cannot be seen from unsupervised clustering.

---

### Study protocol for a comparative effectiveness trial of two models of perinatal integrated psychosocial assessment: the PIPA project [^a0b0944c]. BMC Pregnancy and Childbirth (2017). Low credibility.

Cost-effectiveness analysis

The primary outcome variable for the cost-effectiveness analysis (Primary Aim 2) will be the proportion of 'correct' referrals initiated and 'incorrect' referrals averted. The incremental costs and effects of SAFE START compared to PIPA will be expressed as the incremental cost-effectiveness ratio (ICER). The ICER combines the net changes in effects with the net change in costs between two strategies providing a relative estimate of the cost-effectiveness of the PIPA program relative to the SAFE START program. The ICER per 'correct' referral made and per 'incorrect' referral averted will be estimated using mean costs and effects and represented with 95% confidence intervals for the ICERs using non-parametric bootstrapping techniques. One-way and multivariate probability sensitivity analysis using Monte Carlo simulations across the cost and effect distributions will be undertaken to investigate uncertainty around the ICERs. Based on number of women screened and proportion of 'correct' and 'incorrect' referrals identified, the economic evaluation will allow a budget impact analysis to be undertaken to determine the costs of running SAFE START and PIPA at other maternity hospitals.

Focus groups (qualitative data)

The qualitative focus group data will be analysed by two members of the research team with qualitative expertise using inductive thematic content analysis consistent with an interpretive descriptive approach. Data will be entered into NVivo. Transcribed data will be coded first by using an open coding process to organise the data by identifying broad thematic patterns. The data will then be read and re-read to enable classification into categories and specific themes.

Psychometric properties of screening tools

To examine the psychometric properties of the EPDS, 'Whooley' questions, GAD-2 and MGMQ when used during pregnancy (Secondary Aim), we will conduct a receiver operator curve analysis to calculate the area under the curve and sensitivity, specificity, positive and negative predictive values and positive and negative likelihood ratios negative likelihood ratios of each measure, using the M. I.N. I. as the gold standard (mood and anxiety disorder modules only). Consistent with other studies of this type a value of 0.5 will be added if a 2x2 cell contains zero, in order to calculate the likelihood ratios and associated confidence intervals. In addition the various screening measures will be compared with each other, using each measure's screen-positive threshold (where these thresholds are available).

---

### AgentMD: empowering language agents for risk prediction with large-scale clinical tool learning [^2806f801]. Nature Communications (2025). High credibility.

Applying Medical Calculators to Patients with AgentMD

Figure 1b shows a high-level overview of how AgentMD computes risks from candidate calculators for any given patient in three main steps: tool selection, tool computation, and results summarization. In the tool selection step, AgentMD first retrieves the top ten most relevant calculators based on the matching between patient information and the calculator description with MedCPT, a foundation model for biomedical information retrieval. Then, AgentMD selects the eligible tools from the retrieved candidates using LLMs, with the goal of providing more accurate and explainable tool selections. The tool selection process is essential, as the interpretations of clinical calculators are based on the populations that participated in their original clinical studies. As such, calculators should only be applied to patients who meet the eligibility criteria. For each selected tool, AgentMD computes the risk of the patient by generating Python scripts via the corresponding calculator. During the tool computation process, AgentMD is provided with a Python interpreter to interact with – the environment returns the printed results or error messages when executing the code that AgentMD writes to use the clinical tools. Based on the returned information from the code interpreter, AgentMD either re-tries other code or summarizes the whole interaction history into a paragraph of the risk calculation results. If the required parameters from the clinical calculator are missing from the patient notes, AgentMD will make a range estimation based on the best- and worst-case scenarios.

Chain-of-thought baseline

We have compared AgentMD to the Chain-of-Thought (CoT)baseline in the evaluation with RiskQA. Specifically, CoT is a commonly used prompt engineering method that instructs the LLM to think step-by-step to solve the task. During the evaluation, AgentMD and the CoT baseline are provided with the same question inputs in RiskQA. Their only difference is that AgentMD can use the RiskCalcs toolkit, while the CoT baseline relies on the encoded knowledge within the backbone LLMs. The CoT prompt is shown below:

System

You are a helpful assistant. Your task is to answer a medical examination question. Please indicate your answer choice (A/B/C/D/E) at the end of your answer by "Therefore, the answer is A/B/C/D/E".

User

{RiskQA Question}

{RiskQA Options}

Let's think step-by-step.

---

### Clinical bioinformatician body of knowledge-bioinformatics and software core: a report of the Association for Molecular Pathology [^e576faa8]. The Journal of Molecular Diagnostics (2025). High credibility.

Demultiplexing and alignment — definitions and selection guidance include assigning reads to samples, mapping to a reference, and choosing appropriate references and aligners. Demultiplexing is the process of assigning sequence reads to the sample of origin using adapter sequences, and alignment is the process of mapping sequence reads to the reference genome sequence. The bioinformatician should be informed about the requirements to build references for the chosen alignment software and select the reference based on the assay design, should select DNA- or RNA-specific aligners based on the analyte used, and should assess aligners using benchmarks when multiple suitable options exist. Samples with unique molecular identifiers may require further processing.

---

### Interactive symbolic regression with co-design mechanism through offline reinforcement learning [^e26f7e74]. Nature Communications (2025). High credibility.

This co-design approach can be conceptualized as an environment within an RL framework. At each interaction step, the agent accesses both the point sets and the current expression tree. The expression tree, or parts of it, can originate from three potential sources: (1) decisions made by Sym-Q, (2) hypotheses or modifications introduced through direct human intervention, and (3) partial decision sequences suggested by other symbolic regression models. In this work, we focus only on the first two types of decisions as prior information to the Sym-Q agent. The co-design mechanism enables domain experts to integrate explicit prior knowledge tailored to the problem's context. Their expertise guides the model's decisions, enabling dynamic refinement and optimization of the expression tree. Additionally, operations can be modified or corrected at any stage based on domain expert insights, ensuring that the generated expressions are both accurate and relevant.

Our proposed co-design process, illustrated in Fig. 7, begins with the agent attempting to construct an expression starting from an empty sequence. This initial attempt often results in an expression that only partially fits the data points, revealing a misalignment between the agent's interpretation of the data structure and an accurate solution. In the next step, we employ a co-design mechanism to guide the agent toward a more accurate representation. To expand the expression tree, we introduce a binary operator as the initial token, followed by a specific operator that better captures the shape of the data points. This step leverages human intuition to correct the agent's initial misjudgments, enhancing its problem-solving capabilities. By integrating insights into the agent's behavior with domain expertise, our co-design process becomes both realistic and reliable. This integration significantly improves the agent's ability to generate accurate expressions, making it more effective in addressing complex problems.

Fig. 7
Iterative enhancement through Co-Design in Sym-Q.

This diagram illustrates the iterative refinement process within Sym-Q when initial solutions are sub-optimal.

---

### Clinical bioinformatician body of knowledge-bioinformatics and software core: a report of the Association for Molecular Pathology [^3ee83e99]. The Journal of Molecular Diagnostics (2025). High credibility.

Bioinformatics pipeline life cycle — clinical pipeline stages encompass design, development and implementation with "Novel algorithms or improvments to the existing ones"; testing comprising "Unit testing", "End-to-end testing", and "User acceptance testing"; validation including "Clinical validation" and "Approvals required for implementation"; deployment; and monitoring with "QA/QC monitoring" and "Performance metrics". The caption explains a pipeline "starts with design, followed by implementation", that after "rigorous testing of various types, the pipeline can be validated on the basis of approval processes in the clinical laboratory", that it is "in production after deployment but undergoes a constant monitoring phase for quality control (QC) and performance", and that improvements from monitoring feed updates "bringing it back to the first phase of design"; the legend labels environment type as "Development" and "Production".

---

### Overcoming the design, build, test bottleneck for synthesis of nonrepetitive protein-RNA cassettes [^0e7aa985]. Nature Communications (2021). High credibility.

Next, to sort the variants in accordance with their likelihood of binding the RBP (i.e. similarity of their dose–response to the positive controls), we carried out the following computation (for details, see Methods). First, we characterized all variants by calculating a vector composed of three components: the slope of a linear regression, its goodness of fit (R 2), and standard deviation of the fluorescence value at the three highest induction bins (Fig. 2b, middle). Next, we computed two multivariate Gaussian distributions using the empirical 3-component vectors, that were extracted for the positive and negative controls and for the given RBP, to yield a probability distribution function (pdf) for both the responsive and non-responsive variants, respectively (Fig. 2b, right). The two populations are relatively well-separated from one another, presenting two distinct clusters with minor overlap. Finally, we defined the "responsiveness score" for each variant (R score - see Methods for a formal definition) as the logarithm of the ratio of the probabilities computed by the responsive pdf to the non-responsive pdf. This score was computed for each unique barcode, and the final result for a sequence variant was averaged over up to five vectors, one for every variant barcode that passed the read-number and basal-level thresholds (see Supplementary Fig. 2 and Methods).

---

### A flexible symbolic regression method for constructing interpretable clinical prediction models [^aa7ffc4f]. NPJ Digital Medicine (2023). Medium credibility.

Methods

Feature engineering automation tool (FEAT)

We extended a recent method for learning informative feature representations called FEAT (v0.4.2) to improve its ability to construct interpretable EHR prediction models (Fig. 1) –. Our goal was to build a classification model from a set of N paired samples, with binary labelsand attributes. FEAT attempts to learn a set of features for a logistic model of the formwhereis a feature representation, i.e. a p -dimensional vector of transformations of, andis the associated vector of coefficients. Like other symbolic regression methods, FEAT generates candidate representationsby searching a space of expression trees composed of simple functions.

The goal of FEAT is to estimate the form ofand thecoefficients that minimize two quantities: (1) the balanced logistic loss of theprediction in the training set, and (2) the complexity of(Fig. 1b). In brief, FEAT accomplishes this using a genetic programming approach in which a set of candidate representations iteratively undergo selection, variation, and survival operations (Fig. 1a). The selection step applies epsilon-lexicase selection, which helps preserve representations that perform well for rare and/or difficult subjects. The variation step applies insertion, deletion, point mutation, and crossover functions to selected representations. The survival step uses a variant of the multi-objective optimization algorithm NSGA-IIfor which the main driving concept driving optimization is Pareto dominance: one candidate representation dominates another if it is at least as good in one of the two objectives (e.g. lower complexity) while being better in the other (e.g. lower logistic loss; Fig. 1b). The optimization procedure utilizes this concept to find a set of models that are non-dominated, meaning they are efficient trade-offs between complexity and error.

For model training, FEAT internally splits input data into training (80%) and validation (20%) sets. Model representations and coefficients are learned in the training set. Pareto-optimal models are then evaluated in the validation set and the model with the lowest balanced log-loss is selected. Classifier thresholds were selected in the combined training and validation set to achieve a positive-predictive value (PPV) in the longitudinal, primary care cohort of 0.70.

---

### Question-based computational language approach outperforms rating scales in quantifying emotional States [^abfe438c]. Communications Psychology (2024). Medium credibility.

In both phases, participants were given the freedom to write the narratives and complete the rest of the questionnaire for as long as they desired. The narratives underwent a manual review to guarantee the exclusion of any nonsensical data and no narratives had to be removed as a result. For the narratives, there was no minimum or maximum number of words. On average, participants wrote N = 81.32 words (SD = 40.66) in their narratives. Finally, to ensure that the participants were attentively following the instructions, in both phases four control questions were embedded, one among each of the rating scale questions. Participants were instructed to respond to a question with a pre-given alternative, for example: "Answer 'Agree' to this question". If any one of the four control questions was not answered correctly, that participant's data were removed from the final sample.

Data analysis

The descriptive words generated by the participants were quantified using BERT. The predicted categorization of the four emotions was generated by multinomial logistic regression. The analyses were conducted in SemanticExcel.com, an online tool for statistical analysis of semantic data, where the main author's underlying code is written in MATLAB.

Pre-processing of the semantic data

Descriptive word and autobiographical narrative responses, collected as part of this study, were altered per the procedures provided by Kjell and colleagues. The descriptive word responses generated by the participants were first cleaned using a manual procedure. Misspelled words were corrected by the spelling tool in Microsoft Word, and this was done only when the author's intended meaning was clear; otherwise, their original form was retained. Instances of incorrectly answered control questions, successively repeated words, or where participants had written "N/A" were excluded. Minimal alterations were made to autobiographical narratives, primarily addressing typos and repetitive words (e.g. the - hte; the the). As the alterations were minor, no distinct analysis was carried out on the original narratives. We also clearly instructed the participants not to use the word of interest in their texts or descriptive word responses. No whole autobiographical narratives were removed.

---

### Interpretable generative deep learning: an illustration with single cell gene expression data [^9c17f8ef]. Human Genetics (2022). Medium credibility.

Deep generative models can learn the underlying structure, such as pathways or gene programs, from omics data. We provide an introduction as well as an overview of such techniques, specifically illustrating their use with single-cell gene expression data. For example, the low dimensional latent representations offered by various approaches, such as variational auto-encoders, are useful to get a better understanding of the relations between observed gene expressions and experimental factors or phenotypes. Furthermore, by providing a generative model for the latent and observed variables, deep generative models can generate synthetic observations, which allow us to assess the uncertainty in the learned representations. While deep generative models are useful to learn the structure of high-dimensional omics data by efficiently capturing non-linear dependencies between genes, they are sometimes difficult to interpret due to their neural network building blocks. More precisely, to understand the relationship between learned latent variables and observed variables, e.g., gene transcript abundances and external phenotypes, is difficult. Therefore, we also illustrate current approaches that allow us to infer the relationship between learned latent variables and observed variables as well as external phenotypes. Thereby, we render deep learning approaches more interpretable. In an application with single-cell gene expression data, we demonstrate the utility of the discussed methods.

---

### What are the determinants of gene expression levels and breadths in the human genome? [^1e1dccec]. Human Molecular Genetics (2012). Low credibility.

MATERIALS AND METHODS

Gene expression data

Gene expression data of human and mouse were downloaded from Gene AtlasV2 data set. In this data set, the expression level (signal intensity) is standardized by MAS5.0 algorithm. Gene expression levels are available in 79 tissues in human and 61 tissues in mouse. We excluded cancerous tissues from our analyses. Signals representing the same tissue were averaged. A gene is considered expressed in a tissue if the corresponding signal intensity is above 200. The mean value of the expression levels in all tissues is defined as a measure of expression level. Gene expression breadth is defined as the number of tissues in which a gene is expressed.

We also analyzed the 'tissue specificity index' as a measure of expression pattern, which is defined as:where n is the number of tissues analyzed, E j the expression level of the gene in the j th tissue and E max the maximum expression level of the gene across the n tissues. The higher the tissue specificity index of a gene, the more the tissue-specific. The results using this measure are shown in.

Sequence characteristics

We obtained exon number, gene length traits and UTR lengths from the UCSC genome browser. CAI was calculated using EMBOSS tools. Repetitive sequences, annotated using the Repeat Masker program, were removed from the analyses. G+C content and CpG O/E (observed CpG sites number divided by expected CpG sites number) were calculated using custom codes.

CpG O/E is defined as, where P CpG, P c and P G are the frequencies of CpG dinucleotides, C nucleotides and G nucleotides, respectively.

To estimate evolutionary rates, we used macaque and rat sequences as outgroups to human and mouse data, respectively. We did this for two reasons: first, by choosing outgroup species that are more closely related, we can estimate evolutionary rates with higher confidence (because we avoid the issue of saturation). Secondly, by estimating evolutionary rates from the primate and rodent pairs separately, we generate estimates of evolutionary rates from primate and rodent lineages. The codeml module of the PAML program was used to estimate non-synonymous and synonymous rates between the primate pair (human–macaque) and the rodent pair (mouse–rat).

Data transformation and standardization

Expression level, exon size, intron length, d N and d S were log-transformed to improve normality. All variables (including dependent and independent variables) were standardized as follows:

where x represents the raw variable. After standardization, the standardized variable, X, has mean zero and variance one.

---

### Standards and guidelines for validating next-generation sequencing bioinformatics pipelines: a joint recommendation of the Association for Molecular Pathology and the college of American pathologists [^a9ce5074]. The Journal of Molecular Diagnostics (2018). Medium credibility.

Recommendation 10 — sample identity in next-generation sequencing (NGS) bioinformatics pipelines — states that "The Identity of the Sample Must Be Preserved throughout Each Step of the NGS Bioinformatics Pipeline with a Minimum of Four Unique Identifiers, Including a Unique Location Identifier within the Content of Each Data File Read and/or Generated by the Pipeline". It requires that across common file types (FASTQ, sequence alignment/map/BAM, and VCF or equivalent) "the following four identifiers, with exceptions noted later, must be included": a unique sample identifier, a unique patient identifier, a unique run identifier, and a laboratory location identifier, and that identifiers "must be present within the file's metadata", with the working group recommending they also appear in the file name to aid troubleshooting. The unique sample identifier "must uniquely identify the laboratory sample at the laboratory location represented in identifier 4", and the unique patient identifier "should allow laboratory staff at the laboratory location represented in identifier 4 to uniquely identify the patient"; some laboratories "perform a safety check by generating a genetic fingerprint for the patient (eg, using SNP genotyping technology)" and, if statistically unique and concise, "this data element is acceptable as a unique patient identifier". The unique run identifier "should allow the staff at the testing laboratory to identify uniquely the instrument and run on which the sample was analyzed" and "should also distinguish an original analysis of the sample from any repeated analyses". The laboratory location identifier "allows for easy differentiation of in-house versus external samples with the same sample identifier", and "probably > 100 institutions have a case number of S17-123", with pairing of sample, patient, and location identifiers enabling code that "prevents results going to the wrong patient's record". Because "none of these file formats requires inclusion of sample, patient, run, or test location information in the file's metadata or file name", the guideline advises avoiding Health Level 7 function characters in identifiers and delimiters, including "the pipe or pipe character (|), ampersand (&), caret (^), tilde (~), pound sign (#), and backslash (\)".

---

### Choosing phase II endpoints and designs: evaluating the possibilities [^a7f48ef2]. Clinical Cancer Research (2012). Low credibility.

Selecting the phase II design and endpoint to achieve the best possible chance of success for a confirmatory phase III study in a particular disease and treatment setting is challenging but critical. Simulating from existing clinical trial data sets and from mathematical models can be useful tools for evaluating statistical properties.

---

### Engineering wetware and software for the predictive design of compressed genetic circuits for higher-state decision-making [^8c61951a]. Nature Communications (2025). High credibility.

Algorithmic enumeration of 3-input circuits

A directed acyclic graph (DAG) was constructed to systematically map general circuits to functional outputs, represented by truth tables. In this DAG, the nodes correspond to transcriptional programming cassettes, with the root node representing the cassette that regulates the output protein. Given an input sequence (i.e. a set of ligands), the circuit's functional behavior is recursively computed through the DAG. Circuits are then enumerated in increasing order of complexity, where complexity is defined by the number of nodes in the graph, corresponding to the number of promoters in the circuit. We implemented the entire DAG structure, including nodes, transcription factors, and transcriptional programming cassettes, as distinct object-oriented components using Python. Specifically, the DAG class serves as a wrapper class representing a circuit, encapsulating nodes, transcription factors, and transcriptional programming cassettes. This design enables modular and scalable circuit enumeration while maintaining flexibility for future expansions and optimizations. The source code with implementation instructions is available at.

A finite set of generalized transcription factors, comprised of a set of RCDs capable of processing 3 orthogonal inputs, at least 5 ADRs with a corresponding number of cognate synthetic promoters, and 3 phenotypes (repression, anti-repression, and super-repression), is used as components for circuit design. The enumeration begins with evaluating the functionality of all possible circuits in the simplest design space — those with one constitutive promoter in addition to the promoter regulating the output. Next, circuit complexity is increased by adding either an additional constitutive promoter or a regulated promoter, and all possible circuits within this expanded design space are enumerated. This cycle of increasing complexity followed by circuit enumeration continues until each truth table is associated with a corresponding circuit. By sequentially increasing complexity, the first circuit associated with a particular truth table is guaranteed to be the least complex one that satisfies that functionality. However, if another circuit with the same number of promoters used is identified, it will be added to the list and considered as having multiple solutions. The enumeration can stop once all truth tables have been identified within the search space using a maximum of 9 promoters in total. Optimization for circuit compression is based on selecting the smallest circuit footprint — i.e. the fewest number of promoters — from an enumerated set of placeholder designs for a user-selected truth table. A flowchart of the algorithmic enumeration workflow is provided in the supplementary material, along with formal pseudocode for the enumeration — see Supplementary Figs. 2–17.

---

### Clinical bioinformatician body of knowledge-clinical laboratory regulation and data security core: a report of the Association for Molecular Pathology [^0add346c]. The Journal of Molecular Diagnostics (2025). High credibility.

Coding systems for data retrieval — Bioinformaticians should construct systems to be able to retrieve discrete variant data across multiple patients and populations for laboratory accreditation purposes, as well as for quality assurance and research. Genetic testing reports are often constructed as free text, which can make the retrieval of variants, their metadata, and variant interpretations difficult if these interpretations are not discretely stored in a separate database. Although natural language processing and manual report reviews have been used for years to extract data from reports, this is known to be an expensive endeavor, and natural language processing is still somewhat computationally expensive and prone to errors; manual extraction is less prone to deciphering errors but is more prone to human error and is expensive and time-consuming from a human resource perspective. Such coding systems have advantages and disadvantages; however, they are best used when captured along with discrete data elements in database systems, and an ontology is a specialized type of medical terminology in which the concepts and codes are formally structured and usually hierarchical.

---

### Achieving inclusive healthcare through integrating education and research with AI and personalized curricula [^dca1a780]. Communications Medicine (2025). Medium credibility.

The SDO's AI Data Visualization tool enables users to import their own biomedical datasets for research analysis. This tool supports multi-modal analysis, accommodates a broader array of data formats, and incorporates automatic error handling, all while being compatible with both Python and R. For more details on this innovative approach, see Methods. The visualization component operates under the assumption of two primary user groups:
Non-technical Users: Users with no programming experience or data familiarity use system generated prompts. Here, the platform first summarizes the dataset and leverages LLMs to generate an explanation of the data and potential visualization goals (i.e. prompts); users can then select their desired goal, prompting the system to utilize the LLM output and summary to automatically generate visualization code and produce the corresponding plots. This multi-step process (e.g. data summarization, goal generation, and elaboration, reading user-generated prompt, code creation, error handling, and plot generation) is further detailed in Supplementary Fig. 1c. Users can further explore the data by posing questions, with the platform assisting in code generation and visualization based on the provided query. Figure 5 illustrates the capability of SDO AI-facilitated visualization for autonomous interpretation and visualization of multi-omics and wearable sensor data (Supplementary Data 5). Fig. 5 Autonomous interpretation and visualization of multi-omics and wearable data through language model integration. The text boxes represent goals expressed as prompts, generated by GPT-4o based on summary data and metadata from the SDO platform. a Heart Rate Variability Over Time (Wearable data): Box plot showing weekly heart rate variability across different weeks, providing insights into changes in heart rate dynamics over time. b Dominant Bacterial Genera (Gut 16S data): Bar graph illustrating the relative abundance of the top five most abundant bacterial genera in the dataset, highlighting key players in the microbial community. c White Blood Cell Count Distribution (Clinical data): Histogram showing the frequency distribution of white blood cell counts, crucial for identifying abnormal immune response patterns. d Chromosomal Variant Distribution (Genomics data): Bar chart representing the frequency distribution of genetic variants across different chromosomes, useful for identifying potential chromosomal hotspots of variability. e Protein Interaction in Immune Response (Proteomics data): Correlation matrix displaying relationships among protein expression levels involved in the complement system, aiding in understanding their collective roles in immune response. f Metabolite Intensity Correlations (Metabolomics data): Heatmap showing correlations between spectral intensity measurements of metabolites, revealing interactions and dependencies crucial for metabolic studies. g Gene Expression Correlations (Transcriptomics data): Correlation matrix and heatmap analyzing pairwise relationships between gene expression levels across the genome, providing insights into potential regulatory and co-regulatory networks. H. Immune Cell Distribution (snRNA data): UMAP representation of snRNA immune cells colored by cell type, illustrating the relationship between cell types and their positions in the UMAP harmony space. Users can fine-tune the visualization by providing feedback, such as changing the x-axis title. The data used in this demonstration were sourced from four independent studies: Mishra et al.(Participant ID: A0NVTRV for Fig. 5a), Zhou et al.(Participant ID: ZOZOW1T for Fig. 5b, c, e, f, and g), the 1000 Genomes Projectannotated by the COSMIC68 datasetfor Fig. 5d, and snRNA immune cells processed data from Hickey et al.
Technical Users: Users with an understanding of data analysis but want to save time from low-level programming challenges such as syntax, library management, debugging, and API updates (e.g. physicians, geneticists, or biologists). These users simply input the desired algorithm (see Supplementary Fig. 1c), prompting the system to generate the code and corresponding visualization. It is crucial to note that problem definition and algorithmic thinking — defined as the ability to break down problems into a series of logical steps — are essential elements of this process. Proper problem definition and algorithmic thinking are vital because they guide the AI in generating accurate and relevant code; without these, users may encounter incorrect or inefficient solutions. The AI Tutor supports users by helping them better define their problems and develop algorithmic thinking skills, ensuring the system produces the most appropriate code and visualizations. We showcase the versatility of the SDO platform in addressing various research questions posed by technical users (Fig. 6). Figure 6 a–e demonstrate the robustness and versatility of automated code generation for replicating and interpreting complex visualizations across diverse research domains (Supplementary Data 5). Fig. 6 Automated code generation and execution from natural language inputs for data visualization in multi-omics and wearables research. The text boxes represent expert-curated prompts designed to reproduce each figure using public multi-omics and wearable datasets. a Protein Marker Correlation Analysis: Heatmap displaying pairwise correlation coefficients for protein markers involved in pathological complete response (pCR) analysis. This figure replicates and expands upon the heatmap from Fig. 2 c in McNamara et al. highlighting potential biomarker interactions in cancer pathways. b Heart Rate Analysis for COVID-19 Detection: Line graph depicting daily average resting heart rates from March 22 to May 17th, identifying significant trends that could indicate physiological responses to COVID-19. This visualization is inspired by the NightSignal algorithm and replicates the scenario depicted in Fig. 1 b in Alavi et al. illustrating variations in resting heart rate during the pandemic. c Differential Protein Expression in Oral Cancer: Volcano plot contrasting protein expression profiles between bacteria-positive and -negative regions in oral squamous cell carcinoma. Adapted from Fig. 2 in Galeano Niño et al. this plot aids in understanding micro-niche level variations within tumor environments. d ASCVD Risk Score Distribution Analysis: Combined violin and box plot of atherosclerotic cardiovascular disease (ASCVD) risk scores, comparing standard and adjusted scores across a sample. Based on Fig. 4a in Schössler-Fiorenza Rose et al. it facilitates risk assessment and stratification in clinical research. e Variance Analysis in Multi-Omics Data: Boxplot showing the intra-class correlation coefficients for various biomedical categories, indicating variance levels attributed to participant structure. This visualization is based on Fig. 2a in Zhou et al. which is crucial for evaluating consistency across multi-omic datasets. f scRNA-seq Data Visualization via UMAP (Uniform Manifold Approximation and Projection): UMAP plot derived from scRNA-seq data files (barcodes.tsv, genes.tsv, matrix.mtx), visualizing gene expression patterns across different cell types. The AI visualization component summarizes all input files and iteratively generates the plot. While the output UMAP in F.S2 is not exactly the same as F.S1, by adding the right set of steps, the SDO AI visualization component successfully captured the clusters in F.S2. The prompt is described as the following: Filter the cells in the Seurat object to include only those with an RNA count of less than 100,000. Normalize the data in the Seurat object using the LogNormalize method, with a scale factor of 10,000. Identify the top 2,000 variable features (genes) in the dataset using the variance-stabilizing transformation (VST) method. F.S4 was built on top of F.S2 and a metadata file. The pipeline in each step provides code and documentation of the thought process it went through to generate the visual. The user can use the documentation to guide the model, where necessary, to achieve the desired outcome (image). This process is iterative.

---

### Hafoe: an interactive tool for the analysis of chimeric AAV libraries after random mutagenesis [^05b24c6a]. Gene Therapy (2025). Medium credibility.

Results

Identification of serotype composition and accuracy

We have developed a command-line standalone tool, hafoe, to provide a convenient toolbox for researchers to analyze shuffled libraries of chimeric AAV vectors.

The user provides hafoe with a FASTA file containing parental serotype sequences and with a FASTQ or a CSV file containing circular consensus sequences of the chimeric library. In addition, the user can also provide one or more FASTQ files of enriched variant libraries after selection rounds of variant expression in the target tissues. hafoe returns interactive HTML reports describing the distribution of the parental serotypes in the chimeric libraries, the parental serotype composition and diversity (the — explore option), and the sequence content of most successful chimeric variants enriched in the target tissue (the — identify option) (Fig. 1B).

The core of our algorithm for the identification of the composition is the neighbor-aware serotype identification approach that divides each chimeric vector into overlapping windows of defined length (reads), aligns them to parental serotypes, and reduces the options of multiple alignment by the assumption that neighboring windows should have a similar parental origin (Fig. 2).

We calculate the accuracy of our variant composition assessment method by comparing the actual composition labels of the synthetic data with the predictions obtained by hafoe. We varied different parameters we set for the identification of variant serotype composition, including the fragment size (read length), the distance between consecutive fragments (step size), and the option we use for filtering out parental serotypes of low quality (filtering parameter). With the best combination of parameters (read length 100 nt, step size 10, and "sum of quality scores" filtering parameter) hafoe reached accuracy in the range of 96.3% to 97.5% across the five synthetic datasets (Supplementary Fig. S1).

---

### Automated clinical coding: what, why, and where we are? [^a4ea9569]. NPJ Digital Medicine (2022). Medium credibility.

The main principle of the current deep learning approach is to find a complex function (non-linear and constructed by multiple layers) to match a clinical note of a patient's visit to a set of codes. As we introduced earlier, this is the multi-label classification setting. This approach, however, has several major limitations when applied to clinical coding:
Handling unseen, infrequent, and imbalanced labels: In the MIMIC-III data set, around 5000 codes appear fewer than 10 times in the training data and over 50% of codes never appear. Vanilla deep learning models rely on large amounts of data for training and fail completely for new or unseen labels. Multi-label classification is also very challenging, especially when there are many labels or when the labels are imbalanced.
Lack of symbolic reasoning capabilities: Manual coding involves reasoning beyond just locating concepts in the notes. The coders sometimes need to connect different pieces of information together. The information from different sources may even be contradictory to each other for the same patient. Their decisions are based on a standard coding process, aided by coding guidelines. Deep learning, on the other hand, tries to simply learn from the labelled data the association between texts and codes in different (pre-trained) embedding spaces, without explicitly modelling the reasoning process. Human-like reasoning may be supported by knowledge-based techniques, which can potentially boost the performance and explainability of coding of deep learning methods. The reasoning may include formalising coding guidelines into logical expressionsand creating regular expressions to capture various diagnosis descriptions of a code, and leveraging various semantics in knowledge graphs constructed from several linked ontologies including the target ICD hierarchy.
Handling long documents: Looking for the relevant information of a code from a long document poses a "needle-in-the-haystack" problem. The recent Transformer-based pre-trained language models (e.g. BERT, Bi-directional Encoding Representations from Transformers) usually require a limited length of up to 512 sub-word tokens (where a word can be tokenised into several sub-words) as input due to the memory-demanding self-attention mechanism, while discharge summaries alone in MIMIC-III have on average around 1500 tokens or wordsand up to over 10,000 tokens, not counting other types of clinical notes. More recent studies applied Longformer, TransformerXL, BigBirdto clinical coding to process documents of up to 4,096 tokens, but this is still insufficient for the clinical notes. On the other hand, text redundancy (or "Note Bloat" problem) is prevalent in clinical note creation, as measured in recent studies. This may impede the performance of deep learning models for code prediction, which may be alleviated through text de-duplication based on text similarity measures.

---

### Inatal advance [^42e69fd2]. FDA (2009). Low credibility.

DOSAGE AND ADMINISTRATION:

One tablet daily or as directed by a physician.

---

### Protocol for the evaluation of the population-level impact of Zimbabwe's prevention of mother-to-child HIV transmission program Option B +: a community based serial cross-sectional study [^486ac52f]. BMC Pregnancy and Childbirth (2019). Medium credibility.

Existing studies measuring MTCT do not capture infant deaths and hence likely overestimate impact (e.g. given high mortality among HIV-exposed and HIV-infected infants) and/or mask the intervention's unintended effects (e.g. increase of infant deaths due to replacement feeding). Studies utilizing prospective cohorts of mother-infant pairs allow assessment of MTCT and HIV-free infant survival, but is sensitive to LTFU, which reduces validity, and cohort effects where study participants may have better outcomes due to the care and follow-up they receive, or as a result of being observed (i.e. Hawthorne effect). Furthermore, the high cost of prospective follow-up remains a financial barrier for many impact evaluations.

We address these methodological gaps by conducting an impact evaluation of Option B+ using a community-based design that does not rely on facility level data, thus capturing PMTCT only among women who receive care at health facilities. While our serial community-based cross-sectional design has been used to assess Nevirapine-based PMTCT programs in four African countries, Option A in Zimbabwe, and Option B in Rwanda and Zambia it has not been used to assess the effectiveness of Option B+.

In addition to the lack of evidence on community effectiveness, no studies have assessed whether there is heterogeneity, based on type pf service delivery, in the potential impact of Option B+. The population-level effectiveness of PMTCT programs implementing Option B+ is likely to vary between health facilities For example, providing lifelong ART to all women with HIV who enter the PMTCT cascade of services, as required by Option B+, requires decentralization of ART services as well as integration of PMTCT and ART services. The level of integration of these services may affect the program's impact, as linkage to and retention in treatment and care is likely to be lower in facilities that refer women to ART centers compared to facilities are able to initiate women on ART. We know of no previous studies that have been able to determine impact heterogeneity of Option B+; specifically, we will conduct a novel analysis combining health facility data with aggregate population-level MTCT data from facility catchment areas. This analysis will also inform policymakers about which facility characteristics are associated with higher rates of HIV-free infant survival and less MTCT at 9–18 months.

---

### Illuminating protein space with a programmable generative model [^5410f856]. Nature (2023). Excellent credibility.

Three billion years of evolution has produced a tremendous diversity of protein molecules 1, but the full potential of proteins is likely to be much greater. Accessing this potential has been challenging for both computation and experiments because the space of possible protein molecules is much larger than the space of those likely to have functions. Here we introduce Chroma, a generative model for proteins and protein complexes that can directly sample novel protein structures and sequences, and that can be conditioned to steer the generative process towards desired properties and functions. To enable this, we introduce a diffusion process that respects the conformational statistics of polymer ensembles, an efficient neural architecture for molecular systems that enables long-range reasoning with sub-quadratic scaling, layers for efficiently synthesizing three-dimensional structures of proteins from predicted inter-residue geometries and a general low-temperature sampling algorithm for diffusion models. Chroma achieves protein design as Bayesian inference under external constraints, which can involve symmetries, substructure, shape, semantics and even natural-language prompts. The experimental characterization of 310 proteins shows that sampling from Chroma results in proteins that are highly expressed, fold and have favourable biophysical properties. The crystal structures of two designed proteins exhibit atomistic agreement with Chroma samples (a backbone root-mean-square deviation of around 1.0Å). With this unified approach to protein design, we hope to accelerate the programming of protein matter to benefit human health, materials science and synthetic biology.

---

### Clinical bioinformatician body of knowledge-molecular diagnostics core: a report of the Association for Molecular Pathology [^478d445e]. The Journal of Molecular Diagnostics (2025). High credibility.

Fundamentals of molecular biology, genetics, and genomics — basic concepts for bioinformaticians are summarized as building blocks for bioinformatics processes and organized into gene (genome), transcript (transcriptome), and protein (proteome) categories, with examples including Exon (exome), Intron, Promoter, Untranslated region (UTR), Pseudogenes, Repetitive regions, Homologous regions; Messenger RNA (mRNA), Transfer RNA (tRNA), Ribosomal RNA (rRNA), Noncoding RNA, Direction/strandedness, Splicing, Primary/secondary (alternative); and Amino acids (codons), Domains, Motifs, Families, Isoforms.

---

### Interpretable generative deep learning: an illustration with single cell gene expression data [^6244a722]. Human Genetics (2022). Medium credibility.

To illustrate the utility of LDVAE, we show an example analysis of scRNA-seq data from peripheral blood mononuclear cells (PBMC) (Zheng et al.). We can use LDVAE to infer gene programs from the data. More specifically, the model learns a latent representation of cells where cells with similar transcriptomes are grouped together and potentially depict gene expression phenotypes. However, due to the non-linearities in the networks of scVI, information about each gene's influence on these latent representations stays hidden to the user. Using LDVAE, we can infer the relationship between gene weights and latent variables, which gives us a way to interpret the influence of one or several genes on the complex gene expression phenotypes (Svensson et al.). We train the network on the 2000 most highly variable genes and 7480 cells. The latent factors separate multiple cell types, all of which were defined by marker genes in Zheng et al. (Fig. 6). We can infer the corresponding information about co-expressed genes from the factor model's loadings. More specifically, loadings for S100A8 and S100A9 indicate monocytes, loadings for CD79A and CD79B mark b-cells (Hu et al.), GNLY and NKG7 represent a cluster of natural killer cells (Fig. 6). The model can infer gene programs by examining more latent dimensions for extracting co-expressed genes (Svensson et al.).

Fig. 6
Interpreting latent representations using linearly decoded variational autoencoders (LDVAE). By using the LDVAE approach on the PBMC dataset (Zheng et al.), we can visually examine the learned latent representations, here the first two variables. The color-coded kernel density estimates show the arrangement of three cell types (monocytes, B-cells, and natural killer (NK) cells) (left). The factor loadings of the first two variables can now also be displayed visually (right). Since the first two latent variables (Z 1 and Z 2) learned by the LDVAE approach separate the three cell types, they can be directly linked to factor loadings of co-expressed genes. Hence, variation along the Z 1 axis corresponds to variation in expression of CD79A and CD79B, whereas variation along the Z 2 axis corresponds to variation in expression of NKG7 and GNLY

---

### 8.4% sodium bicarbonate [^5dba0ef7]. FDA (2021). Medium credibility.

Principle Display Panel, Serialized Label NDC - 51662-1256-1

---

### Benchmarking the translational potential of spatial gene expression prediction from histology [^924b7074]. Nature Communications (2025). High credibility.

Fig. 5
Usability score for the selected methods.

Heatmap showing the scores of each method under each usability scoring category. Height of each box represents the weight of each criterion contributing to the overall score plotted in the green bar plot on top. The blue-pink heatmap represents the row-wise average of the category scores over all methods. Source data are provided as a Source Data file.

We unpacked and examined the subcategories within the usability scores that are most needed for improvement. Figure 5 shows a heatmap that highlights subcategories of usability, with pink indicating weakness and blue indicating strength. The weakest usability subcategory was code assurance, which includes unit testing, support, development model and continuous integration. Whilst most methods were available on GitHub and had a support ticketing system, only GeneCodeR and DeepSpaCE created separate releases and branches for development and master code. All methods failed to incorporate any unit testing for the prevention of unexpected code behaviour with any updates. Key strengths included open-source code, coding consistency and exposed important parameters. Subcategories that require substantial improvement (average score = 0) included the need for domain expertise to run, unexpected warnings during compilation, undocumented function outputs, limited tutorials, absence of dummy-proofing and often requiring users to write new code to apply models. Addressing these challenges would enhance overall usability, expanding the methods' user base and research potential.

---

### Electronic documentation in pediatrics: the rationale and functionality requirements [^d98b9f9c]. Pediatrics (2020). High credibility.

Structured pediatric EHR data capture — discrete data enable decision support but require balance with narrative flexibility. Discrete data allow "relatively easy use and reuse for clinical decision support, quality measurement, research, or reporting to regulatory agencies", and can "support clinical decision aids" for vaccinations and screening, yet "free-text or narrative documentation provides its own benefits" for nuanced, patient-centered care. Overemphasis on discrete entry "may increase documentation burden", and some EHRs limit recording immunizations or medications when dates or names are unknown; therefore, incomplete but relevant information should be accommodated. Important discrete elements include "vital signs, growth parameters, immunizations, screening tests" and validated tools (eg, Apgar score, Pediatric Early Warning Score, Pediatric Quality of Life Inventory). Automation can help: barcoding for vaccine administration "eliminates the need to manually document the lot number and expiration date for every vaccine", and "Alternative technologies such as these can both serve to reduce documentation burden and reduce documentation errors".

---

### Syndemic psychosocial conditions among youth living with HIV: a latent class analysis [^e3d014f0]. AIDS and Behavior (2024). Medium credibility.

Alcohol misuse, marijuana misuse, and illicit drug misuse (cocaine, methamphetamines, inhalants, downers, hallucinogens, opioids) were assessed with the Alcohol, Smoking and Substance Involvement Screening Test. For those endorsing ever having used a given substance, 6 items (e.g. "During the past three months, how often have you failed to do what was normally expected of you because of your use of [substance]?") assessed use over the previous 3 months. Likert-scale response options for the first 4 items ranged from 0 = Never to 6 = Daily or almost daily (item-dependent; for some, the Daily or almost daily option was scored as 7 or 8). Response options to the final 2 items included 0 = No, never; 3 = Yes, but not in the past 3 months; and 6 = Yes, in the past 3 months. Items were summed to yield a compose score. Total scores of ≥ 11 on the alcohol items and ≥ 4 on all other items were indicative of misuse. We dichotomized based on these thresholds. As misuse of individual illicit drugs was low, we created a global illicit drug misuse variable: a score of 4 or higher on any of the aforementioned illicit drugs was considered illicit drug misuse.

Food insecurity was assessed with one item asking about the frequency participants or their family had to reduce meal sizes or skip meals due to inadequate food or money. Response options ranged from 1 = Almost every week to 4 = Did not have to skip or cut the size of meals. We dichotomized this item, coding any endorsement (1–3) as 1 and no endorsement (4) as 0. Unstable housing was assessed with several dichotomous items assessing whether participants had spent at least one night in a shelter, in a public place not intended for sleeping, on the street or anywhere outside, temporarily doubled up with a friend or family member, in a temporary housing program, or in a welfare or voucher hotel/motel. We created a binary variable based on these responses, with endorsement of any being coded 1 and non-endorsement of all coded 0. Legal involvement was assessed with two dichotomous items asking whether participants had ever been (a) arrested or (b) put in jail, prison, or juvenile detention. Endorsement of either item was coded 1; non-endorsement of both was coded 0.

---

### Clinical decision rules: how to use them [^62ab325a]. Archives of Disease in Childhood: Education and Practice Edition (2010). Low credibility.

The first of this pair of papers outlined what a clinical decision rule is and how one should be created. This section examines how to use a rule, by checking that it is likely to work (examining how it has been validated), understanding what the various numbers that tell us about "accuracy" mean, and considers some practical aspects of how clinicians think and work in order to make that information usable on the front lines.

---

### Defining documentation requirements for coding quality care in workers' compensation [^af83b13a]. Journal of Occupational and Environmental Medicine (2016). Medium credibility.

Economic benefits of paying for high-value services — after implementation of incentive payments in the Centers of Occupational Health and Education (COHE) program in Washington State, data analysis reported: disability days per claim were reduced by 4.1 days (P = 0.004); disability costs were reduced by $347 per claim after subtracting the costs of the program itself (P < 0.001); medical costs per claim were reduced by $245 after subtracting the costs of the program itself (P < 0.001); there was a 20% reduction in the likelihood of a 1-year disability; there was a 30% reduction in disability in claims for back injuries; patients of participating providers who consistently engaged in the best practices had 57% fewer disability days than those whose providers did not; and disability avoided totaled 4800 days (13.2 years) among over 1000 workers in an industrialized area near Seattle and 5800 days (15.9 years) among 1000 workers in a multi-county rural area including Spokane.

---

### Usefulness of the CODE ST-segment elevation myocardial infarction program to improve quality assurance in patients with ST-segment elevation myocardial infarction [^cc357a58]. The American Journal of Cardiology (2022). Medium credibility.

The highest mortality rate associated with acute coronary syndrome is observed in patients with ST-segment elevation myocardial infarction (STEMI). Quality care in STEMI management depends on timely reperfusion of the ischemic coronary artery. The CODE STEMI program has been developed to reduce delays and serves as a method to improve quality care in patients with STEMI. Our study aimed to investigate the effects of implementing the CODE STEMI program on quality care and hospital marketing strategy. Our research was a descriptive study with mixed evaluation methods. We enrolled 207 patients with STEMI who underwent primary percutaneous coronary intervention from 2015 to 2018. We used quantitative methods by tracking medical records and administrative documents, as well as qualitative methods by observation and in-depth interviews. Statistical analysis was done using Mann-Whitney and chi-square tests. Our study demonstrated reduced door-to-balloon time, total cost, and length of stay of patients with STEMI who were treated with the CODE STEMI program (p < 0.001, p < 0.001, and p = 0.009, respectively). In addition, there was a likely decrease in major adverse cardiac event incidence and mortality rate after the implementation of CODE STEMI. The hospital and patients expressed their satisfaction with the CODE STEMI program. The program proved to have good efficacy, effectiveness, optimality, acceptability, legitimation, and equity. It also met the marketing mix principles, which included increasing the total number of patients with cardiovascular diseases as well as increasing levels of public trust in STEMI management. In conclusion, the CODE STEMI program has a positive impact on quality care and hospital marketing strategy.

---

### Assessment and management of patients at risk for suicide: synopsis of the 2024 U.S. department of veterans affairs and U.S. department of defense clinical practice guidelines [^4090e6fe]. Annals of Internal Medicine (2025). High credibility.

VA/DoD clinical practice guideline — research priorities for suicide risk identification and screening include: Develop and validate machine learning and AI algorithms for predicting actionable suicide risk (person-level) and develop corresponding clinical tools to inform decision making at the point of care; Examine the impact of cultural, societal, and environmental factors to tailor risk screening and assessment with a particular focus on patient preferences; Develop evidence-informed clinical algorithms that assist in directing intervention and seek to prevent or mitigate potential algorithmic bias; Conduct longitudinal studies to track patients' risk over time, allowing for a better understanding of the dynamic nature of suicide risk and the factors that contribute to changes in risk over time; Use Natural Language Processing and other tools to develop measurable concepts that can be identified through unstructured progress notes to decrease burden and assist assessments; Validate risk stratification categories and strategies; and, for risk screening programs, conduct real-world evaluations to decrease risk for future suicidal behaviors and determine whether risk algorithms can reduce or eliminate the need for universal risk screening.

---

### Principles of analytic validation of immunohistochemical assays: guideline update [^657277c4]. Archives of Pathology & Laboratory Medicine (2024). High credibility.

Principles of Analytic Validation of Immunohistochemical Assays: Guideline Update — supplemental digital content presents a table with multiple rows that display repeated per-column qualitative ratings using the labels moderate, low, and serious across entries.

---

### AgentMD: empowering language agents for risk prediction with large-scale clinical tool learning [^0550e98b]. Nature Communications (2025). High credibility.

User:

Here is a PubMed article:

{PubMed article title}

{PubMed article abstract}

Does this article describe a new risk score or risk calculator? In healthcare, a risk score quantitatively estimates the probability of a clinical event or outcome, such as disease development or progression, within a specified period. These scores are derived from algorithms using variables like patient demographics, clinical history, laboratory results, and other relevant health indicators. They aid clinicians in decision-making, allowing for personalized patient care and resource allocation. Simply answer with "yes" or "no":

GPT-3.5-Turbo

{Output}

Then, AgentMD uses GPT-4, the state-of-the-art LLM at the time of this work, to draft 24,190 structured medical calculators from these articles, where each calculator includes key sections such as title, purpose, patient eligibility, medical topics, computing logics (implemented as functions in Python), interpretation of results, and clinical utility. The prompt for drafting the calculator is shown below:

System

You are a helpful assistant programmer for medical calculators. Your task is to read a PubMed article about a medical calculator, and if applicable, write a two-step calculator: (1) calculate a risk score based on multiple criteria; (2) interpret different ranges of the computed risk score into probabilities of risks.

User

Here is a PubMed article:

{PubMed article title}

{PubMed article abstract}

Does the article describe a simple two-step risk calculator, where the first step is to compute a risk score, and the second step is to interpret different risk scores? If no, please directly and only output "NO". Otherwise, please standardize the calculator into:

# Title

The name of the calculator(s).

## Purpose

Describe when this calculator should be used.

## Specialty

should be a list of calculator types, one or more of {specialty list}, separated by ".

---

### Roflumilast (Zoryve) [^8f91de64]. FDA (2024). Medium credibility.

The dosage of roflumilast TOP for treatment of seborrheic dermatitis in both children (in patients ≥ 9 years) is 1 thin layer TOP daily (0.3% foam)

---

### Curriculum development: a how to primer [^3f967878]. Family Medicine and Community Health (2019). Medium credibility.

Step 3. Writing goals and objectives

Although goals and objectives are often thought of as similar, there is a nuanced difference to them that should be considered. A goal is a general statement of the knowledge, skill or attitude to be attained by the learner and is often a description of the important content as determined in your earlier steps. In contrast, an objective is a specific measurable skill or attitude that the learner will be able to demonstrate at the end of the educational activity. While goals are helpful in defining the overall strategy, the objectives are necessary in order to measure if your curriculum is successful.

While writing goals is relatively simple, as they are general statements of knowledge, the writing of an objective is more challenging and will be discussed in more detail. Objectives need to be understood by both learners and instructors, and to that end, need to be as specific and measurable as possible.

This statement asks us to simply consider the basic elements of an objective: 'Who will do how much of what by when? (page 51)'. Perhaps, the most important component of this is the verb, or the 'will do' piece, which should be open to as few interpretations as possible. Good verbs to use may include 'list', 'define', 'execute' and 'differentiate', as opposed to verbs that should be avoided such as 'know', 'understand' or 'appreciate', which are vague and difficult to measure.

Table 2 below provides examples of how an important content area is translated into a goal and an objective and some examples of both poor and well-written objectives.

Table 2
Examples of goals and objectives

---

### Enhancing medical coding efficiency through domain-specific fine-tuned large language models [^ed11ab15]. Npj Health Systems (2025). Medium credibility.

The data generated encompasses several variations:
Reordered Diagnostic Expressions: Rearranging word order in standard ICD-10 descriptions while preserving semantic meaning, to evaluate the model's adaptability to varied phrasing.
Typographical Errors: Injecting spelling errors based on text length(shorter descriptions < 10 words receive 1–2 errors, while longer ones > 20 words may receive up to 4 errors), to assess robustness to misspellings.
Medical Abbreviations: Replacing standard medical terms with common abbreviations (e.g. "unspecified" to "unspec".), testing the model's capacity to interpret frequently used clinical shorthand.
Multiple Concurrent Conditions: Combining two to five ICD-10 descriptions to reflect comorbidities to evaluate the model's ability to process comorbidities.
Sentences with Single Embedded Diagnostic Information: Embedding ICD-10 descriptions into approximately 23 words, a length sufficient to effectively represent a complete diagnostic context while maintaining efficiency, to test the model's ability to extract relevant entities and generate corresponding codes.

---

### Pooled screening for CAR function identifies novel IL-13R α ₂-targeted CARs for treatment of glioblastoma [^39a0b617]. Journal for Immunotherapy of Cancer (2025). Medium credibility.

While several groups have recently reported employing similar strategies for library-based screening of CARs, our study represents the most extensive exploration of signaling diversity in human primary T cells described to date in terms of signaling domain identity, functional family, immune cell source, and spatial arrangement; though screens of this scale in primary cells present unique technical challenges, their representative physiology enables the use of a broader and more complex variety of screening criteria. While this large library size complicates comprehensive sampling of the represented molecular diversity, which would benefit the development of predictive machine learning algorithms and the establishment of structure-function relationships for CAR design, it enables a more extensive search of the theoretical signaling landscape with fewer biases in inferring design principles and should still reveal instances of strong motif or barcode enrichment despite sampling limitations. Increased sequencing coverage of the signaling domain diversity in the unselected population using higher read depth long-read sequencing techniques or defined ICD barcodes would enable deeper statistical insight into domain enrichment, but was not possible in this study given the read-depth limitations of PacBio sequencing. Furthermore, while only a fraction of the library variants was able to express at the cell surface, which reduces the overall coverage of the explored sequence space, it represents which CARs might be functional proteins. We propose that this large library format may be a better tool for novel CAR discovery that can be broadly applied to many different disease targets and selection criteria. Altogether, the modular nature of high throughput CAR screening makes it uniquely poised to reinvigorate CAR discovery and expedite clinical translation in a streamlined process and its principles have broad implications for therapeutic modalities beyond CARs, making it a powerful tool for unlocking next-generation cell therapies.

Data availability

Data and materials availability

The NGS selection data sets have been deposited in the Sequence Read Archive and are available under the accession number PRJNA957857. The scRNA-seq data have been deposited in the Gene Expression Omnibus under accession number GSE259352. All data generated or analyzed during the study (including the DomainSeq-processed CARPOOL selection data) are included in the paper or its online supplemental information.

Code availability

The code used to analyze the domain composition of selected CARs can be accessed in the DomainSeq repository atGordon et al 2.022000000000000e+03

---

### New designs for phase 2 clinical trials [^a90544dc]. Blood (2003). Low credibility.

Conventional phase 2 clinical trials are typically single-arm experiments, with outcome characterized by one binary "response" variable. Clinical investigators are poorly served by such conventional methodology. We contend that phase 2 trials are inherently comparative, with the results of the comparison determining whether to conduct a subsequent phase 3 trial. When different treatments are studied in separate single-arm trials, actual differences between response rates associated with the treatments, "treatment effects", are confounded with differences between the trials, "trial effects". Thus, it is impossible to estimate either effect separately. Consequently, when the results of separate single-arm trials of different treatments are compared, an apparent treatment difference may be due to a trial effect. Conversely, the apparent absence of a treatment effect may be due to an actual treatment effect being cancelled out by a trial effect. Because selection involves comparison, single-arm phase 2 trials thus fail to provide a reliable means for selecting which therapies to investigate in phase 3. Moreover, reducing complex clinical phenomena, including both adverse and desirable events, to a single outcome wastes important information. Consequently, conventional phase 2 designs are inefficient and unreliable. Given the limited number of patients available for phase 2 trials and the increasing number of new therapies that must be evaluated, it is critically important to conduct these trials efficiently. These concerns motivated the development of a general paradigm for randomized selection trials evaluating several therapies based on multiple outcomes. Three illustrative applications of trials using this approach are presented.

---

### Recommendations for the use of in silico approaches for next-generation sequencing bioinformatic pipeline validation: a joint report of the Association for Molecular Pathology, association for pathology informatics, and college of American pathologists [^9e4c7776]. The Journal of Molecular Diagnostics (2023). High credibility.

Table 2 — Recommended applications for different types of in silico data outlines which data types support specific purposes in next-generation sequencing (NGS) bioinformatics: for benchmark bioinformatics tools, Purely simulated data; manipulated assay data (where applicable); for validation of bioinformatics pipeline, (a) new variants use Manipulated assay data, (b) limit of detection uses Mixing multiple samples; downsampling FASTQ files, and (c) lowest number of sequencing reads uses Downsampling FASTQ files; for assessment of the pipeline after updates or version changes, (a) laboratory protocol changes use Biological samples with in silico data as supplementary depending on changes, (b) changes that do not affect limits of tools in the pipeline use Existing assay data with in silico data as supplementary depending on changes, and (c) changes that affect the limits of tools in the pipeline use Manipulated assay data in addition to existing assay data where applicable; proficiency testing uses Manipulated assay data; and variant annotations use Manipulated assay data; VCF file manipulation.

---

### Toward a universal decoder of linguistic meaning from brain activation [^8ab53476]. Nature Communications (2018). Medium credibility.

Decoding methodology

The decoder operates by predicting a semantic vector given imaging data. Each dimension is predicted using a separate ridge regression with the regularization parameters estimated from training data. More formally, given a number of examples by number of voxels imaging data matrix X (training set), and the corresponding number of examples by number of dimensions semantic vector matrix Z, we learn regression coefficients b (a vector with as many entries as voxels) and b 0 (a constant, expanded to a vector) that minimizefor each column z — a semantic dimension — of the Z matrix.

The regularization parameter λ is set separately for each dimension using generalized cross-validation within the training set. Each voxel was mean-normalized across training stimuli in each imaging experiment, as was each semantic vector dimension.

In experiment 1, the decoder was trained within a leave-10-words-out cross-validation procedure. In each fold, the regression parameters for each dimension were learned from the brain images for 170 words, and predicted semantic vectors generated from the brain images for 10 left-out words. The voxelwise normalization was carried out using a mean image derived from the training set, which was also subtracted from the test set. The cross-validation procedure was carried out on data of each of the three paradigms separately or on the dataset resulting from averaging them; we report results for all of these. This resulted in 180 decoded semantic vectors. In experiments 2 and 3, the decoder was trained on brain images for 180 words from experiment 1 and applied to brain images of 384 and 243 sentences, respectively, resulting in 384 and 243 decoded semantic vectors.

Each decoder was trained on images reduced to a subset of 5000 voxels, approximately 10% of the number left after applying a cortical mask. We picked this number as a conservative upper bound for the number of informative voxels, as determined in previous studies. Voxels were selected by the degree to which they were informative about the text-derived semantic vectors for training set images, as measured by ridge regressions on the voxel and its adjacent three-dimensional (3D) neighbors. We learned ridge regression models (regularization parameter set to 1) to predict each semantic dimension from the imaging data of each voxel and its 26 adjacent neighbors in 3D, in cross-validation within the training set. This yielded predicted values for each semantic dimension, which were then correlated with the values in the true semantic vectors. The informativeness score for each voxel was the maximum such correlation across dimensions.

---

### Medical device surveillance with electronic health records [^1ababcfc]. NPJ Digital Medicine (2019). Medium credibility.

Candidate relations were defined as the Cartesian product of all entity pairs defining a relation (i.e. pain/anatomy and implant/complication) for all entities found within a sentence. Candidate events and relations were generated for all sentences in all clinical notes.

Labeling functions leverage existing natural language tools to make use of semantic abstractions, or primitives, which make it easier for domain experts to express labeling heuristics. For example, primitives include typed named entities such as bacterium or procedures, document section structure, as well as negation, hypothetical, and historical entity modifiers provided by well-established clinical natural language processing methods such as NegEx/ConText. These primitives provide a generalized mechanism for defining labeling functions that express intuitive domain knowledge, e.g. "Implant complications found in the 'Past Medical History' section of a note are not present positive events" (Fig. 6). Primitives can be imperfect and do not need to be directly provided to the final discriminative model as features. Critically, the end deep learning discriminative model automatically learns to encode primitive information using only the original text and the training signal provided by our imperfect labels.

Fig. 6
Labeling function examples. Clinical notes (top left) are preprocessed to generate document markup, tagging entities with clinical concepts (blue), parent section headers (green), dates/times (orange), and historical, hypothetical, negated, and time-delta attributes (shades of red). Labeling functions (top right) use this markup to represent domain insights as simple Python functions, e.g. a function to label mentions found in "Past Medical History" as FALSE because they are likely to be historical mentions rather than a current condition. These labeling functions vote {FALSE, ABSTAIN, TRUE} on candidate relationships (bottom, with true labels indicated in the LABEL column) consisting of implant (black rounded box) - complication (grey square box) pairs to generate a vector of noisy votes for each candidate relationship. The data programming step described in Fig. 1 uses this labeling function voting matrix to learn a generative model which assigns a single probabilistic label to each unlabeled candidate relationship

---

### Symbolic metaprogram search improves learning efficiency and explains rule learning in humans [^e46441ae]. Nature Communications (2024). High credibility.

Throughout their lives, humans seem to learn a variety of rules for things like applying category labels, following procedures, and explaining causal relationships. These rules are often algorithmically rich but are nonetheless acquired with minimal data and computation. Symbolic models based on program learning successfully explain rule-learning in many domains, but performance degrades quickly as program complexity increases. It remains unclear how to scale symbolic rule-learning methods to model human performance in challenging domains. Here we show that symbolic search over the space of metaprograms — programs that revise programs — dramatically improves learning efficiency. On a behavioral benchmark of 100 algorithmically rich rules, this approach fits human learning more accurately than alternative models while also using orders of magnitude less search. The computation required to match median human performance is consistent with conservative estimates of human thinking time. Our results suggest that metaprogram-like representations may help human learners to efficiently acquire rules.

---

### The role of home and community-based services in meeting the… [^c39ef3f1]. aspe.hhs.gov (1998). Low credibility.

U. S. Department of Health and Human Services The Role of Home and Community-Based Services in Meeting the Health Care Needs of People with AIDS: Final Report Jennifer Schore, Mary Harrington, and Stephen Crystal Mathematica Policy Research, Inc. March 1998 PDF Version: http: //aspe. hhs. gov/daltcp/reports/aidsfrpt. pdf. Thisreportwaspreparedundercontract#PHS-282-92-0044betweenHHS'sOfficeoftheAssistantSecretaryforPlanningandEvaluation, OfficeofDisability, AgingandLong-TermCarePolicyandMathematicaPolicyResearch, Inc. Foradditionalinformationaboutthestudy, youmayvisittheDALTCPhomepageathttp: //aspe. hhs. gov/_/office_specific/daltcp. cfmorcontacttheASPEProjectOfficer, GavinKennedy, atHHS/ASPE/DALTCP, Room424E, H. HHumphreyBuilding, 200IndependenceAvenue, SW, Washington, DC20201. Hise-mailaddressis: Gavin. Kennedy@hhs. gov. Theopinionsandviewsexpressedinthisreportarethoseoftheauthors. TheydonotnecessarilyreflecttheviewsoftheDepartmentofHealthandHumanServices, thecontractororanyotherfundingorganization. A. CASESTUDYMETHODOLOGYB. CASESTUDYLESSONSIV. THENEEDFORHOME-ANDCOMMUNITY-BASEDSERVICEDATAA. NATIONALANDSTATEDATABASESB. PROVIDERDATABASESC. POTENTIALUSESOFPROVIDERDATAD. NEXTSTEPS. C. FINDINGSD. CONCLUSIONVI. SYNTHESISANDIMPLICATIONSAPPENDICESAPPENDIXA: CASESTUDYOFHOME-ANDCOMMUNITY-BASEDSERVICEDELIVERYINNEWYORKCITYAPPENDIXB: CASESTUDYOFHOME-ANDCOMMUNITY-BASEDSERVICEDELIVERYINLOSANGELES. TABLEV. 2: ProportionUsingEachTypeofHomeCareServices, AIDSDiagnosistoDeathTABLEV. 3: AverageMonthlyExpendituresonHomeCareServicesFromAIDSDiagnosistoDeathTABLEV. 4: AverageMonthlyExpendituresonHomeCareServices, AmongUsersofEachTypeofService, FromAIDSDiagnosistoDeath. ACKNOWLEDGMENTSManypeoplehavecontributedtheirtime, knowledge, andexpertisetothisreport, andwewouldliketothankthemall. Thestaffofeachprogramintheproject'scasestudies, inaddition, generouslysharedwithustheirfrontlineexperiencesservingpeoplewithAIDSeveryday: AIDSProjectLA, AIDSHealthcareFoundation, andAltaMedHealthServicesCorporationinLosAngeles; andGayMen'sHealthCrisis, VisitingNurseServiceofNewYork, andVillageCenterforCareinNewYorkCity. MedicaidandRyanWhitestaffforthestatesofCaliforniaandNewYorkandcitiesofLosAngelesandNewYorkprovideduswithexcellentoverviewsofthoseprograms. StaffwiththeNewYorkCityDivisionofAIDSServicesandfederalHealthResourcesandServicesAdministration, HealthCareFinancingAdministration, andCentersforDiseaseControlandPreventionprovideduswithinformationnotavailableinthepublishedliterature. WealsoreceivedinvaluableassistanceinpreparingthisreportfromourprojectofficerintheOfficeoftheAssistantSecretaryforPlanningandEvaluation, GavinKennedy, andhiscolleagues, aswellasfromanumberofourcolleaguesatMathematica. CraigThorntonsharedwithushispolicyexpertiseandknowledgeofservicedeliverytopeoplewithalltypesofdisabilitiesthroughouttheprojectandprovidedinsightfulcommentsondraftsofthisreport. MarjorieMitchell, CindyCastro, DoreenAmbrose, BesaidaRosado, JillMiller, andJenniferBaskwellmasterfullyword-processedandproducedthisreport. PatriciaCiaccioandWalterBrowerprovidedhelpfuleditorialassistance. Wethankeachone. Thisisthefinalreportfortheproject, "TheRoleofHome-andCommunity-BasedServicesinMeetingtheHealthCareNeedsofPeoplewithAIDS, "conductedbyMathematicaPolicyResearch, Inc. fortheU. S.

---

### Integrating representation learning, permutation, and optimization to detect lineage-related gene expression patterns [^63498268]. Nature Communications (2025). High credibility.

Learning lineage tree-structured gene expression representations with an autoencoder

To learn lineage tree-structured representations of the preprocessed gene expression profileswe use an overparametrized autoencoder that maps each cell's G -dimensional gene expression profile to a 1000-dimensional latent representation. The encoder g ϕ with parameters ϕ consists of 2 fully connected layers with output dimension 1000 with a GELU non-linearity between them. The decoder f θ with parameters θ consists of two fully connected layers with output dimensions 1000 and G respectively and each preceded by a GELU non-linearity (Supplementary Fig. 19 a). We train the autoencoder for 500 epochs with Adam, a batch size of 128, and learning rate of 10 −4 using just a mean-squared error (MSE) reconstruction loss for each batch B :After 500 epochs, we add a tree-likeness loss term, the lineage tree-based triplet loss from Eq. (6), to the reconstruction loss. The full loss for each batch B becomes:where T B is obtained from T by retaining only the leaves in B but preserving distances or equivalently, T B induces the subset of the valid triplets I B = I T ∩ (B × B × B) where I T is the full set of valid triplets across all cells in T defined in Eq. (5). λ is a hyperparameter trading off the importance of the two loss terms. We use λ = 1 in our experiments.

A similar approach can be taken to learn lineage tree-structured representations of other single-cell data modalities with an appropriate model architecture. In settings where it is not obvious how to calculate meaningful distances between the cell profiles in the input space, the negated triplet loss of the lineage tree-structured representations learned by an autoencoder after a fixed number of epochs of training with a triplet loss can be used as a tree-likeness score (Supplementary Fig. 2 a–d). Results for the representation learning-based tree-likeness score under permutation of cells within subtrees of fixed height are invariant across different autoencoder architectures (Supplementary Fig. 19) and choices of λ as long as training is stopped early enough (Supplementary Fig. 20). The triplet loss can also be substituted for any other negated differentiable tree-likeness score, e.g. LAC.

---

### Temporal asymmetries in inferring unobserved past and future events [^c421057d]. Nature Communications (2024). High credibility.

Large-scale analysis of conversational data

At a high level, the goal of our analysis of conversational data was to predict in-text references to past and future events. Manually identifying these references is labor and time-intensive, so it is impractical to scale up manual tagging to millions of documents. Instead, we defined a set of heuristics for predicting when text is referring to real or hypothetical past or future events. Our approach comprised four main steps.

First, we used the nltk packageto segment each document into individual sentences. Each sentence was processed independently of the others. Second, we handled contractions using the contractions package (e.g. "we'll" was split into "we will", and so on). Third, we defined two sets of "keywords" (words and phrases) that tended to be indicative of referring to the past (Tab. S6) or future (Tab. S7). We used ChatGPTto generate each list, with exactly 50 templates per list, using the following prompt:

I'm designing a heuristic algorithm for identifying references (in text) to past and future events. Part of the algorithm will involve looking for specific keywords or phrases that suggest that the text is referring to something that happened (or will happen) in the past and/or future. Could you help me generate a list of 50 keywords or phrases to include in each list (one list for identifying references to the past and a second list for identifying references to the future)? I'd like to be able to paste the lists you generate into two plain text documents with one row per keyword or phrase, and no other content. Please output the lists as a "code" block (enclosed by "'"'..

Fourth, we used part-of-speech tagging (again, using the nltk package) to look for verbs or verb phrases that were in past or future tenses. After the words were tagged with their predicted parts of speech, we used regular expressions (applied to the sequences of tags) to label each verb or verb phrase with a human-readable verb form (e.g. "future perfect continuous passive", "conditional perfect continuous passive", and so on). The regular expressions we used to generate these labels are shown in the Supplementary Table S4, and the part of speech tags are defined in the Supplementary Table S5.

---

### Scalable recombinase-based gene expression cascades [^a910ae72]. Nature Communications (2021). High credibility.

Temporal modulation of the expression of multiple genes underlies complex complex biological phenomena. However, there are few scalable and generalizable gene circuit architectures for the programming of sequential genetic perturbations. Here, we describe a modular recombinase-based gene circuit architecture, comprising tandem gene perturbation cassettes (GPCs), that enables the sequential expression of multiple genes in a defined temporal order by alternating treatment with just two orthogonal ligands. We use tandem GPCs to sequentially express single-guide RNAs to encode transcriptional cascades that trigger the sequential accumulation of mutations. We build an all-in-one gene circuit that sequentially edits genomic loci, synchronizes cells at a specific stage within a gene expression cascade, and deletes itself for safety. Tandem GPCs offer a multi-tiered cellular programming tool for modeling multi-stage genetic changes, such as tumorigenesis and cellular differentiation.

---

### Binary tree-inspired digital dendrimer [^cca48c38]. Nature Communications (2019). High credibility.

Digital polymers with precisely ordered units acting as the coded 0- or 1-bit, are introduced as a promising option for molecular data storage. However, the pursuit of better performance in terms of high storage capacity and useful functions never stops. Herein, we propose a concept of an information-coded 2D digital dendrimer. The divergent growth via thiol-maleimide Michael coupling allows precise arrangements of the 0- and 1-bits in the uniform dendrimers. A protocol for calculating the storage capacity of non-linear binary digital dendrimer is established based on data matrix barcode, generated by the tandem mass spectrometry decoding and encryption. Furthermore, the generated data matrix barcode can be read by a common hand-held device to cater the applications such as item identification, traceability and anticouterfeiting purpose. This work demonstrates the high data storage capacity of a uniform dendrimer and uncovers good opportunities for the digital polymers.

---

### Predictors of WIC uptake among low-income pregnant individuals: a longitudinal nationwide analysis [^4559beed]. The American Journal of Clinical Nutrition (2023). Medium credibility.

Outcome

The primary outcome was a dichotomous variable indicating whether a respondent self-reported WIC receipt during pregnancy in either the PRAMS survey or the linked birth certificate.

Explanatory variables

Individual-level predictors included a respondent's age, race/Hispanic origin, education, family size, household income, language, prepregnancy diabetes, pregnancy intent, and census region of residence. Race and Hispanic origin were self-reported as 2 separate questions in the linked birth certificate data and were categorized in this study as non-Hispanic White, non-Hispanic Black, Hispanic, non-Hispanic Asian/Pacific Islander, non-Hispanic American Indian/Alaska Native, and non-Hispanic other. Pregnancy intent was coded as a binary variable (wanted to get pregnant for the pregnancy in question; compared with those who wanted to get pregnant sooner, later, or not at all). Additional details on covariate categories are included in the Supplementary material.

In addition, while WIC is a federal program, it is administered by state agencies, which results in some state-level variation in enrollment practices. Therefore, this study included yearly state-level predictors: GDP per capita, unemployment rate, state earned income tax credit (EITC) rate, an indicator for whether the governor was a Democrat, and measures of state caseloads of other social programs including the Supplemental Nutrition Assistance Program (SNAP), Temporary Assistance for Needy Families program (TANF), and Medicaid. Additional details are included in the Supplementary material.

---

### Electronic documentation in pediatrics: the rationale and functionality requirements [^139d9ddc]. Pediatrics (2020). High credibility.

Speech recognition in electronic documentation — Speech (voice) recognition software is a growing technology.

---

### Clinical bioinformatician body of knowledge-bioinformatics and software core: a report of the Association for Molecular Pathology [^a9322a42]. The Journal of Molecular Diagnostics (2025). High credibility.

Algorithm development life cycle — clinical machine learning (ML) models in bioinformatics are organized into design and planning, data wrangling, model development and optimization, and clinical testing, beginning with model design and planning and applying validation testing before clinical use, with cycles restarting as assay requirements evolve. Design and planning activities include to "Define value to be predicted", "Identify data set for model training", and "Capture variability expected in production within data sets". Data wrangling tasks include "Data structuring into organized formats", "Data cleaning to remove/correct invalid values and handle missing values", "Data transformation, normalization, and standardization", "Fold data into mutually exclusive training and validation test sets", and "Feature engineering of predictive features for modeling (eg, metrics generated by an upstream pipeline)". Model development and optimization covers "Determine machine learning model class by considering data labels and model complexity vs explainability" (including "Supervised ML (eg, regression, classification)" and "Unsupervised ML (eg, clustering, anomaly detection)"), "Feature selection to identify predictive features", "Iterative model training and testing", and "Optimization to reduce overfitting, such as cross-validation and measurement of learning curves". Clinical testing emphasizes "Calculation of key model evaluation metrics (eg, RMSE, confusion matrix, etc.)", "Evaluation of model bias", "Demonstration of model robustness", and "Model validation (refer to Validation)", with RMSE defined as root mean squared error (RMSE).

---

### Do consumer-directed medicaid supportive services work for… [^a23e7c44]. aspe.hhs.gov (2004). Low credibility.

U. S. Department of Health and Human Services Do Consumer-Directed Medicaid Supportive Services Work for Children with Developmental Disabilities Leslie Foster, Stacy Dale, Randall Brown, Barbara Phillips, Jennifer Schore and Barbara Lepidus Carlson Mathematica Policy Research, Inc. September 2004 PDF Version: http: //aspe. hhs. gov/daltcp/reports/2004/ddkidsMss. pdf. HumphreyBuilding, 200IndependenceAvenue, S. W. Washington, D. C.
20201. Here-mailaddressis: Pamela. Doty@hhs. gov. Theopinionsandviewsexpressedinthisreportarethoseoftheauthors. TheydonotnecessarilyreflecttheviewsoftheDepartmentofHealthandHumanServices, thecontractororanyotherfundingorganization. TABLEOFCONTENTS-BACKGROUND-ANewModelofMedicaidSupportiveServices-CashandCounselingforFloridaChildren-EXPECTEDPROGRAMEFFECTS-PreviousResearch-HypothesesAboutServiceUse-HypothesesAboutCareQuality-HypothesesAboutSubgroupEffects-METHODS-DataCollectionandSample. -InConclusion-APPENDIXA: WaiverBenefits-APPENDIXB: AdditionalInformationonMethods-BaselineCharacteristicsControlledforintheAnalysis-OutcomeMeasuresandReferencePeriods-InterviewingConsiderations-SampleRestrictions-StatisticalPower-APPENDIXC: AdditionalResults. -TABLE2: Treatment-ControlDifferencesinTypesofCaregivers, byWhetherReceivedPCSFromPubliclyFundedCaregiversatBaseline-TABLE3: EstimatedEffectsofConsumerDirectedCareonHoursofAssistanceInPastTwoWeeks, byWhetherReceivedPCSFromPubliclyFundedCaregiversatBaseline. -TABLEB. 2: OutcomeMeasuresUsedintheAnalysis, byType-TABLEB. 3: SamplesUsedintheAnalysis-TABLEB. 4: MinimumDetectableEffects-TABLEC. 1: EstimatedEffectsofConsumerDirectedCareonTypesofAssistance, byWhetherReceivedPCSFromPubliclyFundedCaregiversatBaseline. ACKNOWLEDGMENTSThispaperhasbenefitedgreatlyfromthethoughtfulcommentsandsuggestionsofseveralpeople. Inparticular, weappreciateinputfromtheCashandtheCounselingDemonstrationandEvaluationmanagementteam, includingKevinMahoney, LoriSimon-Rusinowitz, MarieSquillace, andTonyaMoore; andfromexternalreviewersA. E. Benjamin, LauriePowers, andJeanTuller. Inaddition, numerouscolleaguesatMathematicaPolicyResearch, Inc. madethispaperpossible. LiciaGaberandAmyZambrowskiprogrammedtheanalysis, andHenryIreysprovidedthoughtfulcommentsonanearlierdraft. WalterBrowereditedthepaper, andJillMillerproducedit. Theopinionspresentedherearethoseoftheauthorsanddonotnecessarilyreflectthoseofthefunders, theCashandCounselingNationalProgramOffice, thedemonstrationstates, ortheCentersforMedicare&MedicaidServices. ARandomizedDesignandComprehensiveSurveysProvideDefinitiveResultsDemonstrationenrollment, whichoccurredbetweenJune2000andAugust2001, wasopentochildrenwhowere3-17yearsoldandparticipatinginFlorida'sDevelopmentalServiceswaiverprogram. Aftertheirparentscompletedabaselineinterview, the1, 002childrenenrolledinthedemonstrationwererandomlyassignedtoparticipateinCDCortoreceivewaiverservicesasusual.

---

### Large language model agents can use tools to perform clinical calculations [^50362bb6]. NPJ Digital Medicine (2025). Medium credibility.

Presentation and scoring

To evaluate the dynamic programming abilities in ChatGPT, prompts were presented to the web interface of the commercial version of ChatGPT by one of two physician assessors. Each vignette was started in its own conversation to eliminate contamination from prior prompts. If the LLM did not provide a definitive answer but showed progress towards an answer, the user encouraged the conversation with a prompt such as "Please continue". To score the performance of the model, assessors compared ChatGPT's answers with the answer provided by MDCalc with input data from the vignette. For criteria-based models, answers were counted as correct if the reported scores matched. For models with continuous variable outputs, rounding errors were permitted if the calculator did not specify how to round. Some of the calculators involved calculations without a gold standard, such as drug conversions. Responses on these models were considered correct if the underlying math was executed appropriately and the dose given was reasonable. If a model was unable or refused to answer a question even after encouragement with two prompts, a score of "no valid response" was given. The conversation was saved to a database in plain text. When available, a URL for the conversation was saved. These conversations have been provided in an accessible format in our repository.

---

### NTCA guidelines for respiratory isolation and restrictions to reduce transmission of pulmonary tuberculosis in community settings [^776a9939]. Clinical Infectious Diseases (2024). High credibility.

Objectives of NTCA guidelines — the objective is to set forth a framework for public health decision-making regarding community-based RIR to reduce person-to-person TB transmission, incorporating current evidence on the potential infectiousness of PWTB, effectiveness of respiratory isolation to reduce community-level TB incidence, and whether individual patient outcomes are affected by RIR. Through an ethics-informed process, the guidelines include recommendations that aim to provide population benefit while maintaining consideration for the individual patient, may require evaluation and revision of local or state policies, procedures, laws, and health codes pertaining to TB RIR, and highlight the importance of an individualized, patient-centric approach. Considerations for isolation and public health restrictions may differ based on availability and feasibility of TB-elimination elements and public health program structure, including availability of treatment and diagnostic testing, implementation of contact evaluation, and provisions for preventive therapy.

---

### Toward a universal decoder of linguistic meaning from brain activation [^23c51f43]. Nature Communications (2018). Medium credibility.

Experiment 1 on single concept decoding

Our decoder is trained on brain activation patterns in each participant elicited by individual words and corresponding semantic vectors (Fig. 2a). The decoder uses the learned relationship to infer the degree to which each dimension is present in new activation patterns collected from the same participant, and outputs semantic vectors representing their contents (Fig. 2b). Each dimension is predicted using a different ridge regression, with parameters estimated from training data (see "Decoding methodology" in Methods).

Fig. 2
Decoder schematic. a The decoder is trained to take a brain image as input and output the corresponding text semantic vector, for many different image/vector pairs. b, c The decoder is applied to new brain images and outputs decoded semantic vectors, which are then evaluated against text semantic vectors. b A pairwise evaluation, which is correct if vectors decoded from two images are more similar to the text semantic vectors for their respective stimuli than to the alternative. c A rank evaluation, where the decoded vector is compared to the text semantic vectors for a range of stimuli

---

### Symbolic metaprogram search improves learning efficiency and explains rule learning in humans [^bded71d9]. Nature Communications (2024). High credibility.

To generate input/output pairs for each function, we randomly generated one million sets of 11 input/output pairs and selected the best according to a per-function custom scoring function. Input and outputs were restricted to contain 0 to 15 elements. The per-function scoring function always favored variance in input and output length, variance in the elements of the lists, a high number of unique outputs, and a low number of examples in which the input and output were identical. Each was then also customized to favor features relevant to the given concept. For example, a concept indexing the third element might favor inputs with three or more elements, while a concept using the first element as an index might favor lists in which the first element was less than or equal to the length of the list. After selecting a set of examples, we then generated five thousand random orderings and selected the one with the highest score based on: applying the per-concept scoring function to the first five pairs, applying the per-concept scoring function to the last six pairs, whether the input differed from the output in the first example, and the distance between 5 and the length of the first input.

---

### Assessing stigma toward mental, neurological, and substance use disorders in Liberia: a population representative study [^76385ce9]. The American Journal of Tropical Medicine and Hygiene (2024). Medium credibility.

The RIBS has eight questions and is based on the Star Social Distance Scale to assess reported (present and past) and intended (future) behavioral discrimination against people with mental health issues. Items 1–4 of the RIBS assesses self-reported behavior based on four themes (living with, working with, living nearby, and having a close friend experiencing a mental health problem). The binary response options are coded with 1 for "yes" and 0 for "no".

Our decision to include the RIBS was based on it being one of a few validated psychometrically tested instruments for testing behavioral discrimination and analyzing the presence of reported and intended stigmatizing or discriminatory behaviors against people with mental health problems in the general population. In addition, not only is it brief, feasible, and psychometrically robust for assessing mental health–related reported and intended behavioral discrimination, but it is also helpful for evaluating the effectiveness of interventions intended to reduce stigma and/or discrimination related to mental illness. –

---