# Medical Question & Answer

**Sample ID**: 6f3a41b0-bc59-069b-d894-0198f27ffdd0
**Dataset Index**: 178779

---

## Question

give outcomes in statistic descriptive manner

---

## Answer

> Let's see… What do we have here? The user is asking how to present outcomes in a statistically descriptive manner. Let's break this down step-by-step. First, I need to think about the foundational principle that descriptive statistics answer the "who, what, when, where, why, and how much" of the data. Then, I should verify how to classify outcome variables and choose appropriate measures of central tendency and dispersion. Next, I will review how to handle categorical versus continuous outcomes, including non-normal distributions and time-to-event data. After that, I need to check reporting conventions for proportions, means, medians, and confidence intervals. Finally, I will consider special cases like composite outcomes, patient-reported outcomes, and longitudinal data, and close with a concise template and quality checks to ensure clarity and reproducibility [^85777ffe] [^fd36261e].

> Let me first confirm the foundational principle: descriptive statistics are the first step to characterize the study population and outcomes, answering basic questions about who was studied, what was measured, when and where it was measured, why it matters, and how much variability exists, without inferential testing at this stage [^85777ffe] [^758c55f3].

> I need to classify outcomes carefully before choosing summaries. Continuous outcomes are numeric variables like blood pressure or pain scores; categorical outcomes are binary, nominal, or ordinal, such as death, hospitalization, or functional status categories; time-to-event outcomes include survival or time to recovery and require specialized descriptive approaches; patient-reported outcomes often behave as ordinal or bounded continuous variables and may violate assumptions of normality, so I should verify distribution and consider ordinal models when needed [^d67b279d] [^489c318e].

> For continuous outcomes, I should confirm whether the distribution is approximately normal. If so, I will report the mean with standard deviation; if skewed or with outliers, the median with interquartile range is more appropriate, and I should avoid pairing a nonrepresentative mean with an SD that assumes normality. Wait, let me verify that SD is indeed paired with the mean and IQR with the median; yes, that is the correct convention, and I should also note that some journals encourage reporting both absolute and relative effect sizes when relevant [^85777ffe] [^31bb9573] [^fd36261e].

> For categorical outcomes, I will report counts and proportions, ensuring the denominator is clear and that I distinguish row from column percentages when presenting cross-tabulations. Hold on, I should verify that I am not overinterpreting proportions without context; descriptive studies without a comparison group cannot infer causation, so I will keep the presentation strictly descriptive and avoid causal language [^8a915fa6] [^da18bbca].

> For time-to-event outcomes, I should confirm that Kaplan-Meier curves and median times with confidence intervals are the standard descriptive approach, and I need to report censoring patterns and the number at risk over time. Let me double-check that competing risks are acknowledged even if not modeled descriptively; yes, mentioning censoring and competing events is essential for transparency [^8c0b68f2].

> I should confirm reporting conventions for measures of central tendency and spread. Mean with SD for normal continuous data, median with IQR for skewed or ordinal data, and mode when relevant for categorical or multimodal distributions. But wait, what if the distribution is heavy-tailed or has extreme outliers? In that case, I should consider robust measures or transformations and explicitly state why I chose them, aligning with best practices for descriptive reporting [^85777ffe] [^31bb9573].

> Next, I should review how to present confidence intervals for descriptive estimates. Even when not testing hypotheses, 95% CIs for means, medians, or proportions communicate precision and are encouraged by reporting guidelines; I need to ensure I interpret CIs correctly as intervals within which the true population parameter would lie in repeated sampling, not as a test of significance [^fd36261e] [^c0a7d4a0].

> For composite outcomes, I must be careful. Descriptively, I should report the composite event rate and, crucially, the component-specific rates so readers can judge whether the composite effect is driven by more or less clinically important components. Hold on, let's not jump to conclusions about the composite without inspecting components; systematic reviews show that composites can obscure heterogeneous treatment effects across components, so transparent component reporting is essential [^255c93c0] [^55032e11].

> For patient-reported outcomes, I should confirm the instrument's psychometrics and consider anchor-based interpretation. Reporting mean or median change is insufficient without context; I should include the minimal clinically important difference or patient-acceptable symptom state when available, and I need to check whether the scale is ordinal and whether mean-based models are appropriate or whether ordinal models better preserve the data structure [^c9b2374a] [^489c318e].

> For longitudinal or repeated measures, I should verify whether I am summarizing within-subject trajectories or single endpoints. Descriptively, I can report summary statistics per subject and then aggregate, or present spaghetti plots or stacked probability plots to visualize time dynamics; I need to ensure I acknowledge within-subject correlation even if I am not modeling it explicitly in the descriptive phase [^15874075] [^bd9f6943].

> Let me consider a concise reporting template to keep this actionable. For each outcome, I will report the measure of central tendency with the appropriate measure of spread, the sample size and denominator, and a 95% CI when relevant; for categorical outcomes, I will report counts and percentages; for continuous outcomes, I will report mean with SD or median with IQR and justify the choice; for time-to-event, I will report median time with 95% CI and show the Kaplan-Meier curve; and I will explicitly note any missing data and how they were handled in the descriptive summaries [^fd36261e] [^85777ffe].

> Finally, I should double-check quality and transparency. I will prespecify outcomes and analysis plans when possible, report results for all measured outcomes to mitigate selective reporting, and provide enough detail that another investigator could reproduce the descriptive analyses. If I deviate from the plan, I will document and justify the change to maintain interpretability and trustworthiness [^d67b279d] [^e2ae1040].

---

Descriptive outcomes should be reported with **clear, concise statistics** that match the data type and distribution: use **mean ± SD** for normal continuous variables, **median (IQR)** for skewed or ordinal data, and **n (%)** for categorical outcomes [^85777ffe] [^4c1c82d3]. Always include **sample size** and **missing data**, and add **95% CIs** for key estimates to convey precision [^fd36261e] [^c0a7d4a0]. For longitudinal or repeated measures, report change from baseline and use appropriate summary statistics (e.g. mean change, AUC) [^15874075]. Avoid overinterpreting descriptive results; use them to characterize the sample and identify patterns, not to infer causality [^da18bbca].

---

## Continuous outcomes

Continuous outcomes are **reported with measures of central tendency and dispersion** [^85777ffe].

- **Normal distribution**: Mean ± standard deviation (SD) [^85777ffe].
- **Non-normal distribution**: Median with interquartile range (IQR) [^85777ffe].
- **Change from baseline**: Report mean or median change with SD or IQR, and consider percentage change if clinically meaningful [^85777ffe].

---

## Categorical outcomes

Categorical outcomes are **summarized with counts and proportions** [^85777ffe].

- **Binary outcomes**: n (%) with denominator and, when relevant, 95% CI [^d67b279d].
- **Ordinal outcomes**: n (%) for each category; consider reporting the median category if ordinal [^85777ffe].
- **Missing data**: Report the number and proportion of missing values [^d257c7db].

---

## Time-to-event outcomes

Time-to-event outcomes require **survival statistics** [^8c0b68f2].

- **Median time to event**: Report with 95% CI [^notfound].
- **Event rates**: Report the number of events and person-time at risk [^notfound].
- **Kaplan-Meier curves**: Visualize survival probabilities over time [^notfound].

---

## Longitudinal and repeated measures

For longitudinal data, **summarize within-subject changes** and use appropriate metrics [^15874075].

- **Mean change**: Average change from baseline with SD or 95% CI [^85777ffe].
- **Area under the curve (AUC)**: Summarizes repeated measures over time [^notfound].
- **Response rates**: Proportion achieving a clinically meaningful change (e.g. ≥ 50% improvement) [^a2b8d3f2].

---

## Patient-reported outcomes (PROs)

PROs should be reported with **anchor- or distribution-based interpretation** [^c9b2374a].

- **Mean or median scores**: Baseline and follow-up with change and variability [^85777ffe].
- **Minimal clinically important difference (MCID)**: Report the proportion achieving MCID [^c9b2374a].
- **Instrument-specific interpretation**: Use validated thresholds or responder definitions [^6fb31702].

---

## Composite outcomes

Composite outcomes should be **defined clearly** with **component-level detail** [^255c93c0].

- **Composite event rate**: n (%) with 95% CI [^notfound].
- **Component events**: Report each component separately to clarify drivers of the composite [^255c93c0].
- **Clinical relevance**: Discuss whether components have similar importance and direction of effect [^255c93c0].

---

## Reporting standards and best practices

Follow **reporting standards** to ensure clarity and reproducibility [^fd36261e].

- **CONSORT/STROBE**: Use structured reporting for trials and observational studies [^b8dae862].
- **Confidence intervals**: Report 95% CIs for key estimates to indicate precision [^c0a7d4a0].
- **Transparency**: Report all prespecified outcomes, even if not analyzed or significant [^d67b279d].

---

## Common pitfalls and how to avoid them

Avoid these **common mistakes**:

- **Misusing measures of spread**: Do not report SD with median or IQR with mean [^31bb9573].
- **Overinterpreting descriptive results**: Descriptive statistics do not imply causality [^da18bbca].
- **Incomplete reporting**: Always report sample size, missing data, and denominators [^fd36261e].

---

## Example descriptive outcomes table

| **Outcome** | **Statistic** | **Reported value** |
|-|-|-|
| Age (years) | Mean ± SD | 65.3 ± 12.1 |
| BMI (kg/m²) | Median (IQR) | 28.5 (24.3–32.1) |
| Female sex | n (%) | 245 (52.3%) |
| Mortality | n (%) | 45 (9.6%) |
| Time to recovery (days) | Median (95% CI) | 14 (12–16) |
| ≥ 50% pain reduction | n (%) | 178 (38.0%) |

---

Descriptive outcomes should be **presented with appropriate statistics** for the data type and distribution, including sample size, variability, and CIs where relevant. Use these summaries to characterize the sample and identify patterns, reserving inference for hypothesis-testing analyses.

---

## References

### The primary outcome measure and its importance in clinical trials [^e6d26e75]. The Journal of Clinical Psychiatry (2015). Low credibility.

The primary outcome measure is the outcome that an investigator considers to be the most important among the many outcomes that are to be examined in the study. The primary outcome needs to be defined at the time the study is designed. There are 2 reasons for this: it reduces the risk of false-positive errors resulting from the statistical testing of many outcomes, and it reduces the risk of a false-negative error by providing the basis for the estimation of the sample size necessary for an adequately powered study. This article discusses the setting of the primary outcome measure, the need for it, the increased risk of false-positive and false-negative errors in secondary outcome results, how to regard articles that do not state the primary outcome, how to interpret results when secondary outcomes are statistically significant but not the primary outcome, and limitations of the concept of a primary outcome measure in clinical trial research.

---

### Outcome measures and implications for sample-size calculations [^02e14953]. The Journal of Bone and Joint Surgery: American Volume (2009). Low credibility.

This article focuses on items to consider when selecting outcome measures for a clinical study. The choice of outcome measures depends largely on the research question and the study design. Sample-size requirements can vary greatly, depending on the type and the number of outcome measures selected. In this paper, we review the differences between categorical and continuous outcomes as well as the differences between primary and secondary outcomes and we discuss the concept of minimally important differences and the problems associated with composite outcomes. We also provide instruction on how to conduct and present a sample-size calculation.

---

### Methodological aspects of outcomes research [^3e94fa64]. European Spine Journal (2006). Low credibility.

A critical evaluation of existing scientific evidence of treatment efficacy can be an important part of communicating risk and benefits of treatment options to patients during the course of clinical practice. A checklist of key methodological issues to examine when reading a research study is presented and discussed. Steps in reading a paper include: identifying the research question; identifying the manner in which subjects get enrolled in the study; identifying the treatments and outcomes used; identifying the study design and the comparisons being made; evaluating the study methods for the possibility of bias and uncontrolled confounding; assessing whether the statistical analysis used is appropriate for the study design; assessing whether the study has sufficient statistical power to demonstrate hypotheses being tested. Finally, procedures for grading and evaluating evidence, as used by systematic review groups and international best evidence synthesis consensus groups is briefly described.

---

### Guidance on statistical reporting to help improve your chances of a favorable statistical review [^fd36261e]. American Journal of Respiratory and Critical Care Medicine (2020). Medium credibility.

Present Results in a Manner That Allows Readers to Easily Follow and Assess the Analysis

The structure of the results section and content of tables and figures (including those in the supplement) should follow a natural progression and sequentially present the analysis that has been outlined in the methods. As suggested above, the study-specific guidelines available at the EQUATOR Network website (Table 1) provide a natural structure for reporting results. For example, the STROBE checklist would structure the results of an observational cohort or cross-sectional study to begin with a summary of participant eligibility and data availability, followed by descriptive characteristics, a summary of the outcome or summary measures, the primary statistical analysis, and concluding with additional (e.g. sensitivity, exploratory) analyses. In addition, the SAMPL guidelines include specific guidance for reporting descriptive statistics, rates, risks, ratios, correlations, regression models, time-to-event analyses, and Bayesian analyses, among many others. If a study includes adjustment for confounding, we prefer that authors show both unadjusted and adjusted results. We also recommend that authors report results on both the absolute (e.g. risk or mean difference) and relative scales (e.g. risk ratio), when relevant. Predicted values (e.g. adjusted probabilities or marginal means) can also greatly aid the interpretation of an analysis and are easily derived in all popular statistical packages.

---

### Defining the primary outcomes and justifying secondary outcomes of a study: usually, the fewer, the better [^e970c373]. Anesthesia and Analgesia (2017). Low credibility.

One of the first steps in designing and conducting a research study is identifying the primary and any secondary study outcomes. In an experimental, quasi-experimental, or analytic observational research study, the primary study outcomes arise from and align directly with the primary study aim or objective. Likewise, any secondary study outcomes arise from and directly align with any secondary study aim or objective. One designated primary study outcome then forms the basis for and is incorporated literally into the stated hypothesis. In a Methods section, authors clearly state and define each primary and any secondary study outcome variable. In the same Methods section, authors clearly describe how all primary and any secondary study outcome variables were measured. Enough detail is provided so that a clinician, statistician, or informatician can know exactly what is being measured and that other investigators could duplicate the measurements in their research venue. The authors provide published substantiation (preferably) or other documented evidence of the validity and reliability of any applied measurement instrument, tool, or scale. A common pitfall-and often fatal study design flaw-is the application of a newly created ("home-grown") or ad hoc modification of an existing measurement instrument, tool, or scale-without any supporting evidence of its validity and reliability. An optimal primary outcome is the one for which there is the most existing or plausible evidence of being associated with the exposure of interest or intervention. Including too many primary outcomes can (a) lead to an unfocused research question and study and (b) present problems with interpretation if the treatment effect differed across the outcomes. Inclusion of secondary variables in the study design and the resulting manuscript needs to be justified. Secondary outcomes are particularly helpful if they lend supporting evidence for the primary endpoint. A composite endpoint is an endpoint consisting of several outcome variables that are typically correlated with each. In designing a study, researchers limit components of a composite endpoint to variables on which the intervention of interest would most plausibly have an effect, and optimally with preliminary evidence of an effect. Ideally, components of a strong composite endpoint have similar treatment effect, frequency, and severity-with the most important being similar severity.

---

### Core patient-reported outcomes in cancer clinical trials… [^3fc9d2af]. FDA (2024). Medium credibility.

- **Docket Number**:
- FDA-2020-D-2303
- Issued by: This guidance provides recommendations to sponsors for collection of a core set of patient-reported clinical outcomes in cancer clinical trials and related considerations for instrument selection and trial design. Although this guidance focuses on patient-reported outcome measures, some of these recommendations may be relevant to other clinical outcome assessments in cancer clinical trials. Submit Comments You can submit online or written comments on any guidance at any time) If unable to submit comments online, please mail written comments to: Dockets Management Food and Drug Administration 5630 Fishers Lane, Rm 1061 Rockville, MD 20852 All written comments should be identified with this document's docket number: FDA-2020-D-2303.

---

### Descriptive statistics: reporting the answers to the 5 basic questions of who, what, why, when, where, and a sixth, so what? [^85777ffe]. Anesthesia and Analgesia (2017). Low credibility.

Descriptive statistics are specific methods basically used to calculate, describe, and summarize collected research data in a logical, meaningful, and efficient way. Descriptive statistics are reported numerically in the manuscript text and/or in its tables, or graphically in its figures. This basic statistical tutorial discusses a series of fundamental concepts about descriptive statistics and their reporting. The mean, median, and mode are 3 measures of the center or central tendency of a set of data. In addition to a measure of its central tendency (mean, median, or mode), another important characteristic of a research data set is its variability or dispersion (ie, spread). In simplest terms, variability is how much the individual recorded scores or observed values differ from one another. The range, standard deviation, and interquartile range are 3 measures of variability or dispersion. The standard deviation is typically reported for a mean, and the interquartile range for a median. Testing for statistical significance, along with calculating the observed treatment effect (or the strength of the association between an exposure and an outcome), and generating a corresponding confidence interval are 3 tools commonly used by researchers (and their collaborating biostatistician or epidemiologist) to validly make inferences and more generalized conclusions from their collected data and descriptive statistics. A number of journals, including Anesthesia & Analgesia, strongly encourage or require the reporting of pertinent confidence intervals. A confidence interval can be calculated for virtually any variable or outcome measure in an experimental, quasi-experimental, or observational research study design. Generally speaking, in a clinical trial, the confidence interval is the range of values within which the true treatment effect in the population likely resides. In an observational study, the confidence interval is the range of values within which the true strength of the association between the exposure and the outcome (eg, the risk ratio or odds ratio) in the population likely resides. There are many possible ways to graphically display or illustrate different types of data. While there is often latitude as to the choice of format, ultimately, the simplest and most comprehensible format is preferred. Common examples include a histogram, bar chart, line chart or line graph, pie chart, scatterplot, and box-and-whisker plot. Valid and reliable descriptive statistics can answer basic yet important questions about a research data set, namely: "Who, What, Why, When, Where, How, How Much?"

---

### Measuring patient outcomes: a primer [^9526282a]. Injury (2011). Low credibility.

When designing a study, it is important to select appropriate instruments to measure health outcomes. An investigator must have a comprehensive understanding of the disease and its effects on patient health to inform instrument selection. We provide a brief introduction of different ways that health is defined and the properties of a good measurement tool (reliability, validity, sensitivity to change). We describe patient-reported outcomes (PROs) and methods to improve their interpretability, and we include strategies to reduce bias in health measurement and considerations that may improve the feasibility of measuring outcomes in a research study.

---

### Understanding clinical trial outcomes: design, analysis, and interpretation [^9250be4f]. Dermatologic Therapy (2007). Low credibility.

Outcomes (a variable intended for comparison between groups) are integral to the design, conduct, and data analysis of a clinical trial. They are broadly divided into four categories: physician based, patient reported, economic based, and technology based. Each is used in dermatology to some degree, but no consensus exists as to what type of outcome or degree of validation should be employed. This is problematical because poor quality outcomes or their incorrect use may invalidate the results of a clinical trial. Despite their importance, outcome measures in dermatology receive little attention. The present authors aim to provide an overview of important considerations for outcome measures and a practical approach to their analysis.

---

### Stakeholders in outcome measures: review from a clinical perspective [^ca1a86ce]. Clinical Orthopaedics and Related Research (2013). Low credibility.

Background

Modern interest in patient-reported outcomes measures (PROMs) in orthopaedics dates back to the mid-1980s. While gradual growth of activity in this area has occurred over the past 25 years, the extent to which this research methodology is applied in clinical practice to improve patient care is unclear. WHERE ARE WE NOW? Historically, clinical research in orthopaedics has focused on the technical success of treatment, and objective indicators such as mortality, morbidity, and complications. By contrast, the PROMs framework focuses on effects of treatment described in terms of relief of symptoms, restoring functional ability, and improving quality of life. PROMs can be used to study the relative effects of disease, injury, and treatment across different health conditions. WHERE DO WE NEED TO GO? All clinical research should begin with identifying clear and meaningful research questions so that the resources and efforts required for data collection result in useful data. Different consumers of research data have different perspectives on what comprises meaningful information. Involving stakeholders such as patients, providers, payers, and policy-makers when defining priorities in the larger research endeavor is one way to inform what type of data should be collected in a particular study. HOW DO WE GET THERE? Widespread collection of outcomes data would potentially aid these stakeholders by identifying best practices, benefits and costs, and important patient or practice characteristics related to outcomes. Several initiatives currently underway may help systematic collection of PROMs, create efficient systems, and foster collaborations to provide support and resources to minimize costs.

---

### CONSORT 2010 statement: extension to randomised pilot and feasibility trials [^edad8d0a]. BMJ (2016). Excellent credibility.

- Item 12a

- **Standard CONSORT item**: statistical methods used to compare groups for primary and secondary outcomes

- **Extension for pilot trials**: methods used to address each pilot trial objective whether qualitative or quantitative

- Example 1 (descriptive and narrative reporting)

"The feasibility outcomes were reported descriptively and narratively. For the clinical endpoints, only descriptive statistics, mean (standard deviation) for continuous outcomes and raw count (%) for categorical outcomes, were reported".

- Example 2 (confidence intervals)

"For the primary outcomes, the feasibility criteria were the recruitment rate and duration, retention rate, safety, adverse events, compliance, acceptability of the interventions and fatigue. The recruitment rate, consisting of the eligibility and consent rate, was calculated with 95% CI. Medians (range) were reported for ordinal data (fatigue), mean (95% confidence interval (CI)) were reported for continuous data (walking speed and walking distance) and raw count (number, %) was reported for nominal data. Due to the nature of this feasibility study, it was decided not to conduct any efficacy statistical tests on the walking and fatigue data".

- Explanation

A range of methods can be used to address the objectives in a pilot trial. These need not be statistical. Providing information about the methods used ensures that findings can be verified on the basis of the description of the analyses used. The primary focus is on methods for dealing with feasibility objectives. These methods are often based on descriptive statistics such as means and percentages but might also be narrative descriptions (example 1). Typically, any estimates of effect using participant outcomes as they are likely to be measured in the future definitive RCT would be reported as estimates with 95% confidence intervals without P values — because pilot trials are not powered for testing hypotheses about effectiveness.

- Item 12b

- **Standard CONSORT item**: methods for additional analyses, such as subgroup analyses and adjusted analyses

- **Extension for pilot trials**: not applicable

- Explanation

In a definitive trial, analyses of a difference in treatment effect for subgroups or analysis of outcomes adjusted for baseline imbalance might provide useful information. However, such analyses in a pilot trial are not applicable because the primary focus is not on determining treatment effects or differences in effects between subgroups. Rather, the focus is on assessing feasibility or piloting procedures to inform the design of the future definitive RCT.

---

### Statistical considerations in the psychometric validation of outcome measures [^378712ca]. Clinical Orthopaedics and Related Research (2013). Low credibility.

Background

The evaluation of the outcomes of total knee arthroplasty requires measurement tools that are valid, reliable, and responsive to change. However, the accuracy of any outcome measurement is determined by the validity and reliability of the instrument used. To ensure this accuracy, it is imperative that each instrument used in orthopaedics is free of biases leading to inaccurate estimates of treatment effects. WHERE ARE WE NOW? Many patient-derived outcome instruments have been developed and tested through the application of the standard assessments that form the basis of classical test theory: validity, reliability, and responsiveness. These assessments determine if the instrument reliably measures what it is intended to measure, and if it captures differences among groups of patients or changes over time. WHERE DO WE NEED TO GO? Thorough evaluation of the outcome instruments used in orthopaedics is a critical prerequisite for the continued improvement of effective patient care. Additional steps of psychometric testing that are sometimes overlooked include testing for differential item functioning (DIF) and the effects of the mode of administration of the outcome instrument. The use of suitable approaches to test for these potential sources of bias would facilitate the development of more robust outcome assessment in research and clinical practice. HOW DO WE GET THERE? Testing for DIF, including the effects of mode of administration, may be performed using several analytical approaches. This will allow optimal application of each outcome instrument with respect to patient characteristics, time and mode of the administration, and modification, as necessary.

---

### A review for clinical outcomes research: hypothesis generation, data strategy, and hypothesis-driven statistical analysis [^758c55f3]. Surgical Endoscopy (2011). Low credibility.

Analysis phase

Following the process described below will produce the Results section of the manuscript.

Step 1: univariate descriptive analysis

The univariate descriptive analysis describes the entire study population. It is called "univariate" analysis because the population is described one characteristic at a time: average age, proportion males, race, socioeconomic status, insurance status, location, and so on. This is important so that future readers can determine whether the study applies to their patients. Since this section is solely descriptive, no formal statistical testing is necessary or applicable. An example data table for a univariate analysis is presented in Table 1. A study that is only descriptive would likely end after this stage. Analytical studies will continue on through the next few steps.

Table 1
Example of a univariate/demographics table

IQR interquartile range

---

### Population-based applications and analytics using patient-reported outcome measures [^fa7ed2f2]. The Journal of the American Academy of Orthopaedic Surgeons (2023). Medium credibility.

Tools for Risk Adjustment and Prediction

Statistical models are used in prediction to understand the relationship between risk factors and outcomes and determine factors for risk adjustment. Statistical models can provide information on multiple factors simultaneously or factors without simple cut points where simple thresholds and decision support tools can quickly become cumbersome or fail. Regression models are commonly used to estimate the effect of risk factors on the likelihood of an outcome. These models are a simplified mathematical representation of how you think independent variables (also called risk factors or predictors) and dependent variables (also called outcomes) will relate to each other. Examples of predictors are patient characteristics, baseline measurements, and chronic conditions. Regression models can be used to determine which predictors meaningfully affect an outcome and produce coefficients that quantify the effect of that factor. Coefficients can then be applied to the factors for individual patients to predict their likelihood of an outcome.

---

### Medical abortion reporting of efficacy: the MARE guidelines [^4bc142bb]. Contraception (2016). Medium credibility.

MARE guidelines — blinding and statistical methods reporting — If done, who was blinded after assignment to interventions (for example, participants, care providers, those assessing outcomes) and how; if relevant, description of the similarity of interventions; statistical methods used to compare groups for primary and secondary outcomes; and methods for additional analyses, such as subgroup analyses and adjusted analyses.

---

### Contemporary outcome measures in acute stroke research: choice of primary outcome measure [^3b616c0f]. Stroke (2012). Low credibility.

Background and Purpose

The diversity of available outcome measures for acute stroke trials is challenging and implies that the scales may be imperfect. To assist researchers planning trials and to aid interpretation, this article reviews and makes recommendations on the available choices of scales. The aim is to identify an approach that will be universally accepted and that should be included in most acute trials, without seeking to restrict options for special circumstances.

Methods

The article considers outcome measures that have been widely used or are currently advised. It examines desirable properties for outcome measures such as validity, relevance, responsiveness, statistical properties, availability of training, cultural and language issues, resistance to comorbidity, as well as potential weaknesses. Tracking and agreement among outcomes are covered.

Results

Typical ranges of scores for the common scales are described, along with their statistical properties, which in turn influence optimal analytic techniques. The timing of recovery on scores and usual practice in trial design are considered.

Conclusions

The preferred outcome measure for acute trials is the modified Rankin Scale, assessed at 3 months after stroke onset or later. The interview should be conducted by a certified rater and should involve both the patient and any relevant caregiver. Incremental benefits at any level of the modified Rankin Scale may be acceptable. The modified Rankin Scale is imperfect but should be retained in its present form for comparability with existing treatment comparisons. No second measure should be required, but correlations with supporting scales may be used to confirm consistency in direction of effects on other measures.

---

### Presentation of continuous outcomes in randomised trials: an observational study [^bd86091d]. BMJ (2012). Excellent credibility.

Objective

To characterise the percentage of available outcome data being presented in reports of randomised clinical trials with continuous outcome measures, thereby determining the potential for incomplete reporting bias.

Design

Descriptive cross sectional study.

Data Sources

A random sample of 200 randomised trials from issues of 20 medical journals in a variety of specialties during 2007–09.

Main Outcome Measures

For each paper's best reported primary outcome, we calculated the fraction of data reported using explicit scoring rules. For example, a two arm trial with 100 patients per limb that reported 2 sample sizes, 2 means, and 2 standard deviations reported 6/200 data elements (1.5%), but if that paper included a scatterplot with 200 points it would score 200/200 (100%). We also assessed compliance with 2001 CONSORT items about the reporting of results.

Results

The median percentage of data reported for the best reported continuous outcome was 9% (interquartile range 3–26%) but only 3.5% (3–7%) when we adjusted studies to 100 patients per arm to control for varying study size; 17% of articles showed 100% of the data. Tables were the predominant means of presenting the most data (59% of articles), but papers that used figures reported a higher proportion of data. There was substantial heterogeneity among journals with respect to our primary outcome and CONSORT compliance.

Limitations

We studied continuous outcomes of randomised trials in higher impact journals. Results may not apply to categorical outcomes, other study designs, or other journals.

Conclusions

Trialists present only a small fraction of available data. This paucity of data may increase the potential for incomplete reporting bias, a failure to present all relevant information about a study's findings.

---

### Relationship between the asthma control test (ACT) and other outcomes: a targeted literature review [^eec3498b]. BMC Pulmonary Medicine (2020). Medium credibility.

Following identification of articles eligible for full-text review, the number of articles assessing each relationship, as well as the strength, significance, and direction of those relationships, was quantified. We also evaluated the extent to which the tested relationships differed per ACT score and/or asthma severity.

Articles describing a statistical relationship between two continuous variables were defined as reporting a "correlation"; statistical tests used in these studies included Pearson's chi-square test and the Spearman correlation test. Those that instead reported a trend between subgroups or over time, assessed by covariance tests, regression analysis, or empirical observation, were defined as reporting "associations"; in other words, these are assessments of the extent to which two measures co-vary in an expected manner (i.e. an improvement of one measure should represent an improvement in another outcome measure).

Data for each outcome of interest were extracted from the targeted studies by means of a formalized extraction grid. Bibliographic and methodologic details of the study, basic population characteristics, and ACT scores at baseline and later if applicable were extracted. For each outcome of interest that was examined for a relationship with ACT score, the following were included: a description of the outcome and its value; time of measurement (baseline/later); change from baseline; method used to test relationship and the outcome of the test; significance level (range, 95% confidence interval, or p -value); statistical significance of relationship (yes/no); and the direction of the relationship (positive/negative).

---

### Repeated measures designs and analysis of longitudinal data: If at first you Do not succeed-try, try again [^15874075]. Anesthesia and Analgesia (2018). Low credibility.

TECHNIQUES FOR ANALYZING REPEATED MEASURES: GENERAL OVERVIEW

In this tutorial, we broadly distinguish 3 classes of commonly used approaches for the analysis of repeatedly measured data:

The first class uses summary statistics to condense the repeatedly measured information to a single number per subject.
The second class is historically prevalent and comprises the repeated-measures ANOVA type of analysis.
The third class comprises more modern and flexible regression-based techniques that allow for correlated observations; these techniques can accommodate a wide range of outcome data, including continuous, categorical, and count data.

Note that in studies that do not specifically aim to follow outcomes over time, multiple observations from the same subjects may instead be included in the analysis. In such nonlongitudinal studies, it is often more of convenience than scientific interest that multiple measurements are obtained in the same subjects. For example, Rogge et alstudied the agreement between an invasive and a noninvasive blood pressure measurement technique in 29 patients undergoing bariatric operations, by analyzing a total of 6864 pairs of simultaneous invasive and noninvasive measurements. These repeated measurements clearly do not provide independent information, so the analysis adjusted for the nonindependence.

However, such analyses are fundamentally different from the techniques described here, and previous tutorials in this series have discussed this issue in the context of correlation and agreement studies. Likewise, studies involving repetitive observations of subjects until a certain outcome (eg, death) occurs require yet other techniques, and the analysis of such "time-to-event" data will be addressed in a subsequent tutorial.

Before describing the specific techniques, it bears worth noting that the statistical terminology concerning "dependent" and "independent" is somewhat confusing. The factors that are considered to have an effect on an outcome in a statistical model are commonly termed independent variables (or explanatory variables, predictor variables, or covariates), whereas the outcome variable is termed the dependent variable. This has nothing to do with dependent or independent in the sense whether observations of the outcome variable have been made in the same individuals. Most statistical analysis techniques assume observed or measured values of the dependent variable to be independent from each other, and this tutorial describes techniques that can be used when this key assumption is violated.

---

### Expert panel on integrated guidelines for cardiovascular health and risk reduction in children and adolescents: summary report [^a176b33a]. PES (2012). Medium credibility.

Quality criteria for assessment of individual randomized controlled trials — criteria needed for "Y" selection: Requirements for data collection and analysis include study protocol components where "Protocol described for all intervention components/regimens studied" and "Description of extra or unplanned treatments"; outcomes measurement where "Primary and secondary outcome(s)/end point(s) described" and "Primary and secondary outcomes(s)/end point(s) relevant to the objective"; adequate duration/follow-up with "Duration of intervention sufficient to detect meaningful effect on primary and secondary outcomes" and "Period of follow-up long enough for important outcome(s) to occur"; and statistical analysis elements including "Statistical analyses described", "Appropriate statistical test used and assumptions of test not violated", "Statistics reported with levels of significance and/or confidence intervals", "Intent-to-treat analysis of outcomes", "Adequate adjustment for effects of confounding factors that might have affected the outcomes", "Results/findings address statistical significance", and "Confidence interval or power calculations reported for null findings". Additional considerations include clinical significance where "Results/findings address clinical significance", discussion of findings where "Findings and implications discussed" and "Biases and study limitations identified, including assessment of how well an intervention was delivered", and adverse events reporting with "Safety outcomes/adverse events specifically reported" and "Appropriate sample size and duration to detect safety outcome(s)". For generalizability, "Study population is appropriate to answer research question" and "Intervention can be feasibly conducted in a general practice/routine/community setting", and outcomes where "Outcome(s)/end point(s) are associated with an increase or decrease in cardiovascular disease risk factor(s) or cardiovascular disease risk during childhood or adulthood" and "Outcome(s)/end point(s) can be feasibly measured in a general practice/routine/community setting"; the table notes " Consider all criteria listed; however, not all criteria must be met for a 'Y'.

---

### CONSORT-DEFINE explanation and elaboration: recommendations for enhancing reporting quality and impact of early phase dose-finding clinical trials [^62e4ae14]. EClinicalMedicine (2025). Medium credibility.

Explanation

The CONSORT statementand E&Eaddress the importance of describing statistical methods to analyse primary and secondary outcomes at the end of the trial. This CONSORT-DEFINE modified item extends this to require a similar description for statistical methods used for interim analyses, which might not necessarily include the comparison between different doses or interventions. Providing adequate information will ensure that the findings can be replicated based on the description of the methods used, enhancing transparency and interpretation. Providing information about the statistical software and packages (if applicable), including the version number, used for analyses (e.g. dose (de-)escalation decisions and biomarker analyses) is good practice to enable the key results of the trial to be reproduced.

There should be a detailed description of the statistical methods used to address the objectives (item 2b) of an EPDF trial with their prespecified adaptations (item 3a.1). These methods could be based on descriptive statistics (e.g. frequencies, percentages, and means) or narrative descriptions (e.g. a description of adverse events experienced), or statistical models, or a combination of both, used for interim analyses for dose (de-)escalation decisions or other trial adaptations; for example, determining the next participant's dose level or stopping the trial early for poor safety or futility. Methods using statistical models (item 3a.3) to analyse any adaptation outcomes (item 6a) should be detailed for reproducibility and transparency of the adaptation process and results. This may include statistical methods for safety monitoring or data-driven pharmacokinetic/pharmacodynamic modelling if either was used to inform prespecified adaptations. Authors should specify whether a frequentist or Bayesian framework was used and report indications of uncertainty (e.g. using confidence intervals or credible intervals) as appropriate. For Bayesian methods, it is recommended that details on the description of the prior distributions of model parameters be provided in accordance with item 3a.3.

Item 12a.2 [new] For the implemented adaptive design features, statistical methods used for estimation (e.g. safety, dose(s), treatment effects) and to make inferences

---

### Recommendations of the SUFU / AUGS / ICS female stress urinary incontinence surgical publication working group: a common standard minimum data set for the literature [^b8dae862]. Urogynecology (2024). High credibility.

Female SUI surgical studies — study design minimum standards: After utilizing the Delphi process, the working group stated that a detailed description of the following "should be required as a minimum standard" for manuscripts. Transparent description of study design — "The authors should clearly describe the methodological study design" (STANDARD). Criteria defined for success/failure/improved — "the reporting of criteria for both objective and subjective outcome measures, including levels of bother or improvement, should be included in all SUI manuscripts" (STANDARD). Inclusion/exclusion criteria explicitly defined — "The authors should summarize the details of the research protocol, including eligibility criteria" (STANDARD). CONSORT/STROBE content — "A 'Consolidated Standards of Reporting Trials' (CONSORT) or 'STrengthening the Reporting of OBservational studies in Epidemiology (STROBE)' checklists should be used for randomized controlled trials and observational studies, respectively" (STANDARD). Definition/diagnosis — "authors should explicitly define the criteria and diagnostic process for identifying SUI" and relevant outcomes and data sources "should be provided for transparency" (STANDARD). Statistical plan — "The authors must describe the pre-specified statistical plan and analysis" and, when applicable, methods for sample size determination, randomization, participant selection, and blinding "should be reported" (STANDARD).

---

### Minimal clinically important difference: a review of outcome measure score interpretation [^8196dbb8]. Rheumatic Diseases Clinics of North America (2018). Low credibility.

Clinicians, researchers, and outcome stakeholders have the crucial, albeit difficult, task of quantifying when a person or group experiences important change or difference on any given outcome measure, often in response to a specific intervention. The minimal clinically important difference (MCID) provides this quantified value of change/difference for a measure. There are many methods for MCID derivation, which can result in multiple values for the same measure. Thus, it is important for potential users of MCID values to be aware of the nuances of MCID development and cautions for interpreting values. This review outlines MCID-related definitions, methods, and guidelines.

---

### Guidelines for reporting outcomes in trial reports: the CONSORT… [^d67b279d]. JAMA Network (2022). Excellent credibility.

If the outcome will be treated as a continuous, categorical, or time-to-event variable For continuous variables, a measure of central tendency; for categorical and time-to-event data variables, proportion with an event and, if relevant, the specific cutoff values or categories compared When outcome measurements will be obtained. Fully reporting trial outcomes is important for replicating results, knowledge synthesis efforts, and preventing selective nonreporting of results. Composite outcome: A composite outcome consists of ≥ 2 component outcomes. Participants who have experienced any of the events specified by the components are considered to have experienced the composite outcome. 21, 22 CONSORT 2010: Consolidated Standards of Reporting Trials statement that was published in 2010. 2,
3.

This item expands on CONSORT 2010 statement checklist item 6a that recommends completely defining prespecified primary and secondary outcomes, and provides specific recommendations that mirror SPIRIT 2013 statement checklist item 12. 35 CONSORT-Outcomes 2022 extension checklist item 6a. 2 recommends describing each element of an outcome including its measurement variable, specific analysis metric, method of aggregation, and time point. A composite outcome consists of 2 or more component outcomes that may be related. Participants who have experienced any 1 of the defined component outcomes comprising the composite outcome are considered to have experienced the composite outcome.

21, 22 When used, composite outcomes should be prespecified, justified, and fully defined, 51 which includes a complete definition of each individual component outcome and a description of how those will be combined. In addition, outcomes that differ from the prespecified outcomes in measurement variable, analysis metric, method of aggregation, and analysis time point are not prespecified. For example, if the trial reports on treatment success rates at 12 months instead of the prespecified time point of 6 months in the trial protocol, the 12-month rate should be identified as an outcome that was not prespecified with the specific change explained. The addition of undeclared outcomes in trial reports is a major concern. 7, 55 Among 67 trials published in 5 high-impact CONSORT-endorsing journals, there were 365 outcomes added.

7 Less than 15% of the added outcomes were described.

---

### Quantifying clinically significant change: a brief review of methods and presentation of a hybrid approach [^a2b8d3f2]. The American Journal of Sports Medicine (2012). Low credibility.

Treatment outcome researchers in orthopaedics frequently report only tests of statistical significance between group means to evaluate the effectiveness of a given intervention. Although important in establishing that mean differences are not caused by chance, these methods do not reflect the extent to which an intervention produces improvements that are meaningful and represent a return to health. This is an issue that is often of great interest to patients and clinicians. Other methods use a percentage change in an outcome measure (eg, 25% reduction in pain score) to classify treatment responders but often do not indicate whether the treatment restored a patient to normal. Researchers have developed several indices that provide a metric for statistically defining the amount of change that patients consider to be important. In this article, we focus on the concept of "clinical significance" and the different methods that have been developed to define clinically significant change using statistics. We then present a hybrid method that can classify whether a patient has returned to normal function. We apply this method to real patient data to illustrate its use with different outcome instruments commonly used in orthopaedic sports medicine. We advocate that the addition of these methods to reports from clinical outcome studies can deepen our understanding of the impact of interventions on patients' lives.

---

### Definition, reporting, and interpretation of composite outcomes in clinical trials: systematic review [^255c93c0]. BMJ (2010). Excellent credibility.

Introduction

A composite outcome consists of two or more component outcomes. Patients who have experienced any one of the events specified by the components are considered to have experienced the composite outcome. The main advantages supporting the use of a composite outcome are that it increases statistical efficiency because of higher event rates, which reduces sample size requirement, costs, and time; it helps investigators avoid an arbitrary choice between several important outcomes that refer to the same disease process; and it is a means of assessing the effectiveness of a patient reported outcome that addresses more than one aspect of the patient's health status.

Unfortunately, composite outcomes can be misleading. This is especially true when treatment effects vary across components with very different clinical importance. For example, suppose a drug leads to a large reduction in a composite outcome of "death or chest pain". This finding could mean that the drug resulted in fewer deaths and less chest pain. But it is also possible that the composite was driven entirely by a reduction in chest pain with no change, or even an increase, in death.

Studies show that treatment effects often vary, and typically, the effect is smallest for the most important component and biggest for the less important components. Unless authors clearly present data for all components and take care in how they discuss composite findings, it is easy for readers to assume mistakenly that the treatment effect applies to all components. In this study, we systematically examined how composite outcomes were used and how well they were reported in recent randomised trials.

---

### Observational studies: practical tips for avoiding common statistical pitfalls [^d257c7db]. The Lancet Regional Health: Southeast Asia (2024). High credibility.

The core: statistical methods

In this section, I draw attention to some aspects that need to be considered in conducting the statistical analysis that are sometimes overlooked.

Start simple

Statistical analysis is at the core of research articles. Before jumping into data modelling, summary data descriptions should be provided in tables and figures. A strong description of statistics drives the modelling. Often, this seems trivial, and some researchers jump directly to modelling. Descriptive statistics highlight covariate distribution, which can point to categorisation (if a variable is highly skewed), to assess non-linearity, missing data (with imputation methods necessary), univariate association and testing if two (or more) groups are statistically different. It helps to provide subsections describing all the elements: the data set, outcomes, covariates and their characteristics, statistical modelling, and additional sensitivity analysis. Formulas and equations should also be provided, particularly when variants of known methods are used, and it is harder for the reader to grasp what was done.

Modelling approaches

Research questions, sampling, and data collected will drive the modelling approaches. The board class of regression modelling is the cornerstone of many statistical analyses for estimating the association between exposures and outcomes. Researchers often fit simple models, but they do not provide any checks to ensure that model assumptions have been evaluated. Fitting regression models do not end at the coefficients table.

Every model works based on specific assumptions; for example, residual analysis in linear regression models assesses that the model assumption has not been violated, and we can trust estimates. Similarly, in survival analysis, the Cox model requires the proportionality of the hazard to be true. Otherwise, a different model must be fitted.

Data collection drives the modelling choice; for example, data collected as clusters or by group, like individuals in different hospitals or regions, or nested data like children in class, schools and regions, will have to be analysed considering mixed effect (multilevel or hierarchical) models to account for this difference between schools or classes or regions, by including random effects. Furthermore, subgroup analysis can help identify Simpson's paradox.

---

### Reappraisal of ventilator-free days in critical care research [^dd555674]. American Journal of Respiratory and Critical Care Medicine (2019). Medium credibility.

Ventilator-free days (VFDs) are a commonly reported composite outcome measure in acute respiratory distress syndrome trials. VFDs combine survival and duration of ventilation in a manner that summarizes the "net effect" of an intervention on these two outcomes. However, this combining of outcome measures makes VFDs difficult to understand and analyze, which contributes to imprecise interpretations. We discuss the strengths and limitations of VFDs and other "failure-free day" composites, and we provide a framework for when and how to use these outcome measures. We also provide a comprehensive discussion of the different analytic methods for analyzing and interpreting VFDs, including Student's t tests and rank-sum tests, as well as competing risk regressions treating extubation as the primary outcome and death as the competing risk. Using simulations, we illustrate how the statistical test with optimal power depends on the relative contributions of mortality and ventilator duration on the composite effect size. Finally, we recommend a simple analysis and reporting framework using the competing risk approach, which provides clear information on the effect size of an intervention, a statistical test and measure of confidence with the ability to adjust for baseline factors and allow interim monitoring for trials. We emphasize that any approach to analyzing a composite outcome, including other "failure-free day" constructs, should also be accompanied by an examination of the components.

---

### Evaluating the credibility of anchor based estimates of minimal important differences for patient reported outcomes: instrument development and reliability study [^c9b2374a]. BMJ (2020). Excellent credibility.

Introduction

The role of the patient's perspective in clinical research has increased over the past 30 years. Questionnaires looking at health status from the patient's perspective — patient reported outcome measures — is an important strategy in determining the effect of interventions. Despite improvements in establishing their validity, reliability, and responsiveness, interpretation of outcome measures reported by patients is challenging.

Interpretability deals with how to determine differences in scores for patient reported outcome measures that constitute trivial, small but important, moderate, or large differences. To help in the design and interpretation of trials evaluating the effect of an intervention on patient reported outcomes, researchers developed a concept called the minimal important difference (MID). The MID provides a measure of the smallest change in an outcome measure that patients perceive as an important improvement or deterioration, and can be used as a reference point for judging the magnitude of treatment effects in clinical trials and systematic reviews.

The widely accepted optimal approach to establishing an MID for a patient reported outcome measure relates a score on the instrument to an independent measure — an external criterion or anchor — that is understandable and relevant to patients. The most widely used anchor is the patient's global rating of change, also referred to as a transition rating. An example of a typical transition rating question would be "Since last month when we started the new treatment, are you feeling better or worse and, if so, to what extent?", with responses of no change, small but important, moderate or large improvement, or worsening. Other anchors include measures of satisfaction, occurrence of an event, or other patient reported outcome measures assessing health status.

A second, but much less effective, approach to estimating the MID involves distribution based methods. These methods rely solely on the statistical characteristics of the study sample (eg, 0.5 standard deviation of scores for patient reported outcome measures) and fail to incorporate the patient's perspective.

---

### Reporting outcomes and outcome measures in open rhinoplasty: a systematic review [^a465ea97]. Aesthetic Surgery Journal (2020). Medium credibility.

Background

Comparative studies have shown little statistical difference in outcomes following rhinoplasty, demonstrating near equivalent results across all surgical techniques. Cross-study comparisons of these trials are difficult because variation in outcome reporting prevents statistical pooling and analysis.

Objectives

The authors sought to identify all outcomes and outcome measures used to evaluate postoperative results in rhinoplasty.

Methods

An extensive computerized database search of MEDLINE and EMBASE was performed; all trials involving n ≥ 20 patients, aged 18 years and older undergoing a primary, open rhinoplasty procedure, were included for review.

Results

Of the 3235 citations initially screened, 72 studies met the stated inclusion criteria. A total of 53 unique outcomes and 55 postoperative outcome measures were identified. Outcomes were divided into 6 unique domains: objective signs, subjective symptom severity, physical function related to activities of daily living, patient satisfaction, surgeon satisfaction, and quality of life. The identified outcome measures consisted of 5 nasal-specific, author-reported instruments; 5 nasal specific, patient-reported instruments; 5 patient-reported, generic instruments; and 40 author-generated instruments. Of the outcome measures identified, the Rhinoplasty Outcomes Evaluation, Sino-Nasal Outcome Test-22, and FACE-Q were the only instruments to demonstrate adequate validity, reliability, and responsiveness to change in patients who underwent a rhinoplasty procedure.

Conclusions

There is heterogeneity in the outcomes and outcome measures employed to assess postsurgical outcomes following rhinoplasty. A standardized core outcome set is urgently needed to make it possible for future investigators to compare results of various techniques in rhinoplasty surgery.

---

### Basics of statistics for primary care research [^61607a23]. Family Medicine and Community Health (2019). Medium credibility.

Foundational statistical techniques

Statistical analysis is a method of aggregating numeric data and drawing inferences about variables. Statistical procedures may be broadly classified into (1) statistics that describe data — descriptive statistics; and (2) statistics that make inferences about more general situations beyond the actual data set — inferential statistics.

Descriptive statistics

Descriptive statistics aggregate data that are grouped into variables to examine typical values and the spread of values for each variable in a data set. Statistics summarising typical values are referred to as measures of central tendency and include the mean, median and mode. The spread of values is represented through measures of variability, including the variance, SD and range. Together, descriptive statistics provide indicators of the distribution of data, or the frequency of values through the data set as in a histogram plot. Table 1 summarises commonly used descriptive statistics. For consistency, I use the terms independent variable and dependent variable, but in some fields and types of research such as correlational studies the preferred terms may be predictor and outcome variable. An independent variable influences, affects or predicts a dependent variable.

Table 1
Descriptive statistics

---

### Reporting and interpretation of SF-36 outcomes in randomised trials: systematic review [^f91e9f4e]. BMJ (2009). Excellent credibility.

Discordant results

For the primary efficacy outcome and for each of the presented SF-36 assessments we recorded whether the difference between compared arms was statistically significant (P < 0.05) favouring the experimental arm, non-statistically significant, or statistically significant favouring the control arm. For trials with more than two arms we considered the comparison of each experimental intervention against control separately. We considered all comparisons and also present results separately for superiority and non-inferiority trials.

Data on SF-36 outcomes were extracted for the reported analyses that corresponded as closely as possible to the same time points as for primary outcome data. Specifically, when measurements for primary or SF-36 outcomes were carried out at several time points, for primary efficacy outcomes we preferred analyses accounting for multiple measurements (for example, repeated measurement analysis) than analyses of single time points. If the primary outcome was a time to event analysis or incorporated serial longitudinal measurements, we preferred the analysis of serial longitudinal SF-36 measurements; if this was unavailable, we recorded whether there was formal statistically significant difference at any time points when SF-36 had been appraised. When the primary outcome was appraised at a single time point, we recorded the SF-36 outcomes at the single same (or closest) time point. In two comparisons where co-primary outcomes existed and could not be prioritised, we based the evaluation of statistical significance on overall authors' interpretation.

We considered SF-36 results as statistically significant when at least one of the composite or subdomain scores showed a statistically significant result in favour or against the experimental intervention. There were no situations where some of the specific SF-36 scores were significant for the experimental intervention and others were significant against.

For statistically significant SF-36 effects when the respective primary efficacy outcome was discordant, we extracted information on the effect size of SF-36. Roughly, standardised mean differences of less than 0.30 standard deviations are small effects, 0.30–0.80 are moderate, and more than 0.80 are large. The corresponding cut-offs for raw scores are less than 4, 4–10, and more than 10 points.

For comparisons with discordant statistical significance on SF-36 and primary outcome results, we recorded whether the authors had discussed the SF-36 results at all, whether they commented on the discrepancy and if so with what arguments, and if SF-36 findings changed the interpretation of the trial results.

---

### Conscientious use of patient-reported outcome measures in supportive care [^489c318e]. Supportive Care in Cancer (2023). Medium credibility.

Purpose

Patient-reported outcome measures (PRO) are critical tools to developing an understanding of cancer patients' experience. This paper presents some of the lesser-understood implications of using patient-reported outcome measures in clinical research.

Methods

This study uses a combination of literature sources, real-world examples from supportive care studies, and statistical simulations to demonstrate the operating characteristics of patient-reported measures.

Results

It is demonstrated that care must be taken in the analysis of PROs as the assumptions of the most common mean-based approaches are often violated including linearity, normally distributed errors, interference with asymptotic convergence via boundary values, and more. Further, the implications of subjective discretization are shown to reduce the apparent statistical power of PRO-based studies.

Conclusions

PRO-based studies must be designed conscientiously as each PRO item will demonstrate a varying degree of subjectivity in a given population. Sample sizes of randomized studies using PROs must be inflated to account for this. Analyses should consider using ordinal statistical models until such time as the assumptions of mean-based models can be verified.

---

### Reporting of numerical and statistical differences in abstracts: improving but not optimal [^40f51909]. Journal of General Internal Medicine (2002). Low credibility.

Objective

The reporting of relative risk reductions (RRRs) or absolute risk reductions (ARRs) to quantify binary outcomes in trials engenders differing perceptions of therapeutic efficacy, and the merits of P values versus confidence intervals (CIs) are also controversial. We describe the manner in which numerical and statistical difference in treatment outcomes is presented in published abstracts.

Design

A descriptive study of abstracts published in 1986 and 1996 in 8 general medical and specialty journals.

Inclusion Criteria

controlled, intervention trials with a binary primary or secondary outcome. Seven items were recorded: raw data (outcomes for each treatment arm), measure of relative difference (e.g. RRR), ARR, number needed to treat, P value, CI, and verbal statement of statistical significance. The prevalence of these items was compared between journals and across time.

Results

Of 5,293 abstracts, 300 met the inclusion criteria. In 1986, 60% of abstracts did not provide both the raw data and a corresponding P value or CI, while 28% failed to do so in 1Dr. Hux is a Career Scientist of the Ontario Ministry of Health and receives salary support from the Institute for Clinical Evaluative Sciences in Ontario.996 (P < .001; RRR of 53%; ARR of 32%; CI for ARR 21% to 43%). The variability between journals was highly significant (P < .001). In 1986, 100% of abstracts lacked a measure of absolute difference while 88% of 1996 abstracts did so (P < .001). In 1986, 98% of abstracts lacked a CI while 65% of 1996 abstracts did so (P < .001).

Conclusions

The provision of quantitative outcome and statistical quantitative information has significantly increased between 1986 and 1996. However, further progress can be made to make abstracts more informative.

---

### Common errors in statistics and methods [^31bb9573]. BMJ Paediatrics Open (2024). High credibility.

Miscellaneous

Inappropriate descriptive statistics

Just as linear regression is not the only method of regression for a continuous outcome, the mean and median are not the only measures of central tendency. There are also trimmed and winsorized means, geometric means, harmonic means and more. Sometimes, though, there is no good single numerical measure of central tendency, and you need something like a five-number or seven-number summary or a density plot.

Another example is measures of spread that do not match measures of central tendency. Often, authors will recognise that the mean is not the ideal measure of central tendency for a variable. But sometimes, they give the SD anyway. The SD depends on the mean, so, if the mean is not a good measure, the SD cannot be either. Alternative measures of spread include the range and IQR, quartiles and percentiles.

Odds versus risk

Odds is not the same as risk and an OR is not the same as a risk ratio.

When the outcome is relatively common, these two measures can be very different from each other, so while either one can be used in most scenarios, it is important not to confuse the two and to ensure that the reader understands the distinction. It may also be worth reporting summary statistics such as the risk difference or the number needed to treat. An example of the serious consequences of the misinterpretation of odds as risk was the 'pill scare' in the UK in 1995. It was caused by medical professionals and lay people misinterpreting the meaning of an almost twofold increase in risk of venous thrombosis, which sounds dramatic, but was an increase in risk from approximately 1 in 7000 to 2 in 7000. Because of this misinterpretation, many women stopped taking the contraceptive pill, and one consequence of this failure to understand was an increase in pregnancy rates (and pregnancy is associated with a higher risk of thrombosis) and abortion rates in the UK.

Lack of open access protocol to check reporting bias.

Most large clinical studies, and certainly all randomised clinical trials, should have a previously published study protocol, either in a peer-reviewed journal, repository or other stable online location. Systematic reviews and meta-analyses should be registered within registers such as PROSPERO or the Open Science Framework and for larger projects, publication of protocols may also be appropriate. Information on how to access study registration and/or protocols should be provided for all subsequent reports.

---

### An official American Thoracic Society workshop report: developing performance measures from clinical practice guidelines [^7fffb60b]. Annals of the American Thoracic Society (2014). Medium credibility.

Grading of Recommendations Assessment, Development, and Evaluation (GRADE) evidence quality assessment — Under GRADE, the quality of evidence is influenced by "study design, risk of bias, consistency, precision, directness, and publication bias", with randomized trials "presumed to be high in quality, unless compromised", and observational studies "considered to be low in quality by default but can be 'rated up' by demonstrating a large magnitude of effect, a dose–response, or results that would be even more compelling if plausible confounders are taken into account". Table factors for rating down include "Risk of bias", "Heterogeneity", "Imprecision", "Indirectness", and "Publication bias", and for rating up include "Magnitude of effect", "Dose–response", and "Plausible confounding effect", with example comments such as "Unclear allocation concealment, incomplete blinding, selective reporting of results, incomplete accounting of outcomes", "Inconsistent results across studies", "Confidence intervals range from substantial benefit to little or no benefit", and "Studies showing negative or conflicting results may exist but are missing from the literature".

---

### Reporting of effect size and confidence intervals in JAMA otolaryngology-head&neck surgery [^6ff2fd74]. JAMA Otolaryngology — Head & Neck Surgery (2017). Medium credibility.

Importance

Effect sizes and confidence intervals (CIs) are critical for the interpretation of the results for any outcome of interest.

Objective

To evaluate the frequency of reporting effect sizes and CIs in the results of analytical studies.

Design, Setting, and Participants

Descriptive review of analytical studies published from January 2012 to December 2015 in JAMA Otolaryngology-Head & Neck Surgery.

Methods

A random sample of 121 articles was reviewed in this study. Descriptive studies were excluded from the analysis. Seven independent reviewers participated in the evaluation of the articles, with 2 reviewers assigned per article. The review process was standardized for each article; the Methods and Results sections were reviewed for the outcomes of interest. Descriptive statistics for each outcome were calculated and reported accordingly.

Main Outcomes and Measures

Primary outcomes of interest included the presence of effect size and associated CIs. Secondary outcomes of interest included a priori descriptions of statistical methodology, power analysis, and expectation of effect size.

Results

There were 107 articles included for analysis. The majority of the articles were retrospective cohort studies (n = 36 [36%]) followed by cross-sectional studies (n = 18 [17%]). A total of 58 articles (55%) reported an effect size for an outcome of interest. The most common effect size used was difference of mean, followed by odds ratio and correlation coefficient, which were reported 17 (16%), 15 (13%), and 12 times (11%), respectively. Confidence intervals were associated with 29 of these effect sizes (27%), and 9 of these articles (8%) included interpretation of the CI. A description of the statistical methodology was provided in 97 articles (91%), while 5 (5%) provided an a priori power analysis and 8 (7%) provided a description of expected effect size finding.

Conclusions and Relevance

Improving results reporting is necessary to enhance the reader's ability to interpret the results of any given study. This can only be achieved through increasing the reporting of effect sizes and CIs rather than relying on P values for both statistical significance and clinically meaningful results.

---

### Understanding variability: the role of meta-analysis of variance [^a5ed2088]. Psychological Medicine (2024). Medium credibility.

Meta-analyses traditionally compare the difference in means between groups for one or more outcomes of interest. However, they do not compare the spread of data (variability), which could mean that important effects and/or subgroups are missed. To address this, methods to compare variability meta-analytically have recently been developed, making it timely to review them and consider their strengths, weaknesses, and implementation. Using published data from trials in major depression, we demonstrate how the spread of data can impact both overall effect size and the frequency of extreme observations within studies, with potentially important implications for conclusions of meta-analyses, such as the clinical significance of findings. We then describe two methods for assessing group differences in variability meta-analytically: the variance ratio (VR) and coefficient of variation ratio (CVR). We consider the reporting and interpretation of these measures and how they differ from the assessment of heterogeneity between studies. We propose general benchmarks as a guideline for interpreting VR and CVR effects as small, medium, or large. Finally, we discuss some important limitations and practical considerations of VR and CVR and consider the value of integrating variability measures into meta-analyses.

---

### Statistical analysis of noncommensurate multiple outcomes [^9135a51a]. Circulation: Cardiovascular Quality and Outcomes (2011). Low credibility.

Many studies collect multiple outcomes to characterize treatment effectiveness or evaluate risk factors. These outcomes tend to be correlated because they are measuring related quantities in the same individuals, but the common approach used by researchers is to ignore this correlation and analyze each outcome separately. There may be advantages to consider the simultaneous analysis of the outcomes using multivariate methods. Although the joint analysis of outcomes measured in the same scale (commensurate outcomes) can be undertaken with standard statistical methods, outcomes measured in different scales (noncommensurate outcomes), such as mixed binary and continuous outcomes, present more difficult challenges. In this article, we contrast some statistical approaches to analyze noncommensurate multiple outcomes. We discuss the advantages of a multivariate method for the analysis of noncommensurate outcomes, including situations of missing data. A real data example from a clinical trial, comparing bare-metal with sirolimus-eluting stents, is used to illustrate the differences between the statistical approaches.

---

### Are well functioning civil registration and vital statistics systems associated with better health outcomes? [^60010f58]. Lancet (2015). Excellent credibility.

In this Series paper, we examine whether well functioning civil registration and vital statistics (CRVS) systems are associated with improved population health outcomes. We present a conceptual model connecting CRVS to wellbeing, and describe an ecological association between CRVS and health outcomes. The conceptual model posits that the legal identity that civil registration provides to individuals is key to access entitlements and services. Vital statistics produced by CRVS systems provide essential information for public health policy and prevention. These outcomes benefit individuals and societies, including improved health. We use marginal linear models and lag-lead analysis to measure ecological associations between a composite metric of CRVS performance and three health outcomes. Results are consistent with the conceptual model: improved CRVS performance coincides with improved health outcomes worldwide in a temporally consistent manner. Investment to strengthen CRVS systems is not only an important goal for individuals and societies, but also a development imperative that is good for health.

---

### Developing, analyzing, and interpreting outcome assessments to improve endpoints in infectious diseases clinical research [^ea047052]. Clinical Infectious Diseases (2025). Medium credibility.

Evidence from clinical research studies, in addition to patient values and clinical experience, helps practitioners best apply medical interventions to clinical practice. Part of evaluating the evidence is understanding the outcomes used as endpoints in research studies. The most direct evidence of patient benefit comes from endpoints that evaluate the effects of interventions on patient survival, symptoms, and function, and the impact of symptoms and function on health-related quality of life. In this article, we discuss how to develop endpoints for clinical research studies focusing on patient-reported outcome measures, including those measuring patient symptoms, function, and health-related quality of life. We discuss the process for determining (1) which concepts to measure, including which concepts are most relevant to patients (content validity); (2) who performs the measurement and which measurement properties to use (construct validity); and (3) how to analyze and interpret the results obtained using the chosen measurements.

---

### Evaluation of pragmatic oxygenation measurement as a proxy for COVID-19 severity [^c5499545]. Nature Communications (2023). High credibility.

Choosing optimal outcome measures maximizes statistical power, accelerates discovery and improves reliability in early-phase trials. We devised and evaluated a modification to a pragmatic measure of oxygenation function, the [Formula: see text] ratio. Because of the ceiling effect in oxyhaemoglobin saturation, [Formula: see text] ratio ceases to reflect pulmonary oxygenation function at high [Formula: see text] values. We found that the correlation of [Formula: see text] with the reference standard ([Formula: see text]/[Formula: see text] ratio) improves substantially when excluding [Formula: see text] and refer to this measure as [Formula: see text]. Using observational data from 39,765 hospitalised COVID-19 patients, we demonstrate that [Formula: see text] is predictive of mortality, and compare the sample sizes required for trials using four different outcome measures. We show that a significant difference in outcome could be detected with the smallest sample size using [Formula: see text]. We demonstrate that [Formula: see text] is an effective intermediate outcome measure in COVID-19. It is a non-invasive measurement, representative of disease severity and provides greater statistical power.

---

### Comparison of registered and published primary outcomes in randomized controlled trials of orthopaedic surgical interventions [^a2d6bf1d]. The Journal of Bone and Joint Surgery: American Volume (2016). Low credibility.

Background

The selective reporting of a subset of the outcomes that had been originally reported to a registry is a potential threat to the validity of evidence-based medicine. The extent of selective reporting has not been described for randomized controlled trials (RCTs) assessing the effectiveness of orthopaedic surgical interventions. The objective of this study was (1) to determine the percentage of orthopaedic surgical RCTs published in high-impact orthopaedic journals that were reported to have been registered, (2) to evaluate the consistency between the primary outcome measures recorded in the registry and those reported in the article, and (3) to evaluate whether selective reporting favored statistically significant outcomes.

Methods

We searched PubMed for articles on RCTs assessing orthopaedic surgical interventions indexed from January 2010 through December 2014 and published in the ten orthopaedic journals with the highest impact factors. For every article in which the authors reported registration of the RCT, we extracted the number and nature of the outcome measures from the article and the corresponding information from the registry. We then evaluated the consistency between the primary outcome measures reported in the registry and those reported in the published article. Moreover, we evaluated whether selective reporting favored statistically significant outcomes.

Results

Of the 362 articles on orthopaedic surgical RCTs, ninety (24.9%) reported that the RCT had been registered and thirty-four (37.8%) of the ninety had been registered adequately (registered before the study end with a clear description of the primary outcome measure and its time frame, with no substantial change after the study end). Twenty-six reports were eligible for our evaluation of the consistency between the registered primary outcome measures and those reported in the published article. This analysis identified one or multiple major discrepancies for fourteen articles, eight of which favored statistically significant results.

Conclusions

Few articles on orthopaedic surgical RCTs reported registration of the trial, and even fewer of these trials were registered adequately. Inconsistencies between registered primary outcome measures and those reported in the published articles, as well as selective outcome reporting favoring statistically significant outcomes, were prevalent.

Clinical Relevance

Although trial registration is now the rule, it is currently far from optimal for orthopaedic surgical RCTs, and selective outcome reporting is prevalent. Full involvement of authors, editors, and reviewers is necessary to ensure publication of quality, unbiased results.

---

### An official American Thoracic Society workshop report: developing performance measures from clinical practice guidelines [^369e6d46]. Annals of the American Thoracic Society (2014). Medium credibility.

Agency for Healthcare Research and Quality performance measure attributes — Desirable features of performance measures include relevance to stakeholders and addressing important aspects of health, with evidence of a need for the measure; evidence should be explicitly stated, results should be reproducible and truly measure what they purport to measure, and specifications should explicitly define the numerator and denominator with understandable data collection requirements; necessary data sources should be available within the measurement timeframe and data collection costs justified by potential improvement in care or health.

---

### Choosing the appropriate measure of central tendency: mean, median, or mode? [^d99b58e6]. Knee Surgery, Sports Traumatology, Arthroscopy (2023). Medium credibility.

Mean, median, and mode are among the most basic and consistently used measures of central tendency in statistical analysis and are crucial for simplifying data sets to a single value. However, there is a lack of understanding of when to use each metric and how various factors can impact these values. The aim of this article is to clarify some of the confusion related to each measure and explain how to select the appropriate metric for a given data set. The authors present this work as an educational resource, ensuring that these common statistical concepts are better understood throughout the Orthopedic research community.

---

### The effects of increased dose of exercise-based therapies to enhance motor recovery after stroke: a systematic review and meta-analysis [^a1141752]. BMC Medicine (2010). Low credibility.

Data Extraction

Data extraction was undertaken by two reviewers independently using a predesigned form. Any disagreements were resolved by discussion and referral to the original full papers. Trial authors were contacted to clarify results when this was necessary and possible. Data was extracted on:

- Trial design, sample size and attrition;

- Participant characteristics' e.g. age, gender, site of lesion, stroke classification;

- Type of interventions;

- Dose of interventions (sum of treatment hours);

- Measures made at outcome (end of intervention period) and follow-up time-points in terms of average scores for trial groups.

Statistical analysis of outcome and follow-up data

Analysis was undertaken, where possible, on an intention- to-treat basis. Trials were not excluded if data was unavailable for subjects who did not complete all the outcome measures. Data analysis was undertaken using the Cochrane statistical package RevMan 4.2.

Effect sizes were calculated as odds ratios (OR) and 95% confidence interval (CI) for dichotomous outcomes and as weighted mean differences (WMD and 95% CI) for continuous outcomes. WMDs were determined initially using a Fixed Effect Model. Where two or more trials had used the same outcome measure, however, and if there was evidence of heterogeneity, the WMDs were estimated from a Random Effects Model. Where it was not possible to combine and compare the outcome measures reported in different trials, then statistical results were described and tabulated individually. Sub-groups were formed by each follow-up time point. No overall analysis was done since this would involve combining subgroups based on the same individuals and could bias the results.

Synthesis and interpretation

The results of the statistical analysis were interpreted with reference to the risk of bias in trials, and comparability of participants, types of interventions and dose of interventions.

---

### Definition, reporting, and interpretation of composite outcomes in clinical trials: systematic review [^55032e11]. BMJ (2010). Excellent credibility.

Content analysis

Two pairs of independent observers used standardised protocols to assess the definition and quality of reporting of the composite outcomes. Most judgments involved assessments of facts (such as whether the number of components making up the composite changed within the paper). Here, disagreement was almost entirely due to oversight, not a difference in opinion. For the few subjective judgments, we created simple and explicit rules to objectify the process as much as possible. For example, we judged the conclusion of abstracts as falsely suggesting that an effect on the composite also applied to the most important component (when it did not) if all components were listed using "and" or if the composite was named as a class of events. Our rules are provided when we present results. Also, we provide examples to allow readers to decide for themselves whether our judgments were reasonable. We resolved discrepancies involving facts and disagreements by discussion.

Composite definition Two observers (PCG, and LS or SW) independently and blinded to the results selected the most important component of each composite outcome, taking into account the hierarchy for analysing composite outcomes proposed by Lubsen et al, and always choosing death (or disease specific death) if such a component had been used. The observers also rated the gradient of importance for components, and looked for any discussion of the rationale for the composite.

Reporting of composite We assessed the consistency of the components of the composite between the abstract, methods, and results; determined whether data were reported for all components (that is, so that they could be used in a meta-analysis); judged whether the components were of similar importance; and evaluated whether the conclusion presented in the abstract or the discussion section suggested that the intervention was effective for all the components of the composite outcome rather than just for the composite.

Data analysis

We present descriptive statistics and used Fisher's exact test for analysis of binary data. We had planned to estimate an average inflation factor, based on a comparison of the effect for the composite outcome and that for the most clinically important outcome, but realised that this was problematic (see Discussion).

---

### Practice guidelines for obstetric anesthesia: an updated report by the American society of anesthesiologists task force on obstetric anesthesia and the Society for Obstetric Anesthesia and Perinatology [^c29f1235]. Anesthesiology (2016). Medium credibility.

State of the literature — evidence review methods — describe that updated searches covered an 11-yr period from January 1, 2005 to July 31, 2015, with new citations combined with pre-2005 articles resulting in a total of 478 articles that contained direct linkage-related evidence. Outcomes were classified by evidence category and level and designated as either beneficial, harmful, or equivocal, and literature pertaining to 13 evidence linkages contained enough studies with well-defined experimental designs and statistical information sufficient to conduct meta-analyses. For quantitative synthesis, general variance-based effect-size estimates or combined probability tests were obtained for continuous outcome measures, and Mantel–Haenszel odds ratios were obtained for dichotomous outcome measures.

---

### Reporting and interpretation of SF-36 outcomes in randomised trials: systematic review [^74011bd2]. BMJ (2009). Excellent credibility.

We should acknowledge some caveats. Firstly, by selecting high impact journals we identified trials with high visibility and probably also high quality. It is unlikely that this strategy would have selected for discordant results between outcomes. Secondly, selective analysis and reporting bias may affect primary outcomes and not just SF-36, but this should not have increased the perceived rate of discrepancies between outcomes. Thirdly, discordance at the level of statistical significance does not necessarily mean that results for different outcomes differ beyond chance. Among statistically significant results, chance findings and non-clinically important differences are possible, and primary outcomes should be given more weight in the discussion than secondary outcomes. Given that trials are typically powered to address the primary outcome, a significant result in the primary outcome with a non-significant result in quality of life or health survey assessments may sometimes simply reflect lack of power for the quality of life or health survey outcome. Therefore we also examined the SF-36 effect sizes and the circumstances and discussion of discordant results. Fourthly, we did not carry out the same in-depth evaluation for trials where efficacy and SF-36 outcomes were concordant. It is unlikely that authors would then have modified their inferences, but SF-36 may have strengthened the conclusions. Finally, we did not examine trials using only other quality of life or health survey instruments beyond SF-36. However, SF-36 is the most robustly standardised and widely used one, and we wanted to maximise comparability. Although other scales may also be used, one study found that only 4.2% of trials reported any quality of life outcome.

Although quality of life and health survey scales have been used in clinical trials for over 25 years, several issues remain debated. Besides problems of fragmented, selectively reported information, it is sometimes impossible to say whether and which analyses are based on a priori analytical plans. Proper attention to the importance of these outcomes should be given in clinical trials. Otherwise, with a growth in the clinical trials' administrative paperwork, outcomes such as SF-36 may become routine compulsory assessments without a genuine interest to learn from them.

Overall, quality of life and health survey assessments provide a different window into patient outcomes and deserve to be included in more trials with complete reporting of results, and standardised interpretation. Unbiased data on these outcomes may enhance our ability to improve clinical decision making.

---

### Outcome selection and role of patient reported outcomes in contemporary cardiovascular trials: systematic review [^0c9ae332]. BMJ (2010). Excellent credibility.

Statistical analyses

We used descriptive statistics to characterise the included trials. Univariate analyses (χ² test, Fisher's exact test, and Mantel-Haenszel test for trend) explored associations of trial characteristics and type of outcome (patient important: yes or no) and measurement of patient reported outcomes. In the first 50 reviewed trials the prevalence for use of patient reported outcomes was 14%. We therefore estimated that data from about 400 trials would provide sufficient information for assessment of associations between trial features and type of outcomes selected. To allow for several ineligible trials and duplicate reports, we set a target of 500 articles to be reviewed in full text.

Excluding the first 100 trials that were evaluated in the pilot phase for development of the ranking tool, we used information from the next set of randomly selected trials, independently extracted by two reviewers (KR and AM), to estimate the chance corrected inter-rater reliability к. We weighted the outcomes to account for the closeness of different categories, as shown in the matrix (see web extra appendix 3). Assessment of inter-rater reliability based on the final 330 reports showed a substantial chance corrected inter-rater agreement between the two investigators: weighted к = 0.79; quadratic weighted к = 0.77. All analyses were carried out using STATA v 9.1.

---

### Critical review of published microarray studies for cancer outcome and guidelines on statistical analysis and reporting [^970a34df]. Journal of the National Cancer Institute (2007). Low credibility.

Background

Both the validity and the reproducibility of microarray-based clinical research have been challenged. There is a need for critical review of the statistical analysis and reporting in published microarray studies that focus on cancer-related clinical outcomes.

Methods

Studies published through 2004 in which microarray-based gene expression profiles were analyzed for their relation to a clinical cancer outcome were identified through a Medline search followed by hand screening of abstracts and full text articles. Studies that were eligible for our analysis addressed one or more outcomes that were either an event occurring during follow-up, such as death or relapse, or a therapeutic response. We recorded descriptive characteristics for all the selected studies. A critical review of outcome-related statistical analyses was undertaken for the articles published in 2004.

Results

Ninety studies were identified, and their descriptive characteristics are presented. Sixty-eight (76%) were published in journals of impact factor greater than 6. A detailed account of the 42 studies (47%) published in 2004 is reported. Twenty-one (50%) of them contained at least one of the following three basic flaws: 1) in outcome-related gene finding, an unstated, unclear, or inadequate control for multiple testing; 2) in class discovery, a spurious claim of correlation between clusters and clinical outcome, made after clustering samples using a selection of outcome-related differentially expressed genes; or 3) in supervised prediction, a biased estimation of the prediction accuracy through an incorrect cross-validation procedure.

Conclusions

The most common and serious mistakes and misunderstandings recorded in published studies are described and illustrated. Based on this analysis, a proposal of guidelines for statistical analysis and reporting for clinical microarray studies, presented as a checklist of "Do's and Don'ts", is provided.

---

### Your results, explained: clarity provided by row percentages versus column percentages [^8a915fa6]. The Annals of Thoracic Surgery (2016). Low credibility.

The overwhelming majority of clinical research papers begin their Results section with a series of tables summarizing the distribution of significant demographics, clinical characteristics, and potential outcome variables within their study cohort. However, the presentation of these tables often leaves the reader confused or wanting different information than what is presented. One particularly vexing decision in the presentation of categoric variables is choosing to present the "row percentage" in preference to the "column percentage". This report provides a guide for clinical scientists, some of whom may not have access to statistical support, to summarize their descriptive results in the manner that provides the most clarity for the reader.

---

### Analysis of observational studies: a guide to understanding statistical methods [^be80fd31]. The Journal of Bone and Joint Surgery: American Volume (2009). Low credibility.

Observational studies provide an important source of information when randomized controlled trials cannot or should not be undertaken, provided that the data are analyzed and interpreted with special attention to bias. This article highlights the special analytic considerations required for proper reporting and interpretation of observational studies. We review statistical principles that are fundamental to understanding what observational data can offer. The concepts include the relationship between a study sample and the target population, and the two primary forms of statistical analysis: estimation and hypothesis testing. The concept of bias, and confounding in particular, is introduced as an obstacle to drawing valid conclusions from an observational study. The discussion will then focus on the techniques that are most useful in the analysis of the three most common types of observational studies (the case series, the therapeutic study, and the prognostic study). The goal of this review is to empower the reader to take a practical approach to and validly interpret the statistical analysis of these study types.

---

### Outcome reporting bias in trials: a methodological approach for assessment and adjustment in systematic reviews [^e2ae1040]. BMJ (2018). Excellent credibility.

Systematic reviews of clinical trials aim to include all relevant studies conducted on a particular topic and to provide an unbiased summary of their results, producing the best evidence about the benefits and harms of medical treatments. Relevant studies, however, may not provide the results for all measured outcomes or may selectively report only some of the analyses undertaken, leading to unnecessary waste in the production and reporting of research, and potentially biasing the conclusions to systematic reviews. In this article, Kirkham and colleagues provide a methodological approach, with an example of how to identify missing outcome data and how to assess and adjust for outcome reporting bias in systematic reviews.

---

### How to design a pre-specified statistical analysis approach to limit p-hacking in clinical trials: the pre-SPEC framework [^284d76f7]. BMC Medicine (2020). Medium credibility.

Although this framework was developed with a trial's primary outcome in mind, it could also be used for secondary outcomes. As above, where investigators feel that specifying this information would pose an insurmountable barrier to the timely start of the trial, then investigators should simply specify the planned analysis approach as soon after the trial has begun as possible. Importantly, we note that our framework does not require that a detailed Statistical Analysis Plan be written before the trial begins.

We note that the Pre-SPEC framework is not intended to preclude changes or force investigators to stick with an analysis strategy they feel is no longer appropriate. There are sometimes good reasons for investigators to change their statistical methods during the course of the trial, for instance because of an advance in statistical methodology or the implementation of new methods in statistical software packages. Instead, if it is anticipated beforehand that the preferred method of analysis may depend on some aspect of the trial data (for instance, the distribution of outcome data), then the manner in which this decision will be made should be pre-specified, and if the analysis strategy needs to change due to an unanticipated issue (for instance, the occurrence of unanticipated intercurrent events, or new methodology becoming available in statistical software packages), then these changes should be documented and explained. Instead of preventing useful or necessary changes, Pre-SPEC simply increases transparency around the process; as stated in the SPIRIT guidelines, 'An unambiguous, complete, and transparent description of statistical methods facilitates execution, replication, critical appraisal, and the ability to track any changes from the original pre-specified methods'.

---

### The treatment of pediatric supracondylar humerus fractures [^f2f6b32c]. The Journal of the American Academy of Orthopaedic Surgeons (2012). Medium credibility.

Minimal clinically important improvement (MCII) — there were no occurrences of validated MCII outcomes in the included studies for this clinical practice guideline. Wherever possible, treatment effects are considered in terms of MCII in addition to statistical significance; MCII is the smallest clinical change that is important to patients and recognizes that some statistically significant improvements are too small to matter. When possible, results are described using terminology based on Armitage, et al., with associated descriptive terms and conditions outlined in Table 1.

---

### An official American Thoracic Society workshop report: developing performance measures from clinical practice guidelines [^3280055c]. Annals of the American Thoracic Society (2014). Medium credibility.

American Thoracic Society workshop — future directions state that as new guidelines are written there "should be stricter integration with performance measurement — indeed, the two efforts should proceed synchronously"; developers "should think a priori about performance measurement" with quality experts supporting phrasing for translation, and committees "should be multidisciplinary" with broad stakeholder input. Priority research areas include how to identify highest-leverage measures, to "systematically integrate GRADE-based guideline recommendations with outcomes studies" including costs and stakeholder opinions, to study aggregation into bundles because "Bundling together multiple weak recommendations may lead to improvements in outcomes", and to define validation methods before widespread use. The authors add that a rigorous framework "may help prevent adverse consequences, making it more tenable to evaluate measure impact after implementation, rather than before", that measures developed via this process should be compared with those developed "ad hoc", and that acceptability "should be vetted with patients, payers, and other stakeholders", with the approach potentially representing "a significant advancement" toward evidence-based, clinically relevant, patient-centered measures.

---

### KDIGO clinical practice guideline for the care of kidney transplant recipients [^8da72bb1]. American Journal of Transplantation (2009). Medium credibility.

Kidney transplant recipients — data extraction was standardized by the Evidence Review Team (ERT) using forms to capture primary study information; data fields for all topics included study setting, patient demographics, eligibility criteria, kidney transplantation details, numbers of subjects randomized, study design, study funding source, descriptions of interventions (or predictors), comparators, description of outcomes, statistical methods used, results, quality of outcomes (as described below), limitations to generalizability and free-text fields for comments and assessment of biases.

---

### Report of a consensus conference on transplant program quality and surveillance [^25d383c5]. American Journal of Transplantation (2012). Low credibility.

Public reports of organ transplant program outcomes by the US Scientific Registry of Transplant Recipients have been both groundbreaking and controversial. The reports are used by regulatory agencies, private insurance providers, transplant centers and patients. Failure to adequately adjust outcomes for risk may cause programs to avoid performing transplants involving suitable but high-risk candidates and donors. At a consensus conference of stakeholders held February 13–15, 2012, the participants recommended that program-specific reports be better designed to address the needs of all users. Additional comorbidity variables should be collected, but innovation should also be protected by excluding patients who are in approved protocols from statistical models that identify underperforming centers. The potential benefits of hierarchical and mixed-effects statistical methods should be studied. Transplant centers should be provided with tools to facilitate quality assessment and performance improvement. Additional statistical methods to assess outcomes at small-volume transplant programs should be developed. More data on waiting list risk and outcomes should be provided. Monitoring and reporting of short-term living donor outcomes should be enhanced. Overall, there was broad consensus that substantial improvement in reporting outcomes of transplant programs in the United States could and should be made in a cost-effective manner.

---

### Harmonizing heterogeneous endpoints in coronavirus disease 2019 trials without loss of information [^bd9f6943]. Critical Care Medicine (2021). Medium credibility.

Objectives

Many trials investigate potential effects of treatments for coronavirus disease 2019. To provide sufficient information for all involveddecision-makers (clinicians, public health authorities, and drug regulatory agencies), a multiplicity of endpoints must be considered. The objectives are to provide hands-on statistical guidelines for harmonizing heterogeneous endpoints in coronavirus disease 2019 clinical trials.

Design

Randomized controlled trials for patients infected with coronavirus disease 2019.

Setting

General methods that apply to any randomized controlled trial for patients infected with coronavirus disease 2019.

Patients

Coronavirus disease 2019 positive individuals.

Interventions

None.

Measurements and Main Results

We develop a multistate model that is based on hospitalization, mechanical ventilation, death, and discharge. These events are both categories of the ordinal endpoint recommended by the World Health Organization and also within the core outcome set of the Core Outcome Measures in Effectiveness Trials initiative for coronavirus disease 2019 trials. To support our choice of states in the multistate model, we also perform a brief review of registered coronavirus disease 2019 clinical trials. Based on the multistate model, we give recommendation for compact, informative illustration of time-dynamic treatment effects and explorative statistical analysis. A majority of coronavirus disease 2019 clinical trials collect information on mechanical ventilation, hospitalization, and death. Using reconstructed and real data of coronavirus disease 2019 trials, we show how a stacked probability plot provides a detailed understanding of treatment effects on the patients' course of hospital stay. It contributes to harmonizing multiple endpoints and differing lengths of follow-up both within and between trials.

Conclusions

All ongoing clinical trials should include a stacked probability plot in their statistical analysis plan as descriptive analysis. While primary analysis should be on an early endpoint with appropriate capability to be a surrogate (parameter), our multistate model provides additional detailed descriptive information and links results within and between coronavirus disease 2019 trials.

---

### Diagnosis and treatment of bronchial intraepithelial neoplasia and early lung cancer of the central airways: diagnosis and management of lung cancer, 3rd ed: American College of Chest Physicians evidence-based clinical practice guidelines [^b1aecafa]. Chest (2013). Medium credibility.

ACCP Lung Cancer Guidelines methodology — data abstraction, study quality, and statistical analysis describe that data were abstracted from each study with collection of population characteristics, inclusion/exclusion criteria, interventions, and length of follow-up; relevant outcomes included rates of progression, relative sensitivity, and treatment response (including the additional accuracy of AFB over WLB and the effectiveness of different endobronchial therapies); study quality was assessed using ACCP criteria covering design, selection, inclusion/exclusion descriptions, follow-up, outcome ascertainment, statistical analysis, funding, and conflicts of interest, with studies judged as good, fair, or poor; point estimates with 95% CI were reported when available, and no summary estimates were attempted because of heterogeneity across studies, designs, follow-up, and outcome definitions.

---

### Descriptive studies: what they can and cannot do [^da18bbca]. Lancet (2002). Excellent credibility.

Descriptive studies often represent the first scientific toe in the water in new areas of inquiry. A fundamental element of descriptive reporting is a clear, specific, and measurable definition of the disease or condition in question. Like newspapers, good descriptive reporting answers the five basic W questions: who, what, why, when, where. and a sixth: so what? Case reports, case-series reports, cross-sectional studies, and surveillance studies deal with individuals, whereas ecological correlational studies examine populations. The case report is the least-publishable unit in medical literature. Case-series reports aggregate individual cases in one publication. Clustering of unusual cases in a short period often heralds a new epidemic, as happened with AIDS. Cross-sectional (prevalence) studies describe the health of populations. Surveillance can be thought of as watchfulness over a community; feedback to those who need to know is an integral component of surveillance. Ecological correlational studies look for associations between exposures and outcomes in populations-eg, per capita cigarette sales and rates of coronary artery disease-rather than in individuals. Three important uses of descriptive studies include trend analysis, health-care planning, and hypothesis generation. A frequent error in reports of descriptive studies is overstepping the data: studies without a comparison group allow no inferences to be drawn about associations, causal or otherwise. Hypotheses about causation from descriptive studies are often tested in rigorous analytical studies.

---

### Frequency and reasons for outcome reporting bias in clinical trials: interviews with trialists [^9a8c6f0f]. BMJ (2011). Excellent credibility.

Description of protocols

Forty three (73%) trialists sent their full study protocol for review. The quality of the protocols was highly variable, but all lacked some key information such as a clear definition of the primary outcome or a description of the statistical methods planned for the analysis. For the remaining 16 (27%) trialists, no complete protocol was available. Instead, six trialists provided the ethics committee application, two the funding application, one a summary of the full protocol, and three extracts from relevant chapters of doctoral theses. For one trial the protocol sent was simply a letter with the description of the trial that was sent to the funders. For two trialists the full protocol was available but not in English, so an English summary was provided. For the remaining trial we obtained an abridged version of the protocol from the clinical trials.gov website.

In just under half of the protocols and substituted documents (27/59, 46%), one primary outcome was explicitly specified. In four (7%) protocols more than one outcome was specified, and in 28 (47%) none was specified. In three cases where a primary outcome was not explicitly stated, we assumed that the outcome used in the sample size calculation was the primary outcome; in two of these cases, this matched the stated primary outcome in the publication. In the remaining trial, the stated primary outcome in the report did not match the outcome on which the protocol sample size calculation was based. For three further trials where a primary outcome was not explicitly stated, we assumed that the primary outcome related to the main aim as stated in the research question in the protocol. Across the 37 trials with explicit (31 trials) or implicit (6 trials) primary outcomes, a total of 233 secondary outcomes were specified in the protocols (median of five, range 1–17). For the remaining 22 protocols where we could not deduce clearly which outcome was the primary outcome, a total of 132 outcomes were specified in the protocols (median of five, range 1–21). For the 59 protocols together, a total of 419 outcomes were specified; the median number of all outcomes specified was six (range 1–21). A statistical analysis plan was provided in 40 (68%) protocols.

---

### Statistics review 1: presenting and summarising data [^4c1c82d3]. Critical Care (2002). Low credibility.

The present review is the first in an ongoing guide to medical statistics, using specific examples from intensive care. The first step in any analysis is to describe and summarize the data. As well as becoming familiar with the data, this is also an opportunity to look for unusually high or low values (outliers), to check the assumptions required for statistical tests, and to decide the best way to categorize the data if this is necessary. In addition to tables and graphs, summary values are a convenient way to summarize large amounts of information. This review introduces some of these measures. It describes and gives examples of qualitative data (unordered and ordered) and quantitative data (discrete and continuous); how these types of data can be represented figuratively; the two important features of a quantitative dataset (location and variability); the measures of location (mean, median and mode); the measures of variability (range, interquartile range, standard deviation and variance); common distributions of clinical data; and simple transformations of positively skewed data.

---

### Comparison of clinical trial changes in primary outcome and reported intervention effect size between trial registration and publication [^172f7db3]. JAMA Network Open (2019). High credibility.

Outcome of Interest

A major discrepancy was defined if the registered and published primary outcomes were different or were assessed at a different time point. This definition is according to a modified classification by Chan et al. Specifically, the following were considered major discrepancies: (1) a prespecified primary outcome in the trial registration protocol was subsequently reported as a secondary outcome in the final published article; (2) The published primary outcome was described as a secondary outcome in the registry; (3) The prespecified primary outcomes in the trial registration were either omitted or not reported or labeled from the published article; (4) a new primary outcome was introduced in the published article (eg, an outcome that does not appear at all in the registry but is introduced as primary in the article, or one of the components changed among a composite outcome); and (5) the timing of assessment of the primary outcome in the registered protocol and published article differed.

Inconsistencies were independently identified by 2 of us (T.C. and C.L.), and disagreements were resolved by discussion until consensus (κ coefficient, 0.92; 95% CI, 0.88–0.96). If changes to the registered primary outcome were made, they were further reviewed by another one of us (V.C.), and disagreements were resolved by consensus.

Statistical Analysis

Categorical variables were described by frequencies and percentages and quantitative variables by medians and ranges. We used the κ coefficient to determine the degree of agreement between reviewers.

To quantify overall the consequences associated with primary outcome change on the reported intervention effect size across different types of outcomes, we assumed relative risk, hazard ratio, and odds ratio (OR) to be the same measure. This strategy has been used in published meta-analyses of observational studies. In our study, we considered ORs to be a common estimate, but heterogeneity by different types of outcomes was explored in subgroup analyses. For continuous outcomes, we converted them to ORs according to the method by Hasselblad and Hedges by multiplying the standardized mean differences and their SEs by 1.81 to calculate the log ORs and the corresponding SEs.

---

### Minimal important change and difference in health outcome: an overview of approaches, concepts, and methods [^58a52044]. Osteoarthritis and Cartilage (2024). Medium credibility.

Objective

To provide an overview of approaches, concepts, and methods used to define and assess minimal important change and difference in health outcome.

Method

A narrative review of the literature, guided by a conceptual framework.

Results

We distinguish between (i) interpretation of health outcome in individuals versus groups, (ii) change within individuals or groups versus difference between change within individuals or groups; and (iii) the responder approach (based on the proportion of patients that obtain a defined response) versus the group average approach (based on the average amount of change in a group). We review approaches, concepts, and methods.

Conclusion

By bringing together and juxtaposing various approaches, concepts, and methods, we set a precursory step in the direction of consensus building in the field concerned with defining and assessing minimal important change and difference in health outcome. We emphasize the need for conceptual clarification and terminological standardization. We argue that assessing minimal importance of change and difference in health outcome is essentially a value judgment involving a range of considerations and perspectives.

---

### Outcome selection and role of patient reported outcomes in contemporary cardiovascular trials: systematic review [^0c567d54]. BMJ (2010). Excellent credibility.

Secondly, shortcomings in reporting of patient reported outcomes were also observed in trials that assessed benefits of an intervention in light of considerable harms caused by the same intervention. While it could be argued that most interventions are likely to have multiple effects, desired and undesired, expected and unexpected, it would be inappropriate to demand measurement of patient reported outcomes in all trials. Standard safety monitoring of trials (despite all its limitations) and crude reporting of events might be sufficient for clinical decision making in circumstances where the spectrum of harms is reasonably well known and clinically measurable. However, issues arise when potential harms are substantial and their objective evaluation is not directly relevant to patients. One common area identified in this study was the assessment of the risk of bleeding in trials aiming to reduce the risk of cardiovascular events. The investigators used a diversity of methods for measurement and classification of bleeding, with some of these being purely based on objective laboratory measures with little relevance to patients. This bears the risk of invalidating comparisons between the different outcome components within the same study, as well as between studies reporting similar outcomes. A few investigators have attempted to estimate the net clinical effect by combining harms and benefits into a single composite outcome. However, such an approach would assume that detection methods for different types of outcomes are equally sensitive and that different types of events have a similar clinical weight. Since such conditions cannot easily be met in practice, and in the absence of valid and meaningful scoring systems for measurement of such harms, patient reported outcomes could offer an alternative method for assessment of the impact of such effects on patients. Preference based instruments for patient reported outcomes (that is, instruments that provide relative values for different health states), would allow an estimate of the weighted average of benefits and harms from the patient's perspective and could provide a common denominator for different types of outcomes.

---

### Comparison of treatment effect sizes associated with surrogate and final patient relevant outcomes in randomised controlled trials: meta-epidemiological study [^d20a9ff0]. BMJ (2013). Excellent credibility.

Implications of the study

The potential for surrogate outcomes to impact on healthcare policy making and the consequent diffusion of treatments into practice is shown by the fact that 27% of the randomised clinical trials identified during the two year study period reported surrogate primary outcomes. In health technology assessment reports, both in the United Kingdomand internationally, some 1 in 20 base their clinical and economic conclusions on evidence from surrogate outcomes alone. That trial based surrogate outcomes can lead to substantive overestimation of treatment effects that would have been seen if evidence on patient relevant outcomes was available is a salutary message for policy makers when weighing up the evidence from use of surrogate outcomes in their licensing and coverage decisions. Several recent drug appraisals by the National Institute for Health and Clinical Excellence have relied on evidence of clinical effect derived solely from surrogate outcomes. Our findings reinforce the importance of formally evaluating the acceptability of biomarkers and intermediate outcomes as valid surrogate outcomes and quantifying the association of treatment effect between the surrogate and patient relevant final outcomes and its uncertainty. The statistical validation of surrogates and the quantification of the relation between surrogate and final patient relevant outcomes are key problems tackled in NICE's update of its methodological guidance for technology assessment. The updated version of methods guidance makes several requirements for health technology assessment producers when faced with clinical trials with evidence based on surrogate outcomes (that is, a systematic review of the evidence to support the validity of the surrogate outcome, clear statement of how the relation between surrogate and final outcome is modelled in determining cost effectiveness) and exploration of the additional uncertainty associated with this prediction on cost effectiveness estimates.

Clinical trialists and systematic reviewers need to be clearer in their reporting as to whether outcomes are surrogate or final patient relevant, and appropriately frame any conclusions of superiority of interventions when based on surrogate outcomes alone. Others have recently suggested that guidance on surrogates should be incorporated into the CONSORT statement. Novel and adaptive approaches to trial design are needed that allow surrogate endpoints to continue to be used as primary outcomes, while also providing evidence on their validation against patient relevant outcomes.

Conclusions

In the absence of data on final outcome, policy makers need to interpret intervention effects based on surrogate outcomes with caution. Although our results have highlighted the risks, they support the application of methods for the validation and quantification of the relations between surrogate and final patient relevant outcomes in licensing and reimbursement decisions on new and existing treatments.

---

### Outcome selection and role of patient reported outcomes in contemporary cardiovascular trials: systematic review [^0f0d9cd7]. BMJ (2010). Excellent credibility.

Methods

We searched Medline and Embase for articles with any of the subject terms "cardiovascular diseases" or "cardiovascular agents" published between January 2005 and December 2008 in any of the following leading general medical or cardiology journals: New England Journal of Medicine, Lancet, JAMA, Annals of Internal Medicine, PLoS Medicine, BMJ, Circulation, Journal of the American College of Cardiology, Circulation Research, and European Heart Journal (see web extra appendix 1 for the detailed search strategy).

Review methods and data extraction

Figure 1summarises the retrieval and selection process for studies. The combined searches yielded 2374 reports, of which 1376 were excluded on the basis of titles and abstracts. The remaining 998 reports were allocated a unique random number with the use of a random sequence generator. We assessed in full text the first 500 articles according to this random list for their eligiblity, and extracted relevant data from eligible trials. Of these, 87 did not meet the eligibility criteria. We used an electronic data extraction form to capture pivotal information from each trial. We categorised reported outcomes and individual components of any composite outcomes into patient important outcomes (death, major morbidity, patient reported outcome) or intermediate outcomes — that is, those not immediately or directly important to patients, such as biochemical, physiological, or surrogate outcomes. The categorisation of outcome was based on previously published ranking systems, with more emphasis on the methods of outcome measurement (see also web extra appendix 2). From the trials evaluated, we assessed those that were reporting patient reported outcomes to see whether a previously validated instrument was used or whether psychometric properties were reported or referenced in the article. All data were extracted in duplicate. The results and disagreements were discussed until consensus was reached.

Fig 1 Search retrieval and study selection process

---

### Reporting and interpretation of SF-36 outcomes in randomised trials: systematic review [^7eddc8d1]. BMJ (2009). Excellent credibility.

Objective

To determine how often health surveys and quality of life evaluations reach different conclusions from those of primary efficacy outcomes and whether discordant results make a difference in the interpretation of trial findings.

Design

Systematic review.

Data Sources

PubMed, contact with authors for missing information, and author survey for unpublished SF-36 data.

Study Selection

Randomised trials with SF-36 outcomes (the most extensively validated and used health survey instrument for appraising quality of life) that were published in 2005 in 22 journals with a high impact factor.

Data Extraction

Analyses on the two composite and eight subdomain SF-36 scores that corresponded to the time and mode of analysis of the primary efficacy outcome.

Results

Of 1057 screened trials, 52 were identified as randomised trials with SF-36 results (66 separate comparisons). Only eight trials reported all 10 SF-36 scores in the published articles. For 21 of the 66 comparisons, SF-36 results were discordant for statistical significance compared with the results for primary efficacy outcomes. Of 17 statistically significant SF-36 scores where primary outcomes were not also statistically significant in the same direction, the magnitude of effect was small in six, moderate in six, large in three, and not reported in two. Authors modified the interpretation of study findings based on SF-36 results in only two of the 21 discordant cases. Among 100 additional randomly selected trials not reporting any SF-36 information, at least five had collected SF-36 data but only one had analysed it.

Conclusions

SF-36 measurements sometimes produce different results from those of the primary efficacy outcomes but rarely modify the overall interpretation of randomised trials. Quality of life and health related survey information should be utilised more systematically in randomised trials.

---

### Reporting and interpretation of SF-36 outcomes in randomised trials: systematic review [^cdf7362c]. BMJ (2009). Excellent credibility.

Objective To determine how often health surveys and quality of life evaluations reach different conclusions from those of primary efficacy outcomes and whether discordant results make a difference in the interpretation of trial findings.

Design Systematic review.

Data sources PubMed, contact with authors for missing information, and author survey for unpublished SF-36 data.

Study selection Randomised trials with SF-36 outcomes (the most extensively validated and used health survey instrument for appraising quality of life) that were published in 2005 in 22 journals with a high impact factor.

Data extraction Analyses on the two composite and eight subdomain SF-36 scores that corresponded to the time and mode of analysis of the primary efficacy outcome.

Results Of 1057 screened trials, 52 were identified as randomised trials with SF-36 results (66 separate comparisons). Only eight trials reported all 10 SF-36 scores in the published articles. For 21 of the 66 comparisons, SF-36 results were discordant for statistical significance compared with the results for primary efficacy outcomes. Of 17 statistically significant SF-36 scores where primary outcomes were not also statistically significant in the same direction, the magnitude of effect was small in six, moderate in six, large in three, and not reported in two. Authors modified the interpretation of study findings based on SF-36 results in only two of the 21 discordant cases. Among 100 additional randomly selected trials not reporting any SF-36 information, at least five had collected SF-36 data but only one had analysed it.

Conclusions SF-36 measurements sometimes produce different results from those of the primary efficacy outcomes but rarely modify the overall interpretation of randomised trials. Quality of life and health related survey information should be utilised more systematically in randomised trials.

---

### Clinical practice guideline for the pharmacologic treatment of chronic insomnia in adults: an American Academy of Sleep Medicine clinical practice guideline [^ebfbd19e]. Journal of Clinical Sleep Medicine (2017). Medium credibility.

Clinical Practice Guideline: Insomnia — methodologic priorities and overarching conclusions emphasize standardized statistical analysis and data presentation with means ± SD for sleep outcomes, with transformations for non-normally distributed variables and efforts to report means and SD across studies. Beyond individual sleep variables, achieving remission of chronic insomnia disorder should be considered as a clinically relevant outcome for pharmacological trials. The document urges to encourage funding for independent, non-industry investigation of the efficacy and effectiveness of hypnotic medications, notes that adverse event data are not collected and analyzed in standard ways with a need to standardize the assessment of adverse effects data, and highlights daytime safety by recognizing potential risk of motor vehicle or occupational accidents and that further efforts to include objective assessments of performance impairments which may be associated with daytime sedation is encouraged. Pediatric evidence is limited despite use, as virtually no data exists regarding the use of sleep-promoting agents in children and adolescents and such medications are not infrequently used in this age group. The summary states it relies heavily on rigorous evaluation of the quality of evidence for efficacy, based on GRADE and that foundational data are sorely limited, concluding that many commonly used drugs, including some which carry FDA approval for treatment of insomnia, are not recommended, and advising clinicians to continue to exercise sound clinical judgment considering experience, prior response, preferences, and adverse effects.

---

### Comparing methods to denote treatment outcome in clinical research and benchmarking mental health care [^e9bcb47c]. Clinical Psychology & Psychotherapy (2016). Low credibility.

Approaches based on continuous indicators (the size of the pre-to-post-test change; effect size or ΔT) and on categorical indicators (Percentage Improvement and the Jacobson-Truax approach to Clinical Significance) are evaluated to determine which has the best methodological and statistical characteristics, and optimal performance, in comparing outcomes of treatment providers. Performance is compared in two datasets from providers using the Brief Symptom Inventory or the Outcome Questionnaire. Concordance of methods and their suitability to rank providers is assessed. Outcome indicators tend to converge and lead to a similar ranking of institutes within each dataset. Statistically and conceptually, continuous outcome indicators are superior to categorical outcomes as change scores have more statistical power and allow for a ranking of providers at first glance. However, the Jacobson-Truax approach can complement the change score approach as it presents outcome information in a clinically meaningful manner. Copyright © 2015 John Wiley & Sons, Ltd.

Key Practitioners Messages

When comparing various indicators or treatment outcome, statistical considerations designate continuous outcomes, such as the effect size of the pre-post change (effect size or ΔT) as the optimal choice. Expressing outcome in proportions of recovered, changed, unchanged or deteriorated patients has supplementary value, as it is more easily interpreted and appreciated by clinicians, managerial staff and, last but not the least, by patients. If categorical outcomes are used with small datasets, true differences in institutional performance may get obscured due to diminished power to detect differences. With sufficient data, outcome according to continuous and categorical indicators converge and lead to similar rankings of institutes' performance.

---

### A practical guide for understanding confidence intervals and P values [^c0a7d4a0]. Otolaryngology — Head and Neck Surgery (2009). Low credibility.

The 95 percent confidence interval about the mean demarcates the range of values in which the mean would fall if many samples from the universal parent population were taken. In other words, if the same observation, experiment, or trial were done over and over with a different sample of subjects, but with the same characteristics as the original sample, 95 percent of the means from those repeated measures would fall within this range. This gives a measure of how confident we are in the original mean. It tells us not only whether the results are statistically significant because the CI falls totally on one side or the other of the no difference marker (0 if continuous variables; 1 if proportions), but also the actual values so that we might determine if the data seem clinically important. In contrast, the P value tells us only whether the results are statistically significant, without translating that information into values relative to the variable that was measured. Consequently, the CI is a better choice to describe the results of observations, experiments, or trials.

---

### Repeated measures of modified rankin scale scores to assess functional recovery from stroke: AFFINITY study Findings [^1dd88c88]. Journal of the American Heart Association (2022). Medium credibility.

Clinical Perspective

The modified Rankin Scale (mRS) is the most widely used primary outcome measure in stroke trials and is considered a valid tool for evaluating stroke interventions. The mRS has 7 categories to measure functional status, ranging from independent and free of disability (0) to death (6). Historically, in analyses of variables associated with the mRS, mRS scores were dichotomously transformed rather than ordinally preserved, and statistical methods were single measure (usually 3 or 6 months after enrollment) rather than longitudinal. Numerous trials and systematic reviews have found variance in the statistical significance of stroke interventions (as measured by treatment effect on mRS), depending on trial design, choice of outcome (including simplified and utility‐weighted mRS) and statistical methods. While ordinal mRS analyses have increased in number because of greater statistical power and association with long‐term clinical outcomes and costs, these are usually single mRS measurement analyses. A 2018 literature review found 2 studies using repeated measures of the mRS to analyze recovery over time; however, these did not analyze ordinal mRS. Further to this, our literature searches have not revealed studies comparing single‐measure and repeated‐measures ordinal logistic mRS analyses to identify associations with functional recovery. If collected, usage of all serial follow‐up data (rather than study end points) should strengthen validity of any identified associations with mRS by increasing statistical powerand is concordant with the Stroke Recovery and Rehabilitation Roundtable goals in applying repeated measures to understand the natural history of recovery and optimal timing of interventions.

---

### KDIGO 2022 clinical practice guideline for diabetes management in chronic kidney disease [^5df2ee1e]. Kidney International (2022). High credibility.

Methods for guideline development — statistical synthesis, heterogeneity, publication bias, sensitivity analyses, and evidence grading state that meta-analyses used the "Mantel-Haenszel random-effects model for dichotomous outcomes and the inverse variance random-effects model for continuous outcomes". Heterogeneity used "A P value of < 0.1" and "an I² was calculated", and subgroup differences "used the I² statistic and a P value of 0.10 (noting that this is a weak test)". Publication bias assessments "used funnel plots of the log odds ratio (effect vs. standard error of the effect size) when a sufficient number of studies were available (i.e., > 10 studies)". Sensitivity analyses considered "Repeating the analysis excluding unpublished studies", "Repeating the analysis, taking account of the risk of bias, as specified", "Repeating the analysis excluding any very long or large studies, to establish how much they dominate the results", and "Repeating the analysis excluding studies using the following filters: language of publication, source of funding (industry vs. other), and country in which the study was conducted", but "the available data were insufficient to determine the influence of these factors on the effect size of critical and important outcomes". Evidence certainty used GRADE: "The overall quality of the evidence related to each critical and important outcome was assessed using the GRADE approach", and "For outcomes that are based on data from RCTs, the initial grade for the quality of the evidence is considered to be high".

---

### Mean difference, standardized mean difference (SMD), and their use in meta-analysis: as simple as It gets [^265f2068]. The Journal of Clinical Psychiatry (2020). Medium credibility.

In randomized controlled trials (RCTs), endpoint scores, or change scores representing the difference between endpoint and baseline, are values of interest. These values are compared between experimental and control groups, yielding a mean difference between the experimental and control groups for each outcome that is compared. When the mean difference values for a specified outcome, obtained from different RCTs, are all in the same unit (such as when they were all obtained using the same rating instrument), they can be pooled in meta-analysis to yield a summary estimate that is also known as a mean difference (MD). Because pooling of the mean difference from individual RCTs is done after weighting the values for precision, this pooled MD is also known as the weighted mean difference (WMD). Sometimes, different studies use different rating instruments to measure the same outcome; that is, the units of measurement for the outcome of interest are different across studies. In such cases, the mean differences from the different RCTs cannot be pooled. However, these mean differences can be divided by their respective standard deviations (SDs) to yield a statistic known as the standardized mean difference (SMD). The SD that is used as the divisor is usually either the pooled SD or the SD of the control group; in the former instance, the SMD is known as Cohen's d, and in the latter instance, as Glass' delta. SMDs of 0.2, 0.5, and 0.8 are considered small, medium, and large, respectively. SMDs can be pooled in meta-analysis because the unit is uniform across studies. This article presents and explains the different terms and concepts with the help of simple examples.

---

### Standard measures are inadequate to monitor pediatric adherence in a resource-limited setting [^d9efa0d6]. AIDS and Behavior (2011). Low credibility.

Statistical Analyses

Descriptive statistics were used to analyze children's and caregivers' characteristics and variables related to treatment outcome. Normally distributed continuous variables were described using means and standard deviations (SD); non-normally distributed ones using medians and interquartile ranges (IQR). Discrete data were described using counts and percentages. To express adherence as a binary variable, the defined cut-off value was at 95% of the continuous data for all measures. Patients above this cut-off were defined as adherent, patients below as non-adherent. We used Pearson correlations to compare adherence measures and linear regression to assess the associations between adherence and treatment outcome. The performance criteria for all adherence measures were calculated with two-by-two tables and receiver operating characteristic curves. The level of significance for all analyses was P < 0.05. All statistical analyses were performed using STATA 9.0 (StataCorp, College Station, TX).

---

### Methodology for ACOEM's occupational medicine practice guidelines-2025 revision [^51c1eba4]. ACOEM (2025). High credibility.

Cohort study quality appraisal — comparability and outcomes: The checklist requires "Demonstration that outcome of interest was not present at start of study" with options "yes" or "no", evaluates "Comparability of cohorts on the basis of the design or analysis" by whether the study controls for the most important factor and any additional factor, specifies outcome "Assessment of outcome" options including "independent blind assessment", "record linkage", "self report", or "no description", asks "Was follow-up long enough for outcomes to occur?" with "yes (select an adequate follow up period for outcome of interest)" or "no", and categorizes "Adequacy of follow-up of cohorts" as "complete follow-up, all subjects accounted for", "subjects lost to follow-up unlikely to introduce bias, small number lost [select an adequate % in Comments], or description provided of those lost", "follow up rate [select an adequate in Comments] and no description of those lost", or "no statement".

---

### FDA executive summary… [^04dce647]. FDA (2025). Medium credibility.

6.
5. 3 Descriptive Endpoints. 21 6. 6 Single-Arm Cohort Endpoints. 23 6.
6. 1 Primary Endpoint. 23 6.
6.

1. 1 Statistical Analysis Plan.
23. 7.
5. 4 Variability in Trial Enrollment and Results by Clinical Site. 52 7.
5. 5 Pre-specified Subgroup Analyses. 55 7.
5. 6 Post Hoc Subgroup Analyses. 57 7.
5. 7 Primary Endpoint Result for All Available Patients.
61. 7.
5. 8 2-Year Outcomes for Available Patients. 61 7. 6 Single-Arm Cohort Results. 65 7.
6. 1 Primary Endpoint. 65 7.
6. 2 Safety Results. 66 7.
6.
3. mortality or tricuspid valve surgery, number of HF hospitalizations, and health status improvement by ≥ 15 points measured by the Kansas City Cardiomyopathy Questionnaire overall summary score, assessed at 12-month follow-up. The primary endpoint of the Single-Arm Cohort was survival with a KCCQ summary score improvement of ≥ 10 points at 12 months compared to. their symptom perception and symptom severity grading.

For example, it could be expected that patients randomized to the investigational group may report more optimistic subjective information than patients in the control group, who may be more pessimistic based on their treatment assignment, especially when the control group receives a nonpreferred treatment by the patients. associated with TR pathophysiology might be less prone to open-label bias vs. more general health status assessments. Lastly, unequal study patient withdrawal between treatment groups resulting in unequal or not-at-random missing PRO data can also introduce additional challenges in interpreting study results. self-efficacy. Values for each domain and the summary scores range from 0 to 100, with higher scores indicating better health status.

Clinical consensus suggested that a change of 5 points in the KCCQ overall summary score is a small but clinically important change, a change of 10 points represents a moderate to large improvement, and a change of 20 points is a large to very large.

---

### Repeated measures designs and analysis of longitudinal data: If at first you Do not succeed-try, try again [^96586bc0]. Anesthesia and Analgesia (2018). Low credibility.

Anesthesia, critical care, perioperative, and pain research often involves study designs in which the same outcome variable is repeatedly measured or observed over time on the same patients. Such repeatedly measured data are referred to as longitudinal data, and longitudinal study designs are commonly used to investigate changes in an outcome over time and to compare these changes among treatment groups. From a statistical perspective, longitudinal studies usually increase the precision of estimated treatment effects, thus increasing the power to detect such effects. Commonly used statistical techniques mostly assume independence of the observations or measurements. However, values repeatedly measured in the same individual will usually be more similar to each other than values of different individuals and ignoring the correlation between repeated measurements may lead to biased estimates as well as invalid P values and confidence intervals. Therefore, appropriate analysis of repeated-measures data requires specific statistical techniques. This tutorial reviews 3 classes of commonly used approaches for the analysis of longitudinal data. The first class uses summary statistics to condense the repeatedly measured information to a single number per subject, thus basically eliminating within-subject repeated measurements and allowing for a straightforward comparison of groups using standard statistical hypothesis tests. The second class is historically popular and comprises the repeated-measures analysis of variance type of analyses. However, strong assumptions that are seldom met in practice and low flexibility limit the usefulness of this approach. The third class comprises modern and flexible regression-based techniques that can be generalized to accommodate a wide range of outcome data including continuous, categorical, and count data. Such methods can be further divided into so-called "population-average statistical models" that focus on the specification of the mean response of the outcome estimated by generalized estimating equations, and "subject-specific models" that allow a full specification of the distribution of the outcome by using random effects to capture within-subject correlations. The choice as to which approach to choose partly depends on the aim of the research and the desired interpretation of the estimated effects (population-average versus subject-specific interpretation). This tutorial discusses aspects of the theoretical background for each technique, and with specific examples of studies published in Anesthesia & Analgesia, demonstrates how these techniques are used in practice.

---

### Assessment of the frequency and variety of persistent symptoms among patients with COVID-19: a systematic review [^08f30bfd]. JAMA Network Open (2021). High credibility.

Statistical Analysis

We used a descriptive approach to the analysis because the heterogeneity of study designs limited the combinability of most estimates. The median and interquartile range (IQR) were reported for outcomes with 5 or more estimates, and individual values were reported for outcomes with 4 or fewer estimates. We did not report 95% CIs for the reported percentages because they were not directly relevant to inferences and, in most cases, reported frequencies varied more by design than could be attributed to random error. Disease severity at baseline was calculated as a weighted mean (the sum of all severity scores multiplied by the proportion of patients with that score). Severity scores were 0 (asymptomatic), 1 (mild to moderate), 2 (severe), and 3 (critical).

Risk estimates for the outcomes examined in 10 or more studies and for quality-of-life measures are summarized in the text, and outcomes examined in 5 or more studies are displayed graphically (Figure 1). When possible, we explored whether differences in study design could have been associated with variation in estimates between studies.

Figure 1.
Reported Frequencies of Symptoms Examined by 5 or More Studies

The horizontal bar extends from the first to the third quartile, the interquartile range (IQR). The whiskers extend from the upper and lower quartiles to the largest value within 1.5 IQRs of that quartile. The width of the box represents the IQR. The vertical bar represents the median value for the outcome. The circles represent point estimates from each study. Circles beyond the whiskers are considered outliers. Values for anosmia (loss of smell) and ageusia (loss of taste) represent frequency of loss if that loss began during acute stage of infection among studies with available data. Therefore, 7 studies reporting anosmia and 5 studies reporting ageusia were excluded from the figure.

---

### An official American Thoracic Society workshop report: developing performance measures from clinical practice guidelines [^f7ea00e5]. Annals of the American Thoracic Society (2014). Medium credibility.

An official American Thoracic Society workshop report — developing performance measures from clinical practice guidelines defines performance measures as specific, quantifiable representations of a capacity, process, or outcome relevant to health care quality, and notes they address the structure of care, the process of care, or the outcome of care. The report explains that the Grading of Recommendations Assessment, Development, and Evaluation (GRADE) approach separates the strength of a recommendation from the quality of evidence and allows incorporation of patient and stakeholder values, making GRADE-based clinical practice guidelines a useful foundation for developing performance measures tied to patient-centered outcomes. The workshop focused on process-based measures because they are directly related to treatment and diagnostic strategies and are generally more actionable, whereas outcome-based measures, although more important to patients, may not provide specific courses for improvement. It also states that process-based performance measures should ideally be based solely on strong recommendations based on high- or moderate-quality evidence. The intended audience includes performance measure and guideline developers as well as policy makers, clinicians, and researchers interested in performance measurement and quality improvement.

---

### The use of patient-reported outcome measures in clinical practice and clinical decision making [^6fb31702]. The Journal of the American Academy of Orthopaedic Surgeons (2023). Medium credibility.

Patient-reported outcome measures (PROMs) are highly effective measures of quality of care and outcomes that matter to patients regarding their physical, mental, and social health. While PROMs have played a notable role in research and registry reporting, they are also useful as clinical tools. Real-time PROM collection can be integrated into routine clinical care with immediate access to scores within the electronic health record. This can be integral when discussing treatment options and using decision aids. PROM scores can also be useful for postoperative monitoring. Various approaches to quantifying clinical efficacy have been developed, including the minimal clinically important difference, the substantial clinical benefit, and the patient acceptable symptom state (PASS). As the patient experience and patient-reported outcome measurement of health-related outcomes become increasingly emphasized in patient-centered, high value care, so too will the importance of methods to gauge clinical benefit using these instruments for improved clinical decision-making.

---

### A practical overview and reporting strategies for statistical analysis of Survival Studies [^8c0b68f2]. Chest (2020). Medium credibility.

Survival (time-to-event) analysis is commonly used in clinical research. Key features of performing a survival analysis include checking proportional hazards assumptions, reporting CIs for hazards ratios and relative risks, graphically displaying the findings, and analyzing with consideration of competing risks. This article provides a brief overview of important statistical considerations for survival analysis. Censoring schemes, different methods of survival function estimation, and ways to compare survival curves are described. We also explain competing risk and how to model survival data in the presence of it. Different kinds of bias that influence survival estimation and avenues to model the data under these circumstances are also described. Several analysis techniques are accompanied by graphical representations illustrating proper reporting strategies. We provide a list of guiding statements for researchers and reviewers.

---

### Descriptive analysis of cochrane child-relevant systematic reviews: an update and comparison between 2009 and 2013 [^06bb139d]. BMC Pediatrics (2017). Low credibility.

Several observations can be made with regards to the scope of the available primary research for child-relevant systematic reviews. Overall, 9.8% of systematic reviews were empty reviews. This represents a meaningful proportion of child-relevant reviews for which primary research is urgently needed in several topic areas. Of further concern, the median number of studies per review, which was 8 (IQR: 4, 17), was consistent with the findings of the 2009 CHFRR and Moher's 2007 analysis of Cochrane reviews. This is often an insufficient amount of data to have confidence in the precision of results, to perform subgroup analyses, or to evaluate publication bias. Finally, a median of 5 (IQR: 3, 10) studies made up the largest meta-analysis per review. This highlights that a large proportion of meta-analyses will only reflect a subset of the total available evidence in a review. Further analysis of the reasons that studies do not contribute to the largest meta-analysis is needed in order to determine whether the studies are not assessing the same outcomes, not reporting results, or reporting results in a way that cannot be used for the purposes of meta-analysis. Inconsistent use of outcomes and reporting of outcome measures across primary studies is a recognized problem that becomes amplified at the systematic review level, and can impact the evidence available for decision-making. The COMET initiative aims to address these challenges by supporting efforts in the development and application of 'core outcome sets' (standardised outcome sets) within specific clinical areas.

---

### The routine use of patient reported outcome measures in healthcare settings [^a55adfba]. BMJ (2010). Excellent credibility.

The use of patient reported outcome measures might seem to be quite straightforward; however, a number of pitfalls await clinicians with limited expertise. Jill Dawson and colleagues provide a guide for individuals keen to use patient reported outcome measures at a local level.

---

### Regression methods for investigating risk factors of chronic kidney disease outcomes: the state of the art [^479186bb]. BMC Nephrology (2014). Low credibility.

Using a single summary statistic as the outcome variable

Among the 45 occurrences where the outcome variable was a summary statistic of the renal function decline over time, individual slope of GFR was used on 36 occasions, and individual change in GFR compared to baseline was used on 7 occasions. All these analyses consist of two steps. In step 1 the summary statistic for each patient is derived, and step 2 uses this summary statistic as the outcome variable in a linear regression to investigate the association with patients' characteristics at baseline (Table 3). As mentioned by Rosansky, using the slope of renal function for measuring renal trajectory may be considered as "a starting point in application of renal trajectory to clinical management". However, the two-step approach mentioned above is not the best statistical approach to achieve this objective. Indeed, it has numerous disadvantages including an important loss of information, especially if individual values of the summary statistic (e.g. slope) are not first estimated using all available measurements per patient or if many patients have only a single measurement of the renal function. Another important issue for statistical inference is that Step 2 does not account for uncertainty of the summary statistic derived in Step 1. Yet, the summary statistic (e.g. slope) is an estimate, and the accuracy of this estimate for a given patient strongly depends on the number of measurements available for that patient, as well as the time points at which these measurements were taken. Ignoring such uncertainty is likely to produce unreliable confidence intervals of the effect of the risk factor on the slope. Finally, using the slope makes the implicit assumption of a linear trajectory of the quantitative marker for each individual, although this may not apply to a large number of CKD patients. Among the 36 occurrences where the slope was used as the summary statistic, 16 (44.4%) had three or more repeated measurements of renal function per patient, which would have made it possible to use more appropriate statistical methods as described in the next section.

---

### 2018 Infectious Diseases Society of America clinical practice guideline for the management of outpatient parenteral antimicrobial therapy [^fe29a7b0]. Clinical Infectious Diseases (2019). High credibility.

OPAT data synthesis and statistical significance: The evidence was synthesized using strength-of-evidence tables, and due to the heterogeneity and small number of included studies in each individual evidence table, data were not combined quantitatively using metaanalyses. For each question, clinically relevant outcomes were sought; when available from individual studies, point estimates, confidence intervals, and P values were reviewed, corrected values were used if errors were found, and P values of < .05 were considered statistically significant.

---

### A systematic review supporting the Society for Vascular Surgery, the American Venous Forum, and the American Vein and Lymphatic Society guidelines on the management of varicose veins [^a12ffc2b]. Journal of Vascular Surgery: Venous and Lymphatic Disorders (2022). High credibility.

Varicose veins systematic review — statistical analysis methods state that the relative risk (RR) was extracted or calculated for binary outcomes, standardized mean difference (SMD) was calculated for continuous results when studies used different measures for the same effect, and when the same outcome measure was used results were expressed as a weighted mean difference on the original scale; rate ratios for AEs were calculated, effect sizes were combined using the DerSimonian and Laird random effects method, heterogeneity was evaluated using I² indicator, and analyses were based on the intention-to-treat principle for RCTs or the number of patients initially assigned for observational studies.

---

### An official American Thoracic Society workshop report: developing performance measures from clinical practice guidelines [^0802211b]. Annals of the American Thoracic Society (2014). Medium credibility.

Process-based performance measure development — When developing process-based measures, considerations include the purpose of measurement, the importance of the clinical problem, ease of measurement, variability in delivered care, opportunity for improvement, and potential unintended consequences; successful measures are described as important, scientifically sound, and feasible. After selecting a care process, steps include writing an indicator statement, specifying eligibility and the desired action in plain language, defining the denominator and numerator, and specifying how patient preferences are handled, with options such as removing refusals from the numerator, removing refusals from both the numerator and denominator, or adding refusals to both as a marker that the indicator was offered but refused.

---

### Statistical methods for longitudinal research on bipolar disorders [^ad3b4f43]. Bipolar Disorders (2003). Low credibility.

Objectives

Outcomes research in bipolar disorders, because of complex clinical variation over-time, offers demanding research design and statistical challenges. Longitudinal studies involving relatively large samples, with outcome measures obtained repeatedly over-time, are required. In this report, statistical methods appropriate for such research are reviewed.

Methods

Analytic methods appropriate for repeated measures data include: (i) endpoint analysis; (ii) endpoint analysis with last observation carried forward; (iii) summary statistic methods yielding one summary measure per subject; (iv) random effects and generalized estimating equation (GEE) regression modeling methods; and (v) time-to-event survival analyses.

Results

Use and limitations of these several methods are illustrated within a randomly selected (33%) subset of data obtained in two recently completed randomized, double blind studies on acute mania. Outcome measures obtained repeatedly over 3 or 4 weeks of blinded treatment in active drug and placebo sub-groups included change-from-baseline Young Mania Rating Scale (YMRS) scores (continuous measure) and achievement of a clinical response criterion (50% YMRS reduction). Four of the methods reviewed are especially suitable for use with these repeated measures data: (i) the summary statistic method; (ii) random/mixed effects modeling; (iii) GEE regression modeling; and (iv) survival analysis.

Conclusions

Outcome studies in bipolar illness ideally should be longitudinal in orientation, obtain outcomes data frequently over extended times, and employ large study samples. Missing data problems can be expected, and data analytic methods must accommodate missingness.

---

### Editor's choice – European Society for Vascular Surgery (ESVS) 2024 clinical practice guidelines on the management of asymptomatic lower limb peripheral arterial disease and intermittent claudication [^6d765455]. European Journal of Vascular and Endovascular Surgery (2024). High credibility.

Regarding diagnostic investigations for peripheral artery disease, more specifically with respect to patient-reported outcomes, ESVS 2024 guidelines recommend to consider using properly developed and tested patient-reported outcome measures to characterize functional status and health-related QoL when considering indications for treatment, as well as for outcome evaluation and scientific purposes.

---

### Comparison of registered and published outcomes in randomized controlled trials: a systematic review [^30f46c73]. BMC Medicine (2015). Low credibility.

Discussion

We identified 27 studies that compared registered and published trial outcomes. Among these studies, the proportion of discrepant primary outcomes between registration and publication was highly variable, with four studies observing outcome changes in more than 50% of trials, and eight studies observing outcomes changes in fewer than 20%. The frequency with which discrepancies were identified did not appear to vary based on funding source, journal type, or when trials were published, though our ability to detect potential differences was limited by relatively small sample sizes and unstandardized methods across the included studies. Most of the studies identified were similar in regard to the time frame analyzed and the journals and clinical topics studied. Additionally, nearly all of the included studies used a consistent method of categorizing outcome differences, based on the scheme established by Chan et al. One important source of the variability in reported results is that most studies compared published outcomes to the outcome listed in the registry at the time of the search, without attempting to identify the outcome registered at the inception of the study. For the six studies that compared published outcomes with the outcomes registered prior to the initiation of patient enrollment, the frequency of discrepancies were more similar to one another and higher than for the remaining 21 studies. Because the comparison with the outcome prior to patient enrollment is the most relevant for those attempting to understand the impact of outcome switching on the validity of results of clinical trials, the median value from these six studies (41%, IQR 33–48%) provides an informative estimate for characterizing the extent of this problem.

---

### Knee pain and mobility impairments: meniscal and articular cartilage lesions revision 2018 [^0e3f8213]. The Journal of Orthopaedic and Sports Physical Therapy (2018). Medium credibility.

Appendix H — Interventions quality appraisal (AMSTAR and PEDro) — defines the appraisal tools and rating thresholds, and lists all checklist items used to judge study quality. AMSTAR (A Measurement Tool to Assess Systematic Reviews) and PEDro (Physiotherapy Evidence Database) use the same quality-rating scheme: "8 or higher, high; 5, 6, or 7, acceptable; 4 or less, low". AMSTAR items are: 1. "The study addresses a clearly defined research question"; 2. "At least 2 people should select studies and extract data"; 3. "A comprehensive literature search is carried out"; 4. "The authors clearly state if or how they limited their review by publication type"; 5. "The included and excluded studies are listed"; 6. "The characteristics of the included studies are provided"; 7. "The scientific quality of the included studies is assessed and documented"; 8. "The scientific quality of the included studies is assessed appropriately"; 9. "Appropriate methods are used to combine the individual study findings"; 10. "The likelihood of publication bias is assessed"; 11. "Conflicts of interest are declared". PEDro items are: 1. "Eligibility criteria were specified"; 2. "Subjects were randomly allocated to groups (in a crossover study, subjects were randomly allocated an order in which treatments were received)"; 3. "Allocation was concealed"; 4. "The groups were similar at baseline regarding the most important prognostic indicators"; 5. "There was blinding of all subjects"; 6. "There was blinding of all therapists who administered the therapy"; 7. "There was blinding of all assessors who measured at least 1 key outcome"; 8. "Measures of at least 1 key outcome were obtained from more than 85% of the subjects initially allocated to groups"; 9. "All subjects for whom outcome measures were available received the treatment or control condition as allocated, or where this was not the case, data for at least 1 key outcome were analyzed by 'intention to treat'"; 10. "The results of between-group statistical comparisons were reported for at least 1 key outcome"; 11. "The study provides both point measures and measures of variability for at least 1 key outcome".

---

### Evaluating the credibility of anchor based estimates of minimal important differences for patient reported outcomes: instrument development and reliability study [^050ad00b]. BMJ (2020). Excellent credibility.

Patient and public involvement

Patients and the public were not involved in the design, conduct, or reporting in this methodological research, as our instrument is a critical appraisal tool that is intended for researchers and decision makers who require MIDs for interpretation of patient reported outcome measures, including clinical trial investigators, authors of systematic reviews, guideline developers, clinicians, funders, and policy makers.

---

### Use of imaging prior to referral to a musculoskeletal oncologist [^e6077fb0]. The Journal of the American Academy of Orthopaedic Surgeons (2019). High credibility.

Minimally clinically important improvement — treatment effects were considered in terms of the minimally clinically important difference (MCII) in addition to statistical significance, but there were no occurrences of validated MCID outcomes in the included studies; when MCID values from the specific guideline patient population are not available, measures are used in order of priority, including "Another validated measure" and "Statistical Significance".

---

### Health outcomes in primary care: a 20-year evidence map of randomized controlled trials [^9f39ee2c]. Family Practice (2023). Medium credibility.

Statistical analysis

Data were presented as absolute numbers and frequencies for binary and categorical variables, and as median with interquartile range (IQR) for continuous variables. We presented separately characteristics for trials assessing at least one patient health outcome, and for studies assessing only health services outcomes as the primary outcome. Comparisons were performed using the Pearson's chi-square test or Fisher's exact test for discrete variables and the Mann–Whitney test or Kruskal–Wallis test for continuous variables as appropriate. To visualize research gaps, we created heatmaps, grouping studies by outcome categories, types of health care, and types of intervention. For all comparisons, we considered statistically significant an adjusted P -value threshold of less than 0.003 after applying Bonferroni correction for multiple comparisons. Unadjusted P -values were reported throughout the text and tables. All analyses were performed using IBM SPSS Statistics for Windows, Version 26.0 (IBM Corp. Armonk, NY, USA) and Microsoft Excel, MS Office 2019 (Microsoft Corp. Redmond, WA, USA).

---