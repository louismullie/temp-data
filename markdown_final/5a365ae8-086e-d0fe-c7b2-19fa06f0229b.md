# Medical Question & Answer

**Sample ID**: 5a365ae8-086e-d0fe-c7b2-19fa06f0229b
**Dataset Index**: 135825

---

## Question

23. In a regular octagon, three vertices are randomly chosen from the eight vertices to form a triangle. The probability that the triangle has at least one side as a side of the octagon is:
(A) [2/7]
(B) [5/42]
(C) [11/14]
(D) [5/7]
(E) [6/7]

---

## Answer

> Let's see… What do we have here? The user is asking for the probability that a randomly chosen triangle from a regular octagon has at least one side that is also a side of the octagon. Let's break this down step-by-step. First, I need to think about the total number of triangles that can be formed from 8 vertices. Then, I should count how many of those triangles have no side coinciding with an octagon edge, because the complement will give me the desired count. Next, I will verify the complement calculation and finally compute the probability, making sure my arithmetic is correct and aligns with the answer choices.

> Let me first confirm the total number of triangles. Choosing 3 vertices from 8 gives C(8,3) = 56 total triangles, and I should double-check that this is indeed 8!/(3!5!) = 56, which it is [^notfound].

> Now, I need to count triangles that have no side coinciding with an octagon edge. Wait, let me verify the logic here: if no two chosen vertices are adjacent on the octagon, then none of the triangle's sides will be an octagon edge. So I should count 3-element subsets of vertices with no two consecutive. For a cycle of 8 vertices, the number of ways to choose k non-adjacent vertices is given by a standard circular non-adjacent selection formula, and for k = 3 that equals 8/(8−3) × C(8−3, 3) = 8/5 × C(5,3) = 8/5 × 10 = 16. Hold on, I should verify that formula and my arithmetic; yes, the circular non-adjacent count for 3 from 8 is 16, so there are 16 triangles with no octagon-edge side [^notfound].

> Next, I should review the complement. The number of triangles with at least one octagon-edge side is total triangles minus triangles with no octagon-edge side, which is 56 − 16 = 40. I should double-check that subtraction: 56 − 16 is indeed 40, so that looks correct [^notfound].

> Finally, I will compute the probability. The probability is 40/56, which simplifies to 5/7. Let me verify the reduction: divide numerator and denominator by 8 to get 5/7, which matches option D. I should confirm that none of the other options equal 5/7; they do not, so the answer is D [^notfound].

---

The probability that a randomly chosen triangle from a regular octagon has **at least one side as a side of the octagon** is **5/7** [^notfound]. This is found by subtracting the number of triangles with no octagon sides from the total number of triangles, then dividing by the total. There are 56 total triangles, 16 with no octagon sides, so 40 have at least one octagon side, giving 40/56 = 5/7 [^notfound].

---

## Total number of triangles

The total number of triangles is the number of ways to choose 3 vertices from 8, which is:

C(8,3) = [8!/(3!(8−3)!)] = 56

---

## Triangles with no side as a side of the octagon

To count triangles with no side on the octagon, we must select 3 vertices such that **no two are adjacent**. In a regular octagon, this is equivalent to choosing 3 non-adjacent vertices. The number of ways to choose (k) non-adjacent vertices from (n) vertices arranged in a circle is given by:

[n/(n−k)] C(n−k, k)

For (n = 8) and (k = 3):

[8/(8−3)] C(8−3,3) = [8/5] C(5,3) = [8/5] × 10 = 16

So, there are **16 triangles with no side as a side of the octagon** [^notfound].

---

## Triangles with at least one side as a side of the octagon

The number of triangles with at least one side as a side of the octagon is the total number of triangles minus the number with no octagon sides:

56−16 = 40

---

## Probability calculation

The probability is the ratio of favorable outcomes to total outcomes:

[40/56] = [5/7]

---

The probability that a randomly chosen triangle has **at least one side as a side of the octagon** is **5/7** [^notfound].

---

## References

### Backmapping triangulated surfaces to coarse-grained membrane models [^9495d041]. Nature Communications (2020). High credibility.

Fig. 2
Three-dimensional triangulation of a triangle.

a One triangle is divided to four triangles by finding three new vertices in proximity of the mid-point of each of the edges of the original triangle. This increases the vertex number by a factor of 4. b Using the geometrical properties at two neighboring vertexes, an estimate of the geodesic connecting two neighboring vertices can be found (red curve). c By performing several consecutive operations as shown in a, enough points are generated on each triangle for lipid placement.

Step 3: CG protein placement: In this step, each CG protein structure is transferred to its given vertices using. This will guarantee a correct orientation of the proteins both along the normal and on the local plane. To avoid re-equilibration of lipid–protein interactions, instead of transferring only proteins, one can transfer a protein with its neighboring lipids (including lipids underneath for peripheral membrane proteins) from an equilibrated MD structure (Fig. 1).

Step 4: CG lipid placement: We first calculate the number of the required lipids to cover the surface of each monolayer. To do this, we first calculate the total area of the monolayer TS (is monolayer index) aswhereandare the number of proteins and projected area of the transmembrane region of the protein (protein projected area for peripheral membrane proteins). Then, we randomly select a vertex, check if it is in touch with any protein, if not, then place a lipid with a probability of. This step will be iterated untillipids are selected. Our results show a quick convergence of this algorithm (Fig. 1).

---

### Degenerate boundaries for multiple-alternative decisions [^55f4a482]. Nature Communications (2022). High credibility.

Sequential Bayesian inference can be rewritten in a form that more closely resembles a sum of evidence by taking the logarithm of equation (8):For each of the n -choices, the accumulated evidence is the log posterior, with n distinct evidence incrementsand a common subtractive log-marginal term. Calculation of this log-marginal is necessary for multiple (n > 3) choice optimal decision-making, but can be avoided for n = 2 by using the SPRT as mentioned above. A similarterm in a theory of basal ganglia function has been interpreted as suppressing evidence accumulation when choices are ambiguous –.

Within this framework, using that probabilities sum to unity ∑ i P i (t) = 1, the evidence accumulation takes place within an (n − 1)-dimensional linear subspace spanning n vertices { P i = 1, P j ≠ i = 0} of an N -dimensional unit hypercube. This is most easily seen for N = 3 choices, where the decision variables P = (P 1, P 2, P 3) vary within a triangle passing through three vertices {(1, 0, 0), (0, 1, 0), (0, 0, 1)} of the unit cube (Fig. 1, top right panel). So rather than representing evidence accumulation trajectories as a random walk constrained to two dimensions, for multiple alternatives the random walk moves in higher dimensions (a line for two alternatives, a plane/triangle for three, and so on). For simplicity, this vector is initialized with equal prior probabilities 1/ n and updated sequentially until one of the elements reaches a boundary, P i > θ.

Both the SPRT and two-choice sequential Bayesian inference give optimal decisions in that they give the fastest decisions for a given level of accuracy on single trials of known difficulty. Formally, they optimize a cost function, the Bayes risk, that is linear in the mean error rate e and mean decision time T over many trialswith error costs W i > 0 and cost of time c, scaling the expected error and decision time,… This cost function represents the trade-off between speed and accuracy of decision-making: slow but accurate or fast but inaccurate decisions are both costly, and so a balance must be found. For equal decision costs, W 0 = W 1, there is a single threshold that is free parameter, which for optimal decision-making is tuned according to the relative cost c / W of accuracy and time.

---

### Simplicial models of social contagion [^4413a3db]. Nature Communications (2019). High credibility.

Simplicial contagion on synthetic simplicial complexes

A range of models for random simplicial complexes have been proposed so far, starting from the exponential random simplicial complex, the growing and generalized canonical ensemble – and the simplicial configuration modelsto the simplicial activity-driven modelgeneralizing the activity-driven temporal network model. While these yield Erdös−Rényi-like models, of arbitrary complexity, here we are interested in models generating simplicial complexes with simplices of different dimension in which we can control and tune the expected local connectivity, e.g. the number of edges and "full" triangles a node belongs to. We therefore propose a new model to construct random simplicial complexes, the RSC model, which allows us to maintain the average degree of the nodes, 〈 k 1 〉, fixed, while varying at the same time the expected number of "full" triangles (2-simplices) 〈 k Δ 〉 incident on a node. The RSC model of dimension D has D + 1 parameters, namely the number of vertices N and D probabilities { p 1,…, p k,…, p D }, p k ∈ [0, 1], which control for the creation of k -simplices up to dimension D. For the purpose of this study we limit the RSC model to D = 2, which restricts the set of required parameters to (N, p 1, p 2), but the procedure could easily be extended to larger D. The model works as follows. We first create 1-simplices (links) as in the Erdös−Rényi model, by connecting any pair (i, j) of vertices with probability p 1. Similarly, 2-simplices are then created by connecting any triplet (i, j, k) of vertices with probability p 2 ≡ p Δ. Notice that simplicial complexes built in this way are radically different from the clique complexes obtained from Erdös−Rényi graphs, in which every subset of nodes forming a clique is automatically "promoted" to a simplex. Contrarily, in a simplicial complex generated by the RSC model proposed here, a 2-simplex (i, j, k) does not come from the promotion of an "empty" triangle composed by three 1-simplices (i, j), (j, k), (k, i) to a "full triangle" (i, j, k). This also means that the model allows for the presence of (k + 1)-cliques that are not considered k -simplices; therefore, it is able to generate simplicial complexes having both "empty" and "full" triangles, respectively encoding three 2-body interactions and one 3-body interactions (as for instance in Fig. 1b). The expected average numbers of 1- and 2-simplices incident on a node, noted 〈 k 〉 and 〈 k Δ 〉, are easy to calculate (see Methods). Therefore, for any given size N, we can produce simplicial complexes having desired values of 〈 k 〉 and 〈 k Δ 〉 by appropriately tuning p 1 and p Δ. More details about the construction of the model and the tuning of the parameters are provided in the "Methods" section, while the agreement between the expected values of 〈 k 〉 and 〈 k Δ 〉 with the empirical averages obtained from different realizations of the model is discussed in Supplementary Note 1.

---

### Mechanically induced topological transition of spectrin regulates its distribution in the mammalian cell cortex [^26d2d645]. Nature Communications (2024). High credibility.

We also added myosin rods inside the spectrin mesh. These rods attach to spectrin triangles in both extremes and have a cable element with constant. As the myosin linkers, if one of the sides of one spectrin triangle detaches, the corresponding extreme of the myosin rod randomly selects a free spectrin triangle within a radius, to connect. If there are no free spectrin triangles within that radius, then the myosin rod is removed from the network. Furthermore, myosin rods are randomly created and removed from the spectrin mesh at a rateand, respectively. This was implemented in the simulation by adding (removing) a myosin rod at each time-step if a randomly drawn number from a uniform distributionis less than the probability. These rates and the initial number of myosin rods were informed by experimental observations.

The simulation of the full cytoskeleton network composed of edges representing spectrin bundles, stress fibers, myosin linkers or myosin rods, and nodes representing short actin filaments, focal adhesions, or stress fiber connectors, is described below:

At the start of the simulation:
Initialize the network with the spectrin bundles, short actin filaments, stress fibers, focal adhesions, and myosin linkers.
Add the myosin rods to the spectrin mesh to randomly selected locations, verifying that their length is withinand.
At each time-step: For eachedge in the network spanning fromto: Calculate the force generated by the potential energy of their spring and/or cable elementsatand. Remove from the network an edge corresponding spectrin bundles if it generates a force larger than. Update the position of the nodes, by adding the force generated by their edges. For the focal adhesions: If time, move the nodes corresponding to the focal adhesions vertically at velocity, hence. Otherwise, set the velocity to zero. For the edges corresponding to myosin (rods or linkers) attached to a spectrin triangle that lost one of its sides: find a new free spectrin triangle to attach within a radius. If there are no free spectrin triangles within that radius, remove the spectrin edge from the network. If, add a myosin rod to a random location in the spectrin mesh. If, randomly select a myosin rod and remove it from the network.

---

### Physical limits to biomechanical sensing in disordered fibre networks [^be648319]. Nature Communications (2017). Medium credibility.

Random graph captures experimental networks' stiffness range

To investigate the physical origins of the universally broad local stiffness distributions described above, we turned to idealized model networks. One way to generate model disordered networks consists of arranging vertices to lie on a regular lattice, such as a simple-cubic lattice or face-centred-cubic lattice (FCC) in three dimensions(Supplementary Fig. 5a). Connecting these vertices randomly with a fixed probability results in a disordered lattice network. Such lattice networks have provided a useful starting point for characterizing the mechanical response of fibre networks in two dimensions. Interestingly, although three-dimensional simple-cubic lattice and FCC networks also yield a broad range of local stiffnesses (Supplementary Figs 11f and 5b), the geometric s.d. of these distributions (σ loc = 0.29 and 0.37 at, respectively) are considerably smaller than those of the experimental networks at the same value of(Fig. 2c; Supplementary Fig. 1b). Furthermore, the entire local stiffness distribution has almost no dependence on probe length (Supplementary Fig. 7d). A possible reason for these discrepancies is that the disordered lattice networks fail to capture important structural features of real networks at the scale of the cell, such as the fibre length distribution and the random positions of the vertices.

To generate a model network that better matches the structural features of the experimental networks, we begin by distributing vertices randomly throughout a volume. The density of these vertices (which determines the mesh size) is set roughly equal to those of the experimental networks. Pairs of vertices are then connected according to a probability function that depends on the intervertex distance. We chose the probability function to be the simplest form that results in network features consistent with those of the experimental architectures (see Supplementary Methods). The resulting modelled networks are referred to as 'random geometric graphs' (RGGs; Fig. 2b). We computed the local stiffness distribution for the RGG network and found that the geometric s.d. is σ loc = 0.56 at(Fig. 2d), which is approximately equal to those of the experimental networks (Fig. 2c; Supplementary Fig. 1b). Furthermore, we find that both the experimental and RGG networks display an anomalously sensitive dependence of the geometric mean of local stiffness on probe length (Supplementary Fig. 6). Thus, in contrast to the lattice networks, the RGG network appears to quantitatively capture important local mechanical properties of the experimental networks.

---

### Unifying the design space and optimizing linear and nonlinear Truss metamaterials by generative modeling [^91d90a3b]. Nature Communications (2023). High credibility.

We leverage the above representation to construct a dataset containing a large family of truss lattices covering a wide range of mechanical properties. To this end, we begin with a set of three well-known elementary trusses as initial topologies, including the octet, body-centered-cubic, and simple cubic unit cells as 1 × 1 × 1 and 2 × 2 × 2 tessellations. Starting from those, an iterative stochastic perturbation algorithm generates novel structures by randomly inserting/removing both nodes (of edge, vertex, face, or body types) and truss connectivities. New connections are created by connecting newly inserted nodes to at least one of their nearest neighbor nodes. Random perturbations are added to the position of all nodes, while obeying the corresponding positional constraints of the vertex, edge, and face nodes. The above procedure is repeated until the obtained truss lattice satisfies the constraints on the graph connectivity (details provided in Supplementary Note 1). We build a large preliminary library, which includes a variety of trusses — from well-studied structures to unconventional ones, as shown in Fig. 1 c. From the thus-obtained preliminary library, new lattices are created by superimposing two randomly sampled structures (with repetitions allowed). To ensure the physical feasibility of structures generated by random perturbations and superpositions, we enforce the constraint that the lattice is self-connected; i.e. the truss graph must have exactly one maximally connected subgraph spanning the whole graph. Intersecting beams, which may arise from the superposition of different topologies, are fixed by splitting the involved beams and inserting a new node at the intersection point. The resulting lattices constitute the design space as well as the corresponding dataset for ML.

---

### Stabilization mechanism accommodating genome length variation in evolutionarily related viral capsids [^2b4b99bd]. Nature Communications (2025). High credibility.

We proved the theorem as follows. In capsids with hexamers centered on a true 3-fold axis, one can define h H and k H as the hexagonal lattice steps joining a pentamer with the hexamer sitting on a global 3-fold axis, as illustrated for T = 12 in Fig. 4b. By symmetry, the triangle formed by three pentamers is three times larger than the triangle formed by one pentamer and the two vertices placed on nearby 3-fold axes. The normalized area of the small triangle formed by the central hexamer follows the same expression as the classical T-number sequence given by Eq. (1), except that the steps used are h H and k H instead of h and k. The final triangulation number is three times this small triangle, leading to the T-number, T H, for capsids with hexamers centered on global 3-fold axes:

The associated steps h and k can be expressed as a function of h H and k H by h = 2 h H + k H and k = k H − h H (Fig. 4b). Introducing these expressions in Eq. (1) offers an alternative way to derive T H:

Equations (2) and (3) imply that any T-number multiple of three is associated with a capsid with centered hexamers on the global 3-fold axes. This is equivalent to say that any T-number multiplied by three leads to another T-number with hexamers centered on the 3-fold axes. For example, T = 1 leads to T = 3, T = 3 to T = 9, T = 4 to T = 12, etc. This theorem predicts that capsids and procapsids (immature capsids) of tailed phages forming, e.g. T = 3, 9, or 12, will display at least 20 hexamers with 3-fold (or 6-fold) symmetry on the global 3-fold icosahedral axes. This prediction was supported empirically by the published reconstruction of the procapsid of SIO-2 tailed phage, which adopts a T = 12 capsid with non-split hexamers at the true 3-fold axes(Fig. 4c).

---

### Cleaning genotype data from diversity outbred mice [^fbc2be6a]. G3 (2019). Medium credibility.

SNP genotype frequencies:

The SNP genotype frequency distribution across all markers within an animal can be a useful diagnostic. For example, sample contamination can lead to excess heterozygotes. In DO mice, we split the SNP markers into four groups, based on the minor allele frequency (MAF) in the eight founder strains. To do so, we consider only the 68,357 with complete founder genotypes. (For 982 of the polymorphic markers, at least one of the founders has missing genotype.)

To display the genotype frequencies, we use a ternary plot, which makes use of the fact that for any point within an equilateral triangle, the sum of the distances to the three sides is constant. And so the trinomial SNP genotype frequencies for an individual may be represented by a point in the triangle, with the distance to the lower edge being the frequency of heterozygotes, and the distances to the left and right edges being the frequencies of the two homozygotes.

As seen in Figure 4, the DO mice form tight clusters with very similar SNP genotype frequencies, except for a few samples that show high heterozygosity, which are among the samples with high rates of missing genotype data. Another potential outlier is F313, which shows somewhat reduced heterozygosity in SNPs with MAF = 3/8 or 1/2.

Figure 4
SNP genotype frequencies by mouse, for SNPs split by their minor allele frequency (MAF) in the eight founder strains. Trinomial probabilities are represented by points in an equilateral triangle using the distances to the three sides. Pink points indicate the expected distributions.

---

### Efficient classical sampling from gaussian boson sampling distributions on unweighted graphs [^0b2f9fbc]. Nature Communications (2025). High credibility.

Algorithm 2

Double-loop Glauber dynamics

Input: A graph G = (V, E).

Output: A sample of vertex set S of G such that Pr[S] ∝ λ ∣ S ∣ Haf 2 (S).

1 Initialize X 0 as an arbitrary matching in G;

2 Initialize t ← 0, set;

3 while t < T do

4 Choose a uniformly random edge e from E;

5 if e and X t form a new matching then

6 Set X t +1 = X t ∪ { e } with probabilityand otherwise set X t +1 = X t ;7D2

7 else if e is in X t then

8 Uniformly sample a perfect matching E t in the subgraph induced by X t (by running another MCMC on). If e ∉ E t, set X t +1 = X t. If e ∈ E t, set X t +1 = X t \{ e } with probabilityand otherwise set X t +1 = X t;

9 else

10 Set X t +1 = X t;

11 t ← t + 1;7D2

12 Output the vertex set in matching X T ;7D2

We first verify the convergence to the desired distribution. For the edge sets of any two matchings X and X ∪ { e }, on the one hand, from Line 6 in Algorithm 2, On the other hand, if X t = X ∪ { e }, the probability Pr[e ∈ E t] in Line 8 in Algorithm 2 is equal to the ratio of the number of perfect matchings of G X ∪{ e } that contains e to the number of perfect matchings of G X ∪{ e }. The numerator equals the number of perfect matchings of G X. Thus we havewhere the second term comes from the probability of edge e being in E t. Recall that the stationary distribution of matchings should satisfythus if we take λ = c 2, Notice that the number of perfect matchings of G X is Haf(G X). Therefore, when we finally take the vertex set of the sampled matching as the output, the probability of the sampled set of vertices S is:which simulates the output distribution by GBS. We remark that our Algorithm 2 works for any undirected and unweighted graph G.

Next, we rigorously establish that the mixing time of the double-loop Glauber dynamics on dense graphs is at most a polynomial. Specifically, for dense bipartite graphs, we have:

---

### Symmetry control of nanorod superlattice driven by a governing force [^cd7667b5]. Nature Communications (2017). Medium credibility.

Since both the adsorbed R6G chain and the governing force are vertical to the rod side surface, the side facet should be considered. TEM measurements were performed for the tetragonal superlattices (Fig. 3e). In a unit cell of tetragonal superlattice, four nanorods are linked face-to-face and the cross-section of nanorod can be described as an octagon. Further, HRTEM images show the side facets are linked by {100} or {110} ones. Based above TEM observations, we consider nanorod is surrounded by four {100} and four {110} side facets. Adsorption energies for R6G on Au{110} and Au{100} facets are similar (Supplementary Table 1), suggesting that adsorption of R6G on these two facets is equivalent. Obviously, the alignment of two nanorods with larger facets face-to-face can guarantee more R6G chains formed between two nanorods no matter the larger facet is {110} or {100} facet. If {110} facet is larger, two nanorods with opposite {110} facets (denoted as {110}–{110} dimer) would be energetically more favorable than {110}–{100} dimer and {100}–{100} dimer. For the most stable {110}–{110} dimer, the pivotal step to form the hexagonal superlattice or tetragonal superlattice is determined by the location of the next {110}–{110} dimer. The probability of forming different superlattices can be correlated to the overlap area between two neighboring facets in tetragonal and hexagonal superlattices with two {110}–{110} dimers (Tetra-1 and Hexa-1, Fig. 3f). The overlap area (red shadow, Fig. 3f) between two {110}–{110} dimers in Hexa-1 is very small, showing a much lower probability to form the linkage of R6G chain than that in Tetra-1. The other facets combinations also support much lower probabilities of forming the hexagonal superlattices. Energies per unit area of different superlattices confirm that the Tetra-1 is the most stable formation, and with increasing of {110}/{100} ratio the energy per unit area of Tetra-1 decreases accordingly (Fig. 3g). Especially, when the area of {110} and {100} is the same, the formation probability of the tetragonal supperlattice will reach the maximum. Therefore, the tetragonal superlattice is the most thermodynamically favorable, strongly supporting our experimental findings.

---

### Detecting the ultra low dimensionality of real networks [^a2604d73]. Nature Communications (2022). High credibility.

Inferring hidden dimensions

These results suggest a method by which we can infer the hidden dimension D * of a real network. First, we created an ensemble of synthetic surrogates using themodel with different values of the inverse temperature β and dimension D. To preserve the consistency of the hidden degrees in the synthetic networks and of the observed degrees in the original graph, we computed hidden degrees using an iterative processthat forces a match between expected degrees in the model and observed degrees, see subsection "Estimation of the hidden degrees" in the Methods section. This step also ensures that the model reproduces the degree distribution of the real network with high fidelity. After obtaining the hidden degrees, we assigned homogeneous random positions to the N nodes in the D -dimensional similarity space. To keep the process computationally efficient, we restricted the ensemble of synthetic surrogates to feasible values of D and β. The maximum achievable density of edge triangles happens at β = ∞ and decreases monotonously for increasing values of D, as shown above in Fig. 1. Accordingly, we set the maximum dimension to be explored as the maximum dimension able to reproduce the level of edge clustering of the network under study. For each D less than or equal to the maximum value, we decided the set of inverse temperatures, β, to be explored by sampling homogeneously the phase space of edge clustering in a neighborhood of the observed value, see subsection "Estimation of the range of inverse temperatures in the random ensemble" in Methods. The sample of random surrogates was generated using the connectivity law Eq. (1). The densities of edge triangles, squares, and pentagons were then computed for each surrogate in the ensemble and for the real network. Finally, a data-driven classifier was used to infer D * from the surrogates that best approximate the real network in terms of edge cycles.

---

### Feature-based learning improves adaptability without compromising precision [^780b6053]. Nature Communications (2017). Medium credibility.

Experiments 1 and 2

In each of these experiments, subjects completed two sessions (each session composed of 384 trials and lasting about half an hour) of a choice task during which they selected between a pair of objects on each trial (Supplementary Fig. 2a). Objects were one of four colored shapes: blue triangle, red triangle, blue square, and red square. Subjects were asked to choose the object that was more likely to provide a reward in order to maximize the total number of reward points, which would be converted to monetary reward and/or t-points at the end of the experiment.

In each trial, the selection of an object was rewarded only according to its reward probability and independently of the reward probability of the other object. This reward schedule was fixed for a block of trials (block length, L = 48), after which it changed to another reward schedule without any signal to the subject. Sixteen different reward schedules consisting of some permutations of four reward probabilities, (0.1, 0.3, 0.7, and 0.9), were used. In eight of these schedules, a generalizable rule could be used to predict reward probabilities for all objects based on the combinations of their feature values (Supplementary Fig. 2b). In the other eight schedules, no generalizable rule could be used to predict reward probabilities for all objects based on the combinations of their feature values (Supplementary Fig. 2c). For example, the schedule notated as 'Rs' indicates that red objects are much more rewarding than blue objects, square objects are more rewarding than triangle objects, and color (uppercase 'R') is more informative than shape (lower case 's'). In this generalizable schedule, red square was the most rewarding object whereas blue triangle was the least rewarding object. For non-generalizable schedules, only one of the two features was on average informative of reward values. For example, the 'r1' schedule indicated that, overall, red objects were slightly more rewarding than blue objects, but there was no generalizable relationship between the reward values of individual objects and their features (e.g. red square was the most rewarding object, but red triangle was less rewarding than blue triangle). In other words, the non-generalizable reward schedules were designed so that a rule based on feature combination could not predict reward probability on all objects. For example, learning something about a red triangle did not necessarily tell the subject anything about other red objects or other triangle objects.

---

### Complex behavior from intrinsic motivation to occupy future action-state path space [^15c868fd]. Nature Communications (2024). High credibility.

We finally introduced a slight variation to the environment, where the right half of the arena has stochastic state transitions, to showcase the ability of MOP to model risk-sensitive agents. Here, when agents choose an action (force) to be executed, a state transition in the desired direction occurs with probability 1 − η, and a transition corresponding to zero force occurs with probability η (Fig. 4 d). Therefore, a MOP agent that seeks state entropy (β > 0) will show a preference for the right side, where there is in principle higher state entropy resulting from the stochastic transitions over more successor states than on the left side. Indeed, we find that MOP agents spend more time on the right side as β increases, regardless of the probability η (Fig. 4e). For fixed γ, spending more time on the right side can bring the life expectancy to decrease significantly depending on β and η (Supplemental Fig. E.2d, e). Interestingly, for β > 0 there is an optimal value of the noise η that maximizes the fraction of time spent on the right side (Fig. 4f), which is a form of stochastic resonance. Therefore, for different β, qualitatively different behaviors emerge as a function of the noise level η.

MOP agents can also seek entropy of others

Next, we considered an example where an agent seeks to occupy path space, which includes another agent's location as well as its own. The agent can freely move (Fig. 5a; gray triangle) and open or close a fence by pressing a lever in a corner (blue triangle). The pet of the agent (green triangle) can freely move if the fence is open, but when the fence is closed the pet is confined to move in the region where it is currently located. The pet moves randomly at each step, but its available actions are restricted by its available space (Supplemental Sec. E.6).

Fig. 5
Modeling altruism through an optimal tradeoff between own action entropy and other's state entropy.

---

### Structural puzzles in virology solved with an overarching icosahedral design principle [^5f77ba37]. Nature Communications (2019). High credibility.

Methods

The construction of the polyhedral models and their duals is described below.

Construction of polyhedral designs

Consider two lines intersecting at an angle of 60° at the centre of one of the hexagons in the hexagonal (sub)lattice of a given Archimedean lattice. Counting steps between midpoints of adjacent hexagons along these lines via the integer coordinatesand, thencharacterises the positions of other hexagons in the (sub)lattice with respect to the original one, i.e. Using the line connecting the midpoints of these hexagons as the edge of an equilateral triangle of an icosahedral face (Supplementary Fig. 1), the position of the remainder of that surface is uniquely determined, andthus defines a planar embedding of an icosahedral surface into the Archimedean lattice (see examples in Fig. 2). The corresponding polyhedral shape in three dimensions is an icosahedron, obtained via identification of edges of the planar embedding. The numbers of pentagonal, hexagonal, triangular and square faces in the Archimedean lattice overlapping with this icosahedral surface for different values ofandare provided in Supplementary Tables 1–4 for the hexagonal, trihexagonal, snub hexagonal and rhombitrihexagonal lattice, respectively. In particular, an icosahedral face given bycontains either no additional face (hexagonal case), one triangle (trihexagonal case), four triangles (snub hexagonal case), or one triangle and a square (rhombitrihexagonal case), that each form the start of an infinite series of polyhedra.

Construction of the dual lattices

For each polyhedron in the above classification, we construct a dual polyhedron. For this, vertices are positioned at the centres of the polyhedral faces, and vertices associated with adjacent faces connected by straight lines. Since Archimedean lattices have a single type of vertex environment, these dual polyhedra each have a single type of face that corresponds to the fundamental domain of a Laves lattice. These faces are triangles, rhombs, florets and kites for the hexagonal, trihexagonal, snub hexagonal and rhombitrihexagonal lattice, respectively. Using again the planar embedding of an icosahedral surface into the associated Archimedean lattice, we determine the numbers of each such face for polyhedra characterised byandas above; their numbers are listed in Supplementary Table 5.

---

### Graphs [^c6c6d5a7]. Chest (2006). Low credibility.

Two rules of good graphs are presented and explicated.

---

### An [18F] FDG-PET / CT deep learning method for fully automated detection of pathological mediastinal lymph nodes in lung cancer patients [^2989d74a]. European Journal of Nuclear Medicine and Molecular Imaging (2022). Medium credibility.

Classification phase two: false positive reduction

Phase two classified the regions from phase one as negative or positive, i.e. determined if they included a cancerous mediastinal node. In some cases, the regions were large, meaning just predicting the entire region as pathological would not have been useful, as the location of the pathological node would not have been precisely identified. Therefore, the regions were split into overlapping 3D cubes at 8-mm intervals in the x, y, and z axes (forming a regular grid). The cubes were of side length 64 mm.

This gave a total of 46,427 cubes in the training set. Any cubes with a pathological node within 16 mm of their centres were marked as positive, and those without were marked as negative (giving 4,235 positive cubes and 42,192 negative cubes). During training, the negative cubes were randomly undersampled by a factor eight. This considerably sped up training with a negligible effect on performance.

A 50-layer 3D ResNet architecture was used to classify the cubes, based on. The architecture and code for this model are available at. The network was trained for 20 epochs with a batch size of 32 using an Adam optimiser with a learning rate of 10–5 and a cross-entropy loss, by which point the validation loss had stopped decreasing. On-the-fly augmentation was used in the form of random rotations and flips in the x, y, and z axes.

Once trained, this network was used to give a pathological probability for each cube. As the cubes overlapped, there were several predictions for each coordinate. To aggregate these, the predictions at each coordinate were summed, and any coordinate above a threshold of 18 was considered positive (this threshold was found by minimising the sum of the false positives and false negatives on the validation set).

---

### Clustering by measuring local direction centrality for data with heterogeneous density and weak connectivity [^fcddd465]. Nature Communications (2022). High credibility.

Estimation of the number of boundary points for determining T DCM

As shown in Supplementary Fig. 12a, we constructed a Triangulated Irregular Network (TIN) to connect all points. In graph theory, the degree of a vertex is defined as the number of edges incident to the vertex and each edge connects two vertexes. Based on this law, we can obtain:where deg(v i) represents the degree of vertex v i, V denotes the total number of vertexes, and E represents the total number of edges. In a graph, each triangle has three edges and each edge is shared by two triangles except the outermost edges. Actually, for a TIN that has a single connected component, the total number of boundary points is equal to that of the outermost edges, since all the outermost edges are connected end to end by boundary points and form a closed polygon. This law can be summarized as:where F and B refer to the total number of triangles and boundary points respectively. Meanwhile, 2D Euler's formula can be considered as follows:

By combining these formulas, we can infer the solution of B as follows:

However, the number of initial boundary points in the whole TIN is not equal to the total number of boundary points in the separated clusters. To conduct an accurate estimation, the whole TIN should be treated as multiple sub-networks (Supplementary Fig. 12b). Given C clusters, the number of boundary points in clusters can be solved as follows:where F is the total number of intra-cluster triangles in the multiple separated networks. V is known in a given dataset (i.e. n), but F and C are not. The initial F is the total number of triangles in the whole TIN, which includes the triangles connecting different clusters, i.e. cross-cluster triangles whose three vertices are not all in the same cluster (otherwise is intra-cluster triangle). Using the excessive number of triangles would make the number of boundary points B smaller than the true value. To identify the cross-cluster triangles, we set a judgment rule:where v 1, v 2, v 3 are the three vertices of a triangle, and σ (v i, v j) is an indicator function:

---

### Quantifying randomness in real networks [^ade2a2c3]. Nature Communications (2015). Medium credibility.

The general philosophy outlined above is applicable to undirected and directed networks, and it is shared by different approaches, including motifs, graphletsand similar constructions, albeit they violate the inclusiveness condition as we show below. Yet one can still define many different Y -series satisfying both conditions above. Some further criteria are needed to focus on a particular one. One approach is to use degree-based tailored random graphs as null models for both undirectedand directednetworks. The criteria that we use to select a particular Y -series in this study are simplicity and the importance of subgraph- and degree-based statistics in networks. Indeed, in the network representation of a system, subgraphs, their frequency and convergence are the most natural and basic building blocks of the system, among other things forming the basis of the rigorous theory of graph family limits known as graphons, while the degree is the most natural and basic property of individual nodes in the network. Combining the subgraph- and degree-based characteristics leads to dk-series.

---

### From molecular to macroscopic via the rational design of a self-assembled 3D DNA crystal [^7b5f59e7]. Nature (2009). Excellent credibility.

We live in a macroscopic three-dimensional (3D) world, but our best description of the structure of matter is at the atomic and molecular scale. Understanding the relationship between the two scales requires a bridge from the molecular world to the macroscopic world. Connecting these two domains with atomic precision is a central goal of the natural sciences, but it requires high spatial control of the 3D structure of matter. The simplest practical route to producing precisely designed 3D macroscopic objects is to form a crystalline arrangement by self-assembly, because such a periodic array has only conceptually simple requirements: a motif that has a robust 3D structure, dominant affinity interactions between parts of the motif when it self-associates, and predictable structures for these affinity interactions. Fulfilling these three criteria to produce a 3D periodic system is not easy, but should readily be achieved with well-structured branched DNA motifs tailed by sticky ends. Complementary sticky ends associate with each other preferentially and assume the well-known B-DNA structure when they do so; the helically repeating nature of DNA facilitates the construction of a periodic array. It is essential that the directions of propagation associated with the sticky ends do not share the same plane, but extend to form a 3D arrangement of matter. Here we report the crystal structure at 4 A resolution of a designed, self-assembled, 3D crystal based on the DNA tensegrity triangle. The data demonstrate clearly that it is possible to design and self-assemble a well-ordered macromolecular 3D crystalline lattice with precise control.

---

### Quantification of network structural dissimilarities [^3cf663e0]. Nature Communications (2017). Medium credibility.

Identifying and quantifying dissimilarities among graphs is a fundamental and challenging problem of practical importance in many fields of science. Current methods of network comparison are limited to extract only partial information or are computationally very demanding. Here we propose an efficient and precise measure for network comparison, which is based on quantifying differences among distance probability distributions extracted from the networks. Extensive experiments on synthetic and real-world networks show that this measure returns non-zero values only when the graphs are non-isomorphic. Most importantly, the measure proposed here can identify and quantify structural topological differences that have a practical impact on the information flow through the network, such as the presence or absence of critical links that connect or disconnect connected components.

---

### Inverse design of 3D reconfigurable curvilinear modular origami structures using geometric and topological reconstructions [^52f732dd]. Nature Communications (2022). High credibility.

Figure 4 presents an example of reconfigurable structures assembled with varying geometry and topology of modules for a target DOF. Applying the geometric modification in Eqs. (3)–(6) and topological reconstruction in Eqs. (7)–(10) to 28 unit cells within a volumetric mapping size of 40, we obtain sphere reconfigurable structures for various target DOFs. Our algorithm provides multiple solutions, e.g. multiple combinations of geometry and topology for a target. For example, unit cell #1 with a mapping size ofand unit cell #22 with a mapping size ofprovide several topological options with varyingfor target mobility, as shown in Fig. 4.

We plot the DOFs with varying connecting topology while demonstrating various deformation modes, including uniaxial extension and volumetric shrinkage, as shown in Fig. 4 d, e (see the transformation in Supplementary Movie 5). The mobility can also be validated using a linear eigenmode analysis.

The same unit cell, e.g. a cubic unit cell, provides different reconfigurability depending on the variation of the topological connections, which is controlled by the mapping size. Small-sized polyhedrons with amapping can provide a vaster design space in mobility than large-sized polyhedrons with amapping because of the greater chance of available topology, as illustrated in Fig. 4. A unit cell consisting of two tetrahedrons and one octahedron with amapping produces a maximum of 40 connections, resulting in zero mobility, as illustrated in Fig. 3a. Even releasing the connection only produces a narrow range of design space in mobility because of the extruded tubular shapes of the tetrahedron and octahedron being a triangular lattice prism with kinematic mobility of zero.

---

### The burden of proof studies: assessing the evidence of risk [^08990af6]. Nature Medicine (2022). Excellent credibility.

Spline ensemble

To make the risk function estimates robust to knot placement, we created 50 models based on random knot placement samples. Spline estimates depend on the choice of spline parameters, including spline degree, number of knots and knot placement. To mitigate the effect of spline parameter selection on results, we developed an ensemble approach over knot placement, so that the modeler only had to specify the spline degree and number of knots.

Given the degree and number of knots, we automatically sampled a set of knot placements for a feasible knot distribution. For each knot placement, we fit a spline (including nonlinear measurements, shape constraints and trimming), evaluated each resulting model by computing its fit and curvature, and aggregated the final model as a weighted combination of the ensemble.

Sampling knots from simplex

We prefixed a minimal set of rules that describe a feasible set from which to sample knots, and uniformly sample from this set. Given a number of knots, the rules specify feasible ranges for each knot and feasible gaps between knots. Given an intervaldelimited by terminal knots (which are always the minimum and maximum of the data), the feasible region of the interior knotsis denoted by

We enforced the rules

The set of knot placements that satisfy these four rules form a closed polyhedron (a volume in high-dimensional space delineated by hyperplanes). We calculated the vertices of the polyhedron using the double description method in ref. and uniformly sampled knot placements from within the polyhedron. Each knot placement yielded a model, fit using the trimmed constrained spline approach.

Ensemble performance evaluation

Once the ensemble was created, we scored the resulting risk curves using two criteria: model fit (measured using the log-likelihood) and total variation (measured using the highest order derivative). These scores balanced competing objectives of fit and generalizability. Once we had these scores, denoted as s 1 and s 2, we normalized them to the range [0,1]:and applied a logistic transformation. The transformation was used to make the scoring meaningful even in the presence of spurious curves in a large ensemble. We then multiplied the scores to down-weight models that are low under either criterion (fit or total variation). The final weights are normalized to sum to 1.

Using a weighted combination of these metrics, we weighted the 50 models to create the ensemble model.

---

### Distributed attribute representation in the superior parietal lobe during probabilistic decision-making [^e906dd1f]. Human Brain Mapping (2023). Medium credibility.

For each trial in Study 1, the amount and probability of uncertain rewards were displayed on the screen while the fixed but certain reward was verbally told to the participant beforehand and not visually shown, following similar methods as previous studies (Kable & Glimcher; Wang et al.). The amount and the probability attributes appeared side by side divided by a vertical line (see Figure 1a) and whether the amount or the probability appeared on the right side was determined randomly. Participants were asked to respond as quickly as possible within a 3 s trial duration. If no response (Study 1: ranged from 0 to 4.7%, M ± SD = 0.8% ± 1.0%; Study 2: ranged from 0 to 5.6%, M ± SD = 1.3% ± 1.5%) was made within this time window, the task continued to the next trial. To encourage participants to report their true preference for each decision rather than a fixed decision rule (i.e. risk‐aversion) (Tom et al.), we required them to indicate one of four responses to each decision from strongly choosing the certain option to strongly choosing risky option using a four‐button response box (Study 1: the proportion of 1–4 button presses: 20.5%, 19.2%, 40.1%, 19.4%; Study 2: 37.4%, 15.1%, 23.2%, 23.0%). After each decision, the chosen option turned yellow as feedback.

---

### A general method for the creation of dilational surfaces [^6025b588]. Nature Communications (2019). High credibility.

Dilational structures can change in size without changing their shape. Current dilational designs are only suitable for specific shapes or curvatures and often require parts of the structure to move perpendicular to the dilational surface, thereby occupying part of the enclosed volume. Here, we present a general method for creating dilational structures from arbitrary surfaces (2-manifolds with or without boundary), where all motions are tangent to the described surface. The method consists of triangulating the target curved surface and replacing each of the triangular faces by pantograph mechanisms according to a tiling algorithm that avoids collisions between neighboring pantographs. Following this algorithm, any surface can be made to mechanically dilate and could, theoretically, scale from the fully expanded configuration down to a single point. We illustrate the method with three examples of increasing complexity and varying Gaussian curvature.

---

### Evolutionary dynamics on graphs [^c49db24d]. Nature (2005). Excellent credibility.

Evolutionary dynamics have been traditionally studied in the context of homogeneous or spatially extended populations. Here we generalize population structure by arranging individuals on a graph. Each vertex represents an individual. The weighted edges denote reproductive rates which govern how often individuals place offspring into adjacent vertices. The homogeneous population, described by the Moran process, is the special case of a fully connected graph with evenly weighted edges. Spatial structures are described by graphs where vertices are connected with their nearest neighbours. We also explore evolution on random and scale-free networks. We determine the fixation probability of mutants, and characterize those graphs for which fixation behaviour is identical to that of a homogeneous population. Furthermore, some graphs act as suppressors and others as amplifiers of selection. It is even possible to find graphs that guarantee the fixation of any advantageous mutant. We also study frequency-dependent selection and show that the outcome of evolutionary games can depend entirely on the structure of the underlying graph. Evolutionary graph theory has many fascinating applications ranging from ecology to multi-cellular organization and economics.

---

### Rigorous location of phase transitions in hard optimization problems [^d98ebea5]. Nature (2005). Excellent credibility.

It is widely believed that for many optimization problems, no algorithm is substantially more efficient than exhaustive search. This means that finding optimal solutions for many practical problems is completely beyond any current or projected computational capacity. To understand the origin of this extreme 'hardness', computer scientists, mathematicians and physicists have been investigating for two decades a connection between computational complexity and phase transitions in random instances of constraint satisfaction problems. Here we present a mathematically rigorous method for locating such phase transitions. Our method works by analysing the distribution of distances between pairs of solutions as constraints are added. By identifying critical behaviour in the evolution of this distribution, we can pinpoint the threshold location for a number of problems, including the two most-studied ones: random k-SAT and random graph colouring. Our results prove that the heuristic predictions of statistical physics in this context are essentially correct. Moreover, we establish that random instances of constraint satisfaction problems have solutions well beyond the reach of any analysed algorithm.

---

### NEWWS: information on restricted and public use files… [^b8c2e12d]. aspe.hhs.gov (2004). Low credibility.

7 13 0. 1 8 25
0. 3 9 16 0. 2 10: MOST STRONGLY 47
0. 5 VARIABLE FIRST COL LAST COL FORMAT LABEL JCENCOU3 96 100 5. 0 A8: ENCOURAGE A FRIEND. VARIABLE FIRST COL LAST COL FORMAT LABEL JCSRLM2 159 163 5. 0 JOB SEARCH2 RELATIVE START MTH JOB CLUB OR JOB SEARCH, 2ND INSTANCE: RELATIVE START MONTH MONTH 1 = MONTH OF RANDOM ASSIGNMENT N NMISS MEAN STD MIN MAX 158 9517
11. 93 12. 82–77. 00
27. 00 VALUE FREQUENCY PERCENT. VARIABLE FIRST COL LAST COL FORMAT LABEL JCSRLM3 164 168 5. 0 JOB SEARCH3 RELATIVE START MTH JOB CLUB OR JOB SEARCH, 3RD INSTANCE: RELATIVE START MONTH MONTH 1 = MONTH OF RANDOM ASSIGNMENT N NMISS MEAN STD MIN MAX 20 9655
15. 55 7. 28
3. 00 25. 00 VALUE FREQUENCY PERCENT. VARIABLE FIRST COL LAST COL FORMAT LABEL JCSRLM4 169 173 5. 0 JOB SEARCH4 RELATIVE START MTH JOB CLUB OR JOB SEARCH, 4TH INSTANCE: RELATIVE START MONTH MONTH 1 = MONTH OF RANDOM ASSIGNMENT N NMISS MEAN STD MIN MAX 5 9670
13. 40 4. 10
8. 00 18. 00 VALUE FREQUENCY PERCENT.

VARIABLE FIRST COL LAST COL FORMAT LABEL JCERLM1 174 178 5. 0 JOB SEARCH1 RELATIVE END MTH JOB CLUB OR JOB SEARCH, 1ST INSTANCE: RELATIVE END MONTH MONTH 1 = MONTH OF RANDOM ASSIGNMENT N NMISS MEAN STD MIN MAX 1617 8058
4. 89 13. 59–77. 00
34. 00 VALUE FREQUENCY PERCENT. VARIABLE FIRST COL LAST COL FORMAT LABEL JCERLM3 184 188 5. 0 JOB SEARCH3 RELATIVE END MTH JOB CLUB OR JOB SEARCH, 3RD INSTANCE: RELATIVE END MONTH MONTH 1 = MONTH OF RANDOM ASSIGNMENT N NMISS MEAN STD MIN MAX 20 9655
16. 30 7. 11
4. 00 26. 00 VALUE FREQUENCY PERCENT.

---

### Self-assembly of regular hollow icosahedra in salt-free catanionic solutions [^d2614ebc]. Nature (2001). Excellent credibility.

Self-assembled structures having a regular hollow icosahedral form (such as those observed for proteins of virus capsids) can occur as a result of biomineralization processes, but are extremely rare in mineral crystallites. Compact icosahedra made from a boron oxide have been reported, but equivalent structures made of synthetic organic components such as surfactants have not hitherto been observed. It is, however, well known that lipids, as well as mixtures of anionic and cationic single chain surfactants, can readily form bilayers that can adopt a variety of distinct geometric forms: they can fold into soft vesicles or random bilayers (the so-called sponge phase) or form ordered stacks of flat or undulating membranes. Here we show that in salt-free mixtures of anionic and cationic surfactants, such bilayers can self-assemble into hollow aggregates with a regular icosahedral shape. These aggregates are stabilized by the presence of pores located at the vertices of the icosahedra. The resulting structures have a size of about one micrometre and mass of about 1010 daltons, making them larger than any known icosahedral protein assembly or virus capsid. We expect the combination of wall rigidity and holes at vertices of these icosahedral aggregates to be of practical value for controlled drug or DNA release.

---

### Exploring the geometry of treatment networks [^695d4b72]. Annals of Internal Medicine (2008). Low credibility.

Background

Several treatment options exist for many conditions. Randomized trial evidence on the relative merits of various options may be missing or biased.

Purpose

To examine the patterns of trial evidence (network geometry) and explain their implications for the interpretation of the existing evidence on a treatment's relative effectiveness.

Data Sources

PubMed and Thompson ISI Web of Knowledge (last search April 2007).

Study Selection

Published networks of randomized trials that included at least 4 treatments were identified.

Data Extraction

For each network, data on the number of studies per treatment comparison were extracted by one investigator and verified by a second investigator.

Data Synthesis

Indices were adopted from the ecological literature that measure diversity (number of treatments and how often they were tested) and co-occurrence (whether some treatment comparisons were preferred and others avoided). Eighteen eligible treatment networks were identified for different diseases, involving 4 to 16 alternative treatments and 10 to 84 trials. Networks in which 1 option (placebo or no treatment) was the typical comparator were star-shaped, even though several treatments might have had proven effectiveness. Other networks had different shapes. Some showed important co-occurrence that avoided specific head-to-head comparisons. Comparison choices sometimes seemed justified, such as when newer treatments were not compared with older ones already shown to be inferior, whereas other choices seemed to reflect preference bias.

Limitations

Networks evolve over time as new trials accumulate, and their geometry may change. Statistical testing for co-occurrence is underpowered when few trials exist.

Conclusion

Evaluation of the geometry of a treatment network can offer valuable insights for the interpretation of total evidence when many treatment options are available.

---

### The role of prefrontal cortex in the control of feature attention in area V4 [^9ddc09ee]. Nature Communications (2019). High credibility.

During the control sessions, monkey F found the search target on 89% of trials and monkey J on 70% of trials. They found the target after an average of 4.8 and 4.7 saccades, with an average saccadic latency of 241 and 255 ms, for monkeys F and J, respectively. A recent study from our lab showed that the attentional modulation of responses was associated with microsaccades made while animals were fixating a target in a covert spatial attention task. However, similar to what has been found in other studies, these microsaccades only occurred at a median rate of 3.29 Hz, or around once every 300 ms. This was longer than the mean interval between the larger saccades made by the animals during search in the present study, thus precluding the possibility of microsaccades between the larger saccades made during search. In any case, these measures of behavioral performance indicate that the animals used the target template to efficiently guide search as they were significantly smaller than would be expected if the animals had chosen to search the 20-item display serially (i.e. finding the target in 1–20 saccades for an average of 10.5 saccades; one-sample t -test, t 5 = 27.6, P < 10 − 5 and t 7 = 42.2, P < 10 −8, for monkeys F and J, respectively) or randomly (i.e. fixating stimuli for at least 800 ms, the required duration for reward, to determine whether they are the target; t 5 = 78.7, P < 10 −8 and t 7 = 165.1, P < 10 −13). The monkeys' gaze patterns during correct trials (Fig. 1b) also indicated that they used both the color and shape of the target to guide their search. The probability of any given stimulus being fixated during search was strongly influenced by its similarity to the target (ANOVA; F 3,20 = 182 and F 3,28 = 1414, P < 10 −13 and P < 10 −29, for monkeys F and J, respectively), which would otherwise be the same (i.e. 1/20) for all stimuli in the search array. Both a same-color (SMC) and a same-shape (SMS) distractor were significantly more likely to be fixated than a no-share (NS) distractor according to post hoc tests (t -test; SMC vs. NS: t 10 = 13.2 and t 14 = 30.1, P < 10 −6 and P < 10 −13, SMS vs. NS: t 10 = 21.9 and t 14 = 11.2, P < 10 −9 and P < 10 −7, for monkeys F and J, respectively), consistent with previous reports of stimulus selection during conjunction searches. Thus, overall, on trials when the two monkeys correctly found the target, their search behavior was remarkably similar in all measures. Furthermore, all neural data analyzed in this report come from correctly performed trials.

---

### Arrays of individually controlled ions suitable for two-dimensional quantum simulations [^178267d1]. Nature Communications (2016). Medium credibility.

A precisely controlled quantum system may reveal a fundamental understanding of another, less accessible system of interest. A universal quantum computer is currently out of reach, but an analogue quantum simulator that makes relevant observables, interactions and states of a quantum model accessible could permit insight into complex dynamics. Several platforms have been suggested and proof-of-principle experiments have been conducted. Here, we operate two-dimensional arrays of three trapped ions in individually controlled harmonic wells forming equilateral triangles with side lengths 40 and 80μm. In our approach, which is scalable to arbitrary two-dimensional lattices, we demonstrate individual control of the electronic and motional degrees of freedom, preparation of a fiducial initial state with ion motion close to the ground state, as well as a tuning of couplings between ions within experimental sequences. Our work paves the way towards a quantum simulator of two-dimensional systems designed at will.

---

### Constructing future behavior in the hippocampal formation through composition and replay [^09efd61c]. Nature Neuroscience (2025). High credibility.

Discrete and continuous worlds

We implement agents in continuous and discrete environments. Although similar in principle, these two types of agents and environments are slightly different in their practical implementation. The discrete agent behaves on a graph, whereas the continuous agent behaves in a two-dimensional (2D) Euclidean space. Both discrete and continuous settings are deterministic Markov decision processes (MDP).

The discrete environments are defined by a set of locations and actions as in a deterministic MDP. All results here assume rectangular square grid worlds, with actions 'north', 'east', 'south' and 'west'. We generate environments by adding walls and rewards on top of these regular grids. We represent rewards with a population of object vector cells, where each cell fires at a specific vector in relation to the reward, that is, a cell that fires 1 step to the East and so on. We represent each wall with two populations of object vector cells — each vector population centered on one of the wall ends. In more detail, we calculate the object vector population activity at location x relative to object o by concatenating the vectors of one-hot encoded distance from x to o along each action, with −1 distance for actions in the opposite direction. For example, for an object, one step east and three steps south from x, the representation is concatenate(onehot(−1), onehot(1), onehot(3), onehot(−1)). Thus, at any location, there are four vector cells active in the whole population — one for each action. In square grid worlds, this representation has redundancy because east is the opposite of west and north the opposite of south, but this setup allows for accommodating any type of nongrid graph too.

In continuous environments, locations become continuous (x, y) coordinates and actions are steps in a continuous direction. We place walls and rewards within a square 1 m × 1 m arena, at random locations and orientations. Again, we represent rewards by a single population of object vector cells and walls by two populations, one centered on each wall end. A single object vector cell is defined by a 2D Gaussian firing field, tuned to a specific distance and direction from its reference object; the firing fields of the full object vector cell population are distributed on a square grid centered on the object. We thus calculate the object vector population activity at location x relative to object o by evaluating the population of Gaussians centered on o at x.

---

### In search of a drosophila core cellular network with single-cell transcriptome data [^e3c87871]. G3 (2022). Medium credibility.

Evaluating gene and edge commonality distributions

To evaluate commonality and specificity across cell cluster-specific networks, we plot the node and edge commonality distributions. The commonality of a node (gene) refers to the number of cell clusters in which this gene is found to be coexpressed (forming an edge) with at least one other gene. We define the commonality of an edge linking a given pair of genes as the number of cell clusters in which that specific edge is detected. We derived a mathematical approximation for the probability of a gene pair to be coexpressed in a given number of cell types. As we focused on 2,088 commonly expressed genes and selected the top 5% of highly correlated genes in each cell type, in randomized data, a gene pair would have a probability P = 0.05 of being coexpressed in any one cell type. Examining 67 cell clusters and using the binomial distribution, the probability P (k) of a gene pair being coexpressed in k cell types would equalwhere C (67, k) is the combinatorial number describing the number of ways of picking k items from a pool of 67 cell clusters (that is, 67 choose k, or). Following this equation, the probability of a gene pair not being coexpressed in any cell type is 0.0321 at k = 0. Given the 2,088 commonly expressed genes, 2,108,888 nonredundant gene pairs were expected to coexpress in at least one cell type.

Network randomization

To obtain a null distribution for edge commonality distributions, we used a network randomization approach. We randomized the edges in each cell-type-specific network individually, keeping the gene connectivities fixed using the rewire function from the R package iGraph. A set of randomizations for all 67 cell clusters resulted in one pseudo-edge commonality distribution. We performed the randomization procedure 100 times and used the ensemble of the 100 pseudo-edge commonality distributions as the null distribution.

---

### Spatiotemporal regularity and interevent contingencies as information for infants' visual expectations [^7f9dd018]. Infancy (2002). Low credibility.

This study examined infants' use of picture-location contingencies and spatiotemporal regularity in forming visual expectations. Ninety-six 3-month-olds watched an event sequence in which pictures appeared at 3 locations, either in regular left-center-right alternation or in a random center-side pattern. For half of the infants, the content of the central picture was predictive of the location of the upcoming peripheral event. Analyses of anticipations and interpicture fixation shifts revealed that both spatiotemporal regularity and consistent interevent contingencies fostered increased anticipation of peripheral pictures. The type of spatiotemporal sequence that infants observed also differentially biased their responses to test trials that followed the picture sequence: Infants who experienced regular alternation sequences continued the side-to-side pattern during the 2-choice test trials, whereas infants who experienced irregular sequences looked back to the location of the previous picture. Stable interevent contingencies, in contrast, did not bias infants' responses toward the contingent side during the choice test trials.

---

### Self-assembly of tetravalent goldberg polyhedra from 144 small components [^742e87c5]. Nature (2016). Excellent credibility.

Rational control of the self-assembly of large structures is one of the key challenges in chemistry, and is believed to become increasingly difficult and ultimately impossible as the number of components involved increases. So far, it has not been possible to design a self-assembled discrete molecule made up of more than 100 components. Such molecules-for example, spherical virus capsids-are prevalent in nature, which suggests that the difficulty in designing these very large self-assembled molecules is due to a lack of understanding of the underlying design principles. For example, the targeted assembly of a series of large spherical structures containing up to 30 palladium ions coordinated by up to 60 bent organic ligands was achieved by considering their topologies. Here we report the self-assembly of a spherical structure that also contains 30 palladium ions and 60 bent ligands, but belongs to a shape family that has not previously been observed experimentally. The new structure consists of a combination of 8 triangles and 24 squares, and has the symmetry of a tetravalent Goldberg polyhedron. Platonic and Archimedean solids have previously been prepared through self-assembly, as have trivalent Goldberg polyhedra, which occur naturally in the form of virus capsids and fullerenes. But tetravalent Goldberg polyhedra have not previously been reported at the molecular level, although their topologies have been predicted using graph theory. We use graph theory to predict the self-assembly of even larger tetravalent Goldberg polyhedra, which should be more stable, enabling another member of this polyhedron family to be assembled from 144 components: 48 palladium ions and 96 bent ligands.

---

### The expected behaviour of random fields in high dimensions: contradictions in the results of bansal and peterson [^657f9d59]. Magnetic Resonance Imaging (2022). Medium credibility.

Bansal and Peterson (2018) found that in simple stationary Gaussian simulations Random Field Theory incorrectly estimates the number of clusters of a Gaussian field that lie above a threshold. Their results contradict the existing literature and appear to have arisen due to errors in their code. Using reproducible code we demonstrate that in their simulations Random Field Theory correctly predicts the expected number of clusters and therefore that many of their results are invalid.

---

### Machine learning enables completely automatic tuning of a quantum device faster than human experts [^fd71e035]. Nature Communications (2020). High credibility.

Searching for the hypersurface

In each iteration, the algorithm first locates the hypersurface in gate voltage space. To do this, it selects a search direction, specified by a unit vector u which during the first 30 iterations of the algorithm is selected randomly from a hypersphere, restricted to the octant where all gate voltages are negative. The gate voltages are then scanned along a ray beginning at the origin o and parallel to u (Fig. 4 a). During this scan, the current is monitored; when it falls below a threshold of 20% of full scale, this is taken as defining a location v (u) on the hypersurface.

Fig. 4
Characterising the boundary hypersurface using machine learning.

Each panel illustrates a step of the algorithm presented in Fig. 3. The gate voltage space, restricted to two dimensions for illustration, is divided into regions of near-zero (blue) and non-zero current (pink), separated by a boundary hypersurface. a Locating the hypersurface. The gate voltages are scanned along a ray (violet arrow) starting at the origin (white circle) and defined by direction u. By monitoring the current, the intersection with the hypersurface is measured. b To determine whether a region should be pruned, the algorithm scans each gate voltage individually toward the bottom of its range from a location just inside the hypersurface as shown. If only one scan intersects the hypersurface (as in the inset), future exploration of that region is inhibited by displacing the origin as shown. c Based on a short 1D scan, the location is classified according to whether it shows current peaks indicating Coulomb blockade. d If peaks are found, a 2D scan (violet square) is performed in the plane of V 3 and V 7, and is possibly repeated at higher resolution. e From the first thirty measurements (green and yellow circles), the algorithm builds a model of the hypersurface and assigns a probabilitythat peaks will be found. f To refine the model, the algorithm generates a set of candidate search locations (squares), each weighted by its corresponding value of, and selects one at random. A new scan is then performed in the corresponding direction to generate a new measurement of the hypersurface location. Steps d – f are then repeated indefinitely. Inset: Scheme for numerically sampling the hypersurface using simulated Brownian motion. Each point represents a simulated particle moving inside the enclosed volume. The collisions between the particles and the modelled hypersurface generate a set of candidate search locations.

---

### Assembly theory explains and quantifies selection and evolution [^789d4839]. Nature (2023). Excellent credibility.

Within assembly possible, the assembly contingent (A C) is the space of possible configurations of objects where, that is, where selection is possible, and the objects found in the space are controlled by a path-dependency contingent on each object that has already been built. The growth of the assembly contingent is much slower than exponential; indeed, not all possible paths are explored equally. Instead, the dynamics are channelled by constraints imposed by the selectivity emerging along specific paths. Indeed, a signature of selection in assembly spaces is a slower-than-exponential growth of the number of unique objects. To show this, we use a simple phenomenological model of linear polymers to demonstrate how assembly differentiates cases when selection happens. Starting with a single monomer in the assembly pool, the undirected exploration process combines two randomly selected polymers and adds them back to the assembly pool. In the case of directed exploration with selection, the polymer that has been created most recently is selected to join a randomly selected polymer from the assembly pool. For both directed and undirected exploration, this process was iterated up to 10 4 steps and repeated 25 times. For each observed polymer in the assembly pool, the shortest pathway was generated. For each run, the assembly space of multiple coexisting polymers, their joint assembly space, was approximated by the union of the shortest pathways of all observed polymers. An example of joint assembly space in an undirected exploration up to 30 steps is shown in Fig. 4a.

Fig. 4
Undirected and directed exploration in a forward assembly process.

a, The joint assembly space of polymeric chains (with their lengths indicated) after 30 steps created by combining randomly selected polymers from the assembly pool. The length of the realized polymers is shown in blue (observed nodes), whereas nodes shown in black represent polymers that have not been realized but are part of the joint assembly space of all realized objects (contingent nodes). For simplicity of representing the joint assembly space, the edge nodes (shown in red) represent the combined node along the directed graph. b, The comparison between undirected and directed exploration after 100 assembly steps using a graph with radial embedding (observed and contingent nodes shown in red and grey, respectively). c, The mean and standard deviation of the exploration ratio (defined by the ratio of the number of observed nodes and the number of total nodes, which includes observed and contingent nodes) and mean maximum assembly index. n is 25 runs all averaged up to 10 4 assembly steps.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^443e12f1]. Annals of the American Thoracic Society (2023). High credibility.

Unbiased stereological sampling — principles and methods note that "Unbiased sampling designs guarantee statistically representative samples and generally must be spatially randomized", and for anisotropic structures some parameters "require spatial orientation randomization". Named approaches include "systematic uniform random sampling", "isotropic uniform random sampling", "stratified sampling", "fractionator sampling", and "proportionator sampling". Importantly, "Voxel-by-voxel automated image analysis ensures neither accuracy nor precision of the results, nor does it obviate the need to strictly follow stereological sampling principles".

---

### 2023 U.S. department of veterans affairs and U.S. department of defense clinical practice guideline for the management of headache [^7d673313]. Annals of Internal Medicine (2024). High credibility.

Algorithm — this clinical practice guideline (CPG) algorithm is designed to facilitate understanding of the clinical pathway and decision-making process used in managing patients with Headache, and the format represents a simplified flow that helps foster efficient decision making by providers; it includes steps of care in an ordered sequence, decisions to be considered, decision criteria recommended, and actions to be taken; the algorithm is a step-by-step decision tree, with standardized symbols to display each step and arrows connecting numbered boxes indicating the order in which the steps should be followed; shape meanings are specified: rounded rectangles represent a clinical state or condition, hexagons represent a decision point in the process of care formulated as a question that can be answered "Yes" or "No", rectangles represent an action in the process of care, and ovals represent a link to another section within the algorithm; Sidebars 1–7 provide more detailed information, and Appendix K contains alternative text descriptions of the algorithms.

---

### Discovering privileged topologies of molecular knots with self-assembling models [^1b5aac2f]. Nature Communications (2018). Medium credibility.

Despite the several available strategies to build complex supramolecular constructs, only a handful of different molecular knots have been synthesised so far. Here, in response to the quest for further designable topologies, we use Monte Carlo sampling and molecular dynamics simulations, informed by general principles of supramolecular assembly, as a discovery tool for thermodynamically and kinetically accessible knot types made of helical templates. By combining this approach with the exhaustive enumeration of molecular braiding patterns applicable to more general template geometries, we find that only few selected shapes have the closed, symmetric and quasi-planar character typical of synthetic knots. The corresponding collection of admissible topologies is extremely restricted. It covers all known molecular knots but it especially includes a limited set of novel complex ones that have not yet been obtained experimentally, such as 10 124 and 15n 41185, making them privileged targets for future self-assembling experiments.

---

### Detecting the ultra low dimensionality of real networks [^72f4c17c]. Nature Communications (2022). High credibility.

Discussion

Dimensionality is a key concept in the project of understanding the geometrical structure of reality. In recent years, the quest to identify dimensionality has reached computer science and network science where, sustained by phenomena of measure concentration, complex data and interactions only populate a small subspace of their original high-dimensional space. The method we present here does not need any a priori spatial embedding and infers the dimensionality of a graph by exploiting the fact that the densities of edge cycles in its topology carry information about its dimensionality. Our results not only prove that complex networks populate a reduced region of a high-dimensional space but also that they are well represented in hyperbolic geometry with ultra low dimensionality.

This claim is valid for complex networks that are simultaneously sparse, small-world, and highly clustered, and possibly — but not necessarily — with heterogeneous degree distributions. Even if a real network is naturally embedded in Euclidean space, this minimal set of complex features ensures that our methodology is applicable, meaning that explicit Euclidean distances in those systems are not the only factor that determines connectivity in the network and other features may have a role as well. At the same time, our formalism is also valid for geometric random graphs, which are very homogeneous and clustered and non small world networks, which happen in our model when γ > 3 and β > 2 D. This means that graphs in this region of the parameter space are effectively described by the geometry of the D -sphere, which becomes the Euclidean space in the thermodynamic limit, rather than by hyperbolic geometry. Finally, links in D -dimensional lattices are strongly correlated whereas in our model links are statistical independent. D -dimensional lattices are, thus, not suited to being described by our hyperbolic multidimensional model.

---

### The use of opioids in the management of chronic pain: synopsis of the 2022 updated U.S. department of veterans affairs and U.S. department of defense clinical practice guideline [^dded908b]. Annals of Internal Medicine (2023). High credibility.

Algorithm symbol legend defines visual elements as follows: rounded rectangles represent a clinical state or condition, hexagons represent a decision point in the process of care formulated as a question that can be answered "Yes" or "No", rectangles represent an action in the process of care, and ovals represent a link to another section within the algorithm.

---

### Mean-field theory of random close packings of axisymmetric particles [^90ce5be7]. Nature Communications (2013). Medium credibility.

Finding the optimal random packing of non-spherical particles is an open problem with great significance in a broad range of scientific and engineering fields. So far, this search has been performed only empirically on a case-by-case basis, in particular, for shapes like dimers, spherocylinders and ellipsoids of revolution. Here we present a mean-field formalism to estimate the packing density of axisymmetric non-spherical particles. We derive an analytic continuation from the sphere that provides a phase diagram predicting that, for the same coordination number, the density of monodisperse random packings follows the sequence of increasing packing fractions: spheres < oblate ellipsoids < prolate ellipsoids < dimers < spherocylinders. We find the maximal packing densities of 73.1% for spherocylinders and 70.7% for dimers, in good agreement with the largest densities found in simulations. Moreover, we find a packing density of 73.6% for lens-shaped particles, representing the densest random packing of the axisymmetric objects studied so far.

---

### Covariant spatio-temporal receptive fields for spiking neural networks [^3fb65f12]. Nature Communications (2025). High credibility.

Dataset

We simulate an event-based dataset consisting of sparse shape contours, as shown in Fig. 8, based on ref. Events are generated by applying some transformation A to the shapes and subtracting two subsequent frames. We then integrate those differences over time until for each pixel, until reaching a certain threshold and emitting events of either positive or negative polarity. This approximates the dynamics of event cameras, which emit discrete events when the intensity changes by a certain threshold. To obtain sub-pixel accuracy, we perform all transformations in a high-resolution space of 2400 × 2400 pixels, which is downsampled bilinearly to the dataset resolution of 300 × 300.

Fig. 8
Image snapshots from the two synthetic datasets used in this work.

The top row shows the four different spatial scaling, corresponding to shapes of 10, 20, 40, and 80 pixels. The bottom row shows the four different temporal velocities, corresponding to a change of 0.16, 0.32, 0.64, and 1.28 pixels per timestep. Both datasets are extremely sparse, with the shapes barely visible in the images. At larger temporal velocities, the shapes become more discernible.

We use three simple geometries, a triangle, a square, and a circle, that are translated with random motion sampled from the same uniform distribution. The shapes are positioned randomly in the image and oriented by a random, fixed angle. Two datasets are studied: one with varying spatial scales and one with varying temporal velocities. In the spatial scale dataset, the starting scale of the shape is logarithmically distributed from 10 to 80 pixels. In the temporal velocity dataset, the shapes scale logarithmically from ± 0.16 to ± 1.28 pixels. The velocities are normalized to the subsampled pixel space such that, for translational velocity, for example, a velocity of one in the x -axis corresponds to the shape moving one pixel in the x -axis per timestep. The velocity parameterizes the sparsity of the temporal scaling dataset, as seen in the bottom row in Fig. 8 panel, where the shape contours in the left panels are barely visible compared to the faster-moving objects in the right panels. To simulate noise, we add 5 ‰ Bernoulli-distributed background noise.

---

### The management of major depressive disorder: synopsis of the 2022 U.S. department of veterans affairs and U.S. department of defense clinical practice guideline [^3528b93a]. Annals of Internal Medicine (2022). High credibility.

Algorithm for major depressive disorder (MDD) — this CPG's algorithm is designed to facilitate understanding of the clinical pathway and decision making process used in managing patients with MDD; it represents a simplified flow of the management of patients with MDD and helps foster efficient decision making by providers; it includes an ordered sequence of steps of care, decisions to be considered, recommended decision criteria, and actions to be taken; the algorithm is a step-by-step decision tree with standardized symbols and arrows connecting numbered boxes indicating the order in which steps should be followed, and sidebars provide more detailed information; shape legend: rounded rectangles represent a clinical state or condition, hexagons represent a decision point formulated as a question that can be answered "Yes" or "No", rectangles represent an action in the process of care, and ovals represent a link to another section within the algorithm; Appendix G contains alternative text descriptions of the algorithm.

---

### Vortex knots in tangled quantum eigenfunctions [^c02b43ec]. Nature Communications (2016). Medium credibility.

We have identified ∼50,000 knots from around 10 9 curves (of different lengths), computed from about 10 6 random eigenfunctions (at different energies). Some of these knotted curves are relatively short, such as that in Fig. 1d, although the majority of vortex knots are much longer, such as that in Fig. 1e, whose full spatial extent spreads over several periodic cells. At large lengthscales, each curve approximates a Brownian random walk; for closed curves, the radius of gyration scales with the square root of the total length. Most of the vortex knots occur in closed loops; in the 3DHO, however, most vortex lines stretch to spatial infinity as almost-straight lines in the classically forbidden region. Most of the knots in the 3DHO are in these open curves, such as that represented in Fig. 1g. In the periodic cube, many of the longer lines wrap around the cell a non-zero number of times (that is, they have nontrivial homology, so in a tiling of the 3D cells they would be infinitely long and periodic). Since knots in such a periodic space have not been adequately classified mathematically, we only consider knots in closed loops with trivial homology in this system.

Links, which are configurations of two or more vortex curves that are topologically entangled with one another, also occur frequently. In fact, we find links to be more common than knots, consistent with previous investigations of random optical fields at smaller lengthscales in which links are found to be common but knots were not detected. The restriction to knotting in the present study was chosen as it allows comparison with extensive studies of the knotting probability of random curves (as random walks), whereas the study of random linking is not so well developed. Furthermore, linking of open curves (in the 3DHO) is not well defined. For these reasons, our analysis is limited to the statistics of vortex knotting.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^66cbd16d]. Annals of the American Thoracic Society (2023). High credibility.

Quantitative imaging metrics for pulmonary assessment — precision, bias, and unbiased sampling are defined with study design implications. "Bias" refers to systematic errors leading to inaccuracy, whereas "Precision" refers to measurement reproducibility and depends mainly on sampling design and sample characteristics; bias cannot be detected or reduced by more measurements and is avoided a priori by rigorous design using unbiased methods. In a well-designed study, precision is tuned so the within-subject coefficient of error ("noise") is significantly smaller than between-subject biological variation ("signal"). Unbiased sampling designs guarantee statistically representative samples and generally must be spatially randomized, with available approaches including systematic uniform random sampling, isotropic uniform random sampling, stratified sampling, fractionator sampling, and proportionator sampling. The report cautions that voxel-by-voxel automated image analysis ensures neither accuracy nor precision and does not remove the need to follow stereological sampling principles, and illustrates applications where systematic sampling of micro-CT images has been used to compare surface versus core alveoli and to correlate ex vivo small airway gas trapping with in vivo tissue measurements in chronic obstructive pulmonary disease to characterize terminal bronchiole loss, narrowing, and obstruction.

---

### Using symmetry to elucidate the importance of stoichiometry in colloidal crystal assembly [^e9b6cfcf]. Nature Communications (2019). High credibility.

Results

Combining symmetry and stoichiometry

In order to understand how symmetry can be employed to aid in multicomponent crystal structure prediction, consider a primitive cell with periodic boundary conditions, as is typically employed for molecular simulations (cf. Fig. 1a). We may consider discretizing this cell into nodes upon which particles can be placed — although this is only an approximation to the continuous nature of configuration space, this assumption proves very convenient for generating candidate cells and will be relaxed later to make the method fully general. Generating all configurations for a multicomponent mixture on such a grid is effectively impossible for all but the smallest grids due to a combinatorial explosion of the number of possibilities as the size of the cell increases. For instance, in our two-dimensional example, a discretization of the unit cell with area A into equal subunits of size δ 2 leads to N config total configurations:where N i is the number of i -type species (such that).

Fig. 1
Summary of candidate enumeration strategy. a Discretization of a periodic p1 primitive cell into nodes. Symmetry introduces boundary conditions which constrain the placement of particles on the grid as if it were mapped onto the surface of a torus. This represents the orbifold of the p1 wallpaper group. b Fundamental domain and primitive cell for the p6 group; edge nodes are circled in gray on the right and will be wrapped on top of symmetrically equivalent nodes on the left. Here there are 6 node categories we could consider, which may be reduced to 4 if ones with the same stoichiometric contribution are combined. c Representative solutions to the CSP for the p6 lattice shown in b for a binary mixture with a 1:2 stoichiometric ratio of components. Images on the left are drawn from the solutions with the lowest 5% of realizations, and those on the right from the solution with the most. The blue histogram represents the case where node categories with the same stoichiometric contribution are combined, while the orange corresponds to when they are kept separate; the total number of solutions is the same for both

---

### Using symmetry to elucidate the importance of stoichiometry in colloidal crystal assembly [^dff8d404]. Nature Communications (2019). High credibility.

We demonstrate a method based on symmetry to predict the structure of self-assembling, multicomponent colloidal mixtures. This method allows us to feasibly enumerate candidate structures from all symmetry groups and is many orders of magnitude more computationally efficient than combinatorial enumeration of these candidates. In turn, this permits us to compute ground-state phase diagrams for multicomponent systems. While tuning the interparticle potentials to produce potentially complex interactions represents the conventional route to designing exotic lattices, we use this scheme to demonstrate that simple potentials can also give rise to such structures which are thermodynamically stable at moderate to low temperatures. Furthermore, for a model two-dimensional colloidal system, we illustrate that lattices forming a complete set of 2-, 3-, 4-, and 6-fold rotational symmetries can be rationally designed from certain systems by tuning the mixture composition alone, demonstrating that stoichiometric control can be a tool as powerful as directly tuning the interparticle potentials themselves.

---

### A simple rule for the evolution of cooperation on graphs and social networks [^b9bec66b]. Nature (2006). Excellent credibility.

A fundamental aspect of all biological systems is cooperation. Cooperative interactions are required for many levels of biological organization ranging from single cells to groups of animals. Human society is based to a large extent on mechanisms that promote cooperation. It is well known that in unstructured populations, natural selection favours defectors over cooperators. There is much current interest, however, in studying evolutionary games in structured populations and on graphs. These efforts recognize the fact that who-meets-whom is not random, but determined by spatial relationships or social networks. Here we describe a surprisingly simple rule that is a good approximation for all graphs that we have analysed, including cycles, spatial lattices, random regular graphs, random graphs and scale-free networks: natural selection favours cooperation, if the benefit of the altruistic act, b, divided by the cost, c, exceeds the average number of neighbours, k, which means b/c > k. In this case, cooperation can evolve as a consequence of 'social viscosity' even in the absence of reputation effects or strategic complexity.

---

### Graphical data presentation [^8085a383]. Injury (2008). Low credibility.

Figures and charts are the most influential vehicles for distributing scientific information, for affecting decisions as to the acceptance or rejection of a manuscript, and for attracting the attention of the scientific community to study results. Graphical excellence is mainly defined, first, by the highest possible data density (that is, the amount of information provided per graph area); second, by a low ink-to-data ratio (the avoidance of unnecessary shading, three-dimensionality, gridlines and what is often called 'chartjunk'); and third, by clear and unequivocal labelling of axes. The researcher's essential graphical toolbox should contain histograms, bar charts (always with measures of error), box-and-whiskers plots, scatter plots and forest plots.

---

### As-3 [^f4572ab3]. FDA (2025). Medium credibility.

Storage and handling

Store at room temperature (25 °C/77 °F). Avoid excessive heat.

Protect from freezing

---

### Real-space observation of ergodicity transitions in artificial spin ice [^cf112a83]. Nature Communications (2023). High credibility.

Fig. 3
Conditional entropy as a function of temperature.

a Conditional schemes used to estimate the entropy of different subsets of the system. For example, the green coordination three vertices have their entropy estimated by first determining what the entropy of all three spins is when information from the first two is known. b Entropies plotted as a function of temperature compared to the theoretical predictions. The total bitwise entropy, s, is the average of the three types of entropy weighted for their frequency of occurrence in the system. This total entropy quickly drops beneath both theoretical estimates. These entropies were calculated with every XMCD image and their error bars result from standard deviation of the mean over the images at each temperature.

As a first step towards understanding this observed entropy, we compute theoretical thresholds for comparison. Consider a model system that obeys the following rules. In agreement with energetic minimization and the visible spin configurations, we assume that four-nanomagnet and two-nanomagnet vertices are in their Type I and Type α configurations respectively. The number of ways in which Type B vertices may be configured determines the entropy of this model via counting the microstates permitted. We assume an average of two Type B vertices per paired plaquettes (Fig. 2e). Counting this is nontrivial as four of the six Type B sites per paired plaquettes are shared with neighboring sets of paired plaquettes, and therefore the multiplicity of configurations depends on the configuration of their neighbors. The first estimate and highest bound of this counting is the so-called Pauling upper-bound, where the dependence on neighbors is simply ignored, while the second threshold and stricter upper-bound is calculated by a brute-force placement of Type B vertices as shown in Fig. 2e and Supplementary Fig. 2 and in the "Methods" section.

---

### Structural diversity in three-dimensional self-assembly of nanoplatelets by spherical confinement [^f12e4450]. Nature Communications (2022). High credibility.

SA of triangular NPLs in spherical confinement

In the following part of our study, we shifted our focus to a triangular NPLs system, where anisotropy emerges in the flat basal planes of the LaF 3 NPLs stabilized with oleic acid (Fig. 1 b). The LaF 3 NPLs had an equilateral triangular shape with a side length and thickness of 13.8 nm and of 1.6 nm, respectively (total dimension of 16.1 nm × 4.0 nm including ligands; Fig. 5 a, b). The formed defect-rich single-layer superlattice (Fig. 5 a) is reminiscent of the honeycomb structure assembled from beveled Au triangular nanoprisms. The geometry of our LaF 3 NPLs was close to that of NPLs reported in a previous study, which formed a hexagonal close-packed superlattice in bulk. In other regions, the NPLs were found to stack face-to-face, forming ribbonlike arrays (Supplementary Fig. 11 a, b). We found that each triangular NPL stood on the substrate and the NPLs were closely packed side-by-side, where one column of the triangular LaF 3 NPLs oriented on the substrate with their vertices and those in adjacent columns stood on their edges (Fig. 5 b; Supplementary Fig. 11 c). There are two possible ways of stacking such triangular NPLs. The first possible stacking is that the triangular NPLs assemble face-to-face, forming columns. Perpendicular to the direction of the columns, adjacent triangular NPLs form a non-interdigitated yet close-packed structure in the same plane (Supplementary Fig. 12 a). The second possible stacking of the triangular NPLs is similar to the first one except that the adjacent columns shift with respect to each other in the direction of the column, forming interlocked superlattices (Supplementary Fig. 12 b). Views of slightly tilted NPLs standing either with their vertices or edges in an alternative manner on a substrate (Supplementary Fig. 11 b, c) strongly suggest that the self-assembled superlattices correspond to interdigitated ribbons.

---

### Using an algorithmic approach to shape human decision-making through attraction to patterns [^4cbe779a]. Nature Communications (2025). High credibility.

Results

An effective nudge

We submitted two versions of the RaCaS algorithm to the CEC's Dynamic track. Both were based on the principles discussed above and were implemented in an almost identical manner (for the designed difference between them see Supplementary Fig. 5). In fact, the two versions of RaCaS replicated each other's results (see Fig. 2a and Supplementary Fig. 1), including the bias they induced (this and all following tests are two-tailed; mean Bias+ of 70.4% vs. 69.2%; t (260) = 0.563, p = 0.573, two samples t -test; 95% CI: −3%–5%). To quantify support for the null hypothesis, we conducted a Bayesian two samples t -test. This analysis produced a Bayes Factor (BF01) of 6.35 which is conventionally considered as moderate evidence for the lack of a difference between both versions of RaCaS.

Fig. 2
The bias-generating performance of the RaCaS algorithm.

a Boxplots depicting the performance of all the algorithms submitted to the CEC, ordered by magnitude of bias they created. RaCaS is depicted in yellow. Dots indicate individual participants. DS dynamic schedule, SS static schedule. b Two overlaid histograms depicting the proportion of participants showing the magnitude of bias, with 1 denoting maximal bias. RaCaS' performance is shown in yellow and all other competing algorithms appear in gray. The brown dashed line depicts indifference between options. c A raster plot depicting all the RaCaS participants' choices across all 100 trials (the full dataset, ordered by the strength of bias such that the more bias exhibited by a participant, the higher the row depicting their data). A small black bar depicts a single choice of Bias+ and a small white bar, a single choice of Bias−. Small yellow triangles point to the rankings of the three participants depicted in Fig. 1c, d. d The dynamics of the average of Bias+ produced by RaCaS (solid line) and all other competing algorithms (dashed line) across all 100 trials. The RaCaS average bias increased throughout most of the experiment, and always remained well above chance level. The rolling average was calculated with a temporal window of size 10. Bands indicate SEM. Box bounds show interquartile range (IQR), line represent the median, and whiskers extend to points that lie within 1.5 IQRs.

---

### Diffusion sampling schemes: a generalized methodology with nongeometric criteria [^59d3d131]. Magnetic Resonance in Medicine (2023). Medium credibility.

Purpose

The aim of this paper is to show that geometrical criteria for designing multishell q -space sampling procedures do not necessarily translate into reconstruction matrices with high figures of merit commonly used in the compressed sensing theory. In addition, we show that a well-known method for visiting k-space in radial three-dimensional acquisitions, namely, the Spiral Phyllotaxis, is a competitive initialization for the optimization of our nonconvex objective function.

Theory and Methods

We propose the gradient design method WISH (WeIghting SHells) which uses an objective function that accounts for weighted distances between gradients within M-tuples of consecutive shells, with M ranging between 1 and the maximum number of shells S. All the M -tuples share the same weight ω_M. The objective function is optimized for a sample of these weights, using Spiral Phyllotaxis as initialization. State-of-the-art General Electrostatic Energy Minimization (GEEM) and Spherical Codes (SC) were used for comparison. For the three methods, reconstruction matrices of the attenuation signal using MAP-MRI were tested using figures of merit borrowed from the Compressed Sensing theory (namely, Restricted Isometry Property -RIP- and Coherence); we also tested the gradient design using a geometric criterion based on Voronoi cells.

Results

For RIP and Coherence, WISH got better results in at least one combination of weights, whilst the criterion based on Voronoi cells showed an unrelated pattern.

Conclusion

The versatility provided by WISH is supported by better results. Optimization in the weight parameter space is likely to provide additional improvements. For a practical design with an intermediate number of gradients, our results recommend to carry out the methodology here used to determine the appropriate gradient table.

---

### VA / DoD clinical practice guideline for the management of chronic obstructive pulmonary disease [^90bc90ce]. VA/DoD (2021). High credibility.

VA/DoD chronic obstructive pulmonary disease (COPD) algorithm notation — The algorithm is a step-by-step decision tree in which standardized symbols are used to display each step, arrows connect numbered boxes to indicate order, and sidebars provide detailed interpretive information. Rounded rectangles represent a clinical state or condition, hexagons represent a decision point formulated as a question answered "Yes" or "No", rectangles represent an action in the process of care, and ovals represent a link to another section within the algorithm. Appendix J contains alternative text descriptions of the algorithm modules.

---

### A non-spatial account of place and grid cells based on clustering models of concept learning [^59af4302]. Nature Communications (2019). High credibility.

Gridness in trapezoid environments

To simulate the effect of asymmetric boundaries in a trapezoid enclosure on gridness, we took cluster positions from simulations after learning in square environments, and ran an additional learning phase for 250,000 trials. In this new learning phase, the shape of the environment was now a trapezoid (the agent could only move to those locations), and the annealed learning rate schedule continued (starting at 0.0025, reducing to 0.002 at the end). The trapezoid dimensions were 5 × 24 × 50 pixels, closely matching the proportions in(0.2 × 0.9 × 1.9 meters; multiplied by (50/1.9) equals to 5.26, 23.7, and 50).

In order to test whether the asymmetric boundaries of the trapezoid affected gridness, the trapezoid was split into two halves and we computed the grid score for the spatial autocorrelogram on the left (wide) and right (narrow) side of the shape. Due to discretization, we split it as close to equal as possible. The wide half extended from the leftmost pixels to the 17th pixel (338 pixels), and the narrow side extended from the 18th pixel to the 50th pixel (339 pixels).

---

### Solving perfect matchings by frequency-grouped multi-photon events using a silicon chip [^559ac16a]. Nature Communications (2025). High credibility.

In addition, multi-photon processors can enhance classical stochastic algorithms for solving NP problems, such as SAT, max-clique –, and dense subgraph –. The enhancement comes from the positive correlation between perfect matchings counts and the density of the graph. Thus, we can use biased randomness from perfect matchings sampler to enhance stochastic algorithms for these problems. The SAT is the problem of finding an assignment that satisfy the given Boolean formula. A clausal formula can be converted into a graph, and one SAT solution will correspond to a clique of the graph. We set a clausal formula with four clauses. It can be reduced to a 4-Clique problem of an eight-vertex graph shown in Fig. 4 a. The four-photon distribution in 2 h is shown in Fig. 4 b, with a fidelity of 98.25%. The four-vertex subgraph with the most samples is a clique, plotted with the red line in Fig. 4 a. It means that the formula F is satisfiable, and the corresponding assignment is x 1 = True, x 2 = False, and x 3 = True. On the other hand, the densest subgraph problem is defined to find the densest k -vertex subgraph with the largest density from a given n -vertex graph(k < n). No polynomial-time approximation scheme exists for the problem. To avoid being fooled by special graph structures, stochastic algorithms that employ uniform randomness for exploration are always preferable in solving the densest subgraph of a general graph. We verify the enhancement of random search by perfect matchings samples. For example, to find the densest subgraph of a 16-vertex graph with weighted edges as shown in Fig. 4 c, we experimentally obtained the four-photon distribution in 2 h shown in Fig. 4 d, with a fidelity of 92.49%. Note that the perfect matching number of an edge-weighted graph is a more general definition, that the weight of every perfect matching contains a coefficient of the multiplication of the weights of related edges. Then, we use the samples to identify the densest four-vertex subgraph via random search. In the random search, we draw several samples, each containing four vertices, compare the density of the samples, and select the one with the maximum density. We vary the number of samples drawn from 1 to 300 and repeat 400 times for each value. Figure 4 e shows the mean density obtained with a different number of samples. The ideal curve corresponds to the loss-free case. We can see that the protocol whose samples are drawn from the perfect matching solver performs markedly better than the uniform sampling.

---

### Quantitative assessment can stabilize indirect reciprocity under imperfect information [^6cb4a05d]. Nature Communications (2023). High credibility.

Evolutionary dynamics

We now aim to understand the effect of quantitative assessment when players' norms are not fixed, such that their abundance can change over time. We explore how likely it is for a leading eight strategy to evolve, and what cooperation rates are achieved in the population. Similar to our previous setup and to what is common in the literature on indirect reciprocity, we again consider a minimalistic scenario where players can choose from only three norms: a leading eight norm L i, ALLD and ALLC. We assume that the evolution of social norms happens on a longer timescale that is separate from the reputation dynamics. This implies that the reputation dynamics have reached stationarity by the time that social norms change. Iterating the elementary process of reputation updating, we can not only calculate how often on average player i considers player j to be good, but also how often on average they cooperate with j. With the estimated pairwise cooperation rate, with which player i helps player j, we can define the payoff of player i using a fixed strategy as(see "Methods" for details). To model how players adopt new strategies, we then consider simple imitation dynamics –. In every timestep of the evolutionary process, a player i is picked uniformly at random to revise their norm. With probability μ they pick a random new norm. With probability 1 − μ, they randomly choose a role model j to imitate wth a probability P (π i, π j) depending on the difference between the two players' payoffs. This probability takes the form of the Fermi function. The parameter s ≥ 0 describes the strength of selection, which measures how relevant payoffs are for updating strategies. For s = 0, updating happens at random, and as the parameter increases, norms with higher payoffs are more likely to be imitated. We note that for imitation processes to be a reasonable model of strategies spreading, we implicitly assume that people discuss their worldviews and moral guidelines with others.

---

### Mono-planar T-Hex: speed and flexibility for high-resolution 3D imaging [^94f77b72]. Magnetic Resonance in Medicine (2022). Medium credibility.

2 METHODS

2.1 Trajectory design

2.1.1 Tilted hexagonal grids

Fast 3D Fourier encoding is often achieved by stacking identical 2D readouts in the third dimension. EPI and Archimedean spirals then form 2D Cartesian grids in planes orthogonal to the read direction. These planes, which are parallel for EPI and radial about the principal axis of the stack for spirals, shall be referred to as phase‐encoding (PE) planes. However, Cartesian sampling is usuallly suboptimal. Structures with approximately circular or elliptical support are sampled more efficiently on hexagonal grids. In parallel imaging with undersampling, the greater isotropy of hexagonal patterns improves the conditioning of image reconstruction for a given mean sampling density.,

Spiral and EPI trajectories can readily be stacked in such a way that they cover hexagonal grids in their PE planes (Figure 1A). However, due to the finite feasible length of individual readouts, one shot may not suffice to cover an entire nominal k‐space plane when targeting high resolution. The common solution to this problem is to segment each plane into N interleaved shots. However, due to the inflexibility in time allocation arising from N being an integer, it is often impossible to optimize the sequence timing for contrast and SNR as discussed more closely in the Supporting Information.

---

### A natural experiment on the effect of herpes zoster vaccination on dementia [^3913d0dc]. Nature (2025). Excellent credibility.

Fig. 2
The effect of the zoster vaccine on shingles diagnoses.

a – c, Effect estimates of being eligible for (a), and having received (across different follow-up periods (b) and across different grace periods (c)), the zoster vaccine on the probability of having at least one shingles diagnosis during the follow-up period. For a, the MSE-optimal bandwidth is 145.7 weeks (95,227 adults). The grey dots show the mean value for each 10-week increment in week of birth. The grey shading of the dots is proportionate to the weight that observations from this 10-week increment received in the analysis. For b and c, the MSE-optimal bandwidth for our primary specification is 116.9 weeks (76,316 adults). The triangles (rather than points) depict our primary specification. The red (as opposed to white) fillings denote statistical significance (P < 0.05). Grace periods refer to time periods since the index date after which the follow-up time is considered to begin. The grey vertical bars show the 95% CIs around the point estimate of the regression coefficient (two-sided t tests).

We show that our estimated effect is not sensitive to the chosen functional form of the regression used to model the relationship of shingles occurrence with week of birth (Supplementary Fig. 5), the width of the week-of-birth window (bandwidth) around the date-of-birth eligibility cut-off that defines our analysis sample (Supplementary Fig. 6a) or to different grace periods (Fig. 2c). With 'grace periods', we refer to time periods since the index date after which the follow-up time is considered to begin (Methods). There was also a strong indication that the zoster vaccine reduced the probability of having at least one diagnosis of postherpetic neuralgia (a common complication of shingles), although this effect did not reach statistical significance in all specifications (Supplementary Fig. 7).

---

### Rational design of crystalline supermicroporous covalent organic frameworks with triangular topologies [^9e403be5]. Nature Communications (2015). Medium credibility.

Covalent organic frameworks (COFs) are an emerging class of highly ordered porous polymers with many potential applications. They are currently designed and synthesized through hexagonal and tetragonal topologies, limiting the access to and exploration of new structures and properties. Here, we report that a triangular topology can be developed for the rational design and synthesis of a new class of COFs. The triangular topology features small pore sizes down to 12Å, which is among the smallest pores for COFs reported to date, and high π-column densities of up to 0.25nm(-2), which exceeds those of supramolecular columnar π-arrays and other COF materials. These crystalline COFs facilitate π-cloud delocalization and are highly conductive, with a hole mobility that is among the highest reported for COFs and polygraphitic ensembles.

---

### Vortex knots in tangled quantum eigenfunctions [^057acf2e]. Nature Communications (2016). Medium credibility.

Methods

Sampling random eigenfunctions

We generate random eigenfunctions of model systems via standard random superpositions of their degenerate eigenfunctions. Further details and comparisons of these systems, especially their eigenfunctions, are included in Supplementary Note 2.

Tracking vortices (nodal lines)

Within each random eigenfunction, phase vortices are numerically tracked by sampling the wavefunction on a cubic lattice (initial voxel size ∼0.1λ) and checking around each numerical grid plaquette for the circulation of phase indicating the passage of a vortex line through the face of the grid cell. The phase is only guaranteed to circulate in this way when the amplitude is zero but the complex scalar gradient is non-zero (that is, the vortices occur as transverse intersections of real and imaginary nodal surfaces), but this is generically the case in random eigenfunctions; the algorithm would otherwise fail to converge on the vortex line but, as anticipated, we have never observed this to happen. This core procedure does not always fully capture the geometry of the vortex curve (especially where two vortex lines approach closely), and so the above procedure is augmented with a recursive resampling of the numerical grid, at successively higher resolutions, in any location where the vortex line tracking is ambiguous; we call this the RRCG algorithm (see Supplementary Note 2). Since vortex lines must be continuous, the result of the algorithm is a piecewise-linear representation of each vortex in the entire large-scale tangle. The vortex topology is retained, and geometry well recovered, even with a relatively poor initial sampling resolution. The algorithm generalizes to each of periodic boundary conditions, the 3-sphere and the 3DHO via an appropriate choice of lattice boundary conditions, and in the case of the 3-sphere by sampling on multiple Cartesian lattices and joining appropriate boundaries to reproduce the geometry of the sphere.

---

### VA / DoD clinical practice guideline for the non-surgical management of hip&knee osteoarthritis [^4d4c2921]. VA/DoD (2020). High credibility.

Algorithm — hip and knee osteoarthritis (OA) management is presented as a step-by-step decision tree to facilitate understanding of clinical pathways and decision-making, with standardized symbols and ordered flow. This clinical practice guideline includes an algorithm designed to facilitate understanding of the clinical pathways and decision-making processes used in managing patients with OA and includes an ordered sequence of steps of care, recommended observations and examinations, decisions to be considered, and actions to be taken. For each VA/DoD clinical practice guideline, a corresponding clinical algorithm is depicted by a step-by-step decision tree; standardized symbols are used to display each step, and arrows connect the numbered boxes indicating the order in which the steps should be followed. Shape legend for interpreting the algorithm: rounded rectangles represent a clinical state or condition, hexagons represent a decision point formulated as a question that can be answered Yes or No, rectangles represent an action in the process of care, and ovals represent a link to another section within the guideline.

---

### Synopsis of the 2020 U.S. VA / DoD clinical practice guideline for the management of adult overweight and obesity [^f989733d]. Military Medicine (2021). High credibility.

VA/DoD adult overweight and obesity guideline — algorithm structure is described to facilitate understanding of clinical pathways and decision-making in managing patients with overweight or obesity, presented as a step-by-step decision tree that includes an ordered sequence of steps of care, recommended observations and examinations, decisions to be considered, and actions to be taken; standardized symbols indicate elements: rounded rectangles represent a clinical state or condition, hexagons represent a decision point that can be answered Yes or No, rectangles represent an action in the process of care, and ovals represent a link to another section; Appendix K contains alternative text descriptions of the Algorithm.

---

### Efficient classical sampling from gaussian boson sampling distributions on unweighted graphs [^bb10c3c1]. Nature Communications (2025). High credibility.

Theorem 4

Given a bipartite graph G = (V 1, V 2, E) with ∣ V 1 ∣ = m, ∣ V 2 ∣ = n and m ≥ n. If the minimum degree of vertices in V 1 satisfies δ 1 (G) ≥ n − ξ, and the minimum degree of vertices in V 2 satisfies δ 2 (G) ≥ m − ξ for some constant ξ, then for, given error ϵ, we can achieve a sampling in timesuch that the total variation distance between the sampling distribution and the ideal stationary distribution is at most ϵ.

On dense non-bipartite graphs, we have:

Theorem 5

Given a non-bipartite graph G = (V, E) with ∣ V ∣ = 2 n, If the minimum degree of vertices in V satisfies δ 1 (G) ≥ 2 n − 1 − ξ, for some constant ξ, then for, given error ϵ, we can achieve a sampling in timesuch that the total variation distance between the sampling distribution and the ideal stationary distribution is at most ϵ.

We note that our theoretical performance bound in Theorem 4 is stronger for bipartite graphs. This advantage stems from the fact that the subroutine of uniformly sampling perfect matchings is known to be classically more efficient for bipartite graphs. For context, the BipartiteGBS technique also leverages the unique properties of bipartite graphs, though for the different goal of encoding arbitrary matrices for hardness proofs.

Numerical experiments

We conduct experiments to compare our algorithms with prior approaches. All results and plots are obtained by numerical simulations on a 10-core Apple M2 Pro chip with 16 GB memory and an NVIDIA L4 chip with 24 GB GPU memory via python 3.11.0. We use "networkx" libraryto store and manipulate graphs and "thewalrus" libraryto calculate Hafnian from adjacency matrix. Specifically, the graph problems we aim to solve are defined as follows:
Max-Hafnian: Given an undirected graph with non-negative adjacent matrix and target subgraph size k, find the subgraph of size k with the maximum Hafnian value defined as Eq. (2).
Densest k -subgraph: Given an undirected graph with non-negative adjacent matrix and target subgraph size k, find the subgraph of size k with the maximum density value. Density denotes the number of edges divided by the number of vertices.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^6517cbfd]. Annals of the American Thoracic Society (2023). High credibility.

Basic stereological measurements — first-order parameters and probes identify that "First-order parameters are volume (three-dimensional [3D]), surface (two-dimensional [2D]), length (one-dimensional [1D]), and number (zero-dimensional [0D])". Measurements "are performed using geometric test probes, such as points (0D), lines (1D), planes (2D), or volumes (3D)". These probes create countable events in the image, and "Raw counts provide ratios… that are multiplied by the reference space volume to obtain absolute measures for the lung or subcompartment".

---

### Validation of monte carlo estimates of three-class ideal observer operating points for normal data [^26b6e5d9]. Academic Radiology (2013). Low credibility.

Rationale and Objectives

Traditional two-class receiver operating characteristic (ROC) analysis is inadequate for the complete evaluation of observer performance in tasks with more than two classes.

Materials and Methods

Here, a Monte Carlo estimation method for operating point coordinates on a three-class ROC surface is developed and compared with analytically calculated coordinates in two special cases: (1) univariate and (2) restricted bivariate trinormal underlying data.

Results

In both cases, the statistical estimates were found to be good in the sense that the analytical values lay within the 95% confidence interval of the estimated values about 95% of the time.

Conclusions

The statistical estimation method should be key in the development of a pragmatic performance metric for evaluation of observers in classification tasks with three or more classes.

---

### Efficient classical sampling from gaussian boson sampling distributions on unweighted graphs [^7b2529aa]. Nature Communications (2025). High credibility.

Gaussian Boson Sampling (GBS) is a promising candidate for demonstrating quantum computational advantage and can be applied to solving graph-related problems. In this work, we propose Markov chain Monte Carlo-based algorithms to sample from GBS distributions on undirected, unweighted graphs. Our main contribution is a double-loop variant of Glauber dynamics, whose stationary distribution matches the GBS distribution. We further prove that it mixes in polynomial time for dense graphs using a refined canonical path argument. Numerically, we conduct experiments on unweighted graphs with 256 vertices, larger than the scales in former GBS experiments as well as classical simulations. In particular, we show that both the single-loop and double-loop Glauber dynamics improve the performance of original random search and simulated annealing algorithms for the max-Hafnian and densest k-subgraph problems up to 10 ×. Overall, our approach offers both theoretical guarantees and practical advantages for efficient classical sampling from GBS distributions on unweighted graphs.

---

### Confidence reports in decision-making with multiple alternatives violate the Bayesian confidence hypothesis [^6173346e]. Nature Communications (2020). High credibility.

Fig. 2
Models.

a Generative model. Target position is represented by s. Two sources of variability are considered in the model: First, observers have access to noisy measurement x, a Gaussian distribution centered at s with a standard deviation σ. Second, given the same measurement x, the posterior distribution varies across trials due to decision noise, modeled by Dirichlet distribution, of which spread (represented by the shade of the ternary plot) is controlled by a parameter α (see Methods). On each trial, a decisionand a confidence c are read out from the posterior distribution of that trial. b We use ternary plots to represent all possible posterior distributions. For example, a point at the center represents a uniform posterior distribution; at the corners of the ternary plot, the posterior probability of one category is one while the posterior for the other two categories are zeros. c The bar graphs illustrate how confidence is read out from posterior probabilities in each model. For the purposes of these plots, we did not include decision noise here. The color of each ternary plot represents the confidence as a function of posterior distribution for each model. The color is scaled for each ternary plot (independently) to take the whole range of the color bar.

We introduce three models of confidence reports: the Max model, the Entropy model and the Difference model. Each of these models contains two steps: (a) mapping the posterior distribution (q) to a real-valued internal confidence variable; (b) applying three criteria to this confidence variable to divide its space into four regions, which, when ordered, map to the four confidence ratings. The second step accounts for every possible monotonic mapping from the internal confidence variable to the four-point confidence rating. The three models differ only in the first step.

The Max model corresponds to the Bayesian confidence hypothesis. In this model, the confidence variable is the probability that the chosen category is correct, or in other words, it is the highest of the three posterior probabilities (Fig. 2c). In this model, the observer is least confident when the posterior distribution is uniform. Importantly, after the posterior distribution is computed, the posterior probability of the unchosen options does not further contribute to the computation of confidence.

---

### Clotrimazole [^b5a7188a]. FDA. Low credibility.

The dosage of clotrimazole OTIC for treatment of otomycosis in adults is 1 vial OTIC BID for 14 days (1%/0.17 mL)

---

### AGA clinical practice guideline on management of gastroparesis [^9e865be1]. Gastroenterology (2025). High credibility.

Regarding diagnostic investigations for gastroparesis, more specifically with respect to gastric emptying studies, methodology, AGA 2025 guidelines recommend to perform a 4-hour study in all cases, as some patients with normal emptying at 2 hours may be reclassified as delayed at 4 hours.

---

### Standards of care in diabetes – 2025 [^ef149bc3]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for diabetic nephropathy, more specifically with respect to pediatric patients, ADA 2025 guidelines recommend to determine the eGFR at the time of diagnosis and annually thereafter.

---

### European Stroke Organisation (ESO) and European Association of Neurosurgical Societies (EANS) guideline on stroke due to spontaneous intracerebral haemorrhage [^a24b784e]. European Stroke Journal (2025). High credibility.

Regarding diagnostic investigations for intracerebral hemorrhage, more specifically with respect to initial evaluation, EANS/ESO 2025 guidelines recommend to consider using algorithms such as the DIAGRAM for targeted investigation of the cause of spontaneous ICH to improve the performance of prediction regarding the underlying cause, compared to standard care.

---

### Executive summary: diagnosis and management of lung cancer, 3rd ed: American College of Chest Physicians evidence-based clinical practice guidelines [^d4e76b05]. Chest (2013). Medium credibility.

Regarding classification and risk stratification for pulmonary nodules, more specifically with respect to malignancy risk assessment, ACCP 2013 guidelines recommend to consider estimating the pretest probability of malignancy either qualitatively by using clinical judgment and/or quantitatively using a validated model in patients with a solid, indeterminate nodule > 8 mm in diameter.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^7e0b2d1d]. Annals of the American Thoracic Society (2023). High credibility.

Basic stereological measurements for lung imaging — first-order parameters, probes, and reference space are specified. First-order parameters are volume (three-dimensional [3D]), surface (two-dimensional [2D]), length (one-dimensional [1D]), and number (zero-dimensional [0D]). Measurements are performed using geometric test probes, such as points (0D), lines (1D), planes (2D), or volumes (3D), with probe–voxel interactions generating countable events; raw counts yield ratios that are multiplied by the reference space volume to obtain absolute lung or subcompartment measures. It is crucial to define and measure a biologically meaningful reference space for analysis and reporting, because measurements expressed only as ratios are subject to "the reference trap", where changes can arise from the numerator, denominator, or both; an efficient volume measurement method, point counting, is used for acinar components by micro-CT and for extrapulmonary organ volume by CT.

---

### Standards of care in diabetes – 2025 [^31be65a6]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for hypoglycemia, more specifically with respect to screening for cognitive impairment, ADA 2025 guidelines recommend to simplify diabetes treatment plans as much as possible for patients with cognitive impairment and tailor to minimize the risk of hypoglycemia.

---

### Markov chain monte carlo methods for hierarchical clustering of dynamic causal models [^fb89bfda]. Human Brain Mapping (2021). Medium credibility.

In order to gain a better understanding of the failure mode of standard Gibbs, we chose one representative dataset and plotted the posterior estimates of subject assignment estimated for each chain individually (Figure 5). This reveals that for standard Gibbs half of the chains (i.e. Chains 1 and 3) are stuck in a local maxima of the posterior density corresponding to an incorrect clustering. At the same time, Figure 6 shows a difference of over 100 in the mean log‐joint probability between the chains. This indicates that Chains 1 and 3 are most likely stuck in low probability regions of the parameter space which are hard to get out of, for example, a local maximum in the posterior distribution. Conversely, for the improved version, all chains converged to the same area of the parameter space (Figure 5, right) with roughly the same mean log‐joint probability (Figure 6, right). In addition, the estimated posterior subject assignment closely matches the ground truth assignment. Since the initial states of the chains are randomized, this result indicates that our improvements to the Gibbs sampler reduce the chance for chains to be trapped in local maxima of the posterior landscape.

---

### Standards of care in diabetes – 2025 [^9a2ec52c]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for hypoglycemia, more specifically with respect to elderly patients, ADA 2025 guidelines recommend to individualized goal levels for the treatment of hypertension in most older patients.

---

### VA / DoD clinical practice guideline for the primary care management of chronic kidney disease [^13b6e2cb]. DoD/VA (2025). High credibility.

VA/DoD chronic kidney disease (CKD) primary care algorithm legend — The algorithm is a step-by-step decision tree with standardized symbols to display each step. It includes an ordered sequence of steps of care, recommended observations and examinations, decisions to be considered, and actions to be taken; arrows connect the numbered boxes indicating the order in which the steps should be followed, and sidebars provide more detailed information. In the symbol key, rounded rectangles represent a clinical state or condition, hexagons represent a decision point in the guideline formulated as a question that can be answered "Yes" or "No", rectangles represent an action in the process of care, and ovals represent a link to another section within the algorithm.

---

### 2019 ASCCP risk-based management consensus guidelines: methods for risk estimation, recommended management, and validation [^c44c6ea6]. Journal of Lower Genital Tract Disease (2020). High credibility.

Risk-based management uncertainty — confidence interval and decision confidence score: Confidence intervals for each risk estimate were calculated using a normal approximation or exact methods based on the binomial distribution, and a "decision confidence score" is defined that combines the uncertainty in the statistical precision and how close the risk estimates fall to the clinical action thresholds; mathematical details on estimating the recommendation confidence scores are presented in Appendix Part D (http://links.lww.com/LGT/A160).

---

### Meta-reinforcement learning via orbitofrontal cortex [^f751f5a0]. Nature Neuroscience (2023). High credibility.

Quantification of inactivation effects on behavioral history dependence

To quantify the inactivation effects on the behavioral history dependence, we fit the following logistic regression model:whereis the probability of choosing left on trial, is the reward history for left (L) or right (R) side on trial(1 for reward and 0 for no-reward), is the choice history for left (L) or right (R) side on trial(1 for chosen and 0 for unchosen).is 1 on control trials and 0 on inactivation trials, whileis 0 on control trials and 1 on inactivation trials. The model contains separate regression weights for control and inactivation trials. The model was fit to behaviors with L2 regularization (LogisticRegressionCV function in scikit-learn) to prevent overfitting to the data because the number of trials per fit was limited for these inactivation datasets. For the regularization, we selected the inverse of regularization strength from a logarithmic scale between 10 −4 and 10 4 (100 grids) by fivefold cross-validation. We used L-BFGS solvers for the L2 regularizations.

Because the frequency of inactivation trials is only ~13%, the number of control trials is much larger than the number of inactivation trials. To make the history dependence estimations robust against the difference in the trial numbers between control and inactivation trials, we matched the number of control trials to the number of inactivation trials for each model fitting by randomly subsampling the control trials. The subsampling and fitting were repeated with the smallest number of iterations to include every control trial at least once. We took the mean of the regression weights from all the iterations. For |Bias|, we took the absolute value offor each iteration before averaging across iterations.

---

### 2025 ESC guidelines for the management of myocarditis and pericarditis [^b050a21b]. European Heart Journal (2025). High credibility.

Regarding nonpharmacologic interventions for myocarditis, more specifically with respect to physical activity restrictions, ESC 2025 guidelines recommend to restrict physical exercise until remission for at least 1 month in athletes and non-athletes after inflammatory myopericardial syndrome, using an individualized approach to accelerate recovery.

---

### An anatomical study of the arteries of the anterior nasal septum [^ca9aadd3]. Otolaryngology — Head and Neck Surgery (2006). Low credibility.

Objective

The anterior nasal septum is a clinically important area, yet anatomical descriptions of its arterial pattern are hitherto incomplete. This study aimed to test the hypothesis that the arteries of the anterior nasal septum form a fine random network of vessels.

Study Design

The arterial pattern of the nasal septum was traced by microdissection of 12 cadaveric septa (24 sides).

Setting

Each septum was microdissected with a Watson-Barnet dissecting microscope. Specimens were photographed and the vascular patterns were analyzed.

Results

The anterior septum has a consistent large arterial anastomotic triangle receiving major contributions from sphenopalatine, anterior ethmoidal, and superior labial arteries.

Conclusion

Systematic microdissections clearly demonstrate that the arteries of the anterior septum do not form a random network. Instead, they form a consistent arterial anastomotic triangle consisting of large, thin vessels.

Significance

The study provides background material useful to the understanding of nasal physiology and pathology including epistaxis.

---

### Addressing early childhood emotional and behavioral problems [^0988ef83]. Pediatrics (2016). Medium credibility.

Examples of evidence-based treatments for existing diagnoses in young children — This report focuses on programs that target current diagnoses or clear clinical problems (rather than risk) in infants and toddlers.

---

### Experimental measurement of the photonic properties of icosahedral quasicrystals [^a439cd89]. Nature (2005). Excellent credibility.

Quasicrystalline structures may have optical bandgap properties-frequency ranges in which the propagation of light is forbidden-that make them well-suited to the scientific and technological applications for which photonic crystals are normally considered. Such quasicrystals can be constructed from two or more types of dielectric material arranged in a quasiperiodic pattern whose rotational symmetry is forbidden for periodic crystals (such as five-fold symmetry in the plane and icosahedral symmetry in three dimensions). Because quasicrystals have higher point group symmetry than ordinary crystals, their gap centre frequencies are closer and the gaps widths are more uniform-optimal conditions for forming a complete bandgap that is more closely spherically symmetric. Although previous studies have focused on one-dimensional and two-dimensional quasicrystals, where exact (one-dimensional) or approximate (two-dimensional) band structures can be calculated numerically, analogous calculations for the three-dimensional case are computationally challenging and have not yet been performed. Here we circumvent the computational problem by doing an experiment. Using stereolithography, we construct a photonic quasicrystal with centimetre-scale cells and perform microwave transmission measurements. We show that three-dimensional icosahedral quasicrystals exhibit sizeable stop gaps and, despite their quasiperiodicity, yield uncomplicated spectra that allow us to experimentally determine the faces of their effective Brillouin zones. Our studies confirm that they are excellent candidates for photonic bandgap materials.

---

### Inverse design of glass structure with deep graph neural networks [^913cd5d7]. Nature Communications (2021). High credibility.

GNN-guided, property-oriented swap Monte Carlo

In this work, we let the GNN predictions replace the role of energy (E) in Eq. 6 and design a few rules to decide whether to accept a trial (as illustrated in Fig. 3a). The procedure of the GNN-guided, property-oriented swap MC is as follows,

(i) Apply GNN to predict the plastic metric, ln, of each atom in the starting configuration, and record the initial values of the designed target functions. The right panel in Fig. 3a shows the seven target functions (the maximum, mean, and five quantiles) used in this work;

(ii) Select a pair of atoms of different species according to probabilities based on ln (see point (i) in Discussion and Eq. 5 for more details) and swap their species. Then obtain the inherent structure, i.e. the local minimum in PES, using the conjugate-gradient method (see point (ii) in Discussion for the reason why we focus on inherent structures only). The volume is kept fixed to avoid the volumetric effect on plastic resistance. Reconstruct the graph and update the predicted ln for those atoms that are affected by the swap trial (see point (iii) in Discussion for details). Afterwards, for each of the seven target functions, calculate its change after the trial to determine its associated acceptance probability, where Δ F is the target function difference between the trial state and old state, F trial - F old, and C is a constant dependent on the specific target function for tuning the acceptance probability. The ultimate acceptance probability is defined as ∏ p, where p is the probability associated with each target function and ∏ denotes the sequence product. If accepted, the trial configuration will be set as the starting configuration for the next swap trial; and if rejected, the configuration will not change.

(iii) Repeat the (i) and (ii) steps for a fixed number of rounds, until being close to the target property.

---

### VA / DoD clinical practice guideline for management of bipolar disorder [^127e0c9a]. DoD/VA (2023). High credibility.

VA/DoD Clinical Practice Guideline — bipolar disorder algorithm structure and legend define a step-by-step decision tree with standardized symbols and arrows connecting numbered boxes indicating the order in which the steps should be followed; it includes steps of care in an ordered sequence, decisions to be considered, decision criteria recommended, and actions to be taken. Rounded rectangles represent a clinical state or condition; hexagons represent a decision point in the process of care, formulated as a question that can be answered "Yes" or "No".; rectangles represent an action in the process of care; and ovals represent a link to another section within the algorithm. Sidebars 1–7 provide more detailed information to assist in defining and interpreting elements in the boxes, and Appendix M contains alternative text descriptions of the algorithm.

---

### Computed tomography imaging in the context of transcatheter aortic valve implantation (TAVI) / transcatheter aortic valve replacement (TAVR): an expert consensus document of the Society of Cardiovascular Computed Tomography [^6e41ac5f]. Journal of Cardiovascular Computed Tomography (2019). High credibility.

Contouring tools for annular segmentation — Freehand tool or Hounsfield-based contour detection yields a "Non-smoothened, irregular line following path of cursor or detected attenuation threshold" and can cause "Systematic overestimation of perimeter due to non-smoothened contour; Smoothing algorithms, can allow for more realistic perimeter assessment". Polygon methods use "Manual placed segmentation points connected by straight line without interpolation" and "Depending on the number of dots, this may yield a closer estimate of perimeter than freehand contouring without smoothing", whereas spline methods use "Manual placed segmentation points connected by a cubic spline interpolation – 'elastic ruler'" enabling "Accurate quantification of annular perimeter".

---

### Standards of care in diabetes – 2025 [^5eb43e21]. Diabetes Care (2025). High credibility.

Regarding follow-up and surveillance for diabetes mellitus type 1, more specifically with respect to continuous glucose monitoring, ADA 2025 guidelines recommend to implement early continuous glucose monitoring in adult patients with T1DM to improve glycemic outcomes and QoL and minimize hypoglycemia.

---

### 2025 ESC guidelines for the management of myocarditis and pericarditis [^230fe03b]. European Heart Journal (2025). High credibility.

Regarding specific circumstances for acute pericarditis, more specifically with respect to patients with post-cardiac injury syndromes, ESC 2025 guidelines recommend to initiate IL-1 antagonists in patients with refractory post-cardiotomy inflammatory syndrome to prevent recurrences and progression to constriction.

---

### Practical guide to choosing an appropriate data display [^63bd2040]. Otolaryngology — Head and Neck Surgery (2011). Low credibility.

The primary objective of graphing research data is to communicate key information visually in a rapid, accurate, and concise way. Graphs might be considered visual take-home lessons of the major point(s) of the manuscript. In choosing a graph, it is tempting to concentrate only on ways of illustrating summary statements characterizing the group(s). However, individual patients are unique, and their characteristics or outcomes may not be predicted by a group summary. Consequently, if possible, graphs should demonstrate individual responses as well as group summaries. "Graphical literacy", "graphical excellence", and "graphical acumen" are achievable with work and collaboration. To produce a well-designed graph, a combination of by-subject detail and overall results should be the goal within the same illustration. The practice gap addressed in this article is that little attention from authors, reviewers, editors, and publishers seems to be paid to graphical literacy. The purpose of this article is to present some practical guidelines for choosing or evaluating more appropriate data displays.

---

### VA / DoD clinical practice guideline for the management of upper limb amputation rehabilitation [^a33a9ba2]. DoD/VA (2022). High credibility.

VA/DoD upper limb amputation rehabilitation algorithm — overview and symbol legend: The algorithm is designed to facilitate understanding of the clinical pathway and decision making process used in managing patients with upper limb amputation (ULA) and represents a simplified flow including an ordered sequence of steps of care, decisions to be considered, recommended decision criteria, and actions to be taken. It is a step-by-step decision tree with standardized symbols; arrows connect the numbered boxes indicating the order in which the steps should be followed, and sidebars provide more detailed information. Shape meanings are defined as follows: rounded rectangles represent a clinical state or condition; hexagons represent a decision point formulated as a question that can be answered "Yes" or "No"; rectangles represent an action in the process of care; and ovals represent a link to another section within the algorithm. Appendix N contains alternative text descriptions of the algorithm.

---

### Highly parallel and ultra-low-power probabilistic reasoning with programmable gaussian-like memory transistors [^c51d0577]. Nature Communications (2024). High credibility.

Probabilistic inference in data-driven models is promising for predicting outputs and associated confidence levels, alleviating risks arising from overconfidence. However, implementing complex computations with minimal devices still remains challenging. Here, utilizing a heterojunction of p- and n-type semiconductors coupled with separate floating-gate configuration, a Gaussian-like memory transistor is proposed, where a programmable Gaussian-like current-voltage response is achieved within a single device. A separate floating-gate structure allows for exquisite control of the Gaussian-like current output to a significant extent through simple programming, with an over 10000s retention performance and mechanical flexibility. This enables physical evaluation of complex distribution functions with the simplified circuit design and higher parallelism. Successful implementation for localization and obstacle avoidance tasks is demonstrated using Gaussian-like curves produced from Gaussian-like memory transistor. With its ultralow-power consumption, simplified design, and programmable Gaussian-like outputs, our 3-terminal Gaussian-like memory transistor holds potential as a hardware platform for probabilistic inference computing.

---