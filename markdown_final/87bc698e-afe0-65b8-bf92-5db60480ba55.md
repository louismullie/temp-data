# Medical Question & Answer

**Sample ID**: 87bc698e-afe0-65b8-bf92-5db60480ba55
**Dataset Index**: 30677

---

## Question

Find, with explanation, the maximum value of $f(x) = x^3-3x$ on the set of all real numbers $x$ satisfying $x^4+36\leq 13x^2$.

---

## Answer

> Let's see… What do we have here? The user is asking to find the maximum value of the function f(x) = x³ − 3x on the set of real numbers x satisfying the inequality x⁴ + 36 ≤ 13x². Let's break this down step-by-step. First, I need to think about the feasible region defined by the inequality. Then, I should analyze the critical points of f(x) and how they relate to that region. Next, I will evaluate f(x) at the boundary points and any interior critical points within the region. Finally, I will compare these values to determine the maximum and confirm it with a quick sanity check of the function's behavior.

> Let me first confirm the constraint set. The inequality x⁴ + 36 ≤ 13x² can be rewritten as x⁴ − 13x² + 36 ≤ 0, which factors as (x² − 4)(x² − 9) ≤ 0. Wait, let me verify the factorization: yes, (x² − 4)(x² − 9) = x⁴ − 13x² + 36, so that checks out. This product is nonpositive when one factor is nonpositive and the other is nonnegative, which occurs when 4 ≤ x² ≤ 9, meaning x ∈ [−3, −2] ∪ [2, 3]. So the feasible region is the union of two closed intervals, and I need to consider both when maximizing f(x).

> Next, I should review the function to maximize. f(x) = x³ − 3x is a cubic polynomial. Let me think about its derivative to find critical points: f′(x) = 3x² − 3 = 3(x² − 1), which equals zero when x = ± 1. Hold on, I should verify whether these critical points lie in the feasible region. Since the feasible region is [−3, −2] ∪ [2, 3], neither x = 1 nor x = −1 is in the interior of the feasible set, so there are no interior critical points to evaluate. Therefore, the maximum must occur at one of the endpoints of the intervals.

> I will now examine the endpoints. Let me calculate f(x) at x = −3, −2, 2, and 3. f(−3) = (−3)³ − 3(−3) = −27 + 9 = −18, f(−2) = (−2)³ − 3(−2) = −8 + 6 = −2, f(2) = 2³ − 3(2) = 8 − 6 = 2, and f(3) = 3³ − 3(3) = 27 − 9 = 18. Comparing these, the largest value is 18 at x = 3, and the next largest is 2 at x = 2, with negative values at the left endpoints. Wait, let me double-check the arithmetic: yes, f(3) = 18 is correct.

> But wait, what if the function is increasing across the interval and I might miss a higher value just inside the boundary? Hold on, let's not jump to conclusions. Since there are no interior critical points in [2, 3] or [−3, −2], the extrema on each closed interval must occur at the endpoints. Given that f is continuous, I can be confident that the maximum on the feasible set is indeed 18 at x = 3, and the minimum is −18 at x = −3. I should confirm that f is increasing on [2, 3]: f′(x) = 3(x² − 1) is positive when x² > 1, so on [2, 3] the derivative is positive, confirming f is increasing there, which supports that the maximum on the right interval is at x = 3.

> Final answer: The maximum value of f(x) = x³ − 3x subject to x⁴ + 36 ≤ 13x² is 18, achieved at x = 3.

---

The maximum value of (f(x) = x^3–3x) on the set where (x^4 + 36 \\leq 13x^2) is **18**, achieved at (x = 3). The constraint set is (x \\in [-3, -2] \\cup [2, 3]), and the function has no interior critical points there; thus, the maximum occurs at an endpoint, specifically at (x = 3).

---

## Step 1: Determine the constraint set

The inequality (x^4 + 36 \\leq 13x^2) is rearranged to:

[
x^4–13x^2 + 36 \\leq 0
]

Let (y = x^2), then:

[
y^2–13y + 36 \\leq 0
]

Factoring gives:

[
(y - 4)(y - 9) \\leq 0
]

So (4 \\leq y \\leq 9), which means:

[
4 \\leq x^2 \\leq 9
]

Thus, the constraint set is:

[
x \\in [-3, -2] \\cup [2, 3]
]

---

## Step 2: Analyze the function (f(x) = x^3–3x)

Find the derivative:

[
f'(x) = 3x^2–3 = 3(x^2–1)
]

Set (f'(x) = 0):

[
x^2–1 = 0 \\implies x = \\pm 1
]

These critical points are **not in the constraint set**, so the maximum must occur at an endpoint.

---

## Step 3: Evaluate (f(x)) at the endpoints

| **Endpoint** | **F(x) = x³ - 3x** |
|-|-|
| x = -3 | (-3)³ - 3(-3) = -27 + 9 = -18 |
| x = -2 | (-2)³ - 3(-2) = -8 + 6 = -2 |
| x = 2 | 2³ - 3(2) = 8–6 = 2 |
| x = 3 | 3³ - 3(3) = 27–9 = 18 |

---

The maximum value is **18**, achieved at (x = 3).

---

## Conclusion

The maximum value of (f(x) = x^3–3x) on the set where (x^4 + 36 \\leq 13x^2) is **18**, achieved at (x = 3).

---

## References

### Towards a fully automated algorithm driven platform for biosystems design [^ed08b9a9]. Nature Communications (2019). High credibility.

We next sought to illustrate the optimization method with a similar 3-variable function with three inputs and one output to simulate a similar multi-dimensional optimization problem. It is noteworthy that Bayesian optimization has been used in numerous applications – and the purpose of this simulation is testing the algorithm on a simple but similar setting. The search perimeter was set to be 1–24 for each of the inputs and the maximum of the function was set to be 9 (y = f (x 1, x 2, x 3) | x i ɛ {1, 2,…, 24}, f max = 9). The Bayesian optimization algorithm was able to find the maximum value of this function by only evaluating 12 points out of all possible 24 3 = 13,824 points. These 12 evaluations were the result of 12 iterations of learning and testing, with each evaluation being followed by a learn step that produced the next point to evaluate. We then sought to compare this optimization strategy with baseline approach where randomly sampled points are evaluated and all of these points are used to train an Exterior Derivative Estimation (EDE)-based regression model described in previous publications. We found that although the EDE approach shows impressive predictive capability, especially given that all the data have been acquired at once and not through iterative sampling, even after sampling 192 random points, the maximum could not be found (Supplementary Table 1).

---

### Size limits the sensitivity of kinetic schemes [^71d1065f]. Nature Communications (2023). High credibility.

A different measure of sensitivity — the amplification of a fold-change in the input — provides a solution to this problem. Suppose that for some value x 0 of the input parameter x, scaling by a factor a scales the output by b, so f (a x 0) = b f (x 0). Then the quotientcan be thought of as a discrete approximation of the derivative defining the logarithmic sensitivity. And if f (x) is differentiable everywhere, then by the mean value theorem, there must be a value x * of x for whichThis means that careful measurement of any two points on the input-output curve (x versus f (x)) witnesses the (local, infinitesimal) logarithmic sensitivity somewhere. Importantly — unlike in the case of fitting to a Hill function — if error in the measurements is very low, then they are also telling us the derivative for some value of x very accurately.

Equation (25) leads us to another common definition of the effective Hill coefficient:where S 0.9 and S 0.1 are the values of the input variable (in our case, x) required to get 90% and 10% (respectively) of the maximum value of the output variable (in our case, f (x)). Note that (26) is like (25) with a = S 0.9 / S 0.1 and b = 9. It implies that somewhere between S 0.1 and S 0.9 there is a logarithmic sensitivity of H eff /2.

There is yet another common definition, specific to models of binding. Suppose x is the concentration of a ligand and 〈 n b 〉(x) is the expected number of sites bound by a ligand out of a total of n possible binding sites. It is common then, to takeor to report, as the Hill coefficient, the slope of a line fitted to x versusdata on a log-log plot.

---

### Principled approach to the selection of the embedding dimension of networks [^8cd7e63d]. Nature Communications (2021). High credibility.

Principled selection of a suitable value for the embedding dimension

The observed behavior of the loss function L (d) indicates that embedding algorithms may generate sufficiently accurate geometric descriptions with a relatively small value of d. Assuming that the plateau value L ∞ of the loss function L (d) corresponds to the best geometric description that the embedding algorithm can achieve, we indicate withthe minimal d value such that the difference between L (d) and the optimum L ∞ is less than ϵ. As already stated, a key implicit assumption here is that the empirically trained max-dimensional embedding is the best achievable by the algorithm. d o (ϵ) is then the best choice for the embedding dimension that can be made to achieve a geometric representation that uses less training time and computational resources but is still sufficiently similar (i.e. within an ϵ margin) to the best achievable representation of the network. Our formulation of the problem does not explicitly account for the fact that the network at hand may be a specific realization of some underlying stochastic model which the embedding method may rely on. Rather, Eq. (1) defines a low-rank approximation problem, meaning that we aim at finding the low-dimensional (i.e. low-rank) approximation that best describes the high-dimensional (i.e. full-rank) embedding matrix. We differentiate from the standard setting considered in low-rank approximation problems in two main respects. First, we do not rely on a standard metric of distance (e.g. Frobenius distance) between the low- and the full-rank embedding matrices. Instead, we make use of a measure of collective congruence between the embeddings, i.e. Eq. (6). The normalized embedding loss function is equivalent to a metric of distance between matrices only for spectral embedding methods such as LE. However, the equivalence is not immediate for arbitrary embedding methods. Second and more important, in a standard low-rank approximation, one generally aims at finding the best low-rank reduction of a full-rank matrix by constraining the maximum rank of the approximation. In our case instead, we seek the best rank reduction constrained by a maximum amount of tolerated discrepancy between the low- and the high-dimensional embeddings.

---

### Over-exploitation of natural resources is followed by inevitable declines in economic growth and discount rate [^b6b7ffb0]. Nature Communications (2019). High credibility.

We used algorithms that find the exact solutions provided that the resolutions are sufficiently fine. Specifically, to find the optimal solution numerically, our algorithm uses Stochastic Programming with backward induction (Supplementary Note 4). (Note that the model's dynamics are deterministic but the general method is still called stochastic.) To find the market solution, our algorithm also uses Stochastic Programming to solve for a given value of X. But it finds a solution multiple times, each time for a different value of X, until it finds the solution that satisfies the consisteny criterion. These algorithms are coded in C/C++ and are described in detail in Supplementary Note 4.

In turn, in the results shown in Fig. 3, system 2, as well as in Figs. 4 and 5 and in the graphical tool, we assume that the dynamics of c and f follow Eqs. 6–9, but we consider harvest functions that are not given by either the optimal solution or the market solution. In Fig. 3, system 2, we consider harvest functions that follow the market solution until t = t 1 and after t = t 1 + 10, but between these times, the non-sustainable harvest decreases gradually from its maximal level to zero. In Fig. 4, we calculate Δ sus, which is the cumulative discount that emerges if the harvest is entirely sustainable, namely, H n = 0 and H s = x 1 + x 2 if t > 0. Also, in Fig. 4a, we consider three scenarios in which the non-sustainable harvest is higher in the beginning but eventually approaches zero, while H n + H s = x 1 + x 2.

After we determine the harvest functions, the functions c (t) and f (t) are calculated according to Eqs. 6 and 9. In turn, we calculate the discount rate and the cumulative discount according to Eq. 5 (where the cumulative discount is the integral over time of the discount rate). Specifically, for the case in which only sustainable harvest is used (Δ sus in Fig. 4), the discount rates are calculated in Supplementary Note 2 and are given by Eqs. B5 and B12. The prices are given by Eq. A10, and the total product is given by Eq. A11. All of these equations are derived in Supplementary Notes 1, 2.

---

### Using both qualitative and quantitative data in parameter identification for systems biology models [^efc443df]. Nature Communications (2018). Medium credibility.

Fitting with qualitative and quantitative data

In the following sections, we will demonstrate an approach by which we incorporate both qualitative and quantitative data into a parameter identification procedure. We minimize an objective function with contributions from both types of data:where x is the vector of the unknown model parameters. The term f quant (x) is a standard sum of squares over all quantitative data points j :For the term f qual (x), we construct a function that likewise takes a lower value as the qualitative data are better matched by model outputs. We express each qualitative data point as an inequality of the form g i (x) < 0. Given this constraint, we seek to minimize the value of C i · max(0, g i (x)), where C i is a problem-specific constant. In other words, if the constraint g i (x) < 0 is violated, we apply a penalty proportional to the magnitude of constraint violation. The final objective function to be minimized consists of the sum of the penalties arising from all of the individual constraints:In the constrained optimization literature, Eq. (3) is called a static penalty function. The squared difference, max(0, g i (x)) 2, is also sometimes used, but we chose Eq. (3) to avoid overpenalizing single constraints with a large degree of violation.

f tot (x) can be minimized using a standard optimization algorithm such as differential evolutionor scatter search.

Fitting a model of Raf inhibition

Next, we demonstrate usage of a combination of qualitative and quantitative data for parameter identification (which we will also refer to as finding a fit to the data) for a simple biological model. We use synthetic data to parameterize a model, originally described in Kholodenko et al. for the dimerization of the protein kinase Raf and the inhibition of Raf by a kinase inhibitor. This model has relevance for cancer treatment. We show that qualitative data can be used to improve confidence limits on the parameter values.

We consider the model shown in Fig. 2a. Raf (R) is able to dimerize, and each Raf monomer is able to bind an inhibitor (I). The model parameters consist of six equilibrium constants, denoted K 1 through K 6. Note that these six parameters are not independent. By detailed balance, K 4 = K 1 K 3 / K 2, and K 6 = K 4 K 5 / K 2 = , leaving four independent model parameters.

---

### Have we been qualifying measurable residual disease correctly? [^a3800591]. Leukemia (2023). Medium credibility.

Is MRD worst_case an index or a metric for MRD?

Index is defined as a number (such as a ratio) derived from a series of observations and used as an indicator or measure. Metric is defined as a standard of measurement. Some may argueis an index for MRD whilstis a metric. The distinction between index and metric is in some measure semantic. Evenis a statistical construct for estimating likelihood of relapse.is what statisticians call a maximum-likelihood estimate, which is not the same as an estimate for the median (i.e. 50th-percentile) value among all the possible values of true MRD conditional on test result (Supplementary Methods). Whenis zerois actually the 0th-percentile (i.e. the lowest possible) value among all the possible values of true MRD conditional on test result!, on the other hand, is the 95th-percentile value among all the possible values of true MRD conditional on test result.

---

### Using both qualitative and quantitative data in parameter identification for systems biology models [^a956cd8d]. Nature Communications (2018). Medium credibility.

We must choose the penalty constants C i in Eq. (3) based on our confidence in the qualitative data — a larger C i gives the qualitative data more weight compared to the quantitative data. We hand-selected a value of C i = 0.03 for all i, to give roughly equal contributions from the qualitative and quantitative datasets (i.e. such that the blue and orange curves in Fig. 2d, e can be plotted using the same scale). Note that giving equal contributions to both datasets is an arbitrary choice for illustration — with real experimental data, the modeler may choose to give unequal weights based on the relative importance/credibility of the datasets.

We minimized f tot (x) by differential evolution, and found best-fit parameters of K 3 = 5100 μM −1 and K 5 = 0.060 μM −1, which are reasonably close to the ground truth.

To evaluate the strengths of combining quantitative and qualitative data, we performed uncertainty quantification. We used a variant of the profile likelihood approach, a method that is well-established for quantitative fitting. One parameter of interest is held fixed, and the objective function is minimized by varying the remaining parameters. The resulting minimum is taken as the negative log likelihood of the fixed parameter value. The minimization is repeated for many possible fixed values of the parameter to produce a curve (Fig. 2d, e). Note that our objective function is not a likelihood in the rigorous sense, but has a similar interpretation in that a lower objective value indicates the parameter value is more consistent with the data.

We performed profile likelihood analysis using f quant (x), f qual (x), and f tot (x) in turn as the objective function, and considering two free parameters K 3 and K 5 (Fig. 2d, e). We found that each dataset individually provided bounds on possible parameter values, but the tightest bounds were obtained when we combined both datasets.

---

### A dual role in regulation and toxicity for the disordered N-terminus of the toxin graT [^910277d6]. Nature Communications (2019). High credibility.

Small-angle X-ray scattering

SAXS data were collected at SWING beamline (Soleil Synchrotron, Gif-Sur-Yvette, France). This beamline has a SEC system before the measuring capillary. SEC will remove possible aggregates rendering a very homogeneous sample that will then be directly exposed to X-rays for data collection. All the experiments were performed in 50 mM Tris, pH 8.0, 250 mM NaCl and 2 mM TCEP as running buffer.

Each protein was run through a Shodex KW402.5-4F at 0.2 ml/min. Scattering curves covering a concentration range around the peak were normalized and averaged to obtain the final scattering curve. Rg values were derived from the value of I 0 which were obtained by extrapolating to q = 0 using the Guinier approximation as implemented in ATSAS suite. The molecular weights of the different entities were estimated in a concentration independent way using the I 0, Porod volume and Fisher methods.

The use of the dimensionless Kratky plot ((qR g) 2 I(q)/I(0) vs qR g) is a relatively easy way to show that a protein is completely folded, partially folded or completely unstructured. If a protein is globular it follows Guinier's law I(q)/I(0) = exp (−(qR g) 2 / 3). The corresponding dimensionless Kratky plot is a function f(x) = x 2 exp (−x 2 / 3), with x = qR g > 0 with maximum of 1.104 at qR g = √3. On the other hand, an ideally disordered protein follows Debye's law I(q)/I(0) = 2 (x 2 −1−exp (−x 2))/x 4, with x = qR g > 0. In this case the Kratky plot is described by the function f(x) = 2 (x 2 − 1−exp (−x 2))/x 2 which increases monotonically with an asymptote at f(x) = 2. Experimentally, globular proteins show a very similar normalized Kratky plot with a maximum at (√3; 1.1), while partially unstructured proteins show a maximum shifted to higher values in both axes.

---

### A novel oppositional binary crow search algorithm with optimal machine learning based postpartum hemorrhage prediction model [^f6de23e1]. BMC Pregnancy and Childbirth (2022). Medium credibility.

Whereasrepresent the opposite number and x ∈ R denotes a real number determined on range of x ∈[a, b]. While a = 0 and b = 1 Eq. (3) becomes

While there is a point P (x 1, x 2,… x n) in n dimension coordinate and x 1, x 2,…, x n ∈ R later, the opposite pointis determined as its coordinates:

In such cases, have 2 values, x represent initial arbitrary value in [a, b] anddenotes the opposite values of x. They calculate f (x)&in all the iterations of OBCSA, later, employ on the evaluation function g ifselect x or else selectConsequently, the f l would be in range: f l ∈[f l min, f l max]. The opposite numbercan be determined by:

Later, evaluate the fitness for the first f l value and the fitness forin all the iterations. When, they select f l, or elsewould be selected. The stages of presented method can be given in the following.

Step1: The count of crows is n c = 25, f l min = 0.1, f l max = 1.8, A P = 0.3, and the maximal number of iterations is t max = 100.

Step2: The position that represent the features are made by U (0, 1).

Step3: The fitness function (FF) can be determined by

Whereas C represent the classification performance, W represent the weighted factors in the range of zero and one, F all represent the overall amount of features and F sub signifies the length of elected feature.

Step4: The position of the crows are upgraded as Eq. (2)

Step5: Steps 3 & 4 are repetitive till a t max is attained.

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^428bde48]. Nature Communications (2025). High credibility.

Derivation of the generalised δ formulation

Lastly, we introduced a generalised formulation of δ that is adaptive to the number of boundaries present for the variable being measured (Equation (1)). When the upper and lower bound are real numbers, δ (Equation (1)) becomes δ 2 (Equation (33)), and when the variable has only a real number lower bound, δ (Equation (1)) becomes δ L (Equation (37)), each with their associated proofs in previous sections. Here we provide proofs of δ for upper bounded variables and unbounded variables.

For upper bounded variables there exists no commonly used model distribution. The solution, however, was simply to use a conceptually flipped gamma distribution, where the lower bound of zero was instead used as an upper bound. This was done using same approach of rescaling the variable as was done for lower bounded variables. The target is, thus, to rescale a given observation (v) of a variable (V) with any real number upper bound (U) to retrieve x for indexing the gamma pdf and cdf (Equation (20); Equation (28)). This can be done withEssentially, v is rescaled such that its maximum is equal to zero. Similarly, the expected value or mean of the variable (E(V)) can be scaled such that its maximum is zero:where E(X) is equivalent to μ for the gamma distribution (defined in Equation (21)). Much like the use of the general gamma distribution for lower bounded variables, the variance of the upper bounded variable (Var(V)) remains unchanged when converted to the variance of a gamma distributed variable. As a result, the mean-independent measure of dispersion for upper bounded (U) variables, δ U, can be retrieved withThus, upper bounded variables can also be modelled using the generalised δ. In the main text Var(V) is denoted σ 2 and E(V) is denoted μ.

For unbounded variables the Gaussian/normal distribution is the most obvious model. The Gaussian distribution is defined by a parameter for central tenancy (μ) and dispersion/scale (σ 2). Where both parameters are estimated independently for an unbounded variable with the mean (E(V)) and variance Var(V). Thus, the distribution's dispersion is the variance itself (δ 0 = Var(V) = σ 2).

---

### The eighty five percent rule for optimal learning [^74499c58]. Nature Communications (2019). High credibility.

If the decision boundary is set to 0, such that the model chooses option A when h > 0, option B when h < 0 and randomly when h = 0, then the noise in the representation of the decision variable leads to errors with probabilitywhere F (x) is the cumulative density function of the standardized noise distribution, p (x) = p (x |0, 1), and β = 1/ σ quantifies the precision of the representation of Δ and the agent's skill at the task. As shown in Fig. 1b, this error rate decreases as the decision gets easier (Δ increases) and as the agent becomes more accomplished at the task (β increases).

The goal of learning is to tune the parameters ϕ such that the subjective decision variable, h, is a better reflection of the true decision variable, Δ. That is, the model should aim to adjust the parameters ϕ so as to decrease the magnitude of the noise σ or, equivalently, increase the precision β. One way to achieve this tuning is to adjust the parameters using gradient descent on the error rate, i.e. changing the parameters over time t according towhere η is the learning rate and ∇ ϕ ER is the derivative of the error rate with respect to parameters ϕ. This gradient can be written in terms of the precision, β, asNote here that only the first term on the right hand side of Eq. (5) depends on the difficulty Δ, while the second describes how the precision changes with ϕ. Note also that Δ itself, as the 'true' decision variable, is independent of ϕ. This means that the optimal difficulty for training, that maximizes the change in the parameters, ϕ, at this time point, is the value of the decision variable Δ * that maximizes ∂ ER/ ∂β. Of course, this analysis ignores the effect of changing ϕ on the form of the noise — instead assuming that it only changes the scale factor, β, an assumption that likely holds in the relatively simple cases we consider here, although whether it holds in more complex cases will be an important question for future work.

---

### Confidence reports in decision-making with multiple alternatives violate the Bayesian confidence hypothesis [^b245f51e]. Nature Communications (2020). High credibility.

Inference on a given trial. We assume that the observer knows the mean and standard deviation of each category based on the exemplar dots, and that the observer assumes that the three categories have equal probabilities. The posterior probability of category C given the measurement x is then p (C | x) ∝ p (x | C) = N (x; m C, (σ s 2 + σ 2) I). Instead of the true posterior p (C | x), the observer makes the decisions based on q (C | x), a noisy version of the posterior probability. We obtain a noisy posterior q (C | x) by drawing from a Dirichlet distribution. The Dirichlet distribution is a generalization of the beta distribution. Just like the beta distribution is a continuous distribution over the probability parameter of a Bernoulli random variable, the Dirichlet distribution is a distribution over a vector that represents the probabilities of any number of categories. The Dirichlet distribution is parameterized as

Γ represents the gamma function. p is a vector consisting of the three posterior probabilities, p = (p 1, p 2, p 3) = (p (C = 1 | x), p (C = 2 | x), p (C = 3 | x)). q is a vector consisting of the three posterior probabilities perturbed by decision noise, q = (q 1, q 2, q 3) = (q (C = 1 | x), q (C = 2 | x), q (C = 3 | x)). The expected value of q is p. The concentration parameter α is a scalar whose inverse determines the magnitude of the decision noise; as α increases, the variance of q decreases. To make a category decision, the observer chooses the category that maximizes the posterior probability:

---

### Scalable spatiotemporal prediction with Bayesian neural fields [^c7f97742]. Nature Communications (2024). High credibility.

Methods

Posterior inference

Let P (Θ f, Θ y, Y) denote the joint probability distribution over the parameters and observable field in Box 1. The posterior distribution is given by Eq. (13) in the main text. We describe two approximate posterior inference algorithms for BNF. In these sections, we define Θ = (Θ f, Θ y), θ = (θ f, θ y) and r = (s, t).

Stochastic MAP ensembles

A simple approach to uncertainty quantification is based on the "maximum a-posteriori" estimate:We find an approximate solution to the optimization problem (15) using stochastic gradient ascent on the joint log probability, according to the following procedure, where B ≤ N is a mini-batch size and (ϵ 1, ϵ 2,…) is a sequence of learning rates:Repeat until convergence

We construct an overall "deep ensemble"containing M ≥ 1 MAP estimates by repeating the above procedure M times, each with a different initialization of θ 0 and random seed.

Stochastic variational inference

A more uncertainty-aware alternative to MAP ensembles is mean-field variational inference, which uses a surrogate posteriorover Θ to approximate the true posterior(13) given the data. Optimal values for the variational parametersare obtained by maximizing the "evidence lower bound":where Eq. (22) follows from the independence of the priors. Finding the maximum of Eq. (22) is a challenging optimization problem. Our implementation leverages a Gaussian variational posterior q ϕ with KL reweighting, as described in Blundell et al. (Sections 3.2 and 3.4 of ref.).

Mean-field variational inference is known to underestimate posterior variance and can also get stuck in local optima of Eq. (21). To alleviate these problems, we use a variational ensemble that is analogous to the MAP ensemble described above. More specifically, we first perform M ≥ 1 runs of stochastic variational inference with different initializations and random seeds, which gives us an ensemble { ϕ i, i = 1,…, M } of variational parameters. We then approximate the posteriorwith an equal-weighted mixture of the resulting variational distributions.

---

### ECOLE: learning to call copy number variants on whole exome sequencing data [^9f24c69c]. Nature Communications (2024). High credibility.

Problem Formulation

Let X be the set of all exons with available read depth signal and X i indicate the i t h exon where i ∈{1,2. N } and N = ∣ X ∣. Every X i is associated with the following features:, and.is the chromosome of the exon where c h r ∈ {1, 2, 3,… ,24}. 23 and 24 represent chromosomes X and Y, respectively.are the start and end coordinates of the exonic region.is a standardized vector of read depth values at a base pair resolution. Standardization is performed for every read depth value using the global mean and standard deviation of read depth values in the training data. Everyis −1 padded from left to have the maximum length of 1000. We experiment with different maximum length values and proceed with 1000 as, overall, it performs best for ECOLE. Please see Section 3.5 of the Supplementary Text for these experiments. For exons longer than 1000 bps, they are considered if the non-zero read-depth values in that exon are of length < 1000. Y i represents the corresponding ground truth label for exon i, either obtained from CNVnator or from Chaisson et al. depending on the application. Letbe the CNV prediction (i.e. deletion, duplication, no-call) using the model f (a multi-class classifier) which is parameterized by θ. The goal is to find the model parameters θ that minimize the difference between predicted exon-level CNV labels and their ground truth labels.

---

### Learning protocols for the fast and efficient control of active matter [^ad8e1eb4]. Nature Communications (2024). High credibility.

For a given protocol α (t), the time evolution of x and y is given by the equationswhere γ (t) ≡ 1 + μ τ α (t). The system starts in the steady state associated with the trap stiffness α i, and so its initial coordinates are

Ref.sought protocols that carry out the change of trap stiffness α i = 1 → α f = 5 with minimum mean heat, Eq. (2). The theoretical framework used in that work assumes that protocols α (t) are smooth and are not rapidly varying (see Section S2 for a discussion of this point). Here we revisit this problem using neuroevolution. We find that heat-minimizing protocols are not in general slowly varying or smooth, but can vary rapidly and can display jump discontinuities. The protocols we identify produce considerably less heat than do the protocols identified in ref.(see Fig. 1 and Fig. S2).

To learn a protocol α (t) that minimizes heat, we encode a general time-dependent protocol using a deep neural network. We choose the parameterizationwhere g is the output of a neural network whose input is t / t f (restricting the scale of inputs to a range [0, 1] typically allows training to proceed faster than when inputs can be numerically large). We constrain the neural network so that α i ≤ α θ (t) ≤ α f, meaning that it cannot access values of α outside the range studied in ref. When we relax this constraint we find protocols that produce less heat, in general, than the protocols that observe the constraint. We impose the constraint to allow us to make contact with ref. and because experimental systems have constraints on the maximum values of their control parameters. Initially the weights and output of the neural network are zero, and so we start by assuming a protocol that interpolates linearly with time between the initial and final values of α. We train the neural network by genetic algorithm to minimize the order parameter, given by Eq. (2), which we calculate for a given protocol by propagating (3) for time t f, using a forward Euler discretization with step Δ t = 10 −3. An example of the learning process is shown in Fig. S2 a.

---

### Quantifying unobserved protein-coding variants in human populations provides a roadmap for large-scale sequencing projects [^3db7b451]. Nature Communications (2016). Medium credibility.

Methods

UnseenEst algorithm

UnseenEst uses the empirical SFS of a given class of variants in a cohort to estimate its frequency distribution in the population. The inputs into the algorithm are the number of alleles in the sample cohort, k, and the SFS, which is a set of counts, { F i }, where F i is the number of variants that are observed in exactly i out of the k alleles. A key challenge for the method is to accurately estimate the frequency distribution of variants that have empirical count of 0 (that is, they are not observed) in the cohort but are likely to have some small, non-zero frequency in the population.

More concretely, let X denote a discrete set of frequencies x in [0, 1] and let h (x) denote the fraction of all the variants with frequency x. UnseenEst estimates h (x) by finding the set of h (x) that jointly minimizes the value of the expression:

where bin(x, k, i) is the binomial probability of observing i heads in k independent flips of a coin with bias x. The intuition for minimizing this objective is as follows:is the number of variants that we expect to find in i out of k alleles in the cohort and F i is the empirical number of variants observed in i alleles. If h (x) is the true frequency distribution, then the expectation should be close to the empirical, which is why we want to find h (x) that minimizes the objective above. Given an estimate of the frequency distribution h (x), the expected number of unique variants in N alleles can be calculated by the formula.

The standard 3rd order jackknife estimator does not estimate the frequency distribution h (x). Instead, it directly estimates that the number of unique variants in N alleles is g 1 (N, k) F 1 +g 2 (N, k) F 2 +g 3 (N, k) F 3, where F 1, F 2, F 3 are the number of variants observed once, twice and three times in k alleles, and g 1, g 2, g 3 are specific functions of N and k derived from self-consistency requirements. Note that the confidence intervals of the jackknife in Supplementary Fig. 3 are very narrow because there is relatively little variation in the counts F 1, F 2, F 3, when the sample size is large.

---

### Epistatic net allows the sparse spectral regularization of deep neural networks for inferring fitness functions [^7a3a3502]. Nature Communications (2021). High credibility.

Methods

Notation and background

Suppose we are given n (experimental) samples, that is, (sequence, value) pairs from a biological landscape, where x i ∈ {−1, +1} d denotes the binary encoding of d mutational sites in a variant andis its associated fitness value. We are interested in learning a function f (x) that maps all subsets of mutations to fitness values. In other words, we seek to learn a set function, wheredenotes the space of all the binary vectors of length d. A key theoremin mathematics states that any set function (also known as pseudo-Boolean function) f (x) = f (x 1, x 2,…, x d) can be represented uniquely by a multi-linear polynomial over the hypercube (x 1, x 2,…, x d) ∈ {−1, +1} d :whereis a subset of {1, 2, 3,…, d } = [d] andis the WH transform coefficient (or equivalently the epistatic coefficient) associated with the monomial (interaction). For example, the pseudo-Boolean functiondefined over d = 5 mutational sites, has three monomials with orders 2, 1, and 3 and WH coefficients 12, − 3, and 6, respectively. The WH transform of this function is sparse with k = 3 non-zero coefficients out of a total of 2 5 = 32 coefficients. Each monomial can be easily explained, for example, the first monomial in the WH transform, that is 12 x 1 x 4, indicates that mutation sites 1 and 4 are interacting and the interaction enriches fitness because the sign of the coefficient is positive. On the hand, the second monomial − 3 x 3 shows that a mutation at site 3 depletes fitness. The last monomial 6 x 1 x 2 x 5 shows a third-order interaction between mutational sites 1, 2, and 5 which also enrich fitness.

---

### Genetic-optimised aperiodic code for distributed optical fibre sensors [^3064b17a]. Nature Communications (2020). High credibility.

Distributed genetic algorithm

Genetic algorithms (GAs)are efficient search and optimisation techniques based on the principle of natural genetics evolution, aiming at finding optimum or sub-optimum solutions to problems that are difficult to be algebraically solved. Standard GAs perform the search through a search space involving a population of individuals, each of which is randomly set following predefined initial conditions and represents a potential solution for the given problem. During the search process, the algorithm iteratively evaluates the fitness value of each individual and breeds new individuals by carrying out operations found in natural genetics. Once given termination conditions are satisfied, the searching process stops, and the individual exhibiting the best fitness value is output as the solution. The key idea of GAs is that the new generation of population bred after each iteration should involve individuals representing better solutions thanks to the use of classical operators inspired by the advancement in nature. These operators consist of: (1) selection, which selects the potential parents of offspring based on the evaluation of fitness value in such a way that better individuals are often selected for matching; (2) crossover, in which the genetic information from two parent individuals are exchanged to generate two offsprings; and (3) mutation, which creates new genetic information that does not exist in the parents. In order to expand the searching scope and to accelerate the search speed, DGAsthat process multiple populations in parallel are often used. Furthermore, by periodically exchanging individuals among subpopulations via a migration operator, the premature convergence problem existing in conventional GAs can be greatly alleviated.

In this work a dedicatedly designed DGA is adapted to search for the optimal/sub-optimal unipolar binary code sequence u (n) that can provide the smallest possible noise scaling factor Q (the largest possible coding gain G c) for these given input parameters:
the energy enhancement factor F E bound by the physical constraints, as elaborated in Supplementary Notes 3 and 4;
the value of m, which is a number between 3 and 4 according to the empirical results shown in Fig. 2b;
the estimated envelope function f ′(n) = u f (n)/ u (n) imposed by the EDFA saturation, which can be estimated from the selected m and EDFA specifications. Note that f ′(n) is close to the N x -point downsampling of f (n), the latter being rigorously retrieved from the measured c f (n).

---

### Reinforcement learning increasingly relates to memory specificity from childhood to adulthood [^beea5b85]. Nature Communications (2025). High credibility.

To ensure that models within each comparison set were distinguishable from one another, we conducted recoverability analyses in which we generated 100 simulated datasets of 151 simulated agents. Parameters for each simulated agent were drawn randomly from uniform distributions with minima and maxima determined by the minimum and maximum fitted values from the empirical data. In addition, for the model with two initial q values, we constrained their non-transformed values to be at least 1 apart. For the model with a separate counterfactual learning rate, we constrained the non-transformed values of the counterfactual learning rates to be greater than −2 (so that its transformed value would be distinguishable from 0) and to be at least 1 apart from the choice learning rate. We then fit each simulated dataset with all three models in each model comparison set, and fed these first-level fits through the same second-level fitting and model comparison algorithm that we used with our empirical data. We then examined the proportion of the 100 simulated experiments for which the model with the highest exceedance probability matched the true, generating model.

---

### Do CIs give you confidence? [^0c65ec35]. Chest (2012). Low credibility.

This article describes the conceptual basis for the P value and the CI. We show that both are derived from the same underlying concepts and provide useful, but similar information.

---

### Maximum entropy technique and regularization functional for determining the pharmacokinetic parameters in DCE-MRI [^4abea899]. Journal of Digital Imaging (2022). Medium credibility.

Maximum Entropy Technique — Entropy as a Regularization Functional

We propose to solve the ME problem using the regularization method. Generally, there is a unique optimizer to solve eitherorwhereandare two distance measures, is a regularization parameter and q is a priori solution. The important step is to chooseand, and determiningand q. The main part of MET is maximizing Shannon's entropy :subject to the following constraints, which are the expectations of known functions computed numerically based on the data via Taylor's theorem.in which, and, areknown functions. The general forms of these functions are, trigonometric or geometric functions. Entropy can also be used as a regularization functional in Eq. (11). An essential challenge in this method is to specify the regularization parameter. Here, whereis Kullback–Leibler divergenceand g is an initial solution of p. J (p) is convex onand if the solution exists, it will be unique. Using the Lagrangian technique gives the following:withWe mentioned that p may be in nonlinear form. In the following, we briefly describe the MET regularization algorithm (MET/REG)
(1) Assumings as constraints which are the expected value of the known functions, computed numerically from data based on the Taylor's theorem.
(2) Estimating p (x) by minimizingsubject to the known constraints in Eq. (13). Then, g (x) is an initial (empirical) solution for p. Using the Lagrangian, the following equation is solvedand its parameters will be determined by.
(3) Determining the expected value of p, as the solution of the inverse problem. The solutionis a function of dual variablebyin which
(4) If the function g is a separable measure:then p is a separable measure:and then, functionwill be the logarithmic Laplace transform of.

---

### A machine learning approach for online automated optimization of super-resolution optical microscopy [^3757bacc]. Nature Communications (2018). Medium credibility.

Kernel TS for online optimization

Classical formulation of mathematical optimization is to seek for the value x in a domainto maximize (or minimize) a real-valued target function. Here, f (x) corresponds to the expected objective value associated with the parametrization x in the space of optimized parameters. The online optimization problem can be formulated as an episodic game, each episode corresponding to an algorithm selecting a parameter configuration fromand acquiring an image. This image leads to a noisy observation of the target function at the chosen parameterization. An algorithm evolving in this setting faces two challenges: it must estimate the target function from noisy observations acquired at different locations while selecting the right parameters to optimize the target function.

When the target function is known to be smooth but rigid parametric assumptions over this function are not realistic, kernel regression is able to estimate the target function from prior, noisy and not necessarily independent and identically distributed samples. More specifically, under multivariate Gaussian priors, kernel regression can provide the posterior distribution over a function given prior observations. Whereas linear regression assumes that the target function is linear on its input space, kernel regression rather assumes that there exists a function ϕ mappingto another space, such that the target function is described by a linear relation on. Formally, there exists a vector θ such that f (x) corresponds to a inner product between ϕ (x) and θ. (The linear regression setting corresponds to the specific case where ϕ (x) = x.) The inner product between two mapped points ϕ (x) and ϕ (x ′) is given by the kernel k (x, x ′). The choice of kernel controls the smoothness of the resulting regression model, thus encoding information about smoothness assumptions on f. In the experiments, we used an anisotropic Gaussian kernel (here defined for 1D):with bandwidthfor parameter 1 ≤ i ≤ D, withbeing the range of values for parameter i. We assume that the noise on observations of the target function is a zero-mean Gaussian with variance σ 2 and that θ follows a zero-mean multivariate Gaussian distribution with covariance matrix, where I is the identity matrix, and λ > 0. The posterior distribution on f after selecting N parameters x 1,…, x N and observing y N = (y 1,…, y N) is then given bywhereandrespectively denote the predictive mean and (co-)variance (Supplementary Figure 1a), k N (x) Τ = (k (x i, x)) 1 ≤ i ≤ N and K N = [k (x i, x j)] 1 ≤ i, j ≤ N. Computing that posterior technically requires the knowledge of the noise variance σ 2, which is not realistic in practice. Fortunately, this can be circumvented by relying on empirical noise estimates based on lower and upper bounds on the noise variance, along with an upper bound on the norm of θ. All experiments used an upper bound of 5 on the norm of θ and a lower bound of 0.001 on the noise variance. An upper bound of 3 on the noise variance was considered for the maximal intensity objective (in glutamate uncaging), otherwise 0.3. The posterior distribution can be computed from the resulting noise estimate while preserving theoretical guarantees.

---

### Revealing the range of equally likely estimates in the admixture model [^4446a58e]. G3 (2025). Medium credibility.

Many ancestry inference tools, including structure and admixture, rely on the admixture model to infer both, allele frequencies p and individual admixture proportions q for a collection of individuals relative to a set of hypothetical ancestral populations. We show that under realistic conditions the likelihood in the admixture model is typically flat in some direction around a maximum likelihood estimate (q^, p^). In particular, the maximum likelihood estimator is non-unique and there is a complete spectrum of possible estimates. Common inference tools typically identify only a few points within this spectrum. We provide an algorithm which computes the set of equally likely (q~, p~), when starting from (q^, p^). It is analytic for K = 2 ancestral populations and numeric for K > 2. We apply our algorithm to data from the 1000 genomes project, and show that inter-European estimators of q can come with a large set of equally likely possibilities. In general, markers with large allele frequency differences between populations in combination with individuals with concentrated admixture proportions lead to small areas with a flat likelihood. Our findings imply that care must be taken when interpreting results from STRUCTURE and ADMIXTURE if populations are not separated well enough.

---

### A machine learning automated recommendation tool for synthetic biology [^5caa5c39]. Nature Communications (2020). High credibility.

Optimization-suggesting next steps

The optimization phase leverages the predictive model described in the previous section to find inputs that are predicted to bring us closer to our objective (i.e. maximize or minimize response, or achieve a desired response level). In mathematical terms, we are looking for a set of N r suggested inputs, that optimize the response with respect to the desired objective. Specifically, we want a process that:
i. optimizes the predicted levels of the response variable;
ii. can explore the regions of input phase space (in Eq. (1)) associated with high uncertainty in predicting response, if desired;
iii. provides a set of different recommendations, rather than only one.

We are interested in exploring regions of input phase space associated with high uncertainty, so as to obtain more data from that region and improve the model's predictive accuracy. Several recommendations are desirable because several attempts increase the chances of success, and most experiments are done in parallel for several conditions/strains.

In order to meet these three requirements, we define the optimization problem formally aswhere the surrogate function G (x) is defined as:depending on which mode ART is operating in (see the "Key capabilities" section). Here, y * is the target value for the response variable, y = y (x), and Var(y) denote the expected value and variance, respectively (see "Expected value and variance for ensemble model" in Supplementary Information), denotes Euclidean distance, and the parameter α ∈ [0, 1] represents the exploitation-exploration trade-off (see below). The constraintcharacterizes the lower and upper bounds for each input feature (e.g. protein levels cannot increase beyond a given, physical, limit). These bounds can be provided by the user (see details in the "Implementation" section in Supplementary Information); otherwise, default values are computed from the input data as described in the "Input space set" section in Supplementary Information.

---

### Maximizing NMR sensitivity: a guide to receiver gain adjustment [^19dc56cc]. NMR in Biomedicine (2025). Medium credibility.

Unexpectedly, we found a strong dip in SNR at 9.4 T of about 40% for all measured nuclei except 1 H at = 18. As the signal increases linearly, this effect is caused by changes in noise intensity. The plateau was reached at > 30 for 1 H andfor the other nuclei.

These findings showed that the most sensitive RG for X‐nuclei is between 10 and 18 and should be used in most cases without any automatic RG adjustment. Lower RG values are needed when the signal exceeds the threshold, while higher RG values do not provide any benefits. The 1 H channel showed the most gain in SNR up to≈ 30, with minor improvements thereafter but with a potential threat of signal compression. This finding revealed that choosing a bigger RG is not necessarily the right strategy to improve SNR.

3.2 Optimal Settings for a Hyperpolarization Observation

Having measured(Figure 2) and the RRT (for 1 H andfor other nuclei at the 9.4 T system, Figure 1), one can estimate the signal intensity and SNR for a known sample for any parameters:, and.

Using Equations (1) and (5), we calculated the signal and SNR for pure water (55.51 M) and D 2 O (55.21 M) for = 0–90° and RG = 0–101 (Figure 3). Note that the concentration of the 1 H and 2 H spins is twice as large as the concentration of the molecules. For more than half of the combinations ofand, the signal exceeded(white areas in Figure 3, right). These results indicate thatandshould be chosen with care.

Note that if there are no constraints on the flip angle, maximizingis more beneficial for SNR than maximizing RG (Figure S2, SI). For water, the maximum SNR was obtained with = 90° andof 4.9 (1 H) and 12.8 (2 H) — values far below the maximum RG of 101.

In hyperpolarization experiments, andare usually given, whileandcan be adjusted as needed. We calculated the signal and SNR as a function ofandfor a 90 mM 13 C sample polarized to 35% and a 40 mM 15 N sample polarized to 15% — similar values were achieved recently with [1‐ 15 N]nicotinamide (Figure 3). To preserve polarization, the flip anglein hyperpolarized metabolic experiments is typically low. Here, we set the limits to 5° (13 C) and 10° (15 N).

---

### Gene regulatory network inference from sparsely sampled noisy data [^c4f997e1]. Nature Communications (2020). High credibility.

The GP framework can handle combinatorial effects, meaning nonlinearities that cannot be decomposed into f (x 1, x 2) = f 1 (x 1) + f 2 (x 2). This is an important property for modelling a chemical system — such as gene expression — where reactions can happen due to combined effects of reactant species. For example, the dynamics corresponding to a chemical reaction x 1 + x 2 → x 3 cannot be modelled by.

In a classical variable selection problem from input–output data η j = f (ξ j) + v j, for j = 1,…, N, where, the task is to find out which components of ξ does the function f depend on. In the GP framework, variable selection can be done by a technique known as "automatic relevance determination", based on estimating the hyperparameters of the covariance function of f. For BINGO, we have chosen the squared exponential covariance function for each component f i :The hyperparameters β i, j ≥ 0 are known as inverse length scales, sincecorresponds to a distance that has to be moved in the direction of the j th coordinate for the value of function f i to change considerably. If β i, j = 0, then f i is constant in the corresponding direction. The mean function for f i is m i (x) = b i − a i x i where a i and b i are regarded as nonnegative hyperparameters corresponding to mRNA degradation and basal transcription, respectively.

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^327fd22c]. Nature Communications (2023). High credibility.

Thresholding

The curve fold change for the i th curve (cfc i) is defined as the log 2 -ratio between the lowest and highest concentration using the regressed model and it quantifies the drug's effect size or efficacy (Eq. 11).

We transferred the SAM principle of differential T-statistics to the recalibrated F-statistic to obtain equivalent decision boundaries for the dose–response curve analyses. This is possible by recognizing that a dose–response curve converges in the limit to two groups (front plateau group = not affected data points, and back plateau group = affected data points), where the curve fold change is equivalent to a conventional SAM fold change between the two plateau groups. In this case, allowing for the conversion and application of the s 0 SAM principle. CurveCurator simplified this process by calculating the tuning parameter s 0 directly from the user-specified significance and fold change asymptotes (Eq. 12). Whereis the inverse cumulative density function of an F-distribution with degrees of freedom dfn and dfd as determined in the section above. This makes s 0 also a function of the number of data points, which is relevant when a curve has missing values.

The tuning parameter s 0, which defines the hyperbolic decision boundaries, can also be used to transform the curve's recalibrated F-value into the s 0 -adjusted F-value (F adj, i) based on the global s 0 value and the curve's measured fold change (cfc i) (Eq. 13).

We then transform this s 0 -adjusted F-value into a "relevance score" using the cumulative density function F X (x | dfn, dfd). For s 0 = 0.0, this simply corresponds to the p -value of the curve. For s 0 > 0.0, this can no longer be interpreted as a p -value, but it still provides an appropriate ordering of curves by both statistical and biological relevance. Additionally, a −log 10 transformation is applied for visualization purposes and to obtain an interpretable score that ranges from 0 to infinity, where 0 has no relevance (Eq. 13).

---

### A continuous-time maxSAT solver with high analog performance [^87d3d16d]. Nature Communications (2018). Medium credibility.

Algorithm description

Here, we give a simple, nonoptimized variant of the algorithm (see flowchart in Supplementary Fig. 2). Better implementations can be devised, for example with better fitting routines, however the description below is easier to follow and works well. Given a SAT problem, we first determine the b parameter as described previously. Step 1: initially we set, Γ min, = and t max. Unless specified otherwise, in our simulations we used Γ min = 100, Γ max = 2 × 10 6, t max = 50. Step 2: to initialize our statistics, we run Γ min trajectories up to t max, each from a random initial condition. For every such trajectory ω we update the p (E, t) distributions as function of the energies of the orthants visited by ω. We record the lowest energy value found. Step 3: starting from Γ = Γ min + 1 and up to Γ max, we continue running trajectories in the same way and for each one of them check: (a) If, set, update p (E, t) and go to Step 4. (b) If Γ just reached, go to Step 4. (c) If Γ = Γ max, output "Maximum number of steps reached, increase Γ max ", output the lowest energy value found, the predictedand the quality of fit for, then halt. Step 4: using the p (E, t) distributions, estimate the escape rates κ (E) as described in the corresponding Methods section. Step 5: the κ (E) curve is extrapolated to the E − 1 value obtaining κ (E − 1) and then using this we predict(as described in another Methods section). Further extrapolating the κ (E) curve to κ = 0 we obtain(see the corresponding Methods section). Step 6: we check the consistency of the prediction defined here as saturation of the predicted values. We call it consistent, ifhas not changed during the last 5 predictions. If it is not consistent yet, we continue running new trajectories (Step 4). If the prediction is consistent, we check for the following halting conditions: (i) Ifthen we decide the global optimum has been found:and skip to Step 7. (ii) If the fitting is consistently predicting(usually it is very close,) we check the number of trajectories that has attained states with, i.e. = . If it is large enough (e.g. > 100), we decide to stop running new trajectories and setand go to Step 7. (iii) Ifthen we most probably have not found the global optimum yet and we go to Step 4. We added additional stopping conditions that can shorten the algorithm in case of easy problems, see Methods corresponding section, but these are not so relevant. Step 7: the algorithm ends and outputs, values, the Boolean variables corresponding to the optimal state found, along with the quality of fit.

---

### A daptomycin-xylitol-loaded polymethylmethacrylate bone cement: how much xylitol should be used? [^2e0d4269]. Clinical Orthopaedics and Related Research (2013). Low credibility.

Background

The rate of release of an antibiotic from an antibiotic-loaded polymethylmethacrylate (PMMA) bone cement is low. This may be increased by adding a particulate poragen (eg, xylitol) to the cement powder. However, the appropriate poragen amount is unclear.

Questions/Purposes

We explored the appropriate amount of xylitol to use in a PMMA bone cement loaded with daptomycin and xylitol.

Methods

We prepared four groups of cement, each comprising the same amount of daptomycin in the powder (1.36 g/40 g dry powder) but different amounts of xylitol (0, 0.7, 1.4, and 2.7 g); the xylitol mass ratio (X) (mass divided by mass of the final dry cement-daptomycin-xylitol mixture) ranged from 0 to 6.13 wt/wt%. Eight mechanical, antibiotic release, and bacterial inhibitory properties were determined using three to 22 specimens or replicates per test. We then used an optimization method to determine an appropriate value of X by (1) identifying the best-fit relationship between the value of each property and X, (2) defining a master objective function incorporating all of the best fits; and (3) determining the value of X at the maximum master objective function.

Results

We found an appropriate xylitol amount to be 4.46 wt/wt% (equivalent to 1.93 g xylitol mixed with 1.36 g daptomycin and 40 g dry cement powder).

Conclusions

We demonstrated a method that may be used to determine an appropriate xylitol amount for a daptomycin-xylitol-loaded PMMA bone cement. These findings will require in vivo confirmation.

Clinical Relevance

While we identified an appropriate amount of xylitol in a daptomycin-xylitol-loaded PMMA bone cement as a prophylactic agent in total joint arthroplasties, clinical evaluations are needed to confirm the effectiveness of this cement.

---

### Statistical analysis and optimality of neural systems [^bf829ef6]. Neuron (2021). Medium credibility.

Normative theories and statistical inference provide complementary approaches for the study of biological systems. A normative theory postulates that organisms have adapted to efficiently solve essential tasks and proceeds to mathematically work out testable consequences of such optimality; parameters that maximize the hypothesized organismal function can be derived ab initio, without reference to experimental data. In contrast, statistical inference focuses on the efficient utilization of data to learn model parameters, without reference to any a priori notion of biological function. Traditionally, these two approaches were developed independently and applied separately. Here, we unify them in a coherent Bayesian framework that embeds a normative theory into a family of maximum-entropy "optimization priors". This family defines a smooth interpolation between a data-rich inference regime and a data-limited prediction regime. Using three neuroscience datasets, we demonstrate that our framework allows one to address fundamental challenges relating to inference in high-dimensional, biological problems.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^aeeaae45]. Nature Communications (2025). High credibility.

Fitting models to data is an important part of the practice of science. Advances in machine learning have made it possible to fit more-and more complex-models, but have also exacerbated a problem: when multiple models fit the data equally well, which one(s) should we pick? The answer depends entirely on the modelling goal. In the scientific context, the essential goal is replicability: if a model works well to describe one experiment, it should continue to do so when that experiment is replicated tomorrow, or in another laboratory. The selection criterion must therefore be robust to the variations inherent to the replication process. In this work we develop a nonparametric method for estimating uncertainty on a model's empirical risk when replications are non-stationary, thus ensuring that a model is only rejected when another is reproducibly better. We illustrate the method with two examples: one a more classical setting, where the models are structurally distinct, and a machine learning-inspired setting, where they differ only in the value of their parameters. We show how, in this context of replicability or "epistemic uncertainty", it compares favourably to existing model selection criteria, and has more satisfactory behaviour with large experimental datasets.

---

### Power of data in quantum machine learning [^b2a9dd1b]. Nature Communications (2021). High credibility.

Ultimately, to see a prediction advantage in a particular dataset with specific function values/labels, we need a large separation between s C and s Q. This happens when the inputs x i, x j considered close in a quantum ML model are actually close in the target function f (x), but are far in classical ML. This is represented as the final test in Fig. 1 and the methodology here outlines how this result can be achieved in terms of its more essential components.

Projected quantum kernels

In addition to analyzing existing quantum models, the analysis approach introduced also provides suggestions for new quantum models with improved properties, which we now address here. For example, if we start with the original quantum kernel, when the effective dimension d is large, kernel Tr(ρ (x i) ρ (x j)), which is based on a fidelity-type metric, will regard all data to be far from each other and the kernel matrix K Q will be close to identity. This results in a small geometric difference g CQ leading to classical ML models being competitive or outperforming the quantum kernel method. In Supplementary Section 9, we present a simple quantum model that requires an exponential amount of samples to learn using the quantum kernel Tr(ρ (x i) ρ (x j)), but only needs a linear number of samples to learn using a classical ML model.

---

### Photo-activated raster scanning thermal imaging at sub-diffraction resolution [^5194a53a]. Nature Communications (2019). High credibility.

Fig. 2
Proof-of-principle experiments. a, f Transmitted-light images of microfiche samples. b, e Temporal maximum projection of the raw thermo-camera image stacks acquired with modulated illumination on the samples in a and f, respectively; acquisition parameters: N x x N y = 120 × 36, δx = 34.4 μm, Δ x = 60, Δ y = 6, τ on = 300 ms, P = 4.6 mW, beam 1/e 2 diameter 56 ± 2 μm in b, N x x N y = 160 × 42, δx = 15.3 μm, Δ x = 160, Δ y = 6, τ on = 300 ms, P = 15 mW, beam diameter 22 ± 1 μm in e. c, g Super-resolution images of the samples in a and f, obtained from the image stacks employed for b and e, respectively; Δ T min = 0.3 °C in c, 0.7 °C in g. d Black: intensity profile along the dashed line in a upon a lookup-table inversion on the image; red: Δ T max profile along the dashed line in c. h Black: average intensity profile along the vertical direction in f upon lookup-table inversion; red: average Δ T max profile along the vertical direction in g with contrast percentages between adjacent peaks. In d, h, uncertainties on Δ T max values equal the thermo-camera sensitivity 0.1 °C and x -axis error bars for non-zero Δ T max values are extrapolated from the σ x, y -versus-Δ T max plot. Scale bars = 500 μm.

Quantitatively, the accuracy of the image reconstruction algorithm is demonstrated in Fig. 2d by the comparison with the transmitted-light image of the same sample. The spatial profile of Δ T max values drawn along the spacing between the letters B and S of the ink pattern correctly superimposes to the corresponding intensity profile in the transmitted-light image, within the 30–50 μm localization error σ x, y expected from the values of Δ T max = 0.6–0.9 °C (Supplementary Fig. 5e). The agreement between temperature variations and transmitted-light intensity profiles should further increase at increasing observed Δ T max (increasing laser intensity or activation time and decreasing σ x, y). Still, the localization error σ x, y of the reported data-set allows discriminating absorbing structures (in Fig. 2d, the letters B and S) 100 μm apart.

---

### Experimental evolution of yeast shows that public-goods upregulation can evolve despite challenges from exploitative non-producers [^30dd5086]. Nature Communications (2024). High credibility.

Fig. 6
Overproducers suppress competitors by reducing their metabolic efficiency.

a Environmental hexose concentration during batch culture season in 1% sucrose. Hexose concentration in supernatant: + SUC2 > wt (Area Under Curve (inset): Two-sample, two-sided t test: t = 9.00, p = 8.45 x 10–4), n = 3. b Growth yield of wt decreases with increasing resource concentration in a non-linear manner, which also applies to + SUC2 and non-producers because they consume glucose equivalently. The purple line is fitted from Eq. (1) of ref. Arrows indicate yield of mean maximum [hexose] from supernatant of wt (red, 16 h: 0.136 (± 0.005) %) or + SUC2 (blue, 10 h: 0.295 (± 0.068) %) from (a) to demonstrate the estimated growth efficiencies of populations with either wt or + SUC2 producers. c Population carrying capacity (K - density after 36 h) of mixed populations were suppressed to a greater extent by + SUC2 than wt, (GLM (two-sided): F (13,28) = 66.19, p < 2.2 x 10 −16, Adj. R 2 = 0.95; post-hoc Tukey's HSD: Exact p -values for significant between-genotype differences in shaded region: 0.25 = 4.43 x 10 − 9, 0.5 = 1.15 x 10 − 8, 0.75 = 4.00 x 10 − 2). Points show all replicates (n = 3), lines follow mean ± 95% C. I. The dashed horizontal line shows the mean yield of pure non-producer populations. All data are provided in the Source Data File.

---

### Sodium fluoride f 18 [^ca730735]. FDA (2024). Medium credibility.

11.2 Physical Characteristics

Fluoride F 18 decays by positron (β+) emission and has a half-life of 109.7 minutes. Ninety-seven percent of the decay results in emission of the positron with a maximum energy of 633 keV and 3% of the decay results in electron capture with subsequent emission of characteristic X-rays of oxygen. The principal photons useful for diagnostic imaging are the 511 keV gamma photons, resulting from the interaction of the emitted positron with an electron (Table 2). Fluorine F 18 atom decays to stable18O-oxygen.

* Produced by positron annihilation

[3] Kocher, D.C. Radioactive Decay Data Tables DOE/TIC-11026, 69, 1981.

The specific gamma ray constant for fluoride F 18 is 5.7 R/hr/mCi (1.35 x 10-6Gy/hr/kBq) at 1 cm. The half-value layer (HVL) for the 511 keV photons is 4.1 mm lead (Pb). A range of values for the attenuation of radiation results from the interposition of various thickness of Pb. The range of attenuation coefficients for this radionuclide is shown in Table 3. For example, the interposition of an 8.3 mm thickness of Pb with a coefficient of attenuation of 0.25 will decrease the external radiation by 75%.

Table 4 lists the fraction of radioactivity remaining at selected time intervals from the calibration time. This information may be used to correct for physical decay of the radionuclide.

* Calibration time

---

### Inferring time derivatives including cell growth rates using gaussian processes [^d99eaf3f]. Nature Communications (2016). Medium credibility.

Inferring the first and second time derivatives

To determine the time derivative of the data, we use that the derivative of a Gaussian process is another Gaussian process. We can therefore adapt standard techniques for Gaussian process to allow time derivatives to be sampled too.

Building on the work of Boyle, we let g (x) and h (x) be the first and second derivatives with respect to x of the latent function f (x). If f (x) is a Gaussian process then so are both g (x) and h (x). Writing ∂ 1 and ∂ 2 for the partial derivatives with respect to the first and second arguments of a bivariate function, we have

and that

as well as

following ref.

Consequently, the joint probability distribution for y and f *, g * and h * evaluated at points X * is again Gaussian (cf. equation (7)):

where we write K = K (X, X) and K ✱ = K (X *, X *) for clarity.

The covariance function is by definition symmetric: k (x i, x j) = k (x j, x i) from equation (1). Therefore, and so

for all positive integers k and. Consequently, the covariance matrix in equation (13) is also symmetric.

Conditioning on y now gives that the distribution P (f *, g *, h *| X, y, θ, X *) is Gaussian with mean

and covariance matrix

Equation (16) includes equation (9) and shows that

which gives the error in the estimate of the first derivative. Similarly,

is the error in estimating the second derivative.

Using an empirically estimated measurement noise

Although our derivation is given for a Gaussian process where the measurement errors in the data are independent and identically distributed with a Gaussian distribution of mean zero, the derivations are unchanged if the measurement noise has a different s.d. for each time point.

When the magnitude of the measurement noise appears to change with time, we first empirically estimate the relative magnitude of the measurement noise by the variance across all replicates at each time point. We then smooth this estimate over time (with a Gaussian filter with a width of 10% of the total time of the experiment, but the exact choice is not important) and replace the identity matrix, I, in equations (6), (15) and (16) by a diagonal matrix with the relative measurement noise on the diagonal in order to make predictions.

---

### A robust deconvolution method to disentangle multiple water pools in diffusion MRI [^55e260dc]. NMR in Biomedicine (2018). Low credibility.

2.3 Diffusion spectrum subdivision (GMM)

For each component i of the diffusion spectrum, p i (Figure 1 e) is the contribution of a spectral component with diffusion coefficient D i to the measured signal. The direct interpretation of p i can therefore be difficult when the spectrum is non‐sparse, as a result of noise, regularization or physiological complexity.

For this reason, Keil et al. 40 proposed to divide the spectrum into two macro‐compartments (C x), one describing hindered diffusion for diffusion coefficients between 0.5 × 10 −3 and 3 × 10 −3 mm 2 /s, and a second describing blood pseudo‐diffusion for diffusion coefficients greater than 3 × 10 −3 mm 2 /s. To avoid prior assumptions, we propose an automated method to define the number of macro‐compartments and their thresholds. Assuming that, in a noisy experiment (both acquisition noise and biological noise), the diffusivities of each compartment are distributed around a common value, we can use a GMM to identify the number of biological "macro"‐compartments from the voxel‐wise spectral components.

However, in noisy experiments, the variance of each spectrum may be too large to sufficiently separate the macro‐compartments. For this reason, a fast fit of the data was performed with Equation (4), limiting the tolerance of the NNLS solver (lsqnonneg, MATLAB, The Mathworks, Natick, Massachussets, USA) to 10 −2 (Figure 1 d), which in practice resulted in a less accurate and noise‐sensitive solution. This fit was then voxel‐wise averaged over the entire brain and fitted with a GMM (Figure 1 f) to determine the number and location of compartments, where the maximum number was limited to 300. Individual Gaussian components with area overlap above 15% were merged, after which the voxel‐wise diffusion spectrum was subdivided into the resulting macro‐components (Figure 1 h). The number of resulting macro‐compartments is dependent on the overlap setting. An additional explanation on the area overlap setting and how it can affect the results can be found in Supporting Information.

---

### Solving perfect matchings by frequency-grouped multi-photon events using a silicon chip [^559ac16a]. Nature Communications (2025). High credibility.

In addition, multi-photon processors can enhance classical stochastic algorithms for solving NP problems, such as SAT, max-clique –, and dense subgraph –. The enhancement comes from the positive correlation between perfect matchings counts and the density of the graph. Thus, we can use biased randomness from perfect matchings sampler to enhance stochastic algorithms for these problems. The SAT is the problem of finding an assignment that satisfy the given Boolean formula. A clausal formula can be converted into a graph, and one SAT solution will correspond to a clique of the graph. We set a clausal formula with four clauses. It can be reduced to a 4-Clique problem of an eight-vertex graph shown in Fig. 4 a. The four-photon distribution in 2 h is shown in Fig. 4 b, with a fidelity of 98.25%. The four-vertex subgraph with the most samples is a clique, plotted with the red line in Fig. 4 a. It means that the formula F is satisfiable, and the corresponding assignment is x 1 = True, x 2 = False, and x 3 = True. On the other hand, the densest subgraph problem is defined to find the densest k -vertex subgraph with the largest density from a given n -vertex graph(k < n). No polynomial-time approximation scheme exists for the problem. To avoid being fooled by special graph structures, stochastic algorithms that employ uniform randomness for exploration are always preferable in solving the densest subgraph of a general graph. We verify the enhancement of random search by perfect matchings samples. For example, to find the densest subgraph of a 16-vertex graph with weighted edges as shown in Fig. 4 c, we experimentally obtained the four-photon distribution in 2 h shown in Fig. 4 d, with a fidelity of 92.49%. Note that the perfect matching number of an edge-weighted graph is a more general definition, that the weight of every perfect matching contains a coefficient of the multiplication of the weights of related edges. Then, we use the samples to identify the densest four-vertex subgraph via random search. In the random search, we draw several samples, each containing four vertices, compare the density of the samples, and select the one with the maximum density. We vary the number of samples drawn from 1 to 300 and repeat 400 times for each value. Figure 4 e shows the mean density obtained with a different number of samples. The ideal curve corresponds to the loss-free case. We can see that the protocol whose samples are drawn from the perfect matching solver performs markedly better than the uniform sampling.

---

### Data-driven control of complex networks [^85fc9bea]. Nature Communications (2021). High credibility.

Results

Network dynamics and optimal point-to-point control

We consider networks governed by linear time-invariant dynamicswhere, anddenote, respectively, the state, input, and output of the network at time t. The matrixdescribes the (directed and weighted) adjacency matrix of the network, and the matricesand, respectively, are typically chosen to single out prescribed sets of input and output nodes of the network.

In this work, we are interested in solving point-to-point control problems; that is, designing open-loop control policies that steer the network output y (t) from an initial value y (0) = y 0 to a desired one y (T) = y f in a finite number of steps T. If y f is output controllable in T steps (a standing assumption in this paper; we refer to Supplementary Note 1 for more details), then the latter problem admits a solution and, in fact, there are many ways to accomplish such a control task. Here, we assume that the network is initially relaxed (x (0) = 0), and we seek the control inputthat drives the output of the network to y f in T steps and, at the same time, minimizes a prescribed quadratic combination of the control effort and locality of the controlled trajectories.

Mathematically, we study and solve the following constrained minimization problem:where Q ≽ 0 and R ≻ 0 are tunable (positive semidefinite and positive definite, respectively) matrices that penalize output deviation and input usage, respectively, and y T = y (T). Problem (2) generalizes the classic (open-loop) linear–quadratic control framework by including the possibility of minimizing a linear function of the state (as opposed to the whole state) in addition to the control input. Further, we remark that increasing R in Eq. (2) leads to optimal control inputs that achieve the desired final state with increasingly smaller magnitudes. Similarly, the matrix Q in Eq. (2) weighs the norm of the output (state), so that increasing Q forces the optimization problem to generate inputs that limit the norm of the output (state), at the expenses of using a larger control input. In particular, if Q = 0 and R = I, thencoincides with the minimum-energy control to reach y f in T steps.

---

### Minimum electric-field gradient coil design: theoretical limits and practical guidelines [^13a4a404]. Magnetic Resonance in Medicine (2021). Medium credibility.

FIGURE 4
A, A region contours of constant Bz for Y gradient coil, with (red) and without (black) E max optimization, showing that gradient distortion is virtually identical despite the peak E‐field being reduced by a factor of 2.8 for the Y coil. B, Difference in the B y components of the gradient field with versus without E‐field optimization. Values are normalized to the gradient strength yielding units of length (mm) and show a highly uniform concomitant y‐directed field added to the optimized (asymmetric) solution. C, Wire pattern representing the difference between optimized (asymmetric) and nonoptimized (symmetric) solutions with same current scaling as Figure 3. Similar results are obtained for the X gradient coil

The key difference between the E‐field minimized and nonminimized solutions is the addition of a uniform concomitant B 0 field component in the x or y direction. Figure 4B shows a difference plot of the concomitant B y field, normalized to gradient strength (units of millimeters), which is responsible for the reduction in E‐field. Figure 4C shows the winding that would need to be added to the Y symmetric coil (Figure 3B) to obtain the winding pattern of the Y asymmetric coil (Figure 3D), and shows the fundamental difference between the two solutions. The winding in Figure 4C produces a uniform field transverse to the main B 0 field, resulting in a concomitant component with additional magnetic stored energy. Conceptually, this can be thought of as a second coil, which if activated, converts the symmetric coil into an asymmetric coil with reduced E‐field but otherwise identical distortion. The added concomitant field in Figure 4B is highly uniform with a 0.85% variation over the 26‐cm imaging region, resulting in minimal distortion of the desired gradient field; this concomitant field can be compensated with a phase/frequency offset correction by the scanner. Figure 4B shows that the average concomitant field of 88.5 mm within the imaging region can be directly compared with the z 0x defined by Meier et al, 29 in which a value of 127 mm was reported for an asymmetric head gradient. The z 0x for both the ESP and HG2 gradients are approximately 120 mm 30; these are both fully asymmetric designs (single eye per half coil with all return currents flowing above the head), similar to the original concept defined by Roemer. 12 The X coil of Figure 3 requires a smaller concomitant field (z 0y = 57.8 mm) than the Y coil, reflecting a decreased need to pull E‐field off the torso region due to the smaller extent of the body in the anterior–posterior direction.

---

### Modeling direct effects of neural current on MRI [^9f49d09d]. Human Brain Mapping (2009). Low credibility.

We investigate the effect of the magnetic field generated by neural activity on the magnitude and phase of the MRI signal in terms of a phenomenological parameter with the dimensions of length; it involves the product of the strength and duration of these currents. We obtain an analytic approximation to the MRI signal when the neuromagnetically induced phase is small inside the MRI voxel. The phase shift is the average of the MRI phase over the voxel, and therefore first order in that phase; and the reduction in the signal magnitude is one half the square of the standard deviation of the MRI phase, which is second order. The analytic approximation is compared with numerical simulations. For weak currents the agreement is excellent, and the magnitude change is generally much smaller than the phase shift. Using MEG data as a weak constraint on the current strength we find that for a net dipole moment of 10 nAm, a typical value for an evoked response, the reduction in the magnitude of the MRI signal is two parts in 10(5), and the maximum value of the overall phase shift is approximately 4 x 10(-3), obtained when the MRI voxel is displaced 2/3 the size of the neuronal activity. We also show signal changes over a large range of values of the net dipole moment. We compare these results with others in the literature. Our model overestimates the effect on the MRI signal.

---

### A continuous-time maxSAT solver with high analog performance [^b7d612ab]. Nature Communications (2018). Medium credibility.

Performance on random Max 3-SAT problems

We first test our algorithm and its prediction power on a large set (in total 4000) of random Max 3-SAT problems with N = 30, 50, 100 variables and constraint densities α = 8, 10. (In 3-SAT the SAT-UNSAT transition is around α ≃ 4.267). We compare our results with the true minimum values (E min) provided by the exact algorithm MaxSATZ. In Fig. 3, we compare the lowest energy found by the algorithm, the predicted minimumand the final decision by the algorithmwith the true optimum E min, by showing the distribution of their deviations from E min across many random problem instances. We use t max = 25 and at most Γ max = 150,000 runs, after which we stop the algorithm even if the prediction is not final. Thus, one expects that the performance of the algorithm decreases as N increases, (e.g. at N = 100), so that we would need to run more trajectories to obtain the same performance. Nevertheless, the results show that all three distributions have a large peak at 0. Most errors occur in the prediction phase, but many of these can be significantly reduced through simple decision rules (see Methods), because they occur most of the time at easy/small problems, where the statistics is insufficient (e.g. too few points since there are only few energy values). To show how the error in prediction depends on the hardness of problems, we studied the correlation between the errorand the hardness measure applicable to individual instances η = −ln κ /ln N (see ref.), see Fig. 3d (and Supplementary Fig. 4). Interestingly, larger errors occur mainly at the easiest problems with η < 2. Calculating the Pearson correlation coefficient betweenand η (excluding instances where the prediction is correct) we obtain a clear indication that often smaller η (thus for easier problems) generates larger errors. Positive errors are much smaller and shifted toward harder problems. Negative errors mean that the algorithm consistently predicts a slightly lower energy value than the optimum, which is good as this gives an increased assurance that we have found the optimum state. In Supplementary Fig. 4b, we show the correlation coefficients calculated separately for problems with different N and α.

---

### Two-dome structure in electron-doped iron arsenide superconductors [^1e4df636]. Nature Communications (2012). Medium credibility.

Iron arsenide superconductors based on the material LaFeAsO(1-x)F(x) are characterized by a two-dimensional Fermi surface (FS) consisting of hole and electron pockets yielding structural and antiferromagnetic transitions at x = 0. Electron doping by substituting O(2-) with F(-) suppresses these transitions and gives rise to superconductivity with a maximum T(c) of 26 K at x = 0.1. However, the over-doped region cannot be accessed due to the poor solubility of F(-) above x = 0.2. Here we overcome this problem by doping LaFeAsO with hydrogen. We report the phase diagram of LaFeAsO(1-x)H(x) (x < 0.53) and, in addition to the conventional superconducting dome seen in LaFeAsO(1-x)F(x), we find a second dome in the range 0.21 < x < 0.53, with a maximum T(c) of 36 K at x = 0.3. Density functional theory calculations reveal that the three Fe 3d bands (xy, yz and zx) become degenerate at x = 0.36, whereas the FS nesting is weakened monotonically with x. These results imply that the band degeneracy has an important role to induce high T(c).

---

### Sodium fluoride f-18 (sodium fluoride) [^6fb0e5ec]. FDA (2024). Medium credibility.

11 DESCRIPTION

11.1 Chemical Characteristics

Sodium Fluoride F 18 Injection is a positron emitting radiopharmaceutical, containing no-carrier-added, radioactive fluoride F18 that is used for diagnostic purposes in conjunction with PET imaging. It is administered by intravenous injection. The active ingredient, sodium fluoride F18, has the molecular formula Na[18F] with a molecular weight of 40.99, and has the following chemical structure:

Na+18F-

Sodium Fluoride F 18 Injection is provided as a ready-to-use, isotonic, sterile, pyrogen-free, preservative-free, clear and colorless solution. Each mL of the solution contains between 370 MBq to 7,400 MBq (10 mCi to 200 mCi) sodium fluoride F18, at the EOS reference time, in 0.9% aqueous sodium chloride. The pH of the solution is between 4.5 and 8. The solution is presented in 30 mL multiple- dose glass vials with variable total volume and total radioactivity in each vial.

11.2 Physical Characteristics

Fluoride F18 decays by positron (β+) emission and has a half-life of 109.7 minutes. Ninety-seven percent of the decay results in emission of a positron with a maximum energy of 633 keV and 3% of the decay results in electron capture with subsequent emission of characteristic X-rays of oxygen. The principal photons useful for diagnostic imaging are the 511 keV gamma photons, resulting from the interaction of the emitted positron with an electron (Table 2). Fluorine F18 atom decays to stable18O-oxygen.

The specific gamma ray constant (point source air kerma coefficient) for fluoride F18 is 5.7 R/hr/mCi (1.35 x 10-6Gy/hr/kBq) at 1 cm. The half-value layer (HVL) for the 511 keV photons is 4 mm lead (Pb). A range of values for the attenuation of radiation results from the interposition of various thickness of Pb. The range of attenuation coefficients for this radionuclide is shown in Table 3. For example, the interposition of an 8 mm thickness of Pb with a coefficient of attenuation of 0.25 will decrease the external radiation by 75%.

Table 4 lists the fraction of radioactivity remaining at selected time intervals from the calibration time. This information may be used to correct for physical decay of the radionuclide.

---

### Environmental damping and vibrational coupling of confined fluids within isolated carbon nanotubes [^2fa3c369]. Nature Communications (2024). High credibility.

Fig. 4
Relationship between the maximum damping limit and spring constants for a series of DWNTs.

a The parameters (Supplementary Table 12–3) from the regression of the series of scans in Fig. 2a are seen to disperse approximately linearly for the CNT G (29,1)@(35,6). b The linearity is more pronounced for CNT X (28,5)@(28,11). Stage pressure spanstobar with no apparent trend (Supplementary Table 12–1), except a set (red filled diamonds) after 12 h of baking out at 100 °C andbar (Supplementary Table 12–4). c The linearity is also pronounced for CNT F (19,3)@(22,11), again showing no apparent trend with stage pressure but with points from the same location clustering along the line represented as different data sets (Supplementary Table 12–2). The different datasets and symbols in Fig. 4a–c represent values from a series of repeated scans at different locations on the same CNT (Supplementary Table 12–1 – 12–4). d Values for for all three DWNTs approximate a normal distribution (dash line) with a mean 41.8 (cm −1) and standard deviation of 11.5 (cm −1). e Thermal annealing or baking out for 12 h at 100 °C andbar had the only demonstrable effect on the dispersion of these two mechanical parameters with a significant reduction inandafter processing. The data before (Pre-bake,) and after (Post-bake,) still fall identically on the line for CNT X in b, suggesting only a reduction in the number of graphitic tethers. The different symbols in a – d represent Raman scans at different spots in the same CNT and vacuum conditions. f FWHM atandof CNT X change upon thermally annealing. Scans 1–5 are pre-bake (as-grown) CNT X. After 12-hr annealing of CNT X (scans 6–11), remains nearly invariant and FWHM becomes narrow. This result is consistent with a decrease in the inhomogeneity of the system and in the number of carbonaceous tethers (Supplementary Table 12–1 to 12–4 and Supplementary Fig. 12–2). Source data are provided as a Source Data File.

---

### The minimal work cost of information processing [^894e044e]. Nature Communications (2015). Medium credibility.

where the sum ranges only over those x that have a non-zero probability of occurring. In the case of deterministic mappings p (x ′| x)∈{0,1}, this corresponds to the maximum number of input states that map to a same output state. For the AND gate, provided all four states 00, 01, 10 and 11 have non-negligible probability of occurring, there are three input states mapping to the same output state, so (3) gives us simply. Also, in simple examples as considered here, the expression (3) is stable to considering an-approximation (Supplementary Note 4); this quantity is thus physically justified.

Crucially, our result reveals that the minimal work requirement in general depends on the specific logical process, and not only on the input and output states. This contrasts with traditional thermodynamics for large systems, where the minimal work requirement of a state transformation can always be written as a difference of a thermodynamical potential, such as the free energy. For example, the minimal work cost of performing specifically an AND gate may differ from that of another logical process mapping an input distribution (p 00, p 01, p 10, p 11) (with ∑ i p i = 1) to the distribution (p ′ 0, p ′ 1) = (p 00 + p 01 + p 10, p 11) (Recall that the classical counterpart of a quantum state is a probability distribution.). To see this, consider the XOR gate, which outputs a 1 exactly when both inputs are different (see Fig. 2b). The minimal work cost requirement of this gate, as given by (3), is now only kT ln 2, as in the worst case, only a single bit of information is erased (again supposing that all four input states have non-negligible probability of occurring). Now, suppose that, for some reason, the input distribution is such that p 01 + p 10 = p 11, that is, the input 11 occurs with the same probability as of either 01 or 10 appearing. Then, the XOR gate reproduces the exact same output distribution as the AND gate: in both cases, we have p ′ 0 = p 00 + p 10 + p 01 = p 00 + p 11 and p ′ 1 = p 11 = p 01 + p 10. In other words, both logical processes have the same input and output state, yet the XOR gate only requires work kT ln 2 compared with the AND gate, which requires 1.6 kT ln 2. Furthermore, we point out that this difference, which appears small in this case, may be arbitrarily large in certain scenarios (Supplementary Note 4).

---

### Identifying domains of applicability of machine learning models for materials science [^9c5f816f]. Nature Communications (2020). High credibility.

A predictive ML model is then a functionaiming to minimize the expected error (also called prediction risk)measured by some non-negative loss function l that quantifies the cost incurred by predicting the actual property value y with f (x). Examples for loss functions are the squared error, the absolute error, and, for non-zero properties, the relative error. Here P denotes some fixed probability distribution that captures how candidate materials are assumed to be sampled from the materials class (this concept, while commonly assumed in ML, is an unnecessary restriction for high-throughput screening as we discuss in more detail below). Since the true prediction risk is impossible to compute directly without perfect knowledge of the investigated materials class, models are evaluated by the test error (or empirical risk)defined as the average of the individual errors (losses) e i (f) = l (f (x i), y i) on some test set of m reference data points. The samples in this test set are drawn independently and identically distributed according to P and are also independent of the model — which means in practice that it is a random subset of all available reference data that has been withheld from the ML algorithm. In order to reduce the variance of this estimate, a common strategy is cross-validation, where this process is repeated multiple times based on partitioning the data into a number of non-overlapping "folds" and then to use each of these folds as test sets and the remaining data as a training set to fit the model.

This test error properly estimates the model performance globally over the whole representation space X (weighted by the distribution P used to generate the test points). This is an appropriate evaluation metric for selecting a model that is required to work well on average for arbitrary new input materials that are sampled according to the same distribution P. This is, however, not the condition of high-throughput screening. Here, rather than being presented with random inputs, we can decide which candidate materials to screen next. This observation leads to the central idea enabled by the DA analysis proposed in this work: if the employed model is particularly applicable in a specific subdomain of the materials class, and if that subdomain has a simple and interpretable shape that permits to generate new materials from it, then we can directly focus the screening there.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^d6d8fcbc]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Spirometry reporting specifies that numerical values are given for the FEV1, the FVC, and the FEV1/FVC ratio; the latter should be reported as a decimal fraction and the space for percent predicted value left blank, and if bronchodilators are given the LLN column need not be repeated with absolute and percent change given only for FEV1 and FVC. Other numerical values such as the forced inspiratory flow at 75% of FVC (FEF75%) and FEF25–75% are not recommended for routine use. Graph requirements include that for the volume–time curve the volume scale should be at least 10 mm/L, the time scale at least 20 mm/s, and 1 second prior to the start of expiration should be displayed; on the flow–volume plot the flow display should be at least 5 l/min/L/s, and the ratio of flow to volume should be 2 L/s to 1 L, and linear and log scales where values are plotted as z-scores relative to the predicted value (z = 0) give an intuitive sense of severity.

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^cf66bf7c]. Nature Communications (2023). High credibility.

Dose-response curves are key metrics in pharmacology and biology to assess phenotypic or molecular actions of bioactive compounds in a quantitative fashion. Yet, it is often unclear whether or not a measured response significantly differs from a curve without regulation, particularly in high-throughput applications or unstable assays. Treating potency and effect size estimates from random and true curves with the same level of confidence can lead to incorrect hypotheses and issues in training machine learning models. Here, we present CurveCurator, an open-source software that provides reliable dose-response characteristics by computing p-values and false discovery rates based on a recalibrated F-statistic and a target-decoy procedure that considers dataset-specific effect size distributions. The application of CurveCurator to three large-scale datasets enables a systematic drug mode of action analysis and demonstrates its scalable utility across several application areas, facilitated by a performant, interactive dashboard for fast data exploration.

---

### Using synchronized oscillators to compute the maximum independent set [^c5524e12]. Nature Communications (2020). High credibility.

Not all computing problems are created equal. The inherent complexity of processing certain classes of problems using digital computers has inspired the exploration of alternate computing paradigms. Coupled oscillators exhibiting rich spatio-temporal dynamics have been proposed for solving hard optimization problems. However, the physical implementation of such systems has been constrained to small prototypes. Consequently, the computational properties of this paradigm remain inadequately explored. Here, we demonstrate an integrated circuit of thirty oscillators with highly reconfigurable coupling to compute optimal/near-optimal solutions to the archetypally hard Maximum Independent Set problem with over 90% accuracy. This platform uniquely enables us to characterize the dynamical and computational properties of this hardware approach. We show that the Maximum Independent Set is more challenging to compute in sparser graphs than in denser ones. Finally, using simulations we evaluate the scalability of the proposed approach. Our work marks an important step towards enabling application-specific analog computing platforms to solve computationally hard problems.

---

### Sodium fluoride f-18 (sodium fluoride F 18) [^71c0b65f]. FDA (2025). Medium credibility.

11 DESCRIPTION

11.1 Chemical Characteristics

Sodium Fluoride F 18 Injection is a positron emitting radiopharmaceutical, containing no-carrier-added, radioactive fluoride F18 that is used for diagnostic purposes in conjunction with PET imaging. It is administered by intravenous injection. The active ingredient, sodium fluoride F18, has the molecular formula Na[18F] with a molecular weight of 40.99, and has the following chemical structure:

Sodium Fluoride F 18 Injection is provided as a ready-to-use, isotonic, sterile, pyrogen-free, preservative-free, clear and colorless solution. Each mL of the solution contains between 370 MBq and 7,400 MBq (10 mCi to 200 mCi) sodium fluoride F18, at the EOS reference time, in 0.9% aqueous sodium chloride. The pH of the solution is between 4.5 and 8. The solution is presented in XX mL multiple- dose glass vials with variable total volume and total radioactivity in each vial.

11.2 Physical Characteristics

Fluoride F18 decays by positron (β+) emission and has a half-life of 109.7 minutes. Ninety-seven percent of the decay results in emission of a positron with a maximum energy of 633 keV and 3% of the decay results in electron capture with subsequent emission of characteristic X-rays of oxygen. The principal photons useful for diagnostic imaging are the 511 keV gamma photons, resulting from the interaction of the emitted positron with an electron (Table 2). Fluorine F18 atom decays to stable18O-oxygen.

The specific gamma ray constant for fluoride F18 is 5.7 R/hr/mCi (1.35 x 10-6Gy/hr/kBq) at 1 cm. The half-value layer (HVL) for the 511 keV photons is 4.1 mm lead (Pb). A range of values for the attenuation of radiation results from the interposition of various thickness of Pb. The range of attenuation coefficients for this radionuclide is shown in Table 3. For example, the interposition of an 8.3 mm thickness of Pb with a coefficient of attenuation of 0.25 will decrease the external radiation by 75%.

Table 4 lists the fraction of radioactivity remaining at selected time intervals from the calibration time. This information may be used to correct for physical decay of the radionuclide.

---

### Identifying domains of applicability of machine learning models for materials science [^72c4ba76]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ν that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by σ lin (x 1, x 2) ≡ −0.3 ≤ x 1 ≤ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: σ rbf ≡ −3.3 ≤ x 11 ≤ 3.1 ∧ x 2 ≤ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA σ poly (x 1, x 2) ≡ −3.5 ≤ x 2 ≤ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^cbffc27e]. CDC (2011). Medium credibility.

Appendix G. Mathematical formula for sample size — sample size required to detect the difference of GM between two subpopulation groups defines a z test with null hypothesis GM1 = GM2 or GMmax/GMmin = 1 and alternative GM1 ≠ GM2 or GMmax / GMmin > 1, where GMmax = max(GM1, GM2) and GMmin = min(GM1, GM2). It considers power W (say 80% chance) to detect that one group's GM is at least k fold higher and gives the p-value as p = Prob(Z ≥ z) = 1 − φ−1(z) with Z ~ Normal (0,1). Given α (e.g., 0.05) and target ratio k, power is W = Prob (Z > φ−1(1 − α) − log10(k)/(S√(1/n1 + 1/n2))), where S is the expected standard deviation of log10(VL) in the population of interest; when n1 = n2 = n, W = Prob (Z > φ−1(1 − α) − log10(k)/(S√(2/n))). The required sample size is n = 2[(φ−1(1 − α) − φ−1(1 − W))S/log10(k)]^2, and tables are provided for W = 0.8 and W = 0.9 with α = 0.05.

---

### Evaluation of the evenness score in next-generation sequencing [^1f5f1936]. Journal of Human Genetics (2016). Low credibility.

The evenness score (E) in next-generation sequencing (NGS) quantifies the homogeneity in coverage of the NGS targets. Here I clarify the mathematical description of E, which is 1 minus the integral from 0 to 1 over the cumulative distribution function F(x) of the normalized coverage x, where normalization means division by the mean, and derive a computationally more efficient formula; that is, 1 minus the integral from 0 to 1 over the probability density distribution f(x) times 1-x. An analogous formula for empirical coverage data is provided as well as fast R command line scripts. This new formula allows for a general comparison of E with the coefficient of variation (= standard deviation σ of normalized data) which is the conventional measure of the relative width of a distribution. For symmetrical distributions, including the Gaussian, E can be predicted closely as 1-σ(2)/2⩾E⩾1-σ/2 with σ ≤ 1 owing to normalization and symmetry. In case of the log-normal distribution as a typical representative of positively skewed biological data, the analysis yields E≈exp(-σ*/2) with σ*(2) = ln(σ(2)+1) up to large σ (≤ 3), and E≈1-F(exp(-1)) for very large σ (⩾2.5). In the latter kind of rather uneven coverage, E can provide direct information on the fraction of well-covered targets that is not immediately delivered by the normalized σ. Otherwise, E does not appear to have major advantages over σ or over a simple score exp(-σ) based on it. Actually, exp(-σ) exploits a much larger part of its range for the evaluation of realistic NGS outputs.

---

### Follitropin alfa (Gonal-F) [^edcfa993]. FDA (2025). Medium credibility.

2.3 Dosing for Ovulation Induction

Prior to initiation of treatment with GONAL-f:

Perform a complete gynecologic and endocrinologic evaluation
Exclude primary ovarian failure
Exclude the possibility of pregnancy
Demonstrate tubal patency
Evaluate the fertility status of the male partner

The dosing scheme is stepwise and is individualized for each woman [see Clinical Studies (14.1)].

Administer a starting dose of 75 International Units of GONAL-f subcutaneously daily for 14 days in the first cycle of use.
In subsequent cycles of treatment, determine the starting dose (and dosage adjustments) of GONAL-f based on the woman's history of the ovarian response to GONAL-f.
If indicated by the ovarian response after the initial 14 days, make an incremental adjustment in dose of up to 37.5 International Units.
If indicated by the ovarian response, make additional incremental adjustments in the dose, up to 37.5 International Units, every 7 days.
Continue treatment until follicular growth and/or serum estradiol levels indicate an adequate ovarian response.
Consider the following when planning the woman's individualized dose:
Use the lowest dose of GONAL-f consistent with the expectation of good results.
Use appropriate GONAL-f dose adjustment(s) to prevent multiple follicular growth and cycle cancellation.
The maximum, individualized, daily dose of GONAL-f is 300 International Units per day.
In general, do not exceed 35 days of treatment, unless an estradiol rise indicates imminent follicular development.
When pre-ovulatory conditions are reached, administer human chorionic gonadotropin (hCG) to induce final oocyte maturation and ovulation. Human chorionic gonadotropin, hCG, (5,000 USP units) should be given 1 day after the last dose of GONAL-f.
Encourage the woman and her partner to have intercourse daily, beginning on the day prior to the administration of hCG and until ovulation becomes apparent.
Withhold hCG in cases where the ovarian monitoring suggests an increased risk of ovarian hyperstimulation syndrome (OHSS) on the last day of GONAL-f therapy (for example estradiol greater than 2,000 pg per mL) [see Warnings and Precautions (5.2, 5.3, 5.5, 5.11)].
Discourage intercourse when the risk for OHSS is increased [see Warnings and Precautions (5.2, 5.5)].
Schedule a follow-up visit in the luteal phase.
Individualize the initial dose administered in subsequent cycles based on the woman's response in the preceding cycle.
As in the initial cycle, do not administer doses larger than 300 International Units of FSH per day. Administer 5,000 USP units of hCG 1 day after the last dose of GONAL-f to complete follicular development and induce ovulation.
Follow the above recommendations to minimize the chance of development of OHSS.

---

### Input-output maps are strongly biased towards simple outputs [^50fca0dd]. Nature Communications (2018). Medium credibility.

Many systems in nature can be described using discrete input-output maps. Without knowing details about a map, there may seem to be no a priori reason to expect that a randomly chosen input would be more likely to generate one output over another. Here, by extending fundamental results from algorithmic information theory, we show instead that for many real-world maps, the a priori probability P(x) that randomly sampled inputs generate a particular output x decays exponentially with the approximate Kolmogorov complexity [Formula: see text] of that output. These input-output maps are biased towards simplicity. We derive an upper bound P(x)≲[Formula: see text], which is tight for most inputs. The constants a and b, as well as many properties of P(x), can be predicted with minimal knowledge of the map. We explore this strong bias towards simple outputs in systems ranging from the folding of RNA secondary structures to systems of coupled ordinary differential equations to a stochastic financial trading model.

---

### Reconstructing metastatic seeding patterns of human cancers [^2bf00393]. Nature Communications (2017). Medium credibility.

In the evolutionary conflict graph G = (V, E), each node i ∈ V represents a different mutation pattern. For n samples, the number of nodes | V | is given by 2 n. For each pair of evolutionarily incompatible mutation patterns i and j, there exists an edge (i, j)∈ E. The weight (c i) of each node i is given by the reliability scores ω i described in the Bayesian inference model section (Supplementary Fig. 3).

The MILP to find the minimal-weighted set of evolutionarily incompatible mutation patterns is defined by the following objective function and constraints:

This formulation guarantees that the MILP solver finds the minimal value of the objective function such that all constraints are met and hence the nodes in the selected set cover all edges. The evolutionarily compatible and most reliable mutation patterns { i | x i = 0} are given by the complement set of the optimal solution { i | x i = 1} to the MILP.

Day and Sankoff showed that inferring the most likely evolutionary trajectories is a computationally challenging problem (NP -complete). Sophisticated approximation algorithms have been developed in the context of language and cancer evolution. However, medium-sized instances of NP -complete problems are no longer intractable due to the enormous engineering and research effort that has been devoted to ILP solvers. The MILPformulation enables an efficient and robust analysis of large data sets. We prove that an approximation algorithm that would guarantee that its solution is at most 36.06% worse than the optimal solution cannot exist unless the complexity class P = NP (Supplementary Methods, Theorem 1). Salari et al.explored a related approach but approximated two NP -complete problems, possibly leading to suboptimal results. Treeomics produces a mathematically guaranteed to be optimal result without convergence or termination issues. Note that a mathematical optimal solution is not necessarily equivalent to the biological truth, especially in the case of low neoplastic cell content or coverage (Figs 3 and 4). MILPs may also be useful in other areas of phylogenetic inference where methods with strong biological assumptions (for example, constant mutation rates or specific substitution profiles) are not applicable or are computationally too expensive to obtain guaranteed optimal solutions.

---

### Using both qualitative and quantitative data in parameter identification for systems biology models [^fc71653e]. Nature Communications (2018). Medium credibility.

Results

An illustration of the potential value of qualitative data

To demonstrate the potential value of qualitative data, we consider a simple case of solving for the coefficients of polynomial functions.

We consider two polynomial functions: y 1 = ax 2 − bx + c and y 2 = dx + e. Suppose we want to solve for the coefficients a, b, c, d, and e, which we will take to be positive. As the ground truth coefficients to be determined, we choose (a, b, c, d, e) = (0.5, 3, 5, 1, 1.5).

Suppose that a limited amount of quantitative information is available. Namely, it is known that the parabola y 1 contains the points (2, 1) and (8, 13), and the line y 2 contains the point (3.5,5). This is not enough information to solve for any of the coefficients because three points are required to specify a parabola, and two points are required to specify a line (Fig. 1a).

Fig. 1
A simple illustration using polynomial functions. We use qualitative and quantitative information to determine the unknown coefficients. a Visualization of the problem. We seek to find the coefficients of equations for a parabola and a line, with the ground truth shown (blue solid curves). Two points on the parabola and one point on the line are known (black dots). These three points are consistent with infinitely many possible solutions (e.g. orange dashed curves). Qualitative information (colored circles, x -axis) specifies whether the parabola is above (+) or below (−) the line. This information limits the possible values of intersection points x 1 and x 2 to the green shaded segments of the x -axis. b Bounds on coefficient values as a function of the number of qualitative points known. Shaded areas indicate the range of possible values of each coefficient

---

### Miniature origami robot for various biological micromanipulations [^03cd1422]. Nature Communications (2025). High credibility.

Contact operation force control

The thin film pressure sensor PXS-S6ST-50K (GuanZhou PuHui Technology Co. Ltd) is fixed between the moving platform and the needle to measure the normal force along the needle. During the cell contact operation process, we primarily focus on three values: the current value of the sensor F t, the minimum value recorded in this operation F m i n, and the maximum value F m a x.

For the cell touching operation, we set an upper limit for the contact force F U, when F t achieve or above the upper limit, the needle will hold on for a while and retract. The algorithm is shown in Algorithm 1. During the cell puncturing process, precise control of the needle is critical to ensure that cells are punctured rather than cut through. With force measurement, we are able to monitor the change in the contact force. A rapid decrease implies that the cell has already been punctured. Based on this judgment, we set the puncture force threshold R = 0.18, meaning that when F t is below 82% of the maximum puncturing force (F m a x − F m i n), the needle tip will retract. This ensures that only the cell wall is punctured without penetrating the entire cell. The process is demonstrated in Algorithm 2.

Algorithm 1

Cell touching

1: while Touching command starts do

2: if F t < F U then (Contact force not achieve the upper limit)

3: Needle tip keeps going down

4: else

5: Needle tip stays in the current position for a while

6: Needle tip lifts

7: break

8: end if

9: end while

Algorithm 2

Cell puncturing

1: while Puncturing command starts do

2: if F t < F U then (Before puncturing, ensure contact with the cell)

3: Needle tip keeps going down

4: else

5: if (F m a x − F t) ≥ R (F m a x − F m i n) then (A rapid decrease in contact force implies that the cell has been punctured)

6: Needle tip lifts

7: break

8: else

9: Needle tip keeps going down

10: end if

11: end if

12: end while

---

### Continuous growth reference from 24th week of gestation to 24 months by gender [^0f68c406]. BMC Pediatrics (2008). Low credibility.

Appendix

Assuming normality, to estimate the variance, skewness and kurtosis of the population based on the cases in the range of μ ± δσ in a sample for certain values of δ > 0.

Without lost of generality, let μ = 0. Let, where φ (x; σ) is the normal probability distribution function with mean 0 and variance σ 2. It can be deduced that

where Φ (x) is the standard normal cumulative distribution function. Letand. It can be shown that

Letand. It can be shown that

Let p = 2Φ(δ) - 1 be the proportion of observations in the range of μ ± δσ under a normal distribution. Considering a sample of n observations, x 1. x n. Let q 1 be the (1 - p)/2-th quantile and q 2 be the (1 - (1 - p)/2)-th quantile and X r = { x i: q 1 ≤ x i ≤ q 2 } be the reduced sample of size m. An empirical estimate of, j = 2, 3, 4, is given as

The general sample variance, the skewness and the kurtosis are given as

and

respectively. Let, Skew r and Kurt r be the corresponding statistics calculated based on the reduced sample. Substituting – into –, we have the statistics for the full dataset evaluated based on the reduced sample as

and

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^dee6cacc]. CDC (2011). Medium credibility.

Community viral load sample size calculations — sample size to detect differences in geometric mean (GM) viral load depends on power and standard deviation (S). The table specifies "α = 0.05 and W = 0.8," with S columns 1 to 1.5; for k = 3 the minimum sample sizes by S are 54, 66, 78, 92, 106, and 122, and "1.2 is the standard deviation of national VL data," so at S = 1.2 the needed sample size is 78.

---

### Fundamental limits to learning closed-form mathematical models from data [^a345300f]. Nature Communications (2023). High credibility.

Given a finite and noisy dataset generated with a closed-form mathematical model, when is it possible to learn the true generating model from the data alone? This is the question we investigate here. We show that this model-learning problem displays a transition from a low-noise phase in which the true model can be learned, to a phase in which the observation noise is too high for the true model to be learned by any method. Both in the low-noise phase and in the high-noise phase, probabilistic model selection leads to optimal generalization to unseen data. This is in contrast to standard machine learning approaches, including artificial neural networks, which in this particular problem are limited, in the low-noise phase, by their ability to interpolate. In the transition region between the learnable and unlearnable phases, generalization is hard for all approaches including probabilistic model selection.

---

### Genome-driven evolutionary game theory helps understand the rise of metabolic interdependencies in microbial communities [^ab9d7d94]. Nature Communications (2017). Medium credibility.

Automated identification of the Nash equilibria

Upon constructing the payoff matrix, as described above, one can identify the Nash equilibria of the game. We developed NashEq Finder, an optimization-based procedure to automate the identification of all pure strategy Nash equilibria of an n -player game. Here, for ease of presentation, we describe the NashEq Finder formulation for a two-player non-symmetric game. Let P and Q denote the set of all conceivable strategies for player 1 and player 2, respectively. A binary decision variable is defined as follows to capture whether or not each entry pq of the payoff matrix satisfies the conditions of a Nash equilibrium:Now, let the entry pq of the payoff matrix constitutewithandbeing the payoffs of players 1 and 2, respectively. NashEq Finder can be formulated as follows:where, LB 1 and LB 2 are non-zero lower bounds on the payoff values of players 1 and 2, respectively, The objective function of this optimization problem maximizes the values of binary variables corresponding to entries of the payoff matrix. The definition of binary variable w pq is mathematically imposed by Constraints (6) and (7). In particular, Constraint (6) mathematically describes the conditions of a pure strategy Nash equilibrium for player 1, i.e. if player 2's strategy is fixed at q ∈ Q, player 1 attains its maximum payoff by taking strategy p ∈ P. Constraint (7) imposes the same condition for player 2 (see Supplementary Methods for an example detailing how these constraints work). Notice that NashEq Finder is an integer linear program (ILP), which can be always solved to global optimality (if an optimal solution exists). Upon solving this optimization problem, any entry of the payoff matrix for which the corresponding binary variable is equal to one will be a Nash equilibrium. NashEq Finder can thus identify all pure strategy Nash equilibria of the game in one shot by solving this ILP problem. If this optimization problem is infeasible, it means that no pure strategy Nash equilibrium exists. This formulation can be easily generalized for an n -player game. A python script implementing NashEq Finder for an n -player game is available in Supplementary Software 1. A rudimentary assessment of the computational efficiency of the NashEq Finder for the case studies presented in this paper is also provided in Supplementary Methods.

---

### Efficient neural codes naturally emerge through gradient descent learning [^8fec235d]. Nature Communications (2022). High credibility.

What W learns: autoencoding objective

Further describing the growth of singular values requires a choice of objective. The base case of our study is the autoencoding objective defined for a set of inputs X:

Our goal is to determine how W evolves for this cost function. We will examine both the singular vectors and the singular values.

During learning, the singular vectors rotate (recall they are unit length and orthogonal) until they reach a fixed point. For this cost function, it is easy to verify that a fixed point of dynamics is when the singular vectors are equal to the principal components of the inputs (see Supplementary Methods for proof). That is, the vectors are static when Σ x x = V Λ V T and W = V S V T for the same V but potentially different Λ and S. This alignment is especially relevant given the expression for network sensitivity derived above. With the vectors aligned, the sensitivity to each corresponding principal component of the inputs is given by, the squared singular value of W.

The evolution of sensitivity is thus governed by the evolution of singular values. The rate of change of σ i is complicated to calculate because the singular vectors can potentially rotate. However, for the sake of analysis one can examine the case when the singular vectors are initialized at the fixed point mentioned above, as in previous literature. In this set of initial conditions, the time-evolution of each singular value of W is given by refs.:

Note that the rate of learning is controlled by λ i, the standard deviation of the i th principal component of the inputs. The term on the right causes σ i (t) to converge to 1 asymptotically, as is expected as the solution of the full-rank reconstruction problem is W = I. For deeper networks (N ≥ 2), the growth is sigmoidal and approaches a step function as N → ∞ (see ref.). Thus, in this axis-aligned initialization, the singular values σ i (t) are learned in order of the variance of the associated principal components of the inputs.

Together, these results mean that the sensitivity of a linear network's output to the principal components of the inputs evolve in order of variance when trained on input reconstruction. This is exactly the case for the axis-aligned initialization and approximately true for small initializations. For the single-matrix network displayed in the figure in the main text, the sensitivity to the j th PC thus evolves over time t as:

---

### Breathing-adapted imaging techniques for rapid 4-dimensional lung tomosynthesis [^c12056f0]. Advances in Radiation Oncology (2023). Medium credibility.

Figure 2
Simulated sinusoidal breathing waveforms and associated imaging techniques plots for D = 1 mm and f = 5. Left: 1 cm displacement, 12 bpm. Right: 4 cm displacement, 25 bpm. (A) Breathing displacement (cm) versus time. (B) Arc duration (seconds). (C) Frame rate (Hz). (D) Pulse duration (ms). (E) Normalized tube current (mA/mAs AEC). Abbreviation: bpm = breaths per minute.

Figure 3 presents plots of extreme imaging technique values derived from the sinusoidal-based model of Lujan et al with velocity computed as it was for Fig. 2 with breathing displacements of 10, 20, 30, and 40 mm and respiration rates of 12, 15, 20, and 25 bpm.

Figure 3
Imaging technique extreme values derived from simulated sinusoidal breathing waveforms using instantaneous analytical velocity at 12, 15, 20, and 25 breaths per minute and 10 to 40 mm breathing displacements for D = 1 mm and f = 5. (A) Minimum arc duration (seconds). (B) Maximum frame rate (Hz). (C) Minimum pulse duration (ms). (D) Maximum normalized tube current (mA/mAs AEC).

Table 1 presents the extreme scanning parameters for D = 1 and = 5 for volunteers and simulated breathing cycles: (1) volunteers' extreme values of shortest arc duration of 0.066 to 0.267 seconds, fastest frame rate of 228 to 921 Hz, shortest x-ray pulse duration of 1.076 to 4.368 ms, and maximum tube current, normalized toof 18.8 to 76.2 s –1; (2) simulated sinusoidal waveforms scanning parameters, using the average velocity (Equation 2), of shortest arc duration 0.034 to 0.253 seconds, fastest frame rate 241 to 1801 Hz, shortest x-ray pulse duration 0.545 to 4.141 ms, and maximum tube current, normalized to10.0 to 150.3 s –1; (3) simulated sinusoidal waveforms scanning parameters, using the instantaneous velocity (Equation 10), of shortest arc duration 0.029 to 0.245 seconds, fastest frame rate 249 to 2074 Hz, shortest x-ray pulse duration 0.472 to 4.007 ms, and maximum tube current, normalized to20.4 to 173.6 s –1.

---

### Quantitative assessment of full-width at half-maximum and detector energy threshold in X-ray imaging systems [^c65d8c68]. European Journal of Radiology (2024). Medium credibility.

Background

The response function of imaging systems is regularly considered to improve the qualified maps in various fields. More the accuracy of this function, the higher the quality of the images.

Methods

In this study, a distinct analytical relationship between full-width at half-maximum (FWHM) value and detector energy thresholds at distinct tube peak voltage of 100 kV has been addressed in X-ray imaging. The outcomes indicate that the behavior of the function is exponential. The relevant cut-off frequency and summation of point spread function S(PSF) were assessed at large and detailed energy ranges.

Results

A compromise must be made between cut-off frequency and FWHM to determine the optimal model. By detailed energy range, the minimum and maximum of S(PSF) values were revealed at 20 keV and 48 keV, respectively, by 2979 and 3073. Although the maximum value of FWHM occurred at the energy of 48 keV by 224 mm, its minimum value was revealed at 62 keV by 217 mm. Generally, FWHM value converged to 220 mm and S(PSF) to 3026 with small fluctuations. Consequently, there is no need to increase the voltage of the X-ray tube after the energy threshold of 20 keV.

Conclusion

The proposed FWHM function may be used in designing the setup of the imaging parameters in order to reduce the absorbed dose and obtain the final accurate maps using the related mathematical suggestions.

---

### Effects of changing population or density on urban carbon dioxide emissions [^49635916]. Nature Communications (2019). High credibility.

To account for the multicollinearity problem, we have fitted Eqs. (3) and (5) by using the ridge regression approach. This method solves the matrix inversion problem by adding a constant λ to the diagonal elements of X T X, so that the ridge estimator for the linear coefficients is a = (X T X + λ I) −1 X T y, where I is the identity matrix. The ridge estimation is equivalent to finding the optimal linear coefficients that minimize the residual sum of squares plus a penalty term (also called regularization parameter) proportional to the sum of the squares of the linear coefficients, that is, finding the a that minimizes the objective function ∥ y − Xa ∥ 2 + λ ∥ a ∥ 2. The optimal value of λ is usually unknown in practice and needs to be estimated from data. To do so, we have used the approach of searching for the value of λ that minimizes the mean squared error (MSE) in a leave-one-out cross validation strategy. In this approach, we estimate a (for a given λ) using all data except for one point that is used for calculating the squared error. This process is repeated until every data point is used exactly once for estimating the squared error, and then we calculate the value of the MSE for a given λ. The optimal value of λ = λ * is the one that minimizes the average value of the MSE estimated with the leave-one-out cross validation method. We have also standardized all predictors before searching for the optimal value λ *. This is a common practice when dealing with regularization methods and ensures that the penalty term is uniformly applied to the predictors, that is, the normalization makes the scale of the predictors comparable and prevents variables with distinct ranges from having uneven penalization.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^d33c8072]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Quality categories for FVC or FEV1 in adults and children — Grades A–F are defined by the number of acceptable tests and repeatability thresholds. Grade A requires " ≥ 3 acceptable tests with repeatability within 0.150 L for age 2–6, 0.100 L, or 10% of highest value, whichever is greater"; Grade B requires " ≥ 2 acceptable tests with repeatability within 0.150 L for age 2–6, 0.100 L, or 10% of highest value, whichever is greater"; Grade C requires " ≥ 2 acceptable tests with repeatability within 0.200 L for age 2–6, 0.150 L, or 10% of highest value, whichever is greater"; Grade D requires " ≥ 2 acceptable tests with repeatability within 0.250 L for age 2–6, 0.200 L, or 10% of highest value, whichever is greater"; Grade E is "One acceptable test," and Grade F is "No acceptable tests."

---

### Article 5. An introduction to estimation – 2: from z to T [^fb82b806]. Emergency Medicine Journal (2001). Low credibility.

Provided the sample size is large enough (that is, n greater than 100), the z statistic can be used to determine the confidence interval estimation of the population mean even when the sigma is not known. In these cases the estimation of the standard error of the mean is used. The z statistic is also valid when determining the population's proportion based upon a large sample. However, when dealing with smaller samples, the z statistic is replaced by the t statistic. This makes it possible to estimate, in a population with an unknown standard deviation: The probability of getting a sample mean greater than or equal to a particular value The value of a sample mean with a particular probability of occurring The probability of getting a sample mean between two particular values The confidence interval for the estimation of the population mean can also be determined using the t statistic.

---

### Eigenspectra optoacoustic tomography achieves quantitative blood oxygenation imaging deep in tissues [^65f3ba04]. Nature Communications (2016). Medium credibility.

The minimization problem defined by equation (5) is ill-posed and may converge to a wrong solution unless properly constrained. For achieving inversion stability and accurate sO 2 estimation results, the cost function f of equation (5) is simultaneously minimized in a set of grid points placed in the image domain (Fig. 1i), where three-independent constraints are further imposed to the Eigenfluence parameters. These constraints correspond to the relation of the Eigenfluence parameters between neighbour grid points and to the allowed search space for the Eigenfluence parameters:

Since the values of the second Eigenfluence parameter m 2 present a consistent trend of reduction with tissue depth observed both in the case of uniform tissue simulations (see Fig. 1g), as well as in simulations with random structures, m 2 is constrained to obtain smaller values in the case of grid points placed deeper into tissue.
As the light fluence spectrum is bound to vary smoothly in space due to the nature of diffuse light propagation, large variations of the Eigenfluence parameters m 1, and m 3 between neighbour pixels are penalized. This spatial smoothness constraint is achieved through the incorporation of appropriate regularization parameters α i to the cost function for constraining the variation of the model parameters (see equation (6)). The values of the regularization parameters were selected using cross-validation on simulated data sets (Supplementary Note 2).
Since the values of m 1 and m 3 are strongly dependent on background blood sO 2, an initial less accurate estimation of tissue sO 2 can be effectively used to reduce the total search space to a constrained relevant sub-space. The limits of search space for the Eigenfluence parameters m 1 and m 3 corresponding to each grid point are identified in a preprocessing step as analytically described in Supplementary Note 2.

---

### Drawing statistical conclusions from experiments with multiple quantitative measurements per subject [^70dff523]. Radiotherapy and Oncology (2020). Medium credibility.

In experiments with multiple quantitative measurements per subject, for example measurements on multiple lesions per patient, the additional measurements on the same patient provide limited additional information. Treating these measurements as independent observations will produce biased estimators for standard deviations and confidence intervals, and increases the risk of false positives in statistical tests. The problem can be remedied in a simple way by first taking the average of all observations of each specific patient, and then doing all further calculations only on the list of these patient means. A more sophisticated statistical modeling of the experiment, for example in a linear mixed model, is only required if (i) there is a large imbalance in the number of observations per patient or (ii) there is a specific interest in actually identifying the various sources of variation in the experiment.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^bb9b7f12]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Δ Φ, the probability distribution at a point Φ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Φ ∈ {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters α and β, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since α and β are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Φ ∈ [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(α, β) and x 2 = 1 − x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Φ, it may seem natural to select α and β by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### A practical guide for understanding confidence intervals and P values [^c0a7d4a0]. Otolaryngology — Head and Neck Surgery (2009). Low credibility.

The 95 percent confidence interval about the mean demarcates the range of values in which the mean would fall if many samples from the universal parent population were taken. In other words, if the same observation, experiment, or trial were done over and over with a different sample of subjects, but with the same characteristics as the original sample, 95 percent of the means from those repeated measures would fall within this range. This gives a measure of how confident we are in the original mean. It tells us not only whether the results are statistically significant because the CI falls totally on one side or the other of the no difference marker (0 if continuous variables; 1 if proportions), but also the actual values so that we might determine if the data seem clinically important. In contrast, the P value tells us only whether the results are statistically significant, without translating that information into values relative to the variable that was measured. Consequently, the CI is a better choice to describe the results of observations, experiments, or trials.

---

### Fundamental limits to learning closed-form mathematical models from data [^42829259]. Nature Communications (2023). High credibility.

Probabilistic model selection yields quasi-optimal predictions for unobserved data

Probabilistic model selection as described above follows directly (and exactly, except for the explicit approximation in Eq. (4)) from the postulates of Cox's theorem, and is therefore (Bayes) optimal when models are truly drawn from the prior p (m). In particular, the minimum description length (MDL) model is the most compressive and the most plausible one, and any approach selecting, from D, a model that is not the MDL model violates those basic postulates.

We start by investigating whether this optimality in model selection also leads to the ability of the MDL model to generalize well, that is, to make accurate predictions about unobserved data. We show that the MDL model yields quasi-optimal generalization despite the fact that the best possible generalization is achieved by averaging over models, and despite the BIC approximation in the calculation of the description length (Fig. 1). Specifically, we sample a model m * from the prior p (m) described in ref.and in the Methods; for a model not drawn from the prior, see Supplementary Text and Fig. S1. From this model, we generate synthetic datasets, i = 1,…, N (where y i = m * (x i, θ *) + ϵ i and ϵ i ~ Gaussian(0, s ϵ)) with different number of points N and different levels of noise s ϵ. Then, for each dataset D, we sample models from p (m ∣ D) using the Bayesian machine scientist, select the MDL model among those sampled, and use it to make predictions on a test dataset.

---

### The backtracking survey propagation algorithm for solving random K-SAT problems [^f6ee7b94]. Nature Communications (2016). Medium credibility.

Discrete combinatorial optimization has a central role in many scientific disciplines, however, for hard problems we lack linear time algorithms that would allow us to solve very large instances. Moreover, it is still unclear what are the key features that make a discrete combinatorial optimization problem hard to solve. Here we study random K-satisfiability problems with K = 3,4, which are known to be very hard close to the SAT-UNSAT threshold, where problems stop having solutions. We show that the backtracking survey propagation algorithm, in a time practically linear in the problem size, is able to find solutions very close to the threshold, in a region unreachable by any other algorithm. All solutions found have no frozen variables, thus supporting the conjecture that only unfrozen solutions can be found in linear time, and that a problem becomes impossible to solve in linear time when all solutions contain frozen variables.

---

### The XZZX surface code [^8f6ac9c8]. Nature Communications (2021). High credibility.

This structured noise model thus leads to two distinct regimes, depending on which failure process is dominant. In the first regime where, we expect that the logical failure rate will decay like. We find this behaviour with systems of a finite size and at high bias where error rates are near to threshold. We evaluate logical failure rates using numerical simulations to demonstrate the behavior that characterises this regime; see Fig. 6 (a). Our data show good agreement with the scaling ansatz. In contrast, our data are not well described by a scaling.

Fig. 6
Sub-threshold scaling of the logical failure rate with the XZZX code.

a Logical failure rateat high bias near to threshold plotted as a function of code distance d. We use a lattice with coprime dimensions d × (d + 1) for d ∈ {7, 9, 11, 13, 15} at bias η = 300, assuming ideal measurements. The data were collected usingiterations of Monte-Carlo (MC) samples for each physical rate sampled and for each lattice dimension used. The physical error rates used are, from the bottom to the top curves in the main plot, p = 0.19, 0.20, 0.21, 0.22 and 0.23. Error bars represent one standard deviation for the Monte-Carlo simulations. The solid lines are a fit of the data to, consistent with Eq. (2), and the dashed lines a fit to, consistent with Eq. (3) where we would expect, see Methods. The data fit the former very well; for the latter, the gradients of the best fit dashed lines, as shown on the inset plot as a function of, give a linear slope of 0.61(3). Because this slope exceeds the value of 0.5, we conclude that the sub-threshold scaling is not consistent with. b Logical failure ratesat modest bias far below threshold plotted as a function of the physical error rate p. The data (markers) were collected at bias η = 3 and coprime d × (d + 1) code dimensions of d ∈ {5, 7, 9, 11, 13, 15} assuming ideal measurements. Data is collected using the Metropolis algorithm and splitting method presented in refs. The solid lines represent the prediction of Eq. (3). The data show very good agreement with the single parameter fitting for all system sizes as p tends to zero.

---

### Massive computational acceleration by using neural networks to emulate mechanism-based biological models [^2c01f625]. Nature Communications (2019). High credibility.

To identify the minimum size of dataset needed for accurately making predictions, we trained deep LSTM network on different training dataset sizes. The RMSEs are calculated based on predictions of a fixed test dataset, which contains 20,000 samples. Figure 2c demonstrates how the RMSEs of distributional data and peak values decrease with the increase of training data size. Since the x -axis is log-scaled, when the dataset size is beyond 10 4, the rate of error reduction becomes asymptotically smaller. When the data size is 10 5, the RMSE value decreased below a preset threshold (0.3), when we deemed a prediction to be accurate. In general, depending on specific models and the acceptable tolerance of errors, the threshold can be set differently, which could require different data sizes for effective training. This training dataset size is manageable and results in sufficient accuracy for our analysis. Based on error tolerance and numerical data generation efficiency, one can choose the desired dataset size for training the neural network. With an ensemble of deep neural networks, which will be described in the next section, the errors can be further reduced without increasing the dataset size.

---

### Modelling transmission and control of the COVID-19 pandemic in Australia [^449feb81]. Nature Communications (2020). High credibility.

Sensitivity analysis

We performed our sensitivity analysis using the local (point-based) sensitivity analysis (LSA), as well as global sensitivity analysis with the Morris method (the elementary effect method). Each method computes the response of an "output" variable of interest, for example, the generation period, to the change in an "input" parameter, for example, the fraction of symptomatic cases. The response F i, j of the state variable y j to parameter x i from a scaled vector of all k input parameters, X = [0, 1] k, is determined as a finite differencewhere Δ is a discretisation step, dividing each dimension of the parameter space. The distribution of each response F i, j is obtained by repeated random sampling with a number of simulation runs per step. In LSA, an input parameter is varied, while keeping other inputs set at their base points, that is, default values. In the Morris method, an input parameter is varied at a number of different points sampled across the domains of other parameters. The meanof the absolute response ∣ F i, j ∣ serves to capture the influence of the parameter x i on the output y j: a large mean suggests a higher sensitivity. The standard deviation σ i, j of the response F i, j is a complementary measure of sensitivity: a large deviation indicates that the dependency between the input and output is nonlinear. In the Morris method, a large deviation may also indicate that the input parameter interacts with other parameters. Importantly, the responses are not directly comparable across the output variables, and instead are ranked across the inputs for each output. A model is generally considered robust if most of the dependencies are characterised by low means and deviations, with the variations contained within acceptable ranges of the output variables. Appendix D in Supplementary information summarises the investigated ranges and results of the sensitivity analysis.

Intervention strategies

---

### Perioperative management of antiplatelet and anticoagulant therapy in patients undergoing interventional techniques: 2024 updated guidelines from the American Society of Interventional Pain Physicians (ASIPP) [^bc0e11e1]. Pain Physician (2024). High credibility.

Table 6 — pharmacokinetic and pharmacodynamic comparison of ADP-receptor inhibitors — reports the following values by agent: time to maximum effect is clopidogrel 4 hours to 4 days, prasugrel 1 hour, ticlopidine 3–5 days, ticagrelor 2.5 hours; bioavailability is clopidogrel > 50%, prasugrel ≥ 79%, ticlopidine > 80%, ticagrelor 36%; protein binding is clopidogrel 94–98%, prasugrel active metabolite ~98%, ticlopidine 98%, ticagrelor > 99.7%; plasma half-life is clopidogrel 7–8 hours (inactive metabolite), prasugrel ~7 hours (ranges from 2 hours to 15 hours), ticlopidine 12 hours for a single dose or 4–5 days for repeated dose, ticagrelor 7 hours for ticagrelor and 8.5 hours for active metabolite AR-C124910XX; time to 50% of platelet function recovery is clopidogrel 3 days, prasugrel 3 days, ticlopidine 6 days, ticagrelor 1.5 days.

---

### Abstract representations of events arise from mental errors in learning and memory [^748e9b84]. Nature Communications (2020). High credibility.

Estimating parameters and making quantitative predictions

Given an observed sequence of nodes x 1,…, x t −1, and given an inverse temperature β, our model predicts the anticipation, or expectation, of the subsequent node x t to be. In order to quantitatively describe the reactions of an individual subject, we must relate the expectations a (t) to predictions about a person's reaction timesand then calculate the model parameters that best fit the reactions of an individual subject. The simplest possible prediction is given by the linear relation, where the intercept r 0 represents a person's reaction time with zero anticipation and the slope r 1 quantifies the strength with which a person's reaction times depend on their internal expectations.

In total, our predictionscontain three parameters (β, r 0, and r 1), which must be estimated from the reaction time data for each subject. Before estimating these parameters, however, we first regress out the dependencies of each subject's reaction times on the button combinations, trial number, and recency using a mixed effects model of the form 'RT~log(Trial)*Stage+Target+Recency+(1+log(Trial)*Stage+Recency|ID)', where all variables were defined in the previous section. Then, to estimate the model parameters that best describe an individual's reactions, we minimize the RMS prediction error with respect to each subject's observed reaction times, where T is the number of trials. We note that, given a choice for the inverse temperature β, the linear parameters r 0 and r 1 can be calculated analytically using standard linear regression techniques. Thus, the problem of estimating the model parameters can be restated as a one-dimensional minimization problem; that is, minimizing RMSE with respect to the inverse temperature β. To find the global minimum, we began by calculating RMSE along 100 logarithmically spaced values for β between 10 −4 and 10. Then, starting at the minimum value of this search, we performed gradient descent until the gradient fell below an absolute value of 10 −6. For a derivation of the gradient of the RMSE with respect to the inverse temperature β, we point the reader to the Supplementary Discussion. Finally, in addition to the gradient descent procedure described above, for each subject we also manually checked the RMSE associated with the two limits β → 0 and β → ∞. The resulting model parameters are shown in Fig. 4 a, b for random walk sequences and Fig. 4 g, h for Hamiltonian walk sequences.

---

### Deterministic and electrically tunable bright single-photon source [^3a036cc4]. Nature Communications (2014). Medium credibility.

Brightness and extraction efficiency

Finally, we evaluate the brightness as well as the extraction efficiency of the device. For these measurements, the source emission is collected using a microscope objective with a 0.4 numerical aperture, sufficient to collect the mode full radiation pattern. Figure 4f shows the photon count rate for the X line I X measured on the detector as a function of excitation power, reaching values as high as 4.08 ± 0.002 MHz at saturation. The detection efficiency of the whole setuphas been measured to be = 0.0084 ± 0.0008 in the present experimental conditions, leading to a photon rate collected in the first lens of I X / = 0.48 ± 0.05 GHz. Under continuous wave excitation, the photon rate is inversely proportional to the radiative lifetime of the transition. As a result, the source brightness, defined as the number of photons collected per excitation cycle of the QD is given by, where the square root allows correcting from multiphoton emission. As shown on the right scale of Fig. 4f, a source brightness of 37 ± 7% collected photon per cycle is demonstrated for the X line.

The brightness of the source is the product of the extraction efficiency with the occupation factor p X of the QD X state. In the present measurement, the QD is found to be either in the neutral exciton X state or in the charged exciton state CX. To determine the respective occupation factors p X and p CX, we keep the voltage constant and increase the temperature from 35–42 K to bring the CX in resonance into the cavity mode. Given the small temperature change, we assume that capture processes are mostly unchanged. g (2) (0) measurements allow measuring a radiative decay time of τ CX = 0.88 ± 0.16 ns (see Fig. 4e). The intensity of the CX line as a function of power (Fig. 4f) shows brightness at saturation of 17 ± 6%. From this we deduce p X = 0.69 and p CX = 0.31 and estimate the extraction efficiency to be around 53 ± 9% in a 0.4 numerical aperture. Knowing the cavity characteristics, the expected device extraction efficiency is given by. With Q ≈ Q 0 and = 1.1, the expected extraction efficiency is around 52%, very close to the experimental one.

---

### Aqueous asymmetric pseudocapacitor featuring high areal energy and power using conjugated polyelectrolytes and tiCTMXene [^8514e253]. Nature Communications (2025). High credibility.

We further evaluated the 1:3 Ti 3 C 2 T x | ∅ |CPE-K devices with total mass loadings of 3.8 and 15.1 mg cm –2. The CV profile of the Ti 3 C 2 T x | ∅ |CPE-K device exhibits a pseudocapacitive signature, characterized by a broad peak centered at approximately 0.8 V (Fig. 3b). This feature is attributed to the superimposition of the individual voltammograms originating from CPE-K and Ti 3 C 2 T x. (Fig. 3d). When the scan rate is increased, the device can maintain its pseudocapacitive signature up to scan rates of up to 10,000 mV s –1 (Fig. 3d). In fact, the log-log plot of the current versus scan rate showed a b-value of 0.92 and 0.80 for 3.8 and 15.1 mg cm –2, respectively (Fig. 3e). Such rate performance reflects the capacitive and kinetic matching of Ti 3 C 2 T x and CPE-K. Figure 3f shows the GCD curves of the Ti 3 C 2 T x || CPE-K device, which exhibits non-linear behavior at various specific currents, highlighting the pseudocapacitive characteristics resulting from the combination of both Ti 3 C 2 T x and CPE-K. The theoretical specific capacitance of the device can be calculated using Eq. (4):where C s is the specific capacitance of the electrode (82 F g –1 for CPE-K, 235 F g −1 for Ti 3 C 2 T x), m is the mass of an electrode, and m + / m – is the CPE-K:Ti 3 C 2 T x ratio of 3.0. According to the GCD data, the maximum C s of the cell is 32 F g –1, which is close to the theoretical value of 31 F g –1 calculated from Eq. (4).

---

### Standardization of spirometry 2019 update. An official American Thoracic Society and European Respiratory Society technical statement [^f9491ce0]. American Journal of Respiratory and Critical Care Medicine (2019). High credibility.

Spirometry grading for FEV1 and FVC — repeatability thresholds and measurement counts are defined as follows: Grade A requires ≥ 3 acceptable with repeatability "Within 0.150 L" for Age > 6 yr or "Within 0.100 L" for Age ≤ 6 yr*; Grade B requires 2 acceptable with the same thresholds; Grade C requires ≥ 2 acceptable "Within 0.200 L" for Age > 6 yr or "Within 0.150 L" for Age ≤ 6 yr*; Grade D requires ≥ 2 acceptable "Within 0.250 L" for Age > 6 yr or "Within 0.200 L" for Age ≤ 6 yr*; Grade E is " ≥ 2 acceptable OR 1 acceptable" with " > 0.250 L" for Age > 6 yr or " > 0.200 L" for Age ≤ 6 yr*; Grade U is "0 acceptable AND ≥ 1 usable"; and Grade F is "0 acceptable and 0 usable." Repeatability grading is determined separately for pre- and post-bronchodilator maneuver sets, applied to the differences between the two largest FVC and the two largest FEV1 values, and FEV1 and FVC are graded separately; for those age 6 years or younger, limits are "Or 10% of the highest value, whichever is greater."

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^b1513dd5]. CDC (2011). Medium credibility.

Table 1b — Minimum sample size for detecting geometric mean (GM) ratio k with α = 0.05 and W = 0.9 — presents required sample sizes by standard deviation (S) strata and defines parameters; α is significance level, W is power, GM is geometric mean, and S is standard deviation, noting that 1.2 is the standard deviation of national viral load (VL) data. Each jurisdiction will need to assess the standard deviation of their local VL data and then determine the appropriate sample size needed to assess VL, and if the sample size is inadequate to meet the recommended case inclusion criterion, an alternate method may need to be used, such as combining multiple years of data. For k = 3, sample sizes across S = 1, 1.1, 1.2, 1.3, 1.4, 1.5 are 75, 91, 108, 127, 147, 169, and Table 1b uses power = 90%; jurisdictions may also explore differences in means of viral loads, including categorical differences in the proportion with undetectable or very low VL.

---

### 2019 ASCCP risk-based management consensus guidelines: methods for risk estimation, recommended management, and validation [^c44c6ea6]. Journal of Lower Genital Tract Disease (2020). High credibility.

Risk-based management uncertainty — confidence interval and decision confidence score: Confidence intervals for each risk estimate were calculated using a normal approximation or exact methods based on the binomial distribution, and a "decision confidence score" is defined that combines the uncertainty in the statistical precision and how close the risk estimates fall to the clinical action thresholds; mathematical details on estimating the recommendation confidence scores are presented in Appendix Part D (http://links.lww.com/LGT/A160).

---

### A user guide to the American Society of Hematology clinical practice guidelines [^51218549]. Blood Advances (2020). High credibility.

Isof table — components and use are illustrated as an example of an iSoF table can be used to explore an intervention's effect on health outcomes in alternative ways, and it includes the question, outcomes, relative and absolute effects, and interpretations.

---

### An L0-norm-based method for passive shimming design in MRI: a simulation-based study [^551744d0]. NMR in Biomedicine (2025). Medium credibility.

2.2 Linear Programming–Based Solution Approach

The above optimization problem can be solved using the linear programming (LP) method. LP is a technique used to optimize a linear objective function while subject to a set of linear constraints. In the case where the initial solution X 0 is zero, the corresponding optimization problem becomes linear and can be readily expressed as the following LP model:

Subject towhere the objective function is a standard L1‐norm problem and t max represents the maximum thickness allowed for the iron pieces to be placed in the shim pocket.

When the initial solution X 0 is non‐zero, indicating that part of the shim pockets is not empty, the corresponding optimization problem becomes a nonlinear function due to both positive and negative components appearing in the solution vector X. Thus, the objective function becomes a minimization of the absolute value problem. To handle this using the LP model, the non‐linear problem needs to be converted into a linear model, as illustrated below:

Subject towhere V = (v 1, v 2,…, v N) is a vector, whose components are all positive and within the range of 0 to t max, and the constraints above ensure that each component of the solution vector X does not exceed the corresponding components of V.

Once the optimization problem is formulated as a linear model, efficient algorithms like the simplex method can be applied to find the solution. The LP‐based solution is generally effective in generating feasible passive shim patterns. However, as shown in Equations (2) and (3), the mathematical model focuses on the total iron consumption (thickness) and does not directly consider the number of shim pockets involved in shimming. Consequently, the LP approach may produce a solution that requires accessing too many shim pockets. Since the time required for PS in each iteration is directly related to the time taken for each pocket adjustment, the LP‐based solution's operational efficiency is thus not optimized. It is then desirable to develop an optimization model that can minimize the number of pocket adjustments, therefore enhancing shimming efficiency.

---

### Active site localization of methane oxidation on Pt nanocrystals [^574146dd]. Nature Communications (2018). Medium credibility.

Fig. 1
Schematic of in situ Bragg coherent X-ray diffraction imaging (BCDI). The BCDI measurement scheme during methane catalytic oxidation illustrates the acquisition of (111) Bragg coherent diffraction patterns from the same Pt nanocrystal throughout. Slices through the Bragg coherent diffraction patterns are measured for Pt nanocrystals above the catalytic activation temperature in the presence of different gases. a After 36 min in H 2, b 2.6 min, and c 36 min after the O 2 insertion, d ~2.6 min, e 16 min, and f 36 min after CH 4 was introduced. Under a 20% O 2 gas flow, the diffraction pattern becomes distorted (b and c), and continue to evolve when CH 4 is added. The distortion fades in f owing to completion of the methane catalytic oxidation process in 1% CH 4

To understand how distortion evolved during the catalytic process, the Pearson correlation functionwas applied to the total three-dimensional (3D) CXD patterns. The Pearson correlation coefficient for CXD patterns iswhere A i and B i are the scattering intensity values for a particular pixel in the pattern, andthe mean values of the scattering intensities, and σ A and σ B the standard deviations, for the two CXD patterns, A and B. The sum is evaluated over 3D CXD patterns and ρ (A, B) is computed for the selected area around the first fringe where shows maximum distortion. (See Supplementary Fig. 3.) Fig. 2a, b show contour slices through the CXD patterns (Fig. 1a, b, respectively) with the red contour identifying the first fringes. The cross-correlation map in Fig. 2c was computed for the CXD patterns measured in various gas environments by using Eq. 1, and detailed procedure is described in Methods. Gas environments together with the exposure times are indicated on the x and y axis, and the experimental conditions for Fig. 1a–f are also indicated on the map with black arrows. Time 0 indicates the start of each gas flow. The correlation coefficient, 1 means total positive linear correlation, 0 means no linear correlation, and −1 means total negative linear correlation. Immediate distortion appeared after oxygen gas was introduced. The original diffraction pattern was recovered after ~22 min in CH 4. This reversion can be attributed to the available oxygen being consumed by catalytic oxidation. Gas analyses were carried out to check the catalytic reaction and resultant products (CO 2 and H 2 O).

---

### Estimation in medical imaging without a gold standard [^82dda7fe]. Academic Radiology (2002). Low credibility.

Rationale and Objectives

In medical imaging, physicians often estimate a parameter of interest (eg, cardiac ejection fraction) for a patient to assist in establishing a diagnosis. Many different estimation methods may exist, but rarely can one be considered a gold standard. Therefore, evaluation and comparison of different estimation methods are difficult. The purpose of this study was to examine a method of evaluating different estimation methods without use of a gold standard.

Materials and Methods

This method is equivalent to fitting regression lines without the x axis. To use this method, multiple estimates of the clinical parameter of interest for each patient of a given population were needed. The authors assumed the statistical distribution for the true values of the clinical parameter of interest was a member of a given family of parameterized distributions. Furthermore, they assumed a statistical model relating the clinical parameter to the estimates of its value. Using these assumptions and observed data, they estimated the model parameters and the parameters characterizing the distribution of the clinical parameter.

Results

The authors applied the method to simulated cardiac ejection fraction data with varying numbers of patients, numbers of modalities, and levels of noise. They also tested the method on both linear and nonlinear models and characterized the performance of this method compared to that of conventional regression analysis by using x-axis information. Results indicate that the method follows trends similar to that of conventional regression analysis as patients and noise vary, although conventional regression analysis outperforms the method presented because it uses the gold standard which the authors assume is unavailable.

Conclusion

The method accurately estimates model parameters. These estimates can be used to rank the systems for a given estimation task.

---

### Single photon emission computed tomography (SPECT) myocardial perfusion imaging guidelines: instrumentation, acquisition, processing, and interpretation [^097e8756]. Journal of Nuclear Cardiology (2018). Medium credibility.

Table 2 — typical performance parameters for low-energy (< 150 keV) parallel-hole collimators — reports resolution (full width at half maximum [FWHM]) at 10 cm, efficiency (%), and relative efficiency: Ultra-high resolution: 6.0 mm, 0.005, 0.3; High resolution: 7.8 mm, 0.010, 0.6; All/general purpose: 9.3 mm, 0.017, 1.0*; High sensitivity: 13.2 mm, 0.035, 2.1. A footnote specifies that a relative efficiency of 1 corresponds approximately to a collimator efficiency of 0.017%, and defines efficiency as the fraction of gamma rays and X-rays passing through the collimator per gamma ray and X-ray emitted by the source.

---

### Matrix factorization from non-linear projections: application in estimating T2 maps from few echoes [^91a2e8bd]. Magnetic Resonance Imaging (2015). Low credibility.

This work addresses the problem of estimating T2 maps from very few (two) echoes. Existing multi-parametric non-linear curve fitting techniques require a large number (16 or 32) of echoes to estimate T2 values. We show that our method yields very accurate and robust results from only two echoes, where as the curve-fitting techniques require about 16 echoes to achieve the same level of accuracy. We model T2 maps as a rank-deficient matrix. Since the relationship between T2 values and intensity values/K-space samples is not linear, estimating the T2 values requires recovering a low-rank matrix from non-linear projections. We solve this as a non-linear matrix factorization problem. Since the said problem has not been solved before, we propose a simple algorithm for the same.

---

### Experimental noise cutoff boosts inferability of transcriptional networks in large-scale gene-deletion studies [^061df17c]. Nature Communications (2018). Medium credibility.

To further improve the predictive power of our method we included the prior knowledge that transcriptional networks are highly sparse. Sparsity constraints are typically realised by penalising either the existence of links or the link strengths by adding appropriate cost functions, such as L 1 -norm regularised regression (Lasso). Adding a cost function to the main objective comes with the problem to trade-off the log-likelihood against the number of links in the network whose strength is allowed to be non-zero. In the absence of experimentally verified interactions there is no obvious way how to determine a suitable regularisation parameter that weights the likelihood against the cost function, which is one of the great weaknesses of such methods.

In our approach we reduce network complexity by assuming that functionally relevant information in molecular networks can only pass through nodes whose response to perturbations is significantly above the base line that is given by the variability among biological replicates. The individual noise levels can be estimated from natural variations among wild-type experimental replicates (Fig. 3a). The significance level that removes nodes from the network with low signal-to-noise ratio can be set to a desired false discovery rate. It can be shown that removal of noisy nodes imposes a sparsity constraint on the inference problem (Online Methods). The different steps required to arrive at a list of significant links are illustrated in Fig. 4a. In the first step, genes are grouped in clusters that are co-expressed under all perturbations. These clusters are treated as single network nodes in the subsequent steps. In the second step, only those samples are extracted from the data set that correspond to a perturbation of a chosen gene — the source node — with no other genes perturbed (node 5 in Fig. 4a). From this reduced data set, we identify all nodes in the network that change expression above a given significance level upon perturbing the source node. These significantly responding nodes define a subnetwork for each source node, which is typically much smaller in size than the complete network. In the third step, we collect all perturbation data from the complete data set for all nodes that are part of the subnetwork. Before inferring a direct interaction that points from the source node to a given target node in the subnetwork (green arrows in Fig. 4a), we remove all experiments from the data set where the target node is perturbed. The second and third steps essentially realise the counting procedure of inferable links as illustrated in Fig. 2a, with the difference that significant links are identified by PRCs in combination with residual bootstrapping over replicates (Online Methods, Supplementary Note 3). In the fourth step, we collect all clusters of co-expressed genes that contain exactly two nodes, with one of the nodes perturbed and check statistical significance of the directed link between them. In the fifth step, all significant links are collected in an edge list. We refer to these five steps as the clustering method. If we remove all links from the edge list that have more than one node in a source cluster or more than one node in a target cluster, we obtain an edge list that corresponds to links between single genes. This reduced edge list would also arise by skipping the clustering step and we refer to the remaining inference steps that compute links between single genes as subnetwork method.

---

### Comparison of parameter optimization methods for quantitative susceptibility mapping [^26a0417b]. Magnetic Resonance in Medicine (2021). Medium credibility.

Purpose

Quantitative Susceptibility Mapping (QSM) is usually performed by minimizing a functional with data fidelity and regularization terms. A weighting parameter controls the balance between these terms. There is a need for techniques to find the proper balance that avoids artifact propagation and loss of details. Finding the point of maximum curvature in the L-curve is a popular choice, although it is slow, often unreliable when using variational penalties, and has a tendency to yield overregularized results.

Methods

We propose 2 alternative approaches to control the balance between the data fidelity and regularization terms: 1) searching for an inflection point in the log-log domain of the L-curve, and 2) comparing frequency components of QSM reconstructions. We compare these methods against the conventional L-curve and U-curve approaches.

Results

Our methods achieve predicted parameters that are better correlated with RMS error, high-frequency error norm, and structural similarity metric-based parameter optimizations than those obtained with traditional methods. The inflection point yields less overregularization and lower errors than traditional alternatives. The frequency analysis yields more visually appealing results, although with larger RMS error.

Conclusion

Our methods provide a robust parameter optimization framework for variational penalties in QSM reconstruction. The L-curve-based zero-curvature search produced almost optimal results for typical QSM acquisition settings. The frequency analysis method may use a 1.5 to 2.0 correction factor to apply it as a stand-alone method for a wider range of signal-to-noise-ratio settings. This approach may also benefit from fast search algorithms such as the binary search to speed up the process.

---

### An official American Thoracic Society workshop report: developing performance measures from clinical practice guidelines [^7deadca8]. Annals of the American Thoracic Society (2014). Medium credibility.

GRADE recommendation strength — "Strong recommendations can be made when there is moderate- or high-quality evidence" that "the net benefits of the intervention clearly outweigh the net harms," and "can also be made even if the level of evidence is poor but values and preferences strongly support the action." Strong recommendations mean "there is certainty that the desirable consequences of the intervention exceed the undesirable consequences," that further research is unlikely to change certainty, and that "nearly all patients would desire the intervention when given the choice." In contrast, "weak recommendations are common, reflecting residual uncertainty about the quality of evidence, the ratio of benefits to harms, and/or variation in patient preferences," yet they remain important because "they explicitly acknowledge uncertainty while still providing evidence-based guidance to clinical practice," and help identify gaps and justify some practice variation.

---

### Mapping the absolute value of M0 using dipolar field effects [^8d976733]. Magnetic Resonance in Medicine (2002). Low credibility.

The ability to map the spatial variation of the absolute, rather than the relative value of the equilibrium magnetization could be advantageous in many areas of NMR. However, direct measurement of M(0) is usually difficult because of the multiparametric dependence of the NMR signal. Here we propose a technique for mapping the spatial variation of the absolute value of M(0), independent of relaxation weighting and flip angle calibration. This method, which works best at high field strengths, is based on the effect of the dipolar field due to the nuclear magnetization that is normally neglected in liquid-state NMR. The experimental implementation of this sequence at 3.0 T is described, and its initial application to the measurement of the water content of brain tissue is outlined.

---