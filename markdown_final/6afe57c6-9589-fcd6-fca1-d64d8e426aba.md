# Medical Question & Answer

**Sample ID**: 6afe57c6-9589-fcd6-fca1-d64d8e426aba
**Dataset Index**: 312212

---

## Question

Let's determine the value of the sum ∑ n = 1 to ∞ arcctg(2n²).

---

## Answer

> Let's see… What do we have here? The user is asking for the value of the infinite sum (∑ n = 1 to ∞ arcctg(2n²)). Let's break this down step-by-step. First, I need to think about whether there's a telescoping identity for arcctg that could simplify the partial sums. Then, I should verify the identity algebraically and check the limit behavior as n grows. Next, I will compute the telescoping partial sum explicitly and take the limit. Finally, I will double-check the result with a small numerical test to ensure consistency.

> Let me first confirm the telescoping candidate. The identity that often helps with arcctg sums is (arcctg(x) − arcctg(y) = arcctg([(xy + 1)/(y − x)])) when (y > x), and its inverse form (arcctg(a) − arcctg(b) = arcctg([(ab + 1)/(b − a)])) when (b > a). I need to check whether (2n²) can be written as ([(ab + 1)/(b − a)]) for some simple sequences (aₙ) and (bₙ) that telescope.

> Wait, let me verify the algebraic match. If I set (aₙ = 2n − 1) and (bₙ = 2n + 1), then ([(aₙ bₙ + 1)/(bₙ − aₙ)] = [((2n − 1)(2n + 1) + 1)/((2n + 1) − (2n − 1))] = [4n² − 1 + 1]/2 = [4n²/2] = 2n²), which is exactly the argument inside arcctg. So, I can write (arcctg(2n²) = arcctg(2n − 1) − arcctg(2n + 1)), provided (2n + 1 > 2n − 1), which is always true for (n ≥ 1) [^notfound].

> Hold on, let's not jump to conclusions. I should confirm the direction of subtraction. The identity (arcctg(x) − arcctg(y) = arcctg([(xy + 1)/(y − x)])) requires (y > x), so here (2n + 1 > 2n - 1) is satisfied, and the subtraction is correctly ordered as (arcctg(2n − 1) − arcctg(2n + 1)).

> Next, I should review the partial sum. Let (S_N = ∑ n = 1 to N arcctg(2n²) = ∑ n = 1 to N [arcctg(2n − 1) − arcctg(2n + 1)]). This telescopes to (S_N = arcctg(1) − arcctg(3) + arcctg(3) − arcctg(5) +… + arcctg(2N − 1) − arcctg(2N + 1)), so all intermediate terms cancel and (S_N = arcctg(1) − arcctg(2N + 1)).

> I will now examine the limit as (N → ∞). Since (arcctg(x) → 0) as (x → ∞), we have (lim N → ∞ arcctg(2N + 1) = 0), and (arcctg(1) = [π/4]). Therefore, the infinite sum converges to ([π/4] − 0 = [π/4]).

> But wait, what if I made a sign error in the identity application? Let me double-check with a small numerical test. For (N = 1), the partial sum is (arcctg(2) ≈ 0.4636), and the telescoping formula gives (arcctg(1) − arcctg(3) ≈ 0.7854−0.3218 = 0.4636), which matches. For (N = 2), the direct sum is (arcctg(2) + arcctg(8) ≈ 0.4636 + 0.1244 = 0.5880), and the telescoping result is (arcctg(1) − arcctg(5) ≈ 0.7854−0.1974 = 0.5880), again matching. This consistency gives me confidence the algebra and limit are correct.

> Final answer: The sum evaluates to [π/4].

---

The sum (∑ n = 1 to ∞ arcctg(2n²)) equals ([π/4]). This result is obtained by using the identity (arcctg(2n²) = arcctg(2n + 1) − arcctg(2n − 1)), which transforms the series into a telescoping sum. The partial sum collapses to (arcctg(2N + 1) − arcctg(1)), and taking the limit as (N → ∞) yields ([π/2] − [π/4] = [π/4]).

---

## Step-by-step solution

### Step 1: Identify a telescoping pattern

We need to express (arcctg(2n²)) as a difference of two arccotangent terms. Consider the identity:

arcctg(a) − arcctg(b) = arcctg([(ab + 1)/(b − a)])

We want to find (a) and (b) such that:

[(ab + 1)/(b − a)] = 2n²

Let (a = 2n - 1) and (b = 2n + 1). Then:

[((2n − 1)(2n + 1) + 1)/((2n + 1) − (2n − 1))] = [4n² − 1 + 1]/2 = [4n²/2] = 2n²

Thus, we have:

arcctg(2n²) = arcctg(2n − 1) − arcctg(2n + 1)

---

### Step 2: Write the series as a telescoping sum

Using the identity, the series becomes:

∑ n = 1 to ∞ arcctg(2n²) = ∑ n = 1 to ∞ [arcctg(2n − 1) − arcctg(2n + 1)]

This is a telescoping series, where most terms cancel out.

---

### Step 3: Compute the partial sum

The partial sum (S_N) is:

S_N = ∑ n = 1 to N [arcctg(2n − 1) − arcctg(2n + 1)]

Expanding this, we see the cancellation:

S_N = [arcctg(1) − arcctg(3)] + [arcctg(3) − arcctg(5)] +… + [arcctg(2N − 1) − arcctg(2N + 1)]

All intermediate terms cancel, leaving:

S_N = arcctg(1) − arcctg(2N + 1)

---

### Step 4: Take the limit as (N → ∞)

We know that (arcctg(1) = [π/4]) and that (arcctg(x) → 0) as (x → ∞). Therefore:

lim N → ∞ S_N = [π/4] − 0 = [π/4]

---

### Final answer

∑ n = 1 to ∞ arcctg(2n²) = [π/4]

---

## References

### Structure and inference in annotated networks [^535f823a]. Nature Communications (2016). Medium credibility.

Computationally, the most demanding part of the EM algorithm is calculating the sum in the denominator of equation (7), which has an exponentially large number of terms, making its direct evaluation intractable on all but the smallest of networks. Traditionally one gets around this problem by approximating the full distribution q (s) by Monte Carlo importance sampling. In our calculations, however, we instead use a recently proposed alternative method based on belief propagation, which is significantly faster, and fast enough in practice for applications to very large networks.

Final likelihood value

The EM algorithm always converges to a maximum of the likelihood but is not guaranteed to converge to the global maximum — it is possible for there to be one or more local maxima as well. To get around this problem we normally run the algorithm repeatedly with different random initial guesses for the parameters and from the results choose the one that finds the highest likelihood value. In the calculations presented in this paper we did at least 10 such 'random restarts' for each network. To determine which run has the highest final value of the likelihood we calculate the log-likelihood from the right-hand side of (6) using P (A | Θ, s) and P (s | Γ, x) as in equation (2), the final fitted values of the parameters Θ and Γ from the EM algorithm, and q (s) as in equation (7). (As we have said, the right-hand side of (6) becomes equal to the left, and hence equal to the true log-likelihood, when q (s) is given the value in equation (7).)

---

### Fluctuation-learning relationship in recurrent neural networks [^15c311ca]. Nature Communications (2025). High credibility.

Derivations of FLR2 in the asymmetric matrix

In the main text, the relationship between the learning speed and fluctuations before learning is derived under the assumption of the symmetric matrix. Here, we derive FLR2 in an asymmetric random matrix (see "Neural network models" for details of the network), in which the left and right eigenvectors are neither identical nor real. In a similar manner to the symmetric case, we begin byThen, we will show that C o v (x) ξ = V a r ξ (x) ξ + O (1/ N), resulting in FLR2 (Eq. (11)).

Similar to the symmetric case, we define X as a matrix that is composed of the normalized eigenvectors of J, where J has s pairs of the complex right eigenvectors and N − 2 s real eigenvectors.is a complex conjugate of p. For the N dimensional normalized eigenvectors p i of the random matrix J, the elements of p i is also random, and their variance should be O (1/ N) because the square sum of their elements equals unity. Therefore. Thus, although X is not generally a unitary matrix, X satisfiesBy using X, we decompose a vector x as, whereis a coefficient vector. Considering that the eigenvectors of the random matrix X are almost uniformly distributed on the surface of the complex unit sphere S 2 N −1 and the square amplitude of the real part of the eigenvector, ∣ R e (p i)∣ 2 is evaluated as 1/2 for the larger N.

---

### A simple, assumption-free, and clinically interpretable approach for analysis of modified rankin outcomes [^23f78b5c]. Stroke (2012). Low credibility.

Background and Purpose

There is debate regarding the approach for analysis of modified Rankin scale scores, the most common functional outcome scale used in acute stroke trials.

Methods

We propose to use tests to assess treatment differences addressing the metric, "if a patient is chosen at random from each treatment group and if they have different outcomes, what is the chance the patient who received the investigational treatment will have a better outcome than will the patient receiving the standard treatment?" This approach has an associated statement of treatment efficacy easily understood by patients and clinicians, and leads to statistical testing of treatment differences by tests closely related to the Mann-Whitney U test (Wilcoxon Rank-Sum test), which can be tested precisely by permutation tests (randomization tests).

Results

We show that a permutation test is as powerful as are other approaches assessing ordinal outcomes of the modified Rankin scores, and we provide data from several examples contrasting alternative approaches.

Discussion

Whereas many approaches to analysis of modified Rankin scores outcomes have generally similar statistical performance, this proposed approach: captures information from the ordinal scale, provides a powerful clinical interpretation understood by both patients and clinicians, has power at least equivalent to other ordinal approaches, avoids assumptions in the parameterization, and provides an interpretable parameter based on the same foundation as the calculation of the probability value.

---

### Validation of monte carlo estimates of three-class ideal observer operating points for normal data [^26b6e5d9]. Academic Radiology (2013). Low credibility.

Rationale and Objectives

Traditional two-class receiver operating characteristic (ROC) analysis is inadequate for the complete evaluation of observer performance in tasks with more than two classes.

Materials and Methods

Here, a Monte Carlo estimation method for operating point coordinates on a three-class ROC surface is developed and compared with analytically calculated coordinates in two special cases: (1) univariate and (2) restricted bivariate trinormal underlying data.

Results

In both cases, the statistical estimates were found to be good in the sense that the analytical values lay within the 95% confidence interval of the estimated values about 95% of the time.

Conclusions

The statistical estimation method should be key in the development of a pragmatic performance metric for evaluation of observers in classification tasks with three or more classes.

---

### How to evaluate agreement between quantitative measurements [^7316afc8]. Radiotherapy and Oncology (2019). Medium credibility.

When a method comparison study is performed, the aim is to evaluate the agreement of measurements of different methods. We present the Bland-Altman plot with limits of agreement as the correct analysis methodology. We also discuss other scaled and unscaled indices of agreement and commonly used inappropriate approaches.

---

### A brain-wide map of neural activity during complex behaviour [^1caac83e]. Nature (2025). Excellent credibility.

Single-cell correlates of sensory, cognitive and motor variables

We quantified the sensitivity of single neurons to three task variables: visual stimulus (left versus right location of the visual stimulus); choice (left versus right direction of wheel turning); and feedback (reward versus non-reward). We computed the sensitivity metric for each task variable using the condition combined Mann–Whitney U- statistic, (Supplementary Fig. 2a, c). Specifically, we compared the firing rates from those trials with one task-variable value V 1 (for example, trials with the stimulus on the left side) to those with the other value V 2 (for example, with the stimulus on the right side) while holding the values of all other task variables fixed. In this way, we could isolate the influence of individual task variables on neural activity. To compute the U -statistic, we first assigned numeric ranks to the firing rate observations in each trial. We then computed the sum of ranks R 1 and R 2 for the observations coming from n 1 and n 2 trials associated with the task-variable values V 1 and V 2, respectively. The U- statistic is defined as:The probability that the firing rate on V 1 trials is different (greater or smaller) from the firing rate on V 2 trials is computed as 1 − P, where P is given bywhich is equivalent to the area under the receiver operating characteristic curve. The null hypothesis is that the distributions of firing rates on V 1 and V 2 trials are identical.

To obtain a single probability across conditions, we combined observations across different trial conditions j by a sum of U -statistic in these conditions:Here n 1, j and n 2, j are the numbers of V 1 and V 2 trials, respectively, in the condition j.

For the visual stimulus, we compared firing rate in trials with the stimulus on the left versus stimulus on the right during the time window 0–100 ms aligned to the stimulus-onset time. For choice, we compared firing rates in trials with the left versus right choice during the time window −100 to 0 ms aligned to the first wheel-movement time. For the feedback, we compared firing rate in trials with reward versus non-reward during the time window of 0–200 ms aligned to the feedback-onset time.

---

### Statistical methods for best and worst eye measurements [^530ffb40]. Clinical Ophthalmology (2024). Medium credibility.

Materials and Methods

Suppose that the left and right-eye acuity scores of n subjects, are random samples from a bivariate normal distribution with means, standard deviationsand correlation coefficient. Ascan beor, their density function is an equal mixture of bivariate normal densities:

If the means and/or standard deviations are unequal, notice thatremains unaltered forand, the two distinct parameters. For this loss of identifiability,(a) we cannot estimate the parameters of the bivariate normal model, (b) we cannot test for equality of means against one-sided alternatives – only two-sided alternatives can be tested, and (c) the classical likelihood ratio test for means is ruled out. This calls for alternative tests for means/locations, which can be broadly classified based on their assumptions, most of which assume that thepairs are uncorrelated. Next, we first describe two such tests for equality of means and locations.

When Left and Right Eye Scores are Uncorrelated

Moore's Exact F-Test

F -tests compare two independent variance estimates by examining their ratios. Presently, one of these estimates, which appears in the denominator of (1), is the sample variance based on the pairwise sum. In the numerator, there is an average of the squared and unsigned paired differences. Under normality, and assuming, these two are independent and competing estimates of, provided. The estimate in the numerator is also unbiased ifholds true. If the null hypothesis is not true, the numerator overestimates. This leads to the following F testrejecting the null if:

whereis the uppercut-off point. This test has some desirable properties: (a) it is the only exact test available to date, perfectly controlling the risk of false discovery, and (b) it can be computed using pocket calculator/Excel. See the key points below for further details.

Key Points for Equation (1)

The numerator is an average of the squared and unsigned paired differences.
The denominator is the sample variance based on the pairwise sum.
If, on an average, the best and worst eye scores are equal, so the null hypothesis holds true, the ratio will follow F distribution as indicated above; provided the eye scores are not correlated.
The test can be carried out in Excel, comparing the observed value of the ratio, with the appropriate cut-off points from F distribution.

---

### Statistical methods for best and worst eye measurements [^1b236079]. Clinical Ophthalmology (2024). Medium credibility.

When Left and Right Eye Scores are Correlated

Fellow eye scores are typically positively correlated. Even if the correlation is marginally positive, F and Z tests in (1) and (2) are too conservative, often the mean/location difference cannot be detected even if it exists. If the correlation is marginally negative, they are too liberal: often, falsely detect significant differences. Most of the available tests in the literature dealing with unordered pairs are practically useless owing to their assumptionsand. The following battery of Z testsseems to be the only option which works reasonably well when these assumptions are violated.

Test for Equality of Variance

When the left and right-eye scores vary similarly, the regression line of the unsigned paired differenceson the pairwise sum S is constant, and the line is horizontal to x-axis. If, the regression line is non-constant and not independent of S. This can be verified by the scatter plot ofagainst S. Formally, one can apply a likelihood-ratio testin a roundabout manner, although for ease of application, we will go through the scatter plot approach here.

---

### Capacity estimation and verification of quantum channels with arbitrarily correlated errors [^53f0c5a8]. Nature Communications (2018). Medium credibility.

Discussion

In this section, we shall discuss our bound as a bound on the rate, which quantifies the amount of quantum information that can be sent per qubit. This has the advantage that it makes comparisons easier. To discuss our bound on the capacity rate, we have plotted its value as a function of N in Fig. 5. We plotted the bound for the estimation protocol, but qualitatively, the bound for the verification protocol behaves identically, so our discussion applies to both protocols.

Fig. 5
Bound on the rate for the capacity estimation protocol as a function of the number of qubits. This figure shows the bound on the one-shot quantum capacity for the estimation protocol expressed as a rate, that is, the right hand side of inequalitydivided by the number of qubits N. The plots show the bound as a function of N with the parameters as q = 1, and p = 1/2. a We plotted the bound for fixed error rates e x = e z = 5% for a few different values of ε in order to visualize the dependence on the decoding error probability. The lower the allowed decoding error probability ε is set, the higher the number of qubits needs to be in order to get a positive bound on the rate (note that the N -axis is logarithmic). In the asymptotic limit N→ ∞, the bound converges to. If q = 1, this coincides exactly with the (asymptotic) capacity for some important classes of channels, such as depolarizing channels. This shows that our bound is asymptotically optimal, and therefore, improvements are only possible in the finite-size correction terms. b To see the dependence on the error rates, we plotted our bound for a fixed value of ε = 10 −6 for a few different values of e x and e z. The higher the error rate, the higher the number of qubits needs to be in order to achieve a positive rate. For every pair of error rates e x and e z, the bound is monotonically increasing in N and converges to. Therefore, the bound can only be positive whenis positive, which yields an easy criterion for the potential usefulness of a channel with known error rates (although the full version of the bound with the correction terms is not hard to evaluate either)

---

### Machine learning in spectral domain [^b6a6f35e]. Nature Communications (2021). High credibility.

In Fig. 6, we report the results of the tests performed when operating under the deep linear configuration. Symbols are analogous to those employed in Fig. 5. In all inspected cases, the entry layer is made of N 1 = 784 elements and the output one has N ℓ = 10 nodes. The first five points, from left to right, refer to a three layers (linear) neural network. Hence, ℓ = 3 and the size of the intermediate layer is progressively increased, N 2 = 20, 80, 100, 500, 800. The total number of trained eigenvalues is N 2 + N 3, and gets therefore larger as the size of the intermediate layer grows. The successive four points of the collections are obtained by setting ℓ = 4. Here, N 2 = 800 while N 3 is varied (= 100, 200, 400, 600). The training impacts on N 2 + N 3 + N 4 parameters. Finally, the last point in each displayed curve is obtained by working with a five layers deep neural network, ℓ = 5. In particular, N 2 = 800, N 3 = 600 and N 4 = 500, for a total of N 2 + N 3 + N 4 + N 5 tunable parameters. Also in this case, the spectral algorithm performs better than conventional learning schemes constrained to operate with an identical number of free parameters. Similarly, the distribution of the weights of an equivalent perceptron trained in reciprocal space matches that obtained when operating in the space of the nodes and resting on a considerably larger number of training parameters. To sum up, eigenvalues are parameters of key importance for neural networks training, way more strategic than any other set of equivalent cardinality in the space of the nodes. As such, they allow for a global approach to the learning, with significant reflexes of fundamental and applied interest. In all cases here considered, the learning can extend to the eigenvectors: an optimised indentation of the eigen-directions contribute to enhance the overall performance of the trained device.

---

### Recommendations for noninvasive evaluation of native valvular regurgitation: a report from the American Society of Echocardiography developed in collaboration with the Society for Cardiovascular Magnetic Resonance [^f59ef754]. Journal of the American Society of Echocardiography (2017). Medium credibility.

Left ventricular volume calculation by short-axis summation — example and method show that sequential short-axis slices from the mitral annular plane (base) to the apex are summed using volume = area × thickness to compute LV end-diastolic volume (LVEDV) and LV end-systolic volume (LVESV); in the example, LV EDV = 41 mL + 41 mL + 38 mL + 34 mL + 28 mL + 19 mL + 5 mL = 206 mL and LV ESV = 0 mL + 30 mL + 27 mL + 24 mL + 18 mL + 3 mL + 0 mL = 102 ml; this methodology requires no geometric assumptions.

---

### Deciding whether a change in a patient's performance is a genuine change [^7c585da0]. Clinical Rehabilitation (2002). Low credibility.

Our purpose is to help clinicians decide whether a measured change in a patient's performance, usually in the direction of improvement, is likely to represent a true change. We set out the calculations to derive a minimal statistically significant change in certain measures commonly used in evaluating the outcome of rehabilitation. Many of these test measurements, e.g. walking time, are likely to show increased variability within patients as the mean level of performance increases in magnitude. We pay particular attention to the calculations in this case as the data should be transformed to remove the relationship by taking their natural logarithm. Confidence interval calculations for single measurements and for differences between two measurements (using the repeatability coefficient) are given both for data that show and for data that do not show a relation between variation and level of performance.

---

### Accurate detection of identity-by-descent segments in human ancient DNA [^d2b92510]. Nature Genetics (2024). High credibility.

Implementation and runtime

We implemented several computational speed-ups to improve the runtime of our algorithm. First, the forward-backward algorithm is coded in the Cython module to make use of the increased speed of a precompiled C function within our overall Python implementation. Second, our algorithm uses a rescaled version of the forward-backward algorithmwhich avoids computing logarithms of sums that would be computationally substantially more expensive than products and additions. Finally, we make use of the symmetry of the four IBD states. As the transition probabilities between those are fully symmetric, we can reduce the transition matrix from a 5 × 5 to a 3 × 3 matrix by collapsing the three other IBD states into a single 'other IBD' state. After the exponentiation of the 3 × 3 matrix, the original 5 × 5 transition matrix is reconstructed by dividing up the jump rates using the original symmetry.

---

### Evaluation of the evenness score in next-generation sequencing [^1f5f1936]. Journal of Human Genetics (2016). Low credibility.

The evenness score (E) in next-generation sequencing (NGS) quantifies the homogeneity in coverage of the NGS targets. Here I clarify the mathematical description of E, which is 1 minus the integral from 0 to 1 over the cumulative distribution function F(x) of the normalized coverage x, where normalization means division by the mean, and derive a computationally more efficient formula; that is, 1 minus the integral from 0 to 1 over the probability density distribution f(x) times 1-x. An analogous formula for empirical coverage data is provided as well as fast R command line scripts. This new formula allows for a general comparison of E with the coefficient of variation (= standard deviation σ of normalized data) which is the conventional measure of the relative width of a distribution. For symmetrical distributions, including the Gaussian, E can be predicted closely as 1-σ(2)/2⩾E⩾1-σ/2 with σ ≤ 1 owing to normalization and symmetry. In case of the log-normal distribution as a typical representative of positively skewed biological data, the analysis yields E≈exp(-σ*/2) with σ*(2) = ln(σ(2)+1) up to large σ (≤ 3), and E≈1-F(exp(-1)) for very large σ (⩾2.5). In the latter kind of rather uneven coverage, E can provide direct information on the fraction of well-covered targets that is not immediately delivered by the normalized σ. Otherwise, E does not appear to have major advantages over σ or over a simple score exp(-σ) based on it. Actually, exp(-σ) exploits a much larger part of its range for the evaluation of realistic NGS outputs.

---

### Universal scaling in real dimension [^d1d47cd6]. Nature Communications (2024). High credibility.

In order to extract the correlation length exponent from the scaling of 〈 R 2 (N)〉, one needs to identify a suitable region of N where one can reasonably apply the relation in Eq. (3). This universal region can be identified by observing the collapse of different curves 〈 R 2 (N)〉 for different system sizes L. This analysis is reported in Fig. 4, for both σ = 1.8 and σ = 1.1. In the upper panel the collapse in rather evident in a wider region of N, while in the lower panel the collapse region is harder to determine. The universal window is visually identified as the region where the three curves overlap and exhibit a linear behavior. For all σ ≳ 1.5 the universal window roughly corresponds to the region where the disagreement between the three curves remains below 5%. The same analysis has been repeated for all values of σ resulting in very extended and well-defined fitting regions for all σ ≳ 1.5, similarly to what is reported in the upper panel in Fig. 4. For 1.5 ≳ σ ≳ 1.1 the collapse region is less extended but still pronounced enough to obtain reliable ν estimates, at least up to d s ≃ 3.5, see the lower panel in Fig. 4. Finally, in the region σ ≤ 1 (corresponding to d s ≥ 4) it is not possible to identify a clear universal region and, accordingly, the ν estimates become less precise, see the Supplementary Note II. As a consequence, the agreement between our data and the theoretical line is poorer in this region, see Fig. 3.

Fig. 4
Logarithmic derivative of the average spatial extent of the random walk.

The plot displays the logarithmic derivative of the average spatial extent of the random walk as a function of the inverse logarithm of the number of steps N. Data shown for σ = 1.8 and σ = 1.1, corresponding to ρ = 3.8 (top) and ρ = 3.1 (bottom), for different lattice sizes: L × L = 64 × 64 (blue), 128 × 128 (yellow), 256 × 256 (green). The collapse region is well established for larger values of ρ and corresponds to a regime where scaling corrections and finite sizes effects are negligible, so that one can define a ' ', as bracketed by the vertical red dashed lines, and extrapolate from there the critical exponent.

---

### Stochastic representation of many-body quantum States [^5e3c8f08]. Nature Communications (2023). High credibility.

Regression

As formulated here, regression is a fundamental supervised learning task at which NNs excel. Given an ansatzand a set of samplesrepresenting our prior knowledge of the true wavefunction at some points in space, we want to find the best value of ϑ. Perhaps the simplest approach is to minimize the sum of squared residuals:To express the ansatz itself, we use a NN. Our architecture is very minimal and has not been tuned for efficiency: the NNs we used consist of a sequence of dense layers withactivation functions, and finally a linear output layer. This is in some cases be followed by an optional layer enforcing analytically known boundary and/or cusp conditions by multiplying the output of the NN with a hand-chosen parameterized function of the coordinates, such as the asymptotic solution at large distances from the origin. Technical details are provided in the Methods section.

The top panel of Fig. 1 shows what this looks like for a single particle in 1D, where the process is easy to visualize. For an arbitrary initial state (top left panel) and the ground state (top right panel) of a 1D harmonic oscillator, a series of samples and the resulting NN-based regression curves are shown. In the insets of the panel below, an analogous visualization is shown for 2D cuts across 4D wavefunctions of two interacting fermions in a 2D harmonic potential.

Fig. 1
Propagation towards the ground state in a harmonic potential.

a Different steps in the propagation towards the ground state of a particle of mass m = 1 in a 1D harmonic oscillator with frequency ω = 1, with ℏ = 1. The green line is the function fitted by the neural network to a finite set of samples (black dots on the x axis) and their corresponding values (connected by a black line). Starting with an asymmetric guess (τ = 0), the function converges towards the correct solution (dotted orange line) at the center of the trap and acquires the right symmetry (τ = 3). b Extension of the upper system to two fermions in two spatial dimensions. The energy is estimated by Monte Carlo sampling with error bars showing the standard error, and converges to the ground state value of E 0 = 3.010 ± 0.007, which results in a relative error of 0.35% with respect to the exact value of. The inset shows a cut through the wavefunction. Source data are provided as a Source Data file.

---

### Diagnostic methods 2: receiver operating characteristic (ROC) curves [^939c0910]. Kidney International (2009). Low credibility.

Clinical practice commonly demands 'yes or no' decisions; and for this reason a clinician frequently needs to convert a continuous diagnostic test into a dichotomous test. Receiver operating characteristic (ROC) curve analysis is an important test for assessing the diagnostic accuracy (or discrimination performance) of quantitative tests throughout the whole range of their possible values, and it helps to identify the optimal cutoff value. The value of this analysis is not confined to diagnosis in that it may also be applied to assess the prognostic value of biomarkers and to compare their predictive value. ROC curve analysis may also serve to estimate the accuracy of multivariate risk scores aimed at categorizing individuals as affected/unaffected by a given disease/condition. However, one should be aware that, when applied to prognostic questions, ROC curves don't consider time to event and right censoring, and may therefore produce results that differ from those provided by classical survival analysis techniques like Kaplan-Meier or Cox regression analyses.

---

### Article 5. An introduction to estimation – 2: from z to T [^fb82b806]. Emergency Medicine Journal (2001). Low credibility.

Provided the sample size is large enough (that is, n greater than 100), the z statistic can be used to determine the confidence interval estimation of the population mean even when the sigma is not known. In these cases the estimation of the standard error of the mean is used. The z statistic is also valid when determining the population's proportion based upon a large sample. However, when dealing with smaller samples, the z statistic is replaced by the t statistic. This makes it possible to estimate, in a population with an unknown standard deviation: The probability of getting a sample mean greater than or equal to a particular value The value of a sample mean with a particular probability of occurring The probability of getting a sample mean between two particular values The confidence interval for the estimation of the population mean can also be determined using the t statistic.

---

### Simple statistical measures for diagnostic accuracy assessment [^ce91322a]. Journal of Thoracic Oncology (2010). Low credibility.

The aim of diagnostic medicine research is to estimate and compare the accuracy of diagnostic tests to provide reliable information about a patient's disease status and thereby influencing patient care. When developing screening tools, researchers evaluate the discriminating power of the screening test by using simple measures such as the sensitivity and specificity of the test, as well as the positive and negative predictive values. In this brief report, we discuss these simple statistical measures that are used to quantify the diagnostic ability of a test.

---

### Beyond the P: II: precluding a puddle of P values [^1cd5f00e]. Journal of Cataract and Refractive Surgery (2004). Low credibility.

This is the second in a 4-part series discussing the proper use, interpretation, and limitations of P values.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^cbffc27e]. CDC (2011). Medium credibility.

Appendix G. Mathematical formula for sample size — sample size required to detect the difference of GM between two subpopulation groups defines a z test with null hypothesis GM1 = GM2 or GMmax/GMmin = 1 and alternative GM1 ≠ GM2 or GMmax / GMmin > 1, where GMmax = max(GM1, GM2) and GMmin = min(GM1, GM2). It considers power W (say 80% chance) to detect that one group's GM is at least k fold higher and gives the p-value as p = Prob(Z ≥ z) = 1 − φ−1(z) with Z ~ Normal (0,1). Given α (e.g., 0.05) and target ratio k, power is W = Prob (Z > φ−1(1 − α) − log10(k)/(S√(1/n1 + 1/n2))), where S is the expected standard deviation of log10(VL) in the population of interest; when n1 = n2 = n, W = Prob (Z > φ−1(1 − α) − log10(k)/(S√(2/n))). The required sample size is n = 2[(φ−1(1 − α) − φ−1(1 − W))S/log10(k)]^2, and tables are provided for W = 0.8 and W = 0.9 with α = 0.05.

---

### Rounded data have a high potential for false comparative statistical results as shown with elbow arc of motion [^744d058a]. Journal of Shoulder and Elbow Surgery (2018). Low credibility.

Background and Hypothesis

Visually measured range-of-motion (ROM) data are usually rounded to the nearest 5° interval and then recorded. Rounding might significantly influence the outcome of statistical tests.

Methods

We performed numerical simulation of t test application on 2 datasets, as typically reported for the elbow flexion-extension arc of motion. The test was performed on exact data and then repeated on the same data rounded to the nearest 5° interval. The simulation input parameters were as follows: difference in means (1°-30°), standard deviation (1°-30°), and number of cases (15, 30, 60, and 120). Diverging results were counted to find the rate of failure.

Results

Depending on the given difference in means, the given standard deviation, and the number of cases, the failure rate of the t test after rounding reached up to 40%.

Discussion and Conclusion

The accuracy of statistical tests performed on rounded ROM data is limited because of loss of information after rounding to the nearest 5° interval. This affects parametric and nonparametric tests, as well as paired and unpaired tests. In the future, authors should specify how ROM has been measured and recorded, explicitly addressing rounding. Furthermore, to test a zero hypothesis on rounded ROM data, authors should apply our P value (α) correction.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^dee6cacc]. CDC (2011). Medium credibility.

Community viral load sample size calculations — sample size to detect differences in geometric mean (GM) viral load depends on power and standard deviation (S). The table specifies "α = 0.05 and W = 0.8", with S columns 1 to 1.5; for k = 3 the minimum sample sizes by S are 54, 66, 78, 92, 106, and 122, and "1.2 is the standard deviation of national VL data", so at S = 1.2 the needed sample size is 78.

---

### Clinicians are right not to like cohen's κ [^a5f2e987]. BMJ (2013). Excellent credibility.

Clinicians are interested in observer variation in terms of the probability of other raters (interobserver) or themselves (intraobserver) obtaining the same answer. Cohen's κ is commonly used in the medical literature to express such agreement in categorical outcomes. The value of Cohen's κ, however, is not sufficiently informative because it is a relative measure, while the clinician's question of observer variation calls for an absolute measure. Using an example in which the observed agreement and κ lead to different conclusions, we illustrate that percentage agreement is an absolute measure (a measure of agreement) and that κ is a relative measure (a measure of reliability). For the data to be useful for clinicians, measures of agreement should be used. The proportion of specific agreement, expressing the agreement separately for the positive and the negative ratings, is the most appropriate measure for conveying the relevant information in a 2 × 2 table and is most informative for clinicians.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^6517cbfd]. Annals of the American Thoracic Society (2023). High credibility.

Basic stereological measurements — first-order parameters and probes identify that "First-order parameters are volume (three-dimensional [3D]), surface (two-dimensional [2D]), length (one-dimensional [1D]), and number (zero-dimensional [0D])". Measurements "are performed using geometric test probes, such as points (0D), lines (1D), planes (2D), or volumes (3D)". These probes create countable events in the image, and "Raw counts provide ratios… that are multiplied by the reference space volume to obtain absolute measures for the lung or subcompartment".

---

### Accurate detection of identity-by-descent segments in human ancient DNA [^68238e29]. Nature Genetics (2024). High credibility.

Transition probabilities

To calculate the 5 × 5 transition probabilities T to change states from one to the following loci, denoted by l and l + 1, we make use of the genetic map distances obtained from a linkage map, that is a map of the position using Morgans as the unit of length (1 M is the genomic map span over which the average number of recombinations in a single generation is 1).

As in ref. we specify the transition probabilities via a 5 × 5 infinitesimal transition rate matrix Q, from which each transition probability matrix A l → l +1 is obtained through matrix exponentiation using the genetic distance r l between loci l and l + 1

Here, Q is defined by the following three rate parameters: the rate to jump from the non-IBD state into any of the four IBD states (IBDin), the rate to jump from any of the IBD states to the non-IBD states (IBDout) and the rate to jump from any of the IBD states to another one (IBDswitch):where the diagonal elements are defined as Q i i = −∑ j ≠ i Q i j such that the rows of Q sum to zero as required for a transition rate matrix. The rate IBDswitch models phasing errors, as a transition from one IBD state to another means that a different haplotype pair is shared. We note that the probability of the IBD state jumping from 1A/2A to 1B/2B would require phase switch errors to occur in both individuals at the same genomic location, which is highly unlikely; however, we set the transition matrix between all four IBD states symmetric as this allowed us to implement a substantial computational speed up.

Emission probabilities

---

### Directly measuring mean and variance of infinite-spectrum observables such as the photon orbital angular momentum [^a1aca62f]. Nature Communications (2015). Medium credibility.

The standard method for experimentally determining the probability distribution of an observable in quantum mechanics is the measurement of the observable spectrum. However, for infinite-dimensional degrees of freedom, this approach would require ideally infinite or, more realistically, a very large number of measurements. Here we consider an alternative method which can yield the mean and variance of an observable of an infinite-dimensional system by measuring only a two-dimensional pointer weakly coupled with the system. In our demonstrative implementation, we determine both the mean and the variance of the orbital angular momentum of a light beam without acquiring the entire spectrum, but measuring the Stokes parameters of the optical polarization (acting as pointer), after the beam has suffered a suitable spin-orbit weak interaction. This example can provide a paradigm for a new class of useful weak quantum measurements.

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^a549d473]. Nature Communications (2023). High credibility.

Curves were classified as significantly up or down-regulated if a curve's relevance score is above the decision boundary.

For many applications, it is also useful to know which dose–response curves show clear independence of the doses to obtain a high-quality negative data set. It is fundamentally impossible to prove the absence of an effect statistically. Thus, we developed a heuristic approach using the null model to classify only a clear non-responsive line to be not regulated. A clear non-responder has a mean-model intercept close to 1.0 and is allowed to maximally diverge +- fc lim /2. Additionally, the variance around the null model should be low and is quantified by the root-mean-squared error (RMSE). By default, the maximally tolerated variance is an RMSE of 0.1 but can be adjusted by the user. Optionally, the user can add additional criteria, such as a p -value threshold, to be even more restrictive to the non-classification.

False discovery rate estimation

The false discovery rate (FDR) was estimated using a target-decoy approach. Decoy curves were generated based on the sample variance distribution estimated from the input data directly (Eq. 15). From this variance distribution, decoys were constructed similarly to the null curves (Eq. 1).

The decoy curves are then subject to the identical analysis pipeline. Finally, the target-decoy relevance score allows the calculation of a q -value for each curve using the target-decoy approach, as well as the overall FDR corresponding to the user's pre-defined thresholds (Eq. 15). FDR estimation is enabled using the — fdr command line option.

Statistics and reproducibility

No new experimental data were generated for this study. Study designs, including sample sizes, were taken directly from the respective original studies. For the simulated curve data, no statistical method was used to determine sample size. The 5 million curves for each number of curve data points n was chosen as the number of curves that produced sufficient smooth characterization of p -value estimation behavior up to 10 −5. No data were excluded from the analyses. Unless otherwise stated, p -values in this study were generated using the recalibrated F-statistic and multiple testing correction was performed by applying a target-decoy strategy on the relevance score. The CurveCurator version used throughout this manuscript is v0.2.1.

Reporting summary

Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.

---

### Understanding statistical terms: 2 [^351bd399]. Drug and Therapeutics Bulletin (2009). Low credibility.

An increasing number of statistical terms appear in journal articles and other medical information. A working knowledge of these is essential in assessing clinical evidence. With this in mind, we are producing a series of explanatory articles covering various statistical terms and their uses. This, the second article in the series, will focus on some of the most common terms used in reporting the results of randomised controlled trials.1.

---

### A practical guide for understanding confidence intervals and P values [^c0a7d4a0]. Otolaryngology — Head and Neck Surgery (2009). Low credibility.

The 95 percent confidence interval about the mean demarcates the range of values in which the mean would fall if many samples from the universal parent population were taken. In other words, if the same observation, experiment, or trial were done over and over with a different sample of subjects, but with the same characteristics as the original sample, 95 percent of the means from those repeated measures would fall within this range. This gives a measure of how confident we are in the original mean. It tells us not only whether the results are statistically significant because the CI falls totally on one side or the other of the no difference marker (0 if continuous variables; 1 if proportions), but also the actual values so that we might determine if the data seem clinically important. In contrast, the P value tells us only whether the results are statistically significant, without translating that information into values relative to the variable that was measured. Consequently, the CI is a better choice to describe the results of observations, experiments, or trials.

---

### Pointwise error estimates in localization microscopy [^58128e53]. Nature Communications (2017). Medium credibility.

Laplace approximation

An alternative way to approximate the uncertainty of the fit parameters is to Taylor expand the likelihood around the maximum-likelihood parameters θ * to second order, that is

The first-order term is 0, since θ * is a local maximum. This approximates the likelihood by a Gaussian with covariance matrix given by the inverse Hessian, that is, Σ = [∂ 2 ln L / ∂θ 2] −1. In a Bayesian setting, this expresses the (approximate) posterior uncertainty about the fit parameters. The estimated uncertainties (posterior variances) are given by the diagonal entries of the covariance matrix, for example. We compute the Hessian numerically using Matlab's built-in optimization routines, and use the log of the scale parameters b, N, σ for fitting, since they are likely to have a more Gaussian-like posterior.

Prior distributions

For MAP localizations in the main text, we used weak normal priors with standard deviation ln(30) centred on the true value for the log background intensity. For the spot width, we define σ = σ 0 (1+Δ σ), where σ 0 = 0.21 λ /NA is the width of a focused spot, and put exponential priors with mean value one on Δ σ. For asymmetric Gaussian spots with two principal widths, we define analogous excess widths Δ σ 1,2 and use independent exponential priors with mean value 1/2. Other parameters were left with flat priors (for details, see Supplementary Note 3).

---

### Extensive benchmarking of a method that estimates external model performance from limited statistical characteristics [^691db016]. NPJ Digital Medicine (2025). Medium credibility.

One-sided positivity tests

The following tests are performed before running the weighting algorithm, to assess one-sided positivity:
Verify that Φ int and μ ext contain the same features and feature-outcome transformations and do not contain missing values.
Unary variables: In case one of the columns of Φ int has a constant value across all units, make sure that the difference between this value and the corresponding entry in μ ext is less than a threshold (default 0.01).
Binary variables: First, if the external statistics of binary features are exactly the same as one of the internal binary values, we assume that the external cohort is composed of a sub-population with this value and use only the corresponding sub-population in the internal cohort for weighting. Second, we test if univariate weighting on this variable will not result in assigning most of the weights to a few internal units. Specifically, let n 0 and n 1 be the number of internal units with feature values 0 and 1. Let p 0 and p 1 be the proportion of such units in the external dataset. Then, we assume that whenmost weights will be assigned to a few samples and the variance of any weighted estimator will be too large.
Continuous variables: Make sure that the external statistics is within the range of the internal cohort values. We allow a small slack (default 0.01).

Weighting algorithm pre-processing and parameter settings

Before running this algorithm, the features in the internal dataset are normalized by subtracting the mean and dividing by the standard deviations. The means and standard deviation are maintained and used later to apply the same transformation to the external statistics. This step maintains the relationships between the internal cohort and the external statistics while improving numerical stability and standardizing decision threshold. The convergence of the algorithm is examined by testing if the L 2 norm of the distance between the weighted internal and the external statistics is less than a threshold (default value 10 −5). The maximum number of iterations in this benchmark was set to 2000. If the weighting algorithm converges, we test that the maximum standardized mean difference between the weighted internal statistics and the external ones is less than a threshold (default 0.05).

---

### The quantum transition of the two-dimensional ising spin glass [^12bdf2f8]. Nature (2024). Excellent credibility.

Even operators

This case is of utmost relevance because only even excited states can cause the system to leave its ground state in (ideal) quantum annealing for the Hamiltonian (equation (1)). Our approach is not entirely satisfying in this respect because, for a given sample, we obtain the smallest of the two same-parity gaps Δ e and Δ o (one would like to study only Δ e). Fortunately, both gaps have similar magnitudes (Extended Data Fig. 3).

The first optimistic indication comes from the (subtracted) correlation function in Fig. 3a, which goes to zero (within errors) for a moderate value of τ. Indeed, the empirical distribution function for the correlation length η e in Fig. 3b indicates mild sample-to-sample fluctuations and a relatively weak dependence on L. In fact, as shown Fig. 3b, for all L > 12, the probability distribution function turns out to depend on the scaling variable(Setting, the whole curve cannot be made to scale and the resulting estimate z e ≈ 1.7 is lower, see also the concluding paragraph of 'Two-times observables' in Methods). Thus, as anticipated, we conclude that the even symmetry sector shows algebraic scaling for its gap.

Fig. 3
Studying the spectra of even excitations at the critical point.

a, Sample-averaged subtracted correlation function Q 2, s (τ) ('Fitting process and estimating the Euclidean correlation length' in Methods) becomes compatible with zero for moderate values of τ, for all our system sizes. b, Left, after computing the Euclidean correlation lengthfor each sample, we computed for each L the empirical distribution function F (η e), namely the probability F of finding a sample with(note the horizontal error bars). Right, the data in the left-hand panel of b, when plotted as a function of the scaling variable u (equation (4)) do not show any L residual L dependence other than for our smallest sizes L = 8 and 12. Points in a and b are statistical averages, and errors are one standard deviation. Our data set is fully described in Extended Data Table 1.

Source Data

---

### Practice resource-focused revision: standardized pedigree nomenclature update centered on sex and gender inclusivity: a practice resource of the National Society of Genetic Counselors [^155ba9e0]. Journal of Genetic Counseling (2022). High credibility.

General revisions to standardized pedigree nomenclature — The 'E' notation (Evaluation) from the previous recommendations has been eliminated because of a lack of general usage, and the results of the individual's evaluation(s) can be listed below the individual's symbol, but they do not need to be preceded by an 'E'. The Task Force discussed changing the 'relationship line' connecting spouses or partners from a solid to a dotted line, but it was decided that such a change is largely unnecessary and would be disruptive both to practicing clinicians and electronic medical records and pedigree software; in consanguineous unions, a solid double line denotes a genetic relationship.

---

### Continuous growth reference from 24th week of gestation to 24 months by gender [^0f68c406]. BMC Pediatrics (2008). Low credibility.

Appendix

Assuming normality, to estimate the variance, skewness and kurtosis of the population based on the cases in the range of μ ± δσ in a sample for certain values of δ > 0.

Without lost of generality, let μ = 0. Let, where φ (x; σ) is the normal probability distribution function with mean 0 and variance σ 2. It can be deduced that

where Φ (x) is the standard normal cumulative distribution function. Letand. It can be shown that

Letand. It can be shown that

Let p = 2Φ(δ) - 1 be the proportion of observations in the range of μ ± δσ under a normal distribution. Considering a sample of n observations, x 1. x n. Let q 1 be the (1 - p)/2-th quantile and q 2 be the (1 - (1 - p)/2)-th quantile and X r = { x i: q 1 ≤ x i ≤ q 2 } be the reduced sample of size m. An empirical estimate of, j = 2, 3, 4, is given as

The general sample variance, the skewness and the kurtosis are given as

and

respectively. Let, Skew r and Kurt r be the corresponding statistics calculated based on the reduced sample. Substituting – into –, we have the statistics for the full dataset evaluated based on the reduced sample as

and

---

### Breaking free from the hazard ratio: embracing the restricted mean survival time in clinical trials [^1fda049a]. NEJM Evidence (2023). Medium credibility.

Clinicians consider the Cox proportional hazards model to be the default tool of choice for analyzing right-censored time-to-event outcomes in randomized clinical trials. 1 The Cox model estimates the ratio of the hazard of the outcome of interest (e.g., the time to death) between two treatment groups. The hazard is defined as the instantaneous probability of experiencing the event of interest in the next time interval among individuals who had not yet experienced the event. As a statistical measure, the hazard ratio can be difficult to interpret.

---

### Modeling uncertainty in single-copy assays for HIV [^c02085f8]. Journal of Clinical Microbiology (2012). Low credibility.

We present a simple computational model of measurement accuracy for single-copy sensitivity assays (SCA) of HIV RNA that was developed from first principles. The model shows that the SCA is significantly right-skewed. Measured virus concentrations of 1 and 10 virions/ml had overlapping 95% confidence intervals and were statistically indistinguishable.

---

### Harvesting information from kaplan-meier plots: part 1-detecting censoring [^eaf31902]. The Oncologist (2024). Medium credibility.

(1) Censoring that is not informative is acceptable. This includes censoring because an individual has not been enrolled in a study long enough to reach a scheduled assessment. ln the overwhelming majority of trials, except those that report outcomes earlier than they should and that are still immature, all patients will have been enrolled in a study long enough to qualify for the first re-assessment and even the second. To confirm this, look for the day the last patient enrolled and the date of "data cutoff". If this is longer than the time to first assessment or to any assessment of interest, then there should be no censoring before those times on the x -axis of the KM plot, because all such censoring would be considered potentially "informative".
(2) If follow-up has been long enough, the process described for estimating the number censored in that first assessment interval can be repeated for the next assessment interval. In doing this, use the number at risk representing the number going forward as the starting number enrolled and the number you estimated as percent progression-free as the new 100%, with the subsequent estimated progression-free relative to this "new 100%". In our example, 138 becomes the new 173 which was the number initially enrolled, and 93.75% becomes the new 100%. If the percent without progression at the next assessment is 80%, then divide 80 by 93.75 and find 85.3% as the percent progression-free in that interval, and use the number at risk moving forward from that next assessment in your calculations.
(3) Finally, when doing this with real and not "modeled data", the spread of the numbers is larger than the 24–26 we calculated, but the average usually coincides well with the actual number censored. Also, a "short-cut" one can use, for example, in a meeting when a slide is presented, is estimate the 93.75% "visually", multiply this by the number enrolled (173) and subtract the number "at risk" going forward (138), in effect, assume none were censored before assessment, and get a pretty good estimate of the number censored.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^6efded20]. Annals of the American Thoracic Society (2023). High credibility.

Pulmonary vascular metrics — density, tortuosity, and fractal dimension: "Extraction of pulmonary vascular data from imaging data sets can be automated or semiautomated", and "Loss of pulmonary vascular density or distal PA blood volume is associated with right heart failure indices in smokers, emphysema progression, and asthma severity". "Vessel tortuosity can be determined by comparing the linear distance between two points on a vessel with the actual 3D path length", with this "distance metric" increased in pulmonary arterial hypertension and "correlated with mean PA pressure, pulmonary vascular resistance, and arterial and venous oxygen content and saturation". For fractal analysis, "the number of vessel-containing cubes is related to cube size on a double logarithmic plot; fractal dimension is the slope of the linear portion of the plot", or it "can be estimated from CT or MRI perfusion scans by calculating the relative dispersion (standard deviation/ mean)… and evaluating the slope of the relationship between log relative dispersion versus log number of voxels averaged".

---

### Ultra-sensitive measurement of transverse displacements with linear photonic gears [^478a9b17]. Nature Communications (2022). High credibility.

To evaluate and compare the sensitivity of different gears, we then focused our measurements on the linear region. We, therefore, set the optical power to a reference value P 0 = 1 mW and measured the calibration curve P (Δ x) in the linear region, for a total displacement of 2 μm with steps of 20 nm (see Fig. 2 b). To improve the plot clearness, each dataset has been measured in a slightly shifted region with respect to the middle of the fringe (but still abundantly in the linear region of the calibration curve). A comparison between the slopes of the curves clearly shows the advantage in terms of sensitivity that is obtained by decreasing Λ. This can be readily quantified by performing a linear fit for each curve. The best sensitivity S = 124.0 ± 0.1 nW/nm was obtained, as expected, for g -plates with Λ = 50 μm. We, therefore, evaluated the actual resolution of our detection system for this configuration. To this end, we repeatedly measured the optical power for a time interval of 1 s (250 points), before and after a controlled step of the translation stage. We report these results in Fig. 3, where the red dots represent experimental data, while black dashed lines mark the average power as calculated over each set of 250 points. We started with a "large" displacement Δ x = 100 nm, which led to a mean power differenceμW, and gradually decreased the step size. The smallest measured displacement Δ x = 5 nm, corresponding to approximatively λ /125, is still clearly resolved as the measuredμW is significantly larger than its error bar (σ = 0.3 μW), calculated as the quadrature sum of the standard deviations of the power distribution before and after the step, respectively. Further reducing the displacement would not lead to meaningful results, as the nominal displacement accuracy of our translation stage is approximately 2 nm. Let us notice that the power standard deviation when using g -plates with Λ = 50 μm was typically σ P = 0.2 μW, yet it reduced to 0.1 μW when the plates were switched off. Therefore, we ascribe the residual value of the standard deviation of our signal to vibrations of the optomechanical components.

---

### ACC appropriate use criteria methodology: 2018 update: a report of the American college of cardiology appropriate use criteria task force [^cbffef90]. Journal of the American College of Cardiology (2018). Medium credibility.

ACC Appropriate Use Criteria methodology — rating process initiates with an independent first round in which each rating panel member submits numerical ratings independently.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^b1513dd5]. CDC (2011). Medium credibility.

Table 1b — Minimum sample size for detecting geometric mean (GM) ratio k with α = 0.05 and W = 0.9 — presents required sample sizes by standard deviation (S) strata and defines parameters; α is significance level, W is power, GM is geometric mean, and S is standard deviation, noting that 1.2 is the standard deviation of national viral load (VL) data. Each jurisdiction will need to assess the standard deviation of their local VL data and then determine the appropriate sample size needed to assess VL, and if the sample size is inadequate to meet the recommended case inclusion criterion, an alternate method may need to be used, such as combining multiple years of data. For k = 3, sample sizes across S = 1, 1.1, 1.2, 1.3, 1.4, 1.5 are 75, 91, 108, 127, 147, 169, and Table 1b uses power = 90%; jurisdictions may also explore differences in means of viral loads, including categorical differences in the proportion with undetectable or very low VL.

---

### Methodology for ACOEM's occupational medicine practice guidelines-2025 revision [^4a12b394]. ACOEM (2025). High credibility.

Table A-1 — Criteria for Adequate Evidence for Studies of Clinical Assessment Methods lists inclusion criteria for clinical assessment test studies, including "Be published in English in a peer-reviewed scientific publication" and "Evaluate a clinical method currently available to providers in North America and Europe (and the clinical method that is not obsolete or experimental)". Additional criteria include "Provide original data about efficacy (accuracy) of the clinical method for the condition of interest" and "Provide an adequate description of the clinical method (or provide a reference where this information can be found)". Population and quality expectations include evaluating "subjects similar to the target population of interest (in this case, the general population of working age adults), generally with the number of subjects in each arm of the study to achieve acceptable statistical power. However, sometimes statistical power may not be achieved due to ethical constraints or the newness of the treatment" and to "Be rated as high or moderate quality, using the quality rating process". Method performance assessment includes "Evaluate the efficacy (i.e., clinical accuracy) of the assessment method (i.e., the "test") in a group that contains subjects both with and without the condition the test is intended to assess", comparison "to an adequate reference standard for all subjects (not just subjects who tested positive)", blinded interpretation of both the test and the reference standard, and provision of sufficient data to calculate "sensitivity, specificity and positive predictive value(s)", with the note that "The calculation of positive predictive value can be accurately determined only if the study includes the prevalence of the particular condition the test is designed to assess in the target population". The table also notes that "Definition of an "acceptable reference standard" will vary depending on the clinical topic".

---

### Individual differences in belief updating and phasic arousal are related to psychosis proneness [^57e285d5]. Communications Psychology (2024). Medium credibility.

Samples were presented for 250 ms (sample-onset asynchrony (SOA): 400 ms). Samples were centered on polar angles x 1, …, x n drawn from one of two truncated Gaussian distributions p (x | S) with variance σ left = σ right = 27° and means symmetric about the vertical meridian (μ left = −17°, μ right = +17°). These parameters equated to a signal-to-noise ratio (SNR) for the generative distributions of (μ right - μ left) / σ = 1.26. If a draw x i was < -90° (> +90°), it was replaced with −90° (+ 90°). S was chosen at random at the start of each trial and could change with a hazard rate H = p (S n = right | S n-1 = left) = p (S n = left | S n-1 = right) = 0.1 after each sample. 65% of sequences contained 10 samples. The remaining 35% (randomly distributed in each block) contained (with equal probability) 2–9 samples and were introduced to encourage participants to attend to all samples. These task parameters were chosen to ensure expressive belief updating dynamics characterized by periods of strong belief coupled with regular moments of surprise and uncertainty. The motivation behind specific parameter values is provided in Supplementary Table 1.

---

### Guidelines for the appropriate use of bedside general and cardiac ultrasonography in the evaluation of critically ill patients-part II: cardiac ultrasonography [^177de668]. Critical Care Medicine (2016). Medium credibility.

Cardiac ultrasonography — RAND algorithm for consensus and recommendation grading defines entry by either "All panelists gave same number" or the "% of panelists voting in relation to median score". When " ≥ 50% of panelists voted within 1 integer", users "Look at the range of achieving ≥ 80% of panelists", with categories based on " ≥ 80% achieved at 1 integer", " ≥ 80% achieved at 2 integers", or " ≥ 80% achieved at 3 integers". Branches also include " < 50% of panelists voted within 1 integer" and "Other responses or disagreement?", and labeled outputs include "Strong Recommendation".

---

### Uniform spatial pooling explains topographic organization and deviation from receptive-field scale invariance in primate V1 [^509b3ddb]. Nature Communications (2020). High credibility.

Formulating predictions of F 1/ F 0 in the pooling model

We measured phase selectivity at the preferred SF (f o) as F 1/ F 0. Specifically, we obtained the peak-to-peak amplitude from the Fourier transform of the phase tuning curve, and then divided by the mean over phase (i.e. F 1/ F 0). To model F 1/ F 0 as a function of f o in pooled scale invariance, we start with the assumption that the pooled population of scale invariant RFs are simple cells with a half-wave rectifying nonlinearity. Next, we inherit the same Gaussian pooling window that was fit in Eq. (5): σ h (x) = 0.24°. As formalized below, the input from each pooled RF is phase-shifted and Gaussian-weighted according to σ h (x), then they are all summed to yield the output cell's phase tuning (Fig. 4). As the simplest case, we start with a constant relative phase, whereby the absolute phase shift is coupled to the retinotopic shift of the envelope; i.e. the RFs look identical, but are shifted by the retinotopy (Fig. 4c, left). In this case, absolute phase shift (°) equals f o times the retinotopic shift of the RF (" x "deg). To formulate F 1/ F 0 as a function of f o, we start with convolution between the spatial pooling function and a scale invariant Gabor.where G x (0, σ h (x)) is the spatial pooling function — it is a Gaussian in the spatial domain (x) with peak = 1, mean = 0 deg, and SD = σ h (x) deg. The right side of the convolution is a scale invariant Gabor with carrier frequency f o. Importantly, the Gabor is normalized by the square root of its L2 norm, which ensures that the Fourier amplitude at f o (i.e. the F 1) is constant prior to the convolution. We are interested in the Fourier amplitude following the above convolution, which is proportional to the product of two Gaussians in the frequency (f) domain:The left side of the product is the Fourier amplitude of the pooling function and the right side is the Fourier amplitude of the Gabor. Both are unity amplitude Gaussians with the mean and SD in parentheses, in units of cyc/°. Evaluating at f o gives the following, which is proportional to the " F 1":For a half-wave rectifying nonlinearity, the maximum F 1/ F 0 is π, so we scale accordingly to getwhich is a Gaussian centered at 0 cyc/°, with an SD of (σ h (x) 2 π) −1 = 0.66 cyc/°. This function is shown in Fig. 4a (bottom dashed green), which underestimates the population's phase selectivity at each f o. The above equations assume that all the scale invariant inputs have the same relative phase (e.g. they all have even symmetry), which means that any change in the absolute phase is coupled to the RF envelope. For a more accurate model, we reduced the rate of the absolute phase progression with a coefficient, denoted D, in Eq. (6) below. D was fit by minimizing the mean-squared error.

---

### ACC / AHA statement on cost / value methodology in clinical practice guidelines and performance measures: a report of the American college of cardiology / American Heart Association task force on performance measures and task force on practice guidelines [^f82b2c57]. Circulation (2014). Medium credibility.

ACC/AHA levels of evidence qualify certainty and population scope. Level A indicates "Multiple populations evaluated" with "Data derived from multiple randomized clinical trials or meta-analyses". Level B indicates "Limited populations evaluated" with "Data derived from a single randomized trial or nonrandomized studies". Level C indicates "Very limited populations evaluated" with "Only consensus opinion, case studies, or standard of care". The document clarifies that "A recommendation with Level of Evidence B or C does not imply that the recommendation is weak", explains the asterisk as "Data available from clinical trials or registries about the usefulness/efficacy in different subpopulations, such as sex, age, history of diabetes mellitus, history of prior myocardial infarction, history of heart failure, and prior aspirin use", and adds, "For comparative-effectiveness recommendations (Class I and IIa; Level of Evidence A and B only), studies that support the use of comparator verbs should involve direct comparisons of the treatments or strategies being evaluated".

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^d027f21a]. Nature Communications (2025). High credibility.

Understanding the effect of heterogeneity is fundamental to numerous fields. In community ecology, classical theory postulates that habitat heterogeneity determines niche dimensionality and drives biodiversity. However, disparate heterogeneity-diversity relationships have been empirically observed, generating increasingly complex theoretical developments. Here we show that spurious heterogeneity-diversity relationships and subsequent theories arise as artifacts of heterogeneity measures that are mean-biased for bounded continuous variables. To solve this, we derive an alternative mean-independent measure of heterogeneity for beta and gamma distributed variables that disentangles statistical dispersion from mean. Using the mean-independent measure of heterogeneity, true monotonic positive heterogeneity-diversity relationships, consistent with classical theory, are revealed in data previously presented as evidence for both hump-shaped heterogeneity-diversity relationships and theories of an area-heterogeneity trade-off for biodiversity. This work sheds light on the source of conflicting results that have hindered understanding of heterogeneity relationships in broader ecology and numerous other fields. The mean-independent measure of heterogeneity is provided as a solution, essential for understanding true mean-independent heterogeneity relationships in wider research.

---

### Comparing survival curves using an easy to interpret statistic [^e688f598]. Clinical Cancer Research (2010). Low credibility.

Here, I describe a statistic for comparing two survival curves that has a clear and obvious meaning and has a long history in biostatistics. Suppose we are comparing survival times associated with two treatments A and B. The statistic operates in such a way that if it takes on the value 0.95, then the interpretation is that a randomly chosen patient treated with A has a 95% chance of surviving longer than a randomly chosen patient treated with B. This statistic was first described in the 1950s, and was generalized in the 1960s to work with right-censored survival times. It is a useful and convenient measure for assessing differences between survival curves. Software for computing the statistic is readily available on the Internet.

---

### Modelling pathological spread through the structural connectome in the frontotemporal dementia clinical spectrum [^f6521431]. Brain (2025). Medium credibility.

The spread of pathology from an affected brain region (R 2) to an unaffected one (R 1) is given by:

where x 1,2 is the pathology concentration in region R 1,2; c 1,2 is the connectivity between R 1 and R 2; andis the diffusivity constant (the higher the value, the higher is the speed of pathology progression).

Pathology from all brain regions is combined into a vector x (t) = { x i (t)} w, and Equation (1) becomes:

where x (t) = { x i (t)}: represents the amount of diffusion of pathology at node i and time point t starting from an initial distribution at time t = 0(x (0)); L is the graph Laplacian matrix; and t are the time points (in arbitrary units).

From matrix algebra, Equation (1) is satisfied by:

where x (0) is a vector with 1 at the index corresponding seed brain regions where it is thought that the pathology begins to spread, 0 at all other brain regions. Four disease epicentres were identified from the peaks of atrophy of each FTD variant: left temporal pole (svPPA), left insula (bvFTD), right temporal pole (sbvFTD) and left supplementary motor area (nfvPPA), as previously shown.

The graph Laplacian represents the discretization of the Laplacian operator and indicates how a graph differs at one vertex from its values at nearby vertices. It was implemented as follows:

where I is the identity matrix; D is the diagonal matrix whose diagonal entries contain the degree of each node; and C is the averaged connectome of healthy subjects.

Given that brain regions are not the same size, L in Equation (3) is the normalized version of the graph Laplacian operator. It is a symmetric matrix, and its eigenvectors are orthonormal. The solution of Equation (1) was implemented mathematically in Matlab by the eigenvalue decomposition:

where N are the brain regions; λ are the eigenvalues of matrix L; and u are the eigenvectors of matrix L.

Brain Net Viewer was used to create diffusion maps.

---

### Structural analysis and architectural principles of the bacterial amyloid curli [^f7f98c12]. Nature Communications (2023). High credibility.

Fig. 1
Primary sequence analysis of CsgA homologues.

a Distribution of the number of csga-homologues detected per genome; b Distribution of the number of predicted curlin repeats. Inset: number of curlin repeats per protein length, with a slope of n = 23 ± 5 aa; c Consensus sequence of a curlin repeat with the proposed curlin motif denoted underneath; d On-axis view of a β-solenoid fold with conserved inwards facing residues highlighted; e Representative primary sequences (Seq ID in parentheses) of three sequence classes of CsgA, stacked according to their predicted curlin repeats, with respective consensus motifs indicated below (colored red and blue for motif a and B, resp. green when deviating).

The large variation in CsgA repeat numbers prevents investigation of sequence conservation and variability in the amyloid core via global sequence alignment. Rather, we extracted 53574 non-overlapping, linear segments of 24aa that are centered on a QX 10 Q motif (and permutations thereof) which represents the prototypical amyloid kernel of the CsgA curlin repeats. From this we derive a consensus sequencethat exhibits remarkable conservation at key sites and shows curlin repeats can be divided into closely related motifs a and b, typically separated by a 4-residue sequence with X-G-X-X signature, where X is any amino acid (Fig. 1c). In the predicted CsgA structure (see below) these curlin repeat elements form strands 1 and 2 and the connecting loop or β-arc of a β-arch as defined by Henetin et al. As such, the residues indicated in red and blue below the sequence logo(Fig. 1c) and the structural representation (Fig. 1d) map to the inward facing steric zipper residues whereas the residues indicated in black (Fig. 1c) map onto the outward facing surface residues of the strand-arc-strand (i.e. β-arch) motif that constitutes a single curlin repeat. The base signature of the motifs a and b consists of N-$-Ψ 1 -$-Ψ 2 -$-Q where Ψ n are hydrophobic residues (A, I, V, L, F), S or T and point to the inside of the β-arch, together with a well conserved N at the start and Q at the end of the motifs. Residues on locations corresponding to $ are surface exposed and are predominantly polar or charged (T, S, D, E, Q, N, R, Y), suggesting that most curli fibers can be expected to be hydrophilic. Motif b is followed by 4–5 residues, most typically in a X-G-X-X-(X) motif. This loop region forms a second β-arc, connecting motif b to motif a in the subsequent curlin repeat (Fig. 1d).

---

### ACC appropriate use criteria methodology: 2018 update: a report of the American college of cardiology appropriate use criteria task force [^bff81262]. Journal of the American College of Cardiology (2018). Medium credibility.

ACC Appropriate Use Criteria — rating tabulation states that final scores are reported in discrete categories — Appropriate Care, May Be Appropriate Care, and Rarely Appropriate Care — as well as with their numerical median rating, with anonymized individual scores available in an online appendix to the AUC document.

---

### Detecting genotype X age interaction [^0c85d303]. Genetic Epidemiology (2001). Low credibility.

This paper explores several extensions to the variance component method, which incorporate genotype x age interaction effects. We evaluate the performance of these methods for detecting genotype x age interaction in quantitative genetic analyses of a quantitative trait (Q4), contrasting this with false positive detection rates obtained from a phenotype influenced by the same genes but without genotype x age interaction effects (Q3). We then assess the impact on linkage power and false positive rate of allowing a QTL-specific genotype x age interaction in linkage analysis of these same traits.

---

### Decoding individual identity from brain activity elicited in imagining common experiences [^d84003ea]. Nature Communications (2020). High credibility.

fMRI Voxel selection (within regions of interest)

Because not all fMRI voxels contain informative signal, we estimated which ones were likely to be informative using a commonly used strategy introduced by ref. This identifies stable voxels that had a similar response pattern across each run of 20 scenarios. For each participant, and separately for each voxel, we took each pair of runs, and computed the inter-run Pearson correlation in activation across the 20 scenarios. This resulted in 10 correlation coefficients per voxel (derived through intercorrelating the five runs) that were r-to-z transformed. A single stability metric for each voxel was derived by taking the mean of the 10 coefficients.

We then parcellated fMRI data into 90 neuroanatomical ROIs using Automated Anatomical Labeling. We repeated all of our main analyses on the 100 most stable voxels (marked by high positive coefficients) within each ROI, or all voxels in the ROI if there were fewer than the desired number. Stable voxels were extracted from each ROI, and vectorized. A single representation of each scenario was computed by voxel-wise averaging activation across the 5 scenario replicates. Activation in each voxel was then normalized by subtracting the mean and dividing by the standard deviation of that voxel's activation. To test the robustness of our results to this particular parameterization, we repeated analyses on the 50 and 200 most stable voxels per ROI and obtained similar results, which are reported in Supplementary Information.

Transformation of personal models and fMRI data to representational similarity space

To test for a relationship between model and fMRI scenario representations we applied Representational Similarity Analysis (RSA). To set this up, fMRI data and models were re-represented in similarity space. For each of the 26 participants, inter-scenario Pearson correlation matrices were computed separately for the verbal and attribute models (yielding two 20*20 matrices). All correlation coefficients were r-to-z transformed (arctanh). The procedure was repeated on each participants' fMRI data, and performed separately on each ROI to generate a total of 90 correlation matrices per participant. The below diagonal matrix triangle (similarity triangle) was segmented from all (model and fMRI) correlation matrices (which are symmetric on the diagonal). Each similarity triangle was then vectorized to form a similarity vector with 190 entries.

---

### Global disparities in the introduction, scale-up, and effectiveness evaluation of COVID-19 vaccines [^92bcd7d4]. Nature Communications (2025). High credibility.

Fig. 1
Global disparities in COVID-19 vaccine introduction, scale-up, and effectiveness evaluation.

Maps of A COVID-19 vaccine introduction dates, B scale-up to 40% population-wide primary series coverage, and C distribution of available VE studies across different countries and territories. VE vaccine effectiveness.

Fig. 2
Disparities in COVID-19 vaccine introduction, scale-up, and effectiveness evaluation by income quartile.

A Percentage of countries and territories achieving vaccine introduction by 31 December 2021 (left), scale-up to 40% population-wide primary series coverage by 31 December 2021 (middle), and publication of ≥ 1 VE study by 07 January 2024 (right) by GNI quartile (n countries/territories = 51, 51, 51, and 50 for quartiles 4, 3, 2 and 1, respectively). B Date of vaccine introduction (left; n = 51, 51, 51, and 50 for quartiles 4, 3, 2, and 1, respectively), scale-up to 40% population-wide primary series coverage (middle; n = 49, 43, 32, and 22) and first VE study publication (right; n = 24, 15, 7, and 2) by GNI quartile. Only countries and territories that reached the specified milestones are included. C Percentage of countries and territories using different vaccine platforms by GNI quartile as of 07 January 2024 (n = 51, 51, 51, and 50 for quartiles 4, 3, 2, and 1, respectively). D Percentage of primary series VE studies by platform as of 11 January 2024 (n studies = 423). Box plots display median (centre line), upper and lower quartiles (box limits), the minimum value greater than or equal to the lower quartile − 1.5 × interquartile range (lower whisker), and the largest value less than or equal to the upper quartile + 1.5 × interquartile range (upper whisker). GNI per capita Gross National Income, VE vaccine effectiveness.

---

### Evaluating treatment efficacy by multiple end points in phase II acute heart failure clinical trials: analyzing data using a global method [^6d0dca88]. Circulation: Heart Failure (2012). Low credibility.

Background

To assess concomitant simultaneous effects on multiple end points using global statistical methods in phase II acute heart failure studies.

Methods and Results

Using simulations we have assessed different statistical methods to evaluate concomitant effects of a new intervention on dyspnea relief (using 2 measures), length of hospital stay, worsening heart failure to 5 days, mortality, and heart failure readmission to 30 days. Treatment effect scenarios included large (20% to 28% relative improvements) and very large (30% to 43% relative improvements) effects among others. Placebo responses and correlations among end points typical in recent acute heart failure clinical trials were used. Powers for the average Z score exceeded 70% with ≥ 75 patients per group for 35% relative improvement across all 6 end points. Assessing dyspnea alone generally provides lower power than the average Z score approach, with power deducted ≈50% under most of scenarios. Other approaches generally provide lower power than the average Z score method.

Conclusions

Assessing the effects of new therapies on multiple clinical end points using the average Z score enables detection of therapeutic efficacy using sample sizes of 100 to 150 patients per group, approximately double the power achievable assessing the effects on dyspnea alone.

---

### Identifying domains of applicability of machine learning models for materials science [^196a563d]. Nature Communications (2020). High credibility.

Fig. 1
Domains of applicability of three 2d-models of a noisy third-degree polynomial.

Three different models, linear (top), radial basis function (rbf, center), and polynomial (poly, bottom), are shown approximating the same distribution of two independent features x 1 ~ N (0, 2) and x 2 ~ N (0, 2), and the target property, where N (μ, ϵ 2) denotes a normal distribution with mean μ and standard deviation ϵ. Test points are plotted in 3d plots against the prediction surface of the models (color corresponds to absolute error) where the DA is highlighted in gray. The distributions of individual errors for the DA (gray) and globally (black) are shown in the 2d plots of each panel with the mean error (solid) and the 95th percentile (95 perc./dashed) marked by vertical lines. Note that the global error distribution of the linear model has a considerably long tail, which is capped in the image.

---

### An official American Thoracic Society workshop report: developing performance measures from clinical practice guidelines [^7fffb60b]. Annals of the American Thoracic Society (2014). Medium credibility.

Grading of Recommendations Assessment, Development, and Evaluation (GRADE) evidence quality assessment — Under GRADE, the quality of evidence is influenced by "study design, risk of bias, consistency, precision, directness, and publication bias", with randomized trials "presumed to be high in quality, unless compromised", and observational studies "considered to be low in quality by default but can be 'rated up' by demonstrating a large magnitude of effect, a dose–response, or results that would be even more compelling if plausible confounders are taken into account". Table factors for rating down include "Risk of bias", "Heterogeneity", "Imprecision", "Indirectness", and "Publication bias", and for rating up include "Magnitude of effect", "Dose–response", and "Plausible confounding effect", with example comments such as "Unclear allocation concealment, incomplete blinding, selective reporting of results, incomplete accounting of outcomes", "Inconsistent results across studies", "Confidence intervals range from substantial benefit to little or no benefit", and "Studies showing negative or conflicting results may exist but are missing from the literature".

---

### Globally scalable glacier mapping by deep learning matches expert delineation accuracy [^34013973]. Nature Communications (2025). High credibility.

As a measure of confidence, we employed the Shannon-entropy–based metric:whereis the class probability vector, C is the number of classes, anddenotes the logarithm base C.

Estimated confidences may misalign with the actual accuracies. Therefore, it is important to calibrate the derived confidence to ensure accurate and meaningful uncertainty estimates. To do so, we searched for a confidence calibration model R: [0, 1] → [0, 1] that satisfies the following expression as closely as possible for ∀ γ ∈ [0, 1] as N → ∞ :where γ is a confidence, N is the number of samples, y i is a one-hot-encoded reference label, is predicted class probabilities, andstands for the indicator function. The left side of Equation 4 represents the overall model accuracy for the predictions with the confidence level of γ, and the purpose of model calibration is to align the predicted confidence, with this accuracy. We modelled R with kernel ridge regression and fitted it on the confidence estimates and accuracies derived from the validation set.

The quality of the confidence estimates was evaluated with reliability diagrams, which depict bins of the predicted confidence versus the actual accuracy within these bins, and the expected calibration error given as:where B is the number of equally spaced confidence bins, Accuracy i is the overall accuracy in binis the mean predicted confidence in bin i, and b i is the fraction of samples in bin i.

---

### Conventional wisdom [^107ee125]. Nature Genetics (2010). Medium credibility.

Recent agreement on stable reference sequences for reporting human genetic variants now allows us to mandate the use of the allele naming conventions developed by the Human Genome Variation Society.

---

### Identifying domains of applicability of machine learning models for materials science [^72c4ba76]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ν that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by σ lin (x 1, x 2) ≡ −0.3 ≤ x 1 ≤ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: σ rbf ≡ −3.3 ≤ x 11 ≤ 3.1 ∧ x 2 ≤ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA σ poly (x 1, x 2) ≡ −3.5 ≤ x 2 ≤ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^c942b654]. Nature Communications (2025). High credibility.

Mean-independent heterogeneity measures

To create a truly mean-independent measure of heterogeneity the exact relationship between the mean and any measure of heterogeneity must be identified and removed. We derived the variance's mean-dependence relationship for lower-bounded, double-bounded, upper bounded, and unbounded variables that are approximately beta, gamma or normally distributed, where the lower and upper bounds can be any two real numbers (L and U, respectively). The nonlinear relationship between the standard deviation and the mean's distances from the boundaries (μ − L for the lower boundary, and U − μ for the upper boundary) can be made linear by squaring the standard deviation to give greater weight to the tails of the distribution, and retrieve the variance (Fig. 2 e, m). Thus, normalising the variance (σ 2) by the mean's distances from the boundaries yields a mean-independent measure of dispersion (δ). In its generalised formulation, the calculation of δ for any (bounded or unbounded) continuous variable approximated by the beta, gamma or normal distributions is given bywhereis an indicator function withbeing the set of all real numbers, m is the lower (L) or upper (U) bound of the variable, andif, otherwise. Note that any number raised to the zeroth power is equal to one, such that if the lower or upper bound does not exist or is not a real number (e.g.) the boundary has no influence on the dispersion nor δ. Thus, specific cases arise from this generalised formula as combinations of existing or non-existing bounds. For double-bounded variables, the variance's mean-dependence is influenced simultaneously by the mean's distance from the lower and upper bounds, [μ − L] [U − μ], yieldingFor lower-bounded variables, the variance's relationship with the mean is linear with the distance from the lower bound, μ − L (Fig. 2 e), and thusFor upper-bounded variables, the variance's relationship with the mean is linear with the distance from the upper bound, U − μ, givingAll of which arise as mean-independent measures of heterogeneity, as it can be demonstrated from the approximation of single-bounded and double-bounded variables respectively as gamma- and beta-distributed variables (Fig. 2 h, p). Also implicitly included in the generalised formulation (Equation (1)) is the variance as a mean-independent measure of dispersion for unbounded continuous variables, as demonstrated with the normal distributionHence, δ is valid for bounded and unbounded continuous variables, where the bounds can be determined conceptually or experimentally. Bounds should not be determined as the observed minimum or maximum from a small sample without conceptual or experimental support. Examples of potential boundaries include the proportion of land covered by forest (L = 0 and U = 1), animal mass (L = 0 g), beetle size limited by oxygen (L = 0 cm and U = 16 cm), and biochemical fish depth limits (L = 0 m and U = 8200 m) (see Supplementary Note 1 for more details) –. Also note that δ is not scale invariant for single-bounded and unbounded variables. Therefore, variables must have common units for comparison (e.g. the same currency, or the same unit of distance).

---

### Using the mean-to-sigma ratio as a measure of the improperness of binormal ROC curves [^70959dec]. Academic Radiology (2011). Low credibility.

Rationale and Objectives

A basic assumption for a meaningful diagnostic decision variable is that there is a monotone relationship between the decision variable and the likelihood of disease. This relationship, however, generally does not hold for the binormal model. As a result, receiver operating characteristic (ROC)-curve estimation based on the binormal model produces improper ROC curves that are not concave over the entire domain and cross the chance line. Although in practice the "improperness" is typically not noticeable, there are situations where the improperness is evident. Presently, standard statistical software does not provide diagnostics for assessing the magnitude of the improperness.

Materials and Methods

We show how the mean-to-sigma ratio can be a useful, easy-to-understand and easy-to-use measure for assessing the magnitude of the improperness of a binormal ROC curve by showing how it is related to the chance-line crossing. We suggest an improperness criterion based on the mean-to-sigma ratio.

Results

Using a real-data example, we illustrate how the mean-to-sigma ratio can be used to assess the improperness of binormal ROC curves, compare the binormal method with an alternative proper method, and describe uncertainty in a fitted ROC curve with respect to improperness.

Conclusions

By providing a quantitative and easily computable improperness measure, the mean-to-sigma ratio provides an easy way to identify improper binormal ROC curves and facilitates comparison of analysis strategies according to improperness categories in simulation and real-data studies.

---

### Deciphering highly similar multigene family transcripts from iso-seq data with isoCon [^f1336d9d]. Nature Communications (2018). Medium credibility.

Statistical test

We are given two candidate transcripts c and d and sets of reads X c and X d that have been assigned to them. We useto denote each read and let n be the number of reads in. We calculate pairwise alignments fromto d. Next, we construct a multi-alignment matrix A from these pairwise alignments in the same way as for the correction step (for details see Supplementary Note 1). Each entry in A corresponds to either a nucleotide or the gap character. Let V be the index of the columns of A where c and d do not agree. We refer to these positions as variant positions.

Let A i, j denote the character in column j of row i of A. Rowscorrespond to reads x i, while row n + 1 corresponds to c. For, we define a binary variable S i that is equal to 1 if and only if A i, j = A n +1, j for all j ∈ V. That is, S i is 1 if and only if read x i supports all the variants V, i.e. has the same characters as c at positions V in A. We make the following assumptions:
d, acting as the reference sequence in this test, is error-free.
A nucleotide in a read at a position that is not in V and differs from the corresponding nucleotide in d is due to a sequencing error. In other words, at position where c and d agree, then they cannot be both wrong.
The probabilities of an error at two different positions in a read are independent.
S i andare independent random variables for all.

---

### Accurately clustering biological sequences in linear time by relatedness sorting [^d1cda885]. Nature Communications (2024). High credibility.

The intuition underlying phase 2 is to pre-sort sequences within partitions into an order that is similar to where they would be located at the tips of a phylogenetic tree. That is, more similar sequences would be closer to each other after ordering. A straightforward way to arrange sequences in this manner is to imagine an unrooted phylogenetic tree with two leaves (Fig. 1E). A third leaf could be added at any point along the edge between the two existing leaves (Fig. 1F). Knowing the distances (d i) between the third sequence and the other two (d 1 and d 2) helps to decide whether the new leaf belongs closer to one leaf than the other. If two new leaves are both much closer to one of the leaves, it is possible they are also close to each other (Fig. 1G). Ordering sequences in this manner can be thought of as relatedness sorting because more related sequences are closer together after sorting.

Choosing two leaves randomly and calculating their difference in distance (d 1 − d 2) to each sequence results in a vector of relative distances (Fig. 1H). This vector (D 1) is then projected onto the axis of maximum variance shared with another relative distance vector (D 2) calculated from two other random leaves (Fig. 1I). Projecting in this manner is equivalent to performing principle components analysis on D 1 and D 2 to retain only the first principal component (D). Repeatedly projecting new vectors onto D results in more related sequences having more similar relative distances and, thus, moving closer together in the ordering of D. Sorting the distance vector therefore provides an ordering similar to that of the leaves along a phylogenetic tree. After several iterations, large gaps may appear in the relative distance vector that can be used to split the partition into smaller groups (Fig. 1J). Relatedness sorting is continued within each group until the ordering of sequences stabilizes within a tolerance or a maximum number of iterations (parameter B, by default 2,000) is reached (Fig. 1K).

---

### Humans rationally balance detailed and temporally abstract world models [^52a1bfff]. Communications Psychology (2025). Medium credibility.

Trial structure

All trials started from the same 'home' island. On each 'traversal' trial, the game presented two candidate islands, and the participant chose to sail from their home island to either the left or right island by pressing the corresponding arrow on the keyboard. The game would then zoom in on the selected island, and two merchant boats would appear on the left or right side of the island, which incorporated a 1s delay, preventing the participant from entering their second-stage choice until this transition completed. The participant was now provided with a second choice between the two boats, and upon selecting a boat with the left or right arrow, it was revealed whether the chosen merchant would provide treasure - either a stack of gold bars (+1) or a red 'X' (+0) was shown on the screen. Notably, the position of the islands as well as the boats was constant throughout the experiment, though boats were not visible on the screen until the corresponding island was first chosen. On a 'non-traversal' trial, one of the four boats, at random, was shown to the participant at their home island. The participant pressed 'up' to select the boat and reveal whether the merchant would provide treasure. Each boat appeared 50 times, with a randomized order across the 200 'non-traversal' trials. Probability of reward was identical to the probability of reward for that boat on the preceding 'traversal' trial. In effect, these trials served to provide information about how likely one boat was to have reward, independent of the participant's own policy.

---

### ACC appropriate use criteria methodology: 2018 update: a report of the American college of cardiology appropriate use criteria task force [^fe8a6ace]. Journal of the American College of Cardiology (2018). Medium credibility.

American College of Cardiology appropriate use criteria (AUC) terminology — intent and evolution emphasize that the current AUC "should be viewed as an evaluation of the evidence base and rational use of cardiovascular technologies in patient populations, rather than as a judgment of those ordering and delivering the use of such technologies". The original categories were intended to reflect a continuum of benefit and risk but were sometimes interpreted as absolutes for individuals, and terminology was amended to "more closely reflect clinical practice applications, including an expected distribution between each AUC category for every population, methods for documenting exceptions, and proper application to individual patients".

---

### Single nucleus multi-omics regulatory landscape of the murine pituitary [^2d20c3ea]. Nature Communications (2021). High credibility.

Statistics

For the assessment of sex difference in cell type proportions (Fig. 4e, f), we used a two-way analysis of variance (ANOVA) followed by Bonferroni multiple comparisons post-hoc test, with n = 3 biological replicates per cell type, F(11, 47) = 20.80 for snRNAseq, and F(8, 15) = 23.77 for snATACseq.

For analysis of LV sex differences by PLIER (Fig. 7b, Supplementary Fig. 21a), we used a two-tailed Wilcoxon rank-sum test with n = 48 (8 cell types, 6 biological samples), 1 degree of freedom, W = 490, p = 1.108e-05 for the snRNAseq data (Left panel) and with n = 12 (2 cell types, 6 biological samples), 1 degree of freedom, W = 36, p = 0.002165 for the lactotrope and somatotrope snATACseq data (Right panel). For evaluating the cell type differences of gonadotrope-specific LVs by PLIER (Fig. 7b, Supplementary Fig. 21b), we used a two-tailed Kruskal–Wallis analysis to test whether all samples originated from the same distribution. For LV2rna applied to snRNAseq data (Left panel), n = 48 (8 cell types, 6 biological samples), 7 degrees of freedom, χ 2 = 34.561, p = 1.352e-05. For LV2atac applied to snATACseq data (Center panel), n = 42 (7 cell types, 6 biological samples), 6 degrees of freedom, χ 2 = 37.34, p = 1.512e-06. For LV2rna applied to snATACseq data (Right panel), n = 42 (7 cell types, 6 biological samples), 6 degrees of freedom, χ 2 = 30.813, p = 2.752e-05.

---

### Utility-weighted modified rankin scale as primary outcome in stroke trials: a simulation study [^c6b59dde]. Stroke (2018). Low credibility.

Each of the 3 approaches yielded either a significant (P ≤ 0.05) or a nonsignificant treatment effect (P > 0.05, 2 sided). The power (or type 1 error in case of no treatment effect) of each statistical approach was estimated as the proportion of the 10 000 analyses, which resulted in a statistically significant treatment effect.

Associations were expressed as ORs or β with 95% confidence intervals (CIs), averaged over all simulations. All analyses were performed unadjusted and adjusted for the prespecified covariates identical to those mentioned above. Statistical analyses were performed with R software, version 3.3.2 (R Foundation for Statistical Computing, Vienna, Austria). Missing data on time from stroke to randomization (0.4%) and level of vessel occlusion (0.2%) was statistically imputed using simple imputation (replacement by mean or mode, as applicable).

---

### An additive gaussian process regression model for interpretable non-parametric analysis of longitudinal data [^1ceee4bf]. Nature Communications (2019). High credibility.

Model comparison using Bayesian bootstrap

After obtaining the leave-one-out predictive densities (Eqs. (25) or (27)) for a collection of models, we use Bayesian bootstrap to compare the involved models. Let us start with a simple case where two models M 1 and M 2 are compared. In the LOOCV setting, we compare the models by computing the average difference of their log-predictive densitieswhich measures the difference of the average prediction accuracy of the two models. If Eq. (28) is greater than 0, then model M 1 is better than M 2, otherwise model M 2 is better than M 1.

Comparison in Eq. (28) does not provide a probabilistic quantification of how much better one model is compared to the other. We thus approximate the relative probability of a model being better than another model using Bayesian bootstrap, which assumes y i only takes values from the observations y = (y 1, y 2. y N) T and has zero probability at all other values. In Bayesian bootstrap, the probabilities of the observation values follow the N -dimensional Dirichlet distribution Dir(1, 1. 1). More specifically, we bootstrap the samples N B times (b = 1, …, N B) and each time we get the same N observations y, with each observation taking weight w bi (i = 1, …, N) from the N -dimensional Dirichlet distribution. The N B bootstrap samples are then summarised to obtain the probability of M 1 being better than M 2 where δ {⋅} is the Heaviside step function and w bi is the bootstrap weight for the i th data point in the b th bootstrap iteration. We call the result of Eq. (29) LOOCV factor (LOOCVF).

---

### Optimized measurements of separations and angles between intra-molecular fluorescent markers [^5694d47e]. Nature Communications (2015). Medium credibility.

The MLE was done with Supplementary Software. The present method for simultaneous estimation of location and orientation of an isolated dipole we refer to as diPOLE for brevity. Thus, diPOLE uses MLE based on the true theoretical PSF of an isolated fixed dipole that is more or less in focus, but with the theoretical PSF well-approximated analytically to achieve a dramatic and necessary speedup of computation with essentially no cost in the form of degraded accuracy or precision (Supplementary Figs 10 and 11).

Using this methodology, we found that the effective focal depth in each of the two channels differed slightly; specifically, we found that the Cy3 and the Cy5 fluorophores, respectively, yielded defocuses of −30 ± 40 nm and −70 ± 25 nm (mean ± s.d.).

We compared the theoretical PSF (in its accurate analytical approximation) with the experimentally measured intensity distributions of fixed fluorophores in a number of ways. First, we directly compared measured PSFs pixel by pixel to each pixel's expected value (Fig. 2e, f). Discrepancies between measured and expected pixel values were rescaled with their theoretical root mean squared deviation as calculated from the theoretical covariance matrix and assuming photon shot noise to be the only cause of discrepancy. Second, we binned pixels with similar expected values for the light intensity they register. We calculated the mean value for the expected signals of all pixels in a given bin. Against this value, we plotted the mean value and s.e.m. of the actually recorded values for all pixels in the same bin (Fig. 2g, h). Repeating this for the full range of expected pixel values, we expect the resulting points to fall on a straight line through the origin with unit slope for accurate theoretical descriptions of the diffraction-limited patterns. This is clearly our case here. Finally, we assessed the distribution of the rescaled signal fluctuations (Fig. 2e, f). For sufficiently large signals (typically satisfied everywhere because of background fluorescence), such rescaled fluctuations should scatter as a standard normal distribution, for an accurate theoretical PSF (Fig. 2i, j).

---

### Estimating the success of re-identifications in incomplete datasets using generative models [^9c6329d0]. Nature Communications (2019). High credibility.

Theoretical and empirical population uniqueness

For n individuals x (1), x (2), …, x (n) drawn from X, the uniqueness Ξ X is the expected percentage of unique individuals. It can be estimated either (i) by computing the mean of individual uniqueness or (ii) by sampling a synthetic population of n individuals from the copula distribution. In the former case, we havewhere T x = [∃! i, x (i) = x] equals one if there exists a single individual i such as x (i) = x and zero otherwise. T x follows a binomial distribution B (p (x), n). Thereforeand

This requires iterating over all combinations of attributes, whose number grows exponentially as the number of attributes increases, and quickly becomes computationally intractable. The second method is therefore often more tractable and we use it to estimate population uniqueness in the paper.

For cumulative marginal distributions F 1, F 2, …, F d and copula correlation matrix Σ, the algorithm 1 (Supplementary Methods) samples n individuals from q (⋅|Σ,Ψ) using the latent copula distribution. From the n generated records (y (1), y (2), …, y (n)), we compute the empirical uniqueness

Individual likelihood of uniqueness and correctness

The probability distributioncan be computed by integrating over the latent copula density. Note that the marginal distributions X 1 to X d are discrete, causing the inversestoto have plateaus. When estimating p (x), we integrate over the latent copula distribution inside the hypercube:with ϕ Σ the density of a zero-mean multivariate normal (MVN) of correlation matrix Σ. Several methods have been proposed in the literature to estimate MVN rectangle probabilities. Genz and Bretz, proposed a randomized quasi Monte Carlo method which we use to estimate the discrete copula density.

The likelihood ξ x for an individual's record x to be unique in a population of n individuals can be derived from p X (X = x):

Similarly, the likelihoodfor an individual's record x to be correctly matched in a population of n individuals can be derived from. With, the number of potential false positives in the population, we have:

Note that, since records are independent, T follows a binomial distribution B (n − 1, p (x)).

We substitute the expression for ξ x in the last formula and obtain:

---

### Quality metrics for single-photon emission computed tomography myocardial perfusion imaging: an ASNC information statement [^00a42525]. Journal of Nuclear Cardiology (2023). High credibility.

Correlation coefficient calculation example — the page illustrates calculating correlation coefficient R using the CORREL function in Excel and notes that the displayed names are fictional, created from lists of the most common first and last names in the United States.

---

### 2016 SCCT / STR guidelines for coronary artery calcium scoring of noncontrast noncardiac chest CT scans: a report of the Society of Cardiovascular Computed Tomography and Society of Thoracic Radiology [^06afab31]. Journal of Cardiovascular Computed Tomography (2016). Medium credibility.

Ordinal coronary artery calcium (CAC) scoring on nongated CT — standard method: The presence of CAC in the left main, left anterior descending, left circumflex, and right coronary arteries is categorized as absent, mild, moderate, or severe and scored as 0, 1, 2, 3, respectively; CAC is classified as mild if less than 1/3 of the length of the entire artery contained calcification (CAC = 1), moderate if 1/3–2/3 (CAC = 2) and severe if more than 2/3 of the artery showed calcification (CAC = 3); the final score is the sum.

---

### Relatedness coefficients and their applications for triplets and quartets of genetic markers [^84bc206e]. G3 (2024). Medium credibility.

Identity disequilibrium and four markers

Between two diploid individuals, there are 15 patterns of gene identity. A pair of individuals can share two, three, or four markers, and at each level, allelic similarities can describe aspects of relatedness. After, for four marker genes and two individuals, there are nine condensed identity modes, denoted as Δ i (Fig. 2a). There are eight independent parameters of relatedness: three pairwise measures (F A, F B, R), two three-way measures (G A, G B), and three four-marker measures (F AB, R AB, H). At the highest level, the measures are much different as the four-marker measures F AB, R AB are covariances (not identities). The four-marker parameter, H, is the probability that four markers are identical. As well, the quantities must be defined as cumulants. Cumulants equal moments up to order three, but fourth-order moments do not equal fourth-order cumulants.

Fig. 2.
Two cases where four-gene modes of identity are used. a) The general case where each of the nine identity modes are inferred. b) A model to fit progeny pairs to mating system parameters. c) The inference of heritability in the field, where M is the marker and Q the quantitative trait.

The fourth central moment equals(note the covariances between second moments enter this expression) and the normalized cumulant(equivalent to H) equals the probability of identity of all four marker genes. While the variance is a measure of the spread of the distribution, kurtosis is a measure of the "peakedness" of the distribution of random variables, and infrequent extreme deviations contributing excessively to this statistic.

While applications of three and four gene measures are in their infancy, at least, the skew and kurtosis as measured by higher moments can help remove bias in DNA forensics caused by genotyping error.

In the four-marker model, many possibilities exist about attaching meaning to each of the i. One example is the progeny-pair model, where A and B are two progeny of the same mother plant (Fig. 2b). At another more abstract level, two of the genes can be markers and two are quantitative trait loci (Fig. 2c). If identity disequilibrium is present, the regression of phenotypic similarity (QTL) on estimated relationship (markers) gives an estimate of heritability "in the field".

---

### A generic non-invasive neuromotor interface for human-computer interaction [^6aa933c2]. Nature (2025). Excellent credibility.

Wrist

To evaluate continuous closed-loop control with the wrist decoder, users first completed a calibration procedure (rapid wrist flexions and extensions) to determine their minimum and maximum wrist angle velocities predicted by the decoder, v min, v max. Model outputs, v t, were then normalized to these values using a normalization function, η t, and scaled by a constant velocity gain, g v, and handedness normalization parameter, h. To estimate the cursor position, we integrated the velocity starting from x 0 = 0 at the start of the session to determine the unbounded horizontal cursor position, and the cursor position bounded by the edges of the workspace, x t :where Θ (⋅) is the Heaviside function. We used gain g v = 0.75 normalized pixels per radian (determined empirically to work well for comfortable closed-loop control) and set h = 1 if the sEMG wristband is on the right hand (so that wrist flexion/extension maps to left/right, respectively) and −1 if it is on the left hand (so that wrist flexion/extension maps to right/left, respectively). The second equation ensured that the horizontal cursor position, x t, was bounded to the left and right edges of the workspace, −1 and 1.

---

### Guidelines for the appropriate use of bedside general and cardiac ultrasonography in the evaluation of critically ill patients-part I: general ultrasonography [^aa118f36]. Critical Care Medicine (2015). Medium credibility.

Recommendation strength and direction — criteria and wording: a strong recommendation requires no disagreement (voters are ≥ 70%), consensus at least very good (voters with ≥ 80% at one integer), and a median outside the 4–6 middle zone (in 7–9 or 1–3); the two classes are "Strong with" if median score is = 7–9 and "Strong against" if = 1–3. A conditional/weak recommendation requires no disagreement (voters are ≥ 70%) and either "good or some consensus" with any median or a median of 4–6 with any degree of consensus; direction is "Weak against" if the middle 50% interquartile range = 1 to less than or equal to 3 and "Weak with" if = 4–9. No recommendation applies with disagreement (voters are ≥ 70%), no consensus, or a median in the middle region (4–6) with any degree of consensus. Wording by consensus level maps as: "Perfect consensus — Strong — Recommend: must/to be/will" and "Very good consensus — Strong — Recommend: should be/can", while "Good consensus" and "Some consensus" correspond to "Weak/conditional — Suggest: to do" and "Weak/conditional — Suggest: may do", and "No consensus — No — No recommendation was made regarding".

---

### Single photon emission computed tomography (SPECT) myocardial perfusion imaging guidelines: instrumentation, acquisition, processing, and interpretation [^79404bd3]. Journal of Nuclear Cardiology (2018). Medium credibility.

SPECT myocardial perfusion quantitation — normal database–referenced metrics and reversibility criteria are defined as follows: it is customary to use separate, normal databases specific to the imaging camera, the reconstruction method, the patient's gender, as well as the perfusion agent used. Defect extent may be quantitatively expressed as a percentage of the entire LV, or as a percentage of individual vascular territories, and defect severity may be quantitatively expressed as the number of standard deviations by which the average varies from the normal range for that particular segment; segmental scores can also be computed automatically, and an integrated measure of defect extent and severity (total perfusion defect) may provide valuable diagnostic and prognostic information. Quantitative analysis should only be used as an adjunct to, and not a substitute for, visual analysis. Reversibility of perfusion defects may be categorized qualitatively as partial or complete; on a semiquantitative scoring system it may be defined as a greater than or equal to 2-grade improvement or improvement to a score of 1, and on a quantitative polar plot reversibility is typically described as improvement to a pixel density of less than 2.5 standard deviations from the normal reference redistribution or resting database, although the number of pixels required is arbitrary. Reverse redistribution refers to segments with decreased or normal intensity on the initial set of images that show even less relative intensity on the delayed images.

---

### Spontaneous variability in gamma dynamics described by a damped harmonic oscillator driven by noise [^e861d0a6]. Nature Communications (2022). High credibility.

Fig. 4
Gamma-half-cycle amplitudes and durations are positively correlated in gamma-oscillatory epochs.

a For each dataset listed on the x-axis, the three bars show the correlation between the amplitude of a gamma half cycle and the duration of the same (center, red), previous (left, white), and next (right, white) gamma half cycle. On the right, this is shown for the average across all datasets. This was calculated for each time point across trials and averaged across time points for gamma-oscillatory epochs. The data used correspond to the period during the presentation of the visual stimulus. For individual datasets, a null distribution was produced by randomizing the order of duration values across trials, and the resulting means and 99.9% confidence intervals are shown as dots and vertical lines (numbers of trials = 278, 5740, 672, 1075, 672, 320, 142 for respective datasets). For the average across datasets, shown on the right, we performed a two-sided t -test and show the resulting confidence intervals as vertical lines on the observed mean (red bar: p < 6 × 10 −3, white bars for preceding cycle: p = 0.5, white bars for succeeding cycle: p = 0.35). In addition we performed a two-sided non-parametric permutation test (red bar: p < 0.05; white bars: p > 0.05). b Correlation between the amplitude of a gamma half-cycle and the duration of gamma half-cycles before and after it for three different datasets. Note that in monkey I, this is limited to ± 2 cycles, because the signal-to-noise ratio was lower, resulting in shorter gamma-oscillatory epochs. Importantly, all three example datasets show a central peak, despite the fact that they show different longer-term correlations. The gray lines and gray-shaded areas depict the means and 99.9% confidence regions, after randomizing the order of duration values across trials. c Same as a, but showing the correlations between residuals of the regression across adjacent amplitude triplets and the residuals of the regression across adjacent duration triplets (numbers of trials = 278, 5740, 672, 1075, 672, 320, 142 for respective datasets; red bar: p = 23 × 10 −4, two-sided t-test across datasets; p < 0.05, permutation test for individual datasets; white bars p = 0.066 and p = 0.97, respectively for preceding and succeeding cycles, two-sided t -test across datasets; p > 0.05, permutation test for individual datasets). a – c: n = 6 biologically independent experiments. Source data are provided in the Source Data file.

---

### Atlas for reporting PET myocardial perfusion imaging and myocardial blood flow in clinical practice: an information statement from the American Society of Nuclear Cardiology [^e4eb2fb8]. Journal of Nuclear Cardiology (2023). High credibility.

PET myocardial blood flow (MBF) and flow reserve reporting specifies numerical stress MBF in mL/min/g with Optional priority and responses that include global and left anterior descending (LAD), left circumflex (LCx), and right coronary artery (RCA) territory values; a Stress myocardial blood flow conclusion categorizes values as Preserved (> 2 mL/min/g), Mildly reduced (1.5–2 mL/min/g), or Severely reduced (< 1.5 mL/min/g); rest MBF is similarly recorded with Optional priority and territorial outputs, and the rest MBF conclusion notes that Resting PET MBF with N −13 ammonia in healthy volunteers ranges from 0.6 to 1.1 mL/min/g, with a weighted mean of 0.7 mL/min/g. For rubidium-82, 0.7 to 1.2, weighted mean 0.74 mL/min/g; myocardial flow reserve (MFR) is a Recommended numerical ratio of stress and rest MBF with global and territorial reporting.

---

### Guidelines for the appropriate use of bedside general and cardiac ultrasonography in the evaluation of critically ill patients-part I: general ultrasonography [^7247ea7c]. Critical Care Medicine (2015). Medium credibility.

Consensus categories for voting thresholds — used to grade agreement among respondents — are defined as: perfect consensus when all respondents agree on one number between 7 and 9; very good consensus when the median and middle 50% are at one integer or when 80% of respondents are within one integer of the median; good consensus when 50% of respondents are within one integer of the median or 80% are within two integers; some consensus when 50% are within two integers or 80% are within three integers; and no consensus for all other responses or any median with disagreement.

---

### Distinct place cell dynamics in CA1 and CA3 encode experience in new environments [^ebe7daa9]. Nature Communications (2021). High credibility.

Fig. 4
CA3 place fields exhibit higher stability across days than CA1 place fields and rapidly reappear upon re-exposure to the environment.

a Experimental setup. Top: mice are recorded for 20 min in N across 2 days. Bottom: example field of view from one imaging plane from CA1 and CA3 across 2 days showing the same cells. scale bar = 50 µm. b Top, example mouse behavior showing track position for the first lap in F (blue), N day 1 (red), and N day 2 (purple). Bottom, summary across all mice of first lap velocity in F, N day 1, and N day 2. Lap velocities were normalized to the mean velocity in F in each mouse (n = 11 mice). Bootstrapped mean difference (∆) between F and N day 1, and F and N day 2. Note that lap velocity is only different on N day 1. c Left: mean place fields in N on day 1 sorted by track position. Middle: mean activity of the same neurons on the left on day 2 in N. Right: mean place fields in N on day 2 sorted by track position. d Histograms of Pearson's correlation coefficient of average activity map across days in N. Spatial correlations > 0.5 were considered stable place fields and less than 0.5 unstable. CA1 n = 409, CA3 n = 113 neurons with PFs in both sessions. e Fraction of stable place fields across days per mouse in CA1 and CA3. Wilcoxon rank-sum test, two-sided. ✱ p < 0.05, P = 0.0476. f Cumulative fraction plots of Pearson's correlation coefficient in N across days in CA1 and CA3 from the same data shown in c. Wilcoxon rank-sum test, two-sided. ✱✱ p < 0.001, P = 6.9 × 10 −4. g Example of a stable place field (left), an unstable place field with a newly formed place field on day 2 (middle), and a newly formed place field on day 2 (right). Place field transients showed lap-by-lap for all laps in N on day 1 and day 2 from the same three cells. h Histograms of place field onset laps in N on day 2 for stable place fields (left), and newly formed place fields (middle) in CA1 (top) and CA3 (bottom). Right: cumulative fraction plots of histogram data. Wilcoxon rank-sum test, two-sided. ✱✱ p < 0.001, CA1, P = 1.8 × 10 −5, CA3, P = 6.7 × 10 −10. CA1 stable n = 251 PFs, newly formed, n = 333 PFs, CA3 stable n = 72 PFs, newly formed, n = 94 PFs.

---

### Recommendations for cardiac chamber quantification by echocardiography in adults: an update from the American Society of Echocardiography and the European association of cardiovascular imaging [^48287b30]. Journal of the American Society of Echocardiography (2015). Medium credibility.

Supplemental Table 6 — Normal left ventricular (LV) strain values by vendor/software report sample size (n), mean, SD, and LLN as follows: Meta-analysis n 2597 with mean −19.7%; EchoPAC BT 12 n 247 mean −21.5% SD 2.0% LLN −18%; EchoPAC BT 12 n 207 mean −21.2% SD 1.6% LLN −18%; EchoPAC BT 12 n 131 mean −21.2% SD 2.4% LLN −17%; EchoPAC 110.1.3 n 333 mean −21.3% SD 2.1% LLN −17%; QLAB 7.1 n 330 mean −18.9% SD 2.5% LLN −14%; Ultra Extend n 337 mean −19.9% SD 2.4% LLN −15%; VVI 1.0 n 116 mean −19.8% SD 4.6% LLN −11%; VVI 1.0 n 82 mean −17.3% SD 2.3% LLN −13%; Mylab 50 n 30 mean −19.5% SD 3.1% LLN −13%.

---

### Assumptions of expected benefits in randomized phase III trials evaluating systemic treatments for cancer [^a4ba632e]. Journal of the National Cancer Institute (2012). Low credibility.

Background

In designing phase III randomized clinical trials (RCTs), the expected magnitude of the benefit of the experimental therapy (δ) determines the number of patients required and the number of person-years of follow-up. We conducted a systematic review to evaluate how reliably δ approximates the observed benefit (B) in RCTs that evaluated cancer treatment.

Methods

RCTs evaluating systemic therapy in adult cancer patients published in 10 journals from January 1, 2005, through December 31, 2009, were identified. Data were extracted from each publication independently by two investigators. The related-samples Sign test was used to determine whether the median difference between δ and B was statistically significant in different study subsets and was two-sided.

Results

A total of 253 RCTs met the eligibility criteria and were included in the analysis. Regardless of whether benefit was defined as proportional change (median difference between δ and B = -13.0%, 95% confidence interval [CI] = -21.0% to -8.0%), absolute change (median difference between δ and B = -8.0%, 95% CI = -9.9% to -5.1%), or median increase in a time-to-event endpoint (median difference between δ and B = -1.4 months, 95% CI = -2.1 to -0.8 months), δ was consistently and statistically significantly larger than B (P < .001, for each, respectively). This relationship between δ and B was independent of year of publication, industry funding, management by cooperative trial groups, type of control arm, type of experimental arm, disease site, adjuvant treatment, or treatment for advanced disease, and likely contributed to the high proportion of negative RCTs (158 [62.5%] of 253 studies).

Conclusions

Investigators consistently make overly optimistic assumptions regarding treatment benefits when designing RCTs. Attempts to reduce the number of negative RCTs should focus on more realistic estimations of δ. Increased use of interim analyses, certain adaptive trial designs, and better biological characterization of patients are potential ways of mitigating this problem.

---

### APSA new technology committee: innovative therapy white paper-01 / 2014 [^ff6b9e34]. APSA (2016). Medium credibility.

Privileges granted for an innovative procedure — early case selection, review, and competency specify to undertake simple cases before complex ones and, if the technology is adaptable to both simple and complex operations, then simple cases should be scheduled first to acquire experience; a minimum number of cases (at least 5) at the beginning of the experience should be subject to focused review considering procedure-specific outcomes including complications, blood loss, and conversion to a traditional approach; if performance does not meet a competency threshold the team has the option of obtaining additional training, then undergoing re-proctoring and followup competency evaluation, and ultimately a failure to achieve competency targets should result in a withdrawal of privileges for that procedure; the learning curve is defined as the number of procedures required for a surgeon or team to achieve optimal performance as measured by procedural time and patient outcomes and varies substantially between procedures.

---

### Dynamical regimes of diffusion models [^a7936fcc]. Nature Communications (2024). High credibility.

Speciation time

In order to show the existence of regimes I and II, and compute the speciation time, we focus on the following protocol which consists of cloning trajectories. We consider a backward trajectory starting at time t f ≫ 1 from a point x f drawn from a random Gaussian distribution where all components are independent with mean zero and unit variance. This trajectory evolves backward in time, through the backward process until time t < t f. At this time the trajectory has reached the point y (t), at which cloning takes place. One generates for τ < t two clones, starting from the same x 1 (t) = x 2 (t) = y (t), and evolving as independent trajectories x 1 (τ) and x 2 (τ), i.e. with independent thermal noises. We compute the probability ϕ (t) that the two trajectories ending in x 1 (0) and x 2 (0) are in the same class. Defining P (x 1, 0∣ y, t) as the probability that the backward process ends in x 1, given that it was in y at time t, the joint probability of finding the trajectory in y at time t and the two clones in x 1 and x 2 at time 0 is obtained as Q (x 1, x 2, y, t) = P (x 1, 0∣ y, t) P (x 2, 0∣ y, t) P (y, t). Then ϕ (t) is the integral of Q over x 1, x 2, y with the constraint (x 1 ⋅ m)(x 2 ⋅ m) > 0. This simplifies into a one-dimensional integral (see SI Appendix):where G (y, u, v) is a Gaussian probability density function for the real variable y, with mean u and variance v, and, Γ t = Δ t + σ 2 e −2 t. The probability ϕ (t) that the two clones end up in the same cluster is a decreasing function of t, going from ϕ (0) = 1 to ϕ (∞) = 1/2. In the large d limit, the scaling variable controlling the change of ϕ iswhich can be rewritten asby using. This explicitly shows that speciation takes place at the time scale t S on a window of time of order one. As detailed in SI Appendix, this expression for t S coincides with the one obtained from the general criterion (5). We show in Fig. 2 the analytical result from (6) and direct numerical results obtained for increasingly larger dimensions. This comparison shows that our analysis is accurate already for moderately large dimensions. In the limit of infinite d, the analytical curve in Fig. 2 suddenly jumps from one to zero at t / t S = 1, corresponding to a symmetry-breaking phase transition (or a threshold phenomenon) on the time scale t S. In the numerics, following finite size scaling theory, we have defined the speciation time as the crossing point of the curves for different d, which corresponds approximatively to ϕ (t S) = 0.775 and indeed is of the orderfor d → ∞. As it happens in mean-field theories of phase transitions, the large dimensional limit allows to obtain a useful limiting process. In our case, this leads to a full characterization of the asymptotic backward dynamics. At its beginning, i.e. in regime I, the overlap with the centers of the Gaussian model, ± m ⋅ x (t), is of order. Defining, one can obtain a closed stochastic Langevin equation on q in a potential V (q, t) (see SI Appendix), where η (t) is square root of two times a Brownian motion, andAt large d, this potential is quadratic at times, and it develops a double well structure, with a very large barrier, when. The trajectories of q (t) are subjected to a force that drives them toward plus and minus infinity. The barrier between positive and negative values of q becomes so large that trajectories commit to a definite sign of q: this is how the symmetry breaking takes place dynamically at the time scale t S, in agreement with the previous cloning results. Regime II corresponds to the scaling limit q → ∞, where m ⋅ x (t) becomes of order d. In this regime, the rescaled overlap m ⋅ x (t)/ d concentrates, and its sign depends on the set of trajectories one is focusing on. Moreover, the stochastic dynamics of the x i correspond to the backward dynamics for a single Gaussian centered in ± m. This shows that the dynamics generalizes, see SI Appendix (and alsofor similar results).

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^d6d8fcbc]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Spirometry reporting specifies that numerical values are given for the FEV1, the FVC, and the FEV1/FVC ratio; the latter should be reported as a decimal fraction and the space for percent predicted value left blank, and if bronchodilators are given the LLN column need not be repeated with absolute and percent change given only for FEV1 and FVC. Other numerical values such as the forced inspiratory flow at 75% of FVC (FEF75%) and FEF25–75% are not recommended for routine use. Graph requirements include that for the volume–time curve the volume scale should be at least 10 mm/L, the time scale at least 20 mm/s, and 1 second prior to the start of expiration should be displayed; on the flow–volume plot the flow display should be at least 5 l/min/L/s, and the ratio of flow to volume should be 2 L/s to 1 L, and linear and log scales where values are plotted as z-scores relative to the predicted value (z = 0) give an intuitive sense of severity.

---

### On the role of initial velocities in pair dispersion in a microfluidic chaotic flow [^cb75a5d8]. Nature Communications (2017). Medium credibility.

Failure of the linear flow assumption

This raises questions regarding the extent to which elastic turbulence can be regarded as globally smooth, particularly in the presence of boundaries and mean flow. A velocity field consistent with linear flow behaviour would exhibitfor the second order structure function of the longitudinal velocity, where u denotes the relative velocity and u l = u ⋅ R / R; e.g. numerical simulations (Figs. 1 and 6 of ref.). In our flow, clear deviations from linearity are evident already at separations beyond 12 μm, less than 10% of the width and depth of the microfluidic channel, as can be learnt from Fig. 3; a comparison to previous experimental results is drawn in Supplementary Note 2. The inset of Fig. 3 presents the mean squared relative velocities without rescaling; we shall return to these profiles soon.

Fig. 3
Initial relative velocity dependence on the separation distance. The second moments of the relative velocity(blue left - triangles) and the separation velocity(green right-triangle), where u l = u ⋅ R / R, are plotted in the inset (right axis values are half the left ones) as function of the initial separation distance R 0; both ensemble averages are taken at the initial time t 0, when the pairs separation distance is closest to R 0. Rescaling these data by the squared initial separationreveals the deviation from the commonly applied assumption of linear velocity field, as presented on a logarithmic scale in the main plot (right axis values are one order of magnitude smaller than the left ones). Had 〈 u 2 〉 R ∝ R 2 held, the rescaled curves would have remained constant; this is clearly not the case. Indeed, thedata level off as R 0 approaches the smaller distances, providing supporting evidence for the linearity of u l with R at scales smaller than 12 μm. However, this does not hold beyond a tenth of the channel depth. A linear flow regime is not supported by the rescaled relative velocity data, which values keep increasing even for the smallest R 0 values explored here. Further note thatand(inset) are empirical estimators for the second order structure functions of the velocity and the longitudinal velocity, correspondingly; the former is the coefficient of the quadratic term in Eq. (2). The error bars in the inset (smaller than the marker) indicate the margin of error based on a 95% confidence interval

---

### 2020 ACC / AHA clinical performance and quality measures for adults with heart failure: a report of the American college of cardiology / American Heart Association task force on performance measures [^36cec02b]. Circulation: Cardiovascular Quality and Outcomes (2020). High credibility.

ACC/AHA level of evidence — definitions and pairing guidance: Level A includes "High-quality evidence† from more than 1 RCT", "Meta-analyses of high-quality RCTs", and "One or more RCTs corroborated by high-quality registry studies". Level B-R (Randomized) includes "Moderate-quality evidence† from 1 or more RCTs" and "Meta-analyses of moderate-quality RCTs". Level B-NR (Nonrandomized) includes "Moderate-quality evidence† from 1 or more well-designed, well-executed nonrandomized studies, observational studies, or registry studies" and "Meta-analyses of such studies". Level C-LD (Limited Data) includes "Randomized or nonrandomized observational or registry studies with limitations of design or execution", "Meta-analyses of such studies", and "Physiological or mechanistic studies in human subjects". Level C-EO (Expert Opinion) is "Consensus of expert opinion based on clinical experience". The framework states "COR and LOE are determined independently (any COR may be paired with any LOE)" and clarifies "A recommendation with LOE C does not imply that the recommendation is weak… Although RCTs are unavailable, there may be a very clear clinical consensus that a particular test or therapy is useful or effective". Additional notes specify, "For comparative-effectiveness recommendations (COR 1 and 2a; LOE A and B only), studies that support the use of comparator verbs should involve direct comparisons of the treatments or strategies being evaluated" and "The method of assessing quality is evolving, including the application of standardized, widely-used, and preferably validated evidence grading tools; and for systematic reviews, the incorporation of an Evidence Review Committee".

---

### Optimal use of left ventriculography at the time of cardiac catheterization: a consensus statement from the Society for Cardiovascular Angiography and Interventions [^95a53fd4]. Catheterization and Cardiovascular Interventions (2015). Medium credibility.

Radionuclide ventriculography — measurement characteristics and accuracy: Left ventricular counts are gated to the R-R interval and summated over a period (typically 5–10 min) or in a single pass to derive measurements, and because measurements are independent of geometrical assumptions the method is highly accurate and very reproducible with inter-observer variability of < 5%; it can be used to examine both atria and ventricles and the principal disadvantage is radiation exposure.

---

### 2025 ACC / AHA clinical practice guidelines core principles and development process: a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^16b9b9fd]. Journal of the American College of Cardiology (2025). High credibility.

ACC/AHA level (quality) of evidence definitions and related guidance are specified as follows: Level A includes "High-quality evidence* from more than 1 RCT", "Meta-analyses of high-quality RCTs", and "One or more RCTs corroborated by high-quality registry studies". Level B-R (Randomized) includes "Moderate-quality evidence‡ from 1 or more RCTs" and "Meta-analyses of moderate-quality RCTs". Level B-NR (Nonrandomized) includes "Moderate-quality evidence‡ from 1 or more well-designed, well-executed nonrandomized studies, observational studies, or registry studies" and "Meta-analyses of such studies". Level C-LD (Limited Data) includes "Randomized or nonrandomized observational or registry studies with limitations of design or execution", "Meta-analyses of such studies", and "Physiological or mechanistic studies in human subjects". Level C-EO (Expert Opinion) is "Consensus of expert opinion based on clinical experience". Additionally, "COR and LOE are determined independently (any COR may be paired with any LOE)", and "A recommendation with LOE C does not imply that the recommendation is weak… Although RCTs are unavailable, there may be a very clear clinical consensus that a particular test or therapy is useful or effective". The guidance further specifies " The outcome or result of the intervention should be specified (an improved clinical outcome or increased diagnostic accuracy or incremental prognostic information)", adds "† For comparative-effectiveness recommendations (COR 1 and 2a; LOE A and B only), studies that support the use of comparator verbs should involve direct comparisons of the treatments or strategies being evaluated", and notes "‡ The method of assessing quality is evolving, including the application of standardized, widely-used, and preferably validated evidence grading tools; and for systematic reviews, the incorporation of an Evidence Review Committee".

---

### Measurements of the gravitational constant using two independent methods [^a70ae1bd]. Nature (2018). Excellent credibility.

The Newtonian gravitational constant, G, is one of the most fundamental constants of nature, but we still do not have an accurate value for it. Despite two centuries of experimental effort, the value of G remains the least precisely known of the fundamental constants. A discrepancy of up to 0.05 per cent in recent determinations of G suggests that there may be undiscovered systematic errors in the various existing methods. One way to resolve this issue is to measure G using a number of methods that are unlikely to involve the same systematic effects. Here we report two independent determinations of G using torsion pendulum experiments with the time-of-swing method and the angular-acceleration-feedback method. We obtain G values of 6.674184×10–11 and 6.674484×10–11 cubic metres per kilogram per second squared, with relative standard uncertainties of 11.64 and 11.61 parts per million, respectively. These values have the smallest uncertainties reported until now, and both agree with the latest recommended value within two standard deviations.

---

### Outcome and safety 90 days after combined airway valve treatment of the right upper and middle lobes in patients with severe pulmonary emphysema [^ef5a0021]. Respiratory Research (2025). Medium credibility.

Statistical analysis

Data were analysed with R 4.3.1 (R Core Team. R: A language and environment for statistical computing. R Foundation for Statistical Computing. Vienna Austria). Data were analysed for normality using Shapiro-Wilk test and reported as median [25–75th percentile] or mean ± standard deviation otherwise. Descriptive statistical analysis was performed using paired or unpaired Wilcoxon rank sum test, Student's t-test or Pearson's product-moment correlation as appropriate. Due to the nature of the retrospective analysis, the number of measurements (n) varied according to the availability of source data and n is reported individually. P -values less than 0.05 were considered statistically significant.

---

### Constructing future behavior in the hippocampal formation through composition and replay [^09efd61c]. Nature Neuroscience (2025). High credibility.

Discrete and continuous worlds

We implement agents in continuous and discrete environments. Although similar in principle, these two types of agents and environments are slightly different in their practical implementation. The discrete agent behaves on a graph, whereas the continuous agent behaves in a two-dimensional (2D) Euclidean space. Both discrete and continuous settings are deterministic Markov decision processes (MDP).

The discrete environments are defined by a set of locations and actions as in a deterministic MDP. All results here assume rectangular square grid worlds, with actions 'north', 'east', 'south' and 'west'. We generate environments by adding walls and rewards on top of these regular grids. We represent rewards with a population of object vector cells, where each cell fires at a specific vector in relation to the reward, that is, a cell that fires 1 step to the East and so on. We represent each wall with two populations of object vector cells — each vector population centered on one of the wall ends. In more detail, we calculate the object vector population activity at location x relative to object o by concatenating the vectors of one-hot encoded distance from x to o along each action, with −1 distance for actions in the opposite direction. For example, for an object, one step east and three steps south from x, the representation is concatenate(onehot(−1), onehot(1), onehot(3), onehot(−1)). Thus, at any location, there are four vector cells active in the whole population — one for each action. In square grid worlds, this representation has redundancy because east is the opposite of west and north the opposite of south, but this setup allows for accommodating any type of nongrid graph too.

In continuous environments, locations become continuous (x, y) coordinates and actions are steps in a continuous direction. We place walls and rewards within a square 1 m × 1 m arena, at random locations and orientations. Again, we represent rewards by a single population of object vector cells and walls by two populations, one centered on each wall end. A single object vector cell is defined by a 2D Gaussian firing field, tuned to a specific distance and direction from its reference object; the firing fields of the full object vector cell population are distributed on a square grid centered on the object. We thus calculate the object vector population activity at location x relative to object o by evaluating the population of Gaussians centered on o at x.

---

### Isolation by distance in populations with power-law dispersal [^abcefda9]. G3 (2023). Medium credibility.

Distant pairs

For distant samples, we expect substantial isolation by distance. For the pair to coalesce, their lineages must approach within δ of each other. The most likely way for this to happen before a mutation occurs is for one lineage to cover the distance in a long jump. In one dimension, the rate of such jumps is proportional to D α x − α −1 δ: a probability density proportional to D α x − α −1 for a jump to have size ≈ x, multiplied by a target zone of size δ. In two dimensions, the rate is ∝ D α x − α −2 δ 2: there is an extra factor ∝1/ x because the jump not only has to have the right distance but also the right direction, and now it must hit a target with area δ 2. (Here we are focusing just on scaling behavior and neglectingnumerical factors like 2 and π.) Combining the two expressions, the rate of jumps bringing the two lineages together is ∝ D α x − α − d δ d, where d = 1 or 2 is the spatial dimension. The probability that such a jump occurs in the time ∝ 1/ μ before the lineages mutate is therefore approximately ∝ D α x − α − d δ d / μ. The lineages must then coalesce within their neighborhood of ∝ δ d ρ individuals before they mutate. If mutation is frequent compared with coalescence (1/(ρδ d) ≫ μ), this occurs with probability ∝1/ (μρδ d) ≪ 1. We therefore expect that the probability of identity is ψ (x) ∝ D α x − α − d δ d / μ /(μρδ d) = D α x − α − d /(μ 2 ρ), i.e. that there is a power-law dependence of identity on distance, with the same exponent as the dispersal probability density. We can generalize this expression slightly to also include the case in which mutation is rare compared with local coalescence by including a correction factor that depends on the probability of identity ψ (0) for a co-located pair.