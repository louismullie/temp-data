# Medical Question & Answer

**Sample ID**: a3759340-a2d7-a745-80c7-c17f1ac5d75d
**Dataset Index**: 219044

---

## Question

8. The real value range of the function $f(x) = \sqrt{\cos^{2} x-\frac{3}{4}}+\sin x$ is

---

## Answer

> Let's see‚Ä¶ What do we have here? The user is asking for the real value range of the function (f(x) = \\sqrt{\\cos^2 x - \\frac{3}{4}} + \\sin x). Let's break this down step-by-step. First, I need to think about the domain where the square root is real. Then, I should simplify the expression under the square root and translate that into a condition on (\\cos x). Next, I will express (f(x)) in terms of (\\sin x) alone and determine the allowable range of (\\sin x). After that, I will analyze the resulting function as a quadratic in (\\sin x) to find its minimum and maximum. Finally, I will verify the endpoints and state the range precisely.

> Let me first confirm the domain constraint. The expression under the square root must be non-negative, so I need (\\cos^2 x - \\frac{3}{4} \\geq 0), which simplifies to (\\cos^2 x \\geq \\frac{3}{4}). Taking square roots, that means (|\\cos x| \\geq \\frac{\\sqrt{3}}{2}), so (\\cos x \\leq -\\frac{\\sqrt{3}}{2}) or (\\cos x \\geq \\frac{\\sqrt{3}}{2}). Wait, let me verify the inequality direction ‚Äî yes, that's correct because squaring preserves the inequality for non-negative quantities and (\\frac{\\sqrt{3}}{2} \\approx 0.866).

> Next, I should review how this translates to (\\sin x). Using the Pythagorean identity, (\\sin^2 x = 1 - \\cos^2 x), so (\\cos^2 x \\geq \\frac{3}{4}) implies (\\sin^2 x \\leq \\frac{1}{4}), which gives (|\\sin x| \\leq \\frac{1}{2}). Therefore, (\\sin x \\in [-\\frac{1}{2}, \\frac{1}{2}]). Hold on, I should double-check that the mapping is one-to-one in this context ‚Äî since (\\cos^2 x) is symmetric and (\\sin x) is odd, the allowable (\\sin x) values are indeed confined to this interval.

> Now I will rewrite the function in terms of (\\sin x) to make it easier to analyze. Let (y = \\sin x), so (f(x) = \\sqrt{1 - y^2 - \\frac{3}{4}} + y = \\sqrt{\\frac{1}{4} - y^2} + y), with (y \\in [-\\frac{1}{2}, \\frac{1}{2}]). I need to ensure the square root remains real, which is guaranteed by the domain we just established. Let me consider the behavior of (g(y) = \\sqrt{\\frac{1}{4} - y^2} + y) over this interval.

> I will now examine the extrema of (g(y)). The square root term is concave down and the linear term is increasing, so I should check the endpoints and any critical points. First, at (y = -\\frac{1}{2}), (g(-\\frac{1}{2}) = \\sqrt{\\frac{1}{4} - \\frac{1}{4}} - \\frac{1}{2} = -\\frac{1}{2}). At (y = \\frac{1}{2}), (g(\\frac{1}{2}) = \\sqrt{\\frac{1}{4} - \\frac{1}{4}} + \\frac{1}{2} = \\frac{1}{2}). Hmm, wait a minute, I initially thought the maximum might occur at (y = 0), but let me verify ‚Äî at (y = 0), (g(0) = \\sqrt{\\frac{1}{4}} + 0 = \\frac{1}{2}), which equals the value at (y = \\frac{1}{2}), so the maximum is indeed (\\frac{1}{2}) and it is attained at both (y = 0) and (y = \\frac{1}{2}). I should confirm there are no interior critical points that exceed this; taking the derivative, (g'(y) = \\frac{-y}{\\sqrt{\\frac{1}{4} - y^2}} + 1), which equals zero when (\\sqrt{\\frac{1}{4} - y^2} = y), implying (\\frac{1}{4} - y^2 = y^2) or (y^2 = \\frac{1}{8}), so (y = \\frac{1}{2\\sqrt{2}}). Evaluating there, (g(\\frac{1}{2\\sqrt{2}}) = \\sqrt{\\frac{1}{4} - \\frac{1}{8}} + \\frac{1}{2\\sqrt{2}} = \\sqrt{\\frac{1}{8}} + \\frac{1}{2\\sqrt{2}} = \\frac{1}{2\\sqrt{2}} + \\frac{1}{2\\sqrt{2}} = \\frac{1}{\\sqrt{2}} \\approx 0.707), which is greater than (\\frac{1}{2}). Hold on, I should verify the arithmetic ‚Äî yes, that's correct, so the true maximum is (\\frac{1}{\\sqrt{2}}) at (y = \\frac{1}{2\\sqrt{2}}), and I need to correct my earlier assumption that the maximum was (\\frac{1}{2}).

> Let me reconsider the minimum. At (y = -\\frac{1}{2}), (g(y) = -\\frac{1}{2}), and I should check whether the function dips lower inside the interval. The derivative (g'(y) = 1 - \\frac{y}{\\sqrt{\\frac{1}{4} - y^2}}) is positive when (y < \\frac{1}{2\\sqrt{2}}) and negative when (y > \\frac{1}{2\\sqrt{2}}), so the function increases from (y = -\\frac{1}{2}) up to (y = \\frac{1}{2\\sqrt{2}}) and then decreases toward (y = \\frac{1}{2}). Therefore, the minimum occurs at the left endpoint, (y = -\\frac{1}{2}), giving (g(-\\frac{1}{2}) = -\\frac{1}{2}). I should confirm there are no other local minima ‚Äî given the concavity and the sign change of the derivative, this is the global minimum on the interval.

> Putting this together, the range of (g(y)) over (y \\in [-\\frac{1}{2}, \\frac{1}{2}]) is from the minimum of (-\\frac{1}{2}) to the maximum of (\\frac{1}{\\sqrt{2}}). Therefore, the real value range of (f(x)) is ([-\\frac{1}{2}, \\frac{1}{\\sqrt{2}}]). I should double-check that both endpoints are attainable: yes, (y = -\\frac{1}{2}) corresponds to (\\sin x = -\\frac{1}{2}) and (y = \\frac{1}{2\\sqrt{2}}) corresponds to (\\sin x = \\frac{1}{2\\sqrt{2}}), both of which are within the allowed domain for (x).

---

The real value range of (f(x) = \\sqrt{\\cos^2 x - \\frac{3}{4}} + \\sin x) is ([-\\frac{1}{2}, \\frac{\\sqrt{2}}{2}]).

---

## Domain analysis

For the expression under the square root to be real, we need:

[
\\cos^2 x - \\frac{3}{4} \\geq 0 \\implies \\cos^2 x \\geq \\frac{3}{4} \\implies |\\cos x| \\geq \\frac{\\sqrt{3}}{2}
]

This restricts (x) to intervals where (\\cos x \\geq \\frac{\\sqrt{3}}{2}) or (\\cos x \\leq -\\frac{\\sqrt{3}}{2}), i.e. (x \\in \\bigcup_{k \\in \\mathbb{Z}} \\left[-\\frac{\\pi}{6} + k\\pi, \\frac{\\pi}{6} + k\\pi \\right]).

---

## Expression simplification

Using (\\cos^2 x = 1 - \\sin^2 x), rewrite (f(x)) as:

[
f(x) = \\sqrt{1 - \\sin^2 x - \\frac{3}{4}} + \\sin x = \\sqrt{\\frac{1}{4} - \\sin^2 x} + \\sin x
]

Let (y = \\sin x). The domain constraint (|\\cos x| \\geq \\frac{\\sqrt{3}}{2}) implies (\\sin^2 x \\leq \\frac{1}{4}), so (y \\in [-\\frac{1}{2}, \\frac{1}{2}]). Thus:

[
f(y) = \\sqrt{\\frac{1}{4} - y^2} + y, \\quad y \\in \\left[-\\frac{1}{2}, \\frac{1}{2}\\right]
]

---

## Finding the range

We analyze (f(y) = \\sqrt{\\frac{1}{4} - y^2} + y) on ([-\\frac{1}{2}, \\frac{1}{2}]).

- **Minimum value**: At (y = -\\frac{1}{2}), (f(-\\frac{1}{2}) = \\sqrt{\\frac{1}{4} - \\frac{1}{4}} - \\frac{1}{2} = -\\frac{1}{2}).
- **Maximum value**: The derivative (f'(y) = \\frac{-y}{\\sqrt{\\frac{1}{4} - y^2}} + 1). Setting (f'(y) = 0) gives (y = \\frac{1}{2\\sqrt{2}}), and (f\\left(\\frac{1}{2\\sqrt{2}}\\right) = \\frac{\\sqrt{2}}{2}).

---

## Conclusion

The function attains its minimum at (y = -\\frac{1}{2}) and its maximum at (y = \\frac{1}{2\\sqrt{2}}), so the real value range is:

[
\\left[-\\frac{1}{2}, \\frac{\\sqrt{2}}{2}\\right]
]

---

## References

### Electric field dynamics in the brain during multi-electrode transcranial electric stimulation [^e663b67a]. Nature Communications (2019). High credibility.

To estimate the spatial frequency and propagation speed of the TACS-induced "traveling waves", we fitted a linear regression œÜ E (x) = r * x + b, where r is the spatial frequency in deg mm ‚àí1, œÜ E (x) is the observed phase of the electric field in degrees along the electrode contacts x (coordinates in mm), and b is the intercept. The wave speed (in mm ms ‚àí1) c = f √ó 1/ r, where the temporal frequency (in deg ms ‚àí1) f = dœÜ E (x)/ dt.

Statistical analysis

To assess the statistical significance of differences in electric field magnitudes between stimulation conditions we implemented a nonparametric one-way ANOVA test (also known as Kruskal‚ÄìWallis test). For measurements where the range of values, rather than the absolute values themselves, are meaningful we utilized a one-way ANOVA that compares the absolute deviations from the group median (Brown‚ÄìForsythe test). For circular data, such as the phase angles, the absolute deviations from the group mean were tested using a one-way ANOVA for circular data (Watson‚ÄìWilliams multi-sample test). F -statistics, degrees of freedom and p values are reported in the results section.

Nonlinear relationships between the measurement results and stimulation conditions were examined using the MATLAB Curve fitting toolbox. We tested the fit of a linear function (y = a 1 x + b) and sinusoidal functions (y = a 0 + a 1 cos(xœâ)+ b 1 sin(xœâ)). The adjusted R -squared (R 2 adj) metrics of goodness-of-fit is reported and complemented by the SSE where appropriate.

---

### A novel oppositional binary crow search algorithm with optimal machine learning based postpartum hemorrhage prediction model [^f6de23e1]. BMC Pregnancy and Childbirth (2022). Medium credibility.

Whereasrepresent the opposite number and x ‚àà R denotes a real number determined on range of x ‚àà[a, b]. While a = 0 and b = 1 Eq. (3) becomes

While there is a point P (x 1, x 2,‚Ä¶ x n) in n dimension coordinate and x 1, x 2,‚Ä¶, x n ‚àà R later, the opposite pointis determined as its coordinates:

In such cases, have 2 values, x represent initial arbitrary value in [a, b] anddenotes the opposite values of x. They calculate f (x)&in all the iterations of OBCSA, later, employ on the evaluation function g ifselect x or else selectConsequently, the f l would be in range: f l ‚àà[f l min, f l max]. The opposite numbercan be determined by:

Later, evaluate the fitness for the first f l value and the fitness forin all the iterations. When, they select f l, or elsewould be selected. The stages of presented method can be given in the following.

Step1: The count of crows is n c = 25, f l min = 0.1, f l max = 1.8, A P = 0.3, and the maximal number of iterations is t max = 100.

Step2: The position that represent the features are made by U (0, 1).

Step3: The fitness function (FF) can be determined by

Whereas C represent the classification performance, W represent the weighted factors in the range of zero and one, F all represent the overall amount of features and F sub signifies the length of elected feature.

Step4: The position of the crows are upgraded as Eq. (2)

Step5: Steps 3 & 4 are repetitive till a t max is attained.

---

### Evaluation of the evenness score in next-generation sequencing [^1f5f1936]. Journal of Human Genetics (2016). Low credibility.

The evenness score (E) in next-generation sequencing (NGS) quantifies the homogeneity in coverage of the NGS targets. Here I clarify the mathematical description of E, which is 1 minus the integral from 0 to 1 over the cumulative distribution function F(x) of the normalized coverage x, where normalization means division by the mean, and derive a computationally more efficient formula; that is, 1 minus the integral from 0 to 1 over the probability density distribution f(x) times 1-x. An analogous formula for empirical coverage data is provided as well as fast R command line scripts. This new formula allows for a general comparison of E with the coefficient of variation (= standard deviation œÉ of normalized data) which is the conventional measure of the relative width of a distribution. For symmetrical distributions, including the Gaussian, E can be predicted closely as 1-œÉ(2)/2‚©æE‚©æ1-œÉ/2 with œÉ ‚â§ 1 owing to normalization and symmetry. In case of the log-normal distribution as a typical representative of positively skewed biological data, the analysis yields E‚âàexp(-œÉ*/2) with œÉ*(2) = ln(œÉ(2)+1) up to large œÉ (‚â§ 3), and E‚âà1-F(exp(-1)) for very large œÉ (‚©æ2.5). In the latter kind of rather uneven coverage, E can provide direct information on the fraction of well-covered targets that is not immediately delivered by the normalized œÉ. Otherwise, E does not appear to have major advantages over œÉ or over a simple score exp(-œÉ) based on it. Actually, exp(-œÉ) exploits a much larger part of its range for the evaluation of realistic NGS outputs.

---

### Non-linear relationships in clinical research [^668fb19a]. Nephrology, Dialysis, Transplantation (2025). Medium credibility.

Polynomial functions

In Fig. 4, we provide an example of a simple power transformation using the square function, which raises a variable to the power of 2 (x 2), providing a U-shaped curve. Higher powers of x can be also used to determine the shape of the function, such as the cubic function (x 3) which has a distinctive S-shape. These power terms form the core elements of so-called polynomial functions, which along with the coefficients included in the function, offer a flexible way to model various curves. By adjusting the coefficients and power functions, polynomials offer a wide variety of shapes. "Fractional" polynomial functions allow the power terms to be fractions instead of just whole numbers (i.e. x 1/2) [21]. A (fractional) polynomial function often provides sufficient flexibility to follow relatively simple non-linear curves, providing a simpler solution than the more advanced techniques we describe below. However, higher degree polynomials can be sensitive to "noise" in the data, and are not suited for fitting some of the more complex curves (e.g. sudden shifts, discontinuities, or logarithmic curves). They may also suffer from Runge's phenomenon, becoming unstable and oscillating at the edges of the data, and extrapolate poorly beyond the original range of the independent variable.

Regression splines

A powerful approach to dealing with non-linearity is provided by the family of spline functions. Instead of a single function defining the whole curve, such as polynomials or other transformations, splines are constructed by using a series of functions, each defining a different segment of the curve. As splines are constructed segment by segment ‚Äî or piece by piece ‚Äî they are often referred to as "piecewise" functions. Segments are connected to each other using so-called "knots", and the spline is restricted to join at these knots so there are no gaps in the curve. The simplest spline function is the linear spline function. This function assumes linearity within each segment, but the overall curve formed by the connected segments can be non-linear. For a small number of segments, linear spline models can be as easy to interpret as linear regression, as each segment can be represented by a single slope. Figure 3 provides an example of a simple linear spline with two knots (in green). These two knots divide the range of the independent variable into three segments, each with its own slope.

---

### Learning shapes cortical dynamics to enhance integration of relevant sensory input [^5f42e6ef]. Neuron (2023). Medium credibility.

Non-normal dynamics (Figure S1)

We derived expressions relating linear Fisher Information to the dynamics of an arbitrary normal or non-normal network (subject to the same approximations described above). These expressions had a simple and interpretable form in three special cases: two-dimensional networks, normal networks, and non-normal networks with strong functionally-feedforward dynamics. Related findings have been presented previously.

To illustrate our analytical findings for the two-dimensional case, we constructed networks with modes m 1 = [cos Œ∏ 1; sin Œ∏ 1], m 2 = [cos Œ∏ 2; sin Œ∏ 2]. Figure S1A was constructed using the same procedure as for Figure 2, but this time with œÑ 1 = 10, œÑ 2 = 5. For Figure S1B we chose input with isotropic covariance Œ£ Œ∑ = I 2 (where I N is the N x N identity matrix) and Œî g = g (s 2) - g (s 1) = [1; 0]. These inputs were chosen in order to demonstrate the influence of non-normality as clearly as possible. We set œÑ 1 = 10, œÑ 2 = 1,5,7.5,9 and varied Œ∏ 1, Œ∏ 2 from ‚Äì œÄ /2 to œÄ /2 for each value. For each network (defined by the parameters Œ∏ 1, Œ∏ 2, œÑ 1, œÑ 2 using the procedure described for Figure 2), the Fisher Information of the stationary state network responsewas computed by substituting the long-run solution for the mean Œî r = ‚Äì A ‚Äì1 Œî g and the numerical solution to the Lyapunov equation for Œ£ (described above). We normalized this linear Fisher Information by the maximum achievable SNR in any normal network with the same time constants by defining. For each network, we computed the information-limiting correlations as œÅ ILC = Œî r T Œ£Œî r /(Œî r T Œî r Trace(Œ£)). For each choice of œÑ 2, we computed the Pearson correlation between the Fisher information and the information-limiting correlations corr(ùìò F, œÅ ILC), where the correlation was computed over a set of networks spanning the range of Œ∏ 1, Œ∏ 2 ‚àà [- œÄ /2, œÄ /2). We computed this correlation for various settings of Œ£ Œ∑ = [v 1, v 2] [Œª 1, 0; 0, Œª 2] [v 1, v 2] T, by varying the angle of its principal eigenvector v 1 from Œî g and the ratio of its two eigenvalues Œª 2 / Œª 1 with Œª 1 = 1 and Œª 2 ‚àà [0, 1].

---

### Size limits the sensitivity of kinetic schemes [^71d1065f]. Nature Communications (2023). High credibility.

A different measure of sensitivity ‚Äî the amplification of a fold-change in the input ‚Äî provides a solution to this problem. Suppose that for some value x 0 of the input parameter x, scaling by a factor a scales the output by b, so f (a x 0) = b f (x 0). Then the quotientcan be thought of as a discrete approximation of the derivative defining the logarithmic sensitivity. And if f (x) is differentiable everywhere, then by the mean value theorem, there must be a value x * of x for whichThis means that careful measurement of any two points on the input-output curve (x versus f (x)) witnesses the (local, infinitesimal) logarithmic sensitivity somewhere. Importantly ‚Äî unlike in the case of fitting to a Hill function ‚Äî if error in the measurements is very low, then they are also telling us the derivative for some value of x very accurately.

Equation (25) leads us to another common definition of the effective Hill coefficient:where S 0.9 and S 0.1 are the values of the input variable (in our case, x) required to get 90% and 10% (respectively) of the maximum value of the output variable (in our case, f (x)). Note that (26) is like (25) with a = S 0.9 / S 0.1 and b = 9. It implies that somewhere between S 0.1 and S 0.9 there is a logarithmic sensitivity of H eff /2.

There is yet another common definition, specific to models of binding. Suppose x is the concentration of a ligand and ‚å© n b ‚å™(x) is the expected number of sites bound by a ligand out of a total of n possible binding sites. It is common then, to takeor to report, as the Hill coefficient, the slope of a line fitted to x versusdata on a log-log plot.

---

### Microbial production of megadalton titin yields fibers with advantageous mechanical properties [^5b459267]. Nature Communications (2021). High credibility.

To estimate the degree of orientation of the crystallites along the fiber axis, we obtained two azimuthal 1D profiles from the 2D diffraction image (Fig. 2j, k). Figure 2j shows the plot of the diffraction intensity integration as a function of azimuthal angle within the radial range of the equatorial (120) peak, and Fig. 2k is the corresponding plot within the radial range of the equatorial (200) peak. After deconvolution of the 1D profiles, we applied the calculated FWHMs of the crystalline peaks and amorphous components to Herman's orientation function (8) f crystal = (3 < cos 2 œÜ > ‚àí1)/2 (Supplementary Tables 2, 3) ‚Äì. Here œÜ is the angle between the c-axis and the fiber axis and < cos 2 œÜ > is obtained based on the equation (9) < cos 2 œÜ ‚â• 1 ‚àí 0.8 < sin 2 (0.4 √ó FWMH (200)) > ‚àí 1.2 < sin 2 (0.4 √ó FWMH (120)) >. The parameter f crystal is 0 for no preferred orientation and 1 if all crystallites are perfectly aligned ‚Äì.

---

### Accurate spike estimation from noisy calcium signals for ultrafast three-dimensional imaging of large neuronal populations in vivo [^0887e407]. Nature Communications (2016). Medium credibility.

This is achieved by modifying equation (19) as follows:

For this estimation, we use A = 10%, œÑ = 0.8s. œÉ is autocalibrated as explained in the previous section. Standard, experience-inspired default values are used for the nonlinearity parameters, drift parameter and 'calcium event rate' Œª.

Next, the amplitude of single spike transients is best estimated from isolated calcium transients of moderate amplitude. Therefore calcium events that are either too close (< 1 s) to another event, or of amplitude F/F > 25% are excluded. The predicted calcium signals for those excluded events are then subtracted from the original signal, yielding modified calcium signals containing only the 'good' events. Individual event amplitudes and the value of œÑ are then re-estimated so as to maximize the fit to these new signals.

At this point, a histogram of all event amplitudes is constructed (Supplementary Fig. 5a). It is first smoothed, yielding x 1. Thereafter, peaks are enhanced by dividing x 1 by a low-passed version of itself, x 2. A cost function x 3 is then defined as x 3 (A) = x 2 (A)+ x 2 (2 A)/2 over a bounded range [A min = 4%, A max = 10%] (note that ' x 2 (2 A)' is a simplified view: in general 2A is replaced by the actual amplitude of a two-spikes transient taking into account nonlinearities). A first estimate of A is chosen as the value that maximizes x 3 (the green star in Supplementary Fig. 5a). This estimate is used to assign a number of spikes to each individual event (black separation lines and printed spikes numbers): the separations between k and k +1 spikes are set at (k +0.3)* A.

---

### Complex systems and the technology of variability analysis [^092b6848]. Critical Care (2004). Low credibility.

Power law analysis

Power law behaviour may be described by the following equation:

F(x) = ax √ü

Where a and √ü are constants. Taking the logarithm of both sides, a straight line (graph log f [x] versus log x) with slope √ü and intercept log a is revealed:

Log f(x) = log (ax √ü) = log a + log x √ü = log a + √ü log x

Thus, power law behaviour is scale invariant; if a variable x is replaced by Ax', where A is a constant, then the fundamental power law relationship remains unaltered. If dynamics follow a power law, a log‚Äìlog representation of the power spectrum (log power versus log frequency) reveals a straight line, always within a defined range consistent with the size and duration of the system. The straight line is fitted using linear regression, and the slope √ü and intercept can readily be obtained. When √ü = -1, the dynamics are described as 1/f noise. Power law behaviour describes the dynamics of widely disparate phenomena, including heart rate fluctuations, inter-breath intervals, earthquakes, solar flares, stock market fluctuations, and avalanches.

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^66f1bd1f]. Nature Communications (2023). High credibility.

CurveCurator yields well-calibrated p -values using a recalibrated F-statistic

The first step to assess the statistical significance of dose‚Äìresponse curves is to find the best possible fit given the measured dose‚Äìresponse values. As the optimization surface for sigmoidal curves is non-convex (i.e. a surface with many local minima), na√Øve curve fitting algorithms often get stuck in local minima, leading to suboptimal fits and, thereby, overly conservative p -values (Supplementary Fig S1). CurveCurator uses a heuristic that reaches the global minimum in almost all cases in a short period of time (Supplementary Fig. S2). To obtain an empirical null distribution, we simulated 5 million dose‚Äìresponse curves under the null hypothesis, i.e. curves where the response is independent of the dose (i.e. no dose-dependent regulation) for a range of n data points per curve (Fig. 2a). As expected, direct application of the classical F-statistic for linear models to these null dose‚Äìresponse curves yielded poorly calibrated p -values (Supplementary Fig. S3). CurveCurator solved this issue by optimizing both the F-value formula and the parameters of the F-distribution as a function of n to approximate these simulated null distributions accurately (Fig. 2 b, c). The validity of this approach was confirmed by noting that p -values in real experiments in which the vast majority of curves were expected to be unresponsive formed a uniform distribution of truly non-regulated curves plus a small distribution of truly regulated curves enriched at low p -values (Fig. 2d).

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^327fd22c]. Nature Communications (2023). High credibility.

Thresholding

The curve fold change for the i th curve (cfc i) is defined as the log 2 -ratio between the lowest and highest concentration using the regressed model and it quantifies the drug's effect size or efficacy (Eq. 11).

We transferred the SAM principle of differential T-statistics to the recalibrated F-statistic to obtain equivalent decision boundaries for the dose‚Äìresponse curve analyses. This is possible by recognizing that a dose‚Äìresponse curve converges in the limit to two groups (front plateau group = not affected data points, and back plateau group = affected data points), where the curve fold change is equivalent to a conventional SAM fold change between the two plateau groups. In this case, allowing for the conversion and application of the s 0 SAM principle. CurveCurator simplified this process by calculating the tuning parameter s 0 directly from the user-specified significance and fold change asymptotes (Eq. 12). Whereis the inverse cumulative density function of an F-distribution with degrees of freedom dfn and dfd as determined in the section above. This makes s 0 also a function of the number of data points, which is relevant when a curve has missing values.

The tuning parameter s 0, which defines the hyperbolic decision boundaries, can also be used to transform the curve's recalibrated F-value into the s 0 -adjusted F-value (F adj, i) based on the global s 0 value and the curve's measured fold change (cfc i) (Eq. 13).

We then transform this s 0 -adjusted F-value into a "relevance score" using the cumulative density function F X (x | dfn, dfd). For s 0 = 0.0, this simply corresponds to the p -value of the curve. For s 0 > 0.0, this can no longer be interpreted as a p -value, but it still provides an appropriate ordering of curves by both statistical and biological relevance. Additionally, a ‚àílog 10 transformation is applied for visualization purposes and to obtain an interpretable score that ranges from 0 to infinity, where 0 has no relevance (Eq. 13).

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^7cd4081b]. Nature Communications (2025). High credibility.

To explain this difference, in the Supplementary Information we use the 1D rational polynomial form of the matrix-tree expression, Equation (3), to maximize ‚àÇ œÄ S /‚àÇ F with respect to the learnable coefficientsand(treated for now as free and independent). The multi-index Œº used in Equation (2) simplifies here to the single index m. We show thatwhere the derivative is evaluated at the location of the decision boundary F 0 and M R = M max ‚àí M min is the range in exponential powers of e F /2 among all directed spanning trees. This result shows that the sharpness of the classifier is fundamentally limited by the structure of the network. A tighter approximation to the bound can be obtained by replacing M R with, which is the range in exponential powers among only the directed spanning trees rooted on the output nodes. We further explain in the Supplementary Information that the directed spanning trees of the parallelly extended push-pull networks preventfrom scaling with n, whereas the spanning trees for serially extended networks allow, which enables increasingly sharp transitions as more edges are added. In serially extended networks, the structure allows this rangeto grow with the number of added edges n, leading to increasingly sharp transitions. In contrast, parallelly extended networks constrain all output-rooted spanning trees to use the same number of driven edges, keepingfixed and preventing sharper transitions.

Finally, even when the bound in Eq. (8) is large it may not be achieved in practice (see Supplementary Information for details). Saturating the bound requires that the coefficientsbe concentrated on trees with either the smallest or largest possible net input drive. However, in networks such as those with a ladder-like architecture, many spanning trees make intermediate contributions, and equality constraints among the functionsprevent the network from assigning large weights solely to the extremal trees. As shown in Fig. 4 B, overlapping spanning trees entangle the coefficients and reduce the effective degrees of freedom. This structural limitation suggests that sharp decision boundaries may be inherently inaccessible in densely interconnected biochemical networks. This finding resonates with, though is technically distinct from, recent results in refs.

---

### Robust and tunable signal processing in mammalian cells via engineered covalent modification cycles [^3c15b5bf]. Nature Communications (2022). High credibility.

Calculation of fold-changes and robustness scores

For quantifying the effects of EnvZ variants and perturbations, we measured fold-changes by dividing the median output level of each sample by that of the equivalent sample in the absence of the EnvZ variant or perturbation. For perturbation experiments, the level of output absent perturbation is referred to as the nominal output level. Where log 2 -transformed fold-changes are shown for experiments with multiple repeats, the values shown are the mean of the log 2 -transformed fold-changes, rather than the log 2 -transformation of the mean of the fold-changes. This order of operations ensures that standard deviations of the fold-changes can be computed directly on the log 2 -transformed scale.

We computed robustness scores from the fold-changes using the formulae below:

Quantification of cell-to-cell output variance

To measure noise, we computed the interquartile range (IQR) of the output distributions. As we chose the median to represent the middle of the distribution, the IQR is a corresponding non-parametric measurement of noise. Since gene expression noise is approximately log-distributed, we log 10 -transformed the data prior to computing the IQR. As with calculations of the medians, negative fluorescent values were discarded when computing the IQR to avoid artifacts.

Model fitting

Where possible, fluorescent reporters were used to estimating the concentration of a molecular species for the purpose of model fitting.

For fitting all models, we used the MATLAB function "lsqcurvefit", which minimizes the sum of the squares of the residuals between the model and the data. In general, fits were made with cells subsampled from bins, as indicated for each figure. In Supplementary Figure 18, the fits were made using the true/false positive rates for each bin. Fits were always performed individually per experimental repeat, then means and standard deviations were computed for individual fit parameters.

The goodness of fit was measured by computing the normalized root-mean-square error CV(RMSE) using the following formula:Where y (x i) is the value of the data at the input value x i, is the mean of y for all values of x, and f (x i) is the function output at input value x i.

---

### Self-organization of active particles by quorum sensing rules [^8f83d162]. Nature Communications (2018). Medium credibility.

We have also studied how the collective behaviour changes upon further variations of the particles' response to their environment. Figure 5a shows a cluster that is formed by introducing a second concentration threshold c th,2 > c th above which particles recover their motility. This leads to enhanced active particle motion near the cluster centre and preferential escape from this region. As a result, the particle density near the centre decreases which leads to a 'ring'-like structure.

Fig. 5
Experimentally measured structural changes due to motility response variations. a 'Ring': when the local density c exceeds a second threshold c th,2 > c th, the swimming velocity is set to v 0 again. This reduces the density in the centre of the cluster (scale bar 65 Œºm). b 'Ellipse': introducing an angle-dependent term f (Œò) = cos 4 (Œò). c Changing f (Œò) to f (Œò) = sin 4 (Œò) leads to rotation of the ellipse by 90¬∞. d 'Square': doubling the frequency of the angle-dependent term to f (Œò) = cos 4 (2‚ãÖŒò) leads to a square-shaped cluster. In (b ‚Äì d) the arrow shows the direction of the x -axis

We also considered the situation where the concentration profile of signalling molecules becomes non-isotropic, e.g. due to thermophoretic forces induced by external temperature gradients. To account for the angle-dependence of the concentration profile around each particle j, we have modified Eq. (1) by an angle-dependent function f (Œò) towhere Œò ij denotes the angle between the connecting vector of particles i and j and the x -axis. For f (Œò) = cos 4 (Œò) we find an elliptically elongated shape, whose axis is rotated by 90¬∞ for f (Œò) = sin 4 (Œò) (Fig. 5b, c). Choosing f (Œò) = cos 4 (2‚ãÖŒò) we obtain an almost quadratically shaped cluster as shown in Fig. 5d.

---

### Input-output maps are strongly biased towards simple outputs [^38aa07d9]. Nature Communications (2018). Medium credibility.

On its own, Eq. (2) may not be that useful, as K (x | f, n) can depend in a complex way on the details of the map f and the input space size n. To make progress towards map independent statements, we restrict the class of maps. The most important restriction is to consider only (1) limited complexity maps for whichin the asymptotic limit of large x (Supplementary Note 3). Using standard inequalities for conditional Kolmogorov complexity, such asand, it follows for limited complexity maps that. Thus, importantly, Eq. (2) becomes asymptotically independent of the map f, and only depends on the complexity of the output.

We include three further simple restrictions, namely (2) Redundancy: if N I and N O are the number of inputs and outputs respectively then we require, so that P (x) can in principle vary significantly, (3) Finite size: we imposeto avoid finite size effects, and (4) Nonlinearity: We require the map f to be a nonlinear function, as linear transformations of the inputs cannot show bias towards any outputs (Supplementary Note 4). These four conditions are not so onerous. We expect that many real-world maps will naturally satisfy them.

---

### Using both qualitative and quantitative data in parameter identification for systems biology models [^fc71653e]. Nature Communications (2018). Medium credibility.

Results

An illustration of the potential value of qualitative data

To demonstrate the potential value of qualitative data, we consider a simple case of solving for the coefficients of polynomial functions.

We consider two polynomial functions: y 1 = ax 2 ‚àí bx + c and y 2 = dx + e. Suppose we want to solve for the coefficients a, b, c, d, and e, which we will take to be positive. As the ground truth coefficients to be determined, we choose (a, b, c, d, e) = (0.5, 3, 5, 1, 1.5).

Suppose that a limited amount of quantitative information is available. Namely, it is known that the parabola y 1 contains the points (2, 1) and (8, 13), and the line y 2 contains the point (3.5,5). This is not enough information to solve for any of the coefficients because three points are required to specify a parabola, and two points are required to specify a line (Fig. 1a).

Fig. 1
A simple illustration using polynomial functions. We use qualitative and quantitative information to determine the unknown coefficients. a Visualization of the problem. We seek to find the coefficients of equations for a parabola and a line, with the ground truth shown (blue solid curves). Two points on the parabola and one point on the line are known (black dots). These three points are consistent with infinitely many possible solutions (e.g. orange dashed curves). Qualitative information (colored circles, x -axis) specifies whether the parabola is above (+) or below (‚àí) the line. This information limits the possible values of intersection points x 1 and x 2 to the green shaded segments of the x -axis. b Bounds on coefficient values as a function of the number of qualitative points known. Shaded areas indicate the range of possible values of each coefficient

---

### Glycemic deviation index: a novel method of integrating glycemic numerical value and variability [^cedf5e7c]. BMC Endocrine Disorders (2021). Medium credibility.

The functional form of y 1 should meet the exponential increase of the x value at a certain symmetrical point. To ensure that y 1 satisfies the following two criteria: 1) the value range of x is symmetrical around a certain point, 2) the target range of x is also symmetrical around this point; the following equation was constructed:

The solution calculated using MATLAB was: a = ‚àí 0.801, b = 0.672. Inserting a and b into eq. 1‚Äì1 provided:

Furthermore, to adjust the value range to occupy the range of [0,10], and to meet the normal range at [0,1] synchronously, the following formula was introduced (Fig. 1 a):

Fig. 1
Functional images of MGI (a) and SDGI (b). The definitional domain of MGI is [2.8, 33.3], while the definitional domain of SDGI is [0, 7.4]. The value domain of both functions is [0, 10]

Next, to reduce the "neutralization" of hyperglycemia on hypoglycemia, MGI was calculated in two steps. MG 1 and MG 2 represent the average of blood glucose in the hypoglycemic and non-hypoglycemic periods separately, respectively substituted into formula 1‚Äì3 for the calculation to be performed. The weighting coefficient "c" was used to indicate duration in the hypoglycemic range (percentage of readings below 3.9 mmol/L per 72 h). The final formula of MGI was as follows:

The target range of SDG in this study was 0‚Äì1.4 mmol/L, based on previous studies, and the value range was defined as 0‚Äì7.4 mmol/L. The degree of deviation of glycemic variability was represented as the standard deviation of glucose index (SDGI). The SDGI value was exponentially augmented after SDG crossed over the normal threshold (Fig. 1 b). SDGI was listed as follows, where "z" represented the SDG, while "e" and "f" were constant parameters:

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^09a12e26]. Nature Communications (2021). High credibility.

Fig. 6
The feedforward network motif owes its primary functions to a difference in regulatory delays.

a The feedforward motif with delays, in which a single output Z is controlled by an input X via two regulatory arms with differing delays. The straight, short arrow represents the "direct arm" with delay Œ≥ 1 and the longer, curved arrow represents the "indirect arm" with delay Œ≥ 2 > Œ≥ 1. b The ODE model for an incoherent (type 1) feedforward motif, one of 8 possible networks in which the intermediate gene Y is modeled explicitly in the indirect arm. c Simulations of the four feedforward motifs with AND-type and OR-type logic (Fig. 5 and Supplementary Table 1) in response to short and long gain and loss of input signal. Blue curves: inputs (X), orange curves: reporter R activated with high cooperativity by Z. Note that the bottom two rows demonstrate pulse generation, while the top two rows filter short signals. d Response of an incoherent feedforward motif to oscillatory input after initial transients have died away. Z 3 is a 3-frequency Fourier approximation of Z (see E). e Fourier decomposition of Z from (d) by Eq. (26) and by a numerical fit to the data in (d). f Frequency scan (Bode plot) of (d) for 3 values of Œî Œ≥, with the theoretical envelopes from Eq. (28). g The maximum amplitude of the motif in (d) over a range of Œî Œ≥ and the corresponding frequencies at which the maxima occur. Z goes above 1 (activation threshold for R) for a small range of Œî Œ≥. For (c ‚Äì f), Œ∑ 1 = 0.9, Œ∑ 2 = 0.7, n 1 = 2, n 2 = ‚àí2, n 3 = ‚àí20, Œ∑ R = 2, A = 1. For d, f = 0.05, Œî Œ≥ = 4.

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^d55d13ff]. Nature Communications (2023). High credibility.

F-statistics and p -values

The basic idea behind the F-value in classical linear regression problems is to quantify how much better a more complex model (M 1 with k linear parameters) fits the data compared to a simpler model (M 0 with j linear parameters and j < k) given the n observed data points and the corresponding sum-squared errors (SSE) (Eq. 6).

Although not a linear model by nature, the log-logistic function still meets the required assumptions of random sampling, independence of observations, residual normality, and equal variance of the errors. The basic rationality behind CurveCurator's recalibrated F-statistic is similar to the linear F-statistic above. It also quantifies how much better the fitted log-logistic model (M 1) is compared to the mean model (M 0), which describes that there is no relationship between the applied dose and the observed response. We found, however, that n/k was a more appropriate scaling factor for the 4-parameter log-logistic function.

The obtained recalibrated F-value (Eq. 7) can then be used to calculate a p -value that quantifies how often a curve with a similar or bigger F-value can be found by random chance. We observed that these F-values follow a parameterized F-distribution with degrees of freedom that diverged from the case of linear models. Using extensive simulations under the null hypothesis (5 million curves for n = 5‚Ä¶ 50), we obtained a simple quasi-linear function to calculate the "effective "degrees of freedom as a function of n (Eqs. 8‚Äì10).

---

### Phase-encoded fMRI tracks down brainstorms of natural language processing with subsecond precision [^7d590922]. Human Brain Mapping (2024). Medium credibility.

2.8 Functional image analyses

For each BRIK file of a functional scan containing (x, y, z, t) = 64 √ó 64 √ó 55 √ó 256 data points, a 256‚Äêpoint discrete Fourier transform was applied to the time series x m (t) of each voxel m at location (x, y, z) by:where X (œâ) is the Fourier component at each frequency œâ between 0 and 127 cycles per scan, and | X m (œâ)| and Œ∏ m (œâ) represents the amplitude and phase angle, respectively. The task frequency is defined as œâ s (16 cycles per scan), denoting the frequency of periodic fluctuations of blood flow in response to periodic stimuli and tasks. The remaining nontask frequencies are defined as œâ n. The signal and noise are defined as the Fourier components X m (œâ) at frequencies œâ s and œâ n, respectively. The statistical significance of periodic fluctuations of blood flow is evaluated by the signal‚Äêto‚Äênoise ratio, an F ‚Äêratio (Chen et al; Huang et al; Sereno et al; Sereno & Huang; Sood & Sereno,), in each voxel m by:where df s = 2 and df n = 230 are the degrees of freedom of the signal and noise, respectively. The p ‚Äêvalue in each voxel m is estimated by the cumulative distribution function F (2,230) = F (F m; df s, df n) (Chen et al; Huang et al; Press et al.). A complex F ‚Äêvalue, (F m_r, F m_i), incorporating both the F ‚Äêstatistic value and the phase angle, Œ∏ m (œâ s), of each voxel was computed by F m_r = f m cos(Œ∏ m (œâ s)) and F m_i = f m sin(Œ∏ m (œâ s)), where f m is the square root of F m. Voxels containing strong periodic signals at the task frequency (œâ s = 16 cycles per scan) with F (2,230) > 4.7 (p < .01, uncorrected), F (2,230) > 7.1 (p < .001, uncorrected), or F (2,230) > 9.6 (p < .0001, uncorrected) were retained and their phase angles were color‚Äêcoded in a range between 0 and 2œÄ (0‚Äì16 s) and painted on each individual subject's cortical surfaces for each scan using csurf (Figure 3). The complex F ‚Äêvalues of corresponding voxels m were vector‚Äêaveraged (voxel‚Äêwise) across two scans k = {1, 2} of the same task in each session for each subject S using:which was performed by the "Combine 3D Phase Stats" function of csurf. The resulting average complex F ‚Äêvalueswere then painted on individual subject's cortical surfaces.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^345386c9]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Selecting and reporting reference values ‚Äî procedural guidance states that PFT laboratories must select appropriate reference values generated from high-quality data from a large sample of healthy asymptomatic individuals who have never smoked or had other respiratory illness or significant exposures, reports should identify the source of reference values, any change in selected equations should be noted and prior percent predicted values recalculated if possible, pediatric‚Äìadult equation discontinuity is discouraged, and extrapolation beyond the age range should not be done during growth, increases uncertainty in the elderly, and must be noted in technician comments.

---

### A measure of reliability convergence to select and optimize cognitive tasks for individual differences research [^707d40d8]. Communications Psychology (2024). Medium credibility.

Validation of the different fits

To compare the different fits and how well they are able to predict reliability, we ran extensive simulations of synthetic data mimicking real behavioral data from our task battery to supplement our analysis of the large real-world dataset we collected. We sampled beta distributions as they are naturally bounded between 0 and 1 and because tasks in our dataset were well approximated by a beta distribution (see Supplementary Fig. 1). We created a dense sampling of C values in the range observed in our real-world dataset, and also extended this range further to expand our sampling of the space of potential C coefficients. For each distribution, we ran 1000 simulations of an experiment with the same overall C (as if participants took the test many times) for several values of N (number of participants). We then compared the fitted C value from the simulated data to the true C value of the distribution from which the data was generated (see SI for the derivation of the C coefficient for those distributions). Analyses for both the real-world and the simulated datasets are shown in Fig. 4.

Fig. 4
Comparison of three different methods for fitting and estimating the C coefficient.

a Comparison of fitted C value to the true C value of simulated beta distributions (absolute difference). Each distribution was sampled 1000 times. Results are shown for L = 250 trials, N = 100 participants and for C in the range of 2‚Äì80. The plot shows error estimation (median and SD of percent error) of the different fitting methods for a range of values of the C coefficient (2‚Äì80). The shaded red region denotes C coefficient values observed in real-world data (C range: 4‚Äì41) and the unshaded region shows results from C values which we did not observe in our data (42‚Äì80). b The predicted number of trials to achieve reliability of 0.8 using the MV fit from Eq. (3) (purple) and an actual number of trials necessary for a reliability threshold of 0.8 as calculated from the data (black) for measures. Included are all measures that in our data reached reliability of at least 0.8.

---

### Identifying domains of applicability of machine learning models for materials science [^72c4ba76]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ŒΩ that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by œÉ lin (x 1, x 2) ‚â° ‚àí0.3 ‚â§ x 1 ‚â§ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: œÉ rbf ‚â° ‚àí3.3 ‚â§ x 11 ‚â§ 3.1 ‚àß x 2 ‚â§ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA œÉ poly (x 1, x 2) ‚â° ‚àí3.5 ‚â§ x 2 ‚â§ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### Topological one-way fiber of second chern number [^2faec267]. Nature Communications (2018). Medium credibility.

Results

Single Weyl dipole

Our starting point is a photonic crystal containing two Weyl points, which were found in the double gyroid (DG) made of magnetic materials. The DG is a minimal surface that can be approximated by the iso-surface of a single triply periodic function: f (x, y, z) = sin(2 œÄx / a)sin(4 œÄy / a)cos(2 œÄz / a) + sin(2 œÄy / a)sin(4 œÄz / a)cos(2 œÄx / a) + sin(2 œÄz / a)sin(4 œÄx / a)cos(2 œÄy / a). This definition, although having a different form, yields almost identical geometry and band structure to those of the DG defined in ref.by two separate trigonometric functions (one for each gyroid). In Fig. 2a, two cubic cells of the DG are shown, where f (x, y, z) > f 0 = 0.4 is filled with gyroelectric material of dielectric constantand unity magnetic permeability. The rest of the volume is air. In this structure, there exists only two Weyl points (a single "Weyl dipole") separated by about half of the Brillouin zone along z direction, as plotted in Fig. 2b. This means that an infinitesimal supercell modulation of the crystal (in z with a period of 2a) can superimpose the two Weyl points on top of each other to form a 3D Dirac point between four bands(Fig. 2b), which opens a gap under a finite modulation/coupling strength (Fig. 2d). The fact that a bandgap does not close under small perturbations ensures the robustness of this approach: certain mismatch between the Weyl-point separation and the wavevector of the modulation can be tolerated.

Fig. 2
3D Chern crystal from magnetic Weyl crystals. a Two cubic unit cells of the DG photonic crystal magnetized along z. b The band structure of a cubic cell shows two Weyl points, which fold into one 3D Dirac point in the Brillouin zone of the supercell. c The DG photonic crystal whose volume fraction (blue‚Äìred colored) is periodically modulated along z. d The band structure of the 3D Chern crystal whose topological gap frequencies are highlighted in green. Œª 0 is the vacuum wavelength

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^bb9b7f12]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Œî Œ¶, the probability distribution at a point Œ¶ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Œ¶ ‚àà {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters Œ± and Œ≤, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since Œ± and Œ≤ are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Œ¶ ‚àà [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(Œ±, Œ≤) and x 2 = 1 ‚àí x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Œ¶, it may seem natural to select Œ± and Œ≤ by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### Using both qualitative and quantitative data in parameter identification for systems biology models [^a956cd8d]. Nature Communications (2018). Medium credibility.

We must choose the penalty constants C i in Eq. (3) based on our confidence in the qualitative data ‚Äî a larger C i gives the qualitative data more weight compared to the quantitative data. We hand-selected a value of C i = 0.03 for all i, to give roughly equal contributions from the qualitative and quantitative datasets (i.e. such that the blue and orange curves in Fig. 2d, e can be plotted using the same scale). Note that giving equal contributions to both datasets is an arbitrary choice for illustration ‚Äî with real experimental data, the modeler may choose to give unequal weights based on the relative importance/credibility of the datasets.

We minimized f tot (x) by differential evolution, and found best-fit parameters of K 3 = 5100 ŒºM ‚àí1 and K 5 = 0.060 ŒºM ‚àí1, which are reasonably close to the ground truth.

To evaluate the strengths of combining quantitative and qualitative data, we performed uncertainty quantification. We used a variant of the profile likelihood approach, a method that is well-established for quantitative fitting. One parameter of interest is held fixed, and the objective function is minimized by varying the remaining parameters. The resulting minimum is taken as the negative log likelihood of the fixed parameter value. The minimization is repeated for many possible fixed values of the parameter to produce a curve (Fig. 2d, e). Note that our objective function is not a likelihood in the rigorous sense, but has a similar interpretation in that a lower objective value indicates the parameter value is more consistent with the data.

We performed profile likelihood analysis using f quant (x), f qual (x), and f tot (x) in turn as the objective function, and considering two free parameters K 3 and K 5 (Fig. 2d, e). We found that each dataset individually provided bounds on possible parameter values, but the tightest bounds were obtained when we combined both datasets.

---

### Untangling the diffusion signal using the phasor transform [^1407b735]. NMR in Biomedicine (2020). Medium credibility.

4.2 IVIM fitting

The runtimes of all tested fitting algorithms were compared: the phasor‚Äêbased fit used 2.38 x and the segmented fit used 0.0023 x the computation time of the NLLS IVIM fit. A comparison of the nonlinear, segmented and phasor‚Äêbased IVIM fitting methods on simulated data is presented in Figure 3 and the left panel of Figure 4. Figure 3 shows that for fractions f below 0.25 the phasor‚Äêbased method had the highest precision for estimates of f and D: the interquartile range was roughly halved compared with that of the segmented method, which was second best. Figure S2 shows the same data as Figure 3, but for the full range of f (0‚Äê1). Accurately and precisely estimating D* at low fractions f proved difficult for all methods. The phasor method showed the best accuracy, but the worst precision. Table S1 reports mean and median error (accuracy) and interquartile range (precision) for all tested cases.

FIGURE 3
Comparison of IVIM fitting techniques on simulated data: constrained nonlinear least squares (blue), segmented linear least square (orange) and phasor‚Äêbased nonlinear least square (yellow). The SNR level was 30, and the b‚Äêvalue sampling range was 0‚Äê1000 s/mm 2 for both experiments, but the number of b‚Äêvalues was varied: 15 b‚Äêvalues in the top panel and six b‚Äêvalues in the bottom panel. The solid lines represent the median error and the dotted lines the 5% and 95% percentiles. Shading was added to facilitate a comparison of the spread in the errors

FIGURE 4
Probability density functions of estimated IVIM parameters f, D and D* for all tested fitting techniques in both simulated and in vivo measured white matter (WM) and gray matter (GM). In the simulations, the vertical lines indicate the true value of the estimated parameter. Results were obtained using all measured 15 b‚Äêvalues

---

### Holding and amplifying electromagnetic waves with temporal non-foster metastructures [^722cd4f9]. Nature Communications (2025). High credibility.

Results for epsilon-positive-to-negative-to-positive temporal slab: square function of Œµ(t)

In the previous section, we discussed the case where Œµ was rapidly changed from a positive to a negative value and is kept at this value using the concept of the non-Foster phenomenon. However, what would happen if Œµ is then changed back to a positive value after being negative? This section addresses this question by considering a time dependent Œµ (t) defined by two single steps (similar to a square function in time, i.e. two consecutive temporal boundaries) i.e. a temporal slab. We consider again a monochromatic plane wave traveling in an unbounded medium with the time-dependent Œµ as shown in Fig. 3 (the general analytical solution for this scenario can be found in the Supplementary Materials section 1 and 3).

Fig. 3
Analytical results for epsilon-positive-to-negative-to-positive temporal slab: square function of Œµ (t).

A Electric field distribution as a function of space (x) and time (t) when relative Œµ is changed from Œµ 1 = 1 to a positive real value Œµ 2 = 5 at t 1 = 37 T and it is then changed to Œµ 3 = 10 at t 2 = 40 T. B Electric field distribution as a function of time (t) at two spatial locations x = 2Œª 0 (black) and x = 2.25Œª 0 (purple) extracted from panel (A). C Space average of the instantaneous Poynting vector over one-wavelength distance (, blue) along with the momentum (red) as a function of time calculated from panel (A). D Electric field distribution as a function of space (x) and time (t) when relative Œµ is changed from Œµ 1 = 1 to a real negative value Œµ 2 = ‚àí15 (based on the notion of non-Foster) at t 1 = 37 T and then to Œµ 3 = 10 at t 2 = 40 T. (Note in panel D three different ranges of scale bars are used in the three different temporal ranges to show details of the distributions.) E Electric field distribution at x = 2Œª 0 (black) and x = 2.25Œª 0 (purple) extracted from panel (D). F same as (C) but for the case discussed in (D).

---

### How dieting might make some fatter: modeling weight cycling toward obesity from a perspective of body composition autoregulation [^32b99e3d]. International Journal of Obesity (2020). Medium credibility.

Two exponential fits are shown: a generalized linear model (GLM) with 95% confidence intervals (solid line), and a linear model (LM), with R 2 values (dotted line). The four figures correspond to the four methods presented in the main text for computing the value FAT END at the final time END at which the subjects would have completely recovered their initial FFM. The two red squares correspond to the US Army Ranger data points presented in the section on 'Applications of the Model'.

Their exact values depend on the method used for computing the values FAT END when the FFM has been completely recovered and on the type of statistical regression used. As explained previously, we consider four methods for computing FAT END. In Fig. 4, we show the fits obtained from a generalized linear model (GLM) with 95% confidence intervals (solid line) and from a linear model (LM), with R 2 values (dotted line). Following the performance of diagnostics tests to analyze the residuals, it is found that the GLM satisfies the major assumptions of regression analysis better than the LM, especially concerning the normality assumption of the error terms. Nevertheless, as shown in Fig. 4, both models give curves that are close to each other. In simple terms, the main difference between the above LM and GLM approaches to estimate the best parameter values for the constants a and b is the way the error term is handled. In the LM method, we write y = a √ó e bx √ó Œµ, where Œµ is a random error variable that follows a log-normal distribution with parameters Œº = 0 and œÉ 2. Taking the logarithm on both sides of the equality gives, which corresponds to the linear model, where, x ‚Ä≤ = x, Œ≤ 1 = b andis a random error variable that follows a normal distribution with mean Œº = 0 and variance œÉ 2. Note that in the LM method, the error term Œµ is multiplied with the exponential function y = a √ó e bx. In the GLM method, the error term is instead added to the exponential function. Formally, we write y = a √óe bx + Œµ, where Œµ is a random error variable that follows a normal distribution with mean 0 and variance œÉ 2. As a consequence, the GLM admits the possibility of the value y = 0 (which is excluded in LM) and assumes that the variance V (y) is constant, while the LM method assumes that V (y) varies with x. Finally, note that the LM method admits exact analytical solutions for the model parameters a and b, while the GLM method requires numerical optimization algorithms to find the best values (the maximum likelihood estimates) for a and b.

---

### A dual role in regulation and toxicity for the disordered N-terminus of the toxin graT [^910277d6]. Nature Communications (2019). High credibility.

Small-angle X-ray scattering

SAXS data were collected at SWING beamline (Soleil Synchrotron, Gif-Sur-Yvette, France). This beamline has a SEC system before the measuring capillary. SEC will remove possible aggregates rendering a very homogeneous sample that will then be directly exposed to X-rays for data collection. All the experiments were performed in 50 mM Tris, pH 8.0, 250 mM NaCl and 2 mM TCEP as running buffer.

Each protein was run through a Shodex KW402.5-4F at 0.2 ml/min. Scattering curves covering a concentration range around the peak were normalized and averaged to obtain the final scattering curve. Rg values were derived from the value of I 0 which were obtained by extrapolating to q = 0 using the Guinier approximation as implemented in ATSAS suite. The molecular weights of the different entities were estimated in a concentration independent way using the I 0, Porod volume and Fisher methods.

The use of the dimensionless Kratky plot ((qR g) 2 I(q)/I(0) vs qR g) is a relatively easy way to show that a protein is completely folded, partially folded or completely unstructured. If a protein is globular it follows Guinier's law I(q)/I(0) = exp (‚àí(qR g) 2 / 3). The corresponding dimensionless Kratky plot is a function f(x) = x 2 exp (‚àíx 2 / 3), with x = qR g > 0 with maximum of 1.104 at qR g = ‚àö3. On the other hand, an ideally disordered protein follows Debye's law I(q)/I(0) = 2 (x 2 ‚àí1‚àíexp (‚àíx 2))/x 4, with x = qR g > 0. In this case the Kratky plot is described by the function f(x) = 2 (x 2 ‚àí 1‚àíexp (‚àíx 2))/x 2 which increases monotonically with an asymptote at f(x) = 2. Experimentally, globular proteins show a very similar normalized Kratky plot with a maximum at (‚àö3; 1.1), while partially unstructured proteins show a maximum shifted to higher values in both axes.

---

### Resonant generation of propagating second-harmonic spin waves in nano-waveguides [^52dbc111]. Nature Communications (2024). High credibility.

Fig. 2
Experimental evidence for resonant wave interaction.

a BLS intensity detected at a frequency twice the frequency of the initial spin wave f exc as a function of the latter. Note two narrow resonant peaks at f exc = 2.47 and 2.56 GHz. b Complete BLS spectra recorded at two excitation frequencies corresponding to the observed resonances, as labeled. The data are obtained at H = 500 Oe at a distance x = 10 Œºm. Power of the excitation signal P = 0.1 mW.

Figure 2b shows the complete BLS spectra recorded at two excitation frequencies corresponding to the observed resonances. These spectra allow one to simultaneously observe signals at the excitation frequency, as well as those corresponding to the second harmonic. The data show that, at x = 10 Œºm, the intensity of the second harmonic exceeds that of the initially excited wave by more than a factor of two for both resonances. This indicates a very efficient energy transfer from the initially excited wave to the second-harmonic wave.

We study this processes in more detail in the space domain using the space- and phase-resolution of BLS. Figure 3 shows the results of spatial mapping of the intensity and phase (cos(œÜ)) of spin waves corresponding to two observed resonances. Figure 3 a‚Äìc characterizes the resonance at f exc = 2.47 GHz, while Fig. 3 d‚Äìf characterizes the resonance at f exc = 2.56 GHz. Figure 3a shows spatial maps of the intensity and phase corresponding to the initial wave at 2.47 GHz, while Fig. 3b shows the same maps for the second-harmonic wave at 4.94 GHz. The intensity of the initial wave decreases with propagation distance, while the intensity of the second harmonic, which is negligible near the antenna, gradually increases in space. Figure 3c shows a direct comparison of the spatial dependences of the intensities of the initial wave and the second-harmonic wave. The initial wave exhibits a well-defined exponential decay (note the logarithmic scale of the vertical axis) characterized by the decay length of 54 Œºm (for comparison, the independently determined decay length of the wave at 4.94 GHz is 11 Œºm). The intensity of the second harmonic increases in the range x = 0‚Äì10 Œºm and then saturates. The intensities of the two waves quickly become equal at x ‚âà 4 Œºm.

---

### Inferring time derivatives including cell growth rates using gaussian processes [^b5444dae]. Nature Communications (2016). Medium credibility.

Methods

Using a Gaussian process to fit time-series data

In the following, we will denote a Gaussian distribution with mean Œº and covariance matrix Œ£ as(Œº, Œ£) and use the notation of Rasmussen and Williamsas much as possible.

Prior probability

For n data points y i at inputs x i (each x i is a time for a growth curve), we denote the underlying latent function as f (x). We define a covariance matrix k (x, x ‚Ä≤), which has an explicit dependence on hyperparameters Œ∏, and obeys

where the expectations are taken over the distribution of latent functions (samples of f (x)).

We interpret equation (1) as giving the prior probability distribution of the latent functions f (X), where were we use X to denote the inputs x i, such that

where K (X, X) is the n √ó n matrix with components k (x i, x j). With f denoting [f (x 1). f (x n)], this prior probability can be written as

noting the dependence of k (x, x ‚Ä≤; Œ∏) on the hyperparameters Œ∏.

---

### Bayesian source separation for reference function determination in fMRI [^a65ca01a]. Magnetic Resonance in Medicine (2001). Low credibility.

In analyzing fMRI results, identification of significant activation in voxels is a crucial task. A standard method selects a "known" reference function and performs a regression of the time courses on it and a linear trend. Once the linear trend is found, the correlation between the assumed to be known reference function and the detrended observed time-course in each voxel is computed. But the most important question is: How does one choose the reference function? Here, a Bayesian source separation approach to determining the underlying reference function is described and applied to real fMRI data. This underlying reference function is the unobserved response due to the presentation of the experimental stimulus.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^f7d7cfa0]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

American Thoracic Society pulmonary function report ‚Äî core spirometry reporting elements emphasize that a uniform format for the presentation of PFT results in reports to users and in the medical record can reduce potential miscommunication or misunderstanding, and only information with validated clinical application should be included. The normal limit(s) of each test parameter should be displayed, and, consistent with other laboratory values, the measured value should be shown before reference values, ranges, or normal limits. Report and/or display of the displacement of the result from a predicted value in standard deviation units (z-score) can help in understanding abnormality. For spirometry, many parameters can be calculated but most do not add clinical utility and should not be routinely reported; only FVC, FEV1, and FEV1/FVC need be routinely reported. Measurement of slow VC and calculation of FEV1/VC are a useful adjunct in patients with suspected airflow obstruction. Reporting FEV1/FVC (or FEV1/VC) as a decimal fraction, and not reporting it as a percentage of the predicted value for this ratio, will help to minimize miscommunication. The nitrogen washout plot for multibreath tests and the tracings for plethysmograph tests can be shown graphically to aid quality assessment.

---

### Dissection of gene expression datasets into clinically relevant interaction signatures via high-dimensional correlation maximization [^52011f1c]. Nature Communications (2019). High credibility.

Signature focus

Purpose of the signature focus is to define where a detected effect ends. While this may seem easy for plateau-like clusters (having few highly correlated genes or samples while all others only have low correlations), often real effects have no clear edge. In this case, we prefer a smooth decrease in membership weights. The signature focus should not influence ranks of top members, as they need to be determined by signature strengths for optimal bimonotonic regression (cf. Eqs. 29 and 30). But non-members should be identified and excluded by the signature focus to minimize the influence of noise.

The signature focus consists of correlation-based gene and sample weights that allow the computation of all vectors and scores as specific as possible, even if the detected interaction only affects a small subset of measured genes and samples. To retain specificity for such small signatures, we set weight componentsandexactly to zero for all (and potentially very many) genes and samples that have only weak or insignificant correlations to detected signature axes.

For samples, letwhere the second factor decreases quadratically to zero with the noise probability of sample correlations (cf. Supplementary Note 2). Sample weights are defined relative to. Values ‚â• 50% of the maximum of allare already mapped to full weight:

Consequently, weights have no influence on the signature's order of top samples, as is intended. To exclude any unspecific influence of samples with relatively weak or insignificant correlation, we finally set all weights to zero that are lower than two thirds of their quantile:

Analogously, gene weightsare given bywhereand.

For signature size estimation and qualification thresholds, mapping all x -values above 50% to full weight is not optimal. To keep the full dynamic range of weights for these tasks, we additionally define the extended signature focusby increasing the upper threshold from 50% to 100% (i.e.and) and by decreasing the lower specificity threshold from two thirds to 0.4. Otherwise, weightsandare computed in the same way.

Importantly, specificity thresholds of the signature focus exclude noise genes from computation that could otherwise reduce both specificity of signatures and detection robustness. In case of many measured genes and small signatures, this can also speed up computation.

---

### Coding principles and mechanisms of serotonergic transmission modes [^257e73dd]. Molecular Psychiatry (2025). Medium credibility.

Fig. 7
Serotonin release properties encode frequency, number, and synchrony of firing pulses.

A Heatmaps of fluorescence Œî F/F 0 responses electrically evoked by 5-pulse stimuli delivered at varied frequencies. Scale bars applied to all in A. B Plots of whole-cell and single releasing synapse maximal fluorescence Œî F/F 0 responses against frequencies of stimuli containing 3 pulses (Whole cell: 0.5 Hz: 0.49 ¬± 0.15%; 1 Hz: 0.47 ¬± 0.09%; 2 Hz: 0.46 ¬± 0.12%; 8 Hz: 0.46 ¬± 0.13%; 25 Hz: 0.54 ¬± 0.17%; 32 Hz: 0.53 ¬± 0.18%; 64 Hz: 0.57 ¬± 0.13%, n = 5 neurons from 3 animals, fitted with log functions f (x) = 0.47 + 0.013
- Log 2 (x) (r 2 = 0.60, F = 7.55, p = 0.04); Single releasing synapse: 0.5 Hz: 1.11 ¬± 0.15%; 1 Hz: 1.23 ¬± 0.26%; 2 Hz: 1.35 ¬± 0.16%; 8 Hz: 1.43 ¬± 0.10%; 25 Hz: 1.52 ¬± 0.08%; 32 Hz: 1.47 ¬± 0.09%; 64 Hz: 1.79 ¬± 0.13%, n = 5 releasing synapses from 3 animals, fitted with log functions f (x) = 1.21 + 0.08
- Log 2 (x) (r 2 = 0.87, F = 33.64, p = 0.002)) and 5 pulses (Whole cell: 0.5 Hz: 0.40 ¬± 0.07%; 1 Hz: 0.38 ¬± 0.06%; 2 Hz: 0.41 ¬± 0.08%; 8 Hz: 0.44 ¬± 0.07%; 25 Hz: 0.46 ¬± 0.08%; 32 Hz: 0.55 ¬± 0.09%; 64 Hz: 0.80 ¬± 0.10%, n = 9 neurons from 5 animals, fitted with log functions f (x) = 0.38 + 0.042
- Log 2 (x) (r 2 = 0.78, F = 7.57, p = 0.04); Single releasing synapse: 0.5 Hz: 1.29 ¬± 0.11%; 1 Hz: 1.29 ¬± 0.11%; 2 Hz: 1.33 ¬± 0.18%; 8 Hz: 1.58 ¬± 0.20%; 25 Hz: 1.73 ¬± 0.26%; 32 Hz: 1.71 ¬± 0.32%; 64 Hz: 2.09 ¬± 0.27%, n = 9 releasing synapses from 5 animals, fitted with log functions f (x) = 1.30 + 0.10
- Log 2 (x) (r 2 = 0.89, F = 39.53, p = 0.002)). C Heatmaps of stepwise fluorescence Œî F/F 0 responses electrically evoked by 5-pulse stimuli delivered at varied voltage intensities. Note the difference in number of releasing synapses in step 1‚Äì3 fluorescence Œî F/F 0 responses. Scale bars applied to all in C. D Upper, plot correlating numbers of releasing synapses with presumably recruited axons. Note the correlation fitting with a linear function of f (x) = 3.68
- x (r 2 = 0.95, F = 53.23, p < 0.05). Lower, plot of numbers of releasing synapses against 1-, 3-, 5-, and 20-pulse stimuli delivered at 0.5 Hz (1 pulse: 1.30 ¬± 0.21, n = 5 from 3 animals; 3 pulses: 1.60 ¬± 0.40, n = 5 from 3 animals, 5 pulses: 1.77 ¬± 0.28, n = 9 from 5 animals), 1 Hz (3 pulses: 1.80 ¬± 0.2, n = 5 from 3 animals; 5 pulses: 1.92 ¬± 0.26, n = 9 from 5 animals), 2 Hz (3 pulses: 2.00 ¬± 0.45, n = 5 from 3 animals; 5 pulses: 2.15 ¬± 0.22, n = 9 from 5 animals; 20 pulses: 2.72 ¬± 0.48, n = 9 from 5 animals), 8 Hz (3 pulses: 2.20 ¬± 0.37, n = 5 from 3 animals; 5 pulses: 2.39 ¬± 0.31, n = 9 from 5 animals), 25 Hz (3 pulses: 2.60 ¬± 0.40, n = 5 from 3 animals; 5 pulses: 2.88 ¬± 0.30, n = 9 from 5 animals), 32 Hz (3 pulses: 2.80 ¬± 0.37, n = 5 from 3 animals; 5 pulses: 3.10 ¬± 0.56, n = 10 from 5 animals), 64 Hz (3 pulses: 3.60 ¬± 0.40, n = 5 from 3 animals; 5 pulses: 5.16 ¬± 0.50, n = 10 from 5 animals), fitted with a linear function of f (x) = 0.31
- x + 1.75 (r 2 = 0.79, F = 18.62, p < 0.01).

---

### Epistatic net allows the sparse spectral regularization of deep neural networks for inferring fitness functions [^7a3a3502]. Nature Communications (2021). High credibility.

Methods

Notation and background

Suppose we are given n (experimental) samples, that is, (sequence, value) pairs from a biological landscape, where x i ‚àà {‚àí1, +1} d denotes the binary encoding of d mutational sites in a variant andis its associated fitness value. We are interested in learning a function f (x) that maps all subsets of mutations to fitness values. In other words, we seek to learn a set function, wheredenotes the space of all the binary vectors of length d. A key theoremin mathematics states that any set function (also known as pseudo-Boolean function) f (x) = f (x 1, x 2,‚Ä¶, x d) can be represented uniquely by a multi-linear polynomial over the hypercube (x 1, x 2,‚Ä¶, x d) ‚àà {‚àí1, +1} d :whereis a subset of {1, 2, 3,‚Ä¶, d } = [d] andis the WH transform coefficient (or equivalently the epistatic coefficient) associated with the monomial (interaction). For example, the pseudo-Boolean functiondefined over d = 5 mutational sites, has three monomials with orders 2, 1, and 3 and WH coefficients 12, ‚àí 3, and 6, respectively. The WH transform of this function is sparse with k = 3 non-zero coefficients out of a total of 2 5 = 32 coefficients. Each monomial can be easily explained, for example, the first monomial in the WH transform, that is 12 x 1 x 4, indicates that mutation sites 1 and 4 are interacting and the interaction enriches fitness because the sign of the coefficient is positive. On the hand, the second monomial ‚àí 3 x 3 shows that a mutation at site 3 depletes fitness. The last monomial 6 x 1 x 2 x 5 shows a third-order interaction between mutational sites 1, 2, and 5 which also enrich fitness.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^d6d8fcbc]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Spirometry reporting specifies that numerical values are given for the FEV1, the FVC, and the FEV1/FVC ratio; the latter should be reported as a decimal fraction and the space for percent predicted value left blank, and if bronchodilators are given the LLN column need not be repeated with absolute and percent change given only for FEV1 and FVC. Other numerical values such as the forced inspiratory flow at 75% of FVC (FEF75%) and FEF25‚Äì75% are not recommended for routine use. Graph requirements include that for the volume‚Äìtime curve the volume scale should be at least 10 mm/L, the time scale at least 20 mm/s, and 1 second prior to the start of expiration should be displayed; on the flow‚Äìvolume plot the flow display should be at least 5 l/min/L/s, and the ratio of flow to volume should be 2 L/s to 1 L, and linear and log scales where values are plotted as z-scores relative to the predicted value (z = 0) give an intuitive sense of severity.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^a6af3eb5]. Nature Communications (2021). High credibility.

Frequency response of feedforward motifs can be solved analytically and demonstrates low- and band-pass filtering capabilities

Biological regulatory networks often encode information as the change in frequency of an oscillating input, which has been suggested to be more robust to noise than encoding information in absolute concentration ‚Äì. That feedforward loops filter short square pulses suggests more general frequency-filtering capabilities. We therefore analyze feedforward frequency response to sinusoidal input (Fig. 6 d‚Äìg). We show that the response follows the outline of a universal transfer function curve, independent of the logic or delay difference.

Instead of the step input analyzed above, here we consider a sinusoidal inputwhich oscillates between zero and 2 A (twice the amplitude) at a frequency f > 0. Taking the Fourier decomposition of each Hill-regulated term 1/[1 + X n (T)] and plugging into the governing Eq. (21) (Supplementary Note 6) provides the output Z (T) in terms of its magnitude I k and phase œï k as a function of frequency:whereare Fourier coefficients of Hill-regulated terms 1/[1 + X n (T)] at integer multiples k of the fundamental frequency f :for all f > 0. Note that the magnitudes are symmetric to interchange of the two regulation arms (Œ∑ 1 ‚Üî Œ∑ 2, n 1 ‚Üî n 2), while the phases are not.

---

### Clinical utility of polygenic risk scores for embryo selection: a points to consider statement of the American College of Medical Genetics and genomics (ACMG) [^9ccd284a]. Genetics in Medicine (2024). High credibility.

PGT-P results communication ‚Äî Figure 3 illustrates using 3 components (percentile rank, relative risk, and absolute risk): families (a) and (b) have embryos at rank extremes while family (c) shows a broader range; two families display different relative risk ranges, with family x values 0.9, 1.0, 1.1, 1.2 and family y values 0.6, 0.9, 1.1, 1.4; example absolute risk increases span 1.0% - 1.4% versus 10.0% - 14.0% for diseases differing by 10x population prevalence.

---

### Fractional response analysis reveals logarithmic cytokine responses in cellular populations [^b8e8ae49]. Nature Communications (2021). High credibility.

Although we can now measure single-cell signaling responses with multivariate, high-throughput techniques our ability to interpret such measurements is still limited. Even interpretation of dose-response based on single-cell data is not straightforward: signaling responses can differ significantly between cells, encompass multiple signaling effectors, and have dynamic character. Here, we use probabilistic modeling and information-theory to introduce fractional response analysis (FRA), which quantifies changes in fractions of cells with given response levels. FRA can be universally performed for heterogeneous, multivariate, and dynamic measurements and, as we demonstrate, quantifies otherwise hidden patterns in single-cell data. In particular, we show that fractional responses to type I interferon in human peripheral blood mononuclear cells are very similar across different cell types, despite significant differences in mean or median responses and degrees of cell-to-cell heterogeneity. Further, we demonstrate that fractional responses to cytokines scale linearly with the log of the cytokine dose, which uncovers that heterogeneous cellular populations are sensitive to fold-changes in the dose, as opposed to additive changes.

---

### A space-time tradeoff for implementing a function with master equation dynamics [^57fa10b0]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^64462d9d]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Lower limit of normal (LLN) and fixed cutoffs ‚Äî percent predicted values should not be used to define abnormality because the true LLN is age- and/or height-dependent and varies among individuals; fixed cutoffs such as 80% predicted for forced vital capacity (FVC) or 0.70 for the FEV1/FVC ratio are based on middle-aged adults, can yield erroneous decisions in children and in older or shorter adults, and introduce sex bias. The LLN does not necessarily need to be the fifth percentile and can be adjusted with outcome data; caution is warranted near dichotomous boundaries, especially with results from a single occasion. In clinical testing where disease is suspected, the pretest probability is higher and the false positive rate is much lower than the lowest 5% implied by percentile thresholds.

---

### Universality of quantum phase transitions in the integer and fractional quantum hall regimes [^c603f914]. Nature Communications (2024). High credibility.

Measurement of localization exponent Œ≥

We now focus on the localized regime, far away from E c, marked by the black rectangle in Fig. 4 a. Given the presence of strong interactions, it is reasonable to assume that transport in this localized part of the energy spectrum proceeds through Efros‚ÄìShklovskii (ES) type hopping mechanism. The localization exponent Œ≥ determines the T dependence of longitudinal conductance G x x, :withThe pre-factor G 0 ‚àù 1/ T and Œ¥ ŒΩ = (ŒΩ ‚àí ŒΩ c). Figure 4 b shows plots of log(T G x x) versus T ‚àí1/2 at different values of Œ¥ ŒΩ; the dotted lines are linear fits to the data. The linearity of the data at low- T is consistent with transport by the ES hopping mechanism in the FQH regime (Eq. (4)). At high- T (in the region marked in Fig. 4 b by a dotted ellipse), the values of G x x become relatively large, and the plots deviate from a straight line. In passing, we note that as we move progressively closer to the center of the plateau in R x y, where the value of G x x ‚âà 0 at low- T, the linearity of the plots persists to higher temperatures. Fitting T 0 (estimated from Eqn (4)) and ‚à£ Œ¥ ŒΩ C F ‚à£ to Eq. (5), we find the estimated Œ≥ to lie in the range 2.3‚àí2.6 (Fig. 4 c) for FQH plateau-to-plateau transitions, very close to the predicted range of Œ≥ = 2.3‚àí2.5. The fact that the exponent controlling the divergence of the localization length at criticality is almost identical for both FQH and IQH states points to an effective model of localization that is universal across the different statistics of the quasiparticles in these QH phases. Furthermore, from Œ∫ = 1/ z Œ≥ ‚âà 0.41 ¬± 0.03 and Œ≥ ‚âà 2.3, we get z ‚âà 1, as expected for a strongly interacting system.

---

### New approaches to receiver operating characteristic methods in functional magnetic resonance imaging with real data using repeated trials [^bc00ca2b]. Magnetic Resonance in Medicine (2004). Low credibility.

Receiver operating characteristic (ROC) methods are useful tools for evaluating the sensitivity and specificity of various postprocessing algorithms used in fMRI data analysis. New ROC methods using real fMRI data are proposed that improve a previously introduced method by Le and Hu (Le and Hu, NMR Biomed 1997;10:160‚Äì164). The proposed methods provide more accurate means of estimating the true ROC curve from real data and thereby aid in the comparative evaluation of a wide range of postprocessing tools in fMRI. The mathematical relationships between different ROC curves are explored for a comparison of different ROC methods. Examples using real and simulated data are provided to illustrate the ideas involved.

---

### Control nucleation for strong and tough crystalline hydrogels with high water content [^b28620a1]. Nature Communications (2024). High credibility.

SAXS measurement

SAXS was conducted on a Xenocs Xeuss 2.0 System by using an incident Cu-K Œ± X-ray beam that was perpendicular to the sample plane. X-ray wavelength, spot size and distance between specimen and detector were 0.154 nm, 172 √ó 172 Œºm 2 and 1185 mm, respectively. Samples were square slices with a side length of 10 mm. Scattering patterns were collected by a Pilatus 300 k detector. The distance distribution function, P (r), was analyzed by SASfit software.

(1) The inter-crystallite distance (R d) was obtained from the Bragg expression:Where q max is the scattering vector at which Iq 2 shows a maximum value.

(2) The radius of gyration (R g) could be calculated at low q according to Guinier equation:Where I (q) was the scattering intensity and I 0 was the zero angle scattering intensity. R g was used to represent the average size of crystallites.

(3) The pair-distribution function P (r) could be calculated through a Fourier transform of the scattering curve:

(4) The orientation order parameter (f) was calculated to describe the orientation degree of crystal domains. Its value ranges from 0 to 1, where the former corresponds to an isotropic structure and the latter corresponds to a perfect orientation structure along the director. A Maier-Saupe distribution function was used to fit the azimuthal angle profile:where I 0 is the free baseline intensity, œÜ is the azimuthal angle, œÜ 0 is the azimuth at the position of maximal intensity, and œâ is a parameter that determines the width of the distribution. After the fitting, parameter œâ was obtained, and the orientation factor f can be determined using the following formula:where the function P 2 (cos œÜ) is the second-order Legendre polynomial of cos œÜ and often referred to as the Hermans orientation function:

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^cf66bf7c]. Nature Communications (2023). High credibility.

Dose-response curves are key metrics in pharmacology and biology to assess phenotypic or molecular actions of bioactive compounds in a quantitative fashion. Yet, it is often unclear whether or not a measured response significantly differs from a curve without regulation, particularly in high-throughput applications or unstable assays. Treating potency and effect size estimates from random and true curves with the same level of confidence can lead to incorrect hypotheses and issues in training machine learning models. Here, we present CurveCurator, an open-source software that provides reliable dose-response characteristics by computing p-values and false discovery rates based on a recalibrated F-statistic and a target-decoy procedure that considers dataset-specific effect size distributions. The application of CurveCurator to three large-scale datasets enables a systematic drug mode of action analysis and demonstrates its scalable utility across several application areas, facilitated by a performant, interactive dashboard for fast data exploration.

---

### Fundamental limits to learning closed-form mathematical models from data [^a345300f]. Nature Communications (2023). High credibility.

Given a finite and noisy dataset generated with a closed-form mathematical model, when is it possible to learn the true generating model from the data alone? This is the question we investigate here. We show that this model-learning problem displays a transition from a low-noise phase in which the true model can be learned, to a phase in which the observation noise is too high for the true model to be learned by any method. Both in the low-noise phase and in the high-noise phase, probabilistic model selection leads to optimal generalization to unseen data. This is in contrast to standard machine learning approaches, including artificial neural networks, which in this particular problem are limited, in the low-noise phase, by their ability to interpolate. In the transition region between the learnable and unlearnable phases, generalization is hard for all approaches including probabilistic model selection.

---

### Viral kinetics of sequential SARS-CoV-2 infections [^658496bf]. Nature Communications (2023). High credibility.

The likelihood captures two distinct processes: the viral kinetic process, denoted by the bracketed term preceded by a (1‚àí Œª); and false negatives, denoted by the term preceded by a Œª. In the bracketed term representing the modeled viral kinetic process, f N (x | E [y], œÉ (t)) represents the Normal PDF evaluated at x with mean E [y] (generated by the model equations above) and observation noise œÉ (t). F N (0 | E[y], œÉ (t)) is the Normal CDF evaluated at 0 with the same mean and standard deviation. This represents the scenario where the true viral load goes below the limit of detection, so that the observation sits at the limit of detection. I lod is an indicator function that is 1 if y = 0 and 0 otherwise; this way, the F N term acts as a point mass concentrated at y = 0. Last, f Exp (x | Œ∫) is the Exponential PDF evaluated at x with rate Œ∫. We set Œ∫ = log (10) so that 90% of the mass of the distribution sat below 1 Ct unit and 99% of the distribution sat below 2 Ct units, ensuring that the distribution captures values distributed at or near the limit of detection. We did not estimate values for Œª or the exponential rate because they were not of interest in this study; we simply needed to include them to account for some small probability mass that persisted near the limit of detection to allow for the possibility of false negatives.

Model parameters were fit using a Hamilton Monte Carlo algorithm implemented in R (version 4.1.2) and Stan (version 2.21.3). Four chains were run for 2000 iterations each, and the first half of each chain was discarded as burn-in, yielding 4000 total posterior draws. Convergence was assessed using a Gelman‚ÄìRubin statistic of < 1.1 for all parameters and the absence of divergent transitions. Code for the full analysis is available at.

---

### Over-exploitation of natural resources is followed by inevitable declines in economic growth and discount rate [^b6b7ffb0]. Nature Communications (2019). High credibility.

We used algorithms that find the exact solutions provided that the resolutions are sufficiently fine. Specifically, to find the optimal solution numerically, our algorithm uses Stochastic Programming with backward induction (Supplementary Note 4). (Note that the model's dynamics are deterministic but the general method is still called stochastic.) To find the market solution, our algorithm also uses Stochastic Programming to solve for a given value of X. But it finds a solution multiple times, each time for a different value of X, until it finds the solution that satisfies the consisteny criterion. These algorithms are coded in C/C++ and are described in detail in Supplementary Note 4.

In turn, in the results shown in Fig. 3, system 2, as well as in Figs. 4 and 5 and in the graphical tool, we assume that the dynamics of c and f follow Eqs. 6‚Äì9, but we consider harvest functions that are not given by either the optimal solution or the market solution. In Fig. 3, system 2, we consider harvest functions that follow the market solution until t = t 1 and after t = t 1 + 10, but between these times, the non-sustainable harvest decreases gradually from its maximal level to zero. In Fig. 4, we calculate Œî sus, which is the cumulative discount that emerges if the harvest is entirely sustainable, namely, H n = 0 and H s = x 1 + x 2 if t > 0. Also, in Fig. 4a, we consider three scenarios in which the non-sustainable harvest is higher in the beginning but eventually approaches zero, while H n + H s = x 1 + x 2.

After we determine the harvest functions, the functions c (t) and f (t) are calculated according to Eqs. 6 and 9. In turn, we calculate the discount rate and the cumulative discount according to Eq. 5 (where the cumulative discount is the integral over time of the discount rate). Specifically, for the case in which only sustainable harvest is used (Œî sus in Fig. 4), the discount rates are calculated in Supplementary Note 2 and are given by Eqs. B5 and B12. The prices are given by Eq. A10, and the total product is given by Eq. A11. All of these equations are derived in Supplementary Notes 1, 2.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^01148c3a]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Nonparametric tolerance intervals for non-normal distributions: The above estimate of the tolerance interval would only be applicable to a population that is normally distributed, but when the underlying population is often not normal (eg, when there is a natural boundary that the data cannot exceed (ie, 0% or 100%)) it is helpful to define tolerance intervals using nonparametric methods; the one-sided nonparametric tolerance interval can be determined by finding the value for k that satisfies the cumulative binomial equation, where CL is the confidence level (eg, 0.95), and by setting k = 0 (ie, 0 failures) the formula can be simplified.

---

### Comparison of parameter optimization methods for quantitative susceptibility mapping [^26a0417b]. Magnetic Resonance in Medicine (2021). Medium credibility.

Purpose

Quantitative Susceptibility Mapping (QSM) is usually performed by minimizing a functional with data fidelity and regularization terms. A weighting parameter controls the balance between these terms. There is a need for techniques to find the proper balance that avoids artifact propagation and loss of details. Finding the point of maximum curvature in the L-curve is a popular choice, although it is slow, often unreliable when using variational penalties, and has a tendency to yield overregularized results.

Methods

We propose 2 alternative approaches to control the balance between the data fidelity and regularization terms: 1) searching for an inflection point in the log-log domain of the L-curve, and 2) comparing frequency components of QSM reconstructions. We compare these methods against the conventional L-curve and U-curve approaches.

Results

Our methods achieve predicted parameters that are better correlated with RMS error, high-frequency error norm, and structural similarity metric-based parameter optimizations than those obtained with traditional methods. The inflection point yields less overregularization and lower errors than traditional alternatives. The frequency analysis yields more visually appealing results, although with larger RMS error.

Conclusion

Our methods provide a robust parameter optimization framework for variational penalties in QSM reconstruction. The L-curve-based zero-curvature search produced almost optimal results for typical QSM acquisition settings. The frequency analysis method may use a 1.5 to 2.0 correction factor to apply it as a stand-alone method for a wider range of signal-to-noise-ratio settings. This approach may also benefit from fast search algorithms such as the binary search to speed up the process.

---

### Roflumilast (Zoryve) [^e2de6cd7]. FDA (2024). Medium credibility.

The dosage of roflumilast TOP for treatment of plaque psoriasis in adults is 1 application(s) TOP daily (0.3% cream or foam)

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^d4f96178]. Nature Communications (2025). High credibility.

Improving expressivity by increasing input multiplicity

Biologically, F a can be interpreted, for example, as depending on the chemostatted activity of an enzyme (see the Methods). In biochemical kinetics, it is common for some species to be involved in multiple reactions simultaneously, making it plausible for F a to drive multiple edges. We find that allowing for input multiplicity improves classification expressivity, and one way this happens is by lifting the monotonicity constraint. We assume for simplicity that each of the D input variables, whereis the set of input labels, affects the same numberof edges. Setting M > 1 lifts the monotonicity constraint because the condition for œÄ k (F i j) to be a monotonic function is that all other edge parameters are held fixed; with M > 1 this is no longer true since several edge parameters change simultaneously as an input is varied.

To better understand the gain in the decision boundary's flexibility allowed by setting M > 1, in the Supplementary Information we analyze the steady-state representation in the rational polynomial form of the matrix-tree expression, Eq. (2). Considering the case D = 1 and identifying turning points as roots of ‚àÇ œÄ i /‚àÇ F a, we show that the maximum number R of such roots obeyswhich is a direct measure of the classifier's expressivity; see Fig. 3 E for an illustration and the Supplementary Information for a numerical verification up to M = 4. A proof of the scaling 2 M ‚àí 1 for rational polynomials with non-negative coefficients can be found in ref. Thus, once M > 1, œÄ i is no longer subject to the monotonicity constraint and behaves like a non-negative rational polynomial of degree up to 2 M. Input multiplicity thus allows the non-equilibrium biological process to be more expressive and draw out decision boundaries that can classify more complex data structures. Indeed, returning to the previously failed classification with M = 1 (Fig. 3 C), we see that setting M = 2 allows the same network to now learn a decision boundary which successfully encloses the data assigned to class 1 (Fig. 3 F). This implies that classifying a finite band of input signal levels (like a band-pass filter) requires setting M > 1 along the corresponding input dimension. A recent development in synthetic biology has in fact shown in a specific example that drug binding to receptor molecules via two distinct binding pathways can be used to design band-pass-like responses to the drug (Fig. 3 G).

---

### A blueprint for a synthetic genetic feedback optimizer [^a783fbf8]. Nature Communications (2023). High credibility.

We next investigate how the dynamic range of the indicator signals, the Hill coefficient, and the dissociation constants of the logic gates affect closed loop performance. As illustrated in Fig. 4, the behavior is insensitive to the dissociation constants K ‚àß and K ‚à®, to the input dynamic range (determined by Œ± c,2, see Supplementary Section 1.3), and to the Hill coefficient n (Supplementary Fig. 4). Thus, the value of these parameters represent non-critical design choices, unlike the timescale on which the logic module evolves. As expected, the optimizer successfully locates the optimum as long as the dynamics of the logic module evolve on a timescale faster than that of the comparator module (so that the quasi-steady state approximation above is accurate on the slower timescale of the system), however, further speed reduction causes closed loop performance to quickly deteriorate (Supplementary Fig. 5).

Closed loop performance of the simplified optimizer

Before outlining a concrete molecular implementation of the whole integrated system, here we analyze closed loop performance and accuracy when considering the abstract modules introduced above. We first assume that the delay module ensures reference tracking with no error and that the comparator module realizes perfect comparison of its input signals when generating the output indicator signals. We then focus on a less ideal but more practical realization of the optimizer module, and reveal how the presence of errors in delay and comparison affect closed loop performance.

The time it takes to approach the optimum x * depends on the position of the initial value x 0 relative to x *. However, closed loop performance after this initial transient is virtually identical for all (x 0, x *) pairs in the absence of tracking and comparison error (Supplementary Fig. 6a). Importantly, closed loop performance is robust to environmental disturbances that cause drastic shifts in the optimum and also to noise (Fig. 5a). Furthermore, as long as the delay module evolves on a timescale faster than the regulator (œµ d ‚â™ œµ x), closed loop performance is insensitive to the value of œµ d, as expected: e.g. a 10-fold increase in the value of œµ d yields virtually no change (red and green curves in Fig. 5a).

---

### Bayesian log-normal deconvolution for enhanced in silico microdissection of bulk gene expression data [^ac57cdf8]. Nature Communications (2021). High credibility.

Fig. 2
Comparison of normal, negative binomial, and log-normal distribution in fitting linear-scale gene expression data.

a A bar chart of average log-likelihood of the three types of distribution fitted to PBMC single-cell RNA-seq data. The genes were split by DEGs (red; n = 1723) and non-DEGs (blue; n = 1496). b Comparison of the distance of the estimated mode to the true mode (y -axis) per distribution type (x-axis). The standard boxplot notation was used (lower/upper hinges ‚Äî first/third quartiles; whiskers extend from the hinges to the largest/lowest values no further than 1.5 * inter-quartile ranges). c Pairwise comparison of per-gene log-likelihood of log-normal distribution (y -axis) and that of normal (x -axis; top) and negative binomial distribution (x -axis; bottom). The genes were split into non-DEGs (left) and DEGs (right). d Density plots for raw-counts (red) and optimized log-normal (green), normal (blue), and negative binomial distribution (purple) for four example genes (gene name at the top) with low maximum log-likelihood for normal distribution. e, f Maximum log-likelihood values (e) and root mean squared error (root MSE: f) of each gene for log-normal (y -axis) and negative binomial (x -axis) convolutions of T = 8 cell types, applied to TCGA-MESO (left) and TCGA-SARC (right) data.

---

### Designing accurate emulators for scientific processes using calibration-driven deep models [^5e03881e]. Nature Communications (2020). High credibility.

Predictive models that accurately emulate complex scientific processes can achieve speed-ups over numerical simulators or experiments and at the same time provide surrogates for improving the subsequent analysis. Consequently, there is a recent surge in utilizing modern machine learning methods to build data-driven emulators. In this work, we study an often overlooked, yet important, problem of choosing loss functions while designing such emulators. Popular choices such as the mean squared error or the mean absolute error are based on a symmetric noise assumption and can be unsuitable for heterogeneous data or asymmetric noise distributions. We propose Learn-by-Calibrating, a novel deep learning approach based on interval calibration for designing emulators that can effectively recover the inherent noise structure without any explicit priors. Using a large suite of use-cases, we demonstrate the efficacy of our approach in providing high-quality emulators, when compared to widely-adopted loss function choices, even in small-data regimes.

---

### Input-output maps are strongly biased towards simple outputs [^50fca0dd]. Nature Communications (2018). Medium credibility.

Many systems in nature can be described using discrete input-output maps. Without knowing details about a map, there may seem to be no a priori reason to expect that a randomly chosen input would be more likely to generate one output over another. Here, by extending fundamental results from algorithmic information theory, we show instead that for many real-world maps, the a priori probability P(x) that randomly sampled inputs generate a particular output x decays exponentially with the approximate Kolmogorov complexity [Formula: see text] of that output. These input-output maps are biased towards simplicity. We derive an upper bound P(x)‚â≤[Formula: see text], which is tight for most inputs. The constants a and b, as well as many properties of P(x), can be predicted with minimal knowledge of the map. We explore this strong bias towards simple outputs in systems ranging from the folding of RNA secondary structures to systems of coupled ordinary differential equations to a stochastic financial trading model.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^221445c7]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Pulmonary function reporting conclusions ‚Äî The ATS PFT Committee believes that wide adoption of the formats presented above and their underlying principles by equipment manufacturers and pulmonary function laboratories can improve the interpretation, communication, and understanding of test results. Limiting the number of parameters reported and showing the LLN next to the measured value should improve interpretive accuracy, particularly for those less experienced, and reserving the word percent to percent predicted value should reduce errors. Showing the measured values relative to the normal distribution in a simple linear graphic (with or without reporting a z-score) can enhance understanding of the result. Quality review of PFTs needs to move beyond "did, or did not, meet ATS standards", but a grading system is most helpful if the grades have a common meaning; therefore adoption of a uniform system is desirable.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^0e6e1ad3]. Annals of the American Thoracic Society (2023). High credibility.

Regional VÃáA/QÃá heterogeneity metrics ‚Äî regional VÃáA/QÃá heterogeneity can be described with simple functional models corresponding to a few compartments and a narrow unimodal VÃáA/QÃá distribution; small-length scale heterogeneity develops in disease and under mechanical ventilation, resulting in heterogeneous VÃáA/QÃá distributions; metrics of VÃáA/QÃá matching include global heterogeneity and the second moments (log scale) of ventilation (LogSDV) and perfusion (LogSDQ) versus VÃáA/QÃá distributions.

---

### Fundamental limits to learning closed-form mathematical models from data [^5954e9c9]. Nature Communications (2023). High credibility.

Fig. 1
Probabilistic model selection makes quasi-optimal predictions about unobserved data.

We select two models m *, whose expressions are shown at the top of each column. a, b From each model, we generate synthetic datasets D with N points (shown, N = 100) and different levels of noise s œµ (shown, s œµ = 1). Here and throughout the article, the values of the independent variables x 1 and x 2 are generated uniformly at random in [‚àí 2, 2]. Vertical lines show the observation error œµ i for each point in D. For a model not drawn from the prior and data generated differently, see Supplementary Fig. S1. c, d For each dataset D (with dataset sizes N ‚àà {25, 50, 100, 200, 400}), we sample models from p (m ‚à£ D) using the Bayesian machine scientist, select the MDL model (maximum p (m ‚à£ D)) among those sampled, and use this model to make predictions on a test dataset, generated exactly as D. We show the prediction root mean squared error (RMSE) of the MDL model onas a function of N and s œµ. For comparison, we also show the predictions from an artificial neural network (ANN, dotted lines; Methods). Since s œµ is the irreducible error, predictions on the diagonal RMSE = s œµ are optimal. e, f We plot the prediction RMSE scaled by the irreducible error s œµ; optimal predictions satisfy RMSE/ s œµ = 1 (dashed line).

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^990cb6a1]. Annals of the American Thoracic Society (2023). High credibility.

Ventilation‚Äìperfusion heterogeneity metrics ‚Äî modeling description ‚Äî states that regional V_A/Q heterogeneity can be described with simple functional models corresponding to a few compartments and a narrow unimodal V_A/Q distribution; small-length scale heterogeneity develops in disease and under mechanical ventilation, resulting in heterogeneous V_A/Q distributions; and metrics include global heterogeneity and the second moments (log scale) of ventilation (LogSDV) and perfusion (LogSDQ) versus V_A/Q ratios distributions.

---

### Evidence for line width and carrier screening effects on excitonic valley relaxation in 2D semiconductors [^c96359ab]. Nature Communications (2018). Medium credibility.

To further verify Eq. (4) at the finite-temperature conditions and achieve realistic engineering on the excitonic valley polarization, we fabricated 1L-WSe 2 stacked on a multilayer graphene flake, as shown in Fig. 4a (see Methods). For this type of sample, we expect that the exciton linewidth could be narrow because of the reduced scattering on the atomically flat surface of multilayer grapheneand because of the shorter ‚å© œÑ x ‚å™ resulting from charge or energy transfer to the graphene. Modulating the carrier density in 1L-WSe 2 may be possible because of the chemical potential difference between the graphene and 1L-WSe 2.

Fig. 4
Valley polarization engineering. a Optical microscope image of 1L-WSe 2 stacked on a multilayer graphene flake (on-graphene) on a quartz substrate. The scale bar indicates 10 Œºm. b Polarization-resolved PL spectra of the on-graphene 1L-WSe 2 under œÉ + excitation at 10‚Äì160 K. The red and black curves correspond to the œÉ + and œÉ ‚àí PL intensities, I œÉ + and I œÉ ‚àí, respectively. c Comparison of the exciton PL line shapes at 10 K for the on-graphene and the on-quartz 1L-WSe 2 samples. d Œì h as a function of temperature for the on-graphene 1L-WSe 2. Gray-shaded region indicates the uncertainty range of Œì h deduced from the Voigt fitting procedure. Bars are the mean values in the uncertainty range at each temperature. e Time-resolved PL decay profiles of excitons in the on-graphene 1L-WSe 2 measured at 160 K (red), 120 K (green), 100 K (blue), and 80 K (black), respectively. f ‚å© œÑ x ‚å™ in the on-graphene 1L-WSe 2 plotted as a function of temperature. The error bars represent the uncertainties in the fitting procedure. g œÅ x as a function of temperature for the on-graphene 1L-WSe 2 (red circles) compared with those of on-quartz (black circles, same data are shown in Fig. 3a) 1L-WSe 2. The orange-shaded region is the prediction band of œÅ x for the on-graphene 1L-WSe 2 calculated using Eq. (1); œÅ 0 = 0.7, ‚å© œÑ x ‚å™ plotted in f, and œÑ v calculated using Eq. (3) (with the Œì h in the uncertainty range shown in d and E F = 19 meV) were used as input parameters for Eq. (1). The green-shaded region is a prediction band for the on-quartz 1L-WSe 2 with E F = 13 meV. The error bars correspond to the uncertainties of the polarization-resolved measurements. h œÅ x predicted for various E F as functions of temperature. The inset shows œÑ v calculated using Eq. (3) with the mean values of Œì h shown in d and various E F values. Solid and dotted curves are for on-graphene and on-quartz samples, respectively

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^37daae40]. Nature Communications (2025). High credibility.

Fig. 2
The matrix-tree theorem.

A Computing the steady-state occupancy œÄ 1 by summing weights over directed spanning trees. Directed spanning trees are subgraphs containing all graph nodes but no cycles, with edges oriented toward a root node. In each directed spanning tree, the input forces make a positive, negative, or zero contribution to the tree weight. The structural vectorsare shown below each tree; these quantities enter into Equation (3) below. B Schematic illustration of the high-dimensional space of feature vectors œà (i; Œ∏) and œá (i, F). The depicted arrangement of vectors could solve a binary classification problem.

We define the input multiplicityas the number of edges affected per input variable, which we assume to be the same for each input. To focus on the functional way in which the input driving enters the steady-state probabilities, the driving contributions can be factored out in the algebraic expressions for the numerator and denominator of Equation (1). This has been previously been used to make analytical progress for M = D = 1 in, for example, refs. ‚Äì. This equivalent formulation of Eq. (1) suggests that steady states of Markov jump processes implement a rational polynomial function of exponentiated input variables. Defining, we rewrite the matrix-tree expression for œÄ i for general D and M We use the multi-index, whereis the set of D input labels and each componentof the multi-index runs over the values, to enumerate themonomials. These monomials y Œº (F) in Equation (2) combinatorially depend on the different mixtures Œº of input driving, representing a net total Œº a of signed contributions from the input force F a, Œº b such contributions for F b, and so on for each input. The coefficients, which are functions of the parameters Œ∏, are the sums of weights over all directed spanning trees rooted at node i which have the corresponding mixture Œº of signed input contributions. The monomial coefficientsthus represent learnable amplitudes of each polynomial basis function y Œº (F). The coefficients in the denominator are defined as. Classification will be successful if, for F œÅ drawn from class œÅ, the coefficientsand monomials y Œº (F œÅ) are large for the same Œº. In the subsequent sections of the paper and in the Supplementary Information we use the formulation in Equation (2) to show how the classification ability of a non-equilibrium Markov processes may be systematically modulated.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^89cdccc2]. Annals of the American Thoracic Society (2023). High credibility.

V_A/Q distribution modeling ‚Äî derived from cumulative plots of ventilation or perfusion versus V_A/Q ‚Äî is binned into 50 equally spaced log-scale compartments, with shunt and dead space handled separately. Specifically, V_A/Q ratios < 0.005 (shunt) and > 100 (dead space) are calculated as separate components, and relative dispersion and gravitational gradients may also be assessed.

---

### ACR-AAPM-SPR practice parameter for diagnostic reference levels and achievable doses in medical X-ray imaging [^a7ff7c7f]. ACR/AAPM/SPR (2023). High credibility.

Documentation ‚Äî Reporting should be in accordance with the ACR Practice Parameter for Communication of Diagnostic Imaging Findings.

---

### Overcoming the design, build, test bottleneck for synthesis of nonrepetitive protein-RNA cassettes [^0e7aa985]. Nature Communications (2021). High credibility.

Next, to sort the variants in accordance with their likelihood of binding the RBP (i.e. similarity of their dose‚Äìresponse to the positive controls), we carried out the following computation (for details, see Methods). First, we characterized all variants by calculating a vector composed of three components: the slope of a linear regression, its goodness of fit (R 2), and standard deviation of the fluorescence value at the three highest induction bins (Fig. 2b, middle). Next, we computed two multivariate Gaussian distributions using the empirical 3-component vectors, that were extracted for the positive and negative controls and for the given RBP, to yield a probability distribution function (pdf) for both the responsive and non-responsive variants, respectively (Fig. 2b, right). The two populations are relatively well-separated from one another, presenting two distinct clusters with minor overlap. Finally, we defined the "responsiveness score" for each variant (R score - see Methods for a formal definition) as the logarithm of the ratio of the probabilities computed by the responsive pdf to the non-responsive pdf. This score was computed for each unique barcode, and the final result for a sequence variant was averaged over up to five vectors, one for every variant barcode that passed the read-number and basal-level thresholds (see Supplementary Fig. 2 and Methods).

---

### Spin dephasing under nonlinear gradients: implications for imaging and field mapping [^eaf2803c]. Magnetic Resonance in Medicine (2012). Low credibility.

This work examines the prototypical MR echo that would be expected for a voxel of spins evolving in a strong nonlinear field, specifically focusing on the quadratic z(2) - ¬Ω(x(2) + y(2)) field. Dephasing under nonlinear gradients is increasingly relevant given the growing interest in nonlinear imaging, and here, we report several notable differences from the linear case. Most notably, in addition to signal loss, intravoxel dephasing under gradients creating a wide and asymmetric frequency distribution across the voxel can cause skewed and nonlinear phase evolution. After presenting the qualitative and analytical origins of this difference, we experimentally demonstrate that neglecting these dynamics can lead to significant errors in sequences that assume phase evolution is proportional to voxel frequency, such as those used for field mapping. Finally, simplifying approximations to the signal equations are presented, which not only provide more intuitive forms of the exact expression but also result in simple rules to predict key features of the nonlinear evolution.

---

### Complex systems and the technology of variability analysis [^e26f6d93]. Critical Care (2004). Low credibility.

F(n) = v (1/N S k = 1 N [y(k) 2 - y n (k) 2])

By performing this analysis for all values of n, it is possible to calculate the relationship between F(n) and n. Scaling or fractal correlation is present if the data is linear on a graph of log F(n) versus log(n). The slope of the graph has been termed a, the scaling exponent, which will vary from 0.5 (white noise or uncorrelated random data) to 1.5 (Brownian noise or integrated white noise or random walk). When a = 1, behaviour corresponds to the 1/f noise. As a increases above 1 to 1.5, behaviour is no longer determined by a power law. Because the linear relationship between log F(n) and log(n) appears to have two distinct linear segments, one for small (n < 11) and large n (n > 11), the slopes of both lines are calculated separately and termed a 1 and a 2, respectively; repeatedly, a 1 has proven superior to a 2 in terms of prognostic ability.

Approximate entropy

ApEn is a measure of 'irregularity'; smaller values indicate a greater chance that a set of data will be followed by similar data (regularity), and a greater value for ApEn signifies a lesser chance of similar data being repeated (irregularity). To calculate ApEn of a series of data, the data series is evaluated for patterns that recur. This is performed by evaluating data sequences or runs of length m, and determining the likelihood that other runs of length m are similar, within a tolerance r. Thus, two parameters, m and r, must be fixed to calculate ApEn. Increased regularity is associated with illness.

The following is a description of the calculation of ApEn. Given any sequence of data points u(i) from i = 1 to N, it is possible to define vector sequences x(i), which consist of length m and are made up of consecutive u(i), specifically defined by the following:

---

### Parabolic avalanche scaling in the synchronization of cortical cell assemblies [^a0c5a709]. Nature Communications (2023). High credibility.

Complexity analysis

Complexity analysis was performed as described in ref.exploring patterns of depths D in the range of 4‚Äì7. Complexity C was calculated on the population activity as a function of k and thresholding, identically to how epochs were computed to obtain scaling. Subthreshold activity was not evaluated. For each segment of length D, its pattern p is defined as the rank order of the time series (e.g. for a monotonically increasing trace over D = 4 time points, the pattern would be 0123; see Supplementary Fig. 15 for more examples). From all possible length D segments, a probability distributionis obtained (N is the number of possible states, e.g. for D = 3, the possible states are 012, 021, 102, 120, 201, and 210). Next, the Shannon's logarithmic information is computed asand the normalized Shannon entropy is defined as, where P e is the uniform distribution (entropy is maximized for the uniform distribution, therefore 0 ‚â§ H ‚â§ 1). Finally, the complexity measure is defined as

Q 0 is a normalization constant (0 ‚â§ Q J ‚â§ 1) equal to the inverse of the maximum possible value of J [P, P e].

Temporal profiles

Temporal profiles were calculated on individual recordings after thresholding and temporal coarse-graining had been applied. Avalanches were obtained as described for our experimental data and grouped based on duration. Population activity during all avalanches of a certain duration was averaged over all avalanches to get the temporal profile for that duration.

To calculate the profile collapse exponent, œá coll, temporal profiles were first x-rescaled to fit between 0 and 1, then linearly interpolated to N = 500 points. Every interpolated temporal profile in the desired duration range was then y-rescaled by d œá coll ‚àí 1. The exponent œá coll was chosen to minimize the RMS error between all collapsed shapes in the desired duration range. To calculate temporal profiles at a certain coarse-graining k using the original experimental frame rate, avalanche epochs were calculated at the coarse-grained resolution as described above. Then, these epochs were mapped back onto the raster at the original frame rate and temporal profiles were calculated on these remapped epochs. The standard deviations from the mean rescaled shapes were visualized as shaded area if not stated otherwise.

Neural simulations

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^59d10d7c]. Annals of the American Thoracic Society (2023). High credibility.

Ventilation‚Äìperfusion (V A /Q) distribution modeling is binned into 50 equally spaced (log scale) compartments, with V A /Q ratios < 0.005 (shunt) and > 100 (dead space) separately calculated, and relative dispersion and gravitational gradients may also be assessed.

---

### Fludeoxyglucose f 18 injection (fludeoxyglucose F 18) [^3b35097e]. FDA (2020). Medium credibility.

EXTERNAL RADIATION

The specific gamma ray constant for Fluorine F 18 is 6.0 R/hr/mCi (0.3 Gy/hr/kBq) at 1cm. The half-value layer (HVL) for the 511 keV photons is 4.1 mm lead (Pb). A range of values for the attenuation of radiation results from the interposition of various thicknesses of Pb.

The range of attenuation coefficients for this radionuclide is shown in Table 2. For example the interposition of an 8.3 mm thickness of Pb, with a coefficient of attenuation of 0.25, will decrease the external radiation by 75%.

For use in correcting for physical decay of this radionuclide, the fractions remaining at selected intervals after calibration are shown in table 3.

---

### An end-to-end framework for the prediction of protein structure and fitness from single sequence [^c654ac9b]. Nature Communications (2024). High credibility.

Pearson correlation coefficient (r) is a measure used to quantify the strength of the linear relationship between two variables, X and Y. As shown in the Eq. (4), it is computed by calculating the ratio of the covariance between the two variables to the product of their standard errors. The coefficient ranges from ‚àí1 to 1, where ‚àí1 indicates a perfect negative correlation, 0 indicates no correlation, and 1 indicates a perfect positive correlation.where cov denotes the covariance and œÉ stands for the standard deviation (i.e. the square root of variance).

Spearman correlation coefficient (œÅ) is commonly used to describe the strength of a monotonic relationship between two variables. As shown in the Eq. (5), Spearman correlation coefficient is calculated by utilizing the ranked values of a pair of variables (X, Y). This characteristic makes the Spearman correlation coefficient more robust to outliers in the data.where R denotes the ranking operation for the variables.

Kendall correlation coefficient (œÑ) is another non-parametric metric to measure the correlation between ranks of the variables and can be interpreted as the probabilities of observing the agreeable (concordant) and non-agreeable (discordant) pairs (Equation (6)). The Kendall correlation coefficient is more robust than the Spearman correlation coefficient while usually being smaller in magnitude.where n c denotes the number of concordant pairs while n d denotes the number of discordant pairs.

Top K precision is a metric measuring the fraction of the truly top K mutations among the predicted top K mutations (Equation (7)). This metric serves as a reference for the success rate in the real-world protein engineering process.whereand r a n k (Y i) denote the rank (in descending order) of the predicted value and that of the label, respectively, and I is the indicator function.

---

### Semiparametric estimation of the relationship between ROC operating points and the test-result scale: application to the proper binormal model [^9a9d14d8]. Academic Radiology (2011). Low credibility.

Rationale and Objectives

Semiparametric methods provide smooth and continuous receiver operating characteristic (ROC) curve fits to ordinal test results and require only that the data follow some unknown monotonic transformation of the model's assumed distributions. The quantitative relationship between cutoff settings or individual test-result values on the data scale and points on the estimated ROC curve is lost in this procedure, however. To recover that relationship in a principled way, we propose a new algorithm for "proper" ROC curves and illustrate it by use of the proper binormal model.

Materials and Methods

Several authors have proposed the use of multinomial distributions to fit semiparametric ROC curves by maximum-likelihood estimation. The resulting approach requires nuisance parameters that specify interval probabilities associated with the data, which are used subsequently as a basis for estimating values of the curve parameters of primary interest. In the method described here, we employ those "nuisance" parameters to recover the relationship between any ordinal test-result scale and true-positive fraction, false-positive fraction, and likelihood ratio. Computer simulations based on the proper binormal model were used to evaluate our approach in estimating those relationships and to assess the coverage of its confidence intervals for realistically sized datasets.

Results

In our simulations, the method reliably estimated simple relationships between test-result values and the several ROC quantities.

Conclusion

The proposed approach provides an effective and reliable semiparametric method with which to estimate the relationship between cutoff settings or individual test-result values and corresponding points on the ROC curve.

---

### The simplicity of protein sequence-function relationships [^84b22d02]. Nature Communications (2024). High credibility.

Fig. 1
Reference-free analysis (RFA) of genetic architecture.

a Illustration of RFA on a 2-site, 2-state genetic architecture. The four possible genotypes are arranged on a plane with their phenotype indicated by elevation. (First panel) The zero-order effect (e 0) is the mean phenotype of all genotypes, marked by the clear plane with cyan edges. The first-order effect of state A or B at site 1 [e 1 (A) or e 1 (B), green arrows] measures how the mean phenotype of all genotypes containing the state (dashed line) differs from the global mean. The green plane predicts the phenotype based on the state at site 1. (Second panel) First-order effects at site 2 are defined similarly and shown in pink. (Third panel) The first-order model predicts phenotypes by summing the first-order effects of all genetic states plus the global mean, shown as the gray plane tilted in both dimensions; the fraction of phenotypic variance explained is shown. (Fourth panel) The pairwise interaction between states A and B at sites 1 and 2 [e 1, 2 (A, B)] measures how the mean phenotype of all genotypes containing the two states [here just one genotype (A, B)] differs from the first-order prediction. b We implement RFA with a nonlinear link function to incorporate nonspecific epistasis. Each variant's genetic score (s) is the sum of the effects of its genetic states. The link function transforms s of each variant into its phenotype, y. Although the link function can take any form, here we use a simple sigmoid defined by two parameters representing the upper and lower bounds of the measurable phenotype. c (Left) A 5-site, 3-state genetic architecture was simulated by drawing reference-based effects from the standard normal distribution (but setting all fifth-order effects to zero); a small amount of simulated noise was added to the phenotypes. (Middle) Absolute error of RFA terms computed from the simulated measurements. Dashed lines, mean absolute error of individual phenotypes. Supplementary Fig. 1 shows the individual inferred terms. (Right) The fraction of phenotypic variance explained by the true, directly computed, and regression-estimated RFA terms. Supplementary Section 1.1 analyzes additional simulated genetic architectures.

---

### Estimation in medical imaging without a gold standard [^82dda7fe]. Academic Radiology (2002). Low credibility.

Rationale and Objectives

In medical imaging, physicians often estimate a parameter of interest (eg, cardiac ejection fraction) for a patient to assist in establishing a diagnosis. Many different estimation methods may exist, but rarely can one be considered a gold standard. Therefore, evaluation and comparison of different estimation methods are difficult. The purpose of this study was to examine a method of evaluating different estimation methods without use of a gold standard.

Materials and Methods

This method is equivalent to fitting regression lines without the x axis. To use this method, multiple estimates of the clinical parameter of interest for each patient of a given population were needed. The authors assumed the statistical distribution for the true values of the clinical parameter of interest was a member of a given family of parameterized distributions. Furthermore, they assumed a statistical model relating the clinical parameter to the estimates of its value. Using these assumptions and observed data, they estimated the model parameters and the parameters characterizing the distribution of the clinical parameter.

Results

The authors applied the method to simulated cardiac ejection fraction data with varying numbers of patients, numbers of modalities, and levels of noise. They also tested the method on both linear and nonlinear models and characterized the performance of this method compared to that of conventional regression analysis by using x-axis information. Results indicate that the method follows trends similar to that of conventional regression analysis as patients and noise vary, although conventional regression analysis outperforms the method presented because it uses the gold standard which the authors assume is unavailable.

Conclusion

The method accurately estimates model parameters. These estimates can be used to rank the systems for a given estimation task.

---

### Trapping of drops by wetting defects [^1cd66ff7]. Nature Communications (2014). Medium credibility.

Calibration of dissipation coefficient

To determine the damping coefficient of the sliding drops on the homogeneous surface, we measured the steady sliding velocity v 0 away from the trap as a function of inclination angle. Figure 6 shows the expected linear relation between v 0 and sin Œ±. Owing to the finite residual hysteresis, the curves intercept the abscissa at a finite value sin Œ± 0. The balance between gravity, viscous friction and pinning is

we can determine both Œª and F p from the these calibration measurements. The pinning forces are related to contact angle hysteresis by F p = 2 R Œ≥(cos Œ∏ r ‚Äìcos Œ∏ a), where Œ∏ r and Œ∏ a are the receding and the advancing contact angle, respectively. From this expression, we find sin Œ± 0 = 2 R Œ≥(cos Œ∏ r ‚Äìcos Œ∏ a)/ mg, leading to a critical sliding angle Œ± 0 = 1‚Ä¶ 3¬∞, decreasing with increasing drop size, in agreement with the experimental results (Fig. 6).

From the slope of the calibration curves in Fig. 6, we can extract the damping coefficients Œª w = (4.5 ¬± 0.2) √ó 10 ‚àí4, (5.9 ¬± 0.1) √ó 10 ‚àí4, (5.8 ¬± 0.5) √ó 10 ‚àí4 kg s ‚àí1 for the water drops of 20, 40 and 60 Œºl, and Œª wg = (1.22 ¬± 0.03) √ó 10 ‚àí2 kg s ‚àí1 and (1.32 ¬± 0.03) √ó 10 ‚àí2 kg s ‚àí1 for the 40 and 60 Œºl water‚Äìglycerol drops. This weak size dependence is attributed to the different scaling of driving, viscous and pinning forces with the drop size. For the purpose of our analysis, this minor variation can be neglected. Dividing the values of Œª by the drop width 2 R, we find values for the contact line friction coefficient Œæ w ‚âà100 mPa¬∑s and Œæ wg ‚âà5 Pa¬∑s. The value for water is somewhat larger than the one extracted in earlier experiments with sliding drops on dry substrates. The additional contribution may arise from the viscous dissipation of the oil film on the substrate as described by Smith et al.

---

### Reconfigurable metamaterial processing units that solve arbitrary linear calculus equations [^ecc1706e]. Nature Communications (2024). High credibility.

Discussion

Solving calculus equations has significant applications in mechanics, electromagnetics, and other fields. Here, we present two examples of solving practical calculus equations with the proposed MPU. The first one is about earthquake-induced structural vibrations. As shown in Fig. 4a, structural vibrations occur in buildings when subjected to external forces such as earthquakes. Considering or predicting this structural vibration is an important aspect of building design. To simplify the problem, the structural vibration is modeled as a one-dimensional damped-forced vibration, as shown in Fig. 4 b, c. Based on the principle of mechanics, the trajectory of the object x (t) and the external force F (t) are subjected to the following linear differential equation:

Fig. 4
Diagrams of practical examples for solving calculus equations.

a A schematic diagram of the vibration of a building during an earthquake, where the motion trajectory of the building is affected by external forces and damping. b Simplify the motion of the building to one dimension. c It can be modeled by a forced damped vibration model, where the applied external force F(t) and the trajectory x (t) of the object satisfy the following equation: mx "(t) + Œºx '(t) + kx = F (t). d Assume that the external force varies as a function of time as F (t) = cos(2œÄ t / T) + 0.3*sin(3œÄ t / T), where T = 200/9 ns. e It is presumed to have m = 2, Œº = 0.2, and k = ‚àí1.414. The blue line is the result of numerical simulation, while the orange line is the result of MPU, which has good consistency. f In the RLC series circuit, a voltage u (t) is applied over time, and the charge q (t) at the terminals of the capacitor satisfies the equation: Lq "(t) + Rq '(t) + q (t)/ C = u (t). g Assume that the voltage varies as a function of time as u (t) = cos(2œÄ t / T) + 0.5*sin(3œÄ t / T) + sin(6œÄ t / T), where T = 200/9 ns. h It is presumed to have L = 0.2, R = 2, and C = 0.707. The blue line is the result of numerical simulation, while the orange line is the result of MPU, which has good consistency.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^6e468ce5]. Annals of the American Thoracic Society (2023). High credibility.

General principles of biophysical computational models ‚Äî biophysical computational models can be used for forward simulations of function or inverse identification of system structure and parameters. Forward models are described as useful for sensitivity analyses to evaluate the contribution of normal subject variability to imaging metrics to define a threshold for abnormal and can also predict emergent behavior after parameter changes at the cellular or tissue level. Inverse models are described as useful for deriving information that cannot be measured directly, including distributions of airway or vascular obstruction that produce specific VÀôA/QÀô patterns. Both forward and inverse models can be used to analyze, interpret, and predict.

---

### Overcoming the design, build, test bottleneck for synthesis of nonrepetitive protein-RNA cassettes [^ee2c8053]. Nature Communications (2021). High credibility.

In the analysis carried out in this paper, we chose to reduce the parameter space to a 3-dimensional space consisting of the following components: the slope (m) and goodness of fit (R 2) to a simple linear fit of the rescaled fluorescenceto inducer concentration values. The third component is the standard deviation (std) ofcomputed at the three highest concentration induction bins. We term this vector:

Based on the 3-dimensional space (R 2, m, and std) we conducted a multivariant Gaussian fit for the positive and negative control populations (Fig. 2), which in turn allowed us to compute the 3-dimensional pdf(pos, n) and pdf(neg, n). Finally, we computed the R score for each non-control variant by averaging the score over as many barcodes which passed our filters (each variant appeared in our library 5 times). The results of this computation are presented in the heatmaps of Fig. 2 and Fig. S3, which are arranged in accordance with decreasing R score. Up to this point, we have developed the R score to sort the different variants, but did not dive into what it means physically or from a binding perspective. The approach relied on mapping the behavior of the positive binding controls and non-binding negative controls in some 3-dimensional parameter space, and computing the likelihood that a given variant would belong to one or the other group. The R score is the log of the ratio of the two computations. In principle, R score can be computed from any number of probability density functions. We could have used the original 6D space consisting of the 6 inducer concentrations, or chosen any other combination. In the computation below, we will map the 6D space to a 1D space of binding affinities that can be in principle computed from each 6-vector using a Hill function fit. In the case of such a mapping, we can replace eqn. 7 and 8 in the paper with the following terms:

---

### Clinical quantification of myocardial blood flow using PET: joint position paper of the SNMMI cardiovascular council and the ASNC [^483130d2]. Journal of Nuclear Cardiology (2018). Medium credibility.

Physiologic ranges with 13N‚Äìammonia and 82Rb PET ‚Äî reference MBF and myocardial flow reserve (MFR) in healthy subjects: In 23 studies of 363 healthy subjects using 13N‚Äìammonia PET, weighted mean MBF at rest and stress were 0.71 mL/minute/g (range 0.61‚Äì1.1) and 2.58 mL/g/minute (range 1.86‚Äì4.33), with weighted mean MFR 3.54 (range 3.16‚Äì4.48); for 82Rb PET across 382 healthy subjects in 8 studies, weighted means were 0.74 mL/g/minute (range 0.69‚Äì1.15) at rest, 2.86 mL/g/minute (range 2.5‚Äì3.82) at stress, and MFR 4.07 (range 3.88‚Äì4.47). These represent physiologic ranges from young, healthy volunteers without coronary risk factors, and in clinical populations values below these ranges may be seen and may not represent obstructive epicardial CAD, with modest reductions often due to diffuse CAD and microvascular disease.

---

### How measurement noise limits the accuracy of brain-behaviour predictions [^886ef579]. Nature Communications (2024). High credibility.

In order to induce increasing levels of noise in the target variable, we created datasets that correlated with the originally observed (empirical) targets at a pre-specified Pearson's correlation. This method was chosen to increase the interpretability of the resulting attenuation of brain‚Äìbehaviour associations by controlling the amount of noise. The data generation procedure was as follows: First, a random vector was sampled from a standard normal distribution with the same mean and standard deviation as the original empirically acquired data (in the HCP these were age-adjusted and normalised to mean = 100 and SD = 15). Next, we calculated the residuals of a least squares regression of the sampled vector (X) on the empirical data (Y). The resulting orthogonal vector representing the portion of X that is independent of Y was then again combined with the original empirical data Y through scaling by the pre-specified correlation. This adjustment process manipulated the relative contributions of Y and the residuals of X on Y in the resulting simulated vector. The formula used for this process was:where X YœÅ is the new 'simulated' vector that correlates with the empirical data Y at a predefined correlation.represents the residuals of a least squares regression of a randomly sampled vector X against Y. All simulations were created using custom code in R [version 4.0.4] and are provided online.

---

### Sodium fluoride f 18 [^ca730735]. FDA (2024). Medium credibility.

11.2 Physical Characteristics

Fluoride F 18 decays by positron (Œ≤+) emission and has a half-life of 109.7 minutes. Ninety-seven percent of the decay results in emission of the positron with a maximum energy of 633 keV and 3% of the decay results in electron capture with subsequent emission of characteristic X-rays of oxygen. The principal photons useful for diagnostic imaging are the 511 keV gamma photons, resulting from the interaction of the emitted positron with an electron (Table 2). Fluorine F 18 atom decays to stable18O-oxygen.

* Produced by positron annihilation

[3] Kocher, D.C. Radioactive Decay Data Tables DOE/TIC-11026, 69, 1981.

The specific gamma ray constant for fluoride F 18 is 5.7 R/hr/mCi (1.35 x 10-6Gy/hr/kBq) at 1 cm. The half-value layer (HVL) for the 511 keV photons is 4.1 mm lead (Pb). A range of values for the attenuation of radiation results from the interposition of various thickness of Pb. The range of attenuation coefficients for this radionuclide is shown in Table 3. For example, the interposition of an 8.3 mm thickness of Pb with a coefficient of attenuation of 0.25 will decrease the external radiation by 75%.

Table 4 lists the fraction of radioactivity remaining at selected time intervals from the calibration time. This information may be used to correct for physical decay of the radionuclide.

* Calibration time

---

### AHA / ACCF / HRS recommendations for the standardization and interpretation of the electrocardiogram: part IV: the ST segment, T and U waves, and the QT interval: a scientific statement from the American Heart Association electrocardiography and arrhythmias committee, council on clinical cardiology; the American college of cardiology foundation; and the Heart Rhythm Society: endorsed by the international society for computerized electrocardiology [^2c6851b8]. Circulation (2009). Medium credibility.

QT correction for rate ‚Äî preferred methods and reporting thresholds: It is recommended that linear regression functions rather than the Bazett's formula be used for QT-rate correction and that the method used for rate correction be identified in ECG analysis reports, and rate correction of the QT interval should not be attempted when RR interval variability is large or when identification of the end of the T wave is unreliable; Bazett's formula leaves a strong positive residual correlation (r = 0.32) and Fridericia's formula leaves a negative correlation (r = ‚àí0.26 to ‚àí0.32) with heart rate, and the adjusted QT values may be substantially in error, particularly when the heart rate is high; FDA guidelines for industry recommend that 3 severity levels for rate-corrected QT be reported when considering possible QT-prolonging effects of drugs: longer than 350 ms, longer than 480 ms, and longer than 500 ms.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^72ce487a]. Annals of the American Thoracic Society (2023). High credibility.

Attenuation histogram indices definitions (Figure 2) ‚Äî The 15th percentile point in Hounsfield units (HU) is defined as the cutoff value below which 15% of voxels with the lowest density are distributed, and the voxel index at a threshold of ‚àí950 HU is defined as the percentage of voxels with values less than ‚àí950 HU.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^22bd51a1]. Nature Communications (2021). High credibility.

Motif III: logic

A general class of functions used to describe natural,‚Äì and synthetic, biological networks are logic gates, which have two inputs regulating a single output (Fig. 5). Gates exhibit either high ("on") or low ("off") output depending on whether inputs are on or off. For example, the AND gate specifies high output only if both inputs are on. In this section we provide a specific DDE-based framework that covers 14 out of 16 possible 2-input logic operations, and show that these operations form a continuous 2D parameter space.

Fig. 5
A simple approximation for digital logic using a sum of Hill terms recapitulates all monotonic logic functions in a single parameter space.

a A prototypical regulatory network involving logic where X and Y both regulate Z, which must integrate the two signals using some logic before it can in turn activate a downstream reporter R. b Parameter space showing regions where regulation approximately follows 14 of the 16 possible 2-input logic functions depending on the strength of two single-variable Hill regulation terms (Œ∑ Z 1: regulation of Z by X, Œ∑ Z 2: regulation of Z by Y). Network logic can be smoothly altered by varying the parameters (Œ∑ Z 1, Œ∑ Z 2), with a change of sign in (n 1, n 2) required to switch quadrants. The bottom-left quadrant shows that very weak regulation in both terms leads to an always-off (FALSE) function, weak regulation in one arm only leads to single-input (X, Y) functions, strong regulation in both arms leads to an OR function, and regulation too weak in either arm alone to activate an output but strong enough in sum leads to an AND function. The other three quadrants are related by applying NOT to one or both inputs, with function names related by de Morgan's lawNOT(X OR Y) = NOT X AND NOT Y. In particular, X IMPLY Y = NOT(X) OR Y, X NIMPLY Y = X AND NOT(Y), X NOR Y = NOT X AND NOT Y, and X NAND Y = NOT X OR NOT Y. Truth tables for all 16 logic gates are provided in Supplementary Table 1 for reference. The two non-monotonic logic functions, X XOR Y and X XNOR Y, are those 2 of 16 not reproduced directly using this summing approximation. They can be produced by layering, e.g. NAND gates. c Representative time traces for AND (Œ∑ Z 1 = Œ∑ Z 2 = 0.9) and OR (Œ∑ Z 1 = Œ∑ Z 2 = 1.8) gates with n 1 = n 2 = ‚àí2, n 3 = ‚àí20, Œ∑ R = Œ∑ Z 1 + Œ∑ Z 2. The functionwhen n > 0, when n < 0.

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^d027f21a]. Nature Communications (2025). High credibility.

Understanding the effect of heterogeneity is fundamental to numerous fields. In community ecology, classical theory postulates that habitat heterogeneity determines niche dimensionality and drives biodiversity. However, disparate heterogeneity-diversity relationships have been empirically observed, generating increasingly complex theoretical developments. Here we show that spurious heterogeneity-diversity relationships and subsequent theories arise as artifacts of heterogeneity measures that are mean-biased for bounded continuous variables. To solve this, we derive an alternative mean-independent measure of heterogeneity for beta and gamma distributed variables that disentangles statistical dispersion from mean. Using the mean-independent measure of heterogeneity, true monotonic positive heterogeneity-diversity relationships, consistent with classical theory, are revealed in data previously presented as evidence for both hump-shaped heterogeneity-diversity relationships and theories of an area-heterogeneity trade-off for biodiversity. This work sheds light on the source of conflicting results that have hindered understanding of heterogeneity relationships in broader ecology and numerous other fields. The mean-independent measure of heterogeneity is provided as a solution, essential for understanding true mean-independent heterogeneity relationships in wider research.

---

### 2019 methodology for creating expert consensus decision pathways: a report of the American college of cardiology [^7f0b4e46]. Journal of the American College of Cardiology (2019). High credibility.

Expert Consensus Decision Pathway timeline and scope ‚Äî The timeline for development is more compressed than a Clinical Practice Guideline, usually in the range of 8 to 12 months, during which time the Work Group tracks emerging evidence and incorporates it into the pathway. Determination of the scope is a critical early step, and the Work Group should keep the scope wide enough to provide guidance on the most important clinical questions yet not so broad as to be unmanageable within the narrow timeframe typically allotted for content development.

---

### Using the mean-to-sigma ratio as a measure of the improperness of binormal ROC curves [^70959dec]. Academic Radiology (2011). Low credibility.

Rationale and Objectives

A basic assumption for a meaningful diagnostic decision variable is that there is a monotone relationship between the decision variable and the likelihood of disease. This relationship, however, generally does not hold for the binormal model. As a result, receiver operating characteristic (ROC)-curve estimation based on the binormal model produces improper ROC curves that are not concave over the entire domain and cross the chance line. Although in practice the "improperness" is typically not noticeable, there are situations where the improperness is evident. Presently, standard statistical software does not provide diagnostics for assessing the magnitude of the improperness.

Materials and Methods

We show how the mean-to-sigma ratio can be a useful, easy-to-understand and easy-to-use measure for assessing the magnitude of the improperness of a binormal ROC curve by showing how it is related to the chance-line crossing. We suggest an improperness criterion based on the mean-to-sigma ratio.

Results

Using a real-data example, we illustrate how the mean-to-sigma ratio can be used to assess the improperness of binormal ROC curves, compare the binormal method with an alternative proper method, and describe uncertainty in a fitted ROC curve with respect to improperness.

Conclusions

By providing a quantitative and easily computable improperness measure, the mean-to-sigma ratio provides an easy way to identify improper binormal ROC curves and facilitates comparison of analysis strategies according to improperness categories in simulation and real-data studies.

---

### The snm procedure guideline for general imaging 6.0 [^85398c18]. SNMMI (2010). Medium credibility.

Nuclear medicine computer system components ‚Äî The camera head or associated image processing system performs functions including image size, position and zoom; energy correction; spatial distortion correction; other corrections (scatter correction, dead time correction, depth of interaction correction, sensitivity correction); and digital position computation; the interface handles the data in two basic modes: 1) Frame mode: complete images or matrices are available to the attached computer; and 2) List mode: data are passed on to the attached computer as a list of event x, y coordinates, to which time information and energy information may be also attached; for cardiac studies in particular, time lapse averaging is required, such that each image acquired at some specific time within the cardiac cycle is added to other acquired at similar times.

---

### Sex-specific prediction equations for vmax (FRC) in infancy: a multicenter collaborative study [^2cc21a23]. American Journal of Respiratory and Critical Care Medicine (2002). Low credibility.

Measurements of maximal flow at functional residual capacity (Vmax(FRC)) from partial forced expiratory maneuvers remain the most popular method for assessing small airway function in infants and young children. However, the lack of appropriate reference data that are both applicable outside the centers that developed them and reflect the normal variability between healthy subjects has limited interpretation of Vmax(FRC) results in both clinical practice and research. To address this problem, we collated Vmax(FRC) data from 459 healthy infants (226 boys) tested on 654 occasions during the first 20 months of life from three collaborating centers. Multiple linear regression analysis indicated that sex, age, and length were important predictors of Vmax (FRC), which was, on average, 20% higher in girls than in boys during the first 9 months of life. (Vmax(FRC))0.5 (ml x second(-1)) = 4.22 + 0.00210 x length2 (cm) for boys (RSD = 3.01; R2 = 0.48), and -1.23 + 0.242 x length for girls (RSD = 2.72; R2 = 0.49). Alternative models incorporating both age and length z scores are also described. Failure to use sex-specific prediction equations for Vmax(FRC) may preclude detection of clinically significant changes in girls and lead to false reports of diminished airway function in boys. Appropriate use of z scores, which indicate a "normal" range (z scores of 0 ¬± 2) for Vmax(FRC), during infancy should also improve interpretation of both clinical and research studies.

---

### Accurate spike estimation from noisy calcium signals for ultrafast three-dimensional imaging of large neuronal populations in vivo [^11ea5be0]. Nature Communications (2016). Medium credibility.

Time discretization and probability details

The model is discretized at the signal's temporal resolution t (below, t will be used for discrete time indices rather than continuous time). We will note the input as n t (number of spikes between time t ‚àí 1 and t), the hidden state x t (= c t in the simplest model where baseline is constant and known, = (c t, B t) when baseline fluctuates, = (c t, p t, B t) when a rise time was introduced) and the measure y t: = F t.

We detail here this discretization and the full derivations of probability distributions p (x t |x t-1) and p (y t |x t) in the case of the simpler physiological model. The model equations become:

Random variable n t follows an exponential law with parameter ŒªŒît:

The other probability relations defined implicitly in the system (w t and …õ t are independent Gaussian variables with mean zero and variance one) are

Note that the first line of the equation is a simplification for the more rigorous but complicate formula

The last probability needed to fully describe the model is p (x 1) = p (c 1)p(B 1). It is the a priori probability of the hidden state, in absence of any measurement. In practice, we used a uniform distribution for both c 1 and B 1.

Regarding c 1, indeed we found that when the true spiking rate was not known, a uniform probability was better than a distribution determined mathematically based on the value of a priori spiking rate, because if that value was not correct, errors were increased. If the true spiking rate is known however, the following a priori can be used: one can observe that c 1 is a weighted sum of Poisson random variables:

Its probability distribution can thus not only be computed exactly with iterative convolutions but is also well-approximated with a truncated normal distribution:

---

### Standardization of spirometry 2019 update. An official American Thoracic Society and European Respiratory Society technical statement [^7290ab6b]. American Journal of Respiratory and Critical Care Medicine (2019). High credibility.

Standardization of spirometry ‚Äî display and digitization requirements specify acquisition, graph scaling, and start-of-test display. For digitization of the flow or volume signal, the sampling rate must be ‚â• 100 Hz with a minimum resolution of 12 bits. For the flow‚Äìvolume graph, expiratory flow must be plotted upward and expiratory volume toward the right, and a 2:1 aspect ratio must be maintained so that 2 L/s of flow and 1 L of volume are the same distance on their respective axes. For the start of test display, the volume‚Äìtime graph must begin at the point of maximum inspiration or 1 second before Time 0, whichever occurs first, and should continue to the end of the plateau or the beginning of inspiration. Displays of flow versus volume provide more detail than volume‚Äìtime graphs for the first 1 second of the FVC maneuver, whereas volume‚Äìtime graphs provide more detail for the latter part of the maneuver.

---

### Interpreting change in quantitative imaging biomarkers [^20a80f73]. Academic Radiology (2018). Low credibility.

Rationale and Objectives

Quantitative imaging biomarkers (QIBs) are becoming increasingly adopted into clinical practice to monitor changes in patients' conditions. The repeatability coefficient (RC) is the clinical cut-point used to discern between changes in a biomarker's measurements due to measurement error and changes that exceed measurement error, thus indicating real change in the patient. Imaging biomarkers have characteristics that make them difficult for estimating the repeatability coefficient, including nonconstant error, non-Gaussian distributions, and measurement error that must be estimated from small studies.

Methods

We conducted a Monte Carlo simulation study to investigate how well three statistical methods for estimating the repeatability coefficient perform under five settings common for QIBs.

Results

When the measurement error is constant and replicates are normally distributed, all of the statistical methods perform well. When the measurement error is proportional to the true value, approaches that use the log transformation or coefficient of variation perform similarly. For other common settings, none of the methods for estimating the repeatability coefficient perform adequately.

Conclusion

Many of the common approaches to estimating the repeatability coefficient perform well for only limited scenarios. The optimal approach depends strongly on the pattern of the within-subject variability; thus, a precision profile is critical in evaluating the technical performance of QIBs. Asymmetric bounds for detecting regression vs progression can be implemented and should be used when clinically appropriate.

---

### Geriatric emergency department guidelines [^cd3e2fd4]. Annals of Emergency Medicine (2014). Medium credibility.

Short Blessed Test scoring ‚Äî Items with 'Errors (0‚Äì5)' are weighted x 4 for item 1, x 3 for items 2 and 3, and x 2 for items 4, 5, and 6, with a Sum Total calculated within (Range 0‚Äì28); interpretation thresholds are 0‚Äì4 Normal Cognition, 5‚Äì9 Questionable Impairment, and ‚â• 10 Impairment consistent with dementia.

---