# Medical Question & Answer

**Sample ID**: 3e30a823-e0cd-e787-01d7-1f3ab0d5b3a2
**Dataset Index**: 82669

---

## Question

8th Swedish 1968 Problem 1 Find the maximum and minimum values of x² + 2y² + 3z² for real x, y, z satisfying x² + y² + z² = 1.

---

## Answer

> Let's see… What do we have here? The user is asking for the maximum and minimum values of the function f(x, y, z) = x² + 2y² + 3z² subject to the constraint x² + y² + z² = 1. Let's break this down step-by-step. First, I need to think about the objective function and constraint and whether this is a constrained optimization problem. Then, I should verify the method I'll use, likely Lagrange multipliers, and set up the Lagrangian. Next, I will solve the stationarity conditions to find critical points. After that, I need to evaluate the objective at those points and determine which correspond to the maximum and minimum. Finally, I should double-check the reasoning and consider whether any alternative approaches would confirm the result.

> Let me first confirm the problem setup. We want to optimize f(x, y, z) = x² + 2y² + 3z² on the unit sphere defined by g(x, y, z) = x² + y² + z² − 1 = 0. This is a classic constrained optimization problem, and the method of Lagrange multipliers is appropriate here [^b0d02f80].

> I will now examine the Lagrangian. Define L(x, y, z, λ) = x² + 2y² + 3z² − λ(x² + y² + z² − 1). Wait, let me verify the sign convention; I initially thought to add λ times the constraint, but the standard form subtracts λ(g), so I should keep the minus sign to align with the usual formulation and avoid sign errors in the multiplier equations [^b0d02f80].

> Next, I should review the stationarity conditions. Taking partial derivatives and setting them to zero gives: ∂L/∂x = 2x − 2λx = 0, ∂L/∂y = 4y − 2λy = 0, ∂L/∂z = 6z − 2λz = 0, and ∂L/∂λ = −(x² + y² + z² − 1) = 0. Hold on, I should verify each derivative; yes, these are correct, and the last equation simply restates the constraint [^b0d02f80].

> Let me consider the implications of the first three equations. They can be rewritten as x(1 − λ) = 0, y(2 − λ) = 0, and z(3 − λ) = 0. This means that for each variable, either the variable is zero or the multiplier λ equals a specific coefficient from the objective function. I need to ensure I don't jump to conclusions; these cases will partition the critical points based on which variables are active.

> I will now enumerate the cases systematically. Case 1: λ = 1. Then y and z must be zero, so the constraint gives x² = 1, hence x = ± 1, producing the points (± 1, 0, 0). Case 2: λ = 2. Then x and z must be zero, so y² = 1, giving (0, ± 1, 0). Case 3: λ = 3. Then x and y must be zero, so z² = 1, giving (0, 0, ± 1). But wait, what if none of these λ values occur? Then at least two variables would have to be zero, which reduces to one of the above cases anyway, so these are the only critical points on the unit sphere [^b0d02f80].

> Next, I should review the objective values at these points. At (± 1, 0, 0), f = 1² + 2·0² + 3·0² = 1. At (0, ± 1, 0), f = 0² + 2·1² + 3·0² = 2. At (0, 0, ± 1), f = 0² + 2·0² + 3·1² = 3. Hold on, I should verify the arithmetic; yes, these are correct [^b0d02f80].

> Now, I need to determine which is the maximum and which is the minimum. The smallest value is 1 and the largest is 3, so the minimum of f on the unit sphere is 1 and the maximum is 3. I should double-check that no other critical points could yield a larger or smaller value; given the exhaustive case analysis and the fact that the objective is a quadratic form with positive coefficients, these extrema are indeed achieved at the axes aligned with the smallest and largest coefficients, which is consistent with the structure of the problem [^b0d02f80].

> Let me reconsider whether an alternative approach would confirm this. I could parameterize the sphere and substitute, but that would be messier and less general than Lagrange multipliers. Alternatively, I could think about this as optimizing a quadratic form vᵀAv subject to ||v|| = 1, where the eigenvalues of A would give the extrema; here, the matrix is diagonal with entries 1, 2, 3, so the eigenvalues are 1, 2, 3, confirming that the minimum is 1 and the maximum is 3. That cross-check aligns with the Lagrange result, which increases my confidence [^b0d02f80].

> Final answer: The minimum value is 1 and the maximum value is 3 [^b0d02f80].

---

The maximum value of (x² + 2y² + 3z²) subject to (x² + y² + z² = 1) is **3**, achieved when ((x, y, z) = (0, 0, ± 1)). The minimum value is **1**, achieved when ((x, y, z) = (± 1, 0, 0)). These results follow from the method of Lagrange multipliers, which shows that the extrema occur at points where the gradient of the objective function is parallel to the gradient of the constraint, i.e. where (∇f = λ ∇g) [^b0d02f80].

---

## Setting up the problem

We want to find the extrema of the function:

f(x, y, z) = x² + 2y² + 3z²

subject to the constraint:

g(x, y, z) = x² + y² + z² − 1 = 0

---

## Applying the method of Lagrange multipliers

The method of Lagrange multipliers states that at the extrema, the gradient of the objective function is proportional to the gradient of the constraint. Mathematically, this is expressed as:

∇f = λ ∇g

where (λ) is the Lagrange multiplier.

---

## Computing the gradients

First, compute the gradient of (f):

∇f = (∂f/∂x, ∂f/∂y, ∂f/∂z) = (2x, 4y, 6z)

Next, compute the gradient of (g):

∇g = (∂g/∂x, ∂g/∂y, ∂g/∂z) = (2x, 2y, 2z)

---

## Setting up the Lagrange equations

According to the method, we set (\\nabla f = \\lambda \\nabla g), which gives the following system of equations:

2x = λ × 2x
4y = λ × 2y
6z = λ × 2z

Simplifying each equation, we get:

2x(1 − λ) = 0
2y(2 − λ) = 0
2z(3 − λ) = 0

---

## Analyzing the solutions

From the above equations, we can deduce the following cases:

- **Case 1**: (x ≠ 0), (y = 0), (z = 0). This implies (λ = 1).
- **Case 2**: (y ≠ 0), (x = 0), (z = 0). This implies (λ = 2).
- **Case 3**: (z ≠ 0), (x = 0), (y = 0). This implies (λ = 3).

---

## Substituting back into the constraint

We now substitute each case back into the constraint (x² + y² + z² = 1) to find the specific points:

- **Case 1**: (x² = 1 ⇒ x = ± 1). The points are ((± 1, 0, 0)).
- **Case 2**: (y² = 1 ⇒ y = ± 1). The points are ((0, ± 1, 0)).
- **Case 3**: (z² = 1 ⇒ z = ± 1). The points are ((0, 0, ± 1)).

---

## Evaluating the function at critical points

Now, we evaluate (f(x, y, z)) at each of these critical points:

| **Case** | **Point** | **Value of f(x, y, z)** |
|-|-|-|
| 1 | (± 1, 0, 0) | (1² + 2(0)² + 3(0)² = 1) |
| 2 | (0, ± 1, 0) | (0² + 2(1)² + 3(0)² = 2) |
| 3 | (0, 0, ± 1) | (0² + 2(0)² + 3(1)² = 3) |

---

## Conclusion

From the above evaluations, we can conclude that:

- The **maximum value** of (x² + 2y² + 3z²) is **3**, achieved at the points ((0, 0, ± 1)).
- The **minimum value** of (x² + 2y² + 3z²) is **1**, achieved at the points ((± 1, 0, 0)).

---

The maximum value of (x² + 2y² + 3z²) subject to (x² + y² + z² = 1) is **3**, and the minimum value is **1**.

---

## References

### Bayesian tomography of high-dimensional on-chip biphoton frequency combs with randomized measurements [^b26a4537]. Nature Communications (2022). High credibility.

In addition to the parameters forming ρ, the scale factor K must also be suitably parameterized. Following ref. we find it convenient to write K = K 0 (1 + σ z), where K 0 and σ are hyperparameters defined separate of the inference process, and z is taken to follow a standard normal distribution, leading to a normal prior on K of mean K 0 and standard deviation K 0 σ. We take σ = 0.1 and K 0 equal to the sum of the counts in all d 2 bins for the first JSI measurement (r = 1), where the absence of modulation ensures that all initial photon flux remains in measured bins, i.e. This provides an effectively uniform prior, since a fractional deviation of 0.1 is much larger than the maximum amount of fractional uncertaintyexpected from statistical noise at our total count numbers; the use of a normal distribution simplifies the sampling process.

The total parameter set can therefore be expressed as the vector, with the prior distribution

We note that this parameterization entails a total of 4 d 4 + 1 independent real numbers (2 d 4 complex parameters for ρ, one real parameter for K) — noticeably higher than the minimum of d 4 − 1 required to uniquely describe a density matrix. Nevertheless, this ρ (y) parameterization is to our knowledge the only existing constructive method to produce Bures-distributed states, and is straightforward to implement given its reliance on independent normal parameters only.

Following Bayes' rule, the posterior distribution becomeswhereis a constant such that ∫ d x π (x) = 1. We have adopted this notation for Bayes' theorem — rather than the more traditional — to emphasize the functional dependencies on x, which are all that must be accounted for in the sampling algorithm below. From π (x), the Bayesian mean estimator f B of any quantity (scalar, vector, or matrix) expressible as a function of x can be estimated aswhere, in lieu of direct integration, S samples { x (1), …, x (S) } are obtained from the distribution π (x) through Markov chain Monte Carlo (MCMC) techniques, as described below.

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^ade8746b]. Nature Communications (2025). High credibility.

Derivation of δ 2 from the general beta distribution

Given an observation (v) of a random variable (V) with any real number upper (U) and lower (L) bound, x can be retrieved for indexing the beta pdf and cdf (Equation (6); Equation (18)) withEssentially, v is rescaled to a value between 0 and 1 based on its possible minimum and maximum, such that it can be used to index the beta pdf in Equation (6). Likewise, the expected value or mean of the variable, E(V), can also be scaled to a value between 0 and 1 in the same way:where E(X) is equivalent to μ for the beta distribution (defined in Equation (7)). Similarly, 1 − E(X), can be retrieved for V withThe variance of the variable, Var(V), scales based on the squared range between the L and U :where Var(X) is equivalent to σ 2 for the beta distribution (defined in Equation (12)). As a result, the mean-independent measure of dispersion, δ 2, can be retrieved for a double-bounded variable withThus, Equation (33) is a general version of Equation (13) that can be calculated for variables with any real number lower and upper bound. In the main text Var(V) is denoted σ 2 and E(V) is denoted μ for simplicity.

The general beta pdf can be expressed using the standard beta pdf (f (x); Equation (6)) as, where the denominator normalises the pdf to adjust for the modified scale when modelling V, x for indexing the pdf is found for V with Equation (29), and the parameters p and q can be found for V with Equations (30), (33), (10), and (11).

---

### Machine-guided design of cell-type-targeting cis-regulatory elements [^29164698]. Nature (2024). Excellent credibility.

As an example of coordinate calculation, take the point (5, 3, 1). This point would have a radial distance of 5 − 1 = 4 and an angle of deviation from the axis of the first dimension of (3 − 1)/(5−1) × (60°) = 30° (in the direction of the axis of the second dimension). In terms of the MinGap:MaxGap ratio, the angle of deviation from the axis of the first dimension (the dimension of the maximum value) towards the axis of the second dimension would be (1 − (5 − 3)/(5 − 1))(60°) = 30°. Observe that all the points of the form (x + 4, x + 2, x), for any real value of x, will have the same coordinates as the point (5, 3, 1).

A propeller count plot (Fig. 2e (bottom row)) shows the percentage of points that fall in each given area of a propeller dot plot. The teal, yellow and red regions capture sequences in which the median value is closer to the minimum value than to the maximum value. Teal, yellow and red areas represent sequences in which the MinGap:MaxGap ratio is greater than 0.5.

The two synthetic groups in Fig. 2e were randomly subsampled to have exactly 12,000 sequences each and avoid over-plotting compared to the plots of the two natural groups. Supplementary Fig. 9 shows the complete propeller plots broken down by design method.

Oligos with a replicate log 2 [FC] standard error greater than 1 in any cell type were omitted from the plots.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^cbffc27e]. CDC (2011). Medium credibility.

Appendix G. Mathematical formula for sample size — sample size required to detect the difference of GM between two subpopulation groups defines a z test with null hypothesis GM1 = GM2 or GMmax/GMmin = 1 and alternative GM1 ≠ GM2 or GMmax / GMmin > 1, where GMmax = max(GM1, GM2) and GMmin = min(GM1, GM2). It considers power W (say 80% chance) to detect that one group's GM is at least k fold higher and gives the p-value as p = Prob(Z ≥ z) = 1 − φ−1(z) with Z ~ Normal (0,1). Given α (e.g., 0.05) and target ratio k, power is W = Prob (Z > φ−1(1 − α) − log10(k)/(S√(1/n1 + 1/n2))), where S is the expected standard deviation of log10(VL) in the population of interest; when n1 = n2 = n, W = Prob (Z > φ−1(1 − α) − log10(k)/(S√(2/n))). The required sample size is n = 2[(φ−1(1 − α) − φ−1(1 − W))S/log10(k)]^2, and tables are provided for W = 0.8 and W = 0.9 with α = 0.05.

---

### Unbiased estimation of odds ratios: combining genomewide association scans with replication studies [^5dd42c88]. Genetic Epidemiology (2009). Low credibility.

Without loss of generality we assume that the event Q: X 1 ≥ X 2 ≥… ≥ X k has occurred, so that X i = X (i). Let. The pairand Z i = (σ 2, i /σ 1, i) X i)+(σ 1, i /σ 2, i)ϒ i are then sufficient and complete statistics for μ 1, …, μ k. The joint distribution of ϒ i and X 1, …, X k given Q, f (ϒ i, X | Q) is then transformed into f (X, Z i | Q) and. The joint densityis obtained from the integral

which enables the densityto be expressed as the ratio. This is greatly simplified due to numerous cancellations, in particular the selection probabilities which are analogous to those that feature in the denominator of, and cause problems for, formula (1). Using the Rao-Blackwell theorem formula (4), which is, is the uniformly minimum variance unbiased estimator for μ (i)′, conditional on Q (we call it the UMVCUE). This means that, given the ranking in stage 1, the estimator is unbiased for the corresponding effects and has minimum variance among all such unbiased estimators.

We now propose some modifications to this formula in order to apply the same estimation procedure to a genome scan followed by replication. Instead of the magnitude of the point estimates determining the rank order, it is more common in the genomewide setting to rank SNPs according to the statistical significance of their effects, and to restrict attention to those passing an initial P -value threshold p crit. For a one-sided Wald-type test, we can make this extension by conditioning instead on the event

This leads to slight changes in the proof since conditioning on Q * as opposed to Q changes the limits of integration for X i and Y i, with the result that W s, t in (4) becomes

For (8) to work generally we define X (k +1) /σ 1,(k +1) = Φ −1 (1 – p crit). This expression was noted in, though they did not allow for a P -value threshold. If SNPs that confer either an increased or decreased disease risk are of equal interest, as is usually the case, then the rank order of significance should be based on two-sided P -values. This now requires conditioning on the event

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^428bde48]. Nature Communications (2025). High credibility.

Derivation of the generalised δ formulation

Lastly, we introduced a generalised formulation of δ that is adaptive to the number of boundaries present for the variable being measured (Equation (1)). When the upper and lower bound are real numbers, δ (Equation (1)) becomes δ 2 (Equation (33)), and when the variable has only a real number lower bound, δ (Equation (1)) becomes δ L (Equation (37)), each with their associated proofs in previous sections. Here we provide proofs of δ for upper bounded variables and unbounded variables.

For upper bounded variables there exists no commonly used model distribution. The solution, however, was simply to use a conceptually flipped gamma distribution, where the lower bound of zero was instead used as an upper bound. This was done using same approach of rescaling the variable as was done for lower bounded variables. The target is, thus, to rescale a given observation (v) of a variable (V) with any real number upper bound (U) to retrieve x for indexing the gamma pdf and cdf (Equation (20); Equation (28)). This can be done withEssentially, v is rescaled such that its maximum is equal to zero. Similarly, the expected value or mean of the variable (E(V)) can be scaled such that its maximum is zero:where E(X) is equivalent to μ for the gamma distribution (defined in Equation (21)). Much like the use of the general gamma distribution for lower bounded variables, the variance of the upper bounded variable (Var(V)) remains unchanged when converted to the variance of a gamma distributed variable. As a result, the mean-independent measure of dispersion for upper bounded (U) variables, δ U, can be retrieved withThus, upper bounded variables can also be modelled using the generalised δ. In the main text Var(V) is denoted σ 2 and E(V) is denoted μ.

For unbounded variables the Gaussian/normal distribution is the most obvious model. The Gaussian distribution is defined by a parameter for central tenancy (μ) and dispersion/scale (σ 2). Where both parameters are estimated independently for an unbounded variable with the mean (E(V)) and variance Var(V). Thus, the distribution's dispersion is the variance itself (δ 0 = Var(V) = σ 2).

---

### Minimum electric-field gradient coil design: theoretical limits and practical guidelines [^13a4a404]. Magnetic Resonance in Medicine (2021). Medium credibility.

FIGURE 4
A, A region contours of constant Bz for Y gradient coil, with (red) and without (black) E max optimization, showing that gradient distortion is virtually identical despite the peak E‐field being reduced by a factor of 2.8 for the Y coil. B, Difference in the B y components of the gradient field with versus without E‐field optimization. Values are normalized to the gradient strength yielding units of length (mm) and show a highly uniform concomitant y‐directed field added to the optimized (asymmetric) solution. C, Wire pattern representing the difference between optimized (asymmetric) and nonoptimized (symmetric) solutions with same current scaling as Figure 3. Similar results are obtained for the X gradient coil

The key difference between the E‐field minimized and nonminimized solutions is the addition of a uniform concomitant B 0 field component in the x or y direction. Figure 4B shows a difference plot of the concomitant B y field, normalized to gradient strength (units of millimeters), which is responsible for the reduction in E‐field. Figure 4C shows the winding that would need to be added to the Y symmetric coil (Figure 3B) to obtain the winding pattern of the Y asymmetric coil (Figure 3D), and shows the fundamental difference between the two solutions. The winding in Figure 4C produces a uniform field transverse to the main B 0 field, resulting in a concomitant component with additional magnetic stored energy. Conceptually, this can be thought of as a second coil, which if activated, converts the symmetric coil into an asymmetric coil with reduced E‐field but otherwise identical distortion. The added concomitant field in Figure 4B is highly uniform with a 0.85% variation over the 26‐cm imaging region, resulting in minimal distortion of the desired gradient field; this concomitant field can be compensated with a phase/frequency offset correction by the scanner. Figure 4B shows that the average concomitant field of 88.5 mm within the imaging region can be directly compared with the z 0x defined by Meier et al, 29 in which a value of 127 mm was reported for an asymmetric head gradient. The z 0x for both the ESP and HG2 gradients are approximately 120 mm 30; these are both fully asymmetric designs (single eye per half coil with all return currents flowing above the head), similar to the original concept defined by Roemer. 12 The X coil of Figure 3 requires a smaller concomitant field (z 0y = 57.8 mm) than the Y coil, reflecting a decreased need to pull E‐field off the torso region due to the smaller extent of the body in the anterior–posterior direction.

---

### Fundamental limits to learning closed-form mathematical models from data [^5954e9c9]. Nature Communications (2023). High credibility.

Fig. 1
Probabilistic model selection makes quasi-optimal predictions about unobserved data.

We select two models m *, whose expressions are shown at the top of each column. a, b From each model, we generate synthetic datasets D with N points (shown, N = 100) and different levels of noise s ϵ (shown, s ϵ = 1). Here and throughout the article, the values of the independent variables x 1 and x 2 are generated uniformly at random in [− 2, 2]. Vertical lines show the observation error ϵ i for each point in D. For a model not drawn from the prior and data generated differently, see Supplementary Fig. S1. c, d For each dataset D (with dataset sizes N ∈ {25, 50, 100, 200, 400}), we sample models from p (m ∣ D) using the Bayesian machine scientist, select the MDL model (maximum p (m ∣ D)) among those sampled, and use this model to make predictions on a test dataset, generated exactly as D. We show the prediction root mean squared error (RMSE) of the MDL model onas a function of N and s ϵ. For comparison, we also show the predictions from an artificial neural network (ANN, dotted lines; Methods). Since s ϵ is the irreducible error, predictions on the diagonal RMSE = s ϵ are optimal. e, f We plot the prediction RMSE scaled by the irreducible error s ϵ; optimal predictions satisfy RMSE/ s ϵ = 1 (dashed line).

---

### Worldwide divergence of values [^f95bbaf6]. Nature Communications (2024). High credibility.

Item normalization

Supplementary Table 2 shows that items were asked on different scales. Some items involved binary responses (e.g. whether people mention not wanting to be neighbors with someone from a specific group). Others were asked with Likert-type scales, such as the 1–10 scale that people used to rate whether behaviors were morally justifiable. We normalized the item scales using min-max normalization, which is a common approach in data science and machine learning. Given a vector V = [v1, v2,… vn], we can determine min_V as the minimum value in the vector and max_V as the maximum value in the vector. We can create our normalized vector, V' using:

In other words, each element in the new vector is the result of subtracting the minimum value of the original vector from that element, then dividing by the range of the original vector (i.e. the difference between its maximum and minimum values). This procedure results in variables with ranges of 0–1, no matter their original scales. As an alternative to min-max normalization, we also considered a median split approach. We discuss the limitations of this approach in the Supplementary Methods, and show how our main findings replicate with this approach.

---

### A practical solution to pseudoreplication bias in single-cell studies [^2a094b12]. Nature Communications (2021). High credibility.

MAST models a log(x + 1) transformed gene expression matrix as a two-part generalized regression model. As in Finak et al. the addition of random effects for differences among persons is:where Y ig is the expression level for gene i and cell k, Z ki is an indicator for whether gene i is expressed in cell k, X k contains the predictor variables for each cell k, and W k is the design matrix for the random effects of each cell k belonging to each individual j (i.e. the random complement to the fixed X k). β i represents the vector of fixed-effects regression coefficients and γ j represents the vector of random effects (i.e. the random complement to the fixed β i). γ j is distributed normally with a mean of zero and variance. To obtain a single result for each gene, the likelihood ratio or Wald test results from each of the two components are summed and the corresponding degrees of freedom for each component are added. These tests have asymptotic χ 2 null distributions; they can be summed and remain asymptotically χ 2 because Z i and Y i are defined conditionally independent for each gene.

Evaluation of rank-order preservation

To approximate how well rank-order was preserved across each of the ten methods evaluated, we simulated TPM values for an individual gene 2000 times with random fold-changes between 0 and 4. The number of individuals per treatment group was fixed at 30 and the number of cells per individual was fixed at 100. Fold-change was drawn from a uniform distribution with a minimum equal to 0 and a maximum equal to 4. Genes were retained along with their fold-change information to evaluate rank-order correlation and to complete the sensitivity analysis. The genes simulated under the null for the type 1 error rate calculations were used to estimate specificity (1-type error) for each method. With each of the different methods, p -values were computed and were ranked alongside the absolute value of the simulated-log(fold-change) values. Spearman's rank-correlation coefficients were computed between each of the different methods.

---

### Article 5. An introduction to estimation – 2: from z to T [^fb82b806]. Emergency Medicine Journal (2001). Low credibility.

Provided the sample size is large enough (that is, n greater than 100), the z statistic can be used to determine the confidence interval estimation of the population mean even when the sigma is not known. In these cases the estimation of the standard error of the mean is used. The z statistic is also valid when determining the population's proportion based upon a large sample. However, when dealing with smaller samples, the z statistic is replaced by the t statistic. This makes it possible to estimate, in a population with an unknown standard deviation: The probability of getting a sample mean greater than or equal to a particular value The value of a sample mean with a particular probability of occurring The probability of getting a sample mean between two particular values The confidence interval for the estimation of the population mean can also be determined using the t statistic.

---

### Cartography of genomic interactions enables deep analysis of single-cell expression data [^f3c91bd8]. Nature Communications (2023). High credibility.

Efficient computation of the interaction matrix

Computation of the pairwise interaction matrix (eq. (3)) can be intensive because of the inversion of the covariance matrix. Interestingly, the formulation of the interaction strength between two genes (eq. (3)) is same as that of their partial correlation. An efficient way to compute the partial correlation matrix (and thus interaction matrix) is to 1) solve the two associated linear regression problems (shown below), 2) get the residuals from the regression problems, and 3) calculate the correlation between the residuals. Let us assume that X and Y are random variables taking real values (denoting expression levels of two genes), and let Z be the (n − 2)-dimensional vector-valued random variable (denoting expression levels of all other genes). Let us also assume that x i, y i and z i, i = 1, …, m denotes the independent and identically distributed m observations from some joint probability distribution of the random variables X, Y and Z. If we want to find the relationship between the random variables through regression, we have to find the regression coefficient vectorsandsuch thatwith 〈 w, v 〉 the scalar product between the vectors w and v. The residuals can then be computed asThe partial correlation between X and Y can then be expressed as:For independent 2-way (pairwise) interactions between X and Y (Z has no effect on the interaction of X and Y), eq. (16) can be simplified as

---

### Statistical considerations in the evaluation of continuous biomarkers [^c7960d51]. Journal of Nuclear Medicine (2021). Medium credibility.

Discovery of biomarkers has been steadily increasing over the past decade. Although a plethora of biomarkers has been reported in the biomedical literature, few have been sufficiently validated for broader clinical applications. One particular challenge that may have hindered the adoption of biomarkers into practice is the lack of reproducible biomarker cut points. In this article, we attempt to identify some common statistical issues related to biomarker cut point identification and provide guidance on proper evaluation, interpretation, and validation of such cut points. First, we illustrate how discretization of a continuous biomarker using sample percentiles results in significant information loss and should be avoided. Second, we review the popular "minimal- P -value" approach for cut point identification and show that this method results in highly unstable P values and unduly increases the chance of significant findings when the biomarker is not associated with outcome. Third, we critically review a common analysis strategy by which the selected biomarker cut point is used to categorize patients into different risk categories and then the difference in survival curves among these risk groups in the same dataset is claimed as the evidence supporting the biomarker's prognostic strength. We show that this method yields an exaggerated P value and overestimates the prognostic impact of the biomarker. We illustrate that the degree of the optimistic bias increases with the number of variables being considered in a risk model. Finally, we discuss methods to appropriately ascertain the additional prognostic contribution of the new biomarker in disease settings where standard prognostic factors already exist. Throughout the article, we use real examples in oncology to highlight relevant methodologic issues, and when appropriate, we use simulations to illustrate more abstract statistical concepts.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^dee6cacc]. CDC (2011). Medium credibility.

Community viral load sample size calculations — sample size to detect differences in geometric mean (GM) viral load depends on power and standard deviation (S). The table specifies "α = 0.05 and W = 0.8", with S columns 1 to 1.5; for k = 3 the minimum sample sizes by S are 54, 66, 78, 92, 106, and 122, and "1.2 is the standard deviation of national VL data", so at S = 1.2 the needed sample size is 78.

---

### Improved security bound for the round-robin-differential-phase-shift quantum key distribution [^68502abf]. Nature Communications (2018). Medium credibility.

Theorem. For the RRDPS protocol with L-pulse packet, where each packet contains N photons (L ⩾ N + 1), Eve's information can be bounded bywhere, non-negative real parameters x i satisfying. Moreover, if the error rate of raw key bits is E, these parameters x i (i ∈ {1, 2. N + 1}) must satisfy the constraintand,

Corollary. If the photon-number L ≤ N − 2, I AE < 1 always holds.

Based on this theorem, the upper bound of I AE is generalized to find the maximum value of a given function under a constraint defined by E. If we ignore this constraint, we obtainwithout monitoring signal disturbance. Alternatively, if we retain this constraint, a tighter estimation may be achieved. It is remarkable that searching such a maximum value can be effective and concise through a numerical method, since its function is convex. A different, less tight bound improvement was recently reported in ref.

Potential improvements made by our theory

For a QKD protocol, the mutual information between Alice and Bob is given by I AB = 1 − h 2 (E). Thus, there is a maximum value E max of error rate E, which satisfies, when E = E max. Obviously, ifholds, I AB will be no larger than, and no secret key bits can be generated. Thus, E max is the maximum value of tolerable error rate of a QKD protocol. We first compare E max of RRDPS between the original method and our new formulae. In Table 1, we list the results for the cases in which a single-photon source is equipped. One can see that our formulae can increase E max, especially when L is small. It is remarkable to note that for the case L = 3, with our formulae E max can be up to 5%, while the original RRDPS protocol cannot generate secure key bits at all. More importantly, the notable difference between columns two and three in Table 1 implies that our theory leads to increased E max compared to the original RRDPS even when signal disturbance monitoring is still turned off.

---

### Spin dephasing under nonlinear gradients: implications for imaging and field mapping [^eaf2803c]. Magnetic Resonance in Medicine (2012). Low credibility.

This work examines the prototypical MR echo that would be expected for a voxel of spins evolving in a strong nonlinear field, specifically focusing on the quadratic z(2) - ½(x(2) + y(2)) field. Dephasing under nonlinear gradients is increasingly relevant given the growing interest in nonlinear imaging, and here, we report several notable differences from the linear case. Most notably, in addition to signal loss, intravoxel dephasing under gradients creating a wide and asymmetric frequency distribution across the voxel can cause skewed and nonlinear phase evolution. After presenting the qualitative and analytical origins of this difference, we experimentally demonstrate that neglecting these dynamics can lead to significant errors in sequences that assume phase evolution is proportional to voxel frequency, such as those used for field mapping. Finally, simplifying approximations to the signal equations are presented, which not only provide more intuitive forms of the exact expression but also result in simple rules to predict key features of the nonlinear evolution.

---

### Excess of mutational jackpot events in expanding populations revealed by spatial luria-delbrück experiments [^6f3a635e]. Nature Communications (2016). Medium credibility.

Simulated clone size distributions, displayed in Fig. 4b, confirm the predicted scaling form. In real systems, the scaling form has a range of validity, say between some minimal and maximal frequency x min and x max that are set by details of the growth processes, which are beyond the scope of our model. For instance, the maximal frequency x max = O (1) accounts for the discrete nature of growth during the first few cell divisions. Similarly, x min = O (1/ N) may reflect mutations that are born behind the front such that our gene surfing theory does not apply.

The two power-law exponents α and β are constrained by the scaling behaviour of the mean frequency of mutants, as follows. For a given mutation rate μ per cell division, the expected frequency 〈 X tot (t)〉 of mutants at the surface of a growing D -dimensional sphere satisfies

where time is measured in units of generations at the front. Hence, as long as, we have 〈 X tot (t)〉 = μt. Every generation, the radius grows by λ, such that it takes the population t = R / λ generations to grow to size N ∝ R D. The length λ can be interpreted as a measure for the thickness of the growth layer at the edge of the colony in units of the linear dimension of the cells. The mean frequency at radius r then is 〈 X tot (r)〉 = μr / λ, and the population mean follows as. This implies an expected total number of mutants of

As we ignore cell death, there are on average μN mutations occurring during the growth of the population. Hence, the frequency of a single clone is, on average, which constrains the integral over Π(x) since. Inserting the scaling relation reported in equation (1), we obtain for large population N ≫1

---

### Efficient classical sampling from gaussian boson sampling distributions on unweighted graphs [^bb10c3c1]. Nature Communications (2025). High credibility.

Theorem 4

Given a bipartite graph G = (V 1, V 2, E) with ∣ V 1 ∣ = m, ∣ V 2 ∣ = n and m ≥ n. If the minimum degree of vertices in V 1 satisfies δ 1 (G) ≥ n − ξ, and the minimum degree of vertices in V 2 satisfies δ 2 (G) ≥ m − ξ for some constant ξ, then for, given error ϵ, we can achieve a sampling in timesuch that the total variation distance between the sampling distribution and the ideal stationary distribution is at most ϵ.

On dense non-bipartite graphs, we have:

Theorem 5

Given a non-bipartite graph G = (V, E) with ∣ V ∣ = 2 n, If the minimum degree of vertices in V satisfies δ 1 (G) ≥ 2 n − 1 − ξ, for some constant ξ, then for, given error ϵ, we can achieve a sampling in timesuch that the total variation distance between the sampling distribution and the ideal stationary distribution is at most ϵ.

We note that our theoretical performance bound in Theorem 4 is stronger for bipartite graphs. This advantage stems from the fact that the subroutine of uniformly sampling perfect matchings is known to be classically more efficient for bipartite graphs. For context, the BipartiteGBS technique also leverages the unique properties of bipartite graphs, though for the different goal of encoding arbitrary matrices for hardness proofs.

Numerical experiments

We conduct experiments to compare our algorithms with prior approaches. All results and plots are obtained by numerical simulations on a 10-core Apple M2 Pro chip with 16 GB memory and an NVIDIA L4 chip with 24 GB GPU memory via python 3.11.0. We use "networkx" libraryto store and manipulate graphs and "thewalrus" libraryto calculate Hafnian from adjacency matrix. Specifically, the graph problems we aim to solve are defined as follows:
Max-Hafnian: Given an undirected graph with non-negative adjacent matrix and target subgraph size k, find the subgraph of size k with the maximum Hafnian value defined as Eq. (2).
Densest k -subgraph: Given an undirected graph with non-negative adjacent matrix and target subgraph size k, find the subgraph of size k with the maximum density value. Density denotes the number of edges divided by the number of vertices.

---

### Versatile whole-organ / body staining and imaging based on electrolyte-gel properties of biological tissues [^44dc93f3]. Nature Communications (2020). High credibility.

For the c-Fos-labeled whole-brain data, we applied the "3D Find Maxima" function rather than the 3D centroid calculation for 2D Maxima objects adopted in the ClearMap algorithm. The same parameter values were used for the two datasets (MK-801 + or −) of the c-Fos-labeled brains. 0) The original stack covering the whole brain was cropped to prepare 100 partitioned stacks with 10-pixel x – y margins; (1) the "Subtract Background" function (rolling = 1) was applied to the single stack; (2) the stack was expanded three times in the three dimensions (x – y – z); (3) a 3D Minimum filter (radius x = y = z = 1) was applied; (4) a 3D Gaussian blur filter (sigma x = y = z = 1) was applied; (5) the "Set Threshold" function with a manually determined value was applied to eliminate background signals; (6) the "3D Find Maxima"function was applied with a radius size of x = y = z = 2.5 and a noise tolerance of 1. A stack with single pixels of maxima positions and a CSV file of the result table were generated. Every 120 slices, 5 marginal slices were separately calculated and integrated into the single stack generated above; (7) after calculating all 100 stacks, they were resized to the original x – y – z dimensions and then tiled after removing the 10-pixel x – y margins; and (8) the 3D Maximum filter and 3D Gaussian blur filter were finally applied to the stack to adjust the spot size to the labeled nucleus size. It took ~20 h to perform the calculations for ~8 GB of whole-brain data by using a workstation PC (64-bit Windows 10 Pro for Workstations, Intel(R) Xeon(R) E5-2687W v4 @ 3.00 GHz, two processors, 192 GB RAM, and 4× SSD RAID0 storage).

---

### Experimental demonstration of quantum advantage for NP verification with limited information [^776b5b5c]. Nature Communications (2021). High credibility.

Box 1 Protocol for NP verification

Input: Instance of the NP-complete problem and all its relevant parameters: N, δ, etc. after the reduction to a 2-out-of-4 SAT;

Goal: Verification of the solution;

1 Merlin and Arthur jointly perform a pre-calibration of the optical setup, finding the values of the visibility ν N and the transmittivity η;

2 Arthur computes the minimum value of the mean photon number number μ N in order to satisfy the quantum advantage conditions 1–3 and communicates it to Merlin in order to tune the amplitude of his pulses; he also computes the threshold T for accepting a proof;

3 Arthur sends a signal to Merlin to trigger the protocol;

4 Merlin encodes his proof in the phases of the pulses which are then sent to Arthur;

5 Arthur interferes Merlin's pulses with his own and assigns a value x k each time he registers a measurement in the k th pulse:

5.1 x k = 0 for a click in detector D 0 and no click in D 1;

5.2 x k = 1 for a click in detector D 1 and no click in D 0;

5.3 x k is randomly assigned if both detectors click.

6 For all the measured bits that form a clause, Arthur checks the satisfiability;

7 If the number of satisfied clauses is greater than T, Arthur accepts the proof, otherwise he rejects.

Experimental results

We now have all the ingredients to describe the experimental implementation of our verification test and the assessment of the quantum advantage for this task. As we defined previously, we need to satisfy three conditions to show quantum advantage. We need the verification procedure to take time linear in N, to have completeness and soundness such thatand, and that the number of clicks Arthur registers is much smaller than the input size N.

First, as we will see, in our experiment we use indeed a train of coherent pulses of size N and some simple classical post-processing of the measurement results, so our test satisfies condition 1. In fact, the real time to run the verification procedure for N between 5000 and 14,000 was a fraction of a second for the quantum part, a few seconds for the classical post-processing and a couple of minutes for the calibration procedure for each run.

---

### A continuous-time maxSAT solver with high analog performance [^b7d612ab]. Nature Communications (2018). Medium credibility.

Performance on random Max 3-SAT problems

We first test our algorithm and its prediction power on a large set (in total 4000) of random Max 3-SAT problems with N = 30, 50, 100 variables and constraint densities α = 8, 10. (In 3-SAT the SAT-UNSAT transition is around α ≃ 4.267). We compare our results with the true minimum values (E min) provided by the exact algorithm MaxSATZ. In Fig. 3, we compare the lowest energy found by the algorithm, the predicted minimumand the final decision by the algorithmwith the true optimum E min, by showing the distribution of their deviations from E min across many random problem instances. We use t max = 25 and at most Γ max = 150,000 runs, after which we stop the algorithm even if the prediction is not final. Thus, one expects that the performance of the algorithm decreases as N increases, (e.g. at N = 100), so that we would need to run more trajectories to obtain the same performance. Nevertheless, the results show that all three distributions have a large peak at 0. Most errors occur in the prediction phase, but many of these can be significantly reduced through simple decision rules (see Methods), because they occur most of the time at easy/small problems, where the statistics is insufficient (e.g. too few points since there are only few energy values). To show how the error in prediction depends on the hardness of problems, we studied the correlation between the errorand the hardness measure applicable to individual instances η = −ln κ /ln N (see ref.), see Fig. 3d (and Supplementary Fig. 4). Interestingly, larger errors occur mainly at the easiest problems with η < 2. Calculating the Pearson correlation coefficient betweenand η (excluding instances where the prediction is correct) we obtain a clear indication that often smaller η (thus for easier problems) generates larger errors. Positive errors are much smaller and shifted toward harder problems. Negative errors mean that the algorithm consistently predicts a slightly lower energy value than the optimum, which is good as this gives an increased assurance that we have found the optimum state. In Supplementary Fig. 4b, we show the correlation coefficients calculated separately for problems with different N and α.

---

### Episignature analysis of moderate effects and mosaics [^7abeaa40]. European Journal of Human Genetics (2023). Medium credibility.

EWAS, mRMR feature selection, and SVM-training

Differentially methylated CpG-sites were detected by epigenome-wide association analysis (EWAS) using limma _3.42.2 regressing M -values on mutation status, sex, age, and Houseman-estimates of white blood cells. 19 controls of both sexes, 2–50 years old, matched the cases' range of 0–51 yrs and were very unlikely genetically and phenotypically to have methylation aberrations. Cases were chosen as described below ("stepwise re-training"). Primary selection of CpG-sites from EWAS on M-values required genome-wide significance (< 5x10E-08) and an absolute average difference > 0.4.

To find an optimal selection among these sites with regard to their correlation structure we applied the bootstrap ensemble variant (mRMRe.b) of the minimum-redundancy-maximum-relevance feature selection algorithm (mRMRe_2.1.2). It first searches the CpG-site x 1 that has the highest mutual information MI with the phenotype y, MI (x 1, y) = -ln(1- ρ (x 1, y) 2)/2 where ρ is the correlation coefficient. Then, the selection S of CpG-sites is increased one by one, with each added site x i having an optimal trade-off between maximal MI with y and minimal MI with the previously selected sites x j. This is achieved by finding the site x i with maximal score MI (x i, y)-Σ j∈S MI (x i, x j)/| S |. Since this classical mRMR may miss the global optimum, however, ensemble versions have been developed which combine the results of m classical mRMR runs that are either started from each of the m features with the top MI (x, y) values or, what turned out to be even better, are run on m bootstraps of the examined individuals. We used m = 20 bootstraps of the case-control dataset, each running classical mRMR with the recommended max(| S |) = 15 CpG-sites, and united all CpG-sites selected by the 20 runs. (Smaller values of m and max(| S |) were used, however, for reducing the episignature length; see below).

---

### Herbaceous perennial plants with short generation time have stronger responses to climate anomalies than those with longer generation time [^21b1041a]. Nature Communications (2021). High credibility.

Climatic data

To test the effect of temporal climatic variation on demography, we gathered global climatic data. We downloaded 1 km 2 gridded monthly values for maximum temperature, minimum temperature, and total precipitation between 1901 and 2016 from CHELSAcruts, which combines the CRU TS 4.01, and CHELSAdatasets. Gridded climatic data are especially suited to estimate annual climatic means. These datasets include values from 1901 to 2016, which are necessary to cover the temporal extent of all 162 plant populations considered in our analysis. For our temperature analyses, we calculated the mean monthly temperature as the mean of the minimum and maximum monthly temperatures. We used monthly values to calculate the time series of mean annual temperature and total annual precipitation at each site. We then used this dataset to calculate our annual anomalies for each census year, defined as the 12 months preceding a population census. Our annual anomalies are standardized z -scores. For example, if X is a vector of 40 yearly precipitation or temperature values, E calculates the mean, and σ calculates the standard deviation, we compute annual anomalies as A = [X − E (X)]/ σ (X). Therefore, an anomaly of one refers to a year where precipitation or temperature was one standard deviation above the 40-year mean. In other words, anomalies represent how infrequent annual climatic conditions are at a site. Specifically, if we assume that A values are normally distributed, values exceeding one and two should occur every 6 and 44 years, respectively. We used 40-year means because the minimum number of years suggested to calculate climate averages is 30.

---

### Fully automated body composition analysis in routine CT imaging using 3D semantic segmentation convolutional neural networks [^e6f51284]. European Radiology (2021). Medium credibility.

Besides random data augmentations, additional pre-processing steps were performed before feeding the image data into the neural networks. Volumes were downscaled by factor 2 to 128 × 128 on the x -/ y -axes, retaining a slice thickness of 5 mm on the z -axis. CT images are captured as Hounsfield units (HU), which capture fine details and allow for different interpretations depending on which transfer function is used to map HUs to a color (e.g. black/white). Normally, when using floating-point values, the typical scanner quantization of 12 bits can be stored lossless and a network should be able to process all information without any problems. In this work, multiple HU windows [− 1024, 3071], [− 150, 250], and [− 95, 155] were applied to the 16-bit integer data in order to map to [0, 1] with clipping outliers to the respective minimum and maximum values and stacked as channels. Lastly, the network inputs were centered around zero with a minimum value at − 1 and maximum value at + 1.

For supervision, a combination of softmax cross-entropy loss and generalized Sørensen Dice loss was chosen, similar to. Voxels marked with an ignore label do not contribute to the loss computation. Both losses are defined as below: C stands for the total number of classes, which equals six for the problem at hand.and y c, n represent the prediction respectively groundtruth label for class c at voxel location n. The background class is in this work explicitly not covered by the dice loss in order to give the foreground classes more weight in the optimization process. This choice is well known for class imbalanced problems where the foreground class only covers little areas compared with the background class.

The final loss is an equally weighted combination of both losses:

---

### Detection of haplotype-dependent allele-specific DNA methylation in WGBS data [^a60b696c]. Nature Communications (2020). High credibility.

JSD and uncertainty coefficient

Differences between two PDMs p 1 (x) and p 2 (x) are quantified by means of the JSD D (p 1, p 2), where, This quantity is a normalized metric since it takes values between 0 and 1, is symmetric, and satisfies the triangle inequality. Moreover, it achieves its minimum value of 0 if and only if the two PDMs p 1 (x) and p 2 (x) are identical, whereas it takes its maximum value of 1 when the PDMs do not overlap with each other. In addition, the amount of information that the allele of origin A ϵ {1, 2} conveys about the random methylation state X is measured by means of the uncertainty coefficient Q (X; A), given bywhereis the mutual information between the random methylation state X and the allele of origin A, h (X) is the NME of X without knowing its allele of origin, and N is the number of CpG sites. This quantity takes values between 0 and 1, with larger values indicating that the allele of origin conveys more information about the random methylation state.

Since we are considering diploid organisms, we can take the probability of finding one of the two homologous alleles of a given haplotype in a biological sample to be equal to the probability of finding the other allele; i.e. we can set Pr[A = 1] = Pr[A = 2] = 1/2. In this case, it can be shownthat I (X; A) = D 2 (p 1, p 2), where D (p 1, p 2) is the JSD between the PDMs p 1 (x) = Pr[X = x ∣ A = 1] and p 2 (x) = Pr[X = x ∣ A = 2] of the two homologous alleles. This implies thatwhich shows that, in addition to being proportional to the square JSD between the two PDMs p 1 (x) and p 2 (x) associated with the homologous alleles of a haplotype, the test statistic T PDM used by the CPEL method provides a measure of association between the random methylation state and the allele of origin by means of the uncertainty coefficient. Efficient techniques for evaluating the uncertainty coefficient and, therefore, T PDM, within homozygous and heterozygous haplotypes are discussed in Supplementary Methods, Section 9.

---

### The fly connectome reveals a path to the effectome [^be944930]. Nature (2024). Excellent credibility.

We explored the range of timescales for predicted dynamics by examining the eigenvalues in the complex plane (Fig. 3b). Complex eigenvalues are associated with a complex eigenvector whose real and imaginary parts define a plane of rotational dynamics for neural activity. If a pattern of activity is induced within this plane, then neural activity will continue to evolve solely within the plane — that is, it will transition over time between mixtures of the real and imaginary part of the eigenvector. The angle from the positive real axis determines the speed of these rotational dynamics. At 0°, the eigenvalue is real positive and there are no oscillatory dynamics but a simple monotonic decay. A non-zero angle is exactly the angular step size of rotational dynamics at each time step, so small angles imply slow rotational dynamics, and a negative real eigenvalue (180°) implies the fastest possible rotational dynamics (flipping sign at each time step). We find there is a broad distribution of timescales of rotational dynamics. The distribution of angles had modes at 0°, 90° and 180°, implying a preponderance of monotonically decaying modes, rapid transitions between distinct populations and the fastest possible timescale, respectively.

We then investigated whether dynamical modes tended to be independent of each other. When eigenvectors are orthogonal to each other, the dynamics associated with each eigenvector are independent (under assumed white noise inputs). Conversely, if two eigenvectors are highly correlated, then these dynamics are more likely to co-occur. Thus, by examining the correlation between eigenvectors we can test which dynamical motifs will tend to be enlisted simultaneously — potentially because they are involved in similar computations. Examining the correlation matrix of the first 100 eigenvectors (treating the real and complex parts as separate eigenvectors), we found on average correlation was weak with sparsely distributed higher values (Fig. 3c). We observed that early eigenvectors tended to have lower correlation to others (first ten rows and columns dark). Aggregating the correlation of each of the top 1,000 eigenvectors to all others, we found a clear trend in which both the max and average correlation increased with eigenvalue rank (Fig. 3d). Roughly speaking, the top ten dynamical modes will occur independently of each other, whereas the rest will tend to co-occur with at least one other mode. Thus, we predict that dynamical modes will typically operate independently of each other, which bodes well for the project of examining these circuits individually.

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^c942b654]. Nature Communications (2025). High credibility.

Mean-independent heterogeneity measures

To create a truly mean-independent measure of heterogeneity the exact relationship between the mean and any measure of heterogeneity must be identified and removed. We derived the variance's mean-dependence relationship for lower-bounded, double-bounded, upper bounded, and unbounded variables that are approximately beta, gamma or normally distributed, where the lower and upper bounds can be any two real numbers (L and U, respectively). The nonlinear relationship between the standard deviation and the mean's distances from the boundaries (μ − L for the lower boundary, and U − μ for the upper boundary) can be made linear by squaring the standard deviation to give greater weight to the tails of the distribution, and retrieve the variance (Fig. 2 e, m). Thus, normalising the variance (σ 2) by the mean's distances from the boundaries yields a mean-independent measure of dispersion (δ). In its generalised formulation, the calculation of δ for any (bounded or unbounded) continuous variable approximated by the beta, gamma or normal distributions is given bywhereis an indicator function withbeing the set of all real numbers, m is the lower (L) or upper (U) bound of the variable, andif, otherwise. Note that any number raised to the zeroth power is equal to one, such that if the lower or upper bound does not exist or is not a real number (e.g.) the boundary has no influence on the dispersion nor δ. Thus, specific cases arise from this generalised formula as combinations of existing or non-existing bounds. For double-bounded variables, the variance's mean-dependence is influenced simultaneously by the mean's distance from the lower and upper bounds, [μ − L] [U − μ], yieldingFor lower-bounded variables, the variance's relationship with the mean is linear with the distance from the lower bound, μ − L (Fig. 2 e), and thusFor upper-bounded variables, the variance's relationship with the mean is linear with the distance from the upper bound, U − μ, givingAll of which arise as mean-independent measures of heterogeneity, as it can be demonstrated from the approximation of single-bounded and double-bounded variables respectively as gamma- and beta-distributed variables (Fig. 2 h, p). Also implicitly included in the generalised formulation (Equation (1)) is the variance as a mean-independent measure of dispersion for unbounded continuous variables, as demonstrated with the normal distributionHence, δ is valid for bounded and unbounded continuous variables, where the bounds can be determined conceptually or experimentally. Bounds should not be determined as the observed minimum or maximum from a small sample without conceptual or experimental support. Examples of potential boundaries include the proportion of land covered by forest (L = 0 and U = 1), animal mass (L = 0 g), beetle size limited by oxygen (L = 0 cm and U = 16 cm), and biochemical fish depth limits (L = 0 m and U = 8200 m) (see Supplementary Note 1 for more details) –. Also note that δ is not scale invariant for single-bounded and unbounded variables. Therefore, variables must have common units for comparison (e.g. the same currency, or the same unit of distance).

---

### Cross-orientation suppression in visual area V2 [^cfcca9d0]. Nature Communications (2017). Medium credibility.

Eigenvector significance

To determine which eigenvectors of J were significant, we generated shuffled J matrices to determine the distribution of maximum and minimum eigenvalues. We began by subtracting the mean of J to avoid the spurious eigenvalues that can be caused by a nonzero mean of J. We then randomly shuffled the diagonal and off-diagonal elements separately to create random symmetric matrices and build distributions of maximum and minimum eigenvalues. We then checked the eigenvalues of the zero mean J matrix in order of decreasing magnitude against the distributions to determine the probability to see a value of that magnitude in a random matrix. If the probability was < 0.05, we considered the eigenvalue and the corresponding eigenvector to be significant.

Fitting gabors

To characterize the feature selectivity of J, we approximated it using the weighted sum of Gabor wavelets. The equation for the Gabor wavelet is

where

x 0 and y 0 indicate the location of the Gabor wavelet in the image, θ controls the orientation, γ controls the aspect ratio, σ controls the size of the Gabor, λ controls the spatial frequency, and φ controls the spatial phase. A is a normalization constant. With a set of Gabor wavelets g i and corresponding weights w i, an approximation of J can be constructed from the weighted sum

We fit two variations of this model. First, we fit the J matrix as representing independent Gabors, with the number of Gabors equal to the number of significant features. After finding that the Gabors form quadrature pairs (Fig. 4) we performed a second fit using pairs of Gabors with identical parameters except for φ, which was 0 and πi/2 in order to form a quadrature pair.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^d6d8fcbc]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Spirometry reporting specifies that numerical values are given for the FEV1, the FVC, and the FEV1/FVC ratio; the latter should be reported as a decimal fraction and the space for percent predicted value left blank, and if bronchodilators are given the LLN column need not be repeated with absolute and percent change given only for FEV1 and FVC. Other numerical values such as the forced inspiratory flow at 75% of FVC (FEF75%) and FEF25–75% are not recommended for routine use. Graph requirements include that for the volume–time curve the volume scale should be at least 10 mm/L, the time scale at least 20 mm/s, and 1 second prior to the start of expiration should be displayed; on the flow–volume plot the flow display should be at least 5 l/min/L/s, and the ratio of flow to volume should be 2 L/s to 1 L, and linear and log scales where values are plotted as z-scores relative to the predicted value (z = 0) give an intuitive sense of severity.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^bb9b7f12]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Δ Φ, the probability distribution at a point Φ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Φ ∈ {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters α and β, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since α and β are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Φ ∈ [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(α, β) and x 2 = 1 − x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Φ, it may seem natural to select α and β by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^b1513dd5]. CDC (2011). Medium credibility.

Table 1b — Minimum sample size for detecting geometric mean (GM) ratio k with α = 0.05 and W = 0.9 — presents required sample sizes by standard deviation (S) strata and defines parameters; α is significance level, W is power, GM is geometric mean, and S is standard deviation, noting that 1.2 is the standard deviation of national viral load (VL) data. Each jurisdiction will need to assess the standard deviation of their local VL data and then determine the appropriate sample size needed to assess VL, and if the sample size is inadequate to meet the recommended case inclusion criterion, an alternate method may need to be used, such as combining multiple years of data. For k = 3, sample sizes across S = 1, 1.1, 1.2, 1.3, 1.4, 1.5 are 75, 91, 108, 127, 147, 169, and Table 1b uses power = 90%; jurisdictions may also explore differences in means of viral loads, including categorical differences in the proportion with undetectable or very low VL.

---

### Best clinical practices for the sleep center adjustment of noninvasive positive pressure ventilation (NPPV) in stable chronic alveolar hypoventilation syndromes [^45da6a47]. Journal of Clinical Sleep Medicine (2010). Medium credibility.

NPPV titration recommendations — scope, setting, and application — pertain only to nighttime NPPV titration studies, do not address indications for treatment, and may apply to alternative titration methods under physician direction. The optimal nocturnal titration setting is an AASM-accredited sleep center or laboratory with personnel experienced in NPPV titration, and the titration should be reviewed and treatment pressures selected by a physician board certified in sleep medicine. Recommendations for minimum and maximum IPAP and EPAP may be constrained by the specific BPAP device, and they should not be followed in a "cookbook" manner; technologists and clinicians should combine their experience and judgment. Recommendation labels indicate origin: those extracted from active AASM practice parameters are labeled "(Standard)" and those not based on published parameters are labeled "(Consensus)". Unless otherwise stated, the recommendations apply to both pediatric and adult patients undergoing NPPV titration with PSG, and adequate follow-up by a knowledgeable physician during NPPV therapy is essential.

---

### Phenolic multiple kinetics-dynamics and discrete crystallization thermodynamics in amorphous carbon nanostructures for electromagnetic wave absorption [^39e6a9e7]. Nature Communications (2024). High credibility.

We discuss separately the two groups of multi-shell SISB samples that achieved excellent EWA. At 700 °C and 20 wt%, both the triple-shell bowl-shaped hollow PRNs (mbh-PRNs-2) and double-shell short chain-shaped hollow PRNs (dch-PRNs) derived carbon achieve full coverage of X-band and Ku-band in a wide range of thickness (Fig. 5c–f), meaning slight thickness errors are allowed in the production operation. Notably, mbh-PRNs-2 (700 °C, 20 wt%) achieved a maximum EAB of 8.17 GHz and a minimum reflection loss (RL min) of −37.88 dB, while dch-PRNs-2 (700 °C, 20 wt%) achieved a maximum EAB of 8.17 GHz and an RL min of −37.88 dB. Based on the metal backplane model, the incident wave is reflected multiple times between the surface of the dallenbach layer and the metal bottom surface. When an electromagnetic wave is reflected back with an odd number of half wavelengths, the opposite phases of the waves lead to interference phase extinction. As a result, peaks of the reflection loss curves for a given thickness fall on the 1/4 wavelength curves (Supplementary Fig. S62a-d). On the other hand, for a given material, its thickness, frequency, and electromagnetic parameters together determine the normalized input impedance (Z in). As can be observed from the Smith's image consisting of the real and imaginary parts of the complex Z in (Supplementary Fig. S62e, f), the closer the real part is to 1 the closer the imaginary part is to 0, meaning that RL min will be smaller. For EAB, broadband absorption is triggered when Z in values at more frequencies fall in the effective range. Apparently, a unique physical mechanism is triggered by the synergistic regulation of phenolic multiple dynamics and discrete crystallization thermodynamics to obtain suitable dielectric parameters. Therefore, our multi-shell SISB structure amorphous carbon has advanced EWA properties (Fig. 5g and Supplementary Table S8), compared with other carbon materials with special structures (e.g. porous hollow, hollow, macro-porous, bowl-shaped hollow, yolk-shell, etc.).

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^aeeaae45]. Nature Communications (2025). High credibility.

Fitting models to data is an important part of the practice of science. Advances in machine learning have made it possible to fit more-and more complex-models, but have also exacerbated a problem: when multiple models fit the data equally well, which one(s) should we pick? The answer depends entirely on the modelling goal. In the scientific context, the essential goal is replicability: if a model works well to describe one experiment, it should continue to do so when that experiment is replicated tomorrow, or in another laboratory. The selection criterion must therefore be robust to the variations inherent to the replication process. In this work we develop a nonparametric method for estimating uncertainty on a model's empirical risk when replications are non-stationary, thus ensuring that a model is only rejected when another is reproducibly better. We illustrate the method with two examples: one a more classical setting, where the models are structurally distinct, and a machine learning-inspired setting, where they differ only in the value of their parameters. We show how, in this context of replicability or "epistemic uncertainty", it compares favourably to existing model selection criteria, and has more satisfactory behaviour with large experimental datasets.

---

### The minimal work cost of information processing [^894e044e]. Nature Communications (2015). Medium credibility.

where the sum ranges only over those x that have a non-zero probability of occurring. In the case of deterministic mappings p (x ′| x)∈{0,1}, this corresponds to the maximum number of input states that map to a same output state. For the AND gate, provided all four states 00, 01, 10 and 11 have non-negligible probability of occurring, there are three input states mapping to the same output state, so (3) gives us simply. Also, in simple examples as considered here, the expression (3) is stable to considering an-approximation (Supplementary Note 4); this quantity is thus physically justified.

Crucially, our result reveals that the minimal work requirement in general depends on the specific logical process, and not only on the input and output states. This contrasts with traditional thermodynamics for large systems, where the minimal work requirement of a state transformation can always be written as a difference of a thermodynamical potential, such as the free energy. For example, the minimal work cost of performing specifically an AND gate may differ from that of another logical process mapping an input distribution (p 00, p 01, p 10, p 11) (with ∑ i p i = 1) to the distribution (p ′ 0, p ′ 1) = (p 00 + p 01 + p 10, p 11) (Recall that the classical counterpart of a quantum state is a probability distribution.). To see this, consider the XOR gate, which outputs a 1 exactly when both inputs are different (see Fig. 2b). The minimal work cost requirement of this gate, as given by (3), is now only kT ln 2, as in the worst case, only a single bit of information is erased (again supposing that all four input states have non-negligible probability of occurring). Now, suppose that, for some reason, the input distribution is such that p 01 + p 10 = p 11, that is, the input 11 occurs with the same probability as of either 01 or 10 appearing. Then, the XOR gate reproduces the exact same output distribution as the AND gate: in both cases, we have p ′ 0 = p 00 + p 10 + p 01 = p 00 + p 11 and p ′ 1 = p 11 = p 01 + p 10. In other words, both logical processes have the same input and output state, yet the XOR gate only requires work kT ln 2 compared with the AND gate, which requires 1.6 kT ln 2. Furthermore, we point out that this difference, which appears small in this case, may be arbitrarily large in certain scenarios (Supplementary Note 4).

---

### The eighty five percent rule for optimal learning [^5399633a]. Nature Communications (2019). High credibility.

Methods

Optimal error rate for learning

In order to compute the optimal difficulty for training, we need to find the value of Δ that maximizes the learning gradient, ∂ ER/ ∂β. From Eq. (3) we haveFrom here the optimal difficulty, Δ *, can be found by computing the derivative of the gradient with respect to Δ, i.e. Setting this derivative equal to zero gives us the following expression for the optimal difficulty, Δ *, and error rate, ER * where p ′(x) denotes the derivative of p (x) with respect to x. Because β and Δ * only ever appear together in these expressions, Eq. (26) implies that β Δ * is a constant. Thus, while the optimal difficulty, Δ *, changes as a function of precision (Fig. 1c), the optimal training error rate, ER * does not (Fig. 1d). That is, training with the error rate clamped at ER * is guaranteed to maximize the rate of learning.

The exact value of ER * depends on the distribution of noise, n, in Eq. (2). In the case of Gaussian noise, we havewhich implies thatand that the optimal difficulty isConsequently the optimal error rate for Gaussian noise isSimilarly for Laplacian noise and Cauchy noise (p (x) = (π (1 + x 2)) −1) we have optimal error rates of

---

### Identifying domains of applicability of machine learning models for materials science [^72c4ba76]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ν that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by σ lin (x 1, x 2) ≡ −0.3 ≤ x 1 ≤ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: σ rbf ≡ −3.3 ≤ x 11 ≤ 3.1 ∧ x 2 ≤ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA σ poly (x 1, x 2) ≡ −3.5 ≤ x 2 ≤ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### Statistics review 1: presenting and summarising data [^4c1c82d3]. Critical Care (2002). Low credibility.

The present review is the first in an ongoing guide to medical statistics, using specific examples from intensive care. The first step in any analysis is to describe and summarize the data. As well as becoming familiar with the data, this is also an opportunity to look for unusually high or low values (outliers), to check the assumptions required for statistical tests, and to decide the best way to categorize the data if this is necessary. In addition to tables and graphs, summary values are a convenient way to summarize large amounts of information. This review introduces some of these measures. It describes and gives examples of qualitative data (unordered and ordered) and quantitative data (discrete and continuous); how these types of data can be represented figuratively; the two important features of a quantitative dataset (location and variability); the measures of location (mean, median and mode); the measures of variability (range, interquartile range, standard deviation and variance); common distributions of clinical data; and simple transformations of positively skewed data.

---

### Multilayer hazes over saturn's hexagon from cassini ISS limb images [^f2df8e87]. Nature Communications (2020). High credibility.

The wave frequency ω is calculated using c x ≈ 0 ms −1 (hexagon-phase speed) and u = 120 ms −1 (value of the peak of the eastward jet). We take for the horizontal scale the Rossby deformation radius L D ≈ 1500 km because of the prevalence of geostrophic conditions at the hexagon. Then4 × 10 −6 m −1, δ = 1.41, and≈ 5 × 10 −4 s −1, implying that. The acoustic term and the vertical damping scale have similar values in Eq. (3), i.e.≈ 2 × 10 −10 m −2 and therefore the first term dominates in Eq. (3). Then we have

Figure 5 (upper panel) shows the dependence of the vertical wavelength L z = 2π/m for gravity waves as a function of the frequency and for two values of the intrinsic velocity (c x − u) according to Eq. (4). The frequency range is bounded by the Coriolis and Brunt–Väisälä frequencies from Eq. (2). The expected values for the vertical wavelength would be in the range of ∼20–60 km. Figure 5 (bottom panel) shows an idealization of the vertical structure of the amplitude of propagating internal waves with no dissipation (i.e. without damping by eddy and molecular viscosity). The amplitude grows upward ∼exp(z/ 2 H) to make energy density constant with altitude. For hazes produced by condensation, it can be expected that the vertical wavelength is L z ∼ 2Δ z with Δ z the layer thickness. Taking Δ z ∼ 8–18 km (the range of thicknesses for layers L0–L6 in Table 1) gives L z ∼ 16–36 km, consistent with the above calculations. The maximum and minimum of the amplitude oscillation are partially in agreement with the observed pattern in some of the layers (L0–L2, L4, and L6), but differ from these idealized oscillations in others (L3 and L5). Obviously, this is to be expected, since in our analysis we neglect dissipation effects and changes in the scale height. In addition, compression and rarefaction due to the gravity wave passage could affect the particle number density and thickness of the haze layers. However, the lack of data on both the mechanisms subjacent to the gravity wave generation and the initial upward velocity of the parcels in the wave, does not allow us to deepen the analysis.

---

### A practical guide for understanding confidence intervals and P values [^c0a7d4a0]. Otolaryngology — Head and Neck Surgery (2009). Low credibility.

The 95 percent confidence interval about the mean demarcates the range of values in which the mean would fall if many samples from the universal parent population were taken. In other words, if the same observation, experiment, or trial were done over and over with a different sample of subjects, but with the same characteristics as the original sample, 95 percent of the means from those repeated measures would fall within this range. This gives a measure of how confident we are in the original mean. It tells us not only whether the results are statistically significant because the CI falls totally on one side or the other of the no difference marker (0 if continuous variables; 1 if proportions), but also the actual values so that we might determine if the data seem clinically important. In contrast, the P value tells us only whether the results are statistically significant, without translating that information into values relative to the variable that was measured. Consequently, the CI is a better choice to describe the results of observations, experiments, or trials.

---

### Forecasting the magnitude of the largest expected earthquake [^e22981f0]. Nature Communications (2019). High credibility.

To calculate the Bayesian predictive distributions and the probabilities for the largest expected earthquakes, we first generated the Markov chains for all the model parameters { θ, ω } = { β, μ, K, c, p, α } by specifying the training time interval from T 0 to T e and extracting the earthquakes in this interval above a certain threshold m c. The Markov chains were simulated using the Metropolis-within-Gibbs algorithm as described in the "Methods" section. Finally, for each set of the model parameters from the generated MCMC chain, we simulated the ETAS process forward in time using the well-established thinning algorithmand extracted the maximum magnitude event. The distribution of these maxima converges to the Bayesian predictive distribution (see the "Methods" section). For the prior distribution, π (θ, ω), of the model parameters, we used a Gamma distribution (specific values for the mean and variance of the priors for each model parameter are provided in Table 3). We used the point estimates of the ETAS parameters as the mean values for the prior distributions. We also considered a truncated Normal distribution as a prior distribution. Both these distributions produced statistically similar results. As these distributions are used to constrain any prior knowledge on the model parameters, we assumed that this information can be represented by a bell-shaped distribution with a well-defined mean value and variance. The prior distribution plays an important role in the Bayesian analysis and we explore the sensitivity of the obtained results due to the variation of the prior parameters later in the analysis.

For the proposal distribution, we used a truncated Normal distribution with the support defined as the positive real axis x ∈ [0, ∞] to assume the positive values for the model parameters (the variances are provided in Table 3). The choice of the proposal distribution depends on how well it can approximate the posterior distribution. It also plays a critical role for the convergence of the Markov chains. Typically, standard statistical distributions are used. For each combination of the training and forecasting time intervals, we simulated total of 200,000 samples and discarded 100,000 samples as burn-in. One particular example of the Markov chains for the ETAS model parameters is given in Supplementary Fig. 1. For the MCMC sampling of the ETAS parameters we used only events during the training time interval [T 0, T e].

---

### A continuous-time maxSAT solver with high analog performance [^87d3d16d]. Nature Communications (2018). Medium credibility.

Algorithm description

Here, we give a simple, nonoptimized variant of the algorithm (see flowchart in Supplementary Fig. 2). Better implementations can be devised, for example with better fitting routines, however the description below is easier to follow and works well. Given a SAT problem, we first determine the b parameter as described previously. Step 1: initially we set, Γ min, = and t max. Unless specified otherwise, in our simulations we used Γ min = 100, Γ max = 2 × 10 6, t max = 50. Step 2: to initialize our statistics, we run Γ min trajectories up to t max, each from a random initial condition. For every such trajectory ω we update the p (E, t) distributions as function of the energies of the orthants visited by ω. We record the lowest energy value found. Step 3: starting from Γ = Γ min + 1 and up to Γ max, we continue running trajectories in the same way and for each one of them check: (a) If, set, update p (E, t) and go to Step 4. (b) If Γ just reached, go to Step 4. (c) If Γ = Γ max, output "Maximum number of steps reached, increase Γ max ", output the lowest energy value found, the predictedand the quality of fit for, then halt. Step 4: using the p (E, t) distributions, estimate the escape rates κ (E) as described in the corresponding Methods section. Step 5: the κ (E) curve is extrapolated to the E − 1 value obtaining κ (E − 1) and then using this we predict(as described in another Methods section). Further extrapolating the κ (E) curve to κ = 0 we obtain(see the corresponding Methods section). Step 6: we check the consistency of the prediction defined here as saturation of the predicted values. We call it consistent, ifhas not changed during the last 5 predictions. If it is not consistent yet, we continue running new trajectories (Step 4). If the prediction is consistent, we check for the following halting conditions: (i) Ifthen we decide the global optimum has been found:and skip to Step 7. (ii) If the fitting is consistently predicting(usually it is very close,) we check the number of trajectories that has attained states with, i.e. = . If it is large enough (e.g. > 100), we decide to stop running new trajectories and setand go to Step 7. (iii) Ifthen we most probably have not found the global optimum yet and we go to Step 4. We added additional stopping conditions that can shorten the algorithm in case of easy problems, see Methods corresponding section, but these are not so relevant. Step 7: the algorithm ends and outputs, values, the Boolean variables corresponding to the optimal state found, along with the quality of fit.

---

### Pediatric radiopharmaceutical administered doses: 2010 North American Consensus guidelines [^9d1c8674]. Journal of Nuclear Medicine (2011). Medium credibility.

Pediatric nuclear medicine — dose scaling formulas specify how to derive pediatric administered activity from adult dosing: Body mass (straight weight basis) uses (Body mass (kg) × adult dose)/70 kg; body surface area (BSA) uses (BSA (m^2) × adult dose)/1.73 m^2; Webster's formula uses (Age (y) + 1) × (adult dose)/(age (y) + 7).

---

### The snm procedure guideline for general imaging 6.0 [^638aaec4]. SNMMI (2010). Medium credibility.

SNM Procedure Guideline for General Imaging — matrix size and pixel depth in nuclear medicine acquisitions notes that matrix size is almost always a power of 2 with typical values 64 x 64, 128 x 128, 256 x 256 and 512 x 512, and that non-square matrix sizes also exist for whole-body studies. Each pixel can be represented with a single byte (pixel values ranging from 0 to 255 counts) or with 16 bit words (pixel values ranging up to a maximum of 32k or 64k). Overflow occurs when the number of counts recorded at some given position exceeds the maximum number of counts, and overflow is more likely to occur when using a byte matrix.

---

### Anomalous dirac point transport due to extended defects in bilayer graphene [^e97b4839]. Nature Communications (2017). Medium credibility.

Fig. 6
Continuum description of a partial dislocation. We show here the case of a type three partial; types 1 and 2 show similar behaviour. a The displacement vector of the two layers u (x), with the first layer is shifted by u (x) with respect to the second layer. b The strain state of the partial ε xy showing that the partial is under a modest strain taking its maximum value at the partial mid point. c The resulting effective field S (x), see Eqs. (2) and (5), expressed as its decomposition onto a complete set of stacking matrices, AB, AC, AA and τ z stacking, see Eq. (9). Notice that the τ z field is identically zero, and is in fact zero for all partial geometries. d The transport momenta. While this is purely imaginary within the terraces it is seen to acquire a real part within the partial dislocation

The partial dislocation, which interpolates between these two limits, we model using the displacement field u (x) shown in panel of Fig. 6 for the case of a partial Burgers vector a (0, −1/). This form of u (x) is chosen to reproduce as closely as possible the calculated partial structure described in ref. As may be seen — see panel (b) of Fig. 6 — the partial dislocation is strained, with the maximum strain found at the partial core. The resulting interlayer coupling field S (x) can be conveniently expressed by projection onto a complete set of four stacking matrices which, with a c -number coefficient, are sufficient to describe all possible interlayer coupling fields: S (x) = c AB (x) τ AB + c AC (x) τ AC + c AA (x) τ AA + c z (x) τ z. These matrices are given byThis projection, as the τ matrices evidently inhabit the interlayer and sublattice space of the S (x) field, is directly informative of the local stacking of the bilayer. This projection is exhibited in panel (c) of Fig. 6 revealing the expected transition from AB to AC stacking across the partial but also a non-zero AA component of the stacking field, that has its maximum at the partial core. The projection onto the τ z type of stacking is zero throughout the partial dislocation.

---

### Identification of significant chromatin contacts from HiChIP data by fitHiChIP [^b8bcd1cf]. Nature Communications (2019). High credibility.

Merging filter for adjacent loops

Suppose a significant loop reported by FitHiChIP or another method is represented by an ordered pair of interacting fixed-size (here 5 kb) bins (x, y) where x < y. Two loops (x 1, y 1) and (x 2, y 2) are adjacent if their constituent bins are either adjacent or equal, that is, | x 1 − x 2 | ≤ 1 and | y 1 − y 2 | ≤ 1. If we use a 2D contact matrix to represent all possible pairs of bins, and denote a significant loop between two bins x and y as a nonzero entry in location (x, y), the problem of finding a set of mutually adjacent loops reduces to finding non-trivial connected components of a graph using the 8-connectivity rule. We have used Python package networkx to find such components/clusters of adjacent statistically significant loops. For each such component, we extract a subset of loops that are likely representatives of direct interactions (with remaining loops as likely bystanders) in order to improve specificity of our loop calls mainly for regions with large number of adjacent loop calls. A trivial approach is to simply report one loop per connected component that has a minimum p value (denoted as MIN approach). However, such an approach has the obvious downside of eliminating meaningful interactions when multiple independent and direct loops fall into the same component. Therefore, we employ an iterative merging approach to select a subset S from the set of loops K (| S | < | K |) within a connected component. In each iteration, we select the current most significant loop l within K (based on the statistical significance value, or contact count, or any other loop scoring method), and include this loop in the set S if and only if l does not belong within W = B × B (in terms of bins) neighborhood of any loop already in S. We use recovery plots to test multiple values for B (2, 5, and 10) and select the number that performs best in terms of specificity (Supplementary Fig. 7). Unless otherwise stated, we use W = 2 × 2 when merging filter is applied to the results of FitHiChIP and that of existing methods.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^81467424]. CDC (2011). Medium credibility.

Community viral load (VL) calculation — log transformation and mean: We recommend that calculation of Community VL and its related viral load Measures is performed after transformation of viral load results onto the logarithmic base 10 scale, followed by calculation of the mean, and we recommend calculation of geometric mean (GM) for viral load by averaging the log transformed values and transforming the average back to the original (linear) scale; using log base 10 is advantageous because a value of 2 on the log10 scale is 100 on the original scale, 3 corresponding to 1000, 4 corresponding 10000. Early VL work often used the median, but jurisdictions that have successfully engaged HIV-infected persons in care may have over 75% with undetectable VL; the median would be undetectable, so the mean was used, and the logarithmic transformation helps to normalize the distribution of viral load values and reduces the influence of outlying measurements for persons having extreme viremia.

---

### 2D material programming for 3D shaping [^0d7f1bf4]. Nature Communications (2021). High credibility.

Cone singularities

The Riemann theorem suggests that our method can form any curved shapes of thin sheets by conformally morphing 2D hydrogels. However, in practice, the accessible Ω r limits accessible 3D configurations. To mitigate this limitation, we introduced the concept of cone singularities (Fig. 3). The idea is to first map a 3D shape on a cone surface (K = 0) and then unfold the cone surface to the 2D plane (without further area distortion) by cutting the surface through cone points, rather than directly mapping a shape to the plane (as shown in Fig. 2). This approach can significantly reduce Ω r, as a 3D shape can be conformally mapped to a cone surface with lower area distortion than to the plane and the cone surface can be isometrically flattened to the plane.

Fig. 3
Cone singularities.

a Hemispheres formed without cone singularities (left) and with a cone singularity with cone angle θ c = 60° (right). b 2D growth Ω (left) and Ω with a cone singularity with θ c = 60° (right) used to print the hemispheres in a. c Ω computed for a hemisphere using a cone singularity with different θ c as a function of normalized radial position r / R. d Ω r required to form a full hemisphere with different θ c. Ω r = Ω max /Ω min, where Ω max and Ω min are the maximum and minimum values of Ω, respectively. The dashed line indicates Ω r = 2.22. e Experimentally measured (red circles) and computationally calculated (black circles) cap angle φ as a function of θ c. φ is the polar angle from the pole of the cap to its base in a spherical coordinate system. f Experimentally measured (red circles) and computationally calculated (black circles) Gaussian curvature K as a function of θ c. g – i Printing of an almost-complete sphere with 12 cone singularities: target shape (g), experimentally printed structure (h), and Ω with cone singularities (red circles) used to print the structure (i). j – l Printing of a nose with a cone singularity: target shape (j), experimentally printed structure (k), and Ω used to print the structure (l). Scale bars, 2 mm in a; 2 mm in h; 5 mm in i; 2 mm in k; 5 mm in l.

---

### The basic reproduction number of SARS-CoV-2 in wuhan is about to die out, how about the rest of the world? [^cc93c9ef]. Reviews in Medical Virology (2020). Medium credibility.

The virologically confirmed cases of a new coronavirus disease (COVID-19) in the world are rapidly increasing, leading epidemiologists and mathematicians to construct transmission models that aim to predict the future course of the current pandemic. The transmissibility of a virus is measured by the basic reproduction number (R 0), which measures the average number of new cases generated per typical infectious case. This review highlights the articles reporting rigorous estimates and determinants of COVID-19 R 0 for the most affected areas. Moreover, the mean of all estimated R 0 with median and interquartile range is calculated. According to these articles, the basic reproduction number of the virus epicentre Wuhan has now declined below the important threshold value of 1.0 since the disease emerged. Ongoing modelling will inform the transmission rates seen in the new epicentres outside of China, including Italy, Iran and South Korea.

---

### Monolithic silicon carbide metasurfaces for engineering arbitrary 3D perfect vector vortex beams [^e91f7ed7]. Nature Communications (2025). High credibility.

To decode the message, RCP and x -polarized light are incident onto the metasurface ciphertext, and two 3D optical fields, corresponding to two states of the 3D PVVB array (State 1 and State 2), are captured after filtering through an LCP analyzer and a y -direction linear polarizer, as reconstructed in Fig. 4i, j. Projections of these states onto the x-y plane are also shown in supplementary Fig. S6. From State 1, by analyzing the height along the z -direction, the maximum outer ring size, and the annular shape on the x-y plane, the values of v, and p can be identified. For State 2, by counting the number of lobes, N, in the petal-like intensity pattern, the value ofcan be determined as N /2. For instance, a 3D PVVB in the first row and ninth column of State 1 has a height of ~18 μm in the z -direction, a maximum outer diameter of ~42 μm, and a rounded triangular ring shape, with the corresponding values of v, and p recognized as 1, 21 μm, and 3, respectively. The intensity pattern of the 3D PVVB at the same position in State 2 has eight petals, indicating = 4.0. In this way, the four parameters of the entire 3D PVVB array are determined and displayed in Fig. 4k, where v, p, andof each PVVB are labeled with blue, green, yellow, and red backgrounds, respectively. The hexadecimal serial number of the 3D PVVB in the first row and ninth column of the array is recognized as "57" by querying the code chart, and the decoded number of the entire 3D PVVB array is shown in supplementary Fig. S7. By sequentially converting this hexadecimal matrix into a binary matrix, reshaping it into a 24 ⨯ 3 matrix, and splitting each 8-digit binary numeral into single-digit elements, a 24 ⨯ 24 binary matrix is decrypted. Finally, this matrix can be transformed into the original 2D QR code (Fig. 4l).

---

### An achromatic X-ray lens [^d1c0c676]. Nature Communications (2022). High credibility.

The concept of an achromatic doublet can be transferred to the X-ray regime. For X-rays, the refraction and absorption in matter are described by the complex refractive index n, which is related to the atomic scattering factor f a = f 1 + i f 2 of the atoms in the given material:where (1 − δ) and β are the real and imaginary parts of n, λ is the X-ray wavelength, r e the classical radius of the electron, and n a the number of atoms per volume. For the majority of the X-ray regime, the real part f 1 of the atomic scattering factor changes only little with λ, meaning that the material dispersion D of f 1 is close to zero, D = (Δ f 1 / f 1)/(Δ λ / λ) ≈ 0, and δ can be approximated as δ ∝ λ 2 for all materials (see Eq. (1)). Only near the absorption edges, D reaches large positive or even negative values, leading to anomalous dispersion. Combining two refractive X-ray lenses from different materials therefore cannot provide achromatic behaviour over an extended range of X-ray wavelengths.

---

### Effects of changing population or density on urban carbon dioxide emissions [^49635916]. Nature Communications (2019). High credibility.

To account for the multicollinearity problem, we have fitted Eqs. (3) and (5) by using the ridge regression approach. This method solves the matrix inversion problem by adding a constant λ to the diagonal elements of X T X, so that the ridge estimator for the linear coefficients is a = (X T X + λ I) −1 X T y, where I is the identity matrix. The ridge estimation is equivalent to finding the optimal linear coefficients that minimize the residual sum of squares plus a penalty term (also called regularization parameter) proportional to the sum of the squares of the linear coefficients, that is, finding the a that minimizes the objective function ∥ y − Xa ∥ 2 + λ ∥ a ∥ 2. The optimal value of λ is usually unknown in practice and needs to be estimated from data. To do so, we have used the approach of searching for the value of λ that minimizes the mean squared error (MSE) in a leave-one-out cross validation strategy. In this approach, we estimate a (for a given λ) using all data except for one point that is used for calculating the squared error. This process is repeated until every data point is used exactly once for estimating the squared error, and then we calculate the value of the MSE for a given λ. The optimal value of λ = λ * is the one that minimizes the average value of the MSE estimated with the leave-one-out cross validation method. We have also standardized all predictors before searching for the optimal value λ *. This is a common practice when dealing with regularization methods and ensures that the penalty term is uniformly applied to the predictors, that is, the normalization makes the scale of the predictors comparable and prevents variables with distinct ranges from having uneven penalization.

---

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^5c74e315]. American Journal of Kidney Diseases (2006). Medium credibility.

Hemodialysis adequacy — minimum single‑pool Kt/V (spKt/V) targets per treatment to achieve a weekly standard Kt/V (stdKt/V) of approximately 2.0 are stratified by residual urea clearance (K_r): for 2x/wk dialysis with K_r < 2 mL/min/1.73 m2 it is Not recommended, whereas with K_r ≥ 2 mL/min/1.73 m2 it is 2.0*; for 3x/wk the minima are 1.2 (K_r < 2) and 0.9 (K_r ≥ 2); for 4x/wk 0.8 and 0.6; and for 6x/wk (short daily) 0.5 and 0.4. Table 13 states these sessional targets correspond to "a weekly stdKt/V value of 2.0" in patients "undergoing 2 to 6 treatments per week" with adjustment for "a weekly K_r of 2 mL/min"; a "urea clearance of 2 mL/min is approximately 20 L/wk" and with "V = 30 L, it represents about a 0.67 weekly Kt/V unit". It is noted that "the minimum values for spKt/V" do not account for outcome improvements when frequency "is increase to more than 3/wk". The Work Group "recommended targeting an spKt/V value that is about 15% higher than the recommended minimum targets", developed a scheme that "limited the downward adjustment in spKt/V for K_r to 2 mL/min", and stated that "Maintaining a minimum 'total Kt/V' value of 1.2… would allow reduction of the dialysis dose down to near zero".

---

### A continuous-time maxSAT solver with high analog performance [^3d122302]. Nature Communications (2018). Medium credibility.

Many real-life optimization problems can be formulated in Boolean logic as MaxSAT, a class of problems where the task is finding Boolean assignments to variables satisfying the maximum number of logical constraints. Since MaxSAT is NP-hard, no algorithm is known to efficiently solve these problems. Here we present a continuous-time analog solver for MaxSAT and show that the scaling of the escape rate, an invariant of the solver's dynamics, can predict the maximum number of satisfiable constraints, often well before finding the optimal assignment. Simulating the solver, we illustrate its performance on MaxSAT competition problems, then apply it to two-color Ramsey number R(m, m) problems. Although it finds colorings without monochromatic 5-cliques of complete graphs on N ≤ 42 vertices, the best coloring for N = 43 has two monochromatic 5-cliques, supporting the conjecture that R(5, 5) = 43. This approach shows the potential of continuous-time analog dynamical systems as algorithms for discrete optimization.

---

### Universality in long-distance geometry and quantum complexity [^e343ca2d]. Nature (2023). Excellent credibility.

This implies that we can approximate the operatorwith a total cost of about. This operator agrees with our target operatorat leading order in z, and has an inner-product error of about z 2. This can be improved to z 3 by using the next order in the Suzuki–Trotter expansion, but going to even higher orders becomes prohibitively expensive. It is at this point that we make our heuristic step. In the Euclidean group example, we saw that the complexity geometry has so many degrees of freedom that by making minor deformations of the path we can correct small errors at small extra cost, in a way that is not captured by any finite order of the Suzuki–Trotter expansion, and is instead an emergent feature in the IR. Compared with the SU(2) example in the section ' Berger sphere ', the task of compiling in U(2 N) is complicated by the fact that there are many more directions in which to err; on the other hand, there are correspondingly more directions in which we can wiggle the path to eliminate the error, and as a statistical matter, we expect that to dominate. If the small inner-product errors can be corrected by wiggling the path, then we can synthesizefor z < 1 at cost k n 2 (k). To generateat larger values of z, the triangle inequality (for any) guarantees that the complexity grows no faster than linearly with coefficient k n 2 (k). This argument heuristically shows that the binomial metric is in the same universality class as the infinite-cliff metric, and therefore upper-bounds the critical schedule:The upper-bound equation (17) holds at all but the largest k, where the analysis becomes unreliable. Note also that although the binomial metric does not have a curvature as small as the exponential metric, it is still very moderate ∣ κ ∣ ≤ O (N) compared to the cliff metric. The reasoning that leads to equation (17) is heuristic, because to eliminate error it appeals to a statistical argument. In ref. it is shown that there is a weaker result that can be proved. The study also shows that any unitary that can be reached with a path that in the binomial metric has a lengthcan be approximated to within inner-product error ϵ by a path that in the infinite-cliff metric has a lengthOur conjectures imply that this can be improved from polynomial to linear-with-additive-constant and from approximate to exact.

---

### Presence of anti-GAD in a non-diabetic population of adults; time dynamics and clinical influence: results from the HUNT study [^4c6c5b5e]. BMJ Open Diabetes Research & Care (2015). Medium credibility.

The HLA DQA1-DQB1 haplotypes were divided into the following four groups based on known type 1 diabetes risk: (1) very high risk, having the genotype DQA1✱0301 - DQB1✱0302/DQA1✱0501 - DQB1✱0201; (2) high risk, having the genotypes DQA1✱0301 - DQB1✱0302/Z, DQA1✱0501 - DQB1✱0201/Z or DQA1*X-DQB1✱0302/DQA1*X-DQB1*X; (3) neutral, having DQA1*X-DQB1*X/DQA1*X-DQB1*X; (4) strongly protective, having either the genotype DQA1*X-DQB1✱0602/DQA1*X-DQB1✱0603 or one of these haplotypes in combination with one of the haplotypes DQA1*X-DQB1✱0302 or DQA1*X-DQB1*X (see table 3 for more details). For groups 2–4, X indicates a non-defined allele and Z indicates any haplotype defined and non-defined except DQA1✱0301 - DQB1✱0302 and DQA1✱0501 - DQB1✱0201 (potential homozygosity was not excluded).

Statistical analysis

Data are given as numbers and percentage for categorical data and as median (minimum–maximum value) for continuous data. All statistical analyses were performed by the PASW Statistics (V.20, SPSS, Inc, Chicago, Illinois, USA). χ 2 Test or Fisher's exact test (when appropriate) was used to compare differences in categorical data. The Mann-Whitney U test was used to test differences in continuous data between two groups. The Kruskal-Wallis test was used to test differences in continuous data between more than two groups. Logistic regression models were used to examine whether HLA haplotypes were associated with anti-GAD after adjusting for other confounding factors such as age, gender, body mass index (BMI), and first-degree family history of diabetes. A two-tailed p value of 0.05 was considered to be significant.

Ethics

All participants gave their written consent. The study was approved by the Regional Committee for Ethics in Medical Research.

---

### The dark and gloomy brain: Grey matter volume alterations in major depressive disorder-fine-grained meta-analyses [^2bc2cb76]. Depression and Anxiety (2024). Medium credibility.

3.2. Grey Matter Volume Changes in Pure MDD Patients versus Healthy Controls

The first meta-analysis was carried out on 73 contrasts (derived from 67 papers) that included 407 foci and compared 5509 pure MDD patients showing GMV atrophy versus 6618 HC (here, healthy individuals were counted twice in case of contrasts between two different subgroups of patients and the same HC group, as each contrast is taken into account separately). The minimum size for a cluster to be considered statistically significant was 2000 mm 3. The minimum cluster size considered significant in the analyses is automatically calculated by the software GingerALE using the thresholding algorithm of cluster-level inference, which simulates random datasets using the characteristics of the original dataset: number of foci, number of foci groups, and subject sizes. By setting a threshold of p < 0.01, GingerALE finds above that threshold the contiguous volumes, "clusters", and tracks the distribution of their volume. The cluster-level inference-corrected threshold sets the cluster minimum volume such that only, for example, 5% of the simulated data's clusters exceed this size. In other words, cluster-level inference uses a cluster forming threshold (p < 0.01) and the distribution of cluster sizes above the threshold to choose a minimum cluster size. Our results revealed a region of convergence of 3064 mm 3 centered in the left insula (MNI coordinates: X = −47.3, Y = 9.1, Z = −1.7), with four peaks, three of which were located in the left insula (corresponding to Brodmann area (BA) 13) and one in the left superior temporal gyrus (BA 22). The maximum ALE value (0.0341, p < 0.001; z = 5.54) was found within the left insula (MNI coordinates: X = −46, Y = 12, Z = −8). Table 2 provides a summary of all significant results. Figure 2 shows that the significant cluster was lateralized in the left hemisphere and that it included the insula and the superior temporal gyrus.

---

### Quantitative assessment of full-width at half-maximum and detector energy threshold in X-ray imaging systems [^c65d8c68]. European Journal of Radiology (2024). Medium credibility.

Background

The response function of imaging systems is regularly considered to improve the qualified maps in various fields. More the accuracy of this function, the higher the quality of the images.

Methods

In this study, a distinct analytical relationship between full-width at half-maximum (FWHM) value and detector energy thresholds at distinct tube peak voltage of 100 kV has been addressed in X-ray imaging. The outcomes indicate that the behavior of the function is exponential. The relevant cut-off frequency and summation of point spread function S(PSF) were assessed at large and detailed energy ranges.

Results

A compromise must be made between cut-off frequency and FWHM to determine the optimal model. By detailed energy range, the minimum and maximum of S(PSF) values were revealed at 20 keV and 48 keV, respectively, by 2979 and 3073. Although the maximum value of FWHM occurred at the energy of 48 keV by 224 mm, its minimum value was revealed at 62 keV by 217 mm. Generally, FWHM value converged to 220 mm and S(PSF) to 3026 with small fluctuations. Consequently, there is no need to increase the voltage of the X-ray tube after the energy threshold of 20 keV.

Conclusion

The proposed FWHM function may be used in designing the setup of the imaging parameters in order to reduce the absorbed dose and obtain the final accurate maps using the related mathematical suggestions.

---

### Solving the where problem and quantifying geometric variation in neuroanatomy using generative diffeomorphic mapping [^fd44f32f]. Nature Communications (2025). High credibility.

The statistical interpretation allows us to accommodate images with non reference signals, such as missing tissue, tracer injection sites, or other anomalies. At each pixel, the identity of the signal type is modeled as missing data, and maximum likelihood estimators are computed using an Expectation Maximization algorithm, which alternates between the E step: compute posterior probability π i (x) that each pixel corresponds to the reference image rather than one of the non-reference types, and the M step: update parameters by solving a posterior weighted version of the above:As an EM algorithm, this approach is guaranteed to be monotonically increasing in likelihood. An example of posterior weights are shown in the right hand column of Fig. 2 b.

Our approach uses mixtures of Gaussians to model variability in data, to allow large outliers to be accommodated by additional components, even though the Gaussian distribution itself does not have long tails. The Gaussian model allows for closed form expression (in terms of matrix inverse) for contrast transformation parameters. Other groups have used long tailed distributions to model variability and outliers in a robust manner, most notably the exponential distribution for l1 optimization. Techniques such as iteratively reweighted least squares can be applied as in Reuter et al. which lead lead to a weighted least squares problem which is similar to ours.

Nonconvex optimization with low to high dimensional subgroups and resolutions

This registration problem is highly nonconvex, and allows for many local minima. To provide robustness in our solution, we solve a sequence of lower dimensional subproblems, initializing the next with the solution to the previous. (i) 2D slice to slice rigid alignment maximizing similarity to neighbors(ii) 3D affine only alignment, registration using the full model at (iii) low (200 μm), (iv) medium (100 μm), and (v) high (50 μm) resolution. Time varying velocity fields are discretized into 5 timesteps and integrated using the Semi Lagrangian method. For most subproblems, spatial transformation parameters are estimated by gradient descent, and intensity transformation parameters are updated by solving a weighted least squares solution at each iteration. For subproblems that include linear registration only, parameters are estimated using Reimannian gradient descent (discussed in ref.and similar to a second order Gauss–Newton optimization scheme).

---

### An analytical theory of balanced cellular growth [^b0d02f80]. Nature Communications (2020). High credibility.

The maximal balanced growth rate μ * will be a function of ρ. In analogy to the marginal net benefits of cellular components, we define the marginal benefit of the cellular density as the relative fitness increase facilitated by a small increase in ρ,

Using the method of Lagrange multipliers with the growth equation (Eq. (6)) as the objective function, we derive necessary conditions at optimal growth, which we term balance equations:(Theorem 10). Again, the presentation here assumes that there are no dependent reactants, while a corresponding result is derived for the general case with dependent reactants in "Methods" ("Optimal density-constrained balanced growth states"). Both with and without dependent reactants, the optimal state is perfectly balanced: the marginal net benefits of all independent cellular concentrations x i are identical. Thus, if the dry weight density ρ could increase by a small amount (such as 1 mg l −1), then the marginal fitness gain that could be achieved by increasing protein concentration by this amount is identical to that achieved by instead increasing the concentration of any reactant α by the same amount. This should not be surprising: if the marginal net benefit of concentration x i was higher than that of, growth could be accelerated by increasing x i at the expense of.

Equation (10) together with Eq. (9) describes a system of n + 1 equations for n + 1 unknowns, the independent concentrations x i. In realistic cellular systems, this set of equations has a finite number of discrete solutions. Thus, growth rate optimization can be replaced by searching for the solution of the balance equations. If the optimization problem is convex, the conditions given by Eq. (10) are necessary and sufficient, and the solution is unique.

---

### Number needed to treat and number needed to harm are not the best way to report and assess the results of randomised clinical trials [^f243221a]. British Journal of Haematology (2009). Low credibility.

The inverse of the difference between rates, called the 'number needed to treat' (NNT), was suggested 20 years ago as a good way to present the results of comparisons of success or failure under different therapies. Such comparisons usually arise in randomised controlled trials and meta-analysis. This article reviews the claims made about this statistic, and the problems associated with it. Methods that have been proposed for confidence intervals are evaluated, and shown to be erroneous. We suggest that giving the baseline risk, and the difference in success or event rates, the 'absolute risk reduction', is preferable to the number needed to treat, for both theoretical and practical reasons.

---

### Oral anticoagulants in the elderly [^bbd69599]. British Journal of Haematology (2003). Low credibility.

Over a 1-year period, 34,998 prothrombin time tests were performed on 2,379 patients aged 40–89 years with a recommended target International Normalized Ratio (INR) value of 2.5. At least one INR value of ≥ 5 was found in 507/2,379 patients (21.3%). The elderly demonstrated higher maximum and lower minimum INR values, lower warfarin doses, and an increased number of tests performed. After adjustment for these factors, the risk of INR values ≥ 5 increased with age by 15% every 10 years (95% CI 4–28%). We conclude that age is a risk factor for more unstable prothrombin time results.

---

### The bias / variance trade-off when estimating the MR signal magnitude from the complex average of repeated measurements [^e67e7127]. Magnetic Resonance in Medicine (2011). Low credibility.

The signal-dependent bias of MR images has been considered a hindrance to visual interpretation almost since the beginning of clinical MRI. Over time, a variety of procedures have been suggested to produce less-biased images from the complex average of repeated measurements. In this work, we re-evaluate these approaches using first a survey of previous estimators in the MRI literature, then a survey of the methods statisticians employ for our specific problem. Our conclusions are substantially different from much of the previous work: first, removing bias completely is impossible if we demand the estimator have bounded variance; second, reducing bias may not be beneficial to image quality.

---

### Stochastic representation of many-body quantum States [^5e3c8f08]. Nature Communications (2023). High credibility.

Regression

As formulated here, regression is a fundamental supervised learning task at which NNs excel. Given an ansatzand a set of samplesrepresenting our prior knowledge of the true wavefunction at some points in space, we want to find the best value of ϑ. Perhaps the simplest approach is to minimize the sum of squared residuals:To express the ansatz itself, we use a NN. Our architecture is very minimal and has not been tuned for efficiency: the NNs we used consist of a sequence of dense layers withactivation functions, and finally a linear output layer. This is in some cases be followed by an optional layer enforcing analytically known boundary and/or cusp conditions by multiplying the output of the NN with a hand-chosen parameterized function of the coordinates, such as the asymptotic solution at large distances from the origin. Technical details are provided in the Methods section.

The top panel of Fig. 1 shows what this looks like for a single particle in 1D, where the process is easy to visualize. For an arbitrary initial state (top left panel) and the ground state (top right panel) of a 1D harmonic oscillator, a series of samples and the resulting NN-based regression curves are shown. In the insets of the panel below, an analogous visualization is shown for 2D cuts across 4D wavefunctions of two interacting fermions in a 2D harmonic potential.

Fig. 1
Propagation towards the ground state in a harmonic potential.

a Different steps in the propagation towards the ground state of a particle of mass m = 1 in a 1D harmonic oscillator with frequency ω = 1, with ℏ = 1. The green line is the function fitted by the neural network to a finite set of samples (black dots on the x axis) and their corresponding values (connected by a black line). Starting with an asymmetric guess (τ = 0), the function converges towards the correct solution (dotted orange line) at the center of the trap and acquires the right symmetry (τ = 3). b Extension of the upper system to two fermions in two spatial dimensions. The energy is estimated by Monte Carlo sampling with error bars showing the standard error, and converges to the ground state value of E 0 = 3.010 ± 0.007, which results in a relative error of 0.35% with respect to the exact value of. The inset shows a cut through the wavefunction. Source data are provided as a Source Data file.

---

### Optimal experimental design and estimation for q-space trajectory imaging [^efd29e1f]. Human Brain Mapping (2023). Medium credibility.

2.4 Estimators

Here we describe the various estimators that were compared in terms of accuracy and precision. In practice, the DW measurements will be Rician‐distributed, and Basser et al. showed that the log‐transformed signal intensities can be modeled as:whereis the column vector of independent error terms. The ordinary linear least squares (LLS) estimator ofis given by:It is unbiased under the condition thathas expectation zero. If the variance of the error terms can be assumed constant across all measurements (an assumption known as homoscedasticity), the LLS estimator is the best linear unbiased estimator of(van den Bos). However, it can be shown that, whereis the underlying noise‐free signal vector (Basser et al.). This means that the homoscedasticity assumption does not hold for the log‐transformed data and LLS will have a suboptimal precision. To account for this, a weighted linear least squares (WLLS) estimator was proposed by Basser et al. :witha diagonal matrix with the reciprocal squares of the elements of the signal vectoron its diagonal:

Alternatively, the iteratively weighted linear least squares (IWLLS) estimator proposed by Salvador et al. consists of updating the weights of theth iteration with the signal predictions of the previous iteration, up to some maximum:In this work, we set, as the weight matrixdid not differ substantially from the previous iteration.

Finally, we define the unweighted nonlinear least squares (NLS) estimator as:whereis the squared two‐norm.

Because, in this case, the data are no longer log‐transformed, the variance can be assumed to be constant across all measurements, and weights are not required. In this work, the NLS estimator was initialized with LLS.

---

### Genetic variance components estimation for binary traits using multiple related individuals [^f336454d]. Genetic Epidemiology (2011). Low credibility.

Understanding and modeling genetic or nongenetic factors that influence susceptibility to complex traits has been the focus of many genetic studies. Large pedigrees with known complex structure may be advantageous in epidemiological studies since they can significantly increase the number of factors whose influence on the trait can be estimated. We propose a likelihood approach, developed in the context of generalized linear mixed models, for modeling dichotomous traits based on data from hundreds of individuals all of whom are potentially correlated through either a known pedigree or an estimated covariance matrix. Our approach is based on a hierarchical model where we first assess the probability of each individual having the trait and then formulate a likelihood assuming conditional independence of individuals. The advantage of our formulation is that it easily incorporates information from pertinent covariates as fixed effects and at the same time takes into account the correlation between individuals that share genetic background or other random effects. The high dimensionality of the integration involved in the likelihood prohibits exact computations. Instead, an automated Monte Carlo expectation maximization algorithm is employed for obtaining the maximum likelihood estimates of the model parameters. Through a simulation study we demonstrate that our method can provide reliable estimates of the model parameters when the sample size is close to 500. Implementation of our method to data from a pedigree of 491 Hutterites evaluated for Type 2 diabetes (T2D) reveal evidence of a strong genetic component to T2D risk, particularly for younger and leaner cases.

---

### X-ray quasi-periodic eruptions from two previously quiescent galaxies [^88a16648]. Nature (2021). Excellent credibility.

With the observation of the two QPEs we report here we can now argue against at least this type of accretion disk instability as the origin of QPEs. Specifically, the strong asymmetric nature of the eruptions in eRO-QPE1, which show a faster rise and a much slower decay (Fig. 3a), argues against this interpretation. Qualitatively, our data would suggest that QPEs are not related to τ th, because α is not expected to change between the hot and cold phases in AGN. Moreover, if instead it is the front propagation timescales, following τ front ≈ (H / R) −1 τ th (where H is the vertical scale height of the disk), or viscous timescales, following τ visc ≈ (H / R) −2 τ th, that regulates the rise (decay) in the cold (hot) phase, it would imply a thicker flow in the cold and stable phase than in the hot and unstable phase. This runs contrary to the theoretical expectation that unstable flows should be thicker. The limit-cycle oscillation theory further predicts that once the period, duty cycle and luminosity amplitude are known and a viscosity prescription for the accretion flow is adopted, there are only specific values of M BH and α that unstable sources are allowed to follow. Here we adopt for eRO-QPE1 (eRO-QPE2) a peak-to-peak period T pp = 18.5 h (2.4 h), an amplitude A ≈ 294 (11) and a duty cycle D = 41% (19%). The amplitude A is the ratio of the disk luminosity (computed within the 0.001–100-keV range) for peak and quiescence, taken as proxy of the maximum and minimum bolometric luminosity, and D is here defined as the ratio of the flare duration (rise-to-decay T rd) and the period T pp. We begin by adopting a standard viscosity prescription, with the average stress between the annuli proportional to the gas plus radiation pressure P tot. The allowed M BH and α values for eRO-QPE1 (eRO-QPE2) are M BH ≈ 4 × 10 6 M ☉ and α ≈ 5 (M BH ≈ 3 × 10 6 M ☉ and α ≈ 3), therefore an unphysically high viscosity parameter would be required. Considering alternative viscosity prescriptions, for eRO-QPE1 (eRO-QPE2) a more reasonable α ≈ 0.1 or 0.01 would correspond to allowed M BH ≈ 2.4 × 10 3 M ☉ or M BH ≈ 60 M ☉ (M BH ≈ 4.3 × 10 3 M ☉ or M BH ≈ 30 M ☉), respectively. The above combinations are either unphysical or very unlikely. Adopting α ≈ 0.2 and alternative viscosity prescriptions, eRO-QPE1 (eRO-QPE2) would yield an intermediate-mass black hole (IMBH) at M BH ≈ 0.9 × 10 4 M ☉ (M BH ≈ 1.6 × 10 4 M ☉) accreting at ~0.1 (~0.3) of the Eddington limit in quiescence and at ~30 (~3) times the Eddington limit at the peak. However, this IMBH scenario would not account for the opposite asymmetry shown by eRO-QPE1 compared to the theoretical predictions, nor would the resulting thermal timescales be self-consistent for either of the two: for eRO-QPE1 (eRO-QPE2) τ th ≈ 20 s (35 s) at 20 r g adopting M BH ≈ 0.9 × 10 4 M ☉ (M BH ≈ 1.6 × 10 4 M ☉), which is orders of magnitude smaller than the observed QPE durations, and the rise-to-peak times would be only reconciled with τ th at ~780 r g (~250 r g). Analogous results can be obtained using the observed properties of RX J1301.9+2747, adopting T pp ≈ 20 ks (or the second period T pp ≈ 13 ks), D = 6% (9%) and A ≈ 9.4, the latter obtained taking the ratio of the quoted quiescence and peak 0.3–2.0-keV flux as proxy for a bolometric luminosity ratio: adopting α ≈ 0.15 the allowed black hole mass is ~2.2 × 10 4 M ☉ (~1.5 × 10 4 M ☉), much lower than the quoted, ~(0.8–2.8) × 10 6 M ☉.

---

### Validation of monte carlo estimates of three-class ideal observer operating points for normal data [^26b6e5d9]. Academic Radiology (2013). Low credibility.

Rationale and Objectives

Traditional two-class receiver operating characteristic (ROC) analysis is inadequate for the complete evaluation of observer performance in tasks with more than two classes.

Materials and Methods

Here, a Monte Carlo estimation method for operating point coordinates on a three-class ROC surface is developed and compared with analytically calculated coordinates in two special cases: (1) univariate and (2) restricted bivariate trinormal underlying data.

Results

In both cases, the statistical estimates were found to be good in the sense that the analytical values lay within the 95% confidence interval of the estimated values about 95% of the time.

Conclusions

The statistical estimation method should be key in the development of a pragmatic performance metric for evaluation of observers in classification tasks with three or more classes.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^5292d530]. Annals of the American Thoracic Society (2023). High credibility.

Reference space definition for quantitative imaging states that "It is crucial to define and measure a biologically meaningful reference space… as the starting point for analysis and endpoint for data reporting". Ratio-only expressions are vulnerable to the "reference trap", where changes can stem from numerator, denominator, or both. "An efficient way of measuring volume, point counting, is used to quantify acinar components by micro-CT and extrapulmonary organ volume by CT", a forerunner of voxel-by-voxel volume summation by computed tomography (CT).

---

### Feasibility and coexistence of large ecological communities [^60560010]. Nature Communications (2017). Medium credibility.

We considered different values of μ −, μ +, c, and ρ. Their values cannot be chosen arbitrarily, since A must be negative definite. For a choice of c, ρ, and a ratio μ − / μ +, the largest eigenvalue of (A + A T)/2 is linear in μ +(as an arbitrary μ + can be obtained by multiplying A by μ + and then shifting the diagonal). Given the values of μ − / μ +, c and ρ, one can therefore determine μ max, the maximum value of μ + still leading to a negative definite A (that is, the value of μ + such that the largest eigenvalue of (A + A T)/2 is equal to 0). Figure 2 was obtained by considering more than 1,000 parameterizations. Both the ratio μ − / μ + and the coefficient of variation c could assume the values 0.5 or 2, while the correlation ρ assumed values from the set {−0.9, 0.5, 0, 0.5, 0.9}. The value of μ + was set equal to 0.25 μ max and 0.75 μ max. In addition, we considered the case μ − = 0.

Parameterization of food webs

In the case of food webs the adjacency matrix L is not symmetric, L ij = 1 indicating that species j consumes species i. We removed all cannibalsistic loops. Since L ij and L ji are never simultaneously equal to one (there are no loops of length two), we parameterized the off-diagonal entries of A as

while the diagonal was fixed at −1. Both W + and W − are random matrices, where the pairs are drawn from a bivariate normal distribution with marginal means (μ +, μ −) and correlation matrix

We considered considering different values of μ −, μ +, c and ρ. As explained above, given the values of μ − / μ +, c and ρ, one can determine μ max, the maximum value of μ + still corresponding to a negative definite A. Figure 2 was obtained by considering more that 350 parameterizations. Both the ratio μ − / μ + and the coefficient of variation c could assume the values 0.5 or 2, while the correlation ρ assumed either the value −0.5 or 0.5. The value of μ + was set either to 0.25 μ max or 0.75 μ max.

---

### Improved TSE imaging at ultrahigh field using nonlocalized efficiency RF shimming and acquisition modes optimized for refocused echoes (AMORE) [^ce1242c1]. Magnetic Resonance in Medicine (2022). Medium credibility.

2.2 Nonlocalized efficiencyshimming

For nonlocalized efficiency shimming, the goal is to improve B 1 + within the entire imaging FOV rather than a local ROI. Although the Rayleigh quotient can still be used, the resulting shim usually produces more pronounced B 1 + inhomogeneities and undesirable B 1 + nulls, especially with increasing target size. Such issue arises from the fact that voxels closer to coil elements typically have a higher B 1 + and are thus weighted more in the Rayleigh quotient, unless additional constraints such as a specified minimum B 1 + are imposed. In other words, the Rayleigh quotient as a cost function maximizes the total B 1 + within a given region, which is not necessarily equivalent to minimizing B 1 + destructive interferences. To produce a shim that is "efficient" in the sense that destructive B 1 + interferences are minimized within the entire imaging FOV, we propose a more appropriate cost function for nonlocalized efficiency shimming that penalizes underflipping alone without explicitly constraining overflipping:where the voxel‐wise cost function is evaluated by a piecewise linear function:Note thatare the row vectors in; is the magnitude of voxel‐wise B 1 + under the given shim; andis the threshold for minimum B 1 + magnitude to be used during optimization. Whenis large enough, the above cost function will approach the Rayleigh quotient and thus produce similar solutions.

---

### The linear-quadratic model is an appropriate methodology for determining isoeffective doses at large doses per fraction [^9164b9e6]. Seminars in Radiation Oncology (2008). Low credibility.

The tool most commonly used for quantitative predictions of dose/fractionation dependencies in radiotherapy is the mechanistically based linear-quadratic (LQ) model. The LQ formalism is now almost universally used for calculating radiotherapeutic isoeffect doses for different fractionation/protraction schemes. In summary, the LQ model has the following useful properties for predicting isoeffect doses: (1) it is a mechanistic, biologically based model; (2) it has sufficiently few parameters to be practical; (3) most other mechanistic models of cell killing predict the same fractionation dependencies as does the LQ model; (4) it has well-documented predictive properties for fractionation/dose-rate effects in the laboratory; and (5) it is reasonably well validated, experimentally and theoretically, up to about 10 Gy/fraction and would be reasonable for use up to about 18 Gy per fraction. To date, there is no evidence of problems when the LQ model has been applied in the clinic.

---

### Elective female genital cosmetic surgery: ACOG committee opinion, number 795 [^2996ee1b]. Obstetrics and Gynecology (2020). High credibility.

Variability of female genitalia measurements — selected normative values from Table 2 (Mean [in mm], Standard Deviation, Minimum [in mm], Maximum [in mm]) include: Width of clitoris — Mean 4.62, Standard Deviation 2.538, Minimum 1, Maximum 22; Length of clitoris — Mean 6.89, Standard Deviation 4.965, Minimum 0.5, Maximum 34; Length of labia minora (right) — Mean 42.1, Standard Deviation 16.35, Minimum 6, Maximum 100; Width of labia minora (right) — Mean 13.4, Standard Deviation 7.875, Minimum 2, Maximum 61.

---

### A continuous-time maxSAT solver with high analog performance [^60aab956]. Nature Communications (2018). Medium credibility.

Fig. 3
Algorithm statistics over random Max 3-SAT problems. Distribution of differences between the real global minimum E min obtained with the exact algorithm MaxSatz and a the smallest energy found by the algorithm, b the predicted minimum value, and c the final decision of the algorithmshown for problems with different N and α values (see legends). d The percentage of instances indicated by color (see color bar) for different values of the errorand hardness η. Most instances are in therow indicating correct prediction. Large errors occur mainly at smaller η values, and are dominantly negative

Performance evaluation on hard MaxSAT competition problems

Next, we present the performance of our solver on MaxSAT competition problems, from 2016. We are foremost interested if Max-CTDS is capable of predicting the global minimum energy value and finding assignments corresponding to that energy value for hard problems, within a reasonable time.

For illustration purposes, here we first discuss an extremely hard competition problem instance with N = 250 variables and M = 1000 clauses, called "HG-3SAT-V250-C1000-1.cnf", which was reposted for several years. This problem was also used in Fig. 2. No complete competition algorithm could solve this problem. The best complete solver in 2016, the CCLS2akms has found a state with energy value 5, but could not prove that it is optimal within the allotted time (30 min). We ran our algorithm on a regular 2012 iMac 21.5, 3.1 GHz, Intel Core i7 computer and it predicted the lowest energy of 5 (unsatisfied clauses), after 21 min 24 s of running time and produced an assignment for it after 9.168 h of running time. The minimum energy prediction was achieved already after Γ = 7000 trajectories, whereas finding an assignment with this minimum energy took a total of Γ = 189,562 trajectories to run. The minimum energy assignment corresponding tois provided in Supplementary Data 2. (The problem can be downloaded from the competition site). We ran the complete and exact algorithm, MaxSatz, for over 5 weeks on this problem and the smallest energy it found was E = 9. The details of how the Max-CTDS algorithm performs are shown in Fig. 4. Similar figures for other hard problems such as for a 4-SAT problem and a spin-glass problem are shown in Supplementary Figs. 5 and 6.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^6517cbfd]. Annals of the American Thoracic Society (2023). High credibility.

Basic stereological measurements — first-order parameters and probes identify that "First-order parameters are volume (three-dimensional [3D]), surface (two-dimensional [2D]), length (one-dimensional [1D]), and number (zero-dimensional [0D])". Measurements "are performed using geometric test probes, such as points (0D), lines (1D), planes (2D), or volumes (3D)". These probes create countable events in the image, and "Raw counts provide ratios… that are multiplied by the reference space volume to obtain absolute measures for the lung or subcompartment".

---

### Factor analysis of ancient population genomic samples [^d0c730ae]. Nature Communications (2020). High credibility.

The FA model considers an n × p matrix of effect sizes, B, with coefficients defined as i.i.d. random variables with Gaussian prior distribution, N (0, α). The latent matrix, W, representing the data matrix after its correction for temporal drift, has an uninformative prior distribution. Statistical estimates for the matrices W and B are obtained by maximizing a posterior distribution in this Bayesian framework. This approach amounts to finding the minimum of the following loss functionwhere λ = σ 2 / α. The estimate of W represents the best approximation of rank K of the matrix Y, with respect to the following matrix normwith A = P D λ P T and D λ is a diagonal matrix with coefficientsFollowing, the optimal solution is equal to. The K corrected factors forming U and their associated loadings, V, can then be obtained from the SVD of the matrix W. The complexity of the algorithm is of the same order as the SVD algorithm, O (n p K), and allows fast implementations on standard computers. Because FA is defined as an unsupervised regression model, the limitations of FA in terms of sample size are similar to those of linear regression models, ANOVA models or PCA. For the choice of the number of factors, we used K = 2 factors in two-way admixture analyses and K = 3 factors in three-way admixture analyses. For the choice of the drift parameter, we chose the largest value of λ that removed the effect of time in the K th factor, where K was the number of putative ancestral populations (Supplementary Fig. 9).

---

### Learning perturbation-inducible cell States from observability analysis of transcriptome dynamics [^30e97209]. Nature Communications (2023). High credibility.

Sinceis a sum of quadratic forms, the result is that G has non-negative, real-valued eigenvalues. If the eigendecomposition is G = Q D Q −1, then the solution to the optimization problem Eq. (14) iswhere q 1 through q p are the top eigenvectors of the Gram matrix G. The proof of the solution to the optimization problem is provided in the Supplementary Information (Section 1.1). The single set of gene sampling weights that maximize the observability are precisely q 1 and from here on out we call these weights w.

Since transcriptomic data sets typically have few initial conditions, i.e. biological and technical replicates, before solving for w we enrich our data set with N synthetic initial conditions that are randomly sampled aswhere j in {1, 2. r } and r is the number of replicates. The motivation for the artificial data generation is given in ref. where it is shown that artificially generated data points improved the estimate of the DMD model when the data set is affected by noise. N is chosen to be equal to the number of genes to ensure the matrix of initial conditions has full rank. Another issue that we have addressed are the instabilities present in the DMD eigenvalues. Consequently, the observability gramian is not unique and the sum in Eq. (15) diverges to infinity. To mend this issue, we compute the finite-horizon Gram matrix, where the sum in Eq. (13) and Eq. (15) is from 0 to m. This allows for the computation of the finite-horizon signal energy from Eq. (13) where the bounds on the sum are now from i = 0 to i = m.

---

### Exact broadband excitation of two-level systems by mapping spins to springs [^554ab3c2]. Nature Communications (2017). Medium credibility.

Broadband excitation pulses

The dynamic connection between spin and spring has enabled the analytic design of π/2 and π pulses that manipulate the spin magnetization at a single frequency ω. We now apply this discovery to design a control u (t) that simultaneously steers an ensemble of springs between X 0 and X π/2 (or X π), which is called a broadband π/2 (or π) pulse, respectively. The minimum-energy broadband controls can be derived by solving the integral Eq. (5) in function space (since ω becomes a variable) and are composed of the prolate spheroidal wave functions (Supplementary Note 7). Figure 2 and Supplementary Fig. 7 show broadband π/2 and π pulses, respectively, which produce uniform excitation over the designed bandwidth. In practice these pulses can be constructed using the discrete prolate spheroidal sequences available in many scientific programming tools, such as "dpss" in Matlab (Supplementary Note 7).

Fig. 2
Broadband excitation of spin ensembles. The a broadband minimum-energy and b amplitude-limited controls steering a family of harmonic oscillators with frequencies −1 ≤ ω ≤ 1 from X 0 = (0,0) to X π /2 = (π /2,0) with the corresponding trajectories, d and e, respectively, of the harmonic oscillator (red) and nuclear spin (black) for ω = −1. These pulses achieve c high-fidelity magnetization excitation profiles, M x (T), over the frequencies −1 ≤ ω ≤ 1, with average excitation of 1000 (minimum energy in black) and 0.996 (amplitude limited in red), respectively

Practical considerations, e.g. limited power of RF coils, make it critical to design pulses with bounded amplitude. Steering an ensemble of springs with a bounded control is a challenging optimal control problem. However, we show that it can be reduced to a convex optimization problem, which can be effectively solved, and the optimal control has a bang–bang pulse shape (Supplementary Note 7). The bang–bang pulse in Fig. 2 is an example of a bounded amplitude broadband π/2 pulse. The performance (i.e. average excitation) can be adjusted by selecting different amplitude bounds and pulse durations.

---

### The linear-quadratic model and fractionated stereotactic radiotherapy [^9ea87d8c]. International Journal of Radiation Oncology, Biology, Physics (2003). Low credibility.

Purpose

To determine the dose per fraction that could be used when gamma knife or linear accelerator-based stereotactic treatments are delivered in 2 or more fractions.

Methods and Materials

The linear-quadratic (LQ) model was used to calculate the dose per fraction for a multiple-fraction regimen which is biologically equivalent to a given single-fraction treatment. The results are summarized in lookup tables.

Results and Conclusion

The tables can be used by practicing clinicians as a guide in planning fractionated treatment. For the large doses used in typical stereotactic treatments and for small fraction numbers, the model is not very sensitive to the value of the alpha/beta ratio in the LQ model. A simple rule of thumb is found that for two-fraction and three-fraction treatments the dose per fraction is roughly two-thirds and one-half of the single-fraction treatment dose, respectively.

---

### Efficient performance analysis and optimization of transient-state sequences for multiparametric magnetic resonance imaging [^24764705]. NMR in Biomedicine (2023). Medium credibility.

Referring to Equation (3), and as motivated in the Discussion section, we slightly modify the estimation ofby weighing the 10 lowest spatial frequencies with a higher factor (3.0) than all other spatial frequencies.

Using the Nelder–Mead algorithmfrom the Optim.jl package of the Julia programming language, we minimizedover the RF flip angles of. There were no explicit constraints on the flip angles, but to account for RF power constraints, we slightly modified the criterion by introducing a peak‐power‐correction factor, obtaining, whereis the highest flip angle value of the sequence (in rad) andhas been chosen to be 0.57. In addition, we restricted the optimization to 20 equidistant values for the flip angle, and derived the remaining 1100 values by cubic‐spline interpolation, similar to the approach presented by Mickevicius et al.and particularly by Scope Crafts et al. To reduce the sensitivity to local minima, we applied a multilevel approach: first optimizing on 10 equidistant points, using the result as a basis for 15 points and the result thereof for a 20‐point optimization. In each of these three steps, the number of Nelder–Mead iteration steps was limited to 2000.

The optimization process was followed by a selection process because the functionhas multiple local minima. By initiating the optimization with nine slightly different initial states, different local optimawere obtained, with. The superscript "2D" indicates that the optimization was performed in the context of phase encoding (see Figure 4C). The nine resulting shapes in Figure 4A seem mutually very different, indicating that not all of them can be the true optimum. Because we are dealing with a nonconvex minimization problem, the true optimum would require an exhaustive search, which would incur a prohibitive computational burden. But because the nine outcomes are mutually very close, this increases the confidence that all of them are close to the true optimum.

In a similar fashion, we also optimized on, assuming an absence of any encoding, resulting in sequences. Note that this 0D optimization approach is the most frequently adopted in the existing literature.

---

### Experimental noise cutoff boosts inferability of transcriptional networks in large-scale gene-deletion studies [^981a9ed7]. Nature Communications (2018). Medium credibility.

Case N 0 > 1

If more than one node are not perturbed we get from Eq. (11)Non-unique solutions of Eq. (15) can arise if a given fraction of the variance of the receiver node i can be explained by more than one sender node, for example, when a perturbed node j targets two nodes with index i and l. In this case it is unclear from the node activity data whether i is affected directly by j or indirectly through l, or by a combination of both routes. If node l is not perturbed or only weakly perturbed, a statistical criterion is needed to decide about inferability or identifyability of the link j → i, which can be computed numerically as follows: To find out whether j transmits a significant amount of information to i that is not passing through l, we first remove node j from the observable nodes of the network but keep its perturbative effect on other nodes in the data set. We then determine the link strengths A ′ for the remaining network of size N − 1. To construct a possible realisation of A ′ we set in Eq. (15) the non-zero values of Σ 0 to unity and use W = U to arrive at the expressionwith U ′ determined from the sample covariance matrix with the j -th column and j -th row removed. Fixing W and Σ 0 to seemingly arbitrary values does not affect the result we are after. If l is the only unperturbed node besides i, then in the A ′ system l can now be treated as perturbed — as it may receive perturbations from the unobserved node j — and thus Eq. (14) applies. If l is part of many unperturbed nodes that are affected by j, then the knowledge how much each of these nodes contributes to the variance of the target node i (which is determined by W and Σ 0) is irrelevant as we are only interested in the total effect of the alternative routes on node i. Using the inferred link strength from Eq. (16) we can rewrite Eq. (2) as a two-node residual inference problem between j and i, where we obtain a lower bound for link strength from node j to i by using the variation of i that could not be explained by A ′. This concept is similar to computing partial correlations. Defining by, andthe 2 × 2 analogues to the full problem we obtainwiththe covariance matrix of the vectorand, using the scaled variances. Note that A ii < 0 for all i as these elements represent sufficiently strong restoring forces that ensure negative definiteness of A and that we havefrom Eq. (1) in the stationary case. An estimate for the minimum relative link strength from node j to node i can be calculated from Eq. (13) and is given byEq. (18) can be considered as an asymptotically unbiased response coefficient between node 1 as target node and node 2 as source node, as again any dependency on σ 2 has dropped out. An estimate for the maximum relative link strength from node j to node i follows from Eq. (18) with the off-diagonal elements of A ′ set to zero. We classify a link as non-inferable if there exists (i) a significant difference between the minimum und maximum estimated link strength and (ii) a minimum link strength that is not significantly different from noise.

---

### Chiral structures of electric polarization vectors quantified by X-ray resonant scattering [^cbc06896]. Nature Communications (2022). High credibility.

Figure 2f, g shows the REXS intensity and XCD, respectively, calculated using the XAS spectrum. Comparing the calculation results with the experimental data shows that the XCD is maximum at the L 3 t 2g energy (457.8 eV) of Ti 4+ atoms, and the magnitude of the XCD at this energy is also quantitatively consistent with the experimental result. On the other hand, at higher energies, the calculated values of both XCD and scattering intensity do not explain the experimental data well. This is presumed to be because the XAS or XLD used in the calculation is different from the actual component of T 0. Nevertheless, since the XCD experiment of q z rod scan, which will be discussed below, was performed at the maximum XCD energy corresponding to the L 3 t 2g energy, it is expected that the experimental data can be sufficiently explained by the AT T 0 component of the basis atom obtained here near this energy.

---

### The basic reproduction number (R) of measles: a systematic review [^f3ec52f0]. The Lancet: Infectious Diseases (2017). Medium credibility.

The basic reproduction number, R nought (R 0), is defined as the average number of secondary cases of an infectious disease arising from a typical case in a totally susceptible population, and can be estimated in populations if pre-existing immunity can be accounted for in the calculation. R 0 determines the herd immunity threshold and therefore the immunisation coverage required to achieve elimination of an infectious disease. As R 0 increases, higher immunisation coverage is required to achieve herd immunity. In July, 2010, a panel of experts convened by WHO concluded that measles can and should be eradicated. Despite the existence of an effective vaccine, regions have had varying success in measles control, in part because measles is one of the most contagious infections. For measles, R 0 is often cited to be 12–18, which means that each person with measles would, on average, infect 12–18 other people in a totally susceptible population. We did a systematic review to find studies reporting rigorous estimates and determinants of measles R 0. Studies were included if they were a primary source of R 0, addressed pre-existing immunity, and accounted for pre-existing immunity in their calculation of R 0. A search of key databases was done in January, 2015, and repeated in November, 2016, and yielded 10883 unique citations. After screening for relevancy and quality, 18 studies met inclusion criteria, providing 58 R 0 estimates. We calculated median measles R 0 values stratified by key covariates. We found that R 0 estimates vary more than the often cited range of 12–18. Our results highlight the importance of countries calculating R 0 using locally derived data or, if this is not possible, using parameter estimates from similar settings. Additional data and agreed review methods are needed to strengthen the evidence base for measles elimination modelling.

---

### Fundamental limits to learning closed-form mathematical models from data [^a345300f]. Nature Communications (2023). High credibility.

Given a finite and noisy dataset generated with a closed-form mathematical model, when is it possible to learn the true generating model from the data alone? This is the question we investigate here. We show that this model-learning problem displays a transition from a low-noise phase in which the true model can be learned, to a phase in which the observation noise is too high for the true model to be learned by any method. Both in the low-noise phase and in the high-noise phase, probabilistic model selection leads to optimal generalization to unseen data. This is in contrast to standard machine learning approaches, including artificial neural networks, which in this particular problem are limited, in the low-noise phase, by their ability to interpolate. In the transition region between the learnable and unlearnable phases, generalization is hard for all approaches including probabilistic model selection.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^7e0b2d1d]. Annals of the American Thoracic Society (2023). High credibility.

Basic stereological measurements for lung imaging — first-order parameters, probes, and reference space are specified. First-order parameters are volume (three-dimensional [3D]), surface (two-dimensional [2D]), length (one-dimensional [1D]), and number (zero-dimensional [0D]). Measurements are performed using geometric test probes, such as points (0D), lines (1D), planes (2D), or volumes (3D), with probe–voxel interactions generating countable events; raw counts yield ratios that are multiplied by the reference space volume to obtain absolute lung or subcompartment measures. It is crucial to define and measure a biologically meaningful reference space for analysis and reporting, because measurements expressed only as ratios are subject to "the reference trap", where changes can arise from the numerator, denominator, or both; an efficient volume measurement method, point counting, is used for acinar components by micro-CT and for extrapulmonary organ volume by CT.

---

### A bacterial pan-genome makes gene essentiality strain-dependent and evolvable [^deb36d76]. Nature Microbiology (2022). High credibility.

Tn-seq libraries construction

Six independent transposon libraries, each containing 10,000 to 20,000 insertion mutants, were constructed with transposon Magellan6 in the different strains as described previously, with the following modifications: (1) gDNA used was isolated as described for PacBio sequencing, (2) Marc9 transposase expression was induced with 0.5 mM Isopropyl β-D-1-thiogalactopyranoside (IPTG) in 2YT medium with 2% ethanol at 25 °C during 4 h and (3) transformation reactions were scaled up to 4 mL volume. Transposon mutants were recovered in blood agar base no. 2 plates supplemented with 5% sheep's blood with 200 μg mL −1 spectinomycin. Libraries stock cultures were grown several independent times in THY medium, gDNA was isolated using Nucleospin tissue kit (Macherey-Nagel), and Tn-seq sample preparation and Illumina sequencing were done as previously described.

Tn-seq essential genes, fitness and genetic interactions determination

The binomial method of the TRANSIT package was used for determination of essential genes in the 22 strains with constructed libraries. Binomial categorization relies on the calculation of a z -value determined by library saturation, the number of teichoic acid (TA) sites in the gene, and a calculated false discovery rate (FDR) of 5% to set the 'non-essential' and 'essential' z -value thresholds. 'Uncertain' predicted genes have z -values between these thresholds. Ten percent of the sequence of a gene from the 5' and 3' ends was omitted for the calculation of the z -value. To compare between strains and determine the essential genes classes, only strains with a saturation higher than 35% were considered (17 strains). The maximum and minimum z -values observed for each gene across strains were calculated. All genes with a maximum z -value above the essential threshold were included as part of the essentialome. Core genes with a minimum z -value above the essential threshold were classified as universal essentials. Core genes with maximum z -values above the essential threshold (thus part of the essentialome) but minimum z -values below the essential threshold were classified as core strain-dependent essential genes. One caveat to consider about this last classification is that for some genes, minimum z -values are in the uncertain zone (between essential and non-essential thresholds). This could lead to some false positives, especially for those genes with minimum z -values close to the essential threshold.

---

### Recommendations for reducing radiation exposure in myocardial perfusion imaging [^7b583059]. Journal of Nuclear Cardiology (2010). Medium credibility.

Table 3 — myocardial perfusion imaging (MPI) radiation reduction — details feature-specific guidance with stated dose-reduction potential and exact recommendations as follows: Patient selection is "Significant" with the directive to "Apply appropriate use criteria. Consider alternative modalities with comparable diagnostic accuracy without radiation in younger patients. Consider utilization in the following patients in whom MPI has the most clinical utility: intermediate CAD risk, those requiring prognostic or management information, and those with persistent and unexplained symptoms. Layered or serial testing should be avoided". Protocols, radiotracers and imaging systems are "Significant", advising that "The clinical indications and physical stature of each patient should be reviewed and the best combination of radiotracers and protocols selected" and to "use radionuclides with shorter half-life such as Tc-99m and PET tracers, perform stress-only testing, and use weight-based dosing". Reconstruction-FBP is "Standard — No recommendation", whereas Reconstruction-iterative is "Potential for significant — Strongly recommend". Multi-detector systems are "Significant — Strongly recommend minimum of two detectors". New camera geometries are "Significant (same effect as multi-detector systems) — Use when available. Consider for new equipment purchase". Solid-state detector systems are "Minor unless part of a multi-detector or new geometry system — No recommendation". Collimators-custom are "Unproven, probably minor — Further exploration and research", Energy settings are "Probably minor — Further exploration and research", and both Step and shoot and Count consistency are "Minor — No recommendation".

---

### The effect of empathic communication-based training on women's positive birth perception, awareness, and birth experience: a randomized controlled trial [^d8eaa7dd]. BMC Pregnancy and Childbirth (2025). Medium credibility.

Data analysis

During the third and final stage, the data obtained were analyzed using the SPSS 22.0 software package. The analyses were reported using descriptive statistical measures (mean, standard deviation, minimum and maximum values, and percentage). Since the assumptions for parametric tests were met, the independent samples t-test was used to determine differences between the means of two independent groups; and the Chi-Square test was applied to assess differences between categorical variables (the Bonferroni-corrected Z test was applied to identify which groups contributed to significant differences). The Paired Sample t-test was used to evaluate the the pre- and post-education average scores in the experimental and control groups. The significance level was set at 0.05. To assess normality of the data, the Kolmogorov-Smirnov test statistic and p-value were examined, and skewness and kurtosis coefficients were calculated. According to Tabachnick and Fidell's (2013) recommendation, data distribution is considered to be within normal limits if the p-value is greater than 0.05 in the normality test or if the skewness and kurtosis coefficients fall within the ± 2 range.

Findings

No significant differences were observed between the intervention and control groups with respect to socio-demographic variables (p > 0.05) (Table 1).

Table 1
Comparison of training and control groups based on sociodemographic characteristics (n = 72)

X̄ Mean value, SD Standard deviation, Min Minimum value, Max Maximum value

*Chi-square test

Independent samples t-test

No significant differences were observed between the groups for the other obstetric variables (p > 0.05) (Table 2).

Table 2
Comparison of training and control groups based on obstetric characteristics (n = 72

a-b Groups with the same letter indicate a significant difference, X̄ mean value, SS standard deviation, min minimum value, max maximum value

---

### EANM practical guidance on uncertainty analysis for molecular radiotherapy absorbed dose calculations [^6a964723]. European Journal of Nuclear Medicine and Molecular Imaging (2018). Low credibility.

S-factor

Uncertainties associated with S-factors are somewhat less complicated than in the case of cumulated activity, and are predominantly influenced by the uncertainty associated with the volume. The general approach to determining S-factors is to choose a value calculated for a model that closely approximates the organ or region of interest. If a model of the corresponding size does not exist, a scaling can be applied to adjust the S-factor accordingly. Alternatively, an empirical S-factor versus mass representation can be obtained by fitting suitable S-factor data against mass. The implicit assumption is that appropriate models exist for the clinical situation. There are uncertainties associated with deviations between the model and reality (for example, a tumour that is not spherical) but these are outside the scope of this framework. Figure 6 shows, on a log-log scale, an example of S-factor data for unit density spheres of different masses, empirically fitted by the function:

Fig. 6
Example plot of S-factor versus mass for the radionuclides indicated for unit density spheres

It is possible to apply the same principles as employed in the previous section to determine a covariance matrix for the estimated parameters in the fitting function. However, in this instance the standard uncertainties associated with these fit parameters tend to be very small (< 1%) and the mass uncertainty dominates. Therefore, these estimated parameter uncertainties can be ignored and, using Eq. 31, the standard uncertainty in S is:

Given that mass is proportional to volume, and assuming a known tissue density with negligible uncertainty, the fractional standard uncertainty associated with S is:

The fractional standard uncertainty associated with S is thus proportional to the fractional standard uncertainty associated with volume v, the proportionality constant being the magnitude | c 2 | of the slope of the fitting function on a log-log scale.

---

### Optimal experimental design and estimation for q-space trajectory imaging [^3a401661]. Human Brain Mapping (2023). Medium credibility.

Studies that use these novel QTI‐derived tissue metrics as a biomarker could greatly benefit from a precision‐maximizing QTI sampling scheme. Currently, it is not immediately obvious how the b values and b‐tensor shapes of a given number of DW samples should be distributed to achieve maximal precision in the estimation of QTI‐derived tissue metrics. (Coelho et al.) previously explored optimal experiment design for QTI using only simulations, but they did not consider the impact on the final QTI‐derived tissue metrics during their optimization. A common choice is to use a combination of the three major encodings (i.e. linear, planar, and spherical tensor encoding) at approximately regularly spaced b values (Nilsson et al; Szczepankiewicz, Hoge, & Westin). However, these b values might not guarantee the highest precision, as was demonstrated for diffusion kurtosis imaging (DKI) by Poot et al.

Not only the sampling strategy but also the choice of the estimator can considerably impact the bias and precision with which the tissue parameters are estimated. As the DTD model can be linearized using the natural logarithm, the linear least squares (LLS) estimator is a common choice due to its ease of implementation and the closed‐form solution it provides. However, it is known that the variance of the log‐transformed DW signal is no longer constant (Basser et al.). As such, the LLS estimator, which assumes a constant variance across all DW samples, will have suboptimal precision. To combat this, a weighted linear least squares (WLLS) estimator using the squared reciprocals of the noisy signal as weights will provide improved precision (Basser et al.). However, Veraart et al. showed for DKI that using these particular weights for a WLLS estimator may introduce a bias. An iteratively weighted linear least squares (IWLLS) estimator with its weights based on the squared inverse of the predicted signal is expected to provide more accurate and precise parameter estimates compared to WLLS and even nonlinear least squares (NLS; Veraart et al.). Moreover, imposing certain constraints that follow from the physics of diffusion has the potential to dramatically improve the precision of DTD parameters (Basser & Pajevic; Herberthson et al; Tabesh et al; Veraart et al.).

---

### Quantifying the unknown impact of segmentation uncertainty on image-based simulations [^9ef4f637]. Nature Communications (2021). High credibility.

To help further illustrate this critical result, the region between the (μ + σ)- and (μ − σ)-percentile segmentation ratio curves are shaded dark gray and the region between the 10- and 90-percentile segmentation curves is shaded light gray. At the ratio peak, the behavior is intuitive (i.e. monotonic) with the shaded regions enveloping all their respective curves. However, later in time, the ratio gradually becomes increasingly non-monotonic with percentile segmentation. At t = 7.8 s the 10- and 90-percentile segmentation ratios fail to bound other percentile segmentation ratios and ultimately intersect each other moments later. Moreover, each ratio becomes progressively intertwined with others, such that it becomes difficult to predict how a new percentile segmentation ratio calculation would fall on this graph.

The δ τ curves portrayed in Fig. 5 h emphasize the caution required in bounding segmentation uncertainty using only a few simulations. In the first two exemplars, the results are monotonic with increasing percentile segmentations, and the approximation of the uncertainty distribution using a CDF is representative. However, that does not have to be the case with more complicated models. For example, simply using (μ ± σ)-percentile segmentations to bound the segmentation uncertainty not only fails to encompass all of the intermediate percentile values, it also fails to capture the mean behavior. While this does not invalidate our approach to segmentation uncertainty quantification, it does urge caution in blindly performing simulations for only the standard segmentations for a new exemplar without first checking more intermediate segmentations.

---

### A guided multiverse study of neuroimaging analyses [^90128664]. Nature Communications (2022). High credibility.

Similarly to previous work using Bayesian optimization for the navigation of predefined experimental spaces –, the method presented here can help improve the poor reproducibility present across much of (neuro)science. Sequential analysis as applied here is highly formalized, quantifiable and controllable, and as such, it can be readily combined with pre-registration. Furthermore, the route and samples taken by the analysis make it possible to deduce what the hypothesis (encoded as the target function of the optimization algorithm) was at the time of testing. If a different target function was selected, then the algorithm would have taken a different route through the analysis space (see ref.). This means that questionable research practices such as SHARKing may be more difficult to pursue.

As with any analysis approach, using active sampling methodologies comes with inherent trade-offs. Most notably, for more exploitative problems, where the optimal analysis approach is known (or approximately known) a priori or highly theoretically constrained, then the additional costs (in terms of sequential analysis affecting statistical power and computational burden) are a serious limitation. The optimization algorithm finding local minima resulting in poor overall performance is another potential limitation; this will depend heavily on the acquisition function including the type used and hyperparameters controlling exploration and exploitation as well as decisions regarding the GP regression and types of kernels used to model the low-dimensional space. A related issue is the creation of the low-dimensional space itself; this will inevitably involve a trade-off between capturing relevant variance and creating a relatively simple search space, with few dimensions. We show that the search space is coherent (in terms of the placement of similar pipelines near each other — Fig. 1) and the GP regression is able to capture regularities in the space efficiently (Fig. 2). However, for other problems, e.g. involving lower signal-to-noise ratios, more heterogeneous variability across individuals, or more heterogeneous analysis approaches, building a compact search space may be more challenging. Future work is needed to find the most useful acquisition function, GP regression and search spaces for applying active sampling approaches to multiverse analyses.

---

### Healthy housing reference manual [^d866c439]. CDC (2006). Medium credibility.

Table 14.4. Swimming pool operating parameters — water clarity, disinfectant levels, and chemical values are specified, including combined chlorine with minimum None, ideal None, and maximum 0.5; bromine levels of 2 (minimum), 5 (ideal), 10 (maximum), and for wading or shallow pool for children 4, 7, 10; hardness, CaCO3 of 150 (minimum), 200–400 (ideal), 500+ (maximum); stabilizer, cyanuric acid of 10, 30–50, 100 with the comment that if level exceeds 100 ppm partial water replacement is recommended; algae and bacteria listed as None with the directive to shock treat and maintain required disinfectant levels and 7.2 to 7.6 pH; and free chlorine guidance stating continuous levels at 1 to 1.5 ppm minimum, with loss of clarity potentially due to the operator not running the filter 24 hours per day.

---

### Multi-resource dynamic coordinated planning of flexible distribution network [^3a61f116]. Nature Communications (2024). High credibility.

General iterative format

The chance constrains in Eq. (20) are reformulated as follows using the expectation of random variables.wheredenotes the quantile function.andare both non-negative values, which are used to update the upper and lower boundsandof the inequality constraints, respectively. The initialisation conditions are = , = .

In this way, the chance-constrained optimisation model described in Eq. (20) is transformed into a deterministic model, which is solved with the updated bounds in each iteration until all the security constraints satisfy the predefined risk level. Constraints that occur outside of bounds are defined as valid constraints; otherwise, they are defined as invalid constraints. During the iteration process, only the corrections to the valid constraints need to be calculated and iteratively updated, whereas the invalid constraints remain unchanged. Therefore, using updated bounds to iteratively solve the planning scheme not only ensures a predetermined margin of safety, but also prevents the result from being overly conservative. Although this type of iterative algorithm does not have a convergence guarantee, it performs well in practical engineering applications.

---

### Pediatric considerations before, during, and after radiological or nuclear emergencies [^5775781f]. Pediatrics (2018). Medium credibility.

Radiation terminology and SI units — ionizing radiation includes electromagnetic and particulate forms, and key measurement units are defined as follows: the Système International d'Unités [SI] unit Gy (1 Gy = 100 rad) is the absorbed dose; the SI unit Sv (1 Sv = 100 rem) incorporates a weighting/quality factor based on relative biological effectiveness; and the SI unit of activity is Bq, defined as 1 atomic disintegration per second with the conversion 1 Bq = 1/3.7 x 10^10 Ci.