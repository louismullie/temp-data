# Medical Question & Answer

**Sample ID**: ed4ab402-eae7-94f9-212a-1f332a2dfaf7
**Dataset Index**: 37053

---

## Question

A sequence of positive numbers is not necessarily consecutive terms a, b, and c. We know that

[(c − b)/a] + [(a − c)/b] + [(b − a)/c] = 0

Find the difference of the arithmetic sequence.

---

## Answer

> Let's see… What do we have here? The user is asking how to find the common difference of an arithmetic sequence given a certain equation involving three positive numbers a, b, and c. Let's break this down step-by-step. First, I need to think about whether a, b, and c are indeed terms of an arithmetic sequence and what that implies. Then, I should verify the algebraic manipulation of the given equation. Next, I will solve for the relationship between a, b, and c. Finally, I will extract the common difference and confirm the solution makes sense with a quick check.

> Let me first confirm the setup. The problem states that a, b, and c are positive numbers forming an arithmetic sequence, but it does not specify that they are consecutive terms, so I should be cautious and verify whether the order matters for the algebra. In an arithmetic sequence, consecutive terms differ by a constant d, so if b is the middle term, then a = b − d and c = b + d for some common difference d, and if the order is different, the expressions adjust accordingly; the key is that the differences are constant, which I will use to simplify the equation [^notfound].

> Now, I will examine the given equation and plan the algebra. The expression is (c − b)/a + (a − c)/b + (b − a)/c = 0. I need to combine these fractions over a common denominator and see what cancels or factors. Let me think about the common denominator abc, and I should double-check each numerator after clearing denominators to avoid sign errors.

> Clearing denominators, I multiply each term by abc to get bc(c − b) + ac(a − c) + ab(b − a) = 0. Wait, let me verify the expansion: bc(c − b) becomes bc² − b²c, ac(a − c) becomes a²c − ac², and ab(b − a) becomes ab² − a²b, so the combined expression is bc² − b²c + a²c − ac² + ab² − a²b = 0. I should confirm that I have not dropped any terms before proceeding.

> Next, I should factor by grouping to reveal structure. Grouping terms with similar variables, I get (a²c − a²b) + (ab² − b²c) + (bc² − ac²) = 0, which factors to a²(c − b) + b²(a − c) + c²(b − a) = 0. Hold on, I should verify the signs: yes, that matches the previous expansion, so I can proceed confidently.

> I will now substitute the arithmetic-sequence relationships. Let me assume b is the middle term, so a = b − d and c = b + d. Then c − b = d, a − c = −2d, and b − a = d. Substituting these into the factored form gives a²(d) + b²(−2d) + c²(d) = 0, which simplifies to d(a² − 2b² + c²) = 0. I should confirm that the d factors out correctly, and yes, it does.

> Now, I need to solve d(a² − 2b² + c²) = 0. Since a, b, and c are positive, d cannot be zero unless all terms are equal, but wait, let me verify whether d = 0 is actually forced or just one possibility. If d ≠ 0, then a² − 2b² + c² = 0, which rearranges to a² + c² = 2b². Given a = b − d and c = b + d, I can substitute to get (b − d)² + (b + d)² = 2b², which expands to b² − 2bd + d² + b² + 2bd + d² = 2b², simplifying to 2b² + 2d² = 2b², and thus 2d² = 0, which implies d = 0. Hmm, wait a minute, I initially thought this meant d must be zero, but that would imply a = b = c, which contradicts the idea of a nonconstant arithmetic sequence; I should double-check my assumption that d ≠ 0 leads to a contradiction, and indeed it does, forcing d = 0, so the only solution is that a = b = c and the common difference is 0.

> But wait, what if the order is not a, b, c as consecutive terms? Let me reconsider the generality. The algebra shows that regardless of the labeling, the symmetric condition a² + c² = 2b² must hold for the equation to be satisfied when d ≠ 0, and that condition, together with the arithmetic-sequence relationships, inevitably leads to d = 0, so the conclusion remains the same: the common difference is 0 and the sequence is constant.

> Final answer: The common difference of the arithmetic sequence is 0. I should confirm this with a quick sanity check: if a = b = c, then each term in the original expression becomes 0, so the sum is indeed 0, which satisfies the equation and confirms the solution.

---

The given equation simplifies to ((a - b)(b - c)(c - a) = 0), which implies that at least two of (a, b, c) are equal. Since (a, b, c) are terms of an arithmetic sequence, the only way this can happen is if the common difference is **0**. Therefore, the difference of the arithmetic sequence is **0**.

---

## Algebraic simplification

Start with the given equation:

[(c − b)/a] + [(a − c)/b] + [(b − a)/c] = 0

Multiply through by (abc) to clear denominators:

bc(c − b) + ac(a − c) + ab(b − a) = 0

Expand each term:

bc² − b²c + a²c − ac² + ab² − a²b = 0

Rearrange and group terms:

a²c − a²b + ab² − b²c + bc² − ac² = 0

Factor by grouping:

a²(c − b) + b²(a − c) + c²(b − a) = 0

Notice that ((c - b) = -(b - c)) and ((a - c) = -(c - a)), so:

−a²(b − c) − b²(c − a) − c²(a − b) = 0

Multiply by (-1):

a²(b − c) + b²(c − a) + c²(a − b) = 0

This is a known identity that factors as:

(a − b)(b − c)(c − a) = 0

---

## Interpretation of the factored equation

The equation ((a - b)(b - c)(c - a) = 0) implies that at least one of the factors is zero, so at least two of (a, b, c) are equal. In an arithmetic sequence, consecutive terms differ by a constant difference (d). If any two terms are equal, then (d = 0).

---

## Conclusion

The difference of the arithmetic sequence is **0**.

---

## References

### Electrocorticographic evidence of a common neurocognitive sequence for mentalizing about the self and others [^ce6bd6b5]. Nature Communications (2022). High credibility.

Next, we examined the functional specificity of mentalizing-active sites (Fig. 3c, d). Using trial-averaged results, we identified which mentalizing-active sites also produced significant HFB responses to arithmetic (cognitive task) relative to pre-stimulus baseline (p FDR < 0.05). Additionally, we directly compared single-trial peak power across mentalizing and arithmetic using robust regression (p FDR < 0.05, corrected across sites). Sites were considered 'mentalizing-specific' (light and dark turquoise) if they were (1) mentalizing-active but not arithmetic-active, and (2) produced significantly higher peak power for mentalizing over arithmetic. Sites were considered 'non-specific' (lime, pink, and red) if they coactivated for mentalizing and arithmetic, regardless of peak power differences.

Overall, most mentalizing-active sites were non-specific (68% whole-brain), while the remaining mentalizing-specific sites were unevenly distributed across cortex (Fig. 3c, d). Within our ROIs, the lowest mentalizing-specificity was found in visual cortex (1%). Intermediate mentalizing-specificity was found in TPJ (38%), ATL (32%), and PMC (58%). Very high mentalizing-specificity was found in amPFC (87%), dmPFC (94%), and vmPFC (100%). Taken together, these results show a gradient of mentalizing-specificity from visual cortex to mPFC.

---

### Electrocorticographic evidence of a common neurocognitive sequence for mentalizing about the self and others [^03c5dc8b]. Nature Communications (2022). High credibility.

Discussion

Using electrocorticography (ECoG), we probed the neurocognitive substrates of mentalizing at the level of neuronal populations. We found that mentalizing about the self and others recruited near-identical cortical sites (Fig. 5a, b) in a common spatiotemporal sequence (Figs. 5 c and 6). Within our ROIs, activations began in visual cortex, followed by temporoparietal DMN regions (TPJ, ATL, and PMC), and lastly in mPFC regions (amPFC, dmPFC, and vmPFC; Fig. 3e, f). Critically, regions with later activations exhibited greater functional specialization for mentalizing as measured by three metrics: functional specificity for mentalizing versus arithmetic (Figs. 3 c, d and 4b), self/other differentiation in activation timing (Fig. 5c, d), and temporal associations with behavioral responses (Fig. 4d and Table 1). Taken together, these results reveal a common neurocognitive sequence – for self- and other-mentalizing, beginning in visual cortex (low specialization), ascending through temporoparietal DMN areas (intermediate specialization), then reaching its apex in mPFC regions (high specialization).

Our results are consistent with gradient-based models of brain function, which posit that concrete sensorimotor processing in unimodal regions (e.g. visual cortex) gradually yields to increasingly abstract and inferential processing in 'high-level' transmodal regions like mPFC. We found that the strength of self/other differences in activation timing increased along a gradient from visual cortex to mPFC. Specifically, other-mentalizing evoked slower (Fig. 5c) and lengthier (Fig. 5d) activations than self-mentalizing throughout successive DMN ROIs. These self/other functional differences corresponded with self/other differences in RT Behav (Supplementary Fig. 4), although the two were often dissociable (Table 1). Thus, perhaps because we know ourselves better than others, other-mentalizing may require lengthier processing at more abstract and inferential levels of representation, ultimately resulting in slower behavioral responses.

---

### Electrocorticographic evidence of a common neurocognitive sequence for mentalizing about the self and others [^49c17914]. Nature Communications (2022). High credibility.

Functional specificity and selectivity (multi-site)

Functional specificity (Fig. 3c, d) of each site was determined by comparing mentalizing (collapsed across self/other) and arithmetic (cognitive task) results from trial-averaged analysis, along with direct comparisons of single-trial peak power (including trials with nonsignificant activations) for mentalizing versus arithmetic using bisquare robust regression. Specifically, sites were considered 'mentalizing-specific' if they produced (1) significant trial-averaged activations for mentalizing but not arithmetic (p FDR < 0.05, corrected across sites and timepoints), and (2) produced greater peak power for mentalizing over arithmetic (p FDR < 0.05, corrected across sites). Sites with significant trial-averaged coactivations for mentalizing and arithmetic were considered 'non-specific', regardless of peak power differences. All mentalizing-active sites with nonsignificant mentalizing/arithmetic peak power differences were labeled as 'non-specific' (Mentalizing active = Cognitive active).

Self/other selectivity (Fig. 5a, b) of each site was determined by trial-averaged results and direct comparisons of single-trial peak power (including trials with nonsignificant activations) for self- versus other-mentalizing using bisquare robust regression. Sites were considered 'self-only' or 'other-only' (not considering cognitive task) if they only produced (1) significant trial-averaged activations for only one mentalizing type (p FDR < 0.05, corrected across sites and timepoints) and (2) produced greater peak power for that mentalizing type over the other (p FDR < 0.05, corrected across sites). Sites that activated for both mentalizing types were labeled by self/other differences in peak power: 'self-greater' (Self > Other), 'other-greater' (Other > Self), and 'non-selective' (Self = Other). Sites that activated for only one mentalizing type but had nonsignificant self/other differences in peak power were considered 'non-selective' (Self = Other). In sum, self-only and self-greater sites were considered 'self-selective', while other-only and other-greater sites were considered 'other-selective'.

---

### Electrocorticographic evidence of a common neurocognitive sequence for mentalizing about the self and others [^fc57fb69]. Nature Communications (2022). High credibility.

We also found numerous functional distinctions between TPJ and dmPFC, which is surprising given their remarkably similar functional profiles in fMRI literature. Specifically, we found that TPJ produced earlier activations (Fig. 3e, f) that were notably coactive for mentalizing and arithmetic (cognitive task; Fig. 3c, d). Indeed, the onsets of TPJ activations were the earliest of any DMN ROI. In contrast, dmPFC produced significantly later activations (Fig. 3e, f) that were overwhelmingly mentalizing-specific (Fig. 3c, d), indicating that dmPFC sits at a higher level of mentalizing's neurocognitive sequence than TPJ. Moreover, aggregate ROI analyses revealed no significant self/other differences in TPJ (Table 1), while dmPFC featured robust self/other timing differences (Fig. 5c, d), suggesting that dmPFC is more sensitive to variation in mentalistic content. In terms of behavior, although TPJ and dmPFC best predicted RT Behav, dmPFC was the only ROI that predicted Choice Behav (choosing 'true' or 'false' for task prompts; Table 1), implying that dmPFC instantiates more aspects of mentalistic behavior than TPJ. Strikingly, unlike TPJ, dmPFC activation offsets closely preceded behavioral responses (within 124 ± 7 ms; Fig. 4c), suggesting that dmPFC is more deeply involved in the final stages of mentalistic reasoning. Taken together, while TPJ and dmPFC are both clearly crucial for mentalizing performance, dmPFC appears more specialized for mentalizing itself.

---

### Sequences of cognitive decline in typical Alzheimer's disease and posterior cortical atrophy estimated using a novel event-based model of disease progression [^899ca5c3]. Alzheimer's & Dementia (2020). Medium credibility.

Abstract

Introduction

This work aims to characterize the sequence in which cognitive deficits appear in two dementia syndromes.

Methods

Event‐based modeling estimated fine‐grained sequences of cognitive decline in clinically‐diagnosed posterior cortical atrophy (PCA) and typical Alzheimer's disease (tAD) at the UCL Dementia Research Centre. Our neuropsychological battery assessed memory, vision, arithmetic, and general cognition. We adapted the event‐based model to handle highly non‐Gaussian data such as cognitive test scores where ceiling/floor effects are common.

Results

Experiments revealed differences and similarities in the fine‐grained ordering of cognitive decline in PCA (vision first) and tAD (memory first). Simulation experiments reveal that our new model equals or exceeds performance of the classic event‐based model, especially for highly non‐Gaussian data.

Discussion

Our model recovered realistic, phenotypical progression signatures that may be applied in dementia clinical trials for enrichment, and as a data‐driven composite cognitive end‐point.

---

### Appropriate selection of MRI sequences for common scenarios in clinical practice [^27f73d2b]. Pediatric Radiology (2016). Low credibility.

Knowledge about sequence properties is essential to plan and acquire a diagnostic MRI examination. The broad four categories of sequences include spin echo (SE), gradient echo (GRE), inversion recovery (IR) and echoplanar imaging (EPI). Varieties of sequences from these four categories are available for clinical application. They have different contrast mechanisms, spatial and contrast resolution and speed of acquisition. Choice of sequence differs in various scenarios in clinical practice such as solid organ imaging, moving target imaging, bone and bone marrow imaging, cartilage imaging and vessel imaging, taking into consideration properties of sequences to answer the clinical question. Broad classification of sequences and differences in their contrast, spatial and contrast resolution, and speed of acquisition are discussed in this review. A few common clinical scenarios of MRI imaging are illustrated, along with reasons for the given sequence choices.

---

### DNA-aeon provides flexible arithmetic coding for constraint adherence and error correction in DNA storage [^2606965d]. Nature Communications (2023). High credibility.

In vitro results

To validate the ability of DNA-Aeon to decode data stored in DNA, we encoded three different files of size 4.8 KB (a text file containing the German version of the fairy tale sleeping beauty, Dornröschen), 29.9 KB (a PNG of the logo of the MOSLA research cluster), and 47.1 KB (a JPEG of the Enterprise NCC 1701-D) with different parameters. We synthesized the encoded data, followed by PCR amplification and sequencing to digitize the DNA. Information regarding the encoded files, parameters used, and biological processing can be found in the supplement. The raw sequencing data was then processed using parts of the read processing pipeline Natrix. Since it is common in read processing pipelines to have various quality control steps in which reads that do not reach a quality threshold are discarded, we tested different quality thresholds during initial quality control and after the assembly of paired-end reads. The initial decoding was successful for all inputs using 100% of the FASTQ data. We gradually reduced the percentage of the raw reads used for processing and decoding until the decoding failed. The different properties of the last successful decoding for each parameter combination are provided in the supplement and shown in Fig. 5. Using a lower quality threshold both before and after assembly led to less raw sequencing data needed for decoding, while the amount of processed sequencing data needed for successful decoding was between 1.1 and 1.96 times the encoded data. We furthermore tested the influence of adding a 97% similarity clustering as the last step of the processing before encoding. In the clustering approach, sequences are ordered in a list by abundance, with the most common one first, serving as the representative sequence of the first cluster. All sequences that are at least 97% similar to the first sequence are added to this cluster and removed from the list of sequences. This process repeats until no sequences are left in the list, with only the representative sequences being further evaluated. The results are shown in the supplement. The clustering doubled the number of raw sequences required for the MOSLA logo at 0.3 pq, 20 mq, and for 0.6 pq, 20 mq (with mq = the mean minimal quality of reads to not be discarded, and pq = minimal quality of the read assembly to not be discarded). For every other parameter combination, no changes in the amount of required raw sequences were observed. The similarity clustering led to a general decrease in required processed sequences to decode the data, as only 0.741–0.966 times the encoded data were required for successful decoding. The decreased amount of processed data needed for decoding using similarity clustering, while the number of raw sequences required remained the same compared to no clustering for most cases, implies that the clustering led to a decrease in redundant sequences without improving the error correction performance. Given the variation of mapped reads per sequence in the FASTQ files (supplemental Table 8), with some sequences only constituting 0.001% of the raw FASTQ files, DNA-Aeon was able to decode the data with only 0.9–10% of the raw FASTQ data.

---

### Symbolic arithmetic knowledge without instruction [^d0ca42f7]. Nature (2007). Excellent credibility.

Symbolic arithmetic is fundamental to science, technology and economics, but its acquisition by children typically requires years of effort, instruction and drill. When adults perform mental arithmetic, they activate nonsymbolic, approximate number representations, and their performance suffers if this nonsymbolic system is impaired. Nonsymbolic number representations also allow adults, children, and even infants to add or subtract pairs of dot arrays and to compare the resulting sum or difference to a third array, provided that only approximate accuracy is required. Here we report that young children, who have mastered verbal counting and are on the threshold of arithmetic instruction, can build on their nonsymbolic number system to perform symbolic addition and subtraction. Children across a broad socio-economic spectrum solved symbolic problems involving approximate addition or subtraction of large numbers, both in a laboratory test and in a school setting. Aspects of symbolic arithmetic therefore lie within the reach of children who have learned no algorithms for manipulating numerical symbols. Our findings help to delimit the sources of children's difficulties learning symbolic arithmetic, and they suggest ways to enhance children's engagement with formal mathematics.

---

### Haplosaurus computes protein haplotypes for use in precision drug design [^21a50a7a]. Nature Communications (2018). Medium credibility.

Selecting the most appropriate protein sequences is critical for precision drug design. Here we describe Haplosaurus, a bioinformatic tool for computation of protein haplotypes. Haplosaurus computes protein haplotypes from pre-existing chromosomally-phased genomic variation data. Integration into the Ensembl resource provides rapid and detailed protein haplotypes retrieval. Using Haplosaurus, we build a database of unique protein haplotypes from the 1000 Genomes dataset reflecting real-world protein sequence variability and their prevalence. For one in seven genes, their most common protein haplotype differs from the reference sequence and a similar number differs on their most common haplotype between human populations. Three case studies show how knowledge of the range of commonly encountered protein forms predicted in populations leads to insights into therapeutic efficacy. Haplosaurus and its associated database is expected to find broad applications in many disciplines using protein sequences and particularly impactful for therapeutics design.

---

### Opportunities and challenges associated with clinical diagnostic genome sequencing: a report of the Association for Molecular Pathology [^c0e30536]. The Journal of Molecular Diagnostics (2012). Medium credibility.

Genome sequence variant interpretation — expert interpretation is challenged by the evolving nature of understanding differences between individual genomes and benefits from focusing on clinically significant variants, yet this still requires high-quality and ethnicity-specific reference genomes and clinically vetted, regularly updated databases of annotated variants with population frequencies and referenced clinical relevance; accordingly, there is a stated need to consolidate genotype-phenotype databases into a commonly available and perhaps centralized clinical-grade resource that is publicly accessible.

---

### Multistate outbreak of Salmonella Thompson infections linked to seafood exposure-United States, 2021 [^9afe8a7a]. MMWR: Morbidity and Mortality Weekly Report (2023). Medium credibility.

Laboratory Investigation

Whole genome sequencing (WGS) was performed on the 21 Colorado isolates from 2020 and 97 of the 2021 isolates at the CDPHE laboratory following the PulseNet or Illumina MiSeq paired-end WGS standard operating procedure. Assembly and analysis of these isolates was performed using BioNumerics, a software application for managing microbiologic data, following PulseNet guidelines. § Results were submitted to the PulseNet National Database, which was then queried to pull WGS data for all outbreak-associated samples and environmental samples collected by FDA. wgMLST was performed on each genome using BioNumerics to generate a similarity matrix (a matrix of the number of allele differences for all possible pairwise comparisons). A clustering analysis was conducted on the matrix using the unweighted pair group method with arithmetic mean algorithm, a method for constructing dendrograms in a stepwise manner, based on order of similarity (Supplementary Figure).

Overall, WGS results supported the epidemiologic data, indicating a common source of infection for the 2020 and 2021 outbreaks, confirmed in 2021 to be a common seafood distributor and processor. Based on genetic similarity, the largest grouping of outbreak-associated isolates contained 100 samples, including clinical isolates from 2020 and 2021, and nine environmental isolates from 2021, all with zero allele difference. Among 112 clinical isolates, WGS analysis results indicated no concerning antibiotic resistance, signifying no impact on provider treatment recommendations.

---

### Two types of motifs enhance human recall and generalization of long sequences [^c173fea8]. Communications Psychology (2025). Medium credibility.

Whether it is listening to a piece of music, learning a new language, or solving a mathematical equation, people often acquire abstract notions in the sense of motifs and variables-manifested in musical themes, grammatical categories, or mathematical symbols. How do we create abstract representations of sequences? Are these abstract representations useful for memory recall? In addition to learning transition probabilities, chunking, and tracking ordinal positions, we propose that humans also use abstractions to arrive at efficient representations of sequences. We propose and study two abstraction categories: projectional motifs and variable motifs. Projectional motifs find a common theme underlying distinct sequence instances. Variable motifs contain symbols representing sequence entities that can change. In two sequence recall experiments, we train participants to remember sequences with projectional and variable motifs, respectively, and examine whether motif training benefits the recall of novel sequences sharing the same motif. Our result suggests that training projectional and variables motifs improve transfer recall accuracy, relative to control groups. We show that a model that chunks sequences in an abstract motif space may learn and transfer more efficiently, compared to models that learn chunks or associations on a superficial level. Our study suggests that humans construct efficient sequential memory representations according to the two types of abstraction we propose, and creating these abstractions benefits learning and out-of-distribution generalization. Our study paves the way for a deeper understanding of human abstraction learning and generalization.

---

### A space-time tradeoff for implementing a function with master equation dynamics [^57fa10b0]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### Two types of motifs enhance human recall and generalization of long sequences [^19523d9c]. Communications Psychology (2025). Medium credibility.

In this work, we zoom in, refine, and categorize different forms of abstract sequential structures. We define and differentiate between two algebraic abstractions: "projectional motifs", which are patterns derived from sequences using a projectional function, and "variable motifs", which include patterns involving concrete and variable elements. We move beyond grammaticity judgements and examine the role of motifs on sequence memory and recall. We test the learning of these abstract motifs in a much longer sequences than previous work, demanding participants to gradually build up their knowledge of the motif while learning.

We study the effect of memorizing projectional and variable motifs in sequences by asking the following questions: 1. Are sequences constructed according to an underlying motif memorized more accurately than randomly generated sequences, and 2. Are novel sequences sharing the same motif recalled more accurately than random sequences? We ask these questions in two experiments, each studying one proposed motif type. Furthermore, we hypothesized that identifying structures as motifs helps to simply memory representations of long sequences. We implemented this assumption in our computational model, which continuously finds recurring motifs from distinct sequences. The model learns motifs as abstract representations incorporating components from transition probabilities, chunks, and motifs to reduce memory complexity. We look at the learning and transfer abilities of participants in comparison to the model.

---

### Accurately clustering biological sequences in linear time by relatedness sorting [^120d0aa3]. Nature Communications (2024). High credibility.

Two different values of k-mer length (k) are computed: one for use with k-mer matching between sequences and the other for use with rare k-mers. The goal of selecting k for k-mer matching is to ensure that k-mer matches between two sequences happen infrequently by chance, while using only a single value of k across all sequences. Hence, k is calculated such that one in every 100 k-mers is expected to be found by chance in sequences at the 99th percentile of input sequence lengths using an alphabet with a given number of letters (x):

The reasoning behind this formula for k was previously described. In contrast, k for rare k-mers is set such that at most 10% of the selected 20,000 (i.e. parameter A) sequences are expected to share a rare k-mer due to chance. Each of N sequences contributes 50 (by default) rare k-mers, and the objective is to find k such that each rare k-mer will be found by chance in less than 40 (i.e. 20,000/50 * 10%) sequences on average. Thus, k for rare k-mers is calculated as:

---

### Mental compression of spatial sequences in human working memory using numerical and geometrical primitives [^773c6d7b]. Neuron (2021). Medium credibility.

How does the human brain store sequences of spatial locations? We propose that each sequence is internally compressed using an abstract, language-like code that captures its numerical and geometrical regularities. We exposed participants to spatial sequences of fixed length but variable regularity while their brain activity was recorded using magneto-encephalography. Using multivariate decoders, each successive location could be decoded from brain signals, and upcoming locations were anticipated prior to their actual onset. Crucially, sequences with lower complexity, defined as the minimal description length provided by the formal language, led to lower error rates and to increased anticipations. Furthermore, neural codes specific to the numerical and geometrical primitives of the postulated language could be detected, both in isolation and within the sequences. These results suggest that the human brain detects sequence regularities at multiple nested levels and uses them to compress long sequences in working memory.

---

### Two types of motifs enhance human recall and generalization of long sequences [^eb5f10ac]. Communications Psychology (2025). Medium credibility.

In this work, we compared model fit via generative accuracy, which reflects the model's internal memory representation acquired from instruction sequences up to trial n, as it is evaluated on the recalled sequence generated by the model in comparison to the instruction sequence presented on that trial. This method provides one aspect of model fit. Future work could look at other aspects of behavioral-model comparison. One example could be to evaluate the likelihood of participants' recalled sequence given the models and compare the likelihood as a measure of model fit. Alternatively, the complexity of participant generated sequences as parsed by the models can be compared with reaction time data, as less complex sequences would be recalled faster.

Finally, most of our analysis compare model predictions with human behavior on an aggregated level. We encourage future investigations to examine participants' idiosyncratic learning and transfer strategies. Apart from that, our work defines and investigates two particular types of abstraction. We encourage future work to extend the investigation and look at more forms of abstraction or automatic ways of discovering abstraction such as hierarchical clustering and chunking on recursive abstract levels.

---

### Fast sequences of non-spatial state representations in humans [^5fa0fa65]. Neuron (2016). Low credibility.

Negative Results

Between participants, there was no significant relationship between planning time and earnings (p = 0.3 by regression on subject means), between planning time and sequenceness (p = 0.5 by regression on subject means), or between earnings and sequenceness (p = 0.2 by regression on subject means). We note that the absence of evidence in inter-individual differences should be interpreted cautiously, as the small sample size of the study was ill-suited to detect such differences.

Finally, we asked whether we could detect any trial-by-trial relationship between neural sequences and behavior. Across trials, we regressed the magnitude of sequenceness against the actual earnings and found no effect (p = 0.83 by linear mixed effects). We also regressed the magnitude of sequenceness against planning time on the same trial and found no effect (p = 0.27 by linear mixed effects).

---

### Detection of aneuploidies by paralogous sequence quantification [^e6449589]. Journal of Medical Genetics (2004). Low credibility.

Background

Chromosomal aneuploidies are a common cause of congenital disorders associated with cognitive impairment and multiple dysmorphic features. Pre-natal diagnosis of aneuploidies is most commonly performed by the karyotyping of fetal cells obtained by amniocentesis or chorionic villus sampling, but this method is labour intensive and requires about 14 days to complete.

Methods

We have developed a PCR based method for the detection of targeted chromosome number abnormalities termed paralogous sequence quantification (PSQ), based on the use of paralogous genes. Paralogous sequences have a high degree of sequence identity, but accumulate nucleotide substitutions in a locus specific manner. These sequence differences, which we term paralogous sequence mismatches (PSMs), can be quantified using pyrosequencing technology, to estimate the relative dosage between different chromosomes. We designed 10 assays for the detection of trisomies of chromosomes 13, 18, and 21 and sex chromosome aneuploidies.

Results

We evaluated the performance of this method on 175 DNAs, highly enriched for abnormal samples. A correct and unambiguous diagnosis was given for 119 out of 120 aneuploid samples as well as for all the controls. One sample which gave an intermediate value for the chromosome 13 assays could not be diagnosed.

Conclusions

Our data suggests that PSQ is a robust, easy to interpret, and easy to set up method for the diagnosis of common aneuploidies, and can be performed in less than 48 h, representing a competitive alternative for widespread use in diagnostic laboratories.

---

### Molecular constraints on CDR3 for thymic selection of MHC-restricted TCRs from a random pre-selection repertoire [^d4713c1a]. Nature Communications (2019). High credibility.

To further illustrate differences in sequence overlap between MHCr and MHCi repertoires, we constructed Venn diagrams for the 3000 most frequent TCRβ and TCRα sequences among individual animals from three different strains (Fig. 1e). For three MHCr animals, one third of the 3000 TCRβ sequences are shared between individual animals (e.g. 1017 between B6 and B10.BR), with 371 sequences common to all three MHCr strains. However, few of the top 3000 TCRβ sequences in the B6 mouse (~100 out of 3000) were shared with either Quad ko or Quad ko Bcl-2 tg mice, with only 19 TCRβ sequences common to B6 and both MHCi repertoires (Fig. 1e). Thus, MHCr TCRβ and TCRα sequences are more conserved than those of MHCi sequences.

---

### Standards and guidelines for the interpretation of sequence variants: a joint consensus recommendation of the American College of Medical Genetics and genomics and the Association for Molecular Pathology [^d45b0bbe]. Genetics in Medicine (2015). Medium credibility.

Pharmacogenomics (PGx) variant interpretation — Establishing the effects of variants in genes involved with drug metabolism is challenging, in part because a phenotype is only apparent upon exposure to a drug, and variants in genes related to drug efficacy and risk for adverse events have been described and are increasingly used in clinical care. The traditional nomenclature of PGx alleles uses star (*) alleles, which often represent haplotypes, and traditional nucleotide numbering using outdated reference sequences is still being applied, so converting traditional nomenclature to standardized nomenclature using current reference sequences is an arduous task, but it is necessary for informatics applications with next-generation sequencing. Interpreting sequence variants often requires determining haplotype from a combination of variants, and for many PGx genes phenotype is derived from a diplotype, which is the combination of variants or haplotypes on both alleles. Because PGx variants do not tend to cause disease, using terms related to metabolism, efficacy, or "risk", rather than pathogenic, may be more appropriate, and further nomenclature and interpretation guidelines are needed to establish consistency in this field.

---

### Error correction enables use of oxford nanopore technology for reference-free transcriptome analysis [^bb4b7a93]. Nature Communications (2021). High credibility.

We decide that a subsegment b is a trusted context if it occurs at least max(3, mT /min(ed (c ′, b), ed (H (c ′), H (b)))) times as a row in A ′. Here, m is the number of segments sent for correction and T is a parameter with default value of 0.1. The edit distance term lowers the required threshold for more dissimilar segments to the consensus. This means that lower read depth is required to preserve co-occurring variants (relative to the consensus), and is beneficial under the assumption that co-occurring variants are more common across different reads than co-occurring errors. We use the edit distance between both the original and the homopolymer compressed segments in the denominator to account for the higher number of errors associated with homopolymer lengths. If the strings are identical under homopolymer compression (denominator of 0) the variant (which is a homopolymer length variation) is not considered. Furthermore, we require a variation to be present in at least three reads to be considered.

Once the trusted contexts and variants are established, we error correct the segment of the read r, as follows. Let a be the row of A corresponding to r. For every position j in A, we find the trusted variant whose context has the smallest edit distance to a. We replace a [j] with this variant if it is different than a [j]. These updates are then projected back onto the sequence of the read. Optionally, we correct the segments in A ′ belonging to other reads as well (Step 7), described below in heuristic modifications.

---

### Standards and guidelines for the interpretation of sequence variants: a joint consensus recommendation of the American College of Medical Genetics and genomics and the Association for Molecular Pathology [^2bafc813]. Genetics in Medicine (2015). Medium credibility.

Complex trait risk alleles — reporting guidance and scope: In common diseases, risk alleles can explain only up to 10% of population variance, and this recommendation does not address the interpretation and reporting of complex trait alleles; when such alleles are encountered during sequencing of Mendelian genes, the terms "pathogenic" and "likely pathogenic" are not appropriate, and an interim solution is to report these variants as "risk alleles" or in an "other reportable" category, with evidence descriptors such as "established risk allele", "likely risk allele", or "uncertain risk allele".

---

### Neural representation precision of distance predicts children's arithmetic performance [^e8bf2522]. Human Brain Mapping (2025). Medium credibility.

2 Materials and Methods

2.1 Participants

A total of 32 healthy voluntary children were initially recruited for this study. Three participants were excluded due to either incomplete imaging data (N = 1) or the presence of structural image anomalies (N = 2). The ART toolbox was employed to detect artifact points with two criteria: (1) frame‐to‐frame displacement > 0.5 × voxel size; (2) signal amplitude surpassing three standard deviations above the average whole‐brain signal. Subjects were eliminated when more than one‐third of the time points exhibited artifacts. Ultimately, no subject was eliminated due to measurement artifacts. Consequently, 29 participants (16 males, mean age = 8.61 ± 0.92) were included in the analyses. All participants were right‐handed, had normal or corrected‐to‐normal vision, and had no history of neurological diseases. Written informed consent was obtained from the parents of all child participants, and the children themselves also gave their assent to participate in the study. Ethical approval of the study was granted by the Institutional Review Board of the Institute.

2.2 Stimuli and Experimental Design

Each participant completed the dot number comparison task in the scanner and the basic arithmetic fluency test outside the scanner.

In the dot comparison task, all stimuli consisted of arrays of black dots presented on a gray circular area against a black background (Figure 1A). The number of dots ranged from 1 to 9. The total area and density were kept constant across all stimuli, while the element size of each dot and its presentation location were randomized. The numerical distance between stimuli ranged from 1 to 6, and the magnitude of the dot pairs and their numerical distances were orthogonalized. So, 12 basic dot pairs were used: 5–6, 4–6, 5–7, 2–5, 3–6, 4–7, 1–5, 4–8, 2–7, 3–8, 2–8, and 3–9. The order in which the two dot patterns within each pair were presented was counterbalanced, resulting in 24 pairs in total. Each pair was repeated three times, yielding a total of 72 trials. These trials were randomly distributed across three runs, each consisting of four mini‐blocks of six trials. Four rest blocks were interspersed between the task blocks in each run. The stimuli were presented in a pseudo‐random sequence, ensuring that no more than four consecutive trials required the same response side, and the same pair of stimuli was not presented consecutively.

---

### Accurately clustering biological sequences in linear time by relatedness sorting [^6aa50ac0]. Nature Communications (2024). High credibility.

Phase 2: pre-sorting sequences based on k-mer similarity in linear time

The key innovation underlying the Clusterize algorithm is to pre-sort sequences such that accurate clustering can be performed in linear time. Pre-sorting is conducted within each partition until reaching an iteration limit (i.e. parameter B) or the rank ordering of sequences stabilizes. First, a sequence is randomly selected from each partition with a probability proportional to its average variability in rank order (v) multiplied by its length (L): This formula encourages the selection of longer sequences that still have not stably sorted. Then, the k-mer distance (1 - k-mer similarity) is calculated between the sequence and all others, resulting in a vector of distances (d 1). Second, another sequence is randomly selected with probability proportional to d 1 and inversely proportional to the non-overlapping difference in length (Δov) with the first randomly selected sequence: This prioritizes sequences that overlap completely with the first sequence but are dissimilar. Again, the k-mer distance is calculated between the second sequence and all others (d 2). The relative difference between these two vectors (d 1 − d 2) becomes the initial vector (D) upon which the rank order of sequences is determined.

The process of generating new relative distance vectors (D) is repeated up to 2000 iterations (i.e. parameter B) within each partition. I termed this process relatedness sorting because more similar sequences become closer together with additional iterations. Only a single vector is kept after each iteration to limit the memory overhead of phase 2. There are multiple ways in which to combine D i and D i-1 into a new unified vector, D. I chose to project both vectors onto their axis of maximum shared variance after centering both vectors to have a mean of zero (Fig. 1I). This is equivalent to performing principal component analysis on the two column matrix formed by combining D i and D i-1 and retaining only the first principal component (as D). Each vector is ordered with radix sorting to define the rank order of sequences. Clusterize keeps track of the number of positions each sequence moves in the relatedness ordering (v) using an exponentially weighted moving average with smoothing parameter α (0.05): Note that the vector v₀ is initialized with values of 2000 (i.e. parameter C) and values of v are capped at 2000 in each iteration as an upper limit for practicality.

---

### Computational comparison of two draft sequences of the human genome [^9fa702ae]. Nature (2001). Excellent credibility.

We are in the enviable position of having two distinct drafts of the human genome sequence. Although gaps, errors, redundancy and incomplete annotation mean that individually each falls short of the ideal, many of these problems can be assessed by comparison. Here we present some comparative analyses of these drafts. We look at a number of features of the sequences, including sequence gaps, continuity, consistency between the two sequences and patterns of DNA-binding protein motifs.

---

### Neural representation precision of distance predicts children's arithmetic performance [^5ef7ab75]. Human Brain Mapping (2025). Medium credibility.

2.4 Arithmetic Fluency Test

The basic arithmetic fluency test assessed the subject's math performance using timing arithmetic tasks, including addition, subtraction, multiplication and division [adapted from (Wei et al.)]. The Split‐half reliability for the arithmetic assessment was 0.98 (calculated based on the dataset of Li et al.). During the test, a math problem was presented at the center of the screen, with two answer choices displayed below it. For addition and subtraction problems, the deviation of the incorrect answer from the correct one was ± 1 or ± 2. For multiplication and division, the deviation was ± 2 to ensure participants could not rely on clues such as odd or even patterns to identify the correct answer. Participants press "Q" or "P" to select the correct answer, corresponding to the left or right position respectively. Each arithmetic task was timed at 50 s, resulting in a total test of 200 s, with an additional practice section before the formal test. The program recorded the subject's accuracy (ACC) and RT for each problem.

The subject's performance on each type of problem was compared with the dataset of primary school students in the city (Li et al.). The subject's arithmetic performance index (Z_arithmetic) was calculated as follows:where, ADD, SUB, MUL, and DIV are respectively the difference between the number of correct and incorrect problems for each condition. TIME is the time taken to complete each type of problem. Ave and Sd are the average and standard deviation of the group of primary school students for each grade (Table S1).

---

### The functional repertoires of metazoan genomes [^f287a670]. Nature Reviews: Genetics (2008). Medium credibility.

Metazoan genomes are being sequenced at an increasingly rapid rate. For each new genome, the number of protein-coding genes it encodes and the amount of functional DNA it contains are known only inaccurately. Nevertheless, there have been considerable recent advances in identifying protein-coding and non-coding sequences that have remained constrained in diverse species. However, these approaches struggle to pinpoint genomic sequences that are functional in some species but that are absent or not functional in others. Yet it is here, encoded in lineage-specific and functional sequence, that we expect physiological differences between species to be most concentrated.

---

### Clustering huge protein sequence sets in linear time [^67e82254]. Nature Communications (2018). Medium credibility.

Fig. 3
Cumulative distance distribution between representative sequences. We clustered the test set of 123 million sequences at three different sequence identity thresholds (a – c at 50%, 70%, and 90%, respectively). For each clustering, we randomly sampled 1000 representative cluster sequences, compared them to all representative sequences of the clustering, and plotted the fraction whose best match (excluding self-matches) with minimum sequence coverage of 90% had a sequence identity above the x -value. The y -value at the clustering threshold (dashed line) is the fraction of false negatives, pairs of sequences whose similarity was overlooked by the clustering method

---

### Semaglutide (Wegovy) [^2cd797dc]. FDA (2025). Medium credibility.

HR: Hazard ratio; CI: confidence interval; CV: cardiovascular

The treatment effect for the primary composite endpoint, its components, and other relevant endpoints in Study 1 are shown in Table 4.

Table 4. Treatment Effect for MACE and Other Events in Study 1

*p-value < 0.001, one-sided p-value

1Primary endpoint

2Adjusted for group sequential design using the likelihood ratio ordering.

3CV death was the first confirmatory secondary endpoint in the testing hierarchy and superiority was not confirmed.

4Confirmatory secondary endpoint. Not statistically significant based on the prespecified testing hierarchy.

5Not included in the prespecified testing hierarchy for controlling type-I error.

NOTE: Time to first event was analyzed in a Cox proportional hazards model with treatment as factor. For patients with multiple events, only the first event contributed to the composite endpoint.

Table 5. Mean Changes in Anthropometry and Cardiometabolic Parameters at Week 104 in Study 1

1Parameters listed in the table were not included in the pre-specified hierarchical testing.

2Responses were analyzed using an ANCOVA with treatment as fixed factor and baseline value as covariate. Before analysis, missing data were multiple imputed. The imputation model (linear regression) was done separately for each treatment arm and included baseline value as a covariate and was fitted to all subjects with a measurement regardless of treatment status at week 104.

3For body weight the 'change from baseline' and 'difference to placebo' the unit is percentage change from baseline.

4Baseline value is the geometric mean.

The reduction of MACE with WEGOVY was not impacted by age, sex, race, ethnicity, BMI at baseline, or level of renal function impairment.

---

### The dynamics of hippocampal activation during encoding of overlapping sequences [^7c980590]. Neuron (2006). Low credibility.

Sequence disambiguation, the process by which overlapping sequences are kept separate, has been proposed to underlie a wide range of memory capacities supported by the hippocampus, including episodic memory and spatial navigation. We used functional magnetic resonance imaging (fMRI) to explore the dynamic pattern of hippocampal activation during the encoding of sequences of faces. Activation in right posterior hippocampus, only during the encoding of overlapping sequences but not nonoverlapping sequences, was found to correlate robustly with a subject-specific behavioral index of sequence learning. Moreover, our data indicate that hippocampal activation in response to elements common to both sequences in the overlapping sequence pair, may be particularly important for accurate sequence encoding and retrieval. Together, these findings support the conclusion that the human hippocampus is involved in the earliest stage of sequence disambiguation, when memory representations are in the process of being created, and provide empirical support for contemporary computational models of hippocampal function.

---

### Age-related differences in implicit learning of subtle third-order sequential structure [^9ce9f174]. The Journals of Gerontology: Series B, Psychological Sciences and Social Sciences (2007). Low credibility.

Age-related implicit learning deficits increase with sequence complexity, suggesting there might be limits to the level of structure that older adults can learn implicitly. To test for such limits, we had 12 younger and 12 older adults complete an alternating serial reaction time task containing subtle structure in which every third trial follows a repeating sequence and intervening trials are determined randomly. Results revealed significant age deficits in learning. However, both groups did learn the subtle regularity without explicit awareness, indicating that older adults remain sensitive to highly complex sequential regularities in their environment, albeit to a lesser degree than younger adults.

---

### Three-dimensional ultrashort echo time imaging of solid polymers on a 3-tesla whole-body MRI scanner [^1d8765e5]. Investigative Radiology (2008). Low credibility.

Objective

With the introduction of ultrashort echo time (UTE) sequences solid polymeric materials might become visible on clinical whole-body magnetic resonance (MR) scanners. The aim of this study was to characterize solid polymeric materials typically used for instruments in magnetic resonance guided interventions and implants. Relaxation behavior and signal yield were evaluated on a 3-Tesla whole-body MR unit.

Materials and Methods

Nine different commonly used solid polymeric materials were investigated by means of a 3-dimensional (3D) UTE sequence with radial k-space sampling. The investigated polymeric samples with cylindrical shape (length, 150 mm; diameter, 30 mm) were placed in a commercial 8-channel knee coil. For assessment of transverse signal decay (T₂✱) images with variable echo times (TE) ranging from 0.07 milliseconds to 4.87 milliseconds were recorded. Spin-lattice relaxation time (T₁) was calculated for all MR visible polymers with transverse relaxation times higher than T₂✱ = 300 mus using an adapted method applying variable flip angles. Signal-to-noise ratio (SNR) was calculated at the shortest achievable echo time (TE = 0.07 milliseconds) for standardized sequence parameters. All relaxation times and SNR data are given as arithmetic mean values with standard deviations derived from 5 axially oriented slices placed around the isocenter of the coil and magnet.

Results

Six of the 9 investigated solid polymers were visible at TE = 0.07 milliseconds. Visible solid polymers showed markedly different SNR values, ie, polyethylene SNR = 1146 ± 41, polypropylene SNR = 60 ± 6. Nearly mono-exponential echo time dependent signal decay was observed: Transverse relaxation times differed from T₂✱ = 36 ± 5 mus for polycarbonate to T₂✱ = 792 ± 7 mus for polyvinylchloride (PVC). Two of the investigated solid polymers were applicable to T1 relaxation time calculation. Polyurethane had a spin-lattice relaxation time of T1 = 172 ± 1 milliseconds, whereas PVC had T1 = 262 ± 7 milliseconds, respectively.

Conclusions

A variety of solid polymers can be visualized by means of clinical whole-body MR scanners and 3D ultrashort echo time (UTE) sequences. The investigated polymers differ substantially in signal yield, signal-decay, and spin-lattice relaxation time. The knowledge of the signal behavior of solid polymers on whole-body clinical MR scanners may help to select suitable polymeric materials for instruments and implants which are visible using UTE sequences.

---

### Dysfunctional sars-CoV-2-M protein-specific cytotoxic T lymphocytes in patients recovering from severe COVID-19 [^b96c3d57]. Nature Communications (2022). High credibility.

Although obtained TCR sequences showed the heterogeneity of the TCR clones, there were a few TCRs shared among different convalescents; they are demonstrated as common TCRβ-1, -2, -3, and -4 in Fig. 6b and Table 2. Of note, amino acid sequence of CDR3 in common TCRβ-1 was shared among all members of the moderate group of convalescents (CV-001, CV-003, and CV-004) with difference in a single nucleotide in all the subjects. Among TCR clones with common TCRβ−1, clone 151 and clone 893 had identical amino acid sequence of CDR3 in TCRα while clone 91 had similar but different amino acid sequence of CDR3 in TCRα, in which an amino acid at position 4 of CDR3 was different (Table 2). Moreover, amino acid sequence of CDR3 in TCRβ-2 was different only in an amino acid at position 8 from TCRβ-1. Among TCR clones with TCRβ-1 or TCRβ-2, clone 29, clone 151 and clone 893 had identical amino acid sequence of CDR3 in TCRα while clone 209 had very closed, but different amino acid sequence of CDR3 in TCRα, in which an amino acid at position 4 of CDR3 was different from clone 151, 893, 91, and 29. Collectively, we found public TCRαβ motif: "CAVXYNQGGKLIF" for α motif and "CASSDSGXDGYTF" for β motif. The identification of the public TCRs also could highlight the importance of M 198–206 immunoprevalent epitope recognition in SARS-CoV-2 clearance.

Table 2
Identification of Public TCRs

Common TCRs found in two or more subjects were summarized. CDR3 sequences, as well as V, J (and C) usage were shown for each clone. The sequences of common TCRβ-1 and paired TCRα chains were shown in bold, most of which are shared with common TCRβ-2 and paired α chains. ID numbers with brackets indicate the clones having no TCRα information.

---

### Evaluation of SLC6A8 species conservation and the effect of pathogenic variants on creatine transport [^e0788aa4]. HGG Advances (2025). Medium credibility.

Results

The amino acid sequence of CRT1 is highly conserved in multiple species of terrestrial mammals with identical TM domains 1–10

Alignment was performed assessing similarities in the amino acid sequence of CRT1 in various mammalian species; this was achieved to gain further insight into amino acid differences with implications toward which human mutations may be associated with altered function (Figure 1 A). Primate, mouse, rat, dog, and cat, among several other terrestrial mammals, have a highly conserved orthologous amino acid sequence, suggesting this was likely derived through a shared evolutionary history (Figure 1 B). While the sequence in humans and chimpanzees is identical, other non-human primate species have high sequence identities (99.69%), each having only 2 different amino acids from humans, typically occurring in the termini, while the squirrel monkey has only 3 amino acid differences (99.53% identity). All of the other mammalian species studied had > 98% identity (Figure 1 C). The inter-species differences in this highly conserved amino acid sequence were most commonly in the N and C termini or the extracellular/cytoplasmic loops. Amino acid changes in the TM domains were found to be markedly less common; when present, these were limited exclusively to TM11 and TM12. In most circumstances, the amino acid change was within the same class (e.g. branched chain to branched chain, aliphatic to aliphatic [4 of 5]) and less commonly, a change in amino acid class (1 of 5). Comparison of the human sequence with that of the zebrafish (which is 652 amino acids in length), where about 70% of human genes have an ortholog, demonstrated 74.25% identity and 83.43% similarity.

---

### The making of the standard model [^0fc9a4ce]. Nature (2007). Excellent credibility.

A seemingly temporary solution to almost a century of questions has become one of physics' greatest successes.

---

### Two types of motifs enhance human recall and generalization of long sequences [^acc47983]. Communications Psychology (2025). Medium credibility.

Regression coefficient

Apart from having higher average recall accuracy, both motif groups improved their recall accuracy faster. As shown in Fig. 3 c, we analyzed participants' recall key-press correctness by fitting a logistic regression model assuming a random intercept of each participant and a random slope over individual serial positions (explanation on random effect structure selection in method section Random Effect Structure of Regression Analysis). We observed an effect for both Motifs (for Motif 1: β = 0.62, s e = 0.21, z = 2.88, p = 0.003 95% CI = 0.20 to 1.05; for Motif 2: β = 1.10, s e = 0.21, z = 5.17, p < 0.001, 95% CI = 0.69 to 1.53). Apart from that, we observed an interaction effect between the trial number and group (χ²(2) = 51.69, p < 0.001). Participants in the Motif 1 group improved their recall accuracy at a faster rate than participants in the Independent group (β = 0.21, s e = 0.03, z = 7.55, p < 0.001, 95% CI = 0.16 to 0.28); the same effect was present for the Motif 2 group (β = 0.36, s e = 0.03, z = 11.74, p < 0.001, 95% CI = 0.30 to 0.42). Thus, people improved faster on remembering sequences with fixed motifs than sequences without.

---

### DNA synthesis for true random number generation [^4f3c367b]. Nature Communications (2020). High credibility.

Nucleotide-binding prevalence

For every position × along the sequence data, the relative presence of two nucleotides following each other (e.g. AT) is normalized by the relative presence of the single nucleotides (e.g. A and T) at the corresponding position. (e.g. for AT at position x: f x norm (AT) = ((f x (AT)/ f x (A))/(f x +1 (T)). This gives the general formula with nucleotides N 1 and N 2: f x norm (N 1 N 2) = ((f x (N 1 N 2)/ f x (N 1))/(f x +1 (N 2)).

NIST evaluation parameters

NIST evaluation tests were chosen such that for each test, 56 bit streams containing 1096 bits were tested (with the exception of the rank test, where 56 bit streams containing 100,000 bits each were tested). This variant of statistical analyses was chosen and explained by Rojas et al. The tests were applied with the standard parameters as given by the NIST statistical test suite, with the following parameters differing from set values: (1) frequency test, no parameter adjustments; (2) block frequency test, no parameter adjustments (block length, M = 128); (3) cumulative sums test, no parameter adjustments; (4) runs test, no parameter adjustments; (5) longest runs of ones test, no parameter adjustments; (6) rank test, no parameter adjustments; (7) discrete Fourier transform test, no parameter adjustments; (8) approximate entropy test parameter adjustment: block length, m = 5; 9) serial test no parameter adjustments (block length, m = 16). For each statistical test, the NIST software computes a P -value, which gives the probability that the sequence tested is more random than the sequence a perfect RNG would have produced. Thereby, a P -value of 1 indicates perfect randomness, whereas a P -value of 0 indicated complete non-randomness. More specifically, for a P -value ≥ 0.001: sequence can be considered random with a confidence of 99.9%, and for a P -value < 0.001 sequence can be considered non-random with a confidence of 99.9%.

---

### Approximations to the expectations and variances of ratios of tree properties under the coalescent [^cf6c6a02]. G3 (2022). Medium credibility.

We have found that approximations for fixed n and in the limit asare quite accurate in predicting the expected values seen in coalescent simulations of the ratios (Fig. 2). For the variances, the approximations are generally less accurate, although in most cases, graphs of the approximations and simulated values have similar shape (Fig. 4). These approximations are obtained from a Taylor approximation for the variance of a ratio (equation 4), and higher-order approximations of this variance could potentially be applied by use of Taylor's theorem; as the order of the approximation increases, however, the complexity of the resulting formula also increases. For those variances for which the approximation and simulation are not close in Fig. 4, we advise caution in using the variances in settings in which a precise approximation is needed.

---

### Aegilops tauschii genome assembly v6.0 with improved sequence contiguity differentiates assembly errors from genuine differences with the D subgenome of Chinese spring wheat assembly IWGSC refSeq v2.1 [^e983f908]. G3 (2025). Medium credibility.

The number of putative orthologous gene pairs found here, 40,447, greatly exceeded 6,449 orthologous gene pairs previously estimated for the same A. tauschii accession and the same wheat cultivar. While the number of orthologues may be overestimated here, as pointed out in Methods, the difference between the 2 estimates is too large to be attributable only to an overestimation. There would have to be approximately 4 duplication/deletion events of the type described in Methods for every real orthologous gene pair. Fragmented genome sequences or a too-conservative algorithm used bymay have likely caused this large difference.

The average sequence identity for 40,447 putative orthologous gene pairs (Supplementary Table 1) was 99.4% (σ = 1.65%). Of this, the average sequence identity for the HC orthologous genes was 99.6% (σ = 1.2%), and for the LC orthologous genes was 99.3% (σ = 1.3%). The average sequence identity between putative paralogous HC and LC genes was 93.3% (σ = 6.7%) and 95.2% (σ = 4.5%), respectively (Supplementary Table 1).

Collinear putative orthologues were used to identify differences in orientation and locations of sequence segments in Aet v6.0 and CS RefSeq v2.1 assemblies (Table 5). Differences in orientation were allocated into 3 classes based on the number of gene pairs involved (Table 5). Those involving 2 gene pairs were the most common. The second most common type was differences in segment orientation involving 4 or more gene pairs (Table 5).

Table 5.
Structural differences between the Aet v6.0 and CS RefSeq v2.1 pseudomolecules.

Differences in segment location were considered only if the segment involved 3 or more collinear orthologous gene pairs. Differences in segment location within a pseudomolecule were more common than those among pseudomolecules (P = 0.026, paired t -test) (Table 5).

---

### Chemical unclonable functions based on operable random DNA pools [^2d468223]. Nature Communications (2024). High credibility.

Fig. 2
Multi-level challenge-response-pair analysis.

a Positional base content (A, C, G, T) of arbitrary subsets of sequence reads generated as a response from separate challenges with experimental duplicates (response 1 and response 2). As a comparison, the equivalent result generated by non-selectively reading random DNA is shown, as found in Meiser et al. The x -axis represents the 21-mer composition of the orDNA's output segment, the y -axis the (arbitrary) sequence number within the analyzed set. b Relative counts of the 10 most frequent output sequences, for the same challenge response pairs (CRPs) as in (a). c Relative frequency of A, C, G and T across the 21 positions of the output across the entire read set resulting from Illumina sequencing of the same CRPs as in (a). d Examples of datasets as used for further numeric processing, showing the ten most frequent sequences with their absolute read count resulting from the same CRPs as in (a). Source data are provided as a Source Data file.

---

### Age differences in implicit learning of probabilistic unstructured sequences [^716dca3f]. The Journals of Gerontology: Series B, Psychological Sciences and Social Sciences (2011). Low credibility.

Objective

It is unclear whether implicit probabilistic learning, the acquisition of regularities without intent or explicit knowledge, declines with healthy aging.

Methods

Because age differences in previous work might reflect motor or rule learning deficits, we used the implicit Triplets Learning Task with reduced motor sequencing and non-rule-based associations. Fifteen young and 15 old adults responded only to the last event in a series of discrete 3-event sequences or triplets. A randomly chosen set of triplets occurred with high frequency, so there was no underlying rule to be learned.

Results

Both age groups learned associative regularities, but age differences in favor of the young emerged with practice. Discussion. Age differences may reflect the different neural regions that are involved as training progresses, which differ in the extent to which they are compromised by aging.

---

### Learning meaningful representations of protein sequences [^6646110e]. Nature Communications (2022). High credibility.

How we choose to represent our data has a fundamental impact on our ability to subsequently extract information from them. Machine learning promises to automatically determine efficient representations from large unstructured datasets, such as those arising in biology. However, empirical evidence suggests that seemingly minor changes to these machine learning models yield drastically different data representations that result in different biological interpretations of data. This begs the question of what even constitutes the most meaningful representation. Here, we approach this question for representations of protein sequences, which have received considerable attention in the recent literature. We explore two key contexts in which representations naturally arise: transfer learning and interpretable learning. In the first context, we demonstrate that several contemporary practices yield suboptimal performance, and in the latter we demonstrate that taking representation geometry into account significantly improves interpretability and lets the models reveal biological information that is otherwise obscured.

---

### Abstract representations of events arise from mental errors in learning and memory [^748e9b84]. Nature Communications (2020). High credibility.

Estimating parameters and making quantitative predictions

Given an observed sequence of nodes x 1, …, x t −1, and given an inverse temperature β, our model predicts the anticipation, or expectation, of the subsequent node x t to be. In order to quantitatively describe the reactions of an individual subject, we must relate the expectations a (t) to predictions about a person's reaction timesand then calculate the model parameters that best fit the reactions of an individual subject. The simplest possible prediction is given by the linear relation, where the intercept r 0 represents a person's reaction time with zero anticipation and the slope r 1 quantifies the strength with which a person's reaction times depend on their internal expectations.

In total, our predictionscontain three parameters (β, r₀, and r₁), which must be estimated from the reaction time data for each subject. Before estimating these parameters, however, we first regress out the dependencies of each subject's reaction times on the button combinations, trial number, and recency using a mixed effects model of the form 'RT~log(Trial)*Stage+Target+Recency+(1+log(Trial)*Stage+Recency|ID)', where all variables were defined in the previous section. Then, to estimate the model parameters that best describe an individual's reactions, we minimize the RMS prediction error with respect to each subject's observed reaction times, where T is the number of trials. We note that, given a choice for the inverse temperature β, the linear parameters r 0 and r 1 can be calculated analytically using standard linear regression techniques. Thus, the problem of estimating the model parameters can be restated as a one-dimensional minimization problem; that is, minimizing RMSE with respect to the inverse temperature β. To find the global minimum, we began by calculating RMSE along 100 logarithmically spaced values for β between 10⁻⁴ and 10. Then, starting at the minimum value of this search, we performed gradient descent until the gradient fell below an absolute value of 10⁻⁶. For a derivation of the gradient of the RMSE with respect to the inverse temperature β, we point the reader to the Supplementary Discussion. Finally, in addition to the gradient descent procedure described above, for each subject we also manually checked the RMSE associated with the two limits β → 0 and β → ∞. The resulting model parameters are shown in Fig. 4 a, b for random walk sequences and Fig. 4 g, h for Hamiltonian walk sequences.

---

### Evaluation of 16S rRNA gene sequencing for species and strain-level microbiome analysis [^9f0d8739]. Nature Communications (2019). High credibility.

We argue that targeting sub-regions represents a historical compromise, due to technology restrictions. Today, both PacBio and Oxford Nanopore sequencing platforms are capable of routinely producing reads in excess of 1500 bp and high-throughput sequencing of the full 16S gene is becoming increasingly prevalent. We therefore suggest that the justification for this compromise needs to be revisited and we performed a simple in-silico experiment to demonstrate the advantage of full-length 16S sequencing over the targeting of sub-regions.

We downloaded a set of non-redundant (i.e. > 1% different), full-length 16S sequences from a public database (Greengenes). Taking advantage of the fact that a substantial proportion of these sequences incorporated PCR primer-binding sites, we trimmed them to generate in-silico amplicons for different sub-regions, based on the location of PCR primers commonly used in microbiome studies (Fig. 1a and Supplementary Tables 1–2). Assuming each sequence in our downloaded database represented a unique species, we then used a common classification approach (the Ribosome Database Project (RDP) classifier) to calculate the frequency with which in-silico amplicons for each sub-region could provide accurate, species-level taxonomic classification (using the original database as a reference). In a second experiment, we also clustered our in-silico amplicons to generate OTUs at different, commonly used, sequence similarity thresholds (97%, 98%, 99%).

---

### Somatic mutations in 3929 HPV positive cervical cells associated with infection outcome and HPV type [^e65b24b1]. Nature Communications (2024). High credibility.

Different HR-HPV types, defined by ≥ 10% DNA sequence difference in the viral L1 gene Chen, are linked to profound differences in both risk and prevalence of ICC and its histological subtypes. The three most common HPV types detected in ICC worldwide are HPV16, HPV18 and HPV45. HPV18 and HPV45 are genetically similar, with 74% sequence homology and both are part of the Alpha 7 species group, while HPV16 is more genetically distant, with 52% sequence homology to both HPV18 and HPV45, and it is part of the Alpha 9 species group. Within each HR-HPV type there are lineages and sublineages, defined by 0.5–9% DNA sequence difference, and even finer genetic variants, that have been further linked to differences in precancer/cancer risk and lesion histology –. HPV16 is the most common cause of all ICC worldwide, including 62% of the SCC and 56% of the ADC, while HPV18 and HPV45 combined are relatively more common among ADC (43%) than among SCC (15%). Somatic mutations in PIK3CA are more frequent in SCC compared to ADC and in HPV16-positive tumors compared to HPV18 and HPV45 tumors. It is unknown if the different mutation patterns observed across tumors are primarily linked to differences in histology or to the different associated HPV types and lineages/variants, or both.

In this study we took advantage of the Persistence and Progression (PaP) cohort, which collected residual exfoliated cervical cell samples from women routinely screened for cervical cancer precursors at Kaiser Permanente Northern California (KPNC), to investigate host somatic hotspot mutations (i.e. previously reported cancer driver mutations) by deep targeted sequencing (average coverage 820x). We utilized these residual exfoliated cervical cell samples to evaluate important driver mutations, not only in cancer samples, but also in both precancers and transient HPV infections (< cervical intraepithelial neoplasia grade 2 [CIN2] or subsequently cleared infections) at a very low variant allele fraction (VAF). We also evaluated differences in somatic mutations associated with different HPV types and APOBEC3.

---

### Evaluation of imputation performance of multiple reference panels in a Pakistani population [^ad9bfe5b]. HGG Advances (2025). Medium credibility.

Results

We first evaluated imputation accuracy among common variants (minor allele frequency [MAF]1%). When comparing imputed genotypes to targeted sequencing, broadly, there was no difference in imputation accuracy for common variants across the six panels (Figure 1 A; average = 0.66–0.92 across MAF bins; Table S5). Genome-wide, including variants without matched targeted sequencing data, meta-imputation, and ex1KG had the highest imputation accuracy (mean = 0.66–0.92 and 0.65–0.91; Figure 2 A), while 1KG37-SAS had the lowest accuracy (= 0.60–0.88; Figure 2 A and Table S6). The same finding was observed when usingsuch that meta-imputation and ex1KG had the highest average accuracy (mean = 0.74–0.93 and 0.75–0.94; Figure S5), while 1KG37-SAS had one of the lowest average accuracy (= 0.64–0.86; Figure S5).

Figure 1
Imputation accuracy and coverage compared against targeted sequencing in the Pakistani individuals

(A) Imputation accuracy of different imputation panels across different MAF bins, measured by, for variants that were also included in the targeted sequencing.is the squared correlation between imputed genotype dosage and sequenced genotype, with 1 being a perfect match. Significance: ∗ indicates a nominally significant difference in the meanacross different imputation panels (Kruskal-Wallis rank-sum test p < 0.05) at a specific MAF bin; ∗∗ denotes a Bonferroni-corrected significant difference based on the Kruskal-Wallis rank-sum test (p < 0.0035, correcting for the number of MAF bins tested). The horizontal gray lines indicateof 0.6 (dashed line, indicating fairly high imputation quality) and 0.8 (solid line, indicating high imputation quality). The vertical gray lines indicate MAF > 1% (dashed line) and > 5% (solid line); (B) The number of common variants imputed by different panels compared to targeted sequencing; (C) The number of rare variants imputed by different panels compared to targeted sequencing and (D) imputation accuracy of different imputation panels, measured by, for variants with anof ≥ 0.8.

---

### When to keep it simple-adaptive designs are not always useful [^61a5da08]. BMC Medicine (2019). Medium credibility.

Conclusions

We do not aim to suggest that adaptive designs should not be used. In fact, they are frequently the best option for efficiency reasons and from the perspective of trial participants. However, we also wish to ensure it is well understood by investigators that they are not a universal panacea that should be used in all trials. Investigators should carefully consider the potential benefits and drawbacks of the design used as well as whether it is actually feasible to perform with the resources available.

---

### High tip angle approximation based on a modified bloch-riccati equation [^346f756b]. Magnetic Resonance in Medicine (2012). Low credibility.

When designing a radio-frequency pulse to produce a desired dependence of magnetization on frequency or position, the small flip angle approximation is often used as a first step, and a Fourier relation between pulse and transverse magnetization is then invoked. However, common intuition often leads to linear scaling of the resulting pulse so as to produce a larger flip angle than the approximation warrants — with surprisingly good results. Starting from a modified version of the Bloch-Riccati equation, a differential equation in the flip angle itself, rather than in magnetization, is derived. As this equation has a substantial linear component that is an instance of Fourier's equation, the intuitive approach is seen to be justified. Examples of the accuracy of this higher tip angle approximation are given for both constant- and variable-phase pulses.

---

### Methodology for ACOEM's occupational medicine practice guidelines-2025 revision [^29db8ef8]. ACOEM (2025). High credibility.

ACOEM evidence-based recommendations — characteristics include validity that produces similar clinical outcomes in similar cases; reliability/reproducibility such that a different panel of experts would come to the same recommendation given the same evidence base and decision making matrix; clinical applicability to a broad population with the applicable population stated; clinical flexibility identifying known or generally expected exceptions; clarity that is understandable to clinicians and care managers; a multidisciplinary process using common methods of evidence analysis and structured consensus addressing evidence strength and likely benefits, harms, and costs; scheduled review with a recommended schedule to assure currency; documentation of all steps, analyses, discussions, and decisions; transparency with records of deliberation and revisions available; and Board Review allowing ACOEM's Board of Directors to review recommendations and provide comments for the Panel to consider.

---

### Neural association between non-verbal number sense and arithmetic fluency [^b75a38f7]. Human Brain Mapping (2020). Medium credibility.

Although we found significant correlations at occipital electrodes between the N1 component and reaction time, we did not observe any significant correlations between N1 and error rate/accuracy for any task. This could have been because the tasks used in current study were relatively simple for adults. In cases like that, the reaction time can better reflect individual differences. A second explanation considers that the reaction time is the sum of the time used for each mental process used to complete the task. As such, it varies depending on individual variation in all the component mental processes. In contrast, accuracy can only be one of two values (correct or incorrect). Previous studies have also shown that reaction time correlated with the amplitude of the N1 component, while accuracy did not (e.g. Korinth, Sommer, & Breznitz, 2012; Wiegand et al. 2014).

4.2 The significance of single‐trial‐behavior correlation

The traditional mean‐trial ERP‐behavior interindividual correlation approach did not consistently result in any significant correlations. Although some correlations were significant before correcting for multiple comparisons, the same has found when using the pre‐stimulus interval (e.g. from −200 to 0 ms as baseline). The single‐trial ERP‐behavior correlation approach that has been used in fMRI neuroimaging data analysis (Zhou et al. 2018) consistently showed the involvement of both/right lateral occipital cortex for simple arithmetic (addition and subtraction), numerosity comparison, and figure matching.

However, the mean‐trial correlation approach might not efficiently remove enough of the noise and the traditional mean‐trial correlation approach cannot result in a high correlation coefficient (Meyer et al. 2001). The single‐trial ERP‐behavior correlation approach can avoid the influence of noise on statistical analyses. That is, this approach removes much of the noise (i.e. residuals) during the correlation analysis of each trial, and then reference test is conducted on the weak but possibly stable correlation coefficients filtered much of the noise. Thus, the statistical power is greatly enhanced. Note that after correcting for multiple comparisons, we were still able to find significant correlations with this method.

This approach could be used extensively in ERP and fMRI studies. Reaction time reflects a sum of mental processes. The correlation shows how the time‐locked neural responses reflect the behavioral performance. The reaction time of a simple task might be helpful for detecting synchronized neural responses. The single‐trial correlation also provides the opportunity to compare the correlations from different time windows or conditions.

---

### Iterative reconstruction: how it works, how to apply it [^0909ac56]. Pediatric Radiology (2014). Low credibility.

Computed tomography acquires X-ray projection data from multiple angles though an object to generate a tomographic rendition of its attenuation characteristics. Filtered back projection is a fast, closed analytical solution to the reconstruction process, whereby all projections are equally weighted, but is prone to deliver inadequate image quality when the dose levels are reduced. Iterative reconstruction is an algorithmic method that uses statistical and geometric models to variably weight the image data in a process that can be solved iteratively to independently reduce noise and preserve resolution and image quality. Applications of this technology in a clinical setting can result in lower dose on the order of 20–40% compared to a standard filtered back projection reconstruction for most exams. A carefully planned implementation strategy and methodological approach is necessary to achieve the goals of lower dose with uncompromised image quality.

---

### Limits on fundamental limits to computation [^98c96d2d]. Nature (2014). Excellent credibility.

An indispensable part of our personal and working lives, computing has also become essential to industries and governments. Steady improvements in computer hardware have been supported by periodic doubling of transistor densities in integrated circuits over the past fifty years. Such Moore scaling now requires ever-increasing efforts, stimulating research in alternative hardware and stirring controversy. To help evaluate emerging technologies and increase our understanding of integrated-circuit scaling, here I review fundamental limits to computation in the areas of manufacturing, energy, physical space, design and verification effort, and algorithms. To outline what is achievable in principle and in practice, I recapitulate how some limits were circumvented, and compare loose and tight limits. Engineering difficulties encountered by emerging technologies may indicate yet unknown limits.

---

### Two types of motifs enhance human recall and generalization of long sequences [^3f9c897b]. Communications Psychology (2025). Medium credibility.

Model prediction

We trained the variable motif learning model on the same instruction sequences seen by participants. For sequences with the variable motif, the model learned memory representation manifested in chunks and variables. To do so, the model condensed observations of disparate instances of A, C, and E into one variable entity and concatenates the variable entity with the already-acquired fixed sequence parts in its memory. In this way, the motif learning model learned to represent instruction sequences with variable motifs as a chunk with embedded variable entities. Hence, the memory contains both concrete and abstract sequence parts as a low-complexity sequence representation. For control sequences, the model constructed memory pieces by chunking. During recall, sampling entailment chunks of a variable entity introduces memory recall error (χ²(1) = 3.72, p < 0.001, Conditional R² = 0.03). The motif learning model recalled sequences with variable motifs less accurately than fixed sequences (, 95% CI = -0.04 to 0).

Regression coefficient

We then studied factors that influenced the keypress correctness via fitting a logistic mixed-effects regression, assuming a per-participant random intercept and a random slope per serial position (Conditional R² = 0.32). Shown in Fig. 4 c, the regression coefficient suggested that the variable motif group was more prone to recall mistakes than the fixed group (β = −0.99, s e = 0.26, z = −3.78, p < 0.001, 95% CI = −1.51 to -0.48). Apart from that, the variable motif group learned sequences slower than the fixed group (β = −0.33, s e = 0.11, z = −2.98, p = 0.002, 95% CI = −0.54 to −0.11). Training on sequences with variables decreased participants' probability of recalling the correct key and slowed down learning. Overall, the regression analysis was consistent with our predictions.

---

### Recommendations for tumor mutational burden assay validation and reporting: a joint consensus recommendation of the Association for Molecular Pathology, college of American pathologists, and Society for Immunotherapy of Cancer [^63924343]. The Journal of Molecular Diagnostics (2024). High credibility.

Sequencing mode and population-variant filtering for TMB — Laboratories should report the sequencing mode (tumor-germline paired or somatic only) used by the TMB assay; if somatic-only sequencing is performed, filter settings used to remove common population variants should be provided or made available on request. Survey data showed that laboratories were approximately split between performing paired tumor-germline sequencing and somatic-only analysis. As minor differences in allele frequency filtration can have a significant impact on variants identified by somatic-only sequencing and this approach typically overestimates TMB, especially in underrepresented patient populations, the sample and sequencing mode should be clearly delineated in TMB reports, and providing filter settings can help inform users, assist with interpretation of TMB values, and allow meaningful comparison between assays.

---

### Oversimplifying quantum factoring [^9011a539]. Nature (2013). Excellent credibility.

Shor's quantum factoring algorithm exponentially outperforms known classical methods. Previous experimental implementations have used simplifications dependent on knowing the factors in advance. However, as we show here, all composite numbers admit simplification of the algorithm to a circuit equivalent to flipping coins. The difficulty of a particular experiment therefore depends on the level of simplification chosen, not the size of the number factored. Valid implementations should not make use of the answer sought.

---

### No universal mathematical model for thermal performance curves across traits and taxonomic groups [^1b492c98]. Nature Communications (2024). High credibility.

Recommendations

All our results converge on the same conclusion, i.e. that there are no simple and well-defined scenarios in which particular TPC models would be consistently favoured over others. This highlights the importance of comparing the performance of multiple alternative models across a given dataset, rather than selecting a single model in an arbitrary manner. An anticipated objection to this recommendation might be that researchers often choose a particular model because it includes a parameter of interest for their study. We recommend that model selection still be considered because multiple models may share the same parameter. Even when a model does not include the parameter of interest, it may be possible to obtain its value based on other model parameters or from the fitted curve itself. Obtaining estimates of the same parameter using different models additionally allows AICc-weighted model-averaged estimates to be calculated. In cases where none of the fitted models stands out, such estimates should be much more reliable than estimates obtained from a single model. Careful attention should be spent, however, on ensuring that the parameter estimates to be averaged are fully equivalent across models, otherwise the results obtained from averaging will be meaningless. Besides parameter estimates, model averaging can also be applied to the entire TPC, enabling a more objective quantification of its shape.

We should clarify that we do not expect (or recommend) that all possible models be fitted to each new thermal performance dataset. Instead, one could examine the dendrogram that we estimated in the present study (Fig. 3) to choose a subset of sufficiently distinct models (i.e. those branching far from each other) for fitting to a dataset. By using our dendrogram as a guide, researchers no longer need to compare the equations of different models manually, which can often be quite challenging in our experience. For example, our dendrogram shows that the Johnson-Lewin modeland a common modification thereof (the simplified Johnson-Lewin model; see Supplementary Note 7) yield effectively identical fits, consistent with the conclusions of Yin. It is also worth pointing out that the fits of the mechanistic Johnson-Lewin model (and its variants) are typically highly similar to the fits of the phenomenological asymmetric Janisch model (Janisch II; Fig. 3), one of the two earliest developed TPC models.

---

### When to keep it simple-adaptive designs are not always useful [^62f53d1f]. BMC Medicine (2019). Medium credibility.

As researchers with an interest in methodology of adaptive designs, we are often approached to collaborate to apply them in practice. It has been hugely pleasing to us to see increasing enthusiasm amongst clinical investigators for their use. This is often for good reasons, with the benefits of adaptive designs being compelling. However, in some situations, the drawbacks of using an adaptive approach outweigh the benefits. Although reasons for this have been mentioned in disparate papers, primarily statistical, we believe this issue is not sufficiently emphasised in the literature that promotes adaptive trials. Here, we provide some guidance on situations where we believe the trial design should be kept more straightforward.

---

### Two types of motifs enhance human recall and generalization of long sequences [^a28d583e]. Communications Psychology (2025). Medium credibility.

Model comparison

We compared the recall accuracy of the motif learning model with two alternative models: an associative learning model and a chunking model. The motif learning model constructs memory pieces by combining chunking, associative learning, and abstraction via learning projectional motifs. The chunking model contains the same components except for abstraction. The associative learning model learns the first-order transition between observed sequential items. We gave the same instruction sequence to all three models and thereby arrived at an average recall accuracy for each model on each proceeding experimental trial. To do so, we used the same sequences instructed to the participants to train all models. After updating memory components from each trial of sequences, the memory components of the model are used to generate sequences that emulate recall. Then, the model recall accuracy on a particular trial is calculated as the percentage of matching items in the recalled sequence by the models and the instruction sequence. After that, we calculated the group accuracy progression (averaging across participants) for both the model-simulated performance and the participants' performance. The average generative accuracy per trial of the models is compared to the average recall accuracy per trial of the participants.

---

### The effects of structural complexity on age-related deficits in implicit probabilistic sequence learning [^3228c747]. The Journals of Gerontology: Series B, Psychological Sciences and Social Sciences (2016). Low credibility.

Objective

The primary objective was to determine whether age deficits in implicit sequence learning occur not only for second-order probabilistic regularities (event n - 2 predicts n), as reported earlier, but also for first-order regularities (event n - 1 predicts event n). A secondary goal was to determine whether age differences in learning vary with level of structure.

Method

Younger and older adults completed a nonmotor sequence learning task containing either a first- or second-order structure. Learning scores were calculated for each subject and compared to address our research objectives.

Results

Age deficits in implicit learning emerged not only for second-order probabilistic structure, but also for simple, first-order structure. In addition, age differences did not vary significantly with structure; both first and second order yielded similar age deficits.

Discussion

These findings are consistent with the view that there is an associative binding deficit in aging and that this deficit occurs for implicit as well as explicit learning and across simple and more complex sequence structures.

---

### Biochemical testing for congenital disorders of glycosylation: a technical standard of the American College of Medical Genetics and genomics (ACMG) [^fdcba81b]. Genetics in Medicine (2025). High credibility.

ACMG Technical Standard — Carbohydrate-deficient transferrin analysis — The most common test considered when screening for congenital disorders of glycosylation (CDGs) is transferrin isoform analysis, yet more confirmed disorders are identified as having normal transferrin screening results, including many genetic causes that do not affect N-glycosylation and are expected to have normal results. Most laboratories now use tandem mass spectrometry (MS/MS) for transferrin isoform analysis, and the 2 most common techniques are liquid chromatography (LC)-electrospray ionization and matrix-assisted laser desorption/ionization time of flight mass spectrometry (MALDI-TOF MS). Because of differences in mass resolution, MALDI-TOF MS is not able to detect smaller mass differences seen in type II CDGs and has an extremely high sensitivity for type I CDG profiles. Plasma or serum samples can be used, with preparation done manually or using an automated online process; transferrin is enriched using an anti-human transferrin antibody-coupling affinity column, and cleanup can be accomplished via solid-phase extraction either offline before LC injection or online via an automated LC-integrated system. Depending on ionization, detection may be based on multiply charged species requiring deconvolution/reconstruction or on singly charged species. For interpretation, a normal specimen should have large amounts of di-oligo (tetrasialo) transferrin and small (or absent) amounts of mono-oligo (disialo) species, and normal versus abnormal profiles are based on ratios between species rather than absolute abundance; common sequence variations can change profiles and should be distinguished from true abnormals by experienced interpretation; abnormalities of galactosylation can also be identified from transferrin isoform analysis.

---

### Evolutionary genomics: detecting selection needs comparative data [^afbb56e5]. Nature (2005). Excellent credibility.

Positive selection at the molecular level is usually indicated by an increase in the ratio of non-synonymous to synonymous substitutions (dN/dS) in comparative data. However, Plotkin et al. describe a new method for detecting positive selection based on a single nucleotide sequence. We show here that this method is particularly sensitive to assumptions regarding the underlying mutational processes and does not provide a reliable way to identify positive selection.

---

### Two types of motifs enhance human recall and generalization of long sequences [^fb9b3ea3]. Communications Psychology (2025). Medium credibility.

Model simulation

We compared the behavioral results with the model predictions. We used the same sequences instructed to the participants to train the motif learning model, which creates memory representations of sequence motifs from the observational sequences in an abstract space. We then generated sequences based on the representations learned by the model up to the current time point. We came up with generative accuracy as a surrogate for sequence recall accuracy. The generative accuracy was the edit distance between a generative sequence sampled from the model and the instruction sequence in a particular trial. Figure 3 b shows the average generative accuracy of the model. We observed a significant effect of group (χ²(2) = 216.23, p < 0.001, Conditional R² = 0.13), suggesting that participants in the Motif 1 group (, 95% CI = 0.16 to 0.20) and the Motif 2 group (, 95% CI = 0.15 to 0.18) recalled sequences more accurately during the training blocks than the independent group. Similar to participants, the model remembered sequences with underlying motifs more accurately.

---

### Accurately clustering biological sequences in linear time by relatedness sorting [^eb9669e1]. Nature Communications (2024). High credibility.

As relatedness sorting progresses, the rank ordering (v) typically stabilizes, with some sequences stabilizing before others. When a sequence's rank order stabilizes below 1000 (i.e. parameter C /2), its partition is split into separate groups at this sequence (Fig. 1J), as long as each new group would contain at least 2000 sequences. Relatedness sorting is continued within each group until all of its sequences stabilize in rank order below 1000 or the maximum number of iterations (i.e. parameter B) is reached. In practice, most groups stabilize in rank order before 2000 iterations and, thus, parameter B is not a limiting factor. Rank order stability is defined as v ≤ 1000 because the sequences are moving within 2000 positions on average (i.e. ± 1000), which is the number of sequences considered in phase 3 of the algorithm (i.e. parameter C).

Phase 3: clustering sequences in linear time

No sequences were clustered in phases 1 or 2, but the sequences were ordered by relatedness in preparation for clustering. Two linear time strategies are used for clustering sequences: (1) The rare k-mer strategy draws sequences for comparison in the exact same manner as used in phase 1, thereby prioritizing sequences sharing the greatest number of rare k-mers; (2) The relatedness sorting strategy selects proximal sequences from the final rank ordering of sequences determined in phase 2. Up to 2000 (i.e. parameter C) total sequences are drawn from the two strategies in proportion to the number of clustered sequences arising from each strategy. That is, when a sequence is added to an existing cluster, Clusterize determines whether the sequence was in the set of sequences originating from rare k-mers and/or relatedness sorting. The count of sequences drawn from each strategy is incremented in accordance with the clustered sequence's origin(s). In this manner, the more lucrative strategy is emphasized when determining where to draw the next 2000 candidate sequences.

K-mer similarity is calculated to the cluster representatives corresponding to all 2000 sequences, and any cluster representative passing the similarity threshold is used for assignment. If none exists, the sequence is aligned with up to 200 (i.e. parameter E) cluster representatives having the highest k-mer similarities. If none of these are within the similarity threshold, the sequence becomes a new cluster representative. To avoid issues with partial-length sequences, the sequences are visited in order of decreasing length so that the cluster representative is always the longest sequence in its cluster.

---

### Joint sequencing of human and pathogen genomes reveals the genetics of pneumococcal meningitis [^7fa46687]. Nature Communications (2019). High credibility.

Fig. 2
Burden of rare variation between invasive and carriage isolates, based on mapping and calling short variants against a single reference genome. Loss-of-function (LoF) are frameshift or nonsense mutations. a The site frequency spectrum (SFS) stratified by niche and by predicted consequence. Frequency has been normalized with respect to the number of samples in each population. b Histogram of Tajima's D for all coding sequences in the genome, stratified by niche. c Boxplot of the number of rare variants per sample, stratified by niche and predicted consequence. Damaging mutations are LoF mutations and missense mutations predicted damaging by PROVEAN. Centre line is the median, box spans lower to upper quartiles. Whiskers show the outlier range, defined as being > 1.5× the interquartile range above or below the lower and upper quartiles

Instead, to quantify the difference in rare variants between invasive and carriage samples and to identify which regions of the genome are responsible for the excess of rare alleles we calculated Tajima's D, a statistic for determining departures from neutral evolution, for each coding sequence in the genome, and looked for differing signs of selection between cases and controls. Deviations with D < 0 are indicative of selective sweeps and/or recent population expansion, whereas D > 0 is indicative of balancing selection and/or recent population contraction. We compared the distributions of D by gene in each phenotype (Fig. 2b). Genes in invasive isolates had a lower average D (difference in medians −0.34; two-tailed Wilcoxon rank-sum test W = 1996100, p < 10⁻¹⁰) and a more positively skewed D (difference in skewness 0.30; 95% bootstrapped confidence interval 0.17–0.44). This difference in D may be representative of a genuine difference in selection of variants in genes between niche or may be due to a difference in recent population dynamics, for example, due to the bottlenecks for invasion and transmission. It is illustrative to evaluate these findings considering invasive population as a sample from the carriage population, with potentially invasive genotypes amplified. A greater average D is not observed, as would be caused by rare variants becoming common through random subsampling through a bottleneck. The finding of a lower average D would not necessarily always be expected from subsampling low diversity invasive clones from the overall population. Instead, the shift towards lower MAFs suggests that some allelic variants or genotypes found in carriage are disadvantageous in invasive disease or that recent, rare mutations may play a role in invasion.

---

### Recommendations for tumor mutational burden assay validation and reporting: a joint consensus recommendation of the Association for Molecular Pathology, college of American pathologists, and Society for Immunotherapy of Cancer [^1c605b9b]. The Journal of Molecular Diagnostics (2024). High credibility.

Recommendation 7 — tumor mutational burden (TMB) sequencing mode and germline filtering — states: "Laboratories Should Specify the Sequencing Mode (Tumor-Germline Paired or Somatic Only) Used by the TMB Assay During TMB Assay Validation; If Somatic-Only Sequencing Is Performed, Filter Settings Used to Remove Common Population Variants Should also be Documented". Supporting context notes that "approximately half of laboratories were performing paired tumor-germline sequencing, and half were performing somatic-only analysis", and that "it is important to remove germline variants before TMB quantification" with filters and data sources "evaluated during assay validation and described". The document warns that differing filtering approaches and ethnicity-based cutoffs exist and that minor allele-frequency filter differences can markedly affect results, and reports that somatic-only methods "tend to overestimate TMB", with disparities in population databases (gnomAD " > 140,000" total; " > 64,000" non-Finnish European; " < 18,000" Latino or African/African American; " < 10,000" East Asian) and that African ancestry "has been associated with higher TMB", concluding that while routine paired tumor–germline testing may be infeasible, germline filtering methods and limitations "should be properly validated".

---

### Guidance for protocol content and reporting of factorial randomised trials: explanation and elaboration of the CONSORT 2010 and SPIRIT 2013 extensions [^ba890d2c]. BMJ (2025). Excellent credibility.

In general, more complex factorial designs are often harder for readers to understand and to quantify the effects of potential interactions or other challenges on the trial results. This is not in itself a reason to avoid more complex designs, as such designs often bring other benefits. However, it makes transparent reporting of such trials even more essential, and requires special consideration on how key issues can be reported in the face of added complexity.

---

### Opportunities and challenges associated with clinical diagnostic genome sequencing: a report of the Association for Molecular Pathology [^434309ee]. The Journal of Molecular Diagnostics (2012). Medium credibility.

Clinical diagnostic genome sequencing — consensus calling of heterozygous positions depends on sampling depth because each molecule is sequenced independently; the probability of detecting a second allele is dictated by the sampling probability, and if the second allele must be present in at least 20% of calls for a heterozygous consensus, the position needs to be sampled at least 18 times to be at least 99% confident of detecting both alleles.

---

### Predicting sequence-specific amplification efficiency in multi-template PCR with deep learning [^540f5b88]. Nature Communications (2025). High credibility.

Sequence-specific amplification efficiencies for multi-template PCR

In order to translate the observed changes in sequencing coverage to a quantifiable amplification efficiency, a simple fit of the sequencing data to an exponential PCR amplification process, was performed. This fit uses two parameters per sequence: the initial bias caused by uneven coverage after synthesis, and the PCR-induced bias caused by each sequence's individual amplification efficiency (, see Supplementary Fig. 1 for an illustration). The obtained estimates for the initial coverage bias were comparable to experimental data using PCR-free sequencing of oligonucleotide pools(see Fig. 2c, dashed line) and the distributions of amplification efficiencies were comparable across both datasets (see Supplementary Fig. 18). In both datasets the data revealed a small subset of sequences (representing around 2% of the pool) with very poor amplification efficiency (see Fig. 2c, inset). With estimated efficiencies as low as 80% relative to the population mean (equivalent to a halving in relative abundance every 3 cycles), these sequences were often no longer present in the sequencing data after 60 cycles. Additional analyses also precluded any potential bias from demultiplexing and mapping after sequencing (see Supplementary Figs. 38, 49–53).

---

### Considerations for severe acute respiratory syndrome coronavirus 2 genomic surveillance: a joint consensus recommendation of the Association for Molecular Pathology and association of public health laboratories [^5eff2814]. The Journal of Molecular Diagnostics (2025). High credibility.

SARS-CoV-2 genomic surveillance — sampling strategy design: Unbiased sampling is a promising approach for detecting emerging variants, but sequencing all SARS-CoV-2–positive samples is unrealistic; moreover, universal sequencing does not necessarily provide better data than a well-planned sampling strategy that selects a subset of local samples. Sampling options include considering geographic distribution, travel history, clinical presentation, demographic-specific groups, age differences, and other conditions, or using random testing with a predetermined number of samples per day, week, or month.

---

### Estimating genome-wide significance for whole-genome sequencing studies [^debc0c57]. Genetic Epidemiology (2014). Low credibility.

Although a standard genome-wide significance level has been accepted for the testing of association between common genetic variants and disease, the era of whole-genome sequencing (WGS) requires a new threshold. The allele frequency spectrum of sequence-identified variants is very different from common variants, and the identified rare genetic variation is usually jointly analyzed in a series of genomic windows or regions. In nearby or overlapping windows, these test statistics will be correlated, and the degree of correlation is likely to depend on the choice of window size, overlap, and the test statistic. Furthermore, multiple analyses may be performed using different windows or test statistics. Here we propose an empirical approach for estimating genome-wide significance thresholds for data arising from WGS studies, and we demonstrate that the empirical threshold can be efficiently estimated by extrapolating from calculations performed on a small genomic region. Because analysis of WGS may need to be repeated with different choices of test statistics or windows, this prediction approach makes it computationally feasible to estimate genome-wide significance thresholds for different analysis choices. Based on UK10K whole-genome sequence data, we derive genome-wide significance thresholds ranging between 2.5 × 10⁻⁸ and 8 × 10⁻⁸ for our analytic choices in window-based testing, and thresholds of 0.6 × 10⁻⁸ -1.5 × 10⁻⁸ for a combined analytic strategy of testing common variants using single-SNP tests together with rare variants analyzed with our sliding-window test strategy.

---

### Automatic mapping of atoms across both simple and complex chemical reactions [^8e174b4c]. Nature Communications (2019). High credibility.

Here, we describe and extensively validate an algorithm that can map even the most mechanistically complex chemical and biochemical transformations, including those with incomplete stoichiometry. The distinctive feature of our approach is that it supplements graph-theoretical considerations and optimization schemes. with few (20) but judiciously chosen chemical rules/heuristics that allow the algorithm to explore plausible "intermediate" atom assignments, avoid decoy solutions (e.g. those obtained under the assumption of minimal number of bonds being cut –,), and ultimately be guided towards the correct mappings. The advantages of this approach are most manifest for complex to very complex reactions where it reaches 84% mapping correctness vs. up to ~62% of other, state-of-the-art solutions, (and ~86% vs. ~71% over reactions provided with full stoichiometry). We also demonstrate similar improvements in the mapping of reactions from which "synthetic rules" were previously extracted (often incorrectly, as it turns out) and were used as the basis for synthetic design algorithms. Accurate and general-scope mapping algorithms like the one we describe are important to ensure that computers can extract, process, annotate, and apply not only simple chemical transforms but also the advanced chemistries without which any synthesis-design programs cannot address realistic synthetic challenges. The analyses and comparisons described below are based on over 1400 expert-mapped reactions of different complexities (cf. Supplementary Notes 2–5 and file Supplementary Data 1).

---

### Abstract representations of events arise from mental errors in learning and memory [^0595d298]. Nature Communications (2020). High credibility.

Fig. 2
The effects of higher-order network structure on human reaction times.

a Cross-cluster surprisal effect in the modular graph, defined by an average increase in reaction times for between-cluster transitions (right) relative to within-cluster transitions (left). We detect significant differences in reaction times for random walks (p < 0.001, t = 5.77, df = 1.61 × 10⁵) and Hamiltonian walks (p = 0.010, t = 2.59, df = 1.31 × 10⁴). For the mixed effects models used to estimate these effects, see Supplementary Tables 1 and 3. b Modular-lattice effect, characterized by an overall increase in reaction times in the lattice graph (right) relative to the modular graph (left). We detect a significant difference in reaction times for random walks (p < 0.001, t = 3.95, df = 3.33 × 10⁵); see Supplementary Table 2 for the mixed effects model. Measurements were on independent subjects, statistical significance was computed using two-sided F -tests, and confidence intervals represent standard deviations. Source data are provided as a Source Data file.

Thus far, we have assumed that variations in human behavior stem from people's internal expectations about the network structure. However, it is important to consider the possible confound of stimulus recency: the tendency for people to respond more quickly to stimuli that have appeared more recently. To ensure that the observed network effects are not simply driven by recency, we performed a separate experiment that controlled for recency in the modular graph by presenting subjects with sequences of stimuli drawn according to Hamiltonian walks, which visit each node exactly once. Within the Hamiltonian walks, we still detect a significant cross-cluster surprisal effect (Fig. 2a; Supplementary Tables 3–5). In addition, we controlled for recency in our initial random walk experiments by focusing on stimuli that previously appeared a specific number of trials in the past. Within these recency-controlled data, we find that both the cross-cluster surprisal and modular-lattice effects remain significant (Supplementary Figs. 2 and 3). Finally, for all of our analyses throughout the paper we regress out the dependence of reaction times on stimulus recency (Methods). Together, these results demonstrate that higher-order network effects on human behavior cannot be explained by recency alone.

---

### Data storage using peptide sequences [^bf7b4c65]. Nature Communications (2021). High credibility.

Details of the two-stage sequencing method

Figure 5 shows a flowchart illustrating a method of two-stage sequencing. Four steps are involved in the two-stage sequencing method: (1) preprocessing, (2) candidate sequence generation, (3) sequence selection, and (4) candidate refining. As shown in Fig. 5, Steps 1–3 belong to the first stage (Stage 1), while Step 4 is processed in the second stage (Stage 2). In Stage 1 of the two-stage sequencing method, partial sequence is inferred using the preprocessed data after Step 1. In Stage 2, the remaining part of the sequence is determined using the raw data.

Fig. 5
A flowchart illustrating the method of two-stage sequencing.

AAC stands for amino acid combinations.

At Step 1, preprocessing is performed. At Step 2, the preprocessed data from step 1 is used to find the valid paths (sequences), and the number n of candidate sequences is counted. At Step 3, the effects of the following five factors are jointly considered when arriving at the score of a sequence candidate from Step 3.1 to Step 3.5: length of consecutive amino acids retrieved, number of amino acids retrieved, match error, average intensity of amino acids retrieved, and number of occurrences for different ion types with different offsets. The sequences with the longest length of consecutive amino acids retrieved are first selected (Step 3.1). Among the selected sequences, the sequences with the largest number of amino acids retrieved are then selected (Step 3.2). For the sequences with equal length of consecutive amino acids retrieved together with equal number of amino acids retrieved, the match error is evaluated, which is the mean error between the observed mass values for the amino acids retrieved from the experimental spectrum and the actual mass values of the amino acids normalized by the corresponding observed mass values (Step 3.3). If there is more than one sequence with identical match errors, the average intensity of amino acids retrieved is further calculated and a higher score is given to a sequence with a larger average intensity value (Step 3.4). In addition, multiple ion types are usually considered as the important factors in inferring an amino acid, which means that a mass value may correspond to different types of ions in the spectrum. Generally, the more the number of occurrences for different ion types of an amino acid is, the more likely the amino acid is correct. Therefore, for the sequences with equal score after the aforementioned evaluations of Steps 3.1–3.4, the number of occurrences for different ion types is counted to determine the sequence (Step 3.5). The mass offset sets for the N-terminal a-ion, b-ion, and c-ion type sets, i.e. {a, a-H 2 O, a-NH 3, a-NH 3 -H 2 O}, {b, b-H 2 O, b-H 2 O-H 2 O, b-NH 3, b-NH 3 -H 2 O}, and {c, c-H 2 O, c-H 2 O-H 2 O, c-NH 3, c-NH 3 -H 2 O} are {−27, −45, −44, −62}, {+1, −17, −35, −16, −34}, and {+18, 0, −18, +1, −17}, respectively. According to the fragmentation method and the property of the data, all or some of the above ion types can be used flexibly.

---

### Logistic regression: a brief primer [^677107e6]. Academic Emergency Medicine (2011). Low credibility.

Regression techniques are versatile in their application to medical research because they can measure associations, predict outcomes, and control for confounding variable effects. As one such technique, logistic regression is an efficient and powerful way to analyze the effect of a group of independent variables on a binary outcome by quantifying each independent variable's unique contribution. Using components of linear regression reflected in the logit scale, logistic regression iteratively identifies the strongest linear combination of variables with the greatest probability of detecting the observed outcome. Important considerations when conducting logistic regression include selecting independent variables, ensuring that relevant assumptions are met, and choosing an appropriate model building strategy. For independent variable selection, one should be guided by such factors as accepted theory, previous empirical investigations, clinical considerations, and univariate statistical analyses, with acknowledgement of potential confounding variables that should be accounted for. Basic assumptions that must be met for logistic regression include independence of errors, linearity in the logit for continuous variables, absence of multicollinearity, and lack of strongly influential outliers. Additionally, there should be an adequate number of events per independent variable to avoid an overfit model, with commonly recommended minimum "rules of thumb" ranging from 10 to 20 events per covariate. Regarding model building strategies, the three general types are direct/standard, sequential/hierarchical, and stepwise/statistical, with each having a different emphasis and purpose. Before reaching definitive conclusions from the results of any of these methods, one should formally quantify the model's internal validity (i.e., replicability within the same data set) and external validity (i.e., generalizability beyond the current sample). The resulting logistic regression model's overall fit to the sample data is assessed using various goodness-of-fit measures, with better fit characterized by a smaller difference between observed and model-predicted values. Use of diagnostic statistics is also recommended to further assess the adequacy of the model. Finally, results for independent variables are typically reported as odds ratios (ORs) with 95% confidence intervals (CIs).

---

### When to keep it simple-adaptive designs are not always useful [^bc2b8212]. BMC Medicine (2019). Medium credibility.

Background

Adaptive designs are a wide class of methods focused on improving the power, efficiency and participant benefit of clinical trials. They do this through allowing information gathered during the trial to be used to make changes in a statistically robust manner - the changes could include which treatment arms patients are enrolled to (e.g. dropping non-promising treatment arms), the allocation ratios, the target sample size or the enrolment criteria of the trial. Generally, we are enthusiastic about adaptive designs and advocate their use in many clinical situations. However, they are not always advantageous. In some situations, they provide little efficiency advantage or are even detrimental to the quality of information provided by the trial. In our experience, factors that reduce the efficiency of adaptive designs are routinely downplayed or ignored in methodological papers, which may lead researchers into believing they are more beneficial than they actually are.

Main Text

In this paper, we discuss situations where adaptive designs may not be as useful, including situations when the outcomes take a long time to observe, when dropping arms early may cause issues and when increased practical complexity eliminates theoretical efficiency gains.

Conclusion

Adaptive designs often provide notable efficiency benefits. However, it is important for investigators to be aware that they do not always provide an advantage. There should always be careful consideration of the potential benefits and disadvantages of an adaptive design.

---

### Standards and guidelines for the interpretation of sequence variants: a joint consensus recommendation of the American College of Medical Genetics and genomics and the Association for Molecular Pathology [^23ab16cb]. Genetics in Medicine (2015). Medium credibility.

ACMG standards and guidelines — computational (in silico) predictive programs can aid interpretation but have limits: A variety of in silico tools, both publicly and commercially available, can aid in the interpretation of sequence variants, with two main categories that predict missense impact or splicing. Studies have shown performance characteristics "ranging from 65–80% accuracy for missense variants", and tools also tend to have low specificity, resulting in overprediction of deleterious missense changes. Commonly used missense tools in clinical laboratories include PolyPhen2, SIFT, and MutationTaster. For splicing, splice site prediction tools have higher sensitivity (~90–100%) relative to specificity (~60–80%). Because different programs have similar bases, "predictions combined from different in silico tools are considered as a single piece of evidence in sequence interpretation as opposed to independent pieces of evidence", yet "The use of multiple software programs for sequence variant interpretation is also recommended". It is not recommended that these predictions be used as the sole source of evidence to make a clinical assertion.

---

### How to speed up ion transport in nanopores [^dcbc5ef9]. Nature Communications (2020). High credibility.

Using the above voltage steps that each expels one layer of co-ions gives a crude procedure to charge a pore as fast as possible. We can refine this procedure by using many tiny voltage steps, each expelling some small amount of co-ions instead. Accordingly, we rewrite the sum over n as an integral and change the integration variable from n to voltage u, which yields:Equation (2) is an implicit equation for the time-dependent sweep function U (t) in terms of K (U). This equation provides the optimal sweep function for charging to an arbitrary potential difference below U. To demonstrate its applicability, we have numerically determined K (U) for the model supercapacitor shown in Fig. 1d with MD simulations; the results are shown by symbols in Fig. 2a. Assuming that D c does not depend on the applied potential, we performed the integration in eq. (2) numerically using the composite trapezoidal rule (symbols in Fig. 2 b; see also Figs. S5 and S6). The results demonstrate that U (t) can be varied quickly at early times, corresponding to low voltages, because there are still many co-ions close to the pore exit, and hence their 'effusion' path is relatively short (i.e. the path over which the co-ions need to diffuse to exit the pore). At intermediate times, corresponding to intermediate voltages, this effusion path becomes longer due to the reduced co-ion density and one must slow down the variation of the applied potential, to allow the co-ions to leave the pore. Finally, at long times (higher applied potentials), there are practically no co-ions left in the pore and the charging can proceed in an almost stepwise manner.

---

### Flexible multitask computation in recurrent networks utilizes shared dynamical motifs [^51a9771f]. Nature Neuroscience (2024). High credibility.

Sorting the rows and columns of the variance matrix revealed a blockwise structure, where groups of units had large variance for groups of task periods with similar computational requirements (Fig. 3a). Similar computations can be seen in the task period color labels (Stimulus1 and Stimulus2 — for tasks with two sequential stimulus presentations, Memory and Response) and in the task names (Category, DecisionMaking, Memory, and so on) (Fig. 3a, left). For example, task period cluster 2 (Fig. 3a, right) corresponds to reaction-timed response task periods (see Extended Data Fig. 1 for definitions of all tasks). These tasks receive new stimulus information during a response period that must be incorporated into the computation immediately. Therefore, the network cannot prepare a response direction before the fixation cue disappears. On the other hand, in task period cluster 9, the network receives no new information during the response period and must, instead, use the memory of the stimulus to produce the correct output during the response period. These separate blocks in the variance matrix reveal two distinct clusters of units that contribute to response period dynamics: one for tasks with reaction-timed responses and another for tasks with memory-guided responses. Other unit clusters for stimulus (unit clusters k–o, task period clusters 4 and 5) and memory (unit clusters a and b, task period cluster 10) computations are apparent in the block-like structure aligned with task period type (Fig. 3a, task titles and task period color labels to the left of the variance matrix). Qualitative structure and quantitative variance of the fixed points for each task period within unit clusters demonstrate the relationship between dynamical motifs and unit clusters (Extended Data Fig. 5). Block structure in the variance matrix was robust to different network architectures and hyperparameters (examples in Extended Data Fig. 6). To quantify task period similarity across networks with different hyperparameters, we calculated the variance matrix for each trained network. We then sorted task periods according to similarity of rows in one reference network and computed the correlation matrix of the sorted rows in the variance matrix for each network ('Task variance analysis' in Methods). The correlation matrix for each trained network revealed the block-wise similarity of task periods (Fig. 3c). Higher correlation across trained networks compared to untrained networks confirmed that the block structure in the variance matrix emerged from learning the task computations rather than from network design choices or the structure of the inputs (Fig. 3b, right) (Pearson correlation coefficient between correlation matrices; 'Task variance analysis' in Methods).

---

### VA / DoD clinical practice guideline for the management of type 2 diabetes mellitus [^8da629fa]. VA/DoD (2023). High credibility.

Algorithm — This clinical practice guideline's algorithm is designed to facilitate understanding of the clinical pathway and decision-making process used in managing patients with T2DM. It represents a simplified flow of the management of patients with T2DM and helps foster efficient decision making by providers, and it includes steps of care in an ordered sequence, decisions to be considered, decision criteria recommended, and actions to be taken. The algorithm is a step-by-step decision tree; standardized symbols display each step, arrows connect the numbered boxes indicating the order in which the steps should be followed, and sidebars 1–8 provide more detailed information to assist in defining and interpreting elements in the boxes.

---

### Repetitive DNA and next-generation sequencing: computational challenges and solutions [^4ea16b84]. Nature Reviews: Genetics (2011). Medium credibility.

Repetitive DNA sequences are abundant in a broad range of species, from bacteria to mammals, and they cover nearly half of the human genome. Repeats have always presented technical challenges for sequence alignment and assembly programs. Next-generation sequencing projects, with their short read lengths and high data volumes, have made these challenges more difficult. From a computational perspective, repeats create ambiguities in alignment and assembly, which, in turn, can produce biases and errors when interpreting results. Simply ignoring repeats is not an option, as this creates problems of its own and may mean that important biological phenomena are missed. We discuss the computational problems surrounding repeats and describe strategies used by current bioinformatics systems to solve them.

---

### Reporting incidental findings in genomic scale clinical sequencing – a clinical laboratory perspective: a report of the Association for Molecular Pathology [^eb6f7a59]. The Journal of Molecular Diagnostics (2015). Medium credibility.

Comparison with incidental findings in other areas of medicine — WES/WGS versus radiology: Reporting of incidental findings is not a new problem in medicine and incidental findings in radiology are common, with most such incidentalomas proving benign but able to trigger expensive anxiety-inducing work-ups, and a feature common to these examples is that the clinician, although not specifically looking for the incidental finding, cannot avoid the observation. The ACMG report, supported by some commenters, argues that incidental findings from WES/WGS are similar to incidental findings in other areas of medicine and must be reported, as in common practice; however, other commenters and this document point out that additional findings, beyond the genes analyzed to answer the clinical question that prompted testing, are not evident without significant extra effort directed toward that end, and the two types of findings are fundamentally different, one being unavoidable and the other requiring additional analysis and interpretation.

---

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^2c990f5a]. American Journal of Kidney Diseases (2006). Medium credibility.

Hemodialysis adequacy — conductivity-based dosing and alternative denominators — notes consideration of using an anthropometric volume when clearance is measured by conductivity, e.g., "(Kcon × T)/Van… could be used instead of Kt/V urea", but "there were not sufficient data" comparing conductivity and urea kinetics to revise guidelines. Another strategy normalized dose to body surface area (BSA), where a method "gives the same dialysis dose when V = 35 L, but then augments the dose when V is less than this amount and reduces the dose when V is larger", and was limited by "lack of definitive clinical outcomes evidence supporting this approach".

---

### Bayesian data analysis reveals no preference for cardinal tafel slopes in COreduction electrocatalysis [^1b664b4e]. Nature Communications (2021). High credibility.

The Tafel slope is an important parameter to judge the performance of a catalyst because it quantifies the amount of additional applied potential required to observe a logarithmic increase in the measured current. Hence, studies that develop a novel electrocatalytic material often measure current–voltage data in the kinetic control regime, and then use these data to estimate a Tafel slope for their catalytic system. Experimental limitations impose a number of practical constraints on the Tafel slope estimation procedure. First, for several electrochemical reactions (CO 2 reduction, N 2 reduction, organic electrosynthesis, etc.), accurately determining a kinetic current for a specific reaction requires product quantification after a constant potential/current hold. Due to throughput limitations for quantification techniques, the Tafel slope often must be estimated from just a handful of data points (roughly, 3–10). Another important practical constraint arises from limiting-current phenomena observed in many electrocatalytic systems. At sufficiently high overpotentials, the reaction rate exceeds the rate of another physical process required for the reaction to proceed. In this regime, the system is no longer under kinetic control, and the measured current plateaus to a limiting current, becoming independent of the applied voltage. Most commonly, limiting currents in CO 2 reduction systems arise from diffusive transport limitations in the electrolyte, although several other physical reasons for plateau currents have been hypothesized and investigated in the literature –. Consequently, experimentally measured Tafel data usually starts out linear, but curves sublinearly at sufficiently high overpotentials.

In the face of these practical limitations, studies in the literature use a relatively standard protocol for estimating the Tafel slope, depicted schematically in the upper half of Fig. 1. First, a researcher must manually identify an ad hoc cutoff between a linear, kinetically controlled regime (the Tafel regime) and a limiting-current (plateau) regime. All data points in the plateau regime are subsequently discarded, and a Tafel slope is fitted by ordinary least-squares (OLS) linear regression to the data in the Tafel regime. The OLS procedure offers a prescription for extracting the standard error of the Tafel slope; this standard error is sometimes used to construct a confidence interval for the Tafel slope estimate.

Fig. 1
Schematic comparison of the traditional approach to Tafel fitting and the new approach we describe in this paper.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^59838d2f]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Sample size metrics — tolerance intervals versus confidence intervals: Although performance is often stated in terms of confidence intervals (CI), the CI of the mean only gives an estimate of the population mean with a stated level of confidence and does not give an indication of the performance of any given sample; to estimate the distribution of the underlying population and the performance of individual samples, the tolerance intervals should be used, with the lower tolerance interval for a normally distributed population determined as x̄ ± k × s, where s is the sample standard deviation (SD) and k is a correction factor, and for two-sided 95% confidence with n = 20 the k value is 2.75, approaching the z-score as the number of samples increases.

---

### Laboratory testing for preconception / prenatal carrier screening: a technical standard of the American College of Medical Genetics and genomics (ACMG) [^12ca80db]. Genetics in Medicine (2024). High credibility.

ACMG technical standard — detection of different variant types advises that test development should consider the variant types in target genes or regions, that specialized bioinformatics and highly reproducible, uniform data are necessary to detect CNVs, genomic rearrangements, and repeat expansions through NGS, and that sequence features complicating testing must be identified along with when ancillary technologies are needed; common PLP variant types and recommended technologies for ACMG tier 3 genes are in Table 2, and tier 3 genes needing special handling are listed separately in Table 3.

---

### Fast sequences of non-spatial state representations in humans [^59fabd2b]. Neuron (2016). Low credibility.

Length of Sequences

We next asked whether the reverse sequenceness effect was driven by pairs of object representations appearing in isolated sequences (i.e. length 2 sequences) or by longer contiguous sequences. We explored sequences of length 3, 4, and 5. A length 3 reverse sequence would occur if, for example, state S 6 had high probability at time t, state S 5 had high probability at time t + 40 ms, and state S 4 had high probability at time t + 80 ms. The length n sequenceness measure we used was a generalization of the cross-correlation measure used to detect length 2 sequences (see Supplemental Experimental Procedures for details). Because this method does not allow direct comparison of effect magnitude between different sequence lengths, we compared effect reliability using the same multilevel model described above. The fixed effect intercept term, which quantifies overall sequenceness, was reliably different from zero at 40 ms state-to-state lag for length 3 (p = 0.009) and length 4 (p = 0.004) sequences. The intercept term was not different from zero for length 5 sequences (p = 0.4). We conclude that spontaneous state representations tended to occur as fast sequences of up to four consecutive states (Figure 5).

Given that the sequence effect was strongest at a state-to-state lag of 40 ms, we estimated that an entire 4-state sequence, consisting of three transitions, lasted on the order of 120 ms. As in most rodent studies (e.g.,), these sequences were compressed in time relative to real experience. The most visually striking marker of state change during move execution was the visual cross-fade between objects, which took 350 ms. Relative to this, sequences were temporally compressed by a factor of 9. Meanwhile, the duration of an entire state transition, including the time to display reward information, varied from approximately 1 to 4 s. Relative to this benchmark, sequences were temporally compressed by a factor of ∼25–100.

---

### Mixed models for repetitive measurements: the basics [^d40754d3]. Fertility and Sterility (2024). Medium credibility.

Linear mixed models are regularly used within the field of reproductive medicine. This manuscript explains the basics of mixed models, when they could be used, and how they could be applied.

---

### Robust 3D bloch-siegert basedmapping using multi-echo general linear modeling [^86be6432]. Magnetic Resonance in Medicine (2019). Medium credibility.

4 RESULTS

4.1 Numerical simulations

4.1.1 Effect of acquisition order and RF spoiling

For sequential acquisition ordering, once steady‐state was reached for each off‐resonance frequency block (Figure 2 A), the phase difference between these was zero before the BS pulse (Figure 2 C). It was nonzero after the BS pulse (Figure 2 E) because it contained the BSS phase. However, this BSS phase estimate remained constant over time and was independent of the RF spoiling condition.

Figure 2
Numerical simulation results. Phase accrued before the BS pulse in case of sequential acquisitions (A) or interleaved acquisitions (B), in the presence (red and blue curve) of RF spoilingor without RF spoiling (green). Phase difference before the pulses of the two acquisitions with opposite frequencies (C, D) and phase difference after the BS pulse (E, F)

For interleaved acquisitions, the phase before the BS pulse varied from pulse to pulse regardless of pulse number (Figure 2 B). This temporal variance caused a nonzero phase difference between the interleaved acquisitions with opposite BS pulse frequencies, both before (Figure 2 D) and after (Figure 2 F) the BS pulse. Furthermore, the phase difference after the BS pulse, i.e. the estimate of the BSS phase, differed from the phase difference obtained with the sequential approach and depended strongly on the RF spoiling conditions.

4.1.2 Effect of RF spoiling increment

The error in the BSS phase resulting from simulating an interleaved acquisition and using the Classic method depended strongly on the RF spoiling increment used (Figure 3). The greatest errors were predicted for phase increments of 0° (equivalent to no RF spoiling) and 180°. Large errors were also predicted for phase increments of 60° and 120°, whereas no error was predicted at 90°.

Figure 3
Numerical simulation results. Relative error of the estimate ofobtained with interleaved acquisitionswith respect to the estimatedobtained with sequential acquisitions.is computed in 2 ways, the Classic method solid lines, and the GLM method (Equation 4), dashed and dotted lines.is only computed with the Classic method. The error is observed over a large range of RF spoiling increment, for 2 TR values (A), 2values (B), 2 T₁ values (C), and 2 T₂ values (D)

---

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^4165d96c]. American Journal of Kidney Diseases (2006). Medium credibility.

Appendix — methods for adding residual clearance to hemodialyzer clearance notes that dialyzer clearances (spKt/V) required to achieve a stdKt/V of 2.0 volumes per week are tabulated across treatment times from 2 to 8 hours and schedules from 2 to 7 treatments per week, with values determined using a formal 2-compartment mathematical model of urea kinetics and similar results obtainable using a simplified equation; the approach gives results similar to the third data column of Table 18.

---

### Weighting sequence variants based on their annotation increases power of whole-genome association studies [^e16cae01]. Nature Genetics (2016). Medium credibility.

The consensus approach to genome-wide association studies (GWAS) has been to assign equal prior probability of association to all sequence variants tested. However, some sequence variants, such as loss-of-function and missense variants, are more likely than others to affect protein function and are therefore more likely to be causative. Using data from whole-genome sequencing of 2,636 Icelanders and the association results for 96 quantitative and 123 binary phenotypes, we estimated the enrichment of association signals by sequence annotation. We propose a weighted Bonferroni adjustment that controls for the family-wise error rate (FWER), using as weights the enrichment of sequence annotations among association signals. We show that this weighted adjustment increases the power to detect association over the standard Bonferroni correction. We use the enrichment of associations by sequence annotation we have estimated in Iceland to derive significance thresholds for other populations with different numbers and combinations of sequence variants.

---

### Functional synergies yet distinct modulators affected by genetic alterations in common human cancers [^618a2a69]. Cancer Research (2011). Low credibility.

An important general concern in cancer research is how diverse genetic alterations and regulatory pathways can produce common signaling outcomes. In this study, we report the construction of cancer models that combine unique regulation and common signaling. We compared and functionally analyzed sets of genetic alterations, including somatic sequence mutations and copy number changes, in breast, colon, and pancreatic cancer and glioblastoma that had been determined previously by global exon sequencing and SNP (single nucleotide polymorphism) array analyses in multiple patients. The genes affected by the different types of alterations were mostly unique in each cancer type, affected different pathways, and were connected with different transcription factors, ligands, and receptors. In our model, we show that distinct amplifications, deletions, and sequence alterations in each cancer resulted in common signaling pathways and transcription regulation. In functional clustering, the impact of the type of alteration was more pronounced than the impact of the kind of cancer. Several pathways such as TGF-β/SMAD signaling and PI3K (phosphoinositide 3-kinase) signaling were defined as synergistic (affected by different alterations in all four cancer types). Despite large differences at the genetic level, all data sets interacted with a common group of 65 "universal cancer genes" (UCG) comprising a concise network focused on proliferation/apoptosis balance and angiogenesis. Using unique nodal regulators ("overconnected" genes), UCGs, and synergistic pathways, the cancer models that we built could combine common signaling with unique regulation. Our findings provide a novel integrated perspective on the complex signaling and regulatory networks that underlie common human cancers.

---

### FREQ-seq2: a method for precise high-throughput combinatorial quantification of allele frequencies [^f6780a6d]. G3 (2023). Medium credibility.

Indeed, the large sample sizes obtained from this method are another major advantage, one which will only increase with improvements in the read counts and base-pair accuracy of sequencing technologies. Because FREQ-Seq 2 libraries are prepared such that every read ideally contains two independent barcodes that uniquely identify a sample, in addition to known adapter sequences and an allele at the target locus, every read from the raw output of a sequencer is a potentially usable sample. The efficiency of the method is limited only by the precision of the library preparation and sequencing process itself. In real data, some reads must be discarded due to errors and noise, such as in cases where one or more barcodes do not match or where no target allele is present, and here the large sample sizes combined with the barcode redundancy of FREQ-Seq 2 are advantageous.

Out of the 96 barcode combinations in our control samples, the most efficient sample had an effective sample size of over 29,000, which was produced from a small fraction of a single lane on a run-of-the-mill short-read sequencer. Additionally, the lowest-coverage sample still had a sample size in the thousands. This indicates that one could further scale the library to contain many more samples than the 96 we included and achieve a larger sample size for each combination than would be possible using traditional quantification methods, without an increase in cost or sequencing resource usage. At the current levels of sequencing throughput and cost, outstanding quantities of high-precision measurements can be achieved for relatively modest sums.

Our results show that, compared to a manual approach to estimating allele frequencies by counting colonies, FREQ-Seq 2 produces much more stable trajectories, while successfully reproducing a qualitatively similar trend consistent with both theory and empirical data for clonal populations evolving towards a fitness peak. The fitness trajectories are likewise qualitatively similar, and we observe similar final values between the two methods. In our evolution experiments, the FREQ-Seq 2 data exhibits a markedly smoother trajectory for both frequency and fitness across several time points over 2,000 generations. Combined with the small magnitude and uncorrelated nature of its error, FREQ-Seq 2 provides a substantial reduction in error and increase in precision compared to manually counting colonies. This is not surprising, as the method eliminates unpredictable sources of human and experimental error while at the same time massively boosting sample sizes.

---

### Methodology for ACOEM's occupational medicine practice guidelines-2025 revision [^ddfcda33]. ACOEM (2025). High credibility.

ACOEM methodology — baseline comparability measures how well the baseline groups are comparable (e.g., age, gender, prior treatment) and defines scoring criteria: Ratings include a combined assessment for both demographic and outcomes variables. Statistical significance does not need to be present for baseline comparability dissimilarity; a score of "0" may apply if a characteristic(s) or outcome variable(s) difference is likely to affect the study conclusions. Rating is "0" if analyses show that the groups were dissimilar at baseline or it cannot be assessed, and a score of "0" may occur if the baseline comparability is too sparse or key elements are not included pre-intervention. Rating is "0.5" if there is general comparability, though one variable may not be comparable, and a rating of 0.5 should not be given if that variable is a key outcome variable. Rating is "1.0" if there is good comparability for all demographic and outcome variables between the groups at baseline.

---

### Opportunities and challenges associated with clinical diagnostic genome sequencing: a report of the Association for Molecular Pathology [^05d2857b]. The Journal of Molecular Diagnostics (2012). Medium credibility.

Analytical NGS considerations — regulation and validation note that before any new laboratory-developed test is used for patient care, a clinical laboratory undertakes analytical validation, but Food and Drug Administration–approved/cleared tests using next-generation sequencing (NGS) do not exist, so laboratories will for the foreseeable future rely on Clinical Laboratory Improvement Amendment regulations. The purpose of assay validation for a laboratory-developed test is to document that the targeted analyte(s) can be detected in a robust and consistent manner, and in accordance with Clinical Laboratory Improvement Amendment regulations the following analytical characteristics must be documented: accuracy, precision, analytical sensitivity, analytical specificity, reportable range, reference intervals, and any other required performance characteristic. Sequences from NGS are not truly complete because of gaps, GC-rich regions, and bioinformatic limitations regarding indel calling. For nucleic acid sequencing, the standard comparator is traditional Sanger sequencing whose technical capabilities are dwarfed by NGS, and it is practically not feasible to perform Sanger sequencing of whole genomes for reasons of reagent costs, labor, and efficiency of both testing and analysis. All steps of NGS need to be evaluated, including sample library preparation, clonal fragment amplification, the sequencing itself, and data analysis, and quality control metrics need to be determined. Open questions include whether sequencing and documenting representative portions or comparison to online reference sequences would suffice, and what reference materials (RMs) are needed to make validation efficient and meaningful.