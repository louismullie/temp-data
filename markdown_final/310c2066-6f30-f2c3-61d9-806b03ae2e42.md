# Medical Question & Answer

**Sample ID**: 310c2066-6f30-f2c3-61d9-806b03ae2e42
**Dataset Index**: 74641

---

## Question

2. For what value of the parameter a does the domain of the function f(x) = a² + a − 3 − ln²(x + a) and the set of its values not intersect and together cover the entire number line?

If there are multiple possible answers, list them in any order separated by a semicolon.

---

## Answer

> Let's see… What do we have here? The user is asking for the value(s) of the parameter (a) such that the domain of the function (f(x) = a^2 + a - 3 - \\ln^2(x + a)) and the set of its values (range) are disjoint and their union is the entire real line. Let's break this down step-by-step. First, I need to think about the domain of (f) and how it depends on (a). Then, I should determine the range of (f) and its dependence on (a). Next, I will translate the two conditions (disjointness and union covering (\\mathbb{R})) into equations involving (a). Finally, I will solve those equations and verify the solution(s) satisfy both conditions, making sure I don't miss any edge cases or constraints.

> Let me first confirm the domain. The expression (\\ln^2(x + a)) requires (x + a > 0), so the domain is (x > -a), which is the interval ((-a, \\infty)). Wait, let me verify that I'm not overlooking any other constraint; the constant term (a^2 + a - 3) is defined everywhere, so the only restriction is indeed (x + a > 0), hence domain (D = (-a, \\infty)).

> Now, I need to find the range. Let me think about the behavior of (\\ln^2(x + a)). As (x \\to -a^+), (x + a \\to 0^+) and (\\ln^2(x + a) \\to \\infty), so (f(x) \\to -\\infty). As (x \\to \\infty), (\\ln^2(x + a) \\to \\infty), so again (f(x) \\to -\\infty). Hold on, I should verify the maximum. The function (-\\ln^2(u)) achieves its maximum at (u = 1), where (\\ln(u) = 0), so (f(x)) achieves its maximum when (x + a = 1), i.e., (x = 1 - a), and the maximum value is (a^2 + a - 3). Therefore, the range is ((-\\infty, a^2 + a - 3]).

> Next, I should translate the two conditions into math. Disjointness means the domain and range have no overlap: ((-a, \\infty) \\cap (-\\infty, a^2 + a - 3] = \\emptyset). Union covering (\\mathbb{R}) means ((-a, \\infty) \\cup (-\\infty, a^2 + a - 3] = \\mathbb{R}). Let me consider what this implies geometrically: the right endpoint of the range must abut the left endpoint of the domain, so (a² + a − 3 = −a). If I solve that, I should get the candidate value(s) of (a).

> I will now solve the equation (a^2 + a - 3 = -a). Rearranging gives (a² + 2a − 3 = 0), which factors as ((a + 3)(a - 1) = 0), so (a = -3) or (a = 1). Hold on, let's not jump to conclusions; I should verify that each candidate satisfies both the disjointness and union conditions, because the abutting condition is necessary but I need to check sufficiency.

> Let me first test (a = 1). Domain: ((-1, \\infty)). Range: ((-\\infty, 1^2 + 1–3] = (-\\infty, -1]). Disjointness: ((-1, \\infty) \\cap (-\\infty, -1] = \\emptyset), which checks out. Union: ((-1, \\infty) \\cup (-\\infty, -1] = \\mathbb{R}), which also checks out. So (a = 1) is valid.

> Now, I should test (a = -3). Domain: ((3, \\infty)). Range: ((-\\infty, (-3)^2 + (-3) - 3] = (-\\infty, 3]). Disjointness: ((3, \\infty) \\cap (-\\infty, 3] = \\emptyset), which is correct. Union: ((3, \\infty) \\cup (-\\infty, 3] = \\mathbb{R}), which is also correct. So (a = -3) is valid as well.

> Hmm, wait a minute, I initially thought there might be additional constraints from the logarithmic term that could exclude one of these, but I should double-check. The domain depends only on (x + a > 0), and the range analysis is independent of (x) except through the maximum value, so both solutions are mathematically consistent with the function's definition and the stated conditions. Therefore, both (a = 1) and (a = -3) satisfy the requirements.

> Final answer: The parameter values are (1) and (-3). Listing them in any order separated by a semicolon, I get (1; -3).

---

The domain of (f(x) = a^2 + a - 3 - \\ln^2(x + a)) is ((-a, \\infty)), and the range is ((-\\infty, a^2 + a - 3]). For the domain and range to be disjoint and their union to be (ℝ), the supremum of the range must equal the infimum of the domain, i.e. (a² + a − 3 = −a). Solving (a² + 2a − 3 = 0) gives (a = 1) or (a = −3). Both values satisfy the disjointness and coverage conditions, so the possible values of (a) are (1; −3).

---

## Domain of the function

The function (f(x) = a² + a − 3 − ln²(x + a)) is defined when the argument of the logarithm is positive, i.e. (x + a > 0), which implies (x > −a). Therefore, the domain is:

Domain = (−a, ∞)

---

## Range of the function

The term (ln²(x + a)) is always non-negative and can take any non-negative value as (x) varies over the domain. Thus, (−ln²(x + a)) can take any non-positive value, and the maximum value of (f(x)) occurs when (ln²(x + a) = 0), i.e. when (x + a = 1). At this point, (f(x) = a² + a − 3). Therefore, the range is:

Range = (−∞, a² + a − 3]

---

## Condition for disjointness and coverage

For the domain and range to be disjoint and their union to be (\\mathbb{R}), the supremum of the range must equal the infimum of the domain:

a² + a − 3 = −a

Solving this equation:

a² + 2a − 3 = 0

(a + 3)(a − 1) = 0

Thus, (a = -3) or (a = 1).

---

## Verification of solutions

### Case 1: (a = 1)

- **Domain**: ((−1, ∞))
- **Range**: ((−∞, −1])
- **Intersection**: Empty
- **Union**: ((−∞, ∞) = ℝ)

---

### Case 2: (a = −3)

- **Domain**: ((3, ∞))
- **Range**: ((−∞, 3])
- **Intersection**: Empty
- **Union**: ((−∞, ∞) = ℝ)

---

Both values satisfy the conditions, so the possible values of (a) are (1) and (−3).

---

The values of (a) for which the domain and range of (f(x) = a² + a − 3 − ln²(x + a)) are disjoint and together cover (ℝ) are (1) and (−3).

---

## References

### Accurate spike estimation from noisy calcium signals for ultrafast three-dimensional imaging of large neuronal populations in vivo [^0887e407]. Nature Communications (2016). Medium credibility.

This is achieved by modifying equation (19) as follows:

For this estimation, we use A = 10%, τ = 0.8s. σ is autocalibrated as explained in the previous section. Standard, experience-inspired default values are used for the nonlinearity parameters, drift parameter and 'calcium event rate' λ.

Next, the amplitude of single spike transients is best estimated from isolated calcium transients of moderate amplitude. Therefore calcium events that are either too close (< 1 s) to another event, or of amplitude F/F > 25% are excluded. The predicted calcium signals for those excluded events are then subtracted from the original signal, yielding modified calcium signals containing only the 'good' events. Individual event amplitudes and the value of τ are then re-estimated so as to maximize the fit to these new signals.

At this point, a histogram of all event amplitudes is constructed (Supplementary Fig. 5a). It is first smoothed, yielding x 1. Thereafter, peaks are enhanced by dividing x 1 by a low-passed version of itself, x 2. A cost function x 3 is then defined as x 3 (A) = x 2 (A)+ x 2 (2 A)/2 over a bounded range [A min = 4%, A max = 10%] (note that ' x 2 (2 A)' is a simplified view: in general 2A is replaced by the actual amplitude of a two-spikes transient taking into account nonlinearities). A first estimate of A is chosen as the value that maximizes x 3 (the green star in Supplementary Fig. 5a). This estimate is used to assign a number of spikes to each individual event (black separation lines and printed spikes numbers): the separations between k and k +1 spikes are set at (k +0.3)* A.

---

### Size limits the sensitivity of kinetic schemes [^71d1065f]. Nature Communications (2023). High credibility.

A different measure of sensitivity — the amplification of a fold-change in the input — provides a solution to this problem. Suppose that for some value x 0 of the input parameter x, scaling by a factor a scales the output by b, so f (a x 0) = b f (x 0). Then the quotientcan be thought of as a discrete approximation of the derivative defining the logarithmic sensitivity. And if f (x) is differentiable everywhere, then by the mean value theorem, there must be a value x * of x for whichThis means that careful measurement of any two points on the input-output curve (x versus f (x)) witnesses the (local, infinitesimal) logarithmic sensitivity somewhere. Importantly — unlike in the case of fitting to a Hill function — if error in the measurements is very low, then they are also telling us the derivative for some value of x very accurately.

Equation (25) leads us to another common definition of the effective Hill coefficient:where S 0.9 and S 0.1 are the values of the input variable (in our case, x) required to get 90% and 10% (respectively) of the maximum value of the output variable (in our case, f (x)). Note that (26) is like (25) with a = S 0.9 / S 0.1 and b = 9. It implies that somewhere between S 0.1 and S 0.9 there is a logarithmic sensitivity of H eff /2.

There is yet another common definition, specific to models of binding. Suppose x is the concentration of a ligand and 〈 n b 〉(x) is the expected number of sites bound by a ligand out of a total of n possible binding sites. It is common then, to takeor to report, as the Hill coefficient, the slope of a line fitted to x versusdata on a log-log plot.

---

### A novel oppositional binary crow search algorithm with optimal machine learning based postpartum hemorrhage prediction model [^f6de23e1]. BMC Pregnancy and Childbirth (2022). Medium credibility.

Whereasrepresent the opposite number and x ∈ R denotes a real number determined on range of x ∈[a, b]. While a = 0 and b = 1 Eq. (3) becomes

While there is a point P (x 1, x 2,… x n) in n dimension coordinate and x 1, x 2, …, x n ∈ R later, the opposite pointis determined as its coordinates:

In such cases, have 2 values, x represent initial arbitrary value in [a, b] anddenotes the opposite values of x. They calculate f (x)&in all the iterations of OBCSA, later, employ on the evaluation function g ifselect x or else selectConsequently, the f l would be in range: f l ∈[f l min, f l max]. The opposite numbercan be determined by:

Later, evaluate the fitness for the first f l value and the fitness forin all the iterations. When, they select f l, or elsewould be selected. The stages of presented method can be given in the following.

Step1: The count of crows is n c = 25, f l min = 0.1, f l max = 1.8, A P = 0.3, and the maximal number of iterations is t max = 100.

Step2: The position that represent the features are made by U (0, 1).

Step3: The fitness function (FF) can be determined by

Whereas C represent the classification performance, W represent the weighted factors in the range of zero and one, F all represent the overall amount of features and F sub signifies the length of elected feature.

Step4: The position of the crows are upgraded as Eq. (2)

Step5: Steps 3 & 4 are repetitive till a t max is attained.

---

### Evaluation of the evenness score in next-generation sequencing [^1f5f1936]. Journal of Human Genetics (2016). Low credibility.

The evenness score (E) in next-generation sequencing (NGS) quantifies the homogeneity in coverage of the NGS targets. Here I clarify the mathematical description of E, which is 1 minus the integral from 0 to 1 over the cumulative distribution function F(x) of the normalized coverage x, where normalization means division by the mean, and derive a computationally more efficient formula; that is, 1 minus the integral from 0 to 1 over the probability density distribution f(x) times 1-x. An analogous formula for empirical coverage data is provided as well as fast R command line scripts. This new formula allows for a general comparison of E with the coefficient of variation (= standard deviation σ of normalized data) which is the conventional measure of the relative width of a distribution. For symmetrical distributions, including the Gaussian, E can be predicted closely as 1-σ(2)/2⩾E⩾1-σ/2 with σ ≤ 1 owing to normalization and symmetry. In case of the log-normal distribution as a typical representative of positively skewed biological data, the analysis yields E≈exp(-σ*/2) with σ*(2) = ln(σ(2)+1) up to large σ (≤ 3), and E≈1-F(exp(-1)) for very large σ (⩾2.5). In the latter kind of rather uneven coverage, E can provide direct information on the fraction of well-covered targets that is not immediately delivered by the normalized σ. Otherwise, E does not appear to have major advantages over σ or over a simple score exp(-σ) based on it. Actually, exp(-σ) exploits a much larger part of its range for the evaluation of realistic NGS outputs.

---

### Mathematical method to build an empirical model for inhaled anesthetic agent wash-in [^626c6179]. BMC Anesthesiology (2011). Low credibility.

Figure 7
Multicolored surface describing desflurane F A after 3 min for a 176 cm tall patient over the FGF - F D range studied. The FGF - F D combinations that result in a F At of 4.5% (light blue horizontal surface) after 3 min are found by calculating the intersection between this target (the light blue surface) and the multicolored surface; these FGF - F D combinations are presented in B. The asterix (*) indicates the lowest possible FGF that can be used to attain 4.5% desflurane at 3 min because of the maximum 18% vaporizer setting.

To determine which FGF - F D combinations result in a F At of e.g. 4.5% after 3 min, the intersection has to be calculated between this target (the horizontal light blue surface) and the multicolored surface that describes the F A values with the entire FGF-F D range after 3 min (Figure 7). This line can be calculated by entering Ht = 176, Time = 3, and F At = 4.5 in the formula F D = -(e (-FGF*-0.23+FGF*0.24) *(e (FGF*-0.23) *F At *Ht*0.1-e (FGF*-0.23) *FGF*2.55+40.46-e (FGF*-0.23) *40.46+e (FGF*-0.23+Time/-4.08) *40.46-e (Time/-4.08) *40.46))/((-1+e (FGF*0.24))*(-1+e (Time/-4.08))*39.29). This intersection line is shown in Figure 8 and presents the virtually infinite number of FGF - F D combinations that result in an F A of 4.5% after 3 min in a 176 cm tall patient. It also illustrates that it is not possible to achieve this target with a FGF lower than 0.7 L. min -1 because F D would have to be higher than the maximum 18%. Note that the curve increases at high FGF because vaporizer output decreases due to excessive cooling.

---

### An evolutionary theory on virus mutation in COVID-19 [^95e7b9b4]. Virus Research (2024). Medium credibility.

3.4 The stochastic property of the intersection set

Since M = 100,000 corresponds to 16.6 bits of information, which is only 1.3% of the 1273 bits for the entire spike protein, we employ a supplementary method to expand the range of stochastic sampling by randomizing the parameter y. This means that both the intersection set Y(y) and the union set Z(a + x - y) are altered. In Fig. 4, we discovered that the change in the parameter y for a given x significantly affects the tree topology. Therefore, randomizing the parameter y is expected to be an efficient approach to enhance the stochastic information. To gain further insights into y -randomization, we performed six randomizations and treated y as a fluctuating quantity in each group of given x, calculating the statistical distribution of y. The resulting distributions are presented in Table 2.

Table 2
Statistical distribution of y in six-times randomization.

Upon examining Table 2, we observe that the mean value of y remains constant within each group, specifically y = (a /1273) x, where a represents the number of sites in set A(a = 104). This constancy arises from the fact that the probability of a stochastic-produced site occurring in set A is a /1273, and the total occurrence is given by (a /1273) x for x sites. Additionally, the skew value remains approximately constant for a given x and decreases as x increases. Consequently, the parameter y tends towards a normal distribution as x becomes larger. It is worth noting that extending the randomization of y from six times to a greater number of times does not alter the aforementioned conclusion. Thus, randomizing y for a given x to expand the range of stochastic sampling is a viable approach. This technique effectively increases stochastic information and demonstrates the reliability of the stochastic method in the A-X model.

---

### Universal exploration dynamics of random walks [^b718affc]. Nature Communications (2023). High credibility.

Fig. 4
Extensions and applications of the time between visits to new sites for RWs.

Boundary observables for recurrent and marginal RWs: The perimeter of the visited domain and the number of islands enclosed in the support. a The elapsed time τ P for successive increments of the time dependence of the perimeter P (t) of the visited domain. b Distribution F P (τ) of the time elapsed τ P between the first observations of a domain perimeter of length P and P + 2 for simple RWs on the square lattice. c Distribution F I (τ) of the elapsed time τ I between the first occurrence of I and I + 1 islands for Lévy flights of index α = 1.2. Plotted in b and c are the scaled distributionsand Y ≡ F I (τ) versus X = τ. The red dashed lines have slope − 2 μ. The data are for P, I = 50, 100, and 200 (respectively blue stars, orange circles and green squares). Multiple-time covariances and starving RWs. d for Lévy flights of parameter α = 1.5, and we compare Y to(dashed line). The stars, circles, and squares indicate data for t 1 = 10, t 1 = 100, and t 1 = 1000. e, for Lévy flights of parameter α = 1.3. We compare Y to the dashed line proportional to. Data in red and green indicate t 1 = 10 and t 1 = 100. Stars indicate t 2 = 2 t 1 and circles indicate t 2 = 4 t 1. We take t 3 = 4 t 2. f Lifetime at starvation. Blue circles show the mean lifetime versus the metabolic time. The dashed line is proportional to. Non-Markovian examples. Rescaled distribution Y = F n (τ) n 1+1/ μ versus X = τ / n 1/ μ for g Fractional Brownian motion with parameter 1/ H = 1/0.4 = μ = 1 − θ (n = 20, 40 and 80) h Fractional Brownian motion with parameter 1/ H = 1/0.75 = μ = 1 − θ (n = 20, 40 and 80), i True Self Avoiding Walks μ = 2/3 = 1 − θ (n = 200, 400 and 800). For the last three panels, increasing values of n are represented successively by blue stars, orange circles and green squares, and the dashed line is proportional to X −(2− θ).

---

### Identifying domains of applicability of machine learning models for materials science [^72c4ba76]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ν that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by σ lin (x 1, x 2) ≡ −0.3 ≤ x 1 ≤ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: σ rbf ≡ −3.3 ≤ x 11 ≤ 3.1 ∧ x 2 ≤ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA σ poly (x 1, x 2) ≡ −3.5 ≤ x 2 ≤ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### A machine learning approach for online automated optimization of super-resolution optical microscopy [^3757bacc]. Nature Communications (2018). Medium credibility.

Kernel TS for online optimization

Classical formulation of mathematical optimization is to seek for the value x in a domainto maximize (or minimize) a real-valued target function. Here, f (x) corresponds to the expected objective value associated with the parametrization x in the space of optimized parameters. The online optimization problem can be formulated as an episodic game, each episode corresponding to an algorithm selecting a parameter configuration fromand acquiring an image. This image leads to a noisy observation of the target function at the chosen parameterization. An algorithm evolving in this setting faces two challenges: it must estimate the target function from noisy observations acquired at different locations while selecting the right parameters to optimize the target function.

When the target function is known to be smooth but rigid parametric assumptions over this function are not realistic, kernel regression is able to estimate the target function from prior, noisy and not necessarily independent and identically distributed samples. More specifically, under multivariate Gaussian priors, kernel regression can provide the posterior distribution over a function given prior observations. Whereas linear regression assumes that the target function is linear on its input space, kernel regression rather assumes that there exists a function ϕ mappingto another space, such that the target function is described by a linear relation on. Formally, there exists a vector θ such that f (x) corresponds to a inner product between ϕ (x) and θ. (The linear regression setting corresponds to the specific case where ϕ (x) = x.) The inner product between two mapped points ϕ (x) and ϕ (x ′) is given by the kernel k (x, x ′). The choice of kernel controls the smoothness of the resulting regression model, thus encoding information about smoothness assumptions on f. In the experiments, we used an anisotropic Gaussian kernel (here defined for 1D):with bandwidthfor parameter 1 ≤ i ≤ D, withbeing the range of values for parameter i. We assume that the noise on observations of the target function is a zero-mean Gaussian with variance σ 2 and that θ follows a zero-mean multivariate Gaussian distribution with covariance matrix, where I is the identity matrix, and λ > 0. The posterior distribution on f after selecting N parameters x 1, …, x N and observing y N = (y 1, …, y N) is then given bywhereandrespectively denote the predictive mean and (co-)variance (Supplementary Figure 1a), k N (x) Τ = (k (x i, x)) 1 ≤ i ≤ N and K N = [k (x i, x j)] 1 ≤ i, j ≤ N. Computing that posterior technically requires the knowledge of the noise variance σ 2, which is not realistic in practice. Fortunately, this can be circumvented by relying on empirical noise estimates based on lower and upper bounds on the noise variance, along with an upper bound on the norm of θ. All experiments used an upper bound of 5 on the norm of θ and a lower bound of 0.001 on the noise variance. An upper bound of 3 on the noise variance was considered for the maximal intensity objective (in glutamate uncaging), otherwise 0.3. The posterior distribution can be computed from the resulting noise estimate while preserving theoretical guarantees.

---

### Large-scale DNA-based phenotypic recording and deep learning enable highly accurate sequence-function mapping [^6931bd38]. Nature Communications (2020). High credibility.

Normalization of biological replicates

In order to facilitate comparison of biological replicates, we capitalized on the 31 internal-standard RBSs. These serve as internal references spanning a large range of RBS activities and allow to compensate for potential batch effects and other systematic biases between replicates. Formally, for each of the 31 internal-standard RBSs, we denote by x and y the measured normalized integral of the flipping profile (IFP) for the biological replicate to be normalized and the reference replicate, respectively. We fit either a polynomial function of degree two, with f (x) = I + Ax + Bx 2, or its inverse f (x) = g −1 (x) with g (z) = I + Az + Bz 2, such that the mean-squared error between f (x) and y is minimized across the 31 measurement pairs. Moreover, we impose the following constraints on the parameters of f: first, RBSs that show no activity in one replicate should remain inactive in the other replicates (f (0) = 0). Second, RBSs whose discriminators are entirely flipped before induction in one replicate should exhibit that behavior in the other replicates (f (1) = 1). Third, the ranking of RBSs according to their strength should be preserved across replicates (f is monotonically non-decreasing in [0, 1]). It should be noted that, empirically, these assumptions appear to hold across the three biological replicates in this study. Imposing the first two constraints above reduces the number of free parameters of the polynomial function from three to one, resulting in the family of functions parametrized by A: f (x) = Ax + (1 − A) x 2. Moreover, the third constraint translates into the following bounds on the set of allowed values for the free parameter A: 0 ≤ A ≤ 2. This procedure was carried out for each pair of biological replicates. The quality of the resulting fits was then evaluated on the full data sets, excluding the 31 internal-standard RBSs that were used to optimize A.

---

### Incorporating economic evidence in clinical guidelines: a framework from the clinical guidelines committee of the American College of Physicians [^902eae12]. Annals of Internal Medicine (2025). High credibility.

Appendix figure 3 — sample summary of findings table provides a template for reporting economic evaluations, with columns for Outcome, Number of Studies (Reference), Population, Perspective, Incremental Cost (Range), Incremental Benefit (quality-adjusted life-year [QALY], patient-centered outcomes) (Range), Reported incremental cost-effectiveness ratio (ICER) per outcome (Range), Certainty of Body of Evidence, Adjusted ICER per U.S. Currency/Inflation, and Value Threshold†; the rows include Intervention A vs. Intervention B with placeholders ICER per QALY at X years and ICER per QALY at Y years, and footnotes clarify CGC = Clinical Guidelines Committee; ICER = incremental cost-effectiveness ratio; QALY = quality-adjusted life-year, note it was prepared with GRADEpro and modified by the authors, and that economic value is defined by the CGC (willingness-to-pay thresholds).

---

### Exploring physics of ferroelectric domain walls via Bayesian analysis of atomically resolved STEM data [^7e217237]. Nature Communications (2020). High credibility.

Fig. 5
180° Domain wall: GLD models and posterior probability densities.

a The 90% highest posterior density interval for the Bayesian analysis (red band) overlaid on experimental mean values (data points). Dashed blue line is a least square fit. b – g are selected 2D joint probability densities for different parameter combinations for model 2.

Table 2
WAIC model comparison for the 109° and 180° domain walls.

The analysis above illustrates the determination of the physically relevant parameters of the material given the experimental observations as STEM atomic coordinates, and past knowledge in the form of the Bayesian priors on relevant materials parameters. As expected for ferroelectric materials with the extremely narrow domain walls, ultimately this consideration becomes the limiting factor in these studies. In other words, while domain wall shape is a measure of the physics of the order parameter in the system, practically the STEM observations are limited by the discreteness of the lattice and the noise in the system.

To explore the effect of the noise and sampling on the differentiability of specific physical behaviors from the observational data, we perform a range of numerical experiments on synthetic data. Here, the synthetic data set is generated using Model 2, Eq. (3 b), for a first order ferroelectric with a specific set of ground truth parameters, chosen here to be (P s, L c, η) = (0.5, 2.0, 2.0). Varied parameters are noise level, chosen to be Gaussian with the dispersion σ, and number of sampling points in the interval, N, with the x varying from −15 to 15. Correspondingly, the pixel spacing 30/ N provides the measure of the discreteness of the measurements, and should be compared to the domain wall width, controlled by L c and η. The center position of the wall is chosen always at x 0 = 0 and there is no offset P 0. The generated synthetic data set is fit by the Bayesian model corresponding to Model 1, Eq. (3 a), and Model 2, Eq. (3 b), for a range of N and σ values. The point estimates of the recovered domain wall parameters, andare determined and can be compared with the ground truth values. Similarly, the WAIC score for the Models 1 and 2 can be determined.

---

### Artificial-intelligence-driven discovery of catalyst genes with application to COactivation on semiconductor oxides [^71cefbe0]. Nature Communications (2022). High credibility.

The details of SGD

The SGD was done with the RealKD code, modified to include quality functions described by Eqs. (1) and (2) in which the information gain was defined as:here p is the number of samples in a subgroup within the required adsorption energy range divided by the total number of samples in the subgroup. Since Shannon entropy is a symmetric parabola-like function around 0.5, we set here F (Z) = 0 for p ≤ 0.5. Also, x· ln(x) = 0 for x = 0. The search of subgroups is performed using a Monte-Carlo scheme adapted for these tasks.

The cutoff values x, y. used for setting propositions (feature-1 < x, feature-2 ≥ y, etc.) are obtained by k -means clustering, as implemented within RealKD. That is, for a desired number n = k − 1 of cutoff values a set of k representative values of a given feature and k groups (clusters) of the data points are determined that minimize the deviation of all the feature values from the representative values. Thus, each value of the feature in the dataset is assigned to a particular cluster, and the cutoffs are determined as the arithmetic mean between the closest feature values in neighboring clusters. The number k is a parameter, and different k -values can in principle result in different cutoff values. It is worth noting that, due to the stochastic Monte-Carlo sampling, the exact definitions of the subgroups may vary for consecutive runs of the SGD algorithm. We have tested k = 12, 14, and 16 and rerun the algorithm several times for each k. While the results indeed depend on the run and on the k value, the subgroups maximizing the quality function have largely or entirely overlapping populations, and selectors with the same or similar propositions. Here we report selectors that appear most often and have high population and quality function values.

---

### Identifying domains of applicability of machine learning models for materials science [^196a563d]. Nature Communications (2020). High credibility.

Fig. 1
Domains of applicability of three 2d-models of a noisy third-degree polynomial.

Three different models, linear (top), radial basis function (rbf, center), and polynomial (poly, bottom), are shown approximating the same distribution of two independent features x 1 ~ N (0, 2) and x 2 ~ N (0, 2), and the target property, where N (μ, ϵ 2) denotes a normal distribution with mean μ and standard deviation ϵ. Test points are plotted in 3d plots against the prediction surface of the models (color corresponds to absolute error) where the DA is highlighted in gray. The distributions of individual errors for the DA (gray) and globally (black) are shown in the 2d plots of each panel with the mean error (solid) and the 95th percentile (95 perc./dashed) marked by vertical lines. Note that the global error distribution of the linear model has a considerably long tail, which is capped in the image.

---

### Identifying domains of applicability of machine learning models for materials science [^3de76373]. Nature Communications (2020). High credibility.

Table 1
Features used for discovery of domain of applicability (DA) selectors.

The DA optimization and validation can be performed as a by-product from the labels and ML predictions of the test set. However, just as for the ML-model fitting itself, we can only estimate these quantities based on empirical data. For that purpose, it is sensible to also split the test data into two parts: a DA identification set for optimizing the empirical impact and a DA validation set for obtaining an unbiased performance estimate of the identified DA (see Fig. 2 for an illustration of the overall workflow). Technically, the data points withheld in the DA validation set mimic novel independent sample points that can be used to evaluate both the coverage of the DA, as well as, the reduction in model error. As an extension of this, one can also repeat the DA optimization/validation on several splits (cross-validation) to reduce the variance of the coverage and model error estimates and, moreover, to assess the stability of the DA selector elements.

Fig. 2
Workflow for domain of applicability (DA) identification and validation for an ML model.

The DA is described by a selector (σ f) that is comprised of logical conjunctions of a representation space (here symbolized by a single dimension x for simplicity but may be multidimensional). The selector is identified by applying subgroup discovery (SGD) to the individual ML-model errors for subset of test set (DA identification set). An unbiased estimate of the model performance within the DA is obtained on the remaining samples of the test set that were left out of the DA identification (DA validation set).

For ease of notation we assume the DA identification set consists of the first k points of the test set. We end up with the following objective function for the SGD algorithm:where s denotes the number of points in the DA identification set selected by σ and I (σ) = { i: 1 ≤ i ≤ k, σ (x i) = true} denotes the set of selected indices itself. Here, we focus on DA identification based on the relative error, as it is less correlated with the target values than the absolute error. Thus, this choice promotes DAs that contain a representative distribution of target values and, by extension, more distinct and thus more characteristic DAs for the different models (see Supplementary Note 2 for a discussion of the DAs resulting from using the absolute error).

---

### In-memory ferroelectric differentiator [^d3a25719]. Nature Communications (2025). High credibility.

Calculating the derivative of mathematical functions

Based on the above mechanism, we can calculate the derivative of mathematical functions. Figure 3a sketches how the domains configurations of 14 ferroelectric capacitors are used to store digit data from −7 to 7. As a result, the value of the function f (x) = − x can be presented by the evolution of the domain configuration sequence over x. The upward (downward) domains are written by +20 V (−20 V) voltage pulses (0.1 s) on bottom electrode while the top electrodes are grounded. Figure 3b proposes the strategy for calculating the differential value by collecting the charges due to ferroelectric domains switching. The upward (downward) domain switching is labeled purple and green, respectively. Note that this strategy requires a linear relationship between the sum of charges and the number of units performing domain switching. The evolution of transient currents and integral charges as a function of time are systematically readout during the inputting process of programing voltage pulses. Either in the case where the number of capacitors with upward domain switching is increased monotonically from 1 to 14 in steps of 1 (Fig. 3c), or in the case where the number of capacitors with upward domain switching is increased monotonically from 0 to 7 in steps of 1 and then kept constant at 7, while the number of capacitors with downward domain switching first remains constant at 7 and then decreases monotonically from 7 to 0 in steps of 1 (Fig. 3d), it clearly shows a linear relationship with a slope of 2.85 mC/unit between integral charges and hybrid (upward or downward) domains reversals. This linear relationship allows the analogue calculation of the differential value (B-A) between data A and data B, by evaluating the value of integral charges over 2.85 mC when it alters domains configuration from data A to data B.

---

### Identifying domains of applicability of machine learning models for materials science [^9c5f816f]. Nature Communications (2020). High credibility.

A predictive ML model is then a functionaiming to minimize the expected error (also called prediction risk)measured by some non-negative loss function l that quantifies the cost incurred by predicting the actual property value y with f (x). Examples for loss functions are the squared error, the absolute error, and, for non-zero properties, the relative error. Here P denotes some fixed probability distribution that captures how candidate materials are assumed to be sampled from the materials class (this concept, while commonly assumed in ML, is an unnecessary restriction for high-throughput screening as we discuss in more detail below). Since the true prediction risk is impossible to compute directly without perfect knowledge of the investigated materials class, models are evaluated by the test error (or empirical risk)defined as the average of the individual errors (losses) e i (f) = l (f (x i), y i) on some test set of m reference data points. The samples in this test set are drawn independently and identically distributed according to P and are also independent of the model — which means in practice that it is a random subset of all available reference data that has been withheld from the ML algorithm. In order to reduce the variance of this estimate, a common strategy is cross-validation, where this process is repeated multiple times based on partitioning the data into a number of non-overlapping "folds" and then to use each of these folds as test sets and the remaining data as a training set to fit the model.

This test error properly estimates the model performance globally over the whole representation space X (weighted by the distribution P used to generate the test points). This is an appropriate evaluation metric for selecting a model that is required to work well on average for arbitrary new input materials that are sampled according to the same distribution P. This is, however, not the condition of high-throughput screening. Here, rather than being presented with random inputs, we can decide which candidate materials to screen next. This observation leads to the central idea enabled by the DA analysis proposed in this work: if the employed model is particularly applicable in a specific subdomain of the materials class, and if that subdomain has a simple and interpretable shape that permits to generate new materials from it, then we can directly focus the screening there.

---

### Clinical utility of polygenic risk scores for embryo selection: a points to consider statement of the American College of Medical Genetics and genomics (ACMG) [^9ccd284a]. Genetics in Medicine (2024). High credibility.

PGT-P results communication — Figure 3 illustrates using 3 components (percentile rank, relative risk, and absolute risk): families (a) and (b) have embryos at rank extremes while family (c) shows a broader range; two families display different relative risk ranges, with family x values 0.9, 1.0, 1.1, 1.2 and family y values 0.6, 0.9, 1.1, 1.4; example absolute risk increases span 1.0% - 1.4% versus 10.0% - 14.0% for diseases differing by 10x population prevalence.

---

### Realization and topological properties of third-order exceptional lines embedded in exceptional surfaces [^d357310a]. Nature Communications (2023). High credibility.

As the counterpart of Hermitian nodal structures, the geometry formed by exceptional points (EPs), such as exceptional lines (ELs), entails intriguing spectral topology. We report the experimental realization of order-3 exceptional lines (EL3) that are entirely embedded in order-2 exceptional surfaces (ES2) in a three-dimensional periodic synthetic momentum space. The EL3 and the concomitant ES2, together with the topology of the underlying space, prohibit the evaluation of their topology in the eigenvalue manifold by prevailing topological characterization methods. We use a winding number associated with the resultants of the Hamiltonian. This resultant winding number can be chosen to detect only the EL3 but ignores the ES2, allowing the diagnosis of the topological currents carried by the EL3, which enables the prediction of their evolution under perturbations. We further reveal the connection between the intersection multiplicity of the resultants and the winding of the resultant field around the EPs and generalize the approach for detecting and topologically characterizing higher-order EPs. Our work exemplifies the unprecedented topology of higher-order exceptional geometries and may inspire new non-Hermitian topological applications.

---

### Microfluidic multipoles theory and applications [^31dd0050]. Nature Communications (2019). High credibility.

Upon inspection, the problem can be transformed to streamline coordinates (Fig. 2a) using the function

In the streamline domain Φ, the problem is equivalent to a channel geometry with flows of concentration c = 1 and c = 0 separated by a no-flux boundary condition on the origin. At the stagnation point, the no-flux condition is dropped, and the flows are free to mix (Fig. 2a) (see Supplementary Table 2 for more details on the streamline problem). The separating streamline going from the stagnation point to the aspiration aperture corresponds to the semi-infinite segment of the horizontal axis where the fluids can mix. If the Péclet number is high enough (higher than about 10, which is always realized in microfluidics applications), this segment can be taken to have concentration c = 1/2 and the walls of the channel geometry can be safely ignored (Supplementary Note 1). The problem can thus be decomposed in two problems of advection-diffusion around semi-infinite obstacles of fixed concentration. The problem of advection-diffusion around such a semi-infinite obstacle has been extensively studied in theoretical fluid mechanics, notably in the theory of dendrite solidification, and in the study of out of plane flow in Burgers vortex sheets. It yields the solutionwhere Φ stag is the image of the stagnation point and erf(x) is the error function. The sign of ± is determined by whether we have an incoming flow of concentration c = 0 or c = 1. However, neither of these concentration profiles represent the full dipole footprint when transformed. This can be seen physically in the flow dipole, in which there is both incoming fluid at concentration 0 (aspirated from the system's surroundings), and incoming fluid at concentration 1 (injected by the aperture). To solve this issue, we separate the problem into an "interior" and an "exterior" domain at the streamline of concentration c = 1/2 (see checkerboard insets in Fig. 2). There remains a discontinuity in our solution due to the branch cut of the logarithm functions in Eq. (1), but the solution can be made continuous by placing the singularities on the real axis and using it as an axis of symmetry. The final step is then to obtain the entire solution as a piecewise function assembling the "interior" and "exterior" solutions, given by transforming Eq. (6) back to the dipole flow domain Z. The interior and exterior domains can be defined either by checking the sign of Φ in the streamline domain or by using the expression for the separating line in the Z domain in polar coordinates (see Supplementary Note 2).

---

### Reverberant 3D optical coherence elastography maps the elasticity of individual corneal layers [^b12ee4eb]. Nature Communications (2019). High credibility.

Methods

Monte Carlo analysis

A 20 × 20 × 20 mm 3 field (Δ x = Δ y = Δ z = 0.1 mm sampling resolution) in which transversal waves can only propagate at a constant phase speed of c s = 4 m s −1 is generated in MATLAB (The MathWorks, Inc. Natick, MA, USA). Inside the summation of Eq. (1), a single plane and shear wave is defined by, and v ql, with the realization of four uniformly distributed random variables. First, the realization of angles φ and θ, each one covering a range of [0, 2π] radians, for the definition of, followed by the realization of angle α, covering a range of [0, 2π] radians for the definition of, and then the realization of the scalar value v ql, covering a range of [− v max, v max], where v max = 1 m s −1. The termcan be factored from Eq. (1), and the wave number is defined as constant k = ω 0 / c s in the whole field with the selection of f 0 = 2 kHz as the temporal frequency, leading to ω 0 = 2π f 0. A total of 10,000 realizations of all random variables, following the described order, generates a spatial and complex-valued particle velocity vector field V (ε), where. Then, the three components of V (ε) are: and, where, andare unit vectors in the x, y, z directions, respectively. Since in OCT the motion is measured in the z -axis, is the spatial field of interest. Figure 1b shows the real part of, defined as, for an arbitrary instant t 0. Finally, the real part of the complex-valued 2D auto-correlation maps taken fromare calculated in the following planes: (1) xy -plane, (2) xz -plane, and (3) yz -plane as shown in Fig. 1c. Profile plots taken along each axis (Δ x, Δ y, Δ z) are compared to Eqs. (2) and (3) for further validation.

---

### Optimal coding and neuronal adaptation in economic decisions [^48797010]. Nature Communications (2017). Medium credibility.

The results of these analyses provided a classification for neuronal responses. Specifically, each neuronal response was assigned to the variable that explained the response and provided the highest R 2. Thus we identified 447 offer value, 370 chosen value, and 268 chosen juice responses. Subsequent analyses of the same data set demonstrated range adaptationand quantified noise correlations.

Previous work suggested that the encoding of value in OFC was close to linear. In this study, we conducted more detailed analyses to quantify how neuronal responses departed from linearity. Furthermore, we compared the curvature measured in neuronal responses with that measured for the cumulative distribution of the encoded values. The cumulative distribution of encoded values was taken as a benchmark by analogy with sensory systems. For each offer value response, we identified the value levels present in the session (i.e. the unique values). We then calculated the corresponding number of trials, divided it by the total number of trials in the session, and computed the corresponding cumulative distribution function (ntrials CDF). For each value level, we also averaged the firing rates obtained across trials. The range of offered values and the range of firing rates varied considerably from session to session and across the population (Supplementary Fig. 3). Thus to compare the results obtained for different responses, we normalized offer value levels, neuronal firing rates, and ntrials CDF. Value levels were simply divided by the maximum value present in the session. For example, normalized values for juice B were defined as q B / Q B. For neuronal firing rates, we performed the linear regression y = a 0 + a 1 x, where x are normalized value levels. Firing rates were thus normalized with the transformation fr → (fr−a 0)/a 1. Similarly, for ntrials CDF, we performed the linear regression y = b 0 + b 1 x, and we normalized them with the transformation ntrials CDF → (ntrials CDF −b 0)/b 1. Examples of normalized firing rates and normalized ntrials CDF are illustrated in Fig. 1d, g. To estimate the overall curvature, we fit each normalized response function with a 2D polynomial and compared the quadratic coefficient (β 2) with that obtained from fitting the corresponding normalized ntrials CDF. To estimate the overall S shape, we fit the normalized response function with a 3D polynomial and compared the cubic coefficient (β 3) with that obtained for the corresponding normalized ntrials CDF. These measures were used for the population of offer value responses (Fig. 2). Separately, we repeated these analyses for the population of chosen value responses (Supplementary Fig. 2).

---

### A random-walk benchmark for single-electron circuits [^3b516153]. Nature Communications (2021). High credibility.

The random-walk benchmarking addresses the question of uniformity in time of repeated identical operations by error accumulation. The error signal (syndrome) considered here is the discrete charge stored in the circuit after executing a sequence of t operations. The measured deviation x in the number of trapped electrons is modeled by the probabilityfor a random walker to reach integer coordinate x from initial position of x = 0 in t steps (Fig. 1). In the desired high-fidelity limit of near-deterministic on-demand transfer of a fixed number of electrons any residual randomly occurring errors that alter x will be very rare and the walker will remain stationary most of the time, with occasional steps of length one. Here we study to what extent two single-step, x → x ± 1, probabilities P ± describe the statistics of x collected by repeated operation of the circuit, and how deviations from independent-error accumulation can be detected and quantified, revealing otherwise hidden physics. The baseline random-walk model with t - and x -independent P ± predicts the following distribution:withobtained from Eq. (1) by x → − x and P ± → P ∓ (see derivation in Supplementary Note 1). Here the first term of the product describes the decay of fidelity that is exponential in t, while the binomial coefficient and the Gaussian hypergeometric function 2 F 1 (here a polynomial of order at most t) take into account the self-intersecting paths as single-step errors accumulate and partially cancel at large t (Fig. 1 a).

Fig. 1
Schematic of the random-walk benchmark for single-electron transfer utilizing charge counting.

a Sample micrograph and measurement scheme. After the initial charge measurement t clock cycles are applied. The paths taken by 30 simulated walkers (using error rates extracted from the counting statistics) are represented by blue lines, transitioning every clock cycle in x by a step of −1, 0, +1. The frequency with which each branch is visited is indicated by the linewidth. A final charge measurement yields the end-point of the random walk as the difference between initial and final charge. The orange line exemplifies a single random walk with self-intersections. b Signal to noise ratio: a (typical) histogram of the differential charge detection signal with the identified difference in electron number indicated by color. The peak separation is shown in units of the Gaussian noise amplitude σ (black dashed lines indicate the corresponding Gaussian fits). c Measured statistics of finding the walker at position x after t steps.

---

### ACC / AHA statement on cost / value methodology in clinical practice guidelines and performance measures: a report of the American college of cardiology / American Heart Association task force on performance measures and task force on practice guidelines [^cd9422c7]. Circulation (2014). Medium credibility.

ACC/AHA level of value thresholds for guideline recommendations — Table 2 proposes quantifiable incremental cost-effectiveness ratio (ICER) cutoffs tied to quality-adjusted life-year (QALY) gains: High value when ICER < $50 000 per QALY gained; Intermediate value when $50 000 to < $150 000 per QALY gained; Low value when ≥ $150 000 per QALY gained; Uncertain value when evidence is insufficient; and Not assessed when value was not evaluated by the committee. Proposed abbreviations for value levels are H, I, L, U, and NA.

---

### Model-free inference of direct network interactions from nonlinear collective dynamics [^281c378a]. Nature Communications (2017). Medium credibility.

The resulting generic model (Eq. (4)) links state space points x (t) at time t to their rate of change. In particular, the complemented system stateis an element of a higher-dimensional "dynamics space"for each i formed by the state space and the rate of change of unit i. Therefore, the f i specifying the dynamics defines a smooth manifold, with theindicating whether or notis constant in direction x j. cf. Fig. 1.

Fig. 1
Sampling dynamics spaces to reveal network structure. a Eq. (4) determines a mapping from state space components x to rates of change, defining a smooth manifold(in dynamics space) determined by f i and Λ i. b Two-stage decomposition of unknown functions f i, first into interactions of different orders with other units in the network, second each interaction into basis functions, thereby resulting in a linear system (Eq. (6)) restricting the connectivity structure. c Quality of inferences versus the number M of measurements (displayed here for non-periodic dynamics of networks of phase-coupled oscillators of N = 20 and n i = 10 incoming connections per unit). "Continuous sampling" takes as observed dynamics one long trajectory of M sample time steps; "distributed sampling" takes M / m short time series of m = 10 sample time steps starting from different initial conditions, creating a longer time series of length M. The initial conditions were randomly drawn from uniform distributions defined in the interval

In practical scenarios, the functions f i are generally not accessible. We address this challenge in two stages. First, we functionally decompose the dynamics of units i ∈ {1, 2. N } into interaction terms with the entire network aswhere, and, in general, represent the (unknown) K -th order interactions between units j k for all k ∈ {1, 2, …, K } and unit i. Specifically, the decomposition (Eq. (5)) separates contributions to unit i arising from different orders, e.g. pairwise and higher-order interactions with other units in the system. The Λ i are defined such that, if, all functionswith any of the indices j k = r disappear from the right hand side of Eq. (5).

---

### Input-output maps are strongly biased towards simple outputs [^50fca0dd]. Nature Communications (2018). Medium credibility.

Many systems in nature can be described using discrete input-output maps. Without knowing details about a map, there may seem to be no a priori reason to expect that a randomly chosen input would be more likely to generate one output over another. Here, by extending fundamental results from algorithmic information theory, we show instead that for many real-world maps, the a priori probability P(x) that randomly sampled inputs generate a particular output x decays exponentially with the approximate Kolmogorov complexity [Formula: see text] of that output. These input-output maps are biased towards simplicity. We derive an upper bound P(x)≲[Formula: see text], which is tight for most inputs. The constants a and b, as well as many properties of P(x), can be predicted with minimal knowledge of the map. We explore this strong bias towards simple outputs in systems ranging from the folding of RNA secondary structures to systems of coupled ordinary differential equations to a stochastic financial trading model.

---

### ACC / AHA statement on cost / value methodology in clinical practice guidelines and performance measures: a report of the American college of cardiology / American Heart Association task force on performance measures and task force on practice guidelines [^dcb8e5e6]. Circulation (2014). Medium credibility.

WHO‑CHOICE framework adaptation — The committee anchored value definitions to WHO‑CHOICE thresholds, defining highly cost‑effective as less than GDP per capita, cost‑effective as between 1 and 3 times GDP per capita, and not cost‑effective as > 3 times GDP per capita, and selected the Table 2 values as initial threshold recommendations that may need modification as future information or consensus develops.

---

### Cohesin prevents cross-domain gene coactivation [^c3b69488]. Nature Genetics (2024). High credibility.

Fig. 3
Cohesin prevents gene co-bursting along the chromosome.

a, Active genes (208) across Chr 2 in mouse ES cells are targeted by 30 intron probes per gene. Primary probes for each gene contain four unique docking sites, and each docking site can be hybridized to a readout probe corresponding to nine pseudocolors in each readout round. The identity of each bursting gene can be determined according to the barcode after four readout (RO) rounds. The barcode is designed that the gene can be decoded even with one round of drop out. b, One representative cell with decoded genes (color coded). Composite FISH signals (green) were reconstructed by averaging images from all FISH channels from three hybridizations in RO1. The 3D image was rendered by using VVD-viewer; scale bar, 2 μm. c, Heat map of pairwise differential co-bursting frequencies (top) and differential distances (bottom) for 208 genes in Chr 2. The matrix was calculated by subtracting values in the control condition (3 h) from those in the cohesin loss condition. Zoom-in views of the heat map and representative images for three differentially coactivated genes (red, green and blue color coded) after cohesin loss are presented below; scale bar, 2 μm. d, Bursting frequencies for 208 genes (represented by circles) under control and cohesin loss (3 h) conditions. The bursting frequency per gene is calculated by dividing the total number of detected bursting sites (approximately zero to four in a cell) for the gene by the number of cells (N) analyzed under each condition (see Eq. (5)). The red line marks the diagonal y = x. e, f, Box plot of pairwise differential co-bursting frequencies (e) and differential distances (f) for the indicated conditions. A non-parametric two-sided Wilcoxon test was used for statistical testing. The bottom and top whiskers represent 10% and 90% values, respectively. The box represents the range from the 25th percentile to the 75th percentile, the center line represents the median, and the dotted line indicates the zero-change line. The statistics were derived from 21,528 gene pairs with quantifiable values.

---

### Retrieving functional pathways of biomolecules from single-particle snapshots [^3d97af8f]. Nature Communications (2020). High credibility.

Functional paths vs. interpolation between discrete clusters

The curved nature of the route to binding (the white line connecting START to FINISH in Fig. 1a) shows that conformational motions relevant to binding involve an elaborate and changing admixture of conformational coordinates, which cannot be determined from the START and FINISH structures alone. This underscores the difficulty of deducing functional information from discrete structures.

The experimentally determined energy landscapes make it possible to compile molecular movies along functional paths, and compare the results with the changes inferred by maximum-likelihood clusteringof the same snapshots. The RyR1 data analyzed here were previously clustered with RELION 3D classification into 16 discrete conformational classes, 2 of which were labeled as "junk". For each class, the positions and distribution of the snapshots on the energy landscapes are shown in Fig. 6a and Supplementary Fig. 6. It is difficult to discern a systematic relationship between the positions of the different discrete classes on the energy landscapes. Class 2 (no ligands) and class 3 (with ligands) were taken in the previous cluster-based analysis, to represent the functionally relevant extremes of the conformational range. Functional information was then inferred by interpolating between the 3D structures obtained from these two classes. Below we compare the conformational changes deduced from such interpolation with the changes associated with the functional trajectory on the energy landscapes (Fig. 1a).

Fig. 6
Functional path vs. interpolation between discrete clusters.

a Distribution on the energy landscape of Fig. 1a of snapshots assigned by maximum likelihood to two discrete conformational clusters. Each white curve encloses a region of high snapshot density. These two classes were previously identified as representing the extremes of the conformational range, and used to infer function by discrete cluster analysis. b Overall conformational changes along the functional path vs. linear interpolation between the two maximum-likelihood classes previously used for functional inference(see panel a). The ryanodine receptor 1 (RyR1) atomic model is shown in sphere representation, with color indicating the displacement of each atom from its "START" position. c Conformational changes in the activation core domain (residues: 3614–5037) along the functional path of Fig. 1a. The RyR1 backbone structure is shown as sticks, with the color of each atom indicating its atomic displacement (see color bar). The largest atomic displacements along the functional trajectory are confined to a narrow allosteric conduit connecting the ligand-binding sites to the EF hand.

---

### Single photon emission computed tomography (SPECT) myocardial perfusion imaging guidelines: instrumentation, acquisition, processing, and interpretation [^8dfef17d]. Journal of Nuclear Cardiology (2018). Medium credibility.

Reconstruction — filtered backprojection (FBP) is presented as the traditional reconstruction method, assuming perfect line integral count profiles, no attenuation, no scatter, and an infinite number of projections; simple backprojection produces blurring described by 1/r in the Fourier domain, which is suppressed by applying a ramp filter before backprojection, but the ramp filter amplifies high‑frequency noise and can introduce a ramp‑filter artifact with subdiaphragmatic tracer concentration that subtracts counts from the inferior wall in the x‑plane of the heart.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^6517cbfd]. Annals of the American Thoracic Society (2023). High credibility.

Basic stereological measurements — first-order parameters and probes identify that "First-order parameters are volume (three-dimensional [3D]), surface (two-dimensional [2D]), length (one-dimensional [1D]), and number (zero-dimensional [0D])". Measurements "are performed using geometric test probes, such as points (0D), lines (1D), planes (2D), or volumes (3D)". These probes create countable events in the image, and "Raw counts provide ratios… that are multiplied by the reference space volume to obtain absolute measures for the lung or subcompartment".

---

### Validation of monte carlo estimates of three-class ideal observer operating points for normal data [^26b6e5d9]. Academic Radiology (2013). Low credibility.

Rationale and Objectives

Traditional two-class receiver operating characteristic (ROC) analysis is inadequate for the complete evaluation of observer performance in tasks with more than two classes.

Materials and Methods

Here, a Monte Carlo estimation method for operating point coordinates on a three-class ROC surface is developed and compared with analytically calculated coordinates in two special cases: (1) univariate and (2) restricted bivariate trinormal underlying data.

Results

In both cases, the statistical estimates were found to be good in the sense that the analytical values lay within the 95% confidence interval of the estimated values about 95% of the time.

Conclusions

The statistical estimation method should be key in the development of a pragmatic performance metric for evaluation of observers in classification tasks with three or more classes.

---

### Developmental and housekeeping transcriptional programs display distinct modes of enhancer-enhancer cooperativity in drosophila [^06e1588a]. Nature Communications (2024). High credibility.

Fig. 2
Developmental enhancers combine multiplicatively.

a Schematic illustration of an additive model (where the number of RNA molecules produced by each enhancer add up) and a multiplicative model (where the number of RNAs combine multiplicatively). b Scatterplots showing predicted activities (x -axis) based on an additive (left) or a multiplicative model (right) versus observed activities (y -axis), with corresponding R -squared (R 2) values. Enhancer pairs in which both candidate sequences are active are highlighted using density lines (in orange), and dotted lines correspond to identity lines (y = x). c Fraction of enhancer pairs (in which both candidate sequences are active) for which the additive (in white) or the multiplicative (in blue) predicted values were the most accurate. d Fitted multiplicative model with interaction term using the 5' and 3' individual activities to predict the activities of enhancer pairs, with the corresponding adjusted R 2 value (top left). Enhancer pairs in which both candidate sequences are active are highlighted using density lines (in orange). Dotted lines correspond to the identity line (y = x). Fitted coefficients and resulting equation are shown at the bottom. e Box plots showing, for all enhancer pairs, predicted values using the three different models and observed values (x -axis). Lines connect the values for a representative set of pairs, spanning the dynamic range of observed activities. n = 517,071 pairs per box plot; box plots show the median (line), upper and lower quartiles (box) ± 1.5× interquartile range (whiskers), outliers are not shown. f, g Predicted additive (x -axis) versus observed combined activities for ecdysone-inducible enhancers and OSC-specific enhancers (in red) in ecdysone-treated S2 cells (f) or OSC cells (g). As a reference, a subset of pairs containing enhancers that were also active in S2 cells are shown (see color legend). Dotted lines depict the identity lines (where observed combined activities equal expected additive outcomes), and solid lines represent fitted multiplicative models.

---

### Using both qualitative and quantitative data in parameter identification for systems biology models [^a956cd8d]. Nature Communications (2018). Medium credibility.

We must choose the penalty constants C i in Eq. (3) based on our confidence in the qualitative data — a larger C i gives the qualitative data more weight compared to the quantitative data. We hand-selected a value of C i = 0.03 for all i, to give roughly equal contributions from the qualitative and quantitative datasets (i.e. such that the blue and orange curves in Fig. 2d, e can be plotted using the same scale). Note that giving equal contributions to both datasets is an arbitrary choice for illustration — with real experimental data, the modeler may choose to give unequal weights based on the relative importance/credibility of the datasets.

We minimized f tot (x) by differential evolution, and found best-fit parameters of K 3 = 5100 μM −1 and K 5 = 0.060 μM −1, which are reasonably close to the ground truth.

To evaluate the strengths of combining quantitative and qualitative data, we performed uncertainty quantification. We used a variant of the profile likelihood approach, a method that is well-established for quantitative fitting. One parameter of interest is held fixed, and the objective function is minimized by varying the remaining parameters. The resulting minimum is taken as the negative log likelihood of the fixed parameter value. The minimization is repeated for many possible fixed values of the parameter to produce a curve (Fig. 2d, e). Note that our objective function is not a likelihood in the rigorous sense, but has a similar interpretation in that a lower objective value indicates the parameter value is more consistent with the data.

We performed profile likelihood analysis using f quant (x), f qual (x), and f tot (x) in turn as the objective function, and considering two free parameters K 3 and K 5 (Fig. 2d, e). We found that each dataset individually provided bounds on possible parameter values, but the tightest bounds were obtained when we combined both datasets.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^bb9b7f12]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Δ Φ, the probability distribution at a point Φ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Φ ∈ {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters α and β, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since α and β are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Φ ∈ [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(α, β) and x 2 = 1 − x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Φ, it may seem natural to select α and β by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### Neural substrates of cognitive biases during probabilistic inference [^5bc1690b]. Nature Communications (2016). Medium credibility.

Models parameters

For simulations shown in Figures 6 and 7 we used a wide range of parameters to show the robustness of our results and capture inter-subject variability. For those simulations, the model's behaviour was measured over 100,000 simulated trials to accurately capture the average behaviour for a given set of parameters. For simulations of the original model (Fig. 7a–c), we sampled all combinations of parameters (total four parameters) from the following values with the constraint that q p < 2.5 q d: q p = [0.02, 0.04, 0.06, 0.08, 0.10]; q d = [0.02, 0.04, 0.06, 0.08, 0.10, 0.12]; σ D = [0.05, 0.10, 0.20, 0.30]; σ E = [0.10, 0.20, 0.30]. We limited the range to the above values to avoid extreme biases towards the more rewarding option during the choice session, which occurs when the learning rates are greater than 0.12, or when q p and q d are comparable in magnitude. For simulations of the original model with the additional concave f-I response function for the value-encoding neurons (Fig. 7d–f), we used an f-I curve function that captures the average behavioural data and sampled all combinations of other parameters (total seven parameters) from the following values: q p = [0.02, 0.04, 0.06]; q d = [0.04, 0.06, 0.08, 0.10, 0.12]; σ D = [0.05, 0.10, 0.20, 0.30]; and σ E = [0.05, 0.10, 0.15]. For simulations of the model which has all components of the full model except the modulation of the potentiation rate by reward expectation (Fig. 7g–i), we sampled all combinations of parameters (total eight parameters) from the following values: q p = [0.02, 0.04, 0.06]; q d = [0.04, 0.06, 0.08, 0.10, 0.12]; σ D = [0.05, 0.10, 0.20, 0.30]; σ E = [0.05, 0.10, 0.15]; and p ign = [0.2, 0.4, 0.6, 0.8]. For simulations of the complete model, we fixed q r at 0.1 and sampled all combinations of parameters (total ten parameters) from the following values: q p = [0.02, 0.04, 0.06]; q d = [0.04, 0.06, 0.08, 0.10, 0.12]; σ D = [0.05, 0.15, 0.25]; σ E = [0.05, 0.10, 0.15]; p ign = [0.3, 0.6]; and σ r = [0.1, 0.2, 0.3, 0.4, 0.5].

---

### Sas-100 kb… [^918f6c00]. wwwn.cdc.gov (2025). Medium credibility.

VALUE CH0137F 1 = 'Yes' 2 = 'No' 8 = 'Blank, but applicable'; VALUE CH0139F 8888 = 'Blank, but applicable'; VALUE CH0143F 1 = 'Yes' 2 = 'No' 3 = 'Loss' 8 = 'Blank, but applicable';… VALUE CH0207F 1 = 'Farm' 2 = 'Non-Farm'; VALUE CH0208F 1 = 'Interviewed'; VALUE CH0209F 1 = 'Northeast' 2 = 'Midwest' 3 = 'South' 4 = 'West';… VALUE CH0470F 1 = 'Yes' 2 = 'No' 9 = 'Dont know'; VALUE CH0471F 1 = 'Yes' 2 = 'No'; VALUE CH0472F 1 = 'Yes' 2 = 'No' 9 = 'Dont know';… VALUE CH0501F 1 = 'Yes' 2 = 'No' 9 = 'Dont know'; VALUE CH0502F 1 = 'Yes' 2 = 'No' 9 = 'Dont know'; VALUE CH0503F 1 = 'Yes' 2 = 'No' 8 = 'Blank, but applicable' 9 = 'Dont know';… VALUE CH0543F 1 = 'Yes' 2 = 'No' 8 = 'Blank, but applicable'; VALUE CH0544F 1 = 'Yes in 23a or 24a' 2 = 'All other'; VALUE CH0545F 1 = 'Yes' 2 = 'No' 8 = 'Blank, but applicable';… VALUE CH0552F 1 = 'Yes' 2 = 'No' 8 = 'Blank, but applicable'; VALUE CH0553F 1 = 'Yes' 2 = 'No' 8 = 'Blank, but applicable'; VALUE CH0554F 1 = 'Yes' 2 = 'No' 8 = 'Blank, but applicable';

---

### Platform-directed allostery and quaternary structure dynamics of SAMHD1 catalysis [^7d93bd11]. Nature Communications (2024). High credibility.

To assess the degree of disorder and quantify the magnitude and direction of the quaternary conformational changes at each state, we examined the inter- and intra-chain packing within each tetramer. For each chain and isolated regulatory domain, we determined the amount of buried surface resulting from the interaction with the corresponding Dimer-2 mate and also determined the amount of intrachain buried surface for each regulatory domain with its corresponding catalytic domain (Supplementary Table 7). Moreover, for each state of the A-C Dimer-2 we also calculated the radius of gyration (R g) and pair-distribution function, P(r), for all interatomic distances within regulatory domain pairs that reports on the compaction and spacing of the regulatory lobes (Fig. 4b–f). Inspection of the packing of each regulatory domain onto its cognate catalytic domain shows that in the relaxed A-C side it decreases from an average value of 1500 Å 2 in State I, II and III to 1064 Å 2 in State-IV and is further reduced to 772 Å in State-V. By contrast, the buried area on the tense side remains at an average value of 1500 Å across all states (Supplementary Fig. 15 and Supplementary Table 7). The calculation for the interchain packing of entire chains and just the regulatory domains against the Dimer-2 mate is even more telling, where in Tense B-D pairs the regulatory domain packing maintains average values across all states from 710 to 602 Å 2, whilst in relaxed A-C Dimer-2 pairs this value reduces from 722 and 553 Å 2 in State-I and -II to a value of zero resulting from the total uncoupling observed in States III, IV and V (Supplementary Table 7). The same effect is also apparent on inspection of the entire interchain packing where the Tense B-D Dimer-2 maintains an average buried surface area across all states of 1164 Å 2 but in the relaxed A-C Dimer-2 it reduces from an average value of 1160 Å 2 in State-I and -II to only 350 Å in State-V. These measurements are further supported by the pair-distribution functions, where for States-I to -V there is only a modest increase in D max of 96 Å to 101 Å relating to an increase in the C-terminal lobe spacing progressing from State-I and State-V (Supplementary Movie 1). However, the functions also display a large very distinct increase in bi-modality that relates directly to the increased degree of disorder of each lobe-linker dimer going from State-I to State-V (Fig. 4). Moreover, this is apparent in the calculated R g which also reports on compaction and increases from 28.6 Å in State-I to 32.6 Å in State-III and 34.6 Å in State-IV and -V as the linkers become disordered, and the C-terminal lobes separate.

---

### Subnanometre single-molecule localization, registration and distance measurements [^3e60a2a9]. Nature (2010). Excellent credibility.

Remarkable progress in optical microscopy has been made in the measurement of nanometre distances. If diffraction blurs the image of a point object into an Airy disk with a root-mean-squared (r.m.s.) size of s = 0.44lambda/2NA (approximately 90 nm for light with a wavelength of lambda = 600 nm and an objective lens with a numerical aperture of NA = 1.49), limiting the resolution of the far-field microscope in use to d = 2.4s approximately = 200 nm, additional knowledge about the specimen can be used to great advantage. For example, if the source is known to be two spatially resolved fluorescent molecules, the distance between them is given by the separation of the centres of the two fluorescence images. In high-resolution microwave and optical spectroscopy, there are numerous examples where the line centre is determined with a precision of less than 10(-6) of the linewidth. In contrast, in biological applications the brightest single fluorescent emitters can be detected with a signal-to-noise ratio of approximately 100, limiting the centroid localization precision to s(loc) ≥ 1% (≥ 1 nm) of the r.m.s. size, s, of the microscope point spread function (PSF). Moreover, the error in co-localizing two or more single emitters is notably worse, remaining greater than 5–10% (5–10 nm) of the PSF size. Here we report a distance resolution of s(reg) = 0.50 nm (1sigma) and an absolute accuracy of s(distance) = 0.77 nm (1sigma) in a measurement of the separation between differently coloured fluorescent molecules using conventional far-field fluorescence imaging in physiological buffer conditions. The statistical uncertainty in the mean for an ensemble of identical single-molecule samples is limited only by the total number of collected photons, to s(loc) approximately 0.3 nm, which is approximately 3 x 10(-3) times the size of the optical PSF. Our method may also be used to improve the resolution of many subwavelength, far-field imaging methods such as those based on co-localization of molecules that are stochastically switched on in space. The improved resolution will allow the structure of large, multisubunit biological complexes in biologically relevant environments to be deciphered at the single-molecule level.

---

### Learning shapes cortical dynamics to enhance integration of relevant sensory input [^5f42e6ef]. Neuron (2023). Medium credibility.

Non-normal dynamics (Figure S1)

We derived expressions relating linear Fisher Information to the dynamics of an arbitrary normal or non-normal network (subject to the same approximations described above). These expressions had a simple and interpretable form in three special cases: two-dimensional networks, normal networks, and non-normal networks with strong functionally-feedforward dynamics. Related findings have been presented previously.

To illustrate our analytical findings for the two-dimensional case, we constructed networks with modes m 1 = [cos θ 1; sin θ 1], m 2 = [cos θ 2; sin θ 2]. Figure S1A was constructed using the same procedure as for Figure 2, but this time with τ 1 = 10, τ 2 = 5. For Figure S1B we chose input with isotropic covariance Σ η = I 2 (where I N is the N x N identity matrix) and Δ g = g (s 2) - g (s 1) = [1; 0]. These inputs were chosen in order to demonstrate the influence of non-normality as clearly as possible. We set τ 1 = 10, τ 2 = 1,5,7.5,9 and varied θ 1, θ 2 from – π /2 to π /2 for each value. For each network (defined by the parameters θ 1, θ 2, τ 1, τ 2 using the procedure described for Figure 2), the Fisher Information of the stationary state network responsewas computed by substituting the long-run solution for the mean Δ r = – A –1 Δ g and the numerical solution to the Lyapunov equation for Σ (described above). We normalized this linear Fisher Information by the maximum achievable SNR in any normal network with the same time constants by defining. For each network, we computed the information-limiting correlations as ρ ILC = Δ r T ΣΔ r /(Δ r T Δ r Trace(Σ)). For each choice of τ 2, we computed the Pearson correlation between the Fisher information and the information-limiting correlations corr(𝓘 F, ρ ILC), where the correlation was computed over a set of networks spanning the range of θ 1, θ 2 ∈ [- π /2, π /2). We computed this correlation for various settings of Σ η = [v 1, v 2] [λ 1, 0; 0, λ 2] [v 1, v 2] T, by varying the angle of its principal eigenvector v 1 from Δ g and the ratio of its two eigenvalues λ 2 / λ 1 with λ 1 = 1 and λ 2 ∈ [0, 1].

---

### SpliceVarDB: a comprehensive database of experimentally validated human splicing variants [^6aa869e9]. American Journal of Human Genetics (2024). Medium credibility.

Of the variants identified, 8,558 variants were validated more than once; 97% were replicates (Figure 1 B), and the remaining 3% were reported by multiple studies (Figure 1 C). Of the variants validated more than once, 83 were classified as conflicting, meaning that at least one validation determined the variant to be splice-altering whereas another determined the variant to be normal. However, concordance between classifications was generally high, with the conflicting category only applying to 1% of replicates and 8% of variants validated using multiple methods. The validation experiments were primarily conducted with cell lines, accounting for 75% of the experimental validation, compared to 25% that utilized clinical tissue samples (Figures 1 D and 1E).

At the time of publication, SpliceVarDB variants covered 8,362 genes. We referred to established online gene-disease databases to assess the intersection between SpliceVarDB genes and genes implicated in clinical conditions. SpliceVarDB encompasses many of these genes, with coverage ranging from 58.4% in OMIM to 67.8% in ClinGen (Figure 2 A). These genes span a broad range of disease categories, as evidenced by their presence across various clinical domains in ClinGen (Figure 2 B). Notably, genes in SpliceVarDB most comprehensively represent the hereditary cancer domain at 85%. In contrast, the pulmonary domain had the lowest representation, with only 43% of its genes included. Furthermore, SpliceVarDB includes several extensively validated genes, with 24 genes featuring over 100 functionally validated variants (Figure 2 C).

Figure 2
Gene information overview for SpliceVarDB

(A) Overlap between genes from online gene databases and SpliceVarDB. Colored bars depict the numbers of genes in the database and SpliceVarDB, whereas dotted bars indicate the full number of genes in the indicated database.

(B) Overlap between genes in ClinGen Clinical Domains and SpliceVarDB. Colored bars depict the numbers of genes in the clinical domain and SpliceVarDB, whereas dotted bars indicate the full number of genes in the domain.

(C) The variant counts for each gene with greater than 100 functionally validated variants. Variants functionally validated to alter splicing are shown in red, while those that showed no splicing alterations compared to controls are shown in blue, and low-frequency splice-altering variants are in purple.

---

### Principled approach to the selection of the embedding dimension of networks [^0e29bd7c]. Nature Communications (2021). High credibility.

There could be several ways of finding the solution of Eq. (1). Here, given the smoothness of the empirically observed loss functions, we opted for a parametric approach. Specifically, we found thatwhere s and α are fitting parameters, represents well the data in various combinations of embedding methods and embedded networks. Equation (2) is meant to describe the mathematical behavior of the loss function only in the range d ∈ [1, d r). In particular in the definition of Eq. (2), L ∞ appears as the value of the normalized loss function in the limit of infinitely large embedding dimensions. However, such a limit has no physical meaning and is used only to facilitate the mathematical description of the empirical loss function in the regime d ≪ d r. Best fits, obtained by minimizing the mean-squared error, of the function of Eq. (2) are shown in Figs. 1 a, b and 2 a. We performed a systematic analysis on a corpus of 135 real-world networks. We found that best estimatesof the asymptotic value L ∞ for the normalized embedding loss function are generally close to zero (Supplementary Table 1). Best estimatesof the factor s, regulating the rate of the power-law decay toward L ∞, seem very similar to each other, irrespective of the network that is actually embedded. Measured values ofindicate only a mild dependence on the underlying network (see Fig. 3). For uncorrelated clouds of points in d dimension, the central limit theorem allows us to predict that L (d) ~ 1/ d 1/2. Our best estimatesof the decay exponent α are generally greater than those expected for uncorrelated clouds of data points, indicating that the embedding algorithm correctly retains network structural information even in high-dimensional space. In systematic analyses performed on random networks constructed using either the ER and the BA models, we find that the size of the network is not an important factor in determining the plateau value L ∞. The decay exponent α and the rate s are positively correlated with density of the embedded network, but their values become constant in the limit of large network sizes (see Fig. 4).

---

### Principled approach to the selection of the embedding dimension of networks [^e899ba67]. Nature Communications (2021). High credibility.

If we repeat the same analysis using the LINE algorithm, although the rate of decay of the function L (d) is not identical for the two embedding algorithms, we find analogous behavior with L (d) smoothly decaying toward an asymptotic value as d increases (Fig. 1 a, b). However, a faster decay doesn't necessarily imply that the corresponding algorithm is able to represent more efficiently the network than the other algorithm. The reference embeddings used in the definition of L (d) are in fact algorithm dependent, and the quantitative comparison between the L (d) functions of two different algorithms may be not informative.

Similar observations can be made when we perform the geometric embedding of synthetic networks. In Fig. 2, we display L (d) for networks generated according to the SB model. We still observe a saturation of the function L (d). Graph clustering reaches maximal performance at slightly smaller d values than those necessary for the saturation of L (d), and slowly decreases afterwards. The drop of performance for values of the embedding dimension that are too large has been already observed in other papers, see for example refs. Results based on LE embedding provide similar insights (Supplementary Figs. 3–6): L (d) saturates quickly as d increases; the performance in the graph clustering task is optimal before the saturation point of L (d); further, the saturation point is compatible with the presence of a significant gap in the sorted Laplacian eigenvalues.

Fig. 2
Geometric embedding of synthetic networks.

a Normalized loss as a function of the embedding dimension in SB networks. We generate graph instances composed of N = 256 nodes and C = 16 groups with p i n = 0.2 and p o u t = 0.02. Embedding is performed using node2vec. The blue curve in panel a is the best fit curve of Eq. (2) (, and R 2 = 5.93 × 10 −3) with the data points. b Test of performance in recovering the ground-truth community structure in SB graphs as a function of the dimension of the embedding. We measure the recovery accuracy with normalized mutual information (NMI). Symbols refer to average values of NMI computed over 10 instances of the SB model; the shaded region identifies the range of values corresponding to one standard deviation away from the mean.

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^fe47f6c1]. Nature Communications (2025). High credibility.

Derivation of δ 2 from the standard beta distribution

For continuous variables constrained between the interval [0, 1], the beta distribution is a flexible model capable of describing a wide array of distribution shapes, including bell-shaped, left-skewed, right-skewed and multi-modal (U-shaped) distributions (Fig. 2 i). The beta probability density function (pdf) is typically defined by two positive shape parameters, p and q both on the open interval (0, ∞), with the function indexed by continuous values of x constrained between the closed interval [0, 1]. The pdf for the beta distribution iswhere the two exponential functions, x p −1 (1− x) q −1, describe all distribution shapes modelled by the beta distribution (Fig. 2 i), Γ(.) is the gamma function, and Γ(p + q)/[Γ(p)Γ(q)] normalises the beta density function such that the area below the function is equal to 1. Notably, as p and q diverge from one another and their sum remains fixed (constant), the central tendency of the distribution shifts to the left (p < q) or to the right (p > q). To be exact, the mean (μ) and 1 − μ can be found withandwhere μ is constrained between the open interval (0, 1). The interval constraining μ is found with limits of the interval of p and q asand. Note μ = 0.5 when p = q. Meanwhile, as the sum of p and q decreases the dispersion of the distribution increases, irrespective of μ. Specifically, the dispersion can be described withwhere δ 2 is also constrained between the open interval (0, 1). The interval constraining δ 2 is found with limits of the interval of p and q asand. Note that δ 2 is equivalent to the shape parameter described by Damgaard and Irvine, which is better interpreted as dispersion and equivalent to the inverse of the precision parameter described by Ferrari and Cribari-Neto. It can, thus, be shown that the combination of μ and δ 2 are capable of describing all possible values of p and q, and by extension all possible beta distributions, whereand

---

### A recurrent network model of planning explains hippocampal replay and human behavior [^c40ed981]. Nature Neuroscience (2024). High credibility.

Fig. 2
Trained RL agents perform more rollouts in situations where humans spend longer thinking.

a, Performance (quantified as the number of actions taken to reach the goal) as a function of trial number within each episode, computed for both human participants (black) and RL agents (blue). Shading indicates the s.e.m. across human participants (n = 94) or RL agents (n = 5) and mostly falls within the interval covered by the solid lines. The gray line indicates optimal performance, computed separately for exploration (trial 1) and exploitation (trials 2–4; Methods). b, Distribution of human response times (top) and thinking times (bottom), spanning ranges on the order of 1 s (Methods). c, Human thinking time as a function of the step within trial (x axis) for different initial distances to the goal at the beginning of the trial (lines, legend). Shading indicates the s.e.m. across 94 participants. Participants spent more time thinking further from the goal and before the first action of each trial (Extended Data Fig. 3). d, Model thinking times separated by the time within trial and initial distance to goal, exhibiting a similar pattern to human participants. To compute thinking times for the model, each rollout was assumed to last 120 ms as described in the main text. Shading indicates the s.e.m. across five RL agents. The average thinking time can be less than 120 ms because the agents only perform rollouts in some instances and otherwise make a reflexive decision. This is particularly frequent near the goal and late in a trial, where humans also spend less time thinking. e, Binned human thinking time as a function of the probability that the agent chooses to perform a rollout, π (rollout). Error bars indicate the s.e.m. within each bin. The gray horizontal line indicates a shuffled control, where human thinking times were randomly permuted before the analysis. f, Correlation between human thinking time and the regressors (1) π (rollout) under the model; (2) momentary distance to goal; and (3) π (rollout) after conditioning on the momentary distance to goal (Residual; Methods). Bars and error bars indicate the mean and s.e.m. across human participants; gray dots indicate individual participants (n = 94).

---

### A machine learning automated recommendation tool for synthetic biology [^5caa5c39]. Nature Communications (2020). High credibility.

Optimization-suggesting next steps

The optimization phase leverages the predictive model described in the previous section to find inputs that are predicted to bring us closer to our objective (i.e. maximize or minimize response, or achieve a desired response level). In mathematical terms, we are looking for a set of N r suggested inputs, that optimize the response with respect to the desired objective. Specifically, we want a process that:
i. optimizes the predicted levels of the response variable;
ii. can explore the regions of input phase space (in Eq. (1)) associated with high uncertainty in predicting response, if desired;
iii. provides a set of different recommendations, rather than only one.

We are interested in exploring regions of input phase space associated with high uncertainty, so as to obtain more data from that region and improve the model's predictive accuracy. Several recommendations are desirable because several attempts increase the chances of success, and most experiments are done in parallel for several conditions/strains.

In order to meet these three requirements, we define the optimization problem formally aswhere the surrogate function G (x) is defined as:depending on which mode ART is operating in (see the "Key capabilities" section). Here, y * is the target value for the response variable, y = y (x), and Var(y) denote the expected value and variance, respectively (see "Expected value and variance for ensemble model" in Supplementary Information), denotes Euclidean distance, and the parameter α ∈ [0, 1] represents the exploitation-exploration trade-off (see below). The constraintcharacterizes the lower and upper bounds for each input feature (e.g. protein levels cannot increase beyond a given, physical, limit). These bounds can be provided by the user (see details in the "Implementation" section in Supplementary Information); otherwise, default values are computed from the input data as described in the "Input space set" section in Supplementary Information.

---

### Clinicians are right not to like cohen's κ [^a5f2e987]. BMJ (2013). Excellent credibility.

Clinicians are interested in observer variation in terms of the probability of other raters (interobserver) or themselves (intraobserver) obtaining the same answer. Cohen's κ is commonly used in the medical literature to express such agreement in categorical outcomes. The value of Cohen's κ, however, is not sufficiently informative because it is a relative measure, while the clinician's question of observer variation calls for an absolute measure. Using an example in which the observed agreement and κ lead to different conclusions, we illustrate that percentage agreement is an absolute measure (a measure of agreement) and that κ is a relative measure (a measure of reliability). For the data to be useful for clinicians, measures of agreement should be used. The proportion of specific agreement, expressing the agreement separately for the positive and the negative ratings, is the most appropriate measure for conveying the relevant information in a 2 × 2 table and is most informative for clinicians.

---

### Physical limits to sensing material properties [^c3e3f73f]. Nature Communications (2020). High credibility.

All materials respond heterogeneously at small scales, which limits what a sensor can learn. Although previous studies have characterized measurement noise arising from thermal fluctuations, the limits imposed by structural heterogeneity have remained unclear. In this paper, we find that the least fractional uncertainty with which a sensor can determine a material constant λ 0 of an elastic medium is approximately [Formula: see text] for a ≫ d ≫ ξ, [Formula: see text], and D > 1, where a is the size of the sensor, d is its spatial resolution, ξ is the correlation length of fluctuations in λ 0, Δ λ is the local variability of λ 0, and D is the dimension of the medium. Our results reveal how one can construct devices capable of sensing near these limits, e.g. for medical diagnostics. We use our theoretical framework to estimate the limits of mechanosensing in a biopolymer network, a sensory process involved in cellular behavior, medical diagnostics, and material fabrication.

---

### Emergence of cellular nematic order is a conserved feature of gastrulation in animal embryos [^cbd7ae32]. Nature Communications (2025). High credibility.

Fig. 4
Models for nematic order formation.

a – d Results from model (i) (Methods), where each cell orientation is influenced only by four neighbors. a – c Cell orientation at different times t. The parameter = 2.5 × 10− 3. d Time dependent changes in S for all cells in (a). Each curve in gray represents one realization. Data in the brown dot is the mean value averaged over 20 trajectories. The shaded area shows the SEM. The solid line in navy blue corresponds to S = 0. e – h Results from model (ii), where each cell orientation is influenced only by a global field, leading alignment along the X -axis independently. The navy-blue line in (h) is a linear function, listed at the top of the figure. The parameter = 1 × 10 −3. i – l Results from model (iii), Eq. (6). Navy blue line in (l) shows a power-law behavior with the function listed at the top of the figure. The values ofandare 2.5 × 10 −3, 1 × 10 − 3, respectively. The inset in (l) shows the spatial correlation, C S (r), of cells at different times from simulations. The solid lines show the power-law decay.

Next, we generated a model in which each cell is aligned in the same direction to simulate the effect of global cell alignment (with an amplitude of B shown in Eq. (5)), which could be induced by planar cell polarity proteins, for example. With this modification, we find that all the cells in the tissue eventually align globally in the same direction, forming a near-perfect nematic phase (Fig. 4e–g). However, the formation and expansion of a small domain at early times that grows with time, which is found in all three species, cannot be explained. This is because each cell changes its orientation independently due to the absence of local interactions among cells in the model (see also Fig. 4e, f). In addition, the temporal evolution of S(t) grows linearly with time, which contradicts second finding listed above (compare Fig. 1j and Fig. 4h).

---

### Integrative functional genomics identifies regulatory mechanisms at coronary artery disease loci [^6e965fb2]. Nature Communications (2016). Medium credibility.

GWAS enrichment analysis in accessible chromatin regions

GWAS SNP positions were downloaded from the NHGRI-EBI GWAS catalogue. ATAC-seq open chromatin regions were centred and GWAS SNPs were counted in a window of 100 bp for ± 1 kb surrounding centred ATAC-seq regions using the Feature correlation tool from the ChIP-Cor module (part of ChIP-Seq Analysis Server of the Swiss Institute of Bioinformatics). Counts were normalized to the total number of reference counts. The P values for enrichment of GWAS SNPs in ATAC-seq open chromatin regions were calculated as described previously. The P values were computed using binomial cumulative distribution function b (x; n, p) in R (dbinom function). We set the parameter n equal to the total number of GWAS SNPs in a particular GWAS phenotype. Parameter x was set to the number of GWAS SNPs for a given GWAS phenotype that overlap ATAC-seq regions and parameter p was set to the fraction of the uniquely mappable human hg19 genome (2,630,301,437 bp) that is localized in the ATAC-seq open chromatin regions and contains assessed GWAS phenotype SNPs. Calculated binomial P value equals the probability of having x or more of the n test genomic regions in the open chromatin domain given that the probability of that occurring for a single GWAS genomic region is p.

Custom R script was generated to intersect data sets and calculate Fisher's exact P values. Peak intervals from each data set were selected to contain those that have at least 1 bp overlap with a combined set of ENCODE DHS intervals from 125 ENCODE cell lines. ATAC-Seq, and JUN, TCF21, H3K27ac ChIP-Seq intervals were tested either separately or intersected (ATAC-JUN, ATAC-TCF, ATAC-K27, ATAC-K27-JUN and ATAC-K27-TCF). For each analysis, we determined overlaps with CAD SNPs, IBD SNPs, UC SNPs or a whole GWAS catalogue minus CAD SNPs, as a control. Overlaps between datasets, as well as sites present in one or the other tested data set, were determined using the ENCODE DHSs as a background. Finally, we determined DHSs devoid of each of the tested data set intervals. One-sided Fisher's exact test was used to determine whether the observed peak overlap between two tested data set intervals was statistically greater when compared against a background of all DHS peaks. Contingency matrices were made in R and Fisher's exact test was performed using fisher.test R command.

---

### Power of data in quantum machine learning [^03099894]. Nature Communications (2021). High credibility.

Given some set of data, if s K (N) is found to be small relative to N after training for a classical ML model, this quantum model f (x) can be predicted accurately even if f (x) is hard to compute classically for any given x. In order to formally evaluate the potential for quantum prediction advantage generally, one must take s K (N) to be the minimal over efficient classical models. However, we will be more focused on minimally attainable values over a reasonable set of classical methods with tuned hyperparameters. This prescribes an effective method for evaluating potential quantum advantage in practice, and already rules out a considerable number of examples from the literature.

From the bound, we can see that the potential advantage for one ML algorithm defined by K 1 to predict better than another ML algorithm defined by K 2 depends on the largest possible separation betweenandfor a dataset. The separation can be characterized by defining an asymmetric geometric difference that depends on the dataset, but is independent of the function values or labels. Hence evaluating this quantity is a good first step in understanding if there is a potential for quantum advantage, as shown in Fig. 1. This quantity is defined bywhere ∣∣. ∣∣ ∞ is the spectral norm of the resulting matrix and we assume Tr(K 1) = Tr(K 2) = N. One can show that, which implies the prediction error bound. A detailed derivation is given in Supplementary Section C and an illustration of g 12 can be found in Fig. 2. The geometric difference g (K 1 ∣∣ K 2) can be computed on a classical computer by performing a singular value decomposition of the N × N matrices K 1 and K 2. Standard numerical analysis packagesprovide highly efficient computation of a singular value decomposition in time at most order N 3. Intuitively, if K 1 (x i, x j) is small/large when K 2 (x i, x j) is small/large, then the geometric difference g 12 is a small value ~1, where g 12 grows as the kernels deviate.

---

### Number development and developmental dyscalculia [^e45e7f45]. Developmental Medicine and Child Neurology (2007). Low credibility.

There is a growing consensus that the neuropsychological underpinnings of developmental dyscalculia (DD) are a genetically determined disorder of 'number sense', a term denoting the ability to represent and manipulate numerical magnitude nonverbally on an internal number line. However, this spatially-oriented number line develops during elementary school and requires additional cognitive components including working memory and number symbolization (language). Thus, there may be children with familial-genetic DD with deficits limited to number sense and others with DD and comorbidities such as language delay, dyslexia, or attention-deficit-hyperactivity disorder. This duality is supported by epidemiological data indicating that two-thirds of children with DD have comorbid conditions while one-third have pure DD. Clinically, they differ according to their profile of arithmetic difficulties. fMRI studies indicate that parietal areas (important for number functions), and frontal regions (dominant for executive working memory and attention functions), are under-activated in children with DD. A four-step developmental model that allows prediction of different pathways for DD is presented. The core-system representation of numerical magnitude (cardinality; step 1) provides the meaning of 'number', a precondition to acquiring linguistic (step 2), and Arabic (step 3) number symbols, while a growing working memory enables neuroplastic development of an expanding mental number line during school years (step 4). Therapeutic and educational interventions can be drawn from this model.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^d33c8072]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Quality categories for FVC or FEV1 in adults and children — Grades A–F are defined by the number of acceptable tests and repeatability thresholds. Grade A requires " ≥ 3 acceptable tests with repeatability within 0.150 L for age 2–6, 0.100 L, or 10% of highest value, whichever is greater"; Grade B requires " ≥ 2 acceptable tests with repeatability within 0.150 L for age 2–6, 0.100 L, or 10% of highest value, whichever is greater"; Grade C requires " ≥ 2 acceptable tests with repeatability within 0.200 L for age 2–6, 0.150 L, or 10% of highest value, whichever is greater"; Grade D requires " ≥ 2 acceptable tests with repeatability within 0.250 L for age 2–6, 0.200 L, or 10% of highest value, whichever is greater"; Grade E is "One acceptable test", and Grade F is "No acceptable tests".

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^37daae40]. Nature Communications (2025). High credibility.

Fig. 2
The matrix-tree theorem.

A Computing the steady-state occupancy π 1 by summing weights over directed spanning trees. Directed spanning trees are subgraphs containing all graph nodes but no cycles, with edges oriented toward a root node. In each directed spanning tree, the input forces make a positive, negative, or zero contribution to the tree weight. The structural vectorsare shown below each tree; these quantities enter into Equation (3) below. B Schematic illustration of the high-dimensional space of feature vectors ψ (i; θ) and χ (i, F). The depicted arrangement of vectors could solve a binary classification problem.

We define the input multiplicityas the number of edges affected per input variable, which we assume to be the same for each input. To focus on the functional way in which the input driving enters the steady-state probabilities, the driving contributions can be factored out in the algebraic expressions for the numerator and denominator of Equation (1). This has been previously been used to make analytical progress for M = D = 1 in, for example, refs. –. This equivalent formulation of Eq. (1) suggests that steady states of Markov jump processes implement a rational polynomial function of exponentiated input variables. Defining, we rewrite the matrix-tree expression for π i for general D and M We use the multi-index, whereis the set of D input labels and each componentof the multi-index runs over the values, to enumerate themonomials. These monomials y μ (F) in Equation (2) combinatorially depend on the different mixtures μ of input driving, representing a net total μ a of signed contributions from the input force F a, μ b such contributions for F b, and so on for each input. The coefficients, which are functions of the parameters θ, are the sums of weights over all directed spanning trees rooted at node i which have the corresponding mixture μ of signed input contributions. The monomial coefficientsthus represent learnable amplitudes of each polynomial basis function y μ (F). The coefficients in the denominator are defined as. Classification will be successful if, for F ρ drawn from class ρ, the coefficientsand monomials y μ (F ρ) are large for the same μ. In the subsequent sections of the paper and in the Supplementary Information we use the formulation in Equation (2) to show how the classification ability of a non-equilibrium Markov processes may be systematically modulated.

---

### Evolution of new regulatory functions on biophysically realistic fitness landscapes [^3c36e467]. Nature Communications (2017). Medium credibility.

Dynamics of the TF–TF match, M, and the scaled fitness, NF, become smooth and gradual when discrete transitions and the consequent large jumps in fitness are averaged over individual realizations, as in Fig. 4b. Importantly, we learn that the sequence of dominant macrostates leading towards the final (and steady) state, 'Specialize Both', involves a long intermediate epoch when the system is in the 'One TF Lost' state. We examine this sequence of most likely macrostates in detail in Fig. 4c, and visualize it analogously to the map of evolutionary outcomes in steady state shown in Fig. 3c. High Ns and correlation (ρ) values favor trajectories passing through the 'One TF Lost' state, while intermediate Ns (5 ≲ Ns ≲ 20) and low correlation values enable transitions through 'Partial' macrostate; along the latter trajectory, the binding of neither TF is completely abolished. Typical dwell times in dominant states, indicated as contours in Fig. 4c, suggest that specialization via the 'One TF Lost' state should be slower than through the 'Partial' state, which is best seen at t = 1/ μ, where specialization has already occurred at intermediate Ns and low, but not high, ρ values.

It is easy to understand why pathways towards specialization via the 'One TF Lost' state are slow. As the example in Fig. 4a illustrates, so long as one TF maintains binding to both sites and thus network function (especially when signals are strongly correlated), the other TF's specificity will be unconstrained to neutrally drift and lose binding to both sites, an outcome which is entropically highly favored. After the TF's sensory domain specializes, however, the binding has to re-evolve essentially from scratch in a process that is known to be slowunless selection strength is very high. In contrast to this 'Slow' pathway, the 'Fast' pathway via the 'Partial' state relies on sequential loss of "crosstalk" TF–BS interactions, with the divergence of TF consensus sequences followed in lock-step by mutations in cognate binding sites. Specifically, the likely intermediary of the fast pathway is a 'Partial' configuration in which the first TF responds to both signals but only regulates one gene, whereas the second TF is already specialized for one signal, but still regulates both genes.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^89cdccc2]. Annals of the American Thoracic Society (2023). High credibility.

V_A/Q distribution modeling — derived from cumulative plots of ventilation or perfusion versus V_A/Q — is binned into 50 equally spaced log-scale compartments, with shunt and dead space handled separately. Specifically, V_A/Q ratios < 0.005 (shunt) and > 100 (dead space) are calculated as separate components, and relative dispersion and gravitational gradients may also be assessed.

---

### Nonlinear wave evolution with data-driven breaking [^a98d276b]. Nature Communications (2022). High credibility.

Neural network and algorithm

The goal of training is to teach the network the discrepancy between the MNLS propagation prediction and the true propagation from measurements over an arbitrary number of solver propagation steps when started from the same initial condition. As we do not have the phase information that allows us to Fourier transform the envelope between the time and frequency domain at each solver step, we correct the evolution of the time and frequency domain separately, by using two separate Neural Networks with identical architecture. One in the time domain and the other in the frequency domain. Below, we outline the structure of the algorithm for Wave Categories I and II (see SI section 6 for further details). The procedure for Wave Category III is slightly different, as this involved segmenting a continuous time series and is outlined in SI section 5.1.
Obtaining the ground truth at the solver step — The wave tank experiments need to be interpolated such that the ground truth is available at very (fixed-size) solver propagation step Δ ξ instead of only at the wave gauge positions. The Nyquist–Shannon sampling theorem provides a lower bound for the sampling frequency to capture all the information from a continuous signal of final bandwidth. Following this, the slowly varying envelope is sufficiently sampled by our 12 gauges in space, and by the wave gauge sampling frequency in time. As we can only interpret the modulus of the envelope of the measurement and not its phase, we perform a spline interpolation on the modulus of the wave tank measurements separately in the time and frequency domains. Hereby moving from the envelope at discrete locations of the wave gauges a true (ξ wg) to the modulus of the envelope-field at interval Δ ξ for both domains:
Data augmentation — Simply using the entire experiment would not yield enough data, and we, therefore, augment the data by creating smaller propagation segments. In order to account for different stages of evolution of the wave, we can choose training samples starting at different positions ξ 0. Note that this is different from simply cutting the entire propagation comparison of MNLS and experiment into smaller pieces, as each segment requires its own MNLS simulation starting from its own initial condition. For each experiment, three segments lengths are selected. The starting position can only be at wave gauge positions ξ k, as these are the only locations with access to the complex field (modulus and phase). An MNLS simulation can be started from this initial condition a (ξ k, τ), a segment length n steps, yielding a MNLS (ξ k, ξ k + n steps Δ ξ, τ). This prediction can then be compared to the same stretch of the measured field, or true evolution a true in eq. (7). Since only the modulus of the envelope is available at the solver step, an input-output training pair s is the modulus of the envelopeSimilarly, a separate training paircan be created for the modulus of the spectrum of the envelope. To limit the number of free parameters, the number of modes (and, correspondingly, the length of the time-vector) is truncated to n t = 512. The input and output therefore each have matrix dimensions [n steps, n t = 512]. By varying the segment length and the starting gauge, many pairs s ∈ S can be created. Note that the network is never trained on the entire propagation length available in the tank.
Neural Network — A neural network is a nonlinear function F NN of its parameters β (the weights and biases) and the input:The goal is to optimize β such that the mean squared error (MSE) of the prediction and the true value of the envelope is minimized. We employ a separate network for the time and frequency domain. For the time-domain RNN, this cost function is simply defined as:where i the grid point, and N = n steps × n t, the total number of grid points. The definition for the frequency-domain network is the counterpart of eq. (10) for. To find the network parameters β that minimize the cost function, J (β), the Adam stochastic gradient descent method with bounded gradient is used. Details of the optimization include: For this optimization problem, different network architectures can be chosen for F NN. Our work employs an RNN, for which the weights of the hidden states are updated based on the previous propagation step and passed on to the next recursively. As such, RNNs are able to return an output sequence, incorporating the memory of all the propagation steps in the input sequence. This is crucial to detect breaking signatures along the evolution. The main property of the finite-domain correction is that the system evolves for several propagation steps, after which the correction is applied. As a consequence, the correction uses information from future propagation steps and is non-causal. To mediate the problems of vanishing and exploding gradients that a fully-connected RNN would have, we employ the long short-term memory (LSTM) method. The measurement data are divided into a training (80%), validation (15%), and test (5%) set. The first is used to minimize eq. (10), the second to evaluate performance after each training epoch (a complete pass through the training set), and the latter is never used in the optimization process, and only serves to validate the performance of the model. Since wave breaking does not possess any conserved quantities for the envelope evolution, no physical constraints can be added as either soft or hard constraints. As the time-vector is of length 512, consequently, the input and output layers must consist of 512 units. The LSTM layer is connected to the input and consists of 128 hidden units, followed by one dense, or fully-connected, layer with 64 units. To promote generality, the dense layer has a dropout of 0.1, meaning this fraction of neurons will be randomly turned off during training. The dense layer is connected to the output layer. All layers have a leaky rectified linear unit (ReLu) activation function. The resulting model has a total of 369,728 parameters (degrees of freedom). Note that the architecture of the LSTM layer stays the same for all propagation steps; more propagation steps do not add more degrees of freedom. Note that for prediction, the MNLS trajectory must have the same time and propagation step as during training: Δ ξ = Δ ξ train. Similarly for the time step Δ τ = Δ τ train. Finally, the network is trained on data normalized by the maximum value of the initial condition. The MNLS solver has periodic boundary conditions, therefore it is equivariant to translations of the input in τ. To account for this in physical space we add 40 random translationsof the input and output vectors. Note that in frequency space, these shifts do not affect. Additional examples of these translated results are included in SI section 2.2.
MNLS + FDML correction — Once the network is trained, MNLS + FDML model can be utilized as illustrated in figure Fig. 7 d. While the network has been trained on shorter segments, it can be used for longer, or arbitrary propagation length, and on unseen data. Like the MNLS, an initial value or deterministic wave-forecasting problem is solved. That is, for a given initial condition at a position x 0, one wants to know the evolution up to and including a point x 1. If the initial condition is a time-series measurement of the surface elevation, the complex envelope a (x 0, t) has to be obtained using the Hilbert transform. Subsequently, the MNLS solver can be used to propagate the solution forward to x 1. Over this finite stretch of evolution, the RNN applies a correction to the modulus of the time-domain envelope amplitude, or on the frequency-domain envelope amplitude to correct for breaking effects. Figures 1 and 2 show the entire propagation length available in the tank merely because this offers the most extensive comparison.

---

### 2018 ACC / HRS / NASCI / SCAI / SCCT expert consensus document on optimal use of ionizing radiation in Cardiovascular imaging: best Practices for safety and effectiveness: a report of the American college of cardiology task force on expert consensus decision pathways [^cba67b33]. Journal of the American College of Cardiology (2018). Medium credibility.

Spatial resolution — detector input dose and noise in x-ray fluoroscopy: Low signal-to-noise ratio images have a "grainy" appearance because a small number of x-ray photons reach the detector to form an image, and this grainy quality, termed "quantum mottle", becomes smoother as dose increases, improving the ability to perceive image detail. In x-ray fluoroscopic imaging, images of a line pair phantom were acquired at different detector doses ranging from 10 to 1,200 nGy/frame, illustrating that as the number of photons reaching the detector increases, image noise decreases and the image becomes smoother; over a defined range, as image noise decreases, perceptible image spatial resolution increases, but for each imaging modality there is an upper limit of dose beyond which further dose increase does not yield greater image detail of diagnostic importance, and dose is an important determinant of noise, and noise affects the ability to perceive detail.

---

### Retrotransposons are the major contributors to the expansion of theMuller F element [^83631d3a]. G3 (2017). Low credibility.

The discordance between genome size and the complexity of eukaryotes can partly be attributed to differences in repeat density. The Muller F element (∼5.2 Mb) is the smallest chromosome in Drosophila melanogaster, but it is substantially larger (> 18.7 Mb) in D. ananassae To identify the major contributors to the expansion of the F element and to assess their impact, we improved the genome sequence and annotated the genes in a 1.4-Mb region of the D. ananassae F element, and a 1.7-Mb region from the D element for comparison. We find that transposons (particularly LTR and LINE retrotransposons) are major contributors to this expansion (78.6%), while Wolbachia sequences integrated into the D. ananassae genome are minor contributors (0.02%). Both D. melanogaster and D. ananassae F-element genes exhibit distinct characteristics compared to D-element genes (e.g., larger coding spans, larger introns, more coding exons, and lower codon bias), but these differences are exaggerated in D. ananassae Compared to D. melanogaster, the codon bias observed in D. ananassae F-element genes can primarily be attributed to mutational biases instead of selection. The 5' ends of F-element genes in both species are enriched in dimethylation of lysine 4 on histone 3 (H3K4me2), while the coding spans are enriched in H3K9me2. Despite differences in repeat density and gene characteristics, D. ananassae F-element genes show a similar range of expression levels compared to genes in euchromatic domains. This study improves our understanding of how transposons can affect genome size and how genes can function within highly repetitive domains.

---

### KDIGO clinical practice guideline on the evaluation and management of candidates for kidney transplantation [^48cb1ec2]. Transplantation (2020). High credibility.

KDIGO guideline — GRADE system for grading quality of evidence outlines a three-step approach: Step 1 assigns a starting grade by study design (Randomized trials = High; Observational study = Low; Any other evidence = Very Low). Step 2 reduces the grade for specified concerns: Study quality −1 level if serious limitations, −2 levels if very serious limitations; Consistency −1 level if important inconsistency; Directness −1 level if some uncertainty, −2 levels if major uncertainty; and Other −1 level if sparse or imprecise data and −1 level if high probability of reporting bias. Step 3 raises the grade when applicable: Strength of association +1 level if strong (no plausible confounders) or +2 levels if very strong (no major threats to validity), and Other +1 level if evidence of a dose–response gradient or if all residual plausible confounders would have reduced the observed effect. Definitions specify that strong evidence is a significant relative risk of > 2 (< 0.5) from consistent observational evidence with no plausible confounders; very strong evidence is a significant relative risk of > 5 (< 0.2) based on direct evidence with no major threats to validity; sparse evidence includes only one study or total N < 500; imprecision includes a low event rate (0 or 1 event) in either arm or a confidence interval spanning a range > 1. Final grades are defined as: High = further research is unlikely to change confidence in the estimate of the effect; Moderate = further research is likely to have an important impact on confidence in the estimate of effect and may change the estimate; Low = further research is very likely to have an important impact on confidence in the estimate and may change the estimate; Very Low = any estimate of effect is very uncertain.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^d6d8fcbc]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Spirometry reporting specifies that numerical values are given for the FEV1, the FVC, and the FEV1/FVC ratio; the latter should be reported as a decimal fraction and the space for percent predicted value left blank, and if bronchodilators are given the LLN column need not be repeated with absolute and percent change given only for FEV1 and FVC. Other numerical values such as the forced inspiratory flow at 75% of FVC (FEF75%) and FEF25–75% are not recommended for routine use. Graph requirements include that for the volume–time curve the volume scale should be at least 10 mm/L, the time scale at least 20 mm/s, and 1 second prior to the start of expiration should be displayed; on the flow–volume plot the flow display should be at least 5 l/min/L/s, and the ratio of flow to volume should be 2 L/s to 1 L, and linear and log scales where values are plotted as z-scores relative to the predicted value (z = 0) give an intuitive sense of severity.

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^cf66bf7c]. Nature Communications (2023). High credibility.

Dose-response curves are key metrics in pharmacology and biology to assess phenotypic or molecular actions of bioactive compounds in a quantitative fashion. Yet, it is often unclear whether or not a measured response significantly differs from a curve without regulation, particularly in high-throughput applications or unstable assays. Treating potency and effect size estimates from random and true curves with the same level of confidence can lead to incorrect hypotheses and issues in training machine learning models. Here, we present CurveCurator, an open-source software that provides reliable dose-response characteristics by computing p-values and false discovery rates based on a recalibrated F-statistic and a target-decoy procedure that considers dataset-specific effect size distributions. The application of CurveCurator to three large-scale datasets enables a systematic drug mode of action analysis and demonstrates its scalable utility across several application areas, facilitated by a performant, interactive dashboard for fast data exploration.

---

### The snm procedure guideline for general imaging 6.0 [^638aaec4]. SNMMI (2010). Medium credibility.

SNM Procedure Guideline for General Imaging — matrix size and pixel depth in nuclear medicine acquisitions notes that matrix size is almost always a power of 2 with typical values 64 x 64, 128 x 128, 256 x 256 and 512 x 512, and that non-square matrix sizes also exist for whole-body studies. Each pixel can be represented with a single byte (pixel values ranging from 0 to 255 counts) or with 16 bit words (pixel values ranging up to a maximum of 32k or 64k). Overflow occurs when the number of counts recorded at some given position exceeds the maximum number of counts, and overflow is more likely to occur when using a byte matrix.

---

### Heterogeneous dynamics, robustness / fragility trade-offs, and the eradication of the macroparasitic disease, lymphatic filariasis [^7d275bd7]. BMC Medicine (2016). Low credibility.

Table 3
Model-estimated worm breakpoint values for achieving the successful interruption of LF transmission in each of the study sites investigated. Breakpoints are listed in terms of % mf prevalence at three probabilities of elimination for two situations: 1) at the prevailing vector biting rates (i.e. at the observed ABRs); and 2) at the threshold biting rate (TBR) at or below which LF transmission process cannot sustain itself regardless of the level of the infection in human hosts (see text). The first set of the threshold values (at study-specific ABR) is used in modeling the impact of mass drug administration (MDA) alone, while the second set (mf breakpoint values estimated at TBR) is applied for modeling the impact when MDA is supplemented by vector control (VC)

EP, elimination probability

Fig. 3
Mf breakpoints as a function of baseline community annual biting rate (ABR) and microfilaria (mf) prevalence. The mf breakpoints estimated in each site are shown as average values with 95% CIs, calculated as the 2.5th and 97.5th percentiles of the breakpoint distribution in each site, and are plotted against the observed ABRs in each site; filled and open circles, respectively, represent values for the culicine and anopheline settings. The data in (a, b) and (c, d), respectively, represent the mf breakpoints estimated at the observed site-specific ABRs and the corresponding estimated threshold biting rates (TBRs). Both types of mf breakpoints were negatively correlated with ABR, with the fitted dashed lines indicating that overall these data follow a power-law function: f (x) = ax b, with x representing the biting rate values on the x-axis, and f (x) the mf breakpoints on the y-axis. The term a is a constant while b is the power-law exponent, with fitted values of (a, b) as follows: (a) (20.54, −0.5112); (b) (1.335, −0.2184); (c) (54.25, −0.3498); and (d) (4.251, −0.104). All four associated p values were < 0.01. The set of mf breakpoints plotted in each graph were calculated using the best-fitting parameter vectors obtained from model fits to the baseline mf age-profile of each study site. In the plots, individual sites are indicated by their first two letters, except for "Mao" in the culicine settings, in order to distinguish it from "Ma" used for "Mambrui". Inset plots are provided to clarify the variations in the breakpoint values estimated for sites with approximately the same baseline ABR values, which were obscured in the respective main plots

---

### Untangling the diffusion signal using the phasor transform [^1407b735]. NMR in Biomedicine (2020). Medium credibility.

4.2 IVIM fitting

The runtimes of all tested fitting algorithms were compared: the phasor‐based fit used 2.38 x and the segmented fit used 0.0023 x the computation time of the NLLS IVIM fit. A comparison of the nonlinear, segmented and phasor‐based IVIM fitting methods on simulated data is presented in Figure 3 and the left panel of Figure 4. Figure 3 shows that for fractions f below 0.25 the phasor‐based method had the highest precision for estimates of f and D: the interquartile range was roughly halved compared with that of the segmented method, which was second best. Figure S2 shows the same data as Figure 3, but for the full range of f (0‐1). Accurately and precisely estimating D* at low fractions f proved difficult for all methods. The phasor method showed the best accuracy, but the worst precision. Table S1 reports mean and median error (accuracy) and interquartile range (precision) for all tested cases.

FIGURE 3
Comparison of IVIM fitting techniques on simulated data: constrained nonlinear least squares (blue), segmented linear least square (orange) and phasor‐based nonlinear least square (yellow). The SNR level was 30, and the b‐value sampling range was 0‐1000 s/mm 2 for both experiments, but the number of b‐values was varied: 15 b‐values in the top panel and six b‐values in the bottom panel. The solid lines represent the median error and the dotted lines the 5% and 95% percentiles. Shading was added to facilitate a comparison of the spread in the errors

FIGURE 4
Probability density functions of estimated IVIM parameters f, D and D* for all tested fitting techniques in both simulated and in vivo measured white matter (WM) and gray matter (GM). In the simulations, the vertical lines indicate the true value of the estimated parameter. Results were obtained using all measured 15 b‐values

---

### Learning meaningful representations of protein sequences [^e27a58e5]. Nature Communications (2022). High credibility.

Construction of entropy network

To ensure that our VAE decodes to high uncertainty in regions of low data density, we construct an explicit network architecture with this property. That is, the network p θ (X ∣ Z) should be certain about its output in regions where we have observed data, and uncertain in regions where we have not. This has been shown to be important to get well-behaved Riemannian metrics. In a standard VAE with posterior modeled as a normal distribution, this amounts to constructing a variance networkthat increases away from data. However, no prior work has been done on discrete distributions, such as the Categorical distribution C (μ θ (Z)) that we are working with. In this model we do not have a clear division of the average output (mean) and uncertainty (variance), so we control the uncertainty through the entropy of the distribution. We remind that for a categorical distribution, the entropy isThe most uncertain case corresponds to when H (X ∣ Z) is largest i.e. when p (X ∣ Z) i = 1/ C for i = 1. C. Thus, we want to construct a network p θ (X ∣ Z) that assigns equal probability to all classes when we are away from data, but is still flexible when we are close to data. Taking inspiration fromwe construct a function α = T (z), that maps distance in latent space to the zero-one domain. T is a trainable network of the model, with the functional formwith, where κ j are trainable cluster centers (initialized using k -means). This function essentially estimates how close a latent point z is to the data manifold, returning 1 if we are close and 0 when far away. Here K indicates the number of cluster centers (hyperparameter) and β is a overall scaling (trainable, constrained to the positive domain). With this network we can ensure a well-calibrated entropy by pickingwhere. For points far away from data, we have α = 0 and returnregardless of category (class), giving maximal entropy. When near the data, we have α = 1 and the entropy is determined by the trained decoder p θ (X ∣ Z) i.

---

### Functional annotation of enzyme-encoding genes using deep learning with transformer layers [^101c8321]. Nature Communications (2023). High credibility.

In addition to predicting protein characteristics from a primary sequence, it is also crucial to understand the sequence features that impact enzyme function. Recent studies on biological sequences using deep learning have attempted to interpret the inner workings of neural networks through various approaches –. In this study, we conducted an analysis of the self-attention layers within the DeepECtransformer neural network to identify the specific features that AI focuses on to classify enzyme functions. Our results showed that the AI effectively detects functional regions such as active sites and ligand interaction sites, in addition to well-known functional domains such as Pfam domains. These identified motifs have the potential to enhance our understanding of enzyme functions. Our analysis successfully visualized commonly highlighted motifs consisting of 16 amino acid residues, but it is possible that DeepECtransformer may also be capable of detecting longer-range protein interactions. As an example, the self-attention layer of DeepECtransformer identified multiple ligand interaction residues spanning the entire sequence of E. coli 's malate dehydrogenase (Supplementary Fig. 8). Further application of alternative interpretation methods holds the promise of uncovering previously unknown yet critical features of enzymes –.

The use of EC numbers as the standard classification scheme for enzyme functionality has been well-established and continually updated. However, the four-digit structure of EC numbers often introduces ambiguity and makes it challenging to provide clear descriptions of metabolic functions using data-driven classifiers. For instance, a single metabolic reaction can be described by multiple EC numbers (e.g. malate dehydrogenase can be EC:1.1.1.37 and EC:1.1.1.375), and some EC numbers may represent general types of enzymes with ambiguous metabolic reactions (e.g. alcohol dehydrogenase, EC:1.1.1.1). Furthermore, the EC classification scheme is constrained by known substrates and reactions, posing difficulties in extrapolating classifiers for enzymes whose substrates are not clearly known. While GO terms are widely used for protein functionality classification, they may not comprehensively cover all metabolic reactions. As of July 2023, out of the 8056 EC numbers with complete four digits, only 5216 had corresponding GO terms. Hence, establishing a clear scheme for describing metabolic reactions could significantly enhance our understanding of enzyme functionality.

---

### Sas-87 kb… [^6a45bc8b]. wwwn.cdc.gov (2025). Medium credibility.

18 = 'Portable room heaters of any kind' 19 = 'Some other type' 20 = 'None, unit is not heated' 88 = 'Blank, but applicable';… VALUE PE0622F 1 = 'right' 2 = 'left' 3 = 'both'; VALUE PE0623F 1 = 'right' 2 = 'left' 3 = 'both'; VALUE PE0624F 1 = 'right' 2 = 'left' 3 = 'both';… VALUE PE0625F 1 = 'right' 2 = 'left' 3 = 'both'; VALUE PE0626F 1 = 'right' 2 = 'left' 3 = 'both'; VALUE PE0627F 1 = 'right' 2 = 'left' 3 = 'both';… VALUE PE0628F 1 = 'right' 2 = 'left' 3 = 'both'; VALUE PE0629F 1 = 'right' 2 = 'left' 3 = 'both'; VALUE PE0630F 1 = 'right' 2 = 'left' 3 = 'both';… VALUE PE0631F 1 = 'right' 2 = 'left' 3 = 'both'; VALUE PE0632F 1 = 'right' 2 = 'left' 3 = 'both'; VALUE PE0633F 1 = 'right' 2 = 'left' 3 = 'both';… VALUE PE0672F 1 = 'right' 2 = 'left' 3 = 'both'; VALUE PE0673F 1 = 'right' 2 = 'left' 3 = 'both'; VALUE PE0674F 1 = 'right' 2 = 'left' 3 = 'both';… VALUE PE0675F 1 = 'right' 2 = 'left' 3 = 'both'; VALUE PE0676F 1 = 'right' 2 = 'left' 3 = 'both'; VALUE PE0677F 1 = 'right' 2 = 'left' 3 = 'both';

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^5355d6ea]. CDC (2011). Medium credibility.

Community viral load categorical measurement — additional measures and distribution guidance include examining the proportion of HIV-infected persons that meet a standardized definition of in-care, e.g., at least two lab results obtained within a calendar year that are at least 60 days apart. Categorical measures might be more informative than means, the actual distribution of VL results in surveillance populations is highly skewed with a large proportion of the population having undetectable or very low values (e.g., < 200 copies/mL), and jurisdictions should examine their data to determine the local viral load distribution and the best measures to describe it, including outlier cutoff values; such distributions can also be stratified by CD4+ T-lymphocyte count to reflect stage of illness and the U.S. HIV Treatment Guidelines recommendations on initiation of antiretroviral therapy.

---

### Dissecting a heterotic gene through gradedPool-seq mapping informs a rice-improvement strategy [^bd852252]. Nature Communications (2019). High credibility.

Fig. 2
QTL mapping and map-based cloning of OsMADS1 GW3p6. a, b Identification of the genomic region consisting of OsMADS1 GW3p6 via GPS approach. a The p value plot is the result of Ridit analysis for 1000-grain weight before noise-reduction algorithm. The -ln (p value) plot (Y axis) is plotted against SNP positions (X axis) on each of the 12 rice chromosomes. b After the strategy of reducing background noise, the results present as ratio plot. X -axis value is set at a midpoint at each defined genomic interval and Y -axis value corresponds to ratio. c The genotype of chromosome 3 of RIL-79. Black, white and gray bars represent the heterozygous genotype, and homozygous genotypes of GZ and FH respectively. d In the fine-scale mapping (lower bar) generated from the analysis of 1,079 segregating individuals, the QTL GW3p6 falls in the heterozygous interval. The numbers below the bar indicate the number of recombinants between GW3p6 and the molecular markers shown. e Genotyping of progeny homozygous for GW3p6 delimited the locus to a ~5.9 Kb genomic region between markers MP99 and MP100. f, h 1000-grain weight and grain length of recombinant F 2 lines of RIL-79 (L1-L4). Data shown as means ± SD, n = 48. Source data are provided as a Source Data file. g Schematic diagram depicting the structure of OsMADS1 and OsMADS1 GW3p6, red letters stand for the nucleotides of OsMADS1 non-homozygous segment. i The sequencing electrophoresis of non-homologous segment between OsMADS1 and OsMADS1 GW3p6. j The schematic illustration of OsMADS1 functional domains, M represents the MADS domain, I represents the intervening domain, K represents the keratin-like domain, and C represents the C-terminal domain. Source data of Fig. 2f, h are provided in a Source Data file

---

### Early prognostication of neurological outcome by heart rate variability in adult patients with out-of-hospital sudden cardiac arrest [^01469c30]. Critical Care (2019). Medium credibility.

For the geometric-domain variables, the total number of all RR intervals was divided by the height of the histogram of all RR intervals measured on a discrete scale with bins of 7.185 ms (triangular index), and RR interval was plotted as a function of the previous one (Poincaré plot). SD1 and SD2 are the two dispersions (standard deviations [SD]) of projections of the Poincaré plot on the line of identity (y = x) and on the line perpendicular to the line of identity (y = − x), respectively.

For the complexity variables, approximate entropy (ApEn) and sample entropy (SampEn) were computed with a parameter of m = 2 and similarity criterion = 20% of SD. Multiscale entropy (MSE) index was defined as the sum of the sample entropy at a scale factor of 1–20. Detrended fluctuation analysis (DFA) was measured to quantify fractal scaling properties of the RR interval. The scaling properties were defined separately for short-term (4 ≤ n ≤ 16 beats, α 1) and long-term (n > 16 beats, α 2) RR intervals.

DC, rMSSD, pNN50, ln LF power, ln HF power, and LF/HF were computed from the segment of 512 RR intervals, and the averaged values of the entire RR intervals were calculated. AVNN, SDNN, triangular index, ln total power, ln ULF power, ln VLF power, ApEn, SampEn, and MSE index were computed for the entire RR intervals. SD1, SD2, DFA (α 1), and DFA (α 2) were computed from the segment of 1000 RR intervals, and the averaged values of the entire RR intervals were calculated.

All variables except DC were computed with programs downloaded from PhysioNet (/physiotools/matlab/wfdb/wfdb-app-matlab/).

Study endpoint

The primary endpoint was Glasgow Outcome Scale (GOS) on the 14th day after ROSC. The good outcome group included patients with a good recovery (GOS 1) or moderate disability (GOS 2). The poor outcome group included patients with severe disability (GOS 3), a persistent vegetative state (GOS 4), or death (GOS 5). The GOS level was prospectively assessed by ICU physicians until death or hospital discharge.

---

### Clinical management of mosaic results from preimplantation genetic testing for aneuploidy of blastocysts: a committee opinion [^b0f9d6c1]. Fertility and Sterility (2023). High credibility.

Mosaicism reporting in PGT-A — definition and next-generation sequencing (NGS) inference explains that mosaicism is defined as the presence of more than one chromosomally distinct cell line in one individual, and in humans, any variation from 46 chromosomes is considered aneuploid. Mosaicism is diagnosed after a standard cytogenetic karyotype on samples such as blood or amniotic fluid, whereas next-generation sequencing (NGS), the most common method of analysis in PGT-A, uses a bioinformatics algorithm to measure the amount of DNA represented by each chromosome compared with a normal reference, so in trophectoderm biopsy the diagnosis is inferred by observing an intermediate chromosome copy number on an NGS profile rather than by visual observation of distinct euploid and aneuploid cells. An intermediate copy number between 2 and 3 (disomy and trisomy ranges) may be interpreted as mosaic trisomy, whereas between 1 and 2 (monosomy and disomy ranges) may be interpreted as mosaic monosomy.

---

### A conformational transition of the D' D3 domain primes von Willebrand factor for multimerization [^bf2185ef]. Blood Advances (2022). Medium credibility.

Conformational transition in the D′D3 domain is revealed by MT

After selecting specific VWF tethers, we perform an inverted force ramp protocol, starting at 12 pN and decreasing the force iteratively in steps of 0.3 pN until 6 pN (Figure 2A). In the force-plateaus between 12 and 6 pN, we observe rapid, reversible transitions between a maximum of 3 states, named top, middle, and bottom (Figure 2A, orange inset). The population of these states shifts with decreasing force toward the bottom state, and the midpoint force F 1/2, at which transitions from the M state to the T or B state are equally likely, is found to be at around 8 pN. The 3 states are separated by 2 equidistant steps of Δz ≈ 7.6 nm at the applied forces, suggesting that the transitions stem from conformational changes that occur in each of the monomers independently. To uniquely assign the molecular origin of the observed transitions, we performed control measurements on different domain-deletion constructs, in which individual domains are deleted, but which are otherwise identical to the wild-type dimer. We observe the 3-state transitions independently of A2 unfolding (Figure 3A; supplemental Figure 1A) and independently of the deletion of the D4 domain (Figure 3B; supplemental Figure 1A) or deletion of the A1 domain (Figure 3C; supplemental Figure 1B). Furthermore, the characteristic parameters of the transition, namely the midpoint force and the distance between the states are not altered in the domain deletion constructs (Figure 3D-E). Although domain deletion mutant might possibly have locally altered structures or disulfide bonding patterns, given the complexity of VWF, the fact that the observed transition does not change indicates that the domain that harbors the structural change causing the transitions is unaffected in the deletion mutants. AFM images of the deletion constructs furthermore confirm proper dimerization of the deletion constructs (supplemental Figure 2). Therefore, we can exclude an origin within or related to the A1, A2, or D4 domains. When deleting the D′D3 domain, however, the transitions vanish (supplemental Figure 1C), strongly suggesting that a conformational change in this domain causes the transitions.

---

### Estimation in medical imaging without a gold standard [^82dda7fe]. Academic Radiology (2002). Low credibility.

Rationale and Objectives

In medical imaging, physicians often estimate a parameter of interest (eg, cardiac ejection fraction) for a patient to assist in establishing a diagnosis. Many different estimation methods may exist, but rarely can one be considered a gold standard. Therefore, evaluation and comparison of different estimation methods are difficult. The purpose of this study was to examine a method of evaluating different estimation methods without use of a gold standard.

Materials and Methods

This method is equivalent to fitting regression lines without the x axis. To use this method, multiple estimates of the clinical parameter of interest for each patient of a given population were needed. The authors assumed the statistical distribution for the true values of the clinical parameter of interest was a member of a given family of parameterized distributions. Furthermore, they assumed a statistical model relating the clinical parameter to the estimates of its value. Using these assumptions and observed data, they estimated the model parameters and the parameters characterizing the distribution of the clinical parameter.

Results

The authors applied the method to simulated cardiac ejection fraction data with varying numbers of patients, numbers of modalities, and levels of noise. They also tested the method on both linear and nonlinear models and characterized the performance of this method compared to that of conventional regression analysis by using x-axis information. Results indicate that the method follows trends similar to that of conventional regression analysis as patients and noise vary, although conventional regression analysis outperforms the method presented because it uses the gold standard which the authors assume is unavailable.

Conclusion

The method accurately estimates model parameters. These estimates can be used to rank the systems for a given estimation task.

---

### Retrieving functional pathways of biomolecules from single-particle snapshots [^a082fc12]. Nature Communications (2020). High credibility.

Before doing so, however, we note two important points. First, the extent to which maximum-likelihood clustering is based on functionally relevant conformational coordinates is unknown. The position of a discrete cluster on the relevant energy landscape (Fig. 6a) thus represents a projection from an unknown space onto space spanned by the two most important conformational coordinates relevant to ligand binding. As a result, the minimum-energy conformations (START and FINISH) observed on the energy landscapes may differ from the conformations observed with discrete clustering. Second, interpolation ("morphing") between two or more static structures along a putative functional path is widely acknowledged as invalid, but nonetheless often used, because discrete classification methods provide no information about the functional path traversed between the different discrete structures. Sometimes elastic network models or steered or targeted MD are employed to derive functional pathways. While better than linear morphs, such approaches are also known to manifest serious free-energy artifacts.

We now turn to the conformationally active structural domains and their motions. Figure 6 and Supplementary Movies 3 and 4 compare the displacements revealed along the functional path ("functional analysis" for short) with those inferred from interpolating between the two discrete classes, viz. ligand-free closed, and ligand-bound open conformations, as described in a previous study. These two discrete classes lie close to the energy minima on the ± ligand energy landscapes, i.e. the START and FINISH points of the functional path. As such, the comparison offers the most optimistic assessment of functional inference from discrete clusters.

There are major differences between the results obtained from landscape-based functional analysis and those inferred from interpolation between the discrete clusters. These differences include the structural domains involved in motion, as well as the sequence and extent of displacements (Fig. 6b, c and Supplementary Movies 3–6). For example, in contrast to the results from cluster analysis, functional analysis shows that: (a) the N-terminal domains (NTD) lead the sequence of motions; (b) a significant part of the macromolecule remains rigidly static during function; and (c) the motions in the activation core and shell are coupled (Fig. 6b and Supplementary Movies 3–6).

---

### Transient loss of polycomb components induces an epigenetic cancer fate [^7c6af04e]. Nature (2024). Excellent credibility.

Chromatin analysis at irreversible genes

To identify their unique chromatin features, we focused on irreversible (n = 30) and reversible (n = 42) genes that are direct PcG targets and are covered with the H3K27me3 repressive mark in control EDs (for a full list of PcG target genes, see Supplementary Table 2). Both groups show similar H3K27me3 levels in control tissues (Extended Data Fig. 4a), where they are transcribed at similarly low levels (Fig. 3a). They are also induced at comparable levels on constant ph -KD, ruling out the possibility that weaker PcG repression and/or higher transcriptional levels are the reason for irreversible genes being unable to recover normal transcription after transient ph -KD (Fig. 3a).

Fig. 3
PcG repressive landscape is restored after transient ph -KD.

a, Fragments per kilobase of transcript per million mapped reads (FPKM) of irreversible (pink), reversible (green) and unaffected (grey) genes that are direct PcG targets. Two-sided Wilcoxon test: ✱ p < 5 × 10 −2, ✱✱ p < 1 × 10 −3, ✱✱ p < 1 × 10 −5, NS, P > 0.05. Box plots show the median (line), upper and lower quartiles (box) ± 1.5× interquartile range (whiskers), outliers are not shown. b, Number of irreversible (pink) and reversible (green) genes overlapping an H3K27me3 domain (more than or equal to 50% of the gene body) after no ph -KD (control), constant or transient ph -KD. c, Number of irreversible (pink) and reversible (green) genes overlapping at least one H3K27Ac peak (in the gene body or up to 2.5 kb upstream of the TSS) after no ph -KD (control), constant or transient ph -KD. d, Screenshot of PH ChIP–seq, H3K27me3, H2AK118Ub and H3K27Ac CUT&RUNs tracks at representative irreversible (left) or reversible (right) loci under the indicated conditions (left). e, f, For H3K27me3 domains (e) and H3K27Ac peaks (f), fold changes are shown as a function of their average-normalized counts across all samples (baseMean) for constant (left) or transient (right) ph -KD conditions. Significant changes are highlighted using a colour code (colour legend). g, The H3K27me3 fold changes (between constant or transient ph- KD and no ph -KD conditions) at H3K27me3 domains that are found in the control sample (no ph -KD) and overlap irreversible (pink) or reversible (green) genes. All H3K27me3 domains are shown for reference (grey). Two-sided Wilcoxon test: ✱ p < 5 × 10 −2, ✱✱ p < 1 × 10 −2, ✱✱ p < 1 × 10 −3, ✱✱ p < 1 × 10 −5, NS, P > 0.05. Box plots show the median (line), upper and lower quartiles (box) ± 1.5× interquartile range (whiskers), outliers are not shown. h, The H3K27Ac fold changes at H3K27Ac peaks overlapping the H3K27me3 domains found in control sample (no ph -KD) and overlapping the irreversible (pink) or reversible (green) genes. All H3K27Ac peaks overlapping control H3K27me3 domains are shown for reference (grey). Two-sided Wilcoxon test: ✱✱ p < 1 × 10 −5, NS. P > 0.05. Box plots show the median (line), upper and lower quartiles (box) ± 1.5× interquartile range (whiskers), outliers are not shown.

---

### Design of facilitated dissociation enables timing of cytokine signalling [^3723084f]. Nature (2025). Excellent credibility.

Such a mechanism would require new switches that, in state X and throughout the conformational transition, retain an open effector-binding cleft where a flexible effector could associate (Fig. 1e and Supplementary Fig. 4d). Starting from AS0, we constructed a new state X by shifting the two domains from their state Y positions relative to each other (by one heptad along the helix of domain 1 that contacts domain 2), building a new loop between the domains, and optimizing single sequences that could adopt both this new state X and the original effector-bound state Y (Supplementary Fig. 4e and Methods). This approach maintains the open cleft in state X by introducing minimal rotation from state Y and simplifies the multi-state sequence-design challenge by minimizing the local structural differences between the two states. Transition between conformational states occurs by a register shift throughout which the cleft could remain open and bind the effector (Supplementary Fig. 4f). Because these new hosts retain the state Y backbone validated to clash with the target in AS0, we should observe allosteric coupling if the switch works as designed (Fig. 1e).

---

### Abstract representations emerge naturally in neural networks trained to perform multiple tasks [^27480aa5]. Nature Communications (2023). High credibility.

Importantly, each of these three components of our framework is trained in sequence to each other: The input model (Fig. 1 c) is trained first and then frozen. The input model is used to generate the training data for the multi-tasking model (Fig. 1 d), which is trained second. Then, finally, we use our abstraction metrics (Fig. 1 e) to quantify the level of abstraction present in the representation layer of the trained multi-tasking model (and in the trained standard input, as in Fig. 2).

Fig. 2
The input model.

a Schematic of the input model. Here, schematized for D = 2 latent variables. The quantitative results are for D = 5. b The 2D response fields of 25 random units from the high-d input layer of the standard input. c The same concentric square structure shown to represent the latent variables in a after being transformed by the standard input. d (left) The per-unit sparseness (averaged across units) for the latent variables (S = 0, by definition) and standard input (S = 0.97). (right) The embedding dimensionality, as measured by participation ratio, of the latent variables (5, by definition) and the standard input (~190). e Visualization of the level of abstraction present in the high-d input layer of the standard input, as measured by the classifier generalization metric (left) and regression generalization metric (right). In both cases, the y-axis shows the distance from the learned classification hyperplane (right: regression output) for a classifier (right: regression model) trained to decode the sign of the latent variable on the y-axis (right: the value of the latent variable on the y-axis) only on representations from the left part of the x-axis (trained). The position of each point on the x-axis is the output of a linear regression for a second latent variable (trained on the whole latent variable space). The points are colored according to their true category (value). f The performance of a classifier (left) and regression (right) when it is trained and tested on the same region of latent variable space (trained) or trained in one region and tested in a non-overlapping region (tested, similar to e). Both models are trained directly on the latent variables and on the input representations produced by the standard input. The gray line is chance. The standard input produces representations with significantly decreased generalization performance.

---

### The 2023 Society for Vascular Surgery, American Venous Forum, and American Vein and Lymphatic Society clinical practice guidelines for the management of varicose veins of the lower extremities. part II: endorsed by the Society of Interventional Radiology and the Society for Vascular Medicine [^6042c54b]. Journal of Vascular Surgery: Venous and Lymphatic Disorders (2024). High credibility.

Revised Venous Clinical Severity Score (VCSS) — domains and thresholds cover pain or other discomfort, varicose veins, venous edema, skin pigmentation, inflammation, induration, active ulcer parameters, and compression therapy; varicose veins must be ≥ 3 mm in diameter to qualify; active ulcer number categories are 0, 1, 2, and ≥ 3; active ulcer duration categories are N/A, < 3 Months, > 3 Months but < 1 year, and Not healed for > 1 year; active ulcer size categories are N/A, Diameter < 2 cm, Diameter 2–6 cm, and Diameter > 6 cm; and compression therapy scoring is 0: Not used, 1: Intermittent use of stockings, 2: Wears stockings most days, and 3: Full compliance: stockings.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^dee6cacc]. CDC (2011). Medium credibility.

Community viral load sample size calculations — sample size to detect differences in geometric mean (GM) viral load depends on power and standard deviation (S). The table specifies "α = 0.05 and W = 0.8", with S columns 1 to 1.5; for k = 3 the minimum sample sizes by S are 54, 66, 78, 92, 106, and 122, and "1.2 is the standard deviation of national VL data", so at S = 1.2 the needed sample size is 78.

---

### Canadian Urological Association guideline: diagnosis, management, and followup of the incidentally discovered adrenal mass [^e31d2caa]. Canadian Urological Association Journal = Journal de L'Association des Urologues du Canada (2023). High credibility.

Regarding follow-up and surveillance for adrenal incidentaloma, more specifically with respect to laboratory follow-up, CUA 2023 guidelines recommend to do not obtain further follow-up function testing in patients with adrenal lesions growing < 3 mm/year on follow-up imaging.

---

### Faster embryonic segmentation through elevated delta-notch signalling [^e1e4fd29]. Nature Communications (2016). Medium credibility.

An important step in understanding biological rhythms is the control of period. A multicellular, rhythmic patterning system termed the segmentation clock is thought to govern the sequential production of the vertebrate embryo's body segments, the somites. Several genetic loss-of-function conditions, including the Delta-Notch intercellular signalling mutants, result in slower segmentation. Here, we generate DeltaD transgenic zebrafish lines with a range of copy numbers and correspondingly increased signalling levels, and observe faster segmentation. The highest-expressing line shows an altered oscillating gene expression wave pattern and shortened segmentation period, producing embryos with more, shorter body segments. Our results reveal surprising differences in how Notch signalling strength is quantitatively interpreted in different organ systems, and suggest a role for intercellular communication in regulating the output period of the segmentation clock by altering its spatial pattern.

---

### Principles of analytic validation of immunohistochemical assays: guideline update [^cda8439c]. Archives of Pathology & Laboratory Medicine (2024). High credibility.

Supplemental Digital Content — AMSTAR (Assessing the Methodological Quality of Systematic Reviews) appraisal of methodological quality presents checklist items marked with "√" or "x": "Quality assessed & documented" is "√" and "x"; "Quality used appropriately for conclusion" is "√" and "x"; "Methods to combine used appropriately" is "√" and "√"; "Publication bias assessed" is "x" and "x"; and "COI (conflict of interest)" is "√" and "√". The AMSTAR SCORE /11 is 9 and 5.

---

### Statistical considerations in the evaluation of continuous biomarkers [^c7960d51]. Journal of Nuclear Medicine (2021). Medium credibility.

Discovery of biomarkers has been steadily increasing over the past decade. Although a plethora of biomarkers has been reported in the biomedical literature, few have been sufficiently validated for broader clinical applications. One particular challenge that may have hindered the adoption of biomarkers into practice is the lack of reproducible biomarker cut points. In this article, we attempt to identify some common statistical issues related to biomarker cut point identification and provide guidance on proper evaluation, interpretation, and validation of such cut points. First, we illustrate how discretization of a continuous biomarker using sample percentiles results in significant information loss and should be avoided. Second, we review the popular "minimal- P -value" approach for cut point identification and show that this method results in highly unstable P values and unduly increases the chance of significant findings when the biomarker is not associated with outcome. Third, we critically review a common analysis strategy by which the selected biomarker cut point is used to categorize patients into different risk categories and then the difference in survival curves among these risk groups in the same dataset is claimed as the evidence supporting the biomarker's prognostic strength. We show that this method yields an exaggerated P value and overestimates the prognostic impact of the biomarker. We illustrate that the degree of the optimistic bias increases with the number of variables being considered in a risk model. Finally, we discuss methods to appropriately ascertain the additional prognostic contribution of the new biomarker in disease settings where standard prognostic factors already exist. Throughout the article, we use real examples in oncology to highlight relevant methodologic issues, and when appropriate, we use simulations to illustrate more abstract statistical concepts.

---

### Statistical analysis and optimality of neural systems [^bf829ef6]. Neuron (2021). Medium credibility.

Normative theories and statistical inference provide complementary approaches for the study of biological systems. A normative theory postulates that organisms have adapted to efficiently solve essential tasks and proceeds to mathematically work out testable consequences of such optimality; parameters that maximize the hypothesized organismal function can be derived ab initio, without reference to experimental data. In contrast, statistical inference focuses on the efficient utilization of data to learn model parameters, without reference to any a priori notion of biological function. Traditionally, these two approaches were developed independently and applied separately. Here, we unify them in a coherent Bayesian framework that embeds a normative theory into a family of maximum-entropy "optimization priors". This family defines a smooth interpolation between a data-rich inference regime and a data-limited prediction regime. Using three neuroscience datasets, we demonstrate that our framework allows one to address fundamental challenges relating to inference in high-dimensional, biological problems.

---

### Direct observation of exceptional points in coupled photonic-crystal lasers with asymmetric optical gains [^7a084ae8]. Nature Communications (2016). Medium credibility.

Identification of lasing modes

To fully understand our experimental results, we investigated the measured PL spectra as a function of the pump position in the coupled cavities with large-area/small-area graphene covers and without graphene, using the theoretical model of non-Hermitian coupled optical cavities with asymmetric gains in Equation 1 (Fig. 4). As the focused pump light with a spot size of ∼3.0 μm sweeps the coupled cavities in the experiment, the induced optical gains of cavities 1 and 2 experience spatial distributions that vary as a function of the pump position. In our model, we reflect this situation by setting the optical gains of cavities 1 and 2 as simplified Gaussian gain profiles with a full-width at half-maximum of 3.0 μm and central positions of x pump = −0.6 and 0.6 μm, respectively. The gain of cavity 1 also needs to be considered as an effective gain, γ 1, eff = γ 1 − κ graphene, where κ graphene is the optical loss of graphene, while the gain of cavity 2 is not affected by graphene (see Methods). Moreover, as we observed lasing modes in the entire pumping range from −2.2 to 2.2 μm, it is reasonable to define the net optical gains of cavities 1 and 2 as γ 1, eff − γ th (red lines in Fig. 4a–c) and γ 2 − γ th (green lines in Fig. 4a–c), respectively, where γ th is the threshold gain for the lasing of a single PhC cavity. The gain contrast between the net gains was calculated as Δ γ = 1/2 × | γ 2 − γ 1, eff | (black lines in Fig. 4a–c). Using the experimentally determined f 0 and J in each coupled cavity, we calculated Re(f) from Equation 1 and the corresponding resonance wavelength, c /Re(f), where c is the speed of light. The value of J was obtained from the wavelength difference between the bonding and anti-bonding modes in Figs 1i and 2d, and also confirmed by numerical simulations (see Methods).

---

### Truncated product method for combining P-values [^f492b3a2]. Genetic Epidemiology (2002). Low credibility.

We present a new procedure for combining P-values from a set of L hypothesis tests. Our procedure is to take the product of only those P-values less than some specified cut-off value and to evaluate the probability of such a product, or a smaller value, under the overall hypothesis that all L hypotheses are true. We give an explicit formulation for this P-value, and find by simulation that it can provide high power for detecting departures from the overall hypothesis. We extend the procedure to situations when tests are not independent. We present both real and simulated examples where the method is especially useful. These include exploratory analyses when L is large, such as genome-wide scans for marker-trait associations and meta-analytic applications that combine information from published studies, with potential for dealing with the "publication bias" phenomenon. Once the overall hypothesis is rejected, an adjustment procedure with strong family-wise error protection is available for smaller subsets of hypotheses, down to the individual tests.

---

### An official American Thoracic Society workshop report: developing performance measures from clinical practice guidelines [^369e6d46]. Annals of the American Thoracic Society (2014). Medium credibility.

Agency for Healthcare Research and Quality performance measure attributes — Desirable features of performance measures include relevance to stakeholders and addressing important aspects of health, with evidence of a need for the measure; evidence should be explicitly stated, results should be reproducible and truly measure what they purport to measure, and specifications should explicitly define the numerator and denominator with understandable data collection requirements; necessary data sources should be available within the measurement timeframe and data collection costs justified by potential improvement in care or health.

---

### Canadian Urological Association guideline: diagnosis, management, and followup of the incidentally discovered adrenal mass [^a8d9e778]. Canadian Urological Association Journal = Journal de L'Association des Urologues du Canada (2023). High credibility.

Regarding follow-up and surveillance for adrenal incidentaloma, more specifically with respect to imaging follow-up, CUA 2023 guidelines recommend to do not obtain further follow-up imaging in patients with adrenal lesions growing < 3 mm/year on follow-up imaging.

---

### Online legal driving behavior monitoring for self-driving vehicles [^52c5b579]. Nature Communications (2024). High credibility.

The trigger condition for Article 38.1 is that the ego vehicle overlaps with StopLine and the traffic light is yellow or red.

The ego vehicle should not enter the intersection under a red light. When the yellow light is on and the ego vehicle has not yet entered the intersection, it should not enter the intersection.where t in represents the moment when the ego vehicle starts to overlap with the stop line and t y represents the initial moment when the yellow light turns on.

In this study, we analyzed the right-of-way using the example of a left-turn and straight-moving conflict scenario. The corresponding trigger condition is when a left-turning vehicle intrudes into the virtual lane line of the oncoming straight-moving traffic.

Where VirtualLane_O represents the virtual lane of oncoming straight-moving traffic.

When the ego vehicle intrudes into the virtual lane line of the oncoming straight-moving traffic, it should ensure that TTIdiff exceeds. The specific expressions are as follows:where Tgt os represents the oncoming straight-moving vehicles.

Indicators for compliance threshold determination on highway

During the threshold selection process, in addition to excluding abnormal data, we calculated certain indicators to obtain the final thresholds. The removal of abnormal data is described in the Supplementary Methods. The selection of the high-speed thresholdsandis relatively straightforward. However, for determining thethreshold, an optimization algorithm is used to determine the optimal threshold line.

The longitudinal and lateral RSS distances are calculated using parameter values from previous studies. The longitudinal and lateral RSS distances can be calculated as follows:where v r and v f represent the velocities of the rear and forward vehicles, respectively, and v 1 and v 2 represent the lateral velocities of the rear and forward vehicles, respectively.

The fitness function of the threshold line optimization is defined as follows:where f represents the threshold fitting curve ofand f = a ⋅ x + b, with a, and b being undetermined coefficients. P FP represents the false-positive rate, and r FP represents the maximum allowable false-positive rate. The cost function is defined as the weighted sum of N TP and N TN. N TP represents the number of true positives, N TN represents the number of true negatives, Q is the weighting coefficients.

---

### Retrieving functional pathways of biomolecules from single-particle snapshots [^7486df52]. Nature Communications (2020). High credibility.

The majority of experimentally determined energy landscapes have involved one conformational coordinate, have been compromised by low spatial resolution, and/or limited by the inability to account for the multiple energy landscapes associated with the vast variety of biological functions involving ligands and cofactors.

The combination of recent resolution improvements in cryo-EM with new data-analytical and molecular simulation techniques now offers an unprecedented opportunity to identify functional paths on multiple experimentally determined energy landscapes, and compile all-atoms movies of complex biological functions, including those involving ligands and cofactors.

At present, cryo-EM is widely used to infer functional information from static structures obtained by powerful maximum-likelihood classification methods, which sort single-particle snapshots into a user-defined number of discrete conformational clusters. These methods incorporate continuous conformations as admixtures of orientationally independent, rigid domain structures. In general, the positions and sequence of these structures in the macromolecular work cycle are unknown, and it is not always easy to assess the relevance of these structures to function. Under such circumstances, the functional inference is based on interpolations between static structures, if only conceptually. In the absence of additional information, arranging such discrete structures along a functionally relevant pathway is difficult. Since the number of ways in which two discrete structures can be transformed into each other is essentially unlimited, reliable functional inference by discrete clustering is not straightforward.

The primary goals of this paper are as follows: (i) demonstrate that energy landscapes associated with complex biological function can be extracted from experimental data, and corroborated by molecular simulations, (ii) elucidate the conformational paths associated with complex biological function in all-atoms detail, including possible routes for transitions between different landscapes, (iii) establish that motions associated with functional paths on energy landscapes can be significantly different from those inferred by discrete clustering methods, (iv) outline the new biological insights gained by studying the continuous conformational changes associated with function, and (v) render the algorithms used in this paper widely accessible.

We use cryo-EM single-particle snapshots of ryanodine receptor type 1 (RyR1) to exemplify the discovery process facilitated, and the new insights revealed by our approach. RyRs are calcium-activated calcium channels critical to excitation/contraction coupling in heart and skeletal muscle. Malfunctions in RyR1 and RyR2 can lead to calcium leaks deleterious to heart and skeletal muscle function. Insights into RyR channel function and regulation are therefore critical in understanding the role of disease-causing mutations and identifying pharmacological leads.

---

### Assessment of right ventricular function in the research setting: knowledge gaps and pathways forward. An official American Thoracic Society research statement [^095050f9]. American Journal of Respiratory and Critical Care Medicine (2018). Medium credibility.

American Thoracic Society research statement — purpose and topic domains: The purpose of this document is to: 1. Comprehensively elucidate the current understanding of RV function and its assessment in healthy individuals and those with preclinical, acute, and chronic diseases; 2. Identify major knowledge gaps; and 3. Outline specific strategies for addressing these gaps. We sought to address these objectives across three topic domains, including Topic Domain 1 Optimizing the methodology to assess RV function in acute and chronic conditions in pre-clinical models, human studies, and clinical trials; Topic Domain 2: Analyzing Advanced RV Hemodynamic Parameters at Rest and in Response to Exercise; and Topic Domain 3: Deciphering the Underlying Molecular and Pathogenic Mechanisms of RV Function and Failure.

---

### Standards of care in diabetes – 2025 [^31be65a6]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for hypoglycemia, more specifically with respect to screening for cognitive impairment, ADA 2025 guidelines recommend to simplify diabetes treatment plans as much as possible for patients with cognitive impairment and tailor to minimize the risk of hypoglycemia.

---

### Assessment of right ventricular function in the research setting: knowledge gaps and pathways forward. An official American Thoracic Society research statement [^7045d567]. American Journal of Respiratory and Critical Care Medicine (2018). Medium credibility.

Topic domain 1 scope — Topic Domain 1: Optimizing the Methodology to Assess RV Function in Acute and Chronic Conditions in Preclinical Models, Human Studies, and Clinical Trials introduces a subsection titled Normative Indices of RV Morphology and Function.

---

### Large-scale mapping and mutagenesis of human transcriptional effector domains [^449366c2]. Nature (2023). Excellent credibility.

Human gene expression is regulated by more than 2,000 transcription factors and chromatin regulators 1,2. Effector domains within these proteins can activate or repress transcription. However, for many of these regulators we do not know what type of effector domains they contain, their location in the protein, their activation and repression strengths, and the sequences that are necessary for their functions. Here, we systematically measure the effector activity of more than 100,000 protein fragments tiling across most chromatin regulators and transcription factors in human cells (2,047 proteins). By testing the effect they have when recruited at reporter genes, we annotate 374 activation domains and 715 repression domains, roughly 80% of which are new and have not been previously annotated 3–5. Rational mutagenesis and deletion scans across all the effector domains reveal aromatic and/or leucine residues interspersed with acidic, proline, serine and/or glutamine residues are necessary for activation domain activity. Furthermore, most repression domain sequences contain sites for small ubiquitin-like modifier (SUMO)ylation, short interaction motifs for recruiting corepressors or are structured binding domains for recruiting other repressive proteins. We discover bifunctional domains that can both activate and repress, some of which dynamically split a cell population into high- and low-expression subpopulations. Our systematic annotation and characterization of effector domains provide a rich resource for understanding the function of human transcription factors and chromatin regulators, engineering compact tools for controlling gene expression and refining predictive models of effector domain function.

---

### Incorporating economic evidence in clinical guidelines: a framework from the clinical guidelines committee of the American College of Physicians [^4483eea4]. Annals of Internal Medicine (2025). High credibility.

Figure 1 — comparative value of interventions — categorizes value by clinical net benefit and incremental cost for quality-adjusted life-year (QALY) with intervention versus comparator, where the incremental cost dimension reflects the incremental cost-effectiveness ratio per QALY gained. For favorable clinical net benefit, higher cost corresponds to high value (< $100 000), intermediate value ($100 000 to $200 000), or low value (> $200 000) with the intervention more effective and more costly; lower cost corresponds to high value with the intervention more effective and less costly (cost-saving). For unfavorable clinical net benefit, higher cost indicates no value with the intervention less effective and more costly (cost-dominated), and lower cost indicates no value with the intervention less effective and less costly.

---

### Canadian Urological Association guideline: diagnosis, management, and followup of the incidentally discovered adrenal mass [^c26c594d]. Canadian Urological Association Journal = Journal de L'Association des Urologues du Canada (2023). High credibility.

Regarding follow-up and surveillance for adrenal incidentaloma, more specifically with respect to imaging follow-up, CUA 2023 guidelines recommend to do not obtain further follow-up imaging in patients with benign nonfunctioning adenomas < 4 cm, myelolipomas, and other small masses containing macroscopic fat detected on the initial workup for an AI.