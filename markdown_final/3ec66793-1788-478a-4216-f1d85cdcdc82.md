# Medical Question & Answer

**Sample ID**: 3ec66793-1788-478a-4216-f1d85cdcdc82
**Dataset Index**: 163414

---

## Question

A sequence of positive integers a₁, a₂, … is such that for each m and n the following holds: if m is a divisor of n and m < n, then aₘ is a divisor of aₙ and aₘ < aₙ. Find the least possible value of a₂₀₀₀.

---

## Answer

> Let's see… What do we have here? The user is asking for the least possible value of a positive integer sequence element a₂₀₀₀, given that whenever m divides n and m < n, a_m divides a_n and a_m < a_n. Let's break this down step-by-step. First, I need to think about how the divisibility and strict monotonicity constraints propagate along the divisor lattice. Then, I should verify the minimal growth pattern that satisfies these constraints. Next, I will compute the minimal value of a₂₀₀₀ by following that pattern. Finally, I will double-check that no smaller construction is possible and that the proposed sequence indeed meets all conditions.

> Let me first confirm the core constraint structure. The condition "if m divides n and m < n, then a_m divides a_n and a_m < a_n" means that along any chain of proper divisors, the sequence must strictly increase and each earlier term must divide the later terms. This implies that the sequence is strictly increasing along divisor chains and that divisibility is preserved upward, which is a multiplicative analogue of a strictly increasing function on the poset of positive integers ordered by divisibility [^notfound].

> I need to think about the minimal way to grow the sequence. The smallest strictly increasing sequence of positive integers is 1, 2, 3, 4,…, but wait, let me verify whether that satisfies the divisibility requirement. It does not, because 2 does not divide 3, so I cannot simply use the identity function. The next simplest idea is to make each term the smallest integer greater than the previous term that also satisfies the divisibility constraints coming from its divisors, which pushes me toward a multiplicative construction rather than an additive one [^notfound].

> Let me consider the prime factorization perspective. Any integer n can be written as a product of primes, and its divisors correspond to subproducts. To ensure that a_m divides a_n whenever m divides n, a natural minimal choice is to let a_n depend multiplicatively on the exponents in the prime factorization of n. The simplest such form is aₙ = 2^Ω(n), where Ω(n) is the total number of prime factors of n counted with multiplicity, because Ω is additive over multiplication and thus respects the divisibility lattice structure [^notfound].

> Hold on, I should verify that this choice satisfies both required properties. First, if m divides n and m < n, then Ω(m) < Ω(n), so 2^Ω(m) < 2^Ω(n), which gives strict monotonicity. Second, since Ω(m) ≤ Ω(n), 2^{Ω(m)} divides 2^{Ω(n)}, so the divisibility condition holds. Therefore, aₙ = 2^Ω(n) is a valid sequence that meets the constraints [^notfound].

> Now, I will compute a_2000 using this construction. I need to factor 2000: 2000 = 2⁴ × 5³, so Ω(2000) = 4 + 3 = 7, which means a₂₀₀₀ = 2⁷ = 128. Wait, let me double-check that I'm not missing a smaller valid construction; I initially wondered if a_n = n + 1 might work, but that fails divisibility (for example, a_2 = 3 does not divide a_4 = 5), so the exponential construction appears to be the minimal valid pattern here [^notfound].

> But wait, what if I try to make the sequence grow more slowly by using a smaller base or a different multiplicative rule? Let me reconsider. Any slower growth would risk violating either the strict increase or the divisibility condition somewhere in the lattice, and because Ω(n) captures the minimal "height" of n in the divisor poset, 2^{Ω(n)} is the minimal exponential sequence that respects both constraints. Thus, 128 is indeed the least possible value for a₂₀₀₀ under the given rules [^notfound].

---

The least possible value of a₂₀₀₀ is **128**. This is achieved by setting aₙ = 2^Ω(n), where Ω(n) is the total number of prime factors of n counted with multiplicity. This construction ensures that if m ∣ n and m < n, then aₘ ∣ aₙ and aₘ < aₙ, and it minimizes a₂₀₀₀ because Ω(2000) = 7, so a₂₀₀₀ = 2⁷ = 128.

---

## Construction of the sequence

Define the sequence aₙ as aₙ = 2^Ω(n), where Ω(n) is the total number of prime factors of n counted with multiplicity. For example:

- Ω(1) = 0 ⇒ a₁ = 2⁰ = 1
- Ω(2) = 1 ⇒ a₂ = 2¹ = 2
- Ω(4) = 2 ⇒ a₄ = 2² = 4
- Ω(6) = 2 ⇒ a₆ = 2² = 4
- Ω(2000) = 7 ⇒ a₂₀₀₀ = 2⁷ = 128

---

## Verification of the conditions

We need to verify that this sequence satisfies the given conditions: if m ∣ n and m < n, then aₘ ∣ aₙ and aₘ < aₙ.

- **Divisibility**: If m ∣ n, then Ω(m) ≤ Ω(n), so 2^Ω(m) ∣ 2^Ω(n), which means aₘ ∣ aₙ.
- **Strict inequality**: If m ∣ n and m < n, then Ω(m) < Ω(n), so 2^Ω(m) < 2^Ω(n), which means aₘ < aₙ.

Thus, the sequence (a_n = 2^{\\Omega(n)}) satisfies both conditions.

---

## Minimality of (a₂₀₀₀)

To show that 128 is the least possible value, consider that any valid sequence must satisfy aₙ ≥ 2^Ω(n). This is because the sequence must strictly increase along chains of divisors, and the minimal such increase is by a factor of 2 at each prime factor. Since Ω(2000) = 7, the minimal possible value of a₂₀₀₀ is 2⁷ = 128.

---

## Conclusion

The least possible value of a₂₀₀₀ is **128**, achieved by the sequence aₙ = 2^Ω(n).

---

## References

### Ultra-high dynamic range quantum measurement retaining its sensitivity [^ff9759ff]. Nature Communications (2021). High credibility.

So far in the examples with our algorithm, we used halved areas (A n = A 0 /2 n for integer n ≥ 0). Even though the uncertainty is mostly defined by the largest area, and the range by the smallest, the middle areas are important for reaching the lowest uncertainty (see Fig. 3c: they partake in the optimal combination). Adding more areas at integer multiples of the smallest area decreases the uncertainty, though slightly (see Supplementary Note 4).

Algorithm measurement

In Fig. 2b, measurement results of our algorithm in the steep region are plotted (for details of the measurement see Supplementary Note 5), together with the Heisenberg limit (which is only true for a small range and infinite T 2) and the approximate large-range limit explained in Supplementary Note 3. As mentioned before, and just like in Fig. 3d, the focus is on the scaling that originates from the algorithm, hence all overhead time is ignored. Our algorithm is very close to the limit, as could be expected since at long measurement times most time is spent on the sequence with the largest area. Moreover, our results scale approximately as, which is less steep than the Heisenberg-like scaling of, since our algorithm keeps approaching this limit.

When merely halving areas in a measurement sequence, its range is defined by the smallest area. Therefore, it would only improve the uncertainty with respect to the standard single-area measurement, but not the range. In this way, given a limit on the time delay between the π/2-pulses, for example owing to a maximum time resolution or waiting time requirements, the maximum range is restricted. However, the range of our algorithm is the inverse of the greatest common divisor of the frequencies in measured signal of all included areas (see Supplementary Note 6). For halved areas, since all larger areas are integer multiples of the smaller ones, this means that the greatest common divisor is the lowest frequency, thus the one related to the smallest area. To increase the range beyond this limit, we combine areas that are not integer multiples of each other. When purely looking at the range, combining two sequences for slightly different areas increases the range far beyond the standard measurement's range. Thus in principle, the range can be extended unlimitedly. Adding the large areas as well, it is still possible to get arbitrarily close to the ultimate uncertainty (for details see Supplementary Note 6).

---

### Integrating genomics and metabolomics for scalable non-ribosomal peptide discovery [^eff56b8c]. Nature Communications (2021). High credibility.

Using recursive formula (1), NRPminer calculates numCoreNRPs A using parametric dynamic programming in a bottom-up manner: NRPminer first, computes numCoreNRPs A (1, s), for all positive integers s ≤ maxScore A. then proceeds to numCoreNRPs A (2, s) for all such s, and so on, computing numCoreNRPs A (n, s) for all such 0 < s. Using this approach, for each value of i and s, NRPminer computes numCoreNRPs A (i, s) by summing over at most k values. Therefore, NRPminer calculates all values of numCoreNRPs A with time complexity O(k × n × maxScore A).

Given a positive integer N < 10 5, let scor (A, N) be the greatest integer s ′ ≤ maxScore A such that, N ≤ Σ s ' ≤ s ≤ maxScore numCoreNRPs A (n, s).

Then, we define

NRPminer selects, candidateCoreNRPs A (N), defined as the set of all core NRPs of A, with adenylation score at least thresholdScore A (N). NRPminer selects core NRPs candidateCoreNRPs A (N) for downstream spectral analyses. Using this approach, NRPminer is guaranteed to be scalable as at most 10 5 candidate core NRPS are explored per assembly line.

Table 3 presents the values of numCoreNRPs SurugamideAL (8, s) for various values of s. Note that, this table presents the number of core NRP only for a single assembly line, SurugamideAL, corresponding to cyclic surugamides (surugamide A–D). In total, 14,345 core NRPs were retained from the original 3,927,949,830 core NRPs of the 11 assembly lines of surugamide's BGC.

---

### A neural theory for counting memories [^b797eb11]. Nature Communications (2022). High credibility.

Theorem 2'

There is an absolute constant c for which the following holds. Suppose the neural count sketch sees n observations satisfying Assumption 1' with. Pick any 0 < δ < 1.
Suppose that m ≥ 2 k n and thatfor a positive integer f. Then with probability at least 1 − δ, when presented with a query x (i) with 0 ≤ f i ≤ f, the response of the neural count sketch will lie in the range f i ± 1.
Suppose that m ≥ 2 k / ϵ for some ϵ > 0 and that. Then with probability at least 1 − δ, when presented with a query x (i), the response of the neural count sketch will lie in the range f i ± ϵ n.

Note that the query x (i) need not belong to the original sequence of n observations, in which case f i = 0.

Theorem 5 gives bounds that are significantly more favorable for the 1-2-3-many sketch.

Theorem 5'

Suppose the 1-2-3-many sketch, with parameter β = 1, witnesses n observations that satisfy Assumption 1' with. Pick any 0 < δ < 1 and suppose that m ≥ 2 k N and. Then with probability at least 1 − δ, when presented with a query x (i), the response of the sketch will be e − r for some value r that is either f i or f i + 1 when rounded to the nearest integer.

Overall, these mathematical proofs provide bounds on how accurately stimuli can be tracked using the two neural count sketches.

The Drosophila mushroom body implements the anti-Hebbian count sketch

Here, we provide evidence supporting the "1-2-3-many" model from the olfactory system of the fruit fly, where circuit anatomy and physiology have been well-mapped at synaptic resolution. The evidence described below includes the neural architecture of stimulus encoding, the plasticity induced at the encoding-decoding synapse, and the response precision of the decoding (counting) neuron. The latter two we derive from a re-analysis of data detailing novelty detection mechanisms in the fruit fly mushroom body, where odor memories are stored.

---

### Integrating genomics and metabolomics for scalable non-ribosomal peptide discovery [^32f62782]. Nature Communications (2021). High credibility.

Table 3
Number of core NRPs of SurugamideAL (assembly line corresponding to cyclic surugamides A–D) according to their adenylation scores.

Only values of s with non-zero number of cores and corresponding to the top 1000 high-scoring core NRPs are shown.

Problem 2. Given an assembly line A and a positive integer N, generate candidateCoreNRPs A (N), defined as all core NRPs of A with adenylation scores at least thresholdScore A (N).

NRPminer follows a graph-theoretic approach to quickly generate candidateCoreNRPs A (N) by using the computed values of numCoreNRPs. Let G (A) be the acyclic directed graph with nodes corresponding to pairs of positive integers i ≤ n and s ≤ maxScore A, such that numCoreNRPs A (i, s) > 0, denoted by. For every node(i = 1,…, n) and everysuch that numCoreNRPs A (i−1, s−S A (i, a)) > 0, there exists a directed edge fromto. Let Source beand let Sink be the set of all nodessuch that thresholdScore A (N). We call each directed path in G (A) from Source to the nodes in Sink as a candidate path of G(A).

Each candidate path of G (A) corresponds to a distinct core NRP of A with adenylation score at least thresholdScore A (N) and vice versa. Therefore, the problem of finding all core NRPs of A with adenylation score at least thresholdScore A (N) corresponds to the problem of finding all candidate paths of G (A). While enumerating all paths with n nodes in a directed acyclic graph can grow exponentially large (as there can be exponentially many such paths), but due to our choice of thresholdScore A (N), the number of candidate paths of G (A) is bound by 10 5 (or N if). NRPminer uses the default value N = 1000. Moreover, n ≤ 20 (only assembly lines made up of up to 20 A-domains are considered) and k 1.200000000000000e+01

---

### Relative rate and location of intra-host HIV evolution to evade cellular immunity are predictable [^34a620c1]. Nature Communications (2016). Medium credibility.

Sequence compression

Even with the use of sophisticated algorithms, solving the inverse Potts problem remains a challenging computational task. This task is complicated by the large number of parameters in the model, equal to N (q - 1) (N (q - 1)+ 1)/ 2, where N is the length of the protein sequence and q is the number of states, assuming that this number is the same for each residue. Choosing q = 21 for the 20 possible amino acids plus 1 gap state, we would require more than two million variables to parameterize the Potts model for a protein of length 100, a typical length scale for HIV proteins.

Fortunately, it is not necessary to include all possible amino acids at each residue in the model explicitly to obtain a useful characterization of the sequence distribution. We adaptively adjusted the number of states allowed at each residue based on the frequencies with which different amino acids are observed there in the MSA. Our procedure for choosing the number of states q i at each residue i is as follows. First, we order the amino acids at residue i according to how frequently they are observed in the MSA, such that

The Shannon entropy of the distribution of amino acids at this residue can be written as

as the p i *(a) must sum to one when summed over all states a. Then, we set q i equal to the smallest integer q, such that

That is, we choose a number of states q i such that the reduced representation captures at least 90% of the full entropy of the distribution of amino acids at that residue. The q i -1 most frequently observed amino acids at that residue each map to particular Potts states. All the remaining, infrequently observed amino acids map to a single aggregate state.

Our choice of the number of states to model at each residue is adaptive, compressing the amino acid alphabet heavily at residues where little variation is observed, but allowing for a larger number of states when many different amino acids are present at nontrivial frequencies. The particular choice of cutoff given in equation (11) leads to the consideration of multiple states even in conserved proteins such as Gag, while still limiting the number of states sufficiently that the inverse Potts problem remains computationally tractable for the more highly variable proteins studied here, such as Nef and gp41. Successful prediction of higher-order statistics of the sequence distributions suggests that the predictive power of the model is not compromised by our convention for sequence compression (Supplementary Fig. 1).

---

### Integrating genomics and metabolomics for scalable non-ribosomal peptide discovery [^f956b07f]. Nature Communications (2021). High credibility.

For many organisms, the total number of possible core NRPs is prohibitively large, making it infeasible to conduct search against massive spectral repositories. Currently, even the fastest state-of-the-art spectral search methods are slow for searching millions of input spectra against databases with over 10 5 peptides in a modification-tolerant manner as the runtime grows exceedingly large when the database size grows. Supplementary Tables S2 and S7 shows that for 24 (22) out for 27 organisms in XPF dataset and 9 (7) out of 20 organisms in SoilActi dataset, the total number of core NRPs exceed 10 5 (10 6). Therefore, to enable scalable peptidogenomics for NRP discovery, for each constructed assembly line NRPminer, selects a set of candidate core NRPs. To do so, NRPminer starts by finding the number of core NRPs of A according to their adenylation scores (Problem 1) and then it uses these numbers for generating all core NRPs of A with adenylation scores higher than a threshold (Problem 2).

Problem 1. Given A = A 1,…, A n and a positive integer s, find the number of all core NRPs of A with adenylation score equal to s.

Letwhere || shows the number of amino acids in A i. For any positive integers i and s satisfying,1 ≤ i ≤ n and s ≤ maxScore A, let numCoreNRPs A (i, s) denote the number of core NRPs, of assembly line A 1. A i withequal to s. Let numCoreNRPs A (0, s) = 0 for any positive integer s, and numCoreNRPs A (i, s) = 0 for any integer s < 0, across all possible values of i. Then, for any positive integers i and s satisfying 1 ≤ i ≤ n and 0 < s ≤ maxScore A, we have

---

### Average semivariance directly yields accurate estimates of the genomic variance in complex trait analyses [^1ba979c6]. G3 (2022). Medium credibility.

The relationship betweenand

We found that the normalized K, i.e.(9), proposed byand further described by, yields estimates of K that only deviated fromby a single degree of freedom in the denominator of the matrix scaling factor. Although these estimators were derived through different approaches and with different concepts in mind, they are numerically similar, apart from a single degree of freedom difference in the divisor of the GRM:used the number of entries (n), whereas we usedfor calculating the sample variance. instead of being biased by a factor ofis biased by a factor of. Our simulations confirm this deviation and the median genomic variance estimates usingwere slightly larger than, which was equal to the true value in the simulations (Fig. 1). This work, andall arrive at numerically similar solutions through conceptually different derivations, which we feel is indicative of the value of these approaches for the plant, animal, and human genetic studies that rely on genomic relatedness, e.g. GWAS, genomic prediction, or inferring population structure and ancestry.

---

### Accurately clustering biological sequences in linear time by relatedness sorting [^120d0aa3]. Nature Communications (2024). High credibility.

Two different values of k-mer length (k) are computed: one for use with k-mer matching between sequences and the other for use with rare k-mers. The goal of selecting k for k-mer matching is to ensure that k-mer matches between two sequences happen infrequently by chance, while using only a single value of k across all sequences. Hence, k is calculated such that one in every 100 k-mers is expected to be found by chance in sequences at the 99th percentile of input sequence lengths using an alphabet with a given number of letters (x):

The reasoning behind this formula for k was previously described. In contrast, k for rare k-mers is set such that at most 10% of the selected 20,000 (i.e. parameter A) sequences are expected to share a rare k-mer due to chance. Each of N sequences contributes 50 (by default) rare k-mers, and the objective is to find k such that each rare k-mer will be found by chance in less than 40 (i.e. 20,000/50 * 10%) sequences on average. Thus, k for rare k-mers is calculated as:

---

### Accurately clustering biological sequences in linear time by relatedness sorting [^4969738e]. Nature Communications (2024). High credibility.

The choice of which rare k-mers to select from each sequence is critically important. Ideally, rare k-mers should be repeatably selected such that (i) the same rare k-mers are chosen from similar sequences, (ii) rare k-mers are not positionally co-located on the sequence, and (iii) rare k-mers are relatively rare so that the same rare k-mers are more likely to be found in more similar sequences. Hashing functions meet these criteria as they allow for rare k-mers to be repeatably identified across sequences. I found that many different hashing functions were sufficient, and decided to use a standard 32-bit xorshift random number generator for this purpose. The input integer encoded k-mer is used as a seed to produce a 32-bit random number, and an integer remainder is output after integer division by the size (s) of the output hash space (i.e. the modulo operation). Optimal performance was found where the size of the hash space equaled the square root of the number of possible distinct k-mers (i.e.) or the mean sequence length (i.e. mean of w), whichever was greater. This choice of s is intended to prevent many k-mers in the same sequence from being assigned to the same hash bin for very long input sequences.

Thus, each k-mer is randomly projected into the hashing space, and the frequency of each hash bin is tabulated. Up to 50 k-mers corresponding to the least frequent hash bins are selected from each sequence. This vector is ordered with radix sorting, yielding groups of sequences sharing a rare k-mer in much the same manner as described for Linclust. Three vectors of length 50* N are used to store the index of each sequence in the k-mer group, the starting position of the k-mer group, and the ending position of the k-mer group. In this manner, it is possible to efficiently find all sequences sharing a rare k-mer with a particular sequence by looking up the sequences belonging to each of its rare k-mer groups.

---

### Analysis of the first genetic engineering attribution challenge [^814243d6]. Nature Communications (2022). High credibility.

Evaluating negative attribution with rank metrics

In many important practical applications of GEA, the ability to confidently exclude a potential designer (so-called "negative attribution") can be highly valuable, even if the true designer cannot be identified with confidence. In these contexts, a longer list of candidates presented with very high confidence may be more useful than a shorter list presented with lower confidence.

To investigate the degree to which Prediction Track models enable this sort of confident negative attribution, we developed a new metric. The X99 score of a predictor is the minimum positive integer N such that the top-N accuracy of that predictor is at least 99% (Fig. 3a). Analogous metrics can be defined for other accuracy thresholds; for example, the X95 score of a predictor is the smallest value of N such that its top-N accuracy is at least 95%. The lower the values of these two metrics, the better the predictor is able to confidently focus subsequent investigation on a manageable set of candidates.

Fig. 3
Rank metrics for efficient genetic forensics.

a For any given lab-of-origin predictor, the X99 score is the smallest positive integer N such that the top-N accuracy of the predictor is at least 99%. Analogous metrics can be defined for other thresholds; for example, the X95 score is the smallest N such that top-N accuracy exceeds 95%. b X99 & X95 scores achieved by each of the top 100 Prediction Track teams, on a logarithmic scale. c X99 & X95 scores achieved by past ML-based approaches to GEA on the Addgene plasmid database, compared to BLAST (left) and the results of the Genetic Engineering Attribution Challenge (GEAC, right). X99 results for the GEAC 1st place and ensemble models are annotated in orange. Dashed grey horizontal line in (b – c) indicates the total number of labs in the dataset, which represents the largest possible value of any X-metric on this dataset.

---

### Structured sonic tube with carbon nanotube-like topological edge States [^e0dfea24]. Nature Communications (2022). High credibility.

Rolled-up structured sheet

The topology of the rolled-up AGT is determined by a chiral vector C h = N C = n a 1 + m a 2 and a translation vector T as shown in Fig. 2 a. The greatest common divisor of the chiral index (n, m), denoted by N = gcd(n, m), represents the periodic numbers of the units along the circumference. Based on the six-fold symmetry of the underlying lattice, distinct tube geometries can be characterized by the integer pairswith. To illustrate the basic configurations under study, in Fig. 2 b we depict the unit cells of two tubes, where the left panel shows the (1, 0)14-AGT with the ZZ edge and the right panel shows the (1, 1)14-AGT with the AC edge. Further, our numerical computations also show that the way the tubes are rolled up is not the only deciding ingredient to engineer a complete band gap. Its spectral width, as shown in Fig. 2 c, appears to take discrete jumps with the number of circumferential unit cells. Specifically, for each unit number, the (1, 0) N -AGT remains gapless (red dots), whereas a band gap for the (1, 1) N -AGT never shows (blue dots) (see Supplementary Information for details). To give proof of this rule, we fabricate four AGTs with ZZ and AC edge terminations and measure their bulk dispersion. Two (1, 0) N -AGTs are constructed whose well-agreeing numerical and experimental band diagrams are shown in Fig. 2 d and e. According to the rule in Fig. 2 c, the measured (1, 0)14-AGT and (1, 0)15-AGT bands display clearly how a single circumferential increment of the units, leads to an acoustic semiconductor- and metal-like behavior, respectively. In other words, the (1, 0)14-AGT topology entails a complete band gap, whereas the (1, 0)15-AGT always remains gapless. In contrast, as shown in Fig. 2 f and g, gapless dispersion relations have been observed for both AC tubes in accordance with the (1, 1) N -AGT predictions discussed in Fig. 2 c.

---

### Incipient ferroelectricity of water molecules confined to nano-channels of beryl [^141987d5]. Nature Communications (2016). Medium credibility.

Within the crystal, water molecules can form doublets, triplets and so on, disposed in adjacent cages separated by the bottlenecks inside the channels. We can estimate the concentrations of such agglomerations as follows. Let the cluster of k molecules have a length k, the total number of water molecules be M and that of cages N; 1 ≤ k ≤ M. Combinatorial consideration allows us to derive the following expression for the mathematical expectation n k of number of clusters that contain k sequential water molecules and that are enclosed at both ends either by lone empty sites or by a sequence thereof

This precise expression allows simplification to a formula that is more convenient for practical use. Assuming that M, N, M − k and N − k are large and using the Stirling expressionwe get

Let us use the relative fraction m k = k · n k M −1 of water molecules residing in a clusters of length k. With a total concentration of water molecules C w = MN −1 we obtain

This formula is self-consistent since it fulfils the condition. The results of the corresponding calculations are presented in Fig. 8 by solid lines. As an independent check, we have performed numerical computer simulations to find out how M water molecules can be distributed among N empty cavities (M < N), at different values of M. We assume an array of N = 10 5 elements, each of which is assigned a value 0. Using the standard random number generator, an integer number i ∈[1, N] was generated and the i -th element from those not equal to zero was given a value 1. After each such step the values of N and M deceased by 1. The process was repeated as long as the remaining value of M was positive. At the end, the frequencies of occurrence of clusters of varying lengths were determined. The results of such modelling are indicated by dots in Fig. 8. Clearly, both used analyses provide the same results.

---

### Data-driven recombination detection in viral genomes [^4c88d7e9]. Nature Communications (2024). High credibility.

Specificity

We analyzed the false positive rate by evaluating a collection of 3500 randomly selected sequences assigned to the non-recombinant SARS-CoV-2 lineage BA.2. The set was partitioned into seven groups of 500 sequences, and added -respectively to each group- 1, 3, 5, 10, 15, 20, or 30 mutations out of 24,277 that are non-characteristic and with frequency ≥ 1/10 5 in BA.2, at random genomic positions. RecombinHunt classified as non-recombinant -and assigned the correct lineage to- the great majority of the sequences. False positives' ratios ranged from 0.6% to 1.2% to 8.8% for 1, 10 or 30 added mutations; see Table 1 b.

Minimum requirements

Ideally, RecombinHunt can be applied to any virus for which an adequately large collection of viral genome sequences and a structured classification system (clades or lineages) are available; in these settings, we denote as characteristic mutations of each class those showing a frequency above a given threshold. Here, we estimate the minimum number of sequences necessary to derive a stable set of characteristic mutations. To be characteristic of a lineage L, in our model, a mutation M must have a frequency f ≥ threshold above a user-defined value. The minimum number n of genomic sequences required to determine whether the observed frequency of M is above f with a certain level of confidence can be approximated by a Fisher test of a stochastic variable X ~ B i n (n, f) describing the number of independent observations of M in a set of n genome sequences of L; our null hypothesis is then H 0 = X ≥ n *threshold = X ≥ K. Thus, M is characteristic of L if observed at least K times. Based on these assumptions, depending on the threshold and the frequency of M in L, we can compute the minimum value of n required to identify a characteristic mutation with a level of confidence of choice (p-value or test acceptance threshold).

---

### TNM data v1.0… [^0f74c2dd]. staging.seer.cancer.gov (2000). Low credibility.

If you are working with a site/histology that only summary stages and you need to directly code it, please use the SEER Summary Stage 2000 manual. Note 3: In addition to directly assigning TNM, Summary Stage 2000 must be captured. If directly assigning SS2000, use the Nasal Cavity and Middle Ear chapter on page 126 of the SS2000 on-line manual. Next Step
- Parse N clin_n_prefixtmp_clin_n Next Step
- Parse M tmp_clin_mclin_m_prefix Next Step
- Parse T tmp_path_tpath_t_prefix Next Step
- Parse N tmp_path_npath_n_prefix Next Step
- Parse M. tmp_path_mpath_m_prefix Next Step
- Determine Default T Source Ttmp_path_ttmp_clin_ttmp_combined_t Next Step
- Determine Default N tmp_combined_ntmp_clin_ntmp_path_nSource N Next Step
- Determine Default M tmp_path_mtmp_combined_mtmp_clin_mSource M.

Next Step
- TNM 7 Stage Group Clinical Stage Group Next Step
- TNM 7 Path Stage Group Pathologic Stage Group Next Step
- Adjustments for No Clin or No Path tmp_combined_ntmp_combined_mSource TClinical Stage GroupPathologic Stage GroupSource Ntmp_combined_tSource M End Combined Stage.
- Stop if No Clin and No Path Combined MCombined NSource Ttmp_continueCombined TCombined Stage GroupSource Nss2017Source M Next Step
- Combined T Source Ttmp_combined_t Next Step
- Combined N tmp_combined_nSource N Next Step
- Combined M. tmp_combined_mtmp_source_mSource M Next Step
- Determine Combined Prefix cmb_t_prefix Next Step
- Determine Combined Prefix cmb_n_prefix Next Step
- Determine Combined Prefix cmb_m_prefix Next Step
- Concatenate T Combined T Next Step
- Concatenate N Combined N Next Step
- Concatenate M Combined M Next Step
- TNM 7 Path Stage Group Combined Stage Group End.

---

### Biased expectations about future choice options predict sequential economic decisions [^9b0d2bac]. Communications Psychology (2024). Medium credibility.

Next, the model works backwards through the sequence, iteratively using the aforementioned formula forwhen computing each respective action value Q for taking the option and declining the option for each t. Whenever the reward value of taking the current option is considered, the reward function R assigns reward values to options based on their ranks. h represents the relative rank of the current option.

In contrast, the reward value of sampling again is simply the cost to sample C.

This customisable R function allowed us to examine how the Ideal Observer changes its sampling strategy under the different reward payoff schemes used in our studies. Pilot full, Study 1 full, Study 2 and both conditions in Study 3 all involved instructing participants to try to choose the best price possible. In study conditions using these instructions, we implemented a continuous payoff function (resembling that of the classic Gilbert & Mosteller formulation), in which the relative rank of each choice would be rewarded commensurate with the value of its associated option. In Pilot baseline and the baseline, squares, timing, and prior conditions of Study 1, we adapted the payoff scheme to match participants' instructions that they would be paid £0.12 for the best rank, £0.08 for the second-best rank, £0.04 for the third best rank and £0 for any other ranks. Lastly, in the payoff condition of Study 1, we programmed the reward payoff function to match participants' reward of 5 stars for the best rank, 3 stars for the second-best rank, one star for the third-best rank and zero stars for any other ranks.

Another feature added to our implementation of the Ideal Observer, compared to the Gilbert & Mosteller base model, is the ability to update the model's generating distribution from its experience with new samples in a Bayesian fashion, instead of this generating distribution being specified in advance and then fixed throughout the paradigm. This Bayesian version of the optimality model treats option values as samples from a Gaussian distribution with a normal-inverse- χ2 prior. The prior distribution is initialised before experiencing any options with four parameters: the prior mean μ 0, the degrees of freedom of the prior mean κ, the prior variance σ 2 0 and the degrees of freedom of the prior variance ν. The μ 0 and σ 2 0 parameters of this prior distribution are then updated by the model following presentation of each newly sampled option value as each sequence progresses.

---

### Triazolam [^59f91090]. FDA (2025). Medium credibility.

WARNING: RISKS FROM CONCOMITANT USE WITH OPIOIDS; ABUSE, MISUSE, AND ADDICTION; and DEPENDENCE AND WITHDRAWAL REACTIONS

Concomitant use of benzodiazepines and opioids may result in profound sedation, respiratory depression, coma, and death. Reserve concomitant prescribing of these drugs in patients for whom alternative treatment options are inadequate. Limit dosages and durations to the minimum required. Follow patients for signs and symptoms of respiratory depression and sedation [see Warnings and Precautions (5.1), Drug Interactions (7.1)].
The use of benzodiazepines, including triazolam, exposes users to risks of abuse, misuse, and addiction, which can lead to overdose or death. Abuse and misuse of benzodiazepines commonly involve concomitant use of other medications, alcohol, and/or illicit substances, which is associated with an increased frequency of serious adverse outcomes. Before prescribing triazolam and throughout treatment, assess each patient's risk for abuse, misuse, and addiction [see Warnings and Precautions (5.2)].
The continued use of benzodiazepines, including triazolam, may lead to clinically significant physical dependence. The risks of dependence and withdrawal increase with longer treatment duration and higher daily dose. Abrupt discontinuation or rapid dosage reduction of triazolam after continued use may precipitate acute withdrawal reactions, which can be life-threatening. To reduce the risk of withdrawal reactions, use a gradual taper to discontinue triazolam or reduce the dosage [see Dosage and Administration (2.3), Warnings and Precautions (5.3)].

WARNING: RISKS FROM CONCOMITANT USE WITH OPIOIDS; ABUSE, MISUSE, AND ADDICTION; and DEPENDENCE AND WITHDRAWAL REACTIONS

See full prescribing information for complete boxed warning.

Concomitant use of benzodiazepines and opioids may result in profound sedation, respiratory depression, coma, and death. Reserve concomitant prescribing of these drugs in patients for whom alternative treatment options are inadequate. Limit dosages and durations to the minimum required. Follow patients for signs and symptoms of respiratory depression and sedation (5.1, 7.1).
The use of benzodiazepines, including triazolam, exposes users to risks of abuse, misuse, and addiction, which can lead to overdose or death. Before prescribing triazolam and throughout treatment, assess each patient's risk for abuse, misuse, and addiction (5.2).
Abrupt discontinuation or rapid dosage reduction of triazolam after continued use may precipitate acute withdrawal reactions, which can be life-threatening. To reduce the risk of withdrawal reactions, use a gradual taper to discontinue triazolam or reduce the dosage (2.3, 5.3).

---

### Parametric excitation and squeezing in a many-body spinor condensate [^cd85b59e]. Nature Communications (2016). Medium credibility.

Quantum parametric excitation

An alternative way to view the parametric excitation is by considering transitions between the eigenstates of the many-body Hamiltonian. The energy corresponding to the oscillation frequency matches no single atom transition. Rather, it approximately matches the energy difference between two atoms in the m F = 0 state and a pair of atoms in the m F = ± 1 states. Yet, even this is not precise enough as this energy separation also depends on the collective state of the system varying fromfor all atoms in the m F = 0 state tofor all atoms in the m F = ± 1 states. These energy separations can be calculated by diagonalizing the tridiagonal matrix given by

whereand k is the number of pairs of m F = ± 1 atoms in the enumeration of the Fock basis. The Fock basis, | N, M, k 〉, is also enumerated with N the total number of atoms, and M the magnetization, both of which are conserved by the Hamiltonian leaving all dynamics in k. The off-diagonal contributions in equation (28) are due to the many-body interaction given by theterm of the Hamiltonian. This interaction results in mixing of the Fock states, even in the high field limit. Without this interaction, there would be no transitions as the magnetic interactions, both linear and quadratic Zeeman, are diagonal in the Fock basis. In this picture, the integer divisor frequencies of the spectrum correspond to many photon excitations of the system.

In the regime where q = q Z B 2 > 2 c, we treatas a perturbation andwith expansion coefficients

and eigenenergy of the system

The resonance frequency between Fock states is the energy difference between each Fock state

where the last line corresponds to k < < N. To first order, the resonance frequency between Fock states is the same as the frequency obtained from mean field theory equation (13). The factor of two arises from the definition of the resonant frequency f = 2 f 0.

---

### Learning meaningful representations of protein sequences [^6646110e]. Nature Communications (2022). High credibility.

How we choose to represent our data has a fundamental impact on our ability to subsequently extract information from them. Machine learning promises to automatically determine efficient representations from large unstructured datasets, such as those arising in biology. However, empirical evidence suggests that seemingly minor changes to these machine learning models yield drastically different data representations that result in different biological interpretations of data. This begs the question of what even constitutes the most meaningful representation. Here, we approach this question for representations of protein sequences, which have received considerable attention in the recent literature. We explore two key contexts in which representations naturally arise: transfer learning and interpretable learning. In the first context, we demonstrate that several contemporary practices yield suboptimal performance, and in the latter we demonstrate that taking representation geometry into account significantly improves interpretability and lets the models reveal biological information that is otherwise obscured.

---

### Accurately clustering biological sequences in linear time by relatedness sorting [^eb9669e1]. Nature Communications (2024). High credibility.

As relatedness sorting progresses, the rank ordering (v) typically stabilizes, with some sequences stabilizing before others. When a sequence's rank order stabilizes below 1000 (i.e. parameter C /2), its partition is split into separate groups at this sequence (Fig. 1J), as long as each new group would contain at least 2000 sequences. Relatedness sorting is continued within each group until all of its sequences stabilize in rank order below 1000 or the maximum number of iterations (i.e. parameter B) is reached. In practice, most groups stabilize in rank order before 2000 iterations and, thus, parameter B is not a limiting factor. Rank order stability is defined as v ≤ 1000 because the sequences are moving within 2000 positions on average (i.e. ± 1000), which is the number of sequences considered in phase 3 of the algorithm (i.e. parameter C).

Phase 3: clustering sequences in linear time

No sequences were clustered in phases 1 or 2, but the sequences were ordered by relatedness in preparation for clustering. Two linear time strategies are used for clustering sequences: (1) The rare k-mer strategy draws sequences for comparison in the exact same manner as used in phase 1, thereby prioritizing sequences sharing the greatest number of rare k-mers; (2) The relatedness sorting strategy selects proximal sequences from the final rank ordering of sequences determined in phase 2. Up to 2000 (i.e. parameter C) total sequences are drawn from the two strategies in proportion to the number of clustered sequences arising from each strategy. That is, when a sequence is added to an existing cluster, Clusterize determines whether the sequence was in the set of sequences originating from rare k-mers and/or relatedness sorting. The count of sequences drawn from each strategy is incremented in accordance with the clustered sequence's origin(s). In this manner, the more lucrative strategy is emphasized when determining where to draw the next 2000 candidate sequences.

K-mer similarity is calculated to the cluster representatives corresponding to all 2000 sequences, and any cluster representative passing the similarity threshold is used for assignment. If none exists, the sequence is aligned with up to 200 (i.e. parameter E) cluster representatives having the highest k-mer similarities. If none of these are within the similarity threshold, the sequence becomes a new cluster representative. To avoid issues with partial-length sequences, the sequences are visited in order of decreasing length so that the cluster representative is always the longest sequence in its cluster.

---

### Anagrelide [^6d45a681]. FDA (2024). Medium credibility.

5.1 Cardiovascular Toxicity

Torsades de pointes and ventricular tachycardia have been reported with anagrelide. Obtain a pre-treatment cardiovascular examination including an ECG in all patients. During treatment with anagrelide monitor patients for cardiovascular effects and evaluate as necessary.

Anagrelide increases the QTc interval of the electrocardiogram and increases the heart rate in healthy volunteers [see Clinical Pharmacology (12.2)].

Do not use anagrelide in patients with known risk factors for QT interval prolongation, such as congenital long QT syndrome, a known history of acquired QTc prolongation, medicinal products that can prolong QTc interval and hypokalemia [see Drug Interactions (7.1)].

Hepatic impairment increases anagrelide exposure and could increase the risk of QTc prolongation. Monitor patients with hepatic impairment for QTc prolongation and other cardiovascular adverse reactions. The potential risks and benefits of anagrelide therapy in a patient with mild and moderate hepatic impairment should be assessed before treatment is commenced. Reduce anagrelide dose in patients with moderate hepatic impairment. Avoid use of anagrelide in patients with severe hepatic impairment.

In patients with heart failure, bradyarrhythmias, or electrolyte abnormalities, consider periodic monitoring with electrocardiograms [see Clinical Pharmacology (12.2)].

Anagrelide is a phosphodiesterase 3 (PDE3) inhibitor and may cause vasodilation, tachycardia, palpitations, and congestive heart failure. Other drugs that inhibit PDE3 have caused decreased survival when compared with placebo in patients with Class III-IV congestive heart failure [see Drug Interactions (7.2)].

In patients with cardiac disease, use anagrelide only when the benefits outweigh the risks.

5.2 Pulmonary Hypertension

Cases of pulmonary hypertension have been reported in patients treated with anagrelide. Evaluate patients for signs and symptoms of underlying cardiopulmonary disease prior to initiating and during anagrelide therapy [see Adverse Reactions (6.1)].

5.3 Bleeding Risk

Use of concomitant anagrelide and aspirin increased major hemorrhagic events in a postmarketing study. Assess the potential risks and benefits for concomitant use of anagrelide with aspirin, since bleeding risks may be increased. Monitor patients for bleeding, including those receiving concomitant therapy with other drugs known to cause bleeding (e.g., anticoagulants, PDE3 inhibitors, NSAIDs, antiplatelet agents, selective serotonin reuptake inhibitors) [see Drug Interactions (7.3), Clinical Pharmacology (12.3)].

---

### High-rate quantum LDPC codes for long-range-connected neutral atom registers [^5816bb5e]. Nature Communications (2025). High credibility.

For the present purposes, we have chosen the seed codesto be cyclic codes generalizing the repetition code the surface code is built upon. Such a choice both allows for improving the encoding rate and retaining most of the intuitiveness of the surface code, at the price of a non-constant overhead in the asymptotic limit. However, this does not represent a severe issue in the prospect of implementation on near-term quantum computers, as we will discuss more quantitatively in the next sections.

Classical seeds and quantum layout

In this section we review how stabilizers, logical operators, and array shape of a quantum HGP code are determined from its classical seeds. This represents a huge help in designing new LDPC codes tailored to the connectivity of the quantum hardware.

We have chosen cyclic seed codes, i.e. codes with cyclic shift invariant codewords. A square matrix is said to be circulant if its rows are cyclic shifts of the first row. When the parity-check matrix, H, is circulant, the associated code is fully specified by the first row of. Entries c i (i = 0, 1,…, k) can be mapped into coefficients of a degree- k polynomial of the form. More formally, there exists a map, beingthe ring of polynomials dividing x n −1. This map transforms cyclic shifts ininto multiplications by x in, hence cyclic-shift-invariant codes into polynomials invariant under x -multiplication. Due to the ring structure, invariance under multiplication by x is equivalent to invariance under multiplication of any element of the ring, a property defining the so-called ideals of the ring. Thus, there exists a one-to-one correspondence between cyclic codesand ideals of, which in turn are in one-to-one correspondence with unitary mod-2-divisors of x n −1 having a leading coefficient equal to 1. Building blocks of length- n cyclic codes then correspond to factors of x n −1. For k = 1 the repetition code is recovered.

---

### Quetiapine (Seroquel) [^8a8dd992]. FDA (2025). Medium credibility.

Warnings and precautions regarding the use of quetiapine fumarate PO (also known as Seroquel):
- **Anticholinergic syndrome**: use caution in patients with urinary retention, prostatic hypertrophy, constipation, or taking anticholinergic medications.
- **Antipsychotic withdrawal**: do not discontinue abruptly in any patient.
- **Aspiration pneumonia**: use caution in elderly patients, particularly with advanced Alzheimer's disease.
- **Cataract**: maintain a high level of suspicion, as long-term use of quetiapine has been associated with an increased risk of cataracts.
- **Falls**: maintain a high level of suspicion, as quetiapine may cause somnolence, orthostatic hypotension, motor and sensory instability, leading to falls and injuries.
- **Hematologic disorders**: use caution in patients with leukopenia/neutropenia include pre-existing low white cell count (WBC) and history of drug induced leukopenia/neutropenia.
- **Hyperprolactinemia**: maintain a high level of suspicion, as quetiapine has been associated with an increased risk of hyperprolactinemia.
- **Hypotension**: use caution in patients with cardiovascular or cerebrovascular disease, dehydration/hypovolemia, or taking antihypertensive agents.
- **Hypothyroidism**: maintain a high level of suspicion, as quetiapine has been associated with an increased risk of hypothyroidism. Monitor TSH and fT4 levels.
- **Mania**: use caution in patients with bipolar disorder. Screen patients for any personal or family history of bipolar disorder before initiating quetiapine.
- **Metabolic changes**: maintain a high level of suspicion, as quetiapine has been associated with an increased risk of metabolic changes, including hyperglycemia, diabetes mellitus, dyslipidemia, and weight gain.
- **Neuroleptic malignant syndrome**: maintain a high level of suspicion, as quetiapine has been associated with an increased risk of neuroleptic malignant syndrome.
- **Prolonged QT interval**: use caution in patients at increased risk of QT prolongation, such as the elderly, patients with CVD, family history of QT prolongation, congestive HF, or cardiac hypertrophy. Avoid using quetiapine with other QT-prolonging agents, including class IA or III antiarrhythmics, antipsychotics (such as ziprasidone, chlorpromazine, thioridazine), antibiotics (such as gatifloxacin, moxifloxacin), or other medications known to prolong the QT interval (such as pentamidine, levomethadyl acetate, methadone).
- **Seizure**: use caution in patients with seizure disorder or low seizure threshold.
- **Somnolence**: use extreme caution in patients performing activities requiring mental alertness, such as driving or operating machinery.
- **Stroke**: use caution in elderly subjects with dementia.
- **Tardive dyskinesia**: maintain a high level of suspicion, as quetiapine has been associated with an increased risk of tardive dyskinesia.
- **Torsades de pointes, SCD**: use caution, as quetiapine is associated with an increased risk of these adverse events. Avoid using quetiapine in patients with a history of cardiac arrhythmias, such as bradycardia, hypokalemia, hypomagnesemia, or congenital QT prolongation.

---

### Phylogenetically informed predictions outperform predictive equations in real and simulated data [^8b798c0c]. Nature Communications (2025). High credibility.

Inferring unknown trait values is ubiquitous across biological sciences-whether for reconstructing the past, imputing missing values for further analysis, or understanding evolution. Models explicitly incorporating shared ancestry amongst species with both known and unknown values (phylogenetically informed prediction) provide accurate reconstructions. However, 25 years after the introduction of such models, it remains common practice to simply use predictive equations derived from phylogenetic generalised least squares or ordinary least squares regression models to calculate unknown values. Here, we use a comprehensive set of simulations to demonstrate two- to three-fold improvement in the performance of phylogenetically informed predictions compared to both ordinary least squares and phylogenetic generalised least squares predictive equations. We found that phylogenetically informed prediction using the relationship between two weakly correlated (r = 0.25) traits was roughly equivalent to (or even better than) predictive equations for strongly correlated traits (r = 0.75). A critique and comparison of four published predictive analyses showcase real-world examples of phylogenetically informed prediction. We also highlight the importance of prediction intervals, which increase with increasing phylogenetic branch length. Finally, we offer guidelines to making phylogenetically informed predictions across diverse fields such as ecology, epidemiology, evolution, oncology, and palaeontology.

---

### Octreotide (Mycapssa) [^96da3bc4]. FDA (2025). Medium credibility.

Warnings and precautions regarding the use of octreotide acetate DR PO (also known as Mycapssa):
- **Abnormal ECG**: maintain a high level of suspicion, as octreotide has been associated with an increased risk of cardiac conduction abnormalities and other ECG changes, including QT prolongation, axis shift, early repolarization, low voltage, R/S transition, and early R wave progression. Consider adjusting the dosage of concomitantly used drugs that may cause bradycardia, such as β-blockers.
- **Bradycardia**: use caution in patients taking β-blockers or CCBs.
- **Cholelithiasis**: maintain a high level of suspicion, as octreotide may inhibit gallbladder contractility and decrease bile secretion, potentially leading to gallbladder abnormalities, sludge, and associated complications such as cholecystitis, cholangitis, and pancreatitis. Monitor patients periodically.
- **Decreased serum cyclosporine levels**: use caution in patients taking cyclosporine.
- **Decreased serum digoxin levels**: use caution in patients taking digoxin.
- **Decreased serum levonorgestrel levels**: use caution in patients taking levonorgestrel.
- **Decreased serum octreotide levels**: use caution in patients taking PPIs, H2RA, or antacids. Consider increasing octeotide dose.
- **Decreased serum vitamin B12**: maintain a high level of suspicion, as octreotide has been associated with an increased risk of decreased vitamin B12 levels and abnormal Schilling test results. Monitor serum vitamin B12 levels.
- **Hypothyroidism**: maintain a high level of suspicion, as octreotide may suppress the secretion of TSH. Monitor thyroid function periodically.
- **Increased blood glucose, decreased blood glucose**: maintain a high level of suspicion, as octreotide may alter the balance between the counter-regulatory hormones, insulin, glucagon, and GH, resulting in hypoglycemia, hyperglycemia, or diabetes mellitus. Monitor blood glucose levels. Adjust antidiabetic treatment accordingly.
- **Increased serum bromocriptine levels**: use caution in patients taking bromocriptine.
- **Increased serum drug levels**: use caution in patients taking drugs mainly metabolized by CYP3A4 and having a narrow therapeutic index, such as quinidine and terfenadine.
- **Increased serum lisinopril levels**: use caution in patients taking lisinopril.
- **Steatorrhea, fat malabsorption**: maintain a high level of suspicion, as somatostatin analogs, including octreotide, may cause steatorrhea, stool discoloration, diarrhea, and fat malabsorption.

---

### Using synthetic bacterial enhancers to reveal a looping-based mechanism for quenching-like repression [^a46118bf]. Nature Communications (2016). Medium credibility.

To further test the sensitivity of the qrr looping regions to integer multiples of the helical repeat, we checked the average identities of each loop sequence to itself and to all other loop sequences. To do this, we calculated the relative identity of each 9 bp window within a given loop sequence to all other 9 bp windows either on the same sequence, or on all other sequences, noting the distance between the positions of the first bases of compared windows (the relative identity is defined as the number of positions (that is, from 0 to 9) for which both windows contain the same base, divided by the window length). We then computed the mean relative identity for each window separation by averaging over all relative identities in a particular window-separation value. In Fig. 6b we plot the results. The figure shows that the mean relative identity for the annotated qrr enhancers exhibits an oscillatory behaviour, which persists for all possible values of the distance between the windows. Interestingly, the oscillatory pattern is detected not only for cross-correlated enhancers, but also within each enhancer to itself (self, red; other, black), with the first maxima appearing at ∼0 bp displacement and with periodicity of 10.45 bp.

Next, we checked whether there was some underlying signature for a conserved sequence within the looping region. To that end, we computed the average AT/GC content of each position within the loop and plotted the results in Fig. 6c (top). The figure shows that the AT content is enriched at positions that are integer multiples of 10.6 bp with at least six distinct peaks visible in the data. In addition, the minima between the positions of AT enrichment converge on a content value of ∼0.5, which is the value expected for a random allocation of AT or GC at those particular positions. Thus, loop sequences are similar either at the same relative position or alternatively at positions displaced by an integer multiple of the helical repeat from the position of the reference sequence, with a preference to AT segments.

---

### A versatile computational algorithm for time-series data analysis and machine-learning models [^263e8086]. NPJ Parkinson's Disease (2021). Medium credibility.

Now consider the 3x3 neighborhood around each vector element being constructed from.represents our one-dimensional time-series vector that can be used for time-delay embedding with delay t and embedding dimension m to reconstruct the phase-space trajectory. However, instead of considering the Euclidean norm between data points:we will consider vectors V i and V j in relation to their local 3x3 neighborhood such that:where it can be seen that≠even when = (Fig. 1a). It follows then, that directional and curvature information can be captured by different inequality patterning around the 3x3 neighborhood when computed for allby constructing a new matrix that represents an 8-bit binary code for each point-pair's local neighborhood:where g 0 representsand g n = { g 1,…, g 8 } are its eight connected neighbors. Each neighbor which is larger or equal to g 0 is set to 1, while each neighbor that is smaller than g 0 is set to 0. A binary code is thus created by moving around the central point g 0 (here counterclockwise) where a single integer value is calculated based on the sum of the binary code elements (0 or 1) multiplied by the eight 2 p positional weights (increments of powers of 2: 2 0, 2 1, 2 2, 2 3, 2 4, 2 5, 2 6, 2 7) starting with the preceding points (i.e.). This represents 8-bit binary coding where there are 2 8 (256) different possible integer values, ranging from 0 to 255. This newly created matrix now allows for the identification of graded changes in phase-space trajectories that can be used to identify recurring sets of local topological features embedded within the dynamic signal (LoTRA plot; see Fig. 1b, c for example).

---

### Determining sequencing depth in a single-cell RNA-seq experiment [^ce3ab87a]. Nature Communications (2020). High credibility.

An underlying question for virtually all single-cell RNA sequencing experiments is how to allocate the limited sequencing budget: deep sequencing of a few cells or shallow sequencing of many cells? Here we present a mathematical framework which reveals that, for estimating many important gene properties, the optimal allocation is to sequence at a depth of around one read per cell per gene. Interestingly, the corresponding optimal estimator is not the widely-used plug-in estimator, but one developed via empirical Bayes.

---

### The latent cis-regulatory potential of mobile DNA in Escherichia coli [^5c04959a]. Nature Communications (2025). High credibility.

Mutual information

Mutual information is a measure of dependence between two variables. We calculated the mutual informationbetween the nucleotide identity b at position i of daughter sequences of a given parent (1 ≤ i ≤ 150), and the fluorescence score f for daughter sequences of a given parent. To calculate the mutual information for each parent sequence, we used Eq. (2) as previously described in ref.:

In this equation, the variable b represents all possible nucleotides. The variable f represents fluorescence scores rounded to the nearest integer (see "Processing sequencing results" for calculation of these scores); corresponds to the probability (relative frequency) of each sequence variant encoding an A, T, C, or G at position (i); corresponds to the probability (relative frequency) of fluorescence scores being equal to 1, 2, 3, or 4; andis the corresponding joint probability, i.e. the probability of position i encoding an A, T, C, or G, and having a fluorescence score of 1, 2, 3, or 4 arb. units

The concept of mutual information is best illustrated with two simple examples. For the first, we calculate the mutual information for two consecutive and fair coin flips. Here, b equals the possible states of the first coin flip (heads or tails), and f equals the possible states of the second coin flip (heads or tails). For the event of first flipping heads and then tails, the joint probabilityequals the probability of first flipping heads (0.5) and then tails (0.5), which is 0.5 × 0.5 = 0.25. The individual probabilitiesandcorrespond to the probabilities of getting heads on the first toss (0.5) and tails on the second toss (0.5), respectively. For this state (heads flip and then tails flip), and all the other possible states, the right-hand side of Eq. (2) will equal 0, because log 2 (1) = 0, and thus the sum of these values also 0.

---

### Extreme purifying selection against point mutations in the human genome [^e49426ec]. Nature Communications (2022). High credibility.

Approximate model for ultraselection

Following Eq. (1), the log likelihood function is given by, where R = ∑ i Y i is the number of rare variants. When the P i values are small (as is typical), it is possible to obtain a reasonably good closed-form estimator for λ s by making use of the approximation. In this case, where N = ∑ i (1 − Y i) is the number of invariant sites andis the average value of P i at the invariant sites. It is easy to show that this approximate log likelihood is maximized at,

However, this procedure leads to a biased estimator for λ s. A correction for the bias leads to the following, intuitively simple, unbiased estimator:where M = N + R is the total number of sites andis the average value of P i at all sites. In other words, is given by 1 minus the observed number of rare variants divided by the expected number of rare variants under neutrality, which is simply the total number of sites multiplied by the average rate at which rare variants appear.

Full allele-specific model

In practice, we use a model that distinguishes among the alternative alleles at each site and exploits our allele-specific mutation rates. This model behaves similarly to the simpler one described above, but yields slightly more precise estimates in the presence of multi-allelic rare variants.

In the full model, we assume separate indicator variables, and, for the three possible allele-specific rare variants at each site, and corresponding allele-specific rates of occurrence, and(which, notably, sum to the quantity previously denoted P i). We further make the assumption that the different rare variants appear independently. Thus, the likelihood function generalizes to (cf. equation (1)), where we redefineandfor j ∈ {1, 2, 3}. Notice that, when more than one alternative allele is present, will be 1 for more than one value of j.

---

### Statistical ensembles without typicality [^c8e48c72]. Nature Communications (2018). Medium credibility.

Maximum-entropy ensembles are key primitives in statistical mechanics. Several approaches have been developed in order to justify the use of these ensembles in statistical descriptions. However, there is still no full consensus on the precise reasoning justifying the use of such ensembles. In this work, we provide an approach to derive maximum-entropy ensembles, taking a strictly operational perspective. We investigate the set of possible transitions that a system can undergo together with an environment, when one only has partial information about the system and its environment. The set of these transitions encodes thermodynamic laws and limitations on thermodynamic tasks as particular cases. Our main result is that the possible transitions are exactly those that are possible if both system and environment are assigned the maximum-entropy state compatible with the partial information. This justifies the overwhelming success of such ensembles and provides a derivation independent of typicality or information-theoretic measures.

---

### Abstract representations of events arise from mental errors in learning and memory [^748e9b84]. Nature Communications (2020). High credibility.

Estimating parameters and making quantitative predictions

Given an observed sequence of nodes x 1,…, x t −1, and given an inverse temperature β, our model predicts the anticipation, or expectation, of the subsequent node x t to be. In order to quantitatively describe the reactions of an individual subject, we must relate the expectations a (t) to predictions about a person's reaction timesand then calculate the model parameters that best fit the reactions of an individual subject. The simplest possible prediction is given by the linear relation, where the intercept r 0 represents a person's reaction time with zero anticipation and the slope r 1 quantifies the strength with which a person's reaction times depend on their internal expectations.

In total, our predictionscontain three parameters (β, r 0, and r 1), which must be estimated from the reaction time data for each subject. Before estimating these parameters, however, we first regress out the dependencies of each subject's reaction times on the button combinations, trial number, and recency using a mixed effects model of the form 'RT~log(Trial)*Stage+Target+Recency+(1+log(Trial)*Stage+Recency|ID)', where all variables were defined in the previous section. Then, to estimate the model parameters that best describe an individual's reactions, we minimize the RMS prediction error with respect to each subject's observed reaction times, where T is the number of trials. We note that, given a choice for the inverse temperature β, the linear parameters r 0 and r 1 can be calculated analytically using standard linear regression techniques. Thus, the problem of estimating the model parameters can be restated as a one-dimensional minimization problem; that is, minimizing RMSE with respect to the inverse temperature β. To find the global minimum, we began by calculating RMSE along 100 logarithmically spaced values for β between 10 −4 and 10. Then, starting at the minimum value of this search, we performed gradient descent until the gradient fell below an absolute value of 10 −6. For a derivation of the gradient of the RMSE with respect to the inverse temperature β, we point the reader to the Supplementary Discussion. Finally, in addition to the gradient descent procedure described above, for each subject we also manually checked the RMSE associated with the two limits β → 0 and β → ∞. The resulting model parameters are shown in Fig. 4 a, b for random walk sequences and Fig. 4 g, h for Hamiltonian walk sequences.

---

### Multivoxel neurofeedback selectively modulates confidence without changing perceptual performance [^ef5037db]. Nature Communications (2016). Medium credibility.

MVP analyses data sets and cross-validation

For unbalanced data sets (that is, task accuracy — correct versus incorrect trial classification), we used a weighted samples method for the majority group in the training data sets. That is, by design the proportion of correct to incorrect trials is ∼3:1, and therefore each sample in the correct trials group is automatically assigned a weight < 1 (∼0.33), to penalize the sample size bias for the differential ratios.

For each MVPA, we performed a k -fold cross-validation, where the entire data set is repeatedly subdivided into a 'training set' and a 'test set'. The two can be seen as independent data sets that are used to fit the parameters of a model (decoder) and evaluate the predictive power of the trained (fitted) model, respectively. For each behavioural variable of interest, and for each participant the number of folds was automatically adjusted between k = 9 and k = 11 to be a (close) divisor of the number of samples in the data set. Thus, the number of folds in each cross-validation procedure was ∼10, a typical value for k -fold cross-validation procedures. The k -fold cross-validation procedure is critical in any predictive classification problem as it serves the purpose of limiting overfitting, and gives an index of the generalizability of the model. Furthermore, the SLR-based classification was optimized by using an iterative approach. That is, during each fold of the cross-validation, the process was repeated n times (n = 10). On each iteration, the selected features were removed from the pattern vectors, and only features with unassigned weights were used for the next iteration. At the end of the k -fold cross-validation, the test accuracy was averaged for each iteration across folds, to evaluate the accuracy at each iteration. The optimal number of SLRs (number of iterations) was thus chosen and used for the final computation of the decoder used in the neurofeedback training procedure. Importantly, the confidence decoder used in the DecNef was constructed by using only correct trials, as compared with the main analysis for which both correct and incorrect trials were used. Furthermore, the calculation of weights used for DecNef was done using the entire data set and the optimal number of iterations (as described above), while we did cross-validation (using a subset of the data for weights' calculation) to measure decoding accuracy.

---

### Coming of age: ten years of next-generation sequencing technologies [^47cdf4e4]. Nature Reviews: Genetics (2016). Medium credibility.

Since the completion of the human genome project in 2003, extraordinary progress has been made in genome sequencing technologies, which has led to a decreased cost per megabase and an increase in the number and diversity of sequenced genomes. An astonishing complexity of genome architecture has been revealed, bringing these sequencing technologies to even greater advancements. Some approaches maximize the number of bases sequenced in the least amount of time, generating a wealth of data that can be used to understand increasingly complex phenotypes. Alternatively, other approaches now aim to sequence longer contiguous pieces of DNA, which are essential for resolving structurally complex regions. These and other strategies are providing researchers and clinicians a variety of tools to probe genomes in greater depth, leading to an enhanced understanding of how genome sequence variants underlie phenotype and disease.

---

### Efficient generative modeling of protein sequences using simple autoregressive models [^6f72df2d]. Nature Communications (2021). High credibility.

Generative models emerge as promising candidates for novel sequence-data driven approaches to protein design, and for the extraction of structural and functional information about proteins deeply hidden in rapidly growing sequence databases. Here we propose simple autoregressive models as highly accurate but computationally efficient generative sequence models. We show that they perform similarly to existing approaches based on Boltzmann machines or deep generative models, but at a substantially lower computational cost (by a factor between 10 2 and 10 3). Furthermore, the simple structure of our models has distinctive mathematical advantages, which translate into an improved applicability in sequence generation and evaluation. Within these models, we can easily estimate both the probability of a given sequence, and, using the model's entropy, the size of the functional sequence space related to a specific protein family. In the example of response regulators, we find a huge number of ca. 10 68 possible sequences, which nevertheless constitute only the astronomically small fraction 10–80 of all amino-acid sequences of the same length. These findings illustrate the potential and the difficulty in exploring sequence space via generative sequence models.

---

### Clustering huge protein sequence sets in linear time [^67e82254]. Nature Communications (2018). Medium credibility.

Fig. 3
Cumulative distance distribution between representative sequences. We clustered the test set of 123 million sequences at three different sequence identity thresholds (a – c at 50%, 70%, and 90%, respectively). For each clustering, we randomly sampled 1000 representative cluster sequences, compared them to all representative sequences of the clustering, and plotted the fraction whose best match (excluding self-matches) with minimum sequence coverage of 90% had a sequence identity above the x -value. The y -value at the clustering threshold (dashed line) is the fraction of false negatives, pairs of sequences whose similarity was overlooked by the clustering method

---

### An introduction to machine learning [^70a823d9]. Clinical Pharmacology and Therapeutics (2020). Medium credibility.

Different categories of loss functions

Different objective functions can be chosen to measure the distance between observed data and values predicted by the model. Some of the distance metrics used in practice can be associated to a likelihood. The likelihood indicates how probable it is to observe our data according to the selected model. The most common use of a likelihood is to find the parameters that make the model fit optimally to the data (i.e. the maximum likelihood parameter estimates). Usually, the negative logarithm of the likelihood is minimized and considered as objective function because it has favorable numerical properties. Similarly, in ML metrics, such as mean squared error, logistic objective, or cross‐entropy, are used to find optimal parameters or assess the fitness of the model.

In practice, analytical calculation of maximum likelihood or minimal loss may not be feasible, and it is often necessary to use a numerical optimization algorithm to solve for the best parameter values. Gradient descent is such an algorithm, where we first define an objective function for which we want to minimize and then iteratively update the values of the parameters in the direction with the steepest decrease (first‐order derivative) of the objective function until a convergence to a minimum distance is deemed reached. In the scenario of a nonconvex objective function, the success of finding a global minimum, as opposed to landing in some local minima, will depend on the choice of the initial set of parameter values, the learning rate (i.e. step size of each iteration) and the criterion for convergence. The reader can refer to ref. 35 for details on convex and nonconvex optimization processes. Stochastic gradient descent is an additional trick that can further speed up the optimization by randomly sampling a training dataset and summing the distances across this subset of training data points for approximating the objective function.

---

### A continuous-time maxSAT solver with high analog performance [^3d122302]. Nature Communications (2018). Medium credibility.

Many real-life optimization problems can be formulated in Boolean logic as MaxSAT, a class of problems where the task is finding Boolean assignments to variables satisfying the maximum number of logical constraints. Since MaxSAT is NP-hard, no algorithm is known to efficiently solve these problems. Here we present a continuous-time analog solver for MaxSAT and show that the scaling of the escape rate, an invariant of the solver's dynamics, can predict the maximum number of satisfiable constraints, often well before finding the optimal assignment. Simulating the solver, we illustrate its performance on MaxSAT competition problems, then apply it to two-color Ramsey number R(m, m) problems. Although it finds colorings without monochromatic 5-cliques of complete graphs on N ≤ 42 vertices, the best coloring for N = 43 has two monochromatic 5-cliques, supporting the conjecture that R(5, 5) = 43. This approach shows the potential of continuous-time analog dynamical systems as algorithms for discrete optimization.

---

### Abstract representations of events arise from mental errors in learning and memory [^5646fd8b]. Nature Communications (2020). High credibility.

Methods

Maximum entropy model and the infinite-sequence limit

Here we provide a more thorough derivation of our maximum entropy model of human expectations, with the goal of fostering intuition. Given a matrix of erroneous transition counts, our estimate of the transition structure is given by. When observing a sequence of nodes x 1, x 2,…, in order to construct the counts, we assume that humans use the following recursive rule: where B t (i) denotes the belief, or perceived probability, that node i occurred at the previous time t. This belief, in turn, can be written in terms of the probability P (Δ t) of accidentally recalling the node that occurred Δ t time steps from the desired node at time t:

In order to make quantitative predictions about people's estimates of a transition structure, we must choose a mathematical form for P (Δ t). To do so, we leverage the free energy principle: When building mental models, the brain is finely tuned to simultaneously minimize errors and computational complexity. The average error associated with a candidate distribution Q (Δ t) is assumed to be the average distance in time of the recalled node from the target node, denoted. Furthermore, Shannon famously proved that the only suitable choice for the computational cost of a candidate distribution is its negative entropy, denoted. Taken together, the total cost associated with a distribution Q (Δ t) is given by the free energy F (Q) = βE (Q) − S (Q), where β, referred to as the inverse temperature, parameterizes the relative importance of minimizing errors versus computational costs. By minimizing F with respect to Q, we arrive at the Boltzmann distribution p (Δ t) = e − β Δ t / Z, where Z is the normalizing partition function. We emphasize that this mathematical form for P (Δ t) followed directly from our free energy assumption about resource constraints in the brain.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^aeeaae45]. Nature Communications (2025). High credibility.

Fitting models to data is an important part of the practice of science. Advances in machine learning have made it possible to fit more-and more complex-models, but have also exacerbated a problem: when multiple models fit the data equally well, which one(s) should we pick? The answer depends entirely on the modelling goal. In the scientific context, the essential goal is replicability: if a model works well to describe one experiment, it should continue to do so when that experiment is replicated tomorrow, or in another laboratory. The selection criterion must therefore be robust to the variations inherent to the replication process. In this work we develop a nonparametric method for estimating uncertainty on a model's empirical risk when replications are non-stationary, thus ensuring that a model is only rejected when another is reproducibly better. We illustrate the method with two examples: one a more classical setting, where the models are structurally distinct, and a machine learning-inspired setting, where they differ only in the value of their parameters. We show how, in this context of replicability or "epistemic uncertainty", it compares favourably to existing model selection criteria, and has more satisfactory behaviour with large experimental datasets.

---

### Learning the pattern of epistasis linking genotype and phenotype in a protein [^bb5a0648]. Nature Communications (2019). High credibility.

Fig. 4
Practical strategies for learning the epistatic structure: experiment. a, b Estimation of the top epistatic terms (including a broad range of high-order terms) from random samplings of phenotypes for 6–11% of variants, using the method of compressive sensing (CS). c, d Reconstruction of all phenotypes from the estimated epistatic terms in panels (a – b). The data show excellent approximation of relevant high-order epistasis and prediction of phenotypes from sparse sampling of data. e Goodness of phenotype prediction for all 8192 variants as a function of CS-based estimation of epistasis (top 81 or 260 terms) from many trials of sampling the indicated number of variants

In applying this approach for proteins in general, it will be important to understand how the sampling of mutations — the size of the experiment — scales with the number of sequence positions undergoing variation — the size of the problem. Since the latter grows exponentially, it seems likely that the degree of sparsity will be an even greater productive constraint for larger problems.

Practically learning epistasis — a statistical approach

A completely distinct approach for learning the epistatic architecture is suggested by analyzing the statistics of amino acid frequencies in an ensemble of functional sequences. The general idea is that the functional constraints on and between mutated positions should be reflected in the frequencies and correlations of amino acids in sequences that satisfy a threshold of functional selection — a statistical analog of epistatic interactions. To examine this, we used the current data to mimic a collection of functional sequences from the evolutionary record: we constructed a multiple sequence alignment of FP variants with brightness at or above the minimal value of the parental genotypes (mKate2, y > 0.73, n = 2032 sequences), and computed the statistics of amino acid occurrence and correlations at those positions. Representing the two amino acids x at each position i with −1 and +1 respectively, we find that the average value over all n functional sequencesand the joint expectation between pairs of positions i and j closely approximate the background averaged first-order and pairwise epistatic terms determined experimentally (up to a known scaling factor; Fig. 5a, b, "Methods", and ref.). This relationship holds even with sub-sampling functional sequences included in the alignment (Supplementary Fig. 10). From these alignment-derived epistasis terms, it is again possible to quantitatively predict the phenotypes determined (Fig. 5c) to an extent that approaches what is possible by just limiting epistasis to the second order (Fig. 3e).

---

### A rise-to-threshold process for a relative-value decision [^015aaf53]. Nature (2023). Excellent credibility.

A behavioural sequence for egg laying

We took videos of gravid Drosophila in a small chamber with a soft substrate floor and characterized a behavioural sequence for egg laying (see Supplementary Tables 1 and 2 for genotypes and conditions in all experiments). The six-step sequence begins with the fly standing still and performing an abdomen elongation (step 1) followed by a scrunch (step 2) (Fig. 1a). The fly then increases its locomotor speed during a search period (step 3), and finally it performs an abdomen bend for egg deposition (step 4), deposits an egg (step 5) and performs a second abdomen bend (step 6), probably for cleaning the ovipositor.

Fig. 1
oviDN [Ca 2+] dips during ovulation, rises for seconds to minutes and peaks immediately before the abdomen bend for egg deposition.

---

### Detecting recombination hotspots from patterns of linkage disequilibrium [^db63fae1]. G3 (2016). Low credibility.

With recent advances in DNA sequencing technologies, it has become increasingly easy to use whole-genome sequencing of unrelated individuals to assay patterns of linkage disequilibrium (LD) across the genome. One type of analysis that is commonly performed is to estimate local recombination rates and identify recombination hotspots from patterns of LD. One method for detecting recombination hotspots, LDhot, has been used in a handful of species to further our understanding of the basic biology of recombination. For the most part, the effectiveness of this method (e.g., power and false positive rate) is unknown. In this study, we run extensive simulations to compare the effectiveness of three different implementations of LDhot. We find large differences in the power and false positive rates of these different approaches, as well as a strong sensitivity to the window size used (with smaller window sizes leading to more accurate estimation of hotspot locations). We also compared our LDhot simulation results with comparable simulation results obtained from a Bayesian maximum-likelihood approach for identifying hotspots. Surprisingly, we found that the latter computationally intensive approach had substantially lower power over the parameter values considered in our simulations.

---

### A full-stack memristor-based computation-in-memory system with software-hardware co-development [^e758b02f]. Nature Communications (2025). High credibility.

Simulator-in-loop search in PTSS

The schematic of implementation of SIL search with GA in Supplementary Fig. S23. Initially, we introduce the GA to solve the aforementioned optimization problem. GA has a significant advantage in solving black-box optimization problems with large parameter search spaces and are widely used in optimizers of various AI compilers. In this work, we adopt an open-sourced library when implementing GA. When using this method to solve the problem, there are mainly two considerations: the representation of individuals in the GA and the way to obtain the fitness value. For the 'individual' representations in GA, the tunable parameters in this work can all be expressed as integers, hence we use the integer representations in GA. Specifically, the IT can be represented as an integer, such as m, which means m × 100 ns. The IEM can be represented as binary value (0,1). '0' represents fully unroll mode and '1' represents bit-slice unroll mode. Hence, we can combine the integers of the variable parameters (IT, WCN, IEM) of each layer according to the sequence of network layers. The resulting integer set can then serve as a single individual in the GA, as shown in Eq. (4).

The numbers represent the positional relationship of the network layers. By the position information of the parameters in the list, we can determine which layer and which parameter of the network the parameter corresponds to. For obtaining the fitness value, we mainly utilize the software framework proposed in this paper. It allows the new parameter combinations updated by the algorithm in each iteration to be recompiled into new CIM-IR. The new CIM-IR is then passed through the compiler backend to generate code that can run in a simulation environment. Using this generated code and a portion of the training dataset, we can obtain the output result under the current parameter combination. Finally, we use the MSE between this output result and the ideal result as the final fitness value, guiding the GA's search by minimizing the MSE error. In this process, we use a simulator as the inference environment for the algorithm instead of real hardware, mainly considering the inference speed of large volumes of data. Simulators are more flexible than real systems and can leverage the faster computation acceleration of GPUs for large-scale inference, thus speeding up the search process and reducing the time for parameter optimization.

---

### The backtracking survey propagation algorithm for solving random K-SAT problems [^7905ae97]. Nature Communications (2016). Medium credibility.

The SID algorithm then proceed by assigning variables (decimation step). According to SP equations, assigning a variable x i to its most probable value (that is, setting x i = 1 if w i + > w i − and viceversa), the number of clusters gets multiplied by a factor, called bias

With the aim of decreasing the lesser the number of cluster and thus keeping the largest the number of solutions in each decimation step, SID assigns/decimate variables with the largest b i values. In order to keep the algorithm efficient, at each step of decimation a small fraction f of variables is assigned, such that in O (log N) steps of decimation a solution can be found.

After each step of decimation, the SP equations are solved again on the subproblem, which is obtained by removing satisfied clauses and by reducing clauses containing a false literal (unless a zero-length clause is generated, and in that case the algorithm returns a failure). The complexity and the biases are updated according to the new fixed point messages, and a new decimation step is performed.

The main idea of the SID algorithm is that fixing variables that are almost certain to their most probable value, one can reduce the size of the problem without reducing too much the number of solutions. The evolution of the complexity Σ during the SID algorithm can be very informative. Indeed it is found that, if Σ becomes too small or negative, the SID algorithm is likely to fail, either because the iterative method for solving the SP equations no longer converges to a fixed point or because a contradiction is generated by assigning variables. In these cases the SID algorithm returns a failure. On the contrary, if Σ always remains well positive, the SID algorithm reduces so much the problem, that eventually a trivial SP fixed point, is reached. This is a strong hint that the remaining subproblem is easy and the SID algorithm tries to solve it by WalkSat.

A careful analysis of the SID algorithm for random 3-SAT problems of size N = O (10 5) shows that the algorithmic threshold achievable by SID is α a SID = 4.2525 (ref.), which is close, but definitely smaller than the SAT-UNSAT threshold α s = 4.2667.

The running time of the SID algorithm experimentally measured is O (N log(N)).

---

### Experimental noise cutoff boosts inferability of transcriptional networks in large-scale gene-deletion studies [^65bff624]. Nature Communications (2018). Medium credibility.

Case N 0 = 1

We assume that the i -th node is the only unperturbed node in the network and hence set B il = 0 for all l. From Eq. (11) we obtain a unique solution for the i -th row of A relative to the diagonal element, A ii, with U j 1 the j -th element of the eigenvector that has the smallest eigenvalue. Note that the first term in the brackets vanishes as B il = 0 andis the only non-zero element of Σ 0. The important point is that any dependency on σ –which affects eigenvalues but not eigenfunctions–has dropped out, making this method asymptotically unbiased with respect to measurement noise. The fact that we can determine the elements of the i -th row of A only relative to a reference value, A ii, is rooted in fact that we have to determine the N parameters { A i 1,…, A ii,…, A iN } from N − 1 perturbations. As a consequence, the strengths of the links onto the target nodes cannot be compared directly if their restoring forces or degradation rates, A ii, are different. Generally, only relative values of A can be determined, as the average perturbation strength on node i cannot be disentangled from its restoring force A ii –a problem that is typically circumvented by definingfor all i. For the case that all nodes in the network are perturbed one-by-one, we can cycle through the network and remove the perturbations that act on the current receiver node, whereas keeping the perturbations on the remaining nodes. By computing the N corresponding covariance matrices and their eigenvectors, we can infer the complete network structure from Eq. (14) if the data quality is sufficiently high. Note that the method makes use of the fact that multi-node perturbations can be realised by superposition of single-node perturbations, which is a special property of linear models.

---

### Scaling big data and computations on HPC clusters… [^2d5f0bcb]. FDA (2025). Medium credibility.

for { for { for { } } Simulations for { Partial simulations for { Partial simulations. Group-1 1 ·•• -: Prediction-I. "111 + MHiitWii 1 1 0 ·- L ', _ ~ ~. are partitioned into M and N subsets, which are combined into MxN unique pairs and processed in parallel using MxN tasks. A workflow is created to automate all the steps. Contact: > scaffold1. 1|size174461 CCTGCTCGCTGCGAC… 1 M > scaffold4. 1|size125892 ACCTGCTCGCTCGGG… > X17276. 1 Giant. GATCCTCCCCAGGCCT… 1 N > X51700. 1 Bos taurus… GTCCACGCAGCCGCTGAC… segmentation. " This enabled drops in run times from periods such as 28 years ATACACCAGGGC… to days. The computation segmentation is based on the built-in array job facility. of the job schedulers. for Database subsets Database GATCCTCCCCAGGCCCAATGT…

projects reduced run time from 27 days to a single day using 4, 104 tasks, each task taking less than 7 minutes to complete. CAMELYON datasets of 399 WSIs were partitioned into 27, 280 groups of ~230 MB. in use by FDA scientists. The techniques enable reduction of the data subset processed by each job task to a size that fits into the memory of the computing nodes where computations are performed. This avoids expensive I/O for swapping and produces excellent results, enabling substantial drops in run times. The described methods use only open-source. code with no additional hardware cost.
232. **Https**: //doi. org/10. 1177/0037549719878249.

---

### A deep-learning framework for multi-level peptide-protein interaction prediction [^222e6965]. Nature Communications (2021). High credibility.

Residue-level structural and physicochemical properties

We first define an alphabet of 21 elements to describe different types of amino acids (i.e. 20 canonical amino acids and a letter "X" for unknown or non-standard ones). Each type of amino acid is encoded with an integer between 1 and 21. For each amino-acid sequence S = (a 1, a 2. a n), we generate an n × 1 array, in which in the corresponding residue position, each element is an integer representing the amino-acid type.

In addition, although our problem setting assumes that 3D structure data are unavailable, previous studies have suggested that the predicted structures of the amino-acid sequences could still provide useful information. Here, for each amino-acid sequence S = (a 1, a 2. a n), we use SSProto generate an n × 1 array, in which each element is an integer representing the combination of secondary structure class and amino-acid type at the corresponding position (see Supplementary Note 7).

Furthermore, the hydrophobicity, hydrophilicity, and polarity of the R groups of individual amino acids can affect the tendency of the interactions between residues. For each amino-acid sequence S = (a 1, a 2. a n), we generate an n × 1 array, in which each element is an integer representing the combination of the polarity and hydropathy properties of the residue at the corresponding position (see Supplementary Note 7).

Protein evolutionary information

PSSMs are popular representations of protein sequences, which can detect remote homology of the protein sequences. For each protein sequence S = (a 1, a 2. a n) of length n, we use PSI-BLASTto generate a normalized position-specific scoring matrix, an n × 20 array S, in which each element S i, j stands for the probability of the j th amino-acid type at position i in the protein sequence (see Supplementary Note 7).

Intrinsic disorder tendencies to form contacts

It has been reported that the intrinsic disorder-based features in peptide and protein sequences play a crucial role in protein–peptide interactions. Here, for individual residues in the peptide and protein sequences, we first employ IUpred2A, to predict its intrinsic disorder properties. For an amino-acid sequence S of length m, we construct an m × 3 arrays representing three types of disorder scores for individual residues (see Supplementary Note 7).

---

### Data storage using peptide sequences [^f0dce428]. Nature Communications (2021). High credibility.

At step 8, the candidate paths could be constructed by combining the three parts: N-terminus, tag, and C-terminus. At step 9, one could follow steps 3 and 4 of the two-stage sequencing method to select and refine the sequences. Note that a larger value for W sometimes introduced one or more wrong amino acids in the head and/or tail parts of a tag, while a smaller value for W may give more reliable tag but the length of the tag may be limited. Therefore, after step 6, if no valid candidate could be found, one may attempt to reduce the value of W with W = w i by increasing i by 1, i.e. i = i + 1, and repeat the tag, N-terminus and C-terminus finding procedure until the candidate sequence could be found or i = V (where V is the maximum number of iterations).

For the special case when the experimental mass with the highest intensity gave an unreliable message due to noise and uncertainty, a highest-intensity-based tag or a valid path with the highest-intensity-based tag could not be found. In this case, the mass with the second highest intensity was used by setting J = J + 1 and i = 1 to find the second highest-intensity-based tag and the candidates. This process would continue until the sequence could be found or J = J max (where J max is the maximum number of higher-ranking-intensity masses allowed to be the start point to find the tag).

---

### Odd electron wave packets from cycloidal ultrashort laser fields [^c8fab5c5]. Nature Communications (2019). High credibility.

Results

Polarization profile and wave packet

Recently, the intriguing properties of polarization-controlled bichromatic fields have been highlighted in the context of HHG. In general, circularly polarized bichromatic fields exhibit cycloidal polarization profiles. Measured polarization profiles of propeller-type CRCP and heart-shaped corotating circularly polarized (COCP) bichromatic pulses are illustrated in Fig. 1. To discuss the interplay of phase effects in optics and quantum mechanics, we start by considering the properties of the polarization profile of commensurable BiCEPS fields with center frequency ratio ω 1: ω 2 = N 1: N 2. The rotational symmetry of the field is given by, where the minus (plus) sign corresponds to the COCP (CRCP) case and gcd denotes the greatest common divisor. By introducing the CEP φ ce and the relative phases(τ: time delay) and φ 2 of the low- (red) and high-frequency (blue) field, respectively, an (N 1 ω: N 2 ω) BiCEPS pulse rotates in the polarization plane by an angle ofmeasured in ϕ -direction [cf. Fig. 1a and Supplementary Note 1]. The angleapplies to a pair of left-handed circularly polarized (LCP) pulses, whereas a right-handed circularly polarized (RCP) sequence rotates by. An (N 1 ω: N 2 ω) CRCP field, consisting of an LCP (RCP) red and an RCP (LCP) blue field, rotates about. For extremely short few-cycle fields the pulse envelope needs to be taken into account as well. Equation (1) shows that the sense of rotation is opposite for φ 1 and φ 2. Moreover, the sensitivity of the rotation to φ ce, φ 1 and φ 2 is different for CRCP and COCP fields and generally more sensitive for COCP than for CRCP pulses. For example, the rotation of a (3 ω :4 ω) CRCP pulse, as used in the experiment, by the CEP is, whereas the rotation of any COCP pulse is. The CEP-dependent rotation of the field explains, why CEP stability is required in experiments using shaper-generated bichromatic fields.

---

### Fast and sensitive mapping of nanopore sequencing reads with graphMap [^f4e7db26]. Nature Communications (2016). Medium credibility.

Refining alignments using L 1 linear regression. The alignments obtained using LCSk tend to be largely accurate but since its definition lacks constraints on the distance between substrings, the alignments obtained may include outlier matches and incorrect estimation of overall alignment length (Fig. 1e). These outliers are caused by repeats or sequencing errors, but they still satisfy the monotony condition. Similar to the observation presented for region selection, the LCSk list of anchors should ideally be collinear in the 2D query-target coordinate space, with a slope of 45°. All deviations from this line are caused by indel errors, and can be viewed as noise. The filtering of outlier anchors begins by fitting a 2D line with a 45° slope in the query-target space under the least absolute deviation criteria (L 1). Next, a subset of anchors which are located withinfrom either side of the L 1 line is selected, where e is the expected error rate (by default, conservatively set to 45%), T is the target (read) length and the factoris used to convert the distance from target coordinate space to a distance perpendicular to the L 1 line. A confidence intervalis calculated, where d i is the distance from a selected anchor i to the L 1 line (the constant 3 was chosen to mimic a 3σ rule). LCSk is then repeated once again but only on the anchors which are located within the distance ± c from the L 1 line to compensate for possible gaps caused by anchor filtering (Fig. 1e). The use of L 1 filtering was observed to improve the precision of alignment start and end coordinates for many reads, though the overall impact on performance was less significant in comparison to the LCSk stage (Supplementary Data 3).

After filtering, five empirically derived scores that describe the quality of the region are calculated. They include: the number of exact kmers covered by the anchors n kmers, the s.d. σ of anchors around the L 1 line, the length of the query sequence which matched the target (distance from the first to the last anchor) m len, the number of bases covered by anchors (includes only exact matching bases) n cb and the read length. The last four scores are normalized to the range [0,1] with the following equations (4, 5, 6, 7):

where Q is the length of the reference sequence (query in our previous definition). The overall quality of the alignment in a region is then calculated as the product of the normalized scores:

---

### Experimental noise cutoff boosts inferability of transcriptional networks in large-scale gene-deletion studies [^981a9ed7]. Nature Communications (2018). Medium credibility.

Case N 0 > 1

If more than one node are not perturbed we get from Eq. (11)Non-unique solutions of Eq. (15) can arise if a given fraction of the variance of the receiver node i can be explained by more than one sender node, for example, when a perturbed node j targets two nodes with index i and l. In this case it is unclear from the node activity data whether i is affected directly by j or indirectly through l, or by a combination of both routes. If node l is not perturbed or only weakly perturbed, a statistical criterion is needed to decide about inferability or identifyability of the link j → i, which can be computed numerically as follows: To find out whether j transmits a significant amount of information to i that is not passing through l, we first remove node j from the observable nodes of the network but keep its perturbative effect on other nodes in the data set. We then determine the link strengths A ′ for the remaining network of size N − 1. To construct a possible realisation of A ′ we set in Eq. (15) the non-zero values of Σ 0 to unity and use W = U to arrive at the expressionwith U ′ determined from the sample covariance matrix with the j -th column and j -th row removed. Fixing W and Σ 0 to seemingly arbitrary values does not affect the result we are after. If l is the only unperturbed node besides i, then in the A ′ system l can now be treated as perturbed — as it may receive perturbations from the unobserved node j — and thus Eq. (14) applies. If l is part of many unperturbed nodes that are affected by j, then the knowledge how much each of these nodes contributes to the variance of the target node i (which is determined by W and Σ 0) is irrelevant as we are only interested in the total effect of the alternative routes on node i. Using the inferred link strength from Eq. (16) we can rewrite Eq. (2) as a two-node residual inference problem between j and i, where we obtain a lower bound for link strength from node j to i by using the variation of i that could not be explained by A ′. This concept is similar to computing partial correlations. Defining by, andthe 2 × 2 analogues to the full problem we obtainwiththe covariance matrix of the vectorand, using the scaled variances. Note that A ii < 0 for all i as these elements represent sufficiently strong restoring forces that ensure negative definiteness of A and that we havefrom Eq. (1) in the stationary case. An estimate for the minimum relative link strength from node j to node i can be calculated from Eq. (13) and is given byEq. (18) can be considered as an asymptotically unbiased response coefficient between node 1 as target node and node 2 as source node, as again any dependency on σ 2 has dropped out. An estimate for the maximum relative link strength from node j to node i follows from Eq. (18) with the off-diagonal elements of A ′ set to zero. We classify a link as non-inferable if there exists (i) a significant difference between the minimum und maximum estimated link strength and (ii) a minimum link strength that is not significantly different from noise.

---

### Reconstructing missing complex networks against adversarial interventions [^a7d10714]. Nature Communications (2019). High credibility.

Solving the model identification problem in Eq. (4) simultaneously leads us to the solution of the inference problem. However, the marginalization over the latent variable M t, ψ, and π is computationally intractable. To approach this problem, we replace the marginalization process by constructing a series of maximization steps over the incomplete likelihood functionconditioned on the propagated belief about the model parameters.

Formally at i th step by taking the log-likelihood,

constructs an incomplete maximum likelihood function in terms of observable part of the network. It averages out the contribution of the missing information { M t, ψ, π } by using the incomplete MLE forat previous step to infer a current guess on { M t, ψ, π }. Complex models for high dimensional data lead to intractable integrals as the ones in Eq. (6). To overcome this drawback, we approximate the integral conditioned on the current guess on the generative measurevia a Monte-Carlo sampling procedure,

where the samples are drawn from. Update the estimator ofby maximizing, Under regularity conditions, and given a suitable starting value, the resulting sequencewill converge to a local maximizer of the likelihood function by alternating the above procedure until the differencechanges by an arbitrarily small amount. It should be noted that the above procedure not only constructs a MLE for the underlying modelbut also simultaneously returns the most probable guess on M t, ψ and π in a maximum likelihood sense.

The finite sum approximation of the expectation depends on being able to draw samples from the joint distribution. Instead of using uniform sampling that generates unimportant samples in an unprincipled fashion, we need to confine the samples to be drawn from the region where the integrand of Eq. (6) is large. Moreover, the computational intractability of sampling the posterior joint distribution also originates from the factorial dependence of the sample space on the size of the original network and the missing network. This factorial dependence comes from the requirement to infer the time-stamp mapping π and the linking probability measure mapping ψ for each node in the missing network M t. Consider a temporally ordered sequence of subgraph Z t = { z 0, z 1. z t −1 } that corresponds to trajectory of the subgraph removed at each step of the intervention up to time t. Inferring the optimal π and ψ for each node implies that when maximizing Eq. (5) the following relation holds:

---

### Sequence-to-sequence models with attention mechanistically map to the architecture of human memory search [^5ba6fa32]. Communications Psychology (2025). Medium credibility.

Encoding phase

During the encoding phase in the free recall task (pictured in Fig. 2 C), participants study a list of L items one after another (drawn from a total number of N possible items in the experimental word pool). The CMR model proposes that their context slowly drifts towards the memory representations of recently encountered experiences. The state of the context at time stepis given by:whereis the retrieved context (or input embeddings) of the just-encoded item, β ∈ [0, 1] is a parameter determining the rate at which context drifts toward the new context, and ρ is a scalar ensuring ∣∣ c i ∣∣ = 1. The retrieved context x i is further expressed as:whererepresents item-to-context associations that existed prior to the experiment (initialized as an identity matrix, under the simplifying assumption that an item is only associated with its own context), andis a one-hot column vector that is all zeros except at the position that represents an item's identity. Therefore, is the context previously associated with the presented item at encoding step i, which is simply f i.

In addition to updating the context c i, CMR forms associations between items and the evolving context throughout the encoding phase to capture new learning in the experiment – through experimental item-to-context and context-to-item associations held inand. These matrices will be useful later in the recall phase to reactivate the corresponding encoding context of a given item or to retrieve an item corresponding to a given context. They are initialized to zero at the start of the experiment and are updated via the Hebbian outer-product learning rule. Specifically, when an item is encoded at timestep i, an association is formed between the previous context state c i −1 and the presented item f i :Similarly, the association from context to item, is updated according to:

Following equations (9) and (10), after all L items in a list have been studied, the final experimental association matrices before the start of the recall phase can be written as:

---

### Abstract representations emerge naturally in neural networks trained to perform multiple tasks [^55073c20]. Nature Communications (2023). High credibility.

Methods

Abstraction metrics

Both of our abstraction methods quantify how well a representation that is learned in one part of the latent variable space (e.g. a particular context) generalizes to another part of the latent variable space (e.g. a different context). To make this concrete, in both metrics, we train a decoding model on representations from only one — randomly chosen — half of the latent variable space and test that decoding model on representations from the non-overlapping half of the latent variable space.

The classifier generalization metric

First, we select a random balanced division of the latent variable space. One of these halves is used for training, the other is used for testing. Then, we select a second random balanced division of the latent variable space that is orthogonal to the first division. One of these halves is labeled category 1 and the other is labeled category 2. As described above, we train a linear classifier on this categorization using 1000 training stimuli from the training half of the space, and test the classifier's performance on 2000 stimuli from the testing half of the space. Thus, chance is set to 0.05 and perfect generalization performance is 1.

The regression generalization metric

As above, except we train a linear ridge regression model to read out all D latent variables using 4000 sample stimulus representations from the training half of the space. We then test the regression model on 1000 stimulus representations sampled from the testing half of the space. We quantify the performance of the linear regression with its r 2 value:where X is the true value of the latent variables andis the prediction from the linear regression. Because the MSE is unbounded, the r 2 value can be arbitrarily negative. However, chance performance is r 2 = 0, which would be the performance of the linear regression always predicted the mean of X, and r 2 = 1 indicates a perfect match between the true and predicted value.

---

### Fundamental limits to learning closed-form mathematical models from data [^a345300f]. Nature Communications (2023). High credibility.

Given a finite and noisy dataset generated with a closed-form mathematical model, when is it possible to learn the true generating model from the data alone? This is the question we investigate here. We show that this model-learning problem displays a transition from a low-noise phase in which the true model can be learned, to a phase in which the observation noise is too high for the true model to be learned by any method. Both in the low-noise phase and in the high-noise phase, probabilistic model selection leads to optimal generalization to unseen data. This is in contrast to standard machine learning approaches, including artificial neural networks, which in this particular problem are limited, in the low-noise phase, by their ability to interpolate. In the transition region between the learnable and unlearnable phases, generalization is hard for all approaches including probabilistic model selection.

---

### Accurately clustering biological sequences in linear time by relatedness sorting [^6aa50ac0]. Nature Communications (2024). High credibility.

Phase 2: pre-sorting sequences based on k-mer similarity in linear time

The key innovation underlying the Clusterize algorithm is to pre-sort sequences such that accurate clustering can be performed in linear time. Pre-sorting is conducted within each partition until reaching an iteration limit (i.e. parameter B) or the rank ordering of sequences stabilizes. First, a sequence is randomly selected from each partition with a probability proportional to its average variability in rank order (v) multiplied by its length (L): This formula encourages the selection of longer sequences that still have not stably sorted. Then, the k-mer distance (1 - k-mer similarity) is calculated between the sequence and all others, resulting in a vector of distances (d 1). Second, another sequence is randomly selected with probability proportional to d 1 and inversely proportional to the non-overlapping difference in length (Δov) with the first randomly selected sequence: This prioritizes sequences that overlap completely with the first sequence but are dissimilar. Again, the k-mer distance is calculated between the second sequence and all others (d 2). The relative difference between these two vectors (d 1 − d 2) becomes the initial vector (D) upon which the rank order of sequences is determined.

The process of generating new relative distance vectors (D) is repeated up to 2000 iterations (i.e. parameter B) within each partition. I termed this process relatedness sorting because more similar sequences become closer together with additional iterations. Only a single vector is kept after each iteration to limit the memory overhead of phase 2. There are multiple ways in which to combine D i and D i-1 into a new unified vector, D. I chose to project both vectors onto their axis of maximum shared variance after centering both vectors to have a mean of zero (Fig. 1I). This is equivalent to performing principal component analysis on the two column matrix formed by combining D i and D i-1 and retaining only the first principal component (as D). Each vector is ordered with radix sorting to define the rank order of sequences. Clusterize keeps track of the number of positions each sequence moves in the relatedness ordering (v) using an exponentially weighted moving average with smoothing parameter α (0.05): Note that the vector v 0 is initialized with values of 2000 (i.e. parameter C) and values of v are capped at 2000 in each iteration as an upper limit for practicality.

---

### Modelling sequences and temporal networks with dynamic community structures [^7129d625]. Nature Communications (2017). Medium credibility.

Bayesian Markov chains with communities

As described in the main text, a Bayesian formulation of the Markov model consists in specifying prior probabilities for the model parameters, and integrating over them. In doing so, we convert the problem from one of parametric inference where the model parameters need to be specified before inference, to a nonparametric one where no parameters need to be specified before inference. In this way, the approach possesses intrinsic regularisation, where the order of the model can be inferred from data alone, without overfitting.

To accomplish this, we rewrite the model likelihood, using eqs. (1) and (5), asand observe the normalisation constraints,… Since this is just a product of multinomials, we can choose conjugate Dirichlet priors probability densitiesand, which allows us to exactly compute the integrated likelihood, whereand. We recover the Bayesian version of the common Markov chain formulation (see ref.) if we put each memory and token in their own groups. This remains a parametric distribution, since we need to specify the hyperparameters. However, in the absence of prior information it is more appropriate to make a noninformative choice that encodes our a priori lack of knowledge or preference towards any particular model, which amounts to choosing α x = β rs = 1, making the prior distributions flat. If we substitute these values in eq. (19), and re-arrange the terms, we can show that it can be written as the following combination of conditional likelihoods, wherewithbeing the multiset coefficient, that counts the number of m -combinations with repetitions from a set of size n. The expression above has the following combinatorial interpretation: P ({ x t }| b, { e rs }, { k x }) corresponds to the likelihood of a microcanonical modelwhere a random sequence { x t } is produced with exactly e rs total transitions between groups r and s, and with each token x occurring exactly k x times. In order to see this, consider a chain where there are only e rs transitions in total between token group r and memory group s, and each token x occurs exactly k x times. For the first transition in the chain, from a memory x 0 in group s to a token x 1 in group r, we have the probabilityNow, for the second transition from memory x 1 in group t to a token x 2 in group u, we have the probabilityProceeding recursively, the final likelihood for the entire chain iswhich is identical to eq. (21).

---

### Accurate high throughput alignment via line sweep-based seed processing [^26f5da73]. Nature Communications (2019). High credibility.

Methods

Basic notions and planar alignment representation

We consider alignments as a path that follows a sequence of pointsin a two-dimensional plane, where | A | shall denote the alignment length. With this plane, the reference R is on the x-axis and the query Q is on the y-axis, with | Q | < | R |. Please note: The sequences r 0, r 1,…, r | A | and q 0, q 1,…, q | A | are both monotonically increasing. Two consecutive points (r n, q n) and (r n +1, q n +1) are always associated in one of the following four ways:
Deletion: r n +1 = r n + 1, q n +1 = q n; visually this is equal to a horizontal path extension.
Insertion: r n +1 = r n, q n +1 = q n + 1; visually this is equal to a vertical path extension.
Match or Mismatch: r n +1 = r n + 1, q n +1 = q n + 1; visually both appear as diagonal path extensions.

For example, in Fig. 5 a complete alignment via the orange path follows the sequence: [(0, 0), (1, 1), (2, 2), (3, 2), (4, 2), (5, 2), (6, 2), (7, 2), (8, 2), (8, 3), (9, 4), (10, 5), (11, 6), (11, 7), (12, 8), (13, 9)]

The alignment process is decomposed into three separated stages: seeding, seed processing and dynamic programming (DP). In the following three sections, we will explain our approaches for all three stages in detail.

---

### A machine learning automated recommendation tool for synthetic biology [^5caa5c39]. Nature Communications (2020). High credibility.

Optimization-suggesting next steps

The optimization phase leverages the predictive model described in the previous section to find inputs that are predicted to bring us closer to our objective (i.e. maximize or minimize response, or achieve a desired response level). In mathematical terms, we are looking for a set of N r suggested inputs, that optimize the response with respect to the desired objective. Specifically, we want a process that:
i. optimizes the predicted levels of the response variable;
ii. can explore the regions of input phase space (in Eq. (1)) associated with high uncertainty in predicting response, if desired;
iii. provides a set of different recommendations, rather than only one.

We are interested in exploring regions of input phase space associated with high uncertainty, so as to obtain more data from that region and improve the model's predictive accuracy. Several recommendations are desirable because several attempts increase the chances of success, and most experiments are done in parallel for several conditions/strains.

In order to meet these three requirements, we define the optimization problem formally aswhere the surrogate function G (x) is defined as:depending on which mode ART is operating in (see the "Key capabilities" section). Here, y * is the target value for the response variable, y = y (x), and Var(y) denote the expected value and variance, respectively (see "Expected value and variance for ensemble model" in Supplementary Information), denotes Euclidean distance, and the parameter α ∈ [0, 1] represents the exploitation-exploration trade-off (see below). The constraintcharacterizes the lower and upper bounds for each input feature (e.g. protein levels cannot increase beyond a given, physical, limit). These bounds can be provided by the user (see details in the "Implementation" section in Supplementary Information); otherwise, default values are computed from the input data as described in the "Input space set" section in Supplementary Information.

---

### Personal genomic measurements: the opportunity for information integration [^82fb95db]. Clinical Pharmacology and Therapeutics (2013). Low credibility.

High-throughput genomic measurements initially emerged for research purposes but are now entering the clinic. The challenge for clinicians is to integrate imperfect genomic measurements with other information sources so as to estimate as closely as possible the probabilities of clinical events (diagnoses, treatment responses, prognoses). Population-based data provide a priori probabilities that can be combined with individual measurements to compute a posteriori estimates using Bayes' rule. Thus, the integration of population science with individual genomic measurements will enable the practice of personalized medicine.

---

### Two-phase designs for joint quantitative-trait-dependent and genotype-dependent sampling in post-GWAS regional sequencing [^9efc8a74]. Genetic Epidemiology (2018). Low credibility.

The design objective is to select a subset of informative subjects based on available data in phase 1, namely. Inference on the missing‐by‐design sequence variants is conducted using all available data from phases 1 and 2. We define the missing indicator, where N is the number in individuals measured in phase 1. S 2 represents the phase 2 sample of n subjects. We letdenote the set of subjects that are not in the phase 2 sample but are present in the GWAS study.

Regression analysis at the GWAS phase uses the surrogate variable, Z, which does not usually have biological function, say by fitting. Nonetheless, GWAS analyses serve as an efficient screening strategy to identify candidate regions in the genome. On the other hand, in fine‐mapping, both causal and null variants are colocated in the region, and these null variants do not correspond to the usual model‐based null hypothesis, that is, in (1). Rather, a null SNP in the region of interest (denoted by G 0) has no direct association with the QT (Y), but because it may be in LD with G 1 or Z, an indirect association with Y may be detected. This makes it difficult to distinguish among the individual contributions, even in the complete data case (see Faye, Machiela, Kraft, Bull, and Sun (2013) for reranking strategies in this scenario). In addition, we note that including an identified GWAS‐SNP, Z, in strong LD in the causal variant regression model (1) would introduce collinearity and decrease precision, reducing the ability to detect genetic association of a causal variant G with the QT.

---

### Advanced methods for gene network identification and noise decomposition from single-cell data [^74e2d243]. Nature Communications (2024). High credibility.

Usually, CMEs are difficult to solve explicitly. Conventional methods to numerically solve the CME include the simulation-based Monte Carlo methods and the finite state projection (FSP) method. However, both of them scale poorly with the size of the state space and the system dimension.

A simulation-based Monte Carlo method first simulates N trajectories of the system (2), denoted by x 1 (t),…, x N (t), and then uses the empirical distributionto approximate the exact probability distribution. Here, is the indicator function, equal to 1 when its argument is true, otherwise 0. The error of this method can be evaluated by the L 1 distance between p MC (t, ⋅) and p (t, ⋅), which upper bounds the largest possible error of the Monte Carlo in estimating any particular probability. Mathematically, this error converges at the rate of, and its pre-convergence rate factor (defined by) lies in a particular range shown as follows (Supplementary Information, section S2. A) :This suggests that given a large sample size N, the error of the Monte Carlo largely depends on the value of. This value tends to scale poorly with the size of the state space containing most of the probability mass. Particularly, when the probability mass is uniformly distributed on S states, this quantity equals, which can be very large when S is big. Also, since this state size often grows exponentially with the number of species, this quantitytends to scale poorly with the system dimension n. This point can be clearly seen when the molecular counts of different chemical species are independent; in this case, does grow exponentially with n, as its value is equal towith p i (t, ⋅) the marginal distribution for the i -th species. In summary, the error of a Monte Carlo method tends to scale unfavorably with the size of state space and system dimension, and, thus, this method usually performs poorly in high-dimensional problems.

In contrast, the FSP approach, which directly solves a truncated CME on a large but finite state space, is very accurate for arbitrary reaction systems. However, the computational complexity of this method scales cubically with the size of the truncated state space. Moreover, since the size of the finite state space scales exponentially with n, the FSP is computationally demanding for high-dimensional problems.

In conclusion, both Monte Carlo and FSP methods suffer the curse of dimensionality.

---

### An epigenomic roadmap to induced pluripotency reveals DNA methylation as a reprogramming modulator [^316936f9]. Nature Communications (2014). Medium credibility.

Identification of methylated cytosines

At each reference cytosine, the binomial distribution was used to identify whether at least a subset of the genomes within the sample were methylated, using a 0.01 FDR-corrected P value. We identified methyl cytosines while keeping the number of false-positive methylcytosine calls below 1% of the total number of methyl cytosines we identified. The probability P in the binomial distribution B (n, P) was estimated from the number of cytosine bases sequenced in reference cytosine positions in the unmethylated Lambda genome (referred to as the error rate: nonconversion plus sequencing error frequency). We interrogated the sequenced bases at each reference cytosine position one at a time, where read depth refers to the number of reads covering that position. For each position, the number of trials (n) in the binomial distribution was the read depth. For each possible value of n we calculated the number of cytosines sequenced (k) at which the probability of sequencing k cytosines out of n trials with an error rate of p was less than the value M, where M * (number of unmethylated cytosines) < 0.01* (number of methylated cytosines) and if the error rate of p was over 0.01, we assumed that the cytosine was not methylated. In this way, we established the minimum threshold number of cytosines sequenced at each reference cytosine position at which the position could be called as methylated, so that out of all methyl cytosines identified no more than 1% would be because of the error rate.

Calculation of DNA methylation level

If the error rate is less than 0.01 we calculated adjusted DNA methylation level for cytosine as follow:

(a = total Cs, b = number of converted Cs, cr = bisulfite conversion rate).

---

### Accurately clustering biological sequences in linear time by relatedness sorting [^d1cda885]. Nature Communications (2024). High credibility.

The intuition underlying phase 2 is to pre-sort sequences within partitions into an order that is similar to where they would be located at the tips of a phylogenetic tree. That is, more similar sequences would be closer to each other after ordering. A straightforward way to arrange sequences in this manner is to imagine an unrooted phylogenetic tree with two leaves (Fig. 1E). A third leaf could be added at any point along the edge between the two existing leaves (Fig. 1F). Knowing the distances (d i) between the third sequence and the other two (d 1 and d 2) helps to decide whether the new leaf belongs closer to one leaf than the other. If two new leaves are both much closer to one of the leaves, it is possible they are also close to each other (Fig. 1G). Ordering sequences in this manner can be thought of as relatedness sorting because more related sequences are closer together after sorting.

Choosing two leaves randomly and calculating their difference in distance (d 1 − d 2) to each sequence results in a vector of relative distances (Fig. 1H). This vector (D 1) is then projected onto the axis of maximum variance shared with another relative distance vector (D 2) calculated from two other random leaves (Fig. 1I). Projecting in this manner is equivalent to performing principle components analysis on D 1 and D 2 to retain only the first principal component (D). Repeatedly projecting new vectors onto D results in more related sequences having more similar relative distances and, thus, moving closer together in the ordering of D. Sorting the distance vector therefore provides an ordering similar to that of the leaves along a phylogenetic tree. After several iterations, large gaps may appear in the relative distance vector that can be used to split the partition into smaller groups (Fig. 1J). Relatedness sorting is continued within each group until the ordering of sequences stabilizes within a tolerance or a maximum number of iterations (parameter B, by default 2,000) is reached (Fig. 1K).

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^efb30806]. Journal of the American College of Cardiology (2025). High credibility.

Structure of economic value statements in clinical guidelines — In the interest of consistency and comparability across guidelines, we suggest that economic value statements report 3 components: 1) Class of Recommendation; 2) Level of Certainty; and 3) Recommendation, and the economic value statement should ideally be placed immediately following the clinical recommendation for the intervention of interest.

---

### Optimization and control of actuator networks in variable geometry Truss systems using genetic algorithms [^a29f2f97]. Nature Communications (2025). High credibility.

Our optimization framework takes as inputs the truss topology, initial joint positions, and target objectives. The framework optimizes variables including C-network assignments, contraction levels, and actuation sequences. Each design undergoes evaluation across multiple objective functions, generating performance scores in a multi-dimensional evaluation space. These scores then feed into the NSGA-II algorithm, which ranks designs based on Pareto dominance and crowding distance to guide selection. The selected designs get transformed through our tailored genetic operators – specifically designed to maintain connectivity and symmetry constraints – to generate the next generation. To prevent premature convergence and maintain design diversity, we implement an elite pool strategy that temporarily preserves high-performing designs while allowing continued exploration of the design space (see Methods - Elite Pool Details).

Design representation

To efficiently and concisely describe a metatruss design compatible with the genetic algorithm, we use a one-dimensional integer array as its representation. We represent every parameter, including the C-network assignment, contraction level, and actuation sequences in integers, and concatenate them into a 1D integer vector. Specifically, this array is structured into three segments, each encapsulating specific design parameters of the metatruss (Fig. 2 a):
C-network Assignment: The initial segment of the array captures the affiliation of each beam to a specific C-network. Integers within this segment correspond to the indices of C-networks to which each beam is assigned (Fig. 2 b).
Contraction Level: The second segment represents the preset contraction levels for the beams. Each integer in this section signifies a preset level, which corresponds to a predefined contraction ratio (Fig. 2 c).
Actuation Sequences: The final section captures the dynamic aspects of the metatruss design – the actuation sequences. Each integer here indicates the on/off states for the air valves that govern each C-network at every time step (Fig. 2 d). The actuation sequences are flattened into a one-dimensional array and concatenated into the representation.

This encoding represents all the information of a metatruss design as an integer vector that is suitable for the genetic algorithm to optimize. The detailed definition of the representation can be found in Supplementary Note 3: Representation Details.

---

### Rigorous location of phase transitions in hard optimization problems [^d98ebea5]. Nature (2005). Excellent credibility.

It is widely believed that for many optimization problems, no algorithm is substantially more efficient than exhaustive search. This means that finding optimal solutions for many practical problems is completely beyond any current or projected computational capacity. To understand the origin of this extreme 'hardness', computer scientists, mathematicians and physicists have been investigating for two decades a connection between computational complexity and phase transitions in random instances of constraint satisfaction problems. Here we present a mathematically rigorous method for locating such phase transitions. Our method works by analysing the distribution of distances between pairs of solutions as constraints are added. By identifying critical behaviour in the evolution of this distribution, we can pinpoint the threshold location for a number of problems, including the two most-studied ones: random k-SAT and random graph colouring. Our results prove that the heuristic predictions of statistical physics in this context are essentially correct. Moreover, we establish that random instances of constraint satisfaction problems have solutions well beyond the reach of any analysed algorithm.

---

### Explaining a series of models by propagating shapley values [^85e04748]. Nature Communications (2022). High credibility.

Local feature attribution methods are increasingly used to explain complex machine learning models. However, current methods are limited because they are extremely expensive to compute or are not capable of explaining a distributed series of models where each model is owned by a separate institution. The latter is particularly important because it often arises in finance where explanations are mandated. Here, we present Generalized DeepSHAP (G-DeepSHAP), a tractable method to propagate local feature attributions through complex series of models based on a connection to the Shapley value. We evaluate G-DeepSHAP across biological, health, and financial datasets to show that it provides equally salient explanations an order of magnitude faster than existing model-agnostic attribution techniques and demonstrate its use in an important distributed series of models setting.

---

### A complete hierarchy for the pure state marginal problem in quantum mechanics [^3c6d40d7]. Nature Communications (2021). High credibility.

Clarifying the relation between the whole and its parts is crucial for many problems in science. In quantum mechanics, this question manifests itself in the quantum marginal problem, which asks whether there is a global pure quantum state for some given marginals. This problem arises in many contexts, ranging from quantum chemistry to entanglement theory and quantum error correcting codes. In this paper, we prove a correspondence of the marginal problem to the separability problem. Based on this, we describe a sequence of semidefinite programs which can decide whether some given marginals are compatible with some pure global quantum state. As an application, we prove that the existence of multiparticle absolutely maximally entangled states for a given dimension is equivalent to the separability of an explicitly given two-party quantum state. Finally, we show that the existence of quantum codes with given parameters can also be interpreted as a marginal problem, hence, our complete hierarchy can also be used.

---

### Unsupervised vector-based classification of single-molecule charge transport data [^6a217f37]. Nature Communications (2016). Medium credibility.

Results

Vector-based data analysis

Vector-based classification methods are powerful tools for categorizing the data and have found widespread application in such fields as genetics, robotics and neuroscience. Generally, they operate by regarding a data set, for example, an I(s) curve with N current values in the present case, as an N -dimensional vector X n (n = 1. N); a total number of M observations thus results in a data matrix X n, m (m = 1. M) or, in short X m (dropping n for convenience). The Euclidean distance | Δ X| between the two vector points may then be used as a measure for similarity between different data sets m and m ′, equation (1):

where K is an optional normalization constant (if required, so that 0 ≤ Δ X ≤ 1). If X m and X m ′ are identical, then Δ X m, m ′ is zero; if they are very different, Δ X m, m ′ is large. After calculating all combinations of distances between the M data sets, a distance or probability criterion may then be used to classify the data. The computational effort scales with (M −1)· M, according to the variation formula, and can be very significant for some of our larger data sets (for example, involving 70,000 traces, see below).

In the case of I (s) data, however, we found the mutual distance criterion to be insufficient in many cases. A different, somewhat expanded methodology was required, which as we show below, significantly improved the classification performance.

In a first step, we defined an arbitrary, N -component reference vector R, which generally depends on the data to be analysed. It could be determined self-consistently, based on some optimization parameter (for example, to maximize the variance of the existing data around R). We chose a vector with noise-free, exponentially decaying current–distance values, which is similar to the experimental data without molecular binding events (I 0 = 20 nA; decay coefficient β = 1 Å −1), Fig. 1c (dashed, magenta line).

---

### Sequencing studies in human genetics: design and interpretation [^c67315d4]. Nature Reviews: Genetics (2013). Medium credibility.

Next-generation sequencing is becoming the primary discovery tool in human genetics. There have been many clear successes in identifying genes that are responsible for Mendelian diseases, and sequencing approaches are now poised to identify the mutations that cause undiagnosed childhood genetic diseases and those that predispose individuals to more common complex diseases. There are, however, growing concerns that the complexity and magnitude of complete sequence data could lead to an explosion of weakly justified claims of association between genetic variants and disease. Here, we provide an overview of the basic workflow in next-generation sequencing studies and emphasize, where possible, measures and considerations that facilitate accurate inferences from human sequencing studies.

---

### Revised criteria for diagnosis and staging of Alzheimer's disease: Alzheimer's association workgroup [^e8e0709f]. Alzheimer's & Dementia (2024). High credibility.

Integrated biological and clinical staging — table aligns biological stages A–D with clinical stages 0–6, enumerating combinations such as Initial biological stage (A) with 1A, 2A, 3A, and 4–6A and Advanced biological stage (D) with 1D, 2D, 3D, and 4–6D; the typical expected progression is along the diagonal shaded cells from 1A to 4–6D, with individuals above the diagonal often having greater than average comorbid pathology and those below possibly reflecting exceptional cognitive reserve or resilience.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^d82e5daf]. The Journal of Molecular Diagnostics (2017). Medium credibility.

NGS oncology panel validation — sample size calculations and rule of three: Required validation samples can be calculated from r^n = α with n = ln(α)/ln(r); for 95% confidence (α = 0.05) and at least 95% reliability (r = 0.95) the minimum number of samples is 59. For a 95% confidence level, the rule of three applies: counting 100 cells with no positives allows a claim that < 1% of cells have the event, but to be 95% confident that < 1% of cells have it, 300 cells with no positives are needed; to claim reliability ≥ 95% with 95% confidence requires 3 × 20 or 60 samples.

---

### The need for data harmonization – a response to boden and ozonoff [^68de19d2]. American Journal of Industrial Medicine (2010). Low credibility.

Boden and Ozonoff's undercount estimates in their recent Commentary rely on three assumptions for which no quantitative literature references are provided. Alternatively, we show that findings in both studies and published data indicate lower upper-bound estimates for the undercount range. Am. J. Ind. Med. 53:854–855, 2010. (c) 2010 Wiley-Liss, Inc.

---

### Clustering huge protein sequence sets in linear time [^7c9da3ec]. Nature Communications (2018). Medium credibility.

Metagenomic datasets contain billions of protein sequences that could greatly enhance large-scale functional annotation and structure prediction. Utilizing this enormous resource would require reducing its redundancy by similarity clustering. However, clustering hundreds of millions of sequences is impractical using current algorithms because their runtimes scale as the input set size N times the number of clusters K, which is typically of similar order as N, resulting in runtimes that increase almost quadratically with N. We developed Linclust, the first clustering algorithm whose runtime scales as N, independent of K. It can also cluster datasets several times larger than the available main memory. We cluster 1.6 billion metagenomic sequence fragments in 10h on a single server to 50% sequence identity, > 1000 times faster than has been possible before. Linclust will help to unlock the great wealth contained in metagenomic and genomic sequence databases.

---

### Protein sequence modelling with Bayesian flow networks [^57c9aab3]. Nature Communications (2025). High credibility.

In practice, it is possible to derive a continuous-time loss L ∞ where N → ∞. Under the assumption of a uniform prior θ (0), the loss becomes remarkably simple and easy to approximate through Monte Carlo integration,

This loss function reflects the transition from an N -step process to a continuous-time process, where t moves from 0 to 1. The sequence of accuracies α 1… α N is replaced with the monotonically increasing accuracy schedule, whereis the final accuracy at t = 1, and its derivative scales the loss through time,

The continuous-time loss corresponds to a negative variational lower bound, and therefore by optimising the parameters of ϕ to minimise it, we arrive at an approximation of the true distribution p (x) from which x was drawn. To train our models, we follow the general procedure outlined in ref. Specifically, the computation of the loss is described in Box 1.

Entropy encoding

In ref. the current time t is presented to the output network alongside the input distribution θ (t). During initial sampling experiments, we discovered that, during the sampling process, the entropy of the input was noticeably higher at a given time t in comparison to that observed during training. We believe that this phenomenon occurs as the input distribution θ (t) contains additional entropy from uncertainty in the output distribution. When time t is presented as an additional input to the network, this mismatch is, essentially, out of distribution for the output network, hampering performance. To resolve this, we replace the conventional Fourier encoding of time t used inwith a Fourier encoding of the entropy of each variable, appended to its corresponding input distribution before being passed into the network. This effectively makes the model invariant to t, and such an approach mirrors similar techniques used in diffusion models. In general, the use of entropy encoding is a design choice, and alternative representations of progress through the sampling process could suffice.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^7325011f]. Journal of the American College of Cardiology (2025). High credibility.

6.1. The structure of economic value statements in clinical guidelines — in the interest of consistency and comparability, "we suggest that economic value statements report 3 components: 1) Class of Recommendation; 2) Level of Certainty; and 3) Recommendation", and because interpretation occurs in the clinical context, "the economic value statement should ideally be placed immediately following the clinical recommendation for the intervention of interest".

---

### Computational solutions for omics data [^20d6890c]. Nature Reviews: Genetics (2013). Medium credibility.

High-throughput experimental technologies are generating increasingly massive and complex genomic data sets. The sheer enormity and heterogeneity of these data threaten to make the arising problems computationally infeasible. Fortunately, powerful algorithmic techniques lead to software that can answer important biomedical questions in practice. In this Review, we sample the algorithmic landscape, focusing on state-of-the-art techniques, the understanding of which will aid the bench biologist in analysing omics data. We spotlight specific examples that have facilitated and enriched analyses of sequence, transcriptomic and network data sets.

---

### Jumper enables discontinuous transcript assembly in coronaviruses [^07404016]. Nature Communications (2021). High credibility.

There are several avenues for future work. First, JUMPER currently is only applicable to data obtained using technologies that limit positional bias such as oligo(dT) amplification, which targets the poly(A) tail at the 3ʹ end of messenger RNAs. We plan to extend our current model to account for positional and sequencing biases in the data. Doing so will enable us to assemble transcriptomes from sequencing samples that used SARS-CoV-2-specific primers, which form the majority of currently available data. Second, we currently make the assumption of a fixed read length that is much smaller than the length of viral transcripts. We will relax this assumption in order to support long-read sequencing data that have variable read lengths, similar to previous methods such as Bayesemblerand Scallop-LR. Third, we plan to study the effect of mutations (including single-nucleotide variants as well as indels) on the transcriptome. Along the same lines, there is evidence of within-host diversity in COVID-19 patients –. It will be interesting to study whether this diversity translates to distinct sets of transcripts and abundances within the same host. Fourth, there are possibly multiple optimal solutions to the DTA problem that present equally likely viral transcripts with different relative abundances in the sample. A useful direction of future work is to explore the space of optimal solutions similar to the work done in ref. Finally, the approach presented in this paper can extended to the general transcript assembly problem. Although JUMPER can be used for transcript assembly of individual eukaryotic genes (see Supplementary Note C.4), it does not currently support assembly across multiple genes. The extension of the current approach can be facilitated by using the topological ordering of the nodes in a general splice graph that does not have a unique Hamiltonian path, unlike the segment graph considered in the DTA problem. We envision this will facilitate efficient use of combinatorial optimization tools such as integer linear programming to transcript assembly problems.

---

### Revised criteria for diagnosis and staging of Alzheimer's disease: Alzheimer's association workgroup [^cd3b77bb]. Alzheimer's & Dementia (2024). High credibility.

Alzheimer's disease Core 2 biomarkers and discordant profiles — Core 2 biomarkers may have many uses but most often would not be used as standalone diagnostic tests for AD, and caution should be used when an abnormal Core 2 result occurs with a normal Core 1 result because abnormal amyloid PET (or a biofluid surrogate) is nearly always a prerequisite for neocortical AD tauopathy; the A — T2+ biomarker profile is not consistent with a diagnosis of AD, is rare, may reflect values near cutpoints due to measurement variation, and from a neuropathologic perspective corresponds to primary age-related tauopathy, which is not considered to represent AD.

---

### Acute lymphoblastic leukemia, version 2.2024, NCCN clinical practice guidelines in oncology [^b4467201]. Journal of the National Comprehensive Cancer Network (2024). High credibility.

Regarding follow-up and surveillance for acute lymphoblastic leukemia - NCCN, more specifically with respect to assessment of minimal residual disease, NCCN 2024 guidelines recommend to recognize that studies in children and adults with acute lymphoblastic leukemia have demonstrated a strong correlation between the presence of minimal residual disease during remission and risk for relapse, as well as the prognostic significance of minimal residual disease measurements after induction and consolidation therapy.

---

### Bayesian genomic-enabled prediction as an inverse problem [^de34a538]. G3 (2014). Low credibility.

A Bayesian inverse regression model

Several methods have been proposed for mitigating ill-conditioned regression problems by shrinking the solution space via restricting the magnitudes of the estimates and their variance. One of the first proposals was the ridge regression estimator. Since model (3) has the form, with, the ridge regression estimator is:with Γ as a diagonal matrix of dimensions n × n with values on the diagonal. Hereis a vector of parameters that reduce ill-conditioning. If their magnitude increases excessively, this can lead to a poor fit of the model to the data. Therefore, the choice of vector γ is critical.

showed that there is a range of γ values where the mean squared error (MSE) of estimates is smaller than the corresponding MSE of the ordinary least squares solution. They minimized MSE by choosingThis requires knowing the true values of b. However, (9) can be used to justify the choicewhere, andare estimates of b, respectively, thus providing a single (global) shrinkage parameter.

In the Bayesian approach, the prior distribution reflects prior beliefs about, and its variance affects the extent to which the least squares estimate moves toward the prior mean. To construct a model along the lines of, we adopted as the conditional prior distribution ofwhere thecoefficients (given) are conditionally independent. Therefore, the joint posterior distribution of b andin model (3) is given by:where(i = 1,…, n) are variance parameters andis the prior distribution of. Usually the conjugate prior foris a scaled inverse χ 2 distribution withdegrees of freedom and scale parameter, that is, From (12), the conditional posterior distribution of b is:Expression (14) indicates that the conditional posterior mean depends on the data, whereas the variance is a fixed but unknown quantity.

The conditional expectation of b in (14) can also be expressed aswhereis known as the 'weighting factor' that weights the least squares estimate.

Because the magnitude of variance ofgrows with i, suggested applying a lower weighting factor for estimates with larger variance. The weighting factor assigns weights close to one to the first elements of, whereas the remaining elements receive lower weights, and can take on values close to zero, or zero itself.

---

### Real time machine learning prediction of next generation sequencing test results in live clinical settings [^33993a23]. NPJ Digital Medicine (2025). Medium credibility.

The SHAP (SHapley Additive exPlanations)framework was used to analyze the model features important for prediction. SHAP facilitates the interpretation of complex prediction models by summing the contributions of each feature towards predictive performance. The base value for prediction in a classifier model is the prevalence of the positive class, while the contribution of each feature is represented by the difference in prediction values (Δ) attributable to including the feature. Because models (such as the random forest models used here) may have non-linear and non-independent features, a feature's contribution may differ depending on the order in which features are added to the model. SHAP values for each feature are thus calculated by averaging the Δ value across all possible feature addition orderings.

To assess the correlation between different prediction approaches, we calculated Kendall's Tau-b Coefficientvalues between each pair-wise combination of approaches. These non-parametric tests measure rank correlation between predictions. Prediction probabilities from the machine learning model were binned into rank-ordered groups of > 90%, 70–90%, 50–70%, 30–50%, 10–30%, and < 10% to match the ordinal ranking scheme of the physician predictions. We also calculated Spearman Rank Correlation Coefficient values to verify trends seen with Kendall's, although Kendall's was the primary statistic because it is a more robustand conservativecorrelation measurement. Correlation coefficient values can range from −1 to +1, with the sign reflecting the nature of the correlation and the magnitude reflecting the strength of the correlation, where -1 and +1 reflect perfect correlation, and 0 reflects no correlation.

---

### Correction [^26cbbc41]. Journal of Radiology Case Reports (2017). Low credibility.

[This corrects the article on p. 8 in vol. 10.].

---

### How to read articles that use machine learning: users' guides to the medical literature [^f5d20a58]. JAMA (2019). Excellent credibility.

In recent years, many new clinical diagnostic tools have been developed using complicated machine learning methods. Irrespective of how a diagnostic tool is derived, it must be evaluated using a 3-step process of deriving, validating, and establishing the clinical effectiveness of the tool. Machine learning-based tools should also be assessed for the type of machine learning model used and its appropriateness for the input data type and data set size. Machine learning models also generally have additional prespecified settings called hyperparameters, which must be tuned on a data set independent of the validation set. On the validation set, the outcome against which the model is evaluated is termed the reference standard. The rigor of the reference standard must be assessed, such as against a universally accepted gold standard or expert grading.

---

### Quantification of network structural dissimilarities [^3cf663e0]. Nature Communications (2017). Medium credibility.

Identifying and quantifying dissimilarities among graphs is a fundamental and challenging problem of practical importance in many fields of science. Current methods of network comparison are limited to extract only partial information or are computationally very demanding. Here we propose an efficient and precise measure for network comparison, which is based on quantifying differences among distance probability distributions extracted from the networks. Extensive experiments on synthetic and real-world networks show that this measure returns non-zero values only when the graphs are non-isomorphic. Most importantly, the measure proposed here can identify and quantify structural topological differences that have a practical impact on the information flow through the network, such as the presence or absence of critical links that connect or disconnect connected components.

---

### Opportunities and challenges associated with clinical diagnostic genome sequencing: a report of the Association for Molecular Pathology [^ddc6fcf3]. The Journal of Molecular Diagnostics (2012). Medium credibility.

Clinical diagnostic genome sequencing — alignment workflows often use a tiered approach: sequences aligning on a subset of the read are assigned a quality score based on mismatches to the reference, nonplaced reads undergo multistep gapped alignment, and probability values are assigned across solutions with the best alignment kept and scored on the Q scale; this may require use of more than one aligner, and for clinical applications, conservative approaches are most appropriate because high confidence should exist when making a call.

---

### Factors associated with: problems of using exploratory multivariable regression to identify causal risk factors [^50164392]. BMJ Medicine (2025). High credibility.

Given these complexities, it is generally not useful to try to unpick the reasons for an apparent multivariable association related to an exposure that is not the primary exposure. Unfortunately, in "factors associated with" studies, plausible findings are often taken to be true, whereas implausible findings are ignored or dismissed, even though these findings were generated with the same method.

Multiple statistical tests: fishing expeditions, data dredging, and P-hacking

By design, "factors associated with" studies involve multiple statistical tests. Each independent variable has its own P value, which is purportedly a probability that the effect size or something greater might be observed if there was actually no real effect. When none of the variables under investigation are true risk factors and all are unrelated to each other and to the outcome, on average one in 20 will yield a P value below 0.05 — a false positive finding (type I error). The probability of obtaining a false positive finding increases with the number of statistical tests performed and depends on whether any true associations exist — which, in practice, are never known. For instance, when testing 10 independent candidate risk factors, there is roughly a 40% probability of obtaining at least one false positive finding, irrespective of sample size (figure 2, top). If the study includes one true risk factor and a sample size of 250 participants, the probability that an observed statistically significant association reflects a real effect is only about 50% (figure 2, bottom). Researchers might assume a 95% probability that a significant risk factor is real.

Figure 2
How multiple testing affects the interpretation of statistical significance. Top panel: calculated based on the formula P = 1−(0.95 ˆ n), where n is the number of variables studied and P is the probability of at least one false positive result. Bottom panel: power calculations assuming continuous independent and dependent variables and one of the independent variables affecting the dependent variable with a standardised mean difference of 0.1. With a large sample or a large effect size, or both, the positive predictive value is asymptotically 1/(1+(n−1)×0.05), where n is the number of variables. Each variable, other than the true risk factor, has a 0.05 chance of being a false positive result, whereas asymptotically, the true risk factor is always detected

---

### Structural and mechanistic basis for recognition of alternative tRNA precursor substrates by bacterial ribonuclease P [^1cf1cdb7]. Nature Communications (2022). High credibility.

Previous kinetics and HTS-Kin analysis showed that pairing of the 5′ leader with the 3′ RCCA of ptRNA acts as an anti-determinant for RNase P cleavage –. However, interference from stem-loop formation with distal 5' sequences precluded a complete quantitative description of the effects of 3' RCCA pairing. The large effect of N(−1)N(−2) identity on k cat / K m ptRNAs observed previously is illustrated by the distribution of k rel values for ptRNAs with dinucleotide sequences GG, GU, CG, and UG which had the lowest k rel while AU, AA, and AC are optimal. To examine this, we fit the 21C rate constant distribution to a quantitative sequence specificity model that includes both position weight matrix (PWM) scores and interaction terms (IC values) to quantify coupling between positions which can represent structure preferences (Supplementary Fig. 2). The PWM scores confirm the determinants observed in optimal consensus logos and show G is an anti-determinant at N(−1) to N(−4). Analysis of IC values is consistent with dinucleotides GU, GG, and UG that can all form at least 2 pairs with the 3'RCCA have large effects on k cat / K m relative to positive sequence determinants. To test the basis for these effects we compared the k rel values for subsets of ptRNA with optimal C(−4) but either A or G at N(−2). The data show that changing A to G at N(−2) results in a large decrease in k cat / K m when N(−1) is either U or G, but not C or A (Fig. 2e). When N(−4) is G then an A to G change at N(−2) results in a decrease in k cat / K m regardless of N(−1) identity (Supplementary Fig. 3). These results confirm A(−2) and C(−4) are positive determinants and reveal a threshold of two pairs between 5′ leader and 3' RCCA extending the acceptor stem decreases k cat / K m. Among the endogenous 87 ptRNA in E. coli 18 have G(−2)U(−1) and 5 have G(−2)G(−1), while 15 ptRNA have an optimal A(−2)U(−1) (Fig. 2f). Thus, the accommodation of E. coli RNase P with inhibitory pairing interaction involving nucleotides adjacent to the cleavage site is a routine part of its biological function.

---

### Preparedness and response for chikungunya virus introduction in the americas [^254022b8]. CDC (2011). Medium credibility.

Appendix C — IgM antibody capture enzyme-linked immunosorbent assay (ELISA) interpretation specifies that all patient P/N values greater than or equal to 2.0 should be reported as presumptive IgM-positive, as long as they meet the requirements listed above; if an early acute CSF or serum is negative, a convalescent serum specimen must be requested and tested before that patient is reported as negative, since without testing of a convalescent specimen a negative result may reflect testing before antibody has risen to detectable levels, with IgM detectable eight days post-onset of symptoms and persisting for at least 45 days, often for as long as 90 days; the positive P/N cut-off value of 2.0 is empirical, P/N values between 2.0 and 3.0 should be considered suspect false-positives and further tests should be performed, and the P/N value for a specimen at the screening dilution of 1:400 is not quantitative.

---

### Data storage using peptide sequences [^bf7b4c65]. Nature Communications (2021). High credibility.

Details of the two-stage sequencing method

Figure 5 shows a flowchart illustrating a method of two-stage sequencing. Four steps are involved in the two-stage sequencing method: (1) preprocessing, (2) candidate sequence generation, (3) sequence selection, and (4) candidate refining. As shown in Fig. 5, Steps 1–3 belong to the first stage (Stage 1), while Step 4 is processed in the second stage (Stage 2). In Stage 1 of the two-stage sequencing method, partial sequence is inferred using the preprocessed data after Step 1. In Stage 2, the remaining part of the sequence is determined using the raw data.

Fig. 5
A flowchart illustrating the method of two-stage sequencing.

AAC stands for amino acid combinations.

At Step 1, preprocessing is performed. At Step 2, the preprocessed data from step 1 is used to find the valid paths (sequences), and the number n of candidate sequences is counted. At Step 3, the effects of the following five factors are jointly considered when arriving at the score of a sequence candidate from Step 3.1 to Step 3.5: length of consecutive amino acids retrieved, number of amino acids retrieved, match error, average intensity of amino acids retrieved, and number of occurrences for different ion types with different offsets. The sequences with the longest length of consecutive amino acids retrieved are first selected (Step 3.1). Among the selected sequences, the sequences with the largest number of amino acids retrieved are then selected (Step 3.2). For the sequences with equal length of consecutive amino acids retrieved together with equal number of amino acids retrieved, the match error is evaluated, which is the mean error between the observed mass values for the amino acids retrieved from the experimental spectrum and the actual mass values of the amino acids normalized by the corresponding observed mass values (Step 3.3). If there is more than one sequence with identical match errors, the average intensity of amino acids retrieved is further calculated and a higher score is given to a sequence with a larger average intensity value (Step 3.4). In addition, multiple ion types are usually considered as the important factors in inferring an amino acid, which means that a mass value may correspond to different types of ions in the spectrum. Generally, the more the number of occurrences for different ion types of an amino acid is, the more likely the amino acid is correct. Therefore, for the sequences with equal score after the aforementioned evaluations of Steps 3.1–3.4, the number of occurrences for different ion types is counted to determine the sequence (Step 3.5). The mass offset sets for the N-terminal a-ion, b-ion, and c-ion type sets, i.e. {a, a-H 2 O, a-NH 3, a-NH 3 -H 2 O}, {b, b-H 2 O, b-H 2 O-H 2 O, b-NH 3, b-NH 3 -H 2 O}, and {c, c-H 2 O, c-H 2 O-H 2 O, c-NH 3, c-NH 3 -H 2 O} are {−27, −45, −44, −62}, {+1, −17, −35, −16, −34}, and {+18, 0, −18, +1, −17}, respectively. According to the fragmentation method and the property of the data, all or some of the above ion types can be used flexibly.

---

### Standards of care in diabetes – 2025 [^8296f4aa]. Diabetes Care (2025). High credibility.

Regarding screening and diagnosis for diabetes mellitus type 2, more specifically with respect to indications for screening (choice of test), ADA 2025 guidelines recommend to obtain screening for risk of prediabetes and T2DM in asymptomatic adults with an assessment of risk factors or using a validated risk calculator.

---

### Evolutionary genomics: detecting selection needs comparative data [^afbb56e5]. Nature (2005). Excellent credibility.

Positive selection at the molecular level is usually indicated by an increase in the ratio of non-synonymous to synonymous substitutions (dN/dS) in comparative data. However, Plotkin et al. describe a new method for detecting positive selection based on a single nucleotide sequence. We show here that this method is particularly sensitive to assumptions regarding the underlying mutational processes and does not provide a reliable way to identify positive selection.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^3bbad5e9]. Journal of the American College of Cardiology (2025). High credibility.

Economic evaluation — net benefit decision rule states that net health benefit and net monetary benefit should be evaluated over a range of cost-effectiveness thresholds and reported as a function of the threshold; any intervention with a net monetary benefit > 0 or net health benefit > 0 at a particular cost-effectiveness threshold would be considered cost-effective at that threshold, and when evaluating 2 or more mutually exclusive interventions the optimal strategy is the option with the highest net monetary or net health benefit.

---

### A β oligomers trigger and accelerate A β seeding [^64c748f8]. Brain Pathology (2020). Medium credibility.

In vitro experiments showed that oligomer growth follows a defined mechanism that is distinct from fibril growth and independent of the monomer concentration by adding only one monomer at a time 3, 22. Thus, the missing part of preformed oligomers will most likely be replenished by newly generated oligomers, a process that is known to be less dependent on monomer concentration but on time, finally extending the lag phase and shifting the aggregation curve to the right. The fibrils formed afterward by nucleated conformational conversion of A11‐reactive oligomers might grow rapidly due to the increased access of Aβ monomers 17. The monomers, as the fundamental building blocks for all subsequent intermediate forms of Aβ peptides in the aggregation process on the way to insoluble mature fibrils and finally to amyloid plaques, should hence be able to form the initial nucleus that is needed for seed‐induced Aβ deposition.

---

### 2021 ACC / AHA key data elements and definitions for heart failure: a report of the American college of cardiology / American Heart Association task force on clinical data standards (writing committee to develop clinical data standards for heart failure) [^50c52219]. Circulation: Cardiovascular Quality and Outcomes (2021). High credibility.

Development of terminology concepts — the committee aggregated, reviewed, harmonized, and extended terms to develop a controlled, semantically interoperable, machine-computable terminology set usable across contexts; the publication is intended to serve as a common lexicon and base infrastructure for standardization and healthcare interoperability; appendixes list each data element followed by a clinical definition, with allowed responses ("permissible values") serving as acceptable answers; for data elements with multiple permissible values, bulleted lists and multiple rows define each permissible value; where possible, clinical definitions and corresponding permissible value definitions are repeated verbatim from established sources.

---

### Colorectal cancer screening and prevention [^390b568a]. American Family Physician (2025). High credibility.

Regarding screening and diagnosis for colon cancer, more specifically with respect to indications for screening, high-risk individuals, family history, AAFP 2025 guidelines recommend to obtain CRC screening in patients with ≥ 1 first-degree relatives with CRC or adenomatous polyps, starting at 40 years of age or 10 years before the age of the youngest relative at the time of their diagnosis.

---