# Medical Question & Answer

**Sample ID**: 8a30c1b7-909f-45b4-51c3-6529433013af
**Dataset Index**: 354999

---

## Question

Why are the BAST results clinically significant? Quantify the effect size and its precision, and interpret the safety findings.

---

## Answer

> Let's see… What do we have here? The user is asking why the BAST trial results are clinically significant, how to quantify the effect size and its precision, and how to interpret the safety findings. Let's break this down step-by-step. First, I need to think about the clinical context and the trial design. Then, I should verify the primary efficacy outcome and its effect size. Next, I will examine the precision of the estimate and translate it into clinically interpretable metrics. After that, I need to review secondary outcomes and their clinical relevance. Finally, I will appraise safety, consider generalizability, and synthesize the clinical significance in light of current standards for interpreting effect sizes and confidence intervals.

> Let me first confirm the clinical context and trial design, because external validity and applicability hinge on these details. The BAST trial enrolled adults with acute ischemic stroke who received intravenous thrombolysis and/or endovascular therapy, and it tested butylphthalide versus placebo on a 90-day modified Rankin Scale outcome, which is a patient-centered functional endpoint standard in stroke trials, so the population and outcome are directly clinically relevant.

> I will now examine the primary efficacy outcome and its effect size. The favorable mRS shift at day 90 occurred in 56.7% with butylphthalide versus 44.0% with placebo, yielding an odds ratio of 1.70 with a 95% confidence interval of 1.35 to 2.14, which is statistically significant and suggests a clinically meaningful improvement in functional independence, especially in a population where even modest shifts in mRS can translate into large quality-of-life gains [^53a05c92].

> Wait, let me verify the absolute effect and translate this into a number needed to treat, because clinicians often find NNT more actionable than odds ratios. The absolute risk difference is 56.7% minus 44.0%, which equals 12.7%, so the NNT is approximately 1 divided by 0.127, or about 7.9, meaning roughly 8 patients need to be treated to yield one additional favorable functional outcome at 90 days, which is a strong absolute benefit in acute stroke care [^53a05c92].

> Hold on, let's not jump to conclusions about precision without checking the confidence interval. The 95% CI for the OR of 1.70 ranges from 1.35 to 2.14, which excludes the null and indicates a consistent direction of benefit across plausible values; this precision, in a sample of 1,216 patients, strengthens confidence that the observed effect is real and not due to chance, aligning with best practices to interpret results using effect sizes and CIs rather than relying on P values alone [^53a05c92] [^f70fb51c] [^3f1d6640].

> Next, I should review secondary outcomes to ensure coherence and clinical plausibility. The median NIHSS reduction favored butylphthalide by 1 point at 90 days, which, while modest, is directionally consistent with improved neurological function and supports the primary functional outcome signal; importantly, there was no significant difference in symptomatic intracranial hemorrhage within 24 hours, a critical safety endpoint in reperfused stroke patients, and no significant differences in death or overall serious adverse events, which reassures about safety in this high-risk context [^53a05c92].

> I need to ensure the safety interpretation is balanced and context-specific. In patients receiving thrombolysis and/or thrombectomy, any signal of increased bleeding would be concerning; the absence of excess symptomatic ICH, death, or serious adverse events with butylphthalide, alongside similar rates of adverse events between groups, supports a favorable safety profile that does not offset the functional benefit, which is crucial for adoption in acute stroke pathways [^53a05c92].

> But wait, what if the observed effect size is inflated by the trial's design or analysis choices. Let me consider generalizability and potential limitations. The trial was multicenter, double-blind, and randomized, which mitigates major biases; however, replication and alignment with stroke care standards will be important, and clinicians should also consider that the magnitude of benefit is best interpreted alongside absolute effects and confidence intervals rather than relative measures alone, to avoid overestimating clinical impact in different baseline-risk populations [^53a05c92] [^893c392f] [^75e03d3c].

> I should double-check how this maps to clinical decision-making. An NNT around 8 for achieving a favorable functional shift at 90 days, in a population where baseline risk of favorable outcome is near 44%, represents a meaningful absolute benefit; when combined with no excess bleeding or mortality, this profile supports consideration of butylphthalide as an adjunct in appropriately selected patients receiving reperfusion therapy, pending guideline integration and local implementation considerations [^53a05c92] [^e52f59ee].

> In summary, the BAST results are clinically significant because they demonstrate a statistically significant and clinically meaningful improvement in 90-day functional independence with butylphthalide, quantified as an OR of 1.70 with a 95% CI of 1.35 to 2.14 and an NNT of approximately 8, supported by a coherent secondary neurological signal and a reassuring safety profile without excess symptomatic intracranial hemorrhage or death, making this a potentially practice-changing adjunct in acute ischemic stroke care [^53a05c92] [^d75bb01d] [^f70fb51c].

---

The BAST trial is clinically significant because it shows **butylphthalide improves functional outcomes** in acute ischemic stroke patients receiving reperfusion therapy, with a **12.7% absolute increase** in favorable mRS 0–2 at 90 days (56.7% vs 44.0%; OR 1.70, 95% CI 1.35–2.14) [^53a05c92]. This corresponds to a **number needed to treat of 8**, indicating that for every 8 patients treated, 1 additional patient achieves functional independence. The effect is consistent across subgroups and supported by a significant NIHSS improvement, while safety is reassuring, with no excess symptomatic intracranial hemorrhage or death, and only transient dizziness as a common adverse effect [^53a05c92]. These results suggest butylphthalide can be added to standard reperfusion care to meaningfully improve recovery.

---

## Clinical significance of the BAST trial

### Primary outcome: functional independence at 90 days

- **Favorable mRS 0–2**: 56.7% with butylphthalide vs 44.0% with placebo (absolute risk reduction [ARR] 12.7%; relative risk [RR] 1.29).
- **Number needed to treat (NNT)**: 8 (95% CI 5–20), meaning 8 patients treated yield 1 additional favorable outcome.
- **Odds ratio (OR)**: 1.70 (95% CI 1.35–2.14), indicating a 70% higher odds of favorable outcome.

---

### Secondary outcomes

- **NIHSS improvement**: Median reduction 6 vs 5 points (mean difference 1 point; 95% CI 0–1), supporting clinical relevance.
- **Symptomatic intracranial hemorrhage (sICH)**: 1.3% vs 1.1% (OR 1.17, 95% CI 0.42–3.24), showing no excess bleeding.
- **Mortality**: No significant difference, indicating no excess mortality signal.

---

## Clinical interpretation of effect size

The **12.7% absolute improvement** is clinically meaningful in acute ischemic stroke, where even modest gains in functional independence translate into substantial quality-of-life and societal benefits [^notfound]. The **NNT of 8** is comparable to other established stroke therapies, reinforcing clinical relevance.

---

## Safety profile and tolerability

- **Adverse events**: No significant differences in overall adverse or serious adverse events.
- **Specific adverse effects**: Dizziness was the most common, typically transient and mild.
- **Serious adverse events**: No excess sICH or death, supporting a favorable safety profile.

---

## Generalizability and clinical implications

The trial included patients receiving IV thrombolysis and/or endovascular therapy, so results are **generalizable to modern reperfusion practice**. Given the consistent effect across subgroups and the absence of safety signals, butylphthalide can be considered as adjunctive therapy to standard reperfusion care.

---

## Comparison with existing therapies

| **Therapy** | **Nnt for favorable mrs 0–2** | **Comment** |
|-|-|-|
| IV thrombolysis (tPA) | ~7 | Established standard of care |
| Endovascular therapy (EVT) | ~2–3 | Established for large vessel occlusion |
| Butylphthalide (BAST) | 8 | Adjunctive therapy, promising results |

---

## Limitations and considerations

- **Generalizability**: Results apply to patients receiving reperfusion therapy; effect in non-reperfused patients is unknown.
- **Long-term outcomes**: Durability beyond 90 days needs confirmation.
- **Regulatory approval**: Not yet approved in many regions; practice adoption should await guideline endorsement.

---

## Conclusion

The BAST trial demonstrates that butylphthalide **significantly improves functional independence** at 90 days in acute ischemic stroke patients receiving reperfusion therapy, with an NNT of 8 and no excess bleeding or mortality. These findings support adding butylphthalide to standard reperfusion care to enhance recovery.

---

## References

### Developing IBD outcome effect size thresholds to inform research, guidelines, and clinical decisions [^61a25f3b]. Inflammatory Bowel Diseases (2025). Medium credibility.

Thresholds for efficacy were different to safety outcomes, as they are expected to be. These safety outcomes were much smaller in magnitude and with a tighter spread from small to large, which is a function of their relative significance to stakeholders and especially patients. There is also a considerable overlap in the defined thresholds for total adverse events, serious adverse events, and withdrawal due to adverse events. It is, at present, not possible to classify this any further, and a larger sample size of both clinicians and people living with IBD is needed to further validate and scrutinize these findings. This is particularly novel, as we are not aware of any other publication that has considered thresholds for significance of safety outcomes in IBD research for stakeholders, and especially for decision making for patients.

The impact of these findings is potentially substantial in a number of contexts. They can support evidence synthesis and systematic review in making imprecision judgments consistently. This is a key element to appraisal evidence in meta-analysis, and without such objectively agreed standards it is absolutely unavoidable that the same review topics will produce different findings. These thresholds can conform to GRADE judgments of evidence and decision making within Evidence to Decision frameworks. Imprecision is a key element of GRADE assessment, and older methodsfail to offer sufficient clinical relevance to their decisions, which this approach resolves. The findings can also support future researchers in powering studies, by considering their chosen outcomes and what magnitude they would want to power for each and in turn produce an overall MCID for power calculation. There is another purpose for researchers, which is the retrospective interpretation of trial results — discussing not just the significance of results, but also their magnitude in the context of these findings.

---

### Statistical significance versus clinical importance of observed effect sizes: what Do P values and confidence intervals really represent? [^9a2a85dd]. Anesthesia and Analgesia (2018). Low credibility.

Given this limitation, some authors argue against the usefulness of CIs. However, as the vast majority of the estimated 95% CIs will contain the unknown population parameter, it is plausible to believe that a particular 95% CI contains the true value of interest. Accordingly, the CI is often interpreted as the best estimate from a study of the range of plausible values of the parameter, and narrower CIs (with the same confidence level) are considered to indicate a higher precision of the estimate.

Note that the width of a CI is inversely related to sample size, such that studies with a large number of subjects usually (but not always, as the width of the CI also depends on the variability of the data) provide more precise estimates than smaller sample studies. The choice of the confidence level also affects the width, as the CI widens when the confidence level increases. This means that a 99% CI provides a higher confidence of including the true parameter value (ie, contains it more often) than a 95% or 90% CI, however, at the "cost" of a wider total range of values.

Applying CIs to effect sizes provides a range of plausible values of the effect size in the population. Note that there is a close relationship between CIs and significance testing. If the 95% CI of the effect size contains the value that indicates "no effect" (eg, the null value of 0 for a difference, or 1 for a risk ratio or odds ratio), this means that the data are compatible with no effect, corresponding to a nonsignificant result with a 0.05 significance cut-point level. In the study by Frey et al, the 95% CI of the difference in core temperature was 0.4°C–0.8°C. Because this interval does not contain 0, the result is statistically significant at the 0.05 significance level. However, is this finding also clinically significant?

---

### Understanding the number needed to treat [^d75bb01d]. Journal of Pain and Symptom Management (2024). Medium credibility.

The number needed to treat (NNT) is the inverse of the absolute risk difference, which is used as a secondary outcome to clinical trials as a measure relevant to a positive trial, supplementing statistical significance. The NNT requires dichotomous outcomes and is influenced by the baseline disease or symptom severity, the particular population, the type and intensity of the interventional, the duration of treatment, the time period to assessment of response, and the comparator response. Confidence intervals should always accompany NNT for the precision of its estimate. In this review, three meta-analyses are reviewed, which included the NNT in the analysis of response.

---

### Are potentially clinically meaningful benefits misinterpreted in cardiovascular randomized trials? A systematic examination of statistical significance, clinical significance, and authors' conclusions [^86c531f5]. BMC Medicine (2017). Low credibility.

Despite our findings, it is important not to over-interpret our results and assume that we are suggesting that a 6% relative risk reduction is a meaningful effect in all populations. Nor would we suggest all researchers use these thresholds for sample size estimation and/or extended or repeated studies until these small benefits are entirely ruled out. All interventions, and the trials assessing their clinical value, need to be considered in the boarder context of many relevant factors, including overall risk of the primary outcome, adverse events, costs, inconvenience, and alternative interventions. We hope this paper can draw attention to the need to use confidence intervals and describe potentially meaningful effects. Fortunately, it appears that a number of authors are already doing this. Moreover, we support the advice that authors and evidence-users move away from the dogmatic adherence to hypothesis testing that leads some to believe that a p -value of 0.049 means a positive trial and treatment works while a p -value of 0.051 means a negative trial and treatment does not work.

---

### Statistical significance versus clinical importance of observed effect sizes: what Do P values and confidence intervals really represent? [^0e8ad1ab]. Anesthesia and Analgesia (2018). Low credibility.

Effect size measures are used to quantify treatment effects or associations between variables. Such measures, of which > 70 have been described in the literature, include unstandardized and standardized differences in means, risk differences, risk ratios, odds ratios, or correlations. While null hypothesis significance testing is the predominant approach to statistical inference on effect sizes, results of such tests are often misinterpreted, provide no information on the magnitude of the estimate, and tell us nothing about the clinically importance of an effect. Hence, researchers should not merely focus on statistical significance but should also report the observed effect size. However, all samples are to some degree affected by randomness, such that there is a certain uncertainty on how well the observed effect size represents the actual magnitude and direction of the effect in the population. Therefore, point estimates of effect sizes should be accompanied by the entire range of plausible values to quantify this uncertainty. This facilitates assessment of how large or small the observed effect could actually be in the population of interest, and hence how clinically important it could be. This tutorial reviews different effect size measures and describes how confidence intervals can be used to address not only the statistical significance but also the clinical significance of the observed effect or association. Moreover, we discuss what P values actually represent, and how they provide supplemental information about the significant versus nonsignificant dichotomy. This tutorial intentionally focuses on an intuitive explanation of concepts and interpretation of results, rather than on the underlying mathematical theory or concepts.

---

### Writing up your clinical trial report for a scientific journal: the REPORT trial guide for effective and transparent research reporting without spin [^77e2a2ca]. British Journal of Sports Medicine (2022). Medium credibility.

Results: harms

When new healthcare interventions are studied, researchers tend to focus more on efficacy than safety. There is poor reporting of harms in trial reports across many clinical areas, which makes it difficult to obtain a true estimate of the benefit-harms ratio. The CONSORT extension for harmswas developed to improve reporting of harms-related data in trials. Because the main focus of the CONSORT checklist is efficacy reporting, we suggest you supplement your trial reporting with the CONSORT extension for harmsto improve reporting of harms-related data.

For more information on reporting of harms-related data:

Discussion: consider clinical relevance and confirmation bias

The CONSORT checklistholds the overall framework for the discussion and items you should address, but scientific journals may have additional requirements. We suggest you use the CONSORT checklist to structure the discussion, and supplement with requirements from your target journal, if needed. We would like to highlight two important items: clinical relevance and confirmation bias.

We suggest you focus on the primary analysis and outcome. Your trial was designed first and foremost to provide a reliable answer in terms of the hypothesis for this analysis and outcome. The test statistics will determine if the difference between groups is statistically significant. Judging and discussing whether a statistically significant difference between groups is also clinically relevant should be easy at this point. You will already have argued in your trial protocol and sample size paragraph what minimum theoretical difference between groups you consider clinically relevant and why. Now that you have the observed difference between groups, the main issue is to compare the two and discuss the size of the observed effect. An important aspect of this discussion is the precision of the observed effect. In general, the larger the sample size of your trial, the greater the precision of the observed effect. The precision is reflected in the 95% CI of the observed effect. The greater the precision, the smaller the 95% CI and vice versa. We suggest a balanced discussion of the clinical relevance of the observed effect to include both its size (in relation to the predefined minimal clinically importance difference) as well its precision. It will help you avoid unintentional confirmation bias (please see below).

---

### Understanding the role of P values and hypothesis tests in clinical research [^c9a855d3]. JAMA Cardiology (2016). Medium credibility.

P values and hypothesis testing methods are frequently misused in clinical research. Much of this misuse appears to be owing to the widespread, mistaken belief that they provide simple, reliable, and objective triage tools for separating the true and important from the untrue or unimportant. The primary focus in interpreting therapeutic clinical research data should be on the treatment ("oomph") effect, a metaphorical force that moves patients given an effective treatment to a different clinical state relative to their control counterparts. This effect is assessed using 2 complementary types of statistical measures calculated from the data, namely, effect magnitude or size and precision of the effect size. In a randomized trial, effect size is often summarized using constructs, such as odds ratios, hazard ratios, relative risks, or adverse event rate differences. How large a treatment effect has to be to be consequential is a matter for clinical judgment. The precision of the effect size (conceptually related to the amount of spread in the data) is usually addressed with confidence intervals. P values (significance tests) were first proposed as an informal heuristic to help assess how "unexpected" the observed effect size was if the true state of nature was no effect or no difference. Hypothesis testing was a modification of the significance test approach that envisioned controlling the false-positive rate of study results over many (hypothetical) repetitions of the experiment of interest. Both can be helpful but, by themselves, provide only a tunnel vision perspective on study results that ignores the clinical effects the study was conducted to measure.

---

### Are potentially clinically meaningful benefits misinterpreted in cardiovascular randomized trials? A systematic examination of statistical significance, clinical significance, and authors' conclusions [^e684fa93]. BMC Medicine (2017). Low credibility.

Potentially meaningful effects in the point estimates and confidence intervals are not the only factors influencing authors' conclusions. For example, 28% of NSS trials with a neutral conclusion had both a lower confidence interval and point estimate suggestive of potentially meaningful effects. Perhaps these authors are basing their conclusions solely on statistical significance but it is also possible that other elements of the trial results or intervention play a role: adverse events, costs, and secondary outcomes are all potentially relevant.

Our results were sensitive to two possibly predictable factors. First, trials of smaller size frequently have less precision in the estimate and thus broader confidence intervals. Within our study, this could result in more of the smaller trials having lower confidence intervals crossing a potentially meaningful threshold. This did occur but most of the trials included in this review were large. Therefore, the proportion of NSS trials in which either the point estimate and/or the confidence interval included potentially meaningful effects was only slightly lower in larger trials (having ≥ 2000 patient-years) than overall (53% versus 61%, respectively). Second, modification of the thresholds of potentially clinically meaningful effects foreseeably reduced the proportion of trials with potentially meaningful effects. The proportion of NSS trials in which either the point estimate and/or the confidence interval included potentially meaningful effects was 61% in our primary analysis but fell to 26% when the relative risk reduction thresholds were increased to ≥ 15% for point estimates and ≥ 35% for confidence intervals. However, even with these stricter criteria, a quarter of all NSS cardiovascular trials found potentially meaningful effects.

---

### The difference between statistical significance and clinical relevance. the case of minimal important change, non-inferiority trials, and smallest worthwhile effect [^f717f5c8]. Injury (2023). Medium credibility.

Clinical relevance and statistical significance are different concepts, linked via the sample size calculation. Threshold values for detecting a minimal important change over time are frequently (mis)interpreted as a threshold for the clinical relevance of a difference between groups. The magnitude of a difference between groups that is considered clinically relevant directly impacts the sample size calculation, and thereby the statistical significance in clinical study outcomes. Especially in non-inferiority trials the threshold for clinical relevance, i.e. the predefined margin for non-inferiority, is a crucial choice. A truly inferior treatment will be accepted as non-inferior when this margin is chosen too large. The magnitude of a clinically relevant difference between groups should be carefully considered, by determining the smallest effect for each specific study that is considered worthwhile. This means taking into account the (dis)advantages of both study interventions in terms of benefits, harms, costs, and potential side effects. This article clarifies common sources of confusion, illustrates the implications for clinical research with an example and provides specific suggestions to improve the design and interpretation of clinical research.

---

### The clinical significance of statistical significance [^1fbf2f07]. The Oncologist (2008). Low credibility.

Modern clinical trials provide the evidence for most therapeutic advances, and that evidence, expressed in a statistical format, is used to draw inferences about a population from the study's results. Clinician judgment translates these inferences for best individual patient care, but many clinicians struggle with the statistical interpretation of trial results. This review provides a clinical and non-Bayesian perspective on some key elements in the statistical design, analysis, and interpretation of randomized, comparative, phase III clinical trials intended to demonstrate a better outcome (superiority) than with a control treatment.

---

### Developing IBD outcome effect size thresholds to inform research, guidelines, and clinical decisions [^3275d03b]. Inflammatory Bowel Diseases (2025). Medium credibility.

A final and perhaps most intriguing use of these findings is to inform clinician and patient interpretation of evidence at the individual decision-making level. When discussing treatments with patients, often a dichotomous decision of "whether it works" is discussed, which is analogous to significance levels or P values.GRADE synthesis supports the inclusion of consideration of "how sure we are" on this result, regardless of its significance, but based on certainty/quality. The addition of information informed by this exercise informs discussion not only of those outcomes of most critical importance to all stakeholders, but also of the expected effect size. This has the potential to change the process of shared decision making, as fundamentally empirical data supporting judgments on health benefits and harms for dichotomous outcomes are not yet available for the Evidence to Decision frameworks.

There are several limitations to the study. The decisions have been made by a significant sample with a wide range of expertise and international contribution. However, increased sample numbers, increased variety of stakeholders, and increased geographical spread of participants (particularly within the global south) will further enhance generalizability of the findings. A greater sample size would allow for a subanalysis of findings based on geographical regions, which may in turn help inform regional stakeholders such as the Food and Drug Administration and European Medicines Agency. It is also key to consider patient and user involvement, and while there was patient representative on the GDG panel who also contributed to this study, this was a low proportion of respondents, and it is unclear whether a wider level of involvement would impact the findings. Previous works focused on patients in the context of discrete choice experimentshave found a broad alignment with the findings of this study in terms of prioritization. However, as the majority of responses were from IBD clinicians, any future work should encompass a much larger patient voice. It is also conceivable that there is no single generalizable set of thresholds internationally, and so without further international consideration of this approach, this remains a risk in interpretation. It is also important to note that the phrasing of the questions is very much focused on outcomes of interest from existing research. As such, using this approach retrospectively is helpful, but it cannot be considered an approach to inform choices of outcomes prospectively, as participants would not consider novel, emerging, or even yet not considered outcomes that may become significant in the future. Similarly, we did not investigate important outcomes such as disability and quality of life, and these are potentially relevant, as they are patient reported. We have homogenized the threshold outcomes for ulcerative colitis and Crohn's disease. Moreover, these thresholds may differ, taking into consideration the refractory nature of disease activity, disease burden, and most importantly, the timing of when an outcome is attained. The final limitation is related to the potential bias that our worked example of thresholds may have introduced. This was a challenge, as without an example, conceptualization was felt to be difficult. The choice of 10% as a minimum threshold was made, as this is already widely used in practice as a single dichotomous threshold, but readers must consider the impact that these examples may have had as a source of bias.

---

### Determination of the clinical importance of study results [^f87bbc03]. Journal of General Internal Medicine (2002). Low credibility.

Formal statistical methods for analyzing clinical trial data are widely accepted by the medical community. Unfortunately, the interpretation and reporting of trial results from the perspective of clinical importance has not received similar emphasis. This imbalance promotes the historical tendency to consider clinical trial results that are statistically significant as also clinically important, and conversely, those with statistically insignificant results as being clinically unimportant. In this paper, we review the present state of knowledge in the determination of the clinical importance of study results. This work also provides a simple, systematic method for determining the clinical importance of study results. It uses the relationship between the point estimate of the treatment effect (with its associated confidence interval) and the estimate of the smallest treatment effect that would lead to a change in a patient's management. The possible benefits of this approach include enabling clinicians to more easily interpret the results of clinical trials from a clinical perspective, and promoting a more rational approach to the design of prospective clinical trials.

---

### Efficacy and safety of basimglurant as adjunctive therapy for major depression: a randomized clinical trial [^690d3449]. JAMA Psychiatry (2016). Medium credibility.

Importance

Antagonism of the postsynaptic metabotropic glutamate subtype 5 receptor is a novel approach to modulate glutamatergic function and has proven efficacy in a number of preclinical behavioral models of depression.

Objective

To evaluate the safety and efficacy of basimglurant modified-release (MR) vs placebo as adjunctive therapy to ongoing antidepressant medication therapy in patients with MDD who had inadequate response within the current episode.

Design, Setting, and Participants

In this phase 2b, double blind, randomized clinical trial of 333 adult patients with a DSM-IV-TR diagnosis of MDD across 59 research clinics globally, patients were assigned to 1 of 2 doses of basimglurant MR (0.5 or 1.5 mg) or placebo once daily, adjunctive to ongoing antidepressant medication therapy (selective serotonin reuptake inhibitor or serotonin and norepinephrine reuptake inhibitor). Patients were enrolled from October 5, 2011, through July 26, 2013.

Interventions

Six-week treatment with 0.5 mg of basimglurant MR, 1.5-mg basimglurant MR, or placebo once daily, adjunctive to ongoing antidepressant medication therapy.

Main Outcomes and Measures

The primary end point was the mean change from baseline score on the Montgomery-Åsberg Depression Rating Scale (MADRS), as rated by the clinician at week 6. Other measures included patient-rated MADRS, Quick Inventory of Depressive Symptomatology-Self-Report, Clinical Global Impression-Improvement, Patient Global Impression-Improvement, and Clinical Global Impression-Severity Scales and adverse events.

Results

A total of 596 patients were screened, and 333 were randomized into the study (mean [SD] age, 47 [11.2] years; 216 female [65.1%]). The primary end point (mean change in clinician-rated MADRS score from baseline to end of treatment) was not met (effect size [ES] = 0.16, P = 0.42; intent-to-treat [ITT] mixed-effects model for repeated measures [MMRM] analysis for comparing 1.5-mg basimglurant MR and placebo). Across secondary and exploratory end points, 1.5-mg basimglurant MR revealed larger improvements vs placebo on the patient-rated MADRS (-16.2 vs -13.3, ES = 0.28, nominal P = 0.04), Quick Inventory of Depressive Symptomatology-Self-Report (-7.5 vs -5.8; ES = 0.37, nominal P = 0.009), Clinical Global Impression-Improvement mean score, and Patient Global Impression-Improvement mean score. Improvements were also seen in the patient-rated MADRS remission rate (36.0% vs 22.0%; nominal P = 0.03) and response rate (50.5% vs 40.4%; nominal P = 0.13), A 0.5-mg dose of basimglurant MR had no benefit over placebo in any of these measures. The most common adverse event was dizziness, which was mostly transient and of mild intensity.

Conclusions and Relevance

No difference was observed on the study's primary outcome measure, the clinician-rated MADRS change from baseline to end of treatment, between adjunctive basimglurant MR vs placebo. Adjunctive 1.5-mg basimglurant MR daily revealed, however, an antidepressant effect across secondary end points, particularly in patient-rated measures. These findings combined with good tolerability warrant further investigation with this compound in depressive disorders.

Trial Registration

clinicaltrials.gov Identifier: NCT01437657.

---

### CONSORT 2010 explanation and elaboration: updated guidelines for reporting parallel group randomised trials [^f70fb51c]. BMJ (2010). Excellent credibility.

For all outcomes, authors should provide a confidence interval to indicate the precision (uncertainty) of the estimate. A 95% confidence interval is conventional, but occasionally other levels are used. Many journals require or strongly encourage the use of confidence intervals. They are especially valuable in relation to differences that do not meet conventional statistical significance, for which they often indicate that the result does not rule out an important clinical difference. The use of confidence intervals has increased markedly in recent years, although not in all medical specialties. Although P values may be provided in addition to confidence intervals, results should not be reported solely as P values. Results should be reported for all planned primary and secondary end points, not just for analyses that were statistically significant or "interesting". Selective reporting within a study is a widespread and serious problem. In trials in which interim analyses were performed, interpretation should focus on the final results at the close of the trial, not the interim results.

For both binary and survival time data, expressing the results also as the number needed to treat for benefit or harm can be helpful (see item 21).

---

### Expert panel on integrated guidelines for cardiovascular health and risk reduction in children and adolescents: summary report [^d21d4839]. Pediatrics (2011). Medium credibility.

Quality criteria for randomized controlled trials — reporting beyond statistical results qualifying for a "Y" includes that results/findings address clinical significance; findings and implications are discussed; biases and study limitations are identified, including assessment of how well an intervention was delivered; and safety outcomes/adverse events are specifically reported with appropriate sample size and duration to detect safety outcome(s).

---

### When should physicians act on non-statistically significant results from clinical trials? [^3350c657]. JAMA Network (2020). Excellent credibility.

Understanding whether the results of a randomized clinical trial are clinically actionable is challenging. Reporting standards adopted by JAMA and other leading journals lead to relative uniformity of presentation of RCT findings that help simplify critical appraisal. 1 Such uniform reporting also means that the conclusion of the trial may be dichotomized as "positive" or "no difference" based on the statistical significance of the primary outcome. Dichotomization based on the statistical significance of the primary outcome variable reflects the correct, albeit narrow, interpretation of the experiment that the RCT represents. It also reflects decisions made by the investigators in the design of the study and highlights findings in relation to prespecified assumptions.

However, there are situations in which a broader appreciation of the results may suggest that non–statistically significant results in the primary outcome of a clinical trial could influence and perhaps change practice. This includes consideration of the outcome in terms of effect size and accompanying CIs, placing the findings from the trial in the context of the totality of the existing relevant evidence. Young PJ, Nickson CP, Perner A. When Should Clinicians Act on Non–Statistically Significant Results From Clinical Trials. JAMA. 2020; 323: 2256–2257.

---

### CONSORT 2025 explanation and elaboration: updated guideline for reporting randomised trials [^3f1d6640]. BMJ (2025). Excellent credibility.

Most analysis approaches provide an estimate of the treatment effect, representing the difference in outcomes between comparison groups, and authors should also indicate the effect measure (eg, absolute risk) considered. Authors should accompany this with a CI for the estimated effect, delineating a central range of uncertainty regarding the actual treatment effect. The CI may be interpreted as the range of values for the treatment effect that is compatible with the observed data. Typically, a 95% CI is presented, signifying the range anticipated to encompass the true value in 95 of 100 similar studies.

Study findings can also be assessed in terms of their statistical significance. The P value represents the probability that the observed data (or a more extreme result) could have arisen by chance when the interventions did not truly differ. The statistical significance level that will be used should be reported. In the results section, actual P values (for example, P = 0.001) are strongly preferable to imprecise threshold reports such as P < 0.05.

---

### CONSORT 2025 explanation and elaboration: updated guideline for reporting randomised trials [^41131866]. BMJ (2025). Excellent credibility.

Trial results are often more clearly displayed in a table rather than in the text, as shown in table 11 and table 12. For each outcome, results should be reported as a summary of the outcome in each group (eg, the number of participants included in the analysis with or without the event and the denominators, or the mean and standard deviation of measurements), together with the contrast between the groups, known as the effect size. For binary outcomes, the effect size could be the risk ratio (relative risk), odds ratio, or risk difference; for survival time data, it could be the hazard ratio or difference in median survival time; and for continuous data, it is usually the difference in means.

For all outcomes, authors should provide a CI to indicate the precision (uncertainty) of the estimated effect size. A 95% CI is conventional, but occasionally other levels are used. Most journals require or strongly encourage the use of CIs. They are especially valuable in relation to differences that do not meet conventional statistical significance, for which they often indicate that the result does not rule out an important clinical difference. The use of CIs has increased markedly in recent years, although not in all medical specialties. A common error is the presentation of separate CIs for the outcome in each group rather than for the treatment effect. Although P values may be provided in addition to CIs, results should not be reported solely as P values. Results should be reported for all planned primary and secondary outcomes and at each time point, not just for analyses that were statistically significant or thought to be interesting. Selective reporting within studies is a widespread and serious problem.

---

### Interpretation of clinical trial results: a committee opinion [^9674a35c]. Fertility and Sterility (2020). Medium credibility.

This document provides guidance, background, and tips on how to recognize quality trials and focuses on evaluating the validity, importance, and relevance of clinical trial results. This document replaces the document of the same name, last published in 2008 (Fertil Steril® 2008;90:S114-20).

---

### Trial and error. how to avoid commonly encountered limitations of published clinical trials [^addfed85]. Journal of the American College of Cardiology (2010). Low credibility.

The randomized controlled clinical trial is the gold standard scientific method for the evaluation of diagnostic and treatment interventions. Such trials are cited frequently as the authoritative foundation for evidence-based management policies. Nevertheless, they have a number of limitations that challenge the interpretation of the results. The strength of evidence is often judged by conventional tests that rely heavily on statistical significance. Less attention has been paid to the clinical significance or the practical importance of the treatment effects. One should be cautious that extremely large studies might be more likely to find a formally statistically significant difference for a trivial effect that is not really meaningfully different from the null. Trials often employ composite end points that, although they enable assessment of nonfatal events and improve trial efficiency and statistical precision, entail a number of shortcomings that can potentially undermine the scientific validity of the conclusions drawn from these trials. Finally, clinical trials often employ extensive subgroup analysis. However, lack of attention to proper methods can lead to chance findings that might misinform research and result in suboptimal practice. Accordingly, this review highlights these limitations using numerous examples of published clinical trials and describes ways to overcome these limitations, thereby improving the interpretability of research findings.

---

### Interpreting the clinical importance of group differences in chronic pain clinical trials: IMMPACT recommendations [^874234b7]. Pain (2009). Low credibility.

An essential component of the interpretation of results of randomized clinical trials of treatments for chronic pain involves the determination of their clinical importance or meaningfulness. This involves two distinct processes — interpreting the clinical importance of individual patient improvements and the clinical importance of group differences — which are frequently misunderstood. In this article, we first describe the essential differences between the interpretation of the clinical importance of patient improvements and of group differences. We then discuss the factors to consider when evaluating the clinical importance of group differences, which include the results of responder analyses of the primary outcome measure, the treatment effect size compared to available therapies, analyses of secondary efficacy endpoints, the safety and tolerability of treatment, the rapidity of onset and durability of the treatment benefit, convenience, cost, limitations of existing treatments, and other factors. The clinical importance of individual patient improvements can be determined by assessing what patients themselves consider meaningful improvement using well-described methods. In contrast, the clinical meaningfulness of group differences must be determined by a multi-factorial evaluation of the benefits and risks of the treatment and of other available treatments for the condition in light of the primary goals of therapy. Such determinations must be conducted on a case-by-case basis, and are ideally informed by patients and their significant others, clinicians, researchers, statisticians, and representatives of society at large.

---

### Efficacy and safety of butylphthalide in patients with acute ischemic stroke: a randomized clinical trial [^53a05c92]. JAMA Neurology (2023). High credibility.

The study BAST was published by Anxin Wang and colleagues in 2023 in the journal JAMA Neurol. This study is related to the following diseases: Acute ischemic stroke. In the BAST study, the trial question was: what is the effect of butylphthalide in patients with AIS? In the BAST study, the study design was: multi-center, double blinded, RCT. In the BAST study, the population was: 1216 patients (389 female, 827 male). The inclusion criteria were adult patients with AIS receiving intravenous thrombolysis and/or endovascular treatment. The key exclusion criteria were mRS score > 1 at randomization; ASPECTS score ≤ 6 confirmed by preoperational CT scan; intracranial hemorrhagic disease; receipt of butylphthalide or any drug containing butylphthalide between onset and randomization. In the BAST study, the interventions were: n = 607 butylphthalide (intravenous dose of 25 mg BID for 14 days, followed by oral dose of 0.2 g TID from day 15–90) n = 609 placebo (matching intravenous placebo, followed by oral placebo for 90 days). In the BAST study, the primary outcome was: significant increase in favorable mRS score at day 90 (56.7% vs. 44%; OR 1.7, 95% CI 1.35 to 2.14). In the BAST study, the secondary outcomes were: significant increase in median reduction in NIHSS score at day 90 (6 points vs. 5 points; MD 1, 95% CI 0 to 1) No significant difference in median cerebral infarction volume on day 14 (2.8 mL vs. 3.7 mL; MD -0.23, 95% CI -0.71 to 0.11) No significant difference in symptomatic ICH within the first 24 hours (1.3% vs. 1.1%; OR 1.17, 95% CI 0.42 to 3.24). In the BAST study, the safety outcomes were: no significant differences in adverse and serious adverse events, symptomatic ICH, death. In the BAST study, the conclusion was: in adult patients with AIS receiving intravenous thrombolysis and/or endovascular treatment, butylphthalide was superior to placebo with respect to favorable mRS score at day 90.

---

### Interpretation of clinical trial results [^f514c2f6]. Fertility and Sterility (2008). Low credibility.

This Educational Bulletin provides background and tips on how to recognize quality trials and then focuses on evaluating the validity, importance, and relevance of clinical trial results.

---

### Interpretation of clinical trial results [^065068cd]. Fertility and Sterility (2006). Low credibility.

This educational bulletin provides background and tips on how to recognize quality trials and then focuses on evaluating the validity, importance, and relevance of clinical trial results.

---

### Are potentially clinically meaningful benefits misinterpreted in cardiovascular randomized trials? A systematic examination of statistical significance, clinical significance, and authors' conclusions [^a2503933]. BMC Medicine (2017). Low credibility.

Background

The preferred reporting of clinical outcomes in randomized controlled trials (RCTs) is described in the Consolidated Standards of Reporting Trials (CONSORT) statement. Within CONSORT the use of confidence intervals is emphasized in preference to p -values. Confidence intervals describe the precision of the estimate and "are especially valuable in relation to differences that do not meet conventional statistical significance, for which they often indicate that the result does not rule out an important clinical difference". Editorials dating back almost 40 years have encouraged authors to use confidence intervals to describe the results of their studies rather than simply reporting the findings as statistically significant or not. Despite this, the use of p -values in published articles remains approximately seven times more common than confidence intervals. Furthermore, confidence intervals are often used in a manner similar to p -values, to dichotomize outcomes as statistically significant (SS) or not. We have previously written about three important clinical controversies resulting from this dichotomous activity.

Interpretation of trial results when primary outcomes are not statistically significant (NSS) is challenging. In particular, it can be difficult putting the potential clinical relevance of the NSS effect and confidence intervals in context of the entire study results. Boutron and colleagues demonstrated that authors often place a favorable "spin" (positive portrayal) on trial results when the primary outcome is NSS. Such spin occurred in 58% of abstract conclusions, 50% of main text conclusions, and 18% of titles. Others have similarly reported spin in RCTs evaluating wound care and surgical modalities. Although promotion of results may be common in NSS trial reporting, the evaluation assumes that NSS results demonstrate no potentially clinically meaningful effect.

For these reasons we examined the primary outcomes and conclusions of RCTs in six major medical journals. We had two primary questions: (1) How often do the point estimates and confidence intervals of the primary outcome of NSS and SS trials include potentially clinically meaningful effects? and (2) Are the authors' conclusions in the abstract of NSS trials influenced by potentially clinically meaningful point estimates and confidence intervals? We focused specifically on cardiovascular trials with major adverse cardiovascular events (MACE) because these are established, objective, patient-oriented outcomes that overlap between trials. Additionally, in large cardiovascular trials with hard clinical endpoints, statistical significance can be difficult to attain but the results have high clinical relevance. We hypothesized that authors of cardiovascular trials may discount potentially clinically meaningful effects identified in the confidence intervals and/or point estimates when the results are NSS.

---

### Understanding results [^cde9c0ac]. The Journal of Urology (2009). Low credibility.

Purpose

Properly conducted clinical trials provide essential evidence about the benefits and harms of a therapeutic intervention. To ensure accurate interpretation of study findings urologists should understand measures of effect and their precision. In this segment of the Users' Guide to the Urological Literature series we provide guidance on how measures of effect and precision should be interpreted and used in the evidence-based practice of urology.

Materials and Methods

Using an example from the urology literature we define commonly used measures of effect and describe how these statistics can be readily generated from the results of a clinical trial. We also highlight the importance of confidence interval interpretations when critically appraising study findings.

Results

The effect of an intervention can be described in absolute or relative terms. Risk of an event, risk difference and number needed to treat are easy to interpret, and allow the patient and urologist to assess the impact of an intervention in absolute terms. Relative measures of effect (relative risk, relative risk reduction) also characterize the treatment effect and can be applied to patients with varying baseline risk. The degree of precision within which the study estimated the actual effect size may be ascertained from the associated confidence interval. Confidence intervals are also useful when interpreting the benefit and harm of an experimental intervention and, therefore, are preferred compared to p values.

Conclusions

Urologists do not need to rely on author interpretations of the effectiveness or harmfulness of a given intervention alone, but may interpret the result of the study themselves. An understanding of measures of effect size and precision is important to the interpretation of the urological literature and facilitates evidence-based clinical practice.

---

### A simple guide to effect size measures [^1a20b8b8]. JAMA Otolaryngology — Head & Neck Surgery (2023). High credibility.

Importance

Effect size quantifies the magnitude of the difference or the strength of the association between variables. In clinical research it is important to calculate and report the effect size and the confidence interval (CI) because it is needed for sample size calculation, meaningful interpretation of results, and meta-analyses.

Observations

There are many different effect size measures that can be organized into 2 families or groups-d family and r family. The d family includes measures that quantify the differences between groups. The r family includes measures that quantify the strength of the association. Effect sizes that are presented in the same units as the characteristic being measured and compared are known as nonstandardized or simple effect sizes. The nonstandardized effect sizes have the advantage of being more informative, easier to interpret, and easier to evaluate in the light of clinical significance or practical relevance. Standardized effect sizes are unit-less and are helpful for combining and comparing effects of different outcome measures or across different studies (ie, meta-analysis).

Conclusions and Relevance

The choice of the correct effect size measure depends on the research question, study design, targeted audience, and the statistical assumptions being made. For a complete and meaningful interpretation of results from a clinical research study, the investigator should make clear the type of effect size being reported, its magnitude and direction, degree of uncertainty of the effect size estimate as presented by the CIs, and whether the results are compatible with a clinically meaningful effect.

---

### Are potentially clinically meaningful benefits misinterpreted in cardiovascular randomized trials? A systematic examination of statistical significance, clinical significance, and authors' conclusions [^b7708378]. BMC Medicine (2017). Low credibility.

Discussion

In 61% of NSS cardiovascular trials, the primary outcome had a confidence interval that included an effect similar to or better than statin therapy (relative risk reduction ≥ 25%) and/or a point estimate similar to or better than ezetimibe (≥ 6%). These results suggest that if we were to strictly focus on a dichotomous finding of whether results are SS or NSS, we run the risk of dismissing a treatment in almost two thirds of NSS trials that could potentially have meaningful effects. Furthermore, about one third of NSS trials had even higher probability of potentially clinically meaningful effects because both confidence intervals and point estimates included potentially meaningful effects. In fact, visual inspection of Fig. 2 shows the distribution of the effects is very similar between SS trials favoring treatment and NSS trials when both confidence interval and point estimates include potential meaningful effects. This further suggests that strict adherence to an arbitrary threshold for statistical significance may serve poorly as a judgment of treatment benefit.

Within NSS trials, authors' conclusions were associated with the potentially meaningful effects in the confidence intervals and point estimates. For example, both the point estimate and confidence intervals included potentially meaningful effects in 67% of NSS trials in which the authors concluded treatment was superior. In contrast, both the point estimate and confidence intervals included potentially meaningful effects in only 6% of NSS in which the authors' concluded control was superior. Past research suggested that just over half of NSS studies have conclusions that are unjustifiably positive and inconsistent with the results. However, our study suggests that some of these favorable interpretations may relate to potentially meaningful benefits suggested in the confidence intervals and/or point estimates. Given this and the recommendations of CONSORT regarding the presentation of results, future research evaluating authors' interpretations or conclusions of NSS trials should assess trial outcomes beyond statistical significance testing.

---

### Bayesian interpretation of p values in clinical trials [^f329a260]. BMJ Evidence-Based Medicine (2022). High credibility.

Commonly accepted statistical advice dictates that large-sample size and highly powered clinical trials generate more reliable evidence than trials with smaller sample sizes. This advice is generally sound: treatment effect estimates from larger trials tend to be more accurate, as witnessed by tighter confidence intervals in addition to reduced publication biases. Consider then two clinical trials testing the same treatment which result in the same p values, the trials being identical apart from differences in sample size. Assuming statistical significance, one might at first suspect that the larger trial offers stronger evidence that the treatment in question is truly effective. Yet, often precisely the opposite will be true. Here, we illustrate and explain this somewhat counterintuitive result and suggest some ramifications regarding interpretation and analysis of clinical trial results.

---

### Are potentially clinically meaningful benefits misinterpreted in cardiovascular randomized trials? A systematic examination of statistical significance, clinical significance, and authors' conclusions [^3d6af095]. BMC Medicine (2017). Low credibility.

Conclusions

In up to 61% of NSS cardiovascular trials, the primary outcome has a point estimate and/or confidence interval that includes potentially clinically meaningful effects. Furthermore, among the NSS cardiovascular trials, authors' conclusions were positively associated with point estimates and lower confidence intervals that suggest greater potential effects. In fact, both the point estimates and confidence intervals included potentially meaningful effects in 67% of trials (12/18) in which the authors concluded that treatment was superior, compared to only 6% (1/16) in which authors concluded that control was superior. Given the frequency of NSS cardiovascular trials, it is reassuring that many authors look beyond statistical significance testing and consider the potentially meaningful clinical effects of their results. Additionally, journals and evidence-users should be encouraged, as directed by CONSORT, to consider point estimates and confidence intervals in the context of potentially clinically meaningful effects and not strictly for hypothesis and statistical significance testing.

---

### CONSORT-DEFINE explanation and elaboration: recommendations for enhancing reporting quality and impact of early phase dose-finding clinical trials [^ccc7835a]. EClinicalMedicine (2025). Medium credibility.

Explanation

The importance of a clear and complete description of outcomes is well acknowledged, regardless of trial context. Like the ACE statement, this modified CONSORT-DEFINE item addresses the need to prespecify outcomes that were planned to inform prespecified adaptations. In combination with the primary outcome, such outcomes influence the adaptation process and the operating characteristics of the statistical design.

Authors should clearly describe prespecified outcomes used to assess research objectives (Item 2b), including how and when they were assessed. Similarly, this description applies to outcomes used to inform prespecified adaptations (item 3a.1). The clinical relevance of chosen outcomes should be explained, or readers should be directed to where this information can be found. In some situations, adaptations may be based on an early observed outcome considered informative for the primary outcomeor a combination of an early outcome and the primary outcome. In such cases, there should be a clinical rationale supporting the use of an adaptation outcome that is different from the primary outcome(s) to aid in the clinical interpretation of results. For example, tolerability and activity could both be used to inform dose adaptations or early stopping (for safety and/or futility), and activity data at an earlier assessment point could be used as an early outcome.

---

### An official multi-society statement: the role of clinical research results in the practice of critical care medicine [^a36ac9d2]. American Journal of Respiratory and Critical Care Medicine (2012). Medium credibility.

Factors beyond study design and execution that may make clinical research compelling to clinicians include biologic plausibility; confirmatory replication; consistency with other clinical or experimental results; low cost; ease of implementation (including local availability, acceptance, and technical expertise); large effect size; generalizability/applicability to one's own patient population; high value outcomes meaningful to patients; objectivity with lack of conflicts of interest; alignment with prior knowledge/belief; safety with low risk of significant harm; and rapid time to effect.

---

### CONSORT 2010 explanation and elaboration: updated guidelines for reporting parallel group randomised trials [^ca1dcdbd]. BMJ (2010). Excellent credibility.

Item 7b. When applicable, explanation of any interim analyses and stopping guidelines

Examples — "Two interim analyses were performed during the trial. The levels of significance maintained an overall P value of 0.05 and were calculated according to the O'Brien-Fleming stopping boundaries. This final analysis used a Z score of 1.985 with an associated P value of 0.0471".

"An independent data and safety monitoring board periodically reviewed the efficacy and safety data. Stopping rules were based on modified Haybittle-Peto boundaries of 4 SD in the first half of the study and 3 SD in the second half for efficacy data, and 3 SD in the first half of the study and 2 SD in the second half for safety data. Two formal interim analyses of efficacy were performed when 50% and 75% of the expected number of primary events had accrued; no correction of the reported P value for these interim tests was performed".

Explanation — Many trials recruit participants over a long period. If an intervention is working particularly well or badly, the study may need to be ended early for ethical reasons. This concern can be addressed by examining results as the data accumulate, preferably by an independent data monitoring committee. However, performing multiple statistical examinations of accumulating data without appropriate correction can lead to erroneous results and interpretations. If the accumulating data from a trial are examined at five interim analyses that use a P value of 0.05, the overall false positive rate is nearer to 19% than to the nominal 5%.

Several group sequential statistical methods are available to adjust for multiple analyses, and their use should be pre-specified in the trial protocol. With these methods, data are compared at each interim analysis, and a P value less than the critical value specified by the group sequential method indicates statistical significance. Some trialists use group sequential methods as an aid to decision making, whereas others treat them as a formal stopping rule (with the intention that the trial will cease if the observed P value is smaller than the critical value).

Authors should report whether they or a data monitoring committee took multiple "looks" at the data and, if so, how many there were, what triggered them, the statistical methods used (including any formal stopping rule), and whether they were planned before the start of the trial, before the data monitoring committee saw any interim data by allocation, or some time thereafter. This information is often not included in published trial reports, even in trials that report stopping earlier than planned.

---

### Number needed to treat (NNT) in clinical literature: an appraisal [^679efdee]. BMC Medicine (2017). Low credibility.

Background

The concept of "number needed to treat" (NNT) was introduced in the medical literature by Laupacis et al. in 1988. NNT is an absolute effect measure which is interpreted as the number of patients needed to be treated with one therapy versus another for one patient to encounter an additional outcome of interest within a defined period of time. The computation of NNT is founded on the cumulative incidence of the outcome per number of patients followed over a given period of time, being classically calculated by inverting absolute risk reduction (ARR) (also called risk difference [RD]) between two treatment options.

Some characteristics are inherently associated with the concept of NNT. The resulting value is specific to a single comparison between two treatment options within a single study, rather than an isolated absolute measure of clinical effect of a single intervention. Thus, NNT is specific to the results of a given comparison, not to a particular therapy. In addition, three other factors, beyond the efficacy or safety of the intervention and the comparator, influence NNT: baseline risk (i.e. control event rate [CER]), time frame, and outcomes.

The use of NNT has been valuable in daily clinical practice, namely at assisting physicians in selecting therapeutic interventions. Further, this metric has the potential for use as a supportive tool in benefit-risk assessments and in helping regulators make decisions on drug regulation.

The Consolidated Standards of Reporting Trials (CONSORT) statement recommends the use of both relative and absolute measures of effect for randomized controlled trials (RCTs) with binary and time-to-event outcomes. The British Medical Journal (BMJ) requires that, whenever possible, absolute rather than relative risks and NNTs with 95% confidence intervals (CIs) are to be reported in RCTs. Yet, few authors express their findings in terms of NNT or ARR. Relative effect measures, such as relative risk (RR) or odds ratio (OR), are more commonly seen in the scientific literature. Despite the unquestionable usefulness of relative effect measures, they do not reflect baseline risks, making it impracticable to discriminate large from small treatment effects, and leading sometimes to misleading conclusions.

---

### Ophthalmic statistics note 6: effect sizes matter [^7d60ec18]. The British Journal of Ophthalmology (2015). Low credibility.

Why statistical significance alone is not a sufficient basis to interpret the findings of a statistical analysis

Medical statistics plays a key role in clinical research by helping to avoid errors of interpretation due to the play of chance. It is, however, critical to understand the limits of what statistical analysis provides and interpret the findings accordingly. Statistical analysis can summarise the statistical evidence but it cannot tell us whether a difference is important per se, as clinical judgement is needed. Suppose we have a clinical trial which compares two drugs for reducing the intraocular pressure (IOP) in the eye and there is evidence of a statistically significant difference in favour of drug A. Does this mean A is superior to B? Perhaps, if the average difference is 5 mm Hg but what if it is only 0.1 mmg Hg? We would be very unlikely to conclude clinical superiority under these circumstances, or at least it would be necessary to take account of differences in other key outcomes such as adverse events before reaching such a conclusion. It might be thought that the p value from a statistical hypothesis test can serve this purpose but this is not the case. Solely considering whether a p value is 'statistically significant' is not a sufficient basis to interpret the findings of a statistical analysis. There is a number of reasons why this is the case.
First, when a statistical hypothesis test is performed we are usually implicitly testing for evidence of a difference of any magnitude. To equate this difference to clinical importance is to view all statistically significant differences as clinically important and of equal value (at least in a practical sense). Clearly, as in the IOP example above, this may not be the case.
Second, the p value is not a direct measure of the strength of evidence for the null hypothesis. Instead, it is the probability of obtaining a result as, or more extreme than, the one observed assuming the null hypothesis is true. It is therefore, at best, a partial and indirect measure of the evidence regarding the truth of the null hypothesis. In particular, it is not the probability that the null hypothesis is true given the data.
Third, the criterion used for statistical significance, typically a p value of less than 0.05 (ie, 5% significance level), is arbitrary. It is based on convention rather than statistical theory. It guards against making one type of error too often (rejecting the null hypothesis when it is true), but gives no protection against making the other type of error (not rejecting the null hypothesis when it is false).
Fourth, even once we meet our criteria we must take into consideration the magnitude of an effect. It's all fine and well saying we have found a significant difference, but it is crucial to consider how big this difference is and whether we view it as clinically important. We should design our study such that the difference which would be statistically significant would also be considered to be clinically important. It is also true, as covered in an earlier BJO statistics note, that when we don't find a statistically significant difference, we must consider how precise our estimate is (the width of our CI) and what magnitude of effect would be important to us if it did truly exist. A p value neither quantifies the size of an observed effect nor its importance.

---

### Expert panel on integrated guidelines for cardiovascular health and risk reduction in children and adolescents: summary report [^a176b33a]. PES (2012). Medium credibility.

Quality criteria for assessment of individual randomized controlled trials — criteria needed for "Y" selection: Requirements for data collection and analysis include study protocol components where "Protocol described for all intervention components/regimens studied" and "Description of extra or unplanned treatments"; outcomes measurement where "Primary and secondary outcome(s)/end point(s) described" and "Primary and secondary outcomes(s)/end point(s) relevant to the objective"; adequate duration/follow-up with "Duration of intervention sufficient to detect meaningful effect on primary and secondary outcomes" and "Period of follow-up long enough for important outcome(s) to occur"; and statistical analysis elements including "Statistical analyses described", "Appropriate statistical test used and assumptions of test not violated", "Statistics reported with levels of significance and/or confidence intervals", "Intent-to-treat analysis of outcomes", "Adequate adjustment for effects of confounding factors that might have affected the outcomes", "Results/findings address statistical significance", and "Confidence interval or power calculations reported for null findings". Additional considerations include clinical significance where "Results/findings address clinical significance", discussion of findings where "Findings and implications discussed" and "Biases and study limitations identified, including assessment of how well an intervention was delivered", and adverse events reporting with "Safety outcomes/adverse events specifically reported" and "Appropriate sample size and duration to detect safety outcome(s)". For generalizability, "Study population is appropriate to answer research question" and "Intervention can be feasibly conducted in a general practice/routine/community setting", and outcomes where "Outcome(s)/end point(s) are associated with an increase or decrease in cardiovascular disease risk factor(s) or cardiovascular disease risk during childhood or adulthood" and "Outcome(s)/end point(s) can be feasibly measured in a general practice/routine/community setting"; the table notes " Consider all criteria listed; however, not all criteria must be met for a 'Y'.

---

### CONSORT 2025 explanation and elaboration: updated guideline for reporting randomised trials [^893c392f]. BMJ (2025). Excellent credibility.

When the primary outcome is binary, both the relative effect (risk ratio (relative risk) or odds ratio), and the absolute effect (risk difference) should be reported (with CIs) (table 13), as neither the relative measure nor the absolute measure alone gives a complete picture of the effect and its implications. Different audiences may prefer either relative or absolute risk, but both clinicians and lay people tend to overestimate the effect when it is presented solely in terms of relative risk. The magnitude of the risk difference is less generalisable to other populations than the relative risk since it depends on the baseline risk in the unexposed group, which tends to vary across populations. For diseases where the outcome is common, a relative risk near unity might nonetheless indicate clinically important differences in public health terms. In contrast, a large relative risk when the outcome is rare may not be so important for public health (although it may be important to an individual in a high risk category). For both binary and survival time data, expressing the results also as the number needed to treat for benefit or harm can be helpful.

Item 27: All harms or unintended events in each group

---

### Introducing the JAMA guide to statistics and methods… [^655ddc96]. JAMA Network (2014). Excellent credibility.

Customize your JAMA Network experience by selecting one or more topics from the list below. Original research articles in JAMA are selected for publication because the results are valid and findings provide important new clinical, research, or policy-related insights. To be current, clinicians must read and understand the primary research literature. By implication, this means also understanding increasingly complex methodologies and statistical analyses now used in clinical research. Clinicians may not be familiar with research methods introduced after they completed training. Because relatively little emphasis is placed in medical school on research methods and statistics, clinicians may never have learned enough about these topics to properly understand current research articles. As an aid for readers, in this issue of JAMA, we introduce the JAMA Guide to Statistics and Methods.

This new series of articles will provide explanations about statistical analytic approaches and methods used in research reported in JAMA articles, and they will be written in language practicing clinicians can understand. These explanations will be published concurrently with research articles that use the statistical test or methodological approach, thereby providing an example of the topic being discussed. The challenge in balancing statistical rigor with reader comprehension dates back to one of the first uses of a χ2 test in a JAMA article. A randomized clinical trial evaluating azacyclonol for schizophrenia treatment used χ2 analysis to demonstrate a statistically significant treatment effect. 1 The author concluded that since the P value of. 0003 was less than. 05, sufficient evidence existed to establish a hypothesis.

A letter published in response to this paper pointed out that P values do not establish hypotheses: " No p, however small, can ever establish that a hypothesis is correct… p merely is the probability that if a given hypothesis is correct, then chi-square will be found at least as large as it was in fact found. JAMA Guide to Statistics and Methods articles will be written in plain English, avoid complex mathematics, and present material graphically whenever possible. We distinguish between statistics and methods: statistics are mathematical approaches to describing collections of data whereas methods refer to how a study was designed or some other general approach to how a study was organized and conducted. JAMA Guide to Statistics and Methods articles will explain why a particular test or method was used, what its limitations are, discuss risks of bias, and examine why the study authors used the particular test.

These articles will explain how the findings from statistical tests should be interpreted in the accompanying JAMA research article. Also discussed will be the limitations of interpreting the data given the methodology used to examine it.

---

### Empirical consequences of current recommendations for the design and interpretation of noninferiority trials [^e4d071a2]. Journal of General Internal Medicine (2018). Low credibility.

METHODS

We searched MEDLINE using the search terms "noninferiority", "noninferior", "non-inferiority", and "non-inferior" combined with the name of the journals as recognized by MEDLINE and published between June 1, 2011, and November 1, 2016. We limited our search to the five highest-impact general medical journals in order to focus on the most widely cited and possibly highest-quality articles. – We reviewed the resulting abstracts to identify articles that met our inclusion criteria: prospective, parallel-group randomized controlled trials where the primary outcome was tested using a noninferiority hypothesis. We then reviewed the full manuscripts and excluded trials that had a cluster-randomized design, trials where the data were incomplete or could not be summarized, those that used a Bayesian methodology, and those that did not use an AC (e.g. Food and Drug Administration [FDA]-mandated placebo-controlled post-approval safety studies). One author (SKA) abstracted the data from the trials into a standardized form and employed a system of redundant checks to ensure data accuracy. Another author (AMH) checked a sample of the data to ensure accuracy. We tabulated basic data about the trial including methodological and statistical design as reported. In trials where more than one analysis was reported (e.g. intention-to-treat [ITT], per-protocol [PP], as-treated [AT]), we determined whether the results differed depending on the analytical method.

We characterized trial results in terms of the point estimate and confidence intervals in accord with Figure 1 of the CONSORT statement, using a 95% two-sided confidence interval, calculated from the raw numbers provided in the manuscript, and coded the trial authors' conclusions regarding declarations of noninferiority, superiority, inferiority, or inconclusive results, and whether they differed from CONSORT. We used the first outcome mentioned in the manuscript as the primary outcome for our data set when multiple outcomes were reported. Some trials compared multiple interventions, e.g. multiple doses of a new drug against one AC group, and we considered these to represent separate comparisons. In determining whether justification for the selection of a delta value was presented, we coded trials as having "none" if no mention whatsoever was made as to how it was selected, "abstract" if some mention was made but it was vague or irreproducible, and "concrete" if an explicit reproducible justification was provided.

---

### Editorial commentary: the statistical fragility index of medical trials is low by design: critical evaluation of confidence intervals is required [^4e0ccdf8]. Arthroscopy (2024). Medium credibility.

The Fragility Index (FI) provides the number of patients whose outcome would need to have changed for the results of a clinical trial to no longer be statistically significant. Although it's a well-intended and easily interpreted metric, its calculation is based on reversing a significant finding and therefore its interpretation is only relevant in the domain of statistical significance. Its interpretation is only relevant in the domain of statistical significance. A well-designed clinical trial includes an a priori sample size calculation that aims to find the bare minimum of patients needed to obtain statistical significance. Such trials are fragile by design! Examining the robustness of clinical trials requires an estimation of uncertainty, rather than a misconstrued, dichotomous focus on statistical significance. Confidence intervals (CIs) provide a range of values that are compatible with a study's data and help determine the precision of results and the compatibility of the data with different hypotheses. The width of the CI speaks to the precision of the results, and the extent to which the values contained within have potential to be clinically important. Finally, one should not assume that a large FI indicates robust findings. Poorly executed trials are prone to bias, leading to large effects, and therefore, small P values, and a large FI. Let's move our future focus from the FI toward the CI.

---

### Bayesian interpretation of p values in clinical trials [^46a20903]. BMJ Evidence-Based Medicine (2022). High credibility.

Figure 1
Three possible prior distributions for a treatment effect (measured via an OR relative to a placebo control) in a clinical trial. In each case, the region shaded blued represents values of the treatment effect that might be regarded as clinically unimportant. The dashed red line represents an OR of 1.05, the assumed minimum clinically meaningful treatment effect. The alternative region (shaded red) represents plausible treatment effect sizes under the alternative scenario the treatment has an important effect. Three hypothetical alternative effect size distributions are specified corresponding to differing prior opinion regarding the drug's putative effectiveness, labelled 'sceptical', 'standard' and 'optimistic', corresponding to expected ORs of 1.1, 1.3 and 1.5 are shown here. to complete prior specification, the analyst needs to specify the prior probability of a clinically important treatment effect. This probability is assumed to be 20% here (implying the prior probability that the treatment is ineffectual is 80%). Created by the author. OR, odds ratio.

Having specified a prior distribution, a Bayesian asks again 'what is the probability that there is a treatment effect, given the dataset that has resulted?'. With prior distributions specified as in figure 1, this is the data-updated probability that the true OR lies in the region of treatment effect values shaded red. Statistically, this probability is known as the posterior probability of a treatment effect.

---

### Nonconventional clinical trial designs: approaches to provide more precise estimates of treatment effects with a smaller sample size, but at a cost [^cc02c136]. Stroke (2007). Low credibility.

Statistical sciences have recently made advancements that allow improved precision or reduced sample size in clinical research studies. Herein, we review 4 of the more promising: (1) improvements in approaches for dose selection trials, (2) approaches for sample size adjustment, (3) selection of study end point and associated statistical methods, and (4) frequentist versus Bayesian statistical methods. Whereas each of these holds the opportunity for more efficient trials, each are associated with the need for more stringent assumptions or increased complexity in the interpretation of results. The opportunities for these promising approaches, and their associated "costs", are reviewed.

---

### CONSORT-DEFINE explanation and elaboration: recommendations for enhancing reporting quality and impact of early phase dose-finding clinical trials [^74fa6314]. EClinicalMedicine (2025). Medium credibility.

Explanation

In EPDF trials, a common key objective is the estimation of the recommended dose(s), which can be informed by statistical methods estimating toxicity, activity, a combination of both, or other parameters of interest. There is also an increasing use of seamless phase I/II designs with initial dose (de-)escalation, which may then be followed by dose expansion or a randomised dose-ranging part to explore potential promising dose(s) that are tolerable and active. For the dose-ranging part, see the ACE statement, which discusses several statistical issues that may arise when using an adaptive randomised design to estimate treatment effects for key outcomes. Such issues include estimation bias that may result if conventional estimates of treatment effect based on fixed design methods are used. Similar issues arise for single-arm multi-stage designs. Results and conclusions may differ when different analysis methods are used. Hence, there should be a description of the statistical methods used to estimate measures of treatment effects with associated uncertainty and a p -value (when prespecified in the analysis plans) to aid interpretation and reproducibility.

There should be a description of the statistical methods used for estimation of the parameters of interest (such as safety and treatment effects) with associated uncertainty (e.g. confidence intervals or credible intervals). Typical key parameters in EPDF trials include toxicity rates, treatment effects, or recommended dose(s). For instance, the statistical methods and criteria used to select the recommended dose (such as a dose with dose limiting toxicity closest to a prespecified threshold, see both examples) should be described. Authors should specify whether a frequentist or Bayesian framework was used to make inferences and what indications of uncertainty (e.g. confidence intervals or credible intervals) were calculated. Hypothesis tests that are powered to make inferences should be performed and reported if they were prespecified in the protocol. If different statistical methods were used for interim and final analyses, it is importantly to explicitly state that. For rule-based designs, where no statistical methods are utilised for estimation or to make inferences, this item is not applicable.

Item 12b [modified] Statistical methods for additional analyses (e.g. subgroup and adjusted analyses, pharmacokinetics or pharmacodynamics, biomarker correlative analyses)

---

### Interpretation of absolute measures of disease risk in comparative research [^75e03d3c]. Family Medicine (2007). Low credibility.

When comparing two groups, one receiving an experimental intervention and the other a placebo or nothing, researchers often wish to assess the disparity in risk of experiencing an event of interest, such as onset of disease. Relative risk, relative risk reduction, and odds ratio are often used to measure the association between potential benefit or harm and the intervention. However, these summary measures reflect relative disparities and are perhaps less useful in clinical practice than measures of absolute benefit or harm. We demonstrate that relative risk reduction is unaffected by the risk of an event in the control group and hence may either overestimate or underestimate the treatment effect. Absolute risk reduction accounts for the baseline control group event rate and is a more realistic quantification of treatment effect than relative measures. Number needed to treat (NNT) estimates the therapeutic effort needed to prevent one additional adverse event. NNT incorporates both relative risk reduction and the event rate without treatment. For a given relative risk reduction, we demonstrate the NNT will increase as the control event rate decreases. Thus, NNT has more-obvious implications for clinical decision making than risk estimates expressed in relative terms.

---

### An official American Thoracic Society workshop report: developing performance measures from clinical practice guidelines [^7fffb60b]. Annals of the American Thoracic Society (2014). Medium credibility.

Grading of Recommendations Assessment, Development, and Evaluation (GRADE) evidence quality assessment — Under GRADE, the quality of evidence is influenced by "study design, risk of bias, consistency, precision, directness, and publication bias", with randomized trials "presumed to be high in quality, unless compromised", and observational studies "considered to be low in quality by default but can be 'rated up' by demonstrating a large magnitude of effect, a dose–response, or results that would be even more compelling if plausible confounders are taken into account". Table factors for rating down include "Risk of bias", "Heterogeneity", "Imprecision", "Indirectness", and "Publication bias", and for rating up include "Magnitude of effect", "Dose–response", and "Plausible confounding effect", with example comments such as "Unclear allocation concealment, incomplete blinding, selective reporting of results, incomplete accounting of outcomes", "Inconsistent results across studies", "Confidence intervals range from substantial benefit to little or no benefit", and "Studies showing negative or conflicting results may exist but are missing from the literature".

---

### Smaller sample sizes for phase II trials based on exact tests with actual error rates by trading-off their nominal levels of significance and power [^fb367a8c]. British Journal of Cancer (2012). Low credibility.

Our approach to sizing studies is not just limited to single-stage designs, but can also be extended to two-stage and other n -stage designs where exact methods are used. In some two-stage design, trade-offs in the expected sample size are considered for smaller overall sample sizes. By compromising α and power in addition to the expected sample size, it is possible that savings in sample size are even greater. However, additional complexities such as the probability of early termination might also be important when considering any trade-off.

The implications of trading-off type I and II errors is that the risk of a false-positive or -negative may be slightly above or below the conventional 5% and 80%, respectively. The specific type of trade-off is likely to be based on feasibility and may vary from trial to trial. However, in phase II trials, which are often about finding preliminary evidence of effect, a trade-off in either direction may be possible. It is important to point out that such a trade-off does not influence the size of the treatment benefit.

A limitation of our suggested approach is that the final result ideally needs to be considered in relation to the α level used in the sample-size calculation, which is not a round number such as 5% or 10%. However, even when sample sizes come fromor software, the interpretation of the primary result is based on α = 5%, even though the actual value might be 4.5%. Moreover, reported P -values such as '0.052' or '0.057' (in the context of phase III trials) are not readily dismissed for lack of effect, and therefore powering a trial with non-standard α and power may also be considered a reasonable approach for phase II study designs. Nevertheless, the decision on whether or not to investigate a new treatment further should not be based on a single numerical cut-off for α, but perhaps on consideration of several pieces of information, including other clinically important efficacy end points, safety and accrual rates. It is often the case that a smaller treatment effect is observed, and precision would be lost by having a study that is too small, making it difficult to determine whether to investigate the new therapy further or not. We therefore do not recommend that sample sizes be reduced to < 20 patients per treatment group.

---

### CONSORT 2010 explanation and elaboration: updated guidelines for reporting parallel group randomised trials [^86261863]. BMJ (2010). Excellent credibility.

Item 17a. For each primary and secondary outcome, results for each group, and the estimated effect size and its precision (such as 95% confidence interval)

Examples — See tables 5and 6.

Table 5
Example of reporting of summary results for each study group (binary outcomes).* (Adapted from table 2 of Mease et al 103)

*See also example for item 6a.

PsARC = psoriatic arthritis response criteria. ACR = American College of Rheumatology.

Table 6
Example of reporting of summary results for each study group (continuous outcomes). (Adapted from table 3 of van Linschoten 234)

*Function score adjusted for baseline, age, and duration of symptoms.

Explanation — For each outcome, study results should be reported as a summary of the outcome in each group (for example, the number of participants with or without the event and the denominators, or the mean and standard deviation of measurements), together with the contrast between the groups, known as the effect size. For binary outcomes, the effect size could be the risk ratio (relative risk), odds ratio, or risk difference; for survival time data, it could be the hazard ratio or difference in median survival time; and for continuous data, it is usually the difference in means. Confidence intervals should be presented for the contrast between groups. A common error is the presentation of separate confidence intervals for the outcome in each group rather than for the treatment effect. Trial results are often more clearly displayed in a table rather than in the text, as shown in tables 5and 6.

---

### Assessment of inverse publication bias in safety outcomes: an empirical analysis [^c8e22410]. BMC Medicine (2024). Medium credibility.

Background

Randomized clinical trials (RCTs) offer a powerful tool to evaluate the benefits and risks of medical interventions; however, they are often hampered by financial burdens and complex implementation. Meta-analyses are widely used to summarize the results from different RCTs and provide an overall effect estimate, underpinning guidelines in evidence-based medicine. The quality of meta-analytical results depends highly on the risk of bias of the studies included in systematic reviews. The risk of bias occurs at both the within- and between-study levels. Publication bias (PB) is a type of between-study bias, and it generally refers to the preference for publishing studies based on the nature and direction of their findings, for example, those with significant results or large effect sizes. PB in the medical field challenges the validity of evidence synthesis, may lead to incorrect clinical decision-making, and could thus influence evidence-driven medical practice and result in the wastage of medical resources.

While efficacy outcomes of drugs are usually the focus of evidence research, adverse events also play a crucial role in determining the overall safety profile and acceptability of a medication in clinical practice. Assessing safety outcomes is an essential component of systematic reviews and meta-analyses, yet it is often inadequately reported. Recently, Ioannidis discussed the phenomenon of inverse publication bias (IPB), where studies without significant results, i.e. no harmful effects on patients, are more likely to be published. Detecting PB in the assessment of safety outcomes is crucial for ensuring a safe environment in patient care.

The data of adverse events may suffer from low event rates, sometimes leading to many zero events and limited sample size, challenging the detection of IPB. A previous study in 2015 assessed PB in meta-analyses from the Cochrane Database of Systematic Reviews; it found that results showing no evidence of adverse effects were, on average, 78% (95% credible interval: 51 to 113%) more likely to be included than results showing adverse effects in meta-analyses of safety. Another recent study also empirically assessed PB but only included a small number of trials and drugs with a focus on efficacy. To the best of our knowledge, the existing empirical studies on PB did not particularly focus on the assessment of IPB of adverse events, using methods that should be tailored to adverse events. For example, while the usual PB in efficacy outcomes tends to bias away from the null effect, IPB in adverse events favors results closer to the null value.

---

### Six persistent research misconceptions [^4e0a094f]. Journal of General Internal Medicine (2014). Low credibility.

Misconception 6. Significance testing is useful and important for the interpretation of data.

Significance testing has led to far more misunderstanding and misinterpretation than clarity in interpreting study results.–A significance test is a degraded version of the P value, a statistic that blends precision with effect size, thus confusing two essential aspects of data interpretation. Measuring effect size and its precision as separate tasks is a more direct and clearer approach to data interpretation.

For research studies that aim to measure associations, and infer whether they reflect causal connections, focusing on the magnitude of these associations ought to be the primary goal: estimation of effects is decidedly preferable to statistical testing. Ideally, a study estimates the magnitude of the effect size, and analyzes the possible errors that might have distorted it. Systematic errors such as confounding from measured factors can be dealt with through analytic methods; other systematic errors, such as the effects of measurement error or selection bias, can be addressed through sensitivity analyses (also known as bias analysis). Random error is typically expressed through confidence intervals, giving a range of parameter values that are consistent with the data to a specified level.

It is unfortunate that a confidence interval, from which both an estimate of effect size and its measurement precision can be drawn, is typically used merely to judge whether it contains the null value or not, thus converting it to a significance test. Significance tests are a poor classification scheme for study results; strong effects may be incorrectly interpreted as null findings because authors fallaciously interpret lack of statistical significance to imply lack of effect, or weak effects may be incorrectly interpreted as important because they are statistically significant. Rather than be used as surrogate significance tests, confidence intervals ought to be interpreted as quantitative measures indicating magnitude of effect size and degree of precision, with little attention paid to the precise location of the boundaries of the confidence interval. This advice is backed by the Uniform Requirements for Manuscripts Submitted to Biomedical Journals, but nevertheless often overlooked even by reviewers and editors whose journals support the requirements.

---

### Challenges in applying the results of clinical trials to clinical practice [^37258208]. JAMA Ophthalmology (2016). Medium credibility.

The relevance of clinical trial results to clinical practice hinges on 2 critical questions: Will the results be replicated in one's practice, and Are the results clinically important? The answers to the following 5 questions may help one determine how relevant a study result is to clinical practice. First, have steps been taken to minimize bias (eg, masking, randomization)? Second, is the result likely due to the treatment (vs confounding factors)? Third, is the result unlikely to be due to chance? Fourth, is the study population representative of your patients? Fifth, is the totality of the evidence consistent across studies? To determine if a study result is likely to be clinically important, consider a 3-step approach. In step 1, decide, a priori, what a clinically meaningful difference between 2 treatments would be to define regions of beneficial, harmful, and trivial outcomes. In step 2, determine whether the CIs around the average outcome include the range of beneficial outcomes and lie outside the range of harmful outcomes. In step 3, determine the proportion of patients achieving a clinically meaningful benefit. If the CIs mostly include the range of beneficial outcomes and lie outside the range of clinically harmful outcomes and if a substantial proportion of patients achieve a clinically meaningful benefit, then the intervention is probably clinically important. Application of clinical trial results to clinical practice requires critical analysis of the extant literature as well as good clinical judgment.

---

### Guidance for protocol content and reporting of factorial randomised trials: explanation and elaboration of the CONSORT 2010 and SPIRIT 2013 extensions [^7920d93b]. BMJ (2025). Excellent credibility.

Results

CONSORT 2010 item 17a: For each primary and secondary outcome, results for each group, and the estimated effect size and its precision (such as 95% confidence interval)
Extension for factorial trials: For each primary and secondary outcome, results for each main comparison, the estimated effect size and its precision (such as 95% confidence interval), for each primary outcome, the estimated interaction effect and its precision, and if done, estimated interaction effects and precision for secondary outcomes

CONSORT example: "At 3 years there was no evidence of a difference in bleeding on probing between randomised groups, for example, 0 versus 6-monthly: mean difference 0.87%, 95% CI [confidence interval]: −1.6 to 3.3, P = 0.48 or between patients randomised to receive usual or personalised OHA [oral hygiene advice] (mean difference −2.5%, 95%CI: −8.3 to 3.3, P = 0.39). The interaction between personalised OHA and 6-monthly S&P [scale and polish] for bleeding was 1.7 (95% CI −3.8 to 7.3) (that is, neither statistically nor clinically significant)".

Explanation

Evaluation of statistical interactions is essential to the interpretation of most factorial trials, either because the validity of the design rests on the assumption of no interaction (2-in-1 trials) or the interaction itself is of main interest. The size of the estimated interaction effect should be presented for each primary outcome, and for secondary outcomes if done, along with a measure of precision such as 95% confidence interval.2-in-1 trials are typically underpowered to identify all but very large interactions, and thus a large P value should not be taken as evidence that no interaction exists.

---

### Expert perspectives on key parameters that impact interpretation of randomized clinical trials in moderate-to-severe atopic dermatitis [^f9cb52c2]. American Journal of Clinical Dermatology (2022). Medium credibility.

In addition to the points raised herein, across-trial interpretation may benefit from increased reporting of the number needed to treat (NNT) and absolute risk reduction (ARR). The NNT represents the number of participants needed to be treated to achieve one additional positive outcome relative to a control group. The ARR expresses the difference in frequency of negative/unfavorable outcomes in the arms of a trial. Since 2001, the CONSORT group (CONsolidated Standards Of Reporting Trials) recommends reporting the NNT in the results of randomized controlled trials. While NNT and AAR also have limitations related to study design, their regular reporting would provide an additional measure when seeking to understand differences between agents studied in independent trials.

Conclusions

Overall, this article highlights the key aspects of clinical trial design that may impact treatment efficacy, and will limit the validity of side-by-side comparisons of clinical trial data. Our review highlights how certain study parameters may impact other aspects of study design. These interactions pose challenges for medical experts seeking to compare the outcomes of two different trials. The differences between trials may impact active treatment and placebo arms differently, which further complicates comparisons. This raises a need for caution towards trial comparisons and substantiates the need to harmonize study procedures in moderate-to-severe AD trials and establish a standardized approach for analyzing trial results. We welcome further research investigating the impact of these differences, as well as efforts to harmonize AD trials.

---

### Are potentially clinically meaningful benefits misinterpreted in cardiovascular randomized trials? A systematic examination of statistical significance, clinical significance, and authors' conclusions [^8435e5b8]. BMC Medicine (2017). Low credibility.

Background

While journals and reporting guidelines recommend the presentation of confidence intervals, many authors adhere strictly to statistically significant testing. Our objective was to determine what proportions of not statistically significant (NSS) cardiovascular trials include potentially clinically meaningful effects in primary outcomes and if these are associated with authors' conclusions.

Methods

Cardiovascular studies published in six high-impact journals between 1 January 2010 and 31 December 2014 were identified via PubMed. Two independent reviewers selected trials with major adverse cardiovascular events (stroke, myocardial infarction, or cardiovascular death) as primary outcomes and extracted data on trial characteristics, quality, and primary outcome. Potentially clinically meaningful effects were defined broadly as a relative risk point estimate ≤ 0.94 (based on the effects of ezetimibe) and/or a lower confidence interval ≤ 0.75 (based on the effects of statins).

Results

We identified 127 randomized trial comparisons from 3200 articles. The primary outcomes were statistically significant (SS) favoring treatment in 21% (27/127), NSS in 72% (92/127), and SS favoring control in 6% (8/127). In 61% of NSS trials (56/92), the point estimate and/or lower confidence interval included potentially meaningful effects. Both point estimate and confidence interval included potentially meaningful effects in 67% of trials (12/18) in which authors' concluded that treatment was superior, in 28% (16/58) with a neutral conclusion, and in 6% (1/16) in which authors' concluded that control was superior. In a sensitivity analysis, 26% of NSS trials would include potential meaningful effects with relative risk thresholds of point estimate ≤ 0.85 and/or a lower confidence interval ≤ 0.65.

Conclusions

Point estimates and/or confidence intervals included potentially clinically meaningful effects in up to 61% of NSS cardiovascular trials. Authors' conclusions often reflect potentially meaningful results of NSS cardiovascular trials. Given the frequency of potentially clinical meaningful effects in NSS trials, authors should be encouraged to continue to look beyond significance testing to a broader interpretation of trial results.

Electronic supplementary material

The online version of this article (doi:10.1186/s12916-017-0821-9) contains supplementary material, which is available to authorized users.

---

### A structured preapproval and postapproval comparative study design framework to generate valid and transparent real-world evidence for regulatory decisions [^1f3c6594]. Clinical Pharmacology and Therapeutics (2019). Medium credibility.

Like others, we refer to "regulatory‐quality" RWE as arising from studies that enable estimation of a realized causal effect (i.e. the effect the treatment actually had on the outcome in the study population). 6 Valid effect estimation requires that internal validity violations are avoided through the design and/or analysis. 22, 23 This task is most feasible in an RCT, in which bias is reduced via experimental design features, such as randomization and regimented measurement and follow‐up; for this reason, in practice, the RCT is considered the gold standard for measuring causal effects. In RWE studies, many of these RCT design features are absent. As the study becomes more pragmatic, there is more residual uncertainty around the impact of bias on the study results. Thus, we suggest real‐world conditions are allowed when of specific interest or needed to improve study feasibility, and potential biases can arguably be minimized. To assure an interpretable effect, the researcher should be explicit in his/her rationale for any decisions that take the design further away from the RCT standard, so that this uncertainty is minimized to an acceptable level. To frame the decision‐making process, we find it helpful to articulate high‐level priorities based on the stakes involved. For example, if we are concerned about missing the small effect of a treatment on decreased mortality, we might have less tolerance for uncertainty around bias and use more experimental features. In contrast, if we are concerned about confirming a large effect or we are conducting a postapproval study to confirm safety of long‐term use, we may be able to tolerate more uncertainty around bias in favor of a less controlled study that can be completed more quickly. Many of these parameters are shown in Schneeweiss et al.'s 17 Figure 4 and Table 2. Other considerations might include the severity of the outcome (e.g. mortality vs. improvement of a moderate disease symptom) and the overall benefit–risk profile of the medicine (i.e. if a nonnull study result improves or tips the balance to the negative). Stakeholder input (e.g. regulators, patient groups, and clinical experts) is critical in determining what is clinically meaningful, allowably uncertain, and how uncertainty will be characterized or quantified post hoc. A priori quantification of potential biases (see examples in refs. 24, 25, 26, 27, 28) may also be useful, in particular for discussions with regulators around the likelihood of effect estimate interpretability.

---

### A new look at P values for randomized clinical trials [^2a9dcae0]. NEJM Evidence (2024). Medium credibility.

BACKGROUND: We have examined the primary efficacy results of 23,551 randomized clinical trials from the Cochrane Database of Systematic Reviews. METHODS: We estimate that the great majority of trials have much lower statistical power for actual effects than the 80 or 90% for the stated effect sizes. Consequently, "statistically significant" estimates tend to seriously overestimate actual treatment effects, "nonsignificant" results often correspond to important effects, and efforts to replicate often fail to achieve "significance" and may even appear to contradict initial results. To address these issues, we reinterpret the P value in terms of a reference population of studies that are, or could have been, in the Cochrane Database. RESULTS: This leads to an empirical guide for the interpretation of an observed P value from a "typical" clinical trial in terms of the degree of overestimation of the reported effect, the probability of the effect's sign being wrong, and the predictive power of the trial. CONCLUSIONS: Such an interpretation provides additional insight about the effect under study and can guard medical researchers against naive interpretations of the P value and overoptimistic effect sizes. Because many research fields suffer from low power, our results are also relevant outside the medical domain. (Funded by the U.S. Office of Naval Research.)

---

### Indication of measures of uncertainty for statistical significance in abstracts of published oncology trials: a systematic review and meta-analysis [^5012ab2c]. JAMA Network Open (2019). High credibility.

Introduction

Widely publicized clinical findings have often proven difficult to replicate in follow-up analyses. One factor is the high type I error rate associated with the ubiquitous threshold for statistical significance, which is a P value less than.05. Owing in part to concerns that this threshold has raised reproducibility concerns, the American Statistical Association released a consensus statement advising caution in interpretation of P values for hypothesis testing, emphasizing that P values do not measure the effect size of a result, nor do they inform the likelihood that the null hypothesis is false. However, it is common for publications to imply that a P value offers information on the probability of the null hypothesis (that a therapeutic intervention has no efficacy); instead, a P value provides the probability of observing the treatment effect assuming that the null hypothesis is valid. In reality, a P value of.05 continues to serve as a discrete threshold for statistical significance. The.05 threshold is so widely accepted as a threshold for publishable results that many collect data only until their results cross this threshold — a phenomenon known as " P hacking". In response to this controversy, some have advocated for lowering the threshold by an order of magnitudeor have questioned the validity of the concept of statistical significance. Others have emphasized inclusion of a confidence interval in clinical trial reporting to highlight the uncertainty of the results.

There also remains substantial room for improvement in the rigor that is applied to reporting of what are considered to be positive clinical trial results. The magnitude of benefit reported as being clinically relevant declined between 1980 and 2010 in the reporting of clinical trials for non–small cell lung cancer, while at the same time reliance on surrogate end points increased. An analysis of randomized clinical trials (RCTs) with statistically nonsignificant results (ie, results with a P value > .05) reported that the results were presented with "spin" (misrepresentation of study findings) in a significant number of trials; as such, findings were reported in a manner that was inconsistent with the results. Spin has been shown to be associated with clinicians' interpretation of trial results. This style of reporting has implications for the practice of medical oncology in particular. Antineoplastic therapies generally have among the lowest therapeutic indices and highest costs in medicine, meaning that the adverse effects associated with these therapies can outweigh the benefits in many scenarios. If the data supporting a treatment's efficacy are marginal, and therefore the benefit of a treatment is questionable, clinicians need to be aware of this uncertainty and convey it to patients before making an informed decision about treatment.

---

### Influence of trial sample size on treatment effect estimates: meta-epidemiological study [^9c76d951]. BMJ (2013). Excellent credibility.

Implications for researchers

Our results have an important effect on the interpretation of results of clinical trials and meta-analyses in general. The main issue of systematic reviews and meta-analyses is whether the combined treatment effect estimated by synthesising all included studies provides the best estimate of the true treatment effect, or whether the studies overestimate or underestimate the treatment effect. Because larger trialsare probably more pragmatic than smaller trials, with wider eligibility criteriaand greater variability in interventions, treatment effect estimates reported in large trials could be closer to the true treatment effect in real life. Thus, meta-analyses of all available evidence — whatever the trial sample size — might not reflect the true treatment effect.

Several authors have suggested that the results from large randomised controlled trials are inherently superior to those from smaller trials, even when the results of smaller trials are pooled in a meta-analysis, and that an substantially large trial should be conducted to definitively answer the question across a large sample of the population. Glasziou and colleaguesalso proposed relying on the results of the most precise trial if a meta-analysis was not available. Our results raise questions about whether the meta-analysis should be restricted to larger trials (or even to the "largest" trial). The downside of this approach would be imprecise estimates of the treatment effect. Rücker and colleagues recently proposed a method of limit meta-analysis, which allows for predicting treatment effects when the precision of each trial is increased to infinity. This approach is close to the regression based model described by Moreno and colleagues.

---

### Number needed to treat and number needed to harm are not the best way to report and assess the results of randomised clinical trials [^f243221a]. British Journal of Haematology (2009). Low credibility.

The inverse of the difference between rates, called the 'number needed to treat' (NNT), was suggested 20 years ago as a good way to present the results of comparisons of success or failure under different therapies. Such comparisons usually arise in randomised controlled trials and meta-analysis. This article reviews the claims made about this statistic, and the problems associated with it. Methods that have been proposed for confidence intervals are evaluated, and shown to be erroneous. We suggest that giving the baseline risk, and the difference in success or event rates, the 'absolute risk reduction', is preferable to the number needed to treat, for both theoretical and practical reasons.

---

### On reporting of effect size in randomized clinical trials [^947dcd31]. The American Journal of Cardiology (2013). Low credibility.

Published reports of randomized clinical trials tend to emphasize the statistical significance of the treatment effect (p values) rather than its magnitude (effect size), although the clinical importance of the evidence depends more on the latter than on the former. We, therefore, compared the standard measures of effect size (relative and absolute risk reduction) and nonstandard composites of these measures (the product of the relative and absolute risk reductions and information content) with conventional assessments of statistical significance for 100 trials published in The New England Journal of Medicine. The p values were reported for 100% of the trials, relative risk reductions for 89%, and absolute risk reductions for 39%. Only 35% of trials reported both standard measures, and none reported either of the nonstandard measures. The standard measures correlated weakly (unexplained variance 77%). In contrast, the nonstandard measures correlated highly (unexplained variance 1.3%) but correlated weakly with statistical significance (unexplained variance 83%). Consequently, 25% of the trial results were adjudged "clinically unimportant" despite being "statistically significant". In conclusion, our results have shown that composite measures of effect size communicate the clinical importance of trial results better than do conventional assessments of risk reduction and statistical significance.

---

### Reporting of factorial randomized trials: extension of the CONSORT 2010 statement [^9258fbe4]. JAMA (2023). Excellent credibility.

Item 13a. CONSORT 2010: For each group, the numbers of participants who were randomly assigned, received intended treatment, and were analyzed for the primary outcome

Extension for factorial trials: For each main comparison, the number of participants who were randomly assigned, received intended treatment, and were analyzed for the primary outcome

For factorial trials, especially those beyond a 2 × 2 design, it can be difficult for readers to identify the relevant participant flow because this information may differ across main comparisons. Presenting this information for each main comparison increases clarity and understanding.

Item 14a. CONSORT 2010: Dates defining the periods of recruitment and follow-up

Extension for factorial trials: Dates defining the periods of recruitment and follow-up for each factor, noting any differences with reasons

If periods of recruitment are different across factors, participants enrolled after one factor has stopped recruitment will only be eligible to be randomized for the ongoing factor(s), posing similar statistical issues as in a partial factorial design (see CONSORT item 4a).

Box 2
Estimands for Factorial Trials

Item 17a. CONSORT 2010: For each primary and secondary outcome, results for each group and the estimated effect size and its precision (such as 95% CI)

Extension for factorial trials: For each primary and secondary outcome, results for each main comparison, the estimated effect size, and its precision (such as 95% CI)

For each primary outcome, the estimated interaction effect and its precision

If done, estimated interaction effects and precision for secondary outcomes

For factorial trials predicated on the assumption of no interaction (2-in-1 trials) or those in which the interaction is of main interest, evaluation of interactions is essential to interpretation. The size of the estimated interaction effect should be presented along with a measure of precision, such as the 95% CI. For trials in which evaluation of interaction(s) is not deemed essential, this decision should be justified.

---

### CONSORT 2010 explanation and elaboration: updated guidelines for reporting parallel group randomised trials [^24f8d8ab]. BMJ (2010). Excellent credibility.

Explanation — For scientific and ethical reasons, the sample size for a trial needs to be planned carefully, with a balance between medical and statistical considerations. Ideally, a study should be large enough to have a high probability (power) of detecting as statistically significant a clinically important difference of a given size if such a difference exists. The size of effect deemed important is inversely related to the sample size necessary to detect it; that is, large samples are necessary to detect small differences. Elements of the sample size calculation are (1) the estimated outcomes in each group (which implies the clinically important target difference between the intervention groups); (2) the α (type I) error level; (3) the statistical power (or the β (type II) error level); and (4), for continuous outcomes, the standard deviation of the measurements. The interplay of these elements and their reporting will differ for cluster trialsand non-inferiority and equivalence trials.

Authors should indicate how the sample size was determined. If a formal power calculation was used, the authors should identify the primary outcome on which the calculation was based (see item 6a), all the quantities used in the calculation, and the resulting target sample size per study group. It is preferable to quote the expected result in the control group and the difference between the groups one would not like to overlook. Alternatively, authors could present the percentage with the event or mean for each group used in their calculations. Details should be given of any allowance made for attrition or non-compliance during the study.

Some methodologists have written that so called underpowered trials may be acceptable because they could ultimately be combined in a systematic review and meta-analysis, and because some information is better than no information. Of note, important caveats apply — such as the trial should be unbiased, reported properly, and published irrespective of the results, thereby becoming available for meta-analysis. On the other hand, many medical researchers worry that underpowered trials with indeterminate results will remain unpublished and insist that all trials should individually have "sufficient power". This debate will continue, and members of the CONSORT Group have varying views. Critically however, the debate and those views are immaterial to reporting a trial. Whatever the power of a trial, authors need to properly report their intended size with all their methods and assumptions. That transparently reveals the power of the trial to readers and gives them a measure by which to assess whether the trial attained its planned size.

---

### End points in clinical trials: are they moving the goalposts? [^cc6f6527]. Heart (2006). Low credibility.

In selecting and defining composite end points in clinical trials, are we trading off clinical significance for statistical significance?

---

### Application of the reverse fragility index to statistically nonsignificant randomized clinical trial results [^5b021b9d]. JAMA Network Open (2020). High credibility.

Introduction

Interpreting randomized clinical trial (RCT) results and their clinical relevance when P values are marginally above or below the threshold of P = 0.05 is challenging. Although the clinical relevance may not be different, a P value marginally below the P = 0.05 threshold is usually accepted as a favorable finding in a trial, and a P value above the P = 0.05 threshold is considered an unfavorable result. Efficacy of an intervention should be evaluated comprehensively on the basis of the effect size measures, such as relative risk reduction or number needed to treat accompanied by P values and 95% CIs, but clinical research continues to emphasize the prespecified threshold of P = 0.05 when interpreting results. Such reliance on P values invites the risk of a type II error (ie, nonrejection of a false null hypothesis, which is also known as a false-negative or β error), especially in the presence of fewer events, small sample sizes, and/or limited follow-up times. Thus, it is critical to also evaluate the robustness of null trial results in cases in which the clinical consequences of a type II error are more important than those of a type I error (ie, rejection of a true null hypothesis, which is also known as a false-positive or α error), such as in disease states with high mortality and limited therapeutic options and with an acceptable intervention safety profile.

Robustness of statistically significant trials is often evaluated using the fragility index (FI), which is exclusively applied to trials that reach traditional statistical significance. Although a few studies have applied the FI to null clinical trial results of specific diseases, none of these previous studies have systematically assessed the robustness of a large number of statistically nonsignificant phase 3 to 4 trials with an emphasis on interpretability of null clinical trial results.

In this cross-sectional study, we used the concept of a reverse fragility index (RFI) to calculate the minimum number of events needed to change trial results from statistically nonsignificant to statistically significant. Our intent was to provide a measure of confidence in the neutrality of results when assessed from the clinical perspective.

---

### Understanding measures of treatment effect in clinical trials [^cfe07a65]. Archives of Disease in Childhood (2005). Low credibility.

Evidence based medicine implies that healthcare professionals are expected to base their practice on the best available evidence. This means that we should acquire the necessary skills for appraising the medical literature, including the ability to understand and interpret the results of published articles. This article discusses in a simple, practical, 'non-statistician' fashion some of the important outcome measures used to report clinical trials comparing different treatments or interventions. Absolute and relative risk measures are explained, and their merits and demerits discussed. The article aims to encourage healthcare professionals to appreciate the use and misuse of these outcome measures and to empower them to calculate these measures themselves when, as is frequently the case, the authors of some original articles fail to present their results in a more clinically friendly format.

---

### How informative is a negative finding in a small pharmacogenetic study? [^8281c96c]. The Pharmacogenomics Journal (2012). Low credibility.

Many pharmacogenetic studies result in negative findings, such that no statistically significant associations are observed between genetic variants and phenotype. Reasons for negative findings include absence of a genetic effect, not measuring the causal variant, or low power due to small sample sizes, small effect sizes or genetic heterogeneity. Interpretation guidelines for negative findings are available for classical clinical studies. However, pharmacogenetic studies often differ from other clinical studies by being very exploratory and investigating a large number of variants. Nevertheless, we now have a number of well-validated pharmacogenetic effects, which allow us to assess the informativeness of a negative finding by assessing power to detect associations with these validated effects. We propose a strategy for interpretation that supports stronger inferences about the possible range of genetic effects that may be present, but unobserved, in a study. We illustrate our approach by evaluating the negative findings from three studies.

A central question to address is what additional information, aside from failure to reject the null hypothesis of no association between measured genotypes and phenotype, can be drawn from a negative finding. Most genetic studies base their inference primarily on P -values. Such an approach is not without disadvantages. Criticisms of using P -values for inference include the inability to judge the relative probabilities of the null or alternative hypotheses given the data, the abrupt and false dichotomy between significant and not significant, the impact of sample size on the interpretation, and the dependence of power on minor allele frequency. One way to address these shortcomings is to adopt a Bayesian approach, such as estimating the posterior probability of association. Other useful tools include confidence intervals for effect size and the careful investigation of power to determine what effect sizes could be detected from the study at hand and, thus, what effects sizes can be confidently excluded.

---

### Using the number needed to treat in clinical practice [^646ae153]. Archives of Physical Medicine and Rehabilitation (2004). Low credibility.

The number needed to treat (NNT) is gaining attention as a method of reporting the results of clinical trails with dichotomous outcome measures. The NNT is defined as the number of patients who would need to be treated, on average, with a specific intervention to prevent 1 additional bad outcome or to achieve 1 desirable outcome in a given time period. Because it reports outcomes in terms of patient numbers, it is extremely useful to clinicians for making decisions about the effort expended with a particular intervention to achieve a single positive outcome. This special communication describes the NNT statistic and its utility for choosing clinical interventions.

---

### Improving the reporting of clinical trials of infertility treatments (IMPRINT): modifying the CONSORT statement [^3fcb3afd]. Fertility and Sterility (2014). Medium credibility.

IMPRINT infertility trials — outcomes and additional analyses reporting include "For each primary and secondary outcome, results for each group, and the estimated effect size and its precision (such as 95% confidence interval)", with "For binary outcomes, presentation of both absolute and relative effect sizes is recommended". Additional analyses should be described as "Results of any other analyses performed, including subgroup analyses and adjusted analyses, distinguishing prespecified from exploratory".

---

### Statistical significance versus clinical relevance in cardiovascular medicine [^49454e22]. Progress in Cardiovascular Diseases (2001). Low credibility.

Evidence-based medicine is the foundation of everyday clinical practices and large clinical trials investigating the effects of various interventions on morbidity and survival and generally provide the most robust evidence. Cardiovascular medicine is considered one of the most evidence-based disciplines of medicine. However, there are a number of limitations to the general applicability of clinical trial results in cardiovascular medicine. Although generally useful to the clinician, clinical trials have often been suboptimally designed from 1 or several points of view. As a consequence of flaws in the design and the execution of the trials, statistical significance is quite often not equal to clinical relevance. This article outlines some of the shortcomings of designing and carrying out clinical trials, as well as inadequacies concerning the publication, interpretation, and implementation of the trial results. Evidence-based medicine is obviously not always as solid as one might think.

---

### Statistical issues in interpreting clinical trials [^12b26ce8]. Journal of Internal Medicine (2004). Low credibility.

Randomized clinical trial is an important research tool in evaluating new therapeutic agents, devices and procedures. In order to obtain reliable and unbiased results, careful consideration must be given in the design and conduct of the trial. However, bias can be introduced in the analysis of the final data if certain principles are not followed. Several issues are described that make interpretation of analyses challenging. These include the intent-to-treat principle, the use of surrogate outcome measures, subgroup analyses, missing data and noninferiority trials.

---

### Misleading reporting and interpretation of results in major infertility journals [^fb51750b]. Fertility and Sterility (2016). Low credibility.

Objective

To evaluate the proportion of randomized controlled trials (RCTs) published in top infertility journals indexed on PubMed that reported their results with proper effect estimates and their precision estimation, while correctly interpreting both measures.

Design

Cross-sectional study evaluating all the RCTs published in top infertility journals during 2014.

Setting

Not applicable.

Patient(S)

Not applicable.

Intervention(S)

Not applicable.

Main Outcome Measure(S)

Proportion of RCTs that reported both relative and absolute effect size measures and its precision.

Result(S)

Among the 32 RCTs published in 2014 in the top infertility journals reviewed, 37.5% (95% confidence interval [CI] 21.1–56.3) did not mention in their abstracts whether the difference among the study arms was statistically or clinically significant, and only 6.3% (95% CI, 0.8–20.8) used a CI of the absolute difference. Similarly, in the results section, these elements were observed in 28.2% (95% CI, 13.7–46.7) and 15.6% (95% CI, 5.3–32.8), respectively. Only one study clearly expressed the minimal clinically important difference in their methods section, but we found related proxies in 53% (95% CI, 34.7–70.9). None of the studies used CIs to draw conclusions about the clinical or statistical significance. We found 13 studies where the interpretation of the findings could be misleading.

Conclusion(S)

Recommended reporting items are underused in top infertility journals, which could lead to misleading interpretations. Authors, reviewers, and editorial boards should emphasize their use to improve reporting quality.

---

### CONSORT-DEFINE explanation and elaboration: recommendations for enhancing reporting quality and impact of early phase dose-finding clinical trials [^0d7f1a9a]. EClinicalMedicine (2025). Medium credibility.

Example 2

"The safety analysis set included all the subjects who were randomized and received at least 1 dose of BIIB059 or placebo. The PD [pharmcodynamics] analysis set included all the subjects who were randomized and received at least 1 dose of BIIB059 or placebo and had at least 1 sample biomarker or other data collected after BIIB059 administration. The PK [pharmacokinetics] analysis set included all individuals in the safety analysis set for whom at least 1 primary PK parameter could be calculated".

Explanation

A clear description of the analysis populations, also known as analysis sets, will allow the reader to assess whether they are directly relevant to, and guided by, the specific objectives of a given EPDF trial and, thus, whether the trial can address these objectives. The interim and final analysis populations define the participants for whom the results of an EPDF trial will be generalisable. Criteria for participant replacement, which is common in EPDF trials, will help with interpretation and reproducibility, as well as assist readers in determining the credibility of the results.

Authors should clearly define trial-level participant (sub)populations or datasets used in statistical analysis (e.g. dose finding evaluable (see Example 1), response evaluable (see Example 1), safety (both examples)), detailing criteria for participants being evaluable for statistical analysis. The minimum data set that was reviewed prior to making a dose decision should be specified. This should include the minimum number of participants who must have had data at the administered dose(s) and what data (e.g. safety, biomarker, or activity) were reviewed. For instance, escalation decisions could be based on an evaluable population defined as the set of participants who (i) received at least some predetermined amount of the planned doses (e.g. 70%, 85%, or 95%) during the assessment period or (ii) experienced a dose limiting toxicityor dose limiting criteria (see Example 1). For combination EPDF trials, it should be stated whether the predetermined amount for evaluability is based on each individual component in the combination or for the combination as a whole. Information on how unevaluable participants were treated in statistical analysis (e.g. using replacement or best/worst case analysis) and what happened to data collected from participants later found to be ineligible should also be provided (see Example 1). These definitions should also be provided for any interim analysis population(s).

---

### Non-inferiority trials: a systematic review on methodological quality and reporting standards [^fb7e8c65]. Journal of General Internal Medicine (2024). Medium credibility.

While missing outcome data in any trial may be inevitable, its impact may compromise the integrity and interpretability of NI trials even more because of reduced statistical power, increased Type 2 error rates and the potential of biased estimates of the treatment effect size.

Prior studies demonstrated that up to 42% of NI trials had missing outcome data. While our study demonstrated improved handling of missing outcome data from prior studies and between both years, up to one third of NI trials published in 2019 were found to have high or probably high risk of bias related to missing outcome data. Per CONSORT, it is imperative for trials to clearly report if imputation methods are used to handle missing data. Additionally, trials should specify what method is used as well as any assumptions made to minimize biases that may threaten the validity, and more importantly, the conclusions of such trials.

Notably, the primary outcome of the trials in our study changed only slightly between both years, with approximately three-fourths of the studies evaluated concluding NI. Prior studies have demonstrated variable rates of concluding NI, ranging from 48–90%, which may be attributed to such studies being limited to certain specialties or investigating specific interventions. Of note, similar studies have demonstrated that despite the majority of NI trials concluding NI, up to one third of such trials reported potentially misleading conclusions, including superiority or inferiority, due to a discordance between the results presented and the conclusions provided by NI trial investigators, based on FDA recommendations. Thus, journals that publish NI trials must examine for inadequate trial methods that can result in such limitations and potential risks of bias, and ensure they are noted for the audience, regardless of the journal impact factor.

When designing and conducting NI trials, it is imperative to provide justification for the NI trial design, which is often based on the novel therapy offering a benefit or advantage over the standard of care such as improved safety, lower cost or greater convenience. Our study demonstrated improved reporting of the benefit of the new therapy to justify a NI trial design among trials published in 2019 compared to those published in 2014, which was justified based on the intervention having improved safety compared with standard treatment. Similar studies either did not evaluate this or reported the benefit of the new therapy in a generalized manner, such as "improved patient outcomes". Explicit justification for the NI trial design is warranted to provide clinicians with an understanding of what advantages the novel therapy may offer to account for the proposed loss of efficacy in improving patient outcomes.

---

### Comparison of clinical trial changes in primary outcome and reported intervention effect size between trial registration and publication [^7d46135d]. JAMA Network Open (2019). High credibility.

Subgroup Analyses

For multicenter trials, the ROR between changed and unchanged primary outcomes was 0.83 (95% CI, 0.72–0.96). For studies assessing continuous outcomes, the corresponding result was 0.74 (95% CI, 0.57–0.94). For trials using a superiority study design, the ROR between changed and unchanged primary outcomes was 0.82 (95% CI, 0.71–0.94). Overestimation of the reported intervention effect size among trials with primary outcome change could be observed among other subgroups, although they did not all reach statistical significance. In addition, there was no evidence of an interaction between different trial characteristics (eg, study design, multiple centers or single center, and type of outcome) and the estimated intervention effects (Figure 2).

Figure 2.
Subgroup Analyses by Various Study Characteristics

ITT indicates intent to treat; mITT, modified intent to treat; and ROR, ratio of odds ratios.

a Different outcomes could be observed within the same trial.

---

### CONSORT-DEFINE explanation and elaboration: recommendations for enhancing reporting quality and impact of early phase dose-finding clinical trials [^4b960ed1]. EClinicalMedicine (2025). Medium credibility.

Example 1

"Sample sizes were based on clinical considerations (estimated number required to provide safety, tolerability, and pharmacological information and to minimize exposure to healthy subjects at each dose level) rather than statistical considerations. For study 1, the required total sample size was approximately 40 subjects (10 per cohort); for study 2, the required total sample size was approximately 80 subjects (10 per cohort). In study 1, a sample size of 6 subjects was sufficient to provide > 90% power to detect a food effect-related 2-fold increase of maximum observed concentration (Cmax) or area under the concentration–time profile curve (AUC) and 80% power to detect a 1.6-fold increase in both PK [pharmacokinetics] parameters for all doses except 1000 mg, assuming that the predicted within-subject PK variability would not change with food intake".

Example 2

"The single-dose portion of the study was exploratory in nature, and therefore, no formal statistical tests were used to select sample size.… The sample size for the repeat-dose cohort was determined based on a noninformative Bayesian analysis of the percentage lean body mass change from baseline as measured by DXA [dual-energy X-ray absorptiometry]. The repeat-dose cohort was limited to 15 subjects, 9 receiving domagrozumab and 6 receiving placebo, and the sample size was determined sufficient to support a preliminary assessment in percentage lean body mass change from baseline by DXA. Under the Bayesian decision rule, with 9 subjects receiving domagrozumab and 6 subjects receiving placebo, and assuming a 5% standard deviation, the probabilities of declaring statistical difference were 27% without a treatment difference from placebo; 70% with a 3% difference from placebo; and 90% with a 5% difference from placebo. This was a secondary objective for the repeat-dose cohort".

---

### Empirical consequences of current recommendations for the design and interpretation of noninferiority trials [^d27d08bd]. Journal of General Internal Medicine (2018). Low credibility.

Background

Noninferiority trials are increasingly common, though they have less standardized designs and their interpretation is less familiar to clinicians than superiority trials.

Objective

To empirically evaluate a cohort of noninferiority trials to determine 1) their interpretation as recommended by CONSORT, 2) choice of alpha threshold and its sidedness, and 3) differences between methods of analysis such as intention-to-treat and per-protocol.

Design

We searched MEDLINE for parallel-group randomized controlled noninferiority trials published in the five highest-impact general medical journals between 2011 and 2016.

Main Measures

Data abstracted included trial design parameters, results, and interpretation of results based on CONSORT recommendations.

Key Results

One hundred sixty-three trials and 182 noninferiority comparisons were included in our analysis. Based on CONSORT-recommended interpretation, 79% of experimental therapies met criteria for noninferiority, 13% met criteria for superiority, 20% were declared inconclusive, and 2% met criteria for inferiority. However, for 12% of trials, the experimental therapy was statistically significantly worse than the active control, but CONSORT recommended an interpretation of inconclusive or noninferior. A two-sided alpha equivalent of greater than 0.05 was used in 34% of the trials, and in five of these trials, the use of a standard two-sided alpha of 0.05 led to changes in the interpretation of results that disfavored the experimental therapy. In four of the five comparisons where different methods of analysis (e.g., intention-to-treat and per-protocol) yielded different results, the intention-to-treat analysis was the more conservative. In 11% of trials, a secondary advantage of the new therapy was neither reported nor could it be inferred by reviewers.

Conclusions

In this cohort, the design and interpretation of noninferiority trials led to significant and systematic bias in favor of the experimental therapy. Clinicians should exercise caution when interpreting these trials. Future trials may be more reliable if design parameters are standardized.

---

### A guide to appropriately planning and conducting meta-analyses: part 2-effect size estimation, heterogeneity and analytic approaches [^1f3a9cc9]. Knee Surgery, Sports Traumatology, Arthroscopy (2023). Medium credibility.

Meta-analyses by definition are a subtype of systematic review intended to quantitatively assess the strength of evidence present on an intervention or treatment. Such analyses may use individual-level data or aggregate data to produce a point estimate of an effect, also known as the combined effect, and measure precision of the calculated estimate. The current article will review several important considerations during the analytic phase of a meta-analysis, including selection of effect estimators, heterogeneity and various sub-types of meta-analytic approaches.

---

### Current practice in analysing and reporting binary outcome data-a review of randomised controlled trial reports [^2e63db45]. BMC Medicine (2020). Medium credibility.

Methods that allow quantification of the treatment effect and related uncertainty are readily available, such as the calculation for the unadjusted odds ratio and its CI and logistic regression for the adjusted odds ratio.

Reporting the magnitude of the treatment effect is vital for communicating the trial findings in a meaningful, transparent way to all stakeholders, particularly patients and members of the public. Just identifying a statistically significant difference is not enough to confirm action, and finding the absence of evidence of a statistical difference is not enough to conclude no difference between treatments. The crude dichotomisation of findings has thus been heavily criticised, and alternative approaches and remedies suggested. Perhaps, the most successful initiative in this area has been driven by journals like the BMJ and others, who early on promoted reporting the uncertainty around estimates (typically with a CI). However, trial reports often still do not state an estimate of the treatment effect and/or quantify the uncertainty around the estimate. Presumably, many researchers feel the reader can calculate the uncertainty themselves by looking at the observed event rate in each group. However, this stance is not acceptable in our view, given the importance of this value as the main study finding.

Different audiences may be more used to interpreting relative or absolute risk, although effects tend to be overestimated when presented as relative risks. The practicalities of calculating both relative and absolute treatment effects are surprisingly complex. The statistical methods typically used directly calculate one or the other. Some researchers may then believe that they need to use different statistical methods to be able to report both relative and absolute treatment effects, which was the approach chosen in all studies that presented both relative and absolute treatment effect estimates. Performing more analyses raises multiplicity issues: the chance of obtaining spurious significant results increases with the number of tests performed, as does the potential for selecting the more favourable results if the principal analysis is not pre-specified. Recent work on sample size calculations has highlighted the need to clarify what the target difference is when designing the trial.

---

### Reporting of sample size calculation in randomised controlled trials: review [^5fdef88b]. BMJ (2009). Excellent credibility.

Introduction

The importance of sample size determination in randomised controlled trials has been widely asserted, and according to the CONSORT statement these calculations must be reported and justified in published articles. The aim of an a priori sample size calculation is mainly to determinate the number of participants needed to detect a clinically relevant treatment effect. Some have asserted that oversized trials, which expose too many people to the new therapy, or underpowered trials, which may fail to achieve significant results, should be avoided.

The usual conventional approach is to calculate sample size with four parameters: type I error, power, assumptions in the control group (response rate and standard deviation), and expected treatment effect. Type I error and power are usually fixed at conventional levels (5% for type I error, 80% or 90% for power). Assumptions related to the control group are often pre-specified on the basis of previously observed data or published results, and the expected treatment effect is expected to be hypothesised as a clinically meaningful effect. The uncertainty related to the rate of events or the standard deviation in the control groupand to treatment effect could lead to lower than intended power.

We aimed to assess the quality of reporting sample size calculation in published reports of randomised controlled trials, the accuracy of the calculations, and the accuracy of the a priori assumptions.

---

### Evidence-based dermatology: number needed to treat and its relation to other risk measures [^2cd79733]. Journal of the American Academy of Dermatology (2007). Low credibility.

When discussing treatment options with patients, clinicians often use terms such as "frequently" or "rarely" when referring to potential benefits or possible harms. Quantitative measurements of treatment benefits and harms derived from randomized clinical trials or meta-analysis such as odds ratios or risk reduction are more precise terms, yet physicians and their patients find them difficult to understand and they are not, therefore, commonly used in clinical practice. To overcome the lack of intuitiveness for traditional measures of risk estimates derived from clinical trials, the number needed to treat (NNT) has been widely recommended as a measure of effectiveness, and number needed to harm as a way of describing risk of possible adverse events. NNT is simply the number of patients who, on average, would need to be treated with a proposed intervention to demonstrate one additional gain over the standard comparator intervention. NNT is an absolute measure and it is calculated as the inverse of the absolute risk reduction. In this article we describe the usefulness and limits of the NNT with particular reference to dermatology, and compare NNT with other relative measures such as the relative risk and relative risk reduction.

---

### Statistical controversies in clinical research: statistical significance-too much of a good thing… [^85dc04c6]. Annals of Oncology (2016). Low credibility.

The use and interpretation of P values is a matter of debate in applied research. We argue that P values are useful as a pragmatic guide to interpret the results of a clinical trial, not as a strict binary boundary that separates real treatment effects from lack thereof. We illustrate our point using the result of BOLERO-1, a randomized, double-blind trial evaluating the efficacy and safety of adding everolimus to trastuzumab and paclitaxel as first-line therapy for HER2+ advanced breast cancer. In this trial, the benefit of everolimus was seen only in the predefined subset of patients with hormone receptor-negative breast cancer at baseline (progression-free survival hazard ratio = 0.66, P = 0.0049). A strict interpretation of this finding, based on complex 'alpha splitting' rules to assess statistical significance, led to the conclusion that the benefit of everolimus was not statistically significant either overall or in the subset. We contend that this interpretation does not do justice to the data, and we argue that the benefit of everolimus in hormone receptor-negative breast cancer is both statistically compelling and clinically relevant.

---

### CONSORT 2025 explanation and elaboration: updated guideline for reporting randomised trials [^af2fc39e]. BMJ (2025). Excellent credibility.

Explanation

Sensitivity analyses can be important additional analyses to examine the robustness of the primary trial results under a range of assumptions about the data, methods, and models that differ from those of the primary analysis. When the findings from a sensitivity analysis are consistent with the primary trial findings, trialists can be confident that any assumptions in the primary analysis had little impact — strengthening the trial results. Morris and colleagues provide a principled approach to guide any sensitivity analyses by posing three questions to trialists: does the proposed sensitivity analysis address the same question as the primary analysis; is it possible for the proposed sensitivity analysis to return a different result to the primary analysis; and if the results do differ, is there any uncertainty as to which will be believed.

Subgroup analyses are another set of additional analyses that are widely carried out and reported. Here, the focus is on those analyses that look for evidence of a difference in treatment effect in complementary subgroups (eg, older and younger participants), a comparison known as a test of interaction. Empirical analyses of subgroup difference claims for factors such as age, sex, race, ethnicity, and other factors show selective reporting, frequent lack of proper statistical support, and poor independent corroboration.

A common but misleading approach is to compare P values for separate analyses of the treatment effect in each group. Categorising continuous variables to create subgroups is often done for simplicity and because it is perceived as easier to understand and communicate. Major limitations of the approach include the splitting of a continuous variable into discrete subgroups by arbitrarily chosen cut-off points that lack clinical or biological plausibility, which loses information, and thus reduces statistical power. Choosing cut-off points based on achieving statistical significance should be avoided. It is incorrect to infer a subgroup effect (interaction) from one significant (in one subgroup) and one non-significant P value (in another subgroup). The rationale for any subgroups should be outlined (including how they are defined), along with whether the subgroups were specified a priori in the protocol or statistical analysis plan or were done post hoc. Because of the high risk for spurious findings, subgroup analyses are often discouraged. Post hoc subgroup comparisons (analyses done after looking at the data) are especially likely not to be confirmed by further studies. Most of these analyses do not have substantial credibility.

---

### AGA clinical practice guideline on the prevention and treatment of hepatitis B virus reactivation in at-risk individuals [^17b08113]. Gastroenterology (2025). High credibility.

HBVr prevention — certainty assessment and handling of baseline risk precision: Risk of bias in RCTs was assessed using the Cochrane risk-of-bias tool for randomized trials, version 2, and certainty of the evidence was assessed using the GRADE approach with results summarized in evidence profiles. The GRADE domains considered were risk of bias, inconsistency in effect estimates, indirectness, imprecision of effect estimates, risk of publication bias, presence or absence of a large effect estimate, and the potential impact of residual confounding. When baseline risk was estimated from a low event rate or relatively limited sample size, if the 95% CI of the baseline risk (modeled worst-case scenario) crossed clinically important thresholds defining low-, moderate-, and high-risk categories, this was considered to impact precision of the baseline risk estimate.

---

### 2018 Infectious Diseases Society of America clinical practice guideline for the management of outpatient parenteral antimicrobial therapy [^fe29a7b0]. Clinical Infectious Diseases (2019). High credibility.

OPAT data synthesis and statistical significance: The evidence was synthesized using strength-of-evidence tables, and due to the heterogeneity and small number of included studies in each individual evidence table, data were not combined quantitatively using metaanalyses. For each question, clinically relevant outcomes were sought; when available from individual studies, point estimates, confidence intervals, and P values were reviewed, corrected values were used if errors were found, and P values of < .05 were considered statistically significant.

---

### CONSORT 2025 explanation and elaboration: updated guideline for reporting randomised trials [^d1b68950]. BMJ (2025). Excellent credibility.

Some trials may use bayesian methods. In this case, the choices of priors, computational decisions, and any modelling methods used should be described. Most bayesian trials so far have been for early phases of drug development, but this approach can be applicable to any phase. Typically, results are presented as treatment effects along with credible intervals.

Where an analysis lacks statistical power (eg, harms outcomes), authors may prefer descriptive approaches over formal statistical analysis.

While the necessity for covariate adjustments is generally reduced in randomised trials compared with epidemiological studies, considering an adjusted analysis can have value in terms of increased power and precision, particularly if there is an indication that one or more variables may have prognostic value. It is preferable for adjusted analyses to be explicitly outlined in the study protocol (item 3). For instance, it is often advisable to make adjustments for stratification variables, in keeping with the principle that the analysis strategy should align with the study design. In the context of randomised trials, the decision to make adjustments should not be based on whether there are baseline covariates that are statistically significantly different between randomised groups. The testing of baseline imbalance in covariates should be avoided, as if randomisation is properly conducted, then by definition, any differences in baseline covariates between treatment arms are random. The rationale for any adjusted analyses and the statistical methods used should be specified, along with clarifying the choice of covariates that were adjusted for, indicating how continuous variables were handled (eg, linear, modelled with splines), and specifying whether the analysis was planned or post hoc. Reviews of published studies show that reporting of adjusted analyses is inadequate with regard to all of these aspects.

Multiplicity issues are prevalent in trials and merit special consideration, especially in cases involving multiple primary outcomes, multiple time points stemming from repeated assessments of an outcome, multiple planned analyses for an outcome (such as interim or subgroup analyses (item 21d)), or analyses of numerous secondary outcomes (see CONSORT outcomes extension for more details). Any methods used to mitigate or account for multiplicity should be described. If no methods have been used to account for multiplicity (eg, not applicable, or not considered), then this should also be reported, particularly when a large number of analyses has been carried out.

Item 21b: Definition of who is included in each analysis (eg, all randomised participants), and in which group

---

### Statistical significance and clinical relevance… [^c125e947]. JAMA Network (2004). Excellent credibility.

We will discuss statistical power by examining studies too small to identify important differences, studies so large as to identify differences that are not clinically significant, difficult-to-design studies without very large patient populations, and those studies with both adequate power and clinically relevant findings. Dermatologists should not focus on small P values alone to decide whether a treatment is clinically useful; it is essential to consider the magnitude of treatment differences and the power of the study. Was the study large enough to draw these conclusions. Mometasone treatment sites did improve from the baseline scores more than the sites treated with hydrocortisone. 6 After 1 week, mean improvement percentage in the mometasone-treated lesions was 45% compared with the 32% mean improvement percentage for hydrocortisone (P. However, the results of this study, 8 while statistically significant, lack much clinical relevance.

The power of this clinical trial was adequate to detect a difference of as little as 15% between the efficacies of the active drug and the placebo. The observed differences of 12% to 15% in the efficacy of penciclovir and placebo were of this order. This study, by using a very large sample size, detected a difference so small that it is probably not of much clinical benefit to patients. This is an instance of a study using too high a power, thus allowing the detection of a very slight difference. The statistical methods used in this trial were sound, with a sample size of at least 43 in each of the study's 4 arms. 13 The authors assumed an effective rate in the vehicle group and the lowest-concentration tacrolimus group to be 50% and 80%, respectively. Given this assumption, a sample size of 40 patients per group was necessary to have an 80% chance to detect a statistically significant difference.

The marked differences in mean percentage improvement and in pruritus in the treatment group show clear benefit to the patient. A CI of 1. 001–1. 002, indicates that a statistically significant difference was found but that the magnitude of this difference was so small that it would not likely be clinically significant. A CI of 3 to 10 would indicate both a statistically and a clinically meaningful difference. Finally, a CI of 0. 8 to 10 indicates that no statistically significant difference was observed but that the power of the study was not sufficient to rule out a rather large difference between drug and placebo.

---

### Empirical consequences of current recommendations for the design and interpretation of noninferiority trials [^cde3cbfd]. Journal of General Internal Medicine (2018). Low credibility.

DISCUSSION

In our study of 163 noninferiority trials with 182 noninferiority comparisons published in the five highest-impact general medicine journals during a recent 5-year period, we found that current interpretive recommendations lead to significant systematic and directional biases in the analysis and interpretation of noninferiority trials which almost always favor the experimental therapy. To our knowledge, our analysis is the first to abstract raw data, calculate confidence intervals, categorize the results based on CONSORT, and explore whether and how deviation from the customary design parameters of superiority trials impacts the results and conclusions of noninferiority trials. This is important, since decisions about the use of new, often more expensive therapies increasingly rely on data from noninferiority trials. Most previous analyses of such trials have focused on quality and completeness of reporting of design parameters and results. We found that statistical design parameters (excluding justification for delta which is perennially deficient) and results were reported with near 100% completeness in the period we studied. However, despite nearly complete reporting, we found that the interpretation of the results of noninferiority trials can hinge critically on both the choice of design parameters and the method of drawing formal conclusions from the results. In our cohort, the use of CONSORT-recommended interpretation with 95% confidence intervals concealed statistically significant results that disfavored the NT in a substantial number of cases. The use of one-sided confidence intervals also concealed statistically significant results disfavoring the NT in a small number of comparisons. Contrary to the recommended preferential use of PP or AT analyses over ITT analyses, we found that the method of analysis seldom affected the results, and when it did, the ITT analysis was more conservative in four out of five trials. While the issues we describe each affect a minority of the trials we analyzed, the collective effect is substantial.

---

### Guidelines for reporting outcomes in trial reports: the CONSORT-outcomes 2022 extension [^5a610e0c]. JAMA (2022). Excellent credibility.

Importance

Clinicians, patients, and policy makers rely on published results from clinical trials to help make evidence-informed decisions. To critically evaluate and use trial results, readers require complete and transparent information regarding what was planned, done, and found. Specific and harmonized guidance as to what outcome-specific information should be reported in publications of clinical trials is needed to reduce deficient reporting practices that obscure issues with outcome selection, assessment, and analysis.

Objective

To develop harmonized, evidence- and consensus-based standards for reporting outcomes in clinical trial reports through integration with the Consolidated Standards of Reporting Trials (CONSORT) 2010 statement.

Evidence Review

Using the Enhancing the Quality and Transparency of Health Research (EQUATOR) methodological framework, the CONSORT-Outcomes 2022 extension of the CONSORT 2010 statement was developed by (1) generation and evaluation of candidate outcome reporting items via consultation with experts and a scoping review of existing guidance for reporting trial outcomes (published within the 10 years prior to March 19, 2018) identified through expert solicitation, electronic database searches of MEDLINE and the Cochrane Methodology Register, gray literature searches, and reference list searches; (2) a 3-round international Delphi voting process (November 2018-February 2019) completed by 124 panelists from 22 countries to rate and identify additional items; and (3) an in-person consensus meeting (April 9–10, 2019) attended by 25 panelists to identify essential items for the reporting of outcomes in clinical trial reports.

Findings

The scoping review and consultation with experts identified 128 recommendations relevant to reporting outcomes in trial reports, the majority (83%) of which were not included in the CONSORT 2010 statement. All recommendations were consolidated into 64 items for Delphi voting; after the Delphi survey process, 30 items met criteria for further evaluation at the consensus meeting and possible inclusion in the CONSORT-Outcomes 2022 extension. The discussions during and after the consensus meeting yielded 17 items that elaborate on the CONSORT 2010 statement checklist items and are related to completely defining and justifying the trial outcomes, including how and when they were assessed (CONSORT 2010 statement checklist item 6a), defining and justifying the target difference between treatment groups during sample size calculations (CONSORT 2010 statement checklist item 7a), describing the statistical methods used to compare groups for the primary and secondary outcomes (CONSORT 2010 statement checklist item 12a), and describing the prespecified analyses and any outcome analyses not prespecified (CONSORT 2010 statement checklist item 18).

Conclusions and Relevance

This CONSORT-Outcomes 2022 extension of the CONSORT 2010 statement provides 17 outcome-specific items that should be addressed in all published clinical trial reports and may help increase trial utility, replicability, and transparency and may minimize the risk of selective nonreporting of trial results.

---

### External beam radiation therapy for primary liver cancers: an ASTRO clinical practice guideline [^85be03ef]. Practical Radiation Oncology (2021). High credibility.

ASTRO overall quality of evidence (QoE) grades — using randomized controlled trials (RCTs) and observational study designs — are defined and interpreted as follows: High requires "2 or more well-conducted and highly generalizable RCTs or meta-analyses of such trials" and indicates "The true effect is very likely to be close to the estimate of the effect based on the body of evidence". Moderate includes "1 well-conducted and highly generalizable RCT or a meta-analysis of such trials OR 2 or more RCTs with some weaknesses of procedure or generalizability OR 2 or more strong observational studies with consistent findings" with interpretation that "The true effect is likely to be close to the estimate of the effect based on the body of evidence, but it is possible that it is substantially different". Low encompasses "1 RCT with some weaknesses of procedure or generalizability OR 1 or more RCTs with serious deficiencies of procedure or generalizability or extremely small sample sizes OR 2 or more observational studies with inconsistent findings, small sample sizes, or other problems that potentially confound interpretation of data", interpreted as "The true effect may be substantially different from the estimate of the effect. There is a risk that future research may significantly alter the estimate of the effect size or the interpretation of the results". Expert Opinion is "Consensus of the panel based on clinical judgment and experience, due to absence of evidence or limitations in evidence", with "Strong consensus (≥ 90%) of the panel" guiding recommendations, and "A lower quality of evidence, including expert opinion, does not imply that the recommendation is conditional".

---

### Is it time to abandon the use of-values in early phase translational trials: why (effect) size matters [^996244de]. JACC: Basic to Translational Science (2024). Medium credibility.

Among these different methodologies, the SMD is perhaps the easiest method to utilize in small clinical studies that evaluate a variety of different exploratory endpoints. The SMD can be calculated as (mean study drug/device − mean control arm)/standard deviation (SD). An SMD of 0 indicates that the drug/device has equivalent effects to those observed in the control arm, whereas a value > 0 indicates that the drug/device has a beneficial effect when compared with control values, and a value < 0 indicates that a drug/device is worse than no intervention. The inclusion of the SD in the denominator of the equation adjusts for the variability in the measurements in the drug/device and no treatment arms, and standardizes the comparisons of the magnitude of treatment effects, which can be useful when evaluating multiple different exploratory endpoints. In addition to the simplicity of calculating the SMD, there are 3 additional aspects of this methodology that are extremely useful for interpreting the results of exploratory analyses in small phase 1 clinical studies. The first is that the use of effect sizes allows investigators to address the question of whether the magnitude of change between the treatment and control groups is important and clinically meaningful. There are several published guidelines for interpreting effect sizes, of which Cohen's d is the most widely known. Cohen suggested a convention to interpret the magnitude of the effect size, where d = 0.2 is a small effect, d = 0.5 is a moderate effect, and d = 0.8 is a large effect. Cohen originally proposed that a medium-sized effect should represent the average effect size within the field (ie, 50th percentile), with a small effect size associated with the 25th percentile and a large effect size reflecting the 75th percentile. Not surprisingly, subsequent studies have shown that the magnitude of small, medium, and large effects sizes varies from field-to-field, and that Cohen's d can overestimate or underestimate the actual effect size distribution in the published literature. Although a distribution of actual effect sizes has not been assessed for the entire field of cardiovascular medicine, a study that reported on effect sizes and primary outcomes in large cardiovascular-related behavioral randomized clinical trials revealed that effect sizes of the behavioral and physiological outcomes were predominantly in the small (d = 0.2) to medium (d = 0.5) range. However, it should be recognized that clinical significance of an absolute effect size is context-dependent. For example, a small effect size for mortality can make a huge difference for society if a large proportion of the population is affected by the condition (eg, the COVID-19 pandemic). Another useful aspect of reporting effect sizes in small clinical trials is that one can calculate confidence intervals (CIs) around the point estimates for the effect size, which provides a range of plausible values for the variable being measured. There is a close relationship between CIs and statistical significance testing. If the 95% CI for the point estimate of the effect size contains a value that contains the null value of 0, the difference will be nonsignificant if P < 0.05 is used as the cutpoint for determining statistical significance. CIs can also be particularly useful in assessing multiple endpoints in exploratory analyses where statistical methodology for multiplicity of testing has not been implemented. Relevant to the present discussion, if the CIs encompass clinically meaningful differences for a given endpoint but the small sample size yields a nonsignificant result, this can also be informative insofar as it suggests that magnitude of change in the endpoint may be clinically relevant. Last, effect sizes can also be used to perform power calculations for larger phase 2 to 3 clinical trials.

---

### Comparative ocular outcomes of tirzepatide versus other anti-obesity medications in people with obesity [^114303d4]. Communications Medicine (2025). Medium credibility.

Our statistical framework employed Bonferroni correction across six primary ocular outcomes, yielding an adjusted significance threshold of p < 0.0083. While this conservative approach controls family-wise error rate, we recognize the importance of evaluating effect magnitude and confidence intervals alongside significance testing. Results approaching our adjusted threshold require nuanced interpretation beyond binary significance classification. For example, Tirzepatide's association with reduced cataract incidence versus Naltrexone/Bupropion (HR 0.46, 95% CI: 0.23–0.92, p = 0.025) demonstrates both statistical significance and substantial effect size, with the confidence interval excluding unity while encompassing clinically meaningful reductions. The consistency of effect directions across sensitivity analyses provides additional confidence in our findings, independent of individual p -value thresholds. This multi-dimensional analytical approach acknowledges that statistical significance testing represents one component of evidence evaluation, particularly in observational studies where effect size and precision estimates offer essential clinical context. Future research with advanced imaging and extended follow-up may better elucidate long-term ocular effects of these medications. This study offers important strengths, including large sample sizes and real-world TriNetX data that enhance generalizability. The broad range of ocular outcomes examined provides a thorough assessment of anti-obesity medications' ophthalmic effects. Time-varying hazard ratios and Kaplan–Meier curves captured temporal treatment dynamics. These findings carry clinical significance for obesity management. As obesity prevalence rises alongside ocular complications, anti-obesity medications offering both metabolic and ocular benefits may reduce eye disease burden in affected populations. In conclusion, Tirzepatide and Semaglutide demonstrate favorable ocular safety profiles compared to traditional anti-obesity treatments. These results support incorporating ocular considerations into obesity management decisions and indicate that newer incretin-based therapies may offer benefits beyond metabolic effects.

---

### Enhancing interpretability of cancer clinical trial results… [^df99fde5]. JAMA Network (2017). Excellent credibility.

For the illustrative example, the HR procedure indicates that nivolumab significantly prolonged patient OS and was numerically better than docetaxel for PFS. However, the median PFS time of docetaxel was significantly better than that of nivolumab. Therefore, it may be difficult to use median OS and/or PFS to interpret of the HR value clinically. On the other hand, using RMST difference, nivolumab was significantly better than docetaxel for both OS and PFS. We also provide details regarding design of a future study with RMST-based measures. Conclusions and Relevance The design and analysis of a conventional cancer clinical trial can be improved by adopting a robust statistical procedure that enables clinically meaningful interpretations of the treatment effect. The RMST-based quantitative method may be used as a primary tool for future cancer trials or to help us to better understand the clinical interpretation of the HR even when its model assumption is plausible.

A natural summary measure of the treatment effect would be the difference or ratio in 2 median OS times. The sample size estimate for the study could then be based on the desired precision of such a difference or based on the desired statistical power for the corresponding test to detect a significant difference between groups. For docetaxel, RMST estimate is 11. 3 months. The difference in RMST is 1. 7 months in favor of nivolumab. This conclusion is statistically consistent with that from the HR or log-rank test. Graphically this difference is represented by the area between 2 Kaplan-Meier curves in Figure 1A. Note that the standard error estimate for the RMST estimate is obtained without any model assumption in contrast to others proposed in the literature.
9. For PFS, the difference of RMSTs is

1. 3 months, which is also significantly in favor of nivolumab.

This result is consistent with the observed HR less than 1. If the power is less than 90%, we then increase the sample size n and repeat the above process until the empirical power reaches the target level. This results in a total of 336 patients to obtain 90% power to detect a 3-month difference in RMST.

---

### Estimating treatment effects for individual patients based on the results of randomised clinical trials [^e52f59ee]. BMJ (2011). Excellent credibility.

Introduction

Usually the results of trials are implemented in clinical practice by either treating all patients (in the case of a positive trial result) or treating no one (in the case of a negative trial result), expecting the treatment effect for every patient to be similar to the average treatment effect in the original trial. Clinicians intuitively know that this idea is oversimplified because in reality some patients benefit more than average from treatment, whereas others do not or may even be harmed.

The direct translation of trial results to individual patients in clinical practice is, however, complicated by some important limitations. The treatment effects of randomised trials are typically expressed in terms of relative risks or hazard ratios at a group level — that is, treatment versus control. Yet treatment that is associated with a considerable reduction in relative risk will still result in a modest absolute effect when the incidence rate of the disease is low. Absolute risk reduction is usually more informative because it combines the relative risk reduction and the incidence rate of the disease outcome. The absolute risk reduction is sometimes expressed in trial reports as the number needed to treat (NNT). Still, implicit in the use of estimates at group level is that all patients are at average risk and all have the same likelihood of response to treatment. Usually at least one of these two assumptions is untrue because the expected absolute risk reduction resulting from treatment often depends on the characteristics of individual patients.

---

### Current practice in analysing and reporting binary outcome data-a review of randomised controlled trial reports [^e3db8c97]. BMC Medicine (2020). Medium credibility.

Only 8% of the included studies reported both an absolute and relative treatment effect with corresponding CIs, as recommended by the CONSORT guidelines. Similarly, only 10% reported the statistical analysis method used to analyse the primary outcome in the abstract. If the CONSORT guidelines are rarely followed in these regards, either adherence should be more strongly encouraged by journals and peer reviewers, or they should be relaxed. Although it may be helpful to present both relative and absolute treatment effects, providing estimates of the event rates by treatment arm with either type of treatment effect estimate can arguably also convey the full picture of the intervention without requiring two formal tests. Most statistical analysis methods implicitly or explicitly assess a relative treatment effect. They tend to be more precise in our experience, at least in terms of detecting a statistically significant difference, than methods that assess absolute effects. When interpreting relative treatment effects, researchers should bear in mind that odds ratios tend to be higher than risk ratios (in some cases substantially so), and thus, for any relative effect bearing in mind the anticipated control risk rate is critical.

The extent of missing data was generally insufficiently described, leading to a lack of clarity regarding how much outcome data were missing, and hence how reliable the results were. When data were missing, appropriate sensitivity analyses based on varying assumptions about the missing data mechanism, including worst-case scenarios, were rarely performed. Sensitivity analyses for missing data are particularly important for binary outcomes as even small changes in the numbers of events by treatment arm can change the treatment estimates or even the trial conclusions.

The large number of recently published clinical trial reports in a range of journals and clinical areas reviewed is a strength of the methodology used. The papers included are therefore representative of current practice and show that there are problems in published reports across journals and irrespective of markers of journal prestige (e.g. higher impact factors). While there is some indication that treatment effects are more likely to be reported in higher-impact journals, there was room for improvement in terms of reporting of statistical methods, treatment effect estimates, and missing data across the board. Our study was not designed in order to be able to look at the difference between particular subgroups, such as disease area or source of funding. Further research should investigate if there are differences in the analysis and reporting of binary outcomes by study characteristics so that focus and any strategies for improving reporting can be directed where they may be needed most.

---

### A user guide to the American Society of Hematology clinical practice guidelines [^38a45b24]. Blood Advances (2020). High credibility.

Summary of findings (SoF) tables — content and example metrics are described for decision-making, including outcomes prioritized by the panel and potential benefits and harms, certainty in the evidence (GRADE), and effect measures. Relative effect with 95% confidence interval is exemplified for a recurrent major VTE outcome as RR 0.39 (0.21–0.72). Anticipated absolute effects are presented as an estimated risk in the control group of 42 per 1000 and an intervention risk expressed as an absolute difference of 26 fewer per 1000 (33 fewer to 12 fewer).

---

### Development process of a consensus-driven CONSORT extension for randomised trials using an adaptive design [^33ff5d74]. BMC Medicine (2018). Low credibility.

Box 2 Themes that may influence reporting of ADs

1. The risk of introducing operational bias into the conduct of the trial increases when interim data are reviewed.

2. Performing multiple hypotheses tests increases the risk of making inappropriate or unjustified claims about the treatment effect if inappropriate statistical methods are used. This occurs for example when conducting interim analyses; evaluating multiple patient subgroups, treatments, or endpoints; or a combination of these.

3. Not addressing planned adaptations and unplanned changes may potentially invalidate results depending on their form, purpose, and the statistical methods used.

4. Biased estimates of the treatment effect may be produced if inappropriate statistical methods are used for analysis.

5. The risk of making undesirable trial adaptation decisions based on premature or inadequate interim data.

6. The type of adaptive design, form of trial adaptations, and interim adaptation decision rules used may influence the acceptability of results, level of information to disclose, and the applicable statistical methods.

7. Unintended changes in patient characteristics or the estimand (what is to be estimated) before and after trial adaptation may occur, making overall results difficult to interpret.

8. The need for more transparency to enable research consumers to evaluate the appropriateness of the methods, sources of potential bias, and interpretability and trustworthiness of the trial results, as well as to enable other researchers to reproduce trial-related aspects.

---

### Adverse events associated with ERCP [^c70c654e]. Gastrointestinal Endoscopy (2017). Medium credibility.

GRADE system — quality of evidence definitions for guidelines are as follows: High quality means further research is very unlikely to change our confidence in the estimate of effect; moderate quality means further research is likely to have an important impact on our confidence in the estimate of effect and may change the estimate; low quality means further research is very likely to have an important impact on our confidence in the estimate of effect and is likely to change the estimate; and very low quality means any estimate of effect is very uncertain.

---

### Review of current controversial issues in clinical trials [^ba2e8fbd]. General Psychiatry (2021). Medium credibility.

In clinical development, adequate and well-controlled randomised clinical trials are usually conducted to evaluate the safety and efficacy of test treatment under investigation. The purpose is to ensure that there is an accurate and reliable assessment of test treatment under study. In practice, however, some controversial issues inevitably appear despite the compliance of good clinical practice. These debatable issues include, but are not limited to, (1) appropriateness of hypotheses for clinical investigation, (2) feasibility of power calculation for sample size requirement, (3) integrity of randomisation/blinding, (4) strategy for clinical endpoint selection, (5) demonstrating effectiveness or ineffectiveness, (6) impact of protocol amendments and (7) independence of independent data monitoring committee. In this article, these controversial issues are discussed. The impact of these issues in evaluating the safety and efficacy of the test treatment under investigation is also assessed. Recommendations regarding possible resolutions to these issues are provided whenever possible.

---

### Understanding clinical significance in rehabilitation: a primer for researchers and clinicians [^953b8c0a]. American Journal of Physical Medicine & Rehabilitation (2022). Medium credibility.

Abstract

The objective of this review was to summarize the concept of clinical significance and associated methods focusing on the area of rehabilitation to provide a resource to rehabilitation researchers and clinicians. Studies were searched on electronic databases from inception until July 28, 2020, with no date or language limits. Manual searches as well as Scopus forward citation for relevant references were performed. Narrative synthesis of study results was performed. Definitions of the concepts related to clinical significance, ways of calculating, and interpreting each method were provided using rehabilitation examples. An explanation of methods to evaluate clinical significance (distribution, anchor, and opinion-based methods) and their advantages and disadvantages were also provided. Considering the limitations of statistical significance in assuring meaningfulness of results, clinical interpretation of research outcomes and the report of clinical significance in intervention trials should be a priority in rehabilitation research. When possible, the use of multiple methods (distribution, anchor, and opinion based) is recommended. Thus, clinical researchers are encouraged to present results in a manner that allow the rehabilitation professionals to easily interpret and implement those results in their clinical practice.