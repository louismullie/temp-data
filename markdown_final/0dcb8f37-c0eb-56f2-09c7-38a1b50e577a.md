# Medical Question & Answer

**Sample ID**: 0dcb8f37-c0eb-56f2-09c7-38a1b50e577a
**Dataset Index**: 359296

---

## Question

30. Consider a triangle ABC, where AB = 20, BC = 25 and CA = 17. P is a point on the plane. What is the minimum value of 2 × PA + 3 × PB + 5 × PC?

---

## Answer

> Let's see… What do we have here? The user is asking for the minimum value of the expression (2PA + 3PB + 5PC), where (P) is a point in the plane and (A), (B), and (C) are the vertices of a triangle with side lengths (AB = 20), (BC = 25), and (CA = 17). Let's break this down step-by-step. First, I need to think about the geometric setup and whether the triangle inequality holds. Then, I should verify the necessary and sufficient condition for the minimum to occur at a vertex. Next, I will compute the expression at each vertex and identify the smallest value. Finally, I will confirm that no interior point can yield a smaller value and state the answer with a concise justification.

> Let me first confirm the triangle geometry. The side lengths are (AB = 20), (BC = 25), and (CA = 17). I should check the triangle inequalities: (20 + 25 > 17), (20 + 17 > 25), and (25 + 17 > 20), which are all true, so a valid triangle exists and the problem is well-posed [^notfound].

> Now, I need to think about the general theory for minimizing a weighted sum of distances to vertices. For positive weights (w_A), (w_B), (w_C), the function (f(P) = w_A PA + w_B PB + w_C PC) is convex, so its minimum occurs either at a unique interior point (the weighted Fermat point) or at a vertex if the weights are sufficiently skewed. The key condition is that the minimum occurs at a vertex if and only if that vertex's weight is at least as large as the sum of the other two weights, i.e., (w_i ≥ w_j + w_k) for some permutation of the vertices [^notfound].

> Wait, let me verify the condition precisely. The necessary and sufficient condition for the minimum to occur at vertex (C) is (w_C ≥ w_A + w_B). Here, the weights are (w_A = 2), (w_B = 3), and (w_C = 5), so (5 ≥ 2 + 3 = 5), which is satisfied. Therefore, the minimum occurs at vertex (C) [^notfound].

> I should double-check the other vertices to ensure none of them satisfy the same condition. For vertex (A), (2 ≥ 3 + 5 = 8) is false; for vertex (B), (3 ≥ 2 + 5 = 7) is also false. So only vertex (C) meets the criterion, confirming that the global minimum is at (C) [^notfound].

> Next, I will compute the value of the expression at vertex (C). At (P = C), we have (PA = CA = 17), (PB = CB = 25), and (PC = 0), so the expression becomes (2 × 17 + 3 × 25 + 5 × 0 = 34 + 75 + 0 = 109). Hold on, I should verify the arithmetic: (34 + 75) is indeed (109), so the minimum value is (109) [^notfound].

> But wait, what if I mistakenly assumed the minimum could be smaller at an interior point? Let me reconsider. Because the weights satisfy the vertex condition at (C), the convex function attains its minimum at (C) and no interior point can yield a smaller value, so the result is robust to that concern [^notfound].

> Final answer: The minimum value of (2PA + 3PB + 5PC) is (109), achieved when (P) coincides with vertex (C).

---

The minimum value of (2PA + 3PB + 5PC) is **109**, achieved when (P) coincides with vertex (C). This follows from the triangle inequality and the fact that the largest weight (5) equals the sum of the other two weights (2 + 3), so the minimum occurs at (C).

---

## Geometric interpretation and strategy

We need to minimize the weighted sum of distances from a point (P) to the vertices of triangle (ABC). The weights are (w_A = 2), (w_B = 3), and (w_C = 5). The key insight is that when one weight is at least the sum of the other two, the minimum occurs at the vertex with the largest weight. Here, (w_C = 5 ≥ w_A + w_B = 5), so the minimum is at (C).

---

## Verification using the triangle inequality

By the triangle inequality, for any point (P):

- (PA + PC ≥ AC = 17)
- (PB + PC ≥ BC = 25)

Multiply the first by 2 and the second by 3:

- (2PA + 2PC ≥ 34)
- (3PB + 3PC ≥ 75)

Add these inequalities:

2PA + 3PB + 5PC ≥ 109

Equality holds when (P = C), because then (PA = AC = 17), (PB = BC = 25), and (PC = 0), yielding exactly 109.

---

## Conclusion

The minimum value of (2PA + 3PB + 5PC) is **109**, achieved when (P) is at vertex (C).

---

## References

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^efb30806]. Journal of the American College of Cardiology (2025). High credibility.

Structure of economic value statements in clinical guidelines — In the interest of consistency and comparability across guidelines, we suggest that economic value statements report 3 components: 1) Class of Recommendation; 2) Level of Certainty; and 3) Recommendation, and the economic value statement should ideally be placed immediately following the clinical recommendation for the intervention of interest.

---

### Knowledge-based commissioning [^3976e509]. Otolaryngology — Head and Neck Surgery (2007). Low credibility.

People who pay for health care aim to maximize value in allocating resources between and within patient groups. With limited health care budgets, new high value innovations will replace, not supplement, lower value interventions. Every society and its health care system must consider the issue of value and be aware that the values of payer, provider, clinician, and patient will always be different. Knowledge, while necessary, is never sufficient when commissioning health care. At times, decision maker's subjective values may outweigh objective evidence.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^00cf701b]. Journal of the American College of Cardiology (2025). High credibility.

Cost-effectiveness threshold for U.S. clinical guidelines — the writing committee "recommend the use of $120 000 per QALY as the cost-effectiveness threshold for clinical guidelines", with the threshold to be "periodically reexamined", "not be adjusted annually for inflation", and "revised every 10 years (or sooner…)". Supporting estimates include a 2024 Personal Consumption Expenditure update that "suggests a cost-effectiveness threshold of $117 000 per QALY", a welfare economics approach indicating willingness to pay tied to income ("per capita disposable income of $60 300… approximately $142 000 (range: $120 000-$175 000) for 1 QALY"), and a population-level framework using World Bank data estimating "$95 958 ($81 672-$120 181) per QALY", translating to "$112 000 ($96 000-$141 000) per QALY" after GDP growth; while "no single method… can be considered the 'gold standard'", these three approaches "produced similar results".

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^7325011f]. Journal of the American College of Cardiology (2025). High credibility.

6.1. The structure of economic value statements in clinical guidelines — in the interest of consistency and comparability, "we suggest that economic value statements report 3 components: 1) Class of Recommendation; 2) Level of Certainty; and 3) Recommendation", and because interpretation occurs in the clinical context, "the economic value statement should ideally be placed immediately following the clinical recommendation for the intervention of interest".

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^6ce770b5]. Journal of the American College of Cardiology (2025). High credibility.

AHA/ACC cost/value methodology — economic value statement format, placement, and threshold: Guidelines "should use the "Economic Value Statement" instead of the previously used term "Value Statement" to clarify that cost effectiveness is only 1 component of value", and because it is interpreted in context, the economic value statement "should immediately follow the clinical recommendation". Each statement "should consist of the class of recommendation, level of certainty, and recommendation", and "interventions are considered to be cost-effective at a threshold of $120 000 per QALY gained". The recommendation "should include the population, intervention, comparator, cost of the intervention, and ICER with % of simulations below the accepted threshold", and if studies examine a different population, "this should be reflected in the wording of the economic value statement".

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^9d0e2219]. Journal of the American College of Cardiology (2025). High credibility.

Supporting text requirements for economic value statements specify that supporting text should include key elements: a summary of the high-quality CEAs used to generate the statement with rationale for which studies were included and details such as study population, intervention, comparator, clinical outcomes, sources of key effectiveness and safety parameters, perspective, analytic horizon, base-case treatment cost, and source of funding; point estimates and the proportion of simulations that were cost-effective at a specified threshold; and results of key sensitivity analyses, noting that a 1-way sensitivity analysis of intervention cost variation can be helpful, that additional information about sensitivity to intervention cost should be provided, and that inclusion of a figure similar to Figure 4 is encouraged if available.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^293544d0]. Journal of the American College of Cardiology (2025). High credibility.

AHA/ACC 2025 cost/value methodology — coupling performance measures with economic value explains that the current approach to guideline development is that the class of recommendation for the clinical recommendation is based on the strength of the clinical evidence and does not consider the economic value of the intervention, whereas the economic value statement explicitly considers incremental cost-effectiveness; it proposes that guideline writing committees continue this practice for now, but notes it is plausible that in the future economic value assessments may inform class of recommendation, with the Class 1 designation reserved for interventions that have strong clinical evidence of health benefit and are also cost-effective at the accepted threshold; alternatively, when procedures or interventions receive a Class 2 recommendation (ie, probably recommended and may be appropriate for some patients), the economic value assessment may help determine whether an intervention receives a Class 2A designation (moderate benefit and meets the cost-effectiveness threshold) versus a Class 2B designation (benefit only marginally exceeds risk or does not meet the cost-effectiveness threshold); this approach would provide an incentive for stakeholders to price interventions according to the value they generate for patients and society.

---

### A value framework for cancer screening: advice for high-value care from the American College of Physicians [^3f1fb495]. Annals of Internal Medicine (2015). Low credibility.

Experts, professional societies, and consumer groups often recommend different strategies for cancer screening. These strategies vary in the intensity of their search for asymptomatic lesions and in their value. This article outlines a framework for thinking about the value of varying intensities of cancer screening. The authors conclude that increasing intensity beyond an optimal level leads to low-value screening and speculate about pressures that encourage overly intensive, low-value screening.

---

### The American journal of pathology, value beyond simple metrics [^2779c3bc]. The American Journal of Pathology (2017). Low credibility.

This Editorial highlights the value of The American Journal of Pathology beyond normal metrics.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^943345bf]. Journal of the American College of Cardiology (2025). High credibility.

Figure 3 — health opportunity cost approach to US cost-effectiveness thresholds reports 50000 simulations with varying input values, with 23902/50000 (48%) thresholds < $100000/QALY and 7006/50000 (14%) thresholds > $150000/QALY; the base-case estimate is $104,000/QALY in 2019 US dollars with a (95% UI, $51,000–$209,000/QALY), and updating to 2024 US dollars suggests a cost-effectiveness threshold of $117000 per QALY.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^3a8d8d2e]. Journal of the American College of Cardiology (2025). High credibility.

Coupling performance measures with economic value — The current approach bases class of recommendation on the strength of clinical evidence and does not consider the intervention's economic value, whereas the economic value statement explicitly considers incremental cost-effectiveness. The authors propose continuing this separation for now, but state it is plausible that future economic value assessments may inform class of recommendation, with the Class 1 designation reserved for interventions with strong clinical evidence of health benefit that are also cost-effective at the accepted threshold. For Class 2 recommendations, economic value assessment may help determine whether an intervention receives a Class 2A designation (meets the cost-effectiveness threshold) versus a Class 2B designation (does not meet the cost-effectiveness threshold).

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^68cfa501]. Journal of the American College of Cardiology (2025). High credibility.

Cost-effectiveness thresholds — framework and updates explain that the 2014 ACC/AHA statement provided per–capita GDP–based thresholds such that an intervention with an ICER < $50 000 per QALY gained should be considered high value, $50 000 to < $150 000 (1-3x per capita GDP) per QALY gained should be considered intermediate value, and ≥ $150 000 per QALY gained should be considered low value. The World Health Organization's program suggested that costs less than 3 times the national annual GDP per capita per disability-adjusted life year averted should be considered cost-effective, whereas costs less than 1 times should be considered highly cost-effective; however, experts have argued this approach lacks theoretical justification, and World Health Organization experts now also advise against the use of per capita GDP-based cost effectiveness thresholds and instead suggest considering cost-effectiveness information alongside other contextual information in a transparent multicriterion decision process. The writing committee reviewed alternative approaches, including relying on US-based studies; for example, a health opportunity cost analysis estimated that at 2019 prices, for every $1000000 increase in health care expenditures 1860 persons were projected to become uninsured with 81 QALYs lost due to mortality and 15 QALYs lost due to morbidity, implying a cost-effectiveness threshold of $104 000 per QALY ($51 000 to $209 000 per QALY) in 2019 US dollars, and in the probabilistic analysis 40% of the simulations suggested the threshold was less than $100 000 per QALY.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^a83ec36b]. Journal of the American College of Cardiology (2025). High credibility.

Health opportunity cost approach — United States cost-effectiveness threshold estimation indicates that, across 50000 simulations, 23902/50000 (48%) of thresholds were < $100000/QALY and 7006/50000 (14%) were > $150000/QALY; the base-case estimate was $104,000/QALY in 2019 US dollars with a 95% uncertainty interval of $51,000–$209,000/QALY, and updating to 2024 US dollars suggests a threshold of $117000 per QALY.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^11d967f5]. Journal of the American College of Cardiology (2025). High credibility.

Cost/value methodology — analytic time horizon: Economic value statements should be informed by CEAs that adopt an analytic horizon long enough to fully capture health effects and resource use, which is usually a lifetime horizon in cardiovascular CEAs, and economic evaluations of cardiovascular interventions should generally consider the lifetime horizon as the default with any alternatives carefully justified. Some interventions have delayed benefits or costs, and others may have safety concerns many years after the initial intervention; because randomized trials evaluating a novel therapy rarely follow trial subjects for more than 5 years, longer horizons require extrapolation about long-term effectiveness and safety. Interpretation should therefore pay careful attention to sensitivity and scenario analyses, and short analytic horizons may be acceptable for some questions but should be carefully vetted for bias.

---

### Modelling the species-area relationship using extreme value theory [^10b0aaff]. Nature Communications (2025). High credibility.

The nested species-area relationship, obtained by counting species in increasingly larger areas in a nested fashion, exhibits robust and recurring qualitative and quantitative patterns. When plotted in double logarithmic scales it shows three phases: rapid species increase at small areas, slower growth at intermediate scales, and faster rise at large scales. Despite its significance, the theoretical foundations of this pattern remain incompletely understood. Here, we develop a theory for the species-area relationship using extreme value theory, and show that the species-area relationship is a mixture of the distributions of minimum distances to a starting sampling focal point for each individual species. A key insight of our study is that each phase is determined by the geographical distributions of the species, i.e., their ranges, relative to the focal point, enabling us to develop a formula for estimating the number of species at phase transitions. We test our approach by comparing empirical species-area relationships for different continents and taxa with our predictions using Global Biodiversity Information Facility data. Although a SAR reflects the underlying biological attributes of the constituent species, our interpretations and use of the extreme value theory are general and can be widely applicable to systems with similar spatial features.

---

### Value in acute care surgery, part 3: defining value in acute surgical care-It depends on the perspective [^2434a69b]. The Journal of Trauma and Acute Care Surgery (2024). Medium credibility.

Abstract

The prior articles in this series have focused on measuring cost and quality in acute care surgery. This third article in the series explains the current ways of defining value in acute care surgery, based on different stakeholders in the health care system-the patient, the health care organization, the payer and society. The heterogenous valuations of the different stakeholders require that the framework for determining high-value care in acute care surgery incorporates all viewpoints.

Level Of Evidence

Expert Opinion; Level V.

---

### Should cost Be formally incorporated into clinical practice guideline recommendations? [^c0dde58a]. Otolaryngology — Head and Neck Surgery (2019). Medium credibility.

There is increasing discussion worldwide on explicitly including cost as part of the clinical practice guideline development process. While this could enhance our understanding of value-based care and improve resource utilization, there are many practical challenges for cost inclusion. This commentary explores this issue, examining it from multiple angles and giving pros and cons to inclusion in future guidelines.

---

### Multiple tipping points and optimal repairing in interacting networks [^48be4231]. Nature Communications (2016). Medium credibility.

The problem of optimal repairing

Knowing and understanding the phase diagram of interacting networks enable us to answer some fundamental and practical questions. A partially or completely collapsed system of n ≥ 2 interacting networks in which some of them are in the low activity state is a scenario common in medicine, for example, when diseases or traumas affect the human body and a few organs are simultaneously damaged and need to be treated, and the interaction between the organs is critical. It is also common in economics, when two or more coupled sectors of the economyexperience simultaneous problems, or when a few geographical clusters of countries experience economic difficulties. The practical question that arises is: what is the most efficient strategy to repair such a system? Many approaches are possible if resources are unlimited, but this is usually not the case and we would like to minimize the resources that we spend in the repairing process.

For simplicity, consider two interacting networks, both damaged (low activity). Is repairing both networks simultaneously the more efficient approach, or repairing them one after the other? What is the minimum amount of repair needed to make the system fully functional again? In other words, what is the minimum number of nodes we need to repair, to bring the system to the functional 11 ('up–up') state, and how do we allocate repairs between the two networks? An optimal repairing strategy is essential when resources needed for repairing are limited or very expensive, when the time to repair the system is limited, or when the damage is still progressing through the system, threatening further collapse, and a quick and efficient intervention is needed.

We show below that this problem is equivalent to finding the minimum Manhattan distance between the point in the phase diagram where the damaged system is currently situated and the recovery transition lines to the 11 region. The Manhattan distance between two points is defined as the sum of absolute horizontal and vertical components of the vector connecting the points, with defined vertical and horizontal directions. It is a driving distance between two points in a rectangular grid of streets and avenues. In our phase diagram, it is equal to. It turns out that two triple points of the phase diagram play a very important role in this fundamental problem. We find that these special points have a direct practical meaning and are not just a topological or thermodynamic curiosity.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^63fa1003]. Journal of the American College of Cardiology (2025). High credibility.

Cost-effectiveness thresholds for cardiovascular cost-effectiveness analyses (CEAs) — The 2014 ACC/AHA cost-value methodology statement provided thresholds to identify high-, intermediate-, and low-value care based on U.S. per capita gross domestic product, and the writing committee determined that an intervention with an ICER < $50 000 per QALY gained should be considered high value, $50 000 to < $150 000 (1–3× per capita GDP) per QALY gained should be considered intermediate value, and ≥ $150 000 per QALY gained should be considered low value. WHO-origin GDP benchmarks suggested an intervention that costs less than 3 times the national annual GDP per capita per disability-adjusted life year averted should be considered cost-effective and that costs less than 1 times should be considered highly cost-effective, but critics note they "set such a low bar for cost effectiveness that very few interventions with evidence of efficacy can be ruled out"; WHO experts now also advise against per capita GDP-based thresholds and suggest considering cost-effectiveness alongside other contextual information in a transparent multicomponent decision process. Reflecting these concerns, the writing committee reviewed alternative approaches to incorporating CEA results into guidelines, including relying on US-based studies for identifying a cost-effectiveness threshold or thresholds for cardiovascular CEAs, and Vanness et al used a health opportunity cost approach estimating that for every $1 000 000 increase in health care expenditures, 1869 persons become uninsured, resulting in 81 QALYs lost due to mortality and 15 QALYs lost due to morbidity, implying a cost-effectiveness threshold of $104 000 per QALY ($51 000 to $209 000 per QALY) in 2019 US dollars; in the probabilistic analysis, 40% of the simulations suggested the threshold was less than $100 000 per QALY.

---

### Minimum electric-field gradient coil design: theoretical limits and practical guidelines [^075005b9]. Magnetic Resonance in Medicine (2021). Medium credibility.

Regions A and B are geometrically symmetric about the z = 0 plane, with minimum energy solutions (assuming no E‐field constraint) yielding z‐symmetric field solutions and zero B₀ concomitant fields. Hence, the open diamond starting point (representing the minimum energy solution with no E‐field constraints) for the solid and dashed curves (in both black and green cases) coincide. The symmetry is broken by the addition of a patient model with an E‐field constraint.

The knee in the curves marks the point where magnetic energy, current density, and winding complexity increase rapidly as E max is driven to lower values. These complexities render unrealizable or impractical any gradient designs on the vertical portions of the curves to the left of the knee. Examination of winding patterns indicates that solutions that exhibit energy increases > 25% (or even less depending on coil type) above the global minimum energy become unrealizable (eg, demonstrating two or more "eyes" per half coil [four or more total eyes for an x or y coil] in the fingerprint wire patterns).

The red and purple dotted curves in Figure 2 represent two practical cases in which the primary coil currents are restricted to a single inner surface near the patient and the shield coil currents to an outer cylinder, while at the same time adding torque and eddy current constraints. Line segments acd of Figure 1 represent a primary winding surface consisting of a cone and a cylinder, whereas line segment ab represents its cylindrical shield winding; this yields designs similar to those proposed by previous literature 8, 22 and results in the red dotted curves in Figure 2. A second case is defined by restricting primary currents to the cylinder represented by line segment cd and shield currents to the cylinder ab; this yields designs similar to those proposed by previous literature 28 and produces the purple dotted curves in Figure 2. For both configurations, the eddy current surface is a 1‐m‐long, 620‐mm‐diameter cylinder, centered about the gradient isocenter and placed outside the shield winding of the gradient coil. The time = 0 eddy current error is constrained to a peak deviation less than 0.1% of the ideal gradient field on the surface of a 20‐cm‐diameter volume, corresponding to 100 µT for a 1 T/m step change in gradient strength. The torque constraint is set to zero for a uniform z‐directed B₀ main magnetic field. There is no net force for a perfectly uniform main B₀ field in all calculations.

---

### Value in acute care surgery, part 1: methods of quantifying cost [^26d6b79a]. The Journal of Trauma and Acute Care Surgery (2022). Medium credibility.

Background

With health care expenditures continuing to increase rapidly, the need to understand and provide value has become more important than ever. In order to determine the value of care, the ability to accurately measure cost is essential. The acute care surgeon leader is an integral part of driving improvement by engaging in value increasing discussions. Different approaches to quantifying cost exist depending on the purpose of the analysis and available resources. Cost analysis methods range from detailed microcosting and time-driven activity-based costing to less complex gross and expenditure-based approaches. An overview of these methods and a practical approach to costing based on the needs of the acute care surgeon leader is presented.

---

### Advancing value-based medicine: why integrating functional outcomes with clinical measures is critical to our health care future [^c5702769]. Journal of Occupational and Environmental Medicine (2017). Medium credibility.

US health care spending and functional outcomes — economic context and potential impact are summarized as follows: In 2014, US national health expenditures grew 5.3% to $3 trillion, or roughly $9523 per person, and accounted for 17.5% of the gross domestic product (GDP), with 20% attributable to Medicare and about 16%, or $496 billion, to Medicaid; from 2014 to 2024, expenditures are projected to rise at an average annual rate of 5.8%. Research indicates that by focusing on functional outcomes in patient treatment, medical costs can be decreased and the quality of medical outcomes increased.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^4d080661]. Journal of the American College of Cardiology (2025). High credibility.

AHA/ACC cost/value methodology — health outcomes and quality-adjusted life-year (QALY) use notes that "the health outcomes of an intervention relative to the comparator may be a change in the incidence of clinical events… or change in survival (ie, life years gained)", and that the QALY "is calculated by multiplying the health-related quality of life associated with a health state with the time spent in that state". Health-related quality of life is "measured on a scale from 0 to 1, where 0 represents death and 1 represents perfect health". For preference weights, those "elicited from a representative sample of fully informed members of the community are generally preferred", though "some experts have argued that economic and ethical principles support the use of quality-of-life estimates elicited from patients who are directly affected by the condition". Important limitations include that higher baseline quality-of-life gains "generate more QALYs than an equivalent gain in survival" in lower-baseline groups, which "translates to a comparatively higher valuation" and raises concerns about exacerbating health disparities, and the assumption of indifference between quality-of-life versus longevity changes yielding the same net QALYs, "which is often not the case". Alternatives "including the 'equal value of life-years gained' and the 'health years in total'" have "not been uniformly adopted by major health technology assessment agencies"; "cost-effectiveness thresholds have not been established for some of these newer metrics", and results using them are "not directly comparable to the results of studies that report incremental cost per QALYs gained". The statement adds that "economic value statements should incorporate findings from available CEAs that measure health outcomes in terms of QALYs and life years".

---

### High-value, cost-conscious health care: concepts for clinicians to evaluate the benefits, harms, and costs of medical interventions [^558cee2d]. Annals of Internal Medicine (2011). Low credibility.

Health care costs in the United States are increasing unsustainably, and further efforts to control costs are inevitable and essential. Efforts to control expenditures should focus on the value, in addition to the costs, of health care interventions. Whether an intervention provides high value depends on assessing whether its health benefits justify its costs. High-cost interventions may provide good value because they are highly beneficial; conversely, low-cost interventions may have little or no value if they provide little benefit. Thus, the challenge becomes determining how to slow the rate of increase in costs while preserving high-value, high-quality care. A first step is to decrease or eliminate care that provides no benefit and may even be harmful. A second step is to provide medical interventions that provide good value: medical benefits that are commensurate with their costs. This article discusses 3 key concepts for understanding how to assess the value of health care interventions. First, assessing the benefits, harms, and costs of an intervention is essential to understand whether it provides good value. Second, assessing the cost of an intervention should include not only the cost of the intervention itself but also any downstream costs that occur because the intervention was performed. Third, the incremental cost-effectiveness ratio estimates the additional cost required to obtain additional health benefits and provides a key measure of the value of a health care intervention.

---

### Incorporating economic evidence in clinical guidelines: a framework from the clinical guidelines committee of the American College of Physicians [^9e343a12]. Annals of Internal Medicine (2025). Medium credibility.

In recognition of accelerating health care spending and alignment with the American College of Physicians (ACP) principles of promoting high-value care, the ACP Clinical Guidelines Committee (CGC) developed a framework to standardize its approach to identifying, appraising, and considering economic evidence in the development of ACP clinical guidelines. This article presents the CGC's process for incorporating economic evidence, which encompasses cost-effectiveness analyses, economic outcomes in randomized controlled trials, and resource utilization (intervention cost) data. Economic evidence is one component of ACP recommendations. The CGC first and foremost assesses the certainty of evidence for clinical net benefit of interventions; it then considers patient values and preferences, and only then considers economic evidence to develop recommendations.

---

### Foundations of the minimal clinically important difference for imaging [^72f785d9]. The Journal of Rheumatology (2001). Low credibility.

This article develops a generic conceptual framework for defining and validating the concept of minimal clinically important difference. We propose 3 approaches. The first uses statistical descriptions of the population ("distribution based"), the second relies on experts ("opinion based"), and a third is based on sequential hypothesis formation and testing ("predictive/data driven based"). The first 2 approaches serve as proxies for the third, which is an experimentally driven approach, asking such questions as "What carries the least penalty?" or "What imparts the greatest gain?" As an experimental approach, it has the expected drawbacks, including the need for greater resources, and the need to tolerate trial and error en route, compared to the other 2 models.

---

### Incorporating economic evidence in clinical guidelines: a framework from the clinical guidelines committee of the American College of Physicians [^4483eea4]. Annals of Internal Medicine (2025). High credibility.

Figure 1 — comparative value of interventions — categorizes value by clinical net benefit and incremental cost for quality-adjusted life-year (QALY) with intervention versus comparator, where the incremental cost dimension reflects the incremental cost-effectiveness ratio per QALY gained. For favorable clinical net benefit, higher cost corresponds to high value (< $100 000), intermediate value ($100 000 to $200 000), or low value (> $200 000) with the intervention more effective and more costly; lower cost corresponds to high value with the intervention more effective and less costly (cost-saving). For unfavorable clinical net benefit, higher cost indicates no value with the intervention less effective and more costly (cost-dominated), and lower cost indicates no value with the intervention less effective and less costly.

---

### Feasible future global scenarios for human life evaluations [^bb9a7e71]. Nature Communications (2019). High credibility.

Recent trends (results)

We estimate the 2P and annual models for each country, and for the world as a whole. Table 2 present the results. In general, we find excellent agreement between the 2P and annual estimates. Although the annual estimates use more observations and should therefore be more efficient, they have slightly higher standard errors, likely reflecting the short-term variation during the GFC.

Overall, the trend in life evaluations, measured as per-year change on an 11-point scale, is very small (insignificant). The material variables income and life expectancy are globally increasing significantly, according to all our specifications. There are also significant global trends in the non-material variables, whose changes are measured as annual change on a 0–1 scale. Freedom and corruption are both improving in all specifications, while social support is getting worse (diminishing), and significantly so averaged across countries. Thus, the predictor variables are significantly trending but not all in an optimistic or pessimistic direction.

Table 2 also reports, for each variable, the fraction of countries with statistically significant (p ≤ .05) rates of change. These fractions are in all cases considerably > 5%, suggesting that trends, both positive and negative, are sustained (and real). This fact is central to our argument in that it supports our assumption that we can gauge reasonable rates of change based on the distribution of changes in the recent past. The remaining rows of Table 2 show the 10th and 90th, and intermediate, percentiles from these distributions. We consider these to correspond to plausible optimistic and pessimistic trends in the future scenarios. In general, these values are more extreme for the annual model than the 2P model, and we use the latter in order to remain conservative in our scenarios.

Table 2
Estimated recent global trends and distribution

The first four rows show estimated global trends for the period 2005–2016 and between the periods 2005–2007 and 2014–2016. The 2P versions of the five quantiles of the across-countries distributions are used to build our future scenarios

Significance: †.1%, * 1% 5%, 10%

---

### Volume brings value [^c3469659]. Catheterization and Cardiovascular Interventions (2018). Low credibility.

What the article teaches The article highlights the competency of high volume operators across institutions that despite moving to lower volume centers retain their good clinical outcomes. How it will impact practice Although current U.S. based guidelines suggest a volume of at least 50 PCIs annually for operators to retain clinical competency this estimate may be a poor index to guarantee optimal clinical outcomes. What new research/study would help answer the question New studies using a comprehensive definition to define an experienced operator from the Asia-Pacific-a geographic area that has pioneered in complex coronary interventions-need to elute on the threshold of adequate clinical competency stratified according to clinical complexity.

---

### Degenerate boundaries for multiple-alternative decisions [^55f4a482]. Nature Communications (2022). High credibility.

Sequential Bayesian inference can be rewritten in a form that more closely resembles a sum of evidence by taking the logarithm of equation (8):For each of the n -choices, the accumulated evidence is the log posterior, with n distinct evidence incrementsand a common subtractive log-marginal term. Calculation of this log-marginal is necessary for multiple (n > 3) choice optimal decision-making, but can be avoided for n = 2 by using the SPRT as mentioned above. A similarterm in a theory of basal ganglia function has been interpreted as suppressing evidence accumulation when choices are ambiguous –.

Within this framework, using that probabilities sum to unity ∑ i P i (t) = 1, the evidence accumulation takes place within an (n − 1)-dimensional linear subspace spanning n vertices { P i = 1, P j ≠ i = 0} of an N -dimensional unit hypercube. This is most easily seen for N = 3 choices, where the decision variables P = (P 1, P 2, P 3) vary within a triangle passing through three vertices {(1, 0, 0), (0, 1, 0), (0, 0, 1)} of the unit cube (Fig. 1, top right panel). So rather than representing evidence accumulation trajectories as a random walk constrained to two dimensions, for multiple alternatives the random walk moves in higher dimensions (a line for two alternatives, a plane/triangle for three, and so on). For simplicity, this vector is initialized with equal prior probabilities 1/ n and updated sequentially until one of the elements reaches a boundary, P i > θ.

Both the SPRT and two-choice sequential Bayesian inference give optimal decisions in that they give the fastest decisions for a given level of accuracy on single trials of known difficulty. Formally, they optimize a cost function, the Bayes risk, that is linear in the mean error rate e and mean decision time T over many trialswith error costs W i > 0 and cost of time c, scaling the expected error and decision time,… This cost function represents the trade-off between speed and accuracy of decision-making: slow but accurate or fast but inaccurate decisions are both costly, and so a balance must be found. For equal decision costs, W 0 = W 1, there is a single threshold that is free parameter, which for optimal decision-making is tuned according to the relative cost c / W of accuracy and time.

---

### Contemporary issue: health and safety of female wrestlers [^c2819a17]. Current Sports Medicine Reports (2024). High credibility.

Female wrestlers — body fat distribution and MWW threshold reconsideration shows that even in trained, highly fit female wrestlers body fat values are between 18% and 22% and it is extremely rare that the 12% threshold is approached; from Table 2 data, a sample size weighted mean of 20.3% with a pooled standard deviation of 4.2% in a sample of 326 female wrestlers yields a lower boundary of 17.9% body fat, with 12% clearly falling in the unexpected region. In line with these data, the authors state, "we encourage SGB to reevaluate using 12% fat as the minimum set for all female wrestlers" and that "a value like 18% body fat might be a more reasonable minimum", adding that "If 18% were adopted, 0.82 in the MWW calculation would replace 0.88", and clarifying that "biological females with less than 18% body fat at the time of weight class determination would not be disqualified", although preseason values under 18% would prohibit further reductions to reach a lower class.

---

### Value in acute care surgery, part 2: defining and measuring quality outcomes [^6eb5d2f4]. The Journal of Trauma and Acute Care Surgery (2022). Medium credibility.

Abstract

The prior article in this series delved into measuring cost in acute care surgery, and this subsequent work explains in detail how quality is measured. Specifically, objective quality is based on outcome measures, both from administrative and clinical registry databases from a multitude of sources. Risk stratification is key in comparing similar populations across diseases and procedures. Importantly, a move toward focusing on subjective outcomes like patient-reported outcomes measures and financial well-being are vital to evolving surgical quality measures for the 21st century.

---

### GRAF-pop: a fast distance-based method to infer subject ancestry from multiple genotype datasets without principal components analysis [^ae07d506]. G3 (2019). Medium credibility.

Barycentric coordinates were developed by August Ferdinand Möbius (1790–1868) in his book " Der barycentrische calcul " (1827). Nowadays, Möbius is better known for discovering the non-orientable three-dimensional shape known as the Möbius strip, which has been cut in three pieces and stylized into the universal symbol for recycling. In that spirit, our work recycles the technique of barycentric coordinates for application to ancestry inference. Readers may have seen barycentric coordinates used in computer graphics and animation to interpolate colors over any convex shape and to deform shapes smoothly, and for other applications.

For a triangle on a 2D plane, barycentric coordinates solve the following problem: given a point, p, inside the triangle, find the unique masses of three objects summing to 1 such that when they are placed on the three vertices, p is the center of mass. Applying barycentric coordinates to the EFA triangle and treating the mass proportions as the ancestral proportions, we can estimate the population components for each individual based on the projection of that individual's distance data relative to the vertices of the reference triangle.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^d9399e2d]. Journal of the American College of Cardiology (2025). High credibility.

Cost-effectiveness analysis (CEA) metrics and uncertainty — Figure 2 presents definitions and properties of ICER, net monetary benefit (NMB), and net health benefit (NHB). The page states that 'CEAs typically separately report uncertainty intervals for costs and outcomes, which can provide additional insights into the uncertainty of the core components of the ICER estimate'. It adds that 'Net monetary benefit and net health benefit are alternative metrics of cost-effectiveness that have more favorable statistical properties (Figure 2)'. The metric expressions are displayed as 'ICER = Cost of Intervention – Cost of Comparator' with 'Health Outcomes with Intervention – Health Outcomes with Comparator', 'NMB = (Incremental Health Outcomes*CE threshold) – Incremental Costs', and 'NHB = Incremental Health Outcomes – Incremental Costs' with 'CE threshold'.

---

### Addressing the minimum fleet problem in on-demand urban mobility [^a2b962b8]. Nature (2018). Excellent credibility.

Information and communication technologies have opened the way to new solutions for urban mobility that provide better ways to match individuals with on-demand vehicles. However, a fundamental unsolved problem is how best to size and operate a fleet of vehicles, given a certain demand for personal mobility. Previous studies 1–5 either do not provide a scalable solution or require changes in human attitudes towards mobility. Here we provide a network-based solution to the following 'minimum fleet problem', given a collection of trips (specified by origin, destination and start time), of how to determine the minimum number of vehicles needed to serve all the trips without incurring any delay to the passengers. By introducing the notion of a 'vehicle-sharing network', we present an optimal computationally efficient solution to the problem, as well as a nearly optimal solution amenable to real-time implementation. We test both solutions on a dataset of 150 million taxi trips taken in the city of New York over one year 6. The real-time implementation of the method with near-optimal service levels allows a 30 per cent reduction in fleet size compared to current taxi operation. Although constraints on driver availability and the existence of abnormal trip demands may lead to a relatively larger optimal value for the fleet size than that predicted here, the fleet size remains robust for a wide range of variations in historical trip demand. These predicted reductions in fleet size follow directly from a reorganization of taxi dispatching that could be implemented with a simple urban app; they do not assume ride sharing 7–9, nor require changes to regulations, business models, or human attitudes towards mobility to become effective. Our results could become even more relevant in the years ahead as fleets of networked, self-driving cars become commonplace 10–14.

---

### A practical guide for understanding confidence intervals and P values [^c0a7d4a0]. Otolaryngology — Head and Neck Surgery (2009). Low credibility.

The 95 percent confidence interval about the mean demarcates the range of values in which the mean would fall if many samples from the universal parent population were taken. In other words, if the same observation, experiment, or trial were done over and over with a different sample of subjects, but with the same characteristics as the original sample, 95 percent of the means from those repeated measures would fall within this range. This gives a measure of how confident we are in the original mean. It tells us not only whether the results are statistically significant because the CI falls totally on one side or the other of the no difference marker (0 if continuous variables; 1 if proportions), but also the actual values so that we might determine if the data seem clinically important. In contrast, the P value tells us only whether the results are statistically significant, without translating that information into values relative to the variable that was measured. Consequently, the CI is a better choice to describe the results of observations, experiments, or trials.

---

### Ultra-high dynamic range quantum measurement retaining its sensitivity [^6f9c90ae]. Nature Communications (2021). High credibility.

To improve the sensitivity for a large range, initially, a number of measurements with different areas are combined to uniquely define the magnetic field amplitude in a range limited by the measurement with the smallest area (Fig. 1d). Consequently, only part of the measurement time is spent on the largest area, which has the best sensitivity (but a small range), while the remainder of the time is spent on areas with a worse sensitivity. Therefore, the sensitivity of the combined measurement is strictly worse than this best sensitivity. Using halved areas (hence requiring at leastadditional areas) and the same number of iterations for each area and using no optimisations, for roughly the same σ B, the measurement time for the combined sequence. Thus, the sensitivity would becometimes worse, which is already a significant improvement compared to the straightforward case in the last paragraph.

The measurements resulting from different areas are combined via Bayes' theorem. For area A n, the measurement gives signal S n (for example the crosses/circles/triangles on the sinusoids in Fig. 1a, d for three areas). The posterior probability distribution for the magnetic field B given measured signal S n iswiththe prior distribution, independent of B, andwiththe relation between the signal S and the applied field B (the sinusoids in Fig. 1a, d, e). Fig. 1e visualises these equations.is a Poisson distribution (counting photons), but it can be approximated by a normal distribution (green line along y -axis in Fig. 1e) when more than ~10 photons arrive (with continuity correction). This is generally the case when the uncertainty is below the maximum uncertainty, as described later. For the first measurement, the prior distribution is flat since there is no initial knowledge about the field, and for the remainder of the measurements, the previous posterior is the new prior distribution. This results in a combined distribution as demonstrated in Fig. 1f.

---

### On the optimality of 2 ° C targets and a decomposition of uncertainty [^64734e0d]. Nature Communications (2021). High credibility.

Finally, the utility is discounted at three pure rate of time preference values (also called utility discount rates): the low bound of 0.1%, as used in the Stern review, 1.5% and 3%. The latter two values correspond with the values used in DICE-1999 and DICE-2007 (and following versions), respectively. These values span a similar range as a recent expert elicitation of social discount rates, where the 5th and 95th percentiles of the PRTP values are 0% and 3.5%/year, with an average reported value of 1.1%/year.

The effect of using a cubic, instead of a quadratic, MAC is discussed in SI.5.1, as well as the effect of using the full range of PRTP/elasticity of marginal utility combinations from the aforementioned expert elicitation (SI.7).

Analysing the variance using Sobol decomposition

Owing to the relative simplicity of this model, we are able to calculate the optimal carbon price path (both with and without carbon budget) for every combination of parameter values shown in Table 1 (405 scenarios). This allows us to analyse the relative importance of each parameter. We quantify the contribution to the variance of each parameter with the Sobol indices. These are calculated using a Monte Carlo method. This method requires a distribution for each parameter. However, sampling from a continuous distribution for each parameter would require thousands or millions of runs, which is computationally infeasible. For this reason, we approximate the distribution of each parameter by a discrete distribution best matching the normal distribution of the underlying distribution using only the parameter values available in Table 1. Since the values for the mitigation costs and the TCRE represent the 5th, 50th and 95th percentiles, the discrete distribution with equal mean and variance is a weighted distribution where the median value is 3.4 times more likely to be chosen (see Supplementary Information 4.3). A problem with this method is that the SSP, damage function and discount rate do not have an underlying distribution. To still be able to quantify the relative importance of each parameter, we associate a uniform discrete distribution to these parameters. More details of this method are available in SI.4.3.

---

### A rise-to-threshold process for a relative-value decision [^1eaa5419]. Nature (2023). Excellent credibility.

To determine the mean egg-laying rate, we computed # eggs /# frames for each bin. Because the videos were recorded at 2 Hz, we multiplied the value obtained for each bin by 120 to convert it to units of eggs min –1. To determine the confidence interval for each bin we utilized the Clopper–Pearson method, also known as the 'exact' binomial confidence interval, to compute the 90% confidence interval for # eggs /# visits. We then transformed the confidence interval for each bin to units of eggs min –1 by multiplying it by 120 × # visits /# frames. The confidence interval could not be directly calculated from # eggs /# frames because it would then be contingent on the video frame rate.

For these rate curve calculations, search periods with duration shorter than 30 s were set to 30 s. This prevented very brief search periods from introducing fluctuations in the rate functions (by contributing to the numerator and not contributing much to the denominator). By doing so, the rate curves exhibited less variation across replicates or conditions. Note that search periods already had a minimum duration of 9 s, which was automatically determined by the search period calculation (Methods). Altering the definition of the search period, or having no minimum search duration, does not change our stated conclusions from these curves. Additionally, the use of different x -axis bins yields qualitatively similar results and does not change our stated conclusions. Rate functions start with low rates after a transition, at least partially, because flies do not lay eggs on the plastic barrier between substrates (Extended Data Fig. 7e, f) and because flies are, by definition, walking (and not pausing to deposit an egg) during a transition (Extended Data Fig. 7g).

---

### Cutting edge or blunt instrument: how to decide if a stepped wedge design is right for you [^471f52f2]. BMJ Quality & Safety (2021). High credibility.

When might I consider doing a stepped wedge trial?

Research designs are shaped as much by practical constraints as by abstract schemes, and it is always a good idea to start with the constraints and work towards a design, rather than start with a design and try to fit it to constraints. These constraints will be unique to each research context, and box 1 lists some areas to think about. Still, there are some common features of settings where a stepped wedge trial might be considered as a possible design, and we now review these.

Box 1
Practical constraints on the design of a longitudinal cluster randomised trial

Are there limits on the time available to complete the evaluation, on the number of clusters, or on the number of participants (or the rate at which you can recruit participants) at each cluster? These constraints put limits on the overall scale of the evaluation, or force trade-offs between different design characteristics.
How will participants and their data be sampled in your study: as a series of cross-sectional surveys, as a continuous stream of incident cases, as a cohort followed over time, or some other way? Does the timescale divide into cycles, seasons or milestones that influence how you will sample participants and data?
Is there a limit on how many clusters can implement the intervention at the same time in the evaluation? If this is constrained by research resources (eg, if there are only enough trained research staff to implement the intervention one cluster at a time) then implementation must be staggered in some way.
If implementation is to be staggered, is there a minimum 'step length'? If the same team delivers the intervention in different clusters at different steps, then bear in mind it may take some time to get the intervention fully operational at a site, and the team will also need time to relocate from one cluster to the next.

---

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^7f4c4946]. American Journal of Kidney Diseases (2006). Medium credibility.

Minimally adequate hemodialysis — minimum treatment time and Kt/V considerations: A stated concept is maintaining stdKt/V close to 2.0 per week across schedules; for a 2-hour dialysis treatment, an spKt/V of at least 1.4 is required to achieve an stdKt/V of 2.0, and very short treatment times make salt and water balance control difficult. The outcomes evidence for this CPR is not particularly strong: in the HEMO Study, the minimum treatment time was 2.5 hours with no randomized evaluation of treatment time; a study comparing conventional dialysis (3- to 4-hour treatments) with ultrashort high-efficiency hemodiafiltration found no difference in blood pressure control. Very recent studies, including 1 RCT, suggested that treatment time has an impact on outcomes, and cross-sectional data related treatment time inversely to mortality with much of the effect disappearing when patient BSA was included; it was the Work Group's belief that a minimum treatment time of 3 hours.

---

### Multiple tipping points and optimal repairing in interacting networks [^806c4333]. Nature Communications (2016). Medium credibility.

To optimize repairing we need to minimize this metric. Figure 5 shows the solution to the minimization problem and a detailed discussion is provided in the Methods section. The different colours in Fig. 5 correspond to the different optimal repair strategies, which depend on the failure state of the system. If the system is initially at point S 1, both networks are in a low activity state, that is, they are non-functional. Our goal is to decreaseand, and arrive to the region where the system is fully recovered (the green region) by performing a minimal number of repairs, that is, minimal N rep. We find that for any point in the red region there are actually two closest points in the green region, at an equal Manhattan distance away from the red region point. These two points are the triple points R1 and R2 shown in Fig. 5, which also correspond to the triple points in Fig. 2b. Although R1 may be closer to point A than R2 by Euclidian distance, the Manhattan distance is the same. Thus, two equally good repairing strategies are available. One involves allocating more node repairs to network A and the other allocating more repairs to network B. For the yellow regions (points S 2 and S 3), the closest points by Manhattan distance are R1 (for point S 2) or R2 (for point S 3). Here, only one triple point represents the optimal solution. It is noteworthy that the path samples in Fig. 5 are 'zig-zag' in shape (to highlight that we are minimizing); however, even when a diagonal path (direct straight line) to a triple point is used, the Manhattan distance is the same. For the dark blue regions (points S 4 and S 7), the optimal strategy is to decreaseonly, until the system is recovered. Similarly, for the light blue regions (points S 5 and S 6), the optimal strategy is to decrease only.

From our optimal repairing strategy analysis we find that the order of repair (the specific path taken between the initial point and final point) does not affect the final result. Minimizing the Manhattan distance only determines the optimal destination point. Therefore, there is actually a set of paths corresponding to equally optimal repairing processes.

---

### Value-based healthcare: controlling costs through a value analysis committee [^b6297d6e]. Clinical Orthopaedics and Related Research (2024). Medium credibility.

From the Column Editor: The goal of the "Value-based Healthcare" column is to explore strategies that enhance the value of musculoskeletal care by improving health outcomes and reducing the overall cost of care. In this month's guest column, we hear from David M. Kalainov MD, MBA, Clinical Professor of Orthopaedic Surgery at Northwestern University Feinberg School of Medicine, Medical Director of Orthopaedic Surgery at Northwestern Memorial Hospital, and current committee member and past co-chair of the Orthopaedic Surgery Value Analysis Committee at Northwestern Medicine, an 11-hospital healthcare system in the Chicago, IL, USA, metropolitan area. Dr. Kalainov discusses how value analysis committees (VACs), which have been growing in popularity with the shift from fee-for-service to value-based healthcare payment models, may help curb skyrocketing health care costs. Dr. Kalainov presents five principles for running an effective orthopaedic VAC.

 — Kevin J. Bozic MD, MBA

Skyrocketing healthcare expenses in the United States are placing mounting pressures on healthcare organizations to cut expenses, including the rising costs of patient care, inflation, and stricter limits on government and private insurance reimbursements. In 2022, healthcare spending was USD 4.5 trillion, and that number is estimated to hit USD 7.2 trillion by 2031; these numbers are not sustainable.

One potential tool to achieve these reductions is a value analysis committee (VAC), which hospitals have used since the 20th century. The primary function of the VAC is to adopt and standardize products deemed important for patient care. These committees typically are divided into clinical specialties and include hospital-affiliated clinicians, supply chain personnel, and administrative staff. Data analysts, quality improvement specialists, and finance representatives may also be part of the group structure.

Over the past decade, the importance of VACs has grown in parallel with a shift from fee-for-serve to value-based payments for care, and VACs have provided orthopaedic surgeons with important roles in choosing and standardizing procedural solutions.

There are numerous opportunities for orthopaedic surgeons to improve value for patients while simultaneously tempering unnecessary spending. Although cost is important, patient value is prioritized in a properly organized VAC. As a current member and past co-chair of a multihospital orthopaedic surgery VAC, my recommendations for an effective orthopaedic surgery VAC include the following five principles.

---

### From the betweenness centrality in street networks to structural invariants in random planar graphs [^29785c51]. Nature Communications (2018). Medium credibility.

Fig. 3
Effect of edge-density ρ e on the betweenness. N ∼10⁴ nodes were randomly distributed on the 2D plane and their DT was generated. Edges were removed until the desired edge-density ρ e was reached. a – d The average over a hundred realizations of the resulting BC distribution ranging from the MST to the DT with increasing ρ e. The orange shaded area corresponds to fluctuations around the average of the realizations, while the silver and white shades separate the "tree-like" region from the "loop-region" respectively. e – h a single instance of the actual generated network corresponding to each ρ e. Shown in red are the nodes in the 90th percentile and above in terms of their BC value

The simulations indicate the observed bimodality to be a combination of a high betweenness backbone belonging to the MST, and a low betweenness region generated by loops. The transition between the two regimes is determined by the minimum non-zero betweenness value for the MST, which is O (N) and the tail may have different peaks, determined by the distribution of branches emanating from the tree. Progressively decorating the tree with loops leads to arbitrarily low betweenness values due to the creation of multiple alternate paths, thus smoothing out the distribution, as the betweenness transitions from an interval [N, N²] for the MST to a continuous distribution over [1, N²] for the DT.

Spatial distribution of high betweenness centrality nodes

Figure 3e–h shows a single instance of the actual network generated by our procedure for each corresponding edge-density. Highlighted in red are nodes lying in the 90th percentile of betweenness. There is a distinct change in spatial pattern with increasing ρ e; for the MST, they span the network and are tree-like with no apparent spatial correlation; as the network gets more dense, the nodes cluster together and move closer to the barycenter, suggesting a transition between a "topological regime" and a "spatial regime".

To quantify these observed changes, we investigate the behavior of the high BC nodes at and above percentile θ through a set of metrics: the clustering C θ which measures the spread of high betweenness nodes around their center of mass, the anisotropy factor A θ which characterizes the spatial anisotropy of this set of nodes, and finally, the detour factor D which measures the average extent to which paths between two locations deviate from their geodesic distance. (Details on metrics shown in Methods)

---

### Emergent constraints on future precipitation changes [^081006e1]. Nature (2022). Excellent credibility.

Future projections of global mean precipitation change (ΔP) based on Earth-system models have larger uncertainties than projections of global mean temperature changes (ΔT) 1. Although many observational constraints on ΔT have been proposed, constraints on ΔP have not been well studied 2–5 and are often complicated by the large influence of aerosols on precipitation 4. Here we show that the upper bound (95th percentile) of ΔP (2051–2100 minus 1851–1900, percentage of the 1980–2014 mean) is lowered from 6.2per cent to 5.2–5.7per cent (minimum-maximum range of sensitivity analyses) under a medium greenhouse gas concentration scenario. Our results come from the Coupled Model Intercomparison Project phase 5 and phase 6 ensembles 6–8, in which ΔP for 2051–2100 is well correlated with the global mean temperature trends during recent decades after 1980 when global anthropogenic aerosol emissions were nearly constant. ΔP is also significantly correlated with the recent past trends in precipitation when we exclude the tropical land areas with few rain-gauge observations. On the basis of these significant correlations and observed trends, the variance of ΔP is reduced by 8-30per cent. The observationally constrained ranges of ΔP should provide further reliable information for impact assessments.

---

### Statistical considerations in the evaluation of continuous biomarkers [^c7960d51]. Journal of Nuclear Medicine (2021). Medium credibility.

Discovery of biomarkers has been steadily increasing over the past decade. Although a plethora of biomarkers has been reported in the biomedical literature, few have been sufficiently validated for broader clinical applications. One particular challenge that may have hindered the adoption of biomarkers into practice is the lack of reproducible biomarker cut points. In this article, we attempt to identify some common statistical issues related to biomarker cut point identification and provide guidance on proper evaluation, interpretation, and validation of such cut points. First, we illustrate how discretization of a continuous biomarker using sample percentiles results in significant information loss and should be avoided. Second, we review the popular "minimal- P -value" approach for cut point identification and show that this method results in highly unstable P values and unduly increases the chance of significant findings when the biomarker is not associated with outcome. Third, we critically review a common analysis strategy by which the selected biomarker cut point is used to categorize patients into different risk categories and then the difference in survival curves among these risk groups in the same dataset is claimed as the evidence supporting the biomarker's prognostic strength. We show that this method yields an exaggerated P value and overestimates the prognostic impact of the biomarker. We illustrate that the degree of the optimistic bias increases with the number of variables being considered in a risk model. Finally, we discuss methods to appropriately ascertain the additional prognostic contribution of the new biomarker in disease settings where standard prognostic factors already exist. Throughout the article, we use real examples in oncology to highlight relevant methodologic issues, and when appropriate, we use simulations to illustrate more abstract statistical concepts.

---

### Clinical features of patients who needed more than one-time photorefractive keratectomy (PRK), a retrospective cohort study [^38432d39]. BMC Ophthalmology (2025). Medium credibility.

K1, Km and astigmatism in the control group were significantly different from those in the case group (K1 control 43.93 ± 1.17, case 42.75 ± 1.64 P < 0.05*), (Km control 44.43 ± 1.19, case 43.51 ± 1.48, P < 0.05*), and (astigmatism control 1.03 ± 0.57, case 1.82 ± 1.25, P < 0.05*), respectively. The maximum and minimum keratometry of the anterior surface, flat K1, steep K2, and Km, are given in Table 2. Subgroup analysis revealed that there was a significant difference in anterior surface keratometry K1, Km (43.93 ± 1.17, 44.43 ± 1.19) between the control group and the case group that underwent the 2nd surgery with already ablated cornea (42.06 ± 3.84, 42.40 ± 3.43 with p < 0.05 shown in Table 2 and at a 95% confidence interval.

Table 2
Comparison of Pentacam keratometry between both case and control groups

Asterisk (*) shows P -Value is significant, control and case groups were significantly different from each other. Post Hoc least significant difference test interpreted with superscripts 'a' for control, 'b' for case at first surgery, 'c' for case at 2 nd surgery respectively. Any group with more than two superscript (ab or ca or abc) is statistically different between groups with p < 0.05 at 95% CI. All keratometry values are in diopter

Posterior surface keratometry power of K1, K2, and Km in diopters were significantly different between the control versus case groups (−6.32 ± 0.25 vs. −5.88 ± 0.25, −6.61 ± 0.21vs −6.33 ± 0.17, and − 6.47 ± 0.22 vs. −6.10 ± 0.24, respectively). Other characteristics of the posterior surface keratometry variables, flat K1, steep K2, mean keratometry Km and astigmatism, are given in Table 2.

---

### A continuous-time maxSAT solver with high analog performance [^60aab956]. Nature Communications (2018). Medium credibility.

Fig. 3
Algorithm statistics over random Max 3-SAT problems. Distribution of differences between the real global minimum E min obtained with the exact algorithm MaxSatz and a the smallest energy found by the algorithm, b the predicted minimum value, and c the final decision of the algorithmshown for problems with different N and α values (see legends). d The percentage of instances indicated by color (see color bar) for different values of the errorand hardness η. Most instances are in therow indicating correct prediction. Large errors occur mainly at smaller η values, and are dominantly negative

Performance evaluation on hard MaxSAT competition problems

Next, we present the performance of our solver on MaxSAT competition problems, from 2016. We are foremost interested if Max-CTDS is capable of predicting the global minimum energy value and finding assignments corresponding to that energy value for hard problems, within a reasonable time.

For illustration purposes, here we first discuss an extremely hard competition problem instance with N = 250 variables and M = 1000 clauses, called "HG-3SAT-V250-C1000-1.cnf", which was reposted for several years. This problem was also used in Fig. 2. No complete competition algorithm could solve this problem. The best complete solver in 2016, the CCLS2akms has found a state with energy value 5, but could not prove that it is optimal within the allotted time (30 min). We ran our algorithm on a regular 2012 iMac 21.5, 3.1 GHz, Intel Core i7 computer and it predicted the lowest energy of 5 (unsatisfied clauses), after 21 min 24 s of running time and produced an assignment for it after 9.168 h of running time. The minimum energy prediction was achieved already after Γ = 7000 trajectories, whereas finding an assignment with this minimum energy took a total of Γ = 189,562 trajectories to run. The minimum energy assignment corresponding tois provided in Supplementary Data 2. (The problem can be downloaded from the competition site). We ran the complete and exact algorithm, MaxSatz, for over 5 weeks on this problem and the smallest energy it found was E = 9. The details of how the Max-CTDS algorithm performs are shown in Fig. 4. Similar figures for other hard problems such as for a 4-SAT problem and a spin-glass problem are shown in Supplementary Figs. 5 and 6.

---

### Incorporating economic evidence in clinical guidelines: a framework from the clinical guidelines committee of the American College of Physicians [^537f28fa]. Annals of Internal Medicine (2025). High credibility.

Clinical Guidelines Committee (CGC) value thresholds for economic evidence in cost‑effectiveness analyses categorize interventions by incremental cost‑effectiveness ratio (ICER) per quality‑adjusted life‑year (QALY) gained as high value when cost‑saving or < $100 000, intermediate value at $100 000-$200 000, low value at > $200 000, and no value when dominated. The CGC states these thresholds are those most commonly referenced in U.S.-based cost‑effectiveness analyses, may be adjusted on a topic‑by‑topic basis, and that an intervention is dominated by strict dominance or extended dominance.

---

### Worldwide divergence of values [^fce106c7]. Nature Communications (2024). High credibility.

Results

Value divergence at the item level

We first examined general trends towards value convergence or divergence using our value variation measure, which allowed us to estimate effects at the item level. Our results strongly supported value divergence. A mixed effects model with value variation nested in the 40 items found that timepoint has been significantly associated with greater value variation, b = 0.004, SE = 0.0007, t (239) = 5.12, p < 0.001, = 0.17, 95% CIs [0.002, 0.005]. We replicated this result using a different approach in which we correlated timepoint with value variation separately for each of the 40 value items. Of the 40 values, we found that 27 have diverged over time, with a positive median correlation of 0.28 between timepoint and value variation, t (39) = 3.30, p = 0.002, M diff = 0.28, 95% CIs [0.11, 0.45]. Coefficients associated with each item are displayed in Fig. 1.

---

### An official American Thoracic Society workshop report: developing performance measures from clinical practice guidelines [^53d0f733]. Annals of the American Thoracic Society (2014). Medium credibility.

American Thoracic Society workshop — limitations and cautions include that excluding weak recommendations can miss improvement opportunities, since "it is possible that some weak recommendations could improve patient outcomes if broadly applied" and "weakly recommended treatments may improve outcomes on average, even if they harm some patients"; there may be value in reducing variation, but "this approach carries the risk that meaningful opportunities for performance improvement are missed", and codifying weak recommendations "carries greater risks, including wasted resources" and punishing "legitimate practice variation". The approach applies primarily to broad-use measures, as local tracking "may not need to follow such stringent criteria", though developers "should be cognizant of opportunity costs" when considering uniform implementation of weak recommendations. Although GRADE-based recommendations are "subjective and conditional on the varying judgments of the developers", the system documents judgments and values that developers can consider when selecting recommendations to turn into measures.

---

### An official American Thoracic Society workshop report: developing performance measures from clinical practice guidelines [^369e6d46]. Annals of the American Thoracic Society (2014). Medium credibility.

Agency for Healthcare Research and Quality performance measure attributes — Desirable features of performance measures include relevance to stakeholders and addressing important aspects of health, with evidence of a need for the measure; evidence should be explicitly stated, results should be reproducible and truly measure what they purport to measure, and specifications should explicitly define the numerator and denominator with understandable data collection requirements; necessary data sources should be available within the measurement timeframe and data collection costs justified by potential improvement in care or health.

---

### A comparison of phase II study strategies [^ebb760ae]. Clinical Cancer Research (2009). Low credibility.

The traditional oncology drug development paradigm of single arm phase II studies followed by a randomized phase III study has limitations for modern oncology drug development. Interpretation of single arm phase II study results is difficult when a new drug is used in combination with other agents or when progression-free survival is used as the endpoint rather than tumor shrinkage. Randomized phase II studies are more informative for these objectives but increase both the number of patients and time required to determine the value of a new experimental agent. In this article, we compare different phase II study strategies to determine the most efficient drug development path in terms of number of patients and length of time to conclusion of drug efficacy on overall survival.

---

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^5365c971]. American Journal of Kidney Diseases (2006). Medium credibility.

Minimum spKt/V levels for different dialysis schedules (CPR 4.3) explains that prior guidance addressed only thrice-weekly HD, but a cross-sectional USRDS study suggested survival with twice-weekly HD was no worse (possibly better); consequently, the Work Group decided that thrice-weekly HD as a minimum frequency was not necessary for all patients and was comfortable recommending a twice-weekly dialysis schedule only for patients with substantial RKF. The section describes more frequent scheduling as more often than 3 times per week, including short daily sessions ranging from 1.5 to 3 hours (or longer) 4 to 6 times per week, and nocturnal dialysis increasing time from 2.5 to 5 hours to 7 to 10 hours with frequencies from 3 to 6 times per week; avoidance of the 2-day interdialytic interval by every-other-day dialysis is also described. It further states that no RCTs have been conducted to measure hard outcomes comparing conventional thrice-weekly dialysis with either short-daily or nocturnal HD.

---

### A critical assessment of the linear No-threshold hypothesis: what is the evidence? [^c48a0459]. Clinical Nuclear Medicine (2019). Medium credibility.

The Linear No-Threshold (LNT) hypothesis regarding low-level radiation is based on expert opinion. As more experts critically evaluate the absence of convincing evidence supporting the LNT hypothesis, the more likely we will recognize that LNT may create more risk than it mitigates.

---

### Principled approach to the selection of the embedding dimension of networks [^8cd7e63d]. Nature Communications (2021). High credibility.

Principled selection of a suitable value for the embedding dimension

The observed behavior of the loss function L (d) indicates that embedding algorithms may generate sufficiently accurate geometric descriptions with a relatively small value of d. Assuming that the plateau value L ∞ of the loss function L (d) corresponds to the best geometric description that the embedding algorithm can achieve, we indicate withthe minimal d value such that the difference between L (d) and the optimum L ∞ is less than ϵ. As already stated, a key implicit assumption here is that the empirically trained max-dimensional embedding is the best achievable by the algorithm. d o (ϵ) is then the best choice for the embedding dimension that can be made to achieve a geometric representation that uses less training time and computational resources but is still sufficiently similar (i.e. within an ϵ margin) to the best achievable representation of the network. Our formulation of the problem does not explicitly account for the fact that the network at hand may be a specific realization of some underlying stochastic model which the embedding method may rely on. Rather, Eq. (1) defines a low-rank approximation problem, meaning that we aim at finding the low-dimensional (i.e. low-rank) approximation that best describes the high-dimensional (i.e. full-rank) embedding matrix. We differentiate from the standard setting considered in low-rank approximation problems in two main respects. First, we do not rely on a standard metric of distance (e.g. Frobenius distance) between the low- and the full-rank embedding matrices. Instead, we make use of a measure of collective congruence between the embeddings, i.e. Eq. (6). The normalized embedding loss function is equivalent to a metric of distance between matrices only for spectral embedding methods such as LE. However, the equivalence is not immediate for arbitrary embedding methods. Second and more important, in a standard low-rank approximation, one generally aims at finding the best low-rank reduction of a full-rank matrix by constraining the maximum rank of the approximation. In our case instead, we seek the best rank reduction constrained by a maximum amount of tolerated discrepancy between the low- and the high-dimensional embeddings.

---

### Summary benchmarks-full set – 2024 [^c9abe316]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — GRADE evidence quality ratings are defined for forming recommendations for care as follows: "Good quality (GQ): Further research is very unlikely to change our confidence in the estimate of effect", "Moderate quality (MQ): Further research is likely to have an important impact on our confidence in the estimate of effect and may change the estimate", and "Insufficient quality (IQ): Further research is very likely to have an important impact on our confidence in the estimate of effect and is likely to change the estimate; any estimate of effect is very uncertain".

---

### Sources of confidence in value-based choice [^4f84ca43]. Nature Communications (2021). High credibility.

Fig. 5
The efficient coding model.

a The decision process with three distinct process stages, color-coded to match the graphs. The prior matches the distribution of subjective values v of supermarket products. When choosing between two items, subjects look repeatedly at them, spending unequal time on the two options. The subjective values are internally encoded, the corresponding likelihood functionis constrained by the prior p (v) via efficient coding. Lastly, noise that occurs after the decoding is taken into account. b Standardized posterior estimates of the relationship between confidence and variance in the encoding process, the variance in the comparison process, and attentional factors (β θ). is not significantly different from 0 (P = 0.39), both and (β θ) are significantly bigger than zero with P < 0.001. The effect size of is not significantly different from 0. Both the effect sizes of and (β θ) are significantly bigger than zero with P < 0.001. Error bars indicate the mean posterior estimate of the standard deviation. P -values are based on the highest density interval of the posterior estimates. c Left column: the empirical probabilities of choosing the upper item; up: as a function of value difference; down: as a function of the difference in dwell time. Right column: the same as left but for the predicted probabilities by the efficient coding model. The trials are median split in high/low confidence. Value difference and dwell time difference are split into eight groups of equal size. Data are presented as mean values ± SEM. Source data are provided as a Source Data file. d Comparison of parameter estimates of two alternative efficient coding models: a model with agent-specific estimates of σ enc and a model that allows for trial-to-trial fluctuations of σ enc. The median of the posterior estimate of σ enc of the agent-specific model is indicated by the horizontal green line, the shaded grey area indicates the 95% confidence interval. The diagonal green lines represent 100 random samples of the posterior distribution of how σ enc changes with confidence in the model allowing for trial-to-trial fluctuations. e, f Same as (e) but for σ comp and θ. g Comparison of the effect sizes of the posterior estimates of σ enc and σ comp. Vertical red dashed line indicates the median, black lines indicate the 95% confidence interval. h Comparison of the posterior estimates of the intercept of θ in the efficient coding model and the RUM. i Comparison of the posterior estimates of the slope of θ in the efficient coding model and the RUM. For the whole figure n = 33 independent participants.

---

### Death by p-value: the overreliance on p-values in critical care research [^bfbf92b9]. Critical Care (2025). Medium credibility.

The p-value has changed from a versatile tool for scientific reasoning to a strict judge of medical information, with the usual 0.05 cutoff frequently deciding a study's significance and subsequent clinical use. Through an examination of five critical care interventions that demonstrated meaningful treatment effects yet narrowly missed conventional statistical significance, this paper illustrates how rigid adherence to p-value thresholds may obscure therapeutically beneficial findings. By providing a clear, step-by-step illustration of a basic Bayesian calculation, we demonstrate that clinical importance can remain undetected when relying solely on p-values. These observations challenge current statistical paradigms and advocate for hybrid approaches-including both frequentist and Bayesian methodologies-to provide a more comprehensive understanding of clinical data, ultimately leading to better-informed medical decisions.

---

### It's that time again [^2468ea4d]. Nature Neuroscience (2010). Medium credibility.

How do we estimate the duration of a temporal interval in a familiar context? A new study finds that it is appropriate, perhaps even advantageous, to tolerate a small bias in our estimate to reduce the overall temporal uncertainty.

---

### Distinct value computations support rapid sequential decisions [^0e7cf925]. Nature Communications (2023). High credibility.

The model selected a fixed value of the environment associated with the most likely block. This formulation is related to an established approximation for solving partially-observable Markov decision processes (POMDPs) known as the Most Likely State algorithm. This algorithm is well-studied, has precedence in the literature as a heuristic approximation for the full posterior distribution over states, and may be biologically plausible as it is computationally tractable compared to more complex solutions to POMDPs.

Belief state model

Like the inferential model (above), the belief state model has three distinct value parameters and calculates the probability of being in each block using Bayes' Rule. However, rather than selecting the value associated with the most probable block, the model uses the weighted average of each value, weighted by that probability, that is, While this model uses the full posterior distribution over states, model comparison found that it was comparable to the simpler Most Likely State algorithm (above; Supplementary Fig. 8). In fact, when the belief distributions were stable (e.g. in adaptation blocks), these models were identical. For that reason, we exclusively used the Most Likely State model (above) for this paper.

There may be alternative normative strategies for this task given different sets of assumptions. For instance, assuming an infinite time horizon, one might compute the average kappa under the Markov process determining block transitions, starting from the current state. With a sufficiently long time horizon, this average will be dominated by the steady-state distribution of the Markov process, which would predict no contextual modulation of wait times. Given that the rats exhibited strong contextual effects, this strategy is not consistent with their behavior. We therefore did not explore such a model in the current manuscript.

Inferential model with lambda parameter

To account for potentially sub-optimal inference across rats, we developed a second inferential model. This model also uses Bayes' rule to calculate the block probabilities, except with a sub-optimal prior, Prior subopt. Specifically, we introduce a parameter, λ, that generates the sub-optimal prior by weighting between the true, optimal prior (P (B t), Eq. 5), and a flat, uninformative prior (Prior flat, uniformly 1/3), that is, When λ = 1, this model reduces to the optimal inferential model, and when λ = 0, this model uses a flat prior and the block probabilities are driven by the likelihood.

---

### Similarity and economy of scale in urban transportation networks and optimal transport-based infrastructures [^b0dc5065]. Nature Communications (2024). High credibility.

Alternative approaches can be considered to design a network infrastructure from simple mechanisms. Examples are cost-benefit analysis, maximizing for efficiencyaccounting for paths and flows of passengers, or minimizing the total length, as in the Euclidean minimum spanning tree problem. In discrete settings, when an initial network backbone is given, the cost in Eq. (4) has been shown to be implicitly related to the total path length minimization accounting for the passengers' trajectories. One main difference between ours and these types of approaches is that we focus on a continuous space (as opposed to discrete settings) where the only necessary input is a set of origins and destinations, but otherwise no initial backbone network is given. This enables the design of a network from scratch, simulating where nodes and edges should be located in space to minimize the cost.

Once the optimal (μ *, u *) are obtained, one can then use the model described in ref.to extract a final network structure, i.e. a set of nodes, a set of edges connecting them, and their weights proportional to the conductivities. This can then be compared with the one observed from real data and repeated for various values of β. It is important to remark that in our setting, besides the parameter β, the other input quantities that need to be specified are origins and destinations via the function f. By imposing non-zero entries to this function, a user automatically selects a set of nodes that will be necessarily present in the output network. Otherwise, no other set of nodes or edges needs to be given but is rather automatically learned by solving the optimization problem described above. This implies that similarity between simulated and observed networks trivially increases as we add more non-zero terms in f. Here we consider the non-trivial scenario where we fix only a small number of origins and destinations, as described in more detail below.

---

### Coverage bias in small molecule machine learning [^0fce924a]. Nature Communications (2025). High credibility.

We solve this problem using an algorithm engineering approach. It consists of first computing lower bounds on the distance. Should the maximum of these bounds already be larger than T, we report this maximum. Only if both bounds fail, we use an exact algorithm based on an integer linear program (ILP) to compute.

The bounds are similar to those used in ref. but we additionally take the weights of the edges into account. The first bound can be computed in linear time using radix sort. It maps nodes onto each other based on their weighted degree. We first sort both node sets V 1 and V 2 by weighted degree. Then, we iteratively go through both lists by taking and removing the first pair and adding the absolute difference of the weighted degrees to the bound. Should one graph contain more nodes than the other, we also add the weighted degrees of the additional nodes to the bound. In the end, we divide the bound by two, because each edge has been considered twice in this calculation.

The second bound is also based on mapping nodes onto each other, but now we take the atom types of both nodes in an edge into account. We therefore, we construct bipartite graphs with nodes from each atom type on each side. Should the molecules have different numbers of an atom type, we fill up the graphs such that both sides have an equal number of nodes. We now link pairs of nodes in each bipartite graph and set the weight of this edge to the minimum weight to match the two nodes onto each other, which we compute using a complete enumeration approach for mapping the local neighborhoods. We calculate the final bound by computing minimum weight perfect matchings in each of the bipartite graphs, summing up the results and, again, dividing by two. See ref.for correctness proofs. We can compute this bound in cubic time in the number of nodes (heavy atoms), using the Hungarian algorithmor faster methods for finding minimum-weight perfect matchings. In contrast, executing the Integer Linear Program below may require exponential time.

---

### Degenerate boundaries for multiple-alternative decisions [^33b7df25]. Nature Communications (2022). High credibility.

For closer scrutiny of the optimal region, five sections of the reward landscapes for 3AFC curve(θ, α) parameterization are shown in Fig. 6. These sections correspond to values α = {−20, 10, 0, 10, 20} (including the flat boundary α = 0) to provide a detailed look at the reward landscape peaks with 100-fold more samples of θ than in Fig. 5. Evidently, the peak changes with cost ratio c / W and θ (Fig. 6, right column). This analysis leads to the observations: (I) As c / W increases, the reward landscape maximum covers a broader range of θ values (section peaks separate) and appears to acquire slope (peaks diverge in height, with a higher-to-lower pattern). (II) The spread of the average rewards at the peak of each section overlaps (red dashed lines), decreasing as c / W increases but not diminishing to zero. (III) Extracting near-to-optimal parameters by taking all points within a small δ = 0.02 range of the peak average reward yields a set of near-optimal decision boundaries over a broad range of θ and α values (black points in Fig. 6).

Fig. 6
Sections through reward landscapes.

Five sections through the reward landscapes from Fig. 5 a–c for the 3AFC curve(θ, α) parameterization are shown in the left-hand column of panels for α = {−20, −10, 0, 10, 20} (colors in legend). Note that the α = 0 (cyan) sections are flat decision boundaries. Average rewards across the sections are shown in the right-hand column with their mean (solid lines) and spread (transparent area), both are estimated with Gaussian Process regression; 95% confidence bounds. The red dashed lines show the overlap between the spread of all sections at their peak mean rewards. Maximal regions lie within a small fraction (δ = 0.02) of the standard deviation of the peak reward, giving a degenerate set of decision boundaries with close-to-optimal rewards (black points).

---

### California mugwort [^e23881b2]. FDA (2009). Low credibility.

Volume desired × Concentration desired = Volume needed × Concentration available.

Example 1: If a 1:10 w/v extract is available and it is desired to use a 1:1,000 w/v extract substitute as follows:

Vd × Cd = Vn × Ca

10ml × 0.001 = Vn × 0.1

0.1 mL = Vn

Using a sterile technique, remove 0.10 mL of extract from the 1:10 vial and place it into a vial containing 9.90 mL of sterile diluent. The resulting ratio will be a 10 mL vial of 1:1,000 w/v.

Example 2: If a 10,000 pnu/ml extract is available and it is desired to use a 100 pnu/ml extract substitute as follows:

10ml × 100 = Vn × 10,000

0.1 mL = Vn

Using a sterile technique, remove 0.10 mL of extract from the 10,000 pnu/ml vial and place it into a vial containing 9.90 mL of sterile diluent. The resulting concentration will be a 10 mL vial of 100 pnu/ml.

Example 3: If a 10,000 AU/ml or BAU/ml extract is available and it is desired to use a 100 AU/ml or BAU/ml extract substitute as follows: Vd × Cd = Vn × Ca

---

### Severe multi-year drought coincident with hittite collapse around 1198–1196 BC [^0ba927cc]. Nature (2023). Excellent credibility.

We investigated the Gordion chronology, detrending the individual tree-ring width series to derive a maximized climate signal, which we compared with modern analogous climate data for the central Anatolian region (Extended Data Figs. 1, 3–7 and Methods). Taking the lowest 25% of growth values in the Gordion chronology as indicating drier years, these events occur regularly between around 1497 and 797(Fig. 1 and Extended Data Figs. 5 and 8a). For comparison, the Polatlı meteorological station, which offers the nearest modern climate analogue to ancient Gordion (17 km away), records less than 300 mm of annual precipitation for approximately 20% of the period from1929 to2009. Total annual precipitation of 300 mm is regarded as an approximate minimum threshold for a viable wheat harvest. Only 6.25% of years from1929 to2009 recorded less than 250 mm of annual precipitation, less than the amount generally regarded as the minimum for cereal cultivation in the Near East, and an amount that would probably produce serious harvest reduction or failure. Although regional precipitation values vary, the largely semi-arid central Anatolian region demonstrates overall congruent records or trends (Methods). In particular, recent and historical instances of extreme arid years are usually common to much of the greater central Anatolian or core Hittite region, (Extended Data Figs. 6 and 7). Thus, if we regard the relative occurrence ratio of drier years in the recent Polatlı record as indicative for the broadly similar period of the drier later second millenniumin central Anatolia, and use the lowest 20% of values as indicating probable substantial reductions in the harvest, then the Gordion chronology contains only 80 or 85 out of 701 years (11.4%–12.1%) that are part of two or more consecutive such dry years. Between 1270 and 1135, when we also have our best sample replication with 10–18 trees per year, there are only six such sets of years. Only 13 or 16 years (1.9%–2.3%) in total are part of two or more consecutive drought years with the lowest 6.25% of values, representing probable serious harvest reduction or failure. Between around 1270 and 1135there is only one interval in these 135 years — 1198–1196 — with two or more consecutive years in the lowest 6.25% of values (three consecutive years). Further, in the 12-year period from 1198 to 1187, there were between 6 and 8 years (50–67%) in the lowest 20% of values (Fig. 1 and Extended Data Fig. 8a). Smoothing the data with a 28-point Savitzky–Golay filter representing an average human generational timeframe, the period around this time represents either the driest or second-driest multi-year interval between 1400 and 1000. This extremely dry interval stands out as a probably substantial climate challenge to food production and subsistence in central Anatolia that may have defeated normal strategies and storage provision in the Hittite administrative core. The dates — approximately 1198–1196 — are compatible with the historically derived timeframe of Hittite collapse and reorientation. and lend an historical specificity that is usually lacking in general low-frequency arguments suggesting linkages between climate and history.

---

### Bending the cost curve: time series analysis of a value transformation programme at an academic medical centre [^bbc0a10a]. BMJ Quality & Safety (2019). High credibility.

In the ITS analysis, fully adjusted total variable direct costs per case declined an average of 0.22% per month relative to baseline trends (95% CI −0.31 to −0.13), which was partially offset by a non-significant early increase of 1.95% (95% CI −0.12 to 4.06) in costs. This resulted in a net overall reduction in costs of 7.7% by the end of the study period relative to the expected cost without intervention (table 3). The medical cohort declined an average 0.20% per month (95% CI −0.33 to −0.08), while the surgical cohort declined 0.37% per month (95% CI −0.48 to −0.26). In dollar terms, these savings were equivalent to approximately $59.3 million overall ($10.3 million for medicine and $49.0 million for surgery patients). Intervention costs were approximately $1.5 million per year ($5.375 million in the study period), making net savings approximately $53.9 million.

Table 3
Cost outcomes from interrupted time series analysis

Our wage and CMI-adjusted expense per discharge dropped from 13% above the 50th percentile for COTH hospitals in 2014 Q1 to 2% above the 50th percentile in 2017 Q4. Cleverly and Associates data showed that by the end of 2016 our inpatient Medicare loss per case was close to average for academic medical centres.

LOS in days decreased by 0.25% per month relative to prior trends (95% CI −0.34 to 0.17), which represents an 11.8% relative decline over the course of the study period, or a decrease of approximately half a day (table 4).

Table 4
Length of stay, readmission and mortality outcomes from interrupted time series analysis

We did not find any statistically significant changes in the counterbalancing outcomes of 30-day same-hospital readmission rates and in-hospital mortality rates (table 4). On Hospital Compare, NYULH currently has a better than national average hospital-wide readmission rate and better than average mortality in three of the six reported conditions. Moreover, NYULH is currently ranked five stars (the maximum) in overall Medicare quality — an aggregate of 57 quality measures.

---

### Addendum guidelines for the prevention of peanut allergy in the United States: report of the National Institute of Allergy and Infectious Diseases-sponsored expert panel [^a8d05d24]. Annals of Allergy, Asthma & Immunology (2017). Medium credibility.

Peanut introduction recipes — option 2 (thinned smooth peanut butter) specifies 2 teaspoons (9–10 g of peanut butter; approximately 2 g of peanut protein) with preparation steps: Measure 2 teaspoons of peanut butter and slowly add 2 to 3 teaspoons of hot water, stir until peanut butter is dissolved, thinned, and well blended, let cool, and increase water amount if necessary (or add previously tolerated infant cereal) to achieve consistency comfortable for the infant.

---

### Clinical impact of the maximum mean value of home blood pressure on cardiovascular outcomes: a novel indicator of home blood pressure variability [^f824f7ed]. Hypertension (2021). Medium credibility.

[Figure: see text].

---

### Sodium chloride (Sodium chloride 0.9%) [^626ec77f]. FDA (2025). Medium credibility.

SPECIFICATION

50ml: 0.45g; 100ml: 0.9g; 250ml: 2.25g; 500ml: 4.5g; 1000ml: 9g

---

### Measuring low-value care in Medicare [^2988b581]. JAMA Internal Medicine (2014). Medium credibility.

Importance

Despite the importance of identifying and reducing wasteful health care use, few direct measures of overuse have been developed. Direct measures are appealing because they identify specific services to limit and can characterize low-value care even among the most efficient providers.

Objectives

To develop claims-based measures of low-value services, examine service use (and associated spending) detected by these measures in Medicare, and determine whether patterns of use are related across different types of low-value services.

Design, Setting, and Participants

Drawing from evidence-based lists of services that provide minimal clinical benefit, we developed 26 claims-based measures of low-value services. Using 2009 claims for 1,360,908 Medicare beneficiaries, we assessed the proportion of beneficiaries receiving these services, mean per-beneficiary service use, and the proportion of total spending devoted to these services. We compared the amount of use and spending detected by versions of these measures with different sensitivity and specificity. We also estimated correlations between use of different services within geographic areas, adjusting for beneficiaries' sociodemographic and clinical characteristics.

Main Outcomes and Measures

Use and spending detected by 26 measures of low-value services in 6 categories: low-value cancer screening, low-value diagnostic and preventive testing, low-value preoperative testing, low-value imaging, low-value cardiovascular testing and procedures, and other low-value surgical procedures.

Results

Services detected by more sensitive versions of measures affected 42% of beneficiaries and constituted 2.7% of overall annual spending. Services detected by more specific versions of measures affected 25% of beneficiaries and constituted 0.6% of overall spending. In adjusted analyses, low-value spending detected in geographic regions at the 5th percentile of the regional distribution of low-value spending ($227 per beneficiary) exceeded the difference in detected low-value spending between regions at the 5th and 95th percentiles ($189 per beneficiary). Adjusted regional use was positively correlated among 5 of 6 categories of low-value services (mean r for pairwise, between-category correlations, 0.33; range, 0.14–0.54; P ≤ .01).

Conclusions and Relevance

Services detected by a limited number of measures of low-value care constituted modest proportions of overall spending but affected substantial proportions of beneficiaries and may be reflective of overuse more broadly. Performance of claims-based measures in supporting targeted payment or coverage policies to reduce overuse may depend heavily on how the measures are defined.

---

### American Society of Echocardiography minimum standards for the cardiac sonographer: a position paper [^fdbfae37]. Journal of the American Society of Echocardiography (2006). Medium credibility.

Diagnostic ultrasound professional — core functions: Diagnostic ultrasound professionals perform patient assessments, acquire and analyze data obtained using ultrasound and related diagnostic technologies, provide a summary of findings to the physician to aid in patient diagnosis and treatment, and use independent judgment and systematic problem-solving methods to produce high-quality diagnostic information and optimize patient care.

---

### Defining the time-limited trial for patients with critical illness: an official American Thoracic Society workshop report [^d28d917f]. Annals of the American Thoracic Society (2024). High credibility.

Time-limited trials — framing to promote collaboration outlines in Table 3 that they "Should Be" "Framed by uncertainty", "Transparent", "Individualized and adaptable", "Iterative", "Oriented to patients' goals and priorities", "Trials of high-quality, standard-of-care therapies", and "A collaborative process between patients, surrogates, and clinicians", and "Should Not Be" "Framed by notions of "poor prognosis", "Coercive", "Prescriptive or binding", "Time pressured or finite", "Oriented to one specific outcome", "Lower quality care", or "Determined exclusively by clinicians".

---

### American association of clinical endocrinology consensus statement: comprehensive type 2 diabetes management algorithm-2023 update [^5e7b1f6f]. Endocrine Practice (2023). High credibility.

Adiposity-based chronic disease (ABCD) complications-centric model for overweight/obesity management outlines a three-step pathway: "STEP 1: ASSESS BMI", "STEP 2: ASSESS STAGE", and "STEP 3: IMPLEMENT PLAN". Staging criteria specify "STAGE 1 No ABCD Complications", " ≥ 1 Mild/ Moderate ABCD Complication(s)?", and " ≥ 1 Severe ABCD Complication(s)?". Implementation examples include "Maintain or Achieve Optimal Weight", "Structured Diet With Meal Replacements", "Structured Exercise Program with Oversight & Accountability", "Refer for Formal Sleep Study", and "Consider Bariatric Surgical Options".

---

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^6b37ae06]. American Journal of Kidney Diseases (2006). Medium credibility.

Minimally adequate hemodialysis — dose targets and frequency considerations for more-frequent schedules: In developing this CPR, the Work Group decided to target a minimum dialysis dose equivalent to an stdKt/V level of 2.0 per week, and this is the level obtained when one analyzes using a thrice-weekly schedule to an spKt/V of approximately 1.2 per treatment over 3.5 hours. In the absence of RKF, it is not possible to reach an stdKt/V of 2.0 by using a twice-weekly schedule, and kinetic modeling examined spKt/V per treatment needed to reach a weekly stdKt/V value of 2.0 for twice-weekly to 7-times-weekly schedules with dialysis treatment times ranging from 2 to 8 hours. These spKt/V values should be considered minimum values, not target values, and extending dialysis time is much more effective for controlling solute levels when frequency is increased to 4 to 7 treatments per week. For nocturnal HD, similar spKt/V values can be determined for 8-hour treatments and the Kt/V for an 8-hour treatment will be greater than 1.0; hence, the Work Group did not believe that adequacy determined by predialysis or postdialysis BUN monitoring is appropriate for nocturnal HD schedules. Given limited research maturity, the Work Group decided to refrain from proposing guidelines regarding minimally adequate therapy given more frequently than 3 times per week.

---

### A method for examining geographic variation when denominators are unknown, populations vary, and time is of the essence: back surgery as an example [^51c168f8]. Spine (2020). Medium credibility.

Level of Evidence: 3.

---

### Summary benchmarks-full set – 2024 [^ac55ce29]. AAO (2024). High credibility.

Idiopathic macular hole management — stages 3 or 4: For stage 3 or 4, vitreoretinal surgery is performed usually within a few weeks to a few months of diagnosis, with postoperative follow-up at 1–2 days, then 1–2 weeks during which time strict face down positioning is advised; frequency and timing of subsequent postoperative visits varies depending on the outcome of surgery and the patient's clinical course.

---

### A pediatrician's opinion [^75913bd2]. Pediatric Clinics of North America (2009). Low credibility.

Pediatricians are inundated by phrases such as "pay for performance" and "enhancement of payments tied into quality measurements". Although there is no argument that we must provide high quality care to our patients and must continuously improve ourselves, we need flexibility within the managed care criteria. Medicine is not only a science, but it is also an art with many interpretations.

---

### Advancing value-based medicine: why integrating functional outcomes with clinical measures is critical to our health care future [^00922699]. Journal of Occupational and Environmental Medicine (2017). Medium credibility.

University of California WorkStrong program — UC launched a program in 2012 to reduce disability costs, with UC Los Angeles reporting 4181 employees with multiple workers' compensation claims (15,944 injuries), including 968 (23%) with five or more claims and 223 (5%) with 10 or more claims; participation studies indicate improved overall function alongside reduced disability costs, and Figure 1 presents actual versus projected workers' compensation claims 31 months after graduation.

---

### A rise-to-threshold process for a relative-value decision [^87ff48cb]. Nature (2023). Excellent credibility.

Calcium imaging rates of around 0.5 Hz and laser powers of 20–30 mW at the specimen were needed to capture sufficient signal and sample the full presynaptic volume (approximately 50 × 50 × 60 µm³). Imaging rates and laser powers are similar in Extended Data Fig. 6l, m, to aid comparison (mean imaging rate of 0.50 and 0.56 Hz, respectively, with a minimum rate of all data at 0.36 Hz). Because the timestamp assigned to a volume scan was when the volume was half complete, data of around 1 s after cessation of stimulation in Extended Data Fig. 6l, m should minimally include the stimulation period; data 1.4 s (delay for 0.36 Hz) after stimulation do not include the stimulation period at all. These numbers also apply to the increase in ∆ F / F at stimulation onset. Half-decay times are the amount of time after cessation of stimulation required for signal value to return half-way between that at the end of stimulation and 5 s mean prestimulation. The half-decay times reported in the main text are the average of those for the three lower intensities shown in Extended Data Fig. 6d–f. To calculate an expected ∆ F / F half-decay time given a spike rate, GCaMP7f kinetics and calcium imaging rate, we convolved one of our spike-rate traces (second-lowest intensity shown in Extended Data Fig. 6e) with an exponential filter (τ = 300 ms) that estimates the off kinetics of GCaMP7fand then applied a boxcar filter (width, 2.8 s) that simulated the slowest frame rate in all experiments. The half-decay time of this simulated trace was 700 ms.

Substrate transition-triggered averages during calcium imaging or electrophysiology

Substrate transitions were identified using the fly's position on the wheel. For these analyses, substrate transition i was eliminated if substrate transitions i – 1 and i + 1 occurred within 4 s of each other. This empirically prevented events in which the fly rocked on the substrate boundary from being counted as multiple transitions. Note that, for all transition-triggered averages, if the fly were to have transitioned back to the original substrate — say, 20 s after the first transition — the data from 20 s onwards would not contribute to the post-transition average.

---

### Guidelines for the appropriate use of bedside general and cardiac ultrasonography in the evaluation of critically ill patients-part I: general ultrasonography [^d58fab9c]. Critical Care Medicine (2015). Medium credibility.

RAND voting lower-consensus paths — < 50% within 1 integer: If " < 50% of panelists voted within 1 integer", then " ≥ 50% achieved at 1 integer" yields "Some Consensus", while "Other responses or disagreement" lead to "No Consensus"; the bottom labels associate "Conditioned/weak Recommendation" with Some Consensus and "NO Recommendation" with No Consensus, and "Disagreement: if ≥ 30% of panelists voted outside the zone of the median".

---

### KDOQI clinical practice guideline for hemodialysis adequacy: 2015 update [^8d2ce0eb]. American Journal of Kidney Diseases (2015). Medium credibility.

KDOQI HD Adequacy Guideline — hemodialysis treatment time states that a minimum treatment time of 3 hours was previously selected as the "bare minimum", particularly in patients with low Kru (creatinine clearance < 2 mL/min), based on the standard-dose arm time in the HEMO trial (195 ± 23 minutes). Despite many members routinely initiating hemodialysis for 3.5 to 4 hours, the Work Group did not find sufficient evidence to change the minimal treatment time, but acknowledged that many patients require more than 3 hours to achieve optimal volume and metabolic control, and noted that recommended treatment times may be lengthened when time-dependent events include high interdialytic weight gains, poorly controlled blood pressure (BP), difficulty achieving dry weight, or poor metabolic control.

---

### KDIGO 2021 clinical practice guideline for the management of glomerular diseases [^47bee920]. Kidney International (2021). High credibility.

Regarding follow-up and surveillance for minimal change disease, more specifically with respect to assessment of treatment response, KDIGO 2021 guidelines recommend to define relapse as proteinuria > 3.5 g/day or protein-to-creatinine ratio > 3500 mg/g (or 350 mg/mmol) after complete remission has been achieved.

---

### How face perception unfolds over time [^040a5192]. Nature Communications (2019). High credibility.

Onset and peak latency analysis

To test for statistical differences in onset or peak latencies between different face dimensions, we performed bootstrap tests. We bootstrapped the subject-specific time courses (e.g. measured as decoding accuracy, partial correlation or commonality coefficient) 1000 times to obtain an empirical distribution of the onset (i.e. minimum significant time point post stimulus onset) and peak latencies (i.e. maximum correlation value between 80 and 180 ms post stimulus onset). We restricted the time window for the peak analysis to 180 ms post stimulus onset, since we were interested in the first peak occurring after stimulus onset, unconfounded from later peaks (e.g. due to stimulus offset responses). The 2.5 th and the 97.5 th percentile of these distributions defined the 95% confidence interval for onset and peak latency, respectively. For differences between latencies, we computed 1000 bootstrap samples of the difference between two latencies (e.g. onset) resulting in an empirical distribution of latency differences. The number of differences that were smaller or larger than zero divided by the number of permutations defined the p-value (i.e. two-sided testing). These p-values were corrected for multiple comparisons using false discovery rate (FDR) at a 0.05 level.

---

### Quantifying entanglement in a 68-billion-dimensional quantum state space [^b339b52f]. Nature Communications (2019). High credibility.

Adaptive sampling algorithm

For each configuration, experimental data are stored in nodes in a quad-tree decomposition of P whose levels describe increasingly fine detail. The i th node corresponds to a square area ofat locationwith span. Nodes are sampled by placing the corresponding, one-dimensional local patterns on the DMDs and generating a coincidence histogram during acquisition time T a = 0.5 s. Coincidences C i are counted within a 1-ns coincidence window centered on the coincidence peak; accidental coincidences A i are counted in a 1-ns window displaced 2 ns from the coincidence window. Coincidence and accidental values are appended to a list each time the node is sampled. The estimated count rate, whereis a calibrated, relative fiber coupling efficiency. Optionally, A i can be subtracted from C i for accidental removal. Uncertainty is computed by assuming Poissonian counting statistics for C i and A i and applying standard, algebraic propagation of error through the calculation of the entanglement quantity (Eq. (3)).

The data collection algorithm consists of a partitioning phase followed by an iterative phase. During partitioning, the algorithm repeatedly iterates through a scan-list of leaves of the tree. Node i is considered stable when sgn(αR T − R i) is known to at least β standard deviations of certainty, where splitting threshold α (0 ≤ α ≤ 1) and stability criterion β are user-chosen heuristics. Stable nodes are no longer measured. If a node is stable and R i ≥ αR T, the node is split into four equal-sized sub-quadrants, which are initially unstable and added to the scan-list. Optionally, a maximum resolution (maximum tree depth) may be set.

The transition to the iterative phase occurs when the percentage of unstable leaves is < Γ, a user-chosen parameter. At this point, stability is ignored and all leaf nodes are scanned repeatedly and guaranteed to have the same total acquisition time. Various final stopping criteria can be used; we chose a fixed total run time. Note that heuristic parameters α, β, and γ may be changed during operation if desired. For the data shown in this manuscript, α = 0.002, β = 2, and Γ = 0.15 with a 30-h runtime.

The probability distributionis computed by uniformly distributing the estimated count rate (with or without accidental subtraction) from each leaf node across its constituent elements in, followed by normalization.

---

### Six steps in quality intervention development (6SQuID) [^221654f2]. Journal of Epidemiology and Community Health (2016). Low credibility.

Conclusion

In order to improve the effectiveness of public health interventions, a systematic approach to intervention development is required, as well as rigorous evaluation. However, little practical guidance exists for public health practitioners and researchers that explains the essential stages of intervention development. We argue that this process can be broken down into six key steps: defining and understanding the problem; identifying modifiable determinants; deciding on the mechanisms of change; clarifying how these will be delivered; testing and adapting the intervention; and collecting initial evidence of effectiveness. This model imposes somewhat arbitrary cut-offs in the process of intervention development and suggests a linear progression. In practice developers often return to an earlier step in the sequence before reaching step 6, and subsequently 'definitive trials' can lead to further revisions of the intervention. However, we hope that if each of these six steps is carefully addressed in the design of interventions better use will be made of scarce public resources by avoiding the costly evaluation, or implementation, of unpromising interventions.

What is already known on this subject?

There is little practical guidance for researchers or practitioners on how best to develop public health interventions. Existing models are generally orientated towards individual behaviour change and some are highly technical and take years to implement.

What this study adds?

This paper provides a pragmatic six-step guide to develop interventions in a logical, evidence-based way to maximise likely effectiveness. If each step is carefully addressed, better use will be made of scarce public resources by avoiding the costly evaluation, or implementation, of unpromising interventions.

---

### Thinking outside the curve, part I: modeling birthweight distribution [^b60d1510]. BMC Pregnancy and Childbirth (2010). Low credibility.

b. Simulation study on calibrating confidence intervals

For our second simulation study we generated 25 overlapping data sets of size 50,000 from design C in Table 3, the degree of overlap consistent with a population of 200,000. For each of various C between 2.0 and 5.0, we used Equation (7) to form confidence intervals for the mixture parameters p 1, p 2, p 3, p 4, μ 1, μ 2, μ 3, μ 4, σ 1, σ 2, σ 3, σ 4. We recorded how many of the mixture parameters were contained in their respective confidence intervals. This was repeated nine more times, and we tabulated how many of the 120 = 12 × 10 confidence intervals contained their targets. Confidence intervals were also formed using Equation (6) for comparative purposes. The above steps were repeated with overlapping data sets consistent with a population of 1,000,000 and with nonoverlapping data sets consistent with an effectively infinite population.

The results are summarized in Table 5. With an effectively infinite population, only 81.7% of the confidence intervals formed using Equation (6) contained their targets at C = 5.0. The confidence intervals formed using Equation (7) contained their targets 95.0% of the time at C = 2.5. The adjustment suggested by Equation (8) appears reasonable: φ = .05 = 50,000/1,000,000 and N rep = 25 yield C φ = 1.315 C 0, which accords with the 95.8% capture of mixture parameters at C = 3.5 ≈ 1.315 × 2.5 with a population of 1,000,000.

Table 5
Confidence Interval Coverage Probabilities in Simulation Studies

The row with " C " = 2 and "Population size" = 200,000 identifies the numbers and percentages of confidence intervals containing their targets of mixture parameters, based on 10 repetitions in each of which 25 samples of size 50000 were simulated from a 4-component normal mixture with 12 parameters; results under the heading of "Bias adjustment included" are based on Equation (7) with C = 2, results under the heading of "Bias adjustment omitted" are based on Equation (6) with C = 2, and the 25 samples of size 50000 had overlap consistent with a population size of 200,000. Other rows correspond to different choices of C and/or population sizes.

---

### KDIGO 2021 clinical practice guideline for the management of glomerular diseases [^e241c7cc]. Kidney International (2021). High credibility.

Regarding follow-up and surveillance for minimal change disease, more specifically with respect to assessment of treatment response, KDIGO 2021 guidelines recommend to define corticosteroid-resistant MCD as persistence of proteinuria > 3.5 g/day or protein-to-creatinine ratio > 3500 mg/g (or 350 mg/mmol) with < 50% reduction from baseline despite prednisone 1 mg/kg/day or 2 mg/kg every other day for > 16 weeks.

---

### A computable algorithm for medication optimization in heart Failure with reduced ejection fraction [^7942144d]. JACC: Advances (2023). Medium credibility.

Figure 3
Wald and False Discovery Rate P Values for Each Medication Optimization Score Value

The P values were generated from the cut point analysis in HF-ACTION. The horizontal dotted line represents a P value of 0.05. The vertical dotted line represents the statistical cut points. Solid dots represent the Cox model's Wald P values, and open triangles represent the false discovery rate P values. FDR = false discovery rate; MOS = medication optimization score.

---

### Investigating suspected cancer clusters and responding to community concerns: guidelines from CDC and the council of state and territorial epidemiologists [^945666c0]. MMWR: Recommendations and Reports (2013). Medium credibility.

Cancer cluster investigation — Step 2 decision to close or proceed to Step 3 is based on multiple factors, and to interpret the standardized incidence ratio (SIR) the health agency must assess whether there are enough cases and population for statistical stability and, in general, that the population size of a typical census tract is the smallest denominator that will allow reliable results; if the numerator allows stability, agencies should consider whether the 95% CI exclude 1.0, along with environmental contaminants, population changes, and other information beyond the SIR to estimate whether cancers represent an excess and share a common etiology. A SIR of limited magnitude that is not statistically significant with lack of known contaminant association and no increasing trend justifies closing at Step 2, whereas a statistically significant SIR of great magnitude with an increasing trend and a known contaminant argues for continuing to Step 3; example thresholds indicate that an SIR of < 2.0 with CIs surrounding or overlapping 1.0 and < 10 cases might justify closing, while an SIR of > 4 with CIs not overlapping 1.0 and ≥ 10 etiologically linked cases should encourage advancing to Step 3.

---

### Penicillin V potassium [^21243a1a]. FDA (2024). Medium credibility.

The dosage of penicillin V potassium PO for treatment of respiratory infections in adults (mild-to-moderate, pneumococcal) is 250–500 mg PO QID until the patient has been afebrile for at least 2 days

---

### Cabergoline [^f8385306]. FDA (2025). Medium credibility.

The dosage of cabergoline PO for treatment of hyperprolactinemia in adults (idiopathic or due to pituitary adenoma) is 0.25 mg PO 2× per week for 6 months

---

### The neural correlates of value representation: from single items to bundles [^3ab17c49]. Human Brain Mapping (2023). Medium credibility.

The model predicts that choices in which values are close yield a 50% chance of choosing one over the other, while choices that are far apart tend to certain outcomes. The model is then used to construct a likelihood function based on the above probabilities and we applied Maximum Likelihood Estimation techniques to retrieve parameters v j given the observed choices for each individual. In practice, this procedure was implemented in Matlab (MathWorks) with standard algorithms, which apply the likelihood function over the entire data, and estimate the value parameters that best match the observed choices. Based on these retrieved values, we assigned implicit rankings across all options (V), and across options in the CONTROL condition (CV), SCALING condition (ScV), and BUNDLING condition (BV).

A few remarks are in order. First, because the value of the reference option is the same across all trials, parameterizing the subjective value of the on‐screen option or the subjective value of the difference between the two options (value of on‐screen option v j − value of off‐screen reference option v 0) is the same. We can therefore without loss of generality interpret v j as either the value or the relative value of the on‐screen option. Second, the λ model is equivalent to a softmax representation: the probability of choosing an option is a softmax function with temperature 1/ λ. This is the case only because the distribution is an extreme value distribution. Third, our model shares features with models that focus on entropy (Bernacer et al; Goni et al.). This is the case because there is a mathematical connection between entropy and the Random Utility Model (Matějka & McKay,), as there is one between entropy and softmax (which is equivalent to the Boltzmann distribution and maximizes entropy). Last, ideally, we would like to estimate both value parameters and temperature. However, λ is not identified in this simple model (we could rewrite λv j as V j and λv 0 as V 0 without affecting the model), making it impossible to estimate both types of parameters at the same time. To estimate subjective value, we normalize λ to 1.

---

### Standards of care in diabetes – 2025 [^854ffaab]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for hypoglycemia, more specifically with respect to pregnant patients, ADA 2025 guidelines recommend to do not use commonly used estimated HbA1c and glucose management indicator calculations in pregnancy as estimates of HbA1c.

---

### The 30-degree angle revisited [^eec48357]. Journal of the American Academy of Dermatology (2005). Low credibility.

The standard surgical ellipse, with 30 degrees apical angles and a length-to-width ratio of 2 or 3 to 1, works optimally on a flat surface. The same pattern, when used for excisions on strongly convex or concave surfaces, leads to distortions which may require significant revisions. The reason for these discrepancies is explainable by the mathematical differences between flat Euclidian geometry and curved non-Euclidian geometry. Understanding these basic mathematical principles as applied to cutaneous surgery should lead to better preoperative planning, fewer intraoperative surprises, and more pleasing results.

---