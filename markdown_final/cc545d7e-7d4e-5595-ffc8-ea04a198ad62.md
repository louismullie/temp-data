# Medical Question & Answer

**Sample ID**: cc545d7e-7d4e-5595-ffc8-ea04a198ad62
**Dataset Index**: 268592

---

## Question

Statement 1| Besides EM, gradient descent can be used to perform inference or learning on Gaussian mixture model. Statement 2 | Assuming a fixed number of attributes, a Gaussian-based Bayes optimal classifier can be learned in time linear in the number of records in the dataset.

Answer Choices:
A. Statement 1 is true, Statement 2 is partially true
B. Statement 1 is partially true, Statement 2 is false
C. True, True
D. Statement 1 is false, Statement 2 is partially true
E. False, False
F. Both statements are partially true
G. True, False
H. Both statements are partially false
I. False, True
J. Statement 1 is partially true, Statement 2 is true

---

## Answer

> Let's see… What do we have here? The user is asking whether two statements about Gaussian mixture models and Gaussian-based Bayes classifiers are true, partially true, or false, and to select the best answer from the provided choices. Let's break this down step-by-step. First, I need to think about Statement 1 on whether gradient descent can be used for inference or learning in Gaussian mixture models beyond EM. Then, I should verify Statement 2 on whether a Gaussian-based Bayes optimal classifier can be learned in time linear in the number of records when the number of attributes is fixed. Finally, I will reconcile any apparent contradictions and map the conclusion to the answer choices, making sure I cite the references correctly as I go.

> Let me first confirm the context and definitions so I don't jump to conclusions. Gaussian mixture models are typically fit with EM, which iterates between an E-step computing posterior component memberships and an M-step updating means, covariances, and mixing coefficients, and this is well documented in multiple sources including neuroimaging and biostatistics applications [^bfda1b42] [^0464558c]. Gradient descent is a first-order optimization method that iteratively updates parameters in the direction of steepest descent of a loss or negative log-likelihood, and it is widely used in machine learning when analytical solutions are infeasible [^70a823d9].

> Now, Statement 1 claims that besides EM, gradient descent can be used to perform inference or learning on Gaussian mixture models. Wait, let me verify whether gradient-based methods are actually employed for GMMs in practice or theory. I initially thought EM is the only practical route, but hold on, that's too strong; gradient-based optimization can indeed be applied to the GMM likelihood, though EM is the canonical and often more efficient choice in many settings, and gradient descent is commonly used for other probabilistic models with latent variables where exact E-steps are not feasible, such as in variational inference frameworks and deep generative models that include mixture components or latent structure [^92d8aab5] [^bd52fbf3]. I should also note that in some scientific computing contexts, gradient descent is explicitly used to optimize likelihoods of generative models with Gaussian noise assumptions, reinforcing that gradient-based methods are valid for parameter estimation in probabilistic models, even if EM remains dominant for GMMs in many applications [^8ffd6558]. So, yes, gradient descent can be used for GMM learning, albeit EM is often preferred for convergence and simplicity, which makes Statement 1 true rather than false or only partially true [^notfound].

> Next, I should review Statement 2, which asserts that assuming a fixed number of attributes, a Gaussian-based Bayes optimal classifier can be learned in time linear in the number of records. Let me think about the computational steps. For a Gaussian Naive Bayes classifier, training involves computing class priors and, for each attribute, class-conditional means and variances; with a fixed number of attributes, each of these operations scales linearly with the number of records, so the overall training time is O(n) where n is the number of records, which is linear [^notfound]. Hold on, I should verify that this holds even when we consider the full multivariate Gaussian classifier; if we estimate a full covariance matrix, the per-record cost is higher, but with a fixed dimension d, the dominant term remains linear in n, even though the constant factor grows with d^2 for a full covariance, so the asymptotic scaling is still linear in n when d is fixed, which supports the statement's claim [^notfound]. Therefore, Statement 2 is true under the fixed-attribute assumption.

> Hmm, wait a minute, I initially wondered if the need to invert a covariance matrix might make this nonlinear, but that would be a mistake; the inversion is done once after aggregating sufficient statistics over the data, and with fixed dimension, that inversion cost is constant with respect to n, so the overall training complexity remains linear in the number of records, confirming that Statement 2 is true [^notfound].

> Putting this together, Statement 1 is true because gradient descent can indeed be used for GMM parameter estimation even if EM is more common, and Statement 2 is true because with a fixed number of attributes, Gaussian-based Bayes classifiers can be trained in time linear in the number of records. Therefore, the correct choice is C. True, True [^notfound].

---

Both statements are **true**. Statement 1 is true because gradient descent can optimize the likelihood of a Gaussian mixture model, although EM is more common and efficient [^notfound]. Statement 2 is true because, with a fixed number of attributes, a Gaussian-based Bayes optimal classifier has linear time complexity in the number of records, as it requires only computing class priors and class-conditional means/variances in a single pass [^notfound].

---

## Statement 1: Gradient descent for Gaussian mixture models

Gradient descent can be used to perform inference or learning on Gaussian mixture models (GMMs), although expectation-maximization (EM) is the **standard approach** [^0464558c]. EM iteratively updates parameters via E- and M-steps, whereas gradient descent directly optimizes the likelihood or a surrogate loss [^70a823d9]. Gradient descent is valid but typically less efficient than EM for GMMs; it may be used when EM is impractical or when integrating GMMs into larger differentiable systems [^notfound].

---

## Statement 2: Linear time complexity of Gaussian-based Bayes optimal classifier

A Gaussian-based Bayes optimal classifier can be learned in time linear in the number of records when the number of attributes is fixed. Training involves computing class priors and class-conditional means and variances, each of which scales linearly with the number of records; prediction is also linear in the number of records and attributes. Thus, both training and prediction have **linear time complexity** under the fixed-attribute assumption [^notfound].

---

## Conclusion

Both statements are **true**: gradient descent can be used for GMM learning, and a Gaussian-based Bayes classifier can be learned in linear time when the number of attributes is fixed. The correct answer is **C. True, True**.

---

## References

### The neural coding framework for learning generative models [^e1d89fa7]. Nature Communications (2022). High credibility.

Training setup

The parameters of all models, whether they were updated via backprop or by the NGC learning process, were all optimized using stochastic gradient descent using mini-batches (or subsets of data, randomly sampled without replacement) of 200 samples for 50 passes through the data (epochs). For the backprop-based models, we re-scaled the gradientsby re-projecting them to a Gaussian ball with radius of 5.0, which we found ensured stable performance across trials. For each model, upon completion of training, we fit a Gaussian mixture model (GMM) to the topmost neural activity layer (to serve as the model prior). This density estimator was trained with expectation-maximization and contained K = 75 components, where each component k ∈ K defines a multivariate Gaussian with mean μ k and covariance Σ k parameters as well as a mixing coefficient π k.

The density modeling task

Given a dataset X ∈ {0, 1} D × S, where S is the number of vector pattern samples and D is the dimensionality of any given pattern, the goal is to learn a density model of p (X), or the probability of sensory input data, where a subset of B vectors is denoted as x ∈ {0, 1} D × B. We parameterize the probability distribution p (X) via p Θ (X) by introducing learnable parameters Θ. Since computing the marginal log-likelihooddirectly is intractable for all the models in this paper, we estimate it by calculating a Monte Carlo estimate using 5000 samples according to the recipe: (1) sample the GMM prior, (2) ancestrally sample the directed neural generative model (whether it is a baseline or a GNCN) given the samples of the prior.

The reconstruction metric, binary cross-entropy (BCE), also known as the negative Bernoulli log-likelihood, was computed as follows:(measured in nats).is the predicted sensory input (matrix) produced by a model under evaluation.

---

### Learning non-stationary langevin dynamics from stochastic observations of latent trajectories [^3294404b]. Nature Communications (2021). High credibility.

Model selection and uncertainty quantification

So far we validated the inference accuracy by comparing the fitted model and its likelihood with the ground truth. However, in practical applications, the ground truth is unknown, and we need a procedure for selecting the optimal model among many models produced across iterations of the gradient descent. On early iterations, the fitted models miss some features of the correct dynamics (underfitting), whereas on late iterations, the fitted models develop spurious features (overfitting). The model that best captures the correct dynamics is discovered at some intermediate iterations. The standard approach for selecting the optimal model is based on optimizing model's ability to predict new data (i.e.g.eneralization accuracy), e.g. using cross-validation. However, methods optimizing generalization accuracy cannot reliably identify correct features and avoid spurious features when applied to flexible models. An alternative approach for model selection is based on directly comparing features of the same complexity discovered from different data samples. Since true features are the same, whereas noise is different across data samples, the consistency of features inferred from different data samples can separate the true features from noise. Model selection based on feature consistency can reliably identify the correct features for stationary dynamics. Here we extended this method for the case of non-stationary dynamics (Methods section).

---

### Interpretable generative deep learning: an illustration with single cell gene expression data [^92d8aab5]. Human Genetics (2022). Medium credibility.

Fitting deep generative models

Since deep generative models learn a distribution over the observed and latent variables, potentially very complex distributions have to be approximated. Consider, e.g. (Gaussian) mixture models that estimate the parameters of a mixture of (Gaussian) distributions. The probabilities with which observations are assigned to the components of the mixture can be seen as latent factors that have to be inferred. In the popular expectation-maximization (EM) algorithm (Dempster et al.), values for the latent variables computed in a current iteration are plugged in for an iterative optimization. For deep generative approaches, this approach is not possible because the values of the latent variables can not exactly be computed. Instead, they have to be approximated. For approximation, there are two prominent approaches termed (Bayesian) variational inference (Blei et al.) and Gibbs sampling (explained in Resnik and Hardisty). In variational inference, we assume that the distribution of the latent variables is related to a simple distribution (prior distribution). During training, we condition on the prior distribution to infer the posterior distribution of the latent variables. Specifically, we get an approximation for the distribution of the latent variables, given the training (here gene expression) data. Gibbs sampling is a Markov chain Monte Carlo (MCMC) technique. In DGMs, the objective is to infer the joint distribution of observed and latent variables. This is achieved by estimating a series of conditional expectations for latent and observed variables. Specifically, in each iteration, the states of all neurons are sequentially inferred by conditioning on the states of all other neurons. After performing the procedure for several iterations, we arrive at an estimate for the joint distribution of observed and latent variables. The absence of lateral connections in a neural network makes Gibbs sampling comparatively efficient. Nevertheless, this approach is still considerably slower than variational inference, especially when a large number of observed and latent variables are studied. The above presented approaches to approximate the joint distribution of observed and latent variables indicate a close relationship between the training of DGMs and generating synthetic samples.

---

### Visual motion perception as online hierarchical inference [^aa53d6f1]. Nature Communications (2022). High credibility.

In this model, the structure of a scene is fully characterized by the vector of motion strengths, λ = (λ 1. λ m. λ M), which describe the presence (λ m > 0) or absence (λ m = 0) of motion components, as well as their typical speed. In other words, given a reservoir of components, C, which might have been learned to occur in visual scenes in general, knowing λ is equivalent to knowing the motion structure of the scene. Inferring this structure in turn becomes equivalent to inferring the corresponding motion strengths.

An agent faces two challenges when performing inference in this generative model (Fig. 1 h). First, inference needs to be performed on the fly (i.e. online) while sensory information arrives as an ongoing stream of noisy velocity observations. Second, how observed motion is separated into latent motion sources, s, and motion structure, λ, is inherently ambiguous, such that inference needs to resolve the hierarchical inter-dependence between these two factors. We address both challenges by recognizing that motion structure, λ, typically changes more slowly than the often volatile values of motion sources, s, facilitating the use of an online Expectation-Maximization (EM) algorithm to infer both. This separation of time scales yields a system of mutually dependent equations for updating λ and s and furthermore affords a memory-efficient, continuous-time online formulation that is amenable to a neural implementation (see Methods for an outline of the derivation, and Supplementary Note 2 for the full derivation). While the algorithm is approximate, it nonetheless performs adequate online hierarchical inference and closely resembles more accurate solutions, even for deeply nested motion structures (see Supplementary Fig. 1).

---

### Data-driven identification of tumor subregions based on intravoxel incoherent motion reveals association with proliferative activity [^bfda1b42]. Magnetic Resonance in Medicine (2019). Medium credibility.

Soft clustering of data into k classes was performed by fitting a Gaussian mixture model (GMM) using the expectation maximization (EM) algorithm. Each iteration of the algorithm starts with calculation of the class probabilities given the current set of model parameters (E‐step) as:where the superscript (t) shows the iteration index, is the class of voxel i, l is a given class label ranging from 1 to k, is a column vector containing D, f and, is the multivariate Gaussian probability density function with meanand covariance, andis prior class probability, also commonly denoted the mixing coefficient. The E‐step is followed by recalculation of the model parameters given the new class probabilities (M‐step) as:where N is the total number of voxels and T indicates transpose. Fitting the GMM thus comprises estimation of k sets of prior class probabilities, mean vectorsand covariance matrices.

Assuming that tumor subregions typically are larger than the voxel size, neighboring voxels are more likely to belong to the same cluster. Inclusion of this spatial information has previously been shown to enhance the clustering performance. 20 Therefore, a Gaussian hidden Markov random field model (GHMRFM) was implemented as described by Zhang et al. 33 The difference compared with the GMM is the prior class distribution, which is set to depend on the class of neighboring voxels. At each E‐step in the EM‐algorithm, the class probabilities are instead calculated aswhereis given by the Gibbs distributionwhere Z is a normalizing constant anddenotes the classes of voxels in the neighborhood of voxel i, which was defined as the four voxels in the same image slice that shared a side with voxel i, i.e. the closest vertical and horizontal neighbors. Calculation ofnecessitates estimation of the class labels. This was achieved through maximization of the posterior class probability using iterative conditional modes. 33, 34

---

### Solving the where problem and quantifying geometric variation in neuroanatomy using generative diffeomorphic mapping [^76e785d5]. Nature Communications (2025). High credibility.

Inference and the EM-LDDMM likelihood function

Our generative formulation produces an expected target image, and differences from this expectation are well modeled as Gaussian noise. Estimation of the unknown transformation parameters are therefore posed as penalized maximum likelihood. For an atlas image I, a family of observed images J i we learn transformation parameters by solving the optimization problem:

The Reg terms provides necessary regularization for the infinite dimensional diffeomorphism group, and are parameterized by a spatial smoothness scale α, and a magnitude.

Regularization is defined through an inner product and thus metric on the tangent space of the diffeomorphism group. This inner product is large when velocity fields contain quickly varying (high spatial frequency) components, encouraging solutions to be smooth, and guaranteeing that the integral of Eq. (2) is a diffeomorphism. Any minimizer of (4) defines a shortest length geodesic curve. The square error term corresponds to a negative log likelihood of Gaussian noise with meanand variance σ 2.

The algorithm can be sensitive to the choice of the σ parameter. For RGB images between 0 and 1 we use σ = 0.1 as a default. For grayscale images, we first normalize them by the mean of their absolute value, then use 1.0 as a default. These options work reasonably well in most cases. The gaussian mixture modeling problem can be expanded to estimate these parameters as well, but in our experience this often leads to overfitting and poor performance. The algorithm is less sensitive to the σ R parameter. In practice we start with a very large value (little regularization), and reduce it incrementally if the deformations output do not appear realistic. We expect that users can start with the values of σ R used in the online examples, and modify them incrementally to increase or decrease regularization.

---

### Unsupervised approach to decomposing neural tuning variability [^a59bbee1]. Nature Communications (2023). High credibility.

Inference: a two-step estimation procedure

Assume that we have observations based on a set of m stimuli which are sampled from a particular stimulus space. The spike count of individual neuron elicited by individual stimulus s j, j = 1. m, is denoted as n t (s j) for the t -th moment (i.e. the t -th block of trials). We further denote the spike count vector for the t -th moment as.

Intuitively, if we could recover the unobservable mean μ t (s j) for t = 1. B, j = 1. m, fitting the model of stochastic curveusing functional PCA would be straight-forward. Denote. We could estimate the posterior of the hidden firing ratefrom the spike count data using an expectation-maximization (EM) algorithm. Following these ideas, we developed a two-step estimation procedure, as follows.

Step 1: recover the hidden

When the vectoris observable, the likelihood of Poisson model can be written as

Eqn. (5) implies that the logarithm of the firing rate for the sampled stimulus set { s 1, ⋯, s m } during the t -th momentcan be modeled as, where. We then obtain the log-likelihood

In reality the firing rates are not directly observed. However, by treatingas missing data, we could use an EM algorithm to iterate between an E-step and a M-step to optimize the functions. Specifically, the E-step calculates the conditional meanand conditional variance, given the current estimates of the parametersandobtained in M-step. Given the expectations obtained in the E-step, the M-step maximizes ℓ g in Eq. (7), which involves the following two quantities:

---

### The neural coding framework for learning generative models [^cc2d0058]. Nature Communications (2022). High credibility.

Baseline model descriptions

The baseline backprop-based models implemented for this article included a regularized auto-associative (autoencoding) network (RAE), a Gaussian variational autoencoder with fixed (spherical) variance (GVAE-CV), a Gaussian variational autoencoder (GVAE), and an adversarial autoencoder (GAN-AE). All models were constrained to have four layers like the NGC models. For all backprop-based models, the sizes of the layers in between the latent variable and the input layerwere chosen such that the total synaptic weight count of the model was (at most) approximately equal to the GNCNs (autoencoder layer size was tuned on validation data), the linear rectifier was used for the internal activation function, i.e. ϕ ℓ (v) = m a x (0, v), and weight values (for the encoder and decoder) were initialized from a centered Gaussian distribution with a standard deviation σ that was tuned on held-out validation data. To further improve generalization ability, the decoder weights of all autoencoders were regularized and some autoencoder models had specific meta-parameters that were tuned using validation data. Notably, the GAN-AE was the only model that required a specialized gradient descent rule, i.e. Adam, in order to obtain good log-likelihood and to stabilize training (stability is a known issue related to GANs). Finally, we implemented an optimized Gaussian mixture model (GMM) baseline model that was fit to the training data via expectation-maximization with the number of mixture components chosen based on preliminary experiments that yielded best performance. The only GMM implementation detail worthy of note was that we clipped the images sampled from the mixture model to lie in the range [0, 1] (which improved likelihood slightly).

Experimental setup and task design

---

### State estimation of a physical system with unknown governing equations [^bd52fbf3]. Nature (2023). Excellent credibility.

Stochastic variational inference

Here we tackle the problem of state estimation with unknown governing equations using the machinery of stochastic variational inference (SVI). SVI is a powerful approach for approximate Bayesian inference that converts the problem of generating samples from an unnormalized density function into an optimization problem involving maximization of a stochastic approximation of the evidence lower bound (ELBO). Variational inference has been applied to solve a wide variety of problems in machine learning –.

SVI typically requires four ingredients: (1) priors over variables of interest; (2) a parametrized approximate posterior; (3) a tractable expression for the ELBO that admits unbiased stochastic approximations; and (4) a method for approximating gradients of expectations in the ELBO.

In the context of the problem considered here, SVI cannot be applied in a straightforward manner because we seek to estimate a function, x t, given measurements of y t, rather than a finite-dimensional vector. Here we propose a reparametrization trick for approximating expectations under Markov Gaussian processes that enables us to tackle this challenge. A high-level summary of our approach is provided in Fig. 1 and we introduce the core ingredients of our methodology in the sections that follow.

Fig. 1
Main ideas.

a, Given noisy time-series data and an observation model, our goal is to estimate the underlying state and any missing terms in the forward model. b, We specify a hierarchical prior over the state and the governing equations and then infer an approximate posterior over the state and forward model using SVI. c, We arrive with a state estimate, a method for generating forecasts with uncertainty estimates and a Bayesian estimate for missing terms in the governing equations.

---

### Model-based prediction of spatial gene expression via generative linear mapping [^44dd84a6]. Nature Communications (2021). High credibility.

Derivation of the EM algorithm

The goal of the EM algorithm is to maximize the likelihood function p (X | θ) with respect to θ, where X = { x 1, x 2, …, x N } and θ = { π, A, b, Σ }. The generative model of scRNA-seq data point x with latent variables z is formulated, as follows. The probability distribution of z is:where z is a vector in a one-of- K representation that shows from which cells/regions in tissue a scRNA-seq sample originated; z k is the k th element of z; K is the number of the elements of the latent variables z equal to the number of cells in the tissue; and π k is probability that z k = 1. The probability distribution of x conditioned by z is:where N (x | μ, Σ) indicates a Gaussian distribution with mean μ and variance Σ; A k is the M × M diagonal matrix; b k indicates the M elements vector in Eq. (3); and r k indicates the M elements vector describing the metagene-expression level in cell k. The joint probability distribution of x and z is:

Note that the marginalized distribution of z becomes Eq. (8). The likelihood function for the complete dataset { X, Z } is given as:where Z = { z 1, z 2, …, z N }. Therefore, the expectation of its log likelihood function over the posterior distribution of P (Z | X, θ (old)) becomes:where γ nk is the expectation of z nk over P (Z | X, θ (old)) given as:

According to Bayes' theorem:where P (z nk = 1| x n) becomes Eq. (11).

In the E step, γ nk is calculated based on the current parameter values of θ (old). In the M step, we update the parameter values θ by maximizing the Q -function as:

The maximization of Q (θ, θ (old)) with respective to A, b, and Σ is achieved by ∂ Q /∂ A = 0, ∂ Q /∂ b = 0, and ∂ Q /∂ Σ = 0, leading to Eqs. (13–19). π is updated by introducing a Lagrange multiplier to enforce the constraint, leading to Eq. (12).

---

### A probabilistic model of biological ageing of the lungs for analysing the effects of smoking, asthma and COPD [^7baaa573]. Respiratory Research (2013). Low credibility.

Appendix

Below we describe how to estimate the model parameters b, and Σ l and the posterior distributions p (u j, v j |{ l n | c n = j }) and p (u j a + v j |{ l n | c n = j }). In order to avoid underflow/overflow problems, computations were performed in log-scale.

Parameter learning

As explained above, the parameter set b was estimated from the healthy group (Group H) only to make sure that it describes lung function decline in the absence of smoking, asthma and COPD. We learned the two subsets of b corresponding to FEV 1 and FVC separately using ordinary least squares. We then fixed b and estimated parametersand Σ l using an Expectation Maximization (EM) approach. More specifically, the EM approach consisted of iterating the following two steps until convergence:

- **E-Step**: Perform inference on p (ã 1, …, ã N, u 1…, u 8, v 1, …, v 8 | l 1, …, l N) to compute the quantities required to perform the M-Step.

- **M-Step**: Find the values ofand Σ l that maximize the expectation of the complete data log-likelihood

- where 〈·〉 p (·) denotes averaging with respect to p (·) and p (ã 1, …, ã N, u 1…, u 8, v 1, …, v 8 | l 1, …, l N) is computed using the values ofand Σ l estimated in the previous iteration.

The part of the expectation of the complete data log-likelihood that depends onand Σ l is given by

We excluded the parameter set b from the EM approach as we found that otherwise the nonlinearity in FEV 1 and FVC decline with age of healthy individuals would be transferred to the biological age (through high noise variance) so that b would not represent normal lung function decline.

M-Step: Updates for

Setting to zero the derivative of (4) with respect to

we obtain the optimal

where the required moments are estimated as explained below.

M-Step: Updates for Σ l

Setting to zero the derivative of (4) with respect to

where, we obtain the optimal Σ l

---

### An introduction to machine learning [^70a823d9]. Clinical Pharmacology and Therapeutics (2020). Medium credibility.

Different categories of loss functions

Different objective functions can be chosen to measure the distance between observed data and values predicted by the model. Some of the distance metrics used in practice can be associated to a likelihood. The likelihood indicates how probable it is to observe our data according to the selected model. The most common use of a likelihood is to find the parameters that make the model fit optimally to the data (i.e. the maximum likelihood parameter estimates). Usually, the negative logarithm of the likelihood is minimized and considered as objective function because it has favorable numerical properties. Similarly, in ML metrics, such as mean squared error, logistic objective, or cross‐entropy, are used to find optimal parameters or assess the fitness of the model.

In practice, analytical calculation of maximum likelihood or minimal loss may not be feasible, and it is often necessary to use a numerical optimization algorithm to solve for the best parameter values. Gradient descent is such an algorithm, where we first define an objective function for which we want to minimize and then iteratively update the values of the parameters in the direction with the steepest decrease (first‐order derivative) of the objective function until a convergence to a minimum distance is deemed reached. In the scenario of a nonconvex objective function, the success of finding a global minimum, as opposed to landing in some local minima, will depend on the choice of the initial set of parameter values, the learning rate (i.e. step size of each iteration) and the criterion for convergence. The reader can refer to ref. 35 for details on convex and nonconvex optimization processes. Stochastic gradient descent is an additional trick that can further speed up the optimization by randomly sampling a training dataset and summing the distances across this subset of training data points for approximating the objective function.

---

### Model-based clustering of meta-analytic functional imaging data [^8405680e]. Human Brain Mapping (2008). Low credibility.

We present a method for the analysis of meta-analytic functional imaging data. It is based on Activation Likelihood Estimation (ALE) and subsequent model-based clustering using Gaussian mixture models, expectation-maximization (EM) for model fitting, and the Bayesian Information Criterion (BIC) for model selection. Our method facilitates the clustering of activation maxima from previously performed imaging experiments in a hierarchical fashion. Regions with a high concentration of activation coordinates are first identified using ALE. Activation coordinates within these regions are then subjected to model-based clustering for a more detailed cluster analysis. We demonstrate the usefulness of the method in a meta-analysis of 26 fMRI studies investigating the well-known Stroop paradigm.

---

### Exploration-based learning of a stabilizing controller predicts locomotor adaptation [^c7225b9d]. Nature Communications (2024). High credibility.

Performing gradient descent using the matrix G as the gradient is equivalent to reducing the performance of a single stride J i, without considering the long-term implications. Minimizing just the single-stride performance J i may result in unrealistic optima for some performance objectives: turning off the actuators and falling may be optimal when only minimizing energy over one step. So, for non-transient tasks such as steady walking, we hypothesize that the human prioritizes the long-term or steady state performance. This asymptotic or long-horizon performance averages over the noise on any one stride.

To estimate the gradient with respect to long-term performance, the nervous system needs to be able to predict the future. Thus, to predict the long-term consequences of the parameters p i, we posit that the nervous system maintains an internal forward model of the dynamics, that is, how the initial state s i and the parameters p i for a stride affects the state at the end of the stride, equal to the initial state for the next stride s i +1. This internal model of the dynamics is also assumed to be linear for simplicity:Given such an internal model of the dynamics, the nervous system can estimate the future consequences of parameter changes to the steady state (by effectively simulating the internal model to steady state) and thus infer the relevant gradient of J ∞, given by:where the first term G gives the gradient of the short-term energy cost over one step, while the second term corrects for the fact that the steady state value of s will be different from the current initial state s i. This internal model framework also allows the nervous system to minimize performance over an intermediate time horizon by computing and using the gradient of the mean energy cost over the next few strides. We found that minimizing expected performance over just one or two strides into the future can result in unstable learning for energy optimization. In conventional reinforcement learning, a discount factor 0 < γ < 1 is used to modify the function minimized to, which prioritizes near-term performance and down-weights performance in the future. We did not use such a discount factor here, but using γ ≈ 1 is analogous to the asymptotic limit we have chosen, and γ much less than one will give results similar to optimizing over just the next few strides.

---

### Adjusting for genetic confounders in transcriptome-wide association studies improves discovery of risk genes of complex traits [^54846b6a]. Nature Genetics (2024). High credibility.

Inference of the individual-level model

The inference has two main steps. In the first step, we estimate the prior parametersfor the two groups, gene effects and variants effects. In the second step, we use the estimated θ, and compute the PIP of each variable, defined as the posterior probability of γ j = 1 given all the data and parameters.

The parameter estimation is done by maximum likelihood. Let y n ×1 be the data of the response variable, where n is the sample size. Let X n × p = [X 1 X 2… X p] be the data of all the p explanatory variables. The likelihood of our model is given bywhere Γ = [γ 1, γ 2, …, γ p] represents the 'configuration' of the causal (nonzero effect) status of all variables. We note that σ is the standard deviation of the phenotypic variance, and is assumed to be given (see below). To maximize the likelihood, we use the expectation-maximization (EM) algorithm. In the E-step, we obtain the expectation of log-likelihood over Γ, where θ (t) is the parameter value in the t- th iteration. In the M-step, we update θ (t) using the following rules to maximize the expectation from the E-step (Supplementary Note) :where ∣ M k ∣ is the number of variables in group k, is the PIP of variable j given data and current parameter values θ (t) andis the second moment of the posterior effect size of variable j, given that it is a causal variable. The updated rules have simple interpretations. The new parameteris simply the average PIP of all variables in the group k and the newis the weighted average of the second moment of the posterior effect sizes.

---

### CloneSig can jointly infer intra-tumor heterogeneity and mutational signature activity in bulk tumor sequencing data [^2fd7cbbb]. Nature Communications (2021). High credibility.

Given parameters ξ ∈ Σ J, and, we simply model U, S and T as categorical variables:Conditionally on C, we assume that the number of mutated chromosomes M is uniformly chosen between 1 and, i.e.whererepresents the vector of constant probability. Finally, to define the law of B, the number of mutated reads, we follow a standard approach in previous studies that represent ITH as a generative probabilistic model –, where the law of the mutated read counts for a given SNV must take into account the purity of the tumor, the proportion of cells in the tumor sample carrying that mutation (cancer cell fraction, CCF), as well as the various copy numbers of the normal and tumor cells. More precisely, as reviewed by, one can show that the expected fraction of mutated reads (variant allele frequency, VAF) satisfiesNote that this only holds under the classical simplifying assumption that all copy number events are clonal and affect all cells in the sample. If we now denote by ϕ ∈ [0, 1] J the vector of CCF for each clone, and introduce a further parameterto characterize the possible overdispersion of mutated read counts compared to their expected values, we finally model the number of mutated reads using a beta binomial distribution as follows:

Parameter estimation

Besides the tumor purity p, we assume that the matrix of mutational processesis known, as provided by databases like COSMIC and discussed below in Section 4.7. We note that we could consider μ unknown and use CloneSig to infer a new set mutational signatures from large cohorts of sequenced tumors, but prefer to build on existing work on mutational processes in order to be able to compare the results of CloneSig to the existing literature. Besides p and μ, the free parameters or CloneSig are J, the number of clones, and θ = (ξ, ϕ, π, ρ) which define the distributions of all random variables. On each tumor, we optimize θ separately for J = 1 to J m a x = 8 clones to maximize the likelihood of the observed SNV data in the tumor. The optimization is achieved approximately by an expectation-maximization (EM) algorithmdetailed in Supplementary Section 1.1. The number of clones J * ∈ [1, J m a x] is then estimated by maximizing an adaptive model selection criterion, detailed in Supplementary Section 1.2.

---

### RA3 is a reference-guided approach for epigenetic characterization of single cells [^fcac8559]. Nature Communications (2021). High credibility.

Model fitting and parameter estimation

Given the observed scCAS matrixafter TF-IDF transformation and the matrix of covariates, we treat the latent variable matrixas missing data, and use the expectation-maximization (EM) algorithm to estimate the model parameters Θ = (Γ, A, W *, β, σ), where,… For simplicity of the derivation that will follow, we also write, where, and, corresponding to the decomposition of variation introduced previously, and.

In the expectation step (E-step), given the parameters estimated in the previous iteration Θ t −1, the posterior distribution of h j iswhere, andBased on the posterior distribution, we compute the following expectations:

In the maximization step (M-step), we maximize the expected value of the complete log-likelihood with respect to Θ. The corresponding objective function iswhere, andis the k th diagonal element of matrix.

---

### Evaluations of maximization procedures for estimating linkage parameters under heterogeneity [^266b32c2]. Genetic Epidemiology (2004). Low credibility.

Locus heterogeneity is a major problem plaguing the mapping of disease genes responsible for complex genetic traits via linkage analysis. A common feature of several available methods to account for heterogeneity is that they involve maximizing a multidimensional likelihood to obtain maximum likelihood estimates. The high dimensionality of the likelihood surface may be due to multiple heterogeneity (mixing) parameters, linkage parameters, and/or regression coefficients corresponding to multiple covariates. Here, we focus on this nontrivial computational aspect of incorporating heterogeneity by considering several likelihood maximization procedures, including the expectation maximization (EM) algorithm and the stochastic expectation maximization (SEM) algorithm. The wide applicability of these procedures is demonstrated first through a general formulation of accounting for heterogeneity, and then by applying them to two specific formulations. Furthermore, our simulation studies as well as an application to the Genetic Analysis Workshop 12 asthma datasets show that, among other observations, SEM performs better than EM. As an aside, we illustrate a limitation of the popular admixture approach for incorporating heterogeneity, proved elsewhere. We also show how to obtain standard errors (SEs) for EM and SEM estimates, using methods available in the literature. These SEs can then be combined with the corresponding estimates to provide confidence intervals of the parameters.

---

### Complexity control by gradient descent in deep networks [^1d099fb8]. Nature Communications (2020). High credibility.

Overparametrized deep networks predict well, despite the lack of an explicit complexity control during training, such as an explicit regularization term. For exponential-type loss functions, we solve this puzzle by showing an effective regularization effect of gradient descent in terms of the normalized weights that are relevant for classification.

---

### Incorporating genotyping uncertainty in haplotype inference for single-nucleotide polymorphisms [^43b74365]. American Journal of Human Genetics (2004). Low credibility.

The accuracy of the vast amount of genotypic information generated by high-throughput genotyping technologies is crucial in haplotype analyses and linkage-disequilibrium mapping for complex diseases. To date, most automated programs lack quality measures for the allele calls; therefore, human interventions, which are both labor intensive and error prone, have to be performed. Here, we propose a novel genotype clustering algorithm, GeneScore, based on a bivariate t-mixture model, which assigns a set of probabilities for each data point belonging to the candidate genotype clusters. Furthermore, we describe an expectation-maximization (EM) algorithm for haplotype phasing, GenoSpectrum (GS)-EM, which can use probabilistic multilocus genotype matrices (called "GenoSpectrum") as inputs. Combining these two model-based algorithms, we can perform haplotype inference directly on raw readouts from a genotyping machine, such as the TaqMan assay. By using both simulated and real data sets, we demonstrate the advantages of our probabilistic approach over the current genotype scoring methods, in terms of both the accuracy of haplotype inference and the statistical power of haplotype-based association analyses.

---

### State estimation of a physical system with unknown governing equations [^794152e0]. Nature (2023). Excellent credibility.

State estimation is concerned with reconciling noisy observations of a physical system with the mathematical model believed to predict its behaviour for the purpose of inferring unmeasurable states and denoising measurable ones 1,2. Traditional state-estimation techniques rely on strong assumptions about the form of uncertainty in mathematical models, typically that it manifests as an additive stochastic perturbation or is parametric in nature 3. Here we present a reparametrization trick for stochastic variational inference with Markov Gaussian processes that enables an approximate Bayesian approach for state estimation in which the equations governing how the system evolves over time are partially or completely unknown. In contrast to classical state-estimation techniques, our method learns the missing terms in the mathematical model and a state estimate simultaneously from an approximate Bayesian perspective. This development enables the application of state-estimation methods to problems that have so far proved to be beyond reach. Finally, although we focus on state estimation, the advancements to stochastic variational inference made here are applicable to a broader class of problems in machine learning.

---

### Integrative differential expression and gene set enrichment analysis using summary statistics for scRNA-seq studies [^a6568413]. Nature Communications (2020). High credibility.

To achieve both goals, we develop an expectation maximization (EM)-Markov chain Monte Carlo (MCMC) algorithm for parameter estimation. Briefly, we treat the vector of both β and γ as missing data and develop an iterative EM optimization algorithm that alternates between an expectation step and a maximization step. In the expectation step, the expectation of the log likelihood effectively requires computing the posterior probability of each gene being a DE gene, P (γ j = 1|data), through MCMC. In the maximization step, we estimate the enrichment parameter through optimization, which is effectively equivalent to fitting a logistic regression model, where we treat the posterior probabilities for each gene being DE obtained from the expectation step as the outcome variable. While our EM-MCMC algorithm yields accurate parameter estimates, we found that the standard errors for the enrichment parameter obtained through the complete-data log-likelihood function is overly liberal and leads to p -value inflations (λ gc = 1.33; Supplementary Fig. 36A), a phenomenon that has been observed in many other settings. Therefore, we used the Louis methodto obtain the corrected information matrix and produce calibrated p -values (λ gc = 1.06; Supplementary Fig. 36B).

---

### Inferring multimodal latent topics from electronic health records [^91f021ef]. Nature Communications (2020). High credibility.

Stochastic joint collapsed variational Bayesian inference

To learn MixEHR from massive-scale EHR data, we propose a stochastic collapsed variational Bayesian (SCVB) algorithm. The main objective of our SCVB algorithm is to avoid keeping track of all of the latent topic assignmentsand λ l j k v 's while maintaining accurate model inference. Specifically, we first calculate the intermediate variational updates from randomly sampled mini-batches ofpatients:

We then update the latent disease topics using natural gradients, which are the intermediate updates weighted by a fixed learning rate:

Details are described in Supplementary Methods. We observed that the SJCVB0 algorithm achieves comparable performance as the full-batch learning with only a fraction of time and constant memory (Supplementary Fig. 2).

Predictive likelihood

To evaluate model learning and monitor empirical convergence, we performed fivefold cross-validation. For each patient in the validation fold, we randomly selected 50% of their EHR features to infer their disease mixtures and then used the other 50% of the features to evaluate the log predictive likelihood — a common metric to evaluate topic models, :where we inferred the variational expectations of the disease mixture and disease topics as:Sensible models should demonstrate improved predictive likelihood on held-out patients. We evaluated the predictive log likelihood of models with K ∈ {10, 25, 40, 50, 60, 75, 100, 125, 150} using the MIMIC-III data and ultimately set K = 75 as it gave the highest predictive likelihood (Supplementary Fig. 1). Nonetheless, models with K within the range between 50 and 100 have similar performance.

---

### Designing accurate emulators for scientific processes using calibration-driven deep models [^5e03881e]. Nature Communications (2020). High credibility.

Predictive models that accurately emulate complex scientific processes can achieve speed-ups over numerical simulators or experiments and at the same time provide surrogates for improving the subsequent analysis. Consequently, there is a recent surge in utilizing modern machine learning methods to build data-driven emulators. In this work, we study an often overlooked, yet important, problem of choosing loss functions while designing such emulators. Popular choices such as the mean squared error or the mean absolute error are based on a symmetric noise assumption and can be unsuitable for heterogeneous data or asymmetric noise distributions. We propose Learn-by-Calibrating, a novel deep learning approach based on interval calibration for designing emulators that can effectively recover the inherent noise structure without any explicit priors. Using a large suite of use-cases, we demonstrate the efficacy of our approach in providing high-quality emulators, when compared to widely-adopted loss function choices, even in small-data regimes.

---

### Fundamental limits to learning closed-form mathematical models from data [^be546343]. Nature Communications (2023). High credibility.

To address the model-learning question above, we formulate the problem of identifying the true generating model probabilistically, and show that probabilistic model selection is quasi-optimal at generalization, that is, at making predictions about unobserved data. This is in contrast to standard machine learning approaches, which, in this case, are suboptimal in the region of low observation noise. We then investigate the transition occurring between: (i) a learnable phase at low observation noise, in which the true model can in principle be learned from the data; and (ii) an unlearnable phase, in which the observation noise is too large for the true model to be learned from the data by any method. Finally, we provide an upper bound for the noise at which the learnability transition takes place, that is, the noise beyond which the true generating model cannot be learned by any method. This bound corresponds to the noise at which most plausible a priori models, which we call trivial, become more parsimonious descriptions of the data than the true generating model. Despite the simplicity of the approach, the bound provides a good approximation to the actual transition point.

---

### Songbirds work around computational complexity by learning song vocabulary independently of sequence [^c04a7221]. Nature Communications (2017). Medium credibility.

Engineers use efficient methods to solve linear assignment problems. However, it is not known how such problems are solved by biological systems, in which agent-task assignments must be estimated from noisy input, such as the variable song performances of juvenile songbirds and their tutors. Therefore, we find that zebra finches' explorative and greedy strategy is more closely paralleled by machine learning algorithms such as the expectation maximization (EM) algorithm, in which models are gradually matched to observed data in an iterative process. If we view each rendition of a target syllable as an observable data point and each of the birds' own syllables as a Gaussian model with unknown (hidden) parameters, then birds' greedy-competitive strategy of maximizing the overlap between model and data can be approximated by the EM algorithm for Gaussian mixture models (Supplementary Notes, Supplementary Fig. 5).

What sort of neural architecture could implement the greedy-competitive algorithm employed by zebra finches? Answering this question can help to restrict the space of possible neural models for sensorimotor vocal learning. In most models, the brain is assumed to compute a global error by aligning the motor output with the entire target sequence and performing temporal summation over the partial errors. Our findings require the modification of such models and more generally, of models assuming simple one-to-one links between neural representations of motor gestures and sensory targets. The "musical chairs" style competition we observed suggests a more complex dynamic network, where syllable-target associations are shaped according to linear assignment constraints, such as for example Hopfield and Tank's constraint-satisfying attractor network. Our results are consistent with models in which song features are reinforced when they are sufficiently close to any target syllable, and where spectral learning and sequence learning are carried out independently by two separate modules –.

---

### Homopolymer switches mediate adaptive mutability in mismatch repair-deficient colorectal cancer [^78384b21]. Nature Genetics (2024). High credibility.

ORION cell segmentation workflow

In this study, we developed ORION, a cell segmentation workflow for multispectral immunofluorescence imaging (see Extended Data Fig. 5 for an overview). ORION, an unsupervised method, uses an established ellipsoidal modelto identify individual cells and exclude noise and noncell objects. As the ellipsoidal models do not require labeled data and extensive training procedures, they provide promising results in unannotated multiplex IF datasets that include a high degree of cell shape and intensity variations. To this end, initially, the unmixed spectral signatures undergo a Gaussian filter with a 5 × 5 kernel to remove small artifacts. Subsequently, an adaptive thresholding method that performs well in images with foreground and background intensity heterogeneityis applied for the selection of the optimal threshold value for each pixel within its local neighborhood. This requires mean filtering and estimation of the local threshold based on the mean neighborhood pixel intensity. In the resulting M binary image, morphological operations consisting of erosion, dilation and removal of small elements are applied, to suppress small artifacts.

For the separation of touching cells, an improved ellipsoidal modeling approach is performed. Initially, we estimate the distance transformation of the binary image M of p pixels that represents the connected cells and we estimate the regional maxima of this. Given that the number and location of local maxima correspond to those of nuclei, we reject the touching maxima. The remaining maxima comprise the list of candidate seeds. Then, based on the hypothesis that cells can be spatially modeled as ellipsoids E C, the pixels of cells are then modeled using a Gaussian distribution. More specifically, a Gaussian mixture model is applied with the number of clusters C being equal to that of candidate seeds, and the mixture parameters, namely the mean and variance, are estimated using the expectation–maximization (EM) algorithm. For the initialization of the EM algorithm the k -nearest neighbor classification using Euclidean distance as the distance metric is used to estimate the initial parameters. The EM is an iterative method consisting of the following two steps: (1) expectation, which computes the likelihood with respect to the current estimates, and (2) maximization (equation (1)), which maximizes the expected log-likelihood (equation (2)) as follows:where Q is the expected values of the log-likelihood function θ, X is the pixel coordinates, Z is the latent variables andis the current parameters.

---

### Designing accurate emulators for scientific processes using calibration-driven deep models [^a9a8917a]. Nature Communications (2020). High credibility.

In this paper, we present Learn-by-Calibrating (LbC), a nonparametric approach based on interval calibration for building emulators in scientific applications that are effective even with heterogeneous data and are robust to outliers. The notion of interval calibration comes from the uncertainty quantification literature, and can be formally defined as follows: let us assume that the model f is designed to produce prediction intervals, in lieu of simple point estimates, for the response y, i.e. Suppose that the likelihood for the true response y to be contained in the prediction interval is, the intervals are considered to be well-calibrated if the likelihood matches the expected confidence level. For a confidence level α, we expect the interval to contain the true response for 100 × α % of realizations from p (x). Though calibration has been conventionally used for evaluating and correcting uncertainty estimators, this paper advocates for utilizing calibration as a training objective in regression models. More specifically, LbC uses two separate modules, implemented as neural networks, to produce point estimates and intervals, respectively, for the response variable, and poses a bilevel optimization problem to solve for the parameters of both the networks. This eliminates the need to construct priors on the expected residual structure and makes it applicable to both homogeneous and heterogeneous data. Furthermore, by effectively recovering the inherent noise structure, LbC leads to highly robust models.

---

### Analyzing atomic force microscopy images of virus-like particles by expectation-maximization [^0464558c]. NPJ Vaccines (2024). Medium credibility.

Thus, for an application with K intrinsic Gaussian distributions, 3 K -1 parameters must be fit to the data. In context of measuring VLP area (or diameter) by AFM, the observed data has only one variable, VLP dimension. However, there are a presumed greater number of latent variables that define the intrinsic data structure, the underlying K Gaussian distributions. A robust alternative to non-linear least squares methods for fitting a GMM is estimating the latent parameters through a ML approach. ML methods optimize the likelihood of a set of parameters being correct, given a collection of observed data. For a GMM, the optimization criterion is expressed asor alternatively as the log likelihoodwhere the N observations are being described by a K factor GMM. The Expectation-Maximization (EM) algorithm is a robust and efficient method of optimizing Eqs. (4a) and (4b). The EM algorithm introduces a latent variable Z which is the collection of the K intrinsic Gaussian distributions in the GMM. Thus, the complete likelihood of the model iswhere the likelihood of the ϴ parameters, given a set of observations (X) and a presumed latent model (Z) is equal to the joint probability of observing X and Z given the GMM parameters ϴ. The EM model progresses in two main iterative steps. After an initial guess of ϴ, the Expectation Step estimates γ, the distribution of each component in Z given X and ϴ, where the numerator is the probability of X i under component k and the denominator is the sum of probabilities across all components. The Maximization Step calculates new values of ϴ that maximizes the joint distribution of X and Z, where ϴ (t) is the previous estimate of ϴ. The algorithm iterates between the E-Step and M-Step until a convergence criterion, such as the log-likelihood (Eq. (4b) agreeing to 6 decimal places in subsequent iterations or a maximum number of allowed iterations is reached.

Expectation-maximization (EM) algorithm

All analyses were performed using Rin RStudio; the package 'mixtools'was employed to apply a GMM to the VLP areas or widths extracted from the AFM images. Convergence of the EM algorithm was capped at 1000 iterations. A function to construct multi-normal quantile-quantile plots was written in house. A regression function that relates the observed data and theoretical quantiles to statistically validate the GMM goodness of fit was written in house.

---

### Reinforcement learning establishes a minimal metacognitive process to monitor and control motor learning performance [^3f7fd251]. Nature Communications (2023). High credibility.

We next assume that the long-term goal for a learning agent with policyis to maximize expected rewards (and minimize expected punishment)caused by the memory update. Thus, the learner's objective function for motor learning between k and k + 1 is. We take a gradient of this objective function along the parameter θ to obtain the direction of its update, whereis approximated byin accordance with the policy gradient theorem.

Since the policy is Gaussian, the log of the policy function is given by:

Then, the partial derivative of this log policy function is

Rewriting Eq. (3) as, we obtain a gradient descent (or ascent) rule for the policy, where η is the learning rate of reinforcement learning, that is, the meta-learning rate. Thus, after the memory update is conducted at trial k in the lower motor learning layer, the parameter used for the memory update θ (k | k) is updated to θ (k+ 1| k) by the meta-learning rule (Eq. (8)) in the higher meta-learning layer. This updated meta-parameter θ (k+ 1| k) is used for motor learning at the next step k + 1 after shifting θ (k+ 1| k) to θ (k+ 1| k+ 1).

---

### A machine learning automated recommendation tool for synthetic biology [^89e7f76b]. Nature Communications (2020). High credibility.

Rather than providing a single point estimate of ensemble model parameters θ that best fit the training data, a Bayesian model provides a joint probability distribution, which quantifies the probability that a given set of parameters explains the training data. This Bayesian approach makes it possible to not only make predictions for new inputs but also examine the uncertainty in the model. Model parameters θ are characterized by full posterior distributionthat is inferred from level-1 data. Since this distribution is analytically intractable, we sample from it using the Markov Chain Monte Carlo (MCMC) technique, which samples the parameter space with a frequency proportional to the desired posterior(see the "Markov Chain Monte Carlo sampling" section in Supplementary Information).

The important point is that, as a result, instead of obtaining a single value as the prediction for the response variable, the ensemble model produces a full probabilistic distribution that takes into account the uncertainty in model parameters. More precisely, for a new input x * (not present in), the ensemble model F provides the probability that the response is y, when trained with data(i.e. the full predictive posterior distribution):where p (y ∣ x *, θ) is the predictive distribution of y given input x * and model parameters θ (Eq. (2)), is the posterior distribution of model parameters given data, and f ≡ f (x *) for the sake of clarity.

ART is very different from, and produces more accurate predictions than, Gaussian processes, a commonly used machine learning approach for outcome prediction and new input recommendations. ART and Gaussian process regression(GPR) share their probabilistic nature, i.e. the predictions for both methods are probabilistic. However, for GPR the prediction distribution is always assumed to be a Gaussian, whereas ART does not assume any particular form of the distribution and provides the full probability distribution (more details can be found in the "Expected value and variance for ensemble model" section of Supplementary Information). Moreover, ART is an ensemble model that includes GPR as one of the base learners for the ensemble. Hence, ART will, by construction, be at least as accurate as GPR (ART typically outperforms all its base learners). As a downside, ART requires more computations than GPR, but this is not a problem with the small data sets typically encountered in synthetic biology.

---

### Classification of spatiotemporal hemodynamics from brain perfusion MR images using expectation-maximization estimation with finite mixture of multivariate gaussian distributions [^52f46ace]. Magnetic Resonance in Medicine (2007). Low credibility.

The ability to cluster different perfusion compartments in the brain is critical for analyzing brain perfusion. This study presents a method based on a mixture of multivariate Gaussians (MoMG) and the expectation-maximization (EM) algorithm to dissect various perfusion compartments from dynamic susceptibility contrast (DSC) MR images so that each compartment comprises pixels of similar signal-time curves. This EM-based method provides an objective way to 1) delineate an area to serve as the in-plane arterial input function (AIF) of the feeding artery for adjacent tissues to better quantify the relative cerebral blood volume (rCBV), relative cerebral blood flow (rCBF), and mean transit time (MTT); 2) demarcate regions with abnormal perfusion derangement to facilitate diagnosis; and 3) obtain parametric maps with supplementary information, such as temporal scenarios and recirculation of contrast agent. Results from normal subjects show that perfusion cascade manifests (in order of appearance) the arteries, gray matter (GM), white matter (WM), veins and sinuses, and choroid plexus mixed with cerebrospinal fluid (CSF). The averaged rCBV, rCBF, and MTT ratios between GM and WM are in good agreement with those in the literature. Results from a patient with cerebral arteriovenous malformation (CAVM) showed distinct spatiotemporal characteristics between perfusion patterns, which allowed differentiation between pathological and nonpathological areas.

---

### Expectation management [^41e6a304]. The New England Journal of Medicine (2014). Excellent credibility.

As physicians transform observed frequencies from studies into predicted probabilities for a given patient, we generally fail to consider that the predictions we utter about a given therapeutic intervention are themselves part of the intervention.

---

### Bayesian EM algorithm for scoring polymorphic deletions from SNP data and application to a common CNV on 8q24 [^beacd8f3]. Genetic Epidemiology (2009). Low credibility.

Copy number variations (CNVs) in the human genome provide exciting candidates for functional polymorphisms. Hence, we now assess association between CNV carrier status and diseases status by evaluating the signal intensity of SNP genotyping assays. Here, we present a novel statistical method designed to perform such inference and apply this method to a known CNV in a bipolar disorder linkage region. Using Bayesian computations we calculate the posterior probability for carrier status of a CNV in each individual of a sample by jointly analyzing genotype information and hybridization intensity. We model the signal intensity as a mixture of normal distributions, allowing for locus-specific and allele-specific distributions. Using an expectation maximization algorithm we estimate the parameters of these distributions and use these estimates for inferring carrier status of each individual and for the boundaries of the CNV. We applied the method to a sample of 3,512 individuals to a previously described common deletion on 8q24, a region consistently showing linkage to bipolar disorder, and unambiguously inferred 172 heterozygous and 1 homozygous deletion carrier. We observed no significant association between bipolar disorder and carrier status. We carefully assessed the validity of the inferred carrier status and observed no indication of errors. Furthermore, the algorithm precisely identifies the boundaries of the CNV. Finally, we assessed the power of this algorithm to detect shorter CNVs by sub-sampling from the SNPs covered by this deletion, demonstrating that our EM algorithm produces precise estimates of carrier status.

---

### Designing accurate emulators for scientific processes using calibration-driven deep models [^7fb42dc9]. Nature Communications (2020). High credibility.

Since LbC relies entirely on calibration, there is no need for explicit discrepancy metrics like ℓ 2 or Huber for updating the model f. Instead, we employ a hinge-loss objective that attempts to adjust the estimatesuch that the observed likelihood of the true response to be contained in the interval increases:Here, is obtained using the recent state of the parameter ϕ and the randomly chosen α in the current iteration, γ is a predefined threshold (set to 0.05), and the weightspenalize samples with larger intervals. When compared to a competitive optimization algorithm, e.g. adversarial learning, in LbC, both models are working toward the common objective of improving interval calibration. In general, one can improve the calibration of predictions by adjusting the mean estimate to move closer to the true target, or by suitably widening the interval to include the true target even when the mean estimate is bad. Consequently, when the predictor model improves the mean estimates (for a fixed-interval estimator), the current interval estimates become overly optimistic, i.e. even at lower confidence levels, it will produce higher empirical confidence. Hence, in the subsequent iteration, the interval estimator will sharpen the intervals to make the estimates more underconfident, i.e. at higher confidence levels (say 0.9 or 0.99), it might provide lower empirical confidence. Consequently, LbC alternatively adjusts the predictor and interval estimator models to produce predictive models that are both accurate (good-quality mean estimates) and well-calibrated (at all confidence levels). This synergistic optimization process thus leads to superior quality predictions, which we find to be effective, regardless of the inherent residual structure. Figure 5 illustrates the proposed approach and the convergence curves for the two models f and g obtained for the synthetic example in Fig. 1.

Fig. 5
Learn-by-Calibrating (LbC) architecture overview.

LbC uses are two separate networks to obtain point estimates and the intervals, respectively. As shown by the convergence plots during training, the two models synergistically optimize for the overall objective of improving the interval calibration.

---

### Synergizing habits and goals with variational bayes [^e48b98b3]. Nature Communications (2024). High credibility.

Results

Sensorimotor learning and behaving scheme

We consider an agent that dynamically interacts with an environment (see Fig. 1 a as an overview). The agent receives sensory observations and reward signals from the environment, which are used to train its internal world model (for sensory prediction) and action model (or policy in RL terminology). To behave, the agent arbitrates between goal-directed and habitual intentions, and then it executes actions based on the intention to interact with the environment (Fig. 1 c). The agent's experiences of interacting with the world are memorized for learning (Fig. 1 b), a process also known as experience replay. We consider the case in which an agent performs sensorimotor learning for navigation tasks tabula rasa, i.e. through self-exploration.

Fig. 1
The Bayesian behavior framework.

a Overview of the behavioral scheme considered in this work. The agent learns to select the optimal action through reinforcement learning using reward signals and performs predictive coding using sensory observations (learning the world models). b Diagram of our framework during training. The entire model is optimized to minimize RL loss and free energy (Eq. (8)). The model's synaptic weights and biases are updated end-to-end through gradient descent, utilizing memorized experiences. Note that the reward signals affect all networks in learning through backpropagation. c Diagram of our framework during behaving. The goal-directed intentionis obtained by active inference with respect to the goal (sampled from the agent's memorized observations at the positions with maximal reward). The action of the agent is determined by the synergized intention, calculated as the inverse variance weighted average of the two intentions. d Iterative process to minimize the corresponding free energy by optimizing the probability density function (PDF) of the posterior intention (i.e. active inference). The active inference process may stop early if the mean variance ofis small enough to save computation cost. For clarity, a one-dimensional intention is visualized here.

---

### Improved machine learning algorithm for predicting ground state properties [^0c6418ab]. Nature Communications (2024). High credibility.

Theorem 1

(Sample and computational complexity). Givenand a training data setof sizewhere x ℓ is sampled from an unknown distributionandfor any observable O with eigenvalues between −1 and 1 that can be written as a sum of geometrically local observables. With a proper choice of the efficiently computable hyperparameters δ 1, δ 2, and B, the learned function h * (x) = w * ⋅ ϕ (x) satisfieswith probability at least 1 − δ. The training and prediction time of the classical ML model are bounded by.

The output y ℓ in the training data can be obtained by measuringfor the same observable O multiple times and averaging the outcomes. Alternatively, we can use the classical shadow formalism –, that performs randomized Pauli measurements on ρ (x ℓ) to predictfor a wide range of observables O. We can also combine Theorem 1 and the classical shadow formalism to use our ML algorithm to predict ground state representations, as seen in the following corollary. This allows one to predict ground state propertiesfor a large number of observables O rather than just a single one. We present the proof of Corollary 1 in Supplementary Section 3B.

Corollary 1

Givenand a training data setof sizewhere x ℓ is sampled from an unknown distributionand σ T (ρ (x ℓ)) is the classical shadow representation of the ground state ρ (x ℓ) using T randomized Pauli measurements. For, then the proposed ML algorithm can learn a ground state representationthat achievesfor any observable O with eigenvalues between −1 and 1 that can be written as a sum of geometrically local observables with probability at least 1 − δ.

We can also show that the problem of estimating ground state properties for the class of parameterized Hamiltoniansconsidered in this work is hard for non-ML algorithms that cannot learn from data, assuming the widely believed conjecture that NP-complete problems cannot be solved in randomized polynomial time. This is a manifestation of the computational power of data studied in. The proof of Proposition 1 inconstructs a parameterized Hamiltonian H (x) that belongs to the family of parameterized Hamiltonians considered in this work and hence establishes the following.

---

### A finite mixture model for X-chromosome association with an emphasis on microbiome data analysis [^86f81fd7]. Genetic Epidemiology (2019). Medium credibility.

Analysis of the X chromosome has been largely neglected in genetic studies mainly because of complex underlying biological mechanisms. On the other hand, the study of human microbiome data (typically over-dispersed counts with an excess of zeros) has generated great interest recently because of advancements in next-generation sequencing technologies. We propose a novel approach to infer the association between host genetic variants in the X-chromosome and microbiome data. The method accounts for random X-chromosome inactivation (XCI), skewed (or nonrandom) XCI (XCI-S), and escape of XCI (XCI-E). The inference is performed through a finite mixture model (FMM), in which an indicator variable denoting the "true" biological mechanism is treated as missing data. An expectation-maximization algorithm on zero-inflated and two-part models is implemented to estimate genetic effects. We investigate the performance of the FMM along with strategies that assume XCI and XCI-E mechanisms for all subjects compared with alternative approaches. Briefly, an XCI mechanism codes males' genotypes as homozygous females, whereas under XCI-E, males are treated as heterozygous females. By comprehensive simulations, we evaluate tests of the hypothesis under a computationally efficient score statistic. In summary, the FMM renders reduced bias and commensurate power compared to XCI, XCI-E, and alternative strategies while maintaining adequate Type 1 error control. The proposed method has far-reaching applications. In particular, we illustrate its usage on a large-scale human microbiome study, the Genetic, Environmental and Microbial (GEM) project, to test for the genetic association on the X chromosome.

---

### Predictive modeling of heterogeneous treatment effects in RCTs: a scoping review [^3c7cd702]. JAMA Network Open (2025). High credibility.

The PATH Statement recommended risk modeling whenever an RCT demonstrates an overall treatment effect. Risk of study outcomes varies substantially in clinical practice and in most RCT populations, and the absolute benefit from an effective treatment is expected to increase as baseline risk increases, a mathematical relationship that has been called risk magnification. This relationship is often referenced in the evidence-based medicine literature, and is implicit in clinical practice and in guidelines that reserve costly or riskier treatments for patients with higher baseline risk. The PATH Statement recommended use of validated external models for predicting risk if available, but cited evidence that in their absence, valid models can be developed within the RCT population, using baseline covariates and observed study outcomes from both study arms. The PATH Statement pointed out that risk modeling would be unlikely to identify HTE when overall RCT results are null, where both a clinical rationale and the mathematical expectation of risk magnification are lacking. More generally, the PATH Statement urged caution in conducting any HTE analyses when overall results are null.

Effect modeling permits a more robust examination of possible HTE. The PATH Statement recommended its use when there are previously established or strongly suspected effect modifiers. However, it emphasized the vulnerability of effect modeling approaches to over-fittingand false-positive findings when multiple potential interactions are considered and recommended use of statistical methods that aim to reduce over-fitting of data. It additionally suggested validation of effect model findings in external datasets when possible.

We conducted a scoping reviewto describe the frequency with which predictive modeling analyses of RCT data have appeared and cited the PATH Statement since its publication and the consistency of analyses with PATH Statement consensus criteria (eTable 1 in Supplement 1). We also assessed the frequency of claimed HTE and the credibility and clinical importance of these claims. We used criteria adapted from the Instrument to Assess Credibility of Effect Modification Analyses (ICEMAN), and when HTE was found to be credible, we applied the PATH Statement definition of clinical importance: "variation in the risk difference across patient subgroups potentially sufficient to span clinically-defined decision thresholds", ie, supporting differing treatment recommendations for patient subgroups.

---

### A latent outcome variable approach for mendelian randomization using the stochastic expectation maximization algorithm [^b33cda3e]. Human Genetics (2025). Medium credibility.

Latent outcome variable approach (LOVA) for MR using the stochastic expectation maximization (EM) steps

We employ a stochastic or simplified EM algorithm (McLachlan and Krishnan; Michael and Tipping; Ng et al.) where instead of marginalizing the likelihood with respect to the latent variable, we iteratively impute the latent variables to facilitate the estimation of model parameters.

Suppose we know the truewithout error. According to Eq. (1), a new set of phenotypes can be generated as follows:

The new phenotypes, denoted by υ, exclude the vertical pleiotropy effects, allowing only for horizontal pleiotropy effects with residual effects. By using υ, the GWAS summary statistics will distinctly reveal which variants are not associated with the outcome, satisfying the exclusion restriction assumption, a key aspect in MR.

However, given the trueis unknown, we propose employing an iterative EM algorithm:
Initialization: Start with initial estimates forwhereis the iteration number (= 0, 1, 2,…)
Expectation (E) Step: Calculate the expected values of the latent variablegiven the observed data and the current parameter estimates. This step involves imputing the latent values based on the observed data and the current estimates of τ. The calculation ofshould incorporate both the observed data y and the linear predictor, representing the expected value ofgiven the current estimate of τ.
Maximization (M) Step: Update the parameter estimates, τ, to maximize the expected log-likelihood computed in the E-step, i.e. log-L(| y, I) where I is the inclusion indicator for vertical pleiotropy (0 or 1). This step first involves fitting a regression model to estimate the parameter, using the imputed υ values. Then, can be estimated using an IVW model, represented aswhere Ij is the inclusion indicator with I j = 1 if the significance criteria are met; otherwise I j = 0 (see below).
Iteration: Repeat steps 2 and 3 until convergence, where the parameter estimates no longer change significantly between iterations or a convergence criterion is met.

In the EM steps, two key parameters need to be specified by the users: the initial value for τ and the significance level used to select vertical pleiotropy variants (referred to as the inclusion indicator) in each iteration.

---

### Triazolam [^59f91090]. FDA (2025). Medium credibility.

WARNING: RISKS FROM CONCOMITANT USE WITH OPIOIDS; ABUSE, MISUSE, AND ADDICTION; and DEPENDENCE AND WITHDRAWAL REACTIONS

Concomitant use of benzodiazepines and opioids may result in profound sedation, respiratory depression, coma, and death. Reserve concomitant prescribing of these drugs in patients for whom alternative treatment options are inadequate. Limit dosages and durations to the minimum required. Follow patients for signs and symptoms of respiratory depression and sedation [see Warnings and Precautions (5.1), Drug Interactions (7.1)].
The use of benzodiazepines, including triazolam, exposes users to risks of abuse, misuse, and addiction, which can lead to overdose or death. Abuse and misuse of benzodiazepines commonly involve concomitant use of other medications, alcohol, and/or illicit substances, which is associated with an increased frequency of serious adverse outcomes. Before prescribing triazolam and throughout treatment, assess each patient's risk for abuse, misuse, and addiction [see Warnings and Precautions (5.2)].
The continued use of benzodiazepines, including triazolam, may lead to clinically significant physical dependence. The risks of dependence and withdrawal increase with longer treatment duration and higher daily dose. Abrupt discontinuation or rapid dosage reduction of triazolam after continued use may precipitate acute withdrawal reactions, which can be life-threatening. To reduce the risk of withdrawal reactions, use a gradual taper to discontinue triazolam or reduce the dosage [see Dosage and Administration (2.3), Warnings and Precautions (5.3)].

WARNING: RISKS FROM CONCOMITANT USE WITH OPIOIDS; ABUSE, MISUSE, AND ADDICTION; and DEPENDENCE AND WITHDRAWAL REACTIONS

See full prescribing information for complete boxed warning.

Concomitant use of benzodiazepines and opioids may result in profound sedation, respiratory depression, coma, and death. Reserve concomitant prescribing of these drugs in patients for whom alternative treatment options are inadequate. Limit dosages and durations to the minimum required. Follow patients for signs and symptoms of respiratory depression and sedation (5.1, 7.1).
The use of benzodiazepines, including triazolam, exposes users to risks of abuse, misuse, and addiction, which can lead to overdose or death. Before prescribing triazolam and throughout treatment, assess each patient's risk for abuse, misuse, and addiction (5.2).
Abrupt discontinuation or rapid dosage reduction of triazolam after continued use may precipitate acute withdrawal reactions, which can be life-threatening. To reduce the risk of withdrawal reactions, use a gradual taper to discontinue triazolam or reduce the dosage (2.3, 5.3).

---

### Second opinion needed: communicating uncertainty in medical machine learning [^69c8ce7a]. NPJ Digital Medicine (2021). Medium credibility.

For neural networks and deep learning methods, some simple methods to calculate conditional uncertainty estimates include Monte Carlo (MC) Dropoutand ensembling –. MC Dropout consists of randomly removing hidden unit outputs at train and/or test time in a neural network. Outputs in the neural network architecture are set to 0 with probability p according to a Bernoulli distribution. A prediction is made by randomly sampling different configurations and then averaging across these different dropout realizations. MC Dropout was initially introduced as an ad hoc modification to neural networks, but since then have been shown to be an approximation of Bayesian variational inference under a certain set of assumptions. Ensembling is a flexible method that can be applied to a variety of machine learning models. For neural networks, ensemble methods require training multiple networks on the same data then combining predictions from these networks, resembling bootstrap procedures from the statistical literature. In ensembles of M deep neural networks, predictions from the different models are averaged. Predictive uncertainty from both MC Dropout and ensembling can be summarized by calculating the standard deviation (or similar metric of dispersion) from the collection of predictions provided by each approach. Both methods are easy to add to existing neural network models and provide good uncertainty estimates on out of distribution data.

Bayesian methods to calculate predictive uncertainty estimates generally rely on the posterior predictive distribution:where y is the outcome of interest (i.e. heart disease status), X is the data for a specific sample (i.e. a patient's clinical markers), D is the training data of the model, and W are the parameters of the ML model. Once the posterior predictive distribution has been estimated, predictive uncertainty is straight-forward to obtain since one has access to the entire distribution of interest. For neural networks and many machine learning models however, calculating the posterior predictive distribution exactly is analytically intractable in general and requires computational approximations. For instance, the integral over the model weights can be replaced by an average over many samples of model weights obtained from a Markov-Chain Monte Carlo simulation.

---

### An introduction to machine learning [^12dd1b2c]. Clinical Pharmacology and Therapeutics (2020). Medium credibility.

Indicators of model complexity vs. goodness of fit

In pharmacometrics, model selection is usually based on quantitative measures that summarize how well the model fits the data, often with penalties for overfitting. The most commonly used are the Akaike information criterion and Bayesian information criterion. They penalize the number of model parameters and reward goodness of fit, measured through likelihood. The Akaike information criterion is formalized as:with the number of parameters M and the maximum likelihood.

In contrast, the Bayesian information criterion:takes into account the number of data points n.

These model selection approaches are rarely used in ML, partly due to the complexity of datasets and the associated violation of distributional assumptions. Instead, approaches like cross‐validation are more commonly used (Clustering).

---

### PGlycoQuant with a deep residual network for quantitative glycoproteomics at intact glycopeptide level [^c73e4429]. Nature Communications (2022). High credibility.

Fig. 1
The development of pGlycoQuant.

a Current glycopeptide quantitation software tools suffer from suboptimal reproducibility for lower-abundance signals, resulting in a high missing value rate. b pGlycoQuant supports both primary and tandem mass spectrometry quantitation for multiple quantitative strategies. c An embedded deep learning model of ResNet in pGlycoQuant for glycopeptide evidence matching and MBR analysis. d The FQR is estimated by fitting Gaussian mixture distribution with EM algorithm. e The MIR algorithm is proposed as an optional function in pGlycoQuant for increasing quantitative coverage of sialic acid-related glycopeptides.

In the deep learning-based evidence matching model (Supplementary Fig. 2), a glycopeptide evidence is first mapped to a tensor of 512 × 1, and the best signal patterns are retained by the ResNet18 model. Then, a fully connected network that comprehensively utilizes multiple characteristics, including the similarity of isotopic peaks and distance of retention time between glycopeptides, is trained to measure the similarity of two glycopeptide evidences in the same run for metabolic-labeling data or between different runs for label-free data. Finally, the model provides the matching score for each quantitation result as a softmax loss function is used in the network (detailed in the "Methods" section). In the MBR analysis, the matching scores were further used to calculate the FQR by a target-decoy Gaussian mixture model approach fitted with Expectation-Maximization (EM) algorithm for quantitative result control with 1% FQR (Fig. 1c and Supplementary Fig. 3). Details of FQR estimation approach are described in the "Methods" section.

---

### Replicate sequencing libraries are important for quantification of allelic imbalance [^4c9dfc58]. Nature Communications (2021). High credibility.

Fitting AI distribution as beta-binomial mixture

To fit the parameters of a mixture of two proportional beta-binomial distributions, representing observed AI from the pooled replicate in each coverage bin B i :we use Expectation-maximization (EM) algorithm (see Fig. 3 d). Our fitting procedure is similar to the procedure used in the classical Gaussian mixture model.

For fitting procedure, we used a threshold on the total allelic gene coverage (50 for mice and 30 for human). All bins that didn't meet the requirement of minimal 40 observations (genes) were also excluded from all stages of QCC-fitting process.

Starting from initials, and vector of converted allelic imbalance observations, where N i is number of genes in bin B i :we performed iterative EM steps until the difference between parameters of the sequential steps converged (Supplementary Fig. S 13).

E-step:for k ∈ {1, 2}, n ∈ {1, …, N i } and t is number of EM step.

M-step: Since we expectand beta-binomial distributions being symmetric:

Simulation of a pair of replicates

Using fitted triplet of parameters { ω 1 i, α 1 i, α 2 i }, in each bin B i we generated the weighted mixture of two Beta distributions probabilities, for 5000 "genes":Next, for each "gene" a pair of beta-binomial distributed AIs is generated, forming two replicates. The expected AI distribution then can be obtained via subtraction: ξ 1 θ i − ξ 2 θ i.

Quantile analysis and QCC value

To quantify the overdispersion, we performed quantile analysis between observed ΔAI distribution (Fig. 3 c) and expected ΔAI distribution (Fig. 3 e), within the coverage bins. It is a reasonable measure because differences between AI values among replicates generally tend to be symmetric on autosomes in experiments.

For each coverage bin i and a set of quantiles q ∈ {0.2, 0.35, 0.5, 0.65, 0.8, 0.9, 0.95}, the ratios of quantiles of observed ΔAI to quantiles of expected ΔAI were calculated:

Then the obtained ratios were linearly fitted with a constant which we call Quality Correction Constant (QCC), since it reflects the difference between observation and the binomial sampling assumption in the model (see Fig. 3 f).

---

### A variational expectation-maximization framework for balanced multi-scale learning of protein and drug interactions [^89448300]. Nature Communications (2024). High credibility.

Results

MUSE: A balanced expectation-maximization learning framework to learn protein and drug multi-scale information

As shown in Fig. 1, MUSE is a multi-scale learning method that integrates both molecular structure modeling and interaction network learning of protein and drug through a variational expectation-maximization (EM) framework. The EM framework optimizes two modules, the expectation step (E-step) and the maximization step (M-step), in an alternating procedure over multiple iterations. During the E-step, MUSE utilizes the structural information of each biomolecule to learn an effective structural representation for training with known interactions and augmented samples in the M-step. It takes the pair of protein and drug with their atom-level structural information as input and augments with the predicted interactions from the M-step. The M-step takes the molecule-level interaction network, the structural embeddings, and predicted interactions from the E-step as the input, and also outputs the predicted interactions. This iterative optimization between the E-step and the M-step ensures the capturing of both the molecular structures and network information interactively, with different learning rates at two scales. The mutual supervision ensures that each scale model learns in an appropriate manner, enabling the utilization of effective information at different scales. This framework will be demonstrated on several multi-scale tasks for interactions between proteins and drugs. We additionally analyzed that MUSE mitigates the imbalanced characteristics in multi-scale learning and effectively integrates the hierarchical and complementary information from different scales.

Fig. 1
Illustration of the MUSE framework and its applications.

a The vanilla approach for learning multi-scale representations is to combine the two single-scale models and optimize them jointly. b The EM framework (MUSE) for Multi-scale Learning. The Expectation step trains a model with structural information of protein and drug as input to fit the known and pseudo interactions (the predicted interactions from the M-step except the first iteration). The updated interactions and structural embeddings were input to the M-step, where the molecular network was constructed to maximize the prediction of the known interactions and the predicted interactions from the E-step. The updated interactions were sent to E-step for new iterations. c MUSE is generalizable and applicable to multiple prediction tasks: predicting different types of molecular interactions and molecular binding interfaces.

---

### Artificial intelligence and machine learning for cardiovascular computed tomography (CCT): a white paper of the Society of Cardiovascular Computed Tomography (SCCT) [^de0b17dc]. Journal of Cardiovascular Computed Tomography (2024). High credibility.

Reporting and review — AI/ML papers should prioritize transparent reporting, alignment with appropriate guidelines, and study design–specific metrics. Adequate reporting of AI/ML papers is essential to maximize transparency and reproducibility, and a wide range of reporting guidelines exists covering potential study designs and stages of AI/ML model development and testing. Many represent an evolution of existing reporting guidelines updated and refined for AI/ML applications, with overlap that may provide complementary guidance. When an AI/ML specific guideline does not exist for a particular study design, the appropriate EQUATOR network guideline can be used along with MI-CLAIM, MINIMAR, CLAIM or RQS as appropriate. The reporting of how well an AI/ML model performs will require performance metrics tailored to the study design.

---

### Hydrocodone bitartrate (hysingla ER) [^0c03a0bc]. FDA (2023). Medium credibility.

Warnings and precautions regarding the use of hydrocodone bitartrate ER PO (also known as Hysingla ER):
- **Adrenal insufficiency**: use caution in patients taking the drug for a prolonged period (> 1 month).
- **Central sleep apnea**: use caution in patients taking higher doses.
- **Decreased serum hydrocodone level**: use caution in patients taking CYP3A4 inducers (such as rifampin, carbamazepine, or phenytoin) or discontinuing CYP3A4 inhibitors (such as macrolide antibiotics, azole antifungals, or protease inhibitors).
- **Erectile dysfunction, infertility**: use caution in patients taking the drug for a prolonged period.
- **Exacerbation of increased ICP**: use caution in patients with increased ICP, brain tumor, or head injury.
- **Growth suppression**: use caution in patients with chronic corticosteroid therapy. Monitor growth regularly in pediatric patients receiving chronic corticosteroid therapy. Reassess the need for hydrocodone regularly and adjust the corticosteroid therapy as appropriate.
- **Hypotension, syncope**: use caution in patients with reduced blood volume or taking other CNS depressants. Monitor BP after initiation and dose titration. Avoid use in patients with circulatory shock.
- **Mask symptoms of head injury**: use caution in patients with head injury. Avoid use in patients with impaired consciousness or coma.
- **Opioid overdose**: use caution in patients taking CNS depressants or with a history of opioid use disorder or prior opioid overdose. Consider prescribing naloxone based on the patient's risk factors for overdose.
- **Opioid withdrawal syndrome**: do not discontinue abruptly in patients physically dependent on opioids.
- **Opioid withdrawal syndrome**: use caution in patients taking CYP3A4 inducers (such as rifampin, carbamazepine, or phenytoin) or discontinuing CYP3A4 inhibitors (such as macrolide antibiotics, azole antifungals, or protease inhibitors).
- **Opioid withdrawal syndrome**: use extreme caution in patients taking mixed agonist/antagonist analgesics (such as pentazocine, nalbuphine, or butorphanol) or partial agonist analgesics (such as buprenorphine).
- **Prolonged QT interval**: use caution in patients with congestive HF, bradyarrhythmia, electrolyte abnormalities, or taking drugs prolonging QT interval. Avoid use in patients with congenital long QT syndrome. Do not exceed 90 mg BID in patients developing QT prolongation.
- **Seizure**: use caution in patients with seizure disorder.
- **Serotonin syndrome**: use caution in patients taking serotonergic drugs.
- **Serotonin syndrome**: use extreme caution in patients taking MAOIs or within 14 days of stopping treatment.
- **Somnolence**: use extreme caution in patients performing activities requiring mental alertness, such as driving or operating machinery.
- **Sphincter of Oddi dysfunction**: use caution in patients with biliary tract disease and acute pancreatitis.

---

### Algorithms for inferring haplotypes [^7276d8f4]. Genetic Epidemiology (2004). Low credibility.

Haplotype phase information in diploid organisms provides valuable information on human evolutionary history and may lead to the development of more efficient strategies to identify genetic variants that increase susceptibility to human diseases. Molecular haplotyping methods are labor-intensive, low-throughput, and very costly. Therefore, algorithms based on formal statistical theories were shown to be very effective and cost-efficient for haplotype reconstruction. This review covers 1) population-based haplotype inference methods: Clark's algorithm, expectation-maximization (EM) algorithm, coalescence-based algorithms (pseudo-Gibbs sampler and perfect/imperfect phylogeny), and partition-ligation algorithm implemented by a fully Bayesian model (Haplotyper) or by EM (PLEM); 2) family-based haplotype inference methods; 3) the handling of genotype scoring uncertainties (i.e., genotyping errors and raw two-dimensional genotype scatterplots) in inferring haplotypes; and 4) haplotype inference methods for pooled DNA samples. The advantages and limitations of each algorithm are discussed. By using simulations based on empirical data on the G6PD gene and TNFRSF5 gene, I demonstrate that different algorithms have different degrees of sensitivity to various extents of population diversities and genotyping error rates. Future development of statistical algorithms for addressing haplotype reconstruction will resort more and more to ideas based on combinatorial mathematics, graphical models, and machine learning, and they will have profound impacts on population genetics and genetic epidemiology with the advent of the human HapMap.

---

### Scalable spatiotemporal prediction with Bayesian neural fields [^1744654c]. Nature Communications (2024). High credibility.

Baselines

The prediction accuracy on the benchmark datasets in Table 1 using BNF is compared to several state-of-the-art baselines. This evaluation focuses specifically on baseline methods that (i) have high-quality and widely used open-source implementations; (ii) can generate both point and interval predictions; and (iii) are directly applicable to new spatiotemporal datasets (e.g. those in Table 1) without the need to redevelop substantial parts of the model. The methods are:
SSVGP: Spatiotemporal Sparse Variational Gaussian Process. This method handles large datasets (i.e. linear time scaling in the number of time points) by leveraging a state-space representation based on stochastic partial differential equations and Bayesian parallel filtering and smoothing on GPUs. Parameter estimation is performed using natural gradient variational inference.
SGB: Spatiotemporal Gradient Boosting Trees. Prediction intervals are estimated by minimizing the quantile loss using an ensemble of 1000 tree estimators. As this baseline is not a typical time series model, the same covariates [x 1 (s, t), …, x m (s, t)] (5)–(9) provided to BNF are also provides as regression inputs.
SGLMM: Spatiotemporal Generalized Linear Mixed Effects Models. These methods handle large datasets by integrating latent Gaussian-Markov random fields with stochastic partial differential equations. Parameter estimation is performed using maximum marginal likelihood inference. Three observation noise processes are considered: IID: Independent and identically distributed Gaussian errors. AR1: Order 1 auto-regressive Gaussian errors. RW: Gaussian random walk errors.
NBEATS: Neural Basis Expansion Analysis. This baseline employs a "window-based" deep learning auto-regressive model where future data is predicted over a fixed-size horizon conditioned on a window of previous observations and exogenous features. The model is configured with indicators for all applicable seasonal components — e.g. hour of day, day of week, day of month, week of year, month — as well as trend and seasonal Fourier features. The method contains a large number of numeric hyperparameters which are automatically tuned using the NeuralForecastpackage. Prediction intervals are estimated by minimizing quantile loss.
TSR: Trend Surface Regression with Ordinary Least Squares (OLS) (Section 3.2 of ref.). The observation noise model is Gaussian with maximum likelihood estimation of the variance. As with SGB, the regression covariates are identical to those provided to BNF.
BNF: Bayesian Neural Field; using variational and maximum a-posteriori inference.

---

### The adaptive designs CONSORT extension (ACE) statement: a checklist with explanation and elaboration guideline for reporting randomised trials that use an adaptive design [^d765c1d8]. BMJ (2020). Excellent credibility.

In addition to statistical methods for comparing outcomes between groups (item 12a), we specifically encourage authors to clearly describe statistical methods used to estimate measures of treatment effects with associated uncertainty (for example, confidence or credible intervals) and p-value (when appropriate); referencing relevant literature is sufficient. When conventional or naïve estimators derived from fixed design methods are used, it should be clearly stated. In situations where statistical simulations were used to either explore the extent of bias in estimation of the treatment effects (such as) or operating characteristics, it is good practice to mention this and provide supporting evidence (item 24c).

ADs tend to increase the risk of making misleading or unjustified claims of treatments effects if traditional methods that ignore trial adaptations are used. In general, this arises when selecting one or more hypothesis test results from a possible list in order to claim evidence of the desired conclusion. For instance, the risks may increase by testing the same hypothesis several times (for example, at interim and final analyses), hypothesis testing of multiple treatment comparisons, selecting an appropriate population from multiple target populations, adapting key outcomes, or a combination of these. A variety of adaptive statistical methods exist for controlling specific operating characteristics of the design (for example, type I error rate, power) depending on the nature of the repeated testing of hypotheses.

Authors should therefore state operating characteristics of the design that have been controlled and details of statistical methods used. The need for controlling a specific type of operating characteristic (for example, pairwise or familywise type I error rate) is context dependent (for example, based on regulatory considerations, objectives and setting) so clarification is encouraged to help interpretation. How evidence of benefit and/or risk is claimed (part of item 7a) and hypotheses being tested (item 2b) should be clear. In situations where statistical simulations were used, we encourage authors to provide a report, where possible (item 24b).

When data or statistical tests across independent stages are combined to make statistical inference, authors should clearly describe the combination test method (for example, Fisher's combination method, inverse normal method or conditional error function)and weights used for each stage (when not obvious). This information is important because different methods and weights may produce results that lead to different conclusions. Bauer and Einfaltfound low reporting quality of these methods.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^2f0eef03]. Nature Communications (2025). High credibility.

Introduction

Much of our understanding of the natural world is built upon mathematical models. But how we build those models evolves as new techniques and techonologies are developed. With the arrival of machine learning methods, it has become even more feasible to solve inverse problems and learn complex descriptions by applying data-driven methods to scientifically-motivated models –. However, even moderately complex models are generally non-identifiable: many different parameter sets may yield very similar outputs –. With imperfect real-world data, it is often unclear which — if any — of these models constitutes a trustable solution to the inverse problem.

This presents us with a model selection problem which is somewhat at odds with the usual statistical theory. First, we require a criterion which can distinguish between structurally identical models differing only in their specific parameters. Moreover, even with the most tightly controlled experiments, we expect all candidate models to be misspecified due to experimental variations. Those variations also change when the experiment is repeated in the same lab or another lab; in other words, the replication process is non-stationary, even when the data-generation process is stationary within individual trials. While there has been some work to develop model selection criteria which are robust to misspecification –, and some consideration of locally stationary processes, the question of non-stationary replications has received little attention. Criteria which assume the data generating process to be stationary therefore underestimate epistemic uncertainty, which has important repercussions when they are used for scientific induction, where the goal is to learn a model which generalises. In recent years, a few authors have advanced similar arguments in the context of machine learning models.

---

### Fundamental limits to learning closed-form mathematical models from data [^5954e9c9]. Nature Communications (2023). High credibility.

Fig. 1
Probabilistic model selection makes quasi-optimal predictions about unobserved data.

We select two models m *, whose expressions are shown at the top of each column. a, b From each model, we generate synthetic datasets D with N points (shown, N = 100) and different levels of noise s ϵ (shown, s ϵ = 1). Here and throughout the article, the values of the independent variables x 1 and x 2 are generated uniformly at random in [− 2, 2]. Vertical lines show the observation error ϵ i for each point in D. For a model not drawn from the prior and data generated differently, see Supplementary Fig. S1. c, d For each dataset D (with dataset sizes N ∈ {25, 50, 100, 200, 400}), we sample models from p (m ∣ D) using the Bayesian machine scientist, select the MDL model (maximum p (m ∣ D)) among those sampled, and use this model to make predictions on a test dataset, generated exactly as D. We show the prediction root mean squared error (RMSE) of the MDL model onas a function of N and s ϵ. For comparison, we also show the predictions from an artificial neural network (ANN, dotted lines; Methods). Since s ϵ is the irreducible error, predictions on the diagonal RMSE = s ϵ are optimal. e, f We plot the prediction RMSE scaled by the irreducible error s ϵ; optimal predictions satisfy RMSE/ s ϵ = 1 (dashed line).

---

### Evolution after whole-genome duplication: a network perspective [^d3cb51f0]. G3 (2013). Low credibility.

Methods

An EM algorithm for determining the fate of WGD gene pairs

From a network perspective, the fate of a WGD gene pair can be inferred from the shared neighborhoods of the pair. To achieve this task, we developed an expectation-maximization (EM) that is inspired by the work of. The original method ofcharacterizes function by tissue-specific gene expression level, whereas we characterize function by normalized neighborhood sizes. The approach ofdoes not work here because they use sequence similarity of paralogs to construct a phylogenetic tree whose branch lengths serve as a surrogate of time since duplication. We, instead, target WGD pairs in which all the genes were duplicated at the same time.

Our EM algorithm works as follows. Let the neighborhood of paralog genes g 1 and g 2 be both N 0 right after duplication, and be N (g 1) and N (g 2) be the neighborhoods at present. Let the size for normalization be ttl = | N (g 1) ∪ N (g 2)| and defineUnder pure CF, we expect a = b = x = sh = 1; under pure SF, we expect a + b = x = 1, sh = 0; and, under pure NF, we expect a = x (or b = x) and a + b = 1 > x, sh = 0. We further normalize the three values by their maximum value as follows:The probabilistic model for classification can be captured as: (1) SF: a + b = 1; (2) CF: a + 1 = 2 x; and, (3) NF: x = a, x ≤ 0.5. For any given x, y, and z values, it is classified by a plane for the feature points.

---

### Cell type and gene expression deconvolution with bayesPrism enables Bayesian integrative analysis across bulk and single-cell RNA sequencing in oncology [^ce1a0dbb]. Nature Cancer (2022). High credibility.

Choice of hyperparameters and retrieval of output from BayesPrism

We used the default hyperparameters of BayesPrism to perform deconvolution: σ = 2, α = 10–8. We used the default setting for Gibbs sampling as follows: chain.length = 1,000, burn.in = 500 and thinning = 2 (that is, we ran a Markov chain Monte Carlo of 1,000 samples, discarded the first 500 and used every other sample to estimate parameters of interest). The maximum number of iterations of the conjugate gradient method was set to 10 5. All cell type fractions used were the initial cell type fraction estimate (θ 0). The updated cell type fraction estimates, θ f, highly correlates with θ 0 (R > 0.98), and results from all downstream analyses were consistent whether using θ 0 or θ f.

Details of benchmarks are listed in Supplementary Note 3.

Embedding learning analysis

To initialize the malignant gene programs (bases), we used the NMF R packageto learn a linear combination that best approximates the normalized expression of malignant cells inferred by the deconvolution module of BayesPrism (res$first.gibbs.res$Zkg.tum.norm). We optimized the number of malignant bases from two to 12, and chose the number of gene programs (K) that yielded the best cophenetic score before a significant drop began. This strategy selected K = 5 for GBM, K = 4 for HNSCC and K = 7 for SKCM. We then fixed K, randomly initialized NMF 200 times and chose bases that yielded minimal residuals. The optimal bases were then used as the prior for the embedding learning module of BayesPrism, thereby incorporating information from the scRNA-seq reference into the embeddings learned by BayesPrism. Although BayesPrism does not necessarily require the use of input bases learned using external algorithms such as NMF, and can initialize bases using clustering methods when no user-defined input is used, we found that initialization of bases using NMF significantly speeded up the convergence of EM and also facilitated the selection of K when no prior information was provided.

---

### Single photon emission computed tomography (SPECT) myocardial perfusion imaging guidelines: instrumentation, acquisition, processing, and interpretation [^2a3ccaf7]. Journal of Nuclear Cardiology (2018). Medium credibility.

SPECT myocardial perfusion imaging — iterative reconstruction using ordered subsets expectation maximization (OSEM) is almost universally used and accelerates processing by using data subsets (e.g., four iterations with eight subsets roughly eight times faster than non-subsetted EM), and iterative reconstruction is currently preferred to filtered back projection (FBP) for gated and ungated images; newer techniques incorporate adjacent count profiles and 2D slices to model 3D tracer distribution, and mild 3D post-filtering is typically applied.

---

### Recommendations for the appropriate structure, communication, and investigation of tobacco harm reduction claims. An official American Thoracic Society policy statement [^168f29f9]. American Journal of Respiratory and Critical Care Medicine (2018). Medium credibility.

Claims of harm reduction — individual-level impact — Claims of harm reduction must be accompanied by explicit estimates of the anticipated impact on individuals, and inference based on emerging or incomplete evidence is an acceptable technique for estimating the anticipated impact for harm reduction strategies at individuals, particularly in instances where the impact on human illness is expected to be delayed, with the requirement that inferential methods should be explicitly described when making harm reduction claims; inferential claims relying on surrogate markers of disease, or on markers of exposure to risk-associated constituents, should explicitly state the nature of, and degree of certainty in, the relationship between markers and the specific harms reduced; when using number of cigarettes consumed as a surrogate measure of exposure, explicit care should be taken to account for anticipated compensatory adjustments in smoking topography (i.e., physical characteristics of smoking behavior, such as puff count, puff volume, average flow, puff duration, and interpuff interval); when based on animal or cell culture models, comprehensive narrative statements of relevance of the chosen models to actual human experience should accompany the claim; and given our understanding of the protean biopsychosocial consequences of nicotine dependence, the potential for causing or sustaining addiction should be accurately represented in risk trade-off calculations.

---

### Fundamental limits to learning closed-form mathematical models from data [^a345300f]. Nature Communications (2023). High credibility.

Given a finite and noisy dataset generated with a closed-form mathematical model, when is it possible to learn the true generating model from the data alone? This is the question we investigate here. We show that this model-learning problem displays a transition from a low-noise phase in which the true model can be learned, to a phase in which the observation noise is too high for the true model to be learned by any method. Both in the low-noise phase and in the high-noise phase, probabilistic model selection leads to optimal generalization to unseen data. This is in contrast to standard machine learning approaches, including artificial neural networks, which in this particular problem are limited, in the low-noise phase, by their ability to interpolate. In the transition region between the learnable and unlearnable phases, generalization is hard for all approaches including probabilistic model selection.

---

### Multimodal prediction of conversion to Alzheimer's disease based on incomplete biomarkers [^75469425]. Alzheimer's & Dementia (2015). Low credibility.

2.2 Missing data handling

We compared three ways of missing data replacement: mean imputation, imputation by the EM algorithm, and a combined imputation.

In mean imputation, the mean value over all nonmissing data is calculated for each of the features individually. This value is then substituted for all missing values of the respective feature. To account for categorical and discrete variables we first determined the scale for each of the features and then substituted with the mode, median, or mean value accordingly. The main advantage of mean imputation is its low computational cost. However, the estimates are often biased.

A less biased treatment of missing data can be achieved by using the Expectation-Maximization (EM) algorithm. This is an iterative procedure which switches between an expectation (E) and a maximization (M) step until the most likely values for the missing data are found. We used here a regularized version of the EM algorithm described in(The code can be found at).

Because a considerable fluctuation of EM estimates has been found for categorical features, a third imputation technique was implemented which is a combination of mean and EM imputation: for all categorical features mean imputation was performed and the EM imputation was then only carried out on the remaining noncategorical features.

The performance of these three imputation techniques was evaluated on a complete version of the given data set, where only features without missing values (58 instead of 288 features) were contained. Subsequently, we randomly deleted 0% to 90% of the data and calculated the mean squared error between the imputed and true values. Because the combined method produced lower errors for almost all cases than mean imputation and EM algorithm alone (see Fig. 1 B), we decided to use the combined method for all further analyses. Importantly, imputation values were calculated based only on training data and were then used to replace missing values in both training and test data.

Additionally, we investigated which classification algorithm comes closest to recovering the prediction accuracy of the complete data (as above 58 out of 288 features). We therefore performed the different classification analyses (see section 2.4) for the complete data set and a series of incomplete data sets, in which we randomly deleted 0% to 90% of the data (see Fig. 1 C). Missing data were replaced via combined imputation.

---

### Three learning stages and accuracy-efficiency tradeoff of restricted boltzmann machines [^efe3f1d0]. Nature Communications (2022). High credibility.

Restricted Boltzmann Machines (RBMs) offer a versatile architecture for unsupervised machine learning that can in principle approximate any target probability distribution with arbitrary accuracy. However, the RBM model is usually not directly accessible due to its computational complexity, and Markov-chain sampling is invoked to analyze the learned probability distribution. For training and eventual applications, it is thus desirable to have a sampler that is both accurate and efficient. We highlight that these two goals generally compete with each other and cannot be achieved simultaneously. More specifically, we identify and quantitatively characterize three regimes of RBM learning: independent learning, where the accuracy improves without losing efficiency; correlation learning, where higher accuracy entails lower efficiency; and degradation, where both accuracy and efficiency no longer improve or even deteriorate. These findings are based on numerical experiments and heuristic arguments.

---

### Recommendations for the appropriate structure, communication, and investigation of tobacco harm reduction claims. An official American Thoracic Society policy statement [^46405d15]. American Journal of Respiratory and Critical Care Medicine (2018). Medium credibility.

Communication requirements for inferential and implementation claims — comprehensive narrative statements regarding the relevance of laboratory models of harm to actual human experience should accompany harm reduction claims when based on animal or cell culture models; qualitative descriptions of linkage strength and the relevance of exposure conditions should be explicitly stated; when making a harm reduction claim, the range of possible outcomes expected from implementation should be explicitly stated; and a narrative describing mediators, moderators, population distribution, temporality, and limits of certainty should accompany explicit claims.

---

### Fundamental limits to learning closed-form mathematical models from data [^bfc95d4c]. Nature Communications (2023). High credibility.

Sampling models with the Bayesian machine scientist

For a given dataset D, each model m has a description lengthgiven by Eq. (4). The Bayesian machine scientistgenerates a Markov chain of models { m 0, m 1, …, m T } using the Metropolis algorithm as described next.

Each model is represented as an expression tree, and each new model m t +1 in the Markov chain is proposed from the previous one m t by changing the corresponding expression tree: changing an operation or a variable in the expression tree (for example, proposing a new model m t +1 = θ 0 + x 1 from m t = θ 0 * x 1); adding a new term to the tree (for example, m t +1 = θ 0 * x 1 + x 2 from m t = θ 0 * x 1); or replacing one block of the tree (for example, from m t = θ 0 * x 1) (see ref.for details). Once a new model m t +1 is proposed from m t, the new model is accepted using the Metropolis rule.

Note that the only input to the Bayesian machine is the observed data D. In particular, the observational noise s ϵ is unknown and must be estimated, via maximum likelihood, to calculate the Bayesian information criterion B (m) of each model m.

Artificial neural network benchmarks

For the analysis of predictions on unobserved data, we use as benchmarks the following artificial neural network architectures and training procedures. The networks consist of: an input layer with two inputs corresponding to x 1 and x 2; (ii) four hidden fully connected feed-forward layers, with 10 units each and ReLU activation functions; and (iii) a linear output layer with a single output y.

Each network was trained with a dataset D containing N points, just as in the probabilistic model selection experiments. Training errors and validation errors (computed on an independent set) were calculated, and the training process stopped when the validation error increased, on average, for 100 epochs; this typically entailed training for 1000–2000 epochs. This procedure was repeated three times, and the model with the overall lowest validation error was kept for making predictions on a final test set. The predictions onare those reported in Fig. 1.

---

### Elacestrant (Orserdu) [^25e5bc7d]. FDA (2024). Medium credibility.

The dosage of elacestrant PO for treatment of breast cancer in postmenopausal female adults with ESR1 mutation (advanced or metastatic, ER-positive, HER2-negative, progression after ≥ 1 line of endocrine therapy) is 345 mg PO daily until disease progression or unacceptable toxicity

---

### Learning non-stationary langevin dynamics from stochastic observations of latent trajectories [^8ffd6558]. Nature Communications (2021). High credibility.

Contributions of non-stationary components to the accurate inference

We found that accurate inference of non-stationary Langevin dynamics requires incorporating all three non-stationary components: the initial distribution p 0 (x), the boundary conditions, and the absorption operator. The necessity of all components for accurate inference is not obvious. Since spike trains generated from stationary versus non-stationary dynamics appear similar (Fig. 2), one could assume that omitting non-stationary components may affect the inference only insignificantly. To demonstrate how each component contributes to the accurate inference, we focus here on inferring the potential Φ(x) from synthetic data with known ground truth, assuming p 0 (x) and D are provided (we consider simultaneous inference of Φ(x), p 0 (x), D below). We use 200 trials of spike data generated from the model with a linear ground-truth potential and a narrow initial state distribution (full list of parameters in Supplementary Table 1). We simulated a reaction-time task, so that each trial terminates when the latent trajectory reaches one of the decision boundaries producing a non-stationary distribution of latent trajectories (Fig. 2 b).

The inference accurately recovers the Langevin dynamics from these non-stationary spike data when all non-stationary components are taken into account (Fig. 3 a). The GD algorithm iteratively increases the model likelihood (decreases the negative log-likelihood). Starting from an unspecific initial guess, the potential shape changes gradually over the GD iterations. After some iterations, the fitted potential closely matches the ground-truth shape while the log-likelihood of the fitted model approaches the log-likelihood of the ground-truth model. The concurrent agreement of the inferred potential and its likelihood with the ground truth indicates the accurate recovery of the Langevin dynamics. At later iterations, the potential shape can deteriorate due to overfitting, and model selection is required for identifying the model that accurately approximates dynamics in the data when the ground truth is not known(we consider model selection and uncertainty quantification below).

---

### Laboratory perspectives in the development of polygenic risk scores for disease: a points to consider statement of the American College of Medical Genetics and genomics (ACMG) [^50389303]. Genetics in Medicine (2023). High credibility.

Bioinformatic and statistical algorithms for PRS — early models "focused on sites whose disease association passed a threshold for genome-wide significance, often limiting the PRS inputs to few sites, which may be more easily and directly genotyped", whereas more recent models "incorporate information from hundreds of thousands to millions of sites across the genome", and "require either genome sequence or imputation to determine variant status".

---

### A machine learning automated recommendation tool for synthetic biology [^5b926c5b]. Nature Communications (2020). High credibility.

Requirements (i) and (ii) are both addressed by borrowing an idea from Bayesian optimization: optimization of a parametrized surrogate function which accounts for both exploitation and exploration. Namely, our objective function G (x) takes the form of the upper confidence boundgiven in terms of a weighted sum of the expected value and the variance of the response (parametrized by α, Eq. (5)). This scheme accounts for both exploitation and exploration: for the maximization case, for example, for α = 1, we get G (x) = Var(y) 1/2, so the algorithm suggests next steps that maximize the response variance, thus exploring parts of the phase space where our model shows high predictive uncertainty. For α = 0, we get G (x) = E (y), and the algorithm suggests next steps that maximize the expected response, thus exploiting our model to obtain the best response. Intermediate values of α produce a mix of both behaviors. In our example below using ART on simulated data we use a tempering strategy for α (see the "Using simulated data to test ART" section), whereas in the three experimental cases we set α = 0. In general, we recommend setting α to values slightly smaller than one for early-stage DBTL cycles, thus allowing for more systematic exploration of the space so as to build a more accurate predictive model in the subsequent DBTL cycles. If the objective is purely to optimize the response, we recommend setting α = 0.

---

### A benchmark, expand, and calibration (benchExCal) trial emulation approach for using real-world evidence to support indication expansions: design and process for a planned empirical evaluation [^e8044db3]. Clinical Pharmacology and Therapeutics (2025). Medium credibility.

Using a range of estimates for, including the estimate informed by the stage 1 analysis, we can identify a "tipping point"at which the regulatory conclusion would be changed and consider the plausibility of the priors around the tipping point from both clinical and methodologic perspectives. This range of priors could include the 5th, 25th, 50th, 75th, 95th levels of the confidence interval around the estimated, to show the potential range in estimates after accounting for divergences observed between a similar prior RCT and database study. Also of interest may be the tipping point value ofthat would produce a result that would lead the stakeholders to make a different decision. If such a tipping point value is far away from the priors forinformed by the stage 1 study, this may boost confidence in claims of effectiveness for the secondary indication even in the absence of a second‐stage trial. Although P ‐value thresholds and other such cutoffs can be somewhat arbitrary, decision‐makers are often faced with the need to make a binary decision. Therefore, decision‐changing thresholds must be defined in conjunction with the relevant decision‐making organization and aligned with the specific context being considered.

---

### A benchmark, expand, and calibration (benchExCal) trial emulation approach for using real-world evidence to support indication expansions: design and process for a planned empirical evaluation [^58398f38]. Clinical Pharmacology and Therapeutics (2025). Medium credibility.

Real-world evidence involving healthcare database studies is well established for making causal inferences in post-market drug safety studies and methods, data, and research infrastructure for evaluating effectiveness have advanced in recent years. The rapidly expanding field of etiologic research using insurance claims and electronic health records databases is being evaluated for supporting effectiveness claims. One such use case to support regulatory decision-making on effectiveness is for expanding indications beyond existing effectiveness claims. Confidence in the validity of findings from cohort studies conducted using databases (hereafter "database study") to support indication expansions could be increased through a structured benchmarking process of an initial database study against RCT evidence followed by calibration of a subsequent database study based on differences in results observed in the initial RCT-database pair. This paper proposes a benchmark, expand, and calibration (BenchExCal) approach to trial emulation and describes the design and process for evaluating the performance of the approach through both simulation studies; five planned empirical examples are also described. The project will provide insights regarding how a first-stage benchmarking emulation of a completed trial for an existing indication can be used to calibrate, increase confidence, and improve interpretation of the results for a second-stage emulation of a hypothetical trial that could potentially provide evidence for an expanded indication. Although the examples have been selected to provide a variety of learnings, five use cases do not address all clinical and data scenarios that may be encountered when seeking a supplemental indication for a marketed drug.

---

### Statistical analysis and optimality of neural systems [^bf829ef6]. Neuron (2021). Medium credibility.

Normative theories and statistical inference provide complementary approaches for the study of biological systems. A normative theory postulates that organisms have adapted to efficiently solve essential tasks and proceeds to mathematically work out testable consequences of such optimality; parameters that maximize the hypothesized organismal function can be derived ab initio, without reference to experimental data. In contrast, statistical inference focuses on the efficient utilization of data to learn model parameters, without reference to any a priori notion of biological function. Traditionally, these two approaches were developed independently and applied separately. Here, we unify them in a coherent Bayesian framework that embeds a normative theory into a family of maximum-entropy "optimization priors". This family defines a smooth interpolation between a data-rich inference regime and a data-limited prediction regime. Using three neuroscience datasets, we demonstrate that our framework allows one to address fundamental challenges relating to inference in high-dimensional, biological problems.

---

### How to read articles that use machine learning: users' guides to the medical literature [^f5d20a58]. JAMA (2019). Excellent credibility.

In recent years, many new clinical diagnostic tools have been developed using complicated machine learning methods. Irrespective of how a diagnostic tool is derived, it must be evaluated using a 3-step process of deriving, validating, and establishing the clinical effectiveness of the tool. Machine learning-based tools should also be assessed for the type of machine learning model used and its appropriateness for the input data type and data set size. Machine learning models also generally have additional prespecified settings called hyperparameters, which must be tuned on a data set independent of the validation set. On the validation set, the outcome against which the model is evaluated is termed the reference standard. The rigor of the reference standard must be assessed, such as against a universally accepted gold standard or expert grading.

---

### A benchmark, expand, and calibration (benchExCal) trial emulation approach for using real-world evidence to support indication expansions: design and process for a planned empirical evaluation [^1c5770dc]. Clinical Pharmacology and Therapeutics (2025). Medium credibility.

That said, to our knowledge, this project is novel in terms of the plan to systematically demonstrate and evaluate the BenchExCal approach to trial emulation through both simulation and empirical examples. The project will provide insights regarding how a first‐stage emulation of a completed trial for an initial indication can be used to increase confidence and inform the interpretation of a second‐stage emulation of a hypothetical trial designed to provide evidence for a supplemental indication. Although examples have been selected to provide a variety of learnings, five use cases will not be able to address all clinical and data scenarios that may be encountered when seeking a supplemental indication for a marketed drug. Similar follow‐on work will add to this library of learnings.

---

### Model-free estimation of completeness, uncertainties, and outliers in atomistic machine learning using information theory [^c534080d]. Nature Communications (2025). High credibility.

Model-free uncertainty quantification for machine learning potentials

When information theory is used to analyze a single dataset, as in the previous section, environments are compared against other environments in the same dataset. However, it is convenient to test the case when reference datasets { X } do not contain a tested sample Y, often leading to. In this scenario, we propose that differential entropies can be used as a model-free uncertainty estimator for a given dataset. Whereas uncertainty quantification (UQ) methods for MLIPs usually rely on models, — i.e. prediction uncertainties are associated to variances in model predictions — we propose instead that an estimate for UQ in MLIPs can be obtained from the data alone. This approach is similar to Gaussian process regression methods, which compute an uncertainty by inverting a covariance matrix computed for training points, or parametric models on a latent space. Differently from other approaches, however, our method performs a fast non-parametric estimate directly on the atomistic data space. While this approach can be expensive for large datasets, it is easily parallelizable and provides a conservative and deterministic measure that correlates with uncertainties, which are often challenging or expensive to capture within model-based UQ. Our method also makes no assumptions on the generalization power of the model that was trained on these data points, and can be seen as a lower bound of performance (i.e. when generalization is not expected), similarly to what is already performed with Gaussian process regression methods. Detaching the model uncertainty from the data uncertainty also allows our framework to be incorporated into a variety of workflows without modifying architectures, loss functions, or increasing the associated cost of training and evaluating the models like NN ensembles.

---

### The clinical application of polygenic risk scores: a points to consider statement of the American College of Medical Genetics and genomics (ACMG) [^d2d0f0e0]. Genetics in Medicine (2023). High credibility.

Key learning points — at this time, the ACMG advocates against clinical implementation of PRS testing unless the provider and patient have a clear understanding of the limitations of the testing and applicability to the specific patient, including how the results will be used to guide evidence-based clinical care.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^aeeaae45]. Nature Communications (2025). High credibility.

Fitting models to data is an important part of the practice of science. Advances in machine learning have made it possible to fit more-and more complex-models, but have also exacerbated a problem: when multiple models fit the data equally well, which one(s) should we pick? The answer depends entirely on the modelling goal. In the scientific context, the essential goal is replicability: if a model works well to describe one experiment, it should continue to do so when that experiment is replicated tomorrow, or in another laboratory. The selection criterion must therefore be robust to the variations inherent to the replication process. In this work we develop a nonparametric method for estimating uncertainty on a model's empirical risk when replications are non-stationary, thus ensuring that a model is only rejected when another is reproducibly better. We illustrate the method with two examples: one a more classical setting, where the models are structurally distinct, and a machine learning-inspired setting, where they differ only in the value of their parameters. We show how, in this context of replicability or "epistemic uncertainty", it compares favourably to existing model selection criteria, and has more satisfactory behaviour with large experimental datasets.

---

### Colorectal cancer screening and prevention [^390b568a]. American Family Physician (2025). High credibility.

Regarding screening and diagnosis for colon cancer, more specifically with respect to indications for screening, high-risk individuals, family history, AAFP 2025 guidelines recommend to obtain CRC screening in patients with ≥ 1 first-degree relatives with CRC or adenomatous polyps, starting at 40 years of age or 10 years before the age of the youngest relative at the time of their diagnosis.

---

### Laboratory perspectives in the development of polygenic risk scores for disease: a points to consider statement of the American College of Medical Genetics and genomics (ACMG) [^c1008465]. Genetics in Medicine (2023). High credibility.

ACMG statement — measurement of a polygenic risk score (PRS) notes that PRSs generate likelihood ratios, and translation to absolute risk requires an accurate understanding of disease incidence in an individual's population.

---

### Cognitive functioning in untreated glioma patients: the limited predictive value of clinical variables [^44b3a2b6]. Neuro-Oncology (2024). Medium credibility.

Model Training and Validation. — Machine learning models were trained for 4 different objectives.

Predicting if the patient is impaired on at least 1 cognitive test (1 model with a dichotomous outcome).
Predicting the number of tests on which a patient is impaired (1 model with a continuous outcome).
Predicting separately for each cognitive test if a patient is impaired (8 models with dichotomous outcomes).
Predicting a patient's cognitive function separately for each cognitive test (8 models with continuous outcomes).

Objectives 1 and 2 are dependent on the test scores of all 8 cognitive tests. For objectives 3 and 4, models were fit individually for each of the 8 cognitive tests. This results in a total of 18 outcome measures across the 4 objectives.

Thirteen different frequently used machine learning models were evaluated which were selected to span a broad set of characteristics such as their ability to perform regularization, capture nonlinearities, and capture interaction effects. Models were (Logistic) Regression: linear model, ElasticNet: linear model with regularization (L1 and L2), Gaussian Processes: learns a distribution over (complex) functions, Bayesian Ridge: Bayesian linear model with L2 regularization, Bayesian ARD: Bayesian sparse linear model with L1 regularization, K-Nearest Neighbors: predicts based on most similar data points, Decision Tree: simple, interpretable tree-based model, Random Forest: ensemble of decision trees, Support Vector Machines: separates classes or fits data with hyperplanes, can use kernels, XGB Tree: boosted decision trees, builds decision trees sequentially, XGB Linear: similar to XGB Tree but uses linear models, Partial Least Squares: finds new features as linear combinations of original features, Gaussian Mixture Model: classifies based on a mixture of Gaussian distributions.

---

### United European Gastroenterology (UEG) and European Society for Neurogastroenterology and Motility (ESNM) consensus on gastroparesis [^f18c5319]. United European Gastroenterology Journal (2021). High credibility.

Regarding diagnostic investigations for gastroparesis, more specifically with respect to gastric emptying studies, indications, ESNM/UEG 2021 guidelines recommend to obtain gastric emptying testing to establish the diagnosis of GP.

---

### United European Gastroenterology (UEG) and European Society for Neurogastroenterology and Motility (ESNM) consensus on gastroparesis [^2469fbb1]. United European Gastroenterology Journal (2021). High credibility.

Regarding screening and diagnosis for gastroparesis, more specifically with respect to differential diagnosis, ESNM/UEG 2021 guidelines recommend to exclude eating disorders in case of weight loss.

---

### European Respiratory Society guidelines for the diagnosis of primary ciliary dyskinesia [^b3c912a8]. The European Respiratory Journal (2017). Medium credibility.

Regarding diagnostic investigations for primary ciliary dyskinesia, more specifically with respect to genetic testing, ERS 2017 guidelines recommend to follow national and international best practice guidelines for genetic testing and interpretation of results.

---

### Standards of care in diabetes – 2025 [^ef149bc3]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for diabetic nephropathy, more specifically with respect to pediatric patients, ADA 2025 guidelines recommend to determine the eGFR at the time of diagnosis and annually thereafter.

---

### The adaptive designs CONSORT extension (ACE) statement: a checklist with explanation and elaboration guideline for reporting randomised trials that use an adaptive design [^fcaa45bf]. BMJ (2020). Excellent credibility.

Details of pre-planned adaptations with rationale should be documented in accessible study documents for readers to be able to evaluate what was planned and unplanned (such as protocol, interim and final SAP or dedicated trial document). Of note, any pre-planned adaptation that modifies eligibility criteria (such as in population enrichment ADs) should be clearly described.

Adaptive trials use accrued statistical information to make pre-planned adaptation(s) (item 14c) at interim analyses guided by pre-planned decision-making criteria and rules (item 7b). Reporting this statistical information for guiding adaptations and how it is gathered is paramount. Analytical derivations of statistical information guiding pre-planned adaptations using statistical models or formulae should be described to facilitate reproducibility and interpretation of results. The use of supplementary material or references to published literature is sufficient. For example, sample size re-assessment (SSR) can be performed using different methods with or without knowledge or use of treatment arm allocation. Around 43% (15/35) of regulatory submissions needed further clarifications because of failure to describe how a SSR would be performed. Early stopping of a trial or treatment group for futility can be evaluated based on statistical information to support lack of evidence of benefit that is derived and expressed in several ways. For example, conditional power, predictive power, the threshold of the treatment effect, posterior probability of the treatment effect, or some form of clinical utility that quantifies the balance between benefits against harmsor between patient and society perspectives on health outcomes. See box 8 for exemplars.

---

### Advancing principled pharmacoepidemiologic research to support regulatory and healthcare decision making: the era of real-world evidence [^bcfff043]. Clinical Pharmacology and Therapeutics (2025). Medium credibility.

SECTION 1: EVOLUTION OF THE "ERA"

Randomized controlled trials (RCTs) have long been the reference standard for evaluating causal inference (e.g. assessing the efficacy and safety of new therapeutic interventions) due to their high internal validity. Therapies are randomly assigned to patients (study subjects) using a well‐defined randomization schema, and physicians and patients are often blinded to the treatment assignment. Uniform diagnostic criteria for participation and outcome assessment are applied. The potential for confounding is diminished since randomization creates an expectation of balanced prevalence of both known and unknown confounding factors across treatment groups, and the potential for information bias is reduced by blinded treatment assignment and use of uniform assessment criteria.

That said, well‐designed and adequately powered trials are expensive, resource intensive, often take years to complete, and may not be feasible or ethical due to lack of clinical equipoise. Furthermore, trials may address only some of the questions most relevant to patients and physicians, leaving many healthcare decisions without supportive RCT evidence to guide them. Some trials may use surrogate markers for outcomes because the more relevant outcome, for example, mortality, may be unlikely to occur with sufficient frequency during the study period to be informative. Trials may not be large enough or long enough to observe rare events or events with long induction periods. Additionally, RCTs tend to more often enroll well‐educated, affluent patients who are more adherent to therapy and follow‐up, and may underrepresent racial and ethnic minority patients who may be at higher risk of the disease or its sequelae. Similarly, trials may not enroll patients with multimorbidity or receiving treatments outside the study protocol. The number of decisions that must be made far exceeds what is feasible to evaluate in RCTs, and the clinical evidence demands are too pressing to wait for RCT completion. To aid real‐world decisions, particularly when evidence from traditional RCTs is not available, RWE is increasingly being used as a complement to RCTs to generate causal inferences regarding medicine effectiveness and safety.

---

### Roflumilast (Zoryve) [^8f91de64]. FDA (2024). Medium credibility.

The dosage of roflumilast TOP for treatment of seborrheic dermatitis in both children (in patients ≥ 9 years) is 1 thin layer TOP daily (0.3% foam)

---

### Bayesian MRI reconstruction with joint uncertainty estimation using diffusion models [^f00b2c90]. Magnetic Resonance in Medicine (2023). Medium credibility.

Purpose

We introduce a framework that enables efficient sampling from learned probability distributions for MRI reconstruction.

Method

Samples are drawn from the posterior distribution given the measured k-space using the Markov chain Monte Carlo (MCMC) method, different from conventional deep learning-based MRI reconstruction techniques. In addition to the maximum a posteriori estimate for the image, which can be obtained by maximizing the log-likelihood indirectly or directly, the minimum mean square error estimate and uncertainty maps can also be computed from those drawn samples. The data-driven Markov chains are constructed with the score-based generative model learned from a given image database and are independent of the forward operator that is used to model the k-space measurement.

Results

We numerically investigate the framework from these perspectives: (1) the interpretation of the uncertainty of the image reconstructed from undersampled k-space; (2) the effect of the number of noise scales used to train the generative models; (3) using a burn-in phase in MCMC sampling to reduce computation; (4) the comparison to conventional ℓ₁ -wavelet regularized reconstruction; (5) the transferability of learned information; and (6) the comparison to fastMRI challenge.

Conclusion

A framework is described that connects the diffusion process and advanced generative models with Markov chains. We demonstrate its flexibility in terms of contrasts and sampling patterns using advanced generative priors and the benefits of also quantifying the uncertainty for every pixel.

---

### Standards of care in diabetes – 2025 [^1f4eb314]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for diabetes mellitus type 1, more specifically with respect to assessment for hyperglycemic crises, ADA 2025 guidelines recommend to provide structured education on the recognition, prevention, and management of hyperglycemic crisis to patients with T2DM having experienced these events and patients at high risk for these events.

---

### Standards of care in diabetes – 2025 [^87b916c6]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for diabetes mellitus type 1, more specifically with respect to assessment for hypoglycemia, ADA 2025 guidelines recommend to screen patients at risk for hypoglycemia for impaired hypoglycemia awareness at least annually and when clinically appropriate.
Refer patients to a trained healthcare professional for evidence-based intervention to improve hypoglycemia awareness.

---

### Laboratory perspectives in the development of polygenic risk scores for disease: a points to consider statement of the American College of Medical Genetics and genomics (ACMG) [^7691bdc5]. Genetics in Medicine (2023). High credibility.

ACMG polygenic risk scores (PRSs) — key learning points emphasize that currently there are no large-scale studies that have established the CU of PRS or integrated risk models, and that CU should be disease and individual population specific and must be established before broad implementation of PRSs.

---

### Clinical practice guidelines for multiple endocrine neoplasia type 1 (MEN1) [^86070552]. The Journal of Clinical Endocrinology and Metabolism (2012). Medium credibility.

Regarding follow-up and surveillance for multiple endocrine neoplasia type 1, more specifically with respect to surveillance for tumors, parathyroid, ES 2012 guidelines recommend to obtain an annual assessment of plasma calcium and PTH concentrations to screen for primary hyperparathyroidism.

---

### A comparison of approaches to account for uncertainty in analysis of imputed genotypes [^9c9001b8]. Genetic Epidemiology (2011). Low credibility.

The availability of extensively genotyped reference samples, such as "The HapMap" and 1,000 Genomes Project reference panels, together with advances in statistical methodology, have allowed for the imputation of genotypes at single nucleotide polymorphism (SNP) markers that are untyped in a cohort or case-control study. These imputation procedures facilitate the interpretation and meta-analyses of genome-wide association studies. A natural question when implementing these procedures concerns how best to take into account uncertainty in imputed genotypes. Here we compare the performance of the following three strategies: least-squares regression on the "best-guess" imputed genotype; regression on the expected genotype score or "dosage"; and mixture regression models that more fully incorporate posterior probabilities of genotypes at untyped SNPs. Using simulation, we considered a range of sample sizes, minor allele frequencies, and imputation accuracies to compare the performance of the different methods under various genetic models. The mixture models performed the best in the setting of a large genetic effect and low imputation accuracies. However, for most realistic settings, we find that regressing the phenotype on the estimated allelic or genotypic dosage provides an attractive compromise between accuracy and computational tractability.

---

### Learning non-stationary langevin dynamics from stochastic observations of latent trajectories [^006fffb3]. Nature Communications (2021). High credibility.

Fig. 5
Model selection and uncertainty quantification.

a Feature complexityincreases over GD iterations at a rate that varies across data samples. The ground-truth feature complexity (gray line) is achieved on different iteration for different data samples. b JS divergence between the models discovered from data samples 1 and 2 for each level of the feature complexity. The optimal valueis defined as a maximum feature complexity for which JS divergence does not exceed a fixed threshold (dashed line). The dots correspond to the potential in c. c A pair of fitted potentials at(left, gray dot in b),(middle, orange dot in b), and(right, teal dot in b) for data samples 1 and 2 (colors correspond to data in a). In all panels the ground-truth model is shown in gray. d JS divergences between models of the same complexity discovered from two data halves for each of 10 bootstrap samples (red line - same data as in b). The same threshold is used to select a pair of optimal models for each bootstrap sample. e Fitted potential (red) is the average of two potentials at optimalproduced by the model selection (middle panel in c). The confidence bounds (shaded area) are obtained as a pointwise 5% and 95% percentile across twenty potentials produced by the model selection on 10 bootstrap samples in d. The ground-truth model is shown in gray.

Practical applications often require quantifying uncertainty of the inferred model, which can be performed via bootstrapping, as we illustrate using the same data as in Fig. 3 a. To obtain confidence bounds for the inferred model, we generate ten bootstrap samples by sampling trials randomly with replacement from the set of all trials. For each bootstrap sample, we refit the model and perform model selection using our feature consistency method (Fig. 5 d). We then obtain the confidence bounds for the inferred potential by computing a pointwise 5% and 95% percentile across 20 potentials produced by the model selection on ten bootstrap samples (Fig. 5 e). As expected, the uncertainty is largest in the regions where the potential is high, i.e. where the density of latent trajectories and hence the amount of spike data are low.

---

### Standards of care in diabetes – 2025 [^9fec4de9]. Diabetes Care (2025). High credibility.

Regarding follow-up and surveillance for diabetes mellitus type 1, more specifically with respect to continuous glucose monitoring, ADA 2025 guidelines recommend to assess skin reactions, either due to irritation or allergy, and address to aid in successful use of devices.

---

### Efficient probabilistic inference in generic neural networks trained with non-probabilistic feedback [^63be3b73]. Nature Communications (2017). Medium credibility.

Error-based learning accounts for the time course of behavior

The dependence of the networks' performance on the number of training trials (Supplementary Fig. 1) suggests a possible explanation for deviations from optimal inference sometimes observed in experimental studies: i.e. insufficient training in the task. Testing this hypothesis rigorously is complicated by possible prior exposure of the subjects to similar stimuli or tasks under natural conditions. Among the tasks considered in this paper, the binary categorization task minimizes such concerns, because it involves classifying stimuli into arbitrary categories. Moreover, in this task, the behavior of both human and monkey observers were best accounted for by heuristic models that were quantitatively suboptimal, but qualitatively consistent with the optimal inference model. Therefore, we sought to test the insufficient-training hypothesis for suboptimal inference in this task.

The stimulus distributions for the two categories and the decision boundaries predicted by the optimal (OPT) and three suboptimal models (FIX, LIN, and QUAD) are shown in Fig. 9a, b. Different suboptimal models make different assumptions about the dependence of the decision boundary on the sensory noise level, σ (Fig. 9b). In particular, the FIX model assumes that the decision boundary is independent of σ, whereas LIN and QUAD models assume that the decision boundary is a linear or quadratic function of σ, respectively. The learning curve of a monkey subject who performed a large number of trials in this task (monkey L in ref.) is shown in Fig. 9c together with the performance of a neural network that received the same sequence of trials as the subject. The input noise of the network was matched to the sensory noise estimated for the subject and the learning rate of the network was optimized to fit the learning curve of the subject. The neural network was trained online, updating its parameters after each trial, in an analogous manner to how the monkey subject learned the task.

---

### United European Gastroenterology (UEG) and European Society for Neurogastroenterology and Motility (ESNM) consensus on gastroparesis [^916df9ba]. United European Gastroenterology Journal (2021). High credibility.

Regarding screening and diagnosis for gastroparesis, more specifically with respect to differential diagnosis, ESNM/UEG 2021 guidelines recommend to consider obtaining imaging to exclude small bowel obstruction.

---

### Standards of care in diabetes – 2025 [^94855997]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for hypoglycemia, more specifically with respect to pediatric patients (glycemic targets, T1DM), ADA 2025 guidelines recommend to consider setting even less stringent HbA1c goals (such as < 8%; < 64 mmol/mol) in patients with a history of severe hypoglycemia, limited life expectancy, or where the harms of treatment are greater than the benefits.