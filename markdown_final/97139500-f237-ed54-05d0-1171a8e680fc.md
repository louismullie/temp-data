# Medical Question & Answer

**Sample ID**: 97139500-f237-ed54-05d0-1171a8e680fc
**Dataset Index**: 219901

---

## Question

1. Find the value of the expression a³ + b³ + 12ab, given that a + b = 4.

---

## Answer

> Let's see… What do we have here? The user is asking to find the value of the expression (a³ + b³ + 12ab) given that (a + b = 4). Let's break this down step-by-step. First, I need to think about algebraic identities that relate (a³ + b³) to (a + b) and (ab). Then, I should verify how to express (ab) in terms of the given sum. Next, I will substitute and simplify the expression. Finally, I will double-check the arithmetic to ensure accuracy.

> Let me first confirm the key identity. The sum of cubes can be factored as (a³ + b³ = (a + b)(a² − ab + b²)), and I can rewrite (a² − ab + b²) as ((a + b)²−3ab), so substituting that in gives (a³ + b³ = (a + b)((a + b)²−3ab)). Wait, let me verify that expansion: ((a + b)² = a² + 2ab + b²), so ((a + b)²−3ab = a² + 2ab + b²−3ab = a² − ab + b²), which checks out.

> Now, I need to express (ab) in terms of the given sum. Hold on, let's not jump to conclusions; I initially thought (ab = [(a + b)²/2]), but that's incorrect. Let me reconsider: ((a + b)² = a² + 2ab + b²), so (ab = [((a + b)² − (a² + b²))/2]), which still involves (a² + b²). Hmm, wait a minute, I should verify whether I can proceed without knowing (a² + b²) explicitly. Since the expression I need to evaluate is (a³ + b³ + 12ab), and I already have (a³ + b³) in terms of (a + b) and (ab), I can substitute and then see if terms cancel or simplify, even if I don't know (ab) individually yet.

> Substituting the identity into the target expression, I get (a³ + b³ + 12ab = (a + b)((a + b)²−3ab) + 12ab). Given (a + b = 4), this becomes (4(4²−3ab) + 12ab = 4(16−3ab) + 12ab = 64−12ab + 12ab). I should double-check that cancellation: yes, (-12ab + 12ab = 0), so the expression simplifies to 64. Interestingly, the (ab) term cancels out, which means the value is constant for any (a) and (b) such that (a + b = 4), and I do not need to solve for (ab) explicitly [^notfound].

> Let me verify with a concrete example to ensure there's no hidden mistake. If (a = 4) and (b = 0), then (a + b = 4) and the expression is (4³ + 0³ + 12 × 4 × 0 = 64 + 0 + 0 = 64). If (a = 2) and (b = 2), then (a + b = 4) and the expression is (2³ + 2³ + 12 × 2 × 2 = 8 + 8 + 48 = 64). Both cases yield 64, which supports the algebraic result [^notfound].

> Final answer: 64.

---

The value of (a³ + b³ + 12ab) given (a + b = 4) is **64**. This is found by using the identity (a³ + b³ = (a + b)³−3ab(a + b)), substituting (a + b = 4), and simplifying to (64−12ab + 12ab = 64).

---

## Step-by-step solution

### Step 1: Apply the identity for the sum of cubes

Use the identity (a³ + b³ = (a + b)³−3ab(a + b)) to rewrite the expression:

a³ + b³ + 12ab = (a + b)³−3ab(a + b) + 12ab

---

### Step 2: Substitute the given condition

Substitute (a + b = 4) into the expression:

4³−3ab(4) + 12ab

64−12ab + 12ab

---

### Step 3: Simplify the expression

The (−12ab) and (+12ab) terms cancel, yielding:

64

---

## Verification with specific values

To confirm, test with specific (a, b) such that (a + b = 4):

- **Case 1**: (a = 2, b = 2)

2³ + 2³ + 12(2)(2) = 8 + 8 + 48 = 64

- **Case 2**: (a = 1, b = 3)

1³ + 3³ + 12(1)(3) = 1 + 27 + 36 = 64

- **Case 3**: (a = 0, b = 4)

0³ + 4³ + 12(0)(4) = 0 + 64 + 0 = 64

Each case yields **64**, confirming the result.

---

## Conclusion

The value of (a³ + b³ + 12ab) given (a + b = 4) is **64**.

---

## References

### Approaches to working in high-dimensional data spaces: gene expression microarrays [^d36e72ec]. British Journal of Cancer (2008). Low credibility.

MARKER IDENTIFICATION

Marker identification aims to discover those genes and their complex interaction effects that have statistically significant correlations with cancer phenotypes. As it is currently largely unclear how molecular variants and their interactions determine cancer pathogenesis and propensity, marker identification is valuable for improving understanding of the molecular mechanisms of cancers and for suggesting novel drug targets. Discovered markers may also define a subset of networked causal genes that regulate disease phenotype. A review of the current state of this effort is discussed by.

The objectives of feature selection for predictive classification and marker identification bear close resemblance. Although it is tempting to view these two problems as 'one and the same', this is often inappropriate. Inclusion of some true cancer markers in a feature set for cancer classification may provide negligible improvement in classification accuracy even though these markers are significantly associated with the cancer outcome of interest. A trivial example is where two markers are perfectly correlated, in which case only one of the two needs to be included in a predictive feature subset. A more interesting example is the one in which, even though two markers are only partially correlated, a classification model will not perceive any benefit from using both markers. This is illustrated below:

Let A and B take on one of four possible discrete values and suppose the ground-truth statistics on class label C are Prob [C = 'cancer'∣ A = 1] = 1.0; Prob [C = 'cancer'∣ A = i] = 0.5, i = 2,3,4; Prob [C = 'cancer'∣ B = 3] = 1.0; and Prob [C = 'cancer'∣ B = j] = 0.5, j = 1,2,4. Suppose Prob [A = 1] = 0.1; Prob [B = 3] = 0.7; and Prob [B = 3∣ A = 1] = 0.5. Thus, A and B are both informative about the disease (for one value), and these variables are only partially correlated. However, in a small training set, it is quite possible that each time A = 1, B = 3 also occurs, even though Prob [B = 3∣ A = 1] is much less than one. In this case, while association-based marker discovery might include both A and B, classification-based marker discovery would only include B, because the training set suggests no predictive benefit from including A.

---

### ARSDA: a new approach for storing, transmitting and analyzing transcriptomic data [^ed89ed30]. G3 (2017). Low credibility.

Allocating sequence reads in gene family with more than two members

One might, mistakenly, think that it is quite simple to extend Equation (3) for a gene family of two members to a gene family with F members by writingThis does not work. For example, if we have three paralogous genes designated A, B, and C, respectively. Suppose that the gene duplication that gave rise to B and C occurred very recently so that B and C are identical, but A and the ancestor of B and C have diverged for a long time. In this case, N U.B = N U.C = 0 because a read matching B will always match C equally well, but N U. A may be > 0. This will result in unfair allocation of many transcripts from B and C to A according to Equation (4). I outline the approach below for dealing with gene families with more than two members.

With three or more paralogous genes, one may benefit from a phylogenetic tree for proper allocation of sequence reads. I illustrate the simplest case with a gene family with three paralogous genes A, B, and C, idealized into three segments in Figure 3. The three genes shared one identical middle segment with 23 matched reads (that necessarily match equally well to all three paralogues). Genes B and C share an identical first segment to which 20 reads matched. Gene A has its first segment different from that of B and C, and got four matched reads. The three genes also have a diverged third segment where A matched three reads, B matched six and C matched 12. Our task is then to allocate the 23 reads shared by all three, and 20 reads shared by B and C to the three paralogues.

---

### Optimization of b-value schemes for estimation of the diffusion coefficient and the perfusion fraction with segmented intravoxel incoherent motion model fitting [^9fe7e8ff]. Magnetic Resonance in Medicine (2019). Medium credibility.

4 RESULTS

4.1 Optimization

No more than three unique b‐values were found in the optimized b‐value schemes regardless of the total number of acquisitions. These b‐values were always 0 (preset) and the lower and upper limits set in the optimization, i.e. 200 and 800 s/mm². The optimal number of acquisitions of each b‐value was approximately twice as many for the middle and high b‐values (200 and 800 s/mm 2) as for b = 0 (Table 1), although the exact results depended on the total number of acquisitions. As the number of acquisitions was increased, the distribution of acquisitions over b‐values approximately matched the distribution over b‐values for infinite numbers of acquisitions (Table 1).

Optimized b‐value schemes were also generated without an upper b‐value limit and without adjusting for the echo‐time dependence on SNR. Omitting either the upper b‐value limit or the TE‐related SNR penalty had no effect on the solution other than a slightly higher value of the high b‐value (b₂ ≈ 850 s/mm²) (Supporting Information Tables S1 and S2). Omitting both the upper b‐value limit and the TE‐related SNR penalty resulted in minor effects on the proportions of acquisitions at each b‐value, but resulted in a substantially higher value of the high b‐value (b₂ ≈ 1300 s/mm²) (Supporting Information Table S3).

---

### Dynamic and asymmetric colloidal molecules [^6e1c3d4f]. Nature Communications (2025). High credibility.

For N ≥ 4 CMs, we quantify the asymmetric and fluctuated structure profile by using two parameters, δ, representing the deviation between the average (asymmetric) and standard (symmetric) configuration, and Δ, indicating the magnitude of fluctuations around the average configuration (see Methods for definitions). Figure 1 c and Figs. S1c-e display the diagram modifying δ and Δ for 4-CMs and 6-CMs, respectively. Notably, all CMs tend to converge to solutions of Thomson problems as κ σ s and σ c decrease and A r increases. This behaviour is expected as inter-satellite repulsive interactions become sufficiently long-range to stabilise regular configurations corresponding to solutions to the Thomson problem.

We compare the experimental configurations at different TBAB concentrations with the corresponding simulations (see Methods for technical details on the comparison). As the TBAB concentration decreases from 5 μM (κ σ s = 4.5, A r = 4.7 k B T) to 1 μM (κ σ s = 1.9, A r = 6.5 k B T), the configurations of 2, 3, and 4-CMs observed in experiments become more regular, though they still exhibit significant asymmetry. This trend is in full agreement with the simulation predictions. Notably, unlike in simulations, where κ σ and A r can be varied independently, in experiments both the Debye screening length and the particle surface charge are functions of the salt concentration. The tunability of the asymmetry strongly depends on the physicochemical properties of the system, including the particle material, solvent choice, and type of salt solute.

Nevertheless, the optimal ranges for asymmetric CM assembly can be universally adapted to other experimental systems. From our simulations, we have identified that to achieve asymmetric CMs, the range of κ σ s should ideally be between 1 and 20, while the surface potential should range from 10 to 100 k B T. For our system with 1-μm diameter particles, the optimal ζ potential is between 10 and 20 mV, with the TBAB concentration around 1 μM. In an aqueous solvent (where the relative dielectric constant ϵ r ≈ 80), given that A r ∝ ϵ r σ s ζ² and, achieving similar conditions would require adjustments such as using a smaller particle size (σ s between 200 and 500 nm), lower ζ potential (5 to 10 mV), and an increase in ionic strength (n ion between 10 and 100 μM).

---

### Gene regulatory network inference from sparsely sampled noisy data [^c4f997e1]. Nature Communications (2020). High credibility.

The GP framework can handle combinatorial effects, meaning nonlinearities that cannot be decomposed into f(x₁, x₂) = f₁(x₁) + f₂(x₂). This is an important property for modelling a chemical system — such as gene expression — where reactions can happen due to combined effects of reactant species. For example, the dynamics corresponding to a chemical reaction x₁ + x₂ → x₃ cannot be modelled by.

In a classical variable selection problem from input–output data ηⱼ = f(ξⱼ) + vⱼ, for j = 1, …, N, where, the task is to find out which components of ξ does the function f depend on. In the GP framework, variable selection can be done by a technique known as "automatic relevance determination", based on estimating the hyperparameters of the covariance function of f. For BINGO, we have chosen the squared exponential covariance function for each component fᵢ:The hyperparameters βᵢ,ⱼ ≥ 0 are known as inverse length scales, sincecorresponds to a distance that has to be moved in the direction of the j th coordinate for the value of function f i to change considerably. If βᵢ,ⱼ = 0, then fᵢ is constant in the corresponding direction. The mean function for fᵢ is mᵢ(x) = bᵢ − aᵢxᵢ where aᵢ and bᵢ are regarded as nonnegative hyperparameters corresponding to mRNA degradation and basal transcription, respectively.

---

### Weight centile crossing in infancy: correlations between successive months show evidence of growth feedback and an infant-child growth transition [^ce868282]. The American Journal of Clinical Nutrition (2016). Low credibility.

A completely different analytic approach was to consider deviations over all possible time intervals (i.e. not just adjacent intervals) and to identify strongly (positively or negatively) correlated pairs of deviations. Note that for this analysis, pairs of deviations with overlapping time intervals (e.g. d₁₄ and d₂₃) were excluded.

In principle, the correlations would be affected by the measurement ages differing slightly from the target ages. However, it turned out that adjusting for this source of variability rarely altered the correlations by > 0.01, so it was ignored.

The clinical impact of growth feedback was derived as follows. The regression equation corresponding to the correlation r 12.23 is the regression of the second deviation on the first:

where a is the intercept, b the regression coefficient, and ε the error term. Coefficient b and correlation r 12.23 are equally significant by definition and, in practice, were very similar in value because the variances of the 2 deviations were similar. Equation 3 can be rearranged to give the algebraically identical equation that predicts the future z score (z₃) from the recent deviation (z₂ – z₁) and the current z score (z₂):

Now, Equation 4 can be generalized by regressing z₃ on (z₂ – z₁) and z₂, estimating coefficient c for z₂:

It turned out that Equation 5 fit better than Equation 4, and c was usually < 1 (i.e. adjusting for regression to the mean). So, if r 12.23 (and hence b) was significant it was worthwhile to use the deviation (z₂ – z₁) as well as the current z score (z₂) to predict the future z score (z₃). The size of b indicated how much predicted z₃ changed among infants with the same z₂, given a unit increase in (z₂ – z₁). Intercept a showed how heavy the infants were compared with the WHO standard.

---

### Practice guidelines for the prevention, detection, and management of respiratory depression associated with neuraxial opioid administration: an updated report by the American society of anesthesiologists task force on neuraxial opioids and the American Society of Regional Anesthesia and Pain Medicine [^c7d4883b]. Anesthesiology (2016). Medium credibility.

American Society of Anesthesiologists neuraxial opioid guideline — evidence grading and opinion methodology: Category B observational evidence permits directional inference, and for studies that report statistical findings, "the threshold for significance is P < 0.01"; Category B levels comprise "observational comparisons… with comparative statistics" (Level 1), "noncomparative observational studies with associative statistics" (Level 2), "noncomparative observational studies with descriptive statistics" (Level 3), and "case reports" (Level 4). Opinion-based inputs include formal surveys and informal sources; survey types are designated "Category A: Expert Opinion" and "Category B: Membership Opinion", with additional "Category C: Informal Opinion". Survey responses "are recorded using a five-point scale and summarized based on the median values", with median categories defined as follows: "Strongly Agree: Median score of 5 (at least 50% of the responses are 5)… Agree: Median score of 4 (at least 50% of the responses are 4 or 4 and 5)… Equivocal: Median score of 3 (at least 50% of the responses are 3, or no other response category or combination of similar categories contain at least 50% of the responses)… Disagree: Median score of 2 (at least 50% of responses are 2 or 1 and 2)… Strongly Disagree: Median score of 1 (at least 50% of responses are 1)". When equal numbers of distinct responses occur, "the median value is determined by calculating the arithmetic mean of the two middle values".

---

### Genetic instability as a driver for immune surveillance [^93264a26]. Journal for Immunotherapy of Cancer (2019). Medium credibility.

Along with genetic instability, another parameter is key to the dynamics of the system. Regarding the second nullcline, we can see its size is linearly affected by the influx m of immune cells arriving at the tumor site. It is therefore interesting to understand how μ and m are related to cancer-immune scenarios, since this will open the door to further discussion on mutagenic and immune activation therapies.

By solving E₁(c) = E₂(c), we can understand how the values of m and μ affect the nature and number of possible solutions of the system. We here write (ρ_NK + ρ_T(μ)) = ρ for simplicity. The previous identity leads to a cubic expression of the form Ac³ + Bc² + Cc + D = 0, with

The sign of the discrimant Δ = 18ABCD − 4B³D + B²C² − 4AC³ − 27A²D² will define of which combinations of m and μ belong to which scenarios of Fig. 4. Knowing that three real roots exists for Δ > 0 and only one for Δ < 0, the transitions between attractor scenarios happen to occur at Δ = 0. This condition can be used to easily describe the whole bifurcation space as seen in the results and Fig. 4 e and f, showing how mutation frequencies and immune stimulation affect the possible outcomes of the system.

---

### Acyclovir [^20d9b6f7]. FDA (2024). Medium credibility.

The dosage of acyclovir IV for treatment of B virus infection in adults is 12.5–15 mg/kg IV q8h until symptoms resolve and the results of two cultures are negative

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^17be5682]. Nature Communications (2025). High credibility.

Fig. 5
Accurate classification requires non-equilibrium driving.

This plot corresponds to the classification task in Fig. 3 A using B_a and B_b as inputs instead of F_a and F_b. The classification accuracy, defined as the average of π_ρ(F_ρ; θ) over 10³ randomly drawn samples of F_ρ from classes ρ = 1, 2, is shown as a function of F_max, the maximum absolute value of F_ij that is allowed on any edge. Five training trials for each value of F_max were performed, and the gray area illustrates the standard deviation of accuracy over these trials. Insets schematically illustrate the feature vectors in each regime.

An additional perspective on non-equilibrium Markov networks trained for classification can be gained from recent work exploring the analogies between transformers, which implement softmax-based filters over key, query, and value vectors, and models of dense Hopfield networks, in which relaxational dynamics in the landscape of a softmax-like energy function allows for storage of a far greater number of associative memories compared with the usual quadratic Hopfield energy functions. The linear attention-like form of the matrix-tree expression, Eq. (3), motivates consideration of the corresponding energy-like function that describes Markov steady states, which we identify in analogy with ref.as, where ψ (k θ) and χ (k F) are the non-linear feature vectors in the attention function. Plotting this function over the input space (F a, F b) in trained and untrained graphs reveals a landscape characterized by flat, sloping basins delimited by creases of high curvature. In trained graphs some of these creases co-localize with the learned decision boundaries, separating the input space into regions in which different subsets of trees dominate the contribution to. Some of these creases also represent topological features of the graph which cannot be removed by training, such as mismatched limits at the corners of the input domain (e.g. F a → ∞ followed by F b → ∞ or vice versa). In the Supplementary Information we expand on this analogy between the steady states of Markov processes and dense associative memories, but we leave a full treatment of this connection to future work. We note in addition that the form of the classification function (Eqs. (2) and (3)) might support comparisons to other existing machine learning architectures, such as kernel-based classifiers, besides transformers.

---

### A comparison of phase II study strategies [^ebb760ae]. Clinical Cancer Research (2009). Low credibility.

The traditional oncology drug development paradigm of single arm phase II studies followed by a randomized phase III study has limitations for modern oncology drug development. Interpretation of single arm phase II study results is difficult when a new drug is used in combination with other agents or when progression-free survival is used as the endpoint rather than tumor shrinkage. Randomized phase II studies are more informative for these objectives but increase both the number of patients and time required to determine the value of a new experimental agent. In this article, we compare different phase II study strategies to determine the most efficient drug development path in terms of number of patients and length of time to conclusion of drug efficacy on overall survival.

---

### Structure and inference in annotated networks [^005e543f]. Nature Communications (2016). Medium credibility.

Putting it all together, our expression for the log-likelihood is

Neglecting terms beyond first order in small quantities, the first sum can be rewritten as

where we have made use ofand.

The first two terms in (13) are constant for any given network and hence can be neglected — they are irrelevant for comparing the likelihood values between different runs on the same network. The final term can be rewritten using equation (8) as

which is also a constant and can be neglected. Thus, only the third term in (13) need be carried over.

The second sum in (12) is

where we have used equation (8) again in the third equality.

The final sum in (12) is the entropy of the posterior distribution q (s), which is harder to calculate because it requires not just the marginals of q but the entire distribution. We get around this by making the so-called Bethe approximation:

which is exact on trees and locally tree-like networks, and is considered to be a good working approximation on other networks. Substituting this form into the entropy term gives

Finally, combining equations (13)–(17), and substituting into equation (12), our complete expression for the log-likelihood, neglecting constants, is

The run that returns the largest value of this quantity is the run with the highest likelihood and hence the best fit to the model.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^01148c3a]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Nonparametric tolerance intervals for non-normal distributions: The above estimate of the tolerance interval would only be applicable to a population that is normally distributed, but when the underlying population is often not normal (eg, when there is a natural boundary that the data cannot exceed (ie, 0% or 100%)) it is helpful to define tolerance intervals using nonparametric methods; the one-sided nonparametric tolerance interval can be determined by finding the value for k that satisfies the cumulative binomial equation, where CL is the confidence level (eg, 0.95), and by setting k = 0 (ie, 0 failures) the formula can be simplified.

---

### Abstract representations of events arise from mental errors in learning and memory [^e68b0dcf]. Nature Communications (2020). High credibility.

Together, the total cost of a distribution Q is its free energy F (Q) = βE (Q) − S (Q), where β is the inverse temperature parameter, which quantifies the relative value that the brain places on accuracy versus efficiency. In this way, our assumption about resource constraints in the brain leads to a particular form for P: it should be the distribution that minimizes F (Q), namely the Boltzmann distributionwhere Z is the normalizing constant (Methods). Free energy arguments similar to the one presented here have been used increasingly to formalize constraints on cognitive functions, with applications from active inferenceand Bayesian learning under uncertaintyto human action and perception with temporal or computational limitations. Taken together, Eqs. (3)–(5) define our maximum entropy model of people's internal transition estimates.

To gain an intuition for the model, we consider the infinite-time limit, such that the transition estimates become independent of the particular random walk chosen for analysis. Given a transition matrix A, one can show that the asymptotic estimates in our model are equivalent to an average over walks of various lengths, which, in turn, can be fashioned into the following analytic expression, where I is the identity matrix (Methods). The model contains a single free parameter β, which represents the precision of a person's mental representation. In the limit β → ∞ (no mental errors), our model becomes equivalent to maximum likelihood estimation (Fig. 3a), and the asymptotic estimatesconverge to the true transition structure A (Fig. 3b), as expected. Conversely, in the limit β → 0 (overwhelming mental errors), the memory distribution P (Δ t) becomes uniform across all past nodes (Fig. 3a), and the mental representationloses all resemblance to the true structure A (Fig. 3b).

---

### Practice guidelines for central venous access 2020: an updated report by the American society of anesthesiologists task force on central venous access [^a6e34ece]. Anesthesiology (2020). High credibility.

American Society of Anesthesiologists central venous access guidelines — evidence grading and survey methods are defined as follows: "Category A level 1, meta-analysis of randomized controlled trials (RCTs); Category A level 2, multiple RCTs; Category A level 3, a single RCT; Category B level 1, nonrandomized studies with group comparisons; Category B level 2, nonrandomized studies with associative findings; Category B level 3, nonrandomized studies with descriptive findings; and Category B level 4, case series or case reports". "Statistically significant outcomes (P < 0.01) are designated as either beneficial (B) or harmful (H) for the patient; statistically nonsignificant findings are designated as equivocal (E)". "Survey responses for each recommendation are reported using a 5-point scale based on median values from strongly agree to strongly disagree". In addition, "These updated guidelines were developed by means of a five-step process".

---

### Evaluating molecular biomarkers for the early detection of lung cancer: when is a biomarker ready for clinical use? An official American Thoracic Society policy statement [^1f4d7938]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Representative biomarker accuracies meeting a required threshold — Table 6 lists "Representative Biomarker Accuracies That Would Satisfy the Calculation Yielding a Sensitivity/(1 − Specificity) ≥ 4.18", with examples including Sensitivity (%) 40 with Specificity (%) 90.4, Sensitivity (%) 60 with Specificity (%) 85.6, Sensitivity (%) 70 with Specificity (%) 83.4, and Sensitivity (%) 80 with Specificity (%) 80.9.

---

### What are the determinants of gene expression levels and breadths in the human genome? [^bf993525]. Human Molecular Genetics (2012). Low credibility.

DISCUSSION

In this paper, we have demonstrated the use of a novel multivariate multiple linear regression method, to determine relative contributions of various genomic traits explaining different aspects of gene expression. This method can be used in answering many other biological questions where we need to evaluate relative roles of different factors on multiple, related variables. Our results reveal that some genomic traits are significant predictors of both levels and breadths of gene expression, whereas others only affect one aspect of gene expression (Tables 2 and 3). For most genomic traits, the statistical significance was highly similar in the human and mouse data.

When we examine the ratios of regression coefficients across different predictors, an interesting pattern emerges. In most cases, these ratios are greater than 1. In other words, for each genomic trait, the absolute value of the regression coefficient tends to be greater for the expression level than for the expression breadth. This indicates that most genomic traits have stronger relations with expression levels than with expression breadths (however see below for important exceptions). Secondly, in all significant predictors, the signs of coefficients are the same for both levels and breadths of gene expression. In other words, all genomic traits considered here influence expression levels and breadths in similar directions, perhaps due to shared molecular pathways.

---

### A space-time tradeoff for implementing a function with master equation dynamics [^57fa10b0]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### Clinical utility of polygenic risk scores for embryo selection: a points to consider statement of the American College of Medical Genetics and genomics (ACMG) [^9ccd284a]. Genetics in Medicine (2024). High credibility.

PGT-P results communication — Figure 3 illustrates using 3 components (percentile rank, relative risk, and absolute risk): families (a) and (b) have embryos at rank extremes while family (c) shows a broader range; two families display different relative risk ranges, with family x values 0.9, 1.0, 1.1, 1.2 and family y values 0.6, 0.9, 1.1, 1.4; example absolute risk increases span 1.0% - 1.4% versus 10.0% - 14.0% for diseases differing by 10x population prevalence.

---

### State estimation of a physical system with unknown governing equations [^794152e0]. Nature (2023). Excellent credibility.

State estimation is concerned with reconciling noisy observations of a physical system with the mathematical model believed to predict its behaviour for the purpose of inferring unmeasurable states and denoising measurable ones 1,2. Traditional state-estimation techniques rely on strong assumptions about the form of uncertainty in mathematical models, typically that it manifests as an additive stochastic perturbation or is parametric in nature 3. Here we present a reparametrization trick for stochastic variational inference with Markov Gaussian processes that enables an approximate Bayesian approach for state estimation in which the equations governing how the system evolves over time are partially or completely unknown. In contrast to classical state-estimation techniques, our method learns the missing terms in the mathematical model and a state estimate simultaneously from an approximate Bayesian perspective. This development enables the application of state-estimation methods to problems that have so far proved to be beyond reach. Finally, although we focus on state estimation, the advancements to stochastic variational inference made here are applicable to a broader class of problems in machine learning.

---

### Modelling sequences and temporal networks with dynamic community structures [^7129d625]. Nature Communications (2017). Medium credibility.

Bayesian Markov chains with communities

As described in the main text, a Bayesian formulation of the Markov model consists in specifying prior probabilities for the model parameters, and integrating over them. In doing so, we convert the problem from one of parametric inference where the model parameters need to be specified before inference, to a nonparametric one where no parameters need to be specified before inference. In this way, the approach possesses intrinsic regularisation, where the order of the model can be inferred from data alone, without overfitting.

To accomplish this, we rewrite the model likelihood, using eqs. (1) and (5), asand observe the normalisation constraints,… Since this is just a product of multinomials, we can choose conjugate Dirichlet priors probability densitiesand, which allows us to exactly compute the integrated likelihood, whereand. We recover the Bayesian version of the common Markov chain formulation (see ref.) if we put each memory and token in their own groups. This remains a parametric distribution, since we need to specify the hyperparameters. However, in the absence of prior information it is more appropriate to make a noninformative choice that encodes our a priori lack of knowledge or preference towards any particular model, which amounts to choosing α x = β rs = 1, making the prior distributions flat. If we substitute these values in eq. (19), and re-arrange the terms, we can show that it can be written as the following combination of conditional likelihoods, wherewithbeing the multiset coefficient, that counts the number of m -combinations with repetitions from a set of size n. The expression above has the following combinatorial interpretation: P ({ x t }| b, { e rs }, { k x }) corresponds to the likelihood of a microcanonical modelwhere a random sequence { x t } is produced with exactly e rs total transitions between groups r and s, and with each token x occurring exactly k x times. In order to see this, consider a chain where there are only e rs transitions in total between token group r and memory group s, and each token x occurs exactly k x times. For the first transition in the chain, from a memory x 0 in group s to a token x 1 in group r, we have the probabilityNow, for the second transition from memory x 1 in group t to a token x 2 in group u, we have the probabilityProceeding recursively, the final likelihood for the entire chain iswhich is identical to eq. (21).

---

### A neural representation of prior information during perceptual inference [^d456c358]. Neuron (2008). Low credibility.

Perceptual inference is biased by foreknowledge about what is probable or possible. How prior expectations are neurally represented during visual perception, however, remains unknown. We used functional magnetic resonance imaging to measure brain activity in humans judging simple visual stimuli. Perceptual decisions were either biased in favor of a single alternative (A/ approximately A decisions) or taken without bias toward either choice (A/B decisions). Extrastriate and anterior temporal lobe regions were more active during A/ approximately A than A/B decisions, suggesting multiple representations of prior expectations within the visual hierarchy. Forward connectivity was increased when expected and observed perception diverged ("prediction error" signals), whereas prior expectations fed backward from higher to lower regions. Finally, the coincidence between expected and observed perception activated orbital prefrontal regions, perhaps reflecting the reinforcement of prior expectations. These data support computational and quantitative models proposing that a visual percept emerges from converging bottom-up and top-down signals.

---

### 2022 American society of anesthesiologists practice guidelines for management of the difficult airway [^4991ac84]. Anesthesiology (2022). High credibility.

American Society of Anesthesiologists difficult airway guidelines — evidence grading and significance reporting are specified as: Category A, level 1, meta-analysis of randomized controlled trials; Category A, level 2, multiple randomized controlled trials; Category A, level 3, a single randomized controlled trial; Category B, level 1, nonrandomized studies with control group comparisons; Category B, level 2, nonrandomized studies with associative findings; Category B, level 3, nonrandomized studies with descriptive findings; and Category B, level 4, case series or case reports. Statistically significant outcomes (P < 0.01) are designated as beneficial (B) or harmful (H), with statistically nonsignificant findings designated as equivocal (E). When available, Category A evidence is given precedence over Category B evidence for any particular outcome, and lack of sufficient scientific evidence is reported as "insufficient evidence". All meta-analyses are conducted by the ASA methodology group, and a minimum of five independent randomized controlled trials is required for meta-analysis. Survey responses for each recommendation are reported using a five-point scale based on median values from strongly agree to strongly disagree.

---

### Transient power-law behaviour following induction distinguishes between competing models of stochastic gene expression [^6513fe13]. Nature Communications (2025). High credibility.

What features of transcription can be learnt by fitting mathematical models of gene expression to mRNA count data? Given a suite of models, fitting to data selects an optimal one, thus identifying a probable transcriptional mechanism. Whilst attractive, the utility of this methodology remains unclear. Here, we sample steady-state, single-cell mRNA count distributions from parameters in the physiological range, and show they cannot be used to confidently estimate the number of inactive gene states, i.e. the number of rate-limiting steps in transcriptional initiation. Distributions from over 99% of the parameter space generated using models with 2, 3, or 4 inactive states can be well fit by one with a single inactive state. However, we show that for many minutes following induction, eukaryotic cells show an increase in the mean mRNA count that obeys a power law whose exponent equals the sum of the number of states visited from the initial inactive to the active state and the number of rate-limiting post-transcriptional processing steps. Our study shows that estimation of the exponent from eukaryotic data can be sufficient to determine a lower bound on the total number of regulatory steps in transcription initiation, splicing, and nuclear export.

---

### Two-locus identity coefficients in pedigrees [^fe175917]. G3 (2023). Medium credibility.

Two-locus identity coefficients

In this section, we show how the generalized two-locus kinship coefficients allow us to calculate two-locus identity coefficients. We split the presentation into three steps, in increasing order of pedigree complexity, which have different computational demands. We start with the unilineal case, where a simple trick provides efficient calculation of the matrix K.

Unilineal relationships

Letbe a unilineal relationship, andits matrix of two-locus IBD coefficients. Recall from equation (3) that K is uniquely determined by.

As evident from Example 1, we cannot generally computedirectly from. Moreover, equation (10) shows why: the phased components ofcontribute unequally to. However, it turns out that we can recoverby considering a slightly more complex, generalized kinship coefficient, constructed to balance the contributions.

Fig. 3.
The two-locus IBD patternused to computefor unilineal relationships. Two gametes are emitted from each of a and b. At the first locus, all four alleles are IBD. At the second locus, exactly one gamete of a is IBD with exactly one gamete of b.

Theorem 1 enables efficient calculation of, and thereby the entire matrix, for any. The endpointis trivial, as previously explained.

The four phased coefficientscan also be computed using a similar technique as that in Theorem 1. The idea is to find four different generalized IBD patternswhose coefficients are linear expressions in. By choosingsuch that the resulting system of linear equations has full rank, this can then be solved for the phased coefficients. Details are given in Appendix B.

---

### A general model-based causal inference method overcomes the curse of synchrony and indirect effect [^006a4a7d]. Nature Communications (2023). High credibility.

From each time series, the regulation-detection scoreis calculated for every type of 1D regulation σ from X to Y (X, Y = A, B, or C) (Step 1). Because only A ⊣ B satisfies the criteriafor every time series, only A ⊣ B is inferred as 1D regulation. Note that even for the other regulations, can occur for a few time series, leading to a false positive prediction. This can be prevented by using multiple time series. Next, is calculated for every 2D regulation type σ (Step 2). Three types of regulation (, and) satisfy the criteriafor every time series. Among these, we can identify false positive regulations by using a regulation-delta function (Step 3).is equal to zero for every time series, indicating thatandare false positive regulations. Thus, is the only inferred 2D regulation as it satisfies the criteria for the regulation-delta function (and). By merging the inferred 1D and 2D regulations, the regulatory network is successfully inferred. Here, note that regulation A → C is not detected by the 1D regulation–detection score since C has multiple causes. However, the 2D regulation-detection score detects, which contains A → C. This demonstrates the need for multi-dimensional inferences, as the 1D criteria alone would not have been sufficient to fully capture the regulatory relationships in the network. Since this system has three components, we infer up to 2D regulations. If there are N components in the system, we go up to (N − 1)D regulations (Supplementary Fig. 2).

We apply the framework to infer regulatory networks from simulated time-series data of various biological models. In these models, the degradation rates of molecules increase as their concentrations increase, like in most biological systems (i.e. self-regulation is negative). Such prior information, including the types of self-regulation, can be incorporated into our framework. For example, to incorporate negative self-regulation, when detecting N D regulation, one can use the (N + 1)D regulation-detection function and score that include negative self-regulation. Specifically, when inferring 1D positive regulation from X to Y, the criterionis used. To illustrate this, we assume the negative self-regulation to infer the network structures of biological models (see below for details). Note that this assumption is optional for inference (see Supplementary Information for details).

---

### Estimating the statistical significance of gene expression changes observed with oligonucleotide arrays [^ba8afa58]. Human Molecular Genetics (2002). Low credibility.

We present a simple method to assign approximate P-values to gene expression changes detected with Affymetrix oligonucleotide arrays and software. The method pools data for groups of genes and a small number of like-to-like comparisons in order to estimate the significance of changes observed for single genes in comparisons of experimental interest. Statistical significance levels are based on the observed variability in the fractional majority of probe pairs that indicate increasing or decreasing differential expression in comparisons of technical replicates. From this reference distribution or error model, we compute the expected frequency for fractional majorities in comparisons for N ≥ 2. These computed distributions are the source of P-value estimates for changes seen in the experimental comparisons. The method is intended to complement the Affymetrix software and to rationalize gene selection for experimental designs involving limited replication.

---

### Type 2 diabetes therapies: a STEPS approach [^fc25c424]. American Family Physician (2019). Medium credibility.

A concise way to evaluate pharmacotherapy options for type 2 diabetes mellitus is to use the five patient-oriented STEPS criteria: safety, tolerability, efficacy, price, and simplicity. The first-line treatment option, metformin, is safe and fairly well-tolerated, has excellent long-term efficacy for patient-oriented outcomes, is moderately priced, and has a simple dosing regimen. However, most patients with type 2 diabetes require more than one medication. The STEPS approach can help choose subsequent medications if metformin does not provide adequate glycemic control.

---

### Red blood cell transfusion: 2023 AABB international guidelines [^4e948de3]. JAMA (2023). Excellent credibility.

AABB 2023 RBC transfusion — conclusion and threshold: The panel recommends restrictive transfusion strategies with a typical threshold of 7 g/dL for both adult and pediatric patients, and it recognizes additional considerations such as signs, symptoms, comorbid conditions, and patient values and preferences that will differ between patients.

---

### American association for bronchology and interventional pulmonology (AABIP) evidence-based guidelines on bronchoscopic diagnosis and staging of lung cancer [^9631151b]. Journal of Bronchology & Interventional Pulmonology (2025). High credibility.

Figure 6 — lesion size ≤ 3 cm versus > 3 cm across 4 studies (n = 1247) found that the larger lesion size group (> 3 cm) had a higher event rate; no heterogeneity was detected (I² = 0.0%, τ² = 0, p = 0.9696), and the prediction interval was [−0.00; 0.12]. A representative individual study (Al-Sarraf 2008) reported RD 0.07 [-0.04; 0.19] with weights 14.9% (common) and 9.7% (random).

---

### Valproic acid [^4fcab4ba]. FDA (2025). Medium credibility.

The dosage of valproic acid PO for treatment of bipolar disorder in adults (maintenance therapy) is 1,000–3,000 mg PO daily

---

### Computational reconstruction of mental representations using human behavior [^105c6e01]. Nature Communications (2024). High credibility.

Data analysis

We corrected spelling mistakes in the participants' responses using the same procedure as the main experiment and considered plural and singular forms of a word to be equivalent. We computed, for each concept, whether: (1) the correct label was provided more often for the real reconstruction than for null reconstructions on average; (2) the semantic distance of responses to the correct label was lower for the real reconstruction than for null reconstructions on average; (3) the entropy of the response probability distribution was lower for the real reconstruction than for null reconstructions on average; (4) the trace of the covariance matrix of the semantic features of responses was smaller for the real reconstruction than for null reconstructions on average. The resulting vectors for these four metrics (one binary value per concept) were then summed to obtain the numbers of successful concepts reported in the main text. To get uncertainty estimates around these numbers, we recomputed the metrics after randomly resampling the binary vectors 1000 times with replacement. Confidence intervals were obtained by extracting the 2.5th and 97.5th percentiles of this bootstrap distribution. To test the statistical significance of the numbers of successful concepts, we calculated a null distribution for each metric by randomly permuting the 45 concepts (or in the case of analyses 3 and 4, randomly permuting all 4500 responses: 25 participants × 45 concepts × 4 images) 1000 times. We extracted the percentile of the actual value in this null distribution as the p value.

---

### Physicians ez use b-12 compliance [^cd6d22b1]. FDA (2024). Medium credibility.

HOW SUPPLIED

Cyanocobalamin Injection, USP is supplied as follows:

Store at 20° to 25°C (68° to 77°F) [see USP Controlled Room Temperature].

PROTECT FROM LIGHT.

Use only if solution is clear and seal intact.

---

### Evaluating molecular biomarkers for the early detection of lung cancer: when is a biomarker ready for clinical use? An official American Thoracic Society policy statement [^f50ffd58]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Appendix 1: Definitions — biomarker evaluation terminology is clarified, defining a biomarker as a characteristic that is objectively measured and evaluated as an indicator of biological processes or responses, and specifying analytical validity as acceptable performance in measuring or detecting biomarker characteristics. The appendix further defines clinical validity as the demonstrated association of a test result with the presence or absence of the target condition, and clinical utility as a biomarker's ability to improve clinical outcomes when measured and used as directed for its intended use. Additional key terms include context of use (what the test measures, why, and in what populations it should be used), diagnostic accuracy (agreement between the new test and the reference standard), companion diagnostic (a biomarker essential for the safe and effective use of a therapy), and confidence intervals (an interval about a point estimate that quantifies statistical uncertainty due to variability in the subject/sample selection process). The appendix also defines related concepts such as clinical performance, clinical reference standard, diagnostic marker, and lead time.

---

### Triazolam [^59f91090]. FDA (2025). Medium credibility.

WARNING: RISKS FROM CONCOMITANT USE WITH OPIOIDS; ABUSE, MISUSE, AND ADDICTION; and DEPENDENCE AND WITHDRAWAL REACTIONS

Concomitant use of benzodiazepines and opioids may result in profound sedation, respiratory depression, coma, and death. Reserve concomitant prescribing of these drugs in patients for whom alternative treatment options are inadequate. Limit dosages and durations to the minimum required. Follow patients for signs and symptoms of respiratory depression and sedation [see Warnings and Precautions (5.1), Drug Interactions (7.1)].
The use of benzodiazepines, including triazolam, exposes users to risks of abuse, misuse, and addiction, which can lead to overdose or death. Abuse and misuse of benzodiazepines commonly involve concomitant use of other medications, alcohol, and/or illicit substances, which is associated with an increased frequency of serious adverse outcomes. Before prescribing triazolam and throughout treatment, assess each patient's risk for abuse, misuse, and addiction [see Warnings and Precautions (5.2)].
The continued use of benzodiazepines, including triazolam, may lead to clinically significant physical dependence. The risks of dependence and withdrawal increase with longer treatment duration and higher daily dose. Abrupt discontinuation or rapid dosage reduction of triazolam after continued use may precipitate acute withdrawal reactions, which can be life-threatening. To reduce the risk of withdrawal reactions, use a gradual taper to discontinue triazolam or reduce the dosage [see Dosage and Administration (2.3), Warnings and Precautions (5.3)].

WARNING: RISKS FROM CONCOMITANT USE WITH OPIOIDS; ABUSE, MISUSE, AND ADDICTION; and DEPENDENCE AND WITHDRAWAL REACTIONS

See full prescribing information for complete boxed warning.

Concomitant use of benzodiazepines and opioids may result in profound sedation, respiratory depression, coma, and death. Reserve concomitant prescribing of these drugs in patients for whom alternative treatment options are inadequate. Limit dosages and durations to the minimum required. Follow patients for signs and symptoms of respiratory depression and sedation (5.1, 7.1).
The use of benzodiazepines, including triazolam, exposes users to risks of abuse, misuse, and addiction, which can lead to overdose or death. Before prescribing triazolam and throughout treatment, assess each patient's risk for abuse, misuse, and addiction (5.2).
Abrupt discontinuation or rapid dosage reduction of triazolam after continued use may precipitate acute withdrawal reactions, which can be life-threatening. To reduce the risk of withdrawal reactions, use a gradual taper to discontinue triazolam or reduce the dosage (2.3, 5.3).

---

### Effect of inaccurate b-values from imaging gradients on intravoxel incoherent motion [^7b75eaee]. Magnetic Resonance in Medicine (2025). Medium credibility.

3.1.3 Evaluation of b‐value errors onparameter estimates

Simulations were performed for a range of slice thicknesses and in‐plane resolutions to evaluate the effect of each of the imaging parameters on the b‐value separately. For varying slice thickness, the in‐plane resolution was kept constant at 2 × 2 mm², whereas the slice thickness was varied from 2 mm to 10 mm (step size 1 mm). For varying in‐plane resolution, the slice thickness was kept constant at 4 mm, whereas the in‐plane resolution was varied from 1 × 1 mm² to 4 × 4 mm² (step size 0.25 × 0.25 mm²). Additionally, simulations were performed with isotropic resolutions ranging from 1 mm to 4 mm (step size 0.25 mm).

The scenarios described in Table 1, along with Equations ((4), (5), (6))–((4), (5), (6)), were used to calculate b‐values under different assumptions. The b‐values in the set that accounted for both imaging and cross‐terms were regarded as the actual b‐values, hence used as the reference in the evaluation.

Signals were calculated using the bi‐exponential IVIM modelwith the reference b‐values, and ground truth parameter values f = 10%, D✱ = 20 μm²/ms, D = 1 μm²/ms. These values were chosen as representative for the prostateto match the in vivo measurements.

For a more general evaluation of errors, signals were calculated with three different parameter sets, each with one variable adjusted in 50 steps, whereas the others remained constant. The f was varied from 1% to 30%, with D* held constant at 20 μm 2 /ms and D at 1 μm 2 /ms. The D* varied from 5 μm²/ms to 50 μm²/ms, with f held constant at 10% and D at 1 μm²/ms. The D varied from 0.5 μm 2 /ms to 3 μm 2 /ms, with f at 10% and D* at 20 μm 2 /ms.

To estimate the bias in IVIM parameter estimates, the signals were mapped to the assumed b‐values of the scenarios in Table 1, and the IVIM model of Equation (9) was fitted using a single‐step nonlinear least‐squares algorithm with the ground truth values as the initial guess.

3.2 Application on in vivo measurements

---

### U.S. medical eligibility criteria for contraceptive use, 2024 [^cdc9ccea]. MMWR: Recommendations and Reports (2024). High credibility.

U.S. MEC classification system — eligibility criteria for initiation and continuation are "presented in tables (Appendices A, B, C, D, E, and J)", where "the first column indicates the condition", conditions are divided "into subconditions", and subsequent columns classify initiation, continuation, or both into "categories 1, 2, 3, or 4". The last column "further clarifies the numeric category" and "summarizes the evidence", the third column can include comments, and when initiation and continuation differ these are noted; when not specified, "the category is the same for initiation and continuation of use".

---

### An expectation-maximization algorithm for the analysis of allelic expression imbalance [^80a63f76]. American Journal of Human Genetics (2006). Low credibility.

A significant proportion of the variation between individuals in gene expression levels is genetic, and it is likely that these differences correlate with phenotypic differences or with risk of disease. Cis-acting polymorphisms are important in determining interindividual differences in gene expression that lead to allelic expression imbalance, which is the unequal expression of homologous alleles in individuals heterozygous for such a polymorphism. This expression imbalance can be detected using a transcribed polymorphism, and, once it is established, the next step is to identify the polymorphisms that are responsible for or predictive of allelic expression levels. We present an expectation-maximization algorithm for such analyses, providing a formal statistical framework to test whether a candidate polymorphism is associated with allelic expression differences.

---

### Cladribine (Mavenclad) [^adfd0eb6]. FDA (2025). Medium credibility.

2.2 Recommended Dosage

The recommended cumulative dosage of MAVENCLAD is 3.5 mg per kg body weight administered orally and divided into 2 yearly treatment courses (1.75 mg per kg per treatment course) (see Table 1). Each treatment course is divided into 2 treatment cycles:

Administration of First Treatment Course

First Course/First Cycle: start any time.
First Course/Second Cycle: administer 23 to 27 days after the last dose of First Course/First Cycle.

Administration of Second Treatment Course

Second Course/First Cycle: administer at least 43 weeks after the last dose of First Course/Second Cycle.
Second Course/Second Cycle: administer 23 to 27 days after the last dose of Second Course/First Cycle.

Administer the cycle dosage as 1 or 2 tablets once daily over 4 or 5 consecutive days [see How Supplied/Storage and Handling (16.1)]. Do not administer more than 2 tablets daily.

Following the administration of 2 treatment courses, do not administer additional MAVENCLAD treatment during the next 2 years. Treatment during these 2 years may further increase the risk of malignancy [see Warnings and Precautions (5.1)]. The safety and efficacy of reinitiating MAVENCLAD more than 2 years after completing 2 treatment courses has not been studied.

2.3 Missed Dose

If a dose is missed, patients should not take double or extra doses.

If a dose is not taken on the scheduled day, then the patient must take the missed dose on the following day and extend the number of days in that treatment cycle. If two consecutive doses are missed, the treatment cycle is extended by 2 days.

2.4 Administration

MAVENCLAD tablets are taken orally, with water, and swallowed whole without chewing. MAVENCLAD can be taken with or without food.

Separate administration of MAVENCLAD and any other oral drugs by at least 3 hours during the 4 to 5 day MAVENCLAD treatment cycles [see Clinical Pharmacology (12.6)].

MAVENCLAD is a cytotoxic drug. Follow applicable special handling and disposal procedures [see References (15)]. MAVENCLAD is an uncoated tablet and must be swallowed immediately once removed from the blister. If a tablet is left on a surface, or if a broken or fragmented tablet is released from the blister, the area must be thoroughly washed with water.

The patient's hands must be dry when handling the tablets and washed thoroughly afterwards. Avoid prolonged contact with skin.

---

### What are the determinants of gene expression levels and breadths in the human genome? [^c0ffce99]. Human Molecular Genetics (2012). Low credibility.

Conclusions and future directions

Our study presents a novel statistical method to analyze effects of different genomic traits on two distinct aspects of gene expression. We identified several key genomic traits whose relative effects on levels and breadths of gene expression are quantified. This information is useful in decoding regulatory principles of transcription.

Although the current study analyzed only levels and spatial distribution of gene expression, our approach provides a flexible statistical tool to evaluate the roles of additional genomic traits on the regulation of gene expression. In particular, with the improved power for dissecting complex transcriptomes, information on temporal variation of gene expression is soon to accumulate. Moreover, data on genomic traits that may be intimately related to regulation of gene expression, such as epigenetic changes, will soon become available. Our statistical framework can be easily expanded to incorporate additional data.

---

### Clinical policy: critical issues in the evaluation of adult patients presenting to the emergency department with acute blunt trauma [^591108d9]. Annals of Emergency Medicine (2024). High credibility.

Acute blunt trauma massive transfusion — Holcomb et al (2015) reports a pragmatic, phase 3, multisite, randomized clinical trial at 12 level I trauma centers in North America August 2012 and December 2013, enrolling patients with at least 1 U transfused prior to arrival or within 1 hour of admission and predicted need for a massive transfusion defined as = 10 U of RBCs within 24 h by an Assessment of Blood Consumption score of 2 or greater or physician judgment. No significant differences in mortality were detected at 24 h (12.7% in the 1:1:1 group vs 17.0% in the 1:1:2 group; difference, −4.2% [95% CI, −9.6% to 1.1%]) or at 30 d (22.4% vs 26.1%; difference, −3.7% [95% CI, −10.2% to 2.7%]), and a range of intent-to-treat P values for all possible combinations of 30-d outcomes for the 4 patients with missing values did not change these results. Exsanguination, the predominant cause of death within the first 24 h, was decreased in the 1:1:1 group (9.2%) vs the 1:1:2 group (14.6%). Transfusing patients based on an empirical ratio rather than guided solely by laboratory data is considered controversial, the trial was not designed to study this question, and after the ratio-driven intervention clinicians treated patients based on local laboratory-guided standard-of-care practice.

---

### Three learning stages and accuracy-efficiency tradeoff of restricted boltzmann machines [^efe3f1d0]. Nature Communications (2022). High credibility.

Restricted Boltzmann Machines (RBMs) offer a versatile architecture for unsupervised machine learning that can in principle approximate any target probability distribution with arbitrary accuracy. However, the RBM model is usually not directly accessible due to its computational complexity, and Markov-chain sampling is invoked to analyze the learned probability distribution. For training and eventual applications, it is thus desirable to have a sampler that is both accurate and efficient. We highlight that these two goals generally compete with each other and cannot be achieved simultaneously. More specifically, we identify and quantitatively characterize three regimes of RBM learning: independent learning, where the accuracy improves without losing efficiency; correlation learning, where higher accuracy entails lower efficiency; and degradation, where both accuracy and efficiency no longer improve or even deteriorate. These findings are based on numerical experiments and heuristic arguments.

---

### Abstract representations of events arise from mental errors in learning and memory [^7311b854]. Nature Communications (2020). High credibility.

To gain an analytic intuition for the model without referring to a particular random walk, we consider the limit of an infinitely long sequence of nodes. To begin, we consider a sequence x 1, …, x T of length T. At the end of this sequence, the counting matrix takes the form

Dividing both sides by T, the right-hand side becomes a time average, which by the ergodic theorem converges to an expectation over the transition structure in the limit T → ∞, wheredenotes an expectation over random walks in A. We note that the expectation of an identity function is simply a probability, such that, where p i is the long-run probability of node i appearing in the sequence andis the probability of randomly walking from node i to node j in Δ t + 1 steps. Putting these pieces together, we find that the expectationconverges to a concise mathematical form,

Thus far, we have not appealed to our maximum entropy form for P (Δ t). It turns out that doing so allows us to write down an analytic expression for the long-time expectationssimply in terms of the transition structure A and the inverse temperature β. Noting thatand, we have

This simple formula for the representationis the basis for all of our analytic predictions (Figs. 3 c, d and 6 b) and is closely related to notions of communicability in complex network theory.

---

### Variability in low-flow oxygen delivery by nasal cannula evaluated in neonatal and infant airway replicas [^63a777ed]. Respiratory Research (2022). Medium credibility.

Predicting FiO₂ values by a flow-weighted calculation

FiO₂ values predicted using Eqs. (1–3) were compared with the measured FiO₂ values shown in Fig. 4 a. While the predictions appeared reasonably consistent with measured data up to FiO₂ values of approximately 60%, for higher values of FiO₂ the predictions overestimated the measurements. To improve the predictions for high values of FiO₂, an empirical correction to Eq. (3) was investigated:

Fig. 4
Measured values of FiO₂ vs. predicted values by a Eq. (3) b Eq. (4) c Eq. (5). Error bars represent one standard deviation around average measured values. ρ_c is Lin's concordance correlation coefficient

where the constants A and B were determined by least squares fitting.

Figure 4 b compares FiO₂ values predicted using Eq. (4) with measured FiO₂ values. Improvement in the predictions made for high FiO₂ values is evident, while some overprediction of low measured FiO₂ values (below 35%) is introduced. Overall, the use of Eq. (4) in place of Eq. (3) resulted in a modest improvement in the concordance correlation coefficient.

Finally, to avoid overprediction of measured values at either low or high extremes of FiO₂, a piecewise approach was investigated, as described by Eq. (5):

With reference to Fig. 4 c, the use of Eq. (5) provided a further, modest improvement in the concordance correlation coefficient.

---

### Capivasertib (Truqap) [^4a7249fa]. FDA (2025). Medium credibility.

The dosage of capivasertib PO for treatment of breast cancer in female adults with PIK3CA, AKT1, or PTEN mutation (locally advanced or metastatic, HR-positive, HER2-negative, progressed on at least one endocrine-based regimen in the metastatic setting or recurrence on or within 12 months of completing adjuvant therapy) is 400 mg PO BID for 4 days, followed by 3 days off, continued until disease progression or unacceptable toxicity

---

### Phase 0 / microdosing approaches: time for mainstream application in drug development? [^98d5620b]. Nature Reviews: Drug Discovery (2020). High credibility.

Phase 0 approaches - which include microdosing - evaluate subtherapeutic exposures of new drugs in first-in-human studies known as exploratory clinical trials. Recent progress extends phase 0 benefits beyond assessment of pharmacokinetics to include understanding of mechanism of action and pharmacodynamics. Phase 0 approaches have the potential to improve preclinical candidate selection and enable safer, cheaper, quicker and more informed developmental decisions. Here, we discuss phase 0 methods and applications, highlight their advantages over traditional strategies and address concerns related to extrapolation and developmental timelines. Although challenges remain, we propose that phase 0 approaches be at least considered for application in most drug development scenarios.

---

### Six steps in quality intervention development (6SQuID) [^221654f2]. Journal of Epidemiology and Community Health (2016). Low credibility.

Conclusion

In order to improve the effectiveness of public health interventions, a systematic approach to intervention development is required, as well as rigorous evaluation. However, little practical guidance exists for public health practitioners and researchers that explains the essential stages of intervention development. We argue that this process can be broken down into six key steps: defining and understanding the problem; identifying modifiable determinants; deciding on the mechanisms of change; clarifying how these will be delivered; testing and adapting the intervention; and collecting initial evidence of effectiveness. This model imposes somewhat arbitrary cut-offs in the process of intervention development and suggests a linear progression. In practice developers often return to an earlier step in the sequence before reaching step 6, and subsequently 'definitive trials' can lead to further revisions of the intervention. However, we hope that if each of these six steps is carefully addressed in the design of interventions better use will be made of scarce public resources by avoiding the costly evaluation, or implementation, of unpromising interventions.

What is already known on this subject?

There is little practical guidance for researchers or practitioners on how best to develop public health interventions. Existing models are generally orientated towards individual behaviour change and some are highly technical and take years to implement.

What this study adds?

This paper provides a pragmatic six-step guide to develop interventions in a logical, evidence-based way to maximise likely effectiveness. If each step is carefully addressed, better use will be made of scarce public resources by avoiding the costly evaluation, or implementation, of unpromising interventions.

---

### Evaluating molecular biomarkers for the early detection of lung cancer: when is a biomarker ready for clinical use? An official American Thoracic Society policy statement [^4009df9d]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

American Thoracic Society policy statement — analytic performance characteristics and result types are defined. Limit of detection is "The lowest assay level at which the presence of the analyte is detected with reliability in repeated measurement". Linearity is "The ability to provide results directly proportional to the amount of analyte in the test sample within a given measuring range". Measurement accuracy is "The closeness of agreement between a measurement result and an accepted reference value; an aggregate of trueness and precision". Performance around the cutoff is "The measurement accuracy of an assay at biomarker levels near the threshold chosen to distinguish a positive and negative result for the intended use of the test". Precision is "The closeness of agreement of replicate test results under stipulated conditions", with repeatability defined as "Precision when replicate measurements are taken under the same conditions (within a run)" and reproducibility as "Precision when one of the conditions being varied is the laboratory for an in vitro diagnostic measurement". A qualitative result is "A biomarker result consisting of a set number of possible responses (often two)", whereas a quantitative result is "A biomarker result that is numerical in amount or level of a physical quantity", and semiquantitative denotes "Results of a test that fall into an approximate range of values". Signature refers to "Multiple variables combined to provide a single result", and medical tests are "Results of a clinical, imaging, or laboratory-based assay that are used alone or with other information to help assess a subject's health condition of interest, or target condition".

---

### Roflumilast [^fa0d7331]. FDA (2025). Medium credibility.

4 CONTRAINDICATIONS

The use of roflumilast tablet is contraindicated in the following condition:

Moderate to severe liver impairment (Child-Pugh B or C) [see Clinical Pharmacology (12.3) and Use in Specific Populations (8.6)].

Moderate to severe liver impairment (Child-Pugh B or C) (4)

---

### Quantifying the unknown impact of segmentation uncertainty on image-based simulations [^256f92f6]. Nature Communications (2021). High credibility.

Image-based simulation, the use of 3D images to calculate physical quantities, relies on image segmentation for geometry creation. However, this process introduces image segmentation uncertainty because different segmentation tools (both manual and machine-learning-based) will each produce a unique and valid segmentation. First, we demonstrate that these variations propagate into the physics simulations, compromising the resulting physics quantities. Second, we propose a general framework for rapidly quantifying segmentation uncertainty. Through the creation and sampling of segmentation uncertainty probability maps, we systematically and objectively create uncertainty distributions of the physics quantities. We show that physics quantity uncertainty distributions can follow a Normal distribution, but, in more complicated physics simulations, the resulting uncertainty distribution can be surprisingly nontrivial. We establish that bounding segmentation uncertainty can fail in these nontrivial situations. While our work does not eliminate segmentation uncertainty, it improves simulation credibility by making visible the previously unrecognized segmentation uncertainty plaguing image-based simulation.

---

### 2022 AHA / ACC key data elements and definitions for cardiovascular and noncardiovascular complications of COVID-19: a report of the American college of cardiology / American Heart Association task force on clinical data standards [^140e53ac]. Journal of the American College of Cardiology (2022). High credibility.

Appendix 7 — CCTA CAD-RADS scoring — states CAD-RADS (Reporting and Data System) score specifically for CCTA, based on degree of maximal coronary stenosis with permissible values 0, 1, 2, 3, 4, 5, or Unknown defined as: 0: 0%; 1: 1%–24%; 2: 25%–49%; 3: 50%–69%; 4: A. 70%–99% or B. Left main > 50% or 3-vessel obstructive disease; and 5: 100%.

---

### Guidance for incorporating FDA processes into the ACC / AHA clinical practice guideline methodology: a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^0253cb8c]. Journal of the American College of Cardiology (2025). High credibility.

Blood pressure thresholds and recommendations for treatment and follow-up — The algorithm categorizes normal BP, elevated BP, stage 1 hypertension, and stage 2 hypertension with associated actions and reassessment intervals. Normal BP (< 120/80 mm Hg) receives lifestyle therapy with reassess in 1 y, while elevated BP (120–129/ < 80 mm Hg) receives lifestyle therapy with reassess in 3–6 mo. Stage 1 Hypertension (130–139/80–89 mm Hg) begins with lifestyle therapy and reassess in 3–6 mo, then proceeds to the decision nodes "Stage 1 Hypertension? (130–139/80–89 mm Hg)" and "DM, CKD or increased short-term risk of CVD (10-year PREVENT risk ≥ 7.5%)?"; if yes, the pathway specifies BP-lowering medication AND reassess in 1 mo, followed by "BP goal met?" with "YES" leading to reassess in 3–6 mo and "NO" prompting to consider intensification of therapy while optimizing adherence. Stage 2 Hypertension (≥ 140/90 mm Hg) is displayed with lifestyle therapy, a left-hand directive states "Promote Lifestyle Therapy at All Thresholds", and the legend lists recommendation categories COR 1, COR 2a, COR 2b, COR 3: No Benefit, and COR 3: Harm; the figure notes it is "Based on the PREVENT calculator".

---

### A general derivation and quantification of the third law of thermodynamics [^43738480]. Nature Communications (2017). Medium credibility.

The most accepted version of the third law of thermodynamics, the unattainability principle, states that any process cannot reach absolute zero temperature in a finite number of steps and within a finite time. Here, we provide a derivation of the principle that applies to arbitrary cooling processes, even those exploiting the laws of quantum mechanics or involving an infinite-dimensional reservoir. We quantify the resources needed to cool a system to any temperature, and translate these resources into the minimal time or number of steps, by considering the notion of a thermal machine that obeys similar restrictions to universal computers. We generally find that the obtainable temperature can scale as an inverse power of the cooling time. Our results also clarify the connection between two versions of the third law (the unattainability principle and the heat theorem), and place ultimate bounds on the speed at which information can be erased.

---

### Three learning stages and accuracy-efficiency tradeoff of restricted boltzmann machines [^87550a89]. Nature Communications (2022). High credibility.

In practical applications, one does not have access to p (x), but only to a collection of samples(training and/or test data). The empirical counterpart of Δ θ for such a dataset S issee also below Eq. (4). The critical part is again the partition function Z θ. Due to the aforementioned factorization (cf. Eq. (12)), evaluating (13) remains feasible as long as the number of hidden units N is sufficiently small, even if M is large. Similarly, for small N, we can draw independent samples from, without reverting to Markov chains and Gibbs sampling: We first generate independent samplesof the hidden units, using the fact thatremains accessible for small N. Subsequently, we sample configurations of the visible units using. This scheme was utilized to obtain the model test samplesfor the N ≤ 32 examples in Fig. 4. For the examples with N > 32, the samples inwere instead generated via Gibbs sampling according to (5), using 10 parallel chains and storing every τ θ -th sample after 2 × 10⁶ burn-in steps.

The accuracy measuresandinvolve empirical distributions of coarse-grained visible-unit samples. These reduced samples are obtained by using a weighted majority rule: For a partition { L 1, …, L L } of the visible-unit indices {1, …, M } and a threshold r ∈ [0, 1], we defineFor every samplein a given multiset S, the associated coarse-grained sample iswith.

---

### Investigating suspected cancer clusters and responding to community concerns: guidelines from CDC and the council of state and territorial epidemiologists [^945666c0]. MMWR: Recommendations and Reports (2013). Medium credibility.

Cancer cluster investigation — Step 2 decision to close or proceed to Step 3 is based on multiple factors, and to interpret the standardized incidence ratio (SIR) the health agency must assess whether there are enough cases and population for statistical stability and, in general, that the population size of a typical census tract is the smallest denominator that will allow reliable results; if the numerator allows stability, agencies should consider whether the 95% CI exclude 1.0, along with environmental contaminants, population changes, and other information beyond the SIR to estimate whether cancers represent an excess and share a common etiology. A SIR of limited magnitude that is not statistically significant with lack of known contaminant association and no increasing trend justifies closing at Step 2, whereas a statistically significant SIR of great magnitude with an increasing trend and a known contaminant argues for continuing to Step 3; example thresholds indicate that an SIR of < 2.0 with CIs surrounding or overlapping 1.0 and < 10 cases might justify closing, while an SIR of > 4 with CIs not overlapping 1.0 and ≥ 10 etiologically linked cases should encourage advancing to Step 3.

---

### Deep learning suggests that gene expression is encoded in all parts of a co-evolving interacting gene regulatory structure [^5ce90dab]. Nature Communications (2020). High credibility.

Motif co-occurrence uncovers the regulatory rules of gene expression

To determine the functional meaning of the DNA motifs reconstructed from the deep learning relevance profiles (Fig. 3f), we analysed the informative power of the DNA motifs for predicting the specific gene expression levels. For this, we calculated the signal-to-noise ratio (SNR = μ / σ) of expression levels across genes that carried the identified motifs (Fig. 4a and Supplementary Fig. 23). The spread of expression levels of motif-associated genes was over 2.5-fold larger than its expected (median) expression level (Supplementary Fig. 24), resulting in a low median SNR of under 0.4 (Fig. 4a). Of the full 4 order-of-magnitude range of gene expression levels observed in the data (Fig. 1a), only 57% could be recovered with the motifs, as indicated by the average expression level of genes per associated motif (Fig. 4b and Supplementary Fig. 23b). The range of gene expression levels for the identified motifs were thus too dispersed and overlapped, indicating that the predictions made by the model (Figs. 1 f and 3b) were likely not based on single motif occurrences.

---

### Cutting edge or blunt instrument: how to decide if a stepped wedge design is right for you [^471f52f2]. BMJ Quality & Safety (2021). High credibility.

When might I consider doing a stepped wedge trial?

Research designs are shaped as much by practical constraints as by abstract schemes, and it is always a good idea to start with the constraints and work towards a design, rather than start with a design and try to fit it to constraints. These constraints will be unique to each research context, and box 1 lists some areas to think about. Still, there are some common features of settings where a stepped wedge trial might be considered as a possible design, and we now review these.

Box 1
Practical constraints on the design of a longitudinal cluster randomised trial

Are there limits on the time available to complete the evaluation, on the number of clusters, or on the number of participants (or the rate at which you can recruit participants) at each cluster? These constraints put limits on the overall scale of the evaluation, or force trade-offs between different design characteristics.
How will participants and their data be sampled in your study: as a series of cross-sectional surveys, as a continuous stream of incident cases, as a cohort followed over time, or some other way? Does the timescale divide into cycles, seasons or milestones that influence how you will sample participants and data?
Is there a limit on how many clusters can implement the intervention at the same time in the evaluation? If this is constrained by research resources (eg, if there are only enough trained research staff to implement the intervention one cluster at a time) then implementation must be staggered in some way.
If implementation is to be staggered, is there a minimum 'step length'? If the same team delivers the intervention in different clusters at different steps, then bear in mind it may take some time to get the intervention fully operational at a site, and the team will also need time to relocate from one cluster to the next.

---

### Evaluating molecular biomarkers for the early detection of lung cancer: when is a biomarker ready for clinical use? An official American Thoracic Society policy statement [^1c6ced37]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Accuracy guidance for biomarker clinical validation — formulas specify minimal accuracy thresholds tied to action after positive versus negative results. The committee "agreed that it would be helpful to provide guidance about the minimal accuracy, as assessed in the clinical validation phase, that could lead to a positive clinical impact". For tests where "a positive biomarker result leads to an action where a negative biomarker result is associated with standard of care for the population", the threshold is "sensitivity/(1 − specificity) ≥ [(1 − prevalence)/prevalence] × harm/benefit", where harm/benefit is "the ratio of the net harm of a falsely positive test result to the net benefit of a true-positive test result". For tests where "a negative test leads to an action other than standard of care for the population", the threshold is "specificity/(1 − sensitivity) ≥ [prevalence/(1 − prevalence)] × harm/benefit", where harm/benefit is "the ratio of the net harm of a falsely negative test result to the net benefit of a true-negative test result". "Sensitivity/(1 − specificity) is known as the positive likelihood ratio, and specificity/(1 − sensitivity) is 1 divided by the negative likelihood ratio", and "Prevalence refers to the percentage of cases in the intended use population". The harm/benefit term "can be articulated in one of two ways: 1/N, where in the first scenario N is the maximum number of control subjects testing positive that is tolerated to benefit one case subject testing positive".

---

### 2021 ACC / AHA key data elements and definitions for heart failure: a report of the American college of cardiology / American Heart Association task force on clinical data standards (writing committee to develop clinical data standards for heart failure) [^50c52219]. Circulation: Cardiovascular Quality and Outcomes (2021). High credibility.

Development of terminology concepts — the committee aggregated, reviewed, harmonized, and extended terms to develop a controlled, semantically interoperable, machine-computable terminology set usable across contexts; the publication is intended to serve as a common lexicon and base infrastructure for standardization and healthcare interoperability; appendixes list each data element followed by a clinical definition, with allowed responses ("permissible values") serving as acceptable answers; for data elements with multiple permissible values, bulleted lists and multiple rows define each permissible value; where possible, clinical definitions and corresponding permissible value definitions are repeated verbatim from established sources.

---

### Predicting the effect of binding molecules on the shape and mechanical properties of structured DNA assemblies [^ad84aabb]. Nature Communications (2024). High credibility.

Fig. 4
Shape variation.

a Straight 12HB structure. The twist of the 12HB structureswas revealed by the crossing sites of two parallel 6HB structures. b Twist angle. The twist angle of 12HB structures was predicted for different EtBr numbers ranging from 0 to 1600 at intervals of 200 and compared with the previous experimentsin box plots (Sample size for each plot: 10). Each box plot summarized measured data using five points (minimum, maximum, median, and the first and third quartiles). The fitted linear lines showed gradients of −0.14 and −0.17 per 100 EtBr for simulations and experiments, respectively. c, d Conformational change. The overall twist of the 12HB structure was changed by 282 EtBr corresponding to 4 μM. The predicted shapes were converted to S-type ratios and compared with previous experiments. e, f Chirality and diameter change. The chirality and diameter of the 24HB structure were changed as the EtBr number increased. The decrease in diameter by increasing the EtBr number from 0 to 800 at intervals of 200 was compared with the previous report(Sample size for each plot: 10). Each box plot in the diameter results summarized measured data using five points (minimum, maximum, median, and the first and third quartiles).

---

### Uncovering pseudotemporal trajectories with covariates from single cell and bulk expression data [^6a5ad28d]. Nature Communications (2018). Medium credibility.

Methods

We summarise the model specification and inference algorithms below. Further details are reported in Supplementary Methods.

Statistical model

We begin with an N × G data matrix Y where y ng denotes the n th entry in the g th column for n ∈ 1, …, N samples and g ∈ 1, …, G features. Such a matrix would correspond to the measurement of a dynamic molecular process that we might reasonably expect to show continuous evolution such as gene expression corresponding to a particular pathway. It is then trivial to learn a one-dimensional linear embedding that would be our "best guess" of such progression via a factor analysis model:where z n is the latent measure of progression for sample n and λ g is the factor loading for feature g which essentially describes the evolution of g along the trajectory.

However, it is conceivable that the evolution of feature g along the trajectory is not identical for all samples but is instead affected by a set of external covariates. Note that we expect such features to be "static" and should not correlate with the trajectory itself.

Introducing the N × P covariate matrix X with the entry in the n th row and p th column given by x np, we allow such measurements to perturb the factor loading matrixwhere β pg quantifies the effect of covariate p on the evolution of feature g. Despite Y being column-centred we need to reintroduce gene and covariate-specific intercepts to satisfy the model assumptions, giving a generative model of the form

Our goal is inference of z n that encodes progression along with β pg which is informative of novel interactions between continuous trajectories and external covariates. Consequently, we place a sparse Bayesian prior on β pg of the formwhere the posterior of χ pg is informative of the model's belief that β pg is non-zero. The complete generative model is therefore given bywhere τ α, τ λ, a, b, a β, b β, τ q are fixed hyperparameters and q n encodes prior information about z n if available but typically q n = 0 ∀ i in the uninformative case.

Inference

We perform co-ordinate ascent mean field variational inference (see ref.) with an approximating distribution of the form

Due to the model's conjugacy the optimal update for each parameter θ j given all other parameters θ − j can easily be computed viawhere the expectation is taken with respect to the variational density over θ − j. The precise form of the variational updates can be found in Supplementary Text.

---

### Elacestrant (Orserdu) [^25e5bc7d]. FDA (2024). Medium credibility.

The dosage of elacestrant PO for treatment of breast cancer in postmenopausal female adults with ESR1 mutation (advanced or metastatic, ER-positive, HER2-negative, progression after ≥ 1 line of endocrine therapy) is 345 mg PO daily until disease progression or unacceptable toxicity

---

### Incorporating stakeholder feedback in guidelines development for the management of abnormal cervical cancer screening tests [^b167e044]. Journal of Lower Genital Tract Disease (2020). High credibility.

Stakeholder feedback — expedited treatment for immediate risk of CIN 3+ emphasized threshold calibration and patient choice, with themes to raise the threshold (" ≥ 50%") using "lower bound of 95% CI… 50% or higher", consideration that "50% is low… closer to 70%", that "Expedited treatment should be optional", strong patient autonomy ("immediate treatment is chosen ONLY after a discussion with the patient of R/B/A"), and practicality concerns ("may cause insurance problems").

---

### ABM clinical protocol # 35: supporting breastfeeding during maternal or child hospitalization [^cdcf9666]. Breastfeeding Medicine (2021). High credibility.

Use evidence-based guidelines for milk expression and storage — frequency and volume targets advise that "A lactating mother should be expected to breastfeed or express milk as often as she would normally breastfeed", with typical patterns that "An exclusively breastfed infant < 6 months of age will typically nurse 8 to 12 times per 24 hours, whereas a 6–12-month-old infant may nurse only 3 to 4 times daily". Guidance also provides volume goals: "typically 700–980 mL a day (25–35 oz) by 10–14 days and this volume is expected to be maintained for the first 6 months of life", with "Level of evidence: 2. Strength of recommendation: B".

---

### Abstract representations of events arise from mental errors in learning and memory [^748e9b84]. Nature Communications (2020). High credibility.

Estimating parameters and making quantitative predictions

Given an observed sequence of nodes x 1, …, x t −1, and given an inverse temperature β, our model predicts the anticipation, or expectation, of the subsequent node x t to be. In order to quantitatively describe the reactions of an individual subject, we must relate the expectations a (t) to predictions about a person's reaction timesand then calculate the model parameters that best fit the reactions of an individual subject. The simplest possible prediction is given by the linear relation, where the intercept r 0 represents a person's reaction time with zero anticipation and the slope r 1 quantifies the strength with which a person's reaction times depend on their internal expectations.

In total, our predictionscontain three parameters (β, r 0, and r 1), which must be estimated from the reaction time data for each subject. Before estimating these parameters, however, we first regress out the dependencies of each subject's reaction times on the button combinations, trial number, and recency using a mixed effects model of the form 'RT~log(Trial)*Stage+Target+Recency+(1+log(Trial)*Stage+Recency|ID)', where all variables were defined in the previous section. Then, to estimate the model parameters that best describe an individual's reactions, we minimize the RMS prediction error with respect to each subject's observed reaction times, where T is the number of trials. We note that, given a choice for the inverse temperature β, the linear parameters r 0 and r 1 can be calculated analytically using standard linear regression techniques. Thus, the problem of estimating the model parameters can be restated as a one-dimensional minimization problem; that is, minimizing RMSE with respect to the inverse temperature β. To find the global minimum, we began by calculating RMSE along 100 logarithmically spaced values for β between 10⁻⁴ and 10. Then, starting at the minimum value of this search, we performed gradient descent until the gradient fell below an absolute value of 10⁻⁶. For a derivation of the gradient of the RMSE with respect to the inverse temperature β, we point the reader to the Supplementary Discussion. Finally, in addition to the gradient descent procedure described above, for each subject we also manually checked the RMSE associated with the two limits β → 0 and β → ∞. The resulting model parameters are shown in Fig. 4 a, b for random walk sequences and Fig. 4 g, h for Hamiltonian walk sequences.

---

### Interferon β-1b (Betaseron) [^6fd8e231]. FDA (2025). Medium credibility.

3 DOSAGE FORMS AND STRENGTHS

For injection: 0.3 mg lyophilized powder in a single-dose vial for reconstitution.

For injection: 0.3 mg of lyophilized powder in a single-dose vial for reconstitution (3)

---

### Three learning stages and accuracy-efficiency tradeoff of restricted boltzmann machines [^2d291dc7]. Nature Communications (2022). High credibility.

Our second example (cf. Fig. 3) is closer in spirit to traditional machine-learning applications and involves pattern recognition and artificial image generation. The target distribution p (x) generates 5 × 5 pixel images with a "hook" pattern comprised of 15 pixels (see Fig. 3 a) implanted at a random position in a background of noisy pixels that are independently activated (white, x i = 1) with probability q = 0.1 (see also Supplementary Note 2 B for more details). Periodic boundary conditions are assumed, meaning that p (x) is translationally invariant along the two image dimensions.

We also consider a one-dimensional variant of this example with only M = 4 (M = 5) visible units and an implanted "010" ("0110") pattern, cf. Fig. 3 d. In this case, we can solve the continuous-time learning dynamics (η → 0 limit of (4)) for the exact target and model distributions p (x) and, obviating artifacts caused by insufficient training data or biased gradient approximations, see also Supplementary Note 1.

Our third example (cf. Fig. 4 a–c) is a simplified digit reproduction task. Patterns of the ten digits 0 through 9 (see Fig. 4 a) are selected and inserted uniformly at random into image frames of 5 × 7 pixels, with the remaining pixels outside of the pattern again activated with probability q = 0.1 (see Supplementary Note 2 C for details). No periodic boundary conditions are imposed, i.e. the input comprises proper, ordinary images.

In our fourth example (cf. Fig. 4 d, e), we train RBMs on the MNIST dataset, which consists of 28 × 28-pixel grayscale images of handwritten digits. It comprises a training set of 60,000 and a test set of 10,000 images. We convert the grayscale images with pixel values between 0 and 255 to binary data by mapping values 0… 127 to 0 and 128… 255 to 1 (see also Supplementary Note 2 D).

---

### Optimization of b-value schemes for estimation of the diffusion coefficient and the perfusion fraction with segmented intravoxel incoherent motion model fitting [^972cca76]. Magnetic Resonance in Medicine (2019). Medium credibility.

3.2 Simulations

Two b‐value schemes with eight acquisitions were compared to study the usefulness of the optimization procedure; the optimized scheme (b opt; 2 × 0, 3 × 200, 3 × 800 s/mm²; see Table 1) was compared with b‐values linearly distributed in the available range (b lin; 1 × 0, 1 × 200, 1 × 300, 1 × 400, 1 × 500, 1 × 600, 1 × 700, 1 × 800 s/mm²).

Table 1
Optimal number of acquisitions of each optimal b‐value for different total number of acquisitions (n tot) and the proportions for the limiting case of an infinite total number of acquisitions

Note

A set of Monte Carlo simulations with accompanying analyses was used 1) to validate the use of CRLB minimization for optimization of b‐value schemes for segmented IVIM model fitting and 2) to compare the b‐value schemes in terms of estimation variability.

The simulations were performed on a grid of 20 × 20 values of D and f linearly distributed in the ranges D ∊ [0.5 1.5] µm 2 /ms and f ∊ [0.05 0.40]. The ranges were somewhat wider than what was used in the optimization to enable study of the effect of optimization on parameter estimation error also outside the parameter ranges typically found in healthy liver. For each combination of D and f, and for each of the two b‐value schemes 20 000 data series were generated using the IVIM model (Equation 1) with Rician noise at an SNR of 15. Three sets of simulations were run with D✱ = 10, 20, or 50 µm 2 /ms to study the impact of the size of D*, i.e. the size of the signal contribution from the perfusion compartment for b > 0. The parameters D and f were estimated using the segmented fitting procedure where D was first estimated using nonlinear least squares and f was estimated from extrapolation as described in the Theory section. The estimated parameters were constrained to the ranges D ∊ [0 3] µm 2 /ms, f ∊ [0 1] and S 0 ∊ [0 2 S max], where S max is the maximum measured signal value.

---

### Genomic architecture and prediction of censored time-to-event phenotypes with a Bayesian genome-wide analysis [^3599bbfa]. Nature Communications (2021). High credibility.

Parameters for prior distributions: We set priors weakly informative. Otherwise, if available, prior information could be used. To get weakly informative priors, for α prior, we set parameters to be α 0 = 0.01 and κ 0 = 0.01; for, we set parameters to be α σ = 1, β σ = 0.0001; for μ prior, we set parameterand similarly for δ q we set parameter. The choice of prior parameters for π, p L is a vector of ones.

1 Initialise for every i: ε i = y i − μ

2 for i t e r a t i o n ← 1 to I do

3 Add the previous effect to the residual: ε i ← ε i + μ o l d

4 Sample μ using ARS;

5 Subtract the new effect from the residual: ε i ← ε i − μ

6 Shuffle;

7 foreach covariate q do

8

9 Sample δ q using ARS

10 ε i ← ε i − z i q δ q

11 Sample α using ARS;

12 Shuffle;

13 Calculate exponentiated residuals:;

14 foreach marker j do

15 if then

16 Calculate;

17 if then

18

19

20 Calculate;

21 Sample mixture indicator γ j;

22 if γ j > 0 then

23 sample β j from the γ j th conditional distribution using ARS;

24 ε i ← ε i − x i j β j;

25

26 if γ j = 0 then

27 set β j = 0;

28 Sample π;

29 Sample.

---

### Learning meaningful representations of protein sequences [^6646110e]. Nature Communications (2022). High credibility.

How we choose to represent our data has a fundamental impact on our ability to subsequently extract information from them. Machine learning promises to automatically determine efficient representations from large unstructured datasets, such as those arising in biology. However, empirical evidence suggests that seemingly minor changes to these machine learning models yield drastically different data representations that result in different biological interpretations of data. This begs the question of what even constitutes the most meaningful representation. Here, we approach this question for representations of protein sequences, which have received considerable attention in the recent literature. We explore two key contexts in which representations naturally arise: transfer learning and interpretable learning. In the first context, we demonstrate that several contemporary practices yield suboptimal performance, and in the latter we demonstrate that taking representation geometry into account significantly improves interpretability and lets the models reveal biological information that is otherwise obscured.

---

### 2022 ACC expert consensus decision pathway on the evaluation and disposition of acute chest pain in the Emergency department: a report of the American college of cardiology solution set oversight committee [^14476b4c]. Journal of the American College of Cardiology (2022). High credibility.

High-sensitivity cardiac troponin assay thresholds for the modified ESC 0/1-hour CDP (all values ng/L) are listed by assay for LoQ, A, B, C, and D respectively: Roche Elecsys hs-cTnT — 6, 12, 3, 52, 5; Abbott Architect hs-cTnI — 4, 5, 2, 52, 6; Beckman Coulter Access hs-cTnI — 3, 5, 4, 50, 15; Siemens ADVIA Centaur hs-cTnI — 3, 6, 3, 120, 12; Siemens Atellica hs-cTnI — 3, 6, 3, 120, 12; Siemens Dimension Vista hs-cTnI — 3, 5, 2, 107, 19.

---

### 2019 methodology for creating expert consensus decision pathways: a report of the American college of cardiology [^3c363d65]. Journal of the American College of Cardiology (2019). High credibility.

Expert Consensus Decision Pathways methodology — purpose and scope are described as complementing Clinical Practice Guidelines by providing "the how", including practical guidance that transforms recommendations into clinically actionable information, clinical guidance to make sense of quickly-evolving evidence, and dissemination of new information not yet incorporated into guidelines. Expert Consensus Decision Pathways are situated within the American College of Cardiology's "actionable knowledge" strategy and are an important component of "solution sets", articulating concise and focused practical guidance around high-value clinical topics. Of note, the goal of an Expert Consensus Decision Pathway is not to provide a single correct answer, but to encourage clinicians to ask certain questions and consider key factors as they come to their own decision on a treatment plan to be recommended and discussed with their patients. Topics have spanned lipid disorders, antithrombotic therapy and bleeding, valvular heart disease, heart failure, coronary artery disease, cardiovascular risk reduction in type 2 diabetes, and tobacco cessation, with scope widening to include guidance on management of conduction disturbances for transcatheter aortic valve replacement (TAVR) and same-day discharge after percutaneous coronary intervention (PCI), as well as other topics.

---

### Reuse of public genome-wide gene expression data [^51027d20]. Nature Reviews: Genetics (2013). Medium credibility.

Our understanding of gene expression has changed dramatically over the past decade, largely catalysed by technological developments. High-throughput experiments - microarrays and next-generation sequencing - have generated large amounts of genome-wide gene expression data that are collected in public archives. Added-value databases process, analyse and annotate these data further to make them accessible to every biologist. In this Review, we discuss the utility of the gene expression data that are in the public domain and how researchers are making use of these data. Reuse of public data can be very powerful, but there are many obstacles in data preparation and analysis and in the interpretation of the results. We will discuss these challenges and provide recommendations that we believe can improve the utility of such data.

---

### Laboratory perspectives in the development of polygenic risk scores for disease: a points to consider statement of the American College of Medical Genetics and genomics (ACMG) [^50389303]. Genetics in Medicine (2023). High credibility.

Bioinformatic and statistical algorithms for PRS — early models "focused on sites whose disease association passed a threshold for genome-wide significance, often limiting the PRS inputs to few sites, which may be more easily and directly genotyped", whereas more recent models "incorporate information from hundreds of thousands to millions of sites across the genome", and "require either genome sequence or imputation to determine variant status".

---

### Bayesian estimation of gene constraint from an evolutionary model with gene features [^f841d9b8]. Nature Genetics (2024). High credibility.

Measures of selective constraint on genes have been used for many applications, including clinical interpretation of rare coding variants, disease gene discovery and studies of genome evolution. However, widely used metrics are severely underpowered at detecting constraints for the shortest ~25% of genes, potentially causing important pathogenic mutations to be overlooked. Here we developed a framework combining a population genetics model with machine learning on gene features to enable accurate inference of an interpretable constraint metric, s het. Our estimates outperform existing metrics for prioritizing genes important for cell essentiality, human disease and other phenotypes, especially for short genes. Our estimates of selective constraint should have wide utility for characterizing genes relevant to human disease. Finally, our inference framework, GeneBayes, provides a flexible platform that can improve the estimation of many gene-level properties, such as rare variant burden or gene expression differences.

---

### Gene-expression measurement: variance-modeling considerations for robust data analysis [^4e2bc0ca]. Nature Immunology (2012). Medium credibility.

System-wide measurements of gene expression by DNA microarray and, more recently, RNA-sequencing strategies have become de facto tools of modern biology and have led to deep understanding of biological mechanisms and pathways. However, analyses of the measurements have often ignored statistically robust methods that account for variance, resulting in misleading biological interpretations.

---

### Global quantification of mammalian gene expression control [^6eab7aec]. Nature (2011). Excellent credibility.

Gene expression is a multistep process that involves the transcription, translation and turnover of messenger RNAs and proteins. Although it is one of the most fundamental processes of life, the entire cascade has never been quantified on a genome-wide scale. Here we simultaneously measured absolute mRNA and protein abundance and turnover by parallel metabolic pulse labelling for more than 5,000 genes in mammalian cells. Whereas mRNA and protein levels correlated better than previously thought, corresponding half-lives showed no correlation. Using a quantitative model we have obtained the first genome-scale prediction of synthesis rates of mRNAs and proteins. We find that the cellular abundance of proteins is predominantly controlled at the level of translation. Genes with similar combinations of mRNA and protein stability shared functional properties, indicating that half-lives evolved under energetic and dynamic constraints. Quantitative information about all stages of gene expression provides a rich resource and helps to provide a greater understanding of the underlying design principles.

---

### Gene isoforms as expression-based biomarkers predictive of drug response in vitro [^22039b09]. Nature Communications (2017). Medium credibility.

Fig. 3
Comparison of significance and predictive value for multiple data types. The comparison is limited to 1600 genes for which mutations, copy number variation, and expression data were available. a Number of significant (FDR < 5%) biomarkers at the level of gene expression, isoform expression, mutation, and amplification. b Distribution of concordance indices (predictive value) of the significant biomarkers with respect to data type

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^2e5c822c]. Nature Communications (2025). High credibility.

Structural compatibility

We find that for a given graph structure, not every classification problem (i.e. an assignment of input edges, output nodes, and sets of input data) can be solved. For example, we may assign an input F a along edge i ← j and assign node j to be large when F a > 0; this will be very difficult to achieve because all of the spanning trees into node j which involve edge j ← i will have an exponentially small contribution from the input force, making the input feature vector χ (j, F) in Equation (3) small; this cannot be helped no matter how we learn the parameters θ. If we flipped the sign of the input F a in the assignment, then the problem may become solvable. We refer to this mismatch between input force assignment and achievable output node assignment as structural incompatibility. For a fixed set of hyperparameters (i.e. number of nodes, edges, input edges, output nodes, etc.), some problems will be structurally compatible and some will not be. This issue is thus separate from more intrinsic properties of computational expressivity that depend on hyperparameters like M and D, but it implies that we cannot define classification problems completely arbitrarily. We leave to future work a dedicated study of what determines structural compatibility, which may be posed as determining a feasible region as in constrained optimization. Throughout this paper, we bypass this issue to focus on other constraints on expressivity, but we note that structural compatibility represents one limitation on using chemical reaction networks as classifiers. We also note that optimizing the mutual information does not involve specifying output nodes by hand and thus helps to bypass issues of structural compatibility.

Training

We train Markov networks for classification tasks using an approximation to gradient descent, as discussed in refs. This method requires a variational quantitywhich is minimized by the steady-state distribution. Two considerations lead to equivalent choices of: one is thatshould be the KL divergence, which has been shown to act as a Lyapunov function for the evolution of p (t) and is minimized to zero at steady state. The other consideration is based on the observation that, from Equation (3), withis a Boltzmann-like distribution which must hence maximize a constrained entropy functional. We show in the Supplementary Information that both of these considerations lead to equivalent update rules which require numerically estimating the vectors ∂ θ π i during training.

---

### Defining the time-limited trial for patients with critical illness: an official American Thoracic Society workshop report [^cf1ee11e]. Annals of the American Thoracic Society (2024). High credibility.

Time-limited trial in critical care — essential elements and phased process were derived through a Delphi process: investigators "identified 18 potential steps" and found first-round consensus that "11 of these were essential elements"; during the second round, they "identified seven additional essential elements". The committee then "combined 4 related items into two steps (to reach 16 essential elements)" and "organized steps into four phases of care: consider, plan, support, and reassess". Additional steps "may be helpful in some cases, but not necessary for all trials", and involvement of other disciplines is "highly dependent on a patient's specific situation and on the available hospital resources".

---

### Adagrasib (Krazati) [^96b49ceb]. FDA (2024). Medium credibility.

The dosage of adagrasib PO for treatment of non-small cell lung cancer in adults with KRAS G12C mutation (locally advanced or metastatic, received at least 1 prior systemic therapy) is 600 mg PO BID until disease progression or unacceptable toxicity

---

### Diffusion-weighted imaging of prostate cancer: effect of b-value distribution on repeatability and cancer characterization [^a4a00ca4]. Magnetic Resonance Imaging (2015). Low credibility.

Purpose

To evaluate the effect of b-value distribution on the repeatability and Gleason score (GS) prediction of prostate cancer (PCa).

Methods

Fifty PCa patients underwent two repeated 3T diffusion-weighted imaging (DWI) examinations using 12 b values in the range from 0 to 2000s/mm(2) and diffusion time of 20.3ms. Mean signal intensities of regions of interest, placed in PCa using whole mount prostatectomy sections as the reference, were fitted using monoexponential, kurtosis, stretched exponential, and biexponential models. In total, 4083 different b-value combinations consisting of 2 to 12 b values were evaluated. Repeatability was assessed by intraclass correlation coefficient, ICC(3,1), and coefficient of repeatability (CoR). Areas under receiver operating characteristic curve (AUCs) for PCa characterization were estimated while the correlation of the fitted values with GS groups (3+3, 3+4, > 3+4) was evaluated by using the Spearman correlation coefficient (ρ).

Results

The parameters of monoexponential, kurtosis, and stretched exponential models estimated using only 4–5, 5–7, 5–7 b values, respectively, had similar ICC(3,1), CoR, AUC, and ρ values as the parameters estimated using all 12 b values. Optimized b-value distributions demonstrated improved ICC(3,1) and CoR values but failed to improve AUC and ρ values. The parameters of biexponential model demonstrated the worst repeatability and diagnostic performance.

Conclusion

B-value distribution influences mainly the repeatability of DWI-derived parameters rather than the diagnostic performance.

---

### A versatile computational algorithm for time-series data analysis and machine-learning models [^263e8086]. NPJ Parkinson's Disease (2021). Medium credibility.

Now consider the 3x3 neighborhood around each vector element being constructed from.represents our one-dimensional time-series vector that can be used for time-delay embedding with delay t and embedding dimension m to reconstruct the phase-space trajectory. However, instead of considering the Euclidean norm between data points:we will consider vectors V i and V j in relation to their local 3x3 neighborhood such that:where it can be seen that≠even when = (Fig. 1a). It follows then, that directional and curvature information can be captured by different inequality patterning around the 3x3 neighborhood when computed for allby constructing a new matrix that represents an 8-bit binary code for each point-pair's local neighborhood:where g 0 representsand g n = { g 1, …, g 8 } are its eight connected neighbors. Each neighbor which is larger or equal to g 0 is set to 1, while each neighbor that is smaller than g 0 is set to 0. A binary code is thus created by moving around the central point g₀ (here counterclockwise) where a single integer value is calculated based on the sum of the binary code elements (0 or 1) multiplied by the eight 2ᵖ positional weights (increments of powers of 2: 2⁰, 2¹, 2², 2³, 2⁴, 2⁵, 2⁶, 2⁷) starting with the preceding points (i.e.). This represents 8-bit binary coding where there are 2⁸ (256) different possible integer values, ranging from 0 to 255. This newly created matrix now allows for the identification of graded changes in phase-space trajectories that can be used to identify recurring sets of local topological features embedded within the dynamic signal (LoTRA plot; see Fig. 1b, c for example).

---

### Clinical genetic counseling and translation considerations for polygenic scores in personalized risk assessments: a practice resource from the National Society of Genetic Counselors [^28d8464d]. Journal of Genetic Counseling (2023). High credibility.

Polygenic score (PGS) development and validation for clinical use — Ordering providers should have a basic understanding of PGS test development and downstream testing restrictions in order to support appropriate test utilization, and there are no best practices or standards for developing a PGS, such that PGS for the same condition are often not the same. Regulatory requirements for genetic tests to make it to market often require demonstrated analytic and clinical validity, and for PGS and integrated PGS risk models analytic and clinical validity may be restricted to the model parameters used during development. From genome-wide association studies (GWAS) to PGS, the basic steps are to (1) conduct a GWAS to identify variants in a training/source data set, (2) select disease-associated variants that meet prespecified thresholds and combine these to create a PGS, and (3) validate the PGS using an independent dataset; GWAS evaluates associations using "common" variants with "common" population prevalence (≥ 1% frequency in a population).

---

### Red blood cell transfusion: 2023 AABB international guidelines [^ebbef0ce]. JAMA (2023). Excellent credibility.

Rationale for recommendations for adults states, 'The panel recommends that RBC transfusions be administered using a restrictive transfusion strategy of 7 g/dL for most hemodynamically stable adults (strong recommendation, high certainty evidence)', and adds that 'there is no strong clinical or biological basis for expecting different effects between 7 and 8 g/dL'.

---

### Standards and guidelines for the interpretation of sequence variants: a joint consensus recommendation of the American College of Medical Genetics and genomics and the Association for Molecular Pathology [^2bafc813]. Genetics in Medicine (2015). Medium credibility.

Complex trait risk alleles — reporting guidance and scope: In common diseases, risk alleles can explain only up to 10% of population variance, and this recommendation does not address the interpretation and reporting of complex trait alleles; when such alleles are encountered during sequencing of Mendelian genes, the terms "pathogenic" and "likely pathogenic" are not appropriate, and an interim solution is to report these variants as "risk alleles" or in an "other reportable" category, with evidence descriptors such as "established risk allele", "likely risk allele", or "uncertain risk allele".

---

### Amphotericin b (Ambisome) [^178b841e]. FDA (2025). Medium credibility.

The dosage of amphotericin B liposomal IV for treatment of fungal infections in adults with febrile neutropenia is 3 mg/kg IV q24h, administered over 120 minutes

---

### Clinical bioinformatician body of knowledge-bioinformatics and software core: a report of the Association for Molecular Pathology [^6c3195b2]. The Journal of Molecular Diagnostics (2025). High credibility.

Model training, testing, and validation considerations — robust development requires training data that "captures the full range of expected variability… with the necessary clinical sensitivity and specificity for application to future unseen data", with smaller data sets only supporting "simpler models with limited degrees of freedom". Supervised approaches require that "labeled data are required if supervised machine learning models are used". Development involves "multiple rounds of model training and testing" to select the best model, and it is "important to test trained models on independent, unseen data sets to prevent data leakage", while techniques such as "cross-validation and examination of learning curves" help detect overfitting and guide data needs. Training should "minimize common prediction errors, such as bias and variance", and once optimization is satisfactory "the model parameters are locked, and model validation can be performed to measure model accuracy, bias, and robustness", with clinical validation of the new model to "follow the same standards as other bioinformatics software/pipelines".

---

### What are the determinants of gene expression levels and breadths in the human genome? [^1b7aa0f8]. Human Molecular Genetics (2012). Low credibility.

In complex organisms, different tissues express different genes, which ultimately shape the function and phenotype of each tissue. An important goal of modern biology is to understand how some genes are turned on and off in specific tissues and how the numbers of different gene expression products are determined. These aspects are named 'expression breadth' (or 'tissue specificity') and 'expression level', respectively. Here, we show that we can predict substantial amount of variation in levels and breadths of gene expression using genomic information of each gene. Interestingly, many genomic traits are correlated with both aspects of gene expression in similar directions, suggesting shared molecular pathways. However, to elucidate distinctive molecular mechanisms governing gene expression levels and breadths, we need to identify the relative significance of each genomic trait on these two aspects of gene expression. To this end, we developed a novel multivariate multiple regression method. Using this new method, we show that gene compactness (in particular, the mean size of exons), codon usage bias and non-synonymous rates have a stronger influence on expression levels compared with their effects on expression breadths. In contrast, the propensity of promoter DNA methylation is a stronger indicator of expression breadths than of expression levels. Interestingly, intron DNA methylation exhibits an opposite pattern to the promoter DNA methylation in the human genome, suggesting that DNA methylation may play multiple roles depending upon its genomic targets. Furthermore, synonymous rates have stronger associations with expression breadths than with expression levels in the human genome. These findings provide clues toward distinctive molecular mechanisms regulating different aspects of gene expression.

---

### New designs for phase 2 clinical trials [^a90544dc]. Blood (2003). Low credibility.

Conventional phase 2 clinical trials are typically single-arm experiments, with outcome characterized by one binary "response" variable. Clinical investigators are poorly served by such conventional methodology. We contend that phase 2 trials are inherently comparative, with the results of the comparison determining whether to conduct a subsequent phase 3 trial. When different treatments are studied in separate single-arm trials, actual differences between response rates associated with the treatments, "treatment effects", are confounded with differences between the trials, "trial effects". Thus, it is impossible to estimate either effect separately. Consequently, when the results of separate single-arm trials of different treatments are compared, an apparent treatment difference may be due to a trial effect. Conversely, the apparent absence of a treatment effect may be due to an actual treatment effect being cancelled out by a trial effect. Because selection involves comparison, single-arm phase 2 trials thus fail to provide a reliable means for selecting which therapies to investigate in phase 3. Moreover, reducing complex clinical phenomena, including both adverse and desirable events, to a single outcome wastes important information. Consequently, conventional phase 2 designs are inefficient and unreliable. Given the limited number of patients available for phase 2 trials and the increasing number of new therapies that must be evaluated, it is critically important to conduct these trials efficiently. These concerns motivated the development of a general paradigm for randomized selection trials evaluating several therapies based on multiple outcomes. Three illustrative applications of trials using this approach are presented.

---

### Incorporating functional priors improves polygenic prediction accuracy in UK biobank and 23andMe data sets [^c756e290]. Nature Communications (2021). High credibility.

LDpred

The LDpred method is an extension of LDpred-inf that uses a point-normal prior to estimating posterior mean effect sizes via Markov Chain Monte Carlo (MCMC). It assumes a Gaussian mixture prior:with probability p, and β i ~ 0 with probability 1 − p, where p is the proportion of causal SNPs. The method is optimized by considering different values of p (1E−4, 3E−4, 1E−3, 3E−3, 0.01,0.03,0.1,0.3,1); in the special case where 100% of SNPs are assumed to be causal, LDpred is roughly equivalent to LDpred-inf. We excluded SNPs from long-range LD regions (reported in ref.), as our secondary analyses showed that including these regions were suboptimal, consistent with ref.

---

### CLINICAL PRACTICE GUIDELINE METHODOLOGY: methodology [^1ca4ad94]. Obstetrics and Gynecology (2021). High credibility.

ACOG clinical practice guideline development — topic selection, timeline, team composition, and grading indicate that each committee selects 2–3 new topics annually and the development process typically spans 24–36 months from initial topic selection to final publication. The process is led by a document writing team composed of committee members and subject matter experts, and Two committee members are selected to serve on the document writing team with the subject matter expert to develop the outline, synthesize evidence, propose recommendations, and author the manuscript. ACOG's guideline development process incorporates a modified GRADE framework, with recommendations classified by quality of evidence and strength of recommendation, and ungraded Good Practice Statements may be incorporated when evidence is extremely limited or nonexistent.

---

### What are the determinants of gene expression levels and breadths in the human genome? [^644d4e6a]. Human Molecular Genetics (2012). Low credibility.

RESULTS

Strong correlations between expression levels and breadths

We chose to analyze the Novartis Gene Atlas expression data from human and mouse transcriptomes because these data represent comprehensive information on genome-wide gene expression from the largest number of tissues currently available. Importantly, because they are obtained from human and mouse in a similar manner, we can compare results from these two taxa and infer potentially lineage-specific biological differences.

We will refer to the number of tissues in which a gene is expressed as the 'expression breadth'. The expression levels of each gene averaged over all tissues are referred to as the 'expression level'. In both human and mouse, expression breadths are extremely strongly correlated with expression levels [Pearson's correlation r = 0.81 and 0.82 for human and mouse, respectively, P < 2.2 × 10–16 for both comparisons]. An alternative measure of tissue specific patterns of gene expression is the 'tissue specificity index', which incorporates information on the maximum expression level in different tissues (see Materials and Methods). The tissue specificity index, while negatively correlated with the expression breadth (Pearson's correlation r = −0.39 and −0.40, for human and mouse, P < 2.2 × 10 −16 for both comparisons), behaves differently from the expression breadth: for example, the tissue specificity index exhibits low correlation with the expression level (r = −0.03, P = 0.003 and r = −0.13, P < 2.2 × 10 −16 for human and mouse, respectively). However, it is strongly correlated with the maximum expression level of a gene among the tissues examined (r = 0.53 and 0.61 for human and mouse, respectively, P < 2.2 × 10 −16 for both comparisons). In, we provide results of the same analyses using the tissue specificity index and the maximum level of gene expression. The major findings are highly similar to those presented in the main text.

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^31ea71a0]. Nature Communications (2025). High credibility.

Many biological decision-making tasks require classifying high-dimensional chemical states. The biophysical and computational mechanisms that enable classification remain enigmatic. In this work, using Markov jump processes as an abstraction of general biochemical networks, we reveal several unanticipated and universal limitations on the classification ability of generic biophysical processes. These limits arise from a fundamental non-equilibrium thermodynamic constraint that we have derived. Importantly, we show that these limitations can be overcome using common biochemical mechanisms that we term input multiplicity, examples of which include enzymes acting on multiple targets. Analogous to how increasing depth enhances the expressivity and classification ability of neural networks, our work demonstrates how tuning input multiplicity can potentially enable an exponential increase in a biological system's ability to classify and process information.

---

### Deep learning suggests that gene expression is encoded in all parts of a co-evolving interacting gene regulatory structure [^2dbc8031]. Nature Communications (2020). High credibility.

To test if the observed dynamic range of gene expression levels is encoded in the DNA, we extracted the DNA sequences of the regulatory and coding regions of all the genes with a relative standard deviation of expression variation less than 1 (Fig. 1a, b: 4238 genes with RSD < 1). A total of 2150 bp of regulatory sequences. 64 codon frequencies from coding regionsand additional 8 mRNA stability variables, all known to be important for expression regulation, were used for prediction of mRNA levels (Fig. 1d and Supplementary Fig. 1, "Methods"). Using the DNA sequence information as input, we built a regression model based on deep convolutional neural networks (CNNs), capable of identifying functional DNA motifs across the sequence –, and trained the model to predict the median gene expression levels (Fig. 1e, "Methods"). The median value was a good estimator of the expression differences between the genes, as it was strongly correlated with the first principal component of the entire expression matrix (Pearson's r = 0.990, p -value < 2e-16, Supplementary Fig. 3). To avoid potential technical biases related to read-based sequencing, mRNA levels were corrected for gene length bias (Supplementary Fig. 4, "Methods"). Overall, a total of 3433 gene sequences were used for training the model, 381 for tuning the model hyperparameters and 424 for testing. After optimizing the model ("Methods"), its predictive performance on the held out test set (R²_test = 0.822, p -value < 1e-16, Fig. 1f and Supplementary Table 3) demonstrated that the DNA encodes the majority of the information about mRNA expression levels.