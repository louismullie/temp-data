# Medical Question & Answer

**Sample ID**: b3fed380-6953-9b07-7e91-89126be3c285
**Dataset Index**: 197756

---

## Question

1. When $k$ takes any real number, the vertex of the parabola $y = \frac{4}{5}(x-k)^{2}+$ $k^{2}$ lies on the curve ().
(A) $y = x^{2}$
(B) $y = -x^{2}$
(C) $y = x^{2}(x > 0)$
(D) $y = -x^{2}(x > 0)$
(2004, National Junior High School Mathematics Competition Hubei Province Preliminary Contest)

---

## Answer

> Let's see… What do we have here? The user is asking which curve the vertex of the parabola (y = \\frac{4}{5}(x - k)^2 + k^2) lies on as (k) varies over all real numbers. Let's break this down step-by-step. First, I need to think about identifying the vertex coordinates from the vertex form of a parabola. Then, I should verify how the vertex coordinates depend on the parameter (k). Next, I will eliminate the parameter to find the relationship between the vertex's (x) and (y) coordinates. Finally, I will match that relationship to the given options and confirm the domain constraints, if any, to select the correct answer.

> Let me first confirm the vertex form and extract the vertex. The equation is already in vertex form (y = a(x - h)^2 + k), where the vertex is at ((h, k)). Here, (h = k) and the vertical shift is (k^2), so the vertex coordinates are ((k, k^2)). Wait, let me verify that I'm not mixing up the parameter (k) with the vertex-form constant; yes, in this problem the parameter is also labeled (k), so the vertex is indeed ((k, k^2)) as (k) varies over the reals [^notfound].

> Next, I should review how to eliminate the parameter to find the locus. If the vertex is ((x, y) = (k, k^2)), then (x = k) and (y = k^2). Substituting (k = x) into the (y) equation gives (y = x^2). Hold on, I should verify that this substitution is valid for all real (k); since (k) can be any real number, (x) can be any real number, so the locus is the entire parabola (y = x^2) without restriction [^notfound].

> Now, I will examine the options and match the locus. Option A is (y = x^2), which matches exactly what I derived. Options B and D involve (-x^2), which would flip the parabola upside down, inconsistent with (y = k^2 \\geq 0). Option C restricts to (x > 0), but the vertex can occur at any real (x) because (k) is any real number, so restricting to (x > 0) would omit half of the possible vertices. Therefore, the correct choice is A [^notfound].

> But wait, what if the question intends a restricted domain for some reason? Let me reconsider the wording: "when (k) takes any real number", which implies the vertex traces the full curve without artificial restrictions. Thus, the most complete and accurate answer remains (y = x^2) across all real (x), not a half-parabola or an inverted parabola [^notfound].

---

The vertex of the parabola (y = \\frac{4}{5}(x - k)^2 + k^2) is at ((k, k^2)). As (k) varies over all real numbers, the vertex traces the curve (y = x^2). Therefore, the correct answer is **(A) (y = x^2)**.

---

## Step-by-step solution

### Identify the vertex of the parabola

The given equation is in vertex form:

[
y = a(x - h)^2 + k
]

where ((h, k)) is the vertex. Comparing with (y = \\frac{4}{5}(x - k)^2 + k^2), we see:

- (h = k)
- (k = k^2)

Thus, the vertex is at ((k, k^2)).

---

### Determine the locus of the vertex

As (k) varies over all real numbers, the vertex coordinates ((x, y)) satisfy:

[
x = k \\quad \\text{and} \\quad y = k^2
]

Substituting (k = x) into the second equation gives:

[
y = x^2
]

This is the equation of a parabola opening upward with vertex at the origin.

---

### Verify the options

- **Option A**: (y = x^2) — matches the derived locus.
- **Option B**: (y = -x^2) — opens downward, incorrect.
- **Option C**: (y = x^2) with (x > 0) — restricts to the right half, but (k) can be any real number, so the vertex can lie on the left half as well.
- **Option D**: (y = -x^2) with (x > 0) — both wrong direction and wrong domain.

---

### Conclusion

The vertex of the parabola lies on the curve **(y = x^2)** for all real (k), so the correct answer is **(A)**.

---

## References

### Scott's parabola: a cautionary tale in medical innovation [^9e1aeecd]. Plastic and Reconstructive Surgery (2025). Medium credibility.

Summary

Scott's Parabola, named after British gynecologist J.W. Scott, describes the phenomenon in which new medical treatments, techniques, devices, or innovations are prematurely adopted into standard care, only to be abandoned after evidence of inefficacy or patient harm inevitably comes to light. After noticing this life cycle pattern in multiple medical innovations, Scott published his parabola not only to shed light on this phenomenon, but also to train physicians to recognize the cycle in their own practice. Innovations that follow Scott's Parabola experience an initial "spark" characterized by early success and exaggerated anecdotal support, launching them onto the parabolic path. The treatment then undergoes a "rise" in popularity as it is hailed as a silver bullet, reaches its "vertex" when usage peaks and extends to the average consumer, and ultimately "drops" dramatically as harmful or ineffective outcomes emerge, often after many patients have already been adversely affected. Physicians and healthcare providers must not only be aware of this pattern but should also equip themselves with tools to avoid becoming contributors to it. Individual retrospection, critical review of clinical research, intentional patient-physician communication, and robust post-market surveillance all play important roles in preventing the realization of Scott's Parabola in today's healthcare.

---

### Regional changes in the fetal telencephalic wall diffusion metrics across late second and third trimesters [^02101e5c]. Human Brain Mapping (2025). Medium credibility.

2.5 Statistical Analysis

We calculated median FA and MD values for each subject in each ROI. To assess changes in FA and MD throughout gestation, we used mixed‐effects models with subject ID as a random effect to account for repeated measurements due to ROIs being present in both hemispheres. We evaluated two different models for each combination of ROI and diffusion metric based on previously described trajectories of DTI‐derived metrics (Machado‐Rivas et al.): a quadratic model: whereis the gestational age in weeks, is the intercept, is the week coefficient, andis the coefficient of the quadratic week term; and an exponential decay model: whereis the horizontal asymptote on the right side, is the response when, andis the natural logarithm of the rate constant. After fitting the two models, Akaike Information Criteria (AIC) was used for model selection.

We depicted changes over gestation with different estimates depending on the model type. For structures that followed an exponential decay model, we used the decay constant to portray the speed with which changes occur. For structures that followed a parabolic trend, we defined a turning point — which corresponds to the vertex of the parabola — to assess the time in gestation when a change in trend occurred.

We used a wild bootstrapping scheme to calculate accurate confidence intervals for the decay constants and vertices of the parabolas (Dikta and Scheer). Our process involved fitting a quadratic or exponential model, and then computing a newbased on:(whereis the predicted value, is the estimated residual andis a random variable which comes from a Rademacher distribution) for each bootstrap sample. We utilized the newto fit a new model and estimate the vertex of the parabola or decay constant. This process was repeated 10,000 times, and the resulting distribution of estimates was used to calculate 95% confidence intervals.

In order to analyze the rate of change for each curve at any given point and be able to compare them between other cortical or subcortical parcellations and the two diffusion metrics, we applied the previous models to z‐normalized data and subsequently determined their derivatives.

---

### Using both qualitative and quantitative data in parameter identification for systems biology models [^fc71653e]. Nature Communications (2018). Medium credibility.

Results

An illustration of the potential value of qualitative data

To demonstrate the potential value of qualitative data, we consider a simple case of solving for the coefficients of polynomial functions.

We consider two polynomial functions: y 1 = ax 2 − bx + c and y 2 = dx + e. Suppose we want to solve for the coefficients a, b, c, d, and e, which we will take to be positive. As the ground truth coefficients to be determined, we choose (a, b, c, d, e) = (0.5, 3, 5, 1, 1.5).

Suppose that a limited amount of quantitative information is available. Namely, it is known that the parabola y 1 contains the points (2, 1) and (8, 13), and the line y 2 contains the point (3.5,5). This is not enough information to solve for any of the coefficients because three points are required to specify a parabola, and two points are required to specify a line (Fig. 1a).

Fig. 1
A simple illustration using polynomial functions. We use qualitative and quantitative information to determine the unknown coefficients. a Visualization of the problem. We seek to find the coefficients of equations for a parabola and a line, with the ground truth shown (blue solid curves). Two points on the parabola and one point on the line are known (black dots). These three points are consistent with infinitely many possible solutions (e.g. orange dashed curves). Qualitative information (colored circles, x -axis) specifies whether the parabola is above (+) or below (−) the line. This information limits the possible values of intersection points x 1 and x 2 to the green shaded segments of the x -axis. b Bounds on coefficient values as a function of the number of qualitative points known. Shaded areas indicate the range of possible values of each coefficient

---

### The statistical geometry of material loops in turbulence [^5437b3b2]. Nature Communications (2022). High credibility.

As discussed in ref. generically a line element will align with the u 1 -direction and become stretched exponentially with(whose mean asymptotically scales like e β t). The surrounding curve will be forced into the u 1 – u 2 plane by compression in the u 3 -direction. The dominant stretching in the u 1 -direction locally decreases curvature. However, an exception to this generic setting occurs at a finite number of points along the loop when the initial material line lies perpendicular to v 1 (see Fig. 4). In this case, the line element cannot align with u 1 and will align with u 2 instead. The surrounding curve, however, still experiences the stretching in the u 1 -direction. This essentially magnifies the local structure of the curve, which will generically result in a parabolic shape, as illustrated in Fig. 4 b. Therefore parabolas become increasingly good local approximations of the folds.

To reveal the role of the finite-time Lyapunov exponents, let us consider a parabola y = κ 0 x 2 /2 which is already initially lying in the v 1 – v 2 plane. Over time, it is subject to stretchingand, which preserves the parabolic shape, i.e. In this process, the peak curvature increases as long as ρ 1 (t) > 2 ρ 2 (t), i.e. the first FTLE must be more than twice as large as the second one. We illustrate this at the example of a parabola in a linearized flow in Methods, showing that its peak curvature grows asfor some effective initial peak curvature. This equation can already be found in ref. where it is derived for a generic material line. Let us call the growth rate of peaks ρ p (t) = ρ 1 (t) − 2 ρ 2 (t). In turbulence, this growth rate is typically asymptotically positive. In our simulation used for obtaining the FTLEs (see Methods), we can estimate the infinite-time Lyapunov exponents, by taking the mean of the FTLEs at the final time of the simulation, which yields λ 1 ≈ 0.12/ τ η, λ 2 ≈ 0.03/ τ η, λ 3 ≈ −0.15/ τ η, in good agreement with previous literature, and thus.

---

### Thinning choroidal thickness and flattening morphology of higher myopia eyes in Chinese adults with anisometropic myopia: a comparative study [^108258ac]. Clinical Ophthalmology (2025). Medium credibility.

Fitting Curve for Choroidal Curvature Measurement

The fovea was automatically identified using OCT, and the foveal region was marked with a straight line, which served as the reference point in ImageJ software (NIH, Bethesda, MD, USA). A Cartesian coordinate system was then established, with the X-axis representing the horizontal distance from the fovea at seven points above it, and the Y-axis representing choroidal thickness (Figure 1C). A quadratic function was fitted to the choroidal thickness data at these seven points using Microsoft Excel (Microsoft Corp. Redmond, WA, USA). This analysis produced three parameters (A, B, and C), defined by the following equation:

Or

Where parameter A represents the overall morphological steepness of the quadratic function fit to the choroidal profile, parameter B determines the position of the vertex of the parabola. The vertex position, calculated as -B/(2A), corresponds to the theoretical location of the thickest point of the choroid. Parameter C indicates the fitted SFCT. The goodness of fit was evaluated using the coefficient of determination (R²). Additionally, the point-specific gradient of the choroidal thickness profile was calculated as:

which represents the instantaneous gradient of the choroidal thickness profile, given by the quadratic function.

Statistical Analysis

All statistical analyses were conducted using R software (version 4.3.2). Results are expressed as means ± standard deviation [range]. Δ values were calculated by subtracting the refractive error of the higher myopia eye from that of the eye with lower myopia. Paired sample t-tests were performed to compare interocular metrics, while independent sample t-tests were used to compare ocular metrics between the higher and lower myopic eyes in the control and anisometropia groups. Choroidal thickness at each region, Δ values, and curvature parameters (A, B, C, and R 2) were analyzed using paired sample t-tests, within the monocular contiguous region and interocular corresponding regions. Pearson's correlation coefficient was employed to assess relationships between choroidal thickness and ocular metrics. Statistical significance was defined as P < 0.05.

---

### Reference values for methacholine reactivity (SAPALDIA study) [^78b0cfa1]. Respiratory Research (2005). Low credibility.

Prediction equations of 95 th, 90 th, 75 th and 50 th percentiles of the two-point slope are given in Table 3. The corresponding curves for 40 years old subjects are represented in Figure 3. Prediction equations were derived involving age and pre-test (or baseline) FEV 1. Between the lower and upper quartile of FEV 1, these models are of the form: a + b* Age + c* FEV 1 + d* FEV 1 2, whereas no quadratic term in FEV 1 is used below the 1 st and above the 3 rd quartile. We thus used natural quadratic splines with knots at the lower and upper quartiles of FEV 1 to describe the dependency of percentiles of methacholine slope on baseline FEV 1. Therefore, up to the first quartile of FEV 1, each percentile curve of slope for a given age is described by a straight line. Another straight line describes the percentile curve for FEV 1 -values above the upper quartile. These two straight line segments are connected by a parabola segment in such a way that the transition between the different pieces is smooth. Although the coefficients a and c have to vary between the three intervals, the smoothness requirement imposes linear restrictions on them. On the other hand, the coefficient b has the same value everywhere, since the association between slope and age appeared to be approximately linear for all percentiles considered. Consequently, the curves for figure 3 would have to be shifted downward and upward for ages higher and lower than 40 years, respectively. The model shows that, with lower pre-test values of FEV 1, level and spread of the percentiles increases. A horizontal line drawn at y = 2.39% decrease/μmol represents the threshold commonly used to define bronchial hyperreactivity (20% decrease of FEV 1 after a cumulative methacholine dose of ≤ 8.37 μmol). A higher proportion of subjects belong to this "hyperreactive" category at lower values of FEV 1 or lower values of age. Consequently a higher proportion of women are defined as "hyperreactive" (Table 3). We provide an additional Excel file allowing calculation of the percentile of methacholine slope of a subject after introducing his/her age, pre-test FEV 1, and results of methacholine challenge (i.e. methacholine total cumulative dose and percentage of FEV 1 decline at this total cumulative dose) (Additional file 1).

---

### The interplay between genomic copy number variants, sleep, and cognition in the general population [^e5b0e1cd]. Translational Psychiatry (2025). Medium credibility.

Fig. 4
Associations between haploinsufficiency, sleep and cognition.

a, b represent the fitted curves of the quadratic model associating CNVs measured by their sum of 1/LOEUF (for (a) deletions and (b) duplications) with self-reported sleep duration (dark line). The light line represents the GAM model. The dotted vertical line indicates the vertex of the parabola. c, d illustrates the results of the mediation analysis between the sum of LOEUF (for deleted and duplicated genes), sleep duration, and cognitive ability (*p-value < 2e −16). The Average Causal Mediation Effect effect was 0.005 × −0.129 = −0.0007 (p < 2e-16) for deletions and 0.002 × −0.146 = −0.0003 (p < 2e-16) for duplications. The proportion mediated was 3.8% for deletions and 4.5% for duplications.

To further characterize the relationship between CNV burden and sleep duration as well as cognition, we stratified the coding genome into 4 categories with increasing intolerance to haploinsufficiency as measured by LOEUF (highly intolerant < 0.2, intolerant = [0.2;0.35[, mildly-intolerant = [0.35;1[, and tolerant ≥ 1). The three categories of increasingly intolerant genes were associated with increasingly negative effects on general cognitive ability. CNVs encompassing intolerant and highly intolerant genes were associated with increased deviation from the average self-reported sleep duration (eFig. 4). Genes within the intolerant category were associated with worst performance on both executive functioning tasks, but overall results were less robust, likely due to smaller sample sizes (eFig. 4). These results remained unchanged after removing individuals taking medication and above 65 years (eFig. 4).

---

### Temporal profiles of avalanches on networks [^7147b521]. Nature Communications (2017). Medium credibility.

Fig. 3
Nonsymmetric average avalanche shapes. Left column: Average avalanche shapes from Eq. (4), for power-law offspring distribution: q k = Ck − γ for k ≥ 1, with exponent γ = 2.5 and constant C chosen to give the branching number ξ. Each panel in the right column (b, d, e) shows the same function as in the panel to its left, but rescaled as in Fig. 2. a, b Critical case with ξ = 1; c, d subcritical case, ξ = 0.9; e, f supercritical case, ξ = 1.15; the dashed green curve is the same parabola as in Fig. 2

True power-law tails are never seen in real networks, due to finite-size effects. If the offspring distribution instead has a truncated power-law form, with an exponential cutoff for:and with 2 < γ < 3, then the avalanche shapes for all durations T do not collapse onto a single curve, even at criticality. As the asymptotic analysis of Supplementary Note 6 reveals, the shape for large T is determined by the survival function 1 − Q (t): this is the fraction of avalanches that remain alive at a time t after they begin. For the offspring distribution of Eq. (9), the survival function 1 − Q (t) scales asfor early times, but as t −1 for later times; the crossover from one regime to the other is determined by the exponential cutoff κ in the offspring distribution (see Fig. 4a). Therefore, it is possible to observe nonsymmetric shapes for avalanches with relatively short durations T, but the longer-duration avalanches (the T → ∞ limit) revert to the parabolic shape typical of offspring distributions with finite second moment, see Fig. 4b.

---

### Egocentric neural representation of geometric vertex in the retrosplenial cortex [^36eff3f8]. Nature Communications (2024). High credibility.

Vertex score analysis

To define and characterize vertex-selective spatial receptive fields, vertex score was introduced. Vertex score was computed by measuring the distance from a single neuron's peak firing location within spatial receptive fields to the nearest vertex, a similar approach used in defining the corner scoresof subicular neurons that show corner-selective spatial receptive fields. Thus, vertex score (v) of a given neuron was calculated as follows:whereis the distance from the (x, y) coordinate within the spatial receptive field with the peak firing rate to the nearest vertex, which was normalized by the largest possible distance to vertex in a given chamber. In this way, value ranged between 0 to 1, where = 0 indicated peak firing locations of all spatial receptive fields were located farthest away from all vertices and = 1 indicated peak firing locations of all spatial receptive fields were located exclusively at any of the vertices. A neuron was defined as a vertex cell within each chamber if the vertex score for each chamber exceeded the 95 th percentile of randomly shuffled vertex score distribution of all imaged RSC neurons.

Head direction tuning analysis

To analyze the head direction tuning of RSC neuronal spikes near each vertex (Fig. 2d), individual allocentric head direction tuning curves at each vertex were plotted by taking the normalized spike firing rate as a function of allocentric head direction (θallo). For each vertex, spikes that were located within the half-length of edge from the corresponding vertex (triangle: 21.65 cm, square: 17.65 cm, and hexagon: 12.50 cm) were used in the tuning curve analysis. In Supplementary Fig. 14, allocentric head direction tuning curves were plotted by taking the normalized firing rate of all spikes in the open chamber as a function of θallo. Allocentric head direction tuning curve was plotted using 15° bin and smoothed using a moving average filter.

---

### On the convexity of ROC curves estimated from radiological test results [^e12ae042]. Academic Radiology (2010). Low credibility.

Rationale and Objectives

Although an ideal observer's receiver operating characteristic (ROC) curve must be convex-ie, its slope must decrease monotonically-published fits to empirical data often display "hooks". Such fits sometimes are accepted on the basis of an argument that experiments are done with real, rather than ideal, observers. However, the fact that ideal observers must produce convex curves does not imply that convex curves describe only ideal observers. This article aims to identify the practical implications of nonconvex ROC curves and the conditions that can lead to empirical or fitted ROC curves that are not convex.

Materials and Methods

This article views nonconvex ROC curves from historical, theoretical, and statistical perspectives, which we describe briefly. We then consider population ROC curves with various shapes and analyze the types of medical decisions that they imply. Finally, we describe how sampling variability and curve-fitting algorithms can produce ROC curve estimates that include hooks.

Results

We show that hooks in population ROC curves imply the use of an irrational decision strategy, even when the curve does not cross the chance line, and therefore usually are untenable in medical settings. Moreover, we sketch a simple approach to improve any nonconvex ROC curve by adding statistical variation to the decision process. Finally, we sketch how to test whether hooks present in ROC data are likely to have been caused by chance alone and how some hooked ROCs found in the literature can be easily explained as fitting artifacts or modeling issues.

Conclusion

In general, ROC curve fits that show hooks should be looked on with suspicion unless other arguments justify their presence.

---

### What is a pressure-volume curve? [^35dc1437]. Critical Care (2006). Low credibility.

The pressure-volume (PV) curve is a physiological tool proposed for diagnostic or monitoring purposes during mechanical ventilation of acute respiratory distress syndrome. The reduction in compliance measured by the PV curve and the different inflection points on the curve are considered interesting markers of the severity of and the levels of opening and closing pressures. Tracing a curve, however, may in itself influence the degree of opening or distension of the lung, and interpretation of the curve has to take this effect into account. In some individuals tracing the curve may even have moderate hemodynamic effects. Fortunately, on average, most of these effects are transient or negligible and do not invalidate the PV curve measurement.

---

### Disadvantages of using the area under the receiver operating characteristic curve to assess imaging tests: a discussion and proposal for an alternative approach [^b31f6cd7]. European Radiology (2015). Low credibility.

ROC AUC

ROC AUC, the area under the ROC curve, is often used to summarise test performance across all thresholds in the plot. Simplistically, AUC represents how likely it is that the test will rank two patients; one with a lesion and one without, in the correct order, across all possible thresholds. More intuitively, AUC is the chance that a randomly selected patient with a lesion will be ranked above a randomly selected normal patient. A perfect test would have 100% sensitivity with zero false-positives (100% specificity), across all thresholds. This point lies at the extreme top left-hand corner of the ROC plot; AUC = 1 · 0. Such tests don't exist in real life, and we expect some failure to separate normal and abnormal patients. A straight line connecting the extreme bottom-left (sensitivity, FPR: 0,0) and top-right (1,1) corners (the "chance diagonal") describes a test with no discrimination; AUC = 0.5.

---

### Coronavirus disease 2019-the principles of the curve, explained simply [^7d648dd5]. Head & Neck (2020). Medium credibility.

In the setting of the coronavirus disease 2019 pandemic, the concept of the disease curve has become ubiquitous in medicine and across society. Nevertheless, even among medical specialists, there are common misconception about the curve and how it affects population outcomes. This article provides a simple review of the various population dynamics at play. Principles such as the area under the curve and the threshold of capacity are discussed and simply conceptualized. Understanding the fundamental characteristics of a problem can allow us to see it with more clarity. By the end of the article, the reader will gain an effortless a sense of insight on this topic.

---

### The labor curve of the grand multipara: does progress of labor continue to improve with additional childbearing? [^427c01de]. American Journal of Obstetrics and Gynecology (2002). Low credibility.

Objective

Our purpose was to test the hypothesis that progress of labor slows as parity exceeds 4 by comparing labor curves of grand multiparous women (para 5 and over) (GMs) with those of nulliparous and lower-parity multiparous women.

Study Design

Retrospective cohorts of spontaneously laboring, vertex-presenting, term GMs who were admitted to two medical centers during the period from January 1990 through June 1995 were randomly computer-matched to a nulliparous and a lower-parity multiparous control subject, matched for age, hospital, and year of delivery. Cervical examination data were graphed retrospectively from the time of full dilatation. Curves were compared by pairwise likelihood ratio tests, by using a random effects model to adjust for obstetric interventions, with significance set at P < .05.

Results

Pregnancies in 1095 GMs, 1174 lower-parity multiparous women, and 908 nulliparous women were studied. GMs exhibit a longer initial phase of labor than either nulliparous women or lower-parity multiparous women, begin to dilate rapidly at a greater dilatation than nulliparous women, and experience acceleration of labor at a rate no faster than lower-parity multiparous women. The average labor curve of GMs resembles that of nulliparous women before dilatation of 4 cm is attained, then transitions to the typical curve of the lower-parity multiparous women until dilatation of 6 cm is attained and thereafter is indistinguishable from that of the lower-parity multiparous women (P < .001).

Conclusions

Once parity exceeds 4, progress of labor slows. "Poor progress" beyond dilatation of 4 cm should not be considered abnormal for a GM, because she is likely still in the latent phase until dilatation of 6 cm is attained. Nor should she be expected to progress through her active phase any faster than lower-parity multiparous women.

---

### A selection and targeting framework of cortical locations for line-scanning fMRI [^c4b0dec3]. Human Brain Mapping (2023). Medium credibility.

3.2 Functional measures confirm accurate line‐planning

We next assessed line localization based on functional properties. First, we predicted the signal of the target vertex (Figure 4a, orange curve) in response to the stimulus design during the line‐scanning experiment. To deal with potential differences in BOLD amplitudes across sessions and sequences, we performed an additional GLM between the prediction from the target vertex' pRF and line‐scanning data. This resulted in a strong overlap between this prediction and the estimated prediction from fitting the line‐scanning data (Figure 4a, green curve). Despite this marked overlap, we observed slight variations in pRF size and location (Figure 4b). To estimate the out‐of‐experiment variance explained (r 2), we compared the r 2 from the target vertex prediction and the estimates from the line‐scanning fits, and benchmarked this against the null‐model that predicts a signal time course assuming a response that is not spatially selective, that is, a block design reflecting activation whenever the stimulus is on the screen, regardless of position (Figure 4c). The variance explained from the null‐model (block) was significantly lower than the target prediction (t 10 = −6.19, p < .001, Cohen's D = −3.57) and line prediction (t 10 = −10.67, p < .001, Cohen's D = −6.16), whereas the difference between target and line prediction was significant but only for a one‐sided effect (t 10 = 2.30, p < .044, Cohen's D = 1.33). Overall, pRF estimates between target vertex and line‐scanning estimates were very similar (Figure 4a).

---

### Brain maps of general cognitive functioning: neuroimaging and neurobiological signatures [^4df22f34]. Translational Psychiatry (2025). Medium credibility.

Morphometry measures

The morphometry measurements are illustrated in Fig. 1A (middle panel). Volume is the amount of three-dimensional space of a vertex, surface area is the total area of the cortical sheet section of the vertex, and thickness is the distance between the pial and white matter cortical surfaces. If thickness were uniform across the vertex, volume would be the product of surface area and thickness, but this relationship is more complex in practice. For curvature, a value of zero represents no curvature "– "; those with negative values are curving up (convex) "◠"; those with positive values are curving down (concave) "◡". The sulcal depth is a measure of how removed a vertex is from a theoretical mid-surface that is estimated between the gyri and sulci (vertices on the mid surface receive a value of 0). Here, a more positive sulcal depth suggests a deeper location (i.e. away from the scalp) and a more negative value is shallower (i.e. towards the scalp). Deep sulci tend to have more concave curvature, shallower regions tend to have curvature magnitudes nearer to zero, and gyri (defined here as regions with negative values for "sulcal depth") tend to correspond to convex curvature. The measure of curvature provides information about how the cortex folds at the local level, while sulcal depth provides a more macroscopic perspective on the depth of sulci relative to the midpoint of the cortical surface, offering insights into the overall brain folding complexity.

---

### Every individual makes a difference: a trinity derived from linking individual brain morphometry, connectivity and mentalising ability [^8e727323]. Human Brain Mapping (2023). Medium credibility.

2.3: Surface‐based multivariate morphometry statistics

Surface‐based MMS were proposed for brain local structural analysis (Wang et al.). Since then, studies have demonstrated that MMS has a larger effect size than volume, area and other similar morphometry measures (Dong et al; Wang et al; Wu et al.). In this work, we calculated amygdala and hippocampal MMS from raw MR images by using the MRI processing pipeline used in the paper (Dong et al.), as illustrated in Figure 1. Specifically, each hippocampal or amygdala surface was parameterised into 15,000 vertices (Dong et al; Yao et al.). And each vertex MMS has two kinds of morphometry features: radial distance (RD) (Pizer et al; Thompson et al.) and multivariate tensor‐based morphometry (mTBM) (Davatzikos; Thompson et al; Wang et al.). The RD (a scalar) represents the thickness of the shape at each vertex to the medial axis, which reflects the surface differences along the surface normal directions (Wang et al.). The medial axis is determined by the geometric centre of the isoparametric curve on the computed conformal grid (Wang et al.). The axis is perpendicular to the isoparametric curve, so the thickness can be easily calculated as the Euclidean distance between the core and the vertex on the curve. The vertex mTBM (a 31 positive definite matrix) captures deformations within local surfaces, such as rotation, dilation and shears with surfaces perpendicular to RD (Dong et al; Shi et al.). Since the surface of the hippocampus or amygdala in each brain hemisphere has 15,000 vertices, the feature dimensionality of both hippocampus or amygdala for each subject is 60,000 (15,0004) eventually.

---

### A somato-cognitive action network alternates with effector regions in motor cortex [^78b503f7]. Nature (2023). Excellent credibility.

Movement task battery curve fitting

For each vertex within precentral gyrus, we first computed its position along the dorsal–ventral axis of left hemisphere M1. This was done by identifying the closest point within the continuous line of points running down precentral gyrus (defined in 'Seed-based functional connectivity'), and assigning that closest point's ordered position within the line to the vertex.

For every movement, we then plotted that dorsal–ventral M1 position against Z -score activation in each vertex. We then fit two curves to each of these relationships. The first curve was a single-Gaussian model of the form:

Activation = a 1 × exp(−((position- b 1)/ c 1) 2).

The second curve was a double-Gaussian model of the form:

Activation = a 1 × exp(−((position- b 1)/ c 1) 2) + a 2 × exp(−((position- b 2)/ c 2) 2).

The a 1 and a 2 parameters in each model were constrained to be positive (to enforce positive-going peaks). Curve fitting was constrained to be conducted within the general vicinity of the activated area in order to avoid fitting negative activations observed in distant portions of M1. For lower extremity movements, this meant excluding the bottom third of M1; for upper extremity movements, the bottom third of M1 plus the medial wall; for face movements, the top third of M1.

Finally, we tested whether the one- or two-peak models better fit the data. This was done by conducting an F -test between the models, computed as:where SSE represents the sum of squared errors from the model and df represents the degrees of freedom in the model.

The P value was computed from this F by employing the F -statistic continuous distribution function (fcdf.m) in Matlab and using (df 1peak – df 2peaks) and df 2peaks as the numerator and denominator degrees of freedom, respectively.

Movement task battery curve visualization

For each movement, the complete dorsal–ventral M1 position versus Z -score activation profile (from above) was visualized more clearly by fitting a LOWESS curve. These LOWESS curves recapitulated the two-peak activation fits while also revealing additional task responsive cortex.

---

### On the shape of the population ROC curve [^d0731e21]. Academic Radiology (2013). Low credibility.

Rationale and Objectives

Human observers often do not produce empirical operating points near the northeast corner of the receiver operating characteristic (ROC) plot, and thus the local shape of the population ROC curve is unknown.

Materials and Methods

We call attention to occult abnormalities and propose that considerations by human observers of the prior probability of occult abnormalities can cause the shape of the local population ROC curve to be convex, a straight line, or concave near the northeast corner of the ROC plot. We further conducted a set of simulated detection-task (without-search) experiments with human observers and, mathematically, with an ideal observer and a model observer. In the experiments, we used signals, pseudo-signals that were similar to signals, and random image noise. The relative frequency of occult signals was controlled in the experiments.

Results

In the simulated experiments, the population ROC curve of the ideal observer was always convex, but those of the model observer and of human observers were convex, a straight-line, or concave, depending on the relative frequency of occult signals. The population ROC curve for the model observer was identical to that for the ideal observer when knowing the relative frequency of occult signals was not important for the ideal observer, and it was similar to that for human observers otherwise.

Conclusion

Observer consideration of the prior probability of occult abnormalities is important in ROC studies and could cause unexpected shapes of the local population ROC curve. Absence of empirical operating points near the northeast corner of the ROC plot may be caused by occult abnormalities.

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^66f1bd1f]. Nature Communications (2023). High credibility.

CurveCurator yields well-calibrated p -values using a recalibrated F-statistic

The first step to assess the statistical significance of dose–response curves is to find the best possible fit given the measured dose–response values. As the optimization surface for sigmoidal curves is non-convex (i.e. a surface with many local minima), naïve curve fitting algorithms often get stuck in local minima, leading to suboptimal fits and, thereby, overly conservative p -values (Supplementary Fig S1). CurveCurator uses a heuristic that reaches the global minimum in almost all cases in a short period of time (Supplementary Fig. S2). To obtain an empirical null distribution, we simulated 5 million dose–response curves under the null hypothesis, i.e. curves where the response is independent of the dose (i.e. no dose-dependent regulation) for a range of n data points per curve (Fig. 2a). As expected, direct application of the classical F-statistic for linear models to these null dose–response curves yielded poorly calibrated p -values (Supplementary Fig. S3). CurveCurator solved this issue by optimizing both the F-value formula and the parameters of the F-distribution as a function of n to approximate these simulated null distributions accurately (Fig. 2 b, c). The validity of this approach was confirmed by noting that p -values in real experiments in which the vast majority of curves were expected to be unresponsive formed a uniform distribution of truly non-regulated curves plus a small distribution of truly regulated curves enriched at low p -values (Fig. 2d).

---

### Universal survival curve and single fraction equivalent dose: useful tools in understanding potency of ablative radiotherapy [^9be76a26]. International Journal of Radiation Oncology, Biology, Physics (2008). Low credibility.

Purpose

Overprediction of the potency and toxicity of high-dose ablative radiotherapy such as stereotactic body radiotherapy (SBRT) by the linear quadratic (LQ) model led to many clinicians' hesitating to adopt this efficacious and well-tolerated therapeutic option. The aim of this study was to offer an alternative method of analyzing the effect of SBRT by constructing a universal survival curve (USC) that provides superior approximation of the experimentally measured survival curves in the ablative, high-dose range without losing the strengths of the LQ model around the shoulder.

Methods and Materials

The USC was constructed by hybridizing two classic radiobiologic models: the LQ model and the multitarget model. We have assumed that the LQ model gives a good description for conventionally fractionated radiotherapy (CFRT) for the dose to the shoulder. For ablative doses beyond the shoulder, the survival curve is better described as a straight line as predicted by the multitarget model. The USC smoothly interpolates from a parabola predicted by the LQ model to the terminal asymptote of the multitarget model in the high-dose region. From the USC, we derived two equivalence functions, the biologically effective dose and the single fraction equivalent dose for both CFRT and SBRT.

Results

The validity of the USC was tested by using previously published parameters of the LQ and multitarget models for non-small-cell lung cancer cell lines. A comparison of the goodness-of-fit of the LQ and USC models was made to a high-dose survival curve of the H460 non-small-cell lung cancer cell line.

Conclusion

The USC can be used to compare the dose fractionation schemes of both CFRT and SBRT. The USC provides an empirically and a clinically well-justified rationale for SBRT while preserving the strengths of the LQ model for CFRT.

---

### J-shapedness: an often missed, often miscalculated relation: the example of weight and mortality [^9e991138]. Journal of Epidemiology and Community Health (2014). Low credibility.

We present three considerations in analysing the association between weight and mortality, as well as other relations that might be non-linear in nature. First, authors must graphically plot their independent and dependent variables in a continuous manner. Second, authors should assess the shape of that relation, and note its shape. If it is non-linear, and specifically, J-shaped or U-shaped, careful consideration should be given to using the 'best' statistical model, of which multivariate fractional polynomial regression is a reasonable choice. Authors should also refrain from truncating their data to avoid dealing with non-linear relations.

---

### The paradox of aging population and firm digital transformation in China [^9820ded8]. BMC Geriatrics (2024). Medium credibility.

Minimum wage moderating

Further, this paper tested the moderating roles of the institutional environments. Model 1 in Table 4 demonstrated the coefficients in Eq. 3. Referring to the previous analysis, showed to be significantly negative. This indicated that the slopes on both sides of the curve decrease, and the shape of the curve flats out, and H2a was proven. And, we found that < 0, thus, the curve's vertex moved to the right.

Table 4
The moderating roles of institutional environments

✱ p < 0.01

Marketization moderating

Similarly, Model 2 demonstrated the coefficients in Eq. 2.was significantly positive. This indicated that the slopes on both sides of the curve increase, and H2b was proved. And, we found that < 0, thus, the curve's vertex moved to the left.

To test the concavity of the adjusted model, we perform the second derivative of Eqs. 3, 4 and 5. The second-order derivative is given in Eq. 6 as:

The second-order derivatives of all the moderated effects models were calculated to be consistently greater than zero, as shown in the last row of Table 4. This indicates that the curves of the moderated effects models are concave functions.

Marketing capability moderating

Further, this paper tested the moderating effects of corporate strategies. As Model 1 in Table 5 demonstrated, the coefficients in Eq. 4.was significantly positive, indicating that the slopes on both sides of the curve increase, and the shape of the curve flats out. Then, < 0, so the curve's vertex is moved to the left, and H3a was proved.

Table 5
The moderating roles of corporate strategies

✱ p < 0.1, ✱✱ p < 0.05, ✱✱ p < 0.01

Customer concentration moderating

Similarly, Model 2 in Table 5 demonstrated the coefficients in Eq. 5.was significantly positive, indicating that the slopes on both sides of the curve decrease. Then, > 0, so the curve's vertex was moved to the right, H3b was proved.

The second-order derivatives of all the moderated effects models were calculated to be consistently greater than zero, as shown in the last row of Table 5. This indicates that the curves of the moderated effects models are concave functions.

---

### The paradox of aging population and firm digital transformation in China [^a807893a]. BMC Geriatrics (2024). Medium credibility.

In contrast, some scholars argue that aging can have a positive impact due to a feedback mechanism. They believe that population aging forces companies to undergo digital transformation, for example, by encouraging them to enhance employee training and hire highly educated individuals. Other scholars point out that the innovation effect of population aging is greater than its cost effect, asserting that digital transformation is better driven by skilled and experienced employees, including older employees. Indeed, some studies show that population aging pushes companies to invest in R&D for innovation. Thus, we argue that the impact of population aging on firms' digital transformation may be more complex and nonlinear than previously thought.

Current research primarily explores the negative or positive linear effects of population aging on digital transformation, with few studies synthesizing and considering both scenarios. Our study attempts to fill this gap by developing a nonlinear model. Specifically, we propose and validate a nonlinear relationship model to explain the relationship between population aging and digital transformation from the perspectives of labor cost theory, the capital-skill complementarity hypothesis, and human capital externalities. We analyze the boundary mechanisms by which population aging affects digital transformation, considering institutional environment and corporate strategy.

Our empirical tests involve Chinese A-share listed companies from 2001 to 2022, revealing a U-shaped relationship between population aging and digital transformation. From the perspective of the institutional environment, we find that in regions with a higher degree of marketization, the slopes on both sides of the U-shaped relationship curve are steeper, and the vertex shifts to the left. Conversely, in regions with higher minimum wage levels, the slopes on both sides of the U-shaped relationship curve are flatter, and the vertex shifts to the right.

From the perspective of corporate strategy, our study also finds that higher levels of marketing capability make the slopes on both sides of the U-shaped relationship curve steeper, with the vertex shifting to the left. Conversely, higher levels of customer concentration make the slopes on both sides of the U-shaped relationship curve flatter, with the vertex shifting to the right.

---

### It's time to move on from the Bell curve [^d37c37d9]. Muscle & Nerve (2017). Low credibility.

The bell curve was first described in the 18th century by de Moivre and Gauss to depict the distribution of binomial events, such as coin tossing, or repeated measures of physical objects. In the 19th and 20th centuries, the bell curve was appropriated, or perhaps misappropriated, to apply to biologic and social measures across people. For many years we used it to derive reference values for our electrophysiologic studies. There is, however, no reason to believe that electrophysiologic measures should approximate a bell-curve distribution, and empiric evidence suggests they do not. The concept of using mean ± 2 standard deviations should be abandoned. Reference values are best derived by using non-parametric analyses, such as percentile values. This proposal aligns with the recommendation of the recent normative data task force of the American Association of Neuromuscular & Electrodiagnostic Medicine and follows sound statistical principles. Muscle Nerve 56: 859–860, 2017.

---

### Standardization of spirometry 2019 update. An official American Thoracic Society and European Respiratory Society technical statement [^df365e99]. American Journal of Respiratory and Critical Care Medicine (2019). High credibility.

Back-extrapolated volume (BEV) on the volume–time curve — Time 0 is found by drawing a line with a slope equal to peak flow through the point of peak flow on the volume–time curve and setting Time 0 to the point where this line intersects the time axis. The BEV is equal to the volume of gas exhaled before Time 0, which, in these two examples from the same patient, is 0.136 L for the left panel (acceptable) and 0.248 L for the right panel (unacceptable). For this patient, the BEV limit is 5% FVC = 0.225 L.

---

### Verapamil hydrochloride [^502fa733]. FDA (2020). Medium credibility.

The dosage of verapamil hydrochloride IV for treatment of paroxysmal supraventricular tachycardia in adults (pharmacologic conversion) is:

- **Loading**: 5–10 mg IV bolus over at least 2 minutes
- **Subsequently**: 10 mg IV bolus after 30 minutes if necessary

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^327fd22c]. Nature Communications (2023). High credibility.

Thresholding

The curve fold change for the i th curve (cfc i) is defined as the log 2 -ratio between the lowest and highest concentration using the regressed model and it quantifies the drug's effect size or efficacy (Eq. 11).

We transferred the SAM principle of differential T-statistics to the recalibrated F-statistic to obtain equivalent decision boundaries for the dose–response curve analyses. This is possible by recognizing that a dose–response curve converges in the limit to two groups (front plateau group = not affected data points, and back plateau group = affected data points), where the curve fold change is equivalent to a conventional SAM fold change between the two plateau groups. In this case, allowing for the conversion and application of the s 0 SAM principle. CurveCurator simplified this process by calculating the tuning parameter s 0 directly from the user-specified significance and fold change asymptotes (Eq. 12). Whereis the inverse cumulative density function of an F-distribution with degrees of freedom dfn and dfd as determined in the section above. This makes s 0 also a function of the number of data points, which is relevant when a curve has missing values.

The tuning parameter s 0, which defines the hyperbolic decision boundaries, can also be used to transform the curve's recalibrated F-value into the s 0 -adjusted F-value (F adj, i) based on the global s 0 value and the curve's measured fold change (cfc i) (Eq. 13).

We then transform this s 0 -adjusted F-value into a "relevance score" using the cumulative density function F X (x | dfn, dfd). For s 0 = 0.0, this simply corresponds to the p -value of the curve. For s 0 > 0.0, this can no longer be interpreted as a p -value, but it still provides an appropriate ordering of curves by both statistical and biological relevance. Additionally, a −log 10 transformation is applied for visualization purposes and to obtain an interpretable score that ranges from 0 to infinity, where 0 has no relevance (Eq. 13).

---

### Standardization of spirometry 2019 update. An official American Thoracic Society and European Respiratory Society technical statement [^abfb5c44]. American Journal of Respiratory and Critical Care Medicine (2019). High credibility.

Within-maneuver evaluation and back-extrapolated volume (BEV) — at the point of PEF on the volume–time graph, a tangent is drawn and its intersection on the abscissa defines time 0, which becomes the start for all timed measurements; the BEV is the volume of gas that has already been expired from maximal lung volume to time 0 and is included in the FEV1 (forced expiratory volume in 1 second) and FVC (forced vital capacity) measurements; to ensure that the FEV1 comes from a maximal effort, the BEV must be ≤ 5% of the FVC or 0.100 L, whichever is greater; the 0.100-L tolerance is a reduction from the 0.150-L tolerance in the 2005 standards; the hesitation time, defined as the time from the point of maximal inspiration to time 0, should be 2 seconds or less; FEV1 and FVC measurements from a maneuver with BEV exceeding the limit are neither acceptable nor usable, and a large BEV will usually result in an erroneously high FEV1; patients with upper airway obstruction or neuromuscular disease are often unable to initiate a rapid increase in flow, and the BEV limit may be exceeded.

---

### Egocentric neural representation of geometric vertex in the retrosplenial cortex [^0c6d4d3f]. Nature Communications (2024). High credibility.

Egocentric vertex vector analysis

To analyze the egocentric bearing of the mouse to vertex (θego), the angle of the closest vertex relative to θallo at each sample point of behavior data was computed. θego was set to 0°, 90°, –180°/180°, and –90°, when the closest vertex is in front, right, back, and left to mice, respectively. Egocentric distance to vertex (Dego) was calculated by computing the distance from mice to the closest vertex at each sample point of behavior data. In order to analyze the tuning of egocentric bearing (θego) of RSC neuronal spikes (Fig. 2d), individual egocentric bearing tuning curves at each vertex were plotted by taking the normalized firing rate of spikes as a function of θego. For each vertex, spikes that were located within the half-length of edge from the corresponding vertex (triangle: 21.65 cm, square: 17.65 cm, and hexagon: 12.50 cm) were used in the tuning curve analysis. The egocentric bearing tuning curve was plotted using a 15° bin and smoothed using a moving average filter.

To analyze RSC neuronal spike firing characteristics in terms of θego and Dego, the egocentric vertex rate map was analyzed (Figs. 2, 3, 4, Supplementary Fig. 15, 17, 18, 20, 21, 22). The egocentric vertex rate map was plotted by calculating the firing rate in terms of θego and Dego in a polar coordinate in polar plot. θego was divided into 15° bins and egocentric distance was divided into 1.5 cm bins, which yielded 15° × 1.5 cm bins. Firing rates for each bin were calculated for each neuron, by dividing the number of spikes within a given bin by the whole time spent in the bin. Only the bin in which mice spent more than 133 ms (temporal resolution of behavior data) was included in the data analysis. Egocentric vertex rate maps were smoothed using 2D Gaussian kernel σ = 0.7 pixels.

---

### A conformational switch in clathrin light chain regulates lattice structure and endocytosis at the plasma membrane of mammalian cells [^454e5199]. Nature Communications (2023). High credibility.

Conformational changes in endocytic proteins are regulators of clathrin-mediated endocytosis. Three clathrin heavy chains associated with clathrin light chains (CLC) assemble into triskelia that link into a geometric lattice that curves to drive endocytosis. Structural changes in CLC have been shown to regulate triskelia assembly in solution, yet the nature of these changes, and their effects on lattice growth, curvature, and endocytosis in cells are unknown. Here, we develop a new correlative fluorescence resonance energy transfer (FRET) and platinum replica electron microscopy method, named FRET-CLEM. With FRET-CLEM, we measure conformational changes in clathrin at thousands of individual morphologically distinct clathrin-coated structures. We discover that the N-terminus of CLC repositions away from the plasma membrane and triskelia vertex as coats curve. Preventing this conformational switch with chemical tools increases lattice sizes and inhibits endocytosis. Thus, a specific conformational switch in the light chain regulates lattice curvature and endocytosis in mammalian cells.

---

### The paradox of aging population and firm digital transformation in China [^ed400787]. BMC Geriatrics (2024). Medium credibility.

Second, the institutional environments significantly moderate the U-shaped relationship between the aging population and digital transformation. The results of this paper found that the degree of marketization strengthens the U-shaped relationship between the aging population and digital transformation and moves the curve's vertex to the left. While the degree of marketization exacerbates the negative impact of the aging population in the short term, it promotes the digital transformation process for a long time. Moreover, firms in regions with more marketization upgrade their digital transformation earlier. Minimum wage mitigates the U-shaped relationship between the aging population and digital transformation and moves the curve's vertex to the right. While the degree of marketization mitigates the negative impact of the aging population in the short run, it hinders the digital transformation process in the long run. Firms in regions with higher minimum wages upgrade their digital transformation later.

Finally, corporate strategies significantly moderate the U-shaped relationship between the aging population and digital transformation. This paper found that marketing capabilities strengthen the U-shaped relationship between the aging population and digital transformation and move the curve's vertex to the left. While marketing capabilities exacerbate the negative impact of the aging population in the short term, they facilitate the digital transformation process for a long time. Firms with higher marketing capabilities ramp up digital transformation earlier. Customer concentration mitigates the U-shaped relationship between the aging population and digital transformation and moves the curve's vertex to the right. While customer concentration mitigates the negative impact of the aging population in the short term, it hinders the digital transformation process for a long time. Firms with higher customer concentration ramp up their digital transformation later.

---

### Validation of monte carlo estimates of three-class ideal observer operating points for normal data [^26b6e5d9]. Academic Radiology (2013). Low credibility.

Rationale and Objectives

Traditional two-class receiver operating characteristic (ROC) analysis is inadequate for the complete evaluation of observer performance in tasks with more than two classes.

Materials and Methods

Here, a Monte Carlo estimation method for operating point coordinates on a three-class ROC surface is developed and compared with analytically calculated coordinates in two special cases: (1) univariate and (2) restricted bivariate trinormal underlying data.

Results

In both cases, the statistical estimates were found to be good in the sense that the analytical values lay within the 95% confidence interval of the estimated values about 95% of the time.

Conclusions

The statistical estimation method should be key in the development of a pragmatic performance metric for evaluation of observers in classification tasks with three or more classes.

---

### Reassessing the labor curve in nulliparous women [^c4c82a06]. American Journal of Obstetrics and Gynecology (2002). Low credibility.

Objectives

Our purpose was to examine the pattern of labor progression in nulliparous parturients in contemporary obstetric practice.

Study Design

We extracted detailed labor data from 1329 nulliparous parturients with a term, singleton, vertex fetus of normal birth weight after spontaneous onset of labor. Cesarean deliveries were excluded. We used a repeated-measures regression with a 10th-order polynomial function to discover the average labor curve under contemporary practice. With use of an interval-censored regression with a log normal distribution, we also computed the expected time interval of the cervix to reach the next centimeter, the expected rate of cervical dilation at each phase of labor, and the duration of labor for fetal descent at various stations.

Results

Our average labor curve differs markedly from the Friedman curve. The cervix dilated substantially slower in the active phase. It took approximately 5.5 hours from 4 cm to 10 cm, compared with 2.5 hours under the Friedman curve. We observed no deceleration phase. Before 7 cm, no perceivable change in cervical dilation for more than 2 hour was not uncommon. The 5th percentiles of rate of cervical dilation were all below 1 cm per hour. The 95th percentile of time interval for fetal descent from station +1/3 to +2/3 was 3 hours at the second stage.

Conclusion

Our results suggest that the pattern of labor progression in contemporary practice differs significantly from the Friedman curve. The diagnostic criteria for protraction and arrest disorders of labor may be too stringent in nulliparous women.

---

### Triazolam [^59f91090]. FDA (2025). Medium credibility.

WARNING: RISKS FROM CONCOMITANT USE WITH OPIOIDS; ABUSE, MISUSE, AND ADDICTION; and DEPENDENCE AND WITHDRAWAL REACTIONS

Concomitant use of benzodiazepines and opioids may result in profound sedation, respiratory depression, coma, and death. Reserve concomitant prescribing of these drugs in patients for whom alternative treatment options are inadequate. Limit dosages and durations to the minimum required. Follow patients for signs and symptoms of respiratory depression and sedation [see Warnings and Precautions (5.1), Drug Interactions (7.1)].
The use of benzodiazepines, including triazolam, exposes users to risks of abuse, misuse, and addiction, which can lead to overdose or death. Abuse and misuse of benzodiazepines commonly involve concomitant use of other medications, alcohol, and/or illicit substances, which is associated with an increased frequency of serious adverse outcomes. Before prescribing triazolam and throughout treatment, assess each patient's risk for abuse, misuse, and addiction [see Warnings and Precautions (5.2)].
The continued use of benzodiazepines, including triazolam, may lead to clinically significant physical dependence. The risks of dependence and withdrawal increase with longer treatment duration and higher daily dose. Abrupt discontinuation or rapid dosage reduction of triazolam after continued use may precipitate acute withdrawal reactions, which can be life-threatening. To reduce the risk of withdrawal reactions, use a gradual taper to discontinue triazolam or reduce the dosage [see Dosage and Administration (2.3), Warnings and Precautions (5.3)].

WARNING: RISKS FROM CONCOMITANT USE WITH OPIOIDS; ABUSE, MISUSE, AND ADDICTION; and DEPENDENCE AND WITHDRAWAL REACTIONS

See full prescribing information for complete boxed warning.

Concomitant use of benzodiazepines and opioids may result in profound sedation, respiratory depression, coma, and death. Reserve concomitant prescribing of these drugs in patients for whom alternative treatment options are inadequate. Limit dosages and durations to the minimum required. Follow patients for signs and symptoms of respiratory depression and sedation (5.1, 7.1).
The use of benzodiazepines, including triazolam, exposes users to risks of abuse, misuse, and addiction, which can lead to overdose or death. Before prescribing triazolam and throughout treatment, assess each patient's risk for abuse, misuse, and addiction (5.2).
Abrupt discontinuation or rapid dosage reduction of triazolam after continued use may precipitate acute withdrawal reactions, which can be life-threatening. To reduce the risk of withdrawal reactions, use a gradual taper to discontinue triazolam or reduce the dosage (2.3, 5.3).

---

### Acquisition and expression of proximal and distal upper limb stimulus-response curves to transcranial magnetic stimulation [^66c7ee04]. Muscle & Nerve (2002). Low credibility.

Motor cortex stimulus-response (S/R) curves are an indication of cortical excitability and are of relevance to topographical mapping. The aims of this study were to compare two different methods of collecting data to construct a S/R curve for transcranial magnetic stimulation (TMS) in the upper limbs, to identify reliable summary statistics for the S/R curve, and to determine whether S/R curves predicted motor threshold. Motor evoked potentials (MEP) were obtained from biceps brachii (BB) and abductor pollicis brevis (APB) muscles at rest with a circular coil centered at the vertex. Motor threshold was determined using a validated protocol. MEPs were obtained with 1% increments in intensity or from the average of five trials at 5% increments. The S/R relationships were fitted to linear, S-shaped, and Boltzmann functions. A linear function determined from the average of five trials accurately summarized our data (r2 from 0.6 to 0.9, P < 0.05, n = 8, for right APB and from 0.6 to 0.9, P < 0.05, n = 8 for right BB). The X-axis intercept of the line determined using these methods fell between the upper and lower limits of motor threshold in all eight subjects. We propose that MEP values obtained at intervals of 5% averaged over five trials, fitted to a linear function provides a practical means of assessing the S/R characteristics of TMS for proximal and distal upper limb muscles.

---

### Evaluation of the labor curve in nulliparous Japanese women [^bbd9316b]. American Journal of Obstetrics and Gynecology (2010). Low credibility.

Objective

We sought to compare Japanese nulliparous labor progression with Friedman's classic 1955 curve and Zhang's 2002 curve.

Study Design

We developed a labor curve using retrospective record reviews of 2369 Japanese nulliparas, at term, spontaneous labor onset and singleton vertex deliveries of normal birth weight infants.

Results

The new Japanese Suzuki-Horiuchi labor curve with slower cervical dilation in the active phase was like Zhang's and differed from Friedman's curve. Labor length was approximately 5 hours occurring between 4–10 cm compared with Friedman's 2.5 hours and Zhang's 5.5 hours. Even at 10-cm dilation, labor lasted > 2 hours at the 95th percentile of time interval.

Conclusion

Similar to Zhang's curve, the Suzuki-Horiuchi curve was smooth and more gradually sloped than Friedman's curve. Appraise "arrested or protracted labor" with these slower labor curves in mind using Friedman's curve cautiously.

---

### Roflumilast (Zoryve) [^e2de6cd7]. FDA (2024). Medium credibility.

The dosage of roflumilast TOP for treatment of plaque psoriasis in adults is 1 application(s) TOP daily (0.3% cream or foam)

---

### Refractive errors preferred practice pattern ® [^ceea4b31]. Ophthalmology (2023). High credibility.

Eyeglasses — difficulties and complications of wear are linked to lens and frame factors, with the page listing incorrect prescription, base curve and location of the cylinder on the front or back surface, bifocal power and segment position (height and size), tint, anisometropia (if large), prisms or prism effects, pantoscopic tilt, centration of lenses with respect to the pupil, vertex distance, size of frame and fit, contact sensitivity to frame material, and change in lens material. In addition, the lenses in the eyeglasses can cause spherical and chromatic aberrations as well as lens distortions, including magnification (hyperopic lenses) and minification (myopic lenses), and eyeglasses are protective, however, which is especially important for monocular patients. Problems with reading zone size and peripheral distortion increase with stronger addition lenses.

---

### Degenerate boundaries for multiple-alternative decisions [^ef91a93b]. Nature Communications (2022). High credibility.

Fig. 4
Speed-accuracy trade-off curves for 3AFCs optimized over all parameters.

Colors indicate the cost ratio c / W (blue through to yellow) for a the curve parameterization, b the power parameterization, and c the oscil parameterization. The average SAT curve (dashed line) is a piecewise-linear curve from the mean decision time and error over all three parameterizations for each c / W value (see methods). All parameterizations (a – c) give regions of speed-accuracy trade-off that are tightly gathered around the mean SAT curve and depend smoothly on the cost ratio c / W.

Fig. 5
Reward landscapes and effective degeneracy of optima.

a – c show example reward landscapes over parameters of the curve(θ, α) decision boundaries for cost ratios c / W of 0.001, 0.04, and 0.1 for 3AFCs. The regions of maximal reward are outlined (black lines for δ = 0.02; see text) within a projection of the mean reward onto a horizontal plane, which reveals an extended region of nearly-optimal decision boundaries. d – f show the speed-accuracy trade-off for decision boundaries encompassed in the maximal region of a – c, with the black dashed line showing the mean SAT curve from Fig. 4. Notice the "spread" of SAT values for the same cost ratio and similar values of θ. There are apparently two types of degeneracy in the set of optimized decision boundaries: boundary shape (a – c) and SAT (d – f).

---

### CPSN: caputo principal-curve-guided segmentation network on ultrasound kidney databases [^87ef1145]. Journal of Imaging Informatics in Medicine (2025). Medium credibility.

The kidney contour is a critical reference for assessing whether a renal tumor has penetrated the renal capsule and invaded adjacent tissues. Accurate segmentation of the kidney contour is essential for estimating renal volume and constructing patient-specific anatomical models, which are vital for pre-operative planning and image-guided biopsy. Manual delineation of the kidney contour is time-consuming and prone to inter- and intra-observer variations. Addressing the development of novel methods for precise kidney boundary delineation in ultrasonic data is challenging owing to the absence or indistinctness of these boundaries. In this work, a novel segmentation method named a Caputo principal-curve-guided segmentation network (CPSN) integrated principal-curve (PC)-based vertex decision into a Caputo multiple-layer learning network was developed to boost the precision of ultrasound (US) kidney segmentation. First, an initial deep network was designed to extract the rough contour information. Second, the PC-based vertex decision block was adopted to determine the distribution order of vertices, which was subsequently used as the input of the Caputo multiple-layer training network. Third, the Caputo training network was further trained to decrease the global model error and improve alignment between predicted results and ground truth labels. Several experiments were conducted to validate the effectiveness and robustness of our method on multi-institute US kidney databases, which were proven to achieve superior performance compared to other state-of-the-art (SOTA) techniques, with Dice index (DI), Jaccard index (JI), and accuracy (ACC) values of 94.6 ± 3.2%, 93.4 ± 3.7%, and 94.1 ± 3.47%, respectively.

---

### Roflumilast (Zoryve) [^11a5acd0]. FDA (2024). Medium credibility.

The dosage of roflumilast TOP for treatment of plaque psoriasis in both children (in patients 6–12 years) is 1 application(s) TOP daily (0.3% cream)

---

### Elacestrant (Orserdu) [^25e5bc7d]. FDA (2024). Medium credibility.

The dosage of elacestrant PO for treatment of breast cancer in postmenopausal female adults with ESR1 mutation (advanced or metastatic, ER-positive, HER2-negative, progression after ≥ 1 line of endocrine therapy) is 345 mg PO daily until disease progression or unacceptable toxicity

---

### Receiver operating characteristic (ROC) curves: the basics and beyond [^65266ad3]. Hospital Pediatrics (2024). Medium credibility.

Diagnostic tests and clinical prediction rules are frequently used to help estimate the probability of a disease or outcome. How well a test or rule distinguishes between disease or no disease (discrimination) can be measured by plotting a receiver operating characteristic (ROC) curve and calculating the area under it (AUROC). In this paper, we review the features of ROC curves and interpretation of ROC curves and AUROC values. We highlight 5 underappreciated features of ROC curves: (1) the slope of the ROC curve over a test result interval is the likelihood ratio for that interval; (2) the optimal cutoff for calling a test positive depends not only on the shape of the ROC curve, but also on the pretest probability of disease and relative harms of false-positive and false-negative results; (3) the AUROC measures discrimination only, not the accuracy of the predicted probabilities; (4) the AUROC is not a good measure of discrimination if the slope of the ROC curve is not consistently decreasing; and (5) the AUROC can be increased by including a large number of people correctly identified as being at very low risk for the outcome of interest. We illustrate this last concept using 3 published studies.

---

### The burden of proof studies: assessing the evidence of risk [^ae0fefba]. Nature Medicine (2022). Excellent credibility.

Basis splines, measurement mechanism and shape constraints

We used a Bayesian regularized spline to obtain the general shape of the nonlinear relationship. Basis splines represent nonlinear curves as linear combination of recursively generated basis elements. The basis elements were recursively generated using piecewise smooth polynomials, and were roughly localized to certain regions of the exposure variable in the data. Most of the time, quadratic or cubic polynomials were used, often with linear tails in the presence of sparse data. This approach allowed the common restricted cubic spline and constraints on the shape of the relationship (including nondecreasing and nonincreasing).

Given basis functions f 1,…, f k and coefficient vector β = (β 1. β k), the final curve is obtained as a β -linear combination

Specifically, for any given exposure (x), the prediction using the spline model is given bywhere X is a vector containing (f 1 (x),…, f k (x)). Derivatives and integrals of splines can likewise be expressed as linear combinations of spline coefficient β. For additional details about B -splines see Zheng et al.

Many studies of dose–response relationships report relative risks between categories defined by intervals of consumption. In mathematical notation, these observations are given bywhere y ij is the reported relative risk corresponding to measurement j in study i, [a ij, b ij] delineates the reference group exposure interval, and [c ij, d ij] delineates the alternative group exposure interval.

When f (x) is represented using a spline, each integral is a linear function of β similar to equation (1). The model (equation (2)) is then a ratio of linear functions, with the log relative risk given by

Equation (4) is a nonlinear function of the spline coefficients β.

When studying dose–response relationships, we allow for shape constraints of the inferred mean response. For example, for some harmful risks, such as smoking and air pollution, we assume the relative risk is monotonically increasing with exposure. To regularize the splines, and capture biologically plausible limits, we also allow a maximal derivative constraint, which is similar to penalizing total variation or limiting the spline degree. To introduce each of these constraints, we used the fact that derivatives of splines are linear functions of spline coefficients, similar to equation (1).

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^d55d13ff]. Nature Communications (2023). High credibility.

F-statistics and p -values

The basic idea behind the F-value in classical linear regression problems is to quantify how much better a more complex model (M 1 with k linear parameters) fits the data compared to a simpler model (M 0 with j linear parameters and j < k) given the n observed data points and the corresponding sum-squared errors (SSE) (Eq. 6).

Although not a linear model by nature, the log-logistic function still meets the required assumptions of random sampling, independence of observations, residual normality, and equal variance of the errors. The basic rationality behind CurveCurator's recalibrated F-statistic is similar to the linear F-statistic above. It also quantifies how much better the fitted log-logistic model (M 1) is compared to the mean model (M 0), which describes that there is no relationship between the applied dose and the observed response. We found, however, that n/k was a more appropriate scaling factor for the 4-parameter log-logistic function.

The obtained recalibrated F-value (Eq. 7) can then be used to calculate a p -value that quantifies how often a curve with a similar or bigger F-value can be found by random chance. We observed that these F-values follow a parameterized F-distribution with degrees of freedom that diverged from the case of linear models. Using extensive simulations under the null hypothesis (5 million curves for n = 5… 50), we obtained a simple quasi-linear function to calculate the "effective "degrees of freedom as a function of n (Eqs. 8–10).

---

### Transbronchial lung cryobiopsy for interstitial lung disease: early experience, learning curve, and the impact of sedation on complication rates at a single centre in Japan [^5b873694]. BMC Pulmonary Medicine (2024). Medium credibility.

Histopathological assessment

Adequate alveolar tissue was obtained in 97.5% of cases, and the histopathological diagnoses based on the TBLC specimens were as follows: nonspecific interstitial pneumonia in 28 (23.5%), hypersensitivity pneumonitis in 23 (19.3%), drug-induced ILD in 15 (12.6%), idiopathic pulmonary fibrosis in 10 (8.4%), cryptogenic organising pneumonia in 7 (5.9%), connective tissue disease-related ILD in 7 (5.9%), eosinophilic pneumonia in 3 (2.5%) and multi-centric Castleman's disease, pulmonary alveolar proteinosis, sarcoidosis and silicosis in 1 (0.8%) patient each. However, 19 (16.0%) of cases remained as unclassifiable interstitial pneumonia (UCIP) despite the high rate of alveolar-tissue retrieval.

CUSUM learning-curve analysis

As shown in the scatter plot of procedure time per number of biopsies (Fig. 1 A), there was a downward trend in procedure time with increasing case numbers. The fitted curve was plotted in the CUSUM analysis of procedure time per number of biopsies (Fig. 1 B). The formula for the scatter plot polynomial fitting curve was.

Fig. 1
The trend of the procedural time and CUSUM analyses. A: The trend of the procedural time per number of biopsies. B: The learning curve (CUSUM chart) of the procedural time per number of biopsies (56 was the vertex of the curve). C: The learning curve (RA-CUSUM chart) of the procedural time per number of biopsies (58 was the vertex of the curve)

---

### First and second stage labor management: ACOG clinical practice guideline no. 8 [^01a25eb9]. Obstetrics and Gynecology (2024). High credibility.

Friedman compared with Zhang labor curves — Friedman described a sigmoid labor pattern with latent, active, and deceleration phases; the 95th percentile of latent phase duration was 20 hours in nulliparous patients and 14 hours in multiparous patients, the transition from latent to active phase was thought to occur at approximately 4 cm cervical dilation, and the 95th percentile rate of active phase dilation ranged from 1.2 cm/hour in nulliparous patients to 1.5 cm/hour in multiparous patients. Zhang et al analyzed labor progression in a retrospective study at 19 hospitals involving 62,415 parturients with singleton vertex vaginal births and normal perinatal outcomes and reported wide variation in latent phase duration dependent on admission cervical examination and parity, a substantially slower 95th percentile rate of active-phase dilation than Friedman's standard, similar dilation rates from 4 cm to 6 cm across parity, faster dilation beyond 6 cm in multiparous individuals, transition to active phase at 6 cm compared with 4 cm, and no deceleration phase at the end of the first stage.

---

### Verapamil hydrochloride [^e3a8ad82]. FDA (2020). Medium credibility.

The dosage of verapamil hydrochloride IV for treatment of idiopathic fascicular left ventricular tachycardia in adults is:

- **Loading**: 5–10 mg IV bolus over at least 2 minutes
- **Subsequently**: 5–10 mg IV bolus after 30 minutes if necessary

---

### Learning heterogeneous reaction kinetics from X-ray videos pixel by pixel [^b2e965c2]. Nature (2023). Excellent credibility.

We then performed k -fold cross-validation, training the model on k − 1 half-cycles and validated on the other one. Using the one-standard-error rule, we determine the optimal ρ 2 = 0.88. As mentioned in the main text, the training RMSE at ρ 2 = 0.88 is 6.8%. By contrast, the training RMSE at ρ 2 = 0.01 is 6.0%, suggesting overfitting. Conversely, the training RMSE at ρ 2 → ∞ is 10.6% (Supplementary Table 3 and Supplementary Information section 9.4), that is, the kinetic prefactor is spatially uniform (k (x, y) = 1), which indicates underfitting. At larger ρ 2, the bias in g h (c) and j 0 (c) is also expected to be large because the bias increases with ρ 2 regardless of the number of particles (Supplementary Information section 8.2 and Supplementary Fig. 35). For a visual comparison of the RMSE using different parameters, refer to Supplementary Fig. 50.

Most of the particles with three half-cycles show a strong correlation among k (x, y) trained on the basis of the three training datasets, and all three validation curves have clear minima around the optimal ρ 2. The presence of a few particles that show a lack of consistency among k (x, y) in cross-validation reflects the possibility of unmodelled effects in certain half-cycles in the heterogeneous dataset (Supplementary Information section 8.6). Therefore, to avoid bias, the reported spatial heterogeneities k (x, y) in Supplementary Fig. 57 are obtained by training on all half-cycles with equal weights using the optimal regularization coefficient.

---

### Reliable and computationally efficient maximum-likelihood estimation of "proper" binormal ROC curves [^47db8e38]. Academic Radiology (2007). Low credibility.

Rationale and Objectives

Estimation of ROC curves and their associated indices from experimental data can be problematic, especially in multireader, multicase (MRMC) observer studies. Wilcoxon estimates of area under the curve (AUC) can be strongly biased with categorical data, whereas the conventional binormal ROC curve-fitting model may produce unrealistic fits. The "proper" binormal model (PBM) was introduced by Metz and Pan to provide acceptable fits for both sturdy and problematic datasets, but other investigators found that its first software implementation was numerically unstable in some situations. Therefore, we created an entirely new algorithm to implement the PBM.

Materials and Methods

This paper describes in detail the new PBM curve-fitting algorithm, which was designed to perform successfully in all problematic situations encountered previously. Extensive testing was conducted also on a broad variety of simulated and real datasets. Windows, Linux, and Apple Macintosh OS X versions of the algorithm are available online at http://xray.bsd.uchicago.edu/krl/.

Results

Plots of fitted curves as well as summaries of AUC estimates and their standard errors are reported. The new algorithm never failed to converge and produced good fits for all of the several million datasets on which it was tested. For all but the most problematic datasets, the algorithm also produced very good estimates of AUC standard error. The AUC estimates compared well with Wilcoxon estimates for continuously distributed data and are expected to be superior for categorical data.

Conclusion

This implementation of the PBM is reliable in a wide variety of ROC curve-fitting tasks.

---

### Learning curve for laparoscopic major hepatectomy [^15914df1]. The British Journal of Surgery (2015). Low credibility.

Background

Laparoscopic major hepatectomy (LMH) is evolving as an important surgical approach in hepatopancreatobiliary surgery. The present study aimed to evaluate the learning curve for LMH at a single centre.

Methods

Data for all patients undergoing LMH between January 1998 and September 2013 were recorded in a prospective database and analysed. The learning curve for operating time (OT) was evaluated using the cumulative sum (CUSUM) method.

Results

Of 173 patients undergoing major hepatectomy, left hepatectomy was performed in 28 (16·2 per cent), left trisectionectomy in nine (5·2 per cent), right hepatectomy in 115 (66·5 per cent), right trisectionectomy in 13 (7·5 per cent) and central hepatectomy in eight (4·6 per cent). Median duration of surgery was 270 (range 100–540) min and median blood loss was 300 (10–4500) ml. There were 20 conversions to an open procedure (11·6 per cent). Vascular clamping was independently associated with conversion on multivariable analysis (hazard ratio 5·95, 95 per cent c.i. 1·24 to 28·56; P = 0·026). The CUSUMOT learning curve was modelled as a parabola (CUSUMOT = 0·2149×patient number(2) -30·586×patient number-1118·3; R(2) = 0·7356). The learning curve comprised three phases: phase 1 (45 initial patients), phase 2 (30 intermediate patients) and phase 3 (the subsequent 98 patients). Although right hepatectomy was most common in phase 1, a significant decrease was observed from phase 1 to 3 (P = 0·007) in favour of more complex procedures.

Conclusion

The learning curve for LMH consisted of three characteristic phases identified by CUSUM analysis. The data suggest that the learning phase of LMH included 45 to 75 patients.

---

### Semiparametric estimation of the relationship between ROC operating points and the test-result scale: application to the proper binormal model [^9a9d14d8]. Academic Radiology (2011). Low credibility.

Rationale and Objectives

Semiparametric methods provide smooth and continuous receiver operating characteristic (ROC) curve fits to ordinal test results and require only that the data follow some unknown monotonic transformation of the model's assumed distributions. The quantitative relationship between cutoff settings or individual test-result values on the data scale and points on the estimated ROC curve is lost in this procedure, however. To recover that relationship in a principled way, we propose a new algorithm for "proper" ROC curves and illustrate it by use of the proper binormal model.

Materials and Methods

Several authors have proposed the use of multinomial distributions to fit semiparametric ROC curves by maximum-likelihood estimation. The resulting approach requires nuisance parameters that specify interval probabilities associated with the data, which are used subsequently as a basis for estimating values of the curve parameters of primary interest. In the method described here, we employ those "nuisance" parameters to recover the relationship between any ordinal test-result scale and true-positive fraction, false-positive fraction, and likelihood ratio. Computer simulations based on the proper binormal model were used to evaluate our approach in estimating those relationships and to assess the coverage of its confidence intervals for realistically sized datasets.

Results

In our simulations, the method reliably estimated simple relationships between test-result values and the several ROC quantities.

Conclusion

The proposed approach provides an effective and reliable semiparametric method with which to estimate the relationship between cutoff settings or individual test-result values and corresponding points on the ROC curve.

---

### Degenerate boundaries for multiple-alternative decisions [^33b7df25]. Nature Communications (2022). High credibility.

For closer scrutiny of the optimal region, five sections of the reward landscapes for 3AFC curve(θ, α) parameterization are shown in Fig. 6. These sections correspond to values α = {−20, 10, 0, 10, 20} (including the flat boundary α = 0) to provide a detailed look at the reward landscape peaks with 100-fold more samples of θ than in Fig. 5. Evidently, the peak changes with cost ratio c / W and θ (Fig. 6, right column). This analysis leads to the observations: (I) As c / W increases, the reward landscape maximum covers a broader range of θ values (section peaks separate) and appears to acquire slope (peaks diverge in height, with a higher-to-lower pattern). (II) The spread of the average rewards at the peak of each section overlaps (red dashed lines), decreasing as c / W increases but not diminishing to zero. (III) Extracting near-to-optimal parameters by taking all points within a small δ = 0.02 range of the peak average reward yields a set of near-optimal decision boundaries over a broad range of θ and α values (black points in Fig. 6).

Fig. 6
Sections through reward landscapes.

Five sections through the reward landscapes from Fig. 5 a–c for the 3AFC curve(θ, α) parameterization are shown in the left-hand column of panels for α = {−20, −10, 0, 10, 20} (colors in legend). Note that the α = 0 (cyan) sections are flat decision boundaries. Average rewards across the sections are shown in the right-hand column with their mean (solid lines) and spread (transparent area), both are estimated with Gaussian Process regression; 95% confidence bounds. The red dashed lines show the overlap between the spread of all sections at their peak mean rewards. Maximal regions lie within a small fraction (δ = 0.02) of the standard deviation of the peak reward, giving a degenerate set of decision boundaries with close-to-optimal rewards (black points).

---

### The XZZX surface code [^8f6ac9c8]. Nature Communications (2021). High credibility.

This structured noise model thus leads to two distinct regimes, depending on which failure process is dominant. In the first regime where, we expect that the logical failure rate will decay like. We find this behaviour with systems of a finite size and at high bias where error rates are near to threshold. We evaluate logical failure rates using numerical simulations to demonstrate the behavior that characterises this regime; see Fig. 6 (a). Our data show good agreement with the scaling ansatz. In contrast, our data are not well described by a scaling.

Fig. 6
Sub-threshold scaling of the logical failure rate with the XZZX code.

a Logical failure rateat high bias near to threshold plotted as a function of code distance d. We use a lattice with coprime dimensions d × (d + 1) for d ∈ {7, 9, 11, 13, 15} at bias η = 300, assuming ideal measurements. The data were collected usingiterations of Monte-Carlo (MC) samples for each physical rate sampled and for each lattice dimension used. The physical error rates used are, from the bottom to the top curves in the main plot, p = 0.19, 0.20, 0.21, 0.22 and 0.23. Error bars represent one standard deviation for the Monte-Carlo simulations. The solid lines are a fit of the data to, consistent with Eq. (2), and the dashed lines a fit to, consistent with Eq. (3) where we would expect, see Methods. The data fit the former very well; for the latter, the gradients of the best fit dashed lines, as shown on the inset plot as a function of, give a linear slope of 0.61(3). Because this slope exceeds the value of 0.5, we conclude that the sub-threshold scaling is not consistent with. b Logical failure ratesat modest bias far below threshold plotted as a function of the physical error rate p. The data (markers) were collected at bias η = 3 and coprime d × (d + 1) code dimensions of d ∈ {5, 7, 9, 11, 13, 15} assuming ideal measurements. Data is collected using the Metropolis algorithm and splitting method presented in refs. The solid lines represent the prediction of Eq. (3). The data show very good agreement with the single parameter fitting for all system sizes as p tends to zero.

---

### Haemodynamic effects of changes in atrioventricular and interventricular delay in cardiac resynchronisation therapy show a consistent pattern: analysis of shape, magnitude and relative importance of atrioventricular and interventricular delay [^08b0f1c7]. Heart (2006). Low credibility.

Objective

To assess the haemodynamic effect of simultaneously adjusting atrioventricular (AV) and interventricular (VV) delays.

Method

35 different combinations of AV and VV delay were tested by using digital photoplethysmography (Finometer) with repeated alternations to measure relative change in systolic blood pressure (SBP(rel)) in 15 patients with cardiac resynchronisation devices for heart failure.

Results

Changing AV delay had a larger effect than changing VV delay (range of SBP(rel) 21 v 4.2 mm Hg, p < 0.001). Each had a curvilinear effect. The curve of response to AV delay fitted extremely closely to a parabola (average R2 = 0.99, average residual variance 0.8 mm Hg2). The response to VV delay was significantly less curved (quadratic coefficient 67 v 1194 mm Hg/s2, p = 0.003) and therefore, although the residual variance was equally small (0.8 mm Hg2), the R2 value was 0.7. Reproducibility at two months was good, with the SD of the difference between two measurements of SBP(rel) being 2.5 mm Hg for AV delay (2% of mean systolic blood pressure) and 1.5 mm Hg for VV delay (1% of mean systolic blood pressure).

Conclusions

Changing AV and VV delays results in a curvilinear acute blood pressure response. This shape fits very closely to a parabola, which may be valuable information in developing a streamlined clinical protocol. VV delay adjustment provides an additional, albeit smaller, haemodynamic benefit to AV optimisation.

---

### Uncertainty of risk estimates from clinical prediction models: rationale, challenges, and approaches [^96927c16]. BMJ (2025). Excellent credibility.

When using an evaluation dataset, deriving uncertainty of risks fully conditional on all predictor values in the original model is usually difficult, but examining uncertainty conditional on estimated risk is possible by using calibration plots and calibration curves. Calibration examines the agreement between estimated risks (from the existing model) and observed risks (in the evaluation dataset), and the uncertainty in calibration performance stems entirely from the number of participants and observed outcome events, and the participants' distribution of risk estimates, in the evaluation dataset itself.

Evaluation datasets must contain the values of the outcome, and any predictors used in the model, so that the model can be applied to every individual (ie, to make predictions) and comparisons made between predicted and observed outcomes. These comparisons allow the derivation of smoothed calibration curves, which measure the (potentially non-linear) agreement between observed risks and model estimated risks, across the entire range of predictions (ie, estimated risks from 0 to 1). The smoothed curve can be generated using, for example, polynomials, splines, or non-parametric methods, with confidence intervals derived post-estimation using methods such as Fisher's Information or bootstrapping. The curve and confidence interval can be displayed on a calibration plot, as shown in figure 5 from an external validation of a model used to estimate five year recurrence risk after a primary breast cancer diagnosis. The confidence interval (vertical range on the y axis) around the curve at a particular point on the x axis, provides the uncertainty interval for the actual risk of a group of individuals with the same estimated risk from the model (x axis). For example, figure 5 shows that the group of individuals with an estimated risk of 0.8 (x axis) have a 95% uncertainty interval around the curve (y axis) of between 0.78 and 1.00. Thus, if a new individual is estimated a risk of 0.8 by the model, we could use this interval to say: "In a group of 100 individuals with the same estimated risk as you, the model suggests that between about 78 and 100 will have a recurrence by five years." The sample size of the evaluation study can be targeted to reach a particular level of precision in the calibration curve, to reduce the width of these risk conditional uncertainty intervals.

---

### Vigabatrin (Sabril) [^b24a8a3c]. FDA (2025). Medium credibility.

Figure 1. Percent Reduction from Baseline in Seizure Frequency

Study 2

Study 2 (N = 183 randomized, 182 evaluated for efficacy) was a randomized, double-blind, placebo-controlled, parallel study consisting of an 8-week baseline period and a 16-week treatment period. During the first 4 weeks following randomization, the dose of vigabatrin was titrated upward beginning with 1 g/day and increased by 0.5 g/day on a weekly basis to the maintenance dose of 3 g/day.

Results for the primary measure of effectiveness, reduction in monthly complex partial seizure frequency, are shown in Table 9. Vigabatrin3 g/day was statistically significantly superior to placebo in reducing seizure frequency.

Figure 2 presents the percentage of patients (X-axis) with a percent reduction in seizure frequency (responder rate) from baseline to the maintenance phase at least as great as that represented on the Y-axis. A positive value on the Y-axis indicates an improvement from baseline (i.e., a decrease in complex partial seizure frequency), while a negative value indicates a worsening from baseline (i.e., an increase in complex partial seizure frequency). Thus, in a display of this type, a curve for an effective treatment is shifted to the left of the curve for placebo. The proportion of patients achieving any particular level of reduction in seizure frequency was consistently higher for the SABRIL 3 g/day group compared to the placebo group. For example, 39% of patients randomized to SABRIL (3 g/day) experienced a 50% or greater reduction in complex partial seizure frequency, compared to 21% of patients randomized to placebo. Patients with an increase in seizure frequency > 100% are represented on the Y-axis as equal to or greater than -100%.

---

### Rigorous location of phase transitions in hard optimization problems [^d98ebea5]. Nature (2005). Excellent credibility.

It is widely believed that for many optimization problems, no algorithm is substantially more efficient than exhaustive search. This means that finding optimal solutions for many practical problems is completely beyond any current or projected computational capacity. To understand the origin of this extreme 'hardness', computer scientists, mathematicians and physicists have been investigating for two decades a connection between computational complexity and phase transitions in random instances of constraint satisfaction problems. Here we present a mathematically rigorous method for locating such phase transitions. Our method works by analysing the distribution of distances between pairs of solutions as constraints are added. By identifying critical behaviour in the evolution of this distribution, we can pinpoint the threshold location for a number of problems, including the two most-studied ones: random k-SAT and random graph colouring. Our results prove that the heuristic predictions of statistical physics in this context are essentially correct. Moreover, we establish that random instances of constraint satisfaction problems have solutions well beyond the reach of any analysed algorithm.

---

### Some statistical implications of dose uncertainty in radiation dose-response analyses [^68f5bab2]. Radiation Research (2006). Low credibility.

Statistical dose-response analyses in radiation epidemiology can produce misleading results if they fail to account for radiation dose uncertainties. While dosimetries may differ substantially depending on the ways in which the subjects were exposed, the statistical problems typically involve a predominantly linear dose-response curve, multiple sources of uncertainty, and uncertainty magnitudes that are best characterized as proportional rather than additive. We discuss some basic statistical issues in this setting, including the bias and shape distortion induced by classical and Berkson uncertainties, the effect of uncertain dose-prediction model parameters on estimated dose-response curves, and some notes on statistical methods for dose-response estimation in the presence of radiation dose uncertainties.

---

### The natural history of the normal first stage of labor [^73edddaf]. Obstetrics and Gynecology (2010). Low credibility.

Objective

To examine labor patterns in a large population and to explore an alternative approach for diagnosing abnormal labor progression.

Methods

Data from the National Collaborative Perinatal Project were used. A total of 26,838 parturients were selected who had a singleton term gestation, spontaneous onset of labor, vertex presentation, and a normal perinatal outcome. A repeated-measures analysis was used to construct average labor curves by parity. An interval-censored regression was used to estimate duration of labor stratified by cervical dilation at admission and centimeter by centimeter.

Results

The median time needed to progress from one centimeter to the next became shorter as labor advanced (eg, from 1.2 hours at 3–4 cm to 0.4 hours at 7–8 cm in nulliparas). Nulliparous women had the longest and most gradual labor curve; multiparous women of different parities had very similar curves. Nulliparas may start the active phase after 5 cm of cervical dilation and may not necessarily have a clear active phase characterized by precipitous dilation. The deceleration phase in the late active phase of labor may be an artifact in many cases.

Conclusion

The active phase of labor may not start until 5 cm of cervical dilation in multiparas and even later in nulliparas. A 2-hour threshold for diagnosing labor arrest may be too short before 6 cm of dilation, whereas a 4-hour limit may be too long after 6 cm. Given that cervical dilation accelerates as labor advances, a graduated approach based on levels of cervical dilation to diagnose labor protraction and arrest is proposed.

Level Of Evidence

III.

---

### Verapamil hydrochloride [^5bd19dd0]. FDA (2023). Medium credibility.

The dosage of verapamil hydrochloride PO for treatment of left ventricular outflow tract obstruction in adults with hypertrophic cardiomyopathy is:

- **Start at**: 40 mg PO TID
- **Maximum**: 480 mg per day

---

### A continuous-time maxSAT solver with high analog performance [^3d122302]. Nature Communications (2018). Medium credibility.

Many real-life optimization problems can be formulated in Boolean logic as MaxSAT, a class of problems where the task is finding Boolean assignments to variables satisfying the maximum number of logical constraints. Since MaxSAT is NP-hard, no algorithm is known to efficiently solve these problems. Here we present a continuous-time analog solver for MaxSAT and show that the scaling of the escape rate, an invariant of the solver's dynamics, can predict the maximum number of satisfiable constraints, often well before finding the optimal assignment. Simulating the solver, we illustrate its performance on MaxSAT competition problems, then apply it to two-color Ramsey number R(m, m) problems. Although it finds colorings without monochromatic 5-cliques of complete graphs on N ≤ 42 vertices, the best coloring for N = 43 has two monochromatic 5-cliques, supporting the conjecture that R(5, 5) = 43. This approach shows the potential of continuous-time analog dynamical systems as algorithms for discrete optimization.

---

### Revisiting the visual acuity curves. A proposed methodology for the evaluation of postoperative visual acuity in presbyopia [^f119efa0]. Clinical Ophthalmology (2024). Medium credibility.

At this point is it important to note that polynomials of high (8 th) degree are bound to exhibit variations between measured points that are not acceptable in the context of a VA test. On the other hand, spline polynomials, especially cubic ones, exhibit mathematical properties more suitable for modeling VA curves. Such an example is shown in Figure 2, where the 8 th degree polynomial that interpolates the 9 VA values is plotted for intermediate distances. Prominent inacceptable variations are observed, whereas the use of cubic spline exhibits a more plausible interpolating curve. Thus, the cubic spline interpolation, henceforth denoted as spline, is selected to be applied to the available data.

Figure 2
Difference between interpolation methods using 8 th degree polynomials and 3 rd degree spline polynomials.

Figure 3 shows the interpolation curves for 5 random eyes of the validation group, generated for the best combination of n inc = 5 distances, denoted by squares. The VA for the remaining 4 distances are depicted as circles, for evaluation of the interpolation. The absolute difference between the measured and the predicted VA is shown as vertical segments.

Figure 3
Interpolation curves for five random eyes, generated for n inc = 5 diopters (denoted by squares). The VA for the remaining 4 diopters are depicted as circles, for evaluation of the interpolation.

In the context of curve analysis, the area of the curve (AoC) is defined as the area of the region between the interpolation polynomial curve and the X axis. This area is computed by evaluating the integral of the polynomial g (x) within the integration limits, which are set by the minimum and maximum values of the x -axis, with VA given in Letters.

We further investigated another interpolation error metric, based on AoC, in order to compare two VA curves. Unfortunately, the absolute difference of two AoC s is not a reliable metric of the difference between two VACs, since it is known that two curves may present very similar AoC s, with significant shape differences, as shown in Figure 4. In this example, the visual curve was generated a) using 9 measurements (magenta curve) and b) using 5 measurements denoted by "o" (blue curve). It is evident that a new metric was needed to quantify the curves' differences.

---

### Hydrocodone bitartrate (hysingla ER) [^0c03a0bc]. FDA (2023). Medium credibility.

Warnings and precautions regarding the use of hydrocodone bitartrate ER PO (also known as Hysingla ER):
- **Adrenal insufficiency**: use caution in patients taking the drug for a prolonged period (> 1 month).
- **Central sleep apnea**: use caution in patients taking higher doses.
- **Decreased serum hydrocodone level**: use caution in patients taking CYP3A4 inducers (such as rifampin, carbamazepine, or phenytoin) or discontinuing CYP3A4 inhibitors (such as macrolide antibiotics, azole antifungals, or protease inhibitors).
- **Erectile dysfunction, infertility**: use caution in patients taking the drug for a prolonged period.
- **Exacerbation of increased ICP**: use caution in patients with increased ICP, brain tumor, or head injury.
- **Growth suppression**: use caution in patients with chronic corticosteroid therapy. Monitor growth regularly in pediatric patients receiving chronic corticosteroid therapy. Reassess the need for hydrocodone regularly and adjust the corticosteroid therapy as appropriate.
- **Hypotension, syncope**: use caution in patients with reduced blood volume or taking other CNS depressants. Monitor BP after initiation and dose titration. Avoid use in patients with circulatory shock.
- **Mask symptoms of head injury**: use caution in patients with head injury. Avoid use in patients with impaired consciousness or coma.
- **Opioid overdose**: use caution in patients taking CNS depressants or with a history of opioid use disorder or prior opioid overdose. Consider prescribing naloxone based on the patient's risk factors for overdose.
- **Opioid withdrawal syndrome**: do not discontinue abruptly in patients physically dependent on opioids.
- **Opioid withdrawal syndrome**: use caution in patients taking CYP3A4 inducers (such as rifampin, carbamazepine, or phenytoin) or discontinuing CYP3A4 inhibitors (such as macrolide antibiotics, azole antifungals, or protease inhibitors).
- **Opioid withdrawal syndrome**: use extreme caution in patients taking mixed agonist/antagonist analgesics (such as pentazocine, nalbuphine, or butorphanol) or partial agonist analgesics (such as buprenorphine).
- **Prolonged QT interval**: use caution in patients with congestive HF, bradyarrhythmia, electrolyte abnormalities, or taking drugs prolonging QT interval. Avoid use in patients with congenital long QT syndrome. Do not exceed 90 mg BID in patients developing QT prolongation.
- **Seizure**: use caution in patients with seizure disorder.
- **Serotonin syndrome**: use caution in patients taking serotonergic drugs.
- **Serotonin syndrome**: use extreme caution in patients taking MAOIs or within 14 days of stopping treatment.
- **Somnolence**: use extreme caution in patients performing activities requiring mental alertness, such as driving or operating machinery.
- **Sphincter of Oddi dysfunction**: use caution in patients with biliary tract disease and acute pancreatitis.

---

### The SNMMI and EANM practice guideline for renal scintigraphy in adults [^07cb4218]. European Journal of Nuclear Medicine and Molecular Imaging (2018). Medium credibility.

Renal scintigraphy — time zero specification and curve preprocessing: Some quantitative methods require specifying time zero from which other time intervals can be measured, and most authors recommend using peak time of the heart region of interest (ROI) curve. The peak of the heart ROI curve should be visible to ensure acquisition started before the peak, and the raw curve should not start at its maximum in the first frame because this may reflect a point on the descending part of the curve if the study started too late. Before processing, the images or the curve points at the peak of the heart curve should be deleted, renal curves should start from zero or nearly zero counts, and this serves as a cross-check if the heart ROI curve peaks in the first recorded frame.

---

### Standards of care in diabetes – 2025 [^bbf43d9f]. Diabetes Care (2025). High credibility.

Regarding screening and diagnosis for diabetes mellitus type 2, more specifically with respect to diagnosis, ADA 2025 guidelines recommend to obtain confirmatory testing in the absence of unequivocal hyperglycemia (such as hyperglycemic crises).

---

### A scaling law to model the effectiveness of identification techniques [^d51713ae]. Nature Communications (2025). High credibility.

AI techniques are increasingly being used to identify individuals both offline and online. However, quantifying their effectiveness at scale and, by extension, the risks they pose remains a significant challenge. Here, we propose a two-parameter Bayesian model for exact matching techniques and derive an analytical expression for correctness (κ), the fraction of people accurately identified in a population. We then generalize the model to forecast how κ scales from small-scale experiments to the real world, for exact, sparse, and machine learning-based robust identification techniques. Despite having only two degrees of freedom, our method closely fits 476 correctness curves and strongly outperforms curve-fitting methods and entropy-based rules of thumb. Our work provides a principled framework for forecasting the privacy risks posed by identification techniques, while also supporting independent accountability efforts for AI-based biometric systems.

---

### Collective excitations of a bound-in-the-continuum condensate [^453a8256]. Nature Communications (2023). High credibility.

Fig. 4
Momentum-space scan.

a – d Experimental PL maps in the (k x, k y) domain taken at different energies just above the blueshifted BIC polariton condensate (as denoted on the panels). The line k x = 0 above threshold stays fully dark, making the PL profile in the vicinity of the condensate look like two parallel stripes corresponding to the flat parts ofappearing at small non-zero k x. These maps are time-integrated in the pulsed excitation setting. Intensity colorscale is in arb. units and normalized the same way for all panels. Condensate and reservoir densities n 0 = 5 × 10 10 cmcm −2, other parameters as in Fig. 2.

From the point of view of the anisotropy of collective excitations which is illustrated by the above study of the spectrum, it is also interesting to address the limit of very small momenta. Experimentally, since the region of k x corresponding to the positive slope ofis very small, the shift in energy which is acquired within this narrow Δ k x lies within the linewidth. At the same time, it is not possible to follow the change in energy with k y along the positive slope of the saddle (from Fig. 2 a one sees that at k x → 0 the energy-flat part along k y disappears and the dispersion is linear) due to the darkness of the states corresponding to k x = 0 (at any k y). Theoretically, however, taking the limit p → 0 in Eq. (3) allows to obtain the sound velocity which appears to be strongly dependent on direction (on the angle φ in the polar coordinate system):whereare the effective masses of LLP in the x (y)–direction, and s x is the mass-like parameter defined by the radiative losses rate γ and the radiative coupling of the modes U (for more details, see the SI). In a standard case of real positive masses in both directions, for experimentally-relevant parameters (resulting in) the velocity given by Eq. (4) would significantly increase when approaching the x –axis (the direction corresponding to). However, here the expression for the sound velocity is modified by the negativeand by the imaginary contribution s x, which makes the definition of the sound velocity more complicated. In Fig. 5 b we plot the sound velocity dependence on the angle φ revealing a very anisotropic profile. The increase of c s in the x –direction is clearly seen from the difference of slopes in the linearized parts of the Bogoliubov dispersion (see the zoom-in at the low-momenta region in Fig. 5 a): while the slope along k y at k x = 0 corresponds to very low, the shallow parts of the spectrum at finite but small k x result in the sound velocity in the directions slightly deviating from the y –axis being very close to zero (see Fig. 5 c). Much steeper slopes along k x at k y = 0 provide maxima of c s at φ = π m, as shown in Fig. 5 b for various condensate densities. It is important to note that taking into account the non-radiative exciton losses would additionally affect the behavior of the excitation spectrum along k x in a very narrow vicinity of the saddle point. A more detailed discussion of this matter is provided in the SI.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^d6d8fcbc]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Spirometry reporting specifies that numerical values are given for the FEV1, the FVC, and the FEV1/FVC ratio; the latter should be reported as a decimal fraction and the space for percent predicted value left blank, and if bronchodilators are given the LLN column need not be repeated with absolute and percent change given only for FEV1 and FVC. Other numerical values such as the forced inspiratory flow at 75% of FVC (FEF75%) and FEF25–75% are not recommended for routine use. Graph requirements include that for the volume–time curve the volume scale should be at least 10 mm/L, the time scale at least 20 mm/s, and 1 second prior to the start of expiration should be displayed; on the flow–volume plot the flow display should be at least 5 l/min/L/s, and the ratio of flow to volume should be 2 L/s to 1 L, and linear and log scales where values are plotted as z-scores relative to the predicted value (z = 0) give an intuitive sense of severity.

---

### Clotrimazole [^b5a7188a]. FDA. Low credibility.

The dosage of clotrimazole OTIC for treatment of otomycosis in adults is 1 vial OTIC BID for 14 days (1%/0.17 mL)

---

### Imlunestrant [^7b5c5001]. FDA. Low credibility.

Labeled indications for Imlunestrant include:

- Treatment of breast cancer in female adults with ESR1 mutation (advanced or metastatic, ER-positive, HER2-negative, progression after ≥ 1 line of endocrine therapy)

---

### Standards of care in diabetes – 2025 [^ef149bc3]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for diabetic nephropathy, more specifically with respect to pediatric patients, ADA 2025 guidelines recommend to determine the eGFR at the time of diagnosis and annually thereafter.

---

### Creation of the scaphocephalic index: measurement of global and regional severity in scaphocephaly [^00e04e06]. Plastic and Reconstructive Surgery (2024). Medium credibility.

Background

The recently described frontal bossing index (FBI) and occipital bullet index (OBI) allow for quantification of scaphocephaly. A similar index examining biparietal narrowing has not been described. Addition of such an index measuring width would allow for direct evaluation of the primary growth restriction in sagittal craniosynostosis and the formation of an optimized global width/length measure.

Methods

Computed tomography scans and three-dimensional photographs were used to recreate scalp surface anatomy. Equidistant axial, sagittal, and coronal planes were overlaid, creating a Cartesian grid. Points of intersection were analyzed for population trends in biparietal width. Using the most descriptive point coupled with the sellion protrusion to control for head size, the vertex narrowing index is formed. By combining this index with the FBI and OBI, the scaphocephalic index (SCI) is created as a tailored width/length measure.

Results

Using 221 controls and 360 individuals with sagittal craniosynostosis, the greatest difference occurred superiorly and posteriorly at a point 70% of the head's height and 60% of the head's length. This point had an area under the curve of 0.97 and sensitivity and specificity of 91.2% and 92.2%, respectively. The SCI has an area under the curve of 0.9997, sensitivity and specificity greater than 99%, and interrater reliability of 0.995. The correlation coefficient between computed tomography imaging and three-dimensional photography was 0.96.

Conclusions

The vertex narrowing index, FBI, and OBI evaluate regional severity, while the SCI is able to describe global morphology in patients with sagittal craniosynostosis. These measures allow for superior diagnosis, surgical planning, and outcome assessment, independent of radiation.

---

### A probabilistic Bayesian approach to recovermap and phase images for quantitative susceptibility mapping [^b146d367]. Magnetic Resonance in Medicine (2022). Medium credibility.

Purpose

Undersampling is used to reduce the scan time for high-resolution three-dimensional magnetic resonance imaging. In order to achieve better image quality and avoid manual parameter tuning, we propose a probabilistic Bayesian approach to recover < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:msubsup > < mml:mrow > < mml:mi > R < /mml:mi > < /mml:mrow > < mml:mrow > < mml:mn > 2 < /mml:mn > < /mml:mrow > < mml:mrow > < mml:mo > ∗ < /mml:mo > < /mml:mrow > < /mml:msubsup > < /mml:mrow > < mml:annotation > $$ {R}_2^{\ast } $$ < /mml:annotation > < /mml:semantics > < /mml:math > map and phase images for quantitative susceptibility mapping (QSM), while allowing automatic parameter estimation from undersampled data.

Theory

Sparse prior on the wavelet coefficients of images is interpreted from a Bayesian perspective as sparsity-promoting distribution. A novel nonlinear approximate message passing (AMP) framework that incorporates a mono-exponential decay model is proposed. The parameters are treated as unknown variables and jointly estimated with image wavelet coefficients.

Methods

Undersampling takes place in the y-z plane of k-space according to the Poisson-disk pattern. Retrospective undersampling is performed to evaluate the performances of different reconstruction approaches, prospective undersampling is performed to demonstrate the feasibility of undersampling in practice.

Results

The proposed AMP with parameter estimation (AMP-PE) approach successfully recovers < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:msubsup > < mml:mrow > < mml:mi > R < /mml:mi > < /mml:mrow > < mml:mrow > < mml:mn > 2 < /mml:mn > < /mml:mrow > < mml:mrow > < mml:mo > ∗ < /mml:mo > < /mml:mrow > < /mml:msubsup > < /mml:mrow > < mml:annotation > $$ {R}_2^{\ast } $$ < /mml:annotation > < /mml:semantics > < /mml:math > maps and phase images for QSM across various undersampling rates. It is more computationally efficient, and performs better than the state-of-the-art < mml:math xmlns:mml = "http://www.w3.org/1998/Math/MathML" > < mml:semantics > < mml:mrow > < mml:msub > < mml:mrow > < mml:mi > l < /mml:mi > < /mml:mrow > < mml:mrow > < mml:mn > 1 < /mml:mn > < /mml:mrow > < /mml:msub > < /mml:mrow > < mml:annotation > $$ {l}_1 $$ < /mml:annotation > < /mml:semantics > < /mml:math > -norm regularization (L1) approach in general, except a few cases where the L1 approach performs as well as AMP-PE.

Conclusion

AMP-PE achieves better performance by drawing information from both the sparse prior and the mono-exponential decay model. It does not require parameter tuning, and works with a clinical, prospective undersampling scheme where parameter tuning is often impossible or difficult due to the lack of ground-truth image.

---

### Relative, local and global dimension in complex networks [^a94b7a43]. Nature Communications (2022). High credibility.

Results

Graph dimension from diffusion dynamics

We start with the Green's function of the diffusion equation in d dimensionswhich, together with an initial condition as a delta function at some position x 0, provides a solution of diffusion equation as p (x, t) = G t (x − x 0). From hereon, we refer to the time evolution of p (x, t) as the transient response. As already considered in our previous works, these solutions have a maxima in their transient response at any other location x, at timeand amplitudegiven aswhere, without loss of generality, x 0 = 0. Then, the dimension at any point x relative to x 0 can be evaluated to yield the definition of the relative dimensionClearly, on the Euclidean space, the relative dimension is always equal to d, independently of x and x 0. However, if we instead consider a compact subspace, the diffusion dynamics will deviate from those prescribed in Equation (1) due to the presence of boundaries relative to x and x 0.

The key property of Equation (3) that allows us to generalise it to graphs is that the positions x 0 and x are not explicit in the right-hand side but only used as labels to initialise the diffusion dynamics and measure the transient response. Consequently, the relative dimension can be seen as intrinsic as it does not rely on any Euclidean embedding, but only on the existence of a diffusion dynamics on the original space. In particular, on graphs we can use the standard diffusion processfor a time-dependent node vector p (t) with L the normalised graph Laplacian L = K −1 (K − A) (corresponding to Euclidean diffusion in the continuous limit), where K is the diagonal matrix of node degrees. Using a delta function at node i with mass m i, p (0) = (0, 0,…, m i,…, 0), as our initial condition, the j -th coordinate of the solution of Equation (4) (the so-called transient response of j) is given by the heat kernelBy numerically solving (5), we can measure the timeand amplitudeat which a maximum appears in the transient response peak (time evolution) of node j given a delta function initial condition at node i. In analogy to Equation (3), we can then compute the full N × N matrix of relative dimensions with elements

---

### 2014 ESC guidelines on diagnosis and management of hypertrophic cardiomyopathy: the task force for the diagnosis and management of hypertrophic cardiomyopathy of the European Society of Cardiology (ESC) [^11206c8f]. European Heart Journal (2014). Medium credibility.

Regarding diagnostic investigations for syncope, more specifically with respect to initial ECG, ESC 2014 guidelines recommend to obtain a 12-lead ECG to identify the cause of symptoms in patients with unexplained syncope.

---

### Inferring time derivatives including cell growth rates using gaussian processes [^b5444dae]. Nature Communications (2016). Medium credibility.

Methods

Using a Gaussian process to fit time-series data

In the following, we will denote a Gaussian distribution with mean μ and covariance matrix Σ as(μ, Σ) and use the notation of Rasmussen and Williamsas much as possible.

Prior probability

For n data points y i at inputs x i (each x i is a time for a growth curve), we denote the underlying latent function as f (x). We define a covariance matrix k (x, x ′), which has an explicit dependence on hyperparameters θ, and obeys

where the expectations are taken over the distribution of latent functions (samples of f (x)).

We interpret equation (1) as giving the prior probability distribution of the latent functions f (X), where were we use X to denote the inputs x i, such that

where K (X, X) is the n × n matrix with components k (x i, x j). With f denoting [f (x 1). f (x n)], this prior probability can be written as

noting the dependence of k (x, x ′; θ) on the hyperparameters θ.

---

### Practical guide for interpreting and reporting cardiac PET measurements of myocardial blood flow: an information statement from the American Society of Nuclear Cardiology, and the Society of Nuclear Medicine and Molecular Imaging [^b4a2d4ce]. Journal of Nuclear Cardiology (2021). High credibility.

Compartment model — placement of ROIs and time-activity curves includes reviewing myocardial ROI placement in each frame, correcting motion (particularly in the blood pool phase) and interpreting any study with patient motion with caution, and recognizing that dynamic curves show the blood pool and myocardial activities, the model fit, blood pool spillover, and the partial-volume-corrected myocardial curve; the blood pool and myocardial activity curves should start at zero with at least one background frame prior to radiotracer infusion and have single peaks, rest and stress blood pool curves should have similar shapes and peaks with stress peaks somewhat lower, uptake and myocardial blood flow (MBF) maps should have similar regional distributions with homogeneous goodness-of-fit parameter polar maps showing high values for the R^2 map and low values for the χ^2 map, and blood pool spillover polar maps used for partial-volume correction should be carefully examined because asymmetric spillover distribution may suggest patient motion or improper contours.

---

### A Bayesian mixture model for predicting the COVID-19 related mortality in the United States [^24c047b0]. The American Journal of Tropical Medicine and Hygiene (2021). Medium credibility.

Specifically, let the mortality at day t be comprised as, where y i (t) represents the number of deaths at day t attributed to a surge or sub-epidemic indexed by i = 1,…, K, K ≥ 1 is the number of sub-epidemics empirically identified, and E (y i [t]) = μ i (t). The trajectory μ i (t) for surge i can be modeled by a homogeneous Gaussian function μ i (t) = M i *ϕ(t; t peak, i, σ i), where M i is the overall mortality, t peak, i is the time to peak, and σ i represents the width for surge i. The mortality curve for the whole pandemic E (y [t]) = μ(t) is then decomposed as:Letbe the overall mortality of the pandemic, thenis the proportion of all deaths attributed to surge i, with M i = M π i.

Therefore, we proposed the following mixture curve model for modeling the trajectory of daily COVID-19 mortality:where M is the overall mortality and ϕ(t; t peak, i σ i) represents the part of the curve related to some underlying factor(s), π i is the proportion of total deaths that are attributed to such factor(s) indexed by i, and K is the number of sub-curves comprising the mixtures. The number of parameters identifying the curve is 3K: M, π 1…, π K −1, (t peak, 1 σ 1),…, (t peak, K σ K). The ϕ(t; t peak, i σ i) here is the Gaussian density function, defined by the location parameter t peak, i, which represents the time from the first death to apex, and the scale parameter σ i, which represents the spread of the curve. We propose a Bayesian approach for estimating the curve parameters and subsequently any other statistics of interest, with values for the parameters being drawn from their posterior distribution condition on the observed data. We assume the distribution for the observed mortality data y (t) at time t to be negative binomial:where N is the state's population size. A negative binomial distribution function is used for Y (t), as it is appropriate for count data, with. Bayesian Markov Chain Monte Carlo methods are used to make draws from the posterior distribution of the unknown parameters and derive future projections. Weakly informative prior distributions were used for all model parameters(see).

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^bb9b7f12]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Δ Φ, the probability distribution at a point Φ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Φ ∈ {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters α and β, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since α and β are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Φ ∈ [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(α, β) and x 2 = 1 − x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Φ, it may seem natural to select α and β by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### Candidate gene prioritization for chronic obstructive pulmonary disease using expression information in protein-protein interaction networks [^9bda16cf]. BMC Pulmonary Medicine (2021). Medium credibility.

Results

Parameters of the PEP method

Optimal value of parameter

Parameterwas used to evaluate the significance of the COPD disease genes and candidate genes in the weighted COPD PPI network. LOOCV was used to investigate the performance for a range ofvalues (1, 10, 15 and 30), which were shown as ROC curves (Fig. 1). The AUC values corresponding to these ROC curves (> 0.88) showed that good performance could be attained with allvalues. The best performance was achieved when, which implied that the COPD disease genes and candidate genes were of equal importance in the COPD PPI network.

Fig. 1
ROC curves for the PEP method with a, b, c and d

Optimal value of parameter

Theparameter in the PEP method was used to evaluate the significance between vertexes and edges in the weighted COPD PPI network. LOOCV was also conducted to evaluate their performance for differentvalues (Table 2).

Table 2
AUC values using differentvalues of the PEP method

AUC values for = 0 or 1 were lower than those for othervalues. This demonstrated that the performance when the vertex and edge information were considered simultaneously was better than that when only the vertex or the edge information was considered. The AUC value was the highest for = 0.7, demonstrating that vertexes were more important than edges in the COPD PPI network.

As mentioned above, = 1 and = 0.7 were optimal parameter values for the PEP method with and AUC of 0.945, indicating the effectiveness of the PEP method. These optimal values were used to calculate the gene disease risk scores in the following sections.

---

### Joint EANM / SNMMI guideline on radiomics in nuclear medicine: jointly supported by the EANM physics committee and the SNMMI physics, instrumentation and data sciences council [^09c2935c]. European Journal of Nuclear Medicine and Molecular Imaging (2023). High credibility.

Decision curve analysis and model feature response — For models aimed at offering decision support for clinical interventions, decision curves allow visualizing the benefit of using the model, and we recommend the use of a decision curve analysis for both categorical and survival endpoints; clinical or other baseline models may likewise yield a decision curve for comparison. Characterizing the response of the model to the underlying features is important for understanding why the model yields its predictions and may help identify whether a model incorporates relevant information or has learned a spurious correlation; when features are implicit (e.g., a CNN), many characteristics cannot be directly assessed and other techniques developed specifically for these methods should be relied upon.

---

### 2019 ESC / EAS guidelines for the management of dyslipidaemias: lipid modification to reduce cardiovascular risk [^93b222cd]. European Heart Journal (2020). High credibility.

Regarding diagnostic investigations for dyslipidemia, more specifically with respect to lipid profile, tests, EAS/ESC 2020 guidelines recommend to obtain HDL-C to further refine risk estimation using the online SCORE system.

---

### Learning curve for robotic-assisted laparoscopic colorectal surgery [^24c26f9a]. Surgical Endoscopy (2011). Low credibility.

Background

Robotic-assisted laparoscopic surgery (RALS) is evolving as an important surgical approach in the field of colorectal surgery. We aimed to evaluate the learning curve for RALS procedures involving resections of the rectum and rectosigmoid.

Methods

A series of 50 consecutive RALS procedures were performed between August 2008 and September 2009. Data were entered into a retrospective database and later abstracted for analysis. The surgical procedures included abdominoperineal resection (APR), anterior rectosigmoidectomy (AR), low anterior resection (LAR), and rectopexy (RP). Demographic data and intraoperative parameters including docking time (DT), surgeon console time (SCT), and total operative time (OT) were analyzed. The learning curve was evaluated using the cumulative sum (CUSUM) method.

Results

The procedures performed for 50 patients (54% male) included 25 AR (50%), 15 LAR (30%), 6 APR (12%), and 4 RP (8%). The mean age of the patients was 54.4 years, the mean BMI was 27.8 kg/m(2), and the median American Society of Anesthesiologists (ASA) classification was 2. The series had a mean DT of 14 min, a mean SCT of 115.1 min, and a mean OT of 246.1 min. The DT and SCT accounted for 6.3% and 46.8% of the OT, respectively. The SCT learning curve was analyzed. The CUSUM(SCT) learning curve was best modeled as a parabola, with equation CUSUM(SCT) in minutes equal to 0.73 × case number(2) - 31.54 × case number - 107.72 (R = 0.93). The learning curve consisted of three unique phases: phase 1 (the initial 15 cases), phase 2 (the middle 10 cases), and phase 3 (the subsequent cases). Phase 1 represented the initial learning curve, which spanned 15 cases. The phase 2 plateau represented increased competence with the robotic technology. Phase 3 was achieved after 25 cases and represented the mastery phase in which more challenging cases were managed.

Conclusions

The three phases identified with CUSUM analysis of surgeon console time represented characteristic stages of the learning curve for robotic colorectal procedures. The data suggest that the learning phase was achieved after 15 to 25 cases.

---

### The descent curve of the grand multiparous woman [^6b288769]. American Journal of Obstetrics and Gynecology (2003). Low credibility.

Objective

The purpose of this study was to compare the descent curves and second-stage length among grand multiparous, nulliparous, and lower parity multiparous women.

Study Design

Retrospective cohorts of spontaneously laboring, vertex-presenting, term, grand multiparous women (parity ≥ 5) from two medical centers over 5.5 years were matched randomly to nulliparous women and lower parity multiparous women controlled for age, hospital, and year of delivery. Descent curves were modeled from serial cervical examination data by the estimation of the probability of a given station occurring at a given time before delivery with the use of ordinal logistic regression. Curves were compared by Wald tests and adjusted for possible confounders. Second-stage lengths were compared by a Cox proportional hazards model. A probability value of < .05 was considered significant.

Results

Grand multiparous women and lower parity multiparous women maintain a high station up to 1.5 hour before delivery and then rapidly transition to delivery. Nulliparous women transition to lower stations at a more gradual rate throughout the first and second stages. Descent curves differ among parity groups, with grand multiparous women maintaining a higher station for a longer time compared with either lower parity multiparous women or nulliparous women (P < .001). Once full dilation is reached, the median length of the second stage is 0.75, 0.85, and 1.75 hours for grand multiparous women, lower parity multiparous women, and nulliparous women, respectively (hazard ratios were 0.39 for nulliparous women vs grand multiparous women and 0.9 for lower parity multiparous women vs grand multiparous women).

Conclusions

Compared with lower parity multiparous women or nulliparous women, grand multiparous women maintain a higher station for a longer time before delivery but transition rapidly to delivery once full dilation is reached.

---

### Temporally organized representations of reward and risk in the human brain [^5efa3a34]. Nature Communications (2024). High credibility.

Fig. 3
Parallel versus sequential temporal organization for value-based representations.

A Uncertainty estimates around peak decoding accuracies. To compare across ROIs, differences between decoding accuracy and the median (50th percentile) of the permutation-based null distribution, respective to the decoding analysis of each ROI, is depicted. Points and the violin plots show the distribution of differential bootstrapped test accuracies against chance. Box plots depict the median (black point), interquartile range (thick bar), and 95 CI (thin bar) of the bootstrapped distribution (n = 500 bootstraps). B Decoding of expected value across contacts in each ROI, after the onset of card 1. Lines depict mean cross-validated receiver operating characteristic (ROC) area under the curve (AUC), and error bands depict SEM across folds. Horizontal red lines depict the 95th percentile of the permuted null distribution at each time point, and periods of statistical significance are shown in the shaded gray region (cluster corrected FWE < 0.05). C The timing of significant outcome decoding within each ROI follows a temporally cascading pattern. Horizontal boxes depict periods of significant decoding and vertical bars indicate the timepoint of maximum significant ROC AUC. D Outcome decoding follows a sequential temporal organization across regions. Same as (B) but for outcome decoding in the period after card 2 is presented, when the participant has all information to know if their guess was correct and consequently if they won on that trial. Lines depict mean cross-validated ROC AUC, and error bands depict SEM across folds. E Same as (A) but for outcome decoding confidence (n = 500 bootstraps). F Outcome decoding exhibits both temporal and spatial structure. Peak decoding times across ROIs follows a trend along a posterior to anterior axis as indexed by the y-plane centroid of contacts respective to each ROI. Source data are provided as a Source Data file.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^69f4e3a1]. Kidney International (2024). High credibility.

Regarding classification and risk stratification for chronic kidney disease, more specifically with respect to risk prediction, KDIGO 2024 guidelines recommend to use an externally validated risk equation to estimate the absolute risk of kidney failure in patients with CKD stages 3–5.

---

### Infant / toddler pulmonary function tests-2008 revision&update [^2af4fe72]. AARC (2008). Medium credibility.

Passive respiratory mechanics — occlusion techniques may use a single or multiple occlusion technique, and for occlusion techniques, the Hering-Breuer reflex must be invoked to elicit relaxation of the respiratory system; For the both occlusion techniques, a facemask is placed around the infant's nose and mouth; During the single occlusion technique, at least 5 tidal breath are collected and a brief occlusion takes place at end inspiration; During the single occlusion technique, compliance is measured as the change in volume divided by the change in pressure (calculated as the difference between atmospheric pressure and plateau achieved during the occlusion), and other parameters that may be measured include: resistance of the respiratory system and time constants; During the multiple occlusion technique, the airway opening is briefly occluded at different volumes above the end expiratory level, and the measured airway opening pressure and volume are recorded on x-y plots and the slope is analyzed as the compliance of the respiratory system; Modifications of the occlusion techniques include a weighted spirometry technique (very little published since the early 1990s), expiratory volume clamping and assessing compliance using the RVRTC technique from near total lung capacity.

---

### EASL clinical practice guidelines for the management of patients with decompensated cirrhosis [^ff2b2621]. Journal of Hepatology (2018). Medium credibility.

Regarding therapeutic procedures for ascites, more specifically with respect to therapeutic paracentesis, EASL 2018 guidelines recommend to perform large-volume paracentesis, when needed, in patients with AKI or SBP.

---

### Fundamental limits to learning closed-form mathematical models from data [^a345300f]. Nature Communications (2023). High credibility.

Given a finite and noisy dataset generated with a closed-form mathematical model, when is it possible to learn the true generating model from the data alone? This is the question we investigate here. We show that this model-learning problem displays a transition from a low-noise phase in which the true model can be learned, to a phase in which the observation noise is too high for the true model to be learned by any method. Both in the low-noise phase and in the high-noise phase, probabilistic model selection leads to optimal generalization to unseen data. This is in contrast to standard machine learning approaches, including artificial neural networks, which in this particular problem are limited, in the low-noise phase, by their ability to interpolate. In the transition region between the learnable and unlearnable phases, generalization is hard for all approaches including probabilistic model selection.

---

### Learning curve for total thoracoscopic segmentectomy in treating pediatric patients with congenital lung malformation [^189d71bc]. Surgical Endoscopy (2023). Medium credibility.

Background

Total thoracoscopic segmentectomy (TTS) is a technically challenging procedure in children but results in more parenchyma preservation, better pain control, better cosmetic results, and a shorter hospital stay. However, definitive data describing the learning curve of TTS has yet to be obtained. Here, we review the safety and efficiency of our initial experiences with pediatric TTS and evaluate our learning curve.

Methods

This was a retrospective study of all pediatric patients undergoing TTS between December 2016 and January 2020. Pediatric patients who underwent TTS were included, while those undergoing lobectomy or wedge resection were excluded.

Results

One hundred and twelve patients were retrospectively analyzed to evaluate the learning curve and were divided chronologically into three phases, the ascending phase (A), plateau phase (B) and descending phase (C), through cumulative summation (CUSUM) of the operative time (OT). Phases A, B, and C comprised 28, 51, and 33 cases, respectively. OT decreased significantly from phases A to B (p < 0.001) and from phase B to C (p = 0.076). No significant differences were observed in the demographic factors among the three phases. The conversion rate was zero, and the complication rate was 0.9%. Differences in technical parameters, such as length of stay and chest tube duration, were statistically insignificant between phases A and B or B and C. There were no mortalities.

Conclusion

CUSUM OT indicates that the learning curve of at least 79 cases is required for TTS in our institute. We emphasize that the learning curve should be cautiously interpreted because many factors in different institutions may influence the exact parabola and actual learning curve.

---

### 2025 ESC guidelines for the management of myocarditis and pericarditis [^b050a21b]. European Heart Journal (2025). High credibility.

Regarding nonpharmacologic interventions for myocarditis, more specifically with respect to physical activity restrictions, ESC 2025 guidelines recommend to restrict physical exercise until remission for at least 1 month in athletes and non-athletes after inflammatory myopericardial syndrome, using an individualized approach to accelerate recovery.

---

### Splenomegaly: diagnosis and management in adults [^fd18e033]. American Family Physician (2021). High credibility.

Regarding diagnostic investigations for splenomegaly, more specifically with respect to initial assessment, AAFP 2021 guidelines recommend to elicit a history of liver disease and alcohol use in patients presenting with splenomegaly in the US.

---

### Philadelphia chromosome-negative chronic myeloproliferative neoplasms: ESMO clinical practice guidelines for diagnosis, treatment and follow-up [^75dc768e]. Annals of Oncology (2015). Medium credibility.

Regarding classification and risk stratification for polycythemia vera, more specifically with respect to risk of thrombosis, ESMO 2015 guidelines recommend to categorize patients with newly diagnosed myeloproliferative neoplasm at baseline according to the risks associated with the disease.

---