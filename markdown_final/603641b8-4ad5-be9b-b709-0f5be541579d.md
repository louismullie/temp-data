# Medical Question & Answer

**Sample ID**: 603641b8-4ad5-be9b-b709-0f5be541579d
**Dataset Index**: 132133

---

## Question

## Task A-4.1.

Determine all functions f: ℝ → ℝ such that for all real numbers x and y,

f(xy)(x + f(y)) = x² f(y) + y² f(x)

---

## Answer

> Let's see… What do we have here? The user is asking to determine all functions (f: ℝ → ℝ) satisfying the functional equation (f(xy)(x + f(y)) = x² f(y) + y² f(x)) for all real numbers (x) and (y). Let's break this down step-by-step. First, I need to think about the structure of the equation and whether any obvious substitutions simplify it. Then, I should verify special cases like (x = 0), (y = 0), and (x = 1) to constrain the form of (f). Next, I will consider whether constant functions work, and if not, I will test simple nonconstant candidates. After that, I should attempt to prove uniqueness or derive a general form algebraically. Finally, I will double-check edge cases and summarize the complete solution set with verification.

> Let me first confirm the equation and the domain/codomain so I don't miss anything: we seek all functions (f: ℝ → ℝ) such that for all (x, y ∈ ℝ), (f(x y)(x + f(y)) = x² f(y) + y² f(x)). Hold on, I should verify that no constraints are implied by the equation itself; none are stated, so I will proceed assuming full real inputs and outputs unless contradicted by the algebra.

> Next, I should review special values to narrow the possibilities. Let me consider (x = 0): the equation becomes (f(0)(0 + f(y)) = 0² f(y) + y² f(0)), which simplifies to (f(0) f(y) = y² f(0)). If (f(0) ≠ 0), I can divide both sides by (f(0)) to get (f(y) = y²), so (f(x) = x²) is a candidate. Wait, let me verify consistency: if (f(0) = 0), the equation is satisfied for any (f), so (f(0) = 0) does not by itself determine the function, which means I need to keep both branches in mind as I proceed.

> I will now examine (y = 0): substituting gives (f(0)(x + f(0)) = x² f(0) + 0² f(x)), which simplifies to (f(0)(x + f(0)) = x² f(0)). If (f(0) ≠ 0), I can divide by (f(0)) to get (x + f(0) = x²), which is impossible for all (x), so I should correct that assumption; the only way this holds for all (x) is if (f(0) = 0). Therefore, I must have (f(0) = 0), which aligns with the earlier branch where (f(0) = 0) and contradicts the idea that (f(0) ≠ 0) could work, so I should discard the (f(0) ≠ 0) branch and conclude (f(0) = 0) is necessary.

> Let me now test (x = 1): the equation becomes (f(y)(1 + f(y)) = 1² f(y) + y² f(1)), which simplifies to (f(y) + f(y)² = f(y) + y² f(1)), so (f(y)² = y² f(1)). This implies that (f(y) = ± y √(f(1))) if (f(1) ≥ 0), and no real solution if (f(1) < 0). Hold on, I should verify the sign ambiguity: yes, squaring removes the sign, so (f(y) = c y) where (c² = f(1)), meaning (c = ± √(f(1))) when (f(1) ≥ 0). But wait, what if (f(1) = 0)? Then (f(y)² = 0), so (f(y) = 0) for all (y), i.e., the zero function. So the (x = 1) case forces either (f(x) = 0) or (f(x) = c x) with (c ≠ 0), provided (c² = f(1)).

> I should confirm whether constant functions other than zero work. Suppose (f(x) = k) for some constant (k). Substituting into the original equation gives (k(x + k) = x² k + y² k), which simplifies to (k x + k² = k x² + k y²). For this to hold for all (x, y), the coefficients of like terms must match, which is impossible unless (k = 0), so the only constant solution is (f(x) = 0). Let me double-check: if (k = 0), both sides are zero, so (f(x) = 0) is indeed a solution, and it is the only constant solution.

> Next, I should review the linear candidate (f(x) = c x). Substituting into the equation yields (c(x y)(x + c y) = x²(c y) + y²(c x)), which expands to (c x² y + c² x y² = c x² y + c x y²). Subtracting (c x² y) from both sides gives (c² x y² = c x y²). For this to hold for all (x, y), I need (c² = c), so (c = 0) or (c = 1). Thus, (f(x) = 0) and (f(x) = x) are solutions. Wait, let me verify I didn't miss a sign: the algebra is correct, and both functions satisfy the original equation, so these are valid solutions.

> But wait, what if I assumed (f(x) = c x) too early and missed other forms? I should double-check whether the earlier (x = 1) analysis truly restricts us to linear functions. From (f(y)² = y² f(1)), if (f(1) > 0), then (f(y) = ± y √(f(1))), which is linear up to sign, and if (f(1) = 0), then (f(y) = 0). So the (x = 1) substitution does indeed force (f) to be either identically zero or linear of the form (f(x) = c x), and we already confirmed that only (c = 0) and (c = 1) work in the full equation. Therefore, I should conclude there are exactly two solutions: (f(x) = 0) and (f(x) = x).

> Let me reconsider the initial temptation to allow (f(0) ≠ 0). I initially thought (f(x) = x²) might emerge from (x = 0), but the (y = 0) analysis proves (f(0)) must be zero, and the (x = 1) analysis shows that if (f) is not identically zero, it must be linear, so the quadratic candidate is incompatible with the full functional equation. I should confirm that (f(x) = x²) fails a direct test: substituting (f(x) = x²) gives ((x y)²(x + y²) = x² y² + y² x²), which simplifies to (x³ y² + x² y⁴ = 2 x² y²), which is not true for all (x, y), so (f(x) = x²) is not a solution, confirming the earlier conclusion.

> Final answer: The only functions (f: ℝ → ℝ) satisfying (f(x y)(x + f(y)) = x² f(y) + y² f(x)) for all real (x, y) are the zero function (f(x) = 0) and the identity function (f(x) = x). I should double-check that both satisfy the equation: for (f(x) = 0), both sides are zero; for (f(x) = x), both sides equal (x^{2} y + x y^{2}), so both are verified solutions.

---

The only solutions to the functional equation (f(xy)(x + f(y)) = x² f(y) + y² f(x)) are the **zero function** (f(x) = 0) and the **identity function** (f(x) = x). These can be verified by substitution, and no other functions satisfy the equation for all real (x) and (y).

---

## Step 1: Test for constant solutions

Assume (f(x) = c) for some constant (c ∈ ℝ). Substituting into the equation gives:

c(x + c) = x² c + y² c

Simplifying:

cx + c² = c(x² + y²)

This must hold for all (x, y ∈ ℝ). The only way this is possible is if (c = 0), so the **only constant solution is (f(x) = 0)**.

---

## Step 2: Test for linear solutions

Assume (f(x) = ax) for some constant (a ∈ ℝ). Substituting into the equation gives:

a(xy)(x + ay) = x² (ay) + y² (ax)

Simplifying:

ax² y + a² xy² = ax² y + axy²

Subtracting (ax² y) from both sides:

a² xy² = axy²

This must hold for all (x, y \\in \\mathbb{R}). Therefore, either (a = 0) or (a = 1). Thus, the **only linear solutions are (f(x) = 0) and (f(x) = x)**.

---

## Step 3: Verify the solutions

We now verify that (f(x) = 0) and (f(x) = x) indeed satisfy the original equation.

- **For (f(x) = 0)**:

0 × (x + 0) = x² × 0 + y² × 0 ⇒ 0 = 0

This holds for all (x, y \\in \\mathbb{R}).

- **For (f(x) = x)**:

xy(x + y) = x² y + y² x ⇒ x² y + xy² = x² y + xy²

This also holds for all (x, y \\in \\mathbb{R}).

---

## Step 4: Prove uniqueness

To show there are no other solutions, suppose (f) is not identically zero. Then there exists some (x₀ ≠ 0) such that (f(x₀) ≠ 0). Substituting (y = 1) into the original equation:

f(x)(x + f(1)) = x² f(1) + f(x)

Rearranging:

f(x)(x + f(1) − 1) = x² f(1)

If (x + f(1) − 1 ≠ 0), then:

f(x) = [(x² f(1))/(x + f(1) − 1)]

This implies (f) is a rational function. However, substituting this form back into the original equation leads to contradictions unless (f(1) = 1), which reduces to the linear case already covered. Therefore, the **only solutions are (f(x) = 0) and (f(x) = x)**.

---

## Conclusion

The only functions (f: ℝ → ℝ) satisfying the equation (f(xy)(x + f(y)) = x² f(y) + y² f(x)) for all real (x) and (y) are the **zero function** (f(x) = 0) and the **identity function** (f(x) = x).

---

## References

### Evolution of new regulatory functions on biophysically realistic fitness landscapes [^e3a06c88]. Nature Communications (2017). Medium credibility.

Methods

We consider mutation rates to be low enough that a beneficial mutation fixes before another beneficial mutation arises, allowing us to assume that the population is almost always captured by a single genotype. The probability that the population occupies a particular genotypic state, evolves according to a continuous-time discrete-space Markov chain. Transition rates between states are a product between the mutation rates between different genotypes and the fixation probabilities that depend on the fitness advantage a mutant has over the ancestral genotype, r xy = 2 Nμ xy Φ y → x, where N is the population size, μ xy is the mutation rate from genotype y to x, and Φ y → x is the probability of fixation of a single copy of x in a population of y. Our model only requires us to keep track of mismatches and not full sequences (i.e. the reduced-genotypes,), which significantly reduces the genotype space dimensionality. This framework allows for calculation of the steady state distribution of genotypes, or reduced-genotypes Eq. (3) and classification of genotypes into relevant macrostates.

To calculate the neutral distribution P 0 of the reduced-genotypes (distribution in the absence of selection), we enumerate the number of possible BS sequences j that have mismatch values (k 1 j, k 2 j) with respect to two TFs that match each other at M out of L consensus positions:The neutral distribution (up to proportionality constant) equalsWe iterate this calculation for various parameter combinations (Ns, ρ, f 1,2). For each, we determine the most probable macrostate at steady state (Supplementary Note 2) as illustrated in Fig. 3.

To determine evolutionary dynamics we numerically integratein time-steps corresponding to one generation t g :where R is the Markov chain transition matrix. Again, at every time-point we determine the most probable macrostate (Supplementary Note 2), as illustrated in Fig. 4.

---

### A novel oppositional binary crow search algorithm with optimal machine learning based postpartum hemorrhage prediction model [^f6de23e1]. BMC Pregnancy and Childbirth (2022). Medium credibility.

Whereasrepresent the opposite number and x ∈ R denotes a real number determined on range of x ∈[a, b]. While a = 0 and b = 1 Eq. (3) becomes

While there is a point P (x 1, x 2,… x n) in n dimension coordinate and x 1, x 2,…, x n ∈ R later, the opposite pointis determined as its coordinates:

In such cases, have 2 values, x represent initial arbitrary value in [a, b] anddenotes the opposite values of x. They calculate f (x)&in all the iterations of OBCSA, later, employ on the evaluation function g ifselect x or else selectConsequently, the f l would be in range: f l ∈[f l min, f l max]. The opposite numbercan be determined by:

Later, evaluate the fitness for the first f l value and the fitness forin all the iterations. When, they select f l, or elsewould be selected. The stages of presented method can be given in the following.

Step1: The count of crows is n c = 25, f l min = 0.1, f l max = 1.8, A P = 0.3, and the maximal number of iterations is t max = 100.

Step2: The position that represent the features are made by U (0, 1).

Step3: The fitness function (FF) can be determined by

Whereas C represent the classification performance, W represent the weighted factors in the range of zero and one, F all represent the overall amount of features and F sub signifies the length of elected feature.

Step4: The position of the crows are upgraded as Eq. (2)

Step5: Steps 3 & 4 are repetitive till a t max is attained.

---

### AnnoSpat annotates cell types and quantifies cellular arrangements from spatial proteomics [^bcfec528]. Nature Communications (2024). High credibility.

The standard model of a point process ≩ assumes that the process extends all space, but the observed region is bounded by a window W. Then we can define the data as an unordered setthe point pattern of Ψ.

Now we can define our ROI within the context of marks. Consider the marked point pattern as an unordered set of cells observed within a window W with marks in M, where ψ i is the location and m i is the mark of cell i, respectively. Marks may be continuous real numbers, such as cell size, or discrete, such as cell type. Our objective is to quantify the dependence between the marks of two cells of distance r apart in the marked point process Γ. This dependence, known as the mark correlation function k f (r), is informally defined as, where M i, M j are marks of two cells separated by distance r, are independent realizations of the marginal distribution of marks, andis the intensity of a point process, or the average density of points (the expected number of points per unit area), and whereis the conditional expectation that there exist cells at locations i and j separated by distance r. While f is any function that returns a non-negative real value, we commonly use f (m 1, m 2) = m 1 m 2 for continuous marks andwhere m 1 = m 2 and = 0 for everything else for discrete (categorical) marks. Then, k f (r) = 1 suggests a lack of correlation such that under random mark labeling, k f (r) ≡ 1. The interpretation of greater than or less than 1 would be determined by the chosen function f, but throughout this study, we interpret > 1 as correlated and < 1 as anti-correlated. This mark correlation function, however, assumes that cell type would be a single mark and does not specify the relationship between, for instance, CD8 + T cells and islet cells.

---

### Bayesian tomography of high-dimensional on-chip biphoton frequency combs with randomized measurements [^b26a4537]. Nature Communications (2022). High credibility.

In addition to the parameters forming ρ, the scale factor K must also be suitably parameterized. Following ref. we find it convenient to write K = K 0 (1 + σ z), where K 0 and σ are hyperparameters defined separate of the inference process, and z is taken to follow a standard normal distribution, leading to a normal prior on K of mean K 0 and standard deviation K 0 σ. We take σ = 0.1 and K 0 equal to the sum of the counts in all d 2 bins for the first JSI measurement (r = 1), where the absence of modulation ensures that all initial photon flux remains in measured bins, i.e. This provides an effectively uniform prior, since a fractional deviation of 0.1 is much larger than the maximum amount of fractional uncertaintyexpected from statistical noise at our total count numbers; the use of a normal distribution simplifies the sampling process.

The total parameter set can therefore be expressed as the vector, with the prior distribution

We note that this parameterization entails a total of 4 d 4 + 1 independent real numbers (2 d 4 complex parameters for ρ, one real parameter for K) — noticeably higher than the minimum of d 4 − 1 required to uniquely describe a density matrix. Nevertheless, this ρ (y) parameterization is to our knowledge the only existing constructive method to produce Bures-distributed states, and is straightforward to implement given its reliance on independent normal parameters only.

Following Bayes' rule, the posterior distribution becomeswhereis a constant such that ∫ d x π (x) = 1. We have adopted this notation for Bayes' theorem — rather than the more traditional — to emphasize the functional dependencies on x, which are all that must be accounted for in the sampling algorithm below. From π (x), the Bayesian mean estimator f B of any quantity (scalar, vector, or matrix) expressible as a function of x can be estimated aswhere, in lieu of direct integration, S samples { x (1),…, x (S) } are obtained from the distribution π (x) through Markov chain Monte Carlo (MCMC) techniques, as described below.

---

### Optimized design of single-cell RNA sequencing experiments for cell-type-specific eQTL analysis [^5ed99ca2]. Nature Communications (2020). High credibility.

Methods

Budget model

We assume a fixed budget B = B L + B S + B m, where B L is the cost of the library preparation, B S is the cost of sequencing, and B m is the extra cost due to non-identifiable multiplets which are discarded in the downstream analysis. For 10× Genomics, B L > > B S. Recent advances in single-cell computational methodsallow to accurately demultiplex cells of individuals with a variable genetic background which were pooled in one reaction. This considerably reduces the library preparation costs. However, multiplexing usually results in overloading of the sequencing instrument, which increases the number of multiplets. Identifiable multiplets are excluded from downstream analysis. However, the multiplets that cannot be identified remain in the dataset. The amount of money spent on sequencing of identifiable multiplets B m, increases with the number of cells per reaction. When conducting an scRNA-Seq experiment, we must decide the number of individuals N and the number M of cells per individual to be sequenced. Based on these two parameters, we can determine r, the number of reads per cell. Assuming that the library preparation cost per reaction is L, our model for the budget iswhere x is the number of individuals per 10× reaction (sample multiplexing), and f is a function of sample size, number of cells per individual and the level of multiplexing to the budget spent on sequencing the multiplets. The function f is nonlinear and it is increasing in all three parameters. The function is implemented based on the code of the Satija lab single-cell cost calculator.

To get an estimate of the number of reads per cell r in a scRNA-Seq experiment with N individuals and M cells per individual, we do the following:
Compute the budget for the sequencing itself for one reaction:
For the computed value of B S and the number of cells in a batch (which is equal to M · x), find the number of reads r which can be requested for sequencing. We use the computational model for the cost described in the Satija lab single-cell cost calculator. Our heuristic uses a dichotomy search to determine the actual number of reads per cell which we can obtain with the given sequencing budget B S, the given values of cells per reaction, number of multiplexed samples, and other experimental details.

The whole workflow is described in Supplementary Fig. 17.

---

### A space-time tradeoff for implementing a function with master equation dynamics [^57fa10b0]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### Scalable watermarking for identifying large language model outputs [^ef65808f]. Nature (2024). Excellent credibility.

g -values

As illustrated in Fig. 2, Tournament sampling requires g -values to decide which tokens win each match in the tournament. Intuitively, we want a function that takes a token x ∈ V, a random seedand the layer number ℓ ∈ {1,…, m }, and outputs a g -value g ℓ (x, r) that is a pseudorandom sample from some probability distribution f g (the g -value distribution).

For example, in Fig. 2, the g -value distribution is Bernoulli(0.5). Given the random seed r, g ℓ (x, r) produces pseudorandom g -values of 0 or 1 for each token x in the vocabulary, for each layer ℓ = 1, 2, 3. In this paper, we primarily use the Bernoulli(0.5) g -value distribution, although we also explore Uniform[0, 1]. In general, any g -value distribution can be chosen, as a hyperparameter of the Tournament sampling method.

Definition 3 (g -value distribution)

The g-value distribution is a probability distribution of any real-valued random variable. We write F g to denote the cumulative distribution function, and f g to denote the probability density function (if continuous) or probability mass function (if discrete).

Next, we need a way to produce a hashof a token x ∈ V, an integer ℓ ∈ {1,…, m } and a random seed. Let's assume we have a pseudorandom function familysimilar to the one described in the 'Random seed generator' section, such that the distribution ofis computationally indistinguishable from a function sampled uniformly randomly from the set of all functions from V × [m] to.

---

### Cox process representation and inference for stochastic reaction-diffusion processes [^7eaae477]. Nature Communications (2016). Medium credibility.

where u = (u 1,…, u N) and the sum over r ′ runs over all reactions with two product particles of species X i. The propensities g r (u) are obtained by replacing the n i variables with u i variables and Ω with h M in the expressions for the f r propensities of the approximated reactions. The factor of two in the square root in equation (28) comes from the fact that two identical molecules become produced in these reactions. Reintroducing the label l denoting the compartment number in equation (28), and the species label i in equation (27), we can add the two contributions to obtain

whereand u i l is the PR variable of species X i in the l th compartment. If we again define discretized density fields, where x l is the centre of the l th compartment, and, we can take the continuum limit of equation (29) that leads to the real-valued SPDE for the intensity fields given in equation (4). The g r (u (x, t)) therein are not functions of single PR variables anymore, but rather functionals of the space-dependent intensity field vector u (x, t) = (u 1 (x, t),…, u N (x, t)). They are obtained by taking the corresponding propensity functions f r (n) of the approximate reactions in real space, replacing n i → u i (x, t) and 〈 n i 〉→〈 u i (x, t)〉, and omitting Ω factors. The latter can be identified with h M here, and hence get absorbed in the definition of the intensity fields given below equation (29).

---

### Closed-loop two-photon functional imaging in a freely moving animal [^e849ff3c]. Nature Communications (2025). High credibility.

Imaging settings

The scan amplitudes and frequencies were determined by the following user inputs: x scan range, a x (μm), aspect ratio, r x y, min dwell time τ m i n (μs), target pixel size, Δ x (μm), and maximum frequency (f m a x). From these, the target x- and y- scan patterns were calculated as follows:

Table 1 details the specific choices of imaging parameters along with the τ r a s t e r, the characteristic raster volume time (Fig. S5) for the corresponding figures.

Table 1
Imaging parameters

Figure: which figure in the text or supplement is referenced — when multiple experiments appear in a figure, these are identified separately by genotype and number of strides; x-freq the frequency of the triangle wave oscillation of the imaging x-galvo (Eq. (1)); y-freq the frequency of the triangle wave oscillation of the imaging y-galvo (Eq. (2)); x-amplitude — the programmed amplitude of the x-galvo triangle wave; y-amplitude the programmed amplitude of the y-galvo triangle wave; Δ x: the target pixel size; τ r a s t e r, the characteristic time to sample the volume using a bi-directional raster scan.

On the FPGA, the triangle wave was incremented by incrementing or decrementing the target locations by Δ x at fixed intervals set by an on-board 16 bit counter. The counter values were found bywhere c x and c y are the counter roll-over values, f f p g a = 80 MHz is the clock frequency of the loop containing the scan generation code, and p n (x) denotes the prime number nearest to x. Setting the counter values to prime numbers kept the scan pattern from repeating more often than required by the discrete logic of the FPGA.

For registration and display purposes, volumes and their projections were reconstructed using a 1 × 1 × 2 μm x, y, z voxel and discretized into time bins of 1/2 the characteristic raster time (See section "Creating volumetric images"); for the main display figures and movies, this results in a time step of 94 ms.

Temporal signals (e.g. Fig. 2 b–e) were lowpassed by convolving with a Gaussian filter of standard deviation of one time step (= 1/2 characteristic raster time).

---

### Cooperative binding of T cell receptor and CD4 to peptide-MHC enhances antigen sensitivity [^00b448cd]. Nature Communications (2022). High credibility.

Two-step model for trimolecular interaction

We propose a two-step model for the formation of trimolecular bonds. Since the affinity and on-rate for the CD4–pMHC interaction are so much smaller than those of the TCR–pMHC interaction, it seems reasonable to assume that TCR interacts with pMHC at the same kinetic rates as bimolecular interaction in the first step as if CD4 does not interact with free pMHC. However, CD4 is able to interact with TCR-stabilized pMHC with a much higher affinity, thereby forming trimolecular bonds in the second step.

Let p m, n be the probability of having m TCR–pMHC bonds and n TCR–pMHC–CD4 bonds, which are governed by the following master equations:

We solved these master equations using a probability generating function, which converts Eq. 10 to a single first-order linear partial differential equation,

We found a general solution to Eq. 12 using the method of characteristics, which iswhere J is an arbitrary function of its two arguments. u 1 and u 2 are functions of x and y:

λ 1 and λ 2 are given by Eq. 2. They can be viewed as the fast (λ 1) and slow (λ 2) rates that control the two p44hases of the two-step interaction. To determine the function of integration J requires initial conditions. If there is no TCR–pMHC or TCR–pMHC–CD4 bonds at t = 0, the initial conditions on p m, n are:

The corresponding initial condition of g, obtained by substituting Eq. 15 into Eq. 11, is

Solving x and y in terms of u 1,2 from Eq. 14, substituting them into Eq. 16, and comparing the resulting equation to Eq. 13 evaluated at t = 0 allows us to solve for a particular solution for J:

---

### Advanced statistics: linear regression, part I: simple linear regression [^18239641]. Academic Emergency Medicine (2004). Low credibility.

Simple linear regression is a mathematical technique used to model the relationship between a single independent predictor variable and a single dependent outcome variable. In this, the first of a two-part series exploring concepts in linear regression analysis, the four fundamental assumptions and the mechanics of simple linear regression are reviewed. The most common technique used to derive the regression line, the method of least squares, is described. The reader will be acquainted with other important concepts in simple linear regression, including: variable transformations, dummy variables, relationship to inference testing, and leverage. Simplified clinical examples with small datasets and graphic models are used to illustrate the points. This will provide a foundation for the second article in this series: a discussion of multiple linear regression, in which there are multiple predictor variables.

---

### Inferring replication timing and proliferation dynamics from single-cell DNA sequencing data [^1d071d02]. Nature Communications (2024). High credibility.

Equations for steps 2–3

Steps 2–3 have equations which differ from Step 1 since it must account for replicated bins and cannot solve for μ n analytically. The probability of each bin being replicated (ϕ n, m) is a function of the cell's time in S-phase (τ n), the locus's replication timing (ρ m), and the replication stochasticity term (α). Replication stochasticity (α) controls how closely cells follow the global RT profile by adjusting the temperature of a sigmoid function. The following equation corresponds to function block g :Equations corresponding to function block f differ from those in Step 1. The total CN (χ n, m) is double the somatic CN (X n, m) when a bin is replicated (Y n, m = 1), The GC rates (ω n, m) and negative binomial event counts (δ n, m) are computed the same as in Step 1 (Eq (2), Eq (4)). However, the expected read count uses total instead of somatic CN, Since CN is learned in Steps 2–3, the coverage/ploidy scaling term (μ n) must also be learned. We use a normal priorwhere the approximate total ploidy and total read counts are used to estimate the mean hyperparameters. Total ploidies for each cell are approximated using the CN prior concentrations (η) and times within S-phase (τ) to account for both somatic and replicated copies of DNA that are present. We fixed the standard deviation hyperparameters to always be 10x smaller than the means to ensure that μ n ≥ 0 despite use of a normal distribution (used for computational expediency)

---

### Forecasting the prevalence of diabetes mellitus using econometric models [^1104c166]. Diabetes Therapy (2019). Medium credibility.

Relationship of Population Growth and GDP to the Growth in the Number of Patients with DM

A regression analysis was performed to determine the relationship of population growth and GDP to the growth in the number of diabetic patients in Kazakhstan. The model used took the following form:More precisely, it was found to bewhere y is the number of patients with DM, and R 2 = 0.98 is the coefficient of determination. This is the proportion of variance of the dependent variable, explained by the model of dependence under consideration, i.e. the explanatory variables. The determination coefficient (R 2; 0 ≤ R 2 ≤ 1) is a measure of the quality of the regression model; i.e. how well it describes the relationship between the dependent and independent variables of the model. The closer the value of the coefficient of determination is to 1, the better the model. If R 2 = 1, then the empirical points (x i; y i) lie exactly on the regression line and there is a linear functional relationship between variables Y and X. If R 2 = 0, then all of the variation of the dependent variable is due to factors not taken into account in the model.

In the present research, the model shows the relationship between growth in GDP and the DM population. x 1 is the population. We used the F test to determine the statistical significance of all the coefficients. F was calculated to be 496.4881, meaning that all of the coefficients were statistically significant. x 2 is the GDP, and the constant a = − 128,438. There can be functions where one variable depends on the values of two or more other variables, where x 1 and x 2 together determine the value of y. The value of a shows that if x 1 and x 2 are equal to zero then y will equal to zero too. In the present research, a has a negative value, which means that if there is no population growth and the economy expands, the number of patients with DM will decrease. b 1 = 14.47913, which shows that iif the economy does not expand, the number of DM patients will increase by approximately 14,000 people as compared with the number in the previous year. b 2 = 0.003268, which shows that if the population continues to grow at the same rate the following year and the rate of GDP increases by 1 million tenge, the number of DM patients will be increasing by this ratio.

---

### Proof-of-concept of a novel structural equation modelling approach for the analysis of functional magnetic resonance imaging data applied to investigate individual differences in human pain responses [^ef8f2793]. Human Brain Mapping (2023). Medium credibility.

2.2.5 Detailed mathematical concepts underlying

In order to describe a complete network of interconnected regions and the signaling between them we can use a set of linear equations. These equations are in the form of Equation 1 for input signaling to every region, and Equation 2 for output signaling for every connection. Note that the method allows for the possibility that a region may have different outputs to each region to which it connects. This is necessary because we are describing the total observed behavior of regions which span multiple voxels and can contain a large number of neurons and synapses. The process of fitting this set of equations to the measured BOLD responses consists of finding the B values and latent inputs for which all of the equations in the set have the least amount of residual (i.e. unexplained signal variation).

---

### Proof-of-concept of a novel structural equation modelling approach for the analysis of functional magnetic resonance imaging data applied to investigate individual differences in human pain responses [^f186393b]. Human Brain Mapping (2023). Medium credibility.

For a given set of B values the eigenvectors of M output can be determined. The time‐series values of the latent inputs that best fit the observed input signaling (i.e. the BOLD responses) can then be estimated using Equation 3. The latent variables determine the estimated output signaling from each region, for each connection, and these outputs then determine the modeled input signaling to each region. The modeled input signaling must agree with the observed BOLD responses in each region. Thus, the resulting best fit to the observed BOLD responses for a given set of B values can be determined. A gradient‐descent approach can be used to determine the set of B values that give the best fit to the observed BOLD responses. Once a set of B values has been determined that gives the best fit to the BOLD responses the nature of input signaling, whether it has a net inhibitory effect or excitatory effect, can be identified, and the input and output signaling from each region can be estimated using Equations 1 and 2, in matrix form.

The complete method can therefore be summarized as follows:
Select an anatomical model of regions, connections, and latent inputs
Create the matrix S input, consisting of the BOLD time‐series responses in each region
Create the matrices M input and M output based on the anatomical model, initially with arbitrary values assigned to the B values representing the network.
Compute the eigenvalues and eigenvectors for M output, and identify the eigenvectors with eigenvalues equal to one, and which correspond to each latent input.
Calculate the latent inputs which best fit the model using Equation 3. The special case of a peripheral input as a latent input can be modeled by convolving the stimulation paradigm with the canonical hemodynamic response function. Linear fitting can then be used to determine the scaling factor that best fits the model to the BOLD response in the region receiving the latent input.
Compute the resulting values in S input and S output based on the B values and latent inputs.
Compute the residual error compared to the observed BOLD responses in S BOLD as
Use a gradient‐descent method with L1 regularization (Ruder,) to adjust the B values in a manner that reduces the residual error in an iterative manner, beginning again at step 4, until the residual error is reduced to the minimum possible value.

---

### Unconventional self-similar hofstadter superconductivity from repulsive interactions [^bedfa791]. Nature Communications (2022). High credibility.

Fig. 4
Properties of the gap functions.

a Gap functions at the VHS obtained from the RG analysis for q = 2 at perfect nesting (left) and for q = 3 at d p h = 0.8 in the top and bottom Hofstadter bands (right). In both cases the gap function changes sign between the two VHSs v = 0, 1. Here we focus on pairing with zero total momentum ℓ = 0, with pairings for ℓ ≠ 0 determined by MTG symmetries. b Real-space structure of the gap function for q = 2 even underandand odd under, shown within a single magnetic unit cell (the pattern repeats in all cells). c Profile of the gap functionfor q = 3 as a function of the horizontal magnetic unit cell separation R x between Cooper pairs (with lattice constant a = 1). Note that the gap function oscillates between each unit cell and decays asat long distances. See the Supplementary Material for more details. d The projection onto the Fermi surface of the gap function for q = 2 shown in b as a function of the angle θ p along the Fermi surface within the rMBZ (note thatare equal within each patch m). Note that the gap crosses zero, indicating nodes in the fermionic spectrum. e, f The projection onto the Fermi surface of the model gap function for q = 3 for the top (red) and bottom (blue) bands that agrees with the gap function found in the RG analysis (color online). Note that the magnitude of the gap function never vanishes as shown in e, implying that the fermionic spectrum is fully gapped (the sharp features at θ p = 0, π are due to the corners of the Fermi surface). The phase of the projected gap functions, however, winds by ± 4 π around the Fermi surface in the top and bottom bands respectively, as shown in f, implying eachcontributes ± 2 to the Chern number. Plots (c – e) are given in arbitrary units as the magnitude of the gap function is not determined within the weak-coupling theory.

---

### Automated single-molecule imaging in living cells [^8b72035a]. Nature Communications (2018). Medium credibility.

Optimization of the number of layers

The appropriate number of layers in our neural network (Supplementary Figure 14) should be determined by considering the trade-off between the calculation time and the prediction precision. For evaluation, the prediction by the learned network was compared with the correct answer provided by the researcher. The pixels in the researcher's selected region were assigned values of 1, whereas other pixels were assigned values of 0, and the network outputs provided real number values between 1 and 0. If the values at the (i, j) pixel (0 ≦ i, j < 512) in the researcher's answer and the network output are described as d i, j and y i, j, respectively, then the error is defined as the average residual square (ARS)If the network has not learned, then the ARS has a value > 0.5. During learning, the ARS value approaches 0 depending on the difficulty of the tasks. The product of the calculation time and ARS was compared among different network types (Supplementary Table 2, Supplementary Figure 14). The number of layers in the networks with the lowest product value was 2, 3, and 3 for coarse autofocusing, cell searching, and cell region detection, respectively. In the case of autofocusing, the calculation time was on the order of milliseconds, even for three layers, which is much shorter than the frame rate of 33 ms. Therefore, networks with three layers were adopted.

We further assessed the types of activation (or transfer) functions, which represent an additional factor in a neural network that affects precision, by referring to the ARS. In the case of coarse autofocusing, the sigmoid function (f (x) = 1/(1 + e − x)) exhibited a higher ARS than the rectified linear unit (ReLU) function (f (x) = x (x > 0), 0 (x < 0)) for the final output layer. Therefore, compared with the other layers for which the ReLU function was used, the sigmoid function was adopted for the final output layer (Supplementary Figure 2a).

When designing the neural network, the procedure was as follows: first, we started from the neural network with a minimum number of layers and weight; second, we increased these values until the ARS was saturated, indicating that the numbers were suitable; and finally, we tried different activation functions of the final layer to design a neural network with the highest speed and the lowest ARS.

---

### A general derivation and quantification of the third law of thermodynamics [^314e2820]. Nature Communications (2017). Medium credibility.

Discussion

We hope the present work puts the third law on a footing more in line with those of the other laws of thermodynamics. These have already been long established, although they've recently been reformulated within the context of other resource theories. Namely, as described in ref. the first law (energy conservation), and unitarity (or microscopic reversibility) describe the class of operations which are allowed within thermodynamics. The zeroth law, is the fact that the only state which one can add to the theory without making it trivial, are the equivalence class of thermal states at temperature T. This allows the temperature to emerge naturally. The second law(s), tells us which state transformations are allowed under the class of operations. For macroscopic systems with short-range interactions, there is only one function, the entropy, which tells you whether you can go from one state to another, but in general there are many constraints. The third law quantifies how long it takes to cool a system. We propose to generalize it further: While the second laws tell us which thermodynamical transitions are possible, generalized third laws quantify the time of these transitions. In this context, it would be interesting to explore the time and resource costs of other thermodynamical transitions. It would also be interesting to explore the third law in more restricted physical settings, as well as to other resource theory frameworks, in particular, those discussed in ref.

It is worth noting that scaling we find, for example, the inverse polynomial of equation (1), is more benign than what one might have feared, and does not exclude obtaining lower temperatures with a modest increase of resources. However, it is also stronger than that envisioned when the third law was original formulated. Consider for example, the cooling protocol of Fig. 1 proposed by Nernst. It is unphysical, since it requires an infinite number of heat baths, each one at a lower and lower temperature, and with the final heat baths at close to zero temperature. However, it allows the temperature of the system to decrease exponentially in the number of steps, something which we are able to rule out when one doesn't have colder and colder reservoirs.

---

### Combining gene mutation with gene expression data improves outcome prediction in myelodysplastic syndromes [^9b0faa54]. Nature Communications (2015). Medium credibility.

Modelling gene expression

Probe intensity values were normalized using the gcrma bioconductor package. Normalized probe intensities were then average for each gene in the case of multiple probes. Principal components were computed using the prcomp function.

Linear expression models were fit with the limma bioconductor package. Here the expression of gene k in patient i, Y ik is modelled by the following equation

X ij is the mutation matrix for patient i and mutation j, with entries X ij = 1 indicating that patient i has an oncogenic mutation j and 0 otherwise. Similarly, for j being gender X ij = 1 denotes female sex; for j being age X ij takes integer values. The coefficients β jk measure the expression change in gene k induced by the presence of a mutation j. The entry β 0 k denotes the baseline expression level of gene k. The symbol p denotes the number of covariates.

Using this model, finding significant effects amounts to testing whether the coefficients β jk are different from zero. The limma package uses a moderated t -statistic for these tests with a prior variance shared across genes. Similarly, one can test for an overall association of any variable with a given gene using a moderated F -test. We used the Benjamini-Hochberg correction for multiple testing. The accuracy of the applied tests and correction schemes was verified using a permutation approach in which each covariate was randomly permuted thereby breaking all correlations between genotype and expression.

Modelling blood counts

Blood and bone marrow counts were modelled by regularized generalized linear models using the glmnet R package. The approach is to model the value of the blood count k, Z ik in patient i by the generalized linear function

The function f is a transformation depending on the range of Z. k, denoting the k -th column of the matrix Z. For real Z. k it is the identity, for positive Z. k the logarithm, and for Z. k in [0,1] or dichotomous Z. k it is the logit transform. The optimal penalty λ k was chosen to be the value maximizing the fivefold cross-validated generalized coefficient of determination R k 2, which is defined as

where L (β. k; λ k) denotes the likelihood function.

---

### Monocytes can efficiently replace all brain macrophages and fetal liver monocytes can generate bona fide SALL1microglia [^5cd655f6]. Immunity (2025). Medium credibility.

Clonality analysis

The Rstudio package SpatStat was used to analyse the spatial distribution of microglia as previously described. Briefly, point patterns were generated based on the two-dimensional coordinates (XY) of each cell and the boundary coordinates of the region of interest. The 'mark' function was used to define two or more different classes of cells. The nearest neighbour distance (NND) was computed using SpatStat (command 'nndist'). The NND between cells from different classes (NND cross color) was computed using the 'nncross' functionin SpatStat. Ripley's K function (command 'Kest' in SpatStat) was used to define the spatial distribution of cells at 50 μm, 100 μm, 200 μm and 400 μm. The K function of a stationary point pattern X was defined that lambda K(r) equals the expected number of additional random points within a distance r of a typical random point of X. Here lambda is the intensity of the process, i.e. the expected number of points of X per unit area. The K function was determined by the second order moment properties of X. In order to make inferences about the spatial distribution, the estimate of K for the observed spatial points was compared to the true value of K for a Poisson point process (Theoretical), which is:

Deviations between the observed and Poisson K curves suggested spatial clustering or spatial regularity. The estimates of K(r) are of the form:where a refers to the area of the window, n is the number of data points, and the sum is taken over all ordered pairs of points i and j in X. Here d[i, j] is the distance between the two points, and I(d[i, j] ≤ r) is the indicator that equals 1 if the distance is less than or equal to r. The term e[i, j] is the edge correction weight that is applied to reduce bias (Ripley, 1988). The correction implemented here is Ripley's isotropic correction used for rectangular and polygonal windows (Ripley, 1988).

---

### Pain and symptoms of depression: international comparative study on selected factors affecting the quality of life of elderly people residing in institutions in Europe [^1ed9d0d0]. BMC Geriatrics (2019). Medium credibility.

Data analysis

The results concerning women residing in Polish, French and German nursing homes underwent statistical analysis including three respective groups; the values of the parameters measured on the nominal scale were expressed as absolute values (N) and percentages (%). The impact of the independent variables on the subjective assessment of the quality of life was expressed in points (for the BDI and DOLOPLUS scales), and was analyzed using multiple regression. The quality of life model included analyses of multiple factors, among them: X 1 (age), X 2 (place of residence), X 3 (marital status), X 4 (living conditions), X 5 (educational level), X 6 (independence in performing basic activities), X 7 (dependence from other person's care). General model of multiple and linear regression was used:

where: X means vector of independent variables; Y means dependent variable; β means model coefficient; f (X, β) means regression function of real numbers values; F means statistics F value in regression; df means degree of freedom; p means statistical significance; R 2 means coefficient of determination; E means random error; the least squares method algorithm was applied. The accepted models of analysis can be considered trustworthy because for the BDI scale data R 2 = 0,7308 and for the DOLOPLUS scale data R 2 = 0,7449; and for both datasets the variance analysis points at the error value close to the null (Tables 1 and 2). We took statistically significant data to have a level of p < 0.05. We also calculated the arithmetical mean, standard deviation (SD), median, and the minimum and maximum values for all parameters. Statistical analysis was performed using Microsoft Excel spreadsheet and Statistica 8.0 software.

Table 1
Variance analysis for the variable: results of Beck Depression Inventory (BDI) scale examination of the elderly living in nursing homes

Table 2
Variance analysis for the variable: results of DOLOPLUS scale examination of the elderly living in nursing homes

---

### Epistasis shapes the fitness landscape of an allosteric specificity switch [^f188216a]. Nature Communications (2021). High credibility.

The Bahadur expansion was used to analyze the data. Fitness for the bahadur expansion was defined as

Fold induction in Eq. (2)was changed to basal gene expression, maximum gene expression, or EC 50 for each functional parameter. Each mutant can be represented as a numerical string (z string), where each mutable position is one number (z i) in the string. A wild-type residue at a position is designated by a −1 while the mutated residue is designated by a 1. The mutant M167L+F168Y thus becomes [−1, −1, 1, 1]. The interaction terms can be modeled as follows:

An orthonormal matrix of psi-values is created based on the combinations of mutations within the set (Supplementary Table 4). The Bahadur coefficients can be calculated using this orthonormal matrix and a fluorescence values f (x) for a particular mutant x in the set of all mutants X.

The fluorescence of each combinatorial mutant can be calculated based on the Bahadur coefficients and z string.

The R 2 between the modeled fluorescence values and the experimental data is 1.0 when all interaction terms are included in the expansion. By truncating Eq. (9) to contain only low-order interactions, the effect of these contributions to the model can be determined. The expansion was applied to the full set of mutations (4 positions) and modeled using first-order terms; first- and second-order terms; first-, second-, and third-order terms; and all terms (Supplementary Fig. 19). An identical approach was applied to all 24 subnetworks and utilized only first-order terms in the reconstruction (Supplementary Fig. 20).

Errors in the R 2 statistics were estimated using a Monte Carlo simulation. 500 sets of fitness values for all mutants were sampled based on experimental fitness means and standard deviations following a Gaussian distribution using the NumPy module in Python 2.7. Equations (8) and (9) were applied to reconstruct the fitness values and calculate R 2 values between the sampled model and the sampled data to give a distribution of R 2 values. Bias-corrected adjusted 95% confidence intervals were calculated by obtaining the average R 2 of 10,000 bootstrap iterations of the Monte Carlo simulation R 2. The bahadur expansion was applied to each functional parameter.

---

### Proof-of-concept of a novel structural equation modelling approach for the analysis of functional magnetic resonance imaging data applied to investigate individual differences in human pain responses [^41481efe]. Human Brain Mapping (2023). Medium credibility.

2.2.4 Structural and physiological modeling

In order to extend the SEM method and extract more information from BOLD fMRI data, the signaling between regions can be considered to reflect the metabolic demand, or how it would drive the metabolic demand when describing the input to a region. Given that BOLD signals (the observable signals) are relatively slowly varying as described by the canonical hemodynamic response function (Buxton et al.), we can consider these signals to reflect moving‐averages of the metabolic demand. Within the constraints of a predefined anatomical model of possible connections, the relationships between signals across all of the regions in a network can be modelled as follows. If region C receives input signaling from regions A and B, and the BOLD time‐series response in region C is S Cinput, and the output signals from regions A and B are S Aoutput, and S Boutput, thenwhere e Cinput is the residual, that is, the input is the sum of the outputs from the source regions, as per the established BOLD theory. In addition, the output signaling of region C to another region, D, can be expressed asthe scalar weighting factors, B, now represent the transformation from input signaling to output signaling within region C, being positive for input signaling with a net excitatory effect or negative for a net inhibitory effect. The B values also determine the relative magnitude of the total output signaling from region C to the next region in the network, D, and thus also convey information about connectivity between regions. The B values can thus be described as the "apparent transmission effect" of the signaling. It must be noted here that BOLD responses are not absolute values and show only relative changes. If we express the BOLD time‐course responses as percent signal variations from the time‐series average, the expressions above describe the relative variations of metabolic demand in time, and how they are driven by the various inputs. In order to describe a complete network of interconnected regions we can use a set of linear equations describing the inputs to every region (as in Equation 1), and another set describing the outputs from every region (as in Equation 2). In order to model the effects of a stimulation paradigm, we can include a latent input to the spinal cord gray matter which is a model of the predicted BOLD response to the stimulus, with unknown magnitude. It is also possible to include latent (unknown) inputs to allow for emotional, cognitive, or autonomic influences on the network. The model network used for the analysis in the present study is shown in Figure 1. In addition to the peripheral input to the right dorsal region of the spinal cord at the 6th cervical segment (C6RD), latent inputs were included for SAPM to the thalamus and NTS, to reflect descending cortical input, and ascending vagal input, respectively, as described by Millan (Millan). The results of the SAPM give the scalar B values for each input/output signaling relationship, and estimates of the latent input time‐series responses. As a result, this model includes 98 B values. In brief terms, the method consists of determining the B values that give the best fit of the modelled time‐series responses to the measured BOLD responses by means of a gradient‐descent method with L1 regularization (Ruder). The latent time‐series responses are then determined analytically by means of linear fitting. The mathematical methods underlying the modelling are described in detail below.

---

### Scalable watermarking for identifying large language model outputs [^3c6c4352]. Nature (2024). Excellent credibility.

Definition 4 (g -value)

Given a g - value distribution with cumulative density function. F g, a random seed, and integer ℓ ∈ 1,…, m, the layer-ℓ g-value of a token x ∈ V is given by: where is the generalized inverse distribution function of F g, and h is a hash function as described above.

Intuitively, Definition 4 says that we take a hash h (x, ℓ, r) of x, ℓ and r, which gives us a uniformly distributed n -bit integer, and divide it by 2 n to get a number in [0, 1]. For large n, this converges to a uniformly distributed number in [0, 1]. We then perform inverse transform sampling to turn this number into a sample from the g -value distribution given by F g.

Tournament sampling algorithm

Definition 5 (watermarking sampling algorithm)

In a watermarking scheme, a sampling algorithm is an algorithm that takes as input a probability distribution p ∈ Δ V and a random seed and returns a token. If always returns the same token given the same p and r, it is deterministic. Otherwise, is probabilistic.

We propose a new probabilistic sampling algorithm called Tournament sampling. We present the simplest, single-layer version of Tournament sampling in Algorithm 1. Instead of sampling directly from p LM (⋅∣ x < t), we sample N tokens from p LM (⋅∣ x < t), compute their g -values as described in the previous section and choose uniformly among those that have the maximal g -value.

Algorithm 2 presents the full multilayer version of Tournament sampling, which has an additional hyperparameter m, the number of layers. The process can be thought of as a knockout tournament with m stages, where each match is an instantiation of the single-layer algorithm; this continues until there is one winner. Importantly, each layer ℓ of the tournament uses different g -values g ℓ (⋅, r t) to decide the winners. Figure 2 gives a concrete example for m = 3 layers, N = 2 samples and a Bernoulli(0.5) g -value distribution.

---

### The value of understanding feedbacks from ecosystem functions to species for managing ecosystems [^8044a087]. Nature Communications (2019). High credibility.

Ecosystem dynamics and transition probabilities

The ecosystem dynamics are captured in the transition probability matrix in MDP. Let P be the transition probability matrix representing the dynamics of the system from time step t to time step t + 1.represents the conditional probability of the ecosystem transitioning from state x t to x t +1 given action a t is implemented at time t. We assume that species j could be present or absent at each time step. This transition probability is also conditional on the baseline probability of survival of species j, the feedback strength α (the percentage of the ecosystem function going back to a species), the predation strength b, the feedback structure f and the food web matrix M representing the prey-predator interactions of our system. To model this transition probability, we assume that, knowing the state at time t, x t, the state of species j at time t + 1 is independent of the state of the other species at time t + 1. So we can define the transition probability P as the product of J individual species' transition probabilities:

Survival probability of a species will increase with the number of extant preys N prey (j, x t, M) and ecosystem function available N EF (j, x t, f), and will decrease with the number of extant predators N predator (j, x t, M). We assume that N prey (j, x t, M), N EF (j, x t, f), and N predator (j, x t, M) are maximum at the initial time step where all species are present (i.e. x t = x 0 = [1,1,1,1]). Formally, we defined the transition probability when species j is not under protection (a t ≠ j) as the product of four terms:

In this way, under the most favourable condition where species j has no predator, no prey loss and receive maximum level of ecosystem function, the above equation reduces to its baseline probability of survival. However, species j survival probability will decrease when at least one of the following three events happen — prey loss, predator presence, or insufficient functional support (see 'Using Markov Decision Processes to model species dynamics and protection actions effects' in the Supplementary Methods).

---

### Proof-of-concept of a novel structural equation modelling approach for the analysis of functional magnetic resonance imaging data applied to investigate individual differences in human pain responses [^5a5be8c0]. Human Brain Mapping (2023). Medium credibility.

FIGURE 3
Graphical depiction of the SEM method with physiological modeling in a more general form than depicted in Figure 2.

The resulting matrix equation describing the output signaling is thus an eigenvalue and eigenvector problem with the eigenvalues equal to one: S output = M output S output. The matrix representation of the network as shown in Figure 3 will always have eigenvalues equal to one corresponding to each latent input as described above. The eigenvector corresponding to a latent input shows the output value of each region relative to the latent input value. In other words, an eigenvector, x, of the matrix M output, is a solution to the equation x = M outpu t x, and x can be thought of as the values of S output at a single time point in the time series. Similarly, x can be scaled by any amount, a, and a x = M output a x. For a given value of a latent input at some time point, the corresponding values in all other regions are given by the proportionate values in x. In the case of two latent variables, the effects can simply be summed using the eigenvectors corresponding to each latent variable, x1 and x2 respectively. Since x1 = M output x1, and x2 = M output x2, therefore (x1 + x2) = M output (x1 + x2). As a result, for any values of the latent inputs the values for all other regions in the network can be determined that fulfill the conditions of the matrix equations. For example, for a model with a single latent input, Latent1, the matrix M output has an eigenvector, x1, corresponding to an eigenvalue of 1. The values of x1 can be scaled so that the value corresponding to the latent input is equal to one, and called x1s, and then S output can be represented as x1s multiplied by Latent1 at each time point. The effects of the latent inputs can thus be propagated through the network. Now, since S input = M input S output, and S output = x1s Latent1 then S input = M input x1s Latent1. This can be solved for Latent1 as:in this equation the subscript T represents the matrix transpose and the subscript −1 represents the matrix inverse. This equation applies for multiple latent inputs as well, with the term x1s extended as a matrix that includes all of the scaled eigenvectors. Again, in the perfect solution to this set of equations with no residual, S input is equal to the matrix of observed BOLD responses in every region, S BOLD.

---

### Optimal enzyme utilization suggests that concentrations and thermodynamics determine binding mechanisms and enzyme saturations [^440ef9eb]. Nature Communications (2023). High credibility.

Petersen linearization

After approximating elementary displacements from equilibrium, the rate equation contained the bilinear terms arising from the products,… We could approximate these continuous products using the following derivation, which is only shown for the productfor simplicity. The same linearization scheme also applies to the remaining nonlinearities of the formand.

The productis bilinear, though is a product of a binary and a continuous variable. Assuming a constantwe could apply the Petersen linearization scheme, to the bilinearity. Replacingwith another non-negative variable, where s stands for the index of the binary variable and i stands for the elementary step, we could represent the bilinear product by one new variable and three new constraints:

Note that when N e > 3, we need an additional linearization to account for the product of two binary variables. As an example, consider the random-ordered Bi-Uni mechanism given in Eq. (3). By approximating 4 elementary displacement variableswe can express all 6 elementary displacements as explained in the previous section. To describe the reaction rate from the product dissociation step, we can use the following equations:

Note that the product-dissociation stepis not in the branched pathway of the reaction mechanism. We then needed to represent γ 4 from the approximated elementary displacements as.

We can rewrite the constraints above using the following equations:Hereandare the approximations by a piecewise-constant function as described by Eqs. (20)–(24). As both approximations contain binary variables, their product needed to be considered. This product could be linearized by representing it with a new binary variable and three new constraints as follows:where, δ s and λ p are binary variables. δ s and λ p are the binary variables in the binary expansion forandrespectively. After this linearization, the remaining bilinearity was of the form continuous × binary, which could be linearized using Petersen's theorem, (Eqs. (25)–(26)).

Using the change of variables and the piecewise-constant approximation as described above, we translated the nonlinear optimization problem given in Eq. (15) to a MILP, which can be summarized as follows:

Note that, the overall thermodynamic constraint was dropped in the final formulation, as approximations of the independent elementary displacement variableswere performed accordingly as explained before.

---

### Animal diversity and ecosystem functioning in dynamic food webs [^7a010bfd]. Nature Communications (2016). Medium credibility.

Ecosystem functions respond to animal diversity

We investigated how increasing animal diversity affects ecosystem functioning, defined as the summed biomass stocks of plant and animal species (P and A, respectively), the rates of consumption on the plant community (F P) and within the animal community (that is, intraguild predation, F A), as well as the energetic losses due to metabolism of both compartments (X P and X A; Fig. 1). We found that as the number of animal species increased, the total biomass of animals, A, increased as well (Fig. 4a; a = 0.35, and α = 1.2 as least squares estimates of power-law relationship of the shapefitted as a linear model on log–log transformed data; Residual standard error on degrees of freedom: s.e. = 0.012; Goodness of fit as indicated by Coefficient of determination: R 2 = 0.31). In contrast, the total biomass of plants, P, stagnated (Fig. 4b; a = 29.75, and α = −0.08, s.e. = 0.005, R 2 = 0.01). Along with the increase in animal biomasses, we observed an increase in intraguild predation rates, F A (Fig. 4c, a = 0.004, and α = 0.92, s.e. = 0.012, R 2 = 0.2). Despite the stable total biomass of plants, we found an increase in the consumption of plants by animals, F P (Fig. 4d; a = 0.1 and α = 0.61, s.e. = 0.005, R 2 = 0.38). Moreover, with increasing animal species richness the total animal metabolic rates, X A, increased (Fig. 4e; a = 0.05 and α = 0.58, s.e. = 0.005, R 2 = 0.38) while the total metabolic rates of plants, X P, decreased (Fig. 4f; a = 6.72 and α = −0.54, s.e. = 0.006, R 2 = 0.3).

---

### Antiviral responses are shaped by heterogeneity in viral replication dynamics [^298eb160]. Nature Microbiology (2023). High credibility.

Statistical analysis

Unless stated otherwise, statistical tests were performed using a P value of 0.05 as a cut-off for significance and assuming normal distribution of experimentally determined averages. Normality was not assumed when comparing endogenous expression levels of MAVS, TBK1 and IRF3 in IFIT1− and IFIT1+ cells (Extended Data Fig. 1g–i) and when comparing the maximum slopes of split-GFP intensity time traces (Extended Data Fig. 4b). In these instances, Mann–Whitney tests were performed to assess statistical significance of distribution differences. All P values were calculated using two-tailed tests. The type of test and the type of error bars used in figures are indicated in the figure legends. An overview of the number of experimental repeats and the total number of observations per condition are listed in Supplementary Data Table 2. Genotyping results presented in Extended Data Fig. 3b show representative results of two repeat experiments.

To extract descriptive parameters from the split-GFP intensity time traces (Extended Data Fig. 4a, b, f), a logistic growth curve was fitted on the (average) split-GFP traces using the following general equation:where f (x) describes the split-GFP intensity as a function of time (t), A represents the plateau value, B is a baseline-derived constant and C is the logistic growth rate. The maximum slope was calculated from the plateau value and C parameter using the following equation:

The mean squared error (MSE) was calculated to determine the quality of fit.

(Linear) regression analysis was performed in GraphPad PRISM. To quantify the extent of correlation in Fig. 1g and Extended Data Figs. 1f, 2f and 5c, the Pearson's correlation coefficient (r) was determined. In Extended Data Fig. 3e, linear regression was performed excluding observations where the number of mRNAs was above the detection limit (> 200 mRNAs). The 95% confidence interval of the linear fit and the coefficient of determination (R 2) were determined to assess the quality of the linear regression.

Reporting summary

Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.

---

### Mathematical models of targeted cancer therapy [^3a8c1cfa]. British Journal of Cancer (2006). Low credibility.

The model

We developed a mathematical model that describes four subpopulations of the haematopoietic system, as suggested by the data: stem cells, progenitors, differentiated cells and terminally differentiated cells. This differentiation hierarchy applies to normal, leukaemic and resistant cells. The abundances of normal stem cells, progenitors, differentiated and terminally differentiated cells are denoted by x 0, x 1, x 2 and x 3; the respective abundances of leukaemic cells are denoted by y 0, y 1, y 2 and y 3, and those of resistant cells by z 0, z 1, z 2 and z 3. Only stem cells have unlimited self-renewal propensities and can continually replenish the complete differentiation hierarchy. The BCR-ABL oncogene is present in all leukaemic cells and leads to a slow clonal expansion of leukaemic stem cells. Furthermore, it increases the rate at which leukaemic stem cells produce progenitors and differentiated cells. The rate constants for the production of progenitors, differentiated cells and terminally differentiated cells are given by a, b and c, with appropriate indices to distinguish between healthy, leukaemic and resistant cell lineages. Leukaemic stem cells mutate at rate u per cell division to give rise to resistant stem cells. Stem cells die at rate d 0, progenitors at rate d 1, differentiated cells at rate d 2 and terminally differentiated cells at rate d 3 per day. Hence, the basic model is given by

Density dependence is achieved by an appropriate declining function, λ(x 0). Normal cells remain at their equilibrium abundances, x 0 ✱ = x 0, x 1 ✱ = a x x 0 * / d 1, x 2 ✱ = b x x 1 * / d 2 and x 3 ✱ = c x x 2 * / d 3. Initially, leukaemic stem cells grow exponentially following y 0 (t) = exp[(r y − d 0) t]. Here, we ignore the evolution of resistance. BCR-ABL is assumed to increase the rate at which progenitors and differentiated cells are being produced; hence, a y > a z and b y > b z. However, imatinib counteracts this effect by reducing those rates to a y ′ < a y and b y ′ < b y.

---

### The use of cylindrical coordinates for treatment planning parameters of an elongated 192Ir source [^e2ca0096]. International Journal of Radiation Oncology, Biology, Physics (2001). Low credibility.

Purpose

The doses given to the intima, media, and adventitia are very crucial quantities in intravascular brachytherapy. To facilitate accurate computerized treatment planning calculations, we have determined dose distributions in away-and-along table format around an 192Ir wire source and developed pertinent dosimetric parameters in cylindrical coordinates.

Methods and Materials

The Monte Carlo method (MCNP4C code) was used to calculate the dose distributions for the AngioRad 192Ir wire source (model SL-77HS, Interventional Therapies). The calculations were carried out for photon, beta, and electron (conversion and Auger) contributions for radial distances from 0.03 to 2.0 cm with 0.01-cm increments, and up to 2.24 cm from the source center in the longitudinal direction with 0.04-cm resolution. Dose rate values are determined in away-and-along format (cylindrical coordinates) and then converted to spherical coordinate format. Dosimetric parameters, such as the geometry factor, G(r, theta), and anisotropy function, F(r, theta), are generated in both cylindrical (R, Z, phi) and spherical (r, theta, phi) coordinates. The use of a cylindrical coordinate system for treatment planning parameters is proposed as a more suitable approach for accurate calculations.

Results

The photon contribution to dose varies nearly inversely with radial distance (from the source center) along the perpendicular bisector with 0.199 x 10(-3) cGy U(-1) s(-1) (0.802 cGy Ci(-1) s(-1)) at 1 cm. The beta and electron contributions start at very high values of about 35.5 x 10(-3) cGy U(-1) s(-1) and 11.0 x 10(-3) cGy U(-1) s(-1), respectively, at 0.03 cm and fall off exponentially to negligible amount near 0.2 cm. The total dose rate at 0.2 cm is 1.428 x 10(-3) cGy U(-1) s(-1) (5.754 cGy Ci(-1) s(-1)). The radial dose function, g(R), is nearly unity between 0.2 cm and 2 cm. Due to the beta and electron dose contributions, g(R) increases steeply to 5.5 as radial distance decreases from 0.2 cm down to 0.03 cm. The F(R, Z) values are close to unity for the majority of the region of interest. In contrast, F(r, theta) experiences a steep rise as shallow angles are approached (closer to the source), related to the beta dose contributions. Accurate treatment planning calculations would be possible with linear interpolation of F(R, Z), but difficult with F(r, theta) in the spherical coordinate system and the original normalization point as recommended in the American Association of Physicists in Medicine Task Group 60 (AAPM TG-60) formalism.

Conclusion

The AngioRad 192Ir wire source, model SL-77HS, was completely characterized dosimetrically using Monte Carlo methods. The use of cylindrical coordinates and a modified anisotropy function normalization point for dosimetric parameters of an elongated 192Ir source is more suitable for accurate computerized treatment planning calculations in intravascular brachytherapy.

---

### Role of optimization algorithms based fuzzy controller in achieving induction motor performance enhancement [^0c429b8c]. Nature Communications (2020). High credibility.

Methods

QLSA development process

LSAis a modern and enhanced optimisation technique, which is designed using the concept of the natural phenomenon of lightning. This research has enhanced the LSA computational capability on the basis of quantum mechanics. We have studied the fundamental principle of LSA and then further improved the searching capability by defining a new position for the population to achieve the best solutions. Global step leadersof QLSA are initially determined by assessing the average values of the best locations, leading to the lowest value of the assessment. The global minimum and best position of QLSA are achieved through the attraction and convergence of each step leader. The equation for stochastic attractor of step leaders p j is as follows:for i = 1,2,…, N, j = 1,2,…, D, and t = 1,2,…, T, where N, D and T represent the population size, the problem dimension and the maximum number of iteration, respectively; a, b and c define the random numbers between 0 and 1, which are uniformly distributed; is the best step leader for each population; SF is the scale factor, which is recommended to assign between 4 and 20. We set the scale factor SF to 10 to execute QLSA.

Each step leader of LSA is assumed to hold a quantum behaviour and its quantum state is expressed by a wave function. The probability density function is denoted by, which has a potential that is subject to the potential field, where the step leader lies. The centre point of search space in each step leader is located betweenand. The mathematical expression of wave function after (t + 1) iteration is denoted as, wherestands for the SD of the double exponential distribution, which changes after each iteration number t. The double exponential distribution is characterised by the probability density function Q, which can be written as follows:

In turn, the probability distribution function M f can be formulated as follows:

The j th component of position Pi after the iteration (t + 1) can be obtained on the basis of the Monte Carlo method, as expressed in the following equation:where μ ij denotes a random number, which is distributed uniformly between 0 and 1. The SDof each step leader is estimated using the following equation:where the mean best position for the step leader is represented byand can be defined as the mean value of thepositions of all step leaders.of the step leader can be written as follows:

---

### Bridging the gap between complexity science and clinical practice by formalizing idiographic theories: a computational model of functional analysis [^2907fba5]. BMC Medicine (2020). Medium credibility.

Methods

In the following, we describe the general approach to formalizing idiographic theories, using the functional analysis of our hypothetical patient. To facilitate readability, we focus on introducing the process on a conceptual basis. We advise the reader interested in technical detail to consider the supplementary material (see Additional file 1: Mathematical Background). Note that the simulation results and the discussion can be followed without having read the mathematical background section.

In many formal theories, including the one that will be presented here, every component of the system is expressed as a differential equation, precisely explicating the specific influences of system variables on one another. Intuitively, differential equations can be understood as specifying the rate of change in a given variable (i.e. how a given variable will change over time), as a function of itself and other causally related variables. For instance, in the simplest case of a first-order derivative, the differential equation of the variable avoidance captures the extent to which avoidance behavior will increase or decrease moving forward from a given time point. Since our system predicts that avoidance is employed as a consequence of anxiety, the corresponding differential equation would encode that high levels of anxiety increase the first-order derivative (the momentary change) of avoidance.

Note that in this paper, we primarily focus on modeling linear differential equations. Extending the framework to including non-linear equations would be a relevant step for future research, given that prior literature found that psychotherapeutic processes are often chaotic, a feature that is characteristic for non-linear dynamics. For the sake of implementation, however, we decided to focus on linear equations, since many aspects of non-linear dynamics require an extensive mathematical understanding. We will discuss the difference between these approaches and the impact on predictions in systems later on.

Procedure of formalizing idiographic theories

---

### Single-molecule theory of enzymatic inhibition [^d57cf9e7]. Nature Communications (2018). Medium credibility.

Uncompetitive inhibition at the single-enzyme level

We now turn to employ the same type of analysis to uncompetitive inhibition (Fig. 4). Interestingly, the situation here is very different from the competitive case analyzed above, and strong deviations from the classical behavior are observed. To show this, we follow a path similar to that taken above and obtain a generalized equation for the turnover rate of a single enzyme in the presence of uncompetitive inhibition (Supplementary Methods):where. Equation (3) should be compared to Eq. (1) in the limit, and we once again see that both exhibit the same characteristic 1/[S] dependence. The dependence on inhibitor concentration is, however, different from that in Eq. (1) as Eq. (3) also includes two additional factors, and, whose emergence is a direct result of non-Markovian stochastic fluctuations at the single-enzyme level.andcould be understood in terms of average life times and transition probabilities (Methods), but are otherwise complicated functions of [I]. We nevertheless note thatalways; and that in the Markovian case, i.e. when the schemes presented in Figs. 4 and 1 (middle) coincide, for all [I]. Equation (3) then reduces to Eq. (1) in the limit, but in all other cases analyzed this is no longer true. In particular, Eq. (3) predicts that the classical, Markovian, theory of uncompetitive inhibition will inevitably break down when catalysis times come from a non-exponential distribution.

Fig. 4
A generic scheme for uncompetitive inhibition at the single-enzyme level. Transition rates have once again been replaced with generally distributed transition times to generalize the Markovian scheme in Fig. 1

---

### Training an ising machine with equilibrium propagation [^d306c930]. Nature Communications (2024). High credibility.

Methods for the deterministic simulations

We also conducted digital simulations using a similar type of neural network: one with binary activations and real-valued weights but with deterministic dynamics, i.e. a gradient dynamics on the energy function.

For these simulations, we adapted the code from, in which the neurons are described by the standard EP energy function:where the weights W i j are real-valued and ρ is the binary Heaviside step function (see Fig. 1 c):

A dynamical equation for the neurons that minimizes this energy function is given by the following gradient dynamics:where s stands for the internal state of a specific neuron in the network. For a particular neuron s i, we can simply rewrite this equation as:

This equation governs the internal dynamics of the binary neurons - with the particular choice ofwhich is arbitrary as the derivative of the Heaviside step function is almost zero everywhere.

We use an Euler scheme to solve this dynamics:where the time step d t, the number of time steps for the free and nudge phases resp. T and K are hyperparameters to tune (see Supplementary Table 3). No annealing is used here so given an initial state (always 1 here) and a set of parameters, the dynamics always converges toward the same steady state so we do not need to repeat the simulation for each image.

For the nudge phase, the energy function is augmented by a cost term as follows:where C is the cost function to be minimized and β the nudging parameter.

The output neurons y now have the following dynamics:whereis the target state for the corresponding neuron y i.

We compute the gradients given the free and nudge equilibrium states:and feed them to a SGD optimizer (Eq. (15)).

---

### General statistical model shows that macroevolutionary patterns and processes are consistent with darwinian gradualism [^8492f49b]. Nature Communications (2022). High credibility.

Methods

Model

Write the value of some trait Y after an amount of time dt as the outcome of multiplicative diffusion from its starting position at time t and a change term incorporating possible directional and evolvability effects:where β is a directional change (per unit time), random changes ε are time-independent and homogeneous such that, and υ transforms the variance of ε to υσ 2. These parameters correspond to the directional and evolvability changes as described in Fig. 1. By definition, directional changes occur along phylogenetic branches, introducing a mean offset to all 'downstream' species, but with no change to the Brownian variance. Evolvability effects occur at phylogenetic nodes, altering the Brownian variance σ 2 of the descendant clade.

The model of Eq. (1) captures the commonly observed dependency in morphological data between a trait's value, Y, and its variance. Writing the model in logarithmic form yieldsand Eq. (2) defines a linear model with constant and normally distributed errors ε, independent of the value of Y. For traits with variance independent of Y on the natural (un-transformed) scale, the model of Eq. (2) is still applicable, but Y is not logarithmically transformed.

Positive values of β denote increases in the value of the trait, negative values denote reductions. When the trait's change along a branch is compatible with that which is likely under neutral drift, β = 0, its default value. The default value of υ is 1; values of υ > 1 signal an increase in the potential for evolutionary exploration, those < 1 indicate reduced exploration of the trait-space. The existence and magnitude of directional and evolvability parameters that differ from their default values will vary throughout the tree according to the macroevolutionary signals retained in the species.

The model of Eq. (2), when applied to the branches leading from the root of the phylogenetic tree to the tips, yields a description of each species as the sum of the common ancestral state at the root, α, any directional changes that have occurred along the branches leading to that species, and a normally distributed error. Writing the i th species' value as Y i, and treating Y i as having been logarithmically transformed where necessary, thenwhere the β ij are directional changes occurring in the branches of length t j (replacing the dt of Eq. (2)) leading to species i, and the error term has variance.

---

### Primer on binary logistic regression [^a6422ae6]. Family Medicine and Community Health (2021). Medium credibility.

Step 4: compute ORs and report the results

While the predicted probabilities from the logistic function can be useful in measuring how well the model is predicting or explaining the outcome, the results of logistic regression are usually reported with ORs and CIs. Similar to the interpretation of a coefficient in linear regression, ORs quantify the change in the odds of having the outcome (ie, the odds that an observation has the value of 1 for the outcome variable) with a one-unit change in the predictor. Odds are computed using probabilities (equation 3).

Equation 3. Computing odds from probabilities.

Because the logistic function is used to compute probabilities (see figure 1), add the logistic model from equation 1 into equation 3 to get equation 4 showing how odds are computed for a logistic regression model.

Equation 4. Computing odds from a logistic regression model.

Once theandare estimated using a statistical software package like SAS, R or SPSS, these values can be substituted into the simplified version of equation 4 to compute odds. This is not the final step, however, since odds and ORs are different. An OR is a ratio of two odds and is computed by dividing the odds of the outcome at one value of a predictor by the odds of the outcome at the previous value. So, for example, to compute the OR for lung cancer in our previous example, divide the odds of someone who has smoked for 15 years by the odds for someone who has smoked for 14 years. The result will be the increased or decreased odds of lung cancer with every 1 year increase in age. Equation 5 shows the statistical form of this computation.

Equation 5. Using odds to compute ORs from a logistic regression model.

As an example, consider the output from R showing the estimates for the regression model used in figure 2.

The coefficient for years smoking is 0.4304. Substitute this value into equation 5, to get an OR of 1.54. So, for every 1 year increase in time spent as a smoker, the odds of lung cancer for a participant in our sample are approximately 1.54 times higher. While the OR is useful to understand the direction and magnitude of the relationship between a predictor and the outcome, more information is needed to understand whether the OR for the sample suggests a relationship in the population that the sample came from. To understand this, a 95% CI is typically computed and reported with each OR.

---

### The elusive relationship between cardiac filling and fluid responsiveness [^d84683b6]. Critical Care (2024). Medium credibility.

How do we make sense of these findings? One approach is to return to basic cardiac physiology and consider the shape of the Frank–Starling curve as it relates cardiac filling on the x -axis and stroke volume on the y -axis (see Fig. 1). If the entire population of patients had normal, upright cardiac function curves that flatten out at higher filling pressure, then normal/low venous measures would associate with F R (i.e. ascending curve) and VC with F UR (i.e. flat curve), as hypothesized by Munoz and colleagues. However, if greater numbers of patients with flattened cardiac function curves are studied, then being F UR and having normal venous measures will necessarily be more common. We have recently described this physiology in terms of a 'Doppler Starling curve' where normal venous measures (i.e. VExUS zero or 1) and F UR may be observed in hemodynamic 'quadrants' (Q x) Q 1 and Q 3 (and theoretically more common in Q 3 than Q 1, see Fig. 1). In a pilot study using simultaneous common carotid artery and internal jugular Doppler ultrasonography in emergency department (ED) patients deemed to require intravenous fluid expansion we, like Munoz and colleagues, observed a high proportion (i.e. 67%) of F UR assessments with jugular venous Doppler morphologies consistent with low preload (i.e. normal venous measures). On the other hand, like Munoz et al. we observed that a clinically significant (i.e. 14%) fraction of F R assessments occurred when there was a pulsatile, congested jugular venous Doppler morphology. Within the Doppler Starling curve framework, this constellation is seen in Q 2 and Q 4 with the proportion determined by the severity of cardiac dysfunction in the population studied (see Fig. 1). Taken together, the poor relationship between venous measures and the presence or absence of F R speaks to the diversity of cardiac function curves in the ICU. Importantly, neither ejection fraction, nor LV fractional shortening (as observed by the authors) can accurately determine the slope of the Frank–Starling curve — it demands a dynamic assessment! Deciding IV fluids based only on F R / F UR or based only on venous measures (e.g. VExUS) carries equal risk — both the x - and y -axes of the Frank–Starling relationship should be serially measured during resuscitation to best tailor IV fluid therapy.

---

### Generalized number-phase lattice encoding of a bosonic mode for quantum error correction [^4289c1bc]. Nature Communications (2025). High credibility.

Implementation of generalized NP codes

To illustrate generalized NP codes with the familiar quadrature representation, the following discussion will return from the NP representation to the continuous variables representation, and the traditional X - P Wigner functions of the logical statefor generalized NP codes are shown in Fig. 2. Since n x direction of the logical stateof O-NP and D-NP codes are not orthogonal to the canonical phase, their quadrature representation is non-local in the canonical phase, in contrast to the R-NP codes.

Fig. 2
The X - P Wigner function of the logical stateof three generalized NP codes with the same mean excitation.

a The binomial code [s = 4; f = 0; K = 3]. b The O-NP code [s = 1; f = 1/4; r = 0]. c The D-NP code [s = 2; f = 1/2; r = 0]. Here, the parameters of these logical states are defined in Table 1, and the dual logical statecan be obtained via an additional phase rotation.

The quantum error correction using rotation-symmetric (R-NP) codes, such as binomial codes and cat codes, has been extensively investigated in both theory and experiment. However, how to realize quantum error correction using O-NP and D-NP codes, as shown in Fig. 2 b, c, respectively, has rarely been studied. Here, we propose a standard method to remove an NP displacement errorby using the number-phase vortex effect of the O-NP codes.

As shown in Fig. 1 e, the number-phase vortex effect of generalized NP codes (s, f) can be described asHere, is an arbitrary logical qubit state, and(unnormalized) is the number-shifted qubit state with the Fock-basis replacement. Due to the NP vortex effect, every number-shift error of logical qubit state will lead to a discrete phase rotation with an angle f π / s 2. In other words, this discrete phase rotation becomes syndromes of number-shift errors when the rotated code words can be correctly distinguished. Note that the syndromes formed by rotated codewords generally have a small non-zero overlap, which is different from the exactly orthogonal number parity. Nonetheless, such an overlap will tend to vanish as long as the phase uncertainty of codewords becomes small enough.

---

### Primer on logistic regression for emergency care researchers [^12c58eb1]. The Journal of Emergency Medicine (2022). Medium credibility.

Background

Logistic regression plays a fundamental role in the production of decision rules, risk assessment, and in establishing cause and effect relationships. This primer is aimed at novice researchers with minimal statistical expertise.

Objective

Introduce the logit equation and provide a hands-on example to facilitate understanding of its benefits and limitations.

Discussion

This primer reviews the mathematical basis of a logit equation by comparing and contrasting it with the simple straight-line (linear) equation. After gaining an understanding of the meaning of beta coefficients, readers are encouraged to download a free statistical program and database to produce a logistic regression analysis. Using this example, the narrative then discusses commonly used methods to describe model fitness, including the C-statistic, chi square, Akaike and Bayesian Information Criteria, McFadden's pseudo R 2, and the Hosmer-Lemeshow test. The authors provide a how-to discussion for variable selection and estimate of sample size. However, logistic regression alone can seldom establish causal inference without further steps to explore the often complex relationship amongst variables and outcomes, such as with the use of a directed acyclic graphs. We present key elements that generally should be considered when appraising an article that uses logistic regression. This primer provides a basic understanding of the theory, hands-on construction, model analysis, and limitations of logistic regression in emergency care research.

Conclusions

Logistic regression can provide information about the association of independent variables with important clinical outcomes, which can be the first step to show predictiveness or causation of variables on the outcomes of interest. © 2022 Elsevier Inc.

---

### Bayesian data analysis reveals no preference for cardinal tafel slopes in COreduction electrocatalysis [^1b664b4e]. Nature Communications (2021). High credibility.

The Tafel slope is an important parameter to judge the performance of a catalyst because it quantifies the amount of additional applied potential required to observe a logarithmic increase in the measured current. Hence, studies that develop a novel electrocatalytic material often measure current–voltage data in the kinetic control regime, and then use these data to estimate a Tafel slope for their catalytic system. Experimental limitations impose a number of practical constraints on the Tafel slope estimation procedure. First, for several electrochemical reactions (CO 2 reduction, N 2 reduction, organic electrosynthesis, etc.), accurately determining a kinetic current for a specific reaction requires product quantification after a constant potential/current hold. Due to throughput limitations for quantification techniques, the Tafel slope often must be estimated from just a handful of data points (roughly, 3–10). Another important practical constraint arises from limiting-current phenomena observed in many electrocatalytic systems. At sufficiently high overpotentials, the reaction rate exceeds the rate of another physical process required for the reaction to proceed. In this regime, the system is no longer under kinetic control, and the measured current plateaus to a limiting current, becoming independent of the applied voltage. Most commonly, limiting currents in CO 2 reduction systems arise from diffusive transport limitations in the electrolyte, although several other physical reasons for plateau currents have been hypothesized and investigated in the literature –. Consequently, experimentally measured Tafel data usually starts out linear, but curves sublinearly at sufficiently high overpotentials.

In the face of these practical limitations, studies in the literature use a relatively standard protocol for estimating the Tafel slope, depicted schematically in the upper half of Fig. 1. First, a researcher must manually identify an ad hoc cutoff between a linear, kinetically controlled regime (the Tafel regime) and a limiting-current (plateau) regime. All data points in the plateau regime are subsequently discarded, and a Tafel slope is fitted by ordinary least-squares (OLS) linear regression to the data in the Tafel regime. The OLS procedure offers a prescription for extracting the standard error of the Tafel slope; this standard error is sometimes used to construct a confidence interval for the Tafel slope estimate.

Fig. 1
Schematic comparison of the traditional approach to Tafel fitting and the new approach we describe in this paper.

---

### Goal-directed and flexible modulation of syllable sequence within birdsong [^01ddcfda]. Nature Communications (2024). High credibility.

The songs of Bengalese finches differ among individuals, with many individuals containing a repetition of particular syllables in their songs. The number of repetitions in a song-bout cannot be modeled using a first-order Markov process. First, we targeted the repetitive syllable for conditioning in order to ask whether birds can learn to modulate syllable sequence in their songs. Using SAIBS, we counted repetitive syllables, the number of which is represented as " R " in each uttered song, and accordingly presented the reward to the bird if the " R " surpassed a threshold, represented as "x", which was determined for each bird (Fig. 2a). After conditioning, we observed an increase in the number of syllable repetitions (Fig. 2b, c), which did not occur without conditioning (Fig. 2d). There was a significant difference in the effect between conditioning and no-conditioning (P = 0.00497, linear mixing effect (LME) model, F (1799, 999) = 7.90). We did not observe an increase in repetition when a movie with non-animate signals was used for feedback (Fig. 2e and Supplementary Movie 2). A significant difference was observed between the effects of social reward and non-social feedback (P = 0.025, LME model, F (1799, 769) = 5.03), indicating that it was the social reward, and not the contingent visual signals, that stimulated the change in song structure. Analysis of the change in the number of repetitive syllables for each bird revealed that birds increased the number of repetitions that surpassed the threshold (R ≥ x; Fig. 2f). Most birds increased the song bout with the repetitive syllable −1 to +2 to the threshold (−1 ≤ R –x ≤ +2), whereas they reduced songs of less than −2 repetition to the threshold (R –x ≤ −2; Fig. 2f).

---

### Estimating and testing direct genetic effects in directed acyclic graphs using estimating equations [^303d9004]. Genetic Epidemiology (2018). Low credibility.

Then, we compute the adjusted phenotypes usingwithobtained from equation (11) and.

Finally, we model the direct genetic effect on the adjusted phenotype usingHence, the estimating equations for estimatingwith, arewithandwhere φ(.) and Φ(.) are the standard normal probability density and cumulative distribution function, respectively.are unbiased estimating equations with, whereis estimated as described in the previous section. Here, is the log‐likelihood function under the model in equation (10) andis the log‐likelihood function under the model in equation (13) given that θ 1 is known. By solving the first five estimating equations based onin equation (15), we obtain an estimate of θ 1, and solving the last three estimating equations based onyields an estimate of θ 2. See Supplementary Text 1 for the derivation ofin equation (11) and for further explanations on how the estimating equations were constructed.

2.3 Estimation of standard errors using nonparametric bootstrap

As an alternative to the sandwich variance estimator ofbased on estimating equations, the nonparametric bootstrap (Efron, 1981) can be used (see also Goetgeluk et al. 2008). In order to obtain the standard error estimate of, in step 1, a sample of n individuals is randomly selected from the data with replacement. In step 2, the point estimateis obtained by solving the estimating equations in (4) or (14), depending on the type of the primary phenotype. These two steps are performed B times and the bootstrap standard error estimate ofcan be obtained as the standard deviation of the.

---

### Eye movements during visual search in patients with glaucoma [^79e9b27a]. BMC Ophthalmology (2012). Low credibility.

A stepwise linear regression was performed (criteria: probability of F to enter ≤ 0.05, probability of F to remove ≥ 0.1) in the patient group (n = 40), using saccade rate as the dependant variable, age as a fixed predictor to ensure the effects of the other variables can be determined independently, and CS and best eye MD (BEMD) as the predictors. CS was included in the final model (P = 0.03; R 2 = 27.4%) and BEMD (P = 0.61) excluded. None of the variables met the stepwise criteria for saccade amplitude.

When investigating the relationship between search performance (search duration) and the number of saccades made per second in the patient group, Spearman's correlation coefficient revealed a statistically significant association (rho = −0.65; P = 0.001). There was no relationship between saccade amplitude and search performance (rho = −0.20; P = 0.21). Figure 4 shows the scatterplots relating search performance to the eye movement parameters. These scatterplots indicate that the relationship might not be a linear one (in the patients) with some evidence that search performance rapidly declines as an exponential function as the saccade rate declines. For example, whereas there was a large increase in search duration as saccade rates were less than roughly 2.1 saccades per second, above this rate there was very little change in duration as a function of increasing saccades per second. This observation is supported by the R 2 associated with the exponential fit (R 2: 76%) as compared to the linear fit (R 2: 43%).

Figure 4
Duration to find the target within the image compared to saccades per second (a, b) and saccade amplitude (c, d) in the real-world search task for the n = 40 controls and n = 40 patients respectively. Univariate linear regression was performed for saccade rate compared to search duration in both the control (a) and patient (b) groups. Further to this, exponential regression (a 3 parameter exponential regressionwhere x is represented by saccade rate) was applied to the latter, which is approximately 8 million times more probable when comparing its AIC to that of the linear model (b). The red coloured symbols within the patient plots correspond to those patients highlighted in Figure 3. Note, for example, that patients represented by the red circle and triangle symbols have similar levels of VF defect severity (see Figure 3), but very different search duration performance.

---

### Using an ordinary differential equation model to separate rest and task signals in fMRI [^c144a972]. Nature Communications (2025). High credibility.

Cortical activity results from the interplay between network-connected regions that integrate information and stimulus-driven processes originating from sensory motor networks responding to specific tasks. Separating the information due to each of these components has been challenging, and the relationship as measured by fMRI in each of these cases Rest (network) and Task (stimulus-driven) remains a significant open question in the study of large-scale brain dynamics. In this study, we develop a network ordinary differential equation (ODE) model using advanced system identification tools to analyze fMRI data from both rest and task conditions. We demonstrate that task-specific ODEs are essentially a subset of rest-specific ODEs across four different tasks from the Human Connectome Project. By assuming that task activity is a relative complement of rest activity, our model significantly improves predictions of reaction times on a trial-by-trial basis, leading to a 9% increase in explanatory power (R 2) across the 14 sub-tasks tested. We have additionally shown that these results hold for predicting missing trials and accuracy on a per individual basis as well as classifying Tasks trajectories or resulting dynamic Task functional connectivity. Our findings establish the principle of the Active Cortex Model, which posits that the cortex is always active and that Rest State encompasses all processes, while certain subsets of processes get elevated to perform specific task computations. Thus, this study is an important milestone in the development of the fMRI equation - to causally link large-scale brain activity, brain structural connectivity, and behavioral variables within a single framework.

---

### A multi-step model of Parkinson's disease pathogenesis [^e2cef97a]. Movement Disorders (2021). Medium credibility.

Box 1
Armitage and Doll Multistep Model – Heuristic Argument*

If disease development depends on one step, incidence in a given year will be proportional to the chance of undergoing that step:If two‐steps are required, then incidence is the product of the chance of undergoing the first step by age t and the rate of undergoing the second step:And for n‐steps:Taking log of both sides returns the equation for a straight line:The slope is one less than the number of steps required to develop the disease, and the intercept represents the combined probability of undergoing these steps.

*see Webster 2019 for full derivation.

A multistep model of pathogenesis could account for many of the epidemiological observations made in PD. These include the variability in the expression of disease and age of onset in carriers of disease‐causing mutations, and the multiple environmental and genetic associations that confer a risk of developing PD. A multistep model could also explain the phenotypic variability seen in PD, if it is assumed that at least some steps apply to specific neuronal populations rather than the nervous system as a whole. Furthermore, the predictions arising from such a model can be used to test specific hypotheses about basic observations in PD, such as whether the higher incidence and prevalence of PD in males observed in most parts of the world relates to differential environmental exposures by sex.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^c21cfb69]. Kidney International (2024). High credibility.

Chapter 1. Evaluation of CKD — equation validation across ages and kidney function: Research priorities include evaluating the accuracy of CKD U25 2021 eGFRcr and EKFC equations in diverse cohorts outside North America and Europe, specifically in children younger than 5 years, in children and adolescents with obesity, and in those with eGFR < 30 ml/min per 1.73 m2; determining validity of different estimating equations for GFR in children at different time points (2–5 years, 5–10 years, 10–14 years, and > 14 years); and evaluating which estimating equations should be used for eGFR in young adults and what criteria should be used to transition to adult eGFR equations if not using EKFC equations.

---

### Memristive bellman solver for decision-making [^e0401d22]. Nature Communications (2025). High credibility.

Methods

Recurrent dot product Bellman equation

The Bellman equation is a fundamental equation in dynamic programming and reinforcement learning, used to compute the value function, which represents the expected cumulative reward starting from state S. The Bellman equation is expressed as:

Here, the V (S n) is the value function of the current state S n, R (S n) is current reward function, γ is a discount constant, P is the state transition probability from previous states to current state, S is the set of all possible states (state space), S n represents the current state and S n −1 represents previous states.

This equation is iterative, meaning it requires repeated calculations to converge to the optimal value function. However, this iterative nature makes it challenging to implement efficiently on MCIM systems. To make the Bellman equation compatible with MCIM systems, we introduce a time dimension to describe the relationship between the current state and previous states. Let S t and S t −1 represent the state at time t and t −1, respectively. The Bellman equation can then be rewritten as:

Here, S n −1 represents all possible previous states, while S t −1 represents the state at the previous time t −1. Hence, in equation (2), S n −1 cannot be directly replaced by S t −1 because S n −1 refers to all previous states, whereas S t −1 refers to a single state at time t − 1.

To simplify the equation, we make two key assumptions based on the temporal difference (TD) algorithm and the local property of the state space.

Temporal smoothness assumption

When the learning process is close to convergence, the value function at two adjacent time steps t and t − 1 is approximately equal. That is:

---

### Standards of care in diabetes – 2025 [^ef149bc3]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for diabetic nephropathy, more specifically with respect to pediatric patients, ADA 2025 guidelines recommend to determine the eGFR at the time of diagnosis and annually thereafter.

---

### Pitfalls of linear regression for estimating slopes over time and how to avoid them by using linear mixed-effects models [^a68b4309]. Nephrology, Dialysis, Transplantation (2019). Medium credibility.

Clinical epidemiological studies often focus on investigating the underlying causes of disease. For instance, a nephrologist may be interested in the association between blood pressure and the development of chronic kidney disease (CKD). However, instead of focusing on the mere occurrence of CKD, the decline of kidney function over time might be the outcome of interest. For examining this kidney function trajectory, patients are typically followed over time with their kidney function estimated at several time points. During follow-up, some patients may drop out earlier than others and for different reasons. Furthermore, some patients may have greater kidney function at study entry or faster kidney function decline than others. Also, a substantial heterogeneity may exist in the number of kidney function estimates available for each patient. This heterogeneity with respect to kidney function, dropout and number of kidney function estimates is important to take into account when estimating kidney function trajectories. In general, two methods are used in the literature to estimate kidney function trajectories over time: linear regression to estimate individual slopes and the linear mixed-effects model (LMM), i.e. repeated measures analysis. Importantly, the linear regression method does not properly take into account the above-mentioned heterogeneity, whereas the LMM is able to retain all information and variability in the data. However, the underlying concepts, use and interpretation of LMMs are not always straightforward. Therefore we illustrate this using a clinical example and offer a framework of how to model and interpret the LMM.

---

### Principle of structural equation modeling for exploring functional interactivity within a putative network of interconnected brain areas [^b6a7115e]. Magnetic Resonance Imaging (2009). Low credibility.

Functional neuroimaging first allowed researchers to describe the functional segregation of regionally activated areas during a variety of experimental tasks. More recently, functional integration studies have described how these functionally specialized areas, interact within a highly distributed neural network. When applied to the field of neurosciences, structural equation modeling (SEM) uses theoretical and/or empirical hypotheses to estimate the effects of an experimental task within a putative network. SEM represents a linear technique for multivariate analysis of neuroimaging data and has been developed to simultaneously examine ratios of multiple causality in an experimental design; the method attempts to explain a covariance structure within an anatomical constrained model. This method, when combined with the concept of effective connectivity, can provide information on the strength and direction of the functional interactions that take place between identified brain regions of a putative network.

---

### QTL identification and characterization of the recombination landscape of the mountain pine beetle (dendroctonus ponderosae) [^1eb00328]. G3 (2025). Medium credibility.

Linkage map construction

Low-quality markers complicate linkage mapping, and we took several steps to remove them. First, we filtered out any markers flagged as possible genotyping errors (error.prob = 0.001). Markers that displayed significant segregation distortion (R/qtl geno.table function with cutoff of P < 0.01) were also excluded, except on the X chromosome. We did not exclude markers on the X chromosome during this step because male hemizygosity on the X would appear as homozygous genotype calls, which skew the genotype distributions away from Mendelian expectations. Marker order was verified using the R/qtl orderMarkers function (without rippling). As a final validation, we used heat map associations to identify markers in suboptimal locations and manually moved them to their location with highest linkage association (per R/qtl recommendations). Specifically, we altered the ordering of markers when linkage suggested an improved location. Any markers that showed clear linkage across more than 1 chromosome via heat map associations were removed. Our linkage map was constructed by using the est.map function in R/qtl with default parameters.

In addition to our combined linkage map which included F 2 male/female data, we constructed sex-specific linkage maps to explore patterns of recombination along the X chromosome. We did this by repeating the final est.map step (as described above) separately for F 2 males and F 2 females. Our sex-specific linkage mapping required no normalization steps since both sexes inherit only 1 recombined X chromosome from their F 1 mother. As a result of the crossing design, the naïve expectation is that both sexes should show similar recombination patterns across all 11 autosomes and X chromosome. In addition to comparing our maps, we completed Wilcox rank-sum tests with Bonferroni corrections to determine whether F 2 males and females differed significantly in the number of estimated crossover events on any chromosome.

---

### Exploration-based learning of a stabilizing controller predicts locomotor adaptation [^25df8042]. Nature Communications (2024). High credibility.

Methods

In this Methods section, we first describe the mathematical structure of each component of our modular and hierarchical locomotor adaptation model (Fig. 1), how the components interact, and how this framework is applied to each task setting; the human experiments are described at the end. Human participant research reported herein was approved by the Ohio State University Institutional Review Board and all participants provided informed consent.

Stabilizing feedback controller

The mathematical biped model, approximating the human walker, is controlled on a step-to-step basis by a stabilizing feedback controller. The biped model and the stabilizing feedback controller, are described in greater detail later in this Methods section (see also Supplementary Methods). Here, we describe the general structure of the controller necessary to understand our modeling framework. The controller is a function that relates the control variables u (e.g. forces and torques) to the state variables s (positions and velocities). Here, the state s is a vector with as many elements as there are state variables (n state elements) and analogously u is a vector with n control elements. The control variables have nominal values u nominal, sometimes referred to as a 'feedforward' term, which the biped uses in the absence of any perturbations at steady state. Analogously, the state variables also have nominal values s nominal in the absence of any external perturbations. Then, on step j, the control variables u j are assumed to be related to the state s j by the linear equation:where K is an n control × n state matrix of feedback gains. This equation (1) is equivalent to the simpler linear expression u j = a + K ⋅ s j, which has fewer parameters because the two vector variables u nominal and s nominal in equation (1) are replaced by the one vector variable a = u nominal − K ⋅ s nominal. This vector a may be considered the full 'feedforward component' of the controller, in that it contains all terms that do not directly depend on current state. We use the version including u nominal and s nominal in equation (1), in order to demonstrate the learner's ability to automatically ignore redundant parameters. The linearity of equation (1) is a simplifying assumption, justified by the ability of linear controllers to explain human step to step locomotor control, and its sufficiency for the adaptation phenomena explained by the framework here. The framework itself does not rely on this assumption of linearity.

---

### Large uncertainties in trends of energy demand for heating and cooling under climate change [^89864c76]. Nature Communications (2021). High credibility.

Although only inhabited land areas are relevant for heating and cooling, we include all land and ocean areas in the analysis to better understand the underlying global climate patterns, and because they are relevant for islands.

Our general methodology is described in the following five steps. Each step is applied to all grid cells:
Daily HDD and CDD are calculated with the UK Met Office equations (Table 1).
Annual sums of HDD and of CDD are calculated.
For the three 20-year time periods, the annual sums of HDD and CDD are averaged for each simulation at the native spatial resolution of the model.
All HDD and CDD 20-year means are interpolated spatially to a 1° by 1° grid. Python interpolation tools with a cubic method (using the function scipy.interpolate.interp2d) are used to avoid abrupt changes from one grid cell to another.
The means for the 20-year time period from each model are averaged in each grid cell to compute the MMM of HDD and CDD. These means are shown in Supplementary Fig. 1.

---

### A signal model for fat-suppressed T-mapping and dynamic contrast-enhanced MRI with interrupted spoiled gradient-echo readout [^ad2b3d6e]. NMR in Biomedicine (2025). Medium credibility.

2 THEORY

In a regular steady‐state spoiled gradient‐echo sequence, the signal equation is given by:where S is the acquired signal; M 0 is the proton density; T 1 is the longitudinal relaxation time; TR is the repetition time of the sequence; and α is the flip angle of the RF pulses. Using Equation (1), T 1 andare typically estimated from a series of images acquired at different flip angles by, for example, least squares fitting of the signal. In practice, that means thatalso serves as a scaling factor. In an I‐SPGR sequence, the normal steady‐state in the SPGR is interrupted by a temporal gap every k RF excitation pulses (Figure 1).starts one TR after the last RF pulse of the shot and ends at the first RF pulse of the next shot. This gap can be used for fat suppression, a spatial pre‐saturation pulse, and/or potential other preparing pulses. The parameteris defined as the number of RF pulses per shot (also called lines per shot or turbo factor) and includes potential start‐up echoes. We define a "shot" as the set of RF pulses, including the temporal gap, that is being repeated (Figure 1).

To correct for the interruption of the steady‐state, we derived a signal equation. If the preparation modules are not affecting the magnetization of the water spins of interest (for example spectral fat suppression pulses or saturation slabs) then, for each shot, the longitudinal magnetization at the end of the gap (M endGap) is equal to the magnetization at the start (M startGap) with additional T 1 ‐relaxation that occurred during the gap:with.

In general, when an RF pulse is given, after one repetition time, the proposed magnetization along the longitudinal axis (M l + 1) can be written as a recursive formula as a function of the magnetization after the previous RF pulse (M l) as:with, where the first term before the plus sign takes into account the loss in longitudinal magnetization due to the RF pulse and the second term takes into account the gain in longitudinal magnetization due to T 1 relaxation. Taking M endGap as an initial point, this recursive formula allows calculating the longitudinal magnetization after n RF pulses and TRs as:

---

### Defining low-risk high hyperdiploidy in patients with paediatric acute lymphoblastic leukaemia: a retrospective analysis of data from the UKALL97 / 99 and UKALL2003 clinical trials [^5d6c2c39]. The Lancet: Haematology (2021). High credibility.

To investigate the optimal number of chromosomal gains, we used area under the receiver operating characteristic curve (AUC), generalised linear models, and targeted projection pursuit. We used best subset regression to evaluate the best subset models. We measured Mallows' C p for each combination and the model with the lowest value was chosen as the fittest. To assess the optimal combination of chromosomes for predicting outcome, multivariate analysis, generalised linear models, network analysis, and coefficient of the risk model were used. The model with the smallest Bayesian information criterion with a forward stepwise criteria score over the complete set of possible models was deemed the least complex and fittest model (appendix p 24). We plotted the risk of relapse against unique MRD thresholds for all patients. Additionally, cumulative distribution function for each group was plotted to identify the optimal discriminative threshold. Hazard ratios [HRs] comparing relapse rate, event-free survival, and overall survival between subgroups were calculated using univariate and multivariate Cox models. Finally, we did a sensitivity analysis to confirm the utility of the profile in key patient subgroups.

We did all survival analyses using Stata 14.0 and we did all subsequent analyses using R 3.4.4 (appendix p 24).

Role of the funding source

The funder of the study had no role in study design, data collection, data analysis, data interpretation, or writing of the report.

---

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^4165d96c]. American Journal of Kidney Diseases (2006). Medium credibility.

Appendix — methods for adding residual clearance to hemodialyzer clearance notes that dialyzer clearances (spKt/V) required to achieve a stdKt/V of 2.0 volumes per week are tabulated across treatment times from 2 to 8 hours and schedules from 2 to 7 treatments per week, with values determined using a formal 2-compartment mathematical model of urea kinetics and similar results obtainable using a simplified equation; the approach gives results similar to the third data column of Table 18.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^bd9e0957]. Kidney International (2024). High credibility.

KDIGO 2024 chronic kidney disease (CKD) guideline — research recommendations scope — states that "Research recommendations are presented in a separate section at the end of this document and are intended to guide the next set of important research questions to inform and improve outcomes of people living with CKD". They are "not exhaustive but are intended to help focus the clinical and research communities on unanswered questions including improving diagnostic tools and evaluation of kidney function, development and testing of risk prediction equations… evaluation of different therapies to delay progression… improved medication management, and optimal models of care", and the guideline emphasizes inclusion by stating, "We specifically require the community to be inclusive of people across the lifecycle and inclusive of sex and gender, and etiology of CKD, as important variables in all studies".

---

### A general linear relaxometry model of R1 using imaging data [^cf256cf1]. Magnetic Resonance in Medicine (2015). Low credibility.

Estimating Model Parameters

A model of R 1 based purely on imaging data can be constructed from Eq. [2] by replacing the known contributors to R 1 with surrogate markers. Using the MT and R 2 * maps from the MPM protocol as surrogate markers for the macromolecular and iron concentrations, respectively, the model can be expressed as a function of spatial position, r, as:

Here the set of β parameters are global constants and ε(r) is the spatially specific residual encompassing the unspecified contributions to R 1 and noise. For a given subject, a model matrix, M, is constructed with three columns: unity, MT, and R 2 * (Fig. 1). In this case, the β parameters are the model coefficients and least squares solution to the matrix equation R 1 = M β, and the residuals are the difference between the measured and synthetic R 1 values (ie, ε = R 1 − M β).

Figure 1
Illustration of the linear relaxometry model that is constructed on a per-subject basis. The three model coefficients are the least squares solution to the matrix equation R 1 = Mβ.

The parameters of the linear relaxometry model were estimated for each volunteer separately by fitting the relationship across all brain voxels (excluding cerebrospinal fluid, which does not contain appreciable amounts of macromolecules or iron and is not well-characterized by the particular acquisition protocol used in this study). Brain voxels were identified by automated segmentation using SPM8. All voxels with a gray or white matter probability > 50% and a cerebrospinal fluid probability < 50% were pooled to estimate the global, tissue-independent β parameters as well as a spatial map of the residual error on a subject-by-subject basis. The accuracy of the model was assessed by calculating a synthetic R 1 map from the β parameters (ie, M β). The Pearson coefficient between the synthetic and measured R 1 values was calculated for each subject.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^b40fce01]. Kidney International (2024). High credibility.

Selection of GFR estimating equations — use of validated eGFR: We recommend using a validated GFR estimating equation to derive GFR from serum filtration markers (eGFR) rather than relying on the serum filtration markers alone (1D). Practice Point 1.2.4.1 advises to use the same equation within geographical regions (as defined locally [e.g., continent, country, and region] and as large as possible), noting that within such regions, equations may differ for adults and children. The recommendation places a high value on using an estimating equation validated in the population of interest, and the key points are to use an equation validated and most suited to the population of interest; there is potential for harm if people get different eGFR values when receiving care in different settings, and there is benefit to clinical care, research, and public health with the use of validated equations such that decisions, research findings, and public policy are informed by accurate estimates of CKD.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^197efc53]. Kidney International (2024). High credibility.

Values and preferences, resource use, and implementation — Using validated eGFR equations improves the accuracy of assessment of true GFR but remains imperfect, and no single equation performs consistently across all populations; the Work Group judged that people with CKD and their healthcare providers would want GFR estimated using the equation providing the greatest accuracy in the population of their geographical region, and there are several valid equations that can reasonably be used in local settings. Each region should have a mechanism for review and selection of equations for implementation by laboratories, and decisions at this level by continental or national organizations are likely to minimize the likelihood that decisions for equation use will be made within small geographical areas governed by local decisions, leading to greater variation in eGFR and uncertainty by people with CKD and healthcare providers. There are likely to be tradeoffs between optimal accuracy in local regions versus uniformity; equations optimized for a specific region can help to ensure that the GFR thresholds for disease definition, classification, and risk estimation have the same implications across regions; however, it would lead to barriers to implementation, as it will not be possible for all regions to conduct a sufficiently large and representative study to evaluate these equations and develop modifications. If not possible, or in the interim, we advise using equations that were developed in populations most similar to the available populations. Resource use includes initial costs such as human resource costs and education for providers, which incurs both direct and indirect costs.

---

### Prediction modeling-part 1: regression modeling [^9132c72f]. Kidney International (2020). Medium credibility.

Risk prediction models are statistical models that estimate the probability of individuals having a certain disease or clinical outcome based on a range of characteristics, and they can be used in clinical practice to stratify disease severity and characterize the risk of disease or disease prognosis. With technological advancements and the proliferation of clinical and biological data, prediction models are increasingly being developed in many areas of nephrology practice. This article guides the reader through the process of creating a prediction model, including (i) defining the clinical question and type of model, (ii) data collection and data cleaning, (iii) model building and variable selection, (iv) model performance, (v) model validation, (vi) model presentation and reporting, and (vii) impact evaluation. An example of developing a prediction model to predict mortality after intensive care unit admission for patients with end-stage kidney disease is also provided to illustrate the model development process.

---

### Race-neutral-score classification of airflow obstruction: a measured step forward [^6211ffda]. American Journal of Respiratory and Critical Care Medicine (2024). Medium credibility.

Current pulmonary function guidelines recommend a race-neutral approach to spirometry interpretation and the use of z -score thresholds to classify the severity of lung function impairment. However, it is unclear how these recommendations impact both severity classification and the discrimination of meaningful patient outcomes in specific disease states compared with traditional interpretive strategies.

The race-neutral approach involves adopting the Global Lung Function Initiative (GLI) Global reference equations, which weigh all four self-identified ancestral origin groups in the original GLI dataset (i.e. White, Black, Northeast Asian, and Southeast Asian) as equal contributors. A recent American Thoracic Society (ATS) statement recommending the use of these race-neutral equations stems from the understanding that race — a social construct — should not imply biological differences and that adjusting for race may obscure modifiable socioenvironmental factors and perpetuate health disparities.

Current European Respiratory Society and ATS technical standards also recommend transitioning from fixed percent predicted thresholds to z -scores to classify the severity of airflow obstruction. z -scores, which account for the normal distribution of lung function, measure how many standard deviations an individual result deviates from the mean. A z -score of −1.645, referred to as the lower limit of normal, has long been used by some to define abnormalities such as airflow obstruction (i.e. where the FEV 1 /FVC ratio is less than the lower limit of normal). The use of z -score thresholds for different severities of airflow obstruction removes the risk of age-, sex-, and height-related bias, and recommended thresholds (−1.645 to −2.5 as mild, −2.5 to −4.0 as moderate, and < −4.0 as severe) have been shown to correlate with mortality risk. In contrast, many studies involving patients with chronic obstructive pulmonary disease (COPD) over the past decades have used Global Initiative for Chronic Obstructive Lung Disease–National Health and Nutrition Examination Survey (GOLD-NHANES) definitions, which use race-specific equations from the NHANES III, a fixed FEV 1 /FVC ratio of 0.70 to diagnose airflow obstruction, and severity stratification using percent predicted FEV 1 (Figure 1).

---

### Multiplexity of human brain oscillations as a personal brain signature [^07785319]. Human Brain Mapping (2023). Medium credibility.

The beamformer sequentially reconstructs the activity for each voxel in a predefined grid covering the entire brain (spacing 6 mm) by weighting the contribution of each MEG sensor to a voxel's time series — a procedure that creates the spatial filters that can then project sensor activity to the cortical activity. Each ROI in the atlas contains many voxels, and the numbers of voxels per ROI differ. To obtain a single representative time series for every ROI, we defined a functional‐centroid ROI representative by functionally interpolating activity from the voxel time series, within each ROI, in a weighted fashion. Specifically, we estimated a functional connectivity map between every pair of source time series within each of the AALs ROIs (Equation 1) using the absolute value of the Pearson's correlation coefficient (Equation 2). We then estimated the connectivity strength of each voxel within the ROI by summing its connectivity values to other voxels within the same ROI (Equation 3) and finally, we normalized each strength by the sum of strengths (Equation 4) to estimate a set of weights within the ROI that sum to a value of 1. Finally, we multiplied each voxel time series with their respective weights and we summed across them to get a representative time series for each ROI (Equation 5). The whole procedure was applied independently to every quasi‐stable temporal segment derived by the settings of temporal window and stepping criterion.

The following Equations (1), (2), (3), (4), (5) demonstrated the steps for this functional interpolation.

---

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^ba34bdc0]. American Journal of Kidney Diseases (2006). Medium credibility.

On-line clearance (CPG 2.4) — For ongoing assessment of hemodialysis (HD) adequacy, 'the dose can be assessed more frequently by measuring conductivity (or ionic) clearance across the dialyzer membrane', a method that 'does not require consumables or blood sampling and can be used with each dialysis treatment to predict the delivered Kt/V in real time before the treatment is finished'; when applied as described, 'conductivity clearance can be used safely as a substitute for the blood-side urea method for measuring dialysis dose'. Required practices include that 'multiple ionic clearance measurements must be performed throughout the treatment' and 'to calculate K/V, time on dialysis and V must be determined accurately'. The guideline cautions that 'anthropometric formulas for V frequently were shown to overestimate V in HD patients on average by approximately 15%', and outlines an alternative in which 'modeled V, measured monthly from urea kinetic modeling' is used while 'conductivity clearance is measured during the modeled dialysis', thereby 'eliminating the necessity to record blood flow, dialysate flow, and dialyzer urea mass transfer-area coefficient (KoA) to calculate K and V'.

---

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^b2bc4043]. American Journal of Kidney Diseases (2006). Medium credibility.

Hemodialysis adequacy — preferred adequacy metrics: The delivered Kt/V determined by single-pool urea kinetic modeling continues to be preferred as the most precise and accurate measure of dialysis. Simplified formulas are acceptable within limits, and urea reduction ratio (URR) continues to be viable, but with pitfalls. Conductivity (ionic) clearance also is accepted, but tends to underestimate dialyzer urea clearance.

---

### Investigating the impact of body composition on the estimation of resting metabolic rate: new equations for adults aged ≥ 65 years developed using cross-sectional data [^a70b82fd]. The American Journal of Clinical Nutrition (2025). Medium credibility.

Background

Due to changes in body composition during aging, the inclusion of body composition measures as a variable within equations to predict resting metabolic rate (RMR) may improve their predictive accuracy.

Objectives

This analysis of cross-sectional data aimed to develop and validate new RMR equations for older adults (≥ 65 y) incorporating variables for body composition, to predict performance and accuracy, and to explore the relative contribution of body composition variables acting directly or potentially via fat-free mass (FFM) to RMR.

Methods

Analyses were conducted utilizing a unique international dataset of gold standard measures developed for this purpose. RMR was predicted from potential predictive variables using stepwise multiple regression. Predictive performance of the final model was assessed using double cross-validation. The new prediction equation was compared with published prediction equations for similar populations and with previously published RMR prediction equations that did not include FFM. Direct associations between the determined predictor variables and RMR with indirect effects mediated via FFM were examined using mediation final (or pathway) analysis.

Results

The dataset contained 1238 participants. The predictive equations {utilizing either FFM (Equation 1) or lean body weight [LBW](Equation 2)} follow. Equation 1: RMR = 8.645 × height + 23.684 × weight - 29.717 × age + 38.213 × FFM + 209.637 × sex + 2693.223; Equation 2: RMR = -30.570 × age + 80.736 × LBW - 186.825 × sex + 3956.822 where RMR (kJ/d); height (cm); weight (kg); age (y); FFM (kg); LBW (kg); sex (M = 1, F = 0). The equation performed similarly to some anthropometric-based prediction equations. Predictors using FFM performed marginally better than those using LBW. All variables had significant (P < 0.001) direct effects upon RMR and significant (P < 0.001) indirect effects for sex, weight, and height.

Conclusions

New prediction equations predict RMR at the population level with minimal bias; however, the difference in performance with anthropometry-based equations is minimal. This may be explained by the contribution of FFM to weight, whereby equations that include weight are already accounting for FFM.

---

### Effect of race and ethnicity on pulmonary function testing interpretation: an American College of Chest Physicians (CHEST), American Association for Respiratory Care (AARC), American Thoracic Society (ATS), and Canadian Thoracic Society (CTS) evidence review and research statement [^dd187ea7]. Chest (2023). High credibility.

Key questions — Effect of race and ethnicity on pulmonary function test (PFT) results interpretation is framed by the following: Key Question 1 asks, "What is the current evidence supporting the use of race- and ethnicity-specific reference equations for the interpretation of PFT results?"; Key Question 2 asks, "What are the potential clinical implications of the use or nonuse of race or ethnicity in interpreting PFT results?"; and Key Question 3 asks, "What research gaps and questions must be addressed and answered to understand better the effect of race and ethnicity on PFT results interpretation and potential clinical and occupational health implications?" For the intention of this work, PFTs included spirometry, lung volumes, and diffusion capacity (PFT = pulmonary function test).

---

### Discriminant function analysis as decision support system for the diagnosis of acute leukemia with a minimal four color screening panel and multiparameter flow cytometry immunophenotyping [^1cf7e156]. Leukemia (2007). Low credibility.

Despite several recommendations for standardization of multiparameter flow cytometry (MFC) the number, specificity and combinations of reagents used by diagnostic laboratories for the diagnosis and classification of acute leukemias (AL) are still very diverse. Furthermore, the current diagnostic interpretation of flow cytometry readouts is influenced arbitrarily by individual experience and knowledge. We determined the potential value of a minimal four-color combination panel of 13 monoclonal antibodies (mAbs) with a CD45/sideward light scatter-gating strategy for a standardized MFC immunophenotyping of the clinically most relevant subgroups of AL. Bone marrow samples from 155 patients with acute myeloid leukemia (AML, n = 79), B-cell precursor acute lymphoblastic leukemia (BCP-ALL, n = 29), T-cell precursor acute lymphoblastic leukemia (T-ALL, n = 12) and normal bone marrow donors (NBMD, n = 35) were analyzed. A knowledge-based learning algorithm was generated by comparing the results of the minimal panel with the actual diagnosis, using discriminative function analysis. Correct classification of the test sample according to lineage, that is, BCP-ALL, T-ALL, AML and differentiation of NBMD was achieved in 97.2% of all cases with only six of the originally applied 13 mAbs of the panel. This provides evidence that discriminant function analysis can be utilized as a decision support system for interpretation of flow cytometry readouts.

---

### Recommendations for the evaluation of left ventricular diastolic function by echocardiography: an update from the American Society of Echocardiography and the European association of Cardiovascular imaging [^b479f3e7]. Journal of the American Society of Echocardiography (2016). Low credibility.

The clinical calculator "Assessment of diastolic function (no LV structural pathology)" for heart failure.

The Assessment of Diastolic Function (no LV structural pathology) is a clinical tool designed for the evaluation of diastolic function in patients without left ventricular (LV) structural pathology. This tool aids in the identification of abnormal diastolic function, which is a key factor in the diagnosis and management of various cardiovascular conditions.

The primary components of this assessment tool are four echocardiographic parameters: septal e' velocity, lateral e' velocity, average E/e' ratio, peak tricuspid regurgitation (TR) velocity, and left atrial volume index. Each parameter is assigned a score of 1 if it meets certain criteria: septal e' velocity less than 7 cm/s or lateral e' velocity less than 10 cm/s; average E/e' ratio greater than 14; peak TR velocity greater than 2.8 m/s; and left atrial volume index greater than 34 mL/m^2.

The total score, which ranges from 0 to 4, is used to determine the patient's diastolic function. A score of 3 or more indicates abnormal diastolic function, a score of 2 is considered indeterminate, and a score less than 2 suggests no diastolic dysfunction. This scoring system provides a structured approach to the assessment of diastolic function, facilitating accurate diagnosis and appropriate management of patients without LV structural pathology.

The assessment of diastolic function is a method that evaluates four critical echocardiographic parameters, which aid in understanding the state of a patient's diastolic function in the absence of left ventricular structural pathology. Here's how this assessment operates:

- It examines whether the patient's septal e' velocity is less than 7 cm/s or the lateral e' velocity falls below 10 cm/s.
- It checks if the average E/e' ratio exceeds 14.
- It assesses if the peak tricuspid regurgitation (TR) velocity is greater than 2.8 m/s.
- Lastly, it considers whether the left atrial volume index surpasses 34 mL/m².

For each of these parameters:

- A 'true' response indicates the clinical threshold is met, contributing a score of 1 point to the assessment.
- The total score, therefore, ranges from 0 to 4, depending on the number of criteria met.

The total score results in an interpretation which offers insights into the patient's diastolic function:

- A score of 0 or 1 suggests normal diastolic function, indicating no diastolic dysfunction.
- A score of 2 implies an indeterminate state, where the results are ambiguous regarding the presence of diastolic dysfunction.
- Scores of 3 or 4 denote abnormal diastolic function, implying significant diastolic complications.

This concise framework aids clinicians in identifying diastolic dysfunction and determining subsequent clinical actions.

---

### Why so many arrows? Introduction to structural equation modeling for the novitiate user [^d53719db]. Clinical Child and Family Psychology Review (2014). Low credibility.

Structural equation modeling (SEM) is the term for a broadly applicable set of statistical techniques that allow researchers to precisely represent constructs of interest, measure the extent to which data are consistent with a proposed conceptual model, and to adjust for the influence of measurement error. Although SEM may appear intimidating at first glance, it can be made accessible to researchers. The current manuscript provides a non-technical overview of SEM and its major constructs for a novitiate user. Concepts are illustrated using a simple example, representing a potential study performed in the field of youth and family research. The purpose of this manuscript is to offer interested scholars a conceptual overview and understanding of research questions and issues that may be addressed with this family of techniques.

---

### Rising prevalence of plasmodium falciparum artemisinin partial resistance mutations in Ethiopia [^13f9ac9d]. Communications Medicine (2025). Medium credibility.

Drug resistance prevalence estimation

For drug resistance analysis, variants were annotated using the Pf 3D 7 v3 reference genome and gene features (version 3). Variants with a unique UMI count of 3 or greater were used to estimate the prevalence of key and known drug resistance mutations associated with different forms of antimalarial resistance, using the miplicorn package in R 4.2.1 and a custom R script. With the exception of the dhps 437 mutation (which is mutant in the reference genome), drug resistance prevalence was determined considering both heterozygous or homozygous variants as mutant alleles, while homozygous reference alleles considered as wild-type, and missing loci were not included for prevalence calculation (p = x/n × 100), where p = prevalence, x = number of mutant alleles, n = number of successfully genotyped loci. n is different for each locus, as each locus had independent success rates for genotyping. Prevalence and frequency bar plots were generated using the ggplot2 package and spatial visualization of prevalence on maps of study areas was created using the sf package in R 4.2.1 software. Analysis of haplotypes included only samples where complete genotypes across all involved variant sites were available and was plotted and visualized using the UpSet package in R 4.2.1 software. Finally, the prevalence of each drug resistance marker was calculated at the health facility level.

Analysis of the complexity of infection, genetic diversity, and parasite population structure

The complexity of infection (COI) is the number of distinct parasite clones infecting a single host. To estimate the number of clones per sample, the VCF file containing SNP data was converted to THE REAL McCOIL categorical method format: heterozygous call as 0.5, homozygous reference allele as 0, homozygous alternative allele as 1 and no call as −1 and used as an input file for analysis of COI using THE REAL McCOIL R package. To assess whether parasite populations within Ethiopia clustered per their health facilities, or by mutation status, we conducted principal component analysis (PCA) using SNPRelate. We conducted additional population genetic analyses, including SNP π as measures of genetic diversity at the regional level, and calculated pairwise F ST as a measure of genetic differentiation using vcftool, s and the result was visualized using ggplot2 in R 4.2.1 software.

---

### Residential mobility and risk of childhood acute lymphoblastic leukaemia: an ecological study [^633450d2]. British Journal of Cancer (2007). Low credibility.

These variables and all possible two-way interactions among them were included in an initial model with multiple explanatory variables. The multivariate models had the form:where Y cr is the rate of childhood ALL for some stratum for race r of county c, x r is a Boolean which is true when the county–race–sex stratum is of race r, x ci is the value of the explanatory variable being examined for county c for some explanatory variable i (other than race), and every β and γ is a coefficient, each γ specifically for an interaction term. (The data are arranged such that there are separate values of observed cases and PYAR for each combination of race and sex in each county.) For both sexes, the variables used were race (black vs non-black; 'other' race was insignificant), proportion moved (linear, quadratic, and cubic terms), and proportion of households with income ⩾$5000. Successive multivariate models were then constructed for each sex by creating a new set of models, each of which lacked one of the variables of the original model, and choosing the model with the smallest deviance and of which the fit was not statistically and significantly worse (judged by the likelihood ratio test) than the original model; the chosen model became the new 'original' model and the cycle repeated until all derived models were worse than the 'original' model (essentially this is a variant on backwards step-wise selection). Finally, any interactions still present, which were based on small numbers, were dropped.

To assist interpretation, effects of continuous variables (linear, quadratic, or cubic terms) have not been tabulated but instead recategorised into strata in the results (certain results will not make sense unless this is borne in mind). For the variables not included in the final, multivariate model, quartiles weighted by PYAR were used; variables incorporated in the final multivariate model were rendered dichotomous.

---

### Vestronidase alfa (Mepsevii) [^c6831a3e]. FDA (2025). Medium credibility.

2.000000000000000e+00 3 Preparation Instructions

Prepare MEPSEVII according to the following steps using aseptic technique:

1. Determine the number of vials to be diluted based on the patient's actual weight and the recommended dose of 4 mg/kg, using the following calculations (a-b):

a. Total dose (mg) = Patient's weight (kg) x 4 mg/kg (recommended dose)

b. Total number of vials = Total dose (mg) divided by 10 mg/vial

2. Round to the next whole vial and remove the required number of vials from the refrigerator to allow them to reach room temperature. Do not heat, microwave or shake vials.

a. Volume (mL) of calculated dose = Total dose (mg) divided by the 2 mg/mL concentration

3. The final solution will be a 1:1 dilution of MEPSEVII with 0.9% Sodium Chloride Injection, USP. More than 1:1 dilution may be used if the patient can tolerate additional infusion volume, taking into consideration cardiac function and fluid status.

4. For a 1:1 dilution, prepare the solution at room temperature, as follows:

a. Select an empty infusion bag, sized upon the total volume of the final solution.

b. Prior to withdrawing MEPSEVII from the vial, visually inspect the solution for particulate matter and discoloration. Because this is a protein solution, slight flocculation (thin translucent fibers) may occur. The MEPSEVII solution should be colorless to slightly yellow. Discard if the solution is discolored or if there is particulate matter in the solution.

c. Slowly withdraw the volume of the calculated MEPSEVII dose from the appropriate number of vials (step 2a) using caution to avoid excessive agitation and any air or frothing. Use a sufficiently large needle (18 gauge) to minimize bubbles in the solution.

d. Slowly add MEPSEVII to the infusion bag using care to avoid agitation, ensuring liquid to liquid contact without generating bubbles or turbulence.

e. Add 0.9% Sodium Chloride Injection, USP equal to the volume of MEPSEVII to the infusion bag.

f. Gently rock the infusion bag to ensure proper distribution of MEPSEVII. Do not shake the solution.

---

### Race and ethnicity in pulmonary function test interpretation: an official American Thoracic Society statement [^28f88a0c]. American Journal of Respiratory and Critical Care Medicine (2023). High credibility.

Figure 2 — race-specific versus single reference equations — illustrates how equation choice influences pulmonary function interpretation and survival association. The predicted survival for a 55-year-old, non-tobacco-smoking woman living at two times the federal income-poverty ratio is displayed for Black and White participants using each approach, and histograms show distributions of forced expiratory volume in 1 second (FEV1) z-scores when applying race-specific equations compared with a single reference equation (Global Lung Function Initiative reference equations).

---

### Kit for the preparation of technetium tc 99m pentetate (Draximage dtpa) [^b9ed30cf]. FDA (2023). Medium credibility.

Step 4:

When the solvent front has reached the top of the strip, remove the strip with forceps and allow it to dry.

Step 5: System A – Determination of reduced hydrolyzed technetium:

In System A (saline), reduced hydrolyzed technetium (99m TcO 2) stays at the origin (R f 0 to 0.1), while the bound technetium and free pertechnetate (99m TcO 4–) migrates to the solvent front (R f 0.85 to 1.0).
Cut the dried strip 3 cm from the origin.
The short piece is marked as Part I and the long piece is marked as Part II.
Count the pieces in a counter and determine the percentage of reduced hydrolyzed technetium according to the following formula:

Figure 1 System A Diagram

System B – Determination of free pertechnetate:

In System B (acetone), the bound technetium (Tc) and reduced hydrolyzed technetium (99m TcO 2) stay at the origin (R f 0 to 0.1), while free pertechnetate (99m TcO 4–) migrates to the solvent front (R f 0.85 to 1.0).
Cut the dried strip 2 cm from the solvent front end.
The short piece is marked Part III and the long piece is marked Part IV.
Count the pieces in a counter and determine the percentage of free pertechnetate according to the following formula:

Figure 2 System B Diagram

Step 6:

Determine the radiochemical purity according to the following formula:

Use Technetium Tc 99m pentetate injection only if the radiochemical purity is 90% or greater.

2.6 Radiation Dosimetry

The estimated radiation absorbed dose to various organs from an intravenous injection of Tc 99m pentetate in patients with normal and abnormal renal function is shown respectively in Table 4 and Table 5.

The estimated radiation absorbed dose to various organs from the inhalation of Tc 99m Pentetate Injection is shown in Table 6.

---

### Isoflurane and desflurane uptake during liver resection and transplantation [^7d5a8b0b]. Anesthesia and Analgesia (2003). Low credibility.

When reducing fresh gas flows, the course of the vaporizer dial settings required to maintain a constant end-expired concentration of a potent inhaled anesthetic becomes more dependent on the uptake pattern of the inhaled anesthetic. However, the uptake pattern of potent inhaled anesthetics during prolonged procedures remains poorly quantified. Therefore, we determined isoflurane and desflurane uptake (V(iso) and V(des), respectively) during liver resection (LR, n = 17) and orthotopic liver transplantation (OLT, n = 18) using a liquid injection closed-circuit anesthesia technique maintaining the end-expired concentration at 0.8% and 4.5%, respectively. Individual and average uptake curves were fit to a series of mathematical functions and compared with the square root of time and four-compartment models. Cumulative doses of isoflurane and desflurane after 1 and 3 h in the LR group and after 1, 3, and 8 h in the OLT group were correlated with demographic variables and each patient's average cardiac output and cardiac index. Average uptake was best described by a biexponential fit: V(iso) (LR) = 1.5 x (1 - e(-t x 0.525)) + 16.4 x (1 - e(-t x 0.00506)) (R(2) = 0.9996); V(iso) (OLT) = 1.4 + 3.1 x (1 - e(-t x 0.472)) + 26.7 x (1 - e(-t x 0.00307)) (R(2) = 0.9994); V(des) (LR) = 2.7 x (1 - e(-t x 0.763)) + 28.7 x (1 - e(-t x 0.00568)) (R(2) = 0.9984); and V(des) (OLT) = 1.4 x (1 - e(-t x 0.472)) + 26.7 x (1 - e(-t x 0.00307)) (R(2) = 0.9994). Uptake showed significant interindividual variability, and correlations between uptake variables and patient characteristics were inconsistent. The rate of uptake decreased more slowly then predicted by the uptake models. Because neither existing models nor patient characteristics accurately predict uptake in the individual patient, anesthesia techniques involving the use of low fresh gas flows will continue to have to rely on drug monitoring. However, the slowly decreasing rate of uptake during prolonged procedures suggests that the number of vaporizer adjustments to keep the end-expired concentration constant should be limited.

---

### Advancing theoretical understanding and practical performance of signal processing for nonlinear optical communications through machine learning [^ccdbefea]. Nature Communications (2020). High credibility.

Results

Digital back propagation

Let E (z, t) at z = 0 be the electric field of a signal at the transmitter. In the simplest form, signal propagation in optical fibers is described by the stochastic scalar nonlinear Schrodinger equation (NLSE)where n (z, t) are distributed ASE noise from inline amplifiers, which can be modeled as additive white Gaussian noise (AWGN) with zero mean and autocorrelation, where δ is the Kronecker delta function. In this formulation, D and N are the linear and nonlinear operators, respectively, and α, β 2, and γ denote attenuation, group velocity dispersion, and fiber nonlinearity coefficient.

Signal propagations in optical fibers can be numerically simulated using the split-step Fourier method (SSFM) that interleaves the effect of CD/loss and nonlinear-phase rotation over a small length Δ z of fiber. By separately applying the effect of CD/loss and nonlinearity to the signal, each step is analytically tractable. With digital coherent receivers, DBP is the standard technique to compensate fiber nonlinearity and is shown in Fig. 1. The DBP algorithm is a cascade of CD compensation filters D −1 with transfer function, where L s is the step size of the DBP. The nonlinear-phase derotation operation N −1 is defined by, whereis the effective length and ξ k is a scaling factor. The operators D −1 and N −1 attempt to undo the linear and nonlinear effects of NLSE during fiber transmission.

Fig. 1
Fiber propagation model and DBP structure.

The DBPs are interleaving operations of CD compensation and nonlinear-phase derotation. The choice of step size L s is a trade-off between transmission performance and computational complexity.

In standard DBP implementations, ξ k is the same for each stage of the DBP and is empirically optimized using brute-force approaches. The optimal value ξ opt depends on the noise level, dispersion map among other factors. The choice of the step size L s is a trade-off between complexity and performance, and is well-documented in the literature. DBP has since been extended to the case of polarization-multiplexed transmissions, stochastic DBP, and joint-channel DBPfor WDM transmissions, and numerous simplification techniques have been proposed. Unfortunately, as of today, DBP is still relatively complex, and for WDM systems, single-channel DBP provides marginal performance improvements.

---

### Hourly step recommendations to achieve daily goals for working and older adults [^878e0045]. Communications Medicine (2024). Medium credibility.

Two-part model (zero-step counts part and positive-step counts part)

We stratified the hourly step count data into weekdays and weekends (Figs. 2 and 3). Many hours of inactivity resulted in a high proportion of zeroes (46.8% including sleeping hours). Thus, for each hour, we fitted a two-part model to the step count data. The zero-step counts part of the model was specified as a binary logistic regression model to estimate the probability of a participant having any positive step counts. The positive-step counts part of the model was specified as a gamma regression model with a log link function to accommodate the positive support and right skew observed in the data.

Fig. 2
Model fit to the data.

a Model and data for cumulative steps from the start of the day on weekdays. b Model and data for cumulative steps from the start of the day on weekends. The blue-shaded regions denote the 10–25–75–90 percentiles of the data while the solid and dotted lines denote the 10–25–75–90 percentiles of the posterior predictive distribution from the model. In the case where the model fits perfectly to the data, we should expect that the solid lines overlap the edges of the dark blue regions and the dotted lines overlap the edges of the light blue regions (see the legend where the 10–25–75–90 percentiles overlap perfectly).

---

### Alteplase (cathflo Activase) [^cc1fd6ce]. FDA (2024). Medium credibility.

Instructions for Administration

Preparation of Solution

Reconstitute Cathflo Activase to a final concentration of 1 mg/mL:

Aseptically withdraw 2.2 mL of Sterile Water for Injection, USP (diluent is not provided). Do not use Bacteriostatic Water for Injection.
Inject the 2.2 mL of Sterile Water for Injection, USP, into the Cathflo Activase vial, directing the diluent stream into the powder. Slight foaming is not unusual; let the vial stand undisturbed to allow large bubbles to dissipate.
Mix by gently swirling until the contents are completely dissolved. Complete dissolution should occur within 3 minutes. DO NOT SHAKE. The reconstituted preparation results in a colorless to pale yellow transparent solution containing 1 mg/mL Cathflo Activase at a pH of approximately 7.3.
Cathflo Activase contains no antibacterial preservatives and should be reconstituted immediately before use. The solution may be used for intracatheter instillation within 8 hours following reconstitution when stored at 2–30°C (36–86°F).

No other medication should be added to solutions containing Cathflo Activase.

Instillation of Solution into the Catheter

Inspect the product prior to administration for foreign matter and discoloration.
Withdraw 2 mL (2 mg) of solution from the reconstituted vial.
Instill the appropriate dose of Cathflo Activase (see DOSAGE AND ADMINISTRATION) into the occluded catheter.
After 30 minutes of dwell time, assess catheter function by attempting to aspirate blood. If the catheter is functional, go to Step 7. If the catheter is not functional, go to Step 5.
After 120 minutes of dwell time, assess catheter function by attempting to aspirate blood and catheter contents. If the catheter is functional, go to Step 7. If the catheter is not functional, go to Step 6.
If catheter function is not restored after one dose of Cathflo Activase, a second dose of equal amount may be instilled. Repeat the procedure beginning with Step 1 under Preparation of Solution.
If catheter function has been restored, aspirate 4–5 mL of blood in patients ≥ 10 kg or 3 mL in patients < 10 kg to remove Cathflo Activase and residual clot, and gently irrigate the catheter with 0.9% Sodium Chloride Injection, USP.

Any unused solution should be discarded.

---

### Structural basis for adhesion G protein-coupled receptor gpr126 function [^0ede9f40]. Nature Communications (2020). High credibility.

Small-angle X-ray scattering

SAXS measurements were performed at the Advanced Photon Source at Argonne National Laboratory (beamline 18-ID) with an in-line SEC columns (Superdex 200 or Biorad EnRich 5–650 10–300) equilibrated with 20 mM HEPES, pH 7.4, and 150 mM NaCl. Data were analyzed using autorg and datgnom using the commands "autorg –sminrg 0.55 –smaxrg 1.1" and "datgnom '1'.dat -r '2' – skip '3' -o '1'.out", respectively, where '1' is the file name, '2' is the R g determined by autorg, and '3' is the number of points removed at low q as determined from autorg. SAXS curves of molecular models were generated with Crysol version 2.83.

cAMP signaling assay

Full-length wild-type and mutant Gpr126/GPR126 constructs were cloned into pCMV5. All constructs include N-terminal FLAG-tags for measuring cell-surface expression levels. The following primers were used for amplification of zebrafish Gpr126: F: 5'-GCTGACTACAAAGACGATGACGACAAGCTTTGCAATGTGGTGCTCACCGACTCCCAGGGC-3' and R: 5'-CCTGGCCAGGCCTCTGGTCCATGAGGCCCCTTATTGCAGGGTACTATCTGCATTACTGTG-3'. The following primers were used for amplification of human GPR126: F: 5'-GCTGACTACAAAGACGATGACGACAAGCTTTGCGCAAACTGTAGGGTTGTCCTGA-3' and R: 5'-CAGGCCTCTGGTCCATGAGGCCCCTCAACAGGGGCCAGTTTTCACCAG-3'. Zebrafish Gpr126 D134A/F135A mutant constructs were generated using the QuikChange method (Agilent) with primers: F: 5'-GATGGAGGTTTTCTTTAACTCCGCCGCTAGTGTCCAAAAGAAAGGCTTCC-3' and R: 5'-GGAAGCCTTTCTTTTGGACACTAGCGGCGGAGTTAAAGAAAACCTCCATC-3'.

---

### Sucralfate (Carafate) [^75b94925]. FDA (2024). Medium credibility.

P edia t r ic U se

Safety and effectiveness in pediatric patients have not been established.

G e r iatr i c U s e

Clinical studies of CARAFATE (sucralfate) Tablets did not include sufficient numbers of subjects aged 65 and over to determine whether they respond differently from younger subjects. Other reported clinical experience has not identified differences in responses between the elderly and younger patients. In general, dose selection for an elderly patient should be cautious, usually starting at the low end of the dosing range, reflecting the greater frequency of decreased hepatic, renal, or cardiac function, and of concomitant disease or other drug therapy (See DOSAGE AND ADMINISTRATION).

This drug is known to be substantially excreted by the kidney, and the risk of toxic reactions to this drug may be greater in patients with impaired renal function (See PRECAUTIONS, Special Populations: Chronic Renal Failure and Dialysis Patients). Because elderly patients are more likely to have decreased renal function, care should be taken in dose selection, and it may be useful to monitor renal function.

---

### Theory-based signal calibration with single-point T1 measurements for first-pass quantitative perfusion MRI studies [^c5dd43dd]. Academic Radiology (2006). Low credibility.

Rationale and Objectives

The aim of the study is to develop a theory-based signal calibration approach to be used for the conversion of signal-time curves to absolute contrast concentration-time curves for first-pass contrast-enhanced quantitative myocardial perfusion studies.

Materials and Methods

A normalization procedure was used to obtain a theoretical relationship between image signal and T1 and perform rapid single-point T1 measurements. T1 measurements were compared with reference T1 measurements. The method also was used in preliminary in vivo contrast-enhanced first-pass perfusion studies, and its applicability for dual-delay-time acquisitions was shown. A theory-based error sensitivity analysis was used to characterize the robustness of the method.

Results

The normalization procedure was implemented with minimal noise enhancement and insensitivity to small misregistrations through postprocessing techniques. The rapid T1 measurements are in excellent agreement with the reference measurements (R = 0.99, slope = 1.05, bias = -5.96 milliseconds). For in vivo studies, it is possible to simultaneously calibrate the arterial input function and myocardial enhancement curves acquired with different effective trigger delays through appropriate use of the theory-based signal calibration model. With this method, errors of in vivo baseline T1 estimates are large, but the effect of these large errors on the accuracy of contrast agent concentration estimates is limited.

Conclusion

This theory-based signal calibration approach can be used to perform rapid T1 mapping and provides flexibility for in vivo calibration of signal-time curves resulting from dual-delay-time first-pass contrast-enhanced acquisitions.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^705122bc]. Kidney International (2024). High credibility.

KDIGO 2024 CKD risk prediction models — evidence certainty, use, and cautions: To assess certainty, the ERT examined 2 existing systematic reviews, and a 2021 NICE review concluded there was high-quality evidence that chosen risk prediction equations accurately predict kidney failure; the Work Group agreed with this assessment. The Work Group judged that the published externally validated models all had sufficient accuracy to be used in clinical settings, and patients and healthcare providers should be encouraged to use these tools. Potential harms could result from inappropriate use in AKI or AKD or in younger individuals with CKD G1–G2 who may be at high risk of progression but low risk of kidney failure in the next 5 years; in these people, more proximal outcomes such as 40% decline in eGFR or lifetime risk were judged to be more appropriate. Given their value, healthcare providers should consider how to integrate risk prediction models into clinical practice, either in EMRs, laboratory information systems, or other mechanisms.

---

### Leukemic blasts with the paroxysmal nocturnal hemoglobinuria phenotype in children with acute lymphoblastic leukemia [^fd7d7020]. The American Journal of Pathology (2012). Low credibility.

It has been proposed that genomic instability is essential to account for the multiplicity of mutations often seen in malignancies. Using the X-linked PIG-A gene as a sentinel gene for spontaneous inactivating somatic mutations, we previously showed that healthy individuals harbor granulocytes with the PIG-A mutant (paroxysmal nocturnal hemoglobinuria) phenotype at a median frequency (f) of ∼12 × 10(-6). Herein, we used a similar approach to determine f in blast cells derived from 19 individuals with acute lymphoblastic leukemia (ALL) and in immortalized Epstein-Barr virus-transformed B-cell cultures (human B-lymphoblastoid cell lines) from 19 healthy donors. The B-lymphoblastoid cell lines exhibited a unimodal distribution, with a median f value of 11 × 10(-6). In contrast, analysis of the f values for the ALL samples revealed at least two distinct populations: one population, representing approximately half of the samples (n = 10), had a median f value of 13 × 10(-6), and the remaining samples (n = 9) had a median f value of 566 × 10(-6). We conclude that in ALL, there are two distinct phenotypes with respect to hypermutability, which we hypothesize will correlate with the number of pathogenic mutations required to produce the leukemia.

---

### Colorectal cancer screening and prevention [^390b568a]. American Family Physician (2025). High credibility.

Regarding screening and diagnosis for colon cancer, more specifically with respect to indications for screening, high-risk individuals, family history, AAFP 2025 guidelines recommend to obtain CRC screening in patients with ≥ 1 first-degree relatives with CRC or adenomatous polyps, starting at 40 years of age or 10 years before the age of the youngest relative at the time of their diagnosis.

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^4c4b615d]. Kidney International (2024). High credibility.

Initial assessment recommendation and caveats — Practice Point 1.2.2.1: Practice Point 1.2.2.1: Use serum creatinine (SCr) and an estimating equation for initial assessment of GFR. For most clinical circumstances, estimating GFR from SCr is appropriate; equations are available that estimate GFR using SCr and adjusting for sex and age, and professional societies have recommended that GFR estimates should be used in association with SCr reporting; key sources of error include non–steady-state conditions, non-GFR determinants of SCr, measurement error at higher GFR, and interferences with creatinine assays, and GFR estimates are less precise at higher GFR levels, so healthcare providers should remain aware of caveats that may influence accuracy in an individual person.

---

### KDIGO 2025 clinical practice guideline for the evaluation, management, and treatment of autosomal dominant polycystic kidney disease (ADPKD) [^94201e43]. Kidney International (2025). High credibility.

ADPKD predicted GFR by class and age (polynomial model) shows that for Class A, predicted glomerular filtration rate (GFR) values with 95% confidence intervals are 109 (95–123) at age 20–30 years, 110 (99–121) at 30–40, 97 (83–110) at 40–50, and 69 (49–90) at 50–60, with corresponding slopes 0.77, –0.63, –2.03, and –3.42; units are ml/min per 1.73 m2 for GFR and ml/min per 1.73 m2 per year for slopes, and positive values mean GFR increase.

---

### Global strategy for asthma management and prevention [^6d9e72b5]. GINA (2024). High credibility.

Track 1 (preferred) steps 1–2 treatment for adults and adolescents uses as-needed low-dose combination ICS-formoterol, which is used as needed for symptom relief, and before exercise or before expected allergen exposure. Information about Steps 1 and 2 is combined because the recommended treatment (as-needed low-dose ICS-formoterol) is the same. In Track 1, Step 1–2 treatment with as-needed-only low-dose combination ICS-formoterol is recommended for step-down treatment for patients whose asthma is well controlled on low-dose maintenance-and-reliever therapy with ICS-formoterol (Evidence D) or on regular low-dose ICS with as-needed SABA (Evidence A), and for initial asthma treatment for patients previously using SABA alone (or with newly diagnosed asthma), with normal or mildly reduced lung function; some clinical factors, outlined below, may prompt consideration of starting treatment instead at Step 3, with low-dose ICS-formoterol maintenance-and-reliever therapy.

---

### KDOQI US commentary on the KDIGO 2024 clinical practice guideline for the evaluation and management of CKD [^2e68dce4]. American Journal of Kidney Diseases (2025). High credibility.

KDIGO 2024 CKD — guidance to physicians and other health care providers outlines actionable points: "Practice Point 1.2.2.1: Use serum creatinine (SCr) and an estimating equation for initial assessment of GFR". "Recommendation 1.2.2.1: We recommend use of eGFRcr-cys in clinical situations when eGFRcr is less accurate and GFR affects clinical decision-making (original guideline Table 8) (1C)". "Practice Point 1.2.2.2: Where more accurate ascertainment of GFR will impact treatment decisions, measure GFR using plasma or urinary clearance of an exogenous filtration marker". "Practice Point 1.2.2.3: Understand the value and limitations in both eGFR and measured glomerular filtration rate (mGFR) as well as the variability and factors that influence SCr and cystatin C measurements". "Practice Point 1.2.2.4: Interpretation of SCr levels requires consideration of dietary intake". "Practice Point 1.2.2.5: Assess the potential for error in eGFR when using SCr-based estimating equations and cystatin C–based estimated glomerular filtration rate (eGFRcys) in some specific circumstances". "Practice Point 1.2.2.7: Understand the implications of differences between eGFRcr and eGFRcys, as these may be informative, in both direction and magnitude of those differences". "Practice Point 1.2.2.8: Consider timed urine collections for measured creatinine clearance if mGFR is not available and eGFRcr-cys is thought to be inaccurate".

---

### Colorectal cancer screening and prevention [^c270f773]. American Family Physician (2025). High credibility.

Regarding screening and diagnosis for colon cancer, more specifically with respect to indications for screening, general population, aged 76–85 years, AAFP 2025 guidelines recommend to consider obtaining screening for CRC in adults aged 76–85 years at average risk based on overall health status, prior screening history, and patient preferences.

---

### Characterizing the exponential profile of W' recovery following partial depletion [^18c0cc1c]. Medicine and Science in Sports and Exercise (2024). Medium credibility.

Purpose

The aim of this study was to characterize W' recovery kinetics in response to a partial W' depletion. We hypothesized that W' recovery following a partial depletion would be better described by a biexponential than by a monoexponential model.

Methods

Nine healthy men performed a ramp incremental exercise test, three to five constant load trials to determine critical power and W', and 10 experimental trials to quantify W' depletion. Each experimental trial consisted of two constant load work bouts (WB1 and WB2) interspersed by a recovery interval. WB1 was designed to evoke a 25% or 75% W' depletion (DEP 25% and DEP 75%). Subsequently, participants recovered for 30, 60, 120, 300, or 600 s and then performed WB2 to exhaustion to calculate the observed W' recovery (W' OBS). W' OBS data were fitted using monoexponential and biexponential models, both with a variable and with a fixed model amplitude. Root mean square error and Akaike information criterion (AIC c) were calculated to evaluate the models' goodness-of-fit.

Results

The biexponential model fits were associated with overall lower root mean square error values (0.4% to 5.0%) when compared with the monoexponential models (2.9% to 8.0%). However, ΔAIC c resulted in negative values (-15.5 and -23.3) for the model fits where the amplitude was kept free, thereby favoring the use of a monoexponential model for both depletion conditions. For the model fits where the amplitude was fixed at 100%, ΔAIC c was negative for DEP 25% (-15.0) but positive for DEP 75% (11.2). W' OBS values were strongly correlated between both depletion conditions (r = 0.92) and positively associated with V̇O 2peak, critical power, and gas exchange threshold (r = 0.67 to 0.77).

Conclusions

The present study results did not provide evidence in favor of a biexponential modeling technique to characterize W' recovery following a partial depletion. Moreover, we demonstrated that fixed time constants were insufficient to model W' recovery across different depletion levels, and that W' recovery was positively associated with aerobic fitness. These findings underline the importance of employing variable and individualized time constants in future predictive W' models.

---

### Prediction of creatinine clearance from serum creatinine [^4f4cf260]. Nephron (1976). Low credibility.

The clinical calculator "Creatinine Clearance (Cockcroft-Gault Equation)" for diabetic nephropathy, chronic kidney disease, immunoglobulin A nephropathy, membranous nephropathy, focal segmental glomerulosclerosis, anemia of chronic kidney disease, anti-glomerular basement membrane disease, hepatorenal syndrome, immunoglobulin light chain amyloidosis, infection-related glomerulonephritis, kidney transplantation and membranoproliferative glomerulonephritis.

The Cockcroft-Gault equation calculates creatinine clearance to estimate kidney function and guide medication dosing. It considers age, weight, gender, and serum creatinine, incorporating adjustments for body weight when necessary. This widely-used formula helps healthcare providers assess renal function and adjust medication doses accordingly.

The Creatinine Clearance (Cockcroft-Gault Equation) calculator estimates kidney function through the creatinine clearance rate, reflecting how effectively the kidneys filter waste. This tool requires input on age, gender, weight, height, and serum creatinine level.

Key parameters:

- **Age (years)**: 18 to 120
- **Gender**: Male or Female
- **Weight**: in kilograms (20–200 kg) or pounds (44–440 lbs)
- **Height**: in centimeters (120–220 cm) or inches (48–87 in)
- **Creatinine**: in mg/dL (0.1–20) or µmol/L (8.84–1768.4)

The process involves:

1. **Unit Conversion**:
- Convert weight from pounds to kilograms, if necessary, using 1 kg = 2.20462 lbs.
- Convert height from centimeters to inches, if necessary, using 1 inch = 2.54 cm.
- Convert creatinine from µmol/L to mg/dL, if necessary, using 1 mg/dL = 88.4 µmol/L.

2. **Ideal Body Weight (IBW) Calculation**:
- **For males**: IBW = 50 + 2.3 * (height in inches - 60)
- **For females**: IBW = 45.5 + 2.3 * (height in inches - 60)

3. **Adjusted Body Weight (ABW) Calculation**:
- ABW = IBW + 0.4 * (actual weight - IBW)

4. **Creatinine Clearance Calculation**:
- **Original CrCl (using actual weight)**: ((140 - age) * weight) / (72 * creatinine)
- **IBW CrCl (using ideal weight)**: ((140 - age) * IBW) / (72 * creatinine)
- **ABW CrCl (using adjusted weight)**: ((140 - age) * ABW) / (72 * creatinine)

5. **Gender Adjustment**:
- For females, each calculated CrCl value needs multiplying by 0.85.

Final outputs include:

- Original CrCl, reflecting estimation with actual weight.
- IBW CrCl, using ideal weight.
- ABW CrCl, calculated with adjusted weight.
- Weight used for the computation, displayed in kilograms.

These outputs, rendered in mL/min and rounded to one decimal place, facilitate the assessment of kidney function and inform clinical decisions regarding medication dosing. Better kidney filtration capabilities correspond to higher clearance scores, guiding clinicians in interpreting renal health.

---

### Step-by-step progress toward understanding the hepatitis C virus RNA helicase [^fb90344f]. Hepatology (2006). Low credibility.

Helicases are a ubiquitous class of enzymes involved in nearly all aspects of DNA and RNA metabolism. Despite recent progress in understanding their mechanism of action, limited resolution has left inaccessible the detailed mechanisms by which these enzymes couple the rearrangement of nucleic acid structures to the binding and hydrolysis of ATP. Observing individual mechanistic cycles of these motor proteins is central to understanding their cellular functions. Here we follow in real time, at a resolution of two base pairs and 20 ms, the RNA translocation and unwinding cycles of a hepatitis C virus helicase (NS3) monomer. NS3 is a representative superfamily-2 helicase essential for viral replication, and therefore a potentially important drug target. We show that the cyclic movement of NS3 is coordinated by ATP in discrete steps of 11 ± 3 base pairs, and that actual unwinding occurs in rapid smaller substeps of 3.6 ± 1.3 base pairs, also triggered by ATP binding, indicating that NS3 might move like an inchworm. This ATP-coupling mechanism is likely to be applicable to other non-hexameric helicases involved in many essential cellular functions. The assay developed here should be useful in investigating a broad range of nucleic acid translocation motors.

---

### KDIGO clinical practice guideline for the care of kidney transplant recipients [^b096cab4]. American Journal of Transplantation (2009). Medium credibility.

Chapter 8: Monitoring kidney allograft function — glomerular filtration rate (GFR) estimation states that We suggest estimating GFR whenever serum creatinine is measured, (2D) using one of several formulas valid for Pcr steady-state (2C), or the Schwartz formula for children and adolescents. (2C).