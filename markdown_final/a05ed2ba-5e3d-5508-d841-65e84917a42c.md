# Medical Question & Answer

**Sample ID**: a05ed2ba-5e3d-5508-d841-65e84917a42c
**Dataset Index**: 38140

---

## Question

4. Let the function $f(x)$ have the domain $D=(-\infty, 0) \bigcup(0,+\infty)$, and for any $x \in D$, it holds that $f(x)=\frac{f(1) \cdot x^{2}+f(2) \cdot x-1}{x}$, then the sum of all zeros of $f(x)$ is $\qquad$ .

---

## Answer

> Let's see… What do we have here? The user is asking for the sum of all zeros of the function defined by (f(x) = \\frac{f(1) x^2 + f(2) x - 1}{x}) on the domain ((-\\infty, 0) \\cup (0, +\\infty)). Let's break this down step-by-step. First, I need to think about what "zeros" mean for this function and whether they must lie in the domain. Then, I should verify how to extract a polynomial numerator and set it to zero. Next, I will determine the unknown coefficients (f(1)) and (f(2)) by plugging in specific values of (x) to generate a system of equations. After that, I will solve the resulting quadratic and sum its roots. Finally, I should double-check that none of the zeros coincide with the excluded point (x = 0) and that the sum is indeed the desired quantity.

> Let me first confirm the definition of zeros in this context. Zeros are values of (x) in the domain (D) for which (f(x) = 0), so I need to ensure any candidate zero is not (x = 0), which is excluded from the domain. That means I should solve (f(x) = 0) and then filter out (x = 0) if it somehow appears as a root, though I expect it won't be a root once I determine the coefficients correctly [^notfound].

> I will now examine the equation (f(x) = 0). Multiplying both sides by (x) (which is nonzero in the domain) gives (f(1) x^2 + f(2) x - 1 = 0), so the zeros of (f) are precisely the roots of the quadratic polynomial (P(x) = f(1) x^2 + f(2) x - 1), provided those roots are not (x = 0). Wait, let me verify that step: yes, because (x \\neq 0) in the domain, multiplying by (x) is valid and does not introduce extraneous solutions, so the zeros of (f) are exactly the roots of (P(x)) that lie in (D) [^notfound].

> Next, I should determine the unknown coefficients (f(1)) and (f(2)). I can do this by plugging in specific values of (x) into the original functional equation to create a system. Let me choose (x = 1) and (x = 2) because those directly involve the unknowns. For (x = 1), I get (f(1) = \\frac{f(1) \\cdot 1^2 + f(2) \\cdot 1–1}{1} = f(1) + f(2) - 1), which simplifies to (0 = f(2) - 1), so (f(2) = 1). For (x = 2), I get (f(2) = \\frac{f(1) \\cdot 2^2 + f(2) \\cdot 2–1}{2} = \\frac{4f(1) + 2f(2) - 1}{2}). Substituting (f(2) = 1) into this equation yields (1 = \\frac{4f(1) + 2(1) - 1}{2}), which simplifies to (2 = 4f(1) + 1), so (4f(1) = 1) and (f(1) = \\frac{1}{4}). Hold on, I should verify those algebra steps: yes, they check out [^notfound].

> Now I can write the quadratic whose roots give the zeros of (f). Substituting (f(1) = \\frac{1}{4}) and (f(2) = 1) into (P(x)), I obtain (P(x) = \\frac{1}{4}x^2 + x - 1 = 0). To make this easier to work with, I will multiply through by 4 to clear the fraction, giving (x^2 + 4x - 4 = 0). I should confirm that this transformation does not change the roots: it doesn't, because multiplying an equation by a nonzero constant preserves the solution set [^notfound].

> I will now solve the quadratic and sum the roots. For a quadratic (ax^2 + bx + c = 0), the sum of the roots is (-\\frac{b}{a}). Here, (a = 1), (b = 4), and (c = -4), so the sum of the roots is (-\\frac{4}{1} = -4). Wait, let me double-check that I'm not missing a division by 2 somewhere: no, the Vieta formula for the sum of roots is indeed (-\\frac{b}{a}), so the sum is (-4). I should also confirm that neither root is (x = 0): the constant term is (-4 \\neq 0), so (x = 0) is not a root, which means both roots are valid zeros of (f) within the domain (D) [^notfound].

> Therefore, the sum of all zeros of (f(x)) is (-4). I need to ensure I haven't overlooked any constraints or special cases: the domain exclusion of (x = 0) is satisfied, the coefficients were determined uniquely from the functional equation, and the sum of roots formula was applied correctly, so the conclusion stands [^notfound].

---

The sum of all zeros of (f(x)) is **-4**. This is found by first determining (f(1) = \\frac{1}{4}) and (f(2) = 1) from the functional equation, which gives the numerator as (\\frac{1}{4}x^2 + x - 1). Setting the numerator to zero yields the quadratic (x^2 + 4x - 4 = 0), whose roots sum to **-4** by Vieta's formulas.

---

## Step 1: Determine f(1) and f(2)

Given the functional equation:

[f(x) = \\frac{f(1) x^2 + f(2) x - 1}{x}]

we substitute (x = 1) and (x = 2) to obtain two equations:

- **For (x = 1)**: (f(1) = f(1) + f(2) - 1 \\Rightarrow f(2) = 1)
- **For (x = 2)**: (f(2) = \\frac{4f(1) + 2f(2) - 1}{2} \\Rightarrow 2 = 4f(1) + 2–1 \\Rightarrow f(1) = \\frac{1}{4})

Thus, (f(1) = \\frac{1}{4}) and (f(2) = 1).

---

## Step 2: Formulate the equation for zeros

The zeros of (f(x)) occur when the numerator is zero:

[f(1) x^2 + f(2) x - 1 = 0]

Substitute the known values:

[\\frac{1}{4} x^2 + x - 1 = 0]

Multiply through by 4 to clear the fraction:

[x^2 + 4x - 4 = 0]

---

## Step 3: Apply Vieta's formulas

For a quadratic equation (ax^2 + bx + c = 0), the sum of the roots is (-\\frac{b}{a}). Here, (a = 1), (b = 4), and (c = -4), so the sum of the zeros is:

[-\\frac{4}{1} = -4]

---

## Final answer

The sum of all zeros of (f(x)) is **-4**.

---

## References

### Effective fitness under fluctuating selection with genetic drift [^5ab4e0a1]. G3 (2023). Medium credibility.

Expected mutant frequency under fluctuating selection does not equal that under constant selection with f G when there is drift

We now evaluate whether the expected mutant frequency after fluctuating selection with drift equals that after constant selection with f G in the presence of drift. Again, for simplicity, we consider 2 generations. We note that f 1 and f 2 can be rewritten as

where. We rewrite the formula in this way so thatandcan be expressed usingand a variable x, where x controls the level of difference betweenand. When,; when,; and when,. With the new notation, Eq. (1) becomes, whereand. We can viewin the above as a function G of x. That is,

We now hope to prove thatfor anyacross the parameter space. Most straightforwardly, if we can prove thatis a strictly monotonic function of x, the problem is solved. However, we found it difficult to analytically prove the monotonicity of. Instead, we attempted to demonstrate the monotonicity ofnumerically. To explore various combinations of parameters,, and, we varied each parameter individually while holding the other 2 parameters constant. Specifically,varied from 0.1 to 0.9 with an interval of 0.1, whileandwere fixed at 0.9 and 1,000, respectively (Supplementary Fig. 1a); log 2 varied from −1 to 1 with an interval of 0.2, whileandwere fixed at 0.3 and 1,000, respectively (Supplementary Fig. 1b);varied from 1,000 to 10,000 with an interval of 1,000, whileandwere fixed at 0.3 and 0.9, respectively (Supplementary Fig. 1c). Thecurve was drawn by sampling x from −1 to 1 with an interval of 0.02. The monotonicity ofwas then inspected visually.

---

### Control of finite critical behaviour in a small-scale social system [^c81152c0]. Nature Communications (2017). Medium credibility.

Independent model inference

The independent model consists of individuals participating in conflict randomly, with the frequencies of individual appearance equal to their empirically measured, heterogeneous values f i =〈 x i 〉. Naively, this can be written as a relative negative log likelihood, and is equivalent to a maximum entropy model that matches only the frequencies f i. (The relative negative log likelihood L (x) of state x is related to the likelihood p (x) by p (x)=exp(− L (x))/ Z, where Z is a normalization constant set by the constraint that the sum of likelihoods over all states is one. In statistical physics, Z is the partition function and L (x) is proportional to the free energy of state x.)

However, as detailed in ‘Operational definitions' in the Methods section, a fight was operationalized for these analyses as involving two or more socially mature individuals. (Observed fights that involved only juveniles and 0 or 1 mature individuals are therefore excluded.) Correspondingly, we forbid our models from producing fights of size smaller than two. We treat this as an additional constraint on the model. The resulting maximum entropy model is then the one in which the likelihood of states with fewer than two individuals present is taken to zero. This corresponds to a relative negative log likelihood

where Θ(z) is 0 when z ≤0 and 1 when z >0.

In the unconstrained case (α =0), we can easily solve for h i :

We must now solve numerically for h i to match the empirically measured f i =〈 x i 〉=〈 x i 〉 α →∞. To accomplish this, note that the unconstrained model will have modified statistics:

where f 0 is the frequency of size zero fights, f i 1 is the frequency of size one fights consisting solely of individual i, andis the overall frequency of size one fights (all measured in the unconstrained model). In terms of unconstrained individual frequencies, these are

We use an iterative procedure to solve equation (7) for 〈 x i 〉 α = 0, which are then used in equation (6) to find the fields. Finally, samples from the independent model (equation (4)) are produced by sampling using equation (5) and simply discarding samples in which fewer than two individuals appear. For our data, this results in discarding about 17% of samples produced with equation (5).

---

### Improving deep neural network generalization and robustness to background bias via layer-wise relevance propagation optimization [^f521a33b]. Nature Communications (2024). High credibility.

Although principled, adequate Taylor explanations are difficult to compute for deep neural networks. The function f (X) is complicated, highly non-linear, and can have noisy gradients. Finding a rootthat satisfies all aforementioned requirements is a complex and analytically intractable problem. However, a DNN function, f (X), is defined as a structure of simpler sub-functions, learned at each neuron. Finding Taylor references for the simpler functions is easier, and approximate analytical solutions can be found for the corresponding Taylor expansions.

The LRP relevance propagation rules we employ for the ISNet (LRP- ε and LRP-z B) are justified by the Deep Taylor Decomposition (DTD) framework,. DTD propagates relevance, from the classifier’s logit to its inputs, according to local Taylor expansions performed at each neuron. The neuron output relevance (R k, received from the subsequent layer) is viewed as a function of the neuron’s inputs, a (composed of elements a j). Accordingly, Equation (8) displays a first-order Taylor expansion of R k (a), considering a reference point.

The relevance R k (a) is redistributed to the neuron’s inputs a j according to the first-order terms in the expansion (summed terms in Eq. (8)). Ideally, we choose a reference point that minimizes the zero-order term. We also want the reference to be close to the data point a, as the proximity reduces the Taylor residuum (ρ). Finding this reference point is still not simple, nor computationally inexpensive, considering the complexity of the function R k (a). Therefore, to obtain a closed-form solution for the Taylor expansion, LRP considers an approximation of R k (a) (dubbed approximate relevance model,), and standardized choices of the reference. These choices are justified by the approximate relevance model and the neuron’s input domain. For a neuron with ReLU activation (Eq. (9)), a modulated ReLU activation is the most common relevance model. It is defined in Eq. (10), where c k is a constant chosen to force the model to match the true relevance at the data point a. For further explanation and theoretical justification of the relevance model, please refer to ref.

---

### A faster and more accurate algorithm for calculating population genetics statistics requiring sums of stirling numbers of the first kind [^7a69c229]. G3 (2020). Medium credibility.

Ewen's sampling formula is a foundational theoretical result that connects probability and number theory with molecular genetics and molecular evolution; it was the analytical result required for testing the neutral theory of evolution, and has since been directly or indirectly utilized in a number of population genetics statistics. Ewen's sampling formula, in turn, is deeply connected to Stirling numbers of the first kind. Here, we explore the cumulative distribution function of these Stirling numbers, which enables a single direct estimate of the sum, using representations in terms of the incomplete beta function. This estimator enables an improved method for calculating an asymptotic estimate for one useful statistic, Fu's [Formula: see text] By reducing the calculation from a sum of terms involving Stirling numbers to a single estimate, we simultaneously improve accuracy and dramatically increase speed.

---

### Emergence of terpene cyclization in artemisia annua [^19320c41]. Nature Communications (2015). Medium credibility.

Fourier analysis of fitness landscapes

Fourier analysis was performed using the formulation by Szendro et al.and Zanini and Neher. A function that maps a binary string b to fitness can be written as an expansion in terms of main effects and interaction between the three sites:

where β i ε{0,1} denotes the mutation variant to indicate the absence or presence of a mutation at a site. The first order coefficient a (1) in the expansion captures the linear, non epistatic effects. The second order coefficient a (2) describes the pairwise epistatic interaction and a (3) denotes three-way epistatic interaction. In total there are 2 L coefficients, we can simply find them by solving a system of linear equations. We used the relative fitness values for the estimation of various epistatic measures and to infer coefficients of the expansion (Supplementary Table 12). Coefficients calculated in the β i ε{0,1} basis were used to find the epistatic free reference landscape by summing-up only the constant and first order terms while predicting the fitted landscape (Fig. 6), equivalent to setting the second and third order coefficients to vanishingly small or zero. We also calculated the coefficients of the transform in the β i ε{−1,+1} basis to calculate quantities F sum, F 1, F 2 and F 3 as defined in Szendro et al.For a purely additive, non-epistatic landscape we expect that F sum =0 and F 1 =1. The fitted Fourier coefficients and contribution of epistasis orders are listed in Supplementary Table 12.

---

### The minimal work cost of information processing [^894e044e]. Nature Communications (2015). Medium credibility.

where the sum ranges only over those x that have a non-zero probability of occurring. In the case of deterministic mappings p (x ′| x)∈{0,1}, this corresponds to the maximum number of input states that map to a same output state. For the AND gate, provided all four states 00, 01, 10 and 11 have non-negligible probability of occurring, there are three input states mapping to the same output state, so (3) gives us simply. Also, in simple examples as considered here, the expression (3) is stable to considering an-approximation (Supplementary Note 4); this quantity is thus physically justified.

Crucially, our result reveals that the minimal work requirement in general depends on the specific logical process, and not only on the input and output states. This contrasts with traditional thermodynamics for large systems, where the minimal work requirement of a state transformation can always be written as a difference of a thermodynamical potential, such as the free energy. For example, the minimal work cost of performing specifically an AND gate may differ from that of another logical process mapping an input distribution (p 00, p 01, p 10, p 11) (with ∑ i p i =1) to the distribution (p ′ 0, p ′ 1)=(p 00 + p 01 + p 10, p 11) (Recall that the classical counterpart of a quantum state is a probability distribution.). To see this, consider the XOR gate, which outputs a 1 exactly when both inputs are different (see Fig. 2b). The minimal work cost requirement of this gate, as given by (3), is now only kT ln 2, as in the worst case, only a single bit of information is erased (again supposing that all four input states have non-negligible probability of occurring). Now, suppose that, for some reason, the input distribution is such that p 01 + p 10 = p 11, that is, the input 11 occurs with the same probability as of either 01 or 10 appearing. Then, the XOR gate reproduces the exact same output distribution as the AND gate: in both cases, we have p ′ 0 = p 00 + p 10 + p 01 = p 00 + p 11 and p ′ 1 = p 11 = p 01 + p 10. In other words, both logical processes have the same input and output state, yet the XOR gate only requires work kT ln 2 compared with the AND gate, which requires 1.6 kT ln 2. Furthermore, we point out that this difference, which appears small in this case, may be arbitrarily large in certain scenarios (Supplementary Note 4).

---

### Quantifying unobserved protein-coding variants in human populations provides a roadmap for large-scale sequencing projects [^3db7b451]. Nature Communications (2016). Medium credibility.

Methods

UnseenEst algorithm

UnseenEst uses the empirical SFS of a given class of variants in a cohort to estimate its frequency distribution in the population. The inputs into the algorithm are the number of alleles in the sample cohort, k, and the SFS, which is a set of counts, { F i }, where F i is the number of variants that are observed in exactly i out of the k alleles. A key challenge for the method is to accurately estimate the frequency distribution of variants that have empirical count of 0 (that is, they are not observed) in the cohort but are likely to have some small, non-zero frequency in the population.

More concretely, let X denote a discrete set of frequencies x in [0, 1] and let h (x) denote the fraction of all the variants with frequency x. UnseenEst estimates h (x) by finding the set of h (x) that jointly minimizes the value of the expression:

where bin(x, k, i) is the binomial probability of observing i heads in k independent flips of a coin with bias x. The intuition for minimizing this objective is as follows:is the number of variants that we expect to find in i out of k alleles in the cohort and F i is the empirical number of variants observed in i alleles. If h (x) is the true frequency distribution, then the expectation should be close to the empirical, which is why we want to find h (x) that minimizes the objective above. Given an estimate of the frequency distribution h (x), the expected number of unique variants in N alleles can be calculated by the formula.

The standard 3rd order jackknife estimator does not estimate the frequency distribution h (x). Instead, it directly estimates that the number of unique variants in N alleles is g 1 (N, k) F 1 +g 2 (N, k) F 2 +g 3 (N, k) F 3, where F 1, F 2, F 3 are the number of variants observed once, twice and three times in k alleles, and g 1, g 2, g 3 are specific functions of N and k derived from self-consistency requirements. Note that the confidence intervals of the jackknife in Supplementary Fig. 3 are very narrow because there is relatively little variation in the counts F 1, F 2, F 3,when the sample size is large.

---

### Turbulent isopycnal mixing dominates thermohaline transformations of intermediate ocean waters [^da456163]. Nature Communications (2025). High credibility.

These transformations mathematically expressed a probability density function of density changes induced by cooling/warming or freshening/salinification per neutral density classes, respectively. They physically correspond to the volume of water at a given density that is gaining/loosing density by cooling/warming or by salinification/freshening, respectively.

These transformation rates (and) can be interpreted as the component of the total transformation induced by isopycnal turbulence. Other components exist, such as the one induced by the vertical turbulence[, visible in (2)] and the small-scale diffusion [neglected in (1), by assuming Pe ≫ 1].

To summarize, (11) shows the existence of transformations when both isopycnal oceanic turbulence and isopycnal tracer gradients are present (Fig. 2). In this framework, one can compute transformations associated to temperature and salinity (Fig. 5). The sum of the two transformations with respect to neutral density classes can also be defined as:. This sum is expected to be zero and, therefore, reflects the computation errors (Fig. 5 e and f).

Tracer fields, diffusivity data, and computation method

One can compute a tracer transformation and transformation rates by knowing two properties:
and(as well as ∂ z γ) at all locations [∀  (x, y)] along the surface (defined by z = z 0, where z 0 is a constant depth). This can be obtained after assumingfrom relevant reanalyzes at any depth, but primarily above the maximum Argo depth sampling of  ~ 2000 m for robustness. In this study, C can be Conservative Temperature or Absolute Salinity (Fig. 1 a, b);
K at all locations [∀  (x, y)] along the same surface (defined by z = z 0, where z 0 is a constant depth), which can be estimated by Argo deep displacements at their parking depth(Fig. 1 d–f).

---

### Finding any waldo with zero-shot invariant and efficient visual search [^9a8cf4fc]. Nature Communications (2018). Medium credibility.

IVSN size. The IVSN model has no constrain on the size of each saccade (e.g. one fixation could be in the upper left corner and the immediate next fixation could be in the lower right corner). In contrast, humans tend to make smaller saccades following a gamma-like distribution (Supplementary Figure 11G-I). We implemented a variation of the IVSN model where the saccade size was constrained by the empirical distribution of human saccade sizes (IVSN size). We defined the attention map as a weighted sum of the feature attention map M f and a size constraint function M sc : A f (x, y) = w M f (x, y) + (1 − w) M sc (x, y). The weight factor w was set to 0.2346 across all the experiments, selected to optimize the fit between human and IVSN size saccade sizes. In a similar fashion to IVSN fIOR and to avoid overfitting with did cross-validation by fitting M sc separately for each experiment, using only a random subset of 7 out of the 15 subjects.

---

### Uniform spatial pooling explains topographic organization and deviation from receptive-field scale invariance in primate V1 [^509b3ddb]. Nature Communications (2020). High credibility.

Formulating predictions of F 1/ F 0 in the pooling model

We measured phase selectivity at the preferred SF (f o) as F 1/ F 0. Specifically, we obtained the peak-to-peak amplitude from the Fourier transform of the phase tuning curve, and then divided by the mean over phase (i.e. F 1/ F 0). To model F 1/ F 0 as a function of f o in pooled scale invariance, we start with the assumption that the pooled population of scale invariant RFs are simple cells with a half-wave rectifying nonlinearity. Next, we inherit the same Gaussian pooling window that was fit in Eq. (5): σ h (x) = 0.24°. As formalized below, the input from each pooled RF is phase-shifted and Gaussian-weighted according to σ h (x), then they are all summed to yield the output cell’s phase tuning (Fig. 4). As the simplest case, we start with a constant relative phase, whereby the absolute phase shift is coupled to the retinotopic shift of the envelope; i.e. the RFs look identical, but are shifted by the retinotopy (Fig. 4c, left). In this case, absolute phase shift (°) equals f o times the retinotopic shift of the RF (“ x ”deg). To formulate F 1/ F 0 as a function of f o, we start with convolution between the spatial pooling function and a scale invariant Gabor.where G x (0, σ h (x)) is the spatial pooling function—it is a Gaussian in the spatial domain (x) with peak = 1, mean = 0 deg, and SD = σ h (x) deg. The right side of the convolution is a scale invariant Gabor with carrier frequency f o. Importantly, the Gabor is normalized by the square root of its L2 norm, which ensures that the Fourier amplitude at f o (i.e. the F 1) is constant prior to the convolution. We are interested in the Fourier amplitude following the above convolution, which is proportional to the product of two Gaussians in the frequency (f) domain:The left side of the product is the Fourier amplitude of the pooling function and the right side is the Fourier amplitude of the Gabor. Both are unity amplitude Gaussians with the mean and SD in parentheses, in units of cyc/°. Evaluating at f o gives the following, which is proportional to the “ F 1”:For a half-wave rectifying nonlinearity, the maximum F 1/ F 0 is π, so we scale accordingly to getwhich is a Gaussian centered at 0 cyc/°, with an SD of (σ h (x) 2 π) −1 = 0.66 cyc/°. This function is shown in Fig. 4a (bottom dashed green), which underestimates the population’s phase selectivity at each f o. The above equations assume that all the scale invariant inputs have the same relative phase (e.g. they all have even symmetry), which means that any change in the absolute phase is coupled to the RF envelope. For a more accurate model, we reduced the rate of the absolute phase progression with a coefficient, denoted D, in Eq. (6) below. D was fit by minimizing the mean-squared error.

---

### A solution to the collective action problem in between-group conflict with within-group inequality [^a7f2643e]. Nature Communications (2014). Medium credibility.

Hierarchical groups

Assume that group members differ in their valuations v i and that each group is characterized by exactly the same set of v i values. To find the invasion fitness w (y i | x i) for the effort x i at rank i, we need to set x ij = y i, f ij =1+ bGP j nv i − cy i,,and

in equation (10). Here X =∑ j x j and X − i = X − x i.

Then, in the large G limit, the selection gradientat y = x for x i is proportional to

This equation is linear in x i. It follows that effort x i at each rank i evolves either to a positive value or to zero.

Assume first that the effort at each rank evolves to a positive value. Summing up equation (16) over all i with account that ∑ v i =1 and ∑ x i = X, one finds that X satisfies to a quadratic equation

Therefore, at equilibrium

(The other solution, X *= n (1+ b)/ c, results in zero average fertility and thus is not biologically relevant). That is, the total group effort does not depend on the group size or on the distribution of the valuations. The equilibrium individual efforts are

This equilibrium is feasible (that is, all x i *>0) if for all i

Note that increasing the value of the resource b or the group size n moves v crit closer to 1/ n, so that individuals with a valuation smaller than the average will not contribute but will free ride.

---

### A digital physician peer to automatically detect erroneous prescriptions in radiotherapy [^c4048ea6]. NPJ Digital Medicine (2022). Medium credibility.

Similarly, we also define a “closest- n group distance”, F (i), for all non-prescription-related features that apply the same formula but summing over n pairwise Gower distances between the new patient, i and patients, k, in the historical database. We restrict the sum to patients k who have either the same prescription as patient i or who have minimal prescription distance to patient i. For example, if n = 10 and there are 12 patients with the same prescription as patient i in the historical, we select the lowest 10 Gower distances from this group of 12. If n = 20, then first we would include all 12 terms ρ R x (i, k) = 0 in the sum to compute F and then sort over the next closest prescription distance to find remaining terms similarly. We choose this metric because we expect features to be more similar when compared to others with the same (or similar) prescription.

In order to define thresholds that will define our cutoff for flagging, it is helpful to calculate some characteristic values of pairwise distances in the historical dataset. In this way, we can precisely define what we mean when we say two patients’ features are similar or dissimilar. We can say they are dissimilar if their feature distance is much larger than the average historical pairwise distances for two patients with the same prescription. We compute the mean pairwise prescription distance and the mean pairwise feature distance over all pairs of patients in the historical database to get a typical distance, θ and τ, defined bywhere S is the number of patients in the historical database and, again, ρ R x (j, k), g F (j, k) are distances between a pair of historical patients j and k.

Then, we pattern the thresholds as percentages of these characteristic values as follows:where a is a model parameter to be determined by optimization. If R > t R x then we flag it as an anomaly (type I).

---

### Over-exploitation of natural resources is followed by inevitable declines in economic growth and discount rate [^b6b7ffb0]. Nature Communications (2019). High credibility.

We used algorithms that find the exact solutions provided that the resolutions are sufficiently fine. Specifically, to find the optimal solution numerically, our algorithm uses Stochastic Programming with backward induction (Supplementary Note 4),. (Note that the model’s dynamics are deterministic but the general method is still called stochastic.) To find the market solution, our algorithm also uses Stochastic Programming to solve for a given value of X. But it finds a solution multiple times, each time for a different value of X, until it finds the solution that satisfies the consisteny criterion. These algorithms are coded in C/C++ and are described in detail in Supplementary Note 4.

In turn, in the results shown in Fig. 3, system 2, as well as in Figs. 4 and 5 and in the graphical tool, we assume that the dynamics of c and f follow Eqs. 6 – 9, but we consider harvest functions that are not given by either the optimal solution or the market solution. In Fig. 3, system 2, we consider harvest functions that follow the market solution until t = t 1 and after t = t 1 + 10, but between these times, the non-sustainable harvest decreases gradually from its maximal level to zero. In Fig. 4, we calculate Δ sus, which is the cumulative discount that emerges if the harvest is entirely sustainable, namely, H n = 0 and H s = x 1 + x 2 if t > 0. Also, in Fig. 4a, we consider three scenarios in which the non-sustainable harvest is higher in the beginning but eventually approaches zero, while H n + H s = x 1 + x 2.

After we determine the harvest functions, the functions c (t) and f (t) are calculated according to Eqs. 6 and 9. In turn, we calculate the discount rate and the cumulative discount according to Eq. 5 (where the cumulative discount is the integral over time of the discount rate). Specifically, for the case in which only sustainable harvest is used (Δ sus in Fig. 4), the discount rates are calculated in Supplementary Note 2 and are given by Eqs. B5 and B12. The prices are given by Eq. A10, and the total product is given by Eq. A11. All of these equations are derived in Supplementary Notes 1, 2.

---

### A novel oppositional binary crow search algorithm with optimal machine learning based postpartum hemorrhage prediction model [^f6de23e1]. BMC Pregnancy and Childbirth (2022). Medium credibility.

Whereasrepresent the opposite number and x ∈ R denotes a real number determined on range of x ∈[a, b]. While a =0 and b =1 Eq. (3) becomes

While there is a point P (x 1, x 2, …… x n) in n dimension coordinate and x 1, x 2,………, x n ∈ R later, the opposite pointis determined as its coordinates:

In such cases, have 2 values, x represent initial arbitrary value in [a, b] anddenotes the opposite values of x. They calculate f (x)&in all the iterations of OBCSA, later, employ on the evaluation function g ifselect x or else selectConsequently, the f l would be in range: f l ∈[f l min, f l max]. The opposite numbercan be determined by:

Later, evaluate the fitness for the first f l value and the fitness forin all the iterations. When, they select f l, or elsewould be selected. The stages of presented method can be given in the following.

Step1: The count of crows is n c =25, f l min =0.1, f l max =1.8, A P =0.3, and the maximal number of iterations is t max =100.

Step2: The position that represent the features are made by U (0, 1).

Step3: The fitness function (FF) can be determined by

Whereas C represent the classification performance, W represent the weighted factors in the range of zero and one, F all represent the overall amount of features and F sub signifies the length of elected feature.

Step4: The position of the crows are upgraded as Eq. (2)

Step5: Steps 3 & 4 are repetitive till a t max is attained.

---

### Dynamical regimes of diffusion models [^a7936fcc]. Nature Communications (2024). High credibility.

Speciation time

In order to show the existence of regimes I and II, and compute the speciation time, we focus on the following protocol which consists of cloning trajectories. We consider a backward trajectory starting at time t f ≫ 1 from a point x f drawn from a random Gaussian distribution where all components are independent with mean zero and unit variance. This trajectory evolves backward in time, through the backward process until time t < t f. At this time the trajectory has reached the point y (t), at which cloning takes place. One generates for τ < t two clones, starting from the same x 1 (t) = x 2 (t) = y (t), and evolving as independent trajectories x 1 (τ) and x 2 (τ), i.e. with independent thermal noises. We compute the probability ϕ (t) that the two trajectories ending in x 1 (0) and x 2 (0) are in the same class. Defining P (x 1, 0∣ y, t) as the probability that the backward process ends in x 1, given that it was in y at time t, the joint probability of finding the trajectory in y at time t and the two clones in x 1 and x 2 at time 0 is obtained as Q (x 1, x 2, y, t) = P (x 1, 0∣ y, t) P (x 2, 0∣ y, t) P (y, t). Then ϕ (t) is the integral of Q over x 1, x 2, y with the constraint (x 1 ⋅ m)(x 2 ⋅ m) > 0. This simplifies into a one-dimensional integral (see SI Appendix):where G (y, u, v) is a Gaussian probability density function for the real variable y, with mean u and variance v, and, Γ t = Δ t + σ 2 e −2 t. The probability ϕ (t) that the two clones end up in the same cluster is a decreasing function of t, going from ϕ (0) = 1 to ϕ (∞) = 1/2. In the large d limit, the scaling variable controlling the change of ϕ iswhich can be rewritten asby using. This explicitly shows that speciation takes place at the time scale t S on a window of time of order one. As detailed in SI Appendix, this expression for t S coincides with the one obtained from the general criterion (5). We show in Fig. 2 the analytical result from (6) and direct numerical results obtained for increasingly larger dimensions. This comparison shows that our analysis is accurate already for moderately large dimensions. In the limit of infinite d, the analytical curve in Fig. 2 suddenly jumps from one to zero at t / t S = 1, corresponding to a symmetry-breaking phase transition (or a threshold phenomenon) on the time scale t S. In the numerics, following finite size scaling theory, we have defined the speciation time as the crossing point of the curves for different d, which corresponds approximatively to ϕ (t S) = 0.775 and indeed is of the orderfor d → ∞. As it happens in mean-field theories of phase transitions, the large dimensional limit allows to obtain a useful limiting process. In our case, this leads to a full characterization of the asymptotic backward dynamics. At its beginning, i.e. in regime I, the overlap with the centers of the Gaussian model,  ± m ⋅ x (t), is of order. Defining, one can obtain a closed stochastic Langevin equation on q in a potential V (q, t) (see SI Appendix),where η (t) is square root of two times a Brownian motion, andAt large d, this potential is quadratic at times, and it develops a double well structure, with a very large barrier, when. The trajectories of q (t) are subjected to a force that drives them toward plus and minus infinity. The barrier between positive and negative values of q becomes so large that trajectories commit to a definite sign of q : this is how the symmetry breaking takes place dynamically at the time scale t S, in agreement with the previous cloning results. Regime II corresponds to the scaling limit q → ∞, where m ⋅ x (t) becomes of order d. In this regime, the rescaled overlap m ⋅ x (t)/ d concentrates, and its sign depends on the set of trajectories one is focusing on. Moreover, the stochastic dynamics of the x i correspond to the backward dynamics for a single Gaussian centered in  ± m. This shows that the dynamics generalizes, see SI Appendix (and alsofor similar results).

---

### Forecasting of the first hour aftershocks by means of the perceived magnitude [^44febb3a]. Nature Communications (2019). High credibility.

The parameter μ M in Eq. (3) represents the maximum value ofand we define it the perceived magnitude since it provides the magnitude of ground shaking felt close to the station. We find that the three mainshocks present roughly the same value of μ M, a result expected for the Kos and Lesvos earthquakes with comparable magnitudes and similar epicentral distance from the recording station. In the case of the Ischia earthquake, this result can be primarily attributed to the smaller epicentral distance from the recording station (see Table 1) but also to soil conditions at the seismic station location. Concerning the functional dependence of the scaling function F (x) in Eq. (3), the shape of the envelope function is dominated by different mechanisms such as source properties, attenuation, scattering effects, etc. The scaling collapse of Fig. 2c suggests that these features are, at a first approximation, incorporated in two seismic source-dependent parameters: the perceived magnitude μ M and the horizontal time rescaling τ M. This result is confirmed by the scaling collapse also obtained for the other mainshocks, when plottingas a function of (t − t 0)/ τ M (Fig. 3). In all cases we find that, for x < 10, F (x) ≃ log 10 (x) − q log 10 (x + x 0) with q ≃ 3.5 (continuous curve in Fig. 2c) and x 0 = (q − 1) q q /(1− q) ≃ 0.43 such that the maximum value of F (x) is equal to zero. We assume the same functional dependence of F (x) for all mainshocks in order to reduce the number of fitting parameters in our method. This fit corresponds to a power law decay of the amplitude of coda-waves of V (t)which anticipates the exponential decay at longer timescales. We wish to emphasize that our results are weakly affected by the specific choice of F (x) and we expect that other choices for F (x) lead to similar results.

---

### Origin of band gaps in graphene on hexagonal boron nitride [^dae619f3]. Nature Communications (2015). Medium credibility.

Moire pattern scalar functions and vector fields

The scalar functions used to obtain the moiré superlattice pattern for the height and the displacement vectors from their gradients have used Φ written as a Fourier expansion in G vectors as

where C G is in general a complex number, and we retain up to three nearest G vectors for the scalar field that preserves the symmetry of triangular lattices. The parameters C 0,, C 1,, C 2 andare real valued constants and we defined auxiliary functions f and g in terms of the triangular lattice structure factors similar as those used in a general tight-binding model of G. The Fourier expansion coefficients within the first shell consisting ofand the first shellare often good representation of the solutions that vary smoothly in real space and respect the symmetry of the triangular superlattice. For brevity in notation, here we use (x, y) to indicate the (d x, d y). The f function

is defined in terms of the structure factors

where G 1 =4π/3 a, where a is the real-space periodicity of the moiré superlattice and j =1, 2. These are momentum space analogues of the real space intersublattice hopping structure factors in a honeycomb lattice. The explicit form of the functions defined along the symmetry lines x =0 or y =0 can be obtained from sums of

The analytical expression for the g function shell contribution reduces to a simpler form

where G 2 =4 π, that for the symmetry lines reduces to

The vector fields such as in-plane forces, displacement vectors and stresses can be obtained as gradients of the scalar potentials given by the above forms that can preserve the symmetry of the triangular moiré superlattice. The vector field that can be obtained from the gradient of the scalar field is

---

### The minimal work cost of information processing [^927873c9]. Nature Communications (2015). Medium credibility.

Classical mappings and dependence on the logical process

Our result, which is applicable to arbitrary quantum processes, applies to all classical computations as a special case. Classically, logical processes correspond to stochastic maps, of which deterministic functions are a special case. As a simple example, consider the AND gate. This is one of the elementary operations computing devices can perform, from which more complex circuits can be designed. The gate takes two bits as input, and outputs a single bit that is set to 1 exactly when both input bits are 1, as illustrated in Fig. 2a.

The logical process is manifestly irreversible, as the output alone does not allow to infer the input uniquely. If one of the inputs is zero, then the logical process effectively has to reset a three-level system to zero, forgetting which of the three possible inputs 00, 01 or 10 was given; this information can be viewed as being discarded, and hence dumped into the environment. We can confirm this intuition with our main result, using the fact that a general classical mapping is given by the specification of the conditional probability p (x ′| x) of observing x ′ at the output if the input was x. Embedding the classical probability distributions into the diagonals of quantum states, the infinity norm in expression (2) becomes simply

---

### A genetic basis for the variation in the vulnerability of cancer to DNA damage [^483d5597]. Nature Communications (2016). Medium credibility.

Integral survival

The area under the curve was estimated by trapezoidal approximation. First, x axis values representing radiation doses 1, 2, 3, 4, 5, 6, 8, and 10 Gy were log 2 transformed. The survival values for each trapezoid were multiplied by the dose interval, [f (X 1)+ f (X 2)/2] × Δ X, summed and re-scaled by multiplying by (7÷log 2 10) so that integral survival is defined from 0 (completely sensitive) to 7 (completely resistant).

Clonogenic survival

Cells were plated at appropriate dilutions, irradiated and incubated for 7–21 days for colony formation. For chemical radiosensitization measurements, drug was added 24 h before irradiation. Colonies were fixed in a solution of acetic acid and methanol 1:3 (v/v) and stained with 0.5% (w/v) crystal violet as previously described. A colony was defined to consist of 50 cells or greater. Colonies were counted digitally using ImageJ software as described. Integration of survival as a function of dose, or area under the curve, was calculated using Prism, GraphPad Software (La Jolla, CA).

Information-based association score

The association between genomic alterations (for example, mutations or SCNA) or ssGSEA profiles for each gene set and the radiation response profile was determined using the IC.

---

### Large-scale DNA-based phenotypic recording and deep learning enable highly accurate sequence-function mapping [^6931bd38]. Nature Communications (2020). High credibility.

Normalization of biological replicates

In order to facilitate comparison of biological replicates, we capitalized on the 31 internal-standard RBSs. These serve as internal references spanning a large range of RBS activities and allow to compensate for potential batch effects and other systematic biases between replicates. Formally, for each of the 31 internal-standard RBSs, we denote by x and y the measured normalized integral of the flipping profile (IFP) for the biological replicate to be normalized and the reference replicate, respectively. We fit either a polynomial function of degree two,with f (x) = I + Ax + Bx 2, or its inverse f (x) = g −1 (x) with g (z) = I + Az + Bz 2, such that the mean-squared error between f (x) and y is minimized across the 31 measurement pairs. Moreover, we impose the following constraints on the parameters of f : first, RBSs that show no activity in one replicate should remain inactive in the other replicates (f (0) = 0). Second, RBSs whose discriminators are entirely flipped before induction in one replicate should exhibit that behavior in the other replicates (f (1) = 1). Third, the ranking of RBSs according to their strength should be preserved across replicates (f is monotonically non-decreasing in [0, 1]). It should be noted that, empirically, these assumptions appear to hold across the three biological replicates in this study. Imposing the first two constraints above reduces the number of free parameters of the polynomial function from three to one, resulting in the family of functions parametrized by A : f (x) = Ax + (1 − A) x 2. Moreover, the third constraint translates into the following bounds on the set of allowed values for the free parameter A : 0 ≤ A ≤ 2. This procedure was carried out for each pair of biological replicates. The quality of the resulting fits was then evaluated on the full data sets, excluding the 31 internal-standard RBSs that were used to optimize A.

---

### Zero-preserving imputation of single-cell RNA-seq data [^735252af]. Nature Communications (2022). High credibility.

A key challenge in analyzing single cell RNA-sequencing data is the large number of false zeros, where genes actually expressed in a given cell are incorrectly measured as unexpressed. We present a method based on low-rank matrix approximation which imputes these values while preserving biologically non-expressed genes (true biological zeros) at zero expression levels. We provide theoretical justification for this denoising approach and demonstrate its advantages relative to other methods on simulated and biological datasets.

---

### In-memory ferroelectric differentiator [^d3a25719]. Nature Communications (2025). High credibility.

Calculating the derivative of mathematical functions

Based on the above mechanism, we can calculate the derivative of mathematical functions. Figure 3a sketches how the domains configurations of 14 ferroelectric capacitors are used to store digit data from −7 to 7. As a result, the value of the function f (x) = − x can be presented by the evolution of the domain configuration sequence over x. The upward (downward) domains are written by +20 V (−20 V) voltage pulses (0.1 s) on bottom electrode while the top electrodes are grounded. Figure 3b proposes the strategy for calculating the differential value by collecting the charges due to ferroelectric domains switching. The upward (downward) domain switching is labeled purple and green, respectively. Note that this strategy requires a linear relationship between the sum of charges and the number of units performing domain switching. The evolution of transient currents and integral charges as a function of time are systematically readout during the inputting process of programing voltage pulses. Either in the case where the number of capacitors with upward domain switching is increased monotonically from 1 to 14 in steps of 1 (Fig. 3c), or in the case where the number of capacitors with upward domain switching is increased monotonically from 0 to 7 in steps of 1 and then kept constant at 7, while the number of capacitors with downward domain switching first remains constant at 7 and then decreases monotonically from 7 to 0 in steps of 1 (Fig. 3d), it clearly shows a linear relationship with a slope of 2.85 mC/unit between integral charges and hybrid (upward or downward) domains reversals. This linear relationship allows the analogue calculation of the differential value (B-A) between data A and data B, by evaluating the value of integral charges over 2.85 mC when it alters domains configuration from data A to data B.

---

### A solution to the collective action problem in between-group conflict with within-group inequality [^1e238285]. Nature Communications (2014). Medium credibility.

Equation (19) shows that individual effortsincrease with valuation v i. In contrast, individual reproductive success at equilibrium is

and thus decreases with valuation. That is, the higher costs paid by high-rank individuals negate their larger shares of the reward.

If not all group members make a positive effort, then the derivations become more complex. Let there be n e contributors and v be their total valuation (v =∑ v i where the sum is over n e contributors). Then, summing up equation (16) over all contributors, at equilibrium their total effort X satisfies to a quadratic equation

Of the two solutions of this quadratic, only the smallest one, X *, is relevant biologically (the other solution leads to negative fertilities). Note that cX *≤ n e + bn Σ v i < n (1+ b). (The first inequality follows from the requirement that all f i values are non-negative:, which one then sums up over all contributing individuals).

The individual effortscan be found from equation (16) by substituting X * into

This equilibrium is biologically feasible (that is, all), if

Equation (23) shows that individual effort increases with valuation. However, within-group reproductive success

and is decreasing with valuation v i because cX *≤ n (b +1).

By differentiating the expression for X *, one can show that X * always increases with v and generally decreases with n e. (The latter requires that v exceeds a certain threshold that depends on b and n but is relatively small). That is, decreasing the number of contributors and/or increasing their total share of the reward results in increasing group effort. Moreover, analytical derivation and numerical simulations strongly suggest that the derivative of v crit with respect to X is positive, so that the threshold valuation increases with decreasing the number of contributors. This leads to a recursive procedure for finding the equilibrium: (i) setfor all ranks for which inequality (20) is not satisfied; (ii) using appropriate values of n e and v, find X * from equation (22) ; (iii) if there are ranks for which inequality (24) is not satisfied, set their contributionsto zero, recompute n e and v, and return to step (ii).

---

### Effective plans for hospital system response to earthquake emergencies [^b4d880be]. Nature Communications (2020). High credibility.

Optimization of performance metric

We evaluate both waiting times and effective use of ambulances as the system performance metric, thus the metric includes two objective functions. The first objective function C 1 (X) measures waiting time across the city as the average time that a patient would take since the earthquake until completing treatment in the operating room.

X represents a vector containing all the decision variables of flow x i, j (t) in edges and the storage y i (t) in the triage nodes. The numerator of C 1 (X) represents the total number of patients passing through the operating rooms (from each triage node, i ∈ Γ to the corresponding discharge node j = i + n h ∈ Λ) multiplied by their respective times to complete treatment, whereas the denominator is the total number of patients arriving to the triage areas. The time horizon t f is carefully chosen to have enough modeling time to treat the patients. However, in few simulations with significant number of patients and not many functional operating rooms, a couple of terms are added to the numerator, one with the remainder patients in the triage areas, t f × ∑ i ∈ Γ y i (t f) × d t, and another with the remainder patients in the ambulances,, in order to properly incorporate the unmet demands at the end of the simulation into C 1 (X).

The second objective function measures ambulance usage as the total number of patients transported in ambulances. The objective function C 2 (X) is normalized by the total number of patients analogously to C 1 (X).

We define a system cost C (X) as a weighted sum of C 1 (X) and C 2 (X) to find a Pareto-optimal solution.

After assessing multiple α 1 and α 2 values, we minimized C (X) using values of 0.90 and 0.1, respectively. Smaller α 2 values resulted in inefficient ambulance usage with small reductions in waiting times, requiring some patients to be transferred multiple times in ambulances before being treated. Larger α 2 significantly increased waiting times, thus these α 2 values do not appropriately represent that the priority in the formulation is to minimize waiting times over to use ambulances with efficiency. We find the best set of decisions, vector that contains the values of flow variables x i, j (t) and storage variables y i (t) which minimize C (X).

---

### To infinity and some glimpses of beyond [^0f9a5f29]. Nature Communications (2017). Medium credibility.

Results

Ordinary differential equations

The standard textbook ODE for collapse in finite time (and its solution by direct integration) reads:The collapse time t * = 1/ x (0), is fully determined by the initial condition, and the textbook presentation usually stops here. A numerical solver would overflow close to (but before reaching) t *; yet we can bypass this infinity by appropriately transforming the dependent variable x near the singularity. Indeed, the good quantit y y ≡ 1/ x ≡ x −1, satisfies the good differential equation d y /d t = −1; this equation will help cross the infinity (for x) by crossing zero and smoothly emerging on the other side (for y). Once infinity is crossed, we can revert to integrating the initial (bad, but now tame again) equation for x.

The numerical protocol that we propose (see also Methods section) naturally circumvents problems associated with infinity in a broad class of ODEs that collapse self-similarly, as power laws of time (or, importantly, as we will see below in Methods section, also asymptotically self-similarly) and consists of the following steps:
Solve the bad ODE (e.g. Eq. (1)) for a while, continuously monitoring, during the integration, its growth toward collapse.
If/when the approach to collapse is detected, estimate its (asymptotically) self-similar rate (the exponent of the associated power law, e.g. −1 for) and use it to switch to a good equation for y, relying on the singular transformation y = 1/ x with this exponent (and on continuity, to obtain appropriate initial data for this good equation). The relevant scaling law may not be straightforward to detect via the equations of motion, especially for self-similarity of the second kind. Nevertheless, a numerical identification utilizing, e.g. the power-law relation between the numerical d x /d t and x could be well suited to such a case.
Run this good equation for y until 0 (or ∞ for the former, bad equation) is safely crossed, computationally observing for x an (asymptotically) self-similar return from infinity.
Finally, transform back to the bad equation (now tamed, as infinity has been crossed) and march it further forward in time.

---

### Linkage analysis and map construction in genetic populations of clonal F1 and double cross [^34184c9e]. G3 (2015). Low credibility.

Recombination frequency estimation in scenarios 2 and 3 in clonal F 1 progenies

In scenario 2, locus 1 has four genotypes A 1 C 1, A 1 D 1, B 1 C 1, and B 1 D 1, and locus 2 has two genotypes X 2 C 2 and X 2 D 2. In scenario 3, locus 1 has four genotypes A 1 C 1, A 1 D 1, B 1 C 1, and B 1 D 1, and locus 2 has two genotypes A 2 X 2 and B 2 X 2. Table 3 shows theoretical frequencies of the eight identifiable genotypes at the two loci. The theoretical frequencies do not contain the female recombination frequency in scenario 2, and they do not contain the male recombination frequency in scenario 3. Therefore, r F cannot be estimated in scenario 2; r M cannot be estimated in scenario 3. MLE of r M in scenario 2 can be calculated from its likelihood functions, given in Equation (4).

Table 3 
Theoretical frequencies of the eight identifiable genotypes in the clonal F 1 population

where n i is the observed sample size for the i th genotype (Table 3), n i : j is the sum of n i to n j, and n is the total sample size (i.e. n = n 1:8). Define the estimate of r in Equation (5).It can be easily seen that the estimate thus defined is less than 0.5. In addition, the estimate in Equation (5) is MLE of r, when directly calculated from its likelihood function.

MLE of r F in scenario 3 can be calculated from its likelihood function, given in Equation (6).where n i is the observed sample size of the i th genotype (Table 3), n i : j is the sum of n i to n j, and n is the total sample size (i.e. n = n 1:8). Define the estimate of r in Equation (7).Similar to Equation (5), the estimate thus defined is less than 0.5, and is MLE of r.

---

### Linkage analysis and map construction in genetic populations of clonal F1 and double cross [^238cf0af]. G3 (2015). Low credibility.

Recombination frequency estimation in scenarios 5 and 7 in clonal F 1 progenies

In scenario 5, locus 1 has two genotypes X 1 C 1 and X 1 D 1, and locus 2 has two genotypes X 2 C 2 and X 2 D 2. In scenario 6, locus 1 has two genotypes A 1 X 1 and B 1 X 1, and locus 2 has two genotypes A 2 X 2 and B 2 X 2. Table 5 shows theoretical frequencies of the four identifiable genotypes at the two loci. Obviously, theoretical frequencies do not contain the female recombination frequency in scenario 5 and do not contain the male recombination frequency in scenario 7. Thus, r F cannot be estimated in scenario 5; r M cannot be estimated in scenario 7. MLE of r M in scenario 5 can be calculated from its likelihood functions, given in Equation (9).

Table 5 
Theoretical frequencies of the four identifiable genotypes in the clonal F 1 population

where n i is the observed sample size of the i th genotype (Table 5), n i : j is the sum of n i to n j, and n is the total sample size (i.e. n = n 1:4). Define the estimate of r in Equation (10).

MLE of r F in scenario 7 can be calculated from its likelihood functions, given in Equation (11). Define the estimate of r in Equation (12).Similar to Equation (5) and Equation (7), the estimates defined in Equation (10) and Equation (12) are less than 0.5, and are MLE of r for scenarios 5 and 7, respectively.

---

### A solution to the collective action problem in between-group conflict with within-group inequality [^7f1d8bb1]. Nature Communications (2014). Medium credibility.

Methods

Fitness

In our models, the fitness of individual i from group j can be written as

To study our models, we use the evolutionary invasion analysis (adaptive dynamics) approach, which focuses on the invasion fitness w (y | x), that is, the fitness of a rare mutant with trait y in a monomorphic population of individuals with trait x. We assume that the effort for each rank is specified genetically and that there is only a single segregating mutation across all the ranks at any one time.

Egalitarian groups

Assume that group members share the reward equally so that nv i =1 for each i. To evaluate the change in individual effort x per generation, one needs to calculate the selection gradient of the invasion functionat y = x. To find the invasion fitness w (y | x), we need to set x ij = y,, f ij =1+ bGP j − cy,and

in equation (10). In the limit of large G, the selection gradientat y = x is proportional to 1+ b − cnx. Then, one finds that at equilibrium

Summing up the individual contributions, the total group effort X *=(1+ b)/ c and thus it does not depend on n.

Let r 0 and r 1 be the probabilities that a randomly chosen group-mate of a mutant and a randomly chosen individual from a different group have the same mutation. Then, the equilibrium effort x * is increased by a factor

Note that if female offspring disperse randomly between groups while male offspring stay in the natal group, the males within the group will be genetically related with average relatedness. If the number of groups is large, r 1 will be very close to zero.

Assume that the probability of success is. Then,

One can show that with relatedness and decisiveness, the total group effort X * increases with the group size n (assuming r 1 << r 0).

---

### Cooperating with machines [^7ef161ab]. Nature Communications (2018). Medium credibility.

Since Alan Turing envisioned artificial intelligence, technical progress has often been measured by the ability to defeat humans in zero-sum encounters (e.g., Chess, Poker, or Go). Less attention has been given to scenarios in which human-machine cooperation is beneficial but non-trivial, such as scenarios in which human and machine preferences are neither fully aligned nor fully in conflict. Cooperation does not require sheer computational power, but instead is facilitated by intuition, cultural norms, emotions, signals, and pre-evolved dispositions. Here, we develop an algorithm that combines a state-of-the-art reinforcement-learning algorithm with mechanisms for signaling. We show that this algorithm can cooperate with people and other algorithms at levels that rival human cooperation in a variety of two-player repeated stochastic games. These results indicate that general human-machine cooperation is achievable using a non-trivial, but ultimately simple, set of algorithmic mechanisms.

---

### Neural representations of situations and mental States are composed of sums of representations of the actions they afford [^f2cc265a]. Nature Communications (2024). High credibility.

Human behavior depends on both internal and external factors. Internally, people's mental states motivate and govern their behavior. Externally, one's situation constrains which actions are appropriate or possible. To predict others' behavior, one must understand the influences of mental states and situations on actions. On this basis, we hypothesize that people represent situations and states in terms of associated actions. To test this, we use functional neuroimaging to estimate neural activity patterns associated with situations, mental states, and actions. We compute sums of the action patterns, weighted by how often each action occurs in each situation and state. We find that these summed action patterns reconstructed the corresponding situation and state patterns. These results suggest that neural representations of situations and mental states are composed of sums of their action affordances. Summed action representations thus offer a biological mechanism by which people can predict actions given internal and external factors.

---

### A space-time tradeoff for implementing a function with master equation dynamics [^57fa10b0]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### Linkage analysis and map construction in genetic populations of clonal F1 and double cross [^4052a946]. G3 (2015). Low credibility.

Recombination frequency estimation in scenarios 6 and 8 in clonal F 1 progenies

In scenario 6, locus 1 has two genotypes X 1 C 1 and X 1 D 1, and locus 2 has three genotypes A 2 A 2, A 2 B 2, and B 2 B 2. In scenario 8, locus 1 has two genotypes A 1 X 1 and B 1 X 1, and locus 2 has three genotypes A 2 A 2, A 2 B 2, and B 2 B 2. Table 6 shows theoretical frequencies of the six identifiable genotypes at the two linked loci. The theoretical frequencies do not contain the female recombination frequency in scenario 6 and do not contain the male recombination frequency in scenario 8. Thus, r F cannot be estimated in scenario 6, and r M cannot be estimated in scenario 8. MLE of r M in scenario 6 can be calculated from its likelihood function, given in Equation (13).

Table 6 
Theoretical frequencies of the six identifiable genotypes in the clonal F 1 population

where n i is the observed sample size of the i th genotype (Table 6) and n i : j is the sum of n i to n j. Define the estimate of r in Equation (14).

Maximum likelihood estimates of r F in scenario 8 can be calculated from its likelihood function, given in Equation (15). Define the estimate of r in Equation (16).Similar to Equation (5), Equation (7), Equation (10), and Equation (12), the estimates defined in Equation (14), Equation (15), and Equation (16) are less than 0.5 and are MLE of r for scenarios 6 and 8, respectively.

---

### One-shot learning for solution operators of partial differential equations [^d6bcc900]. Nature Communications (2025). High credibility.

Learning the solution operator for a linear PDE

We first demonstrate the capability of our method with a pedagogical example of a one-dimensional Poisson equationwith the zero Dirichlet boundary condition, and the solution operator is.

As described in the one-shot learning method based on the principle of locality, we first need to select a “canonical” local domain, which is a line segment in 1D case. We choose the simplest local solution operator using 3 nodes (Fig. 2 b)with Δ x = 0.01 from. The training datasetonly has one data point(Fig. 3 a), whereis generated based on. Using the training dataset, we placeat different locations to generate data points and train the local solution operator. For a new PDE condition f, we apply our trainedand enforce the zero Dirichlet boundary condition at x = 0 and x = 1 to find the solution. In Fig. 3 c, we show examples of testing cases f = f 0 + Δ f with different σ test. When σ test is larger, there is a more significant discrepancy between f 0 and f.

Fig. 3 
1D Poisson equation.

a The training data includes a randomgenerated from GRF and the corresponding solution. b The convergence of L 2 relative errors of different approaches for various test cases. The shaded regions represent one standard deviation of the L 2 relative errors from 100 test cases. c Testing examples of random f = f 0 + Δ f with Δ f sampled from GRF of σ test = 0.02, 0.05, 0.10, 0.15 and l test = 0.1. d Prediction example of different approaches for various test cases. e Prediction errors (the geometric mean and standard deviation) when training by f random with amplitude σ train and testing on functions with amplitude σ test. The length scales l train = 0.01 and l test = 0.10 are fixed. f Prediction errors (the geometric mean and standard deviation) when training by f random with different length scale l train and amplitude σ train = 0.10, and testing on functions with amplitude σ test = 0.10 and l test = 0.10.

---

### The eighty five percent rule for optimal learning [^5399633a]. Nature Communications (2019). High credibility.

Methods

Optimal error rate for learning

In order to compute the optimal difficulty for training, we need to find the value of Δ that maximizes the learning gradient, ∂ ER/ ∂β. From Eq. (3) we haveFrom here the optimal difficulty, Δ *, can be found by computing the derivative of the gradient with respect to Δ, i.e.Setting this derivative equal to zero gives us the following expression for the optimal difficulty, Δ *, and error rate, ER * where p ′(x) denotes the derivative of p (x) with respect to x. Because β and Δ * only ever appear together in these expressions, Eq. (26) implies that β Δ * is a constant. Thus, while the optimal difficulty, Δ *, changes as a function of precision (Fig. 1c), the optimal training error rate, ER * does not (Fig. 1d). That is, training with the error rate clamped at ER * is guaranteed to maximize the rate of learning.

The exact value of ER * depends on the distribution of noise, n, in Eq. (2). In the case of Gaussian noise, we havewhich implies thatand that the optimal difficulty isConsequently the optimal error rate for Gaussian noise isSimilarly for Laplacian noise and Cauchy noise (p (x) = (π (1 + x 2)) −1) we have optimal error rates of

---

### Methods for analysis of skewed data distributions in psychiatric clinical studies: working with many zero values [^bca98a55]. The American Journal of Psychiatry (2004). Low credibility.

Objective

Psychiatric clinical studies, including those in drug abuse research, often provide data that are challenging to analyze and use for hypothesis testing because they are heavily skewed and marked by an abundance of zero values. The authors consider methods of analyzing data with those characteristics.

Method

The possible meaning of zero values and the statistical methods that are appropriate for analyzing data with many zero values in both cross-sectional and longitudinal designs are reviewed. The authors illustrate the application of these alternative methods using sample data collected with the Addiction Severity Index.

Results

Data that include many zeros, if the zero value is considered the lowest value on a scale that measures severity, may be analyzed with several methods other than standard parametric tests. If zero values are considered an indication of a case without a problem, for which a measure of severity is not meaningful, analyses should include separate statistical models for the zero values and for the nonzero values. Tests linking the separate models are available.

Conclusions

Standard methods, such as t tests and analyses of variance, may be poor choices for data that have unique features. The use of proper statistical methods leads to more meaningful study results and conclusions.

---

### Complex modeling with detailed temporal predictors does not improve health records-based suicide risk prediction [^b4f4b041]. NPJ Digital Medicine (2023). Medium credibility.

Variables describing total days’ supply of X dispensed in specific time periods. Missing days supply will be treated as 0 (i.e. ignored) in all sums.
R10 Total days’ supply of X dispensed in last 1 month.
R11 Total days’ supply of X dispensed in last 3 months.
R12 Total days’ supply of X dispensed in last 12 months.
R13 Total days’ supply of X dispensed in last 24 months.
R14 Total days’ supply of X dispensed in last 60 months.

Variables describing “rate” of days’ supply of X in specific time periods while enrolled. Missing days supply will be treated as 0 (i.e. ignored) in all sums.
R15 Days’ supply of X dispensed in last 3 months divided by # of months enrolled in last 3 months.
R16 Days’ supply of X dispensed in last 12 months divided by # of months enrolled in last 12 months.
R17 Days’ supply of X dispensed in last 24 months divided by # of months enrolled in last 24 months.
R18 Days’ supply of X dispensed sin last 60 months divided by # of months enrolled in last 60 months.

Variables describing timing of X:
R19 Most recent month with X; for those who do not have X (ever), set value for most recent month with X to −5.
R20 Most recent month with X; for those who do not have X (ever), set value for most recent month with X to 65.
R21 First observed month with X; for those who do not have X (ever), set value for most recent month with X to −5.
R22 First observed month with X; for those who do not have X (ever), set value for most recent month with X to 65.

Information on days’ supply of most recent month with X:
R23 Binary indicator for if the person is likely to have drugs on hand the day of the index visit, calculated as days’ supply of most recent month with X divided by 30.4375 minus the # of months ago X occurred. (Yes, this will be a bit crude, but hopefully it will have some predictive power).
R24 Days’ supply of most recent month with X. Those who do not have X (ever), set days’ supply to 0 for those with missing or invalid (i.e. negative) days’ supply values, also set them to zero.

---

### Zero shot molecular generation via similarity kernels [^e324c847]. Nature Communications (2025). High credibility.

Introduction

The combinatorial scaling of the available chemical space with molecule size is one of the main challenges in the design of new molecules and materials. Generative modelling aims to solve this by directly proposing structures with desirable properties, without exhaustively enumerating and screening the candidates. Recently, diffusion-based models have achieved impressive results in molecular dockingand generation of linkers, drug-like molecules, and crystal structures,.

Diffusion models are trained to reverse a stochastic noising process, which gradually corrupts samples of training data until they are indistinguishable from samples drawn from an uninformative prior distribution, such as a standard Gaussian –. Once the reverse of this process is learnt, it can be applied to transform easily-sampled random noise into independent samples from the approximate data distribution. In particular, diffusion models regress the score s (x, t), defined as the gradient of the log-likelihood of a time-dependent distribution p (x ; t) that interpolates between the data distribution at t = 0, p (x ; 0) = p data (x), and a Gaussian distribution at t = T,.Using the score, we can generate new samples by numerically integrating a stochastic differential equation describing the time reversal of the noising process, which has a known analytic form. Because the data distribution is provided only in the form of samples, the score is intractable analytically. However, it can be efficiently learnt via an implicit objective that has a closed form when the limiting distribution at t = T is taken to be Gaussian, an approach known as denoising score matching –.

---

### Geomagnetic spikes on the core-mantle boundary [^ccf78f2d]. Nature Communications (2017). Medium credibility.

Values of A, σ, θ c and φ c are varied simultaneously to match the observed spike intensity and width. Any single peak in intensity can always be produced by varying A and σ. Spike width is defined with respect to the peak intensity. The latitudinal extent of the spike is hard to estimate, partly because there is little coeval data north and south of Jordan and Israel and partly because of the latitudinal increase in dipole field strength. We therefore define the regional spike width δ x (r) as the longitudinal range over which F > F max / x, where x is a parameter that is >1. At 1000 BC the CALS10k.2 and PFM9K field modelsgive F (a)≈60–65 μT at latitude λ =30° north, where a is Earth's radius; together with the value of F max (a) from Fig. 4d, we obtain x ≈1.6–2.0. Individual data (Fig. 4b) yield a similar result and so we focus on x =2, although using x =1.6 makes little difference.

With=0, decreasing σ towards zero reduces the width of the surface spike towards a minimum of(Supplementary Figs 1–3), much larger than the observed value of δ 2 (a)≈20°. This behaviour can be understood by considering the limit of a delta function spike (σ →0) at the CMB, which has equal power at all wavenumbers. The field decays with increasing r as (a / r) l +2 (Methods) and so the small-scale structure that is added to the CMB spike by decreasing σ is geometrically attenuated; the field exhibits no spike-like feature at Earth's surface (Fig. 5). A suite of models with=0 predict that the dominant surface expression of the spike is a change in the l =1 dipole component of the field for all 0.5°≤ σ ≤40° and 200≤ A ≤800 mT, although in relative terms it does not make a large immediate change to an existing dipole moment. Withdefined by the CALS10k.2 field model at 1000 BC the spike is much wider, even for small values of σ, owing to lateral intensity variations in the field model (compare Supplementary Fig. 3 and Fig. 4d).

---

### Large meta-analysis of multiple cancers reveals a common, compact and highly prognostic hypoxia metagene [^cfd01b36]. British Journal of Cancer (2010). Low credibility.

Materials and methods

Seed clustering

The process begins with k seed genes, ∏={ π 1, π 1,… π k } (‘gene’ is used throughout for convenience, although ‘transcript’ is generally more accurate). Spearman's correlation, ρ, is computed between seeds and genes Y ={ y 1, y 2,… y m } in a data set of n samples, X ={ x 1, x 2,… x n }. For each seed/gene pair, their ‘affinity’ is defined as:where θ t and θ s define extent and sharpness of the cluster. When θ s → 0, δ reduces to the step function with δ =0 if ρ 2 < θ t, δ =1 if ρ 2 > θ t. In this limit, the method is parameter free, and this will be used in this study. θ t is defined objectively using a probability threshold, α, of observing a given correlation if the null hypothesis (i.e. no association) was true. This needs to be corrected for multiple testing to account for the size of Y ; here, α =0.05 after Bonferroni correction was considered. Finally, a membership function is defined:

An increasing γ indicates stronger membership of a gene to a seed cluster.

Shared neighbourhood

The shared neighbourhood, S, between two seeds is defined as:where γ is the membership (Eq. 2). Two seeds are considered to carry a high degree of related information if their clusters share many genes (high S values). A sign function is also defined:where sgn(x) is the sign function: sgn(x)=1 if x >0, sgn(x)=−1 if x <0. If two seeds are correlated with their shared features in the same direction, F =1 (seeds are fully concordant); if they are correlated with their shared features in opposite direction, F =−1.

---

### Experimental relativistic zero-knowledge proofs [^6725418d]. Nature (2021). Excellent credibility.

Protecting secrets is a key challenge in our contemporary information-based era. In common situations, however, revealing secrets appears unavoidable; for instance, when identifying oneself in a bank to retrieve money. In turn, this may have highly undesirable consequences in the unlikely, yet not unrealistic, case where the bank's security gets compromised. This naturally raises the question of whether disclosing secrets is fundamentally necessary for identifying oneself, or more generally for proving a statement to be correct. Developments in computer science provide an elegant solution via the concept of zero-knowledge proofs: a prover can convince a verifier of the validity of a certain statement without facilitating the elaboration of a proof at all 1 . In this work, we report the experimental realization of such a zero-knowledge protocol involving two separated verifier-prover pairs 2 . Security is enforced via the physical principle of special relativity 3 , and no computational assumption (such as the existence of one-way functions) is required. Our implementation exclusively relies on off-the-shelf equipment and works at both short (60m) and long distances (≥400m) in about one second. This demonstrates the practical potential of multi-prover zero-knowledge protocols, promising for identification tasks and blockchain applications such as cryptocurrencies or smart contracts 4 .

---

### Animal diversity and ecosystem functioning in dynamic food webs [^7a010bfd]. Nature Communications (2016). Medium credibility.

Ecosystem functions respond to animal diversity

We investigated how increasing animal diversity affects ecosystem functioning, defined as the summed biomass stocks of plant and animal species (P and A, respectively), the rates of consumption on the plant community (F P) and within the animal community (that is, intraguild predation, F A), as well as the energetic losses due to metabolism of both compartments (X P and X A ; Fig. 1). We found that as the number of animal species increased, the total biomass of animals, A, increased as well (Fig. 4a ; a =0.35, and α =1.2 as least squares estimates of power-law relationship of the shapefitted as a linear model on log–log transformed data; Residual standard error on degrees of freedom: s.e. = 0.012; Goodness of fit as indicated by Coefficient of determination: R 2 =0.31). In contrast, the total biomass of plants, P, stagnated (Fig. 4b ; a =29.75, and α =−0.08, s.e.=0.005, R 2 =0.01). Along with the increase in animal biomasses, we observed an increase in intraguild predation rates, F A (Fig. 4c, a =0.004, and α =0.92, s.e.=0.012, R 2 =0.2). Despite the stable total biomass of plants, we found an increase in the consumption of plants by animals, F P (Fig. 4d ; a =0.1 and α =0.61, s.e.=0.005, R 2 =0.38). Moreover, with increasing animal species richness the total animal metabolic rates, X A, increased (Fig. 4e ; a =0.05 and α =0.58, s.e.=0.005, R 2 =0.38) while the total metabolic rates of plants, X P, decreased (Fig. 4f ; a =6.72 and α =−0.54, s.e.=0.006, R 2 =0.3).

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^aeeaae45]. Nature Communications (2025). High credibility.

Fitting models to data is an important part of the practice of science. Advances in machine learning have made it possible to fit more-and more complex-models, but have also exacerbated a problem: when multiple models fit the data equally well, which one(s) should we pick? The answer depends entirely on the modelling goal. In the scientific context, the essential goal is replicability: if a model works well to describe one experiment, it should continue to do so when that experiment is replicated tomorrow, or in another laboratory. The selection criterion must therefore be robust to the variations inherent to the replication process. In this work we develop a nonparametric method for estimating uncertainty on a model's empirical risk when replications are non-stationary, thus ensuring that a model is only rejected when another is reproducibly better. We illustrate the method with two examples: one a more classical setting, where the models are structurally distinct, and a machine learning-inspired setting, where they differ only in the value of their parameters. We show how, in this context of replicability or "epistemic uncertainty", it compares favourably to existing model selection criteria, and has more satisfactory behaviour with large experimental datasets.

---

### A cochlear-bone wave can yield a hearing sensation as well as otoacoustic emission [^4a2d305b]. Nature Communications (2014). Medium credibility.

The coefficients G 1 (k) and G 2 (k) are defined as follows:

Here we have used the abbreviation L (k)=[2 i ω ρ (F 1 + F 2) w bm +3 F 1 F 2 Z bm (x)]/[3 A 1 A 2 Z bm (x)] with F 1/2 = A 1/2 k 2 − cω 2 ρ. The dispersion relation L(k)=0 has been derived from the eigenvalues of the matrix equation (10).

The Green’s functions for bone stimulation can be derived analogously. Assume that both cochlear chambers, at a certain longitudinal location x 0, are sinusoidally compressed and expanded:

We make the following ansatz for the Greens functions:

which yields the amplitude equations

The solutions are

with L (k) as given above. In the symmetric case of equal chamber areas, A 1 = A 2, we obtain W 1 (k)= W 2 (k). No basilar-membrane displacement then arises, because the pressures in both chambers are equal.

When attempting to compute the integral in the ansatz for the Green’s functions (equations (25) and (29)), we encounter a problem: the integrand has a singularity at the wave vectors k for which L (k)=0, that is, at those wave vectors that obey the dispersion relation. However, we can employ the residue theorem of complex analysis to compute the integrals. Indeed, for propagation apical of the generation site, that is at a location x < x 0, we can close the contour in the upper-half plane because the integrand there is exponentially suppressed. The integral then only involves a contribution from the poles in the upper-half plane. In the case of basilar-membrane stimulation, we obtain a contribution proportional to. The pressures p 1/2 (− k bm, ω, x 0) represent the pressures of the basilar membrane mode in the two chambers

Analogous results can be obtained for the cochlear-bone wave with k cb (x).

In the opposite case, for a cochlear location basal to the generation site, x > x 0, the integration path can be closed in the lower-half plane.

---

### Zero-sum beliefs and the avoidance of political conversations [^59f88b7e]. Communications Psychology (2024). Medium credibility.

Although researchers have argued that exposure to diverse views may help reduce political divisions in society, people often avoid discussing politics with ideologically opposed others. We investigate the avoidance of political conversations surrounding highly contested elections in Israel and the U.S. Specifically, we examine the relationship between people's belief that politics is a zero-sum game and their tendency to avoid talking about politics with ideologically opposed others. In two studies conducted in the days leading up to their countries' elections, we found that Israeli and American voters who view politics as zero-sum avoided political discussions with ideologically opposed others. Furthermore, zero-sum beliefs about politics statistically predicted the avoidance of political conversations through two distinct mechanisms: perceived conflict and a lack of receptiveness to opposing views. Finally, in a longitudinal design, we found that zero-sum beliefs about politics statistically and robustly predicted the avoidance of political conversation one week later.

---

### Topaz-denoise: general deep denoising models for cryoEM and cryoET [^64ace2ba]. Nature Communications (2020). High credibility.

Loss functions and the Noise2Noise framework

The Noise2Noise framework takes advantage of the observation that we can learn models that recover statistics of the noise distribution given paired noisy observations of the same underlying signal. Given a ground truth signal, y, we observe images of this signal that have been corrupted by some probabilistic noise process, x ~ Noise (y). Given paired noisy observations for matched signals, x a ~ Noise (y) and x b ~ Noise (y), we can learn a function that recovers statistics of this distribution. This is accomplished by finding parameters of the denoising function, f with parameters θ, such that the error between the denoised sample f (x a) and raw x b are minimized. The form of this error function determines what statistics of the noise distribution we learn to recover. Given a dataset, X, containing many such image pairs, minimizing the L2 error over paired samples,

finds f with mean-seeking behavior. Minimizing the L1 error over paired samples,

finds f with median-seeking behavior. Finally, minimizing the L0 error over paired samples,

finds f with mode-seeking behavior. This last objective is not differentiable and requires a smoothing term to minimize with standard gradient descent. We refer the reader to Lehtinen et al.for details on this training objective. In practice, the errors of denoising both x a and x b are used for training.

Training details

For neural networks, weights are initialized using the default initialization in PyTorch. For affine models, weights are initialized to zero. All models are trained using the Adagrad variant of stochastic gradient descentwith a learning rate of 0.001 for 100 epochs. We train on 800 by 800 patches randomly sampled from each micrograph using a minibatch size of 4. As data augmentation, these patches are randomly rotated and mirrored. In order to avoid interpolation artifacts, images are only rotated by 90°, 180°, or 270°. Images are first normalized at the whole micrograph level by subtracting the mean pixel intensity and dividing by the standard deviation of the pixel intensities. Models were trained on a single NVIDIA V100 GPU with 32 GB of VRAM. Training took about 15 h per model.

---

### Constructing future behavior in the hippocampal formation through composition and replay [^d2fa35df]. Nature Neuroscience (2025). High credibility.

Compositional codes facilitate zero-shot behavior

To explore this potential for zero-shot generalization, we compare two agents (Fig. 2a). In both cases, we train a feedforward network, f, through supervised learning on ground-truth optimal policies to predict optimal actions, a, given a state representation, s. For the first agent (traditional), states are unrelated to environmental features—they only represent space, x. For the second agent (compositional), states are compositions of vector cells to environment features—they combine walls, objects and rewards (w, o and r). We do not include space (x) as part of this composition as it is not required for action selection (but we will need the spatial component later to build compositional maps in memory). In each case, we sample multiple environments with walls, objects and rewards, placed at random. The optimal policy is defined as the local actions that minimize the number of steps to reward from each location. We then sample (state representation and optimal action) pairs (s and a) as training examples to train a feedforward network that maps state representations to optimal actions through supervised learning. We evaluate the network’s performance by measuring whether following the learned policy successfully navigates to the goal from a set of test locations (Policies that generalize).

---

### Neural representations of situations and mental States are composed of sums of representations of the actions they afford [^a4a5ae6c]. Nature Communications (2024). High credibility.

As the voxelwise reliability threshold increases, pattern similarity reliability tends to first increase – as irrelevant voxels are discarded – and then decrease – as important voxels are discarded. We selected an optimal voxelwise threshold using the rise-and-fall dynamic. Specifically, we selected the last voxelwise threshold for which the pattern similarity reliability continued to increase consistently (i.e. the last voxelwise threshold before the first observed decrease in pattern similarity reliability). Voxels with reliabilities above this threshold were included in the subsequent pattern summation and pattern similarity analyses.

It is important note that, like other feature selection techniques, reliability-based feature selection has the potential to produce circularity, if it is inappropriately applied,. In particular, if the units of analysis over which one calculates reliability are also the only independent or dependent variable in one’s analysis, then this carries the risk for circularity. For example, if we were to try to perform a 60-way classification on the situations (i.e. where the situations are the labels), based on the corresponding patterns of brain activity, using reliability-based feature selection would positively bias performance. In that case, it would be necessary to perform feature selection using separate fMRI data from the classification. However, in the present case, the results hinge on a third variable – co-occurrence ratings – which were not involved in the feature selection process. As a result, feature selection and subsequent analyses can be perform on the same data without bias.

---

### Holding and amplifying electromagnetic waves with temporal non-foster metastructures [^110f6371]. Nature Communications (2025). High credibility.

Now, at t > t 2, the permittivity of the medium is changed back to a positive value. Remarkably, this causes the wave inside the medium to thaw, allowing it to propagate again. However, we note that for t > t 2, in addition to the frequency change fromto, two interesting things happen: i) the wave behaves almost like a “standing wave” since the FW and BW waves have approximately (but not exactly) the same amplitude and ii) the amplitude of the electromagnetic wave has been increased up to ~30 times compared to its value for t < t 1. It is important to highlight the fact that for t > t 2, we do not have a perfect standing wave, because strictly speaking the amplitudes of the FW and the BW waves, albeit very high, are not exactly the same. In fact, we show analytically that there is a net non-zero Poynting vector value in the direction of + x, which guarantees the conservation of momentum. To better observe these results, the electric field distribution at times t < t 1, t 1 < t < t 2, and t > t 2 are also shown in Supplementary Materials section 6 (Figs. S 3F–H, respectively). For the sake of completeness, the evaluated electric field at the two single locations x /λ 0 = 2 and x /λ 0 = 2.25 are also shown in Fig. 3E. Additionally, the instantaneous Poynting vector for the total, FW and BW waves as a function of space and time are shown in the Supplementary Materials Fig. S 5. By comparing the results for the negative square function (Fig. 3D–F) with those of the positive square function (Fig. 3A–C) we can see how the phenomena of holding and amplifying the wave and the frequency conversion can be achieved with the proposed temporal slab involving non-dispersive negative permittivity (based on the non-Foster notion). See Supplementary Movie 1, 2 for the cases shown in Fig. 3A–C and Fig. 3D–F, respectively.

---

### Should the PANSS be rescaled? [^d91cc2a9]. Schizophrenia Bulletin (2010). Low credibility.

The design of the Positive and Negative Syndrome Scale (PANSS) with item levels ranging from 1 to 7 leads to the trivial result that the 30-item scale's zero level (no symptoms) is 30. This causes serious problems when ratios are calculated which always implicitly depend on a natural zero point (equals 0). Recent publications concerning efficacy of antipsychotics correctly suggest a subtraction of 30 points to every PANSS before calculating percent change (PC). Nevertheless, the traditional approach using uncorrected scores is still in common practice. This analysis aims to clarify which approach is the most appropriate from a statistical perspective.For analysis, data from a naturalistic study on 400 patients with a schizophrenic spectrum disorder and simulated data sets were used. While calculations concerning absolute score values and their differences are not affected, considerable problems arise in calculations of PC and related response criteria. Even significance levels of estimated treatment effects change, depending on the structure of the data (eg, baseline symptom severity). Using a PANSS version with items ranging from 0 to 6 would avoid such often neglected pitfalls.

---

### A primacy code for odor identity [^1b4cc55d]. Nature Communications (2017). Medium credibility.

Concentration series electrophysiology

NeuroNexus A64 Poly5 2 × 32 probes were used to record acutely from awake animals. Units were detected using Spyking Circus software (v0.3.0). Significant odor responses were defined by comparing inhalation-aligned blank odor response distributions with responses to odor presentations (Supplementary Fig. 3A–D). To establish these distributions, blank and odor responses were bootstrapped across trials, summed across trials, and smoothed with a 3-sigma Gaussian kernel of width 30 ms. For each time point, the bootstrapped firing rates were fitted with a Gaussian and the Cohen’s D (discriminability) score was calculated for each time bin. Response latencies were defined as the first time-bin in with a D-score above 5, where overlap is < 0.01 (Supplementary Fig. 3E, G).

Computational model

Our model was based on random and sparse connectivity between the MT cells and the PC cells, as well as within the cortex as detailed below. Our simulation included 300 MT cells and 1000 cells in the PC. Neurons in PC formed random sparse inhibitory recurrent connections to other cells in PC with 50% probability. Non-zero inhibitory recurrent connection weights within PC were. MT cells also formed random excitatory connections with the PC cells with 13% probability (each PC cell received inputs from 40 MT cells). Non-zero projections from MT cells to a PC cell were. In the case of order dependent model, we used.

The state of each PC neuron was defined by the input that this neuron receives u i that satisfied the equation

Here τ = 0.05 is the time constant measured in the fraction of sniffing cycle. The activation state for each PC neuron had a hysteretic dependence on its inputs f i = F ± (u i). The activation function F ± was single valued for values of input variable u satisfying u > u + = 0.2 and u < u − = −150. For these values of parameters, the activation function F ± was equal to 1 and 0, respectively. Within the bistable range, i.e. for u − ≤ u ≤ u +, F ± was bistable and remained constant depending on prior history. Therefore, if a neuron was activated, the activation function within the bistable range remained equal to 1, whereas for an inactivated neuron, the activation function was 0. Activation occurred when inputs exceeded u +, and inactivation happened when inputs fell below u −.

---

### Biased expectations about future choice options predict sequential economic decisions [^9b0d2bac]. Communications Psychology (2024). Medium credibility.

Next, the model works backwards through the sequence, iteratively using the aforementioned formula forwhen computing each respective action value Q for taking the option and declining the option for each t. Whenever the reward value of taking the current option is considered, the reward function R assigns reward values to options based on their ranks. h represents the relative rank of the current option.

In contrast, the reward value of sampling again is simply the cost to sample C.

This customisable R function allowed us to examine how the Ideal Observer changes its sampling strategy under the different reward payoff schemes used in our studies. Pilot full, Study 1 full, Study 2 and both conditions in Study 3 all involved instructing participants to try to choose the best price possible. In study conditions using these instructions, we implemented a continuous payoff function (resembling that of the classic Gilbert & Mosteller formulation), in which the relative rank of each choice would be rewarded commensurate with the value of its associated option. In Pilot baseline and the baseline, squares, timing, and prior conditions of Study 1, we adapted the payoff scheme to match participants’ instructions that they would be paid £0.12 for the best rank, £0.08 for the second-best rank, £0.04 for the third best rank and £0 for any other ranks. Lastly, in the payoff condition of Study 1, we programmed the reward payoff function to match participants’ reward of 5 stars for the best rank, 3 stars for the second-best rank, one star for the third-best rank and zero stars for any other ranks.

Another feature added to our implementation of the Ideal Observer, compared to the Gilbert & Mosteller base model, is the ability to update the model’s generating distribution from its experience with new samples in a Bayesian fashion, instead of this generating distribution being specified in advance and then fixed throughout the paradigm. This Bayesian version of the optimality model treats option values as samples from a Gaussian distribution with a normal-inverse- χ2 prior. The prior distribution is initialised before experiencing any options with four parameters: the prior mean μ 0, the degrees of freedom of the prior mean κ, the prior variance σ 2 0 and the degrees of freedom of the prior variance ν. The μ 0 and σ 2 0 parameters of this prior distribution are then updated by the model following presentation of each newly sampled option value as each sequence progresses.

---

### Hydrocodone bitartrate (hysingla ER) [^0c03a0bc]. FDA (2023). Medium credibility.

Warnings and precautions regarding the use of hydrocodone bitartrate ER PO (also known as Hysingla ER): 
- Adrenal insufficiency: use caution in patients taking the drug for a prolonged period (> 1 month).
- Central sleep apnea: use caution in patients taking higher doses.
- Decreased serum hydrocodone level: use caution in patients taking CYP3A4 inducers (such as rifampin, carbamazepine, or phenytoin) or discontinuing CYP3A4 inhibitors (such as macrolide antibiotics, azole antifungals, or protease inhibitors).
- Erectile dysfunction, infertility: use caution in patients taking the drug for a prolonged period.
- Exacerbation of increased ICP: use caution in patients with increased ICP, brain tumor, or head injury.
- Growth suppression: use caution in patients with chronic corticosteroid therapy. Monitor growth regularly in pediatric patients receiving chronic corticosteroid therapy. Reassess the need for hydrocodone regularly and adjust the corticosteroid therapy as appropriate.
- Hypotension, syncope: use caution in patients with reduced blood volume or taking other CNS depressants. Monitor BP after initiation and dose titration. Avoid use in patients with circulatory shock.
- Mask symptoms of head injury: use caution in patients with head injury. Avoid use in patients with impaired consciousness or coma.
- Opioid overdose: use caution in patients taking CNS depressants or with a history of opioid use disorder or prior opioid overdose. Consider prescribing naloxone based on the patient's risk factors for overdose.
- Opioid withdrawal syndrome: do not discontinue abruptly in patients physically dependent on opioids.
- Opioid withdrawal syndrome: use caution in patients taking CYP3A4 inducers (such as rifampin, carbamazepine, or phenytoin) or discontinuing CYP3A4 inhibitors (such as macrolide antibiotics, azole antifungals, or protease inhibitors).
- Opioid withdrawal syndrome: use extreme caution in patients taking mixed agonist/antagonist analgesics (such as pentazocine, nalbuphine, or butorphanol) or partial agonist analgesics (such as buprenorphine).
- Prolonged QT interval: use caution in patients with congestive HF, bradyarrhythmia, electrolyte abnormalities, or taking drugs prolonging QT interval. Avoid use in patients with congenital long QT syndrome. Do not exceed 90 mg BID in patients developing QT prolongation.
- Seizure: use caution in patients with seizure disorder.
- Serotonin syndrome: use caution in patients taking serotonergic drugs.
- Serotonin syndrome: use extreme caution in patients taking MAOIs or within 14 days of stopping treatment.
- Somnolence: use extreme caution in patients performing activities requiring mental alertness, such as driving or operating machinery.
- Sphincter of Oddi dysfunction: use caution in patients with biliary tract disease and acute pancreatitis.

---

### Uniform spatial pooling explains topographic organization and deviation from receptive-field scale invariance in primate V1 [^75690b95]. Nature Communications (2020). High credibility.

In the “Methods”, we used pooled scale invariance to formulate F 1/ F 0 as a function of f o, which is the following Gaussian:The SD is (σ h (x) 2 πD) −1, where σ h (x) = 0.24° is the pooling window that was computed to account for RF width in the previous section. D is a free parameter that scales the rate of absolute phase progression within the pooling window. D = 0 and 1 correspond to constant “absolute” and “relative” phase, respectively (Fig. 4a, dashed green lines). The absolute phase is referenced by the fovea, whereas the relative phase is referenced by the center of the RF envelope. A least-squares fit to the entire data set yields D = 0.54 (Fig. 4a, solid green), indicating that the absolute phase is more clustered than a model whereby the population maintains the constant relative phase. This parameter estimate was similar when measured independently for the 3 ROIs: D = [0.55 0.56 0.58]. From this model, we derived the result that complex cells tend to have wider RFs than simple cells (Fig. 4b green). Specifically, the model domain in Fig. 4b is the ratio between two predicted RF widths—pooled scale invariance (Eq. (1)) and scale invariance (Eq. (5)). The domain of the data, however, is the ratio between measured RF width and the scale invariance prediction. Unity on the x -axis of Fig. 4b corresponds to a standard Gabor model with 2–3 subfields, and higher values indicate additional subfields within the envelope.

Finally, the link between f o and F 1/ F 0 predicts a previously unidentified map of F 1/ F 0 (Fig. 4d), which we found by comparing neuronal pairs at varying distance. Like f o, nearby neurons were more likely than chance to have similar F 1/ F 0 (Fig. 4f). To summarize, for a given pooling window in visual degrees (σ h (x)), scale invariant inputs with higher f o will have a broader distribution of absolute spatial phase, resulting in a summed output with a relatively large drop in phase selectivity.

---

### Evolutionary instability of zero-determinant strategies demonstrates that winning is not everything [^541475f0]. Nature Communications (2013). Medium credibility.

Zero-determinant strategies are a new class of probabilistic and conditional strategies that are able to unilaterally set the expected payoff of an opponent in iterated plays of the Prisoner's Dilemma irrespective of the opponent's strategy (coercive strategies), or else to set the ratio between the player's and their opponent's expected payoff (extortionate strategies). Here we show that zero-determinant strategies are at most weakly dominant, are not evolutionarily stable, and will instead evolve into less coercive strategies. We show that zero-determinant strategies with an informational advantage over other players that allows them to recognize each other can be evolutionarily stable (and able to exploit other players). However, such an advantage is bound to be short-lived as opposing strategies evolve to counteract the recognition.

---

### Low-noise GaAs quantum dots for quantum photonics [^585737ac]. Nature Communications (2020). High credibility.

Auto-correlation under different excitation schemes

We investigate the stability of the QD under different excitation schemes. We start with continuous-wave (CW) excitation. We perform auto-correlation measurements on X 1− at a constant gate voltage while exciting the QD with (i) an above-band laser (λ = 632.8 nm), (ii) a laser resonant with the p -shell, and (iii) a laser resonant with the s -to- s transition. The results are shown in (i) Supplementary Fig. 6 a, (ii) Supplementary Fig. 6 b, and (iii) Fig. 2 g, respectively. In all three cases, the g (2) stays very flat and close to one—there is no blinking even on a long time-scale. This shows that the QD is a very stable quantum emitter under all three CW excitation schemes. From an applications point of view, it is usually necessary to drive the QD with a resonant pulsed laser. We investigate the auto-correlation under resonant π -pulse excitation in Fig. 2 e. An evaluation of this g (2) -measurement on a longer time-scale is plotted in Supplementary Fig. 6 c, where the y -axis is displayed on a logarithmic scale to resolve the central peak. To investigate whether a strong π -pulse introduces any blinking, we plot the g (2) -measurement in a histogram plot (Supplementary Fig. 6 d) by summing up the coincidence events for every single pulse. This sum is divided by the expectation value for a perfectly stable source: the normalisation factor is x 1 x 2 T int / f rep, where f rep is the repetition rate of the pulsed laser, x 1, x 2 represent the count rates of the two detectors used for a T int -long g (2) -measurement. A derivation of the normalisation factor is given in Supplementary Fig. 6. Importantly, the histogram bars at non-zero time delay are flat and very close to one; the bar at zero delay is close to zero. This shows that the QD is a stable single-photon emitter for resonant π -pulse excitation.

---

### Neural representations of situations and mental States are composed of sums of representations of the actions they afford [^92cbaa98]. Nature Communications (2024). High credibility.

The first round of stimulus optimization aimed to maximize variance in stimulus co-occurrences. The goal of this second round was instead to minimize redundancy. This included minimizing redundancy between the stimuli themselves and minimizing redundancy between the psychological dimensions that described these stimuli. To achieve these goals, we used a greedy search procedure. This procedure began by randomly selecting 60/100 stimuli within a given class (situations, states, and actions were optimized independently in this round). Then, on each of 10,000 iterations, a potential substitution of one of the 60 stimuli for one of the left-out 40 was considered. This substation was evaluated on two criteria. First, we estimated the redundancy between stimuli by computing the Euclidean distances between them within the respective psychological rating space (e.g. ACT-FAST for actions). To do so, we computed the distance between each stimulus and each other stimulus within the set of 60 that would be generated if the substitution took place. We then averaged these minima across the 60 stimuli. Second, we computed the correlations between the rated psychological dimensions across the stimuli. We computed summed the (negative) maximum and mean absolute correlations between these dimensions as a measure of redundancy (i.e. maximizing orthogonality). We then summed these two criteria to produce a final measure of how well the substitution under consideration would achieve our goal of redundancy minimization. If the substitution met that goal better than the current set of 60, then that substitution would be performed. If not, the current set of 60 would be retained. In either case, another random substitution would be considered on the next iteration.

In sum, these stimulus selection procedures allowed us to generate sets of 60 situations, mental states, and actions which maximized variance in co-occurrence rates between these classes and minimized redundancy of stimuli and psychological dimensions within these classes. These final sets of stimuli were rated by a separate sample of online participants, who judged the likelihood of co-occurrences between them (e.g. how likely is it that a certain action would occur in a certain situation?). Participants made these judgements on 100-point line scales. We averaged across participants (an average of 10 ratings per stimulus pair) to generate composites that would be used in our primary imaging hypothesis tests. These same sets of stimuli were also viewed and judged by participants in the fMRI scanner, as described below.

---

### A non-spatial account of place and grid cells based on clustering models of concept learning [^b4162208]. Nature Communications (2019). High credibility.

Simulation procedure and model specifications

At the beginning of the learning phase of each simulation run, we set the number of clusters, number of learning trials, the environment (square, circle), the learning rate, and the learning update batch size. The number of clusters were set (ranging from 10 to 30) and were initiated at random locations in the environment. The shape of the environment was defined by a set of points that could be visited by the agent. The square environment was 50 by 50, where each point was a location specified by a value on the x- and y-axis. The circular environment was defined by drawing a circle in Matlab with a radius of 50, and selecting the points within the bounds of the circle. The starting position and movement trajectory of the agent was then determined as a random walk over one million trials. The agent started at a random position and steps in the horizontal and vertical axes were computed separately. On each trial, the agent could go up, down, or stay on the vertical axis, and left, right, or stay on the horizontal axis. The step was sampled from [−4, −2, −1, −1, 0, 1, 1, 2, 4], where negative values are steps to the left, positive steps are steps to the right, and zero means stay. Movement on the vertical dimension was determined in the same way, but negative values were upward steps and positive values were downwards steps. If the generated step brought the agent out of the environment, the step was cancelled and a new step was generated as above.

We considered a simple winner-take-all network in which only the cluster at position pos i closest to stimulus x (agent’s location) had a non-zero activation. Bold type is reserved for vectors. The distance between pos i and x is defined as:In the Kohonen learning rule, cluster i updates its position pos i to move toward stimulus x according to:where η t is the learning rate at time t. In the present simulations, we used batch updating to increase numerical stability in which 200 updates were performed simultaneously. The learning rate η for batch time t followed an annealing schedule:where η 0 is the initial learning rate set to 0.25 and ρ is the annealing rate set to 0.02.

---

### Spirometry in occupational health-2020 [^f2a95710]. Journal of Occupational and Environmental Medicine (2020). High credibility.

Spirometry zero-flow error—under-recorded flows indicates an error pattern where the forced vital capacity (FVC) is much more reduced than forced expiratory volume in 1 second (FEV1), falsely increasing the FEV1/FVC and possibly masking true airways obstruction; the directive is DELETE THIS TEST. Prevention guidance states to block the sensor when the spirometer is zeroed and hold the sensor still during subject testing to avoid this problem.

---

### Spirometry in occupational health-2020 [^8fd61518]. Journal of Occupational and Environmental Medicine (2020). High credibility.

Spirometry zero-flow error—over-recorded flows describes a consistent zero-flow reference error set only once per test before a complete set of maneuvers, for which the instruction is DELETE THIS TEST. The left test shows the zero-flow error while the right shows valid results despite being from the same subject, and the error elevates the FVC more than the FEV1 and falsely reduces the FEV1/FVC, which may erroneously indicate an “obstructive impairment” pattern. Detection cues include extended tails to the right on flow-volume curves and volume-time curves that climb at a constant rate without reaching a plateau until the spirometer terminates data collection, and prevention is to block the sensor when the spirometer is zeroed and hold the sensor still during subject testing.

---

### Measurements of the size and correlations between ions using an electrolytic point contact [^aa8339ca]. Nature Communications (2019). High credibility.

Noise estimation

To estimate the noise, the pore current traces were processed in four steps:

First, a log-log plot of the PSD as a function of frequency was used to determine the 1/ f noise intercept S 1/ f (1 Hz) at log 10 (1 Hz). Second, the fitting of this value, S 1/ f (1 Hz) normalized by the square of the open pore current I 0 2, i.e.against I 0 2 was performed to determine the current threshold in I T above which correlative ion motion was presumed to dominate. Third, a further linear fitting of a number of traces n ≥ 3 of these current thresholds I T (a n) against activity permitted extrapolation to the threshold I T0 associated with vanishingly small activity (infinite dilution). The y -intercept of this plot at zero-activity is the quantity of interest because it represents that current value for which the pore would exhibit correlated ion noise even at an effective concentration of zero. In this way, the current threshold was estimated at infinite dilution for a given pore and electrolyte, where supposedly no ion–ion interactions occur. Finally, this threshold was plotted as a function of pore diameter to determine the diameters (x -intercepts) for all ions. The x -intercept of this plot at d i = 0 was of interest because it represents that pore diameter at which correlated ion motion would be universally observed for the ion at any I 0 > 0. Thus, the limiting pore diameter for which an ion produces correlated ionic noise was associated with its confinement and was therefore a measure of its physical size as defined in detail below.

---

### Elacestrant (Orserdu) [^25e5bc7d]. FDA (2024). Medium credibility.

The dosage of elacestrant PO for treatment of breast cancer in postmenopausal female adults with ESR1 mutation (advanced or metastatic, ER-positive, HER2-negative, progression after ≥ 1 line of endocrine therapy) is 345 mg PO daily until disease progression or unacceptable toxicity

---

### Nucleation and growth of orbital ordering [^24ef36e0]. Nature Communications (2020). High credibility.

Fig. 4 
Analyses of the time dependence in the strain.

a, b ln(−ln(1 − x (t))) vs. ln t, where x (t) = |Δ L (t)/Δ L (∞)| (Δ L is the strain along the b axis) a above 73 K and b below 69 K. The dashed line in b is the curve fitted by the sum of Eq. (1) and Eq. (2). c ln(−ln(1 − x (t))) vs. ln(t − τ 0), where τ 0 is obtained by fitting with Eq. (1). d Transformation time τ, e Avrami exponent n, and f diffusion component βτ 1/2 and the transient time τ 0 as a function of T obtained by the fitting with the sum of Eq. (1) and Eq. (2). The solid line in d is the fitting with Eq. (3).

On the hand, at lower temperatures (T ≤ 69 K shown in Fig. 4b), the clear kink before the linear increase with t is smeared out but a gradual increase appears for small t in ln(−ln(1 − x (t))), which is followed by an increase with a larger slope. It was found that the slope for the initial increase in the plot of ln(x (t)) vs. ln t is close to 1/2 (see Supplementary Note 6), suggesting that the initial increase is dominated by one-dimensional diffusive motion of the interface. Here, we assume that a transformation of a part of the region in the crystal is dominated by diffusive motion, and thus, V (t) is given by the sum of the nucleation-growth formula given by Eq. (1) and that of the diffusive motion given byand fits the data obtained at T ≤ 77 K. An example of the fitting result is shown by the dashed line in Fig. 4b for the data obtained at 55 K. This one-dimensional diffusive motion is probably caused by the anisotropic growth of domains dominated by the bulk elastic energy.

---

### A general derivation and quantification of the third law of thermodynamics [^43738480]. Nature Communications (2017). Medium credibility.

The most accepted version of the third law of thermodynamics, the unattainability principle, states that any process cannot reach absolute zero temperature in a finite number of steps and within a finite time. Here, we provide a derivation of the principle that applies to arbitrary cooling processes, even those exploiting the laws of quantum mechanics or involving an infinite-dimensional reservoir. We quantify the resources needed to cool a system to any temperature, and translate these resources into the minimal time or number of steps, by considering the notion of a thermal machine that obeys similar restrictions to universal computers. We generally find that the obtainable temperature can scale as an inverse power of the cooling time. Our results also clarify the connection between two versions of the third law (the unattainability principle and the heat theorem), and place ultimate bounds on the speed at which information can be erased.

---

### Single photon emission computed tomography (SPECT) myocardial perfusion imaging guidelines: instrumentation, acquisition, processing, and interpretation [^c79b2831]. Journal of Nuclear Cardiology (2018). Medium credibility.

Spatial resolution—Spatial resolution quantifies the size of the smallest object that can be resolved reliably and is often expressed as the full width at half maximum (FWHM) of a point spread function, and the total resolution of the acquired projection data depends on the intrinsic resolution.

---

### Neural representations of situations and mental States are composed of sums of representations of the actions they afford [^fcd1d03a]. Nature Communications (2024). High credibility.

Fig. 4 
Preregistered confirmatory pattern summation analyses corroborated our hypotheses that frequency-weighted sums of action patterns could reconstruct situation patterns and mental state patterns.

Sums of mental state patterns did not reconstruct situation patterns. Reported p -values were derived from permutation testing on two-tailed, one-sample t -tests, with familywise error rate controlled via the maximal statistic (n = 28 participants). Y -axis values reflect differences in Fisher z-transformed [Z(r)] pattern correlations between matched and mis-matched patterns. Error bars represent 95% bootstrapped confidence intervals around the mean (black). Individual subjects are shown in grey. Source data are provided as a Source Data file.

In addition to these preregistered confirmatory analyses, we also preregistered a matching set of pattern summation analyses reversing the direction of the summation (Fig. S5). These exploratory analyses examine whether action representations can be thought of as sums of the situations and mental states in which they occur, or whether mental states representations can be thought of as sums of the situations in which they occur. In these analyses, we found that summed situation patterns do not reconstruct action-specific patterns (mean Z (r) difference matched-mismatched = 0.0011, d = 0.29, p corrected = 0.37) nor mental state-specific patterns (mean Z (r) difference matched-mismatched = −0.00011, d = −0.081, p corrected = 1.00). However, summed mental state patterns did significantly reconstruct action-specific patterns (mean Z (r) difference matched-mismatched = 0.0018, d = 0.50, p corrected = 0.033). This suggests that the way the brain represents actions can be explained, at least in part, in terms of the mental states that potentiate those actions.

---

### Spirometry in occupational health-2020 [^4fefdefd]. Journal of Occupational and Environmental Medicine (2020). High credibility.

Occupational spirometry—zero-flow reference errors: “Error: Inconsistent Zero-Flow Errors Causing Flows to be Over-Recorded. DELETE THIS TEST.” The cause is described as “This spirometer’s zero-flow reference point was set at different incorrect levels before the first two maneuvers, causing the volume-time curves (bottom figure) to be splayed apart and extended tails to be drawn to the right on the flow-volume curves (top figure).” The figure highlights the effect that “FVC elevated more than FEV1 -> FEV1/FVC decreases,” and prevention is stated as “Block sensor when the spirometer is zeroed and hold sensor still during subject testing to avoid this problem.”

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^d6d8fcbc]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Spirometry reporting specifies that numerical values are given for the FEV1, the FVC, and the FEV1/FVC ratio; the latter should be reported as a decimal fraction and the space for percent predicted value left blank, and if bronchodilators are given the LLN column need not be repeated with absolute and percent change given only for FEV1 and FVC. Other numerical values such as the forced inspiratory flow at 75% of FVC (FEF75%) and FEF25–75% are not recommended for routine use. Graph requirements include that for the volume–time curve the volume scale should be at least 10 mm/L, the time scale at least 20 mm/s, and 1 second prior to the start of expiration should be displayed; on the flow–volume plot the flow display should be at least 5 l/min/L/s, and the ratio of flow to volume should be 2 L/s to 1 L, and linear and log scales where values are plotted as z-scores relative to the predicted value (z = 0) give an intuitive sense of severity.

---

### A practical model-based segmentation approach for improved activation detection in single-subject functional magnetic resonance imaging studies [^c981c138]. Human Brain Mapping (2023). Medium credibility.

3.3.4 Choice of

Many methods (Melnykov & Maitra,) exist for assessingin a mixture model. The most popular are Akaike's information criterion (AIC) (Akaike,) and Bayes information criterion (BIC) (Schwarz,), respectively. We use BIC and selectto be the one minimizing. Herecontains MLEs ofobtained for a given, andis the number of unrestricted parameters to be estimated in the model. Our approach to choosinguses BIC, but additionally incorporates Kass and Raftery's suggestion that only reductions of BIC beyond 10 providing very strong evidence in favor of the more parameter‐rich (larger‐ K) model. Formally, we choose the number of components as the firstfor which, for. We setif nosatisfies this condition.

3.4 Merging of components and voxel classification

3.4.1 Merging with inactive component

Having determined, we obtain an initial maximum a posteriori (MAP) classification of each voxel using the mixture model and estimated MLEs. Constraint iii. of (1) on the activated beta components ideally means that for, we get beta components with parameter estimatessatisfying. This last constraint on the expectation of the (activated) beta components logically follows from our expectation that these p ‐values are low for the activated components. We useto match the most common threshold used in declaring an alternative hypothesis significant. However, the spatial penalty can also yield beta components with true mean, because the spatial context does not involve the constraint iii or the observed p ‐value. We merge such components with the inactivated (zeroth) component via a hypothesis test of constraint iii. Specifically, for the k th estimated beta component, we testagainst. We use the likelihood ratio test statistic (LRTS), whereis the number of voxels classified to the k th component,are the p ‐values of those voxels,andare MLEs under, andare MLEs under. The p ‐value of each of theseLRTSs is calculated using, then converted to q ‐values to control for false discoveries (Benjamini & Hochberg,). Components for which we fail to rejectare merged with the inactivated component. We writeas the number of activated comments remaining after these mergers.

---

### Many-body localization enables iterative quantum optimization [^43c931b6]. Nature Communications (2022). High credibility.

Fig. 5 
Exact diagonalization.

Low-energy spectra of Eq. (3) for different χ. System size is N = 10. Red and orange straight lines are the reference and ground-state energies, correspondingly at χ = 0. They are repeated on all panels as a guide for eye. When χ = 0, energies are straight lines with slopes (11). For larger χ, energies are curved down, due to the level repulsion. Simultaneously the gaps open up.

For χ ≫ 1, one may start assuming B z = 0. At B x / J ≪ 1, Eq. (10) is approximate byThis may be viewed as a result of the second order, in B x, perturbation of the SK model. The energy of the local minimum goes down due to the level of repulsion, and the second-order perturbation comes from the one-spin flip states. The factor 1/(2 f l J) describes the average inverse energy difference between the local minimum and one-spin flip states. The distribution of f l ’s is approximated by a uniform box in the interval 1/4 < f l < 3/4. Finally at B x ≫ J, the system is fully polarized with Σ l ≈ − N B x. Equation (10) is the simplest way to interpolate between all these limits, which works extremely well for small system-size simulations.

To perform larger system-size simulations, leading to Fig. 4, we statistically generate multiple local minima energy curves according to Eqs. (9)–(13). The distribution of SK local energies, E l is taken from refs.and is assumed to be statistically independent from the other random parameters, m l and f l. We simulated system sizes up to N = 200 and verified that the qualitative features of Fig. 4 are robust against variations in specific distributions of the random parameters. The first order (red dashed) line in Fig. 1 is determined by the position offor a fixed reference state, whileis given by the last intersection of the reference state.

---

### How many days was that? We' re still not sure, but we' re asking the question better! [^b41c7375]. Medicine and Science in Sports and Exercise (2008). Low credibility.

Unreliable measures limit the ability to detect relationships with other variables. Day-to-day variability in measurement is a source of unreliability. Studies vary substantially in numbers of days needed to reliably assess physical activity. The required numbers of days has probably been underestimated due to violations of the assumption of compound symmetry in using the intraclass correlation. Collecting many days of data become unfeasible in real-world situations. The current dilemma could be solved by adopting distribution correction techniques from nutrition or gaining more information on the measurement model with generalizability studies. This would partition the variance into sources of error that could be minimized. More precise estimates of numbers of days to reliably assess physical activity will likely vary by purpose of the study, type of instrument, and characteristics of the sample. This work remains to be done.

---

### Artificial-intelligence-guided design of ordered gas diffusion layers for high-performing fuel cells via Bayesian machine learning [^7fa8b167]. Nature Communications (2025). High credibility.

Algorithm 1

Bayesian Optimization

1 Input : Objective function f, domain χ, initial observation D 0

2 Output : The best input x * ∈ χ and its function values f (x *)

3 Initialize Gaussian Process (GP) with prior p (f)

4 Define acquisition function α (x ; D)

5 D ← D 0

6 for t = 1,2, …, T do

7  Select x t ∈ χ by optimizing the acquisition function:

8

9  Evaluate the objective function y t = f (x t) by multi-scale modeling:

10   1. Pore-scale calculation of GDLs

11    2. Cell-scale simulation of fuel cells

12     Augment the data: D ← D ∪ {(x t, y t)}

13 Update the GP model with the new data D

14 end for

15based on the GP posterior

16 return x *, f (x)

BO is used to maximize the LCD with the objective function by multi-scale modeling, within the domain (χ) of porosity, pore radius, and fiber orientation. During each iteration in the BO algorithm, we employed the acquisition function for selecting the next x t, evaluated the objective function and updated the Gaussian process model, to progressively approach the global optimum for complex fuel cell simulations. Figure 1c, d shows the convergence of optimum LCD within 40 iterations, where the optimal structures mainly lie in the regions with porosity in the range of 0.7–0.9, fiber radius of 3–6 µm, and a fiber orientation near 0°, representing a highly aligned fiber structure (Fig. 1e) with the reconstructed structure as a representative. However, fabricating aligned carbon fiber electrodes remains challenging, as fiber-fabricating techniques like wet-spinning and melt-spinning typically result in fibers with diameters much larger than 10 μm and poorly adjusted fiber orientation, and traditional electrospinning technique is commonly used for fabricating nanoscale fibers that are randomly dispersed.

---

### Correlations as a resource in quantum thermodynamics [^075da07c]. Nature Communications (2019). High credibility.

The presence of correlations in physical systems can be a valuable resource for many quantum information tasks. They are also relevant in thermodynamic transformations, and their creation is usually associated to some energetic cost. In this work, we study the role of correlations in the thermodynamic process of state formation in the single-shot regime, and find that correlations can also be viewed as a resource. First, we show that the energetic cost of creating multiple copies of a given state can be reduced by allowing correlations in the final state. We obtain the minimum cost for every finite number of subsystems, and then we show that this feature is not restricted to the case of copies. More generally, we demonstrate that in the asymptotic limit, by allowing a logarithmic amount of correlations, we can recover standard results where the free energy quantifies this minimum cost.

---

### Spirometry in occupational health-2020 [^b8efc964]. Journal of Occupational and Environmental Medicine (2020). High credibility.

Spirometry test acceptability in adults—maneuvers are acceptable when all of the following are met: Maximal inhalation; a good start of exhalation with extrapolated volume <5% of FVC or 0.10 L, whichever is greater, that is, no excessive hesitation; free from artifacts; no cough during first second of exhalation (for FEV1); no glottis closure or abrupt termination (for FVC); no early termination or cutoff (for FVC), with timed expiratory volumes still reportable but FVC reported only with qualification; maximal effort provided throughout the maneuver; no obstructed mouthpiece; no extra breaths taken through the nose; and no zero-flow errors. FVC and FEV1 are each considered separately for acceptability; FEV1 acceptability does not consider anything after the first second, whereas FVC does.

---

### CONSORT-DEFINE explanation and elaboration: recommendations for enhancing reporting quality and impact of early phase dose-finding clinical trials [^d15faaad]. EClinicalMedicine (2025). Medium credibility.

Example 3

“Noribogaine and noribogaine glucuronide concentrations above the limit of quantification were used to calculate pharmacokinetic parameters using model independent methods. The maximum plasma concentration (Cmax) and time to maximum plasma concentration (Tmax) were the observed values. Plasma concentration data in the post-distribution phase of the plasma concentration–time plot were fitted using linear regression to the formula ln C = ln C o —t. Kel, where C o was the zero-time intercept of the extrapolated terminal phase and Kel was the terminal elimination rate constant. The half life (t 1/2) was determined using the formula t 1/2 = 0.693/Kel. The area under the concentration–time curve (AUC) from time zero to the last determined concentration–time point (tf) in the post-distribution phase was calculated using the trapezoidal rule. The area under the curve from the last concentration–time point in the post distribution phase (C tf) to time infinity was calculated from AUC t-∞ = C tf /Kel. The concentration used for C tf was the last determined value above the LLOQ [lower limit of quantitation] at the time point. The total AUC 0–∞ was obtained by adding AUC tf and AUC t–∞. Noribogaine apparent clearance (CL/F) was determined using the formula CL/F = Dose/AUC 0–∞ x 1,000, and apparent volume of distribution (Vd/F) was determined using the formula Vd/F=(CL/F)/Kel. Total urine noribogaine was the sum of both analytes.

Summary statistics (means, standard deviations, and coefficients of variation) were determined for each dose group for safety laboratory test data, ECG [electrocardiography] and pharmacokinetic parameters, and pharmacodynamic variables. Categorical variables were analyzed using counts and percentages. Dose proportionality of AUC and Cmax was assessed using linear regression. The effect of dose on pharmacodynamic parameter values over time was assessed using two-factor analysis of variance (ANOVA). Pairwise comparisons (with Tukey–Kramer adjustment) between each dose group to the placebo were conducted at each time point using the least-squares estimates obtained from the ANOVA, using SAS Proc Mixed (…).”

---

### Practical guide for interpreting and reporting cardiac PET measurements of myocardial blood flow: an information statement from the American Society of Nuclear Cardiology, and the Society of Nuclear Medicine and Molecular Imaging [^641ba6b3]. Journal of Nuclear Cardiology (2021). High credibility.

Net retention model—placement of regions of interest (ROIs) and time-activity curves needs inspection to ensure the myocardial ROI excludes adjacent non-cardiac structures and recognition that the net retention model does not solve for the partial volume correction and is an external parameter determined by scanner resolution and reconstruction parameters that must be adjusted whenever different scanners or reconstruction filters are used; blood pool curves for this model should demonstrate near zero counts in the first dynamic frame of the 82Rb acquisition, a strong peak of activity between 25 and 75 seconds after start of infusion, and good blood pool clearance prior to the myocardial uptake frame with the myocardium clearly defined and brighter than the blood pool during the final uptake frame of the dynamic series.

---

### Practical guide for interpreting and reporting cardiac PET measurements of myocardial blood flow: an information statement from the American Society of Nuclear Cardiology, and the Society of Nuclear Medicine and Molecular Imaging [^461b4a68]. Journal of Nuclear Cardiology (2021). High credibility.

Know your software (retention and compartment models)—compartment and retention model approaches for myocardial blood flow (MBF) are described as follows: The compartment model approach for calculating MBF requires a series of dynamic images that record tracer kinetics into and washout from the myocardium and uses a non-linear fit of the model to the acquired data to solve for the regional MBF, tracer washout, and partial volume artifacts. The retention model approach assumes tracer retention is determined by the blood pool concentration and irreversible extraction into the myocardium; the amount of radiotracer retained in the myocardium is corrected for or normalized to the amount of radiotracer delivered via arterial blood (“the arterial input function”) and thus the integral of the arterial radiotracer concentration (assumed to be derived from the initial two-minute images), is corrected for the flow-dependent or K1-specific extraction fraction, and by assuming the tracer does not significantly washout determines blood flow based on tracer uptake; this model does not consider washout, assumes a partial volume correction factor, and its shortcoming can be somewhat mitigated by using a short dynamic scan acquisition time.

---

### Sugammadex (Bridion) [^4e89ba97]. FDA (2025). Medium credibility.

The dosage of sugammadex sodium IV for induction of reversal of neuromuscular blockade in both children (induced by rocuronium or vecuronium, spontaneous twitch response recovery has reached 1-2 post-tetanic counts and no twitch responses to train-of-four stimulation) is 4 mg/kg IV bolus

---

### Diagnosis and classification of diabetes: standards of care in diabetes – 2025 [^568cafd9]. Diabetes Care (2025). High credibility.

Type 1 diabetes natural history and progression risk indicate that stage 1, defined by the presence of two or more autoantibodies and normoglycemia, carries a 5-year risk of developing symptomatic type 1 diabetes of ~44%. In stage 2, risk is ~60% by 2 years and ~75% within 5 years of developing a clinical diagnosis of type 1 diabetes. A consensus group provides expert recommendations on what should be monitored and how often these factors should be monitored in individuals with presymptomatic type 1 diabetes.

---

### Standards of care in diabetes – 2025 [^5eb43e21]. Diabetes Care (2025). High credibility.

Regarding follow-up and surveillance for diabetes mellitus type 1, more specifically with respect to continuous glucose monitoring, ADA 2025 guidelines recommend to implement early continuous glucose monitoring in adult patients with T1DM to improve glycemic outcomes and QoL and minimize hypoglycemia.

---

### Colorectal cancer screening and prevention [^390b568a]. American Family Physician (2025). High credibility.

Regarding screening and diagnosis for colon cancer, more specifically with respect to indications for screening, high-risk individuals, family history, AAFP 2025 guidelines recommend to obtain CRC screening in patients with ≥ 1 first-degree relatives with CRC or adenomatous polyps, starting at 40 years of age or 10 years before the age of the youngest relative at the time of their diagnosis.

---

### Dispositional free riders do not free ride on punishment [^21cbfce3]. Nature Communications (2018). Medium credibility.

First phase—measuring individual cooperative dispositions (study 1)

Individual dispositions towards cooperation were measured using the one-shot public goods game described above, played in a variant of the strategy method. Each participant first decided on an unconditional contribution to the public good, and then on a series of conditional contributions, indicating her preferred contribution for each average (integer) contribution of the other group members. We refer to the resulting set of choices as the contribution schedule. For one randomly selected member of each group, the contribution schedule determined the actual contribution to the public good, ensuring that both decisions (unconditional and conditional contribution) are potentially payoff relevant. We also elicited participants’ beliefs about their group members’ average contribution: participants earned three MU for guessing the average contribution of others correctly, two MU for a deviation of one point, one MU for a deviation of two points, and zero MU for a higher deviation.

The contribution schedules were used to classify participants according to their disposition towards cooperation. The criteria were based on past work using this method: DCC increase their contributions in the average contributions of others; they either have a positive Spearman’s rank correlation coefficient, significant at the 1% level, between their own contribution and the others’ average contribution, or display a monotonically increasing schedule, with at least one increase. DFR contribute exactly zero for each and every possible average contribution of others. Participants who were not classified as either DCC or DFR were excluded from the analysis, as our main hypothesis does not make unambiguous behavioural predictions for them.

---

### Elranatamab-bcmm (Elrexfio) [^ad17527e]. FDA (2025). Medium credibility.

2.1 Important Dosing Information

Administer ELREXFIO subcutaneously according to the step-up dosing schedule to reduce the incidence and severity of cytokine release syndrome (CRS).

Administer pre-treatment medications prior to each dose in the ELREXFIO step-up dosing schedule, which includes step-up dose 1, step-up dose 2, and the first treatment dose as recommended [see Dosage and Administration (2.2, 2.3)] .

ELREXFIO should only be administered by a qualified healthcare professional with appropriate medical support to manage severe reactions such as CRS and neurologic toxicity, including ICANS [see Warnings and Precautions (5.1, 5.2)] .

Due to the risk of CRS, patients should be hospitalized for 48 hours after administration of the first step-up dose, and for 24 hours after administration of the second step-up dose.

2.2 Recommended Dosage

For subcutaneous injection only.

The recommended dosing schedule for ELREXFIO is provided in Table 1. The recommended dosages of ELREXFIO subcutaneous injection are: step-up dose 1 of 12 mg on Day 1, step-up dose 2 of 32 mg on Day 4, followed by the first treatment dose of 76 mg on Day 8, and then 76 mg weekly thereafter through week 24.

For patients who have received at least 24 weeks of treatment with ELREXFIO and have achieved a response [partial response (PR) or better] and maintained this response for at least 2 months, the dose interval should transition to an every two-week schedule. For patients who have received at least 24 weeks of treatment with ELREXFIO at the every two-week dosing schedule and have maintained the response, the dose interval should transition to an every four-week schedule.

Continue treatment with ELREXFIO until disease progression or unacceptable toxicity.

Administer pre-treatment medications prior to each dose in the ELREXFIO step-up dosing schedule, which includes step-up dose 1, step-up dose 2, and the first treatment dose as recommended [see Dosage and Administration (2.3)] .

---

### Colorectal cancer screening and prevention [^c270f773]. American Family Physician (2025). High credibility.

Regarding screening and diagnosis for colon cancer, more specifically with respect to indications for screening, general population, aged 76-85 years, AAFP 2025 guidelines recommend to consider obtaining screening for CRC in adults aged 76-85 years at average risk based on overall health status, prior screening history, and patient preferences.

---

### 2018 ACC / HRS / NASCI / SCAI / SCCT expert consensus document on optimal use of ionizing radiation in Cardiovascular imaging: best Practices for safety and effectiveness: a report of the American college of cardiology task force on expert consensus decision pathways [^18e1837f]. Journal of the American College of Cardiology (2018). Medium credibility.

Ionizing radiation stochastic effects—cancer and heritable risks are characterized by a probabilistic dose relationship without a known threshold, with severity not related to dose and the probability thought to increase approximately linearly with dose; the likelihood that a stochastic event will occur does not reach 100% even at very large doses and, at small doses, risk is small and difficult to distinguish statistically from zero. Mechanistically, a single x-ray photon ionizing a strategic atom within DNA that encodes a critical gene could create an oncogene, supporting the concept that there is no threshold dose below which stochastic risk is zero. For heritable effects, radiation injury to DNA in germ cells could cause a clinically important mutation transmitted to offspring, but such effects have only been demonstrated in animal models, have not been observed in humans with statistical significance, and the absence of any significant finding in humans indicates a very low probability detectable only in exposures to very large populations.

---

### Clotrimazole [^b5a7188a]. FDA. Low credibility.

The dosage of clotrimazole OTIC for treatment of otomycosis in adults is 1 vial OTIC BID for 14 days (1%/0.17 mL)

---

### Standards of care in diabetes – 2025 [^31be65a6]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for hypoglycemia, more specifically with respect to screening for cognitive impairment, ADA 2025 guidelines recommend to simplify diabetes treatment plans as much as possible for patients with cognitive impairment and tailor to minimize the risk of hypoglycemia.

---

### Sugammadex (Bridion) [^73923e6c]. FDA (2025). Medium credibility.

The dosage of sugammadex sodium IV for induction of reversal of neuromuscular blockade in adults (induced by rocuronium or vecuronium, spontaneous twitch response recovery has reached 1-2 post-tetanic counts and no twitch responses to train-of-four stimulation) is 4 mg/kg IV bolus

---

### Standards of care in diabetes – 2025 [^14753c0e]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for chronic kidney disease, more specifically with respect to patients with diabetes mellitus (monitoring of renal function), ADA 2025 guidelines recommend to monitor urinary albumin (such as spot urinary albumin-to-creatinine ratio) and eGFR 1-4 times per year, depending on the stage of the disease, in patients with established diabetic kidney disease.

---

### Norethindrone and ethinyl estradiol (cyonanz) [^22a22b26]. FDA (2021). Medium credibility.

 ______________________________________________________________________________________________________________ FINALLY, IF YOU ARE STILL NOT SURE WHAT TO DO ABOUT THE PILLS YOU HAVE MISSED: 

Use a BACK-UP METHOD anytime you have sex.

 KEEP TAKING ONE "ACTIVE" PILL EACH DAY until you can reach your healthcare professional.

How to Use the Blister Packs for the 28 Tablets

The blister pack comes to you set up for Sunday Start. If your physician has instructed you to start pill-taking on the SUNDAY after your menstrual period has begun or if your period begins on Sunday you will take your first pill that very same day. Start with the Sunday pill in row #1 . Take one active pill daily for 21 days followed by one green placebo pill daily for 7 days.
If you start pill-taking on a day other than SUNDAY, peel the attached sticker by selecting the day of the week you plan to start. Place the sticker over the pre-printed days of the week and make sure it lines up with the pills. This sticker will help to remind you to take your pill every day. Take the white to off-white pill which corresponds to the day in row #1 during the first 24 hours of your period and then take one pill each day, left to right, until the last pill in row # 3 has been taken. Once all of the active pills have been taken, proceed to row #4 and take the 7 “reminder” pills to complete the pack.
When you start taking the pill for the first time, use an additional method of protection until you have taken your first 7 pills.
Your blister pack containing 28 individually sealed pills. Note that the pills are arranged in four numbered rows of 7 pills each with the days of the week printed above them. The 21 (white to off-white) pills are “active” birth control pills. The 7 green pills are inactive; their purpose is to help make your pill-taking schedule simple and easy to remember.
You must take your pills in order, one pill each day at intervals of 24 hours. Always start with the white to off-white pills in row #1. To remove a pill, push down on the pill with your thumb and forefinger so that the pill releases through the back of the dispenser. See diagram below:

---

### Diagnosis, management and treatment of the Alport syndrome-2024 guideline on behalf of ERKNet, ERA and ESPN [^45eede36]. Nephrology, Dialysis, Transplantation (2025). High credibility.

Regarding diagnostic investigations for Alport syndrome, more specifically with respect to hearing assessment, ERA/ERN ERKNet/ESPN 2025 guidelines recommend to consider obtaining a hearing evaluation at diagnosis or upon reaching adulthood in female patients with X-linked alport syndrome, and then every 5 years in the absence of hearing loss symptoms.

---

### Cladribine (Mavenclad) [^adfd0eb6]. FDA (2025). Medium credibility.

2.2 Recommended Dosage

The recommended cumulative dosage of MAVENCLAD is 3.5 mg per kg body weight administered orally and divided into 2 yearly treatment courses (1.75 mg per kg per treatment course) (see Table 1). Each treatment course is divided into 2 treatment cycles:

Administration of First Treatment Course

First Course/First Cycle: start any time.
First Course/Second Cycle: administer 23 to 27 days after the last dose of First Course/First Cycle.

Administration of Second Treatment Course

Second Course/First Cycle: administer at least 43 weeks after the last dose of First Course/Second Cycle.
Second Course/Second Cycle: administer 23 to 27 days after the last dose of Second Course/First Cycle.

Administer the cycle dosage as 1 or 2 tablets once daily over 4 or 5 consecutive days [see How Supplied/Storage and Handling (16.1)]. Do not administer more than 2 tablets daily.

Following the administration of 2 treatment courses, do not administer additional MAVENCLAD treatment during the next 2 years. Treatment during these 2 years may further increase the risk of malignancy [see Warnings and Precautions (5.1)] . The safety and efficacy of reinitiating MAVENCLAD more than 2 years after completing 2 treatment courses has not been studied.

2.3 Missed Dose

If a dose is missed, patients should not take double or extra doses.

If a dose is not taken on the scheduled day, then the patient must take the missed dose on the following day and extend the number of days in that treatment cycle. If two consecutive doses are missed, the treatment cycle is extended by 2 days.

2.4 Administration

MAVENCLAD tablets are taken orally, with water, and swallowed whole without chewing. MAVENCLAD can be taken with or without food.

Separate administration of MAVENCLAD and any other oral drugs by at least 3 hours during the 4 to 5 day MAVENCLAD treatment cycles [see Clinical Pharmacology (12.6)].

MAVENCLAD is a cytotoxic drug. Follow applicable special handling and disposal procedures [see References (15)]. MAVENCLAD is an uncoated tablet and must be swallowed immediately once removed from the blister. If a tablet is left on a surface, or if a broken or fragmented tablet is released from the blister, the area must be thoroughly washed with water.

The patient's hands must be dry when handling the tablets and washed thoroughly afterwards. Avoid prolonged contact with skin.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^681fd2f9]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Using reference data in interpretation—ATS/ERS recommend using limits of normal, with thresholds set to the fifth percentile (z-score −1.645) so that 95% of a healthy population falls within the normal range; use the upper limit where appropriate such as for lung volumes.

---

### Technical update on HIV-1 / 2 differentiation assays [^50eb7933]. CDC (2016). Medium credibility.

HIV-1/HIV-2 differentiation testing—alternatives when the FDA-approved antibody differentiation immunoassay cannot be used states that there may be circumstances under which a laboratory is unable to adopt the FDA-approved HIV-1/HIV-2 antibody differentiation immunoassay, and in this situation a laboratory has alternatives for that step in the algorithm, some of which could delay turnaround time for test results. Send specimens to another laboratory that offers the FDA-approved supplemental HIV antibody differentiation assay. Refer to CDC/APHL laboratory testing guidance section I, “Alternative Testing Sequences When Tests in the Recommended Algorithm Cannot be Used”.

---

### Should we consider gene X environment interaction in the hunt for quantitative trait loci? [^19ae45c9]. Genetic Epidemiology (2001). Low credibility.

We address the question of whether one can obtain increased power for finding a quantitative trait locus (QTL) if a gene x environment (G x E) interaction is incorporated directly into the linkage analysis. We consider both parametric and nonparametric analysis approaches to including G x E interaction. For the former, we utilize joint segregation and linkage analysis to estimate simultaneously the recombination fraction and a G x E interaction effect, as well as the remaining model parameters. The nonparametric approach is based on an extension of the Haseman-Elston method applied to sib pairs to include a regression of the squared trait difference on marker-identity-by-descent (IBD) probability (pi), the sibling covariate sum (z), and pi x z. We utilize 50 replicates of the simulated data and compare empirical power of the various approaches to detect MG4, a locus that is involved in a strong interaction with age for Q4 and in a weaker interaction with environmental factor E2 for Q3. Using the parametric approach, including a G x age effect does increase power for detecting linkage between MG4 and Q4 compared with ignoring the interaction (powers 58% and 38%, respectively, to exceed a lod score of 3.0). On the other hand, including a G x E2 interaction has little effect on the power to detect linkage between MG4 and Q3. The nonparametric approach leads to qualitatively similar findings. We conclude that it is beneficial to incorporate G x E interaction into a linkage analysis, provided the interaction effect is of sufficiently strong magnitude.

---

### Standards of care in diabetes – 2025 [^9fec4de9]. Diabetes Care (2025). High credibility.

Regarding follow-up and surveillance for diabetes mellitus type 1, more specifically with respect to continuous glucose monitoring, ADA 2025 guidelines recommend to assess skin reactions, either due to irritation or allergy, and address to aid in successful use of devices.

---