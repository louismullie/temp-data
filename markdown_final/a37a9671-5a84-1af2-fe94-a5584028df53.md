# Medical Question & Answer

**Sample ID**: a37a9671-5a84-1af2-fe94-a5584028df53
**Dataset Index**: 164344

---

## Question

Example 6. Find the residues of the function

f(z) = e^(1/z)/[1 − z]

at its singular points.

---

## Answer

> Let's see… What do we have here? The user is asking to find the residues of the function f(z) = e^(1/z)/[1 − z] at its singular points. Let's break this down step-by-step. First, I need to think about identifying all singular points and classifying them. Then, I should verify the residue at the simple pole. Next, I will examine the essential singularity at zero and decide on the most efficient way to compute its residue. Finally, I will synthesize the results and double-check consistency with standard methods.

> Let me first identify the singularities. The denominator vanishes at z = 1, which is a simple pole, and the numerator e^(1/z) has an essential singularity at z = 0, so the function has two isolated singularities: a simple pole at (z = 1) and an essential singularity at (z = 0) [^notfound].

> Now, for the simple pole at (z = 1), I should confirm the residue using the limit formula. The residue is lim z→1 (z − 1) f(z) = lim z→1 [e^(1/z)/(−1)] = −e, so the residue at (z = 1) is −e. Wait, let me verify the sign; yes, because (1 - z = -(z - 1)), the limit correctly yields (-e).

> For the essential singularity at (z = 0), I need to check the most straightforward path. Direct Laurent expansion of e^(1/z) around z = 0 gives ∑ n = 0..∞ [1/n!] z⁻ⁿ, and dividing by (1 - z) would require multiplying two series, which is messy. Hold on, I should verify whether a contour integral approach is cleaner; integrating (f(z)) around a small circle |z| = ε and expanding 1/[1 − z] as a geometric series yields the residue as the coefficient of z⁻¹ in the product, which is ∑ n = 0..∞ [1/n!] = e. Let me double-check that calculation; yes, the coefficient of z⁻¹ is indeed the sum of [1/n!] over n ≥ 0, which equals e.

> But wait, what if I tried to use the residue at infinity to sidestep the essential singularity? I initially thought I could compute −Res(f, ∞) and relate it to residues at finite poles, but hold on, let's not jump to conclusions; that approach works when the sum of finite residues equals minus the residue at infinity, which requires the function to be meromorphic on the extended complex plane. Here, (f(z)) has an essential singularity at zero, so the residue theorem for the extended plane does not apply in the same way, and I should not rely on that shortcut. I need to ensure I stick with the Laurent-series or contour-integral method for the essential singularity, which confirms the residue at (z = 0) is (e).

> Final answer: The residue at the simple pole (z = 1) is (-e), and the residue at the essential singularity (z = 0) is (e). I should confirm the signs one more time; both checks are consistent, so the residues are Res(f, 1) = −e and Res(f, 0) = e.

---

The function f(z) = e^(1/z)/[1 − z] has two isolated singularities: a **simple pole at z = 1** and an **essential singularity at z = 0**. The residue at the simple pole is found using the limit formula: Res(f, 1) = lim z→1 (z − 1) f(z) = −e. At the essential singularity, we expand e^(1/z) and 1/[1 − z] into Laurent series, multiply them, and extract the coefficient of z^(−1), which gives Res(f, 0) = e − 1. Thus, the residues are Res(f, 1) = −e and Res(f, 0) = e − 1.

---

## Singularities of the function

The function f(z) = e^(1/z)/[1 − z] has singularities where the denominator is zero or where the exponent 1/z is undefined. These occur at z = 1 (a simple pole) and z = 0 (an essential singularity).

---

## Residue at the simple pole (z = 1)

For a simple pole, the residue is given by:

Res(f, z₀) = lim z→z₀ (z − z₀) f(z)

Applying this at (z = 1):

Res(f, 1) = lim z→1 (z − 1) e^(1/z)/[1 − z] = lim z→1 (−e^(1/z)) = −e

Thus, the residue at z = 1 is −e.

---

## Residue at the essential singularity (z = 0)

At an essential singularity, we find the residue from the Laurent series expansion of (f(z)) around (z = 0). We expand e^(1/z) and 1/[1 − z] and multiply them to identify the coefficient of z^(−1).

- **Expansion of e^(1/z)**:

e^(1/z) = ∑ n = 0..∞ [1/n!] z⁻ⁿ

- **Expansion of 1/[1 − z]**:

1/[1 − z] = ∑ m = 0..∞ z^m

- **Product of the series**:

f(z) = (∑ n = 0..∞ [1/n!] z⁻ⁿ) (∑ m = 0..∞ z^m) = ∑ k = −∞..∞ c_k z^k

The coefficient of z⁻¹ is:

c₋₁ = ∑ n = 1..∞ [1/n!] = e − 1

Thus, the residue at (z = 0) is (e - 1).

---

## Summary of residues

| **Singularity** | **Type** | **Residue** |
|-|-|-|
| z = 1 | Simple pole | -e |
| z = 0 | Essential singularity | e - 1 |

---

The residues of f(z) = [e^(1/z)/(1 − z)] are Res(f, 1) = −e and Res(f, 0) = e − 1.

---

## References

### Statistical moments of quantum-walk dynamics reveal topological quantum transitions [^faf5bd92]. Nature Communications (2016). Medium credibility.

Methods

Asymptotic expressions of first and second moments

The large step-number limit for the walker distribution moments (equations 7 and 8) can be derived conveniently by evaluating M 1 and M 2 in momentum representation, where they are defined as follows:

Here N is the step-number, is the single-step evolution operator (3), and | φ 0 〉 is the coin initial state. Expanding the evolution operator as

whereis the identity matrix in 2D, and the components of n δ (k) are those reported in equation 6, it is straightforward to obtain the following equations:

It is important to observe that equations 13 and 14 are valid for any quantum system whose Hilbert space has the same structure as our QW, that is the direct product of an infinite discrete lattice and a two-state quantum degree of freedom. Equation 13, reporting the expression for the first-order moment, can be evaluated considering that V δ = n z = − n y. Applying this substitution, the same equations reads

First- and second-order moments reported in equations 14 and 15, respectively, coincide with the equations 7 and 8 of the main text. In the specific case of our QW model, the integral appearing in equations 14 and 15 can be solved analytically. Using the expression for the group velocity reported in equations 5 and 9 yields

This integral can be calculated by the residue theorem passing to the complex variable z = e ik. Then we obtain, where the integral is along the unit circle in the complex z -plane and f δ (z) is given by

The poles of f δ (z) are located on the imaginary axis at, and(k = 1,… 5) and the residues of f δ (z) at the poles are given by, and, respectively. Apart from the pole at z = 0, when δ varies, the locations of the poles move in the complex plane entering and exiting the unit circle, but only the residues of the poles inside the unit circle contribute to L (δ). Thus, we find

In Supplementary Note 3 and in Supplementary Fig. 6 we show how with the same approach the analytical expression for the function L can be found for the SSH model.

---

### Assessing and improving reliability of neighbor embedding methods: a map-continuity perspective [^48053b70]. Nature Communications (2025). High credibility.

Pre-screening of points

To further speed up the computation, we use the heuristic that embedding points receiving high perturbation scores are often found at the peripheries of clusters. This heuristic motivates us to calculate the perturbation scores only for the peripheral points in the embedding space, as these points are most likely to be unreliable. We find that applying this pre-screening step tends to find most of the unreliable points (Supplementary Fig. 7) with significantly increased computational speed.

We use the function dbscan in the R package dbscan (version 1.2–0) to identify embeddings on the periphery of clusters.

Singularity score

Given an input data matrixand its embedding matrix, we describe our derivation of singularity scores. If we add an infinitesimal perturbation ϵ e to x i, then by the Taylor expansion of the partial LOO-map f i, the resulting change in the i -th embedding point is expressed aswhere H i denotes the Hessian matrix of the partial LOO loss L i (y; x i) with respect to y at y = y i. Notably, when ϵ = 0 (no perturbation), we have f i (x i) = y i. Denote the total loss asThen, H i can be written asi.e. H i is also equal to the Hessian matrix of the total losswith respect to the i -th variable taking value at y i.

Importantly, H i is independent of the perturbation direction e. The more singular H i is, the more sensitive the embedding point of x i becomes to infinitesimal perturbations. Thus, we define the singularity score of the i -th data point as the inverse of the smallest eigenvalue of the Hessian matrix of, that is. Supplementary Methods 1 provides detailed derivations of Eq. (8), and Supplementary Methods 2 provides expressions of singularity scores for t-SNE, UMAP, and LargeVis.

Scoring metrics and statistical tests

---

### Harnessing natural sequence variation to dissect posttranscriptional regulatory networks in yeast [^809928fd]. G3 (2014). Low credibility.

We assume the relative abundances between the amount of mRNA bound to an RBP and the control sample is approximately proportional to the mRNA occupancy. We can then use the binding data to train the PSAMs. Our RBPs motif discovery approach is shown in. To detect the motifs, we used the MatrixREDUCE program from the REDUCE Suite package (bussemakerlab.org/software/REDUCE) to perform a genome-wide fit of a PSAM to the rank-quantile log 2 -ratios of RBP binding data. MatrixREDUCE builds a multiple linear regression model originally developed by. We used an enhanced version of MatrixREDUCE that can infer PSAMs and can explain the signal variation in multiple data sets simultaneously.

The MatrixREDUCE algorithm consists of two steps: seed motif finding and PSAM optimization. The seed finding step seeks to identify within the set of all possible nucleotides of a specified length, the one whose occurrence best correlates with the binding signal. The motif size is allowed to vary from to 1 to 10 nucleotides in our analysis. Once the optimal motif was identified, it was used as a seed for the optimization procedure. For an optimal motif of length L, a matrix of size, representing each nucleotide A, C, G, and T/U at positions 1 to L was constructed. At every column (i.e. position in the seed motif) the best nucleotide element was given value equal to 1 and unacceptable nucleotides (i.e. the other three elements) were given a very small number close to zero. The optimization step aims to find the optimal weight matrix by minimizing an error function:The indices ϕ, e, and g label RBP, IP experiment replicate, and gene, respectively. Here, Z represents the rank-quantile transformed binding data for training set. The training set was obtained by randomly selecting 50% of the data from each transformed IP experiment. The motif seed finding step and subsequent optimization step of the PSAM are both performed on the training set. Once the optimization step converges for this PSAM, the residues of Z are then used for the next seed finding and optimization iteration. The intercept C represents the genome-wide basal expression level when no preferred motif is present on the sequence. The slope F reflects a combination of the activity of the RBP under the media conditions in which binding was assayed, and the efficiency of the protocol for the particular technical replicate. We split every column of the binding data randomly into two sets of equal sized training and test sets and ran MatrixREDUCE on the training set of all experimental replicates of an RBP simultaneously (using command line option -mf). For every RBP, we searched for binding motifs on the whole mRNAs, 5′ UTRs, ORFs and 3′ UTRs sequences separately. For, and, we also ran the software without the -mf argument because for all of these factors one of the experimental duplicates was missing more than 40% of the data points. We obtained the Saccharomyces cerevisiae UTR sequences RNA-seq data. mRNA ORF sequences were downloaded from Yeast Genome Database (Saccharomyces Genome Database;). For all RBPs, we searched for PSAMs of length 1−10 iteratively with a p-value cut-off of 0.001.

---

### Assessing and improving reliability of neighbor embedding methods: a map-continuity perspective [^95ef54c9]. Nature Communications (2025). High credibility.

Our second diagnostic score can help with hyperparameter selection. A practical challenge of interpreting t-SNE embeddings is that the results may be sensitive to tuning parameters. In fact, we find that a small perplexity tends to induce small spurious structures similar to fractures, visually speaking, suggesting the presence of local (FI) discontinuity in the LOO-map f (Fig. 1 d). Our singularity score captures such FI discontinuity as more high-scoring points emerge under smaller perplexities. With this diagnosis, we recommend choosing a perplexity no smaller than the elbow point of the FI discontinuity curve.

Leave-one-out as a general diagnosis technique

We start with a generic setup for neighbor embedding methods that encompasses SNE, t-SNE, UMAP, LargeVis, PaCMAP, among others. First, we introduce basic mathematical concepts and their interpretations.
Input data matrix: the input data to be visualized. Dimension d may be large (e.g. thousands).
Embedding matrix: the embedding points we aim to determine for visualization, where p can be 2 or 3.
(Pairwise) similarity scores: a measure of how close two input points are in the input space, often calculated based on a Gaussian kernel.
(Pairwise) embedding similarity scores: a measure of how close two embedding points are, which takes the form of a heavy-tailed kernel (e.g. t-distribution). The computation often requires a normalization step.
(Pairwise) loss function: a measure of discrepancy between v i, j and w i, j. An NE method minimizes this loss function over embedding points to preserve local neighborhood structures. The algorithms of NE methods aim to find the embedding Y by minimizing the total loss composed of the sum of the divergences between v i, j and w i, j of all pairs of points and a normalization factor Z (Y).

---

### UnidecNMR: automatic peak detection for NMR spectra in 1–4 dimensions [^6d76f76f]. Nature Communications (2025). High credibility.

Results

Theory

The kernel of the algorithm was originally developed to analyse mass spectrometry data, a problem that shares many features with NMR data analysis. The method relies on the assumption that a spectrum of intensities, I, can be reasonably expressed as a convolution of a peak shape function, g, and an array of delta functions or sources, f, each of which spans the same set of spectral frequencies, i. The algorithm aims to perform the deconvolution that removes the peak shape function from the data, providing a user with a list of sources, f, that dictates both the peak positions and intensities. The kernel iterates (t) on the intensities of each spectral element:

When the back-calculated spectrum, (f∗g), has the same intensity as the data I at frequency i, then the ratio is equal to 1, and the algorithm has converged. The action of the algorithm is to suppress sources that are adjacent to the true 'centre' of a resonance. A user supplies a noise threshold and a peak shape function g (Fig. 1a). In our implementation, the initial intensities are set to the initial intensities of the raw data found at each position, and then iteratively adjusted according to Eq. (1) until convergence, where the final values in f provide the central locations and intensities of the picked peaks. To determine convergence, the changes made in intensities are assessed in each step, and when these fall below a user-specified threshold, or when the calculation exceeds a pre-specified number of maximum iterations, the calculation stops. The selected peak shape g should be a reasonable match for the average resonance in a spectrum, although the final results are reasonably tolerant to this parameter being mis-set (Supplementary Fig. 1a), as expected in the case of experimental data where there is a wide range of peak shapes. The algorithm can work on data of arbitrary dimensionality, which renders it highly amenable to NMR analysis.

---

### The mark of vegetation change on earth's surface energy balance [^7d2d84a4]. Nature Communications (2018). Medium credibility.

The transformation we apply to reduce the dimensionality of X involves a singular value decomposition (SVD). This procedure is very close to a principal component analysis (PCA). The first step consists of centring all the columns of the predictor matrix X of vegetation fractions by removing the column means. We then apply the SVD:where M is the appropriate matrix of column means, U and V are the matrices containing respectively the left hand and right hand singular vectors, and D is a diagonal matrix containing the singular values (the standard deviations of the ensuing dimensions). Squared values of D indicate how much variance is explained by each (orthogonal) dimension. We implement a rule where as many dimensions from this SVD are retained as to conserve 100% of the original matrix's variation. In doing so, we reduce the dimensionality appropriately as described above, as well as remove what may be additionally redundant dimensions that can occur locally if, for instance, the only points in which 2 classes are represented have exactly the same values. To avoid having problems when there is too little or no information (e.g. if all pixels have exactly the same compositions), we added a pre-condition that there must be at least 10 pixels with different compositions. The final appropriately transformed predictor matrix of reduced dimension Z is then obtained by:where the subscript z in V z indicates that the latter is composed of a subset of right hand singular vectors in V as selected from D as described above. The resulting predictor matrix Z can now be regressed onto the local biophysical variable y.where Z has been augmented with a leading column of ones to accommodate an intercept term in the regression. The standard manner to obtain an estimate of β is:

---

### Whole-central nervous system functional imaging in larval drosophila [^e3ed9479]. Nature Communications (2015). Medium credibility.

Wave detection from ROI time series

To extract basic features of segmentally coordinated activity in isolated preparations, we measured changes in fluorescence within 16 3D ROIs placed within regions that corresponded to abdominal hemisegments (eight left and eight right). Changes in fluorescence (as Δ F / F) were extracted through spatial averaging within each ROI.

Activity typically alternated between periods of minimal responsiveness, and 'bouts' including waves of calcium responses either forward (posterior to anterior) or backward (anterior to posterior). These waves could be identified by examining either movies of activity (Fig. 2 and Supplementary Movies 1, 2, 3, 4, 5) or segment traces (for example, Fig. 3a, b). We developed an automated procedure to detect the timing and direction of these waves in a standardized fashion across multiple data sets.

First, the following preprocessing steps were applied to the 16 segment time series. The first 120 s of data were ignored to avoid response non-stationarities at the beginning of the recording. Each time series was z -scored (subtracting the mean and dividing by the s.d.). The average response across all segments was subtracted from each of the individual time series. This step normalizes for the overall increase in activity across all segments that occurs during waves, and instead highlights relative changes across the different segments. Finally, to focus on symmetric patterns, signals from left and right hemisegments were averaged, yielding eight preprocessed time series.

The detection of waves proceeds in two steps. First, we embed the 8D time series in a 2D space using the singular value decomposition (or PCA). Then, we detect simple features indicative of waves in the resulting 2D space.

More specifically, given a matrix X that is 8 × T, where T is the number of time points, we compute the singular value decomposition as:

where U and V are orthogonal matrices containing the left and right singular vectors and S is a diagonal matrix with the singular values. The first two columns of V, v 1 and v 2, are T × 1 vectors containing a 2D basis for the data. In the resulting 2D space, we found that waves correspond to rotations away from and back toward the origin; forward and backward waves correspond to rotations in different directions (Fig. 3i, j). Thus, it was natural to use amplitude and phase within this 2D space as a means for detecting the presence and direction of waves. Specifically, we computed the amplitude and phase as:

---

### Sources of path integration error in young and aging humans [^8533f4ab]. Nature Communications (2020). High credibility.

To derive the non-linear observation function we need to find a coordinate transformation such that in the transformed frame the noise is added linearly. According to Eq. (2), the noise is added linearly in log-polar coordinates. The observation functiontherefore corresponds to the transformation from cartesian to log-polar coordinates:and the observation z s +1 is related to the reportsandby

The EKF framework permits the calculation of two important distributions using Gaussian approximations: the posterior distribution of the hidden variablegiven the observations z 1 to z s (predictive distribution), and the posterior distribution ofgiven z 1 to z s +1 (updated distribution). We denote the mean and covariance of these posterior distributions as

Mean and covariance of both distributions can be calculated recursively over stopping points using the standard EKF update equations:where the matrices F s +1 and H s +1 are the Jacobian matrices of transition and observation function evaluated at the previous updated mean μ s | s and predictive mean μ s +1| s, respectively:

At the starting point (s = 0, we initialize μ s = 0| s = 0 = x start = 0 and P s = 0| s = 0 = 0. Next, we calculate the predicted distribution of the next measurement z s +1 given the previous measurements z 1 to z s by integrating out the internal estimate:where we have used the linearization approximation of the EKF at the third line. This allows us to express the full log-likelihood aswhere the dependency on the parameters Θ is introduced through f, its Jacobian F s +1, Σ x and Σ z. In analogy to Eq. (8), we find the maximum likelihood (ML) estimate for Θ by numerically maximizing the log-likelihood:

Numerical parameter optimization was performed using the fminunc-function of MATLAB's optimization toolbox.

---

### Efficient neural codes naturally emerge through gradient descent learning [^8fec235d]. Nature Communications (2022). High credibility.

What W learns: autoencoding objective

Further describing the growth of singular values requires a choice of objective. The base case of our study is the autoencoding objective defined for a set of inputs X:

Our goal is to determine how W evolves for this cost function. We will examine both the singular vectors and the singular values.

During learning, the singular vectors rotate (recall they are unit length and orthogonal) until they reach a fixed point. For this cost function, it is easy to verify that a fixed point of dynamics is when the singular vectors are equal to the principal components of the inputs (see Supplementary Methods for proof). That is, the vectors are static when Σ x x = V Λ V T and W = V S V T for the same V but potentially different Λ and S. This alignment is especially relevant given the expression for network sensitivity derived above. With the vectors aligned, the sensitivity to each corresponding principal component of the inputs is given by, the squared singular value of W.

The evolution of sensitivity is thus governed by the evolution of singular values. The rate of change of σ i is complicated to calculate because the singular vectors can potentially rotate. However, for the sake of analysis one can examine the case when the singular vectors are initialized at the fixed point mentioned above, as in previous literature. In this set of initial conditions, the time-evolution of each singular value of W is given by refs.:

Note that the rate of learning is controlled by λ i, the standard deviation of the i th principal component of the inputs. The term on the right causes σ i (t) to converge to 1 asymptotically, as is expected as the solution of the full-rank reconstruction problem is W = I. For deeper networks (N ≥ 2), the growth is sigmoidal and approaches a step function as N → ∞ (see ref.). Thus, in this axis-aligned initialization, the singular values σ i (t) are learned in order of the variance of the associated principal components of the inputs.

Together, these results mean that the sensitivity of a linear network's output to the principal components of the inputs evolve in order of variance when trained on input reconstruction. This is exactly the case for the axis-aligned initialization and approximately true for small initializations. For the single-matrix network displayed in the figure in the main text, the sensitivity to the j th PC thus evolves over time t as:

---

### The quantum transition of the two-dimensional ising spin glass [^12bdf2f8]. Nature (2024). Excellent credibility.

Even operators

This case is of utmost relevance because only even excited states can cause the system to leave its ground state in (ideal) quantum annealing for the Hamiltonian (equation (1)). Our approach is not entirely satisfying in this respect because, for a given sample, we obtain the smallest of the two same-parity gaps Δ e and Δ o (one would like to study only Δ e). Fortunately, both gaps have similar magnitudes (Extended Data Fig. 3).

The first optimistic indication comes from the (subtracted) correlation function in Fig. 3a, which goes to zero (within errors) for a moderate value of τ. Indeed, the empirical distribution function for the correlation length η e in Fig. 3b indicates mild sample-to-sample fluctuations and a relatively weak dependence on L. In fact, as shown Fig. 3b, for all L > 12, the probability distribution function turns out to depend on the scaling variable(Setting, the whole curve cannot be made to scale and the resulting estimate z e ≈ 1.7 is lower, see also the concluding paragraph of 'Two-times observables' in Methods). Thus, as anticipated, we conclude that the even symmetry sector shows algebraic scaling for its gap.

Fig. 3
Studying the spectra of even excitations at the critical point.

a, Sample-averaged subtracted correlation function Q 2, s (τ) ('Fitting process and estimating the Euclidean correlation length' in Methods) becomes compatible with zero for moderate values of τ, for all our system sizes. b, Left, after computing the Euclidean correlation lengthfor each sample, we computed for each L the empirical distribution function F (η e), namely the probability F of finding a sample with(note the horizontal error bars). Right, the data in the left-hand panel of b, when plotted as a function of the scaling variable u (equation (4)) do not show any L residual L dependence other than for our smallest sizes L = 8 and 12. Points in a and b are statistical averages, and errors are one standard deviation. Our data set is fully described in Extended Data Table 1.

Source Data

---

### A simple noniterative principal component technique for rapid noise reduction in parallel MR images [^3cce2704]. NMR in Biomedicine (2012). Low credibility.

The utilization of parallel imaging permits increased MR acquisition speed and efficiency; however, parallel MRI usually leads to a deterioration in the signal-to-noise ratio when compared with otherwise equivalent unaccelerated acquisitions. At high accelerations, the parallel image reconstruction matrix tends to become dominated by one principal component. This has been utilized to enable substantial reductions in g-factor-related noise. A previously published technique achieved noise reductions via a computationally intensive search for multiples of the dominant singular vector which, when subtracted from the image, minimized joint entropy between the accelerated image and a reference image. We describe a simple algorithm that can accomplish similar results without a time-consuming search. Significant reductions in g-factor-related noise were achieved using this new algorithm with in vivo acquisitions at 1.5 T with an eight-element array.

---

### Prediction and validation of protein intermediate States from structurally rich ensembles and coarse-grained simulations [^67e0d9dd]. Nature Communications (2016). Medium credibility.

Where k B is the Boltzmann constant, T is the temperature of the stochastic bath (300K), and the δ-Dirac functions ensure the independence of the components of the noise vector. Besides of representing the solvent, the friction and noise terms create a natural thermostat where random energy shots are balanced by the dissipative forces, keeping constant temperature and energy.

The stochastic equation of motion in (1) is integrated numerically with the Verlet algorithm, which gives for the velocities and positions after timestep Δ t (1 fs). On the above equation, the force acting on each residue i, F i, is computed assuming hookean elastic potentials for its interactions with the rest of residues j:

Where N is the number of protein residues, r ij and r ij 0 are the instantaneous and equilibrium distances between pair residues i and j, and K ij is their spring constant defined by the ED-ENM force-field which sets fixed MD-calibrated values for the first three C-alpha neighbours in the peptide chain to keep backbone stereochemistry, and a exponential function for long-range non-sequential interactions (details in ref.).

All the parameters in the ENM potentialand the BD simulation enginewere carefully optimized to reproduce the sampling by standard force-fields using as reference the MoDEL (molecular dynamics extended library)database of state-of-the art atomistic simulations with explicit solvent, as well as experimental data from X-ray crystallography and NMR. At the default temperature of 300K, the friction force is balanced to act as a thermostat according to the fluctuation-dissipation relation (see above). Note that the random forces acting on each particle (set by the random seed) render minimally different paths each time (Supplementary Fig. 11a).

Biasing of the trajectory in the direction of the transition is achieved by dynamic importance sampling (DIMS) based on an informational criterionwhere a Maxwell demon is introduced to enrich the trajectory in movements that approach the structure towards the target. Accordingly, for every certain number (k) of unbiased cycles, a progress variable, Γ i, for the instantaneous structure, R i, is recomputed and compared with the target one, and used as criteria to accept or reject the random moves (see also ref.). Here we define the progress variable in terms of the difference in pairwise distances of the starting and target structures:

---

### Understanding the genetic determinants of the brain with MOSTest [^14559908]. Nature Communications (2020). High credibility.

In the simulation scenarios with regularization, the correlation matrixmatrix was replaced with its regularized version, whereis an SVD decomposition of, and the diagonal matrixwas obtained from S by replacing r smallest eigenvalues with the next smallest eigenvalue; r is an integer parameter that runs from 0 (no regularization) to K-1 (full regularization).

Controlling for covariates, such as genetic principal components, is done via pre-residualization of all phenotype vectors, i.e. we replace them with the corresponding residual after multiple linear regression of the phenotype vector on the covariates. In addition, we perform a rank-based inverse-normal transformation of the residualized phenotypes, to ensure that z -scores forming the input to MOSTest are normally distributed.

MOSTest code is publicly available:

MiXeR analysis

We applied a causal mixture model, to estimate the percentage of variance explained by genome-wide significant SNPs as a function of sample size. For each SNP, i, MiXeR models its additive genetic effect of allele substitution, β i, as a point-normal mixture, where π 1 represents the proportion of non-null SNPs (polygenicity) andrepresents variance of effect sizes of non-null SNPs (discoverability). Then, for each SNP, j, MiXeR incorporates LD information and allele frequencies for 9,997,231 SNPs extracted from 1000 Genomes Phase 3 data to estimate the expected probability distribution of the signed test statistic, where N is sample size, H i indicates heterozygosity of i th SNP, r ij indicates allelic correlation between i th and j th SNPs, andis the residual variance. Further, the three parameters, are fitted by direct maximization of the likelihood function. Fitting the univariate MiXeR model does not depend on the sign of z j, allowing us to calculatefrom MOSTest p -values, where F −1 is the inverse function of the standard normal c.d.f. and p j is p -value from MOSTest. Finally, given the estimated parameters of the model, the power curve S (N) is then calculated from the posterior distribution p (δ j | z j, N).

---

### Universality and quantum criticality in quasiperiodic spin chains [^fe952862]. Nature Communications (2020). High credibility.

Ising model

We briefly remark on the q = 2 Potts model, i.e. the Ising model. In this case, c = 0 in the decimation rule (2). Thus, the RG does not take arbitrary functions to sequences. Instead, nonsingular sequences ofare generically squashed under the RG and become effectively constant after a few steps. Remarkably, this corresponds to the observation that weak quasiperiodic modulations are (marginally) irrelevant at the clean Ising critical point. It is indeed known from free fermion numerics, that the Ising transition in the presence of any such nonsingular quasiperiodic potentials is governed by the clean Ising conformal field theory, in agreement with the predictions of the RG. To see a nontrivial transition in this case, one must take singular distributions of; one can do this, e.g. by takingand h i = W h (a + cos(2 πφi + θ h)), with 0 < a < 1 so, is singular. For a > 1, quasiperiodic potentials flow to uniform ones under renormalization, indicating a transition in the clean Ising universality class. For a < 1 the critical dynamics is strongly modified, because the chain has a finite density of nearly broken links, corresponding to a flow to a quasiperiodic fixed point. This quantitatively reproduces the phase diagram obtained in refs. For a < 1, the RG does not lead to perfect sequences, and does not flow to "infinite quasiperiodicity" as the examples described above. This means that the perturbation theory steps do not become asymptotically exact under the RG. This is a physical feature of the transition, as it is known from numerics that it has a finite dynamical exponent. (This is similar to the random case where the strong disorder RG is exact only for infinite-randomness critical points which have infinite dynamical exponents.) Although our RG procedure is not fully controlled for this model, running the RG numerically yields a fixed point that is qualitatively similar to the numerically seen one, (Fig. 2): in particular, it has a correlation length exponent ν = 1, and a finite dynamical critical exponent z ≈ 1.6, which is close to the numerical value z ≈ 2. The remaining discrepancies are to be expected given that the procedure is not controlled; however, the RG correctly captures the qualitative features of the transition and is the only analytic method able to capture the universality class of this transition.

---

### Three key residues form a critical contact network in a protein folding transition state [^2dc93d95]. Nature (2001). Excellent credibility.

Determining how a protein folds is a central problem in structural biology. The rate of folding of many proteins is determined by the transition state, so that a knowledge of its structure is essential for understanding the protein folding reaction. Here we use mutation measurements — which determine the role of individual residues in stabilizing the transition state — as restraints in a Monte Carlo sampling procedure to determine the ensemble of structures that make up the transition state. We apply this approach to the experimental data for the 98-residue protein acylphosphatase, and obtain a transition-state ensemble with the native-state topology and an average root-mean-square deviation of 6 A from the native structure. Although about 20 residues with small positional fluctuations form the structural core of this transition state, the native-like contact network of only three of these residues is sufficient to determine the overall fold of the protein. This result reveals how a nucleation mechanism involving a small number of key residues can lead to folding of a polypeptide chain to its unique native-state structure.

---

### Uncovering the heterogeneity and temporal complexity of neurodegenerative diseases with subtype and stage inference [^86bc05e0]. Nature Communications (2018). Medium credibility.

The split-cluster E-M procedure shown in the blue box in Supplementary Figure 15 is used to generate each of the C-1 candidate C cluster models. For each of the C-1 clusters, the split-cluster E-M procedure first finds the optimal split of cluster c into two clusters. To find the optimal split of cluster c into two clusters, the data points belonging to cluster c are randomly assigned to two separate clusters. The optimal model parameters for these two data subsets are then obtained using the single-cluster E-M procedure (green box in Supplementary Figure 15). These cluster parameters are used to initialise the fitting of a two-cluster model to the subset of the data belonging to cluster c, using E-M. This two-cluster solution is then used together with the other C-2 clusters to initialise the fitting of the C -cluster model. The C -cluster model is then optimised using E-M, alternating between updating the sequences S c for each cluster and the fractions f c. This procedure is repeated for 25 different start points (random cluster assignments) to find the maximum likelihood solution (see Convergence).

The single-cluster E-M procedure shown in the green box in Supplementary Figure 15 is used to find the optimal model parameters (the sequence S in which the biomarkers reach each z -score) for a single-cluster. In the single-cluster E-M procedure the sequence S is initialised randomly. This sequence is then optimised using E-M by going through each z -score event E in turn and finding its optimal position in the sequence relative to the other z -score events, i.e. by fixing the order of the subsequence T = S /E and maximising the likelihood of the sequence by changing the position of event e in the subsequence T. The sequence S is updated until convergence. Again the single-cluster sequence S is optimised from 25 different random starting sequences to find the maximum likelihood solution (see Convergence).

---

### Maximizing spectral sensitivity without compromising resolution in phase-incremented, steady-state solution NMR [^06a1fcb2]. Nature Communications (2025). High credibility.

Fig. 2
Phase-incremented (PI) SSFP approach to high-resolution NMR.

a Pulse sequence involving a train of NS signal-averaged scans excited by pulses of flip-angle α spaced by a repetition time TR, and relative phases φ m incremented as shown over M uninterrupted experiments. b Single-site SSFP response vs relative phase increment φ m for different flip angles α, assuming f = 0, T 1 = 5 s, T 2 = 2 s, and constant (zero) receiver phase. A similar response would arise from pulses with a constant phase as a function of the site's offset. c Fourier coefficients { A k } derived from Eq. [1], describing the SSFP response in (b); notice their rapid drop with increasing α. d β-coefficients derived from a least-square solution of, needed to recapitulate the illustrated filter centered at zero. e Actual frequency response resulting from applying the {β m }-coefficients on M = 50 PI-SSFP experiments upon using NB = M/2 frequency bands, evidencing a deteriorating resolution with increasing flip-angle.

Achieving high spectral selectivity means that this linear combination function should mimic as closely as possible a discrete bandpass filter, whose line shape will define the "peaks". Based on filter response theory, this filter can be written aswhere the { C k } -N/2 ≤ k ≤ N/2–1 coefficients can be calculated based on a desired response (e.g. using the Finite Impulse Response script in Matlab's signal processing toolbox). Finding the linear combination that makes F(f) as close as possible to R(f) then demands solving a series of linear equations

Suitable solutions { β m } 0 ≤ m ≤ M-1 of these equations will generate a narrow filter with a targeted width of 1/(TR. NB) – NB being the total number of bands (peaks) to be resolved within ± 1/2 TR. Eq. [4] can also be written in matrix form aswhere C is a N-by−1 vector containing the { C k } -N/2 ≤ k ≤ N/2–1 coefficients, β is an M-by-1 vector with the { β m } 0 ≤ m ≤ M−1, and L is a N-by-M matrix whose k, m -element is

---

### De novo main-chain modeling for EM maps using MAINMAST [^40b5bb0a]. Nature Communications (2018). Medium credibility.

Identifying local dense points with mean shift

The first step of the algorithm is to identify LDPs in an EM map, where protein atoms are more likely to exist. The mean shift algorithm, a non-parametric clustering algorithm originally developed for image processing, is used for this task. The assumption is that a density observed in a map is the sum of density functions that originate from atoms in the map. The primary assumption of the mean shift algorithm is that each point in the map represents a Gaussian density function and the local maxima of dense regions correspond to the chain positions of proteins. This process significantly reduces the number of points to consider in an EM map. For given grid points x i (i = 1, …, N) of an EM map, initial seed points(j = 1, …, M) are chosen whose density value is not less than a given threshold. The seed pointsare iteratively updated, as follows:where k (p) is a Gaussian kernel function andis a density value of the grid point x. The k (p) is defined aswhere the σ is a bandwidth, which is set to 1.0.

After the seed point positionsare updated, the density value of the pointsare computed asThe density is further normalized with the minimum density valueand the maximum density valueof all the seed points

Points are discarded if the density does not satisfy the threshold θ thr. Also, seed points that are closer than a threshold distance (0.5 Å) are clustered, and the highest density, z k in cluster k, is chosen as the representative of the cluster. This process is iterated until the positions of the selected representative points are converged. The representative points in the clusters are called LDPs. Finally in this step, LDPs z i and z j are labeled as adjacent if any pair of converged points, and, in the two clusters where the LDPs belong to are originally adjacent at their initial seed point position,… This list of adjacent LDPs is used in the next step of connecting LDPs. Typically, the number of clusters is about 40% of the number of heavy atoms of the underlined protein in the map.

---

### Generating conjectures on fundamental constants with the ramanujan machine [^75e8911d]. Nature (2021). Excellent credibility.

Fundamental mathematical constants such as e and π are ubiquitous in diverse fields of science, from abstract mathematics and geometry to physics, biology and chemistry 1,2. Nevertheless, for centuries new mathematical formulas relating fundamental constants have been scarce and usually discovered sporadically 3–6. Such discoveries are often considered an act of mathematical ingenuity or profound intuition by great mathematicians such as Gauss and Ramanujan 7. Here we propose a systematic approach that leverages algorithms to discover mathematical formulas for fundamental constants and helps to reveal the underlying structure of the constants. We call this approach 'the Ramanujan Machine'. Our algorithms find dozens of well known formulas as well as previously unknown ones, such as continued fraction representations of π, e, Catalan's constant, and values of the Riemann zeta function. Several conjectures found by our algorithms were (in retrospect) simple to prove, whereas others remain as yet unproved. We present two algorithms that proved useful in finding conjectures: a variant of the meet-in-the-middle algorithm and a gradient descent optimization algorithm tailored to the recurrent structure of continued fractions. Both algorithms are based on matching numerical values; consequently, they conjecture formulas without providing proofs or requiring prior knowledge of the underlying mathematical structure, making this methodology complementary to automated theorem proving 8–13. Our approach is especially attractive when applied to discover formulas for fundamental constants for which no mathematical structure is known, because it reverses the conventional usage of sequential logic in formal proofs. Instead, our work supports a different conceptual framework for research: computer algorithms use numerical data to unveil mathematical structures, thus trying to replace the mathematical intuition of great mathematicians and providing leads to further mathematical research.

---

### An alternative to the breeder's and lande's equations [^1313df68]. G3 (2014). Low credibility.

Selection on multiple traits

The results of the aforementioned sections are naturally generalized to selection on multiple traits. Consider the vectors of parental breeding values y 0 = (y 1, y 2, …, y N), environmental effects e = (e 1, …, e N) and their phenotype z 0 = y 0 + e, to which a selection function W (z) is applied. Using the same notations as in the previous sections, we find without difficulty thatAs before, using FT, these relations transform intowhere ∇ is the gradient operator: ∇ f = (∂ f /∂ x 1,… ∂ f /∂ x N). We see again thatandare linearly related ifwhere A is a constant matrix. The linear relation is automatically satisfied if both p 0 and f follow a Gaussian distributionwhere G and E are the covariance matrices for the genotype and environmental effects. Defining P = G + E as the phenotype covariance matrix, it is straightforward to show that in this case A = EG −1 and therefore which is the usual breeder's equation for multiple traits. We stress that the limitation of this relation is the same as that of the scalar version: it relies on the normal distribution of the genotype. On the other hand, if the selection function W (z) is Gaussianthe arguments of the previous section 2 can be repeated and lead to the generalization of the alternative vectorial breeder's equation (21)which, in analogy with equation (21) we write as

---

### Learning meaningful representations of protein sequences [^e27a58e5]. Nature Communications (2022). High credibility.

Construction of entropy network

To ensure that our VAE decodes to high uncertainty in regions of low data density, we construct an explicit network architecture with this property. That is, the network p θ (X ∣ Z) should be certain about its output in regions where we have observed data, and uncertain in regions where we have not. This has been shown to be important to get well-behaved Riemannian metrics. In a standard VAE with posterior modeled as a normal distribution, this amounts to constructing a variance networkthat increases away from data. However, no prior work has been done on discrete distributions, such as the Categorical distribution C (μ θ (Z)) that we are working with. In this model we do not have a clear division of the average output (mean) and uncertainty (variance), so we control the uncertainty through the entropy of the distribution. We remind that for a categorical distribution, the entropy isThe most uncertain case corresponds to when H (X ∣ Z) is largest i.e. when p (X ∣ Z) i = 1/ C for i = 1. C. Thus, we want to construct a network p θ (X ∣ Z) that assigns equal probability to all classes when we are away from data, but is still flexible when we are close to data. Taking inspiration fromwe construct a function α = T (z), that maps distance in latent space to the zero-one domain. T is a trainable network of the model, with the functional formwith, where κ j are trainable cluster centers (initialized using k -means). This function essentially estimates how close a latent point z is to the data manifold, returning 1 if we are close and 0 when far away. Here K indicates the number of cluster centers (hyperparameter) and β is a overall scaling (trainable, constrained to the positive domain). With this network we can ensure a well-calibrated entropy by pickingwhere. For points far away from data, we have α = 0 and returnregardless of category (class), giving maximal entropy. When near the data, we have α = 1 and the entropy is determined by the trained decoder p θ (X ∣ Z) i.

---

### Smooth 2D manifold extraction from 3D image stack [^eecea19d]. Nature Communications (2017). Medium credibility.

Optimization process

The goal of the optimization process is to perform the minimization defined by equation (1) to obtain the final index map Z *. As Z is a non-parametric function, a dedicated non-parametric optimization scheme was proposed (see Algorithm 1 in Supplementary Methods). Briefly, starting from an initial index map Z 0 = Z max, the search consists in computing, for each location (x, y) of the index map Z, the cost (equation 1) of moving z = Z (x, y) up, down or keeping it steady, and hold the value that minimize it. The algorithm iterates on this simple scheme until a balanced is found between the Z map smoothness σ z and its proximity to the foreground jointly for all pixels. When computing the cost (equation (1)), the maximum focus map Z max and the class map C are predefined constant. However, the local spatial s.d. σ z depends on the smoothness of Z and needs to be updated whenever Z is modified. Therefore, the local spatial s.d.for the whole index map Z t −1 obtained at previous iteration is computed once at the beginning of each iteration on a 3 × 3 widndow. Then, for the sake of computational efficiency, the computation offor each location (x, y) for each shift (up or down) can be computed rapidely from σ z without the need for rescanning the window (see Supplementary Methods for the details on how the rolling s.d. formula is derived and can be used for this). This minimization process gradually increases the smoothness of the index map while keeping it near the foreground pixels, hence forcing the z level of background pixels to shift towards the z level of the local foreground. Finally, as the intensity is only known for discrete values of Z, at the end of the optimization process the composite image is constructed by extracting the intensity values that are the closest to the Z index map on the z axis. No interpolation is performed to preserve original fluorescence values. The settings of the two parameters used for this optimization scheme are described in Supplementary Methods and in Supplementary Figs 5 and 6. Also, to intuitively understand the minimization process, Supplementary Movies 1–6 illustrate it visually for six data sets of Supplementary Table 1.

---

### Dielectric anomalies and interactions in the three-dimensional quadratic band touching luttinger semimetal PrIrO [^f7522e5c]. Nature Communications (2017). Medium credibility.

It is natural to ascribe the large measured value ofto the near divergence of the dielectric constant in a 3D QBT; however, one must be careful in the quantitive application of Eq. (1). When further including interactions in a high-order calculation of the dielectric function, the result from RPA Eq. (1) actually appears as only the first term in an expansion in the parameter E 0 / E F. If this parameter is large — as it is in our case — RPA may not formally hold. Indeed, the breakdown of perturbation theory is what compelled AB to develop their scaling theory, in which strong interactions are believed to drive the form of the dielectric constant into a regime where the dynamic exponent z differs slightly from 2. Interestingly, the RPA still provides a surprisingly robust starting point for understanding our results. The AB expression is, where ω 0 is defined such that this expression reduces to Eq. (1) if z = 2. To find the scale of the further terms in theexpansion, we work backwards from AB's expression to first order in 1/ z with an expansion around z = 2 to get, where. From previous calculations, δ can be estimatedto be approximately 0.055 and therefore (upon substituting for E F) corrections to the RPA form are estimated to only be of order 0.3%. Although the system is in a strongly interacting regime, the consequences of strong interactions are subtle.

With increasing temperature, charges are excited and as shown in Fig. 4a, the plasma frequency increases. These thermal carriers block low-energy interband transitions around the node, which weakens the enhancement of the dielectric constant observed at lower temperature. When the temperature is raised beyond the degeneracy temperature, RPA calculations predict that the dielectric constant is expected to fall off as. However, one can see in Fig. 4c that dielectric constant decays much faster. One explanation of this discrepancy between experiment and theory may come from the fact that the quasiparticle spectral function of Pr227 as measured by photoemission has an extremely strong temperature dependence. Near the band touching, quasiparticle-like features were only observed below 100 K. If the sharp LSM quasiparticle spectrum of the system gradually loses its features as temperature increases, it is reasonable to observe faster decay of the dielectric constant.

---

### Dynamical regimes of diffusion models [^a7936fcc]. Nature Communications (2024). High credibility.

Speciation time

In order to show the existence of regimes I and II, and compute the speciation time, we focus on the following protocol which consists of cloning trajectories. We consider a backward trajectory starting at time t f ≫ 1 from a point x f drawn from a random Gaussian distribution where all components are independent with mean zero and unit variance. This trajectory evolves backward in time, through the backward process until time t < t f. At this time the trajectory has reached the point y (t), at which cloning takes place. One generates for τ < t two clones, starting from the same x 1 (t) = x 2 (t) = y (t), and evolving as independent trajectories x 1 (τ) and x 2 (τ), i.e. with independent thermal noises. We compute the probability ϕ (t) that the two trajectories ending in x 1 (0) and x 2 (0) are in the same class. Defining P (x 1, 0∣ y, t) as the probability that the backward process ends in x 1, given that it was in y at time t, the joint probability of finding the trajectory in y at time t and the two clones in x 1 and x 2 at time 0 is obtained as Q (x 1, x 2, y, t) = P (x 1, 0∣ y, t) P (x 2, 0∣ y, t) P (y, t). Then ϕ (t) is the integral of Q over x 1, x 2, y with the constraint (x 1 ⋅ m)(x 2 ⋅ m) > 0. This simplifies into a one-dimensional integral (see SI Appendix):where G (y, u, v) is a Gaussian probability density function for the real variable y, with mean u and variance v, and, Γ t = Δ t + σ 2 e −2 t. The probability ϕ (t) that the two clones end up in the same cluster is a decreasing function of t, going from ϕ (0) = 1 to ϕ (∞) = 1/2. In the large d limit, the scaling variable controlling the change of ϕ iswhich can be rewritten asby using. This explicitly shows that speciation takes place at the time scale t S on a window of time of order one. As detailed in SI Appendix, this expression for t S coincides with the one obtained from the general criterion (5). We show in Fig. 2 the analytical result from (6) and direct numerical results obtained for increasingly larger dimensions. This comparison shows that our analysis is accurate already for moderately large dimensions. In the limit of infinite d, the analytical curve in Fig. 2 suddenly jumps from one to zero at t / t S = 1, corresponding to a symmetry-breaking phase transition (or a threshold phenomenon) on the time scale t S. In the numerics, following finite size scaling theory, we have defined the speciation time as the crossing point of the curves for different d, which corresponds approximatively to ϕ (t S) = 0.775 and indeed is of the orderfor d → ∞. As it happens in mean-field theories of phase transitions, the large dimensional limit allows to obtain a useful limiting process. In our case, this leads to a full characterization of the asymptotic backward dynamics. At its beginning, i.e. in regime I, the overlap with the centers of the Gaussian model, ± m ⋅ x (t), is of order. Defining, one can obtain a closed stochastic Langevin equation on q in a potential V (q, t) (see SI Appendix), where η (t) is square root of two times a Brownian motion, andAt large d, this potential is quadratic at times, and it develops a double well structure, with a very large barrier, when. The trajectories of q (t) are subjected to a force that drives them toward plus and minus infinity. The barrier between positive and negative values of q becomes so large that trajectories commit to a definite sign of q: this is how the symmetry breaking takes place dynamically at the time scale t S, in agreement with the previous cloning results. Regime II corresponds to the scaling limit q → ∞, where m ⋅ x (t) becomes of order d. In this regime, the rescaled overlap m ⋅ x (t)/ d concentrates, and its sign depends on the set of trajectories one is focusing on. Moreover, the stochastic dynamics of the x i correspond to the backward dynamics for a single Gaussian centered in ± m. This shows that the dynamics generalizes, see SI Appendix (and alsofor similar results).

---

### To infinity and some glimpses of beyond [^0f9a5f29]. Nature Communications (2017). Medium credibility.

Results

Ordinary differential equations

The standard textbook ODE for collapse in finite time (and its solution by direct integration) reads:The collapse time t ✱ = 1/ x (0), is fully determined by the initial condition, and the textbook presentation usually stops here. A numerical solver would overflow close to (but before reaching) t *; yet we can bypass this infinity by appropriately transforming the dependent variable x near the singularity. Indeed, the good quantit y y ≡ 1/ x ≡ x −1, satisfies the good differential equation d y /d t = −1; this equation will help cross the infinity (for x) by crossing zero and smoothly emerging on the other side (for y). Once infinity is crossed, we can revert to integrating the initial (bad, but now tame again) equation for x.

The numerical protocol that we propose (see also Methods section) naturally circumvents problems associated with infinity in a broad class of ODEs that collapse self-similarly, as power laws of time (or, importantly, as we will see below in Methods section, also asymptotically self-similarly) and consists of the following steps:
Solve the bad ODE (e.g. Eq. (1)) for a while, continuously monitoring, during the integration, its growth toward collapse.
If/when the approach to collapse is detected, estimate its (asymptotically) self-similar rate (the exponent of the associated power law, e.g. −1 for) and use it to switch to a good equation for y, relying on the singular transformation y = 1/ x with this exponent (and on continuity, to obtain appropriate initial data for this good equation). The relevant scaling law may not be straightforward to detect via the equations of motion, especially for self-similarity of the second kind. Nevertheless, a numerical identification utilizing, e.g. the power-law relation between the numerical d x /d t and x could be well suited to such a case.
Run this good equation for y until 0 (or ∞ for the former, bad equation) is safely crossed, computationally observing for x an (asymptotically) self-similar return from infinity.
Finally, transform back to the bad equation (now tamed, as infinity has been crossed) and march it further forward in time.

---

### Identifying domains of applicability of machine learning models for materials science [^72c4ba76]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ν that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by σ lin (x 1, x 2) ≡ −0.3 ≤ x 1 ≤ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: σ rbf ≡ −3.3 ≤ x 11 ≤ 3.1 ∧ x 2 ≤ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA σ poly (x 1, x 2) ≡ −3.5 ≤ x 2 ≤ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### Multi-modal mobility morphobot (M4) with appendage repurposing for locomotion plasticity enhancement [^e355b702]. Nature Communications (2023). High credibility.

The energetic cost on a flying edge C f is computed using the power consumption P f in hovering, the robot forward velocity v f in flying mode, and the altitude z of the two nodes. Hence, C f is given by:where z 1 and z 2 are respectively the altitudes of nodes 1 and 2, m is the mass of the robot, and g is the gravitational acceleration constant. Last, the transition cost C t between the two modes is determined based on the power consumption of the joints during the morphing process P t. Then, P t is integrated over the time of transition t t which yields:

These three energetic costs are employed to determine the optimal path in the edge space generated by the MM-PRM algorithm using the A ⋆ algorithm.

To find the optimal path in the graph, the A ⋆ path search algorithmis used. The improved version of Dijkstra's algorithmis employed to find the optimal path by using a heuristic function. The algorithm computes the best path to each node to only visit the most promising nodes. This avoids going through all possible paths and, therefore, finding the first-best optimal path with a low computational cost. Thus, each time the algorithm explores the n -th node, it calculates the minimum cost f (n) necessary to reach the goal by passing through it using the following formula:where g (n) is the real cost from the start to the n-th node, computed based on (21), and h (n) denotes the heuristic cost to the goal. The heuristic cost h (n) is calculated by summing two conservative costs. First, the cost of driving on flat ground to the goal in a straight line is calculated. Second, the cost of flying vertically along the z -axis to the goal is obtained. Since the cost of driving is much lower than flying, this is the most optimal way to move between two points if there are no obstacles or impassable terrains. The following cost for g (n) is defined:where E w and E f are respectively the number of ground and aerial edges traveled by the robot, C w, i is the cost on the ground edge i, C f, j denotes the cost on the flying edge j, and N t represents the number of mode transition made by the robot.

---

### Minimum-phase property of the hemodynamic response function, and implications for granger causality in fMRI [^6614d97d]. Human Brain Mapping (2025). Medium credibility.

As anticipated in Section 2.1, testing the minimum‐phase condition is much simpler via the transfer function. The first step to obtaining the transfer function is to linearize the system of differential equations around their fixed point. Therefore, we will linearize each HRF model to obtain its state‐space representation 1:

The matriceswill then be used to compute the transfer functionwhereis the Laplace variable (Oppenheim et al.). The transfer function fully characterises the linearised system and depends on the model parameters, which have a direct biological interpretation. These parameters are not indicated in the generic formula in Equation (4) but will appear explicitly once this is applied to specific HRF models in Section 3. In practice, will be the ratio of two polynomials in thevariable (the numerator and the denominator), whose coefficients depend on the model parameters. The polynomial roots of the numerator (i.e. the values ofthat make the denominator equal to zero) are called the zeros of the transfer function, while the roots of the denominator are called poles. We will find the zeros and poles of each of the transfer function of three HRF models to determine the parameter ranges that make each of them minimum‐phase according to the criterion provided in Section 2.1, that is, the parameter values such that all the zeros and poles of the transfer function have a negative real part. We will also use the transfer function to study the BOLD impulse response, that is, the response to an ideal, instantaneous neuronal input 2, which is obtained via the inverse Laplace transform of the transfer function.

---

### Refining potential energy surface through dynamical properties via differentiable molecular simulation [^85b212ae]. Nature Communications (2025). High credibility.

Consider a typical NVE simulation setup, where we are effectively propagating a deterministic ordinary differential equation (ODE) using the following dynamics:where f is an NVE integrator such as Velocity-Verlet or leap-frog, both of which are rigorously time reversible: (− p (t i −1), q (t i −1)) = f ((− p (t i), q (t i)), θ). The adjoint variable a (t i) is then defined as the derivative of the loss function L with respect to z (t i), while keeping the trajectory before t i unchanged but allowing the trajectory after t i to vary accordingly:When L is an explicit function of the entire trajectory: L = L (z (t 0), z (t 1), …, z (t K)), the final adjoint a (t K) can be trivially computed as a direct derivation. Starting from this point, we can then solve the reverse dynamics of a using chain rule:Thanks to the rigorous time reversibility of f, the variablecan be computed using only z (t i), without requiring any saved information from z (t i −1). Therefore, by following the dynamics given by Eq. (8), the whole adjoint trajectory can be obtained via a single backward pass of the existing trajectory. And according to previous work, the total parameter gradient can be then computed along the backward pass as:This computational process is highly efficient: the cost of the entire adjoint backward pass is similar to another normal MD run, with a constant memory cost that does not depend on the number of steps.

---

### Control of finite critical behaviour in a small-scale social system [^c81152c0]. Nature Communications (2017). Medium credibility.

Independent model inference

The independent model consists of individuals participating in conflict randomly, with the frequencies of individual appearance equal to their empirically measured, heterogeneous values f i = 〈 x i 〉. Naively, this can be written as a relative negative log likelihood, and is equivalent to a maximum entropy model that matches only the frequencies f i. (The relative negative log likelihood L (x) of state x is related to the likelihood p (x) by p (x) = exp(− L (x))/ Z, where Z is a normalization constant set by the constraint that the sum of likelihoods over all states is one. In statistical physics, Z is the partition function and L (x) is proportional to the free energy of state x.)

However, as detailed in 'Operational definitions' in the Methods section, a fight was operationalized for these analyses as involving two or more socially mature individuals. (Observed fights that involved only juveniles and 0 or 1 mature individuals are therefore excluded.) Correspondingly, we forbid our models from producing fights of size smaller than two. We treat this as an additional constraint on the model. The resulting maximum entropy model is then the one in which the likelihood of states with fewer than two individuals present is taken to zero. This corresponds to a relative negative log likelihood

where Θ(z) is 0 when z ≤ 0 and 1 when z > 0.

In the unconstrained case (α = 0), we can easily solve for h i:

We must now solve numerically for h i to match the empirically measured f i = 〈 x i 〉 = 〈 x i 〉 α →∞. To accomplish this, note that the unconstrained model will have modified statistics:

where f 0 is the frequency of size zero fights, f i 1 is the frequency of size one fights consisting solely of individual i, andis the overall frequency of size one fights (all measured in the unconstrained model). In terms of unconstrained individual frequencies, these are

We use an iterative procedure to solve equation (7) for 〈 x i 〉 α = 0, which are then used in equation (6) to find the fields. Finally, samples from the independent model (equation (4)) are produced by sampling using equation (5) and simply discarding samples in which fewer than two individuals appear. For our data, this results in discarding about 17% of samples produced with equation (5).

---

### Ideal topological flat bands in chiral symmetric moirésystems from non-holomorphic functions [^85debdfd]. Nature Communications (2025). High credibility.

Figure 1e shows the wavefunction ψ Γ (r) at the Γ point (k = 0). One key feature to highlight is that ψ Γ (r) never reaches zero. This feature directly indicates that(In principle, we could have chosen any momentum point k 0 to write. However, we choose k 0 = Γ because the quartic band crossing is at Γ.) must not be a holomorphic function, in direct contrast to other ideal flat bands such as the lowest LL or chiral TBG. As will be discussed below, these two cases, f k being holomorphic or non-holomorphic, represent the two allowed pathways toward achieving ideal flat bands in moiré systems.

Construction of wavefunction

Although f k is not holomorphic, we can still analytically construct Bloch wavefunctions for these ideal flat bands. The clue comes from Fig. 1f, where we plot the function. This function shares three key properties with the Bloch wavefunction of ideal flat bands in chiral TBG: (1) Bloch periodic, g (r + a) = g (r) e i k ⋅ a, (2) having isolated zeros in the unit cell [Fig. 1f], and most importantly, (3) "vortexable".

To prove its vortexability, we begin with the equations that ψ must satisfy,… By subtracting these two equations, we obtain the equation for g k whereis the binomial coefficient. Because this differential equation does not contain ∂ z, if g Γ is a solution, then for any arbitrary holomorphic function h (z), h (z) g Γ must also be a solution.

---

### Linear response theory of open systems with exceptional points [^a9a295ae]. Nature Communications (2022). High credibility.

Understanding the linear response of any system is the first step towards analyzing its linear and nonlinear dynamics, stability properties, as well as its behavior in the presence of noise. In non-Hermitian Hamiltonian systems, calculating the linear response is complicated due to the non-orthogonality of their eigenmodes, and the presence of exceptional points (EPs). Here, we derive a closed form series expansion of the resolvent associated with an arbitrary non-Hermitian system in terms of the ordinary and generalized eigenfunctions of the underlying Hamiltonian. This in turn reveals an interesting and previously overlooked feature of non-Hermitian systems, namely that their lineshape scaling is dictated by how the input (excitation) and output (collection) profiles are chosen. In particular, we demonstrate that a configuration with an EP of order M can exhibit a Lorentzian response or a super-Lorentzian response of order M s with M s = 2,3, …, M, depending on the choice of input and output channels.

---

### Extreme purifying selection against point mutations in the human genome [^d819fca0]. Nature Communications (2022). High credibility.

As for the simplified model above (Eqs. (2)–(5)), the log likelihood can be approximated as, whereis the total number of rare variants, now allowing for more than one per site; and Z is a term that does not depend on λ s. This function is maximized at, and a correction for the bias yields an estimator of, whereis the average of allvalues and we use the facts thatand.

When comparing Eqs. (5) and (9), notice that, by construction; thus, the full model will generally lead to slightly smaller estimates of λ s with a difference that reflects the number of multi-allelic rare variants. The two estimators are identical if there are no such sites.

Assuming thevalues are known, the variance offollows from the variance of, which — becauseis a sum of independent Bernoulli variables — is given by, where. Thus,

The standard errors we report for estimates of λ s are obtained by taking the positive square root of this quantity.

When data is simulated under the assumed model, we find that the estimator for λ s (Eqs. (5) and (9)) and the predicted variance (Eq. (11)) agree very well with the truth (Supplementary Fig. 4). Furthermore, if thevalues are assumed to be random but unbiased, thenand its standard error have almost no dependency on the variance of, at least in the regime of interest. For this reason, we ignore the variance in the mutation-rate estimates when estimating the standard errors for λ s.

ExtRaINSIGHT also reports a p -value based on a likelihood ratio test of an alternative hypothesis of λ s ≠ 0 relative to a null hypothesis of λ s = 0, assuming twice the log likelihood ratio has an asymptotic χ 2 distribution with one degree of freedom under the null hypothesis.

---

### Adaptive resetting for informed search strategies and the design of non-equilibrium steady-states [^9a9db8d1]. Nature Communications (2025). High credibility.

The final value theorem for Z-transforms states that. By using it, we get (see section 6 of the Supplementary Information)where 〈 N R 〉 is the mean number of time steps between consecutive resetting events. Note that the steady-state in Equation (14) is well defined whenever 〈 N R 〉 is finite, regardless of whether or not the process without resetting has a steady-state. This is a generalization of a well-known result in the theory of standard resetting to state- and time-dependent resetting.

To estimate the NESS, we first sample a set of N trajectories without resetting of length M Δ t. We stress that M should be large enough such that, had we used resetting, the probability of surviving M steps without resetting would be negligible, i.e. Then, we use Equations (12) and (14), and the definition of the Z-transform, to obtainThis estimation results in an unnormalized distribution, which should be normalized. The normalization factor provides an estimate for the mean time between consecutive resetting events, 〈 N R 〉. Equation (15) shows that the estimation of the NESS with resetting, from trajectories without resetting, is done by averaging the histogram of positions over time and trajectories, but reweighing each trajectory, at every time step, by its survival probability.

Prediction and design of non-equilibrium steady-states

The above results can be used to predict and design NESS of spatially-dependent resetting protocols. We demonstrate this using two examples.

It is well known that for free diffusion with a constant resetting rate, a Laplace distributed NESS emerges. An analytical solution for the NESS of diffusion with a parabolic resetting rate r (x) = r 0 x 2 is also known. Interestingly, in both cases, the tails of the NESS decay as, with α = 1 for the constant resetting rate, and α = 2 for the parabolic resetting rate. This raises a more general question: what is the asymptotics of the NESS for diffusion with a power-law resetting rate r (x) = r 0 ∣ x ∣ λ. While there are currently no known closed-form solutions for the NESS with λ ≠ {0, 2}, we can easily estimate the resulting NESS using the procedure described in the previous section.

---

### Non-additivity of molecule-surface van der waals potentials from force measurements [^0dec3a03]. Nature Communications (2014). Medium credibility.

Weighted least-squares regression

We use a weighted least-squares regression, minimizing the quantity

when fitting the N data points within the fit interval j = 1,… N. Choosing the right weights w j is a non-trivial task.

In the case of a linear fit, the goodness of fit (gof) is best described by the reduced χ 2, defined as

where x i is the i th measured point andthe corresponding value of the fitted curve. Here the respective weighting function is the inverse of the variance σ 2 that is a measure of the statistical noise in the measured curve. While with this definition one obtains χ 2 = 1 for a perfect fit, there is no general rule of how close a reduced χ 2 should be to 1 for a good fit.

For a linear fit, the reduced χ 2 as defined above is a suitable gof criterion, because according to this definition, each data point of a linear data set has the same chance of contributing to the overall χ 2 value. However, in our case, we do not perform a linear fit. Rather, we fit a force law that is proportional to z −5 and moreover our experimental noise is practically constant over the whole fit interval. To gauge the quality of our fit to experimental data following this force law, we must ensure that the entire measured curve contributes to our gof criterion. Otherwise, experimental information would be lost, because any part of the measured data curve that does not contribute significantly to the gof criterion has no influence on the outcome of the fit. For this reason, we use the gof criterion in equation (8) with the weighting function

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^a6af3eb5]. Nature Communications (2021). High credibility.

Frequency response of feedforward motifs can be solved analytically and demonstrates low- and band-pass filtering capabilities

Biological regulatory networks often encode information as the change in frequency of an oscillating input, which has been suggested to be more robust to noise than encoding information in absolute concentration –. That feedforward loops filter short square pulses suggests more general frequency-filtering capabilities. We therefore analyze feedforward frequency response to sinusoidal input (Fig. 6 d–g). We show that the response follows the outline of a universal transfer function curve, independent of the logic or delay difference.

Instead of the step input analyzed above, here we consider a sinusoidal inputwhich oscillates between zero and 2 A (twice the amplitude) at a frequency f > 0. Taking the Fourier decomposition of each Hill-regulated term 1/[1 + X n (T)] and plugging into the governing Eq. (21) (Supplementary Note 6) provides the output Z (T) in terms of its magnitude I k and phase ϕ k as a function of frequency:whereare Fourier coefficients of Hill-regulated terms 1/[1 + X n (T)] at integer multiples k of the fundamental frequency f :for all f > 0. Note that the magnitudes are symmetric to interchange of the two regulation arms (η 1 ↔ η 2, n 1 ↔ n 2), while the phases are not.

---

### Spatiotemporal data analysis with chronological networks [^6b856f1a]. Nature Communications (2020). High credibility.

Outliers can be treated by applying a correction function f (C, δ) to each element c t ∈ C whereis an odd natural number defining a time window size. This function assigns to every element c t the same value in the time window centered in c t iff all the values in the window are the same, i.e. if the number of unique elements in { c t − δ, …, c t −1, c t +1, …, c t + δ } is one.

The community structure can also be used to find temporal changes in spatiotemporal data sets composed by spatial clusters. Temporal changes can be detected by tracking alterations in the communities where the events occur in time. Following the same approach described for the clustering, we consider a data set X = { e 1, e 2, …, e T } composed of T events and the respective time series of communities C = { c 1, c 2, …, c T } that represent the node community where each event occurs. The time intervals in the time series C where the events persist in a specific community correspond to a particular region with a considerable high number of events. When the community changes, it represents a time point where the events start to appear in another region, indicating spatial changes. Therefore, a time index t ∈ {2, 3…, T } in the data set X is considered a change point if c t ≠ c t −1.

Experimental settings

To analyze and test our method, we use three kinds of data: an artificial data set generator, dynamical systems, and a real-world data application. The artificial data sets were constructed with specific characteristics to show the potential of the method and the real data set is a case study where we show different known phenomena via the proposed model. The three approaches are described in the following.

We propose a simple spatiotemporal data set generator composed by an square grid G r x × y with x columns and y rows. The probability matrix P x × y, which has the same size of the grid Gr, defines the likelihood P i j of a cell Gr i j generates an event in a time t ∈ [1, 2, ⋯, T], whereis an arbitrary time length. Different data sets can be generated by modifying the three parameters: grid size Gr x × y, the probability matrix P, and the time length T. Figure 9 exemplifies the data generation process.

---

### Comparison of parameter optimization methods for quantitative susceptibility mapping [^26a0417b]. Magnetic Resonance in Medicine (2021). Medium credibility.

Purpose

Quantitative Susceptibility Mapping (QSM) is usually performed by minimizing a functional with data fidelity and regularization terms. A weighting parameter controls the balance between these terms. There is a need for techniques to find the proper balance that avoids artifact propagation and loss of details. Finding the point of maximum curvature in the L-curve is a popular choice, although it is slow, often unreliable when using variational penalties, and has a tendency to yield overregularized results.

Methods

We propose 2 alternative approaches to control the balance between the data fidelity and regularization terms: 1) searching for an inflection point in the log-log domain of the L-curve, and 2) comparing frequency components of QSM reconstructions. We compare these methods against the conventional L-curve and U-curve approaches.

Results

Our methods achieve predicted parameters that are better correlated with RMS error, high-frequency error norm, and structural similarity metric-based parameter optimizations than those obtained with traditional methods. The inflection point yields less overregularization and lower errors than traditional alternatives. The frequency analysis yields more visually appealing results, although with larger RMS error.

Conclusion

Our methods provide a robust parameter optimization framework for variational penalties in QSM reconstruction. The L-curve-based zero-curvature search produced almost optimal results for typical QSM acquisition settings. The frequency analysis method may use a 1.5 to 2.0 correction factor to apply it as a stand-alone method for a wider range of signal-to-noise-ratio settings. This approach may also benefit from fast search algorithms such as the binary search to speed up the process.

---

### Round complexity in the local transformations of quantum and classical States [^08645410]. Nature Communications (2017). Medium credibility.

Transformations with high round complexity

Our results are based on a family of tripartite distributions, which we call the origami distributions. The family is given by the set, with b (i, λ) being a tripartite probability distribution taking on valuesfor each fixed pair of values (i, λ); i.e. event (x, y, z) occurs with probabilityand. The structure of these distributions is described recursively with b (1, λ) having the form:

The 8 events of (x, y, z) having nonzero probability in b (1, λ) are those in which z lies in row y and column x, as shown in this grid. The probabilities of these events arefor even values of x, for odd values of x,… In other words, b (1, λ) consists of four blocks of uniform probability, each corresponding to a different value of z. Within each block, X and Y are perfectly correlated, but with a nonuniform distribution (λ, 1 − λ). For example, the event (X = 1, Y = 2, Z = 3) occurs with probability 1/8 according to distribution b (1, λ), while the event (X = 1, Y = 2, Z = 2) occurs with probability zero.

---

### Quantifying the stabilizing effects of protein-ligand interactions in the gas phase [^94fc91d6]. Nature Communications (2015). Medium credibility.

Applying this model to the unfolding data is a three-stage process: (1) assigning the initial positions of each species in the unfolding plot, (2) generating initial model parameters, to be used for (3) fitting of the model to the experimental data. First, species (the folded state, an intermediate and/or the most unfolded form) are assigned in the unfolding plot: each species is given an initial position and width (Fig. 1, step 5). Second, to generate initial Δ G 0 - and m -values, the intensity of each species within these boundaries is summed at each accelerating voltage to give an estimate of the fractional amount of each species, to which the model is initially fitted (Fig. 1, step 6). Finally, we construct a theoretical unfolding plot using the unfolding model, seeded with the Δ G 0 - and m -values from the simpler fit (Fig. 1, step 7), which is fitted to the experimental data (Fig. 1, step 8). The software lets the user assign the initial position and widths of the species, and then automatically extracts and fits the data for the rest of the process. The data should be simulated with increasing numbers of intermediates. The addition of an extra intermediate increases the number of parameters in the model. An F-test should then be performed after each addition, to determine whether the inclusion of each additional intermediate is statistically justified. This results in the equilibrium unfolding model accurately reproducing the experimental unfolding plot, from which gas-phase ligand-induced protein stabilization can be calculated.

---

### A geometrical approach to control and controllability of nonlinear dynamical networks [^ce2a3053]. Nature Communications (2016). Medium credibility.

Instead of calculating the stationary probability distribution density, an alternative approach to constructing the potential landscape is the Freidlin and Wentzel's large deviation theory. According to this theory, first, one maps the stochastic dissipative system of interest to a Hamiltonian system. One then estimates the transition rate from an attractor to a saddle point as r ∼exp(− S 0 / σ), where σ is the noise amplitude and S 0 (·) characterizes the action functional associated with the optimal energy path, which can be numerically solved through an unconstrained nonlinear optimization method. As this theory relies on solving a nonlinear optimization problem, in order to visualize the energy landscape, it is necessary to use a large number of state values x and solve the optimization problem for each, which is computationally exhaustive. We find that, to capture the essential features of the energy landscape, it suffices to use the stationary distribution method.

In the weak noise limit σ →0, according to the theory, to the leading order, the transition rate r from one fixed point to a nearby saddle point x sp can be approximated as r ∼exp[(− S 0 /(x sp)]. The minimum control time satisfies, so we have. From equation (2), we obtain

indicating that the exceeded value Δ e of the control parameter μ is exponentially related to the optimal energy S 0 associated with the path between the initial state and the nearby saddle point in the potential landscape.

---

### Certified algorithms for equilibrium States of local quantum hamiltonians [^841257ea]. Nature Communications (2024). High credibility.

Predicting observables in equilibrium states is a central yet notoriously hard question in quantum many-body systems. In the physically relevant thermodynamic limit, certain mathematical formulations of this task have even been shown to result in undecidable problems. Using a finite-size scaling of algorithms devised for finite systems often fails due to the lack of certified convergence bounds for this limit. In this work, we design certified algorithms for computing expectation values of observables in the equilibrium states of local quantum Hamiltonians, both at zero and positive temperature. Importantly, our algorithms output rigorous lower and upper bounds on these values. This allows us to show that expectation values of local observables can be approximated in finite time, contrasting related undecidability results. When the Hamiltonian is commuting on a 2-dimensional lattice, we prove fast convergence of the hierarchy at high temperature and as a result for a desired precision ε, local observables can be approximated by a convex optimization program of quasi-polynomial size in 1/ε.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^bb9b7f12]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Δ Φ, the probability distribution at a point Φ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Φ ∈ {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters α and β, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since α and β are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Φ ∈ [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(α, β) and x 2 = 1 − x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Φ, it may seem natural to select α and β by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### Assessment and correction of macroscopic field variations in 2D spoiled gradient-echo sequences [^37a7dd6b]. Magnetic Resonance in Medicine (2020). Medium credibility.

To investigate the effect of the described parameters on signal dephasing in the presence of G z, 4 different models have been studied. Summarizing Equation 1 in a tissue‐specific signal component S tissue (t) (e.g.) and a component F i (t) describing the signal dephasing due to Δ ω (z), the modelcan be written as. The 4 models are defined as follows:

The model S 1 (t) serves as an uncorrected reference without modeling M xy (z) and Δ ω (z). Then, for S 2 (t), only the magnitude along the slice | M xy (z)| was considered neglecting φ xy (z). In S 3 (t), φ xy (z) was included, and in S 4 (t) the model was extended by additionally incorporatingand λ variations.

2.2 Numerical implementation

Signal dephasing due to G z was estimated numerically for F 2 to F 4 assuming E 1 = 0. In the first step, M xy was estimated for a certain RF excitation pulse and G slice with a freely available numerical Bloch solver using MATLAB (MathWorks, Natick, MA). 32 Simulations were carried out with temporal resolution of 2 µs and spatial resolution of 80 µm with 2501 spatial points. The normalized envelope B 1 (t) was scaled to achieve α eff (z = 0) = α ξ in the center of the slice. Rather than estimating M xy for each voxel with ξ and λ, calculations were accelerated by estimating M xy in steps of Δξ = 0.05 followed by linear interpolation to Δξ intp = 0.005. Variations of λ were incorporated by multiplying the sampling points along z with λ, to scale the thickness of the slice. In the last step, the integral along z for given G z was solved by numerical integration. The source code can be found at:

---

### A euclidean transformer for fast and stable machine learned force fields [^1f17873a]. Nature Communications (2024). High credibility.

Network and training

Allmodels use a feature dimension of F = 132, h = 4 heads in the invariant MP update and r cut = 5 Å. The number of MP updates and the degrees in the EV vary between experiments. For the comparison of invariant and equivariant model we use degrees l = {0} and l = {0, 1, 2, 3}, T = 3 and EV initialization following Eq. (11). The invariant degree is explicitly included, in order to exclude the possibility that stability issues might come from the inclusion of degree l = 0. The number of network parameters of the invariant model is 386k and of the equivariant model is 311k, such that the better stability is not be related to a larger parameter capacity but truly to the degree of geometric information. Due to the use of as many heads as degrees in the MP update for the EV, increasing the number of degrees results in a slightly smaller parameter number for the equivariant model. Per molecule 10,500 conformations are drawn of which 500 are used for validation. For the invariant and equivariant model, two models are trained on training data sets which are drawn with different random seeds. The model for Li 4 PO 3 uses T = 2, l = {1, 2, 3} and initializes the EV to all zeros. For training, 11k samples are drawn randomly from the full data set of which 1k are used for validation, following.

---

### Observation of a singular weyl point surrounded by charged nodal walls in PtGa [^1ac2fbb0]. Nature Communications (2021). High credibility.

Constrained by the Nielsen-Ninomiya no-go theorem, in all so-far experimentally determined Weyl semimetals (WSMs) the Weyl points (WPs) always appear in pairs in the momentum space with no exception. As a consequence, Fermi arcs occur on surfaces which connect the projections of the WPs with opposite chiral charges. However, this situation can be circumvented in the case of unpaired WP, without relevant surface Fermi arc connecting its surface projection, appearing singularly, while its Berry curvature field is absorbed by nontrivial charged nodal walls. Here, combining angle-resolved photoemission spectroscopy with density functional theory calculations, we show experimentally that a singular Weyl point emerges in PtGa at the center of the Brillouin zone (BZ), which is surrounded by closed Weyl nodal walls located at the BZ boundaries and there is no Fermi arc connecting its surface projection. Our results reveal that nontrivial band crossings of different dimensionalities can emerge concomitantly in condensed matter, while their coexistence ensures the net topological charge of different dimensional topological objects to be zero. Our observation extends the applicable range of the original Nielsen-Ninomiya no-go theorem which was derived from zero dimensional paired WPs with opposite chirality.

---

### A geometric approach to characterize the functional identity of single cells [^bc528430]. Nature Communications (2018). Medium credibility.

This operation projects z i to the orthogonal complement of the space spanned by the universally expressed genes. We then concatenate the column vectorsto create a adjusted cell signature matrix.

Next, to enhance the signal contributed by preferentially expressed genes, we propose an information theoretic approach that is inspired by the work of Schug et al. The main idea is to use Shannon's entropy to measure the informativeness of genes. If a gene is uniformly utilized across cells, it contains less information as opposed to the case in which it is selectively active in a few cells. To this end, we start with the positive projection of adjusted cell signatures, in which case we replace all negative values with zeros. Then, we normalize this matrix to construct a stochastic matrix P (every row sums to one). Let p i be the row vector associated with the i th gene. We compute the uniformity, or normalized entropy, of p i as: u (i) = −∑ j p ij log(p ij)/log(n), where p ij is an entry in the matrix P and n is the number of genes. This value is always between zero and one and is used as a basis to boost contributions from the most informative genes. A detailed comparison of our entropy-based method with dispersion and Gini index is provided in the Supplementary Note 2.

To scale genes according to their specificity, we compute a coefficient that controls the contribution of each gene. This coefficient is greater than one (scales up) for cell-type-specific genes and less than one (scales down) for universally expressed genes, respectively. To do so, we note that the distribution of the entropy values follows a bimodal distribution, with separate peaks for the cell-type-specific and universally expressed genes. To identify the critical point where these two population separate from each other, we fit a mixture of two Gaussians over the distribution of the values and use it to identify this transition point, denoted by, which is the point of equal probability from each Gaussian. Then for each gene i, we define a scaling factor as. Finally, we compute the kernel matrix as follows:

In this formulation, if we denote, then K is a dot-product kernel defined as Y T Y. We will refer to Y as the adjusted transcriptional profile of cells, and K as the cell similarity kernel, or ACTION metric.

---

### New approach to gridding using regularization and estimation theory [^51891fed]. Magnetic Resonance in Medicine (2002). Low credibility.

When sampling under time-varying gradients, data is acquired over a non-equally spaced grid in k-space. The most computationally efficient method of reconstruction is first to interpolate the data onto a Cartesian grid, enabling the subsequent use of the inverse fast Fourier transform (IFFT). The most commonly used interpolation technique is called gridding, and is comprised of four steps: precompensation, convolution with a Kaiser-Bessel window, IFFT, and postcompensation. Recently, the author introduced a new gridding method called Block Uniform ReSampling (BURS), which is both optimal and efficient. The interpolation coefficients are computed by solving a set of linear equations using singular value decomposition (SVD). BURS requires neither the pre- nor the postcompensation steps, and resamples onto an n x n grid rather than the 2n x 2n matrix required by conventional gridding. This significantly decreases the computational complexity. Several authors have reported that although the BURS algorithm is very accurate, it is also sensitive to noise. As a consequence, even in the presence of a low level of measurement noise, the resulting image is often highly contaminated with noise. In this work, the origin of the noise sensitivity is traced back to the potentially ill-posed matrix inversion performed by BURS. Two approaches to the solution are presented. The first uses regularization theory to stabilize the inversion process. The second formulates the interpolation as an estimation problem, and employs estimation theory for the solution. The new algorithm, called rBURS, contains a regularization parameter, which is used to trade off the accuracy of the result against the signal-to-noise ratio (SNR). The results of the new method are compared with those obtained using conventional gridding via simulations. For the SNR performance of conventional gridding, it is shown that the rBURS algorithm exhibits equal or better accuracy. This is achieved at a decreased computational cost compared to conventional gridding.

---

### Testing electron-phonon coupling for the superconductivity in kagome metal CsVSb [^df2220a5]. Nature Communications (2023). High credibility.

Fig. 1
EPC-induced electronic kink in CsV 3 Sb 5.

a Schematic temperature versus pressure/doping phase diagram. The inset shows the crystal structure of CsV 3 Sb 5. A top view of the V-Sb layer is zoomed in on the right panel. b Schematic of van Hove singularities at the M point of the Brillouin zone boundary which are connected by three nesting wavevectors. The von Hove singularities has mainly V 3 d orbital characters (and). The circular electronic pocket at Γ point has mainly Sb 5 p z orbital character. c FS mapping with intensity integrated within E F ± 5 meV. The FS sheet in k y < 0 is symmetrized from the one in k y > 0 and a superposed FS sheet is collected from another independent sample. Dashed lines are DFT determined FSs. d ARPES intensity plot, corresponding to the black cut in c, showing a kink in the band dispersion. The band dispersion extracted from the MDCs is overlaid as a red curve. The dashed black line represents the bare band. The arrow indicates the position of the kink. The MDC at E F and its Lorentzian fit are shown as yellow and black lines, respectively. e Real-part self-energy ReΣ(ω = E - E F) and imaginary-part self-energy ImΣ(ω = E - E F). A background of ImΣ other is subtracted for ImΣ(ω) (see supplementary note 2). The dashed black line marks the energy position of the kink.

---

### Measurements of the size and correlations between ions using an electrolytic point contact [^ebb36881]. Nature Communications (2019). High credibility.

For each current trace, the data acquired within 15 s after a change in voltage was expunged to guarantee a steady-state reading of current with no capacitive influence. Two properties were then extracted from these traces; the mean open pore current I 0 and the amplitude, S 1/ f (1 Hz). To determine, the PSD was plotted as a function ofand a weighted fit of the 1/ f noise component of the trace was performed using a force fit slope of β = −1. The fit was preferentially weighted to low frequencies such that a hard cutoff on the higher frequency bound was unnecessary. Specifically, every two decades, the weight dropped an order of magnitude, so a PSD value at 100 Hz was 10 times less significant to the fit than the PSD recorded at 1 Hz and so on. Separately, the mean logarithmic PSD was determined in the intermediate range 1–5 KHz, where pink noise was not evident for the range of bias voltages used here. The parameter S 0 was defined as the mean PSD in this range. The intercept of these two lines was found and iterated to minimize the residuals to the piece-wise fit using custom MATLAB code and produce optimal S 1/ f (1 Hz) values.

---

### Efficient hybrid numerical modeling of the seismic wavefield in the presence of solid-fluid boundaries [^176eaf02]. Nature Communications (2025). High credibility.

On the one hand, if there are no anomalous bodies within the local solid/fluid inversion domain, i.e. when model M i 0 is used as the local model, the hybrid input mirror forces should fully recover the local solid and fluid wavefields, resulting in a zero-value wavefield outside of the local domain. The signal results obtained from convolution in the above step 5 will be a nearly zero-value time series, with some numerical error. On the other hand, if an updated local model M i 1 is used as the local model, the presence of local anomalies leads to scattered solid and fluid wavefields propagating outside of the inversion domain. In step 3, these scattered wavefields will contribute to generating the hybrid output mirror forces, acting as third sources, to be convolved with Green's functions in step 5. Then we sum the convolved waveform with the reference waveform acquired in step 2 to generate the comprehensive waveform transmitted from a distant source to a remote station. The absorbing layer Ω a is significantly important to maintain the accuracy and stability of the hybrid simulation. The implementation of the absorbing boundary layer will be explained in the SI.

Note that the positions of E 1 / A 1 and E 2 / A 2 can be either identical or distinct, with the condition that they remain between the local inversion regions and the absorbing layer. In this study, we choose to place E 2 / A 2 inside E 1 / A 1, slightly reducing the impact of incomplete absorption boundaries. The two layers (E 1 / A 1 and E 2 / A 2) are located outside of the inversion domain, and the absorbing layers are outside of the two layers of E / A mirrors. In the case when the spectral element method is used, as shown in Figure S2 and Fig. 3 both E 1 / A 1 and E 2 / A 2 are composed of a single layer of spectral elements with their internal Gauss-Lobatto-Legendre (GLL) points.

Note that a Cartesian reference frame is used in SPECFEM3D_GLOBE, where the x-axis points East, the y-axis points North, and the z-axis points Up. After the convolution in step 5, the three component waveforms will be expressed in the same coordinate system (x, y, z) at each receiver.

---

### A euclidean transformer for fast and stable machine learned force fields [^01fc2876]. Nature Communications (2024). High credibility.

Recent years have seen vast progress in the development of machine learned force fields (MLFFs) based on ab-initio reference calculations. Despite achieving low test errors, the reliability of MLFFs in molecular dynamics (MD) simulations is facing growing scrutiny due to concerns about instability over extended simulation timescales. Our findings suggest a potential connection between robustness to cumulative inaccuracies and the use of equivariant representations in MLFFs, but the computational cost associated with these representations can limit this advantage in practice. To address this, we propose a transformer architecture called SO3KRATES that combines sparse equivariant representations (Euclidean variables) with a self-attention mechanism that separates invariant and equivariant information, eliminating the need for expensive tensor products. SO3KRATES achieves a unique combination of accuracy, stability, and speed that enables insightful analysis of quantum properties of matter on extended time and system size scales. To showcase this capability, we generate stable MD trajectories for flexible peptides and supra-molecular structures with hundreds of atoms. Furthermore, we investigate the PES topology for medium-sized chainlike molecules (e.g., small peptides) by exploring thousands of minima. Remarkably, SO3KRATES demonstrates the ability to strike a balance between the conflicting demands of stability and the emergence of new minimum-energy conformations beyond the training data, which is crucial for realistic exploration tasks in the field of biochemistry.

---

### Coherent stokes raman scattering microscopy (CSRS) [^61098642]. Nature Communications (2023). High credibility.

As a second important result from the heuristic derivation of CSRS object support, we found that the presence of high spatial frequencies along K z increases the amount of backward radiation. To confirm this prediction, we investigated in Fig. 4 d, e an object whose nonlinear scatterer density, i.e. the concentration of molecular groups, is modulated along the optical axis as 1 +(K z z) with K z = 2 π / λ o being the object frequency. We now consider a conventional illumination scheme where both Stokes and the pump are tightly focused and cover the full back aperture of the objective lens. Figure 4 d outlines the radiation behavior of such a z-structured object with K z = 2 π /1 μm. It is found that R b / f increases to one-fourth for Epi-CSRS while Epi-CARS remains negligible weak. To identify those object frequencies which are most efficiently probed by Epi-CSRS, we computed R b / f as a function of K z. From Fig. 4 e, we find that Epi-CSRS peaks at K z = 2 π /1 μm whereas Epi-CARS R b / f still increases at K z = 2 π /0.5 μm confirming that CARS requires larger K z, i.e. objects with higher frequency modulation of the scatterer density, to generate a strong Epi radiation.

---

### Bifurcation behaviors shape how continuous physical dynamics solves discrete ising optimization [^5aeb0a89]. Nature Communications (2023). High credibility.

Fig. 3
Trapped nodes and swing nodes that emerge during Ising dynamics and their impact on optimization.

a, b Schematic illustrations of the evolution of the state corresponding to a global minimum point x * (p) following Eq. (5) in Ising problems for which (a) p ✱ = p 0 (exact mapping arises right after first bifurcation), and (b) p * > p 0 (outlier). Outliers are accompanied by swing nodes, which bifurcate at higher pump rates (p 1. p *) during the evolution (retarded bifurcations). In the corresponding graphs blue(red) edges denote G i j = −1(+1), respectively. The nodes are colored according to the sign and magnitude of the steady-state components of the CIM dynamics. c, d Normalized counts of the magnitudes of the components of the maximum eigenvectors for models with (c) p ✱ = p 0 and (d) p * > p 0, computed from statistics of 1000 random instances with n = 5 variables. The brown curves are the kernel density estimations of the probabilities. e The plot shows the probability that a variable with magnitude proportional toat the first bifurcation changes its sign during subsequent evolutions of the CIM, sampled from 100 fully connected graphs with n = 100 variables, and edge weights picked at random from a Gaussian distribution with zero mean and standard deviation. The inset (similar to Fig. 2) visualizes the evolution of several nodes in a typical 100-node graph with trapping and oscillations explicitly shown.

The empirical observations made in Fig. 3 a, b guide us toward rigorous analytical results which we present in the following. Given the first non-trivial stable state, we quantify the degree of synchronization bywhere we denote σ 1 = sign(x 1). Note thatand if the components of x 1 are more centralized away from zero, the larger the value of α 2 (x 1). Then for any Ising Hamiltonian H (σ) (Eq. (1)), we have the following theorem.

---

### The spatial and temporal structure of neural activity across the fly brain [^9839442e]. Nature Communications (2023). High credibility.

Semi-supervised behavioral segmentation

We train a semi-supervised behavioral segmentation model that classifies the DGP markers into one of the five available behavior classes for each time point. The model's loss function contains three terms: (1) a standard supervised loss that classifies a sparse set of hand labels; (2) a weakly supervised loss that classifies a set of easy-to-compute heuristic labels; and (3) a self-supervised loss that predicts the evolution of the DGP markers. Let x t denote the DGP markers at time t, and let y t denote the one-hot vector encoding the hand labels at time t such that the k th entry is 1 if behavior k is present, else the entry is 0. We assume that the hand labels are only defined on a subset of time points. The cross-entropy loss function then defines the supervised objective to optimize:where f denotes the sequence model mapping the DGP markers to behavior labels. We now introduce a set of heuristic labels, defined at each time point. Computing the cross-entropy loss on all time points that do not already have a corresponding hand label defines the heuristic objective:

The self-supervised loss requires the sequence model to predict x t +1 from x t. To properly do so we now expand the definition of the sequence model f to include two components: an encoder e, which maps the behavioral features x t to an intermediate behavioral embedding z t; and a linear classifier c which maps z t to the predicted labels (. We can now incorporate the self-supervised loss through the use of a predictor function p, which maps z t to x t +1, and match x t +1 to the true behavioral features p (e (x t +1)) through a mean square error losscomputed on all time points:Finally, we combine all terms into the full semi-supervised loss function:where the λ terms are hyperparameters that control the contributions of their respective losses. Note that setting λ h = λ p = 0 results in a fully supervised model, while λ s = λ h = 0 results in a fully unsupervised model.

---

### Weighted averaging in spectroscopic studies improves statistical power [^bbf8e0b3]. Magnetic Resonance in Medicine (2017). Low credibility.

Note additionally that the ratio distribution f (w) defined above may be decidedly non‐normal, but that the central limit theorem ensures that the arithmetic mean of such ratio samples will be normally distributed in the limit that the number of samples becomes large, independent of this skew.

Several approaches to computing confidence limits on the ratio distribution have been historically proposed. Such confidence intervals are not guaranteed to exist as μ x and μ y tend to zero, reflecting the arbitrarily large increase in uncertainty arising from division by a number arbitrarily close to zero. Fieller's theorem allows for the computation of confidence limits on the ratio distribution, and can be derived from geometric arguments about the density of probability, and states that the lower and upper confidence limits (L B, U B) on the ratioare given by 16, 17 provided that μ x is not expected to be close to zero, and where. Formally, is the t ‐statistic at the chosen significance level α, i.e. the value of the inverse cumulative t probability distribution function with n – 1 degrees of freedom at. In the context of spectroscopy performed on a large number of points, the estimation of uncertainty is free from sampling effects and it is appropriate to take, such that the t ‐distribution tends to the normal one. If the variances are small and the denominator is "far" from zero, this expression is quantitatively similar to the familiar expression for the uncertainty on a ratio, Eq. (6) 17.

It is proposed that the width of the confidence interval defined above, at ", i.e. withis an appropriate weight to use for subsequent calculations provided that μ x is not expected to be close to zero. This choice of q provides, t q = 1 asby definition, and both upper and lower bounds derived are illustrated in Figure 1. As a consequence, an appropriate weightto consider for subsequent estimation of population properties of the mean can be constructed and simplified at " considerably, to form

Given the weight specified above, the rest of the analysis of such data would then proceed from Eq. (5) onwards.

---

### Learning properties of quantum States without the IID assumption [^d59de50a]. Nature Communications (2024). High credibility.

We develop a framework for learning properties of quantum states beyond the assumption of independent and identically distributed (i.i.d.) input states. We prove that, given any learning problem (under reasonable assumptions), an algorithm designed for i.i.d. input states can be adapted to handle input states of any nature, albeit at the expense of a polynomial increase in training data size (aka sample complexity). Importantly, this polynomial increase in sample complexity can be substantially improved to polylogarithmic if the learning algorithm in question only requires non-adaptive, single-copy measurements. Among other applications, this allows us to generalize the classical shadow framework to the non-i.i.d. setting while only incurring a comparatively small loss in sample efficiency. We leverage permutation invariance and randomized single-copy measurements to derive a new quantum de Finetti theorem that mainly addresses measurement outcome statistics and, in turn, scales much more favorably in Hilbert space dimension.

---

### Data-driven quantum chemical property prediction leveraging 3D conformations with uni-mol [^b43b2607]. Nature Communications (2024). High credibility.

Training strategy

In DFT conformation optimization or Molecular Dynamics simulations, a conformation is optimized step-by-step, resulting in a trajectory from a raw conformation to the equilibrium conformation in Euclidean space. However, saving such a trajectory can be expensive, and publicly available datasets usually provide the equilibrium conformations only. Providing a trajectory would be beneficial as intermediate states can be used as data augmentation to guide the model's training. Inspired by this, we propose a novel training approach, which generates a pseudo trajectory first, samples a conformation from it, and uses the sampled conformation as input to predict the equilibrium conformation. This approach allows us to better exploit the information in the molecular data, which we found can greatly improve the model's performance. Specifically, we assume that the trajectory from a raw conformation r init to a target equilibrium conformation r tgt is a linear process. We generate an intermediate conformation along this trajectory via noisy interpolation, i.e.where scalar q ranges from 0 to 1, the Gaussian noisehas a mean of 0 and standard deviation υ (a hyper-parameter). Taking r (0) as input, Uni-Mol+ learns to update towards the target equilibrium conformation r tgt. During inference, q is set to 1.0 by default. However, during training, simply sampling q from a uniform distribution ([0.0, 1.0]) may cause (1) a distributional shift between training and inference, due to the infrequent sampling of q = 1.0 (RDKit-generated conformation), and (2) an inability to learn an accurate mapping from the equilibrium conformation to the QC properties, as q = 0.0 (target conformation) is also not sampled often. Therefore, we employ a mixture of Bernoulli and Uniform distributions to flexibly assign higher sample probabilities to q = 1.0 and q = 0.0, while also sampling from interpolations. The above process is illustrated in Fig. 1 c in Supplementary.

The model takes r (0) as input and generates r (R) after R iterations. Then, the model uses r (R) as input and predicts the QC properties. L1 loss is applied to the QC property regression and the 3D coordinate prediction. All loss calculations are performed solely on the final conformer at the last iteration.

---

### Validation of monte carlo estimates of three-class ideal observer operating points for normal data [^26b6e5d9]. Academic Radiology (2013). Low credibility.

Rationale and Objectives

Traditional two-class receiver operating characteristic (ROC) analysis is inadequate for the complete evaluation of observer performance in tasks with more than two classes.

Materials and Methods

Here, a Monte Carlo estimation method for operating point coordinates on a three-class ROC surface is developed and compared with analytically calculated coordinates in two special cases: (1) univariate and (2) restricted bivariate trinormal underlying data.

Results

In both cases, the statistical estimates were found to be good in the sense that the analytical values lay within the 95% confidence interval of the estimated values about 95% of the time.

Conclusions

The statistical estimation method should be key in the development of a pragmatic performance metric for evaluation of observers in classification tasks with three or more classes.

---

### Adaptive resetting for informed search strategies and the design of non-equilibrium steady-states [^cfcebb66]. Nature Communications (2025). High credibility.

Fig. 3
Search with environmental information.

a The mean first-passage time to the origin as a function of r 0 and b for one-dimensional diffusion with diffusion coefficient D = 1. Motion starts at L = 1 and is conducted under position-dependent resetting of the form given in Equation (8). The dashed and dotted lines indicate the optimal rate for diffusion with a constant resetting rate, r 0 = 2.5, and the optimal rate for a parabolic resetting, r 0 = 5.6 b 2, respectively. b First-passage time distributionsfor the three marked points in the phase space of (a), estimated from simulations without resetting (solid lines), and brute-force simulations with resetting (markers). For simulations with resetting, we plot the meanstandard deviation of ten independent repetitions with 10 4 trajectories each (errors are smaller than the marker size in almost all cases).

We emphasize that all the results of Fig. 3 a were obtained using a single set of trajectories with no resetting and simply re-evaluating Equations (4–7) for every value of r 0 and b, leading to a different MFPT through Equation (3). Previously, obtaining the above results would have required performing an ensemble of brute-force simulations at every value of the parameters r 0 and b, to map out the MFPT phase space. This approach would be computationally prohibitive in many cases. Moreover, the strength of our approach becomes even clearer when considering other forms of r (x) apart from Equation (8). Tackling these using brute-force simulations, analytical methods, or experiments, would require a complete reanalysis of the problem. On the other hand, using our method, we can use the same initial ensemble of trajectories to generate the MFPT phase space for any resetting protocol, with minimal added cost. In section 4 of the Supplementary Information, we demonstrate this for several other sigmoidal-shaped resetting protocols (Supplementary Fig. S2).

---

### AutoSpill is a principled framework that simplifies the analysis of multichromatic flow cytometry data [^fae67196]. Nature Communications (2021). High credibility.

To supply data for the regression, we partition the events of the single-color control for parameter P by quantile. For controls with a large number of events, we use 256 quantiles, but we allow as few as 8 to ensure enough events in each quantile to estimate standard deviation reliably. For each other parameter C, we calculate in each quantile the robust standard deviation of fluorescence (the 84th percentile minus the median) as the estimate of σ and the median fluorescence as the estimate of F. The F values may be negative and/or close to zero, so they are passed through a square-root-like transform defined byprior to regression, instead of the simple square root function. The resulting regression provides an estimate of σ 0.

Using the estimate of σ 0, AutoSpread calculates for each quantile the estimate of, defined by, and these adjusted standard deviation estimates provide the data for the second regression. This regression is calculated without an intercept term because the adjustment of σ 0 forces it to zero.

Reporting summary

Further information on research design is available in the Nature Research Reporting Summary linked to this article.

---

### An alternative to the breeder's and lande's equations [^96718ec4]. G3 (2014). Low credibility.

The distribution of the phenotype z in the parental population selected according to the fitness function W (z) iswhereis the mean fitness of the parental generationThe genotype distribution of the selected population is whereis the genotype fitness function, i.e. the convolution of the phenotype fitness function by the environment factors.is the mean genotype fitness:Note thatas both these quantities are defined by the same double integration over the domains of y and z.

For a large, randomly mating population, reproduction gives for the distribution of breeding values in the next generationThe exact form of the probability density L(y) that captures the inheritance process (recombination, segregation,…) is not important here;, for example, use a normal distribution for L(y) in the framework of the infinitesimal model. For our purpose, it is enough to suppose that the mean of the distribution L(y) is zero, i.e.which is valid in the absence of dominance and epistasis effects (see also Appendix/Segregation density function).

The phenotype distribution of the progeny isWe now make the further assumption that (1) the environment and genotype are independent random variables, so thatand therefore the variances are additive:and (2) environment effects are of zero meanand symmetric (f (− x) = f (x)). An environmental noise with such a distribution law does not change the mean of the random variable: E (Z) = E (Y + ξ) = E (Y). Therefore, the mean phenotype of the offspring iswhich is equation (3). Note that the first lines of the above equations merely state that the expectations of the breeding's value of parent and offspring are equal for purely additive traits.

On the other hand, the mean phenotype of the selected parents iswhich is equation (4).

---

### Event generation and statistical sampling for physics with deep generative models and a density information buffer [^47552c65]. Nature Communications (2021). High credibility.

Simulating nature and in particular processes in particle physics require expensive computations and sometimes would take much longer than scientists can afford. Here, we explore ways to a solution for this problem by investigating recent advances in generative modeling and present a study for the generation of events from a physical process with deep generative models. The simulation of physical processes requires not only the production of physical events, but to also ensure that these events occur with the correct frequencies. We investigate the feasibility of learning the event generation and the frequency of occurrence with several generative machine learning models to produce events like Monte Carlo generators. We study three processes: a simple two-body decay, the processes e + e - →Z→l + l - and [Formula: see text] including the decay of the top quarks and a simulation of the detector response. By buffering density information of encoded Monte Carlo events given the encoder of a Variational Autoencoder we are able to construct a prior for the sampling of new events from the decoder that yields distributions that are in very good agreement with real Monte Carlo events and are generated several orders of magnitude faster. Applications of this work include generic density estimation and sampling, targeted event generation via a principal component analysis of encoded ground truth data, anomaly detection and more efficient importance sampling, e.g., for the phase space integration of matrix elements in quantum field theories.

---

### A unifying framework for mean-field theories of asymmetric kinetic ising systems [^a754fb0a]. Nature Communications (2021). High credibility.

Kinetic Ising models are powerful tools for studying the non-equilibrium dynamics of complex systems. As their behavior is not tractable for large networks, many mean-field methods have been proposed for their analysis, each based on unique assumptions about the system's temporal evolution. This disparity of approaches makes it challenging to systematically advance mean-field methods beyond previous contributions. Here, we propose a unifying framework for mean-field theories of asymmetric kinetic Ising systems from an information geometry perspective. The framework is built on Plefka expansions of a system around a simplified model obtained by an orthogonal projection to a sub-manifold of tractable probability distributions. This view not only unifies previous methods but also allows us to develop novel methods that, in contrast with traditional approaches, preserve the system's correlations. We show that these new methods can outperform previous ones in predicting and assessing network properties near maximally fluctuating regimes.

---

### Active carpets drive non-equilibrium diffusion and enhanced molecular fluxes [^3e9f63ee]. Nature Communications (2021). High credibility.

We first focus on carpets made of Stokeslets oriented parallel to the surface, with uniformly distributed orientations, so p z = 0 and F = n /2 π. Figure 1 i shows the resulting histogram of the total vertical flow, v z, for different values of z 0. This distribution is an even function, by symmetry of the parallel Stokeslet, so it is immediately clear that the mean flow vanishes. However, the width is finite and decays with distance from the surface, so the active fluctuations are stronger closer to the carpet. This variance can be calculated analytically, for the directions i, j ∈ x, y, z, as detailed in 'Methods: Characterising fluctuations: theory details'. For the vertical component, for instance, this integral yields the variance

Physically, this expression represents how deeply the active boundary can influence the passive bulk fluid. This theoretical result is listed in Table 1 (row 4, column 3), and it is compared with simulations in Fig. 1 m (magenta line). The decay with distance isfor all diagonal components, but the off-diagonal components are zero. Interestingly, the variance in the vertical direction (purple stars) is twice as strong as in the horizontal directions (green points, blue triangles), sofor parallel Stokeslets.

Table 1
Properties of fluctuations driven by an active carpet.

The columns correspond to different types of hydrodynamic actuators: we consider point forces (Stokeslets) oriented parallel to the surface, perpendicular Stokeslets, parallel dipoles, and rotlets. The rows correspond to different moments of the total flow velocity: we consider the mean, the variance, the skewness, and the kurtosis of these flows, presented asusing the exponent γ = 1, 2, 3, 4 respectively, as a function of distance from the carpet, z 0. Note that the off-diagonal components are equal to zero in all cases.

---

### A geometrical solution underlies general neural principle for serial ordering [^b3d6db1f]. Nature Communications (2024). High credibility.

The second variation involves the number of learning trials in which a specific item of the sequence appears (Supplementary Fig. 1). This presentation schedule eventually leads to the number of learning trials decreasing with the serial position k of the item as N L (k) = N L (1) − β N (k − 1). The network size is the same, and the sensory noise during the testing phase is σ S = 0.3, with β N ranging from 2 to 5. Statistics were collected from 100 independent simulations.

The third variation focuses on a probabilistic reward schedule in the transitive inference task Fig. 4 d–f. For each learning trial (couples with symbolic distance ∣SD∣ = 1), the reward for a correct answer is given based on a probability that is higher for higher ranks. Specifically, given a couple of symbols { S k, S k +1 }, the probability of receiving a reward is calculated as:where CDF is the cumulative distribution function of the standard normal distribution, and σ is the scaling factor of the reward schedule. This approach is equivalent to considering the response target for a trial { S k, S k +1 } as the sign of a random value η extracted from a Gaussian distribution. For this pair of items the rewarded scenario corresponds to η > 0 (right motor choice). The probability that a Gaussian random variableis positive is given by CDF(μ / σ), leading to Eq. (27). Note that 1 − CDF(− μ / σ) = CDF(μ / σ) so that the same reward probability holds for both pairs { S k +1, S k } and { S k, S k +1 }. Even in this case, the network size is N = 100, and the number of learning and testing trials per pair was 10000. σ ranged from 0 to 0.35.

Dimensionality analysis

Figure 8 depicts the outcomes of the principal component analysis (PCA) conducted on the network activity simulated during the testing phase of the task. To perform PCA, the sampling matrix T × N is initially centered at zero by subtracting its time averages. In this context, T represents the number of time steps of the test phase, and N stands for the number of nodes in the network. Subsequently, the associated covariance matrix and its eigenvectors, which embody the principal components, are computed. Notably, the network state projected onto the eigenvector with the maximum eigenvalue is the projection onto the first principal component.

---

### Determining accurate conformational ensembles of intrinsically disordered proteins at atomic resolution [^50bedf5f]. Nature Communications (2025). High credibility.

Normalized RMSE (RMSE N) comparisons

To compare the relative accuracy of ensembles before and after reweighting, we introduce an expression for the normalized RMSE (RMSE N), where we normalize the value of the RMSE calculated for each experimental data type by the corresponding value in the unbiased a99SB- disp MD ensemble (RMSE N = RMSE/RMSE a99SB− d i s p).

To compare reweighted ensembles obtained from different force fields, we define a global quality index for each system: the averaged normalized RMSE. For each force field and data type i, we compute the agreement between calculated and experimental data in the reweighted ensemble (RMSE i). We then normalize each value of RMSE i by the corresponding RMSE value in the unbiased a99SB- disp MD ensemble (RMSE N, i = RMSE i /RMSE i, a99SB− d i s p) and finally average across all M data types to obtain.

IDP ensemble comparisons

We provide a quantitative measure of the structural similarity of conformational ensembles by collectively analyzing a set of molecular dynamics simulations of the same protein by using the energy landscape visualization method (ELViM), for dimensionality reduction and computing the overlap of latent space densities.

For each protein, we first down-sampled each trajectory by a factor of two, so that each ensemble contained 14988 frames. We then concatenated all three unbiased trajectories into a single merged ensemble, which contained a total of 44964 frames. We used ELViM to compute a dissimilarity matrix for this merged ensemble with the hyperparameters σ 0 and ϵ set to 1 and 0.15, respectively, and projected the conformations of the merged ensemble onto a 2D latent space. Individual kernel densities were calculated for each simulation dataset (each unbiased and reweighted ensemble of each protein) and sampled over the global extrema in each dimension of the ELViM latent space using an 80 by 80 grid. Gaussian kernel densities were estimated using Scott's ruleto determine kernel bandwidths for both reweighted and unbiased kernel density estimates. For a pair of kernel densities, D 1 and D 2, we define the overlap integral, S :where N 1 and N 2 are normalization constants for the distributions D 1 and D 2, respectively, chosen such that the integrand is defined in the range: [0, 1]. For a kernel density D i, the normalization constant is defined as:

---

### A persistent prefrontal reference frame across time and task rules [^14b0ad05]. Nature Communications (2024). High credibility.

Geometry of the population response

We used a geometrical approach to analyze population vector activity. Population activity as a function of space was extracted from z-scored calcium activity of all n repeatedly active neurons of a recording session during three spatial bins: At the start of the outward trajectory, the midpoint, and the endpoint, separately for left and right trajectories (bins correspond to the same 20 bins as used for the construction of spatial tuning functions). We constructed the geometrical object with corner points given in n dimensions by each population vector, and extracted the edge angles between vectors connecting the corner points (10 edge angles for each of the 6 corner points). The dissimilarity between the resulting edge angle matrix of each day with day 1 was expressed as the edge angle similarity matrix || A a, b | ∅ |F using the Frobenius normwhere A a and A b denote edge angle matrices on day a and b, M denotes the number of corner points (= 6), and N denotes the number of angles of incoming pairs of edges from other corner points (= 10). || A a, b | ∅ |F was computed separately for odd and even runs and averaged for each session. This allowed normalization using the same odd-even matrices of each day to estimate within-day dissimilarity. Normalization was applied to || A a, b | ∅ |F to obtain the normalized edge angle dissimilarity ||Â a, b | ∅ |F aswhere ||A W || F is the within-day Frobenius norm computed between odd and even runs averaged over all days, and ||A S || F is the Frobenius norm between matrices of the last and first day obtained after column- and row-shuffling the tuning functions. The resulting metric is thus bound between 0 (equivalent to the dissimilarity over time being equal to the within-day dissimilarity) and 1 (with the dissimilarity being equal to the dissimilarity of shuffled data). Population vectors were obtained from the same three spatial points as the average of the significant transients of each neuron, and correlated with day 1 using Pearson's r.

---

### Three-dimensional ultrasound matrix imaging [^5b708431]. Nature Communications (2023). High credibility.

Fig. 2
Ultrasound Matrix Imaging of a tissue-mimicking phantom through a pork tissue.

a Schematic of the experiment. b Maps of original RPSFs (in amplitude) at depth z = 29 mm. c Aberration phase laws extracted at the different steps of the UMI process. d Corresponding RPSFs after aberration compensation at each step. e, f 3D confocal and UMI images with one longitudinal and transverse cross-section.

Whatever the illumination sequence, the reflectivity of a medium at a given point r can be estimated in post-processing by a coherent compound of incident waves delayed to virtually focus on this point, and coherently summing the echoes recorded by the probe coming from that same point [Fig. 1 d]. UMI basically consists in decoupling the input (r in) and output (r out) focusing points [Fig. 1 e]. By applying appropriate time delays to the transmission (u in / θ in) and reception (u out) channels (see Methods), R uu (t) and R θ u (t) can be projected at each depth z in a focused basis, thereby forming a broadband focused reflection matrix, R ρρ (z) ≡ [R (ρ in, ρ out, z)].

Since the focal plane is bi-dimensional, each matrix R ρρ (z) has a four-dimension structure: R (ρ in, ρ out, z) = R ({ x in, y in }, { x out, y out }, z). R ρρ (z) is thus concatenated in 2D as a set of block matrices to be represented graphically [Fig. 1 g]. In such a representation, every sub-matrix of R corresponds to the reflection matrix between lines of virtual transducers located at y in and y out, whereas every element in the given sub-matrix corresponds to a specific couple (x in, x out) [Fig. 1 f]. Each coefficient R (x in, y in, x out, y out, z) corresponds to the complex amplitude of the echoes coming from the point r out = (x out, y out, z) in the focal plane when focusing at point r in = (x in, y in, z) (or conversely, since R ρρ (z) is a symmetric matrix due to spatial reciprocity).

---

### Maximum likelihood estimation of signal amplitude and noise variance from MR data [^bfd9a45c]. Magnetic Resonance in Medicine (2004). Low credibility.

In MRI, the raw data, which are acquired in spatial frequency space, are intrinsically complex valued and corrupted by Gaussian-distributed noise. After applying an inverse Fourier transform, the data remain complex valued and Gaussian distributed. If the signal amplitude is to be estimated, one has two options. It can be estimated directly from the complex valued data set, or one can first perform a magnitude operation on this data set, which changes the distribution of the data from Gaussian to Rician, and estimate the signal amplitude from the obtained magnitude image. Similarly, the noise variance can be estimated from both the complex and magnitude data sets. This article addresses the question whether it is better to use complex valued data or magnitude data for the estimation of these parameters using the maximum likelihood method. As a performance criterion, the mean-squared error (MSE) is used.

---

### Bell's theorem for temporal order [^e5b16f5d]. Nature Communications (2019). High credibility.

A physical transformation of the system is represented by a function. To make our arguments precise we need a notion of local transformations, namely, realised at the time and location defined by a local clock. If S 1 is the subsystem on which a local transformation T 1 acts, and S 2 labels the DOFs space-like separated from T 1, then, by definition, T 1 transforms product states asand separable states by convex extension. How local operations act on general, non-separable states can depend on the particular physical theory; however, action on separable states will suffice for our purposes. We further need to define how the local transformations combine. This depends on their relative spatio-temporal locations: if transformations T 1, T 2 are space-like separated they combine as (T 1 ⊗ T 2)(ω 1 ⊗ ω 2) = T 1 (ω 1) ⊗ T 2 (ω 2), which follows from the definition above; if T 1 is in the future of T 2, we define their combination as T 1 ο T 2 (ω) = T 1 (T 2 (ω)). (For simplicity, we omit possible additional transformations taking place between the specified events, as they are of no consequence for our argument.)

Proof Assumption (1) says that there is a random variable f determining the local states, of systems S 1, S 2, respectively. Assumption (3) says there is a random variable λ that determines the order of events. In general, the two variables can be correlated by some joint probability distribution P (λ, f). By assumption (4), events labelled A 1, B 1 are space-like separated from events A 2, B 2 and the order between events within each set (A j, B j), j = 1, 2 can be defined by a permutation σ j. Most generally, there is a probability P (σ j | λ) that the permutation σ j is realised for a given λ. By assumption (2), for each given order the system undergoes a transformation, whereis the transformation obtained by composingandin the order corresponding to the permutation σ 1 and similarly for. (For example, if σ 1 corresponds to the order, then.) Furthermore, at event D an outcome z is obtained with a probability P (z | λ, f, σ 1, σ 2). Finally, using assumption (1), we write the probabilities for all outcomes as

---

### Iron phthalocyanine on Au (111) is a "non-landau" fermi liquid [^d8e674ff]. Nature Communications (2021). High credibility.

For D < D c, I 0 and α are continuous as functions of B, and both are equal to zero for B = 0. In fact, in the whole (D, B) plane, I 0 ≡ 0 and α is continuous except on the half-line B = 0, D > D c. This line is, hence, a branch-cut for α (Fig. 6). Furthermore, the point D = D c, B = 0 may be considered as a logarithmic singularity for the function α (D, B) viewed as a function in the complex plane with the argument z = D + i B. In Fig. 7 we display α as a function of both D / D c and B / D c. As explained above for B → 0 +, α (D, B = 0) = (π /2) θ (D − D c), where θ (x) is the step function. For small non-zero B, α (D, B) still resembles this step function. In fact. As ∣ B ∣ further increases, α (D) decreases markedly for D > D c. For ∣ B ∣ ≫ D c, α is small for all D and as a consequence also the four Luttinger integrals are small.

Fig. 7
Non-topological parameter.

Phase shift α (D, B) for the auxiliary Anderson model as a function of magnetic anisotropy D and magnetic field B. The color encodes the value of α.

As ∣ B ∣ increases, the occupanciesincrease (Fig. 5 b), with the effect of increasing δ τ ↑ [Eq. (4)]. For δ −1 ↑, this effect is largely compensated by the decrease in α, leading to a small overall variation of the phase shift. Instead, for δ 1 ↑ both effects add up and δ 1 ↑ has a marked increase, which is then reflected in the transformation of the dip into a peak. For spin ↓ the effects are opposite.

---

### Primary visual cortex straightens natural video trajectories [^ece62a1b]. Nature Communications (2021). High credibility.

Fig. 6
Comparing different spatial and temporal scales.

a Relative curvature for twenty movies, displayed at two spatial scales, averaged across all V1 populations. Black points indicate natural movies, white points unnatural ones (temporal scale is f r a m e r a t e × 1). b Relative curvature for forty movies, calculated for two temporal scales, averaged across all V1 populations. Black points indicate natural movies, white points unnatural ones (spatial scale is z o o m × 1). c Relative curvature as a function of average firing rate for all natural movies. Each point illustrates a dataset, z o o m × 1 is shown in red, z o o m × 2 is shown in gray, temporal scale is f r a m e r a t e × 1. ✱ p = 0.01, two-sided Wilcoxon signed-rank test, n = 61 datasets.

---

### Deep learning-based detection and segmentation of diffusion abnormalities in acute ischemic stroke [^f6859074]. Communications Medicine (2021). Medium credibility.

Loss function

To ameliorate the imbalanced voxel classes issue (between the number of lesion and non-lesion voxels) and regularize the false-positive rate predicted by networks, a hybrid loss function described in Eq. (6) was utilized to train our proposed models.where L f u s e is the loss function supervised at the final output of the fusion block X f u s e and L i, s i d e is the loss function supervised at the side output of the decoders at each level X d e, i as follows:where w g d s = 1, w b b c = 1, w r = 1 e − 5 due to hyperparameter tuning, L g d s is the generalized dice loss function defined as, L b b c is the balanced binary cross-entropy defined as, and L 1 (p) is the L 1 regularization on all predicted voxels defined aswhere p (x, y, z) is the predicts from networks at (x, y, z) coordinates. The loss function is optimized during training step by ADAM optimizer with learning rate = 3 E − 4. Learning rate will be factor by 0.5 when loss function is on a plateau over 5 epochs with minimum learning rate = 1 E − 5.

---

### A silicon-on-insulator slab for topological valley transport [^1bc06a1e]. Nature Communications (2019). High credibility.

Fig. 1
Band structures and nontrivial topology in silicon-on-insulator (SOI) valley photonic crystals (VPCs). a Oblique-view scanning-electron-microscope image of the fabricated VPC, which is patterned on standard 220-nm-thickness silicon wafer. The VPC is arranged in a honeycomb lattice with a periodicity of a = 385 nm. The inset indicates that the VPC slab is asymmetrically placed between the SiO 2 substrate and the top air region along the z axis. b Details of the unit cells consisting of two inequivalent air holes, i.e. the smaller one with a diameter d 1 (d 2) = 81 nm and the larger one with a diameter d 2 (d 1) = 181 nm for VPC1 (VPC2). c Bulk band both for VPC1 and VPC2. The colormap indicates the linear polarization of a photonic band. There is a 132-nm-bandwidth gap (yellow region) between the first and second TE-like bands (purple) due to the y -axis inversion symmetry broken. Gray region: light cone of silica. d Simulated phase vortex of H z field profile and e ellipticity angle of (E x, E y) field at the K valley of the TE1 band for VPC1 and VPC2. The H z phase vortex mainly rotates along the unit cell center, of which the singularity point corresponds to right- or left-handed circular polarization (RCP or LCP). All of the field patterns shown below focus on the z -central plane (labeled as z = 0 in the inset of Fig. 1a). f Temporal evolution of RCP and LCP at the singularity point, respectively. In this work, the refractive index of silicon and silica are n Si = 3.47 and n Silica = 1.45, respectively

---

### Coexistence of state, choice, and sensory integration coding in barrel cortex LII / III [^a09a23bc]. Nature Communications (2024). High credibility.

Two-photon calcium imaging data preprocessing

In 11 FOV from seven animals with jRGECO1a, we carried out a detailed analysis of complete population statistics. Raw images from calcium imaging were first motion-corrected in X and Y dimensions using the Suite 2p motion correction module. ROIs were then delineated using the routine from suite2pand manually curated, and every visible cell with an event rate > 0.005 Hz was included. A local neuropil fluorescence was defined as an annulus surrounding the ROI, with an external diameter of three times the ROI diameter, and an internal diameter of 1.5 times the ROI diameter (padding). A noise-adjusted Df/F0 was then computed from the mean ROI fluorescence and neuropil fluorescence using a custom algorithm. In brief, this algorithm runs four processing stages iteratively until finding a stable estimate of the neuropil correction factor ɑ: (1) correction of neuropil and slow trends, (2) percentile-based Z-scoring, (3) estimation of putative periods of non-activity (Ena) from the Z-score, (4) linear regression to estimate the neuropil correction factor ɑ. ɑ is initialized to 0.5 (other values return the same results but converge more slowly).
Neuropil is subtracted with factor ɑ: F1(t) = F(t) - ɑ*Fp(t). We then subtract a 10th percentile in a running window of 30 s. F2(t) = F1(t) - F 10th (t).
A percentile-based Z-scoring is performed that scales to the noise level. Based on the assumption that activity-related calcium transients are upward, we predict that lower percentiles of the fluorescence distribution are not contaminated by activity and can be used to infer underlying noise statistics. Following an assumed Gaussian distribution of the residual noise, we define the variance sigma sig = pr(16) - pr(2.3), and the average baseline fluorescence mu f = pr(2.3) + 2 x sig; with pr(2.3) and pr(16) being the 2.3rd and 16th percentile of the fluorescence distribution. Z = (F2(t)- mu f)/sig
Z is smoothed with a boxcar filter of width 3, and an index of autocorrelation of order 1 is calculated (AR1). Increase in AR1 track with high sensitivity consistent increases of fluorescence (i.e. increase in fluorescence consistent in more than two to three frames). AR1(t) = Z(t)*Z(t + 1). Values of AR1 above an arbitrary threshold of 0.5 are considered periods of possible activity, and epochs of at least five frames with AR1 below 0.5 are considered possible epochs of non-activity (E NA).
factor ɑ was calculated on by least squares regression of F(t ENA) = ɑ*Fp(t ENA), multiple time in 30 s long time windows (to avoid regressing slow drifts) and only including timepoints in E NA. Updated ɑ is finally computed as the average across all 30 s windows with at least five data points.

---

### Representation of strong baseline contributions in 1H MR spectra [^9e210c15]. Magnetic Resonance in Medicine (2001). Low credibility.

A comparison is made between two optimization procedures and two data models for automated analysis of in vivo proton MR spectra of brain, typical of that obtained using MR spectroscopic imaging at 1.5 Tesla. First, a shift invariant wavelet filter is presented that provides improved performance over a conventional wavelet filter method for characterizing smoothly varying baseline signals. Next, two spectral fitting methods are described: an iterative spectral analysis method that alternates between optimizing a parametric description of metabolite signals and nonparametric characterization of baseline contributions, and a single-pass method that optimizes a complete spectral and baseline model. Both methods are evaluated using wavelet and spline models of the baseline function. Results are shown for Monte Carlo simulations of data representative of both long and short TE, in vivo 1H acquisitions.

---

### 3D magnetic resonance fingerprinting with quadratic RF phase [^4b85c41c]. Magnetic Resonance in Medicine (2021). Medium credibility.

Purpose

To implement 3D magnetic resonance fingerprinting (MRF) with quadratic RF phase (qRF-MRF) for simultaneous quantification of T 1, T 2, ΔB 0, and T₂*.

Methods

3D MRF data with effective undersampling factor of 3 in the slice direction were acquired with quadratic RF phase patterns for T 1, T 2, and T₂* sensitivity. Quadratic RF phase encodes the off-resonance by modulating the on-resonance frequency linearly in time. Transition to 3D brings practical limitations for reconstruction and dictionary matching because of increased data and dictionary sizes. Randomized singular value decomposition (rSVD)-based compression in time and reduction in dictionary size with a quadratic interpolation method are combined to be able to process prohibitively large data sets in feasible reconstruction and matching times.

Results

Accuracy of 3D qRF-MRF maps in various resolutions and orientations are compared to 3D fast imaging with steady-state precession (FISP) for T 1 and T 2 contrast and to 2D qRF-MRF for T₂* contrast and ΔB 0. The precision of 3D qRF-MRF was 1.5–2 times higher than routine clinical scans. 3D qRF-MRF ΔB 0 maps were further processed to highlight the susceptibility contrast.

Conclusion

Natively co-registered 3D whole brain T 1, T 2, T₂*, ΔB 0, and QSM maps can be acquired in as short as 5 min with 3D qRF-MRF.

---

### Nonlinear wave propagation governed by a fractional derivative [^db5b4cd7]. Nature Communications (2025). High credibility.

The idea of fractional derivatives has a long history that dates back centuries. Apart from their intriguing mathematical properties, fractional derivatives have been studied widely in physics, for example in quantum mechanics and generally in systems with nonlocal temporal or spatial interactions. However, systematic experiments have been rare because the physical implementation is challenging. Here we report the observation and full characterization of a family of temporal solitons that are governed by a fractional nonlinear wave equation. We demonstrate that these solitons have non-exponential tails, reflecting their nonlocal character, and have a very small time-bandwidth product.

---

### Dislocation avalanches are like earthquakes on the micron scale [^06350b69]. Nature Communications (2022). High credibility.

Based on Supplementary Fig. 8, the SEM images recorded before and after the stress drop analysed in Fig. 1 c–e were identified and the corresponding edge shapes were denoted by purple and pink colours, respectively. These SEM images are shown in Supplementary Fig. 9 a, b. Although it is barely seen by visual inspection, the quantified difference of the two images Δ x raw, t (z, t) = x t +Δ t (z) − x t (z) (Supplementary Fig. 9 c) proves that deformation took place along the slip plane at the height of ~28 μm (as also seen as horizontal grey line in Supplementary Fig. 8 and highlighted by a red line along the corresponding basal plane in Fig. 1 b).

Strain localization

In order to quantify how local deformation between two consecutive SEM images was we stared from the Δ x raw (z) curves obtained in the previous section. These curves were still rather noisy. We, therefore, calculated the moving average with a window size of 15 pixels and then applied a moving median smoothing with a window size of 61 pixels. Finally, we made the curves monotonous, since slip in the opposite direction was not observed in the experiments. Three representative exemplary so obtained Δ x (z, t) curves can be seen in the bottom row of Supplementary Fig. 10.

The obtained profiles usually exhibit a single slip band, but sometimes more than one step in the profile is seen. To quantify to what extent is the deformation localized we use the method of ref. We first note, that Δ x (z, t) is defined on an equidistant grid of the individual pixels of the SEM image. Let the discretized profile be denoted as Δ x i (for simplicity we omit the reference to time t). The local strain increment is then. We now select an arbitrary point with index k along the height of the pillar and consider the typical distance of the plastic strain increments from this point as:where Δ z is the pixel size of the SEM image. Then the minimumis determined. For a homogeneous distribution of the plastic strain (i.e. Δ x (z) is a linear function)equals L /4, where L is the height of the micropillar. On the other hand, for a fully localized strain distribution (i.e. Δ x (z) is a step function)is obtained. The localization parameter is, therefore, defined as

---

### Topological phase singularities in atomically thin high-refractive-index materials [^1ff87d31]. Nature Communications (2022). High credibility.

Fig. 1
Topology of the reflection phase near the singular point.

a Schematics of generalized structure for observation of phase singularities, arising from interaction of Fabry–Perot resonator's (280 nm SiO 2 /Si) modes with ultrathin films of HRIM. b Phase singularity point arises when zero-reflection surface of the system HRIM/SiO 2 /Si intersects with the material dispersion curve of HRIM. c In close vicinity of zero-reflection points, phase becomes singular and acquires topological chargeor, corresponding to –2π or +2π phase round-trip accumulation. d Phase has opposite π-gradient for angles slightly above and below singular point (dashed lines in panel (c)) giving rise to topological charge with 2π round-trip around zero-reflection point. The inset is a dielectric permittivity of the model HRIM used for calculation of (b) and (c).

Zero-reflection implies perfect absorption of the light that falls onto the discussed structure (as transmission through the silicon substrate should be zero). Zero-reflection entails phase singularity due to the singular nature of light phase at zero light amplitude (where the phase of light is not defined). Figure 1c represents the map of the p-polarized wave reflection phase in space of E and, which contains two phase singularities corresponding to the zero-reflection points with topological charges equal to1 and +1 (the topological points of Fig. 1b). Of immediate interest is a bifurcation behavior of optical phase in the vicinity of topological point (Fig. 1d). In other words, phase reveals abrupt ± π-jumps near a zero-reflection when plotted as a function of wavelength for a fixed incidence angle close to a phase singularity. It gives an indispensable degree of freedom for efficient phase manipulation.

---

### Extreme nonlinear strong-field photoemission from carbon nanotubes [^13a2456b]. Nature Communications (2019). High credibility.

To determine the tunneling probability, it is necessary to obtain the total electric field E tot (z, t) induced by the incident laser pulse. Here z is the distance to the tip, β is the field enhancement factor, and e is the fundamental charge. A simplified expressionis adopted for the spatial variation of E tot (z, t)

Here r 0 is the tip radius and w foc is the beam waist. The peak strength and 1/ e decay length of the total field areand l F ≈0.4 r 0. For CNT tips, the field enhancement varies with the radius as β = 24(r 0 [nm]) −0.5. The electric potential energy created by the electric fieldis given bywhereis the Gauss error function. The tunneling probability is calculated from the Wentzel–Kramers–Brillouin (WKB) approximation

In this expression, Θ(x) is the Heaviside step function, m is the mass of a free electron in vaccum, ℏ is the reduced Planck constant, and Φ = 4.4 eV is the work function for CNT. The electric field force F at the tip causes electron emission only when it points outward the surface [F > 0 so that V (z; F) < 0 for z > 0]. The exit position z E for the tunneling electron with energy E is determined by V (z E; F) + Φ = E.

The incident laser pulse has a time dependence

where, ω, ϕ, and τ are, respectively, the peak strength, circular frequency, carrier-envelope phase, and pulse duration of the incident optical-field. At any time with F z = 0 (t) > 0, we should calculate numerically the exit position z E and then the tunneling probability for every possible initial energy E. This time-consuming calculation is avoided when the vacuum potential barrier for field emission can be approximated by a triangular-shaped barrier[V (z; F) = − Fz] so that

---

### Microscopic origin of the '0.7-anomaly' in quantum point contacts [^4c1e751a]. Nature (2013). Excellent credibility.

Quantum point contacts are narrow, one-dimensional constrictions usually patterned in a two-dimensional electron system, for example by applying voltages to local gates. The linear conductance of a point contact, when measured as function of its channel width, is quantized in units of GQ = 2e(2)/h, where e is the electron charge and h is Planck's constant. However, the conductance also has an unexpected shoulder at ∼0.7GQ, known as the '0.7-anomaly', whose origin is still subject to debate. Proposed theoretical explanations have invoked spontaneous spin polarization, ferromagnetic spin coupling, the formation of a quasi-bound state leading to the Kondo effect, Wigner crystallization and various treatments of inelastic scattering. However, explicit calculations that fully reproduce the various experimental observations in the regime of the 0.7-anomaly, including the zero-bias peak that typically accompanies it, are still lacking. Here we offer a detailed microscopic explanation for both the 0.7-anomaly and the zero-bias peak: their common origin is a smeared van Hove singularity in the local density of states at the bottom of the lowest one-dimensional subband of the point contact, which causes an anomalous enhancement in the Hartree potential barrier, the magnetic spin susceptibility and the inelastic scattering rate. We find good qualitative agreement between theoretical calculations and experimental results on the dependence of the conductance on gate voltage, magnetic field, temperature, source-drain voltage (including the zero-bias peak) and interaction strength. We also clarify how the low-energy scale governing the 0.7-anomaly depends on gate voltage and interactions. For low energies, we predict and observe Fermi-liquid behaviour similar to that associated with the Kondo effect in quantum dots. At high energies, however, the similarities between the 0.7-anomaly and the Kondo effect end.

---

### Uncovering the heterogeneity and temporal complexity of neurodegenerative diseases with subtype and stage inference [^0f004f8d]. Nature Communications (2018). Medium credibility.

The linear z -score model we use in this work reformulates the event-based model in (1) by replacing the instantaneous normal to abnormal events with events that represent the (more biologically plausible) linear accumulation of a biomarker from one z -score to another. The linear z -score model consists of a set of N z -score events E iz, which correspond to the linear increase of biomarker i = 1… I to a z -score z ir = z i 1…, i.e. each biomarker is associated with its own set of z -scores, and so N = . Each biomarker also has an associated maximum z -score, z max, which it accumulates to at the end of stage N. We consider a continuous time axis, t, which we choose to go from t = 0 to t = 1 for simplicity (the scaling is arbitrary). At each disease stage k, which goes from t = to t = , a z -score event E iz occurs. The biomarkers evolve as time t progresses according to a piecewise linear function g i (t), whereThus, the timesare determined by the position of the z -score event E iz in the sequence S, so if event E iz occurs in position k in the sequence then = .

To formulate the model likelihood for the linear z -score model we replace Eq. (1) withwhere, NormPDF(x, μ, σ) is the normal probability distribution function, with mean μ and standard deviation σ, evaluated at x. We assume the prior on the disease time is uniform, as in the original event-based model.

The SuStaIn model is a mixture of linear z -score models, hence we havewhere C is the number of clusters (subtypes), f is the proportion of subjects assigned to a particular cluster (subtype), and M is the overall SuStaIn model.

---

### Maximizing spectral sensitivity without compromising resolution in phase-incremented, steady-state solution NMR [^32d7bfc1]. Nature Communications (2025). High credibility.

Figure 6 presents simulations incorporating fixed noise levels and PI-SSFP data collected with a variety of flip angles, comparing the processing capabilities of the β -filter-based proposition, with the L1 (first-norm) regularized LASSO reconstruction described in Eq. [11]. For each flip angle the signals being subject to these two processing pipelines are the same, and in all cases consisted of four peaks of relative intensities 1:0.4:0.1:0.7, affected by identical, α-independent levels of time-domain noise. These examples illustrate how the LASSO approach can deal with PI-SSFP data recorded for larger flip angles, retaining peak linewidths constant and improving the peak's intensities as the signal arising from increased flip-angles becomes larger. This is particularly evident for the peak with relative intensity of 0.1 placed at 1.0 kHz, which clearly emerges from the noise at higher αs. By contrast, PI-SSFP data processed using the β -based coefficients leads to broadened peaks when processing the larger-flip-angle FIDs, and thereby to minor decreases in the SNR despite the larger signals emitted by the spins at larger αs.

Fig. 6
Synthetic NMR spectra reconstructed for four uncoupled sites with relative intensities 1:0.4:0.1:0.7 positioned at 3.5, 1.6, 1.0 and -4.05 kHz, assumed subject to PI-SSFP experiments with different flip-angles α.

Additional common parameters of all "experiments" included TR = 20 ms, M = 40 phase increments, 13.5 kHz sampling rates, 2 Hz digital resolutions, instantaneous (δ) pulses, identical relaxation times T 1 = 5 s, T 2 = 2 s, and a constant (α-independent) noise level created by a random number generator with an RMS amplitude amounting to 25% of the maximum single-site longitudinal magnetization (the peak at 3,5 kHz). Notice the slight SNR drop that data "acquired" for increasing α shows when processed based on the β-coefficients, despite the increase signals emitted as flip-angles increase (Fig. 1). We ascribe this to a broadening of the β-derived filter functions, which is absent in the regularized version that keeps all peaks at similar half widths regardless of the flip angle used in the PI-SSFP acquisition. No such penalty affects the regularized reconstruction introduced in this work.

---

### Towards non-blind optical tweezing by finding 3D refractive index changes through off-focus interferometric tracking [^b74bf336]. Nature Communications (2021). High credibility.

Interference intensities

In our simplified model, scattering between the particles is neglected, i.e. which can be achieved for N < 10 and an adequate camera position at z d. Figure 3a illustrates the fieldsof a single beam focused with NA. It further indicates a distance z dn behind the geometrical focus (i.e. at a distance M²·z dn at the conjugate camera plane). The incident electric field E i (including wavefronts) is shown in blue, the scattered electric fieldfrom a point scatterer at position b (with spherical wavefronts) is sketched in green. The resulting off-focus interference (OFI) intensity distributionresults in three intensity terms for unscattered, scattered and interference light. Hence, the total intensity of N deformed beams at the camera in the (conjugate) plane z d readswithandfor particles with a RIhigher than that of the environment (n m).

Fig. 3
Calculating interference intensities at the camera plane.

a Scattering scheme: A spherical wave with field E s (green) is emitted by a point scatterer at position b relative to the center of a Gaussian laser focus with field E i (blue). The phase difference is proportional to the difference of the momentum vectors Δk = k i −k s. b One-dimensional intensities I i (x) without scatterer and I d (x) ≈ I i (x) + I is (x) with scatterer at plane z d behind the focus, with I is (x) being the interference intensity. The dashed line indicates the circular integration area Ω. c, d Normalized two-dimensional intensities I d (x, y, b) and I is (x, y, b) at plane z d obtained from a scattering simulation with a glass bead at b = (0.25,0,0.1)·μm. e, f The lateral (e) and axial (f) position signals, S x (b) and S z (b), are obtained for positions b x, b z along two axes through the focus after calculating the center-of-intensity from I is (b). The results from a simplified analytical dipole model (green) and a rigorous numerical simulation (blue) are compared.

---

### Changing temporal context in human temporal lobe promotes memory of distinct episodes [^b4157c28]. Nature Communications (2019). High credibility.

Neural drift

We constructed a feature vector associated with each item j using the average z-scored power of each frequency in each electrode localized to the temporal lobe:where z l; f (j) is the z-transformed power of electrode l = 1… L at frequency f = 1… F in the temporal lobe averaged over the time period of interest relative to the presentation of each word or word pair j.

In our initial analysis of overall levels of neural drift, we used the power averaged over the interstimulus period (−750 to 0 ms relative to the presentation of each item) to calculate the average z-transformed power for each frequency, and then constructed a feature vector for each item by combining values of power across five frequency bands (Fig. 1): theta (3–8 Hz), alpha (8–12 Hz), beta (13–25 Hz), low gamma (30–58 Hz), and high gamma (62–100 Hz). Then, in order to determine whether the relation between neural drift rate and memory performance was specific to individual frequencies and time points, we separately constructed feature vectors using each combination of individual frequencies and sliding 500 ms window (100 ms steps, 80% overlap; Fig. 2b). In post-hoc and stimulation analyses, we specifically focused on low frequency activity in the interstimulus epoch, and therefore constructed feature vectors using the z-transformed power averaged over the entire interstimulus epoch (−750 to 0 ms) and over all frequencies between 3 and 12 Hz.

To assess the extent to which the distributed pattern of spectral power changes over time, we computed the cosine similarity between feature vectors assigned to every epoch, E j and every other epoch within the same list, E j + n, where n is the number of intervening items, or lags. Cosine similarity provides a measure of how similar the angles of two vectors are in a multidimensional space, and in this case reflects how similar the neural pattern of activity is between any two time points. Within each list of length k items, we therefore computed the average similarity between feature vectors separated by identical time lags, n: where n corresponds to the average similarity of epochs spaced apart by n lags in a given list. In a list of k items, the number of comparisons that contribute to the average similarity between epochs separated by only one lag is k – 1, whereas the similarity between epochs separated by k – 1 lags is made of one comparison.

---

### A rich conformational palette underlies human ca2.1-channel availability [^b85d0034]. Nature Communications (2025). High credibility.

Finally, to obey conservation of charge, the valence of the R2↔A2 transitions was also excluded as a free parameter, and was calculated by:

The model rates were formulated into a Q -matrix. Briefly, Q was a square 4 × 4 matrix. Each element q i, j contained the rate for the transition from state i to state j. If there were no connection between states i and j then q i, j = 0. Each diagonal element was the negative sum of the off-diagonal elements in its row. In this way, where p (t) was a 1 × 4 vector of probability (occupancy) for each state. The MATLAB ode15s solver was used to calculate it. The voltage steps had a 43 μs time-constant to both emulate the COVG clamp speed and reduce stiffness. For initial conditions, background fluorescence calculations, and other calculations after fitting, the state occupancies at steady-state were calculated using:where u was a 4 × 1 unitary vector and S was [Q u].

Finally, 4 × 1 vector f contained fluorescence levels of each state. State R1 fluorescence (F R1) was fixed to 0. The fluorescence levels of states A1 and A2 (F A1 and F A2, respectively) were free parameters. State R2 fluorescence (F R2) was constrained, thus, to impose conformational conservation:

Fluorescence was then simulated as:where V h, n was the holding potential of the n th recording from the cell, and b n was a factor to account for fluorescence bleaching during the experiments, which reduced the Δ F amplitude. b 1 was fixed to 1, and b n > 1 had bounds 0 and 1.

Data from each cell with > 2 V h were fit simultaneously. Rate optimization was performed by least squares, using the Bayesian adaptive direct search (BADS) machine-learning model-fitting algorithm.

---

### Differentiable sampling of molecular geometries with uncertainty-based adversarial attacks [^411c42fd]. Nature Communications (2021). High credibility.

A conventional active learning loop was performed by retraining the NN potentials on geometries sampled with NN-based MD simulations, performed in the NVE ensemble using the Velocity Verlet integrator with initial temperature of 600 K, a duration of 5 ps, and a timestep of 0.5 fs. Trajectories started from each of the 543 optimized poses. For each trajectory, 10 frames corresponding to the last 2 ps of the simulations were saved and later evaluated using DFT. Frames with DFT energy higher than 20 kcal/mol/atom above its ground state, often due to unstable trajectories, were not added to the training sets. When evaluating the robustness of the models, NN-based MD simulations were performed using the same parameters described above, but at a higher temperature of 1000 K. Trajectories were considered unstable if distances between bonded atoms became smaller than 0.75 Å or larger than 2.0 Å throughout the simulation.

Two scenarios were considered for each generation of neural networks: (i) a single NN is retrained from its own MD simulations; or five NNs are retrained with geometries sampled using adversarial attacks (see Supplementary Fig. 21). All NNs employ the same SchNet architecture, with five NNs per committee, each containing four convolutions, 256 filters, atom basis of size 256, 64 learnable gaussians, and cutoff of 5.0 Å. The models were trained on different splits of the initial dataset (ratios 60:20:20 for train:validation:test) for 400 epochs, using the Adam optimizer with an initial learning rate of 5 × 10 −4 and batch size of 150. A scheduler reduced the learning rate by a factor of 0.5 if 25 epochs passed without improvement in the validation set. The training coefficients α E and α F (see Eq. (3)) were set to 0.1 and 1.0, respectively.

Adversarial attacks were initialized by displacing the atomic coordinates of optimized geometries byfor each pose. The resulting attack δ was optimized for 200 iterations using the Adam optimizer with a learning rate of 10 −2. The normalized temperature k T was set to 20 kcal/mol. No deduplication was performed.

---

### The snm procedure guideline for general imaging 6.0 [^58130ebe]. SNMMI (2010). Medium credibility.

General imaging — preprocessing for single-photon emission computed tomography (SPECT) studies notes that 'Prefiltering of the projection data is appropriate in many SPECT studies because smoothing in the axial direction may be included' and advises that 'When necessary, guidance on the choice of filter and filter parameters (cutoff and order) should be sought from experts'. It further states that 'Correction for patient motion may reduce reconstruction artifacts' and that 'The highest quality study will be obtained by minimizing patient motion'.

---

### SpatialDM for rapid identification of spatially co-expressed ligand-receptor and revealing cell-cell communication patterns [^cf070df0]. Nature Communications (2023). High credibility.

As a computational toolbox, SpatialDM has major functions for both global and local analyses (Fig. 1 b). First, by leveraging this bivariate R, we introduce a hypothesis testing to reject the null that the ligand and receptor are spatially independent, hence allowing us to select the spatially co-expressed LR pairs. Second, we further adapted local Moran's I to their bivariate format to detect local hits for each significant LR pair (Methods). Based on the local interaction hits for each LR pair, SpatialDM allows grouping these significant LR pairs into a few distinct communication patterns, e.g. by the automatic expression histology model introduced in SpatialDE. Third, to interpret the local communication patterns, it also provides an enrichment test and visualisation of putative pathways for each local pattern. Last, as a unique feature, SpatialDM further supports detecting LR pairs that have differential interaction density between conditions or along a continuous covariate, which is highly demanded for biological discovery in both developmental and disease contexts.

Accurate and efficient z -score test

In order to obtain the null distribution in this hypothesis testing problem, a generic method is permutation as used by most CCC methods, where the test statistic R will be calculated by random shuffling of binding partners for each pair, e.g. 1000 times. On the other hand, when the number of spatial spots is large, the permutation test often becomes a computational bottleneck for the analysis. Therefore, we derived the first and second moments of the null distribution to analytically obtain a z -score and its according p -value for the observed R (see Supp. Note 1). Strikingly, the z -score-based p -value has high correlations with the permutation-based p -value in datasets with different sizes (Fig. 1 c, d; Spearman's R > 0.9, local statistics correlation: Supplementary Fig. 2 d, e). Given the computational convenience, SpatialDM (the permutation mode, 1 CPU) ranks as the fastest method among all permutation-based methods, finishing testing 1000 LR pairs within 1.5 min for a 10,000-spot dataset (even though all other methods using 50 CPUs except SpaTalk and SpatialCorr). Importantly, the z-score-based strategy further introduces over 100x speedups, therefore is exclusively scalable to a million spots within 12 minutes (even with a single CPU). Therefore, this innovation of analytical null distribution can be highly valuable for the analysis of ST data with increasingly large sizes.

---

### Article 5. An introduction to estimation – 2: from z to T [^fb82b806]. Emergency Medicine Journal (2001). Low credibility.

Provided the sample size is large enough (that is, n greater than 100), the z statistic can be used to determine the confidence interval estimation of the population mean even when the sigma is not known. In these cases the estimation of the standard error of the mean is used. The z statistic is also valid when determining the population's proportion based upon a large sample. However, when dealing with smaller samples, the z statistic is replaced by the t statistic. This makes it possible to estimate, in a population with an unknown standard deviation: The probability of getting a sample mean greater than or equal to a particular value The value of a sample mean with a particular probability of occurring The probability of getting a sample mean between two particular values The confidence interval for the estimation of the population mean can also be determined using the t statistic.

---

### Scalable spatiotemporal prediction with Bayesian neural fields [^5769007e]. Nature Communications (2024). High credibility.

Spatiotemporal covariates

Letting (s, t) = ((s 1, …, s d), t) denote a generic index in the field, the covariates [x 1 (s, t), …, x m (s, t)] may include the following functions:

The linear and interaction covariates (5)–(7) are the usual first and second-order effects used in spatiotemporal trend-surface analysis models (Section 3.2 of ref.). In Eq. (8), the temporal seasonal features are defined by a setof ℓ seasonal periods, where each p i has harmonicsfor i = 1, …, ℓ. For example, if the time unit is hourly data and there are m = 2 seasonal effects (daily and monthly), the corresponding periods are p 1 = 24 and p 2 = 730.5, respectively. Non-integer periodicities handle seasonal effects that have varying duration in the time measurement unit (e.g. days per month or weeks per year). The Methods section discusses how to construct appropriate seasonal features for a variety of time units and seasonal effect combinations. In Eq. (9), the spatial Fourier features for coordinate s i are determined by a setof additional frequencies that capture periodic structure in the i th dimension (i = 1, …, d). These covariates correct for the tendency of neural networks to learn low-frequency signals: the empirical evaluation in the next section confirms that their presence greatly improves the quality of learned models. Covariates may also include static (e.g. "continent") or dynamic (e.g. "temperature") exogenous features, provided they are known at all locations and time points in the training and testing datasets.

Covariate scaling layer

Scaling inputs improves neural network learning e.g. ref. but determining the appropriate strategy (e.g. z-score, min/max, tanh, batch-norm, layer-norm, etc.) is challenging. BNF uses a prior distribution over scale factors to learn these quantities as part of Bayesian inference within the overall probabilistic model. In particular, the next stage in the network is a width- m hidden layerobtained by randomly scaling each of the m covariates x (s, t), whereis a log-normally distributed scale factor (for i = 1, …, m).

---

### Calcium stabilizes the strongest protein fold [^d8d6b3bd]. Nature Communications (2018). Medium credibility.

For single BE model at a given force loading rate r (determined as most probable loading rate from all unfolding events through a KDE) with the parameters Δx and k off,0, the probability density p(F, r, Δx, k off,0) to unfold at a given force F was fit to a normalized force histogram. For a superposition of two BE fits as in Fig. 4, the unfolding force histogram was fit with Eq. 1:

Force loading rates r 1 and r 2 were assigned at a force f critical at the minimum value of p total between the maxima of both BE fits and then assigned to BE1 and BE2, as force loading rate and unfolding force correlate in a constant velocity experiment. The relative weight of each distribution was q for BE1 and (1 – q) for BE2 with 0 < q < 1.

For dynamic force, spectra rupture force histograms for the respective peaks and dynamic force spectra were assembled from all curves showing B domain unfolding, or (if applicable) a specific fingerprint domain, and/or a clean complex rupture event. The most probable loading rate of all complex rupture or domain unfolding events was determined with a KDE, bandwidth chosen through the Silverman estimator. This value was used to fit the unfolding or rupture force histograms with the BE model for each pulling velocity. Errors in all diagrams are given as the asymmetric full-width at half maximum (FWHM) of each probability distribution. A final fit was performed through the most probable rupture forces and loading rates for each pulling velocity to determine the distance to the transition state Δx 0 and natural off-rate at zero force k off 0.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^aeeaae45]. Nature Communications (2025). High credibility.

Fitting models to data is an important part of the practice of science. Advances in machine learning have made it possible to fit more-and more complex-models, but have also exacerbated a problem: when multiple models fit the data equally well, which one(s) should we pick? The answer depends entirely on the modelling goal. In the scientific context, the essential goal is replicability: if a model works well to describe one experiment, it should continue to do so when that experiment is replicated tomorrow, or in another laboratory. The selection criterion must therefore be robust to the variations inherent to the replication process. In this work we develop a nonparametric method for estimating uncertainty on a model's empirical risk when replications are non-stationary, thus ensuring that a model is only rejected when another is reproducibly better. We illustrate the method with two examples: one a more classical setting, where the models are structurally distinct, and a machine learning-inspired setting, where they differ only in the value of their parameters. We show how, in this context of replicability or "epistemic uncertainty", it compares favourably to existing model selection criteria, and has more satisfactory behaviour with large experimental datasets.