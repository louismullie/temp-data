# Medical Question & Answer

**Sample ID**: a137dc06-da4f-9a3c-536c-5e1d5de76a27
**Dataset Index**: 269234

---

## Question

A circle is inscribed in a square of side m, then a square is inscribed in that circle, then a circle is inscribed in the latter square, and so on. If Sₙ is the sum of the areas of the first n circles so inscribed, then, as n grows beyond all bounds, Sₙ approaches:
(A) [π m²/2]  (B) [3π m²/8]  (C) [π m²/3]  (D) [π m²/4]  (E) [π m²/8]

---

## Answer

> Let's see… What do we have here? The user is asking about the limit of the sum of areas of an infinite sequence of inscribed circles, where a circle is inscribed in a square, then a square is inscribed in that circle, then a circle is inscribed in that square, and so on. Let's break this down step-by-step. First, I need to think about the geometry of the first circle and how its area relates to the side length m. Then, I should verify the scaling factor that relates successive circle areas. Next, I will set up the infinite series and confirm it is geometric. After that, I need to check the convergence conditions and compute the sum. Finally, I will match the result to the provided options and double-check for any missteps in radius or area calculations.

> Let me first confirm the initial setup. A circle inscribed in a square of side m has diameter equal to m, so its radius is m/2 and its area is π(m/2)^2 = πm^2/4. Wait, let me verify that: yes, radius m/2 gives area πr^2 = π(m/2)^2 = πm^2/4, so the first circle contributes πm^2/4 to the total.

> Next, I should review the scaling between successive circles. When a square is inscribed in a circle, the square's diagonal equals the circle's diameter. If the circle has radius r, the square has diagonal 2r, so its side is 2r/√2 = r√2. Then, a circle inscribed in that square has diameter equal to the square's side, so its radius is r√2/2 = r/√2. Hold on, I should verify the area scaling: the new area is π(r/√2)^2 = πr^2/2, which is exactly half the previous circle's area. So each circle's area is 1/2 of the prior circle's area.

> I will now examine the infinite series. The areas form a geometric sequence with first term a = πm^2/4 and common ratio r = 1/2. The sum of an infinite geometric series is a/(1 − r) when |r| < 1, which applies here. Thus, the total area approaches (πm^2/4)/(1 − 1/2) = (πm^2/4)/(1/2) = πm^2/2. Let me double-check the arithmetic: dividing by 1/2 is the same as multiplying by 2, so πm^2/4 times 2 is indeed πm^2/2.

> Now, I should match this to the options. The limit is πm^2/2, which corresponds to option A. Wait, let me verify there isn't a subtlety I missed about alternating inscriptions or orientation affecting the scaling; no, the geometric progression of areas by a factor of 1/2 holds regardless of orientation, so the conclusion stands.

> Final answer: The sum approaches πm^2/2, which is option A.

---

The sum of the areas of the first (n) circles approaches (\\frac{\\pi m^2}{2}) as (n \\to \\infty). This is because each inscribed circle has half the area of the previous one, forming a geometric series with first term (\\frac{\\pi m^2}{4}) and common ratio (\\frac{1}{2}), so the infinite sum is (\\frac{\\pi m^2}{4} \\cdot \\frac{1}{1 - \\frac{1}{2}} = \\frac{\\pi m^2}{2}).

---

## Step-by-step solution

### Step 1: area of the first circle

The first circle is inscribed in a square of side (m), so its diameter is (m) and its radius is (r_1 = \\frac{m}{2}). The area is:

[
A_1 = \\pi r_1^2 = \\pi \\left(\\frac{m}{2}\\right)^2 = \\frac{\\pi m^2}{4}
]

---

### Step 2: area of the second circle

A square is inscribed in the first circle; its diagonal equals the circle's diameter, (m). If the square's side is (s), then (s\\sqrt{2} = m), so (s = \\frac{m}{\\sqrt{2}}). The second circle is inscribed in this square, so its diameter is (s) and its radius is (r_2 = \\frac{s}{2} = \\frac{m}{2\\sqrt{2}}). The area is:

[
A_2 = \\pi r_2^2 = \\pi \\left(\\frac{m}{2\\sqrt{2}}\\right)^2 = \\pi \\frac{m^2}{8} = \\frac{1}{2} \\cdot \\frac{\\pi m^2}{4} = \\frac{1}{2} A_1
]

---

### Step 3: general pattern

Each subsequent circle is inscribed in a square that was inscribed in the prior circle, so the radius scales by (\\frac{1}{\\sqrt{2}}) each time. Thus, the area scales by (\\left(\\frac{1}{\\sqrt{2}}\\right)^2 = \\frac{1}{2}). The areas form a geometric sequence:

[
A_n = A_1 \\cdot \\left(\\frac{1}{2}\\right)^{n-1} = \\frac{\\pi m^2}{4} \\cdot \\left(\\frac{1}{2}\\right)^{n-1}
]

---

### Step 4: sum of the infinite series

The total area is the infinite geometric series:

[
S = \\sum_{n = 1}^{\\infty} A_n = \\frac{\\pi m^2}{4} \\sum_{n = 0}^{\\infty} \\left(\\frac{1}{2}\\right)^n = \\frac{\\pi m^2}{4} \\cdot \\frac{1}{1 - \\frac{1}{2}} = \\frac{\\pi m^2}{4} \\cdot 2 = \\frac{\\pi m^2}{2}
]

---

The limit of the sum of the areas is (\\boxed{\\frac{\\pi m^2}{2}}), which corresponds to option **A**.

---

## References

### Biased expectations about future choice options predict sequential economic decisions [^9b0d2bac]. Communications Psychology (2024). Medium credibility.

Next, the model works backwards through the sequence, iteratively using the aforementioned formula forwhen computing each respective action value Q for taking the option and declining the option for each t. Whenever the reward value of taking the current option is considered, the reward function R assigns reward values to options based on their ranks. h represents the relative rank of the current option.

In contrast, the reward value of sampling again is simply the cost to sample C.

This customisable R function allowed us to examine how the Ideal Observer changes its sampling strategy under the different reward payoff schemes used in our studies. Pilot full, Study 1 full, Study 2 and both conditions in Study 3 all involved instructing participants to try to choose the best price possible. In study conditions using these instructions, we implemented a continuous payoff function (resembling that of the classic Gilbert & Mosteller formulation), in which the relative rank of each choice would be rewarded commensurate with the value of its associated option. In Pilot baseline and the baseline, squares, timing, and prior conditions of Study 1, we adapted the payoff scheme to match participants' instructions that they would be paid £0.12 for the best rank, £0.08 for the second-best rank, £0.04 for the third best rank and £0 for any other ranks. Lastly, in the payoff condition of Study 1, we programmed the reward payoff function to match participants' reward of 5 stars for the best rank, 3 stars for the second-best rank, one star for the third-best rank and zero stars for any other ranks.

Another feature added to our implementation of the Ideal Observer, compared to the Gilbert & Mosteller base model, is the ability to update the model's generating distribution from its experience with new samples in a Bayesian fashion, instead of this generating distribution being specified in advance and then fixed throughout the paradigm. This Bayesian version of the optimality model treats option values as samples from a Gaussian distribution with a normal-inverse- χ2 prior. The prior distribution is initialised before experiencing any options with four parameters: the prior mean μ 0, the degrees of freedom of the prior mean κ, the prior variance σ 2 0 and the degrees of freedom of the prior variance ν. The μ 0 and σ 2 0 parameters of this prior distribution are then updated by the model following presentation of each newly sampled option value as each sequence progresses.

---

### Universality in long-distance geometry and quantum complexity [^47f080b3]. Nature (2023). Excellent credibility.

A careful analysis, confirms this picture and shows that the very largest additive discrepancy from thedistance is found near the cut locus, so for all U and, As, theandBerger spheres agree on distances to within a picometre. (A two-dimensional example that shows the same phenomenon is given in Supplementary Information 3).

Finally, let us examine the role of curvature. At large, the metric becomes strongly curved: the easy–easy section becomes very negatively curved, and the easy–hard sections become very positively curved. We call the σ x and σ y directions easy because they are cheap to move it, whereas the σ z direction is hard to move in for. The curvature length, which is also the distance to the cut locus in the hard direction, becomes very short. The high curvature explains how the metric can hide lots of volume at short distances that are invisible at long distances. Consider an operational definition of volume that counts how many marbles can be packed into the space: if we can cram in n (r) marbles each of radius r, then the volume is proportional to. Although the volume grows without bound as, the effective volume n (r) r 3 at any finite value of r does not. Instead, the effective volume grows like r −1 as we take r smaller and only levels off atonce r is less than the curvature length. Thus even asthe effective volume, as probed by experiments with finite resolution, stays finite.

Euclidean group

Consider parallel parking a unicycle.

The unicycle starts facing parallel to the curb and ends facing parallel to the curb but displaced sideways by z. The configuration space of the unicycle is its possible locations { y, z } and orientations { θ }, forming the Euclidean group SE(2). There are three primitive operations: roll forwards or backwards; turn; or drift sideways (perpendicular to the rolling direction). We model the difficulty of any parking manoeuvre withThe cost of parkingis the length ∫d s of the shortest path that connects our starting configuration { θ, y, z } = {0, 0, 0} to our parking spot {0, 0, z }.

---

### Modelling the species-area relationship using extreme value theory [^10b0aaff]. Nature Communications (2025). High credibility.

The nested species-area relationship, obtained by counting species in increasingly larger areas in a nested fashion, exhibits robust and recurring qualitative and quantitative patterns. When plotted in double logarithmic scales it shows three phases: rapid species increase at small areas, slower growth at intermediate scales, and faster rise at large scales. Despite its significance, the theoretical foundations of this pattern remain incompletely understood. Here, we develop a theory for the species-area relationship using extreme value theory, and show that the species-area relationship is a mixture of the distributions of minimum distances to a starting sampling focal point for each individual species. A key insight of our study is that each phase is determined by the geographical distributions of the species, i.e., their ranges, relative to the focal point, enabling us to develop a formula for estimating the number of species at phase transitions. We test our approach by comparing empirical species-area relationships for different continents and taxa with our predictions using Global Biodiversity Information Facility data. Although a SAR reflects the underlying biological attributes of the constituent species, our interpretations and use of the extreme value theory are general and can be widely applicable to systems with similar spatial features.

---

### Feasibility and coexistence of large ecological communities [^02d41162]. Nature Communications (2017). Medium credibility.

Shape of the feasibility domain

So far, we have focused on the volume of the parameter space resulting in feasiblity. However, two systems having the same Ξ can still have very different responses to parameter perturbations, just as two triangles having the same area need not to have sides of the same length (Fig. 1). The two extreme cases correspond to (a) an isotropic system in which if we start at the barycentre of the feasibility domain, moving in any direction yields roughly the same effect (equivalent to an equilateral triangle); (b) anisotropic systems, in which the feasibility domain is much narrower in certain directions than in others (as in a scalene triangle). For our problem, the domain of growth rates leading to coexistence is — once the growth rates are normalized — the (S −1)-dimensional generalization of a triangle on a hypersphere. For S = 3, this domain is indeed a triangle lying on a sphere as shown in Fig. 1. If all the S (S −1)/2 sides of this (hyper-)triangle are about the same length, then different perturbations will have similar effects on the system. On the other hand, if some sides are much shorter than others, then there will be changes of conditions which will more likely have an impact on coexistence than others. We therefore consider a measure of the heterogeneity in the distribution of the side lengths (Fig. 1 and Supplementary Note 10). The larger the variance of this distribution, the more likely it is that certain perturbations can destroy coexistence, even when Ξ is large and the perturbation small. This way of measuring heterogeneity is particularly convenient because it is independent of the initial conditions. Moreover, the length of each side can be directly related to the similarity between the corresponding pair of species (Supplementary Note 10), drawing a strong connection between the parameter space allowing for coexistence and the phenotypic space. As in the case of Ξ, this measure is a function of the interaction matrix and corresponds to a geometrical property of the coexistence domain.

---

### Brain-inspired replay for continual learning with artificial neural networks [^13399442]. Nature Communications (2020). High credibility.

Regularization-based methods

For the regularization-based methods (SI, EWCand online EWC), to penalize changes to parameters important for previously learned tasks, a regularization term was added to the loss: The value of hyperparameter λ was set by a grid search (see Supplementary Figs. 1–3; for SI this hyperparameter is typically referred to as c).

For SI, to estimate the importance of parameter i, after every task k the contribution of that parameter to the change in loss was first calculated as:with N iters the number of iterations per task, the value of the i th parameter after the t th training iteration on task k andthe gradient of the loss with respect to the i th parameter during the t th training iteration on task k. To get the estimated importance of parameter i for the first K − 1 tasks, these contributions were normalized by the square of the total change of parameter i during training on that task plus a small dampening term ξ (set to 0.1, to bound the normalized contributions when a parameter's total change goes to zero), after which they were summed over all tasks so far:with, whereindicates the value of parameter i right before starting training on task k. The regularization term of SI during training on task K > 1 was then given by:wherebyis the value of parameter i after finishing training on task K − 1.

---

### Glassiness and exotic entropy scaling induced by quantum fluctuations in a disorder-free frustrated magnet [^fd792599]. Nature Communications (2014). Medium credibility.

Proving the perimeter scaling of entropy

The hexagon representation for the sign state of each bipyramid allows us to establish bounds on the number of LC states, N (L, V), for a system of volume V and circumference L. We find that, where K 1, K 2 are constants. In particular, for V ~ L 2, we have: concluding that (up to a possible logarithmic correction) the number of states is extensive in the boundary length.

The lower bound is easy to establish: for a given boundary length L, we can construct explicitly a number of states which scales as. One way of doing so is by starting from one of the long-range structures as shown in Fig. 1d, e and Supplementary Fig. 6. These structures support straight quasi one-dimensional modes that change the state of the bipyramid along them. For a square sample of side L, we can put up to L -independent parallel modes of this type, which supplies us with the lower bound.

To show the upper bound, we recast the sign states as a tiling problem. We use the hexagonal tiles depicted in Fig. 1c to obtain a representation of the system as a network of lines (see also Supplementary Figs 5 and 6). The resultant network may be considered as a fully packed network of rectilinear stripes of alternating colour on a lattice, made of straight lines, π /6 degree turns ('elbows') and junctions as shown in Fig. 3b. The network has the following properties (for detailed discussion, see Supplementary Note 1): P1. Lines cannot terminate, and P2. There are no closed loops. Property P1 can be verified by inspection of possible termination points, and ruling each of them out. To prove property P2, assume the contrary and consider a closed loop of black colour, inside which there must be loops of smaller and smaller sizes. As the colours alternate, we must have an enclosed simply connected region that is entirely black or entirely blue. As we do not have an entirely blue or entirely black hexagon in our disposal, such a region must be of limited thickness, therefore the inner region must be made of lines with termination points. By property P1, such termination points are not allowed.

---

### Correction to: a method to quantify the "cone of economy" [^c425b20a]. European Spine Journal (2018). Low credibility.

Unfortunately, in the abstract at the results section units have been published incorrectly.

---

### Graphs [^c6c6d5a7]. Chest (2006). Low credibility.

Two rules of good graphs are presented and explicated.

---

### Solving the where problem and quantifying geometric variation in neuroanatomy using generative diffeomorphic mapping [^fd44f32f]. Nature Communications (2025). High credibility.

The statistical interpretation allows us to accommodate images with non reference signals, such as missing tissue, tracer injection sites, or other anomalies. At each pixel, the identity of the signal type is modeled as missing data, and maximum likelihood estimators are computed using an Expectation Maximization algorithm, which alternates between the E step: compute posterior probability π i (x) that each pixel corresponds to the reference image rather than one of the non-reference types, and the M step: update parameters by solving a posterior weighted version of the above:As an EM algorithm, this approach is guaranteed to be monotonically increasing in likelihood. An example of posterior weights are shown in the right hand column of Fig. 2 b.

Our approach uses mixtures of Gaussians to model variability in data, to allow large outliers to be accommodated by additional components, even though the Gaussian distribution itself does not have long tails. The Gaussian model allows for closed form expression (in terms of matrix inverse) for contrast transformation parameters. Other groups have used long tailed distributions to model variability and outliers in a robust manner, most notably the exponential distribution for l1 optimization. Techniques such as iteratively reweighted least squares can be applied as in Reuter et al. which lead lead to a weighted least squares problem which is similar to ours.

Nonconvex optimization with low to high dimensional subgroups and resolutions

This registration problem is highly nonconvex, and allows for many local minima. To provide robustness in our solution, we solve a sequence of lower dimensional subproblems, initializing the next with the solution to the previous. (i) 2D slice to slice rigid alignment maximizing similarity to neighbors(ii) 3D affine only alignment, registration using the full model at (iii) low (200 μm), (iv) medium (100 μm), and (v) high (50 μm) resolution. Time varying velocity fields are discretized into 5 timesteps and integrated using the Semi Lagrangian method. For most subproblems, spatial transformation parameters are estimated by gradient descent, and intensity transformation parameters are updated by solving a weighted least squares solution at each iteration. For subproblems that include linear registration only, parameters are estimated using Reimannian gradient descent (discussed in ref.and similar to a second order Gauss–Newton optimization scheme).

---

### Glass-cutting medical images via a mechanical image segmentation method based on crack propagation [^b06e7ef0]. Nature Communications (2020). High credibility.

Initial crack

In each partial model, we set an initial crack. The initial crack will grow along grooves under external load until reaching object boundaries. For the first local model, the initial crack needs to be specified. After completing crack propagation in the current region, the next local model can be determined according to the position and the orientation of the crack. The new area will partially overlap the old one so that it can contain the end portion of the crack in the previous area as the initial crack.

The initial crack in the first local model can be provided either manually or automatically (Fig. 10b). In the manual method (top half of Fig. 10b), one needs only to choose a short line a few pixels in length along the target boundary as the position of the initial crack in the mechanical model. This selection is simple and accurate because the target boundary is generally very clear in such a location and there is an obvious groove corresponding to the target boundary in the mechanical model. For the automatic method (bottom half of Fig. 10b), the initial crack can be generated by the following procedure: define the side length of square ABCD representing the local image to be 2 L. The target boundary intersects with the two parallel edges AD and BC. Draw a straight line starting from P M1, the midpoint of the side AB, and passing through P M2, the midpoint of the side DC, ending at the point O. The distance from P M1 to O is N times the length of AB (N > 1). Therefore, a triangle AOB is formed with angle at point O equal to φ = 2 arctan(1/(2 N)). Dividing φ by an integer m defines the angle increment, Δ φ = 2 arctan(1/(2 N))/ m. Starting at line OA, and sweeping toward line OB by incrementing the angle by Δ φ each iteration, the ray with angle φ i = i Δ φ (i = 0, 1, 2,…) will intersect the target boundary at the point P i in this local image area. Because the grayscale value of the pixels at the target boundary changes significantly, an edge-detection operator, such as the Canny edge detector, can easily identify the coordinates of the points P i. After detecting several boundary points, the initial crack of the first local model can be automatically formed by connecting these points to form a small line segment.

---

### Cutting edge or blunt instrument: how to decide if a stepped wedge design is right for you [^471f52f2]. BMJ Quality & Safety (2021). High credibility.

When might I consider doing a stepped wedge trial?

Research designs are shaped as much by practical constraints as by abstract schemes, and it is always a good idea to start with the constraints and work towards a design, rather than start with a design and try to fit it to constraints. These constraints will be unique to each research context, and box 1 lists some areas to think about. Still, there are some common features of settings where a stepped wedge trial might be considered as a possible design, and we now review these.

Box 1
Practical constraints on the design of a longitudinal cluster randomised trial

Are there limits on the time available to complete the evaluation, on the number of clusters, or on the number of participants (or the rate at which you can recruit participants) at each cluster? These constraints put limits on the overall scale of the evaluation, or force trade-offs between different design characteristics.
How will participants and their data be sampled in your study: as a series of cross-sectional surveys, as a continuous stream of incident cases, as a cohort followed over time, or some other way? Does the timescale divide into cycles, seasons or milestones that influence how you will sample participants and data?
Is there a limit on how many clusters can implement the intervention at the same time in the evaluation? If this is constrained by research resources (eg, if there are only enough trained research staff to implement the intervention one cluster at a time) then implementation must be staggered in some way.
If implementation is to be staggered, is there a minimum 'step length'? If the same team delivers the intervention in different clusters at different steps, then bear in mind it may take some time to get the intervention fully operational at a site, and the team will also need time to relocate from one cluster to the next.

---

### A landscape of response to drug combinations in non-small cell lung cancer [^e3f133bc]. Nature Communications (2023). High credibility.

Fig. 7
Relationship between sensitivity to single agents and combinatorial outcome.

A Combinations rarely affect viability beyond effects observable with single agents. The effect of combination treatment on each cell line was compared to the overall sensitivity to single agents observed across all cell lines. The percentile of cell lines in which the combination effect is superior to the effect of single agents in any cell lines (super sensitive cell lines) is shown in three categories (0, no observable supersensitive lines, 0–10% and over 10%). The cell lines are further broken down based on their response to single agents (color). B Network view of drug combinations resulting in super-sensitization. Anchors are displayed as squares and library drugs as circles. Library drugs that are also used as anchor drugs are represented by squares. C Supersensitive events are enriched in synergies. The percentile of synergistic events (cell lines) are compared between combinations that yield super sensitization versus (n = 92) those that do not (n = 4990). One-sided Wilcoxon rank-sum test p value is shown. D Synthetic lethal drug pairs: Network of drug combinations for drugs that yield substantial viability effect while single agents are deemed inactive. E Drugs with targets that are in close interaction with each other based on previously determined protein–protein interaction network are more likely to yield synergies than those targeting un-connected proteins (n = 157 'top-synergy', n = 58 "no-synergy"). One-sided Wilcoxon rank-sum test p value is shown. F Drugs that target members of a given biological pathway are more likely to yield synergy than those that target members of different pathways (n = 635 'top-synergy', n = 784 "no-synergy"). One-sided Wilcoxon rank-sum test p value is shown. The box plots for figures (C), (E), (F) have median center, 25 and 75 percentiles (Q1 and Q3) as bounds of the box, the minima and maxima being Q1–1.5 x IQR and Q3 + 1.5 x IQR (with IQR being interquartile range). G Synergistic drug pairs for which targets are encoded by genes engaged in a synthetic lethal interaction based on TCGA data analysis. H Synergistic drug pairs with experimental evidence for a synthetic lethal relationship.

---

### A quantitative analysis of various patterns applied in lattice light sheet microscopy [^5ecc1523]. Nature Communications (2022). High credibility.

Real-space and frequency-space comparisons of Gaussian, multi-Bessel (MB)-square, and hexagonal lattice light sheets

We start our characterization by investigating 20 μm-long light sheets that are optimized for imaging adherent cells. We show the notation for the coordinate system used in the rest of the paper in Fig. 1a. We define beam length by the full-width half maximum (FWHM) of the intensity profile along the propagation direction y (Fig. 1b, c). We note that other metrics to describe propagation length such as those based on optical sectioningalso give similar results (Table S1 and Fig. S 1a). To quantify the optical sectioning of different light sheets, we also plot out both the excitation axial profile and the cumulative intensity along the axial direction at both the beam center and the propagation length FWHM (Fig. 1d, e). To start, we compared a 0.21 NA Gaussian beam, an MB-square lattice with NA = 0.35/0.25 (max and min NA respectively) and a hexagonal lattice with NA = 0.46/0.36. For Gaussian beams, there is a unique relationship between sigma, beam thickness, and propagation length. For MB-square and hexagonal lattice beams, beams of the same propagation length, but with different structure can be generated by tuning the difference between the min and max NA (∆NA), the attenuation envelope imposed at the sample plane (∆k z at the pupil), and the spacing of the MB-square Bessel array (∆k x at the pupil). The effect of these parameters for each beam type will be described in the following section. Here we kept a constant ∆NA of 0.1 for both MB-square and hexagonal lattice beams and chose an MB-square lattice spacing such that the side beamlets were just outside the inner bounding annulus, as had been done previously. Both MB-square and hexagonal lattices improved axial resolution compared to a Gaussian beam of the same propagation length (Fig. 1f, h) as measured by comparing the overall PSF axial FWHM (1.18λ for the hexagonal lattice, 1.62λ for MB-square lattice, and 1.83λ for Gaussian, Table S2). These differences became even more prominent when comparing overall PSFs away from the beam focus, for example at the propagation profile FWHM, which in this situation is 10 microns away from the beam focus (Fig. 1g, i). Axial FWHM of the overall PSFs at this location were 1.27λ for the hexagonal lattice, 1.6λ for the MB-square lattice and 2.43λ for the Gaussian beam. Plots of the main lobe thickness as defined in Methods confirmed these results, showing that the main lobe thickness values for both MB-square and hexagonal lattice light sheets remain nearly constant throughout the beam propagation length up to 22λ while the Gaussian main lobe thickness increases two-fold over this range (Fig. S 1a). We also replicated these results experimentally for each of the beams here and demonstrated very good agreement with the optical simulations (Fig. S 2). Together, these results demonstrate a clearly distinguishable difference between Gaussian, MB-square, and hexagonal lattice light sheets. MB-square and hexagonal lattice light sheets have higher axial resolution and maintain this resolution over a larger portion of the propagation length than Gaussian beams. As a final example, we investigated flat top beams of the same length that are generated by clipping a uniform illumination stripe at the pupil plane with a mask. These beams result in an electric field resembling a Sinc function at the sample with intensity sidelobes that are in between those of a Gaussian beam and a MB-square lattice. Flat-top beams displayed intermediate properties, showing similar axial resolution to Gaussian at the beam focus, but less degradation along the beam propagation direction. As a tradeoff, flat-top beams had lower axial resolution, but better optical sectioning than either MB-square or hexagonal lattice light sheets at all locations (Fig. S 3e and Table S2).

---

### Quantifying entanglement in a 68-billion-dimensional quantum state space [^b339b52f]. Nature Communications (2019). High credibility.

Adaptive sampling algorithm

For each configuration, experimental data are stored in nodes in a quad-tree decomposition of P whose levels describe increasingly fine detail. The i th node corresponds to a square area ofat locationwith span. Nodes are sampled by placing the corresponding, one-dimensional local patterns on the DMDs and generating a coincidence histogram during acquisition time T a = 0.5 s. Coincidences C i are counted within a 1-ns coincidence window centered on the coincidence peak; accidental coincidences A i are counted in a 1-ns window displaced 2 ns from the coincidence window. Coincidence and accidental values are appended to a list each time the node is sampled. The estimated count rate, whereis a calibrated, relative fiber coupling efficiency. Optionally, A i can be subtracted from C i for accidental removal. Uncertainty is computed by assuming Poissonian counting statistics for C i and A i and applying standard, algebraic propagation of error through the calculation of the entanglement quantity (Eq. (3)).

The data collection algorithm consists of a partitioning phase followed by an iterative phase. During partitioning, the algorithm repeatedly iterates through a scan-list of leaves of the tree. Node i is considered stable when sgn(αR T − R i) is known to at least β standard deviations of certainty, where splitting threshold α (0 ≤ α ≤ 1) and stability criterion β are user-chosen heuristics. Stable nodes are no longer measured. If a node is stable and R i ≥ αR T, the node is split into four equal-sized sub-quadrants, which are initially unstable and added to the scan-list. Optionally, a maximum resolution (maximum tree depth) may be set.

The transition to the iterative phase occurs when the percentage of unstable leaves is < Γ, a user-chosen parameter. At this point, stability is ignored and all leaf nodes are scanned repeatedly and guaranteed to have the same total acquisition time. Various final stopping criteria can be used; we chose a fixed total run time. Note that heuristic parameters α, β, and γ may be changed during operation if desired. For the data shown in this manuscript, α = 0.002, β = 2, and Γ = 0.15 with a 30-h runtime.

The probability distributionis computed by uniformly distributing the estimated count rate (with or without accidental subtraction) from each leaf node across its constituent elements in, followed by normalization.

---

### Normal tissue architecture determines the evolutionary course of cancer [^82c89f64]. Nature Communications (2021). High credibility.

Fig. 4
Three-dimensional model of tumor evolution constrained by ductal network structure.

a Realistic three-dimensional topology of breast ductal networks (reconstructed with data from anthropomorphic breast phantoms in) provides full three-dimensional maps to seed and constrain tumor evolution simulations. b Tumor evolution is shown for varied points of initiation (z-dimension shown in a), to identical sizes (N = 10 4 cells). See attached videos V 4 and V 5. Two parameterizations are shown: DC13 (solid lines; μ p = 10 −2; μ d = 10 −3.0; s d = 10 −1; s p = 10 −3; indicated as red circle (a) in Fig. 3 g) and DC16 (solid lines; μ p = 10 −2; μ d = 10 −2.5; s d = 10 −2; s p = 10 −3; indicated as green circle (b) in Fig. 3 l). The slope of trajectories (schematic) on this phase portrait is termed "realized tumor Darwinian fitness" and is dependent on spatial constraints at the point of tumor initiation. Simulations closer to the ductal root (e.g. blue curve, z = 25%) in larger, less constrained branches are characterized by a steady left-to-right (neutral) evolution and constant acquisition of new clones. Simulations further from the ductal root (e.g. green, z = 75%) in smaller, more constrained branches are characterized by clonal sweeping (bottom-to-top evolution) early, but with a shift toward neutrality (left-to-right) at later times. Trajectories may be compared to alternative models with identical parameterizations: non-spatial model (dashed yellow), unconstrained two-dimensional model (dashed orange), or unconstrained three-dimensional model (dashed purple). c The analysis is repeated subject to spatial constraints on ten highly distinct anthropomorphic breast phantom ductal network reconstructions. In general, ductal branches far from the root decrease in size and increase in number (see Supplementary Fig. 5). d All simulations tend to follow an initially Darwinian evolutionary trajectory (steep slope) followed by a transition to neutral evolution (shallow slope). e, f An alternative metric of tumor neutrality: the linearity of cumulative mutation distribution with respect to inverse allelic frequency, sometimes called the 1/f power-law distribution. Although all tumors initially are quantitatively shown to be non-neutral, all eventually progress to neutral or nearly-neutral state (the neutral minimum threshold of 0.98shown in black). Tumors with initially high spatial constraints (green violin plots) transition to neutral evolution more slowly than those with less spatial constraints (blue violin plots). High spatial constraints enable accelerated evolution over a range of ductal networks in (c) (see Supplementary Fig. 4). Violin plots show median (diamond), 25/75 percentiles, and smallest/largest values within 1.5 times the interquartile range above/below quartiles) for N = 10 simulations for each z value.

---

### An introduction to machine learning [^70a823d9]. Clinical Pharmacology and Therapeutics (2020). Medium credibility.

Different categories of loss functions

Different objective functions can be chosen to measure the distance between observed data and values predicted by the model. Some of the distance metrics used in practice can be associated to a likelihood. The likelihood indicates how probable it is to observe our data according to the selected model. The most common use of a likelihood is to find the parameters that make the model fit optimally to the data (i.e. the maximum likelihood parameter estimates). Usually, the negative logarithm of the likelihood is minimized and considered as objective function because it has favorable numerical properties. Similarly, in ML metrics, such as mean squared error, logistic objective, or cross‐entropy, are used to find optimal parameters or assess the fitness of the model.

In practice, analytical calculation of maximum likelihood or minimal loss may not be feasible, and it is often necessary to use a numerical optimization algorithm to solve for the best parameter values. Gradient descent is such an algorithm, where we first define an objective function for which we want to minimize and then iteratively update the values of the parameters in the direction with the steepest decrease (first‐order derivative) of the objective function until a convergence to a minimum distance is deemed reached. In the scenario of a nonconvex objective function, the success of finding a global minimum, as opposed to landing in some local minima, will depend on the choice of the initial set of parameter values, the learning rate (i.e. step size of each iteration) and the criterion for convergence. The reader can refer to ref. 35 for details on convex and nonconvex optimization processes. Stochastic gradient descent is an additional trick that can further speed up the optimization by randomly sampling a training dataset and summing the distances across this subset of training data points for approximating the objective function.

---

### Coherent movement of error-prone individuals through mechanical coupling [^09e9d878]. Nature Communications (2023). High credibility.

Figure 2b–d show the trajectories taken by 1 × 1, 4 × 4 and 7 × 7 Kilobot Soft Robots, respectively, in all trials. It is apparent that the 1 × 1 robot (essentially a single Kilobot with open-loop control) is unable to move in a coherent direction. By contrast, the 7 × 7 robot deviates far less from the reference trajectory. Lacking feedback that is external to the robot, however, the robot will not stay on course indefinitely.

Figure 2e shows the length of the trajectory at the end of the trial as a function of the Kilobot Soft Robot's size S. For each size, we report on the top the number of failed trials. The mean value (blue solid line) shows that the trajectory length decreases, converging to the optimum value (green dotted line), as the robot size S increases. In other words, the motion of the Kilobot Soft Robot becomes more accurate, the more modules it has.

The performance improvement in terms of accuracy comes at the expense of a reduced average linear speed, as shown in Fig. 2f. Although accuracy tends to improve with robot size S, the speed seems to settle to a constant value of about 0.35 cm/s (successful trials only). Comparing the robot's speed with that of individual Kilobots (about 1.2 cm/s), we observe that the Kilobot Soft Robot moves at about 29% of the maximum speed of its constituent modules.

Finally, for each trial, we evaluated the extent to which the robots' shapes were distorted with respect to the reference shape, a square lattice. In the reference shape, all angles formed between adjacent links to neighbouring modules and internal to the robot have the same value of 90°. The root-mean-square distortion is computed as the mean squared error from 90° for all angles. Figure 2g reports the average of the root-mean-square distortion over time. We observe relatively small values of distortion in successful trials (green circles) for robots of any size. Large distortions are observed in the unsuccessful trials (red crosses). The latter indicate that a high proportion of elastic links ended up not being in the correct configuration, which may cause the robot to cease motion.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^b09550c3]. Annals of the American Thoracic Society (2023). High credibility.

Airway comparative analysis — matching approaches and caveats: Table 5 outlines selection criteria with example metrics and considerations, including designation of "Terminal bronchiole", designating a generation number for each airway, path distance (e.g., 5 cm from carina or pleura), lumen size (e.g., 2-mm-diameter airways), and airways at a stipulated inner perimeter with wall thickness of airways with inner perimeter of 10 mm (Pi10); considerations note pathology may alter airway status, anatomical variations in branching pattern, body and lung size variations, risk of bias when pathology alters airway dimension (e.g. COPD may narrow airway lumens; therefore, 2-mm airways may occur at a different generation compared with subjects without COPD), and that regression assumptions should be confirmed with potential effects of outliers assessed.

---

### Rigorous location of phase transitions in hard optimization problems [^d98ebea5]. Nature (2005). Excellent credibility.

It is widely believed that for many optimization problems, no algorithm is substantially more efficient than exhaustive search. This means that finding optimal solutions for many practical problems is completely beyond any current or projected computational capacity. To understand the origin of this extreme 'hardness', computer scientists, mathematicians and physicists have been investigating for two decades a connection between computational complexity and phase transitions in random instances of constraint satisfaction problems. Here we present a mathematically rigorous method for locating such phase transitions. Our method works by analysing the distribution of distances between pairs of solutions as constraints are added. By identifying critical behaviour in the evolution of this distribution, we can pinpoint the threshold location for a number of problems, including the two most-studied ones: random k-SAT and random graph colouring. Our results prove that the heuristic predictions of statistical physics in this context are essentially correct. Moreover, we establish that random instances of constraint satisfaction problems have solutions well beyond the reach of any analysed algorithm.

---

### Clustering knowledge and dispersing abilities enhances collective problem solving in a network [^a4ac9110]. Nature Communications (2019). High credibility.

NK spaces are analytically useful for agent-based models but are conceptually abstract and difficult to ground to real-world examples. Here, we can instead understand an NK problem's complexity in terms of how incremental contributions made by any subset of activities are also contingent on other activities. Thus, a solution that an agent comes up within the NK space — captured by a unique sequence of 1s and 0s — reflects a basket of activities people undertake to solve a real-world problem, where a 1 represents the presence of an activity and a 0 its absence.

There are many examples of social collectives solving a common problem that are analogous to the NK space setup. In these instances, the solutions and performances are visible at least some subset of other actors, and there are no positive or negative externalities from one actor to another (beyond the informational spillover). For instance, consider national ministries of health of various countries. They all have different approaches to healthcare (e.g. insurance markets like in Germany or Austria or a single-public option like in Canada or the U.K.). And the improvement or decline of one country's healthcare does not adversely affect or hamper the healthcare of another. The interest of some countries in the policies employed by other countries may be limited by physical adjacency or similar political systems or cultures, so their information about the "problem space" is more localized.

---

### Turn-key constrained parameter space exploration for particle accelerators using Bayesian active learning [^9cfab3cd]. Nature Communications (2021). High credibility.

Introduction

Particle accelerators have enabled ground-breaking discoveries in the fields of chemistry, biology, and physics. They are also increasingly deployed for societal applications, such as in medicalor industrialfields. During operation, accelerator parameters need to be tuned to produce beams with specific characteristics that match the needs for front-end applications. Measuring these beam properties as a function of one or more input parameters using limited diagnostics and time-consuming measurements is a necessary part of operations, experimental planning, and tolerance determination. This comes at the expense of reducing accelerator availability for experimenters. These challenges are shared by many different scientific fields, which try to characterize complex, highly nonlinear and correlated systems, using difficult to execute scientific measurements and complicated diagnostics.

Due to the complex and time-consuming nature of accelerator measurements, characterization of the beam response to input parameters is often limited to simple, uniformly spaced, grid-like parameter scans in one or two dimensions. This limitation results from the poor scaling of grid-like scans to higher-dimensional spaces, where the number of samples grows exponentially with the number of input parameters. Furthermore, despite its simplicity at face value, it is often difficult to determine the ideal properties of parametric scans which will result in successful and efficient sampling. A predefined grid spacing ultimately limits the ability to resolve fine features while potentially oversampling slow variations of the measured parameters. As a result, specifying the scan parameters a priori requires prior information about the measurement's functional dependence on each parameter. This slows down frequent routine studies and makes characterization of novel measurements difficult to execute successfully.

The existence of tight constraints in input space which determine if measurements are viable further complicates this process. Upper and lower input parameter limits are often determined by practical constraints of conducting measurements. For example, transverse beam size measurements on diagnostic screens are limited by the screen size (available field of view), which in turn, imposes limits on the strength of upstream focusing magnet parameters. Simulation studies or extra measurements are needed beforehand to determine these limits. Even when simulations are available, they do not necessarily represent realistic machine behavior. Measurements, on the other hand, may become inaccurate due to time dependent changes in the accelerator, further complicating this problem. Furthermore, while these limits can be easily determined for a single parameter experimentally, it becomes practically infeasible to efficiently determine limits in higher-dimensional input spaces, as they are often correlated with multiple parameters. Limitations such as these are shared among many types of scientific experiments.

---

### Conserved linear dynamics of single-molecule brownian motion [^a1816615]. Nature Communications (2017). Medium credibility.

Simulation of a single-molecule random walk

The simulated trajectories were constructed using a routine written in Matlab starting at (x, y) = (0,0). The random step sizes were generated using a distribution function (R) expected from the normal diffusion theory of a Brownian particle:

where r and Δ t denote the step size and the time lag, respectively. The step directions (angles between successive displacements) were generated based on random angles between 0 and 360°.

Probability of square lattice occupancy

We performed lattice occupancy analyses of the diffusion trajectories using a routine written in Matlab (Supplementary Data 1, Supplementary Note 2). The spatial positions obtained from the tracking algorithm were mapped onto a square lattice of side-length (m), which equalled the pixel size of the camera (m = 0.16 μm). The experimental probability was fitted to equation (4) to calculate the P 25 value.

Detrended fluctuation analysis

For the DFA to be statistically robust, we joined the single-molecule trajectories end-to-end. The combined trajectories of both the DNA and the nanospheres have an approximate length of N = 30,000 Δ t. The DFAwas performed using a routine written in Matlab (Supplementary Note 2). We generated integrated time series of P 25 (P 25 (k) (k = 1,2,…, N)), Z (i) by subtracting the mean P 25 value (〈 P 25 〉)5 and integrating the time series:

The profile Z (i) of length N was then divided into non-overlapping segments (s) of equal size (l). The local trend in each segment (Z l (i)) was calculated by subtracting the linear fit of the data:

---

### Methodologies for the development of CHEST guidelines and expert panel reports [^380f4748]. Chest (2014). Medium credibility.

Finalizing recommendations — recommendations are formulated by content experts informed by the evidence; cost considerations may be included only if published formal cost-benefit analyses are available, with resource consultants or health economists conducting analyses as needed, and when cost constraints exist implementers are encouraged to use ADAPTE strategies; patient values and preferences are reflected in recommendations or remarks and are especially pertinent in weaker recommendations when wide variability in choices is anticipated; recommendations must be specific and actionable with as much detail as evidence allows, and measure developers are cautioned not to convert lower-evidence recommendations into performance measures.

---

### An event-based architecture for solving constraint satisfaction problems [^f2e7014a]. Nature Communications (2015). Medium credibility.

Constraint satisfaction problems (CSPs) are a fundamental class of problems in computer science with wide applicability in areas such as channel coding, circuit optimizationand scheduling. Algorithms for solving CSPs are typically run on classical von Neumann computing platforms that were not explicitly designed for these types of problems. This paper addresses the question: how can we implement a more efficient computing substrate whose architecture and dynamics better reflect the distributed nature of CSPs?

Many dynamical systems that have been proposed for solving CSPs violate the 'physical implementability' condition. Non-physicality arises from the use of variables that can grow without bounds as the system is searching for solutions. On the other hand, there is a long, well-established tradition of studying physically realizable dynamical systems, for example, in the form of artificial neural networks, to solve CSPs or 'best-match problems'. Early attempts in this field used attractor networks, such as Hopfield networks, to solve NP hard (non-deterministic polynomial-time hard) problems such as the travelling salesman problem. These attractor networks, however, would often get stuck at locally optimal solutions. To overcome this problem, stochastic mechanisms were proposed, which require explicit sources of noise to force the network to continuously explore the solution space. While noise is an inextricable part of any physical system, dynamically controlling its power to balance 'exploratory' versus 'greedy' search, or to move the network from an exploratory phase to a greedy one according to an annealing schedule, is not a trivial operation and puts an additional overhead on the physical implementation.

---

### A continuous-time maxSAT solver with high analog performance [^3d122302]. Nature Communications (2018). Medium credibility.

Many real-life optimization problems can be formulated in Boolean logic as MaxSAT, a class of problems where the task is finding Boolean assignments to variables satisfying the maximum number of logical constraints. Since MaxSAT is NP-hard, no algorithm is known to efficiently solve these problems. Here we present a continuous-time analog solver for MaxSAT and show that the scaling of the escape rate, an invariant of the solver's dynamics, can predict the maximum number of satisfiable constraints, often well before finding the optimal assignment. Simulating the solver, we illustrate its performance on MaxSAT competition problems, then apply it to two-color Ramsey number R(m, m) problems. Although it finds colorings without monochromatic 5-cliques of complete graphs on N ≤ 42 vertices, the best coloring for N = 43 has two monochromatic 5-cliques, supporting the conjecture that R(5, 5) = 43. This approach shows the potential of continuous-time analog dynamical systems as algorithms for discrete optimization.

---

### Surveillance, case investigation and contact tracing for monkeypox [^aeb8b3fb]. Geneva: World Health Organization (2022). High credibility.

Regarding diagnostic investigations for mpox, more specifically with respect to initial evaluation, WHO 2022 guidelines recommend to investigate the suspected presence of similar illnesses in the patient's community or amongst contacts (also known as "backwards contact tracing").

---

### Surveillance, case investigation and contact tracing for monkeypox [^2648e206]. Geneva: World Health Organization (2022). High credibility.

Regarding follow-up and surveillance for mpox, more specifically with respect to contact tracing, WHO 2022 guidelines recommend to use the following indicators for monitoring the quality of mpox contact tracing:

- proportion of probable and confirmed cases with identified contacts

- number of contacts per probable and confirmed case

- proportion of contacts with complete follow-up information.

---

### To improve quality, leverage design [^4a59f7b2]. BMJ Quality & Safety (2022). High credibility.

The DD model

The DD model is organised into four major stages — discover, define, develop and deliver. The two diamonds represent distinct periods of thinking — first to fully characterise the problem (discover, define), and then to craft human-centred solutions (develop, deliver). In the discovery and define periods, teams work to understand a process through the eyes of the user, conducting interviews and observations with users to generate insights about their behaviours and unmet needs and ultimately producing a shared problem definition. In DT, problems are often reframed as 'How might we… ' statements that foster a creative exploration of the challenge. In the develop and deliver stages, teams leverage their prior insights about unmet human needs to craft innovative solutions that directly address those needs, cocreating with users and other stakeholders to develop and test solution prototypes in a rapid, iterative fashion to identify and learn from early failures.

The diamond shape itself represents a key design principle — 'diverge, then converge'. In the diverging stages, teams capture and display as many ideas as possible without judgement, creating a psychologically safe space for creative exploration as all ideas are welcome. Once teams have 'diverged' in their thinking, they apply a more rigorous lens and 'converge' on a shared problem definition or chosen solution.

---

### Symmetry breaking in optimal transport networks [^f4124a5b]. Nature Communications (2024). High credibility.

The main problem in network design is fundamentally different. We are given the density of population and we are looking for the network that minimizes some objective function involving some average time, in general (although other choices are possible, see for example). In this setting, there are usually two different transport modes, a slow one representing for example cars on the road network, and a fast one representing the subway or some rapid transit network. The natural framework here is then the one of multiplex networks comprising two different transportation networks, one known while the structure of the second one is to be determined (for multiplexes in the context of optimization see for example). A practical realization of this problem concerns the specific case of subways (for a network analysis of subways, see for example,–). In most large cities, a subway system has been built and later enlarged, with current total lengths varying from a few kilometers to a few hundred kilometers. The geometry of these networks, as its total length increases, varies from simple lines to more complex shapes with loops for larger networks. In particular, for the largest networks, convergence to a structure with a well-connected central core and branches reaching out to suburbs has been observed.

Algorithmic aspects of network design have been studied within computational geometry (e.g.chapter 9) and location science (e.g.and references therein), and some simpler problems of this type have been addressed previously. For instance, the problem of the quickest access between an area and a given point was discussed in. In network science, the optimization problem is traditionally recast as a navigation problem in lattices with long-range connections. However, our specific question – optimal network topologies as a function of population distribution and network length – is largely an open problem. In, some results were obtained in two-dimensional systems by comparing a priori defined optimal network configurations. First, it was shown that, if the goal is reaching a single point in the plane, then the optimal network is necessarily a tree. Second, the paper hinted at the possibility of the existence of transitions between optimal configurations when the length of the network changes. More precisely, it has been shown that as the length of the network increases resources go preferentially to radial branches and that there is a sharp transition at a critical value of the length where a loop appears.

---

### Random synaptic feedback weights support error backpropagation for deep learning [^3531c7f7]. Nature Communications (2016). Medium credibility.

Normalized squared error

We used a normalized squared error measure for regression problems where the units of error are not particularly meaningful. The loss for each model, in this case the sum of squared errors, was normalized by the sum of the squared error that one would achieve if using the sample mean as the model. This is the natural, albeit uncommon, normalization term for the sum of squared errors loss. The normalization term is thus

Here t indexes the batch, T is the length of the batch and i indexes the dimensions of the output. This measure generates learning curves that are almost always contained within the range [0,1], except in the case that a model 'blows up' and has worse error than when the learning is started.

A deep network that integrates its activity and spikes

As a proof of principle that feedback alignment can operate in a simple network architecture where forward and backward dynamics are simultaneous, we constructed a network of neurons that continuously integrate their activity (Fig. 4). In designing this model, we aimed for a level of detail that conveyed this essential point, while remaining simple enough to allow large-scale simulations. The consideration of scalability was particularly relevant as we aimed to demonstrate efficacy in a real-world learning task. The dynamics of the neural activities and synaptic plasticity operate simultaneously, but for clarity we describe them separately. Network and plasticity dynamics are similar in some respects to those developed by Urbanczik and Senn. The architecture and operation of the network are diagrammed in the Supplementary Information (Supplementary Figs 3 and 4).

---

### Acute lymphoblastic leukemia, version 2.2024, NCCN clinical practice guidelines in oncology [^b4467201]. Journal of the National Comprehensive Cancer Network (2024). High credibility.

Regarding follow-up and surveillance for acute lymphoblastic leukemia - NCCN, more specifically with respect to assessment of minimal residual disease, NCCN 2024 guidelines recommend to recognize that studies in children and adults with acute lymphoblastic leukemia have demonstrated a strong correlation between the presence of minimal residual disease during remission and risk for relapse, as well as the prognostic significance of minimal residual disease measurements after induction and consolidation therapy.

---

### Yoked surface codes [^9e0c45c6]. Nature Communications (2025). High credibility.

Yoked surface code patches change the length and number of shortest error paths. In 1D yoked surface codes, shortest error paths correspond to two shortest error paths in different surface code patches. This doubles the code distance, but introduces quadratic scaling in r, the number of rounds between yoke checks. Furthermore, since a logical failure can occur over any two patches in the code, the number of shortest error paths also scales quadratically in n. Based on this, we expect the scaling of the 1D yoked logical error rate to be, where λ 1 denotes the increased error suppression factor obtained from doubling the code distance, which we would expect to be at most. Note that the quadratic scaling versus r is only for numbers of rounds up to the yoke check period. Shortest paths from patches in different check periods don't combine to form a shortest error path in the concatenated code (although they can form an effective measurement error). Consequently, we expect the scaling versus rounds to be quadratic up to the check period, and then linear afterwards.

A similar argument holds for 2D yoked surface codes, for which the distance quadruples, but now the error scaling becomes quartic versus the number of rounds r between yoke checks, while remaining quadratic in the number of patches n. The quartic scaling versus r is similar: within the support of a weight four logical error, any combination of error paths within those patches can cause the error. The quadratic scaling versus n is because weight four logical errors correspond to four logical errors landing on the corners of a rectangle within the grid. There are Θ (n 2) of these rectangles specified by their endpoints across a diagonal. Based on this, we expect the scaling of the 2D yoked logical error rate to be, where λ 2 denotes the increased error suppression factor obtained from quadrupling the code distance, which we would expect to be at most.

We emphasize that these are simple path-counting heuristics, where we can empirically fit the different λ factors and coefficients. However, these can of course break down. For example in 2D, the set of higher-weight inner logical errors leading to failure could scale as o (r 4). Despite this, we observe good agreement between simulation and these heuristic scaling laws.

---

### Phase 0 / microdosing approaches: time for mainstream application in drug development? [^98d5620b]. Nature Reviews: Drug Discovery (2020). High credibility.

Phase 0 approaches - which include microdosing - evaluate subtherapeutic exposures of new drugs in first-in-human studies known as exploratory clinical trials. Recent progress extends phase 0 benefits beyond assessment of pharmacokinetics to include understanding of mechanism of action and pharmacodynamics. Phase 0 approaches have the potential to improve preclinical candidate selection and enable safer, cheaper, quicker and more informed developmental decisions. Here, we discuss phase 0 methods and applications, highlight their advantages over traditional strategies and address concerns related to extrapolation and developmental timelines. Although challenges remain, we propose that phase 0 approaches be at least considered for application in most drug development scenarios.

---

### Optimizing selectivity of the cerebellar cognitive affective syndrome scale by use of correction formulas, and validation of its German version [^ecb96f67]. Journal of Neurology (2025). Medium credibility.

Uncorrected total failed test items

Patients with cerebellar degeneration and patients with cerebellar stroke failed on average more test items than their respective controls (Cer-deg: 2.7 ± 2.1 vs. Con-deg: 1.4 ± 1.3, n = 176 each; Cer-str: 2.0 ± 1.9 vs. Con-str: 1.2 ± 1.4; n = 21 each). The difference between patients with cerebellar degeneration and their controls was significant [unpaired mean difference (MD): 1.3; 95% confidence interval (CI) lower bound, upper bound: 1.0, 1.7; p < 0.001, two-sided permutation t test; Fig. 3 A], while the comparison between patients with cerebellar stroke and their controls did not reach significance [MD: 0.8; CI: − 0.2, 1.8; p = 0.137; Fig. 3 C].

Fig. 3
Uncorrected total failed test items and total sum raw score. The total number of failed test items and the total sum raw score are shown for patients with cerebellar degeneration (orange; A, B) and patients with cerebellar stroke (orange; C, D) and their respective matched controls (blue; A – D). Each circle represents one participant. Error bars display means and standard deviations. Significant group comparisons are indicated by asterisks. n.s. not significant, SD standard deviation

Uncorrected total sum raw score

Patients with cerebellar degeneration and patients with cerebellar stroke had on average a lower total sum raw score than their respective controls (Cer-deg: 87.2 ± 13.9 vs. Con-deg: 99.2 ± 9.4; Cer-str: 93.6 ± 11.7 vs. Con-str: 99.3 ± 10.7). The difference between patients with cerebellar degeneration and their controls was significant [MD: − 12.0; − 14.5, − 9.5; p < 0.001, two-sided permutation t test; Fig. 3 B], whereas the difference between patients with cerebellar stroke and their controls was not [MD: − 5.7; CI: − 12.4, 0.7; p = 0.100; Fig. 3 D].

---

### Diagnosis, management and treatment of the Alport syndrome-2024 guideline on behalf of ERKNet, ERA and ESPN [^45eede36]. Nephrology, Dialysis, Transplantation (2025). High credibility.

Regarding diagnostic investigations for Alport syndrome, more specifically with respect to hearing assessment, ERA/ERN ERKNet/ESPN 2025 guidelines recommend to consider obtaining a hearing evaluation at diagnosis or upon reaching adulthood in female patients with X-linked alport syndrome, and then every 5 years in the absence of hearing loss symptoms.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^6517cbfd]. Annals of the American Thoracic Society (2023). High credibility.

Basic stereological measurements — first-order parameters and probes identify that "First-order parameters are volume (three-dimensional [3D]), surface (two-dimensional [2D]), length (one-dimensional [1D]), and number (zero-dimensional [0D])". Measurements "are performed using geometric test probes, such as points (0D), lines (1D), planes (2D), or volumes (3D)". These probes create countable events in the image, and "Raw counts provide ratios… that are multiplied by the reference space volume to obtain absolute measures for the lung or subcompartment".

---

### The problem with pay-for-performance schemes [^756a6421]. BMJ Quality & Safety (2019). High credibility.

'The Problem with… ' series covers controversial topics related to efforts to improve healthcare quality, including widely recommended but deceptively difficult strategies for improvement and pervasive problems that seem to resist solution.

---

### As-3 [^f4572ab3]. FDA (2025). Medium credibility.

Storage and handling

Store at room temperature (25 °C/77 °F). Avoid excessive heat.

Protect from freezing

---

### Clustering knowledge and dispersing abilities enhances collective problem solving in a network [^9f543c96]. Nature Communications (2019). High credibility.

The severity of this problem's complexity is not additive or linear in nature, but interdependent on all of the sub-problems that need to be solved for this to work. Furthermore, adopting a solution might have unintended, if not non-linear, consequences to the system's overall outcome that were not obvious or foreseeable. For instance, allowing students to use their mobile phones in schools might seem like a good idea to improve class engagement through other media, but an unintended consequence might be distraction or discouraging deeper critical thinking. The objective of the NK space is to capture the complex interaction among activities that yield performance. The NK problem space's popularity in modeling human decision-making stems from its verisimilitude with the complex and multidimensional problems that face problem-solving tasks, and because researchers can easily generate a large number of statistically similar problem spaces for robustness checks. (We note, however, in order to make sure our results are not an artifact of the idiosyncrasies of the NK problem space, we replicated all of our results in another rugged problem space, the Traveling Salesperson Problem — findings available in Supplemental Methods in Supplementary Fig. 2 through 9 and 11).

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^d5a733c3]. Annals of the American Thoracic Society (2023). High credibility.

Airway comparative analysis — matching strategies and considerations: Table 5 outlines selection criteria including anatomical property via designation of "Terminal bronchiole", branching hierarchy by "Designating a generation number for each airway", distance from a landmark using "Path distance (e.g., 5 cm from carina or pleura)", airway dimension via "Lumen size (e.g., 2-mm-diameter airways)", and airways of a stipulated inner perimeter using "Wall thickness of airways with inner perimeter of 10 mm (Pi10)". Considerations note that "Pathology may alter airway status", "There are anatomical variations in branching pattern; certain nomenclature systems do not cover the entire airway tree", and there is "Risk of bias when pathology alters airway dimension (e.g. COPD may narrow airway lumens; therefore, 2-mm airways may occur at a different generation compared with subjects without COPD)", and "Regression assumptions should be confirmed in the condition(s) under study; potential effects of outliers on regression equation should be assessed"; definitions specify "PiX = predicted wall thickness of airways with inner perimeter of X" and "Pi10 = predicted wall thickness of airways with inner perimeter of 10 mm".

---

### The backtracking survey propagation algorithm for solving random K-SAT problems [^f6ee7b94]. Nature Communications (2016). Medium credibility.

Discrete combinatorial optimization has a central role in many scientific disciplines, however, for hard problems we lack linear time algorithms that would allow us to solve very large instances. Moreover, it is still unclear what are the key features that make a discrete combinatorial optimization problem hard to solve. Here we study random K-satisfiability problems with K = 3,4, which are known to be very hard close to the SAT-UNSAT threshold, where problems stop having solutions. We show that the backtracking survey propagation algorithm, in a time practically linear in the problem size, is able to find solutions very close to the threshold, in a region unreachable by any other algorithm. All solutions found have no frozen variables, thus supporting the conjecture that only unfrozen solutions can be found in linear time, and that a problem becomes impossible to solve in linear time when all solutions contain frozen variables.

---

### A continuous-time maxSAT solver with high analog performance [^87d3d16d]. Nature Communications (2018). Medium credibility.

Algorithm description

Here, we give a simple, nonoptimized variant of the algorithm (see flowchart in Supplementary Fig. 2). Better implementations can be devised, for example with better fitting routines, however the description below is easier to follow and works well. Given a SAT problem, we first determine the b parameter as described previously. Step 1: initially we set, Γ min, = and t max. Unless specified otherwise, in our simulations we used Γ min = 100, Γ max = 2 × 10 6, t max = 50. Step 2: to initialize our statistics, we run Γ min trajectories up to t max, each from a random initial condition. For every such trajectory ω we update the p (E, t) distributions as function of the energies of the orthants visited by ω. We record the lowest energy value found. Step 3: starting from Γ = Γ min + 1 and up to Γ max, we continue running trajectories in the same way and for each one of them check: (a) If, set, update p (E, t) and go to Step 4. (b) If Γ just reached, go to Step 4. (c) If Γ = Γ max, output "Maximum number of steps reached, increase Γ max ", output the lowest energy value found, the predictedand the quality of fit for, then halt. Step 4: using the p (E, t) distributions, estimate the escape rates κ (E) as described in the corresponding Methods section. Step 5: the κ (E) curve is extrapolated to the E − 1 value obtaining κ (E − 1) and then using this we predict(as described in another Methods section). Further extrapolating the κ (E) curve to κ = 0 we obtain(see the corresponding Methods section). Step 6: we check the consistency of the prediction defined here as saturation of the predicted values. We call it consistent, ifhas not changed during the last 5 predictions. If it is not consistent yet, we continue running new trajectories (Step 4). If the prediction is consistent, we check for the following halting conditions: (i) Ifthen we decide the global optimum has been found:and skip to Step 7. (ii) If the fitting is consistently predicting(usually it is very close,) we check the number of trajectories that has attained states with, i.e. = . If it is large enough (e.g. > 100), we decide to stop running new trajectories and setand go to Step 7. (iii) Ifthen we most probably have not found the global optimum yet and we go to Step 4. We added additional stopping conditions that can shorten the algorithm in case of easy problems, see Methods corresponding section, but these are not so relevant. Step 7: the algorithm ends and outputs, values, the Boolean variables corresponding to the optimal state found, along with the quality of fit.

---

### Diminishing benefits of urban living for children and adolescents' growth and development [^80d194c8]. Nature (2023). Excellent credibility.

P is multiplied by the estimated precision parameters, and, thus upweighting or downweighting the strength of its penalties and ultimately determining the degree of smoothing at each level. For each of the four precision parameters, we used a truncated flat prior on the s.d. scale. We truncated these priors such that log ≤ 20 for each of the four. This upper bound is enforced as a computational convenience, whereby models with log > 20 are treated as equivalent to a model with log = 20 as they essentially have no extralinear variability in time. In practice, this upper bound had little effect on the parameter estimates. Furthermore, we ordered thea priori as follows: < < <. This prior constraint conveys the natural expectation that, for example, the global height or BMI trend has less extralinear variability than the trend of any given region, which in turn has less variability than those of constituent countries.

The matrixhas rank− 2, corresponding to a flat, improper prior on the mean and the slope of the's, the's and the's and, and is not invertible. Thus, we had a proper prior in a reduced-dimension space, with the prior expressed as follows:

Note that ifhad a non-zero mean, this would introduce non-identifiability with respect to. By the same token, would not be identifiable ifhad a non-zero time slope, and similarly for the other means and slopes. Thus, to achieve identifiability of the, and, we constrained the mean and slope ofand of each, andto be zero. Enforcing orthogonality between the linear and nonlinear portions of the time trends meant that each can be interpreted independently.

---

### Conserved linear dynamics of single-molecule brownian motion [^652431c5]. Nature Communications (2017). Medium credibility.

Mean-squared displacement analysis

MSD was calculated by using the following expression:

where x i+n and y i+n describe the spatial positions after time interval Δ t, given by the frame number, n, after starting at positions x i and y i. D is the diffusion constant. The theoretical MSDs at 1Δ t (MSD-1Δ t) and at 10Δ t (MSD-10Δ t) were calculated from the theoretical diffusion constant (D) by using equation (9). The theoretical D was calculated by using the experimental MSD value at 1Δ t and 10Δ t.

To generate a temporal profile of the absolute diffusion modes, we used the experimental replicates and the S r A r replicates to calculate the MSD-Δ t profile of a sliding window of width 50Δ t. The MSD-Δ t profiles (time lags between 1Δ t and 25Δ t) were fitted to the normal (equation (9)), directed(equation (10)), and confined(equation (11)) diffusion models.

where v is the drift velocity,

where L is the side length of the confined area and σ xy is the positional accuracy in x and y dimensions. The directed- and confined-like motions were quantified by v and L, respectively. Diffusion modes whose MSD-time profile fitted to a linear trend or those that showed extreme irregularities that could not be fitted by using equations (9)–(11), were considered with no specific diffusion mode. We set a side-length limit of L = 700 nm (equation (11)) to avoid mistakenly classifying the MSD-Δ t plots. This limit, which approximately equalled 4 R g, was empirically driven from the MSD-time profile and represented the maximum side-length that could describe confined-like motion of DNA within the 50Δ t window. When the fitting operation of the MSD-time profile to equation (S6) retrieved L larger than 700 nm, we considered the chi-square of the linear (equation (9)) and the directed (equation (10)) motion profiles.

---

### European Stroke Organisation (ESO) and European Association of Neurosurgical Societies (EANS) guideline on stroke due to spontaneous intracerebral haemorrhage [^a24b784e]. European Stroke Journal (2025). High credibility.

Regarding diagnostic investigations for intracerebral hemorrhage, more specifically with respect to initial evaluation, EANS/ESO 2025 guidelines recommend to consider using algorithms such as the DIAGRAM for targeted investigation of the cause of spontaneous ICH to improve the performance of prediction regarding the underlying cause, compared to standard care.

---

### Colorectal cancer screening and prevention [^390b568a]. American Family Physician (2025). High credibility.

Regarding screening and diagnosis for colon cancer, more specifically with respect to indications for screening, high-risk individuals, family history, AAFP 2025 guidelines recommend to obtain CRC screening in patients with ≥ 1 first-degree relatives with CRC or adenomatous polyps, starting at 40 years of age or 10 years before the age of the youngest relative at the time of their diagnosis.

---

### Summary benchmarks-full set – 2024 [^6f68d3ad]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — use and ethics note states that "The PPPs are intended to serve as guides in patient care, with greatest emphasis on technical aspects", that "true medical excellence is achieved only when skills are applied in a such a manner that the patients' needs are the foremost consideration", and that "The Academy is available to assist members in resolving ethical dilemmas that arise in the course of practice".

---

### Azacitidine [^9f2efca4]. FDA (2017). Low credibility.

The dosage of azacitidine SC for treatment of myelodysplastic syndromes in adults is:

- **Start at**: 75 mg/m² SC q24h for 7 days for the first cycle
- **Maintenance**: 75–100 mg/m² SC q24h for 7 days, repeated every 4 weeks, for at least 4–6 cycles

---

### Standards of care in diabetes – 2025 [^31be65a6]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for hypoglycemia, more specifically with respect to screening for cognitive impairment, ADA 2025 guidelines recommend to simplify diabetes treatment plans as much as possible for patients with cognitive impairment and tailor to minimize the risk of hypoglycemia.

---

### Clustering knowledge and dispersing abilities enhances collective problem solving in a network [^77613a31]. Nature Communications (2019). High credibility.

Here, however, we find quite different results: intermixing hampers systemic performance. Furthermore, there is a tradeoff in systemic performance over time, as seen in Fig. 6b. Networks with some form of intermixing perform better in the short-run. However, the network with the minimal possible intermixing performs worse in the short-run but best in the long-run. In other words, this suggests an "all or nothing" trade-off over time: the configuration with the least amount of intermixing possible (minimal) performs worse in the short-run, but better in the long-run, while any gradation of increased intermixing yields the same rank-ordering in performance as their counterpart networks in the diversity of ability simulations.

Agents in the intermixed network setups explore for solutions early on in the simulation, which is a double-edged sword. In the short-run, with more intermixing, agents quickly turn to exploitation, as they merely take solutions that are marginally better relative to other setups but are still mediocre in absolute terms. Said differently, agents in setups with at least some intermixing quickly coalesce to whatever the few agents that did explore the problem space found, which are often not the best solutions possible. So, exploring the problem space using worse solutions often leads to modest gains. However, the agents in setups with minimal intermixing are more commonly exploring, rather than exploiting, because their neighbors' initial solutions are less optimal than their counterparts in intermixed networks. This is because there is minimal exposure to diversity in these minimal setups, which inadvertently produces poor solutions in the short-run but allows for more exploration to find better solutions in the long-run. In other words, by finding better solutions through exploration, agents more often uncover pathways to better solutions earlier on. The end result is that the long-run performance of setups with minimal intermixing is best.

---

### Guideline no. 422d: menopause and sexuality [^3f837be9]. Journal of Obstetrics and Gynaecology Canada (2021). High credibility.

Regarding diagnostic investigations for menopause, more specifically with respect to evaluation of sexual dysfunction, SOGC 2021 guidelines recommend to categorize the patient's problem as related to desire, arousal, pain, or orgasm, in order to facilitate treatment and to triage care.

---

### Six steps in quality intervention development (6SQuID) [^221654f2]. Journal of Epidemiology and Community Health (2016). Low credibility.

Conclusion

In order to improve the effectiveness of public health interventions, a systematic approach to intervention development is required, as well as rigorous evaluation. However, little practical guidance exists for public health practitioners and researchers that explains the essential stages of intervention development. We argue that this process can be broken down into six key steps: defining and understanding the problem; identifying modifiable determinants; deciding on the mechanisms of change; clarifying how these will be delivered; testing and adapting the intervention; and collecting initial evidence of effectiveness. This model imposes somewhat arbitrary cut-offs in the process of intervention development and suggests a linear progression. In practice developers often return to an earlier step in the sequence before reaching step 6, and subsequently 'definitive trials' can lead to further revisions of the intervention. However, we hope that if each of these six steps is carefully addressed in the design of interventions better use will be made of scarce public resources by avoiding the costly evaluation, or implementation, of unpromising interventions.

What is already known on this subject?

There is little practical guidance for researchers or practitioners on how best to develop public health interventions. Existing models are generally orientated towards individual behaviour change and some are highly technical and take years to implement.

What this study adds?

This paper provides a pragmatic six-step guide to develop interventions in a logical, evidence-based way to maximise likely effectiveness. If each step is carefully addressed, better use will be made of scarce public resources by avoiding the costly evaluation, or implementation, of unpromising interventions.

---

### AGA clinical practice guideline on management of gastroparesis [^9e865be1]. Gastroenterology (2025). High credibility.

Regarding diagnostic investigations for gastroparesis, more specifically with respect to gastric emptying studies, methodology, AGA 2025 guidelines recommend to perform a 4-hour study in all cases, as some patients with normal emptying at 2 hours may be reclassified as delayed at 4 hours.

---

### Evaluation of thresholding methods for activation likelihood estimation meta-analysis via large-scale simulations [^e796624d]. Human Brain Mapping (2022). Medium credibility.

2.3 Outcome measures

We first compared the thresholding methods based on their sensitivity, which describes the method's power to find convergence when more than one study activates the target location. Sensitivity was quantified by the average rate of significant findings in a 4 mm radius around the "true location" over the 500 iterations of each parameter combination and should be high. The opposite outcome measure to sensitivity is the susceptibility to spurious convergence, which is measured by the average rate of significant clusters outside of a 4 mm radius around the "true location" over the 500 iterations of each parameter combination. Such clusters reflect the chance of incidental convergence under the known spatial independence of results, which should be as low as possible. The next outcome measure was cluster size, which is quantified by the average size of significant clusters that includes the location of true activation. Lastly, we looked at computational efficiency, measured by the time it takes for a single null permutation to be calculated for each thresholding methods, averaged over 50 permutations for each of the 30 dataset sizes we looked at.

---

### Joint testing of rare variant burden scores using non-negative least squares [^a3ab9846]. American Journal of Human Genetics (2024). Medium credibility.

We routinely use nested burden scores as this allows researchers to evaluate the effect of gradually increasing the size of the set of variants, i.e. starting with the most deleterious pLoF variants, then adding in missense variants with decreasing deleteriousness scores. Another possibility would be to use non-overlapping sets of variants, but users would not then be able to see the effect of gradually increasing the variant set and could miss signals found only by combining sets together.

We envision that the constrained linear regression approach can also be applied to other association tests of multiple variables: joint testing of multiple traitsand/or multiple SNPs. To enable these future extensions, we made an important practical contribution with approximate weight calculation in a mixture of chi-squared distributions for constrained linear regression, a problem not addressed in the previous theoretical works. The constrained test can also operate using association summary statistics and correlation matrix of tested variables rather than individual-level data. We derived this summary-statistics option (Equation 2) through reformulation of the constrained regression problem (Equation 1) in terms of joint effect sizes and the residual variance. The (unconstrained) joint effect estimates can be inferred from the marginal effect estimates using existing methods.

In summary, we developed a scalable multivariate version of the one-sided test with application to testing multiple burden scores. The SBAT method is available in the REGENIE software and together with other gene-based tests will empower further discovery from sequencing association studies.

---

### Elacestrant (Orserdu) [^25e5bc7d]. FDA (2024). Medium credibility.

The dosage of elacestrant PO for treatment of breast cancer in postmenopausal female adults with ESR1 mutation (advanced or metastatic, ER-positive, HER2-negative, progression after ≥ 1 line of endocrine therapy) is 345 mg PO daily until disease progression or unacceptable toxicity

---

### Acute lymphoblastic leukemia, version 2.2024, NCCN clinical practice guidelines in oncology [^b0575bfd]. Journal of the National Comprehensive Cancer Network (2024). High credibility.

Regarding follow-up and surveillance for acute lymphoblastic leukemia - NCCN, more specifically with respect to follow-up, NCCN 2024 guidelines recommend to obtain periodic BCR::ABL1 transcript-specific quantification in patients with Philadelphia chromosome-positive acute lymphoblastic leukemia.

---

### Standards of care in diabetes – 2025 [^ef149bc3]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for diabetic nephropathy, more specifically with respect to pediatric patients, ADA 2025 guidelines recommend to determine the eGFR at the time of diagnosis and annually thereafter.

---

### Managing menopause [^0a83dde9]. Journal of Obstetrics and Gynaecology Canada (2014). Medium credibility.

Regarding diagnostic investigations for menopause, more specifically with respect to evaluation of sexual dysfunction, SOGC 2014 guidelines recommend to categorize the problem according to desire, arousal, pain, or orgasm problems in order to facilitate treatment and triage care.

---

### A user guide to the American Society of Hematology clinical practice guidelines [^51218549]. Blood Advances (2020). High credibility.

Isof table — components and use are illustrated as an example of an iSoF table can be used to explore an intervention's effect on health outcomes in alternative ways, and it includes the question, outcomes, relative and absolute effects, and interpretations.

---

### Local structure-function relationships in human brain networks across the lifespan [^65711e92]. Nature Communications (2022). High credibility.

Fig. 2
Explaining regional FC patterns with dynamic, topological, and geometric factors.

a Distributions of variance explained by different factors. Points represent brain regions and the mean variance explained across the entire cohort. Each point represents a subject (N s u b = 70 subjects following exclusion for motion and data quality; FC and SC averaged over two scan sessions). In each boxplot, the "box" denotes interquartile range (IQR), the horizontal bar indicates the median value, and the whiskers include points that are within 1.5 × IQR of upper and lower bounds of the IQR (25th and 7th percentiles). Any points that fall beyond the whiskers are, by convention, considered outliers. b Spatial embedding of predictors. The coordinates were determined using the following procedure: 1) calculating the similarity of regional correlations for every pair of predictors, 2) thresholding this matrix to retain the k = 4 nearest neighbors for each predictor, and 3) performing a principal component analysis of the thresholded and symmetrized matrix. Coordinates represent the first two principal components, PC1 and PC2. Broadly, predictors that are near/distant to one another in principal component space exhibit similar/dissimilar patterns of regional correlations. In this plot, edges exist between predictors if they are nearest neighbors. Note that here the principal component analysis was carried out without z-scoring or centering the columns of the correlation matrix. The size of points is proportional to the mean variance explained across all brain regions. Source data are provided as a Source Data file.

---

### Data-driven grading of acute graft-versus-host disease [^340b1045]. Nature Communications (2023). High credibility.

Fig. 4
Hierarchical and partitional clustering of the training cohort (n = 2319) as alternative data-driven approaches to aGVHD grading and multivariate competing-risk-regression of the validation cohort (n = 700) with the PC1 grading.

a Agglomerative hierarchical clustering (HClust) dendrogram of the training cohort on the basis of their target organ involvements. An HClust distance threshold of 30 split the cohort into 4 clusters, which were numerically ordered. Grade I: n = 763; II: n = 1149, III: n = 191, IV: n = 216. Red dashed line indicates cutoff level for four grades. b Kaplan–Meier OS curve with 95% CI of 4 HClust-aGVHD grades (I–IV). Strata are compared with the two-sided log-rank test. c K-means partitional clustering performance indicators SSD (sum of squared distances, green dashed) and silhouette coefficient (green), labels on each figure side. Red dashed line indicates cutoff level with four grades; gray dashed line shows cutoff level with 8 grades, the optimal number determined by both methods (n = 8, Sil = 0.62). We evaluated a further cutoff point with 14 clusters in the supplementary notes. d Kaplan–Meier OS curve with 95% CI of K-means-4 grades (I–IV). Strata are compared with the two-sided log-rank test. e Multivariate competing risk regression analysis for 12 months NRM on the test cohort (n = 541 evaluable for all covariates) using the PC1 aGVHD grades as a time-dependent variable. The multivariate model was adjusted for potentially confounding variables, covariates as listed in e. Horizontal bars represent 95% CI. P -values are computed based on the Wald-test. The hazard ratio (HR) is a measure of the ratio of the hazard between two groups. A value of 1 is the reference, HR < 1 corresponds to lower risk and HR > 1 to higher risk of NRM than the reference. The HR of PC1 grade II was 2.12 (95% confidence interval, CI, 1.17–3.83, grade III HR 7.2 (95%CI 4.72–10.99) and grade IV HR 16.30 (95%CI 8.12–32.75). Significant covariates in this NRM model were diagnoses (acute lymphoblastic leukemia (ALL) HR 2.3 (95%CI 1.17–4.66), myelodysplastic syndromes (MDS) HR 1.74 (95%CI 1.06–2.84), other diagnoses HR 3.8 (95% CI 1.27–11.88), year of HCT HR 0.91, 95% CI 0.85–0.97, and EBMT risk score HR 1.40, 95%CI 1.21–1.63. The covariates, donor age, donor sex, donor type, Karnofsky performance index ≥ 80 were not significant in univariate regression analysis and hence not included in the multivariate model. Source data are provided as a source data file.

---

### Bumetanide [^d0437f55]. FDA (2025). Medium credibility.

WARNING

Bumetanide injection is a potent diuretic which, if given in excessive amounts, can lead to a profound diuresis with water and electrolyte depletion. Therefore, careful medical supervision is required, and dose and dosage schedule have to be adjusted to the individual patient's needs [see Dosage and Administration].

---

### Standards of care in diabetes – 2025 [^5eb43e21]. Diabetes Care (2025). High credibility.

Regarding follow-up and surveillance for diabetes mellitus type 1, more specifically with respect to continuous glucose monitoring, ADA 2025 guidelines recommend to implement early continuous glucose monitoring in adult patients with T1DM to improve glycemic outcomes and QoL and minimize hypoglycemia.

---

### Surveillance, case investigation and contact tracing for monkeypox [^c60202f9]. Geneva: World Health Organization (2022). High credibility.

Regarding follow-up and surveillance for mpox, more specifically with respect to contact tracing, WHO 2022 guidelines recommend to define a contact as a person having ≥ 1 of the following exposures with a probable or confirmed case of mpox in the period beginning with the onset of the source case's first symptoms and ending when all scabs have fallen off:

- face-to-face exposure (including health workers without appropriate personal protective equipment)

- direct physical contact, including sexual contact

- contact with contaminated materials, such as clothing or bedding.

---

### Initial diagnostic workup of acute leukemia: guideline from the college of American pathologists and the American Society of Hematology [^f6113ea7]. Archives of Pathology & Laboratory Medicine (2017). Medium credibility.

Regarding classification and risk stratification for acute lymphoblastic leukemia, more specifically with respect to classification, ASH/CAP 2017 guidelines recommend to use the current WHO terminology for the final diagnosis and classification of acute leukemia.

---

### Ph-positive acute lymphoblastic leukemia-25 years of progress [^600234fb]. The New England Journal of Medicine (2025). Excellent credibility.

The following constitutes key background information on acute lymphoblastic leukemia:

- **Definition**: ALL is a hematological malignancy that originates from B- or T-lymphoid progenitor cells and is characterized by the abnormal proliferation and accumulation of immature lymphoid cells within the bone marrow and lymphoid tissues.
- **Pathophysiology**: The pathophysiology of ALL is complex and involves a variety of genetic abnormalities that disrupt normal cell maturation and promote the uncontrolled proliferation of immature lymphoid cells.
- **Epidemiology**: The incidence of ALL is estimated at 1.8 per 100,000 person-years in the US and 2.75 per 100,000 person-years in Europe.
- **Disease course**: ALL typically presents with symptoms such as fever, pallor, bleeding, lymphadenopathy, and hepatosplenomegaly. Patients may also have lymphoblasts in the peripheral blood and bone marrow. In some cases, musculoskeletal manifestations may be the only initial presenting symptom.
- **Prognosis and risk of recurrence**: The prognosis is based on age at diagnosis, WBC count at presentation, and the presence of certain genetic changes. The outcome for older adults (≥ 40 years) and patients with relapsed or refractory ALL remains poor.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^7e0b2d1d]. Annals of the American Thoracic Society (2023). High credibility.

Basic stereological measurements for lung imaging — first-order parameters, probes, and reference space are specified. First-order parameters are volume (three-dimensional [3D]), surface (two-dimensional [2D]), length (one-dimensional [1D]), and number (zero-dimensional [0D]). Measurements are performed using geometric test probes, such as points (0D), lines (1D), planes (2D), or volumes (3D), with probe–voxel interactions generating countable events; raw counts yield ratios that are multiplied by the reference space volume to obtain absolute lung or subcompartment measures. It is crucial to define and measure a biologically meaningful reference space for analysis and reporting, because measurements expressed only as ratios are subject to "the reference trap", where changes can arise from the numerator, denominator, or both; an efficient volume measurement method, point counting, is used for acinar components by micro-CT and for extrapulmonary organ volume by CT.

---

### Comparing accuracies of length-type geographic atrophy growth rate metrics using atrophy-front growth modeling [^fbef3263]. Ophthalmology Science (2022). Medium credibility.

Purpose

To compare the accuracies of the previously proposed square-root-transformed and perimeter-adjusted metrics for estimating length-type geographic atrophy (GA) growth rates.

Design

Cross-sectional and simulation-based study.

Participants

Thirty-eight eyes with GA from 27 patients.

Methods

We used a previously developed atrophy-front growth model to provide analytical and numerical evaluations of the square-root-transformed and perimeter-adjusted growth rate metrics on simulated and semisimulated GA growth data.

Main Outcome Measures

Comparison of the accuracies of the square-root-transformed and perimeter-adjusted metrics on simulated and semisimulated GA growth data.

Results

Analytical and numerical evaluations showed that the accuracy of the perimeter-adjusted metric is affected minimally by baseline lesion area, focality, and circularity over a wide range of GA growth rates. Average absolute errors of the perimeter-adjusted metric were approximately 20 times lower than those of the square-root-transformed metrics, per evaluation on a semisimulated dataset with growth rate characteristics matching clinically observed data.

Conclusions

Length-type growth rates have an intuitive, biophysical interpretation that is independent of lesion geometry, which supports their use in clinical trials of GA therapeutics. Taken in the context of prior studies, our analyses suggest that length-type GA growth rates should be measured using the perimeter-adjusted metric, rather than square-root-transformed metrics.

---

### Correlative single molecule lattice light sheet imaging reveals the dynamic relationship between nucleosomes and the local chromatin environment [^3337ac63]. Nature Communications (2024). High credibility.

Pair correlation function calculation and fractal dimension estimation

The normalized pair correlation function (G(r)) was calculated in a similar approach as described in, where

Here N is the total number of localizations in the actual data, and N R is the total number of localizations randomly distributed with the same density as the actual data. DD(r) is the average number of pairs of objects with separation r in the actual data, and RR(r) is the average number of pairs of objects with separation r in the randomly distributed data. To account for diminished localization detection efficiency away from the focal plane, we adopted an approached similar to. Briefly, for each cell, we plotted a histogram of the number of localizations found at each z-plane. We fit this histogram to a Gaussian function and then scaled the number of randomly distributed points in 3D space by this same Gaussian function. In this manner, the spatially random points were subject to the same axial sampling bias as our experimental measurements.

Based on the G(r) curve, we performed a linear regression on to log(G(r)) and log(r) to extract the decaying exponent γ of G(r). The lower bound of the linear regression was set at 35 nm. The upper bound of the linear regression was determined through iteratively increasing the maximum value until the r 2 value of the fit fell below 0.98 ranging from the 35 nm to an upper value that was determined by a r 2 value of 0.98. The slope of this regression represents γ and decays approximately as

The fractal dimension is calculated as

Extracted parameters were only excluded from downstream analysis if the r 2 value of the linear fit was > 0.95. (Fig. 3K inset).

Whole cell G(r) analysis

For each r -value in our G(r) plot, spanning from 75 nm to 505 nm, we fit two linear regressions. The first regression used values from 75 nm to the r -value, while the second regression covered values from the r -value up to 505 nm. We then calculated the summed squared residual for both regressions across the respective ranges. The optimal hinge point was selected based on the r -value that produced the smallest combined summed squared residual. The estimated fractal dimension was derived from the slopes of these regressions, consistent with previously detailed methods (Supplementary Fig. 4D).

---

### Statistical consideration for research [^048a38a2]. Emergency Medicine Journal (2003). Low credibility.

The seventh paper in this series discusses the importance of statistical techniques in research.

---

### Colorectal cancer screening and prevention [^c270f773]. American Family Physician (2025). High credibility.

Regarding screening and diagnosis for colon cancer, more specifically with respect to indications for screening, general population, aged 76–85 years, AAFP 2025 guidelines recommend to consider obtaining screening for CRC in adults aged 76–85 years at average risk based on overall health status, prior screening history, and patient preferences.

---

### How to write a problem solving in clinical practice paper [^a300cd92]. Archives of Disease in Childhood: Education and Practice Edition (2013). Low credibility.

An overview of the concept of problem solving in clinical practice, and how to go about writing an article to be submitted.

---

### Finding shortest and nearly shortest path nodes in large substantially incomplete networks by hyperbolic mapping [^5a5c615c]. Nature Communications (2023). High credibility.

Discussion

There is no one-size-fits-all solution to the shortest path problem. In order to identify shortest path nodes in a partially known network, one needs to know both the mechanisms of network formation and the character of missing data. Distance to geodesic, in this respect, assumes that link formation in the network is captured by its latent geometry, and unknown links are missing uniformly at random.

The first condition is a must: one cannot expect to identify shortest paths in non-geometric networks using geometric methods. More than that, the embedding space must agree with network topology, and learned embeddings must be of sufficiently high accuracy. How do we know if the network of interest has effective hyperbolic geometry? It is well known that spatial network models built in hyperbolic spaces are sparse, self-similar, have strong clustering coefficients, and are characterized by scale-free degree distributions. These topological properties can be regarded as necessary conditions of network hyperbolic geometricity. Many real networks, including the ones in this study, have been shown to meet these necessary conditions. Whether the same topological properties are also sufficient conditions of network hyperbolic geometricity is an open research problem. The first result in this direction is the equivalence between network ensembles with fixed clustering and expected degree and random geometric graphs on a straight line. The extension of this equivalence result to random geometric graphs in hyperbolic spaces is not easy.

---

### An examination, with a meta-analysis, of studies of childhood leukaemia in relation to population mixing [^50b58e3b]. British Journal of Cancer (2012). Low credibility.

Table 2 shows CL data below age 2 years separately from other ages. No appreciable difference is evident in the CL excess at 0 and 1 year (RR 1.51; 1.17, 1.92) from that in later childhood age groups.

---

### Standards of care in diabetes – 2025 [^9fec4de9]. Diabetes Care (2025). High credibility.

Regarding follow-up and surveillance for diabetes mellitus type 1, more specifically with respect to continuous glucose monitoring, ADA 2025 guidelines recommend to assess skin reactions, either due to irritation or allergy, and address to aid in successful use of devices.

---

### ESTES guidelines: acute mesenteric ischaemia [^f0f5bc7b]. European Journal of Trauma and Emergency Surgery (2016). Medium credibility.

Regarding follow-up and surveillance for acute mesenteric ischemia, more specifically with respect to second-look procedures, ESTES 2016 guidelines recommend to reassess ischemic bowel after adequate fluid resuscitation and revascularization. Perform a second-look procedure to assess the viability of the bowel if any doubt remains.

---

### Report of acute lymphoblastic leukemia in two sisters and successful treatment of both [^7097f5c3]. Pediatric Hematology and Oncology (2003). Low credibility.

The authors report on 2 sisters with acute lymphoblastic leukemia diagnosed at age 30 months and 3.5 months. The younger one was referred 15 months after the first. The reason for reporting this case is family risk and also infant under 12 months, especially under 6 months, of age have extremely poor prognosis, but both sister are at present time on complete remission.

---

### Ankle stability and movement coordination impairments: ankle ligament sprains [^10f2910a]. The Journal of Orthopaedic and Sports Physical Therapy (2013). Medium credibility.

Ankle ligament sprain — square hop assesses jumping activity limitation (ICF category) as "the amount of time needed to hop in and out of a 40 x 40-cm square in a clockwise or counterclockwise direction… 5 times around the square". The measurement method specifies "a square 40 x 40 cm is marked on the floor with tape", starting outside the square and hopping "as fast as possible… then hop out the side of the square that is clockwise if the right limb is being tested", with left-limb exit instructions beginning on this page.

---

### Acute lymphoblastic leukaemia in adult patients: ESMO clinical practice guidelines for diagnosis, treatment and follow-up [^3f7d5aec]. Annals of Oncology (2016). Medium credibility.

Regarding classification and risk stratification for acute lymphoblastic leukemia, more specifically with respect to risk assessment, ESMO 2016 guidelines recommend to obtain risk stratification using a combination of prognostic factors at diagnosis and treatment-related parameters, preferentially minimal residual disease. Stratify patients as standard-risk or high-risk.

---

### Diagnosis and classification of diabetes: standards of care in diabetes – 2025 [^568cafd9]. Diabetes Care (2025). High credibility.

Type 1 diabetes natural history and progression risk indicate that stage 1, defined by the presence of two or more autoantibodies and normoglycemia, carries a 5-year risk of developing symptomatic type 1 diabetes of ~44%. In stage 2, risk is ~60% by 2 years and ~75% within 5 years of developing a clinical diagnosis of type 1 diabetes. A consensus group provides expert recommendations on what should be monitored and how often these factors should be monitored in individuals with presymptomatic type 1 diabetes.

---

### Summary benchmarks-full set – 2024 [^c9abe316]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — GRADE evidence quality ratings are defined for forming recommendations for care as follows: "Good quality (GQ): Further research is very unlikely to change our confidence in the estimate of effect", "Moderate quality (MQ): Further research is likely to have an important impact on our confidence in the estimate of effect and may change the estimate", and "Insufficient quality (IQ): Further research is very likely to have an important impact on our confidence in the estimate of effect and is likely to change the estimate; any estimate of effect is very uncertain".

---

### A comparison of phase II study strategies [^ebb760ae]. Clinical Cancer Research (2009). Low credibility.

The traditional oncology drug development paradigm of single arm phase II studies followed by a randomized phase III study has limitations for modern oncology drug development. Interpretation of single arm phase II study results is difficult when a new drug is used in combination with other agents or when progression-free survival is used as the endpoint rather than tumor shrinkage. Randomized phase II studies are more informative for these objectives but increase both the number of patients and time required to determine the value of a new experimental agent. In this article, we compare different phase II study strategies to determine the most efficient drug development path in terms of number of patients and length of time to conclusion of drug efficacy on overall survival.

---

### Addressing early childhood emotional and behavioral problems [^0988ef83]. Pediatrics (2016). Medium credibility.

Examples of evidence-based treatments for existing diagnoses in young children — This report focuses on programs that target current diagnoses or clear clinical problems (rather than risk) in infants and toddlers.

---

### ESICM / ESCMID task force on practical management of invasive candidiasis in critically ill patients [^551f3d5b]. Intensive Care Medicine (2019). High credibility.

Regarding medical management for invasive candidiasis, more specifically with respect to step-down therapy, ESCMID/ESICM 2019 guidelines recommend to discontinue antifungal therapy in patients with suspected (but not proven) invasive candidiasis with negative blood cultures and/or other negative culture specimens taken from suspected infectious foci before starting antifungal therapy.

---

### 2022 ESC guidelines for the management of patients with ventricular arrhythmias and the prevention of sudden cardiac death [^9a594a67]. European Heart Journal (2022). High credibility.

Regarding medical management for myocarditis, more specifically with respect to management of cardiac arrhythmias, catheter ablation, ESC 2022 guidelines recommend to consider performing catheter ablation as an alternative to ICD therapy, after discussion with the patient and provided that established endpoints have been reached, in patients with hemodynamically well-tolerated sustained monomorphic VT occurring in the chronic phase of myocarditis, preserved LV function, and a limited scar amenable to ablation.

---

### Pediatric acute lymphoblastic leukemia, version 2.2025, NCCN clinical practice guidelines in oncology [^741bdb67]. Journal of the National Comprehensive Cancer Network (2025). High credibility.

Response in bone marrow and peripheral blood — complete response (CR) in pediatric acute lymphoblastic leukemia (ALL) requires absence of circulating blasts and extramedullary disease, bone marrow with trilineage hematopoiesis and fewer than 5% blasts (M1) or < 1% blasts by flow or molecular testing, absolute neutrophil counts (ANCs) > 1000/μL and platelet counts > 100,000/μL, and no recurrence for at least 4 weeks. A complete response with incomplete count recovery (CRi) applies when CR criteria are met except the ANC remains < 1000/μL or the platelet count remains < 100,000/μL. In general, ORR is the sum of CR and CRi values, and MRD assessment is not included in morphologic assessment and should be obtained.

---

### 2024 ESC guidelines for the management of peripheral arterial and aortic diseases [^544225cc]. European Heart Journal (2024). High credibility.

Regarding surgical interventions for Marfan syndrome, more specifically with respect to aortic aneurysm repair, ESC 2024 guidelines recommend to perform surgery in patients with MS with aortic root disease with a maximal aortic sinus diameter ≥ 50 mm.

---

### Standards of care in diabetes – 2025 [^94855997]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for hypoglycemia, more specifically with respect to pediatric patients (glycemic targets, T1DM), ADA 2025 guidelines recommend to consider setting even less stringent HbA1c goals (such as < 8%; < 64 mmol/mol) in patients with a history of severe hypoglycemia, limited life expectancy, or where the harms of treatment are greater than the benefits.

---

### Age-related macular degeneration preferred practice pattern ® [^50d0f0cd]. Ophthalmology (2025). High credibility.

Regarding classification and risk stratification for age-related macular degeneration, more specifically with respect to risk factors (other factors), AAO 2025 guidelines recommend to recognize that an increased waist/hip ratio for men is associated with an increased risk of both early and late AMD. Markers of inflammation, such as CRP, may be associated with a higher risk of AMD progression. Other possible factors with inconclusive findings include: hormonal status; sunlight exposure; alcohol use; vitamins B and D status. A Cochrane systematic review in 2016 concluded insufficient evidence to define a role of statins in AMD onset or progression.

---

### Diagnosis, management and treatment of the Alport syndrome-2024 guideline on behalf of ERKNet, ERA and ESPN [^00ae4ba3]. Nephrology, Dialysis, Transplantation (2025). High credibility.

Regarding diagnostic investigations for Alport syndrome, more specifically with respect to genetic testing, ERA/ERN ERKNet/ESPN 2025 guidelines recommend to include COL4A3/4/5 gene testing when obtaining genetic testing to investigate proteinuric kidney disease, unexplained kidney failure, or possible cystic kidney disease.

---

### Standards of care in diabetes – 2025 [^d4969f8c]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for diabetes mellitus type 1, more specifically with respect to screening for PAD, ADA 2025 guidelines recommend to assess lower extremity pulses, capillary refill time, rubor on dependency, pallor on elevation, and venous filling time for initial screening of PAD. Obtain ankle-brachial index with toe pressures and further vascular assessment as appropriate in patients with symptoms of claudication or decreased or absent pedal pulses.

---

### ESTES guidelines: acute mesenteric ischaemia [^d3170bfe]. European Journal of Trauma and Emergency Surgery (2016). Medium credibility.

Regarding follow-up and surveillance for acute mesenteric ischemia, more specifically with respect to second-look procedures, ESTES 2016 guidelines recommend to perform exteriorization of the bowel if there is still doubt about its viability during the second-look procedure.

---

### Guideline no. 422d: menopause and sexuality [^5e20690c]. Journal of Obstetrics and Gynaecology Canada (2021). High credibility.

Regarding diagnostic investigations for vulvovaginal atrophy, more specifically with respect to evaluation of sexual dysfunction, SOGC 2021 guidelines recommend to categorize the patient's problem as related to desire, arousal, pain, or orgasm, in order to facilitate treatment and to triage care.

---

### Surveillance, case investigation and contact tracing for monkeypox [^37d66d41]. Geneva: World Health Organization (2022). High credibility.

Regarding follow-up and surveillance for mpox, more specifically with respect to contact tracing, WHO 2022 guidelines recommend to consider prompting case-patients to identify contacts across a number of contexts, including household, workplace, school/nursery, sexual contacts, healthcare (including laboratory exposure), houses of worship, transportation, sports, bars/restaurants, social gatherings, festivals, and any other recalled interactions. Consider using attendance lists, and passenger manifests, further to identify contacts.