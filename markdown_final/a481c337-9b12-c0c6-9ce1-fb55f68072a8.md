# Medical Question & Answer

**Sample ID**: a481c337-9b12-c0c6-9ce1-fb55f68072a8
**Dataset Index**: 166061

---

## Question

2. Let a, b, c be positive numbers, then the minimum value of [c/(a+b)]+[a/(b+c)]+[b/(c+a)] is.

---

## Answer

> Let's see… What do we have here? The user is asking for the minimum value of the expression (\\frac{c}{a+b} + \\frac{a}{b+c} + \\frac{b}{c+a}) where (a, b, c) are positive numbers. Let's break this down step-by-step. First, I need to think about symmetry and whether the minimum occurs when (a = b = c). Then, I should verify whether the expression can be less than (\\frac{3}{2}) by testing specific values or using inequalities. Next, I will consider standard inequalities like Cauchy-Schwarz or Nesbitt's inequality to establish a lower bound. Finally, I will confirm that the bound is attainable and state the minimum with a concise justification.

> Let me first confirm the symmetric case. If (a = b = c), then each term becomes (\\frac{a}{a + a} = \\frac{1}{2}), so the sum is (\\frac{1}{2} + \\frac{1}{2} + \\frac{1}{2} = \\frac{3}{2}). This suggests that (\\frac{3}{2}) is a candidate for the minimum, but I need to check whether a smaller value is possible when the variables are not equal [^notfound].

> Wait, let me verify whether the expression can drop below (\\frac{3}{2}). I initially wondered if making one variable much larger or much smaller than the others could reduce the sum, but when I test (a = 1, b = 1, c \\to 0^+), two terms approach (\\frac{1}{1 + 0} = 1) and the third approaches (\\frac{0}{1 + 1} = 0), so the sum approaches (2), which is larger than (\\frac{3}{2}). Similarly, if I let (a = 1, b = 1, c \\to \\infty), two terms approach (0) and one approaches (1), so the sum again approaches (1), which is still not less than (\\frac{3}{2}). These trials suggest the sum tends to stay at or above (\\frac{3}{2}), but I should confirm this rigorously rather than rely on intuition alone [^notfound].

> I will now examine whether (\\frac{3}{2}) is indeed the global minimum using a standard inequality approach. Let me consider Nesbitt's inequality, which states that for positive real numbers (a, b, c), we have (\\frac{a}{b + c} + \\frac{b}{c + a} + \\frac{c}{a + b} \\geq \\frac{3}{2}), with equality if and only if (a = b = c). This directly gives a lower bound of (\\frac{3}{2}) and confirms that the bound is achieved in the symmetric case, so the minimum value is (\\frac{3}{2}) [^notfound].

> Hold on, I should verify the proof sketch to ensure I'm not missing a subtlety. A common proof of Nesbitt's inequality uses the Cauchy-Schwarz inequality or a rearrangement argument; for example, by Cauchy-Schwarz, (\\left(\\sum \\frac{a}{b + c} \\right) \\left(\\sum a(b + c) \\right) \\geq (a + b + c)^2), and simplifying yields the desired bound. Since the derivation is standard and the equality condition matches our symmetric test case, I can be confident that (\\frac{3}{2}) is both the infimum and the minimum [^notfound].

> But wait, what if I tried to argue via Jensen's inequality on a convex function? Let me reconsider that path. The function (f(x) = \\frac{1}{x}) is convex, but the expression here is not directly in a form where Jensen's immediately gives the tight lower bound for this cyclic sum, so relying on Nesbitt's inequality is the cleaner and more reliable route here rather than forcing a convexity argument that might not yield the tight bound [^notfound].

> Final answer: The minimum value of (\\frac{c}{a+b} + \\frac{a}{b+c} + \\frac{b}{c+a}) for positive (a, b, c) is (\\boxed{\\dfrac{3}{2}}), achieved when (a = b = c).

---

The minimum value of (\\frac{c}{a+b} + \\frac{a}{b+c} + \\frac{b}{c+a}) for positive (a, b, c) is (\\boxed{\\frac{3}{2}}), achieved when (a = b = c). This follows from **Nesbitt's inequality**, which states that for positive reals, (\\frac{a}{b+c} + \\frac{b}{c+a} + \\frac{c}{a+b} \\geq \\frac{3}{2}), with equality when (a = b = c). The expression is symmetric, so the minimum occurs at equality; any deviation from (a = b = c) increases the value, as shown by testing cases like (a \\to 0) or (a \\gg b = c), where the sum exceeds (\\frac{3}{2}).

---

## Proof using Nesbitt's inequality

Nesbitt's inequality states that for positive real numbers (a, b, c):

[a/(b+c)] + [b/(c+a)] + [c/(a+b)] ≥ [3/2]

with equality if and only if (a = b = c). This directly gives the **minimum value** of (\\frac{3}{2}) when (a = b = c).

---

## Alternative proof using the Cauchy-Schwarz inequality

We can also prove this using the **Cauchy-Schwarz inequality**. Let:

S = [a/(b+c)] + [b/(c+a)] + [c/(a+b)]

By Cauchy-Schwarz:

([a/(b+c)] + [b/(c+a)] + [c/(a+b)]) (a(b+c) + b(c+a) + c(a+b)) ≥ (a + b + c)²

Simplifying the second factor:

a(b+c) + b(c+a) + c(a+b) = 2(ab + bc + ca)

Thus:

S × 2(ab + bc + ca) ≥ (a + b + c)²

S ≥ [(a + b + c)²/(2(ab + bc + ca))]

Since ((a + b + c)² ≥ 3(ab + bc + ca)) (by the AM-GM inequality), we have:

S ≥ [3(ab + bc + ca)/(2(ab + bc + ca))] = [3/2]

Equality holds when (a = b = c), confirming the **minimum value** of (\\frac{3}{2}).

---

## Verification with specific cases

- **Case 1**: (a = b = c = 1)

[1/(1+1)] + [1/(1+1)] + [1/(1+1)] = [1/2] + [1/2] + [1/2] = [3/2]

- **Case 2**: (a = 1, b = 1, c \\to 0)

[1/(1+0)] + [1/(0+1)] + [0/(1+1)] = 1 + 1 + 0 = 2 > [3/2]

- **Case 3**: (a = 2, b = 1, c = 1)

[2/(1+1)] + [1/(1+2)] + [1/(2+1)] = 1 + [1/3] + [1/3] = 1 + [2/3] = [5/3] > [3/2]

These cases show that the **minimum value** of (\\frac{3}{2}) is achieved only when (a = b = c).

---

## Conclusion

The minimum value of (\\frac{c}{a+b} + \\frac{a}{b+c} + \\frac{b}{c+a}) for positive (a, b, c) is (\\boxed{\\frac{3}{2}}), achieved when (a = b = c). This result is supported by Nesbitt's inequality and the Cauchy-Schwarz inequality, and verified by specific cases.

---

## References

### What does COVID-19 testing results really say? The real statistics concealed behind the accessible data [^1ae5c113]. Journal of Medical Virology (2021). Medium credibility.

Accurate and comprehensive testing is crucial for practitioners to portray the pandemic. Without testing there is no data; yet, the exact number of infected people cannot be determined due to the lack of comprehensive testing. The number of seropositive for SARS-CoV-2 infection is obviously relative to the extent of testing. However, the true number of infections might be still far higher than the reported values. To compare the countries based on the number of seropositive for SARS-CoV-2 infection is misleading, as there may not be enough tests being carried out to properly monitor the outbreak. In this paper, we closely look through the COVID-19 testing results. Herein, we try to draw conclusions based on the reported data: first, the presence of a possible relationship between COVID-19 transition and patients' age will be assessed. Then, the COVID-19 case fatality rate (CFR) is compared with the age-demographic data for different countries. Based on the results, a method for estimating a lower bound (minimum) for the number of actual positive cases will be developed and validated. Results of this study have shown that CFR is a metric reflecting the spread of the virus, but is a factor of the extent of testing and does not necessarily show the real size of the outbreak. Moreover, no large difference in susceptibility by age has been found. The results suggest the similarity between the age distribution of COVID-19 and the population age-demographic is improving over the course of the pandemic. In addition, countries with lower CFRs have a more similar COVID-19 age distribution, which is a result of more comprehensive testing. Finally, a method for estimation of the real number of infected people based on the age distributions, reported CFRs, and the extent of testing will be developed and validated.

---

### Looking for the optimal DTI acquisition scheme given a maximum scan time: are more b-values a waste of time? [^9d84e4f8]. Magnetic Resonance Imaging (2009). Low credibility.

In this study we used simulated data to investigate how valuable the use of multiple b-values is, in terms of improving the accuracy and reproducibility of Diffusion Tensor Imaging (DTI) results. Our results show that the systematic bias of the estimated scalar diffusion parameters [apparent diffusion coefficient and fractional anisotropy (FA)] - due to the Rician distribution of magnetic resonance noise - can be minimized by increasing the number of b-values and not by increasing the number of sampling directions. In addition, the use of more than one b-value accounts better for the wide range of diffusivities found in the human brain by bringing closer together the FA estimates for fibres with different mean diffusivities. It is also shown that while for tractography studies we should use as many sampling directions as allowed by scan time limitations, for follow-up, intersubject or multicenter studies, the use of more than one b-value will improve the accuracy of the scalar diffusion parameters, as long as the minimum number of directions required for robust estimation of each parameter is still used.

---

### The fragility index is typically misinterpreted and of low value: clinical trials are designed to be fragile [^ca1a936e]. Arthroscopy (2024). Medium credibility.

The Fragility Index (FI) is defined as the number of patients whose outcome would need to change to reverse a statistically significant finding to a nonsignificant finding. The FI is nothing more than a repackaging of statistical significance based on the P value, perpetuating (1) ignoring of results that are "not" statistically significant; (2) treating results that are statistically significant as certain; and (3) distracting from evaluation of clinical significance. A well-designed trial includes a sample size calculation to determine the minimum number of patients required to observe a difference between study groups (if a difference exists). By including this minimum number, clinical trials are designed to be fragile, yet subsequently criticized as such, leading readers to the incorrect conclusion that the studies are flawed. It's time to move past systematic reviews focused on the FI.

---

### Ideal proportion of the population to be patch tested: how many should we be doing? [^5c04ad38]. Contact Dermatitis (2021). Medium credibility.

Background

How many patients should we be patch testing? A previous study suggested that the minimum proportion of a population to be patch tested for allergic contact dermatitis was 1:700 annually.

Objectives

To evaluate if the current minimum rate for patch testing has changed over the 20years since the previous study in order to maximize the value.

Methods

In cooperation with the British Society for Cutaneous Allergy, a proforma for collation of retrospective data between January 2015 and December 2017 was sent to patch-test centers in the United Kingdom (UK) and the Republic of Ireland (ROI). The number of positive tests was analyzed against the proportion of population tested to see what proportion of the population would yield the greatest number of positive results.

Results

Responses from 11 centers showed that the minimum number needed to patch test had increased to 1:550 per head of population per year using the current criteria.

Conclusions

In agreement with previous studies, we should be patch testing more people than we are. We could reduce the threshold for referral of patients we patch test to derive the most benefit from this investigation.

---

### Variable selection strategies and its importance in clinical prediction modelling [^535144a3]. Family Medicine and Community Health (2020). Medium credibility.

Bayesian information criterion

BIC is another variable selection criterion that is similar to AIC, but with a different penalty for the number of variables (parameters) included in the model. Like AIC, BIC also balances between simplicity and goodness of model fitting. In practice, for a given data set, BIC is calculated for each of the candidate models, and the model corresponding to the minimum BIC value is chosen. BIC often chooses models that are more parsimonious than AIC, as BIC penalises bigger models more due to the larger penalty term inherent in its formula.

Although there are similarities between AIC and BIC, and both criteria balance simplicity and model fit, differences exist between them. The underlying theory behind AIC is that the data stem from a very complex model, there are many candidate models to fit the data and none of the candidate models (including the best model) are the exact functional form of the true model. In addition, the number of variables (parameters) in the best model may not include all variables (parameters) in the true model. In other words, a best model is only an approximation of the true model and a true model that perfectly represents reality does not exist. Conversely, the underlying theory behind BIC is that the data are derived from a simple model and there exists a candidate model that represents the true model. Depending on the situation, however, each criterion has an advantage over the other. There are many studies that have compared AIC and BIC and recommended which one to use. If our objective is to select a best model that will provide maximum predictive accuracy, then AIC is superior (because there is no true model, and the best model is selected to maximise the predictive accuracy and represent an approximate true relation). However, if the goal is to select a correct model that is consistent, then BIC is superior (because BIC consistently selects the correct model from among the candidate models that best represent the true model). For large data sets, the performance of both criteria improves, but with different objectives.

---

### Is maximum positive predictive value a good indicator of an optimal screening mammography practice? [^f6c3a8b3]. AJR: American Journal of Roentgenology (2005). Low credibility.

Objective

Positive predictive value (PPV1) has been used as one important indicator of the quality of screening mammography programs. We show how the relationship between sensitivity and recall rate may affect the operating point at which optimal (maximum) PPV1 occurs.

Conclusion

Optimal (maximum) PPV1 can occur at any sensitivity level and should not be used as the sole indicator for practice optimization because it does not take into account the number of cancers that would be missed at that sensitivity.

---

### Tumor burden modeling versus progression-free survival for phase II decision making [^7a7c1bfd]. Clinical Cancer Research (2013). Low credibility.

Randomized Phase II oncology trial endpoints for decision making include both progression-free survival (PFS) and change in tumor burden as measured by the sum of longest diameters (SLD) of the target lesions. In addition to observed SLD changes, tumor shrinkage and growth parameters can be estimated from the patient-specific SLD profile over time. The ability of these SLD analyses to identify an active drug is contrasted with that of a PFS analysis through the simulation of Phase II trials via resampling from each of 6 large, Phase II and III trials, 5 of which were positive and one negative. From each simulated Phase II trial, a P value was obtained from 4 analyses-a log-rank test on PFS, a Wilcoxon rank-sum test on the minimum observed percentage change from baseline in SLD, and 2 nonlinear, mixed-effects model analyses of the SLD profiles. All 4 analyses led to approximately uniformly distributed P values in the negative trial. The PFS analysis was the best or nearly the best analysis in the other 5 trials. In only one of the positive studies did the modeling analysis outperform the analysis of the minimum SLD. In conclusion, for the decision to start a Phase III trial based on the results of a randomized Phase II trial of an oncology drug, PFS appears to be a better endpoint than does SLD, whether analyzed through simple SLD endpoints, such as the minimum percentage change from baseline, or through the modeling of the SLD time course to estimate tumor dynamics.

---

### An introduction to machine learning [^70a823d9]. Clinical Pharmacology and Therapeutics (2020). Medium credibility.

Different categories of loss functions

Different objective functions can be chosen to measure the distance between observed data and values predicted by the model. Some of the distance metrics used in practice can be associated to a likelihood. The likelihood indicates how probable it is to observe our data according to the selected model. The most common use of a likelihood is to find the parameters that make the model fit optimally to the data (i.e. the maximum likelihood parameter estimates). Usually, the negative logarithm of the likelihood is minimized and considered as objective function because it has favorable numerical properties. Similarly, in ML metrics, such as mean squared error, logistic objective, or cross‐entropy, are used to find optimal parameters or assess the fitness of the model.

In practice, analytical calculation of maximum likelihood or minimal loss may not be feasible, and it is often necessary to use a numerical optimization algorithm to solve for the best parameter values. Gradient descent is such an algorithm, where we first define an objective function for which we want to minimize and then iteratively update the values of the parameters in the direction with the steepest decrease (first‐order derivative) of the objective function until a convergence to a minimum distance is deemed reached. In the scenario of a nonconvex objective function, the success of finding a global minimum, as opposed to landing in some local minima, will depend on the choice of the initial set of parameter values, the learning rate (i.e. step size of each iteration) and the criterion for convergence. The reader can refer to ref. 35 for details on convex and nonconvex optimization processes. Stochastic gradient descent is an additional trick that can further speed up the optimization by randomly sampling a training dataset and summing the distances across this subset of training data points for approximating the objective function.

---

### Single cell RNA analysis identifies cellular heterogeneity and adaptive responses of the lung at birth [^92e0fdf9]. Nature Communications (2019). High credibility.

Cluster/cell type-specific differentially RNA expression was tested using a nonparametric binomial test, which compares the frequency of gene expression in the cluster with its frequency in all the other cells. To define cluster/cell type-specific signature genes, we considered genes that satisfied: (1) < 0.05 false discovery rate (FDR) of the binomial test, (2) minimum two-fold effective size, (3) detection in at least 20% of cells in the cluster, and (4) detection in < 40% of all cells. The effective sizeof a gene in a cluster/cell type is the ratio between the gene's expression frequency in the cluster/cell type and its expression frequency in all the other cells. For each cluster/cell type, genes were ranked by sensitivity and FDR value, and then up to the top 100 genes which satisfied the above criteria constituted the cluster/cell type specific signature.

Calculation of sensitivity-based enrichment score

The enrichment of a gene in a specific Drop-seq cell type is calculated as follows: enrichment = (a / b)/(c / d), where a is the number of cells positively expressing gene A in cell type X, b is the total number of cells positively expressing gene A, c is the number of cells in cell type X, and d is the total number of cells. In the calculation, (a / b) represents the sensitivity of gene expression in the given cell type.

---

### Relations among sensitivity, specificity and predictive values of medical tests based on biomarkers [^b2c612cd]. General Psychiatry (2021). Medium credibility.

Numerical study

In this section, we use some numerical examples to show how these four quantities change. Note that given a cut-off c of the biomarker, sensitivity, specificity, positive and negative predictive value are all functions of c from which we can identify the changing pattern of each pair of them.

For simplicity, assume the biomarker X has a standard normal distribution. The risk function is assumed to be of the form of the logistic function

From the probability theory, we know that the expected value of the risk function should equal the proportion p of the disease. Given each p, we determine unique parameter β 0 in the risk function by numerical integration. For example, p = 0.1, β 0 = −2.5642; p = 0.2, β 0 = −1.5601; and p = 0.3, β 0 = −1.0187.

Table 2 shows values of Se (c), Sp (c), PPV(c) and NPV(c) for c = −2.5,–2.0, −1.0, 0, 1, 2 and 3. We can see that Se and NPV are increasing functions of c, while Sp and PPV are decreasing functions of c. They are consistent with our conclusion developed above.

Table 2
Numerical results from simulations

---

### Rationale, considerations, and goals for atrial fibrillation centers of excellence: a Heart Rhythm Society perspective [^a328eb9f]. Heart Rhythm (2020). High credibility.

Atrial fibrillation centers of excellence — procedure volume considerations report that "some experts have advocated mandatory minimum numbers for invasive procedures (eg, catheter ablation or appendage occlusion) in order to optimize quality", and that "These minimum volume standards are relatively easy to implement and track over time". The page adds that "There is a positive correlation between case volume and treatment results and an inverse correlation with complication rates", yet "many questions regarding minimum volume requirements remain unanswered, including the optimum cutoff, the lack of scientific data for any clear value for cutoffs, and whether volume requirements should be per center or per operator".

---

### West nile virus in the United States: guidelines for surveillance, prevention, and control. 4th revision [^e79a0702]. CDC (2013). Medium credibility.

Mosquito-based surveillance indicators — West Nile virus (WNV) vector surveillance uses indices including vector abundance, number of positive pools, percent of pools positive, infection rate, and vector index. Vector abundance provides a measure of the relative number of mosquitoes in an area during a particular sampling period and is expressed as the number/trap night, and numerous trapping locations and regular collecting are required to obtain spatially and temporally representative data. Number of positive pools is the total of the number of WNV positive mosquito pools detected in a given surveillance location and period, but expressing WNV activity as the number of positive pools is not recommended. Percent of pools positive is calculated by expressing the number of WNV-positive pools divided by the total number of pools tested as a percentage, and using percent of pools positive as an index of WNV activity is not recommended. The infection rate in a vector population is an estimate of the prevalence of WNV-infected mosquitoes in the population and is a good indicator of human risk; there are two commonly used methods for calculating and expressing the infection rate, and the minimum infection rate (MIR) is calculated by dividing the number of WNV-positive pools by the total number of mosquitoes.

---

### A novel oppositional binary crow search algorithm with optimal machine learning based postpartum hemorrhage prediction model [^f6de23e1]. BMC Pregnancy and Childbirth (2022). Medium credibility.

Whereasrepresent the opposite number and x ∈ R denotes a real number determined on range of x ∈[a, b]. While a = 0 and b = 1 Eq. (3) becomes

While there is a point P (x 1, x 2,… x n) in n dimension coordinate and x 1, x 2, …, x n ∈ R later, the opposite pointis determined as its coordinates:

In such cases, have 2 values, x represent initial arbitrary value in [a, b] anddenotes the opposite values of x. They calculate f (x)&in all the iterations of OBCSA, later, employ on the evaluation function g ifselect x or else selectConsequently, the f l would be in range: f l ∈[f l min, f l max]. The opposite numbercan be determined by:

Later, evaluate the fitness for the first f l value and the fitness forin all the iterations. When, they select f l, or elsewould be selected. The stages of presented method can be given in the following.

Step1: The count of crows is n c = 25, f l min = 0.1, f l max = 1.8, A P = 0.3, and the maximal number of iterations is t max = 100.

Step2: The position that represent the features are made by U (0, 1).

Step3: The fitness function (FF) can be determined by

Whereas C represent the classification performance, W represent the weighted factors in the range of zero and one, F all represent the overall amount of features and F sub signifies the length of elected feature.

Step4: The position of the crows are upgraded as Eq. (2)

Step5: Steps 3 & 4 are repetitive till a t max is attained.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^b1513dd5]. CDC (2011). Medium credibility.

Table 1b — Minimum sample size for detecting geometric mean (GM) ratio k with α = 0.05 and W = 0.9 — presents required sample sizes by standard deviation (S) strata and defines parameters; α is significance level, W is power, GM is geometric mean, and S is standard deviation, noting that 1.2 is the standard deviation of national viral load (VL) data. Each jurisdiction will need to assess the standard deviation of their local VL data and then determine the appropriate sample size needed to assess VL, and if the sample size is inadequate to meet the recommended case inclusion criterion, an alternate method may need to be used, such as combining multiple years of data. For k = 3, sample sizes across S = 1, 1.1, 1.2, 1.3, 1.4, 1.5 are 75, 91, 108, 127, 147, 169, and Table 1b uses power = 90%; jurisdictions may also explore differences in means of viral loads, including categorical differences in the proportion with undetectable or very low VL.

---

### Can you handle the truth (and know it when you see it)? Understanding sensitivity, specificity, predictive values, and ROC curves [^c815747d]. Journal of Minimally Invasive Gynecology (2005). Low credibility.

The interpretation of diagnostic and screening tests requires a basic knowledge of sensitivity, specificity, and predictive values. This article provides an overview of these measures of test performance, a brief description of receiver operating characteristic curves, and relevant illustrations from the medical literature.

---

### Accurate localization microscopy by intrinsic aberration calibration [^1c3a83e7]. Nature Communications (2021). High credibility.

Axial dependence of aberration effects

We emphasize a critical result that is fundamentally problematic for super-resolution. Intrinsic aberrations affect apparent lateral positions, causing systematic errors that depend on axial position (Fig. 2a). These errors approach the imaging resolution, rendering much smaller values of localization precision potentially meaningless or even misleading. To achieve lateral localization accuracy that is truly superior to the imaging resolution, both axial localization and complete calibration of the field dependences are potentially necessary. Fortunately, intrinsic aberrations also encode axial information into emitter images, providing a latent capability for axial localization.

Fig. 2
Effects of intrinsic aberrations on apparent lateral position and particle image shape.

These data are from a representative calibration particle at a representative location in the imaging field. a Scatter plot showing apparent lateral position as a function of actual axial position. White data markers indicate the actual lateral position, which we define at the axial position of best focus z f. Uncertainties are smaller than the data markers. b – f Plots showing the dependence on axial position of the parameters (b), (c), (d) A, (e) ρ, and (f), whereis the amplitude after normalization to its value in the image for which ρ = ρ 0, with ρ 0 set to the minimum value of | ρ |. Fits of bivariate Gaussian models to emitter images determine the (black data markers) parameter values, and (green lines) polynomials model the z dependence for (b – c) lateral correction, (d) determination of the axial position of best focus z f, and (e - f) axial localization. Residual values indicate an uncertainty for each parameter. Values in the bottom panels are uncertainties of (b - c) apparent lateral positionandfrom the polynomial models, and (e – f) z positionfrom inversion of the polynomial models.

---

### A new approach to the validation of tissue microarrays [^006a05a6]. The Journal of Pathology (2006). Low credibility.

Although tissue microarrays (TMA) have been widely used for a number of years, it is still not clear how many core biopsies should be taken to determine a reliable value for percentage positivity or how much heterogeneity in marker expression influences this number. The first aim of this study was to validate the human visual semi-quantitative scoring system for positive staining of tumour tissue with the exact values determined from computer-generated images. The second aim was to determine the minimum number of core biopsies needed to estimate percentage positivity reliably when the immunohistochemical staining pattern is heterogeneous and scored in a non-binary way. Tissue sections from ten colorectal cancer specimens were stained for carbonic anhydrase IX (CA IX). The staining patterns were digitized and 400 artificial computer-generated images were generated to test the accuracy of the human scoring system. To determine the minimal number of core biopsies needed to account for tumour heterogeneity, 50 (artificial) core biopsies per section were taken from the tumoural region of the ten digitally recorded full tissue sections. Based on the semi-quantitative scores from the 50 core biopsies per section, 2500 x n (n = 1–10 core biopsies) experimental core biopsies were then generated and scores recorded. After comparison with field-by-field analysis from the tumoural region of the whole tissue section, the number of core biopsies that need to be taken to minimize the influence of heterogeneity could be determined. In conclusion, visual scoring accurately estimated the percentage positivity and the percentage tumour present in a section, as judged by comparison with the artificial images. The exact number of core biopsies that has to be examined to determine tumour marker positivity using TMAs is affected by the degree of heterogeneity in the expression pattern of the protein, but for most purposes at least four is recommended.

---

### Age-based differences in the predictive accuracy of a one-size-fits-all risk-cutoff value in prenatal integrated screening for down syndrome [^91f8313c]. Prenatal Diagnosis (2017). Low credibility.

Objective

The objective of this study is to assess variation in detection and false positive rates and adverse pregnancy outcomes across different age groups when a one-size-fits-all risk-cutoff value, such as 1/270, is used in integrated screening for Down syndrome.

Method

A Monte Carlo simulation was utilized to estimate the detection and false positive rates as well as adverse pregnancy outcomes.

Results

Using a one-size-fits-all risk-cutoff value, such as 1/270, can result in considerably high variations in detection and false positive rates across maternal ages and lead to a higher than the minimum possible total number of adverse outcomes.

Conclusion

Our findings indicate that the one-size-fits-all risk-cutoff value of 1/270, commonly used in DS screening, should be revisited and alternative (possibly age-based) cutoff values and strategies should be considered. © 2017 John Wiley & Sons, Ltd.

---

### Logistic regression: a brief primer [^677107e6]. Academic Emergency Medicine (2011). Low credibility.

Regression techniques are versatile in their application to medical research because they can measure associations, predict outcomes, and control for confounding variable effects. As one such technique, logistic regression is an efficient and powerful way to analyze the effect of a group of independent variables on a binary outcome by quantifying each independent variable's unique contribution. Using components of linear regression reflected in the logit scale, logistic regression iteratively identifies the strongest linear combination of variables with the greatest probability of detecting the observed outcome. Important considerations when conducting logistic regression include selecting independent variables, ensuring that relevant assumptions are met, and choosing an appropriate model building strategy. For independent variable selection, one should be guided by such factors as accepted theory, previous empirical investigations, clinical considerations, and univariate statistical analyses, with acknowledgement of potential confounding variables that should be accounted for. Basic assumptions that must be met for logistic regression include independence of errors, linearity in the logit for continuous variables, absence of multicollinearity, and lack of strongly influential outliers. Additionally, there should be an adequate number of events per independent variable to avoid an overfit model, with commonly recommended minimum "rules of thumb" ranging from 10 to 20 events per covariate. Regarding model building strategies, the three general types are direct/standard, sequential/hierarchical, and stepwise/statistical, with each having a different emphasis and purpose. Before reaching definitive conclusions from the results of any of these methods, one should formally quantify the model's internal validity (i.e., replicability within the same data set) and external validity (i.e., generalizability beyond the current sample). The resulting logistic regression model's overall fit to the sample data is assessed using various goodness-of-fit measures, with better fit characterized by a smaller difference between observed and model-predicted values. Use of diagnostic statistics is also recommended to further assess the adequacy of the model. Finally, results for independent variables are typically reported as odds ratios (ORs) with 95% confidence intervals (CIs).

---

### Principles of analytic validation of immunohistochemical assays: guideline update [^1d73ee65]. Archives of Pathology & Laboratory Medicine (2024). High credibility.

KQ2 — minimum case numbers for analytic validation specifies, for nonpredictive marker immunohistochemistry (IHC) assays being validated for clinical use, testing "At least 20 cases (10 positive and 10 negative)" versus "Any other number of cases", and for both FDA approved/cleared predictive marker IHC assays and laboratory-developed predictive marker assays being validated for clinical use, testing "40 cases (20 positive and 20 negative)" versus "Any other number of cases", with outcome "Diagnostic accuracy".

---

### The certainty behind reporting a significance result: what the clinician should know [^c7e6f766]. American Journal of Physical Medicine & Rehabilitation (2019). Medium credibility.

The P value is the most common method used in medical literature for the result of a statistical test. It is the probability of the data with a true null hypothesis and is calculated using a formal statistical test after the appropriate model has been determined to analyze study data. The P value is dependent on the effect size, sample size, and a measure of variability within the outcomes. For many years, the P value has been set at 0.05, which is an arbitrary cutoff. It is important to understand that setting the cutoff at 0.05 may be correct for some study designs but not in others. Therefore, we recommend that in addition to the P value, another metric should be reported that specifies the magnitude of the effect such as effect size, confidence interval of the effect size, or fragility index.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^4d373546]. The Journal of Molecular Diagnostics (2017). Medium credibility.

NGS oncology panel validation — agreement and predictive value definitions: Positive percentage agreement (PPA) is PPA = [A/(A + C)], and positive predictive value (PPV) is PPV = [A/(A + B)].

---

### West nile virus in the United States: guidelines for surveillance, prevention, and control. 4th revision [^4b48057f]. CDC (2013). Medium credibility.

West Nile virus (WNV) mosquito surveillance — minimum infection rate (MIR), maximum likelihood estimate (MLE), and Vector Index (VI) — is defined and applied as follows. The minimum infection rate (MIR) is based on the assumption that infection rates are generally low and that only one mosquito is positive in a positive pool. The maximum likelihood estimate (MLE) does not require that assumption and provides a more accurate estimate when infection rates are high, thus it is the preferred method of estimating infection rate particularly during outbreaks, while the MLE and MIR are similar when infection rates are low. Infection rate indices have been used successfully in systems associating infection rates with human risk, and a Microsoft Excel® Add-In to compute infection rates from pooled data is available. The Vector Index is an estimate of the abundance of infected mosquitoes that incorporates vector species, relative abundance, and the WNV infection rate into a single index; it is calculated by multiplying the average number of mosquitoes collected per trap night by the proportion infected with WNV and is expressed as the average number of infected mosquitoes collected per trap night, with individual VIs summed across key vector species to represent a combined estimate of infected vector abundance. Increases in VI reflect increases in risk of human disease, and the accuracy of the Vector Index is dependent upon the number of trap nights used to estimate abundance and the number of specimens tested for virus to estimate infection rate. Mosquito-based surveillance indicators have two important roles: they can provide quantifiable thresholds for proactive vector control efforts, and surveillance can determine when proactive measures have been insufficient and more aggressive measures, such as wide-scale aerial application of mosquito adulticides and more aggressive public education messaging, are required to prevent or stop an outbreak.

---

### Standards and guidelines for validating next-generation sequencing bioinformatics pipelines: a joint recommendation of the Association for Molecular Pathology and the college of American pathologists [^5b5f657c]. The Journal of Molecular Diagnostics (2018). Medium credibility.

Recommendation 2 — Validation of the NGS bioinformatics pipeline must be appropriate and applicable for the intended clinical use, specimen, and variant types detected by the NGS test; samples should be selected to verify the analytical sensitivity and positive predictive value and must be appropriate for the intended use of the test, including the specimen types to be examined and the variant types expected to be reproducibly and accurately detected. The validation sample set should include representative samples with one or more variants having depth of coverage and allele fractions at the test's intended limit of detection (LOD). For an NGS bioinformatics pipeline, the LOD is represented by two data points: the minimum required depth of coverage at the variant site and the minimum variant allele fraction, both of which have to be satisfied before a variant can be positively called; therefore, validation samples should be included that are at the intended LOD for both depth of coverage and variant allele fraction. Validation samples may naturally harbor the variant at low allele fraction, or they may be artificially designed by limiting the input quantity of nucleic acid to lower depth of coverage or by diluting a higher–allele fraction sample with normal DNA; it is insufficient to set a minimum variant allele fraction when the minimum depth of coverage has not been specified and vice versa. Validation of the LOD is critical for all specimens and is especially critical when low-cellularity samples, such as pancreatic cysts or cerebrospinal fluid, are included; if additional reference ranges have been set to determine germline heterozygosity, homozygosity, hemizygosity, or mosaicism, the validation sample set should include clinically significant variants with variant allele fractions within each established reference range. Recommendation 15 describes specifics regarding representative variants, Recommendation 14 details permissible use of in silico samples, and Recommendation 17 describes the requirement for appropriate revalidation when any assay component changes.

---

### Pancreatic adenocarcinoma, version 2.2021, NCCN clinical practice guidelines in oncology [^e4d7074c]. Journal of the National Comprehensive Cancer Network (2021). High credibility.

Lymph node counts and lymph node ratio — A systematic review showed that the number of positive lymph nodes and lymph node ratio are associated with OS in patients with pancreatic cancer, and the CAP recommendations include a count of the number of lymph nodes recovered and the number of involved nodes. Retrospective analyses show that patients with N0 disease have a better prognosis with an increasing number of examined lymph nodes, and based on these data, groups have recommended the minimum number of lymph nodes examined to be from 11 to 17 to provide optimal staging and to serve as a quality indicator. For patients with N1 disease, lymph node ratio (positive nodes/nodes examined) appears to be related to prognosis; for instance, in one analysis, patients with < 15% of examined positive nodes had a 5-year survival rate of 21.7%, while those with > 15% positive nodes had a 5.2% 5-year survival rate (P = 0.0017). The Panel believes that every effort should be made to identify all regional lymph nodes within the pancreatectomy specimen.

---

### Receiver operating characteristic (ROC) curves: the basics and beyond [^65266ad3]. Hospital Pediatrics (2024). Medium credibility.

Diagnostic tests and clinical prediction rules are frequently used to help estimate the probability of a disease or outcome. How well a test or rule distinguishes between disease or no disease (discrimination) can be measured by plotting a receiver operating characteristic (ROC) curve and calculating the area under it (AUROC). In this paper, we review the features of ROC curves and interpretation of ROC curves and AUROC values. We highlight 5 underappreciated features of ROC curves: (1) the slope of the ROC curve over a test result interval is the likelihood ratio for that interval; (2) the optimal cutoff for calling a test positive depends not only on the shape of the ROC curve, but also on the pretest probability of disease and relative harms of false-positive and false-negative results; (3) the AUROC measures discrimination only, not the accuracy of the predicted probabilities; (4) the AUROC is not a good measure of discrimination if the slope of the ROC curve is not consistently decreasing; and (5) the AUROC can be increased by including a large number of people correctly identified as being at very low risk for the outcome of interest. We illustrate this last concept using 3 published studies.

---

### Recommendations for cell-free DNA assay validations: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^981931bb]. The Journal of Molecular Diagnostics (2023). High credibility.

Template cell-free DNA (cfDNA) input reporting — quantitative reporting and method associations are detailed. Only 35% (429/1228) of publications reported a minimum or mean value of template cfDNA used in the assay, increasing to 61% (19/31) in CVPs. Among publications that did not report template cfDNA input, 44% (352/799) of total publications and 67% (8/12) of CVPs used a PCR-based strategy (eg, end point PCR, droplet digital PCR, or real-time quantitative PCR), and in this PCR subset only a small number indicated a minimum or mean volume requirement rather than a cfDNA quantification (total publications: 15%, 52/352; CVPs: 38%, 3/8). Approximately one-third of total publications (36%, 288/799) and CVPs (36%, 5/14) that did not report the cfDNA quantity used an NGS strategy. Provided that samples are not contaminated by excess genomic DNA, increased cfDNA input concentrations can increase molecular complexity and thereby increase the assay sensitivity to detect rarer events.

---

### Logistic analysis of choice data: a primer [^e53e7925]. Neuron (2022). Medium credibility.

Logistic regressions were developed in economics to model individual choice behavior. In recent years, they have become an important tool in decision neuroscience. Here, I describe and discuss different logistic models, emphasizing the underlying assumptions and possible interpretations. Logistic models may be used to quantify a variety of behavioral traits, including the relative subjective value of different goods, the choice accuracy, risk attitudes, and choice biases. More complex logistic models can be used for choices between good bundles, in cases of nonlinear value functions, and for choices between multiple options. Finally, logistic models can quantify the explanatory power of neuronal activity on choices, thus providing a valid alternative to receiver operating characteristic (ROC) analyses.

---

### Improved prediction of protein-protein interactions using alphaFold2 [^4d43094d]. Nature Communications (2022). High credibility.

Ranking models

To analyse the ability of AF2 to distinguish correct models as well as interacting from non-interacting proteins, we analyse the separation between acceptable and incorrect models as a function of different metrics on the development set: the number of unique interacting residues (Cβs from different chains within 8 Å from each other), the total number of interactions between Cβs from different chains (referred to as the number of interface contacts), average predicted lDDT (plDDT) score from AF2 for the interface, the minimum of the average plDDT for both chains and the average plDDT over the whole heterodimer.

We use these metrics as a threshold to build a confusion matrix, where true/false positives (TP and FP respectively) are correct/incorrect docking models which places above the threshold and false/true negatives (FN and TN respectively) are correct/incorrect docking models which scores below the threshold. From the built confusion matrix, we derive the true positive rate (TPR), false positive rate (FPR) defined as:

Then, we calculate TPR and FPR for each possible value assumed by the set of dockings given a single metric and plot TPR as a function of FPR in order to obtain an ROC curve. We compute the area under curve (AUC) for ROC curves obtained for each metric to compare different metrics. The AUC is defined as:

The TPR and FPR for different thresholds are used to calculate the fraction of models that can be called correct out of all models and the positive predictive value (PPV). The fraction of acceptable and incorrect models are obtained by multiplying the TPR and FPR with the SR. Multiplying the FPR with the SR results in the false discovery rate (FDR) and the PPV can be calculated by dividing the fraction of acceptable models by the sum of the acceptable and incorrect models. The PPV, FDR and SR are defined as:

pDockQ

As it is not only desirable to know when a model is accurate but also how accurate this model is, we developed a predicted DockQ score, pDockQ. This score is created by fitting a sigmoidal curve (Fig. 2c) using "curve_fit" from SciPy v.1.4.1, to the DockQ scores using the average interface plDDT multiplied with the logarithm of the number of interface contacts, with the following sigmoidal equation:whereand we obtain L = 0.724, x 0 = 152.611, k = 0.052 and b = 0.018.

---

### Greenness and hospital admissions for cause specific mental disorders: multicountry time series study [^a0fbae0a]. BMJ (2025). Excellent credibility.

Estimation of greenness related hospital admissions

We estimated the number of hospital admissions and fractions of mental health related hospital admissions statistically attributable to exposure to greenness, referred to as greenness related hospital admissions, across different settings (urban, semi-urban, rural). These estimates reflect both protective (negative) and adverse (positive) associations and were calculated using methods adapted from previous work.

For each location, month, and greenness level scenario, we estimated the attributable fraction of hospital admissions using country and cause specific exposure-response coefficients and the difference between the observed greenness level and a specified counterfactual greenness value. The number of greenness related hospital admissions was then derived by multiplying the attributable fraction by the corresponding number of hospital admissions. We obtained total greenness related hospital admissions by summing across time points and locations. The country specific attributable fractions were computed by dividing the total number of greenness related hospital admissions by the total number of hospital admissions over the study period. Supplementary method 1.2 provides further details and related calculations.

We applied this method in two ways. We estimated greenness related hospital admissions using the observed greenness level compared to a counterfactual scenario defined as the minimum NDVI observed across all locations in each country. We then evaluated greenness related hospital admissions under two hypothetical greenness intervention scenarios: a uniform increase in greenness across all locations by 1%, 5%, and 10% relative to location special original greenness level, and a targeted intervention increasing greenness in areas of low baseline greenness (bottom 25th centile) to the 75th centile within each country.

Analyses were performed using R (v4.2.2). Original maps were created in ArcGIS (v10.8.2).

Patient and public involvement

Patients and members of the public were not involved in the design, conduct, reporting, or dissemination plans of our research because it was a secondary analysis of existing routinely collected data, with no direct contact with patients. We acknowledge the value of patient and carer input and will consider incorporating qualitative data and participatory approaches in future work when feasible.

---

### Value of "minimum menstrual age" in determining early pregnancy failure [^1f808844]. Journal of Ultrasound in Medicine (2011). Low credibility.

Objectives

The purposes of this study were to assess a discriminatory "minimum menstrual age" (28 days + number of days elapsed between the first positive pregnancy test result and sonogram) for the diagnosis of early pregnancy failure when no embryonic/fetal heartbeat is seen and to compare minimum menstrual age dating with last menstrual period and sonographic dating.

Methods

We conducted a retrospective study of 338 initial first-trimester sonographic examinations among women with suspected early pregnancy failure. A minimum menstrual age for each was calculated, and pregnancy outcomes were assessed. The predictive value of the minimum menstrual age for the pregnancy outcome was assessed at both 42 and 49 days and compared to that of dating by the last menstrual period and sonography.

Results

Among a study cohort of 338 patients, the average gestational age calculated by the last menstrual period was 53 days; by sonography, it was 50 days; and by the minimum menstrual age, it was 35 days (P < .01). All cases in which there was no sonographically detectable embryonic heartbeat above a minimum menstrual age of 42 days resulted in pregnancy failure.

Conclusions

The minimum menstrual age is a conservative estimate of the gestational age, with an estimated positive predictive value of 100% for early pregnancy failure when no embryonic heartbeat is seen after 42 days. The minimum menstrual age can be combined with other existing diagnostic clues to predict early pregnancy failure with greater accuracy.

---

### Noninvasive liver disease assessment to identify portal hypertension: systematic and narrative reviews supporting the AASLD practice guideline [^565a0359]. Hepatology (2025). High credibility.

Outcome measures and analysis — diagnostic accuracy endpoints and meta-analytic methods were prespecified, with minimum study counts and handling of small evidence bases. Outcomes corresponded to sensitivity, specificity, positive (PPV) and negative predictive values, and AUROC. Stata version 14 was used to synthesize and pool sensitivity, specificity, positive likelihood ratio, negative likelihood ratio, and diagnostic OR with 95% CIs after extracting the true positive, false positive, true negative, and false negative data from included studies. A minimum of 4 studies are needed for each analysis, heterogeneity was assessed using the Q-statistic and I-squared values, and if < 4 studies were available, accuracy metrics were reported as ranges. A bivariate regression model was used to pool data accounting for the correlation between sensitivity and specificity.

---

### Statistical mechanics for metabolic networks during steady state growth [^8aeef127]. Nature Communications (2018). Medium credibility.

Competitive growth dynamics

The second possibility is that the Boltzmann distribution emerges from competitive growth dynamics. Since its historical origins in statistical physics, much research has been devoted to uncovering the dynamical roots of Boltzmann distributions, whose study highlighted important concepts and applications, ranging from ergodicity to fluctuation-response relations. The same questions naturally arise in the context of its application in metabolism. It has been shown that the maximum entropy distribution at a fixed average growth rate is recovered independently and justified dynamically as the steady state of logistic growth. Since the logistic growth is the standard model used to experimentally fit optical density curves, this link also provides a possible interpretation of the maximum entropy parameter β, as we discuss below.

Consider a population of initial size N 0 in a medium with carrying capacity N C and assume that the intrinsic growth rates of individuals, λ i, are sampled independently from a distribution q (λ), defined over the feasible polytope. In the simplest setting, upon neglecting growth state transitions, the number n i of cells with growth rate λ i will evolve in time according to

Then, with

Under a mean field approximation, the steady states of these dynamics are distributions with maximum entropy form at a fixed average growth rate, where the asymptotic optimization parameter, is given implicitly by the equation

Equation (13) can be viewed as a relationship between quantities that can be independently estimated for a specific experimental setup: the inoculum size (N 0) and carrying capacity (N C) on the one hand, as well as the typical value of β, via Eq. (8) or direct fitting of measured metabolic fluxes, on the other.

Taken together, the two mechanisms, active regulation and competitive growth dynamics, need not be exclusive, and can operate concurrently. A simple diagnostic that could provide insight into the relative importance of both mechanisms is to examine whether the relationship of Eq. (13) is satisfied. If it were, it would suggest that the Boltzmann distribution is dynamical in origin. If, on the other hand, the values of β inferred from fitting the maximum entropy model were higher than those derived from the N C / N 0 ratio and Eq. (13), additional active regulation may be at work. In the Results section and Supplementary Methods, we provide estimates of these quantities for the experiments under consideration.

---

### Fractional response analysis reveals logarithmic cytokine responses in cellular populations [^3e14a7a3]. Nature Communications (2021). High credibility.

Fractional response curves

Outcomes of physiological processes, e.g. of inflammation or stress responses, depend on the number of cells with specific responses, rather than on their mean or median, which constitutes the fraction of cells with a given response as a biologically relevant variable. We proposed, therefore, to quantify dose–responses in terms of cellular fractions and show here how this can be achieved for multivariate data.

We first introduced the fractional response curve (FRC) that quantifies fractions of cells that exhibit different responses to a change in dose, or in fact any other experimental condition. For each subsequent dose, the increase of FRC reflects the fraction of cells that exhibit responses different from lower doses. Adding cumulatively distinct fractions results in counting the number of distinct response distributions.

For an illustration of FRC, in addition to the formal definition derived in Methods, we considered a simple hypothetical example involving one signaling effector and three doses, although the approach extends to a general multivariate scenario. Response distributions to three doses, x 1, x 2, x 3, which can be interpreted as control, intermediate, and high dose, are shown in Fig. 2a. When dose 1 was considered alone, fractions of cells with all possible responses sum up to 1 (Fig. 2b). Therefore, we defined the value of the FRC for dose 1 to be 1, and write r(x 1) = 1. We then asked what fraction of the cellular population exhibits different responses after the change from dose 1 to dose 2. The fraction of cells exhibiting different responses is equivalent to the overall increase in the frequency of responses (Fig. 2c, green region). The overall fractional increase, denoted as ∆ r, is calculated as the area of the green region, and ∆ r = 0.31, represents the 31% of the cellular population exhibiting different responses due to dose increase. Therefore, we defined the value of the FRC for dose 2 to be the sum of the previous value and the fractional increment, r (x 2) = r (x 1) + ∆ r = 1.31. When dose 3 was considered, the fraction of cells that exhibited different responses is again equivalent to the overall increase in the frequency of different responses, now compared with the two lower doses (Fig. 2d). As before, the overall increase, ∆ r, is equivalent to the area of the yellow region (Fig. 2d), with ∆r = 0.74, representing 74% of cells stimulated with dose 3 exhibiting responses different to populations stimulated with lower doses. Again, the value of the FRC for dose 3 was defined as the sum of the previous value and the fractional increment, r(x 3) = r(x 2) + ∆r = 2.05. Changes in the FRC show what fraction of cells exhibit different responses owing to the dose increase. Adding subsequent fractional increments, ∆r, leads to the value of FRC expressed in terms of the cumulative fraction of cells that exhibit different responses due to dose change.

---

### Detecting disabilities in everyday life: evidence from a geriatric assessment [^572fdf79]. BMC Geriatrics (2022). Medium credibility.

Variable selection

To identify the most relevant geriatric covariates driving ADL, we applied LASSO. This method is applicable for high-dimensional data reduction and feature selection as it performs both variable selection and regularization to enhance the detection accuracy and interpretability of the resulting model. The basic intuition behind LASSO is that it forces the sum of the absolute value of the regression coefficients to be less than a fixed value (known as the regularization parameter, henceforth λ). As a result, less important coefficients are shrinked to zero, i.e. are excluded from impacting the model. Since optimal selection of the regularization parameter is critical, we used 100-fold cross-validation to detect optimal values of λ based on the resulting regression errors. We chose two different λ -specifications in the variable selection process. The first one leads to the minimum mean cross-validation error (λ min), and the second one is the largest value of λ such that the error is within one standard error of the cross-validated errors for λ min. We refer to this as λ 1 s e.

Model evaluation

Based on our definition of neediness in everyday life, we examined the extent to which our geriatric covariates were able to explain our indicator variable ADL*. For this purpose, we used a logistic regression framework and ROC analysis for both λ -specifications.

Furthermore, one might be interested in the univariate effects of the selected variables on ADL* and in the cut-off values of these geriatric covariates as they are essential for clinical decision-making. In medical practice, for example, cut-off values support the diagnosis of an impairment and thus the initiation of appropriate therapies or assistance. Of the standard approaches to estimate those cut-off values, we used the Youden index, which defines the optimal cut-off as the point maximizing the Youden function, which is the difference between the true positive rate and false positive rate over all possible cut-off values. To assess the strength of each optimal cut-off value, we reclassified the geriatric covariate into a binary variable with a value of 1 if an ADL disability was indicated and 0 otherwise. Then, we used the binary covariate as an explanatory variable for ADL* in a logistic regression to determine its odds ratio, i.e. the factor by which the probability of an ADL disability is greater when the binary covariate under consideration indicates such a disability than when it does not.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^8c95db08]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Next-generation sequencing (NGS) oncology panel validation — joint consensus recommendations from the Association of Molecular Pathology with liaison representation from the College of American Pathologists address NGS test development, optimization, and validation, including panel content selection and rationale for an optimization and familiarization phase conducted before test validation; utilization of reference cell lines and reference materials for evaluation of assay performance; determining of positive percentage agreement and positive predictive value for each variant type; and requirements for minimal depth of coverage and minimum number of samples to establish test performance characteristics, emphasizing the laboratory director's role in an error-based approach to identify and address potential sources of errors through test design, method validation, or quality controls, and intending to assist clinical laboratories with validation and ongoing monitoring of NGS testing for detection of somatic variants to ensure high quality of sequencing results.

---

### On the accuracy and precision of PLANET for multiparametric MRI using phase-cycled bSSFP imaging [^200debf7]. Magnetic Resonance in Medicine (2019). Medium credibility.

Based on the number of degrees of freedom in the ellipse fitting procedure, the minimally required number of RF phase increment steps is 6. This would be enough for accurate parameter estimation using data without noise, but for realistic SNRs (100‐250 in the performed experiments) the precision in the estimated parameters increases when the number of RF phase increment steps is increased. Interestingly, the precision does not further increase much more than 10 RF phase increment steps. The accuracy does not depend on the number of RF phase increment steps much. Therefore, using 8 to 10 RF phase increment steps is sufficient, as using more RF phase increment steps results in a longer acquisition time and does not influence the precision.

The minimum SNR required to achieve a precision of 5% of the mean values in T 1, T 2, and ∆f 0 estimation using the optimized TR/FA/ N cycles combination is lower than what we typically obtained in the phantom (average SNR of 250) for a voxel size of 1.41.43 mm 3 and in vivo (average SNR = 150) for a voxel size of 1.51.54 mm 3. Interestingly, similar results were found in the work by Björk et al 37 for the minimum required SNR; however, they concluded that the application of their method was not feasible at common SNRs, which were rather low in their work. However, they calculated the minimum SNR to achieve the precision of 5% of the true parameter values, not the mean.

For a two‐component system like WM, however, the use of the optimal parameter combination defined previously would result in underestimation of T 1 and T 2 values. Unlike a single‐component relaxation model, a two‐component model describes the signal as a complex sum of weighted signals from the two components with different frequency distributions and different relaxation parameters. The frequency shift between the components in combination with the difference of their volume fractions causes asymmetries in the bSSFP profile, which has an effect on the performance of the method. The ellipse of the dominant component is disturbed by the presence of the second component. Their weighted complex sum generally does not have an elliptical shape and cannot be fitted as an ellipse. However, the myelin component has shorter relaxation times and a smaller volume fraction than the main component has, and their weighted complex sum can still be fitted reasonably well as an ellipse, but with different "observed" T 1 and T 2 values.

---

### Lymph node ratio is less prognostic in melanoma when minimum node retrieval thresholds are not MET [^dacd75cb]. Annals of Surgical Oncology (2017). Low credibility.

Background

Lymph node ratio (LNR), positive nodes divided by nodes examined, has been proposed for prognostication in melanoma to mitigate problems with low node counts. However, it is unclear if LNR offers superior prognostication over total counts of positive nodes and nodes examined. Additionally, the prognostic value of LNR may change if a threshold number of nodes are examined. We evaluated whether LNR is more prognostic than positive nodes and nodes examined, and whether the prognostic value of LNR changes with minimum thresholds.

Methods

Using the National Cancer Data Base Participant User File, we identified 74,692 incident cases with nodal dissection during 2000–2006. We compared LNR versus counts of examined and positive nodes based on Harrell's C, a measure of predictive ability. We then stratified by total nodes examined: greater versus fewer than ten for axillary lymph node dissection (ALND) and greater versus fewer than five for inguinal lymph node dissection (ILND).

Results

Overall, LNR had a Harrell's C of 0.628 (95% confidence interval [CI] 0.625–0.631). Examined and positive nodes were not significantly different from this, with a Harrell's C of 0.625 (95% CI 0.621–0.630). In ALND, LNR had a Harrell's C of 0.626 (95% CI 0.610–0.643) with ≥ 10 nodes versus 0.554 (95% CI 0.551–0.558) < 10 nodes. In ILND, LNR had a Harrell's C of 0.679 (95% CI 0.664–0.694) with ≥ 5 nodes versus C of 0.601 (95% CI 0.595–0.606) < 5 nodes.

Conclusions

LNR provides no prognostic superiority versus counts of examined and positive nodes. Moreover, the prognostic value of LNR diminishes when minimum node retrieval thresholds are not met.

---

### ACC / AHA statement on cost / value methodology in clinical practice guidelines and performance measures: a report of the American college of cardiology / American Heart Association task force on performance measures and task force on practice guidelines [^5057d50c]. Journal of the American College of Cardiology (2014). Medium credibility.

Arguments in favor of incorporating resource and value considerations state that economic evaluations, including cost-effectiveness analyses, can help decision makers appreciate the implications of choices and clarify factors influencing relative benefits, and that economic analyses can guide those making coverage decisions and inform developers of practice guidelines to ensure that recommendations yield the greatest value from available healthcare resources. The document notes that currently algorithms for diagnosis or management of disease states, including appropriate use of clinical interventions, typically consider a broad spectrum of differential diagnoses and patient care approaches, which essentially assume that resources are unlimited, and emphasizes that the principal consideration is not cost awareness but a comprehensive, informed, and evidence-based approach in which incentives are balanced by the need to consume resources more wisely.

---

### When are metabolic ratios superior to absolute quantification? A statistical analysis [^ded5da1b]. NMR in Biomedicine (2017). Low credibility.

Metabolite levels measured using magnetic resonance spectroscopy (MRS) are often expressed as ratios rather than absolute concentrations. However, the inter-subject variability of the denominator metabolite can introduce uncertainty into a metabolite ratio. In a clinical setting, there are no guidelines on whether ratios or absolute quantification should be used for a more accurate classification of normal versus abnormal results based on their statistical properties. In a research setting, the choice of one over the other can have significant implications on sample size, which must be factored in at the study design stage. Herein, we derive the probability distribution function for the ratio of two normally distributed random variables, and present analytical expressions for the comparison of ratios with absolute quantification in terms of both sample size and area under the receiver operator characteristic curve. The two approaches are compared for typical metabolite values found in the literature, and their respective merits are illustrated using previously acquired clinical MRS data in two pathologies: mild traumatic brain injury and multiple sclerosis. Our analysis shows that the decision between ratios and absolute quantification is non-trivial: in some cases, ratios might offer a reduction in sample size, whereas, in others, absolute quantification might prove more desirable for individual (i.e. clinical) use. The decision is straightforward and exact guidelines are provided in the text, given that population means and standard deviations of numerator and denominator can be reliably estimated.

---

### Visual prototypes in the ventral stream are attuned to complexity and gaze behavior [^e4a7d3c6]. Nature Communications (2021). High credibility.

Complexity metrics

To compute the mean complexity ratio for each image, we used MATLAB's discrete cosine transform function (dct2.m). First, each color image was transformed to grayscale by retaining only the L* channel of the CIE 1976 L*a*b* color space. Next, pixel values were centered by subtracting the mean value. The image was then transformed into discrete cosine coefficients, which were then squared to yield the two-dimensional power-spectrum. To get the fraction of power contained in each component, the power of each component was divided by the sum of the entire power-spectrum. The components were then sorted in descending order by fractional power. The sum of the first N fractional power components yields the total fractional power contained in those N components. When they are sorted in descending order, this is the minimum N required to reach that fractional power. We designate nine fractional power levels ranging from 99.8% to 50%, per the formula f = 1–2 (−9 to −1)) and find the minimum number of components, N, necessary to reach each fractional power level. To calculate the final complexity ratio, we average N across all nine fractional power levels and divide by the total number of components (which is equal to the number of pixels in the original image).

To compute the median number of parts that composed each prototype, mean-shift clustering was used. Each image was resized to dimensions of (100x100)-pixels, and then inputted into the custom function Ms2.m using the default bandwidth of 0.2; this particular function clusters pixels based on color and spatial relationships (proximity). It returns several estimated clusters along with the number of pixels associated with each cluster; we retained all clusters with more than 10 pixels.

---

### Efficacy of ChAdOx1 nCoV-19 (AZD1222) vaccine against SARS-CoV-2 variant of concern 202012 / 01 (B.1.1.7): an exploratory analysis of a randomised controlled trial [^dd3febff]. Lancet (2021). Excellent credibility.

Vaccine efficacy was calculated as 1 – relative risk (ChAdOx1 nCoV-19 vs MenACWY groups) calculated from a robust Poisson model in SAS version 9.4. The model contained terms for treatment group and vaccine group (LD/SD or SD/SD). The logarithm of the period at risk was used as an offset variable in the model to adjust for volunteers having different follow-up times during which the events occurred. The code for model fitting is provided in the appendix (p 12).

To determine whether vaccination with ChAdOx1 nCoV-19 was associated with reduced viral load, NAAT Ct values, as a proxy for viral load, were analysed from weekly swabs from home test kits processed in Lighthouse Laboratories. Ct values for swabs done at symptomatic visits were not available. The minimum Ct value across the N and ORF1ab genes from each NAAT was computed for each swab and the minimum Ct value across all positive swabs for each participant was compared between vaccine groups using a Wilcoxon rank sum test. To determine whether vaccination was associated with a reduced number of weeks of NAAT positivity, the number of weeks from the first NAAT positive swab until the last NAAT positive swab during the NAAT-positive period was calculated. Three consecutive NAAT negative swabs were considered to indicate the end of the NAAT-positive period. The number of weeks positive was compared between vaccine groups using a Wilcoxon rank sum test.

---

### An introduction to machine learning [^790bfe14]. Clinical Pharmacology and Therapeutics (2020). Medium credibility.

Performance measures for model assessment

For regression models, we typically use the mean squared error, or other types of average objective functions, to compare model performance on training and test set. For two‐class classification problems, common performances measures are often derived from the "confusion matrix" shown in Figure 5 and briefly described below.
Precision, corresponding to the ratio of correctly predicted positive values to the total number of predicted positive values.
Recall, also called true positive rate (TPR) corresponding to the ratio of correctly predicted positive values to the total number of positive values in the dataset.
False Positive Rate (FPR), corresponds to the proportion of negative values predicted incorrectly.
Accuracy, corresponding to the number of correctly predicted values divided by the total number of predicted values.
Area under the ROC curve (AUC): Receiver operating characteristic (ROC) curves show the TPR (recall) and FPR dependence. In binary classification, each point on the ROC curve is located by choosing different thresholds for classification ofin positive or negative class. The top left corner of an ROC curve is the ideal case with 100% of positive values correctly classified (TPR = 1) and 0% of positive values incorrectly predicted at 0 (FPR = 0). As it is ideal to maximize the TPR while minimizing the FPR, a larger area under the ROC curve (AUC) is better.

Figure 5
Confusion matrix for two‐class problems. The confusion matrix indicates how successful the algorithm was at predicting labels in a binary classification problem where labels take values 0 (called "negative") or 1 (called "positive") by evaluating the predicted vs. the real labels. Every data point in the test set belongs to one of the four categories and different measures can be derived from these numbers.

Some of these metrics could be generalized for multiclass problems, where there are more than two different labels in the dataset. However, the metrics mentioned above are noncontinuous with respect to model parameters, hence, parameter optimization may be challenging when they are used as objective function. A continuous alternative and widely used metric previously mentioned in the section " Model fitting " is cross‐entropy (ESL, chapter 9), 9 which not only accounts for the most likely prediction but also for the prediction score (prediction confidence).

---

### ACC / AHA statement on cost / value methodology in clinical practice guidelines and performance measures: a report of the American college of cardiology / American Heart Association task force on performance measures and task force on practice guidelines [^dcb8e5e6]. Circulation (2014). Medium credibility.

WHO‑CHOICE framework adaptation — The committee anchored value definitions to WHO‑CHOICE thresholds, defining highly cost‑effective as less than GDP per capita, cost‑effective as between 1 and 3 times GDP per capita, and not cost‑effective as > 3 times GDP per capita, and selected the Table 2 values as initial threshold recommendations that may need modification as future information or consensus develops.

---

### Robust estimates of overall immune-repertoire diversity from high-throughput measurements on samples [^96ca99ba]. Nature Communications (2016). Medium credibility.

Sampling

We assume that each clone in the individual contributes cells to the sampled population according to a Poisson distribution. This will be true if (i) clones are well mixed in the blood or evenly distributed in the tissue being sampled, (ii) the parent population is sufficiently large that the Poisson estimate for the probability of, for example, a singleton contributing > 1 cell is negligible and (iii) no single clone is a large fraction (∼30% or more) of the parent population. In practice, condition (iii) is satisfied by counting large clones directly (see the 'Fitting').

Fitting

The largest clones may be represented by hundreds or even thousands of cells in a sample. For such large clones, sampling error is small: the relative size of the clone in the sample and in the individual will be about the same. As a result, clones that are large enough to have sufficiently small sampling error do not have to be fit by EM, and instead can simply be added to the MLE. We found that using a threshold of 30 cells, and therefore applying EM only to clones that contribute ≤ 30 cells to the sample and then adding larger clones back to the resulting MLE gives results that are indistinguishable from applying EM on the entire sample distribution, but with vast gains in speed. (Note that observing an absence of clones at a given size counts towards the number of observations used for calculating the Akaike information criterion.)

Scanning

In the standard EM algorithm, the exact sizes and frequencies of clones in the final MLE can vary depending on the sizes and frequencies used at the start of each iteration, reflecting different local maxima. To find global maxima, we developed a 'scanning' approach in which we applied EM to many starting clone sizes and frequencies (56 in our implementation), ranking results by maximum likelihood (after first adjusting likelihoods according to the number of ways to choose clones in each distribution; see Supplementary Methods). In each round, we perform an additional fit with starting clone sizes and frequencies at an average of the two top-ranked results. We then select the resulting best-ranked fit from the starting points. Runtime and (to some extent) accuracy correlate with the number of starting points.

---

### Analysis of the fatality rate in relation to testing capacity during the first 50 days of the COVID-19 epidemic in Italy [^159791be]. The American Journal of Tropical Medicine and Hygiene (2020). Medium credibility.

Statistical modeling.

A nonnegative logistic function was fitted on the cumulative number of cases and deaths as a function of time:The constrain of nonnegativity was necessary to avoid obtaining negative theoretical values in the first days of the epidemic. Parameters A, B, C, and D were determined based on the best fit model of nonlinear regression with the Levenberg–Marqardt algorithm. To verify the solidity of the model, a χ 2 test was performed on the absolute frequencies of the theoretical and observed data at 10 equal intervals.

The analysis of the best-fit parameters was interpolated daily to evaluate the trend as a function of time of the values of parameter A, which represents the horizontal asymptote of the function, that is, the predicted number of cases and deaths at the end of the epidemic. The parameters A, B, C, and D of the logistic curve were fitted daily, and their value was set each day according to the best fit models.

---

### Sample pooling: burden or solution? [^74e00891]. Clinical Microbiology and Infection (2021). Medium credibility.

Specific consideration related to SARS-CoV-2

Asymptomatic SARS-CoV-2-positive patients on average have lower viral copy numbers than symptomatic patients. For symptomatic patients, viral load is highest around the onset of symptoms and decreases shortly after. When a low concentration of SARS-CoV-2 virus is diluted into a pool of negative samples there will be a risk that levels are close to the limit of detection, thus increasing false-negative rates. Therefore, an obvious issue for SARS-CoV-2 pool testing using Approach 1 (Fig. 6) is the reduced sensitivity due to sample dilution. In principle, pooling one positive sample with seven negative samples will cause eight-fold viral dilution in the pool, which equates with three increased cycles of RT-PCR. A pool of 32 samples will increase the cycle number by 5. False negatives are inevitable if samples' viral loads are close to the detection limit. In fact, false-negative results are always a concern, even for individual testing. Research has shown that the probability of a false-negative result in an infected person decreases from 100% on day 1 to 67% on day 4 after exposure. On the day of symptom onset, typically 4 days after exposure, the median false-negative rate is 38%, which further decreases to 20% on day 8 (3 days after symptom onset). This time point may be the optimal time for RT-PCR testing since the median false-negative rate increases from day 9 onward to 66% on day 21. The fact that pool testing will likely have even higher false-negative rates is a concern, especially if the goal is to screen individuals at an early stage of infection.

---

### Explaining facial action units' correlation with hypomimia and clinical scores in Parkinson's disease [^d1e7585d]. NPJ Parkinson's Disease (2025). Medium credibility.

Classification

We have used XGBoost model to classify PD vs. HC, with Var _ Δ AU features as input. Our scheme was data-driven, meaning that our goal was to use all 17 features and let the XGBoost model automatically identify the important AUs for discriminating PD vs HC. It's worth noting that the XGBoost model is based on a decision tree approach, and feature selection is inherent to it. If a feature is irrelevant, the model will naturally handle it. The hyperparameters for the XGBoost model, selected based on common practices in literature, are defined as follows:
scale_pos_weight: This parameter is used in algorithms like XGBoost to give more importance to the minority class in a classification problem with imbalanced classes. It helps the model to be more sensitive to the minority class, reducing bias towards the majority class. For this parameter, we employed a common approach in XGBoost by setting the balance ratio as the ratio of the number of negative instances (HC) to positive instances (PD).
n_estimators: This represents the number of trees in the ensemble. The range values are from 1–250.
max_depth: This parameter determines the maximum depth of each tree. The values range from 1–14.
gamma: This parameter specifies the minimum loss reduction required to make a further partition on a leaf node of the tree. The values range from 0–400, with a mix of specific values.
min_child_weight: This parameter defines the minimum sum of instance weight (hessian) needed in a child. The values range from 1–14.
subsample: This parameter denotes the fraction of samples to use for training each tree. The values range from 0.1 to 1.0.
learning_rate: This parameter controls the step size at each iteration while moving toward a minimum of the loss function. The values are a mix of very small numbers and more typical learning rates, ranging from 1 × 10 −12 to 3.
colsample_bytree: This parameter denotes the subsample ratio of columns when constructing each tree. The values are: [0.3, 0.5, 0.8, 1].
reg_lambda: This parameter represents the L2 regularization term on weights. The values range from very small values to larger integers from 1 × 10 −5 to 95.

---

### Interpreting trial results-time for confidence and magnitude and not P values please [^f6868fcd]. Kidney International (2019). Medium credibility.

The problems with the P value as the single metric to summarize the results of a study are being recognized. It captures a single domain-random error-but it is relatively uninformative about more critical domains for deciding whether the results should be applied to clinical care and policy. Alternatives include the components of the outcomes reported (relevance, magnitude, frailty, and net benefit) and confidence (risk of bias and directness).

---

### Small bowel adenocarcinoma, version 1.2020, NCCN clinical practice guidelines in oncology [^63a721f0]. Journal of the National Comprehensive Cancer Network (2019). High credibility.

Regarding diagnostic procedures for small bowel adenocarcinoma - NCCN, more specifically with respect to histopathology (lymph nodes), NCCN 2019 guidelines recommend to obtain a minimum evaluation of 8 lymph nodes in patients (per AJCC recommendation), recognizing that no clear number of minimum lymph nodes to predict complete lymph node negativity has been established by the College of American Pathologists.

---

### Microcalcification segmentation from mammograms: a morphological approach [^7cee7e31]. Journal of Digital Imaging (2017). Low credibility.

Results and Discussion

After the parameters necessary to control the segmentation had been established, the method of detecting and segmenting microcalcifications was tested on the remaining 200 mammograms, using segmentations done manually by a radiologist and GTA contours. The results of these segmentations are presented in Table 2. Table 2 presents calculated statistical parameters such as the maximum value (max), minimum value (min), the mean value (mean), and the standard deviation (SD) of the following calculated indices: SI, OF, OV, and EF. Figure 6 shows a graph of data from Table 2. Table 3, in turn, presents
The number of ROIs depending on the false positive per image (FPI) examples obtained (there are altogether 200 ROIs).
The mean sensitivity values of detected microcalcifications depending on the detected false-positive (FP) examples.
Standard deviations (SD), minimum values (min), and maximum values (max).

Table 2
Measurements (min, max, mean, SD — standard deviation) of indices: similarity (SI), overlap fraction (OF), overlap value (OV), and extra fraction (EF) for segmentations carried out on mammograms from the DDSM database, specifically 100 benign cases, 100 malignant cases, and jointly on all analyzed 200 ROIs

Fig. 6
Graphs of the mean value and the standard deviation based on measurements of four indices: SI, OF, OV, and EF for the applied method, compared to the contours drawn by the radiologist based on the data from Table 2

Table 3
Experiment results for the test set based on 200 mammograms from the DDSM database

The first row shows the number of ROIs depending on the number of false positives per image (FPI), and the subsequent rows their corresponding mean values of sensitivity, standard deviations (SD), minimum values (min), and maximum values (max)

---

### A problem of hospital hygiene: the presence of aspergilli in hospital wards with different air-conditioning features [^4ce54465]. American Journal of Infection Control (2006). Low credibility.

A total of 1,030 microbiological samples were taken in 3 hospital wards with different air-conditioning features: no conditioning system (ward A), a conditioning system equipped with minimum efficiency reporting value (MERV) filters (ward B), and a conditioning system thoroughly maintained and equipped with high-efficiency particulate air (HEPA) filters (absolute) (ward C). The air in each ward was sampled, and the bacterial and fungal concentrations were determined by active and passive methods. The concentration of fungi on surfaces was also determined. Active sampling showed positive samples in wards A and B only, with average values of 0.50 colony-forming units (CFU)/m(3) (95% CI, 0.30 to 0.70) in A and 0.16 CFU/m(3) (95% CI, 0.13 to 0.20) in B. Passive sampling was positive only in ward A (mean, 0.14 CFU/cm(2)/h; 95% CI, 0.13 to 0.15). Aspergillus was found in 27% and 22% of sampled surfaces in wards A and B, respectively, but in no samples from ward C. The most commonly found species was A. fumigatus (76% of cases in A and 34% of cases in B). The results show that the use of air-conditioning systems markedly reduces the concentration of aspergilli in the environment. Proper maintenance of these systems is clearly fundamental if their efficacy is to be ensured.

---

### An additive selection of markers to improve diagnostic accuracy based on a discriminatory measure [^96f44714]. Academic Radiology (2013). Low credibility.

Rationale and Objectives

The estimation of the area under the receiver operating characteristic (ROC) curve (AUC) often relies on the assumption that the truly positive population tends to have higher marker results than the truly negative population. The authors propose a discriminatory measure to relax such an assumption and apply the measure to identify the appropriate set of markers for combination.

Materials and Methods

The proposed measure is based on the maximum of the AUC and 1-AUC. The existing methods are applied to estimate the measure. The subset of markers is selected using a combination method that maximizes a function of the proposed discriminatory score with the number of markers as a penalty in the function.

Results

The properties of the estimators for the proposed measure were studied through large-scale simulation studies. The application was illustrated through a real example to identify the set of markers to combine.

Conclusion

Simulation results showed excellent small-sample performance of the estimators for the proposed measure. The application in the example yielded a reasonable subset of markers for combination.

---

### Rotator cuff tendinopathy diagnosis, nonsurgical medical care, and rehabilitation: a clinical practice guideline [^a4079329]. The Journal of Orthopaedic and Sports Physical Therapy (2025). High credibility.

Grades of recommendations — This table defines grades A–F and links each to a strength-of-evidence statement and corresponding Level of Obligation. Grade A is defined as "A preponderance of level I and/or level II studies support the recommendation", with Level of Obligation (Based on Treatment Effects): "Must: benefits substantially outweigh harms; Should: benefits moderately outweigh harms; May: benefits minimally outweigh harms or benefit/harm ratio is value dependent; Should not: harms minimally or moderately outweigh benefits or evidence of no effect; Must not: harms largely outweigh benefits". Grade B is "A single high-quality randomized controlled trial or a preponderance of level II studies support the recommendation", with obligations "Should: benefits substantially outweigh harms; May: benefits moderately or minimally outweigh harms or benefit/harm ratio is value dependent; Should not: evidence that harms outweigh benefits or evidence of no effect". Grade C is "A single level II study or a preponderance of level III and IV studies, including statements of consensus by content experts, support the recommendation", with obligations "Should: benefits substantially outweigh harms; May: benefits moderately or minimally outweigh harms or benefit/harm ratio is value dependent; Should not: harms minimally or moderately outweigh benefits". Grade D is "Higher-quality studies conducted on this topic disagree with respect to their conclusions", with obligation "May: conflicting evidence, the benefit/harm ratio is value dependent". Grade E is "A preponderance of evidence from animal or cadaver studies, from conceptual models/principles, or from basic science/bench research support this conclusion", with obligations "May: in the absence of evidence from clinical studies, theoretical and/or foundational evidence supports benefit" and "Should not: in the absence of evidence from clinical studies, theoretical and/or foundational evidence suggests strong risk of harms". Grade F is "Best practice based on the clinical experience of the guideline development team supports this conclusion", and the table also specifies Level of Obligation (Based on Assessment/Diagnosis): "Must: strongly supported by consensus-based best practice/standard of care; Should: moderately supported by best practice/standard of care; May: supported by expert opinion in the absence of consensus; Should not: best practice/standard of care indicates potential harms; Must not: potential harms are strongly supported by consensus-based best practice/standard of care".

---

### A critique of the false-positive report probability [^0ef34eb3]. Genetic Epidemiology (2009). Low credibility.

The false positive report probability (FPRP) was proposed as a Bayesian prophylactic against false reports of significant associations. Unfortunately, the derivation of the FPRP is unsound. A heuristic derivation fails to make its point, and a formal derivation reveals a probabilistic misrepresentation of an observation. As a result, the FPRP can yield serious inferential errors. In particular, the FPRP can use an observation that is many times more likely under the null hypothesis than under the alternative to infer that the null hypothesis is far less probable than the alternative. Contrary to its intended purpose, the FPRP can promote false positive results. It should not be used. A modified FPRP is derived, but it appears to have limited application and does not address the problem of false reports of significant associations. The conditional error probability is a possible replacement for the FPRP.

---

### Optimization of breeding program design through stochastic simulation with evolutionary algorithms [^bfcc418f]. G3 (2025). Medium credibility.

Step 3: Select parameter settings

Based on the results from the previous step, we want to identify the most promising areas of the search space to investigate further in later steps. For this, we select the most promising parameter settings to be uses as "parents" for the next iteration, considering three strategies. The number of selected parents based on each step varies based on the iterations of the EA with values given below for iteration 11 onwards (see Table 1). In these iterations, 30 out of 300 parameter settings are selected.

Table 1.
Number of parameter settings selected as parents (step 3) and generated settings (step 4) during the EA optimization process.

Step 3.1: Highest value of the objective function

In the first sub-step, 20 of the 30 most promising parameter settings are identified ("selected") based on the value of the objective function that was derived solely based on the simulation of the parameter settings itself. This guarantees that the best-performing parameter settings are prioritized in the reproduction process, subsequently enhancing the likelihood of generating successful parameter settings.

Step 3.2: Highest expected value of the objective function

In the second sub-step, seven parameter settings are identified based on the highest expected value of the objective function. For this, we are employing the kernel regression method suggested in. By this, instead of evaluating each parameter setting individually, kernel regression computes a weighted average of performance values for multiple parameter settings. In contrast to step 3.1, this approach avoids bias towards region with more parameter settings tested overall. In contrast to, we are here proposing the use of an adaptive bandwidth by using the empirical standard deviation in the individual parameter in the current iteration.

Step 3.3: Highest value of the objective function from the previous iteration

In the third sub-step, three parameter settings from previous iterations are used. This typically refers to the best-performing parameter settings from the second-to-last iteration; however, if the expected performance of any previously suggested optima from any iteration based on kernel regression (see step 5) is higher than all parameter settings of the current iteration, these are added instead. Hereby, the risk of discarding potentially promising parameter settings due to stochasticity is reduced.

Management of parameter setting variability/heterogeneity/value distribution

We implement a strategy to manage variation within the EA, ensuring that highly similar parameter settings are not selected. For additional details on this approach, refer to.

---

### A systematic study of key elements underlying molecular property prediction [^21881191]. Nature Communications (2023). High credibility.

Classification metrics

In binary classification tasks, each molecule is assigned a probability of belonging to the positive (or active) class. When the predicted probability is greater than a threshold value (between 0 and 1), the molecule is classified as positive (or active), otherwise negative (or inactive). In total, there are four possible outcomes: true positive (TP), false positive (FP), true negative (TN) and false negative (FN). Based on the TP and FP rates across different probability thresholds, the receiver operating characteristic curve can be plotted with the area under the ROC curve as AUROC. Similarly, based on precision and recall, the precision-recall curve can be plotted to derive AUPRC. AUROC usually ranges from 0.5 (random classification) and 1 (perfect classification); if a classifier performs worse than random guessing, AUROC can be lower than 0.5. AUROC is more robust in the case of imbalanced datasets, but it may not be suitable when the minor class is of greater interest. In such cases, AUPRC is an alternative, with a baseline value as the fraction of the minor class.

Despite the usefulness of AUROC and AUPRC, these "collective" metrics may not be directly pertinent to virtual screening, a common application for molecular property prediction. In fact, the primary goal of early drug discovery is to rank molecules based on the predicted activity, thus avoiding the intractable number of false positives or false negatives in experimental assays. Given a set of predicted actives or inactives, depending on the screening goal, we argue that positive predictive value (PPV; Equation (1)) and negative predictive value (NPV; Equation (2)) are more relevant to virtual screening and drug design, as discussed in "Are the models properly evaluated?". Unlike AUROC and AUPRC, which are averaged across different probability thresholds, a threshold is determined first before deriving TP, FN, TN and FP, based on which PPV and NPV are calculated. When the datasets are balanced, the threshold is set as 0.5 whereas for imbalanced datasets, the threshold may be adjusted. In our study, we used Youden's J statistic, the vertical distance between ROC curve and a random chance line, to derive a threshold which maximizes the J statistic.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^e9e9d51f]. Annals of the American Thoracic Society (2023). High credibility.

Optimizing accuracy and precision — within a quantitative data set, accuracy refers to the closeness of the data to the true value and depends on both sensitivity and specificity, with the definition expressed as accuracy = number of (true-negative + true-positive)/(all negative + all positive).

---

### ACOEM practice guidelines: elbow disorders [^4e95c510]. Journal of Occupational and Environmental Medicine (2013). Medium credibility.

ACOEM elbow disorders — strength-of-evidence criteria define levels and thresholds as follows: levels are labeled "A = Strong evidence base", "B = Moderate evidence base", "C = Limited evidence base", and "I = Insufficient evidence†"; A requires "Two or more high-quality studies", B requires "At least one high-quality study or multiple moderate-quality studies", C requires "At least one study of moderate quality", and I applies when "Evidence is insufficient or irreconcilable". Study quality scoring specifies "High-quality studies are scored 8.0 to 11.0 points. Moderate-quality studies are scored 4.0 to 7.5 points". The guideline notes "Insufficient evidence recommendations are, by definition, consensus-based guidance".

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^fb845bcc]. Annals of the American Thoracic Society (2023). High credibility.

Optimizing accuracy and precision — In a quantitative data set, accuracy refers to closeness to the true value, depends on both sensitivity and specificity, and is defined as accuracy = number of (true-negative + true-positive)/(all negative + all positive).

---

### Mechanistic insights into transcription factor cooperativity and its impact on protein-phenotype interactions [^eb856482]. Nature Communications (2020). High credibility.

In vitro and in vivo positional improvement correlations

Similar to the Δ R 2 p calculation in SELEX data (see section: Shape profiles calculation), we generated ΔROC-AUC values per position for in vivo data (labeled as ΔAUC below), calculating changes in classification performance after addition (PWM + shape i) and removal (shape - shape i) of shape features in each position i on in vivo models. In each matched TF–TF dataset with CAP-SELEX and ChIP-seq data, we aligned and compared Δ R 2 p values obtained from in vitro 1mer + shape models and ΔROC-AUC per position values obtained from in vivo PWM + shape models using Spearman correlation. To estimate a false discovery rate (FDR) threshold for these correlation values, we scrambled correlation values in each model once and recalculated correlations.

Forkhead-Ets cooperative k -mers in ChIP-seq data

Selected sequences from our structural validation were mapped into co-occupied peaks to assess their enrichment versus single TF occupied peaks across TF pairs from the Forkhead and Ets families. Briefly, we mapped consensus sequences representing ω -none and ω motifs (GTAAACAGGAA and AACAACAGGAA, respectively), against Forkhead and Ets ChIP-seq in pairs, allowing up to one mismatch in each reported site. This threshold is chosen as it increases the recovery of sequences similar to each pattern, with a minimum overlap between hits in both categories. To compare the number of hits between co-occupied and single TF occupied peaks in each TF pair combination, we calculated the odds ratio (OR) of the number of sequences that do or do not containing either of these patterns in co-occupied versus single TF occupied regions:

OR = (a / b) / (c / d)

a is the number of co-occupied peaks with the motif, b is the number of co-occupied peaks without the motif, c is the number of single TF occupied peaks with the motif, and d is the number of single TF occupied peaks without the motif. We used a Fisher's exact test to assess the significance of these effect sizes across all assessed TF pairs, correcting p -values for multiple testing with the Benjamini Hochberg procedure.

---

### Quantitative assessment of full-width at half-maximum and detector energy threshold in X-ray imaging systems [^c65d8c68]. European Journal of Radiology (2024). Medium credibility.

Background

The response function of imaging systems is regularly considered to improve the qualified maps in various fields. More the accuracy of this function, the higher the quality of the images.

Methods

In this study, a distinct analytical relationship between full-width at half-maximum (FWHM) value and detector energy thresholds at distinct tube peak voltage of 100 kV has been addressed in X-ray imaging. The outcomes indicate that the behavior of the function is exponential. The relevant cut-off frequency and summation of point spread function S(PSF) were assessed at large and detailed energy ranges.

Results

A compromise must be made between cut-off frequency and FWHM to determine the optimal model. By detailed energy range, the minimum and maximum of S(PSF) values were revealed at 20 keV and 48 keV, respectively, by 2979 and 3073. Although the maximum value of FWHM occurred at the energy of 48 keV by 224 mm, its minimum value was revealed at 62 keV by 217 mm. Generally, FWHM value converged to 220 mm and S(PSF) to 3026 with small fluctuations. Consequently, there is no need to increase the voltage of the X-ray tube after the energy threshold of 20 keV.

Conclusion

The proposed FWHM function may be used in designing the setup of the imaging parameters in order to reduce the absorbed dose and obtain the final accurate maps using the related mathematical suggestions.

---

### Fractional response analysis reveals logarithmic cytokine responses in cellular populations [^fe6ff37c]. Nature Communications (2021). High credibility.

Fig. 2
Fractional response analysis.

a Hypothetical response distributions to three different doses encoded by colors. Distributions are represented as a probability density, which is proportional to the frequency of cells with a given response level. b – d Quantification of the fraction of cells that exhibit different responses due to dose increase, ∆ r, and constriction of FRC, for responses presented in a. Each panel from b to d corresponds to subsequent changes in dose. The color regions mark the overall increase in frequency due to considering the dose marked by the color. The area of the colored region quantifies ∆ r. The value of the FRC for each dose is obtained by adding the increment, ∆ r. e Quantification of the number of distinct distributions induced by the three considered doses. f Dose-typical responses for the response distributions of a. g Dissection of the responses to dose 2 into responses typical to any of the three doses. The fraction of cells typical to a given dose is marked with the corresponding color. The surface area of each color quantifies the typical fraction. h The fractions of cells stimulated with one dose (rows) with responses typical to any of the doses (columns). i The FRC together with the bands representing cell-to-cell heterogeneity as quantified in h. For each reference dose (x- axis), the fractions of cells stimulated with the reference dose that exhibit responses typical to other doses can be plotted in the form of color bands around the curve. The color encodes the dose a given fraction refers to. The height of the band marks the size of the fraction (y-axis). Fractions corresponding to doses higher than the reference dose are plotted above the curve, whereas to doses lower than the reference dose below the curve.

---

### Validation of monte carlo estimates of three-class ideal observer operating points for normal data [^26b6e5d9]. Academic Radiology (2013). Low credibility.

Rationale and Objectives

Traditional two-class receiver operating characteristic (ROC) analysis is inadequate for the complete evaluation of observer performance in tasks with more than two classes.

Materials and Methods

Here, a Monte Carlo estimation method for operating point coordinates on a three-class ROC surface is developed and compared with analytically calculated coordinates in two special cases: (1) univariate and (2) restricted bivariate trinormal underlying data.

Results

In both cases, the statistical estimates were found to be good in the sense that the analytical values lay within the 95% confidence interval of the estimated values about 95% of the time.

Conclusions

The statistical estimation method should be key in the development of a pragmatic performance metric for evaluation of observers in classification tasks with three or more classes.

---

### Variable selection strategies and its importance in clinical prediction modelling [^ef5e3f8f]. Family Medicine and Community Health (2020). Medium credibility.

Clinical prediction models are used frequently in clinical practice to identify patients who are at risk of developing an adverse outcome so that preventive measures can be initiated. A prediction model can be developed in a number of ways; however, an appropriate variable selection strategy needs to be followed in all cases. Our purpose is to introduce readers to the concept of variable selection in prediction modelling, including the importance of variable selection and variable reduction strategies. We will discuss the various variable selection techniques that can be applied during prediction model building (backward elimination, forward selection, stepwise selection and all possible subset selection), and the stopping rule/selection criteria in variable selection (p values, Akaike information criterion, Bayesian information criterion and Mallows' C p statistic). This paper focuses on the importance of including appropriate variables, following the proper steps, and adopting the proper methods when selecting variables for prediction models.

---

### Application of response surface methods to determine conditions for optimal genomic prediction [^9a46ec61]. G3 (2017). Low credibility.

Background

Response surfaces and approximation methods

RSMs are used to approximate functional relationships between a response variable y and a set of design variables which can be used to find the combination of factor levels for which the response variables are optimized. In this context, the term optimize refers to either maximize or minimize. RSMs were first introduced by, and are used in many experimental disciplines, including physical, biological, environmental, and chemical sciences; engineering; and economics although we are unaware of prior work on RSMs for evaluation of statistical techniques. The primary advantage of RSMs is that the number of experimental treatment combinations required to find the optimum experimental conditions can be (much) less than the total number of treatment combinations composing the entire response surface.

To illustrate the response surface for two variables, we simulated hypothetical yield data which are influenced by temperature and drought. Average daily temperature is simulated to be betweenandand drought is betweenand 4 standard precipitation index (SPI). Negative values for the drought index indicate conditions that are dryer than normal. The model we used to simulate yield Figure 1A shows the response surface, and Figure 1B shows the contour plot of the simulated yield. The simulation was performed in R and the code can be found in Supplemental Material.

Figure 1
(A) The response surface of yield in relation to temperature and drought. (B) The contour plot (level curves) of the response surface of yield.

For these simulated data it is clear that the yield is maximized when temperature is betweenandand drought is ∼2 SPI. In most situations, the true response surface is unknown, and is influenced by more than two design variables where visualization of the data are difficult.

A model for the relationship between response y and the p design variables, …, can be written in the formwhere f is the true, unknown response function and ϵ is the error term. The error term is often assumed to have a normal distribution with mean 0 and variancealthough other distributions can be modeled. If we assume that ϵ has a distribution that has mean 0, then the expected value of the response in terms of the natural variables can be written asThe design variablesare commonly referred to as natural variables because they have the natural units of the measurements. In an RSM the natural variables are transformed into coded variables:All of the coded variables have mean 0 and the same variances. It is convenient to code the low level of the factor variables asand the high level as 1.

---

### Distinguishing preferences of human APOBEC3A and APOBEC3B for cytosines in hairpin loops, and reflection of these preferences in APOBEC-signature cancer genome mutations [^80c96b9d]. Nature Communications (2024). High credibility.

Pairwise comparison of UPD-seq datasets

We conducted a comprehensive analysis of the uracilation at hairpins with loop lengths between 3 nt to 6 nt, considering nucleotide sequence of the hairpin loops, and the position of the cytosine within the loop. A table was created with all possible combinations. Applying a minimum stem strength of 10 to select for hairpin-forming sequences. Stem strength is defined by the number of base-pairs at the stem sequence (ss = 3X G:C pairs + 1X A:T pairs). We calculated the uracilation index (UI) for each combination and sample. We visualized the relationships between samples using a heatmap plot, which shows the pairwise correlations computed from the UI values.

ANOVA analysis of UI data for cytosines in different sized loops

A one-way ANOVA analysis of UI data was performed under the assumption of unequal variance to compare the sample groups and find loop length and positions that are different among the groups. After adjusting the P -values from this step to correct for multiple testing hypotheses, we performed a post-hoc t-test comparing the UI values at each of these hairpin sizes. The t-test compared the A3A, A3B-CTD, or A3B-full against the EV values. Once again, the P -values were corrected for multiple testing hypotheses. The bars with high statistical significance are marked with " (P ≤ 0.05) or " (P ≤ 0.01).

Construction of the river plots

For each set of targeted cytosines, we gathered all the cytosines with a minimum uracilation fraction of 0.04 (to remove noise) and a maximum uracilation fraction of 0.8 (to remove clonal mutations) and created GRange objects. We adapted and modified several functions from the MutationalPatterns R packageto normalize the number of uracilated positions by the total number of available genomic sites with the same sequence. The river plots were generated to visualize the normalized transition probabilities and the distribution of nucleotides surrounding a uracilated cytosine.

---

### Degenerate boundaries for multiple-alternative decisions [^4882240f]. Nature Communications (2022). High credibility.

Complex decision boundaries are consistent with the speed-accuracy curve

It is well established that humans and animals generate speed-accuracy trade-off (SAT) curves during decision-making experiments, showing the mean error against mean decision time, where each point on the curve can be accorded a cost ratio c / W of time to errors (equation (1)). For 2AFC tasks, the trend is such that as the value of c / W increases, speed is favored over the accuracy, and so the mean decision time decreases with a compensatory increase in mean decision error. This trade-off is instantiated by the decision rule (learned for each c / W value) with the SAT curve implicitly parameterized by decision boundary parameters. For 2AFCs, this is a single parameter: the flat boundary threshold θ. For n AFCs with n > 2, these are complex boundary functions (equation (3)) with sets of parameters (α, β,…). If the SAT curve is truly a curve, rather than a region, then multiple parameter combinations would give the same SAT, since a curve requires just one implicit parameter.

To examine the SAT curves for each parameterization, we optimized the boundary parameters for a range of cost ratios c / W and then plotted the resulting accuracies and decision times (Fig. 4). Optimal parameters θ, α, β were found by stochastic optimization over the reward landscapes, using Monte Carlo simulation over a grid search of parameters to generate a visualization of the reward landscapes (Fig. 5 a–c). As rewards are highly stochastic, smoothed landscapes were averaged over 100,000 decision trajectories. Optimal parameters were extracted by taking all points with a mean reward for which, whereis the maximum mean reward of the noisy landscape, σ is the spread of rewards, and δ = 0.02 is a small parameter (see Methods for details). Since the variation in expected reward is so small as to be negligible for a real decision-maker with limited capacity for sampling rewards from the environment, we define these boundaries as constituting a set of "good enough" boundaries that are in practice as effective as a true optimum within each parameterization.

---

### Performance assessments of diagnostic systems under the FROC paradigm: experimental, analytical, and results interpretation issues [^3b435b24]. Academic Radiology (2008). Low credibility.

As use of free response receiver-operating characteristic (FROC) curves gains more acceptance for quantitatively assessing the performance of diagnostic systems, it is important that the experimentalist understands the possible role of this approach as one of the experimental design paradigms that are available to him or her among all other approaches as well as some of the issues associated with FROC type studies. In a number of experimental scenarios, the FROC paradigm and associated analytical tools have theoretical and practical advantages over both the binary and the ROC approaches to performance assessments of diagnostic systems, but it also has some limitations related to experimental design, data analyses, clinical relevance, and complexity in the interpretation of the results. These issues are rarely discussed and are the focus of this work.

---

### Receiver operating characteristic curve in diagnostic test assessment [^a2029d3c]. Journal of Thoracic Oncology (2010). Low credibility.

The performance of a diagnostic test in the case of a binary predictor can be evaluated using the measures of sensitivity and specificity. However, in many instances, we encounter predictors that are measured on a continuous or ordinal scale. In such cases, it is desirable to assess performance of a diagnostic test over the range of possible cutpoints for the predictor variable. This is achieved by a receiver operating characteristic (ROC) curve that includes all the possible decision thresholds from a diagnostic test result. In this brief report, we discuss the salient features of the ROC curve, as well as discuss and interpret the area under the ROC curve, and its utility in comparing two different tests or predictor variables of interest.

---

### ACC appropriate use criteria methodology: 2018 update: a report of the American college of cardiology appropriate use criteria task force [^0573d900]. Journal of the American College of Cardiology (2018). Medium credibility.

ACC Appropriate Use Criteria (AUC) rating panel composition and balance — Rating panels consist of 13 to 17 members and must be composed of an odd number of individuals; at a minimum they include a practicing physician with expertise in the clinical topic, a practicing physician in a closely related discipline, a primary care physician, an expert in statistical analysis, and an expert in clinical trial design. Rating panel members may serve more than 1 function; the Task Force may incorporate a public-sector member and/or a health services payer representative, and a fellow-in-training may be included. To maintain balance, specialists whose key area of interest is the primary focus of the specific set of AUC should be a minority of rating panel members.

---

### West nile virus in the United States: guidelines for surveillance, prevention, and control. 4th revision [^b3fb8cd7]. CDC (2013). Medium credibility.

Vector index (VI) for West Nile virus mosquito surveillance combines "vector species presence, vector species density, vector species infection rate" into "a single value", and "the VI is simply the estimated average number of infected mosquitoes collected per trap night summed for the key vector species in the area". For calculation, N̄ is defined as "Average Density; (number per trap night for a given species)" and P̂ as "EstimatedinfectionRate (proportion of the mosquito population WNV positive)". An example dataset for two primary vectors shows columns "Cx. tarsalis" and "Cx. pipiens", with "Total 442 233", "Average per Trap Night 74 39", and "Standard Deviation 41 21" as the density inputs for "Step 1: Calculate mosquito density".

---

### Relations among sensitivity, specificity and predictive values of medical tests based on biomarkers [^59e58e2b]. General Psychiatry (2021). Medium credibility.

Diagnostic tests are usually based on some quantitative biomarkers. Two key parameters used to characterise the quality of a test are test sensitivity and specificity. Predictive values of the disease status based on test results are also of interest in medical research and public health management. In this paper, we study the relations among sensitivity, specificity and predictive values of the test. The core concept is risk function, which is assumed to be an increasing function of the biomarker. Our results show that test sensitivity and specificity change in opposite directions. The positive predictive value and the sensitivity also change in opposite directions. Likewise, the negative predictive value and the specificity change in opposite directions.

---

### The Infectious Diseases Society of America guidelines on the diagnosis of COVID-19: antigen testing (January 2023) [^d77aaaa3]. Clinical Infectious Diseases (2024). High credibility.

Pretest probability assumptions for symptomatic and asymptomatic individuals — Prevalence, defined by surveillance nucleic acid amplification test (NAAT) results over the last 14 days, was used to set baseline risks; for asymptomatic cases the panel applied 1%, 5%, and 10% pretest probability, and for symptomatic patients used 5%, 20%, to 50% pretest probability, with these values chosen based on reported prevalence sources.

---

### Application of response surface methods to determine conditions for optimal genomic prediction [^b53958dd]. G3 (2017). Low credibility.

Table 4
Starting values for the factors ind, m, QTL, epi, and h in terms of natural units and coded units

Average responsesand their difference from 500 replicates of the half-fractional factorial of 16 factor combinations (Table 5) were used to determine subsequent subsets of factor combinations that should be close to the optimum response.

Table 5
Mean accuracy of BLUP, mean accuracy of SVM, and the difference between the mean accuracy of SVM and mean accuracy of BLUP for 16 combinations of factors

The mean accuracy difference between the SVM and BLUP, and the estimates of the regression coefficients in terms of the coded units for the responsefor the half factorial wereBased on the estimated coefficients, increasing the number of progeny, the proportion of total genetic variance due to epistasis, and the heritability, and decreasing the number of markers and QTL should improve the response. The coefficients with the smallest magnitudes include number of markers and QTL. Thus, these are not as influential as the other factors in terms of the next set of factor combinations which will improve the response.

To determine the next set of factor combinations, we define a basis and calculate the step sizes (increments) for each factor. Table 6 shows the low and high levels (level 1 and level 2) of the factors, the average of the two levels of the factors (which is the base in later calculations), and the distance between the average and either of the initial values (which is used for calculating the coordinates of the response surface which need to be evaluated next).

Table 6
Levels of factors, average of the levels of the factors, and half of the difference between the levels of factors

For establishing which treatment combinations need to be evaluated next to find the maximum response, the step size of the input variables needs to be determined. First, an input variable needs to be chosen (called the basis) which will also influence the step size of the other variables. It is beneficial to choose a variable for which the most information is available, but it is also a common practice to choose the basis with the largest absolute value of the estimated regression coefficient.

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^327fd22c]. Nature Communications (2023). High credibility.

Thresholding

The curve fold change for the i th curve (cfc i) is defined as the log 2 -ratio between the lowest and highest concentration using the regressed model and it quantifies the drug's effect size or efficacy (Eq. 11).

We transferred the SAM principle of differential T-statistics to the recalibrated F-statistic to obtain equivalent decision boundaries for the dose–response curve analyses. This is possible by recognizing that a dose–response curve converges in the limit to two groups (front plateau group = not affected data points, and back plateau group = affected data points), where the curve fold change is equivalent to a conventional SAM fold change between the two plateau groups. In this case, allowing for the conversion and application of the s 0 SAM principle. CurveCurator simplified this process by calculating the tuning parameter s 0 directly from the user-specified significance and fold change asymptotes (Eq. 12). Whereis the inverse cumulative density function of an F-distribution with degrees of freedom dfn and dfd as determined in the section above. This makes s 0 also a function of the number of data points, which is relevant when a curve has missing values.

The tuning parameter s 0, which defines the hyperbolic decision boundaries, can also be used to transform the curve's recalibrated F-value into the s 0 -adjusted F-value (F adj, i) based on the global s 0 value and the curve's measured fold change (cfc i) (Eq. 13).

We then transform this s 0 -adjusted F-value into a "relevance score" using the cumulative density function F X (x | dfn, dfd). For s 0 = 0.0, this simply corresponds to the p -value of the curve. For s 0 > 0.0, this can no longer be interpreted as a p -value, but it still provides an appropriate ordering of curves by both statistical and biological relevance. Additionally, a −log 10 transformation is applied for visualization purposes and to obtain an interpretable score that ranges from 0 to infinity, where 0 has no relevance (Eq. 13).

---

### Association between neurite metrics and tau / inflammatory pathology in Alzheimer's disease [^5c46ea34]. Alzheimer's & Dementia (2020). Medium credibility.

3.3 Correlations with neuropsychological measurements

Significant correlations with cognitive functions were found only in the AD‐S group, and the other unillustrated analyses in Figures 4 and 5 were not significant. The list of insignificant analyses can be found in Supplementary Table S1. In additionally, Supplementary Figure S2 (D‐F) presents the numerical correlations.

FIGURE 4
Correlations with delayed recall (A‐C) and orientation (D‐F) in the AD‐S group. (A) 11 C‐PiB, (B) 18 F‐THK5351, and (C) the ODI with delayed recall. (D) 18 F‐THK5351, (E) the NDI, and (F) the ODI with orientation. Color bars denote the statistically significant T‐values from minimum to maximum

FIGURE 5
Correlations with executive function in the AD‐S group. (A) 11 C‐PiB, (B) 18 F‐THK5351, (C) the NDI, and (D) the ODI. Color bars denote the statistically significant T‐values from minimum to maximum

With delayed recall, both 11 C‐PiB and 18 F‐THK5351 showed negative correlations in the cortex, with the latter showing a greater and broader association (Figure 4 ‐A, B). In addition, the ODI was also positively correlated with delayed recall in the bilateral posterior cingulate gyri (Figure 4 ‐C).

The right mesial temporal lobe was negatively associated with orientation in 18 F‐THK5351, the NDI, and the ODI (Figure 4 ‐D–F), whereas 11 C‐PiB showed no significant correlations. In addition, 18 F‐THK5351 presented significant negative correlations with orientation in the bilateral mesial and lateral temporal lobes, cingulate gyri, and orbitofrontal cortices (Figure 4 ‐D).

Finally, the right posterior cingulate gyrus was correlated with executive function in all modalities (negatively in PET and positively in NODDI) (Figure 5 ‐A–D). In addition, the bilateral frontal lobes and lateral parietal lobes were also negatively correlated with executive function in 11 C‐PiB and 18 F‐THK5351 (Figure 5 ‐A, B).

---

### Dose-volume conundrum for response of prostate cancer to brachytherapy: summary dosimetric measures and their relationship to tumor control probability [^18a27989]. International Journal of Radiation Oncology, Biology, Physics (2004). Low credibility.

Purpose

Although it is known that brachytherapy dose distributions are highly heterogeneous, the effect of particular dose distribution patterns on tumor control probability (TCP) is unknown. It is unlikely that clinical results will throw light on the question in the near future, given the long follow-up and detailed dosimetry required for each patient. We used detailed dose distribution data from 50 patients combined with radiobiologic parameters consistent with what is known about TCP curves for prostate cancer to study the changes in TCP that accompany gross dosimetric measures and particular dosing irregularities (e.g., moderate underdosing of large volumes vs. extreme underdosing of small volumes).

Methods and Materials

For each of the 50 patients with organ-confined prostate cancer who had undergone 125I prostate implants alone at our clinic, postimplant CT scans were obtained approximately 1 month after implantation. Dose distribution information was obtained from postimplant dosimetry. The percentage of the prostate volume receiving a specified dose was recorded from the respective differential dose-volume histograms in 10-Gy bins. In addition, the percentage of prostate volume underdosed at varying fractions of the prescription dose were determined, as was the minimal prostate dose. The log-normal distributions of the radiobiologic parameters [ln(initial clonogen number), alpha, and alpha/beta] were adjusted so that the predicted population parameters (steepness and location) of the dose-response curves for external beam radiotherapy agreed with the published estimates. The variability in the dose-volume details was increased by scaling the dose distributions by factors ranging from 0.7 to 1.5, thereby simulating, for each of the patients, nine new patients with different total doses but identical relative distributions of the dose over the voxels. Radiobiologic variability between the selected dose distributions was then removed by averaging > 50 randomly chosen sets of radiobiologic parameters from the log-normal distributions to estimate the TCP for each of the dose distributions, giving some insight into the TCP variations with conventional dosimetric indexes and different patterns of underdosing.

Results

Using the 450 dose distributions created by expanding the 50-patient data set, the volume of the prostate that was extremely underdosed (between 50% and 70% of the prescription dose) was related to the volume that was moderately underdosed (between 80% and 100% of the prescription dose). We found that the individual TCP is greatly dependent on the inhomogeneous dose distribution and the dosimetric indexes, such as the volume of prostate receiving 100% of the prescribed dose (V100) and the maximal dose received by 90% of the prostate volume (D90), which, by themselves, are not always accurate predictors of control probabilities. In a multivariate analysis of the dependence of TCP on these parameters (V100, D90, minimal dose, and moderately and severely underdosed volumes), only D90 and the minimal dose were statistically significant. Generally speaking, however, a lower minimal dose means a lower TCP.

Conclusion

The work described here was an hypothesis-generating study. Our results showed that even if the V100 and D90 are nearly identical for 2 patients, there can be (and frequently are) significant differences in the dose distributions in the subvolumes of the prostate. Under simulated dose-response conditions (i.e., with variations in the dose distribution), the D90 and minimal dose significantly affected the TCP but the V100 and the volumes moderately or severely underdosed did not. In general, one must consider the totality of the dose distribution to evaluate the dosimetric quality of a low-dose-rate prostate implant. TCP is not a monotonic function of extreme or moderate underdosing. In some instances, extreme underdosing of relatively small volumes may result in a greater TCP than moderate underdosing of relatively large volumes and vice versa.

---

### Standards and guidelines for validating next-generation sequencing bioinformatics pipelines: a joint recommendation of the Association for Molecular Pathology and the college of American pathologists [^4cee71e7]. The Journal of Molecular Diagnostics (2018). Medium credibility.

NGS bioinformatics validation — characteristics and variability of included studies: Sequencing technologies used for validation included optical- and semiconductor-based platforms, and most studies used a gene-panel approach (3 to 297 genes) with one study validating a whole-exome sequencing assay. The number of samples used for validation ranged from 5 to 297. Cell lines and formalin-fixed, paraffin-embedded tissues were the most common sample types; others included peripheral blood, bone marrow, body fluids, and frozen tissue. Minimum depth of coverage and allelic fraction ranged from 200× to 500× and from 2.5% to 10%, respectively. Reporting of performance characteristics (analytic sensitivity, analytic specificity, and positive predictive value) was highly variable, was not stratified by variant type in some studies, and reporting of CI was inconsistent, reinforcing the need for guidance in NGS bioinformatics pipeline validation.

---

### Estimation of multicomponent reactions' yields from networks of mechanistic steps [^614d71b2]. Nature Communications (2024). High credibility.

Having defined the rate constants, we quantify the changes in the extentof reaction i (Eq. 4) and concentrationsof species(Eq. 5). In both equations, stands for the stoichiometric coefficient of compound x in reaction i.

For a given set of model's 23 free parameters, these equations are numerically integrated using final difference method. Integration is initiated for the concentrations of the starting materials equal to their stoichiometric coefficients in a given MCR, and concentrations of all other species assigned to zero. The yield for a particular MCR is then calculated as the product-to-initial substrate concentration ratio at the end of integration. The length of integration is defined as some global parameter N (to be optimized) multiplied by the number of steps in a given reaction pathway.

Parameters' optimization

The model's 23 parameters are optimized on the training set of 20 reaction networks underlying the known MCRs (from Fig. 1a; in total, spanning 993 mechanistic steps). This optimization (i) aims to maximize the correlation coefficient between the calculated and experimental yields; and (ii) is performed using the Bayesian optimization algorithm with the Gaussian process as a surrogate model, as implemented in OpenBox library. Specifically, we used expected improvement as an acquisition function and radial basis function (RBF) kernel. Search space comprised of continuous variables along with their ranges and starting values (see argsParser.py for details of space definition) as well as constraints defining relations between variables (e.g. to enforce that "Fast" reaction is faster than "Slow", and "Slow" is faster than "Very Slow"; function buildOpenBoxSpaceFromDict in optscan.py file). For each model considered, five independent runs were performed for at least 100 optimization steps each. Each optimization aimed to maximize the coefficient of determination of linear regression (without intercept, i.e. passing through (0,0)) between experimentally reported yields and yields predicted by the model (see optFunctionOpenBox function in optscan.py file for implementation details). The model offering the highest correlation was then taken. With the parameters thus optimized, the model was used to estimate the yields of 10 MCRs from the test set (Fig. 1b).

---

### A data reduction and compression description for high throughput time-resolved electron microscopy [^d1f0866b]. Nature Communications (2021). High credibility.

The calibration (summarized in Supplementary Note 11) begins by first estimating a single global threshold that separates signal from noise for all pixels. Assuming the histogram of dark values are normally distributed, this global threshold is estimated based on a user-specified upper limit on the tolerable false positive rate of a surmised normal distribution(r). However, because individual pixels behave differently from each other, using the same threshold for all pixels can severely bias electron counting. To remove this bias the global threshold has to be adapted for each pixel individually based on the pixel's gain and dark noise level.

Now we are ready to estimate the effective detectable electron count on the detector from the dataset directly. Given the low dose rate in this calibration dataset, only in a small fraction of frames does an individual pixel see electron events. Therefore, a pixel's median across all calibration frames is effectively its dark noise level, at this dose rate. Given the compact PSF and high SNR of DE-16 detectors, to calculate each pixel's gain, we assume that direct electron hits result in larger intensities than those due to charge sharing, even when the pixels have different gains. If the calibration dataset has a total dose of N e/pixel, where N is sufficiently small such that the probability of two electrons hitting the same pixel is negligible, then a pixel's gain is the median of the N largest intensities it has across all calibration frames. Therefore, we first estimate the total dose per pixel in the calibration dataset using a few randomly selected small two-dimensional (2D) patches. Separate thresholds are identified for individual pixels in these patches in a similar manner to the global threshold (i.e. assuming normality in the dark distribution and using a false positive rate parameter r). These thresholds are used to identify the connected components in each selected 2D patch across all frames in the calibration dataset. The number of connected components emanating from the central pixel of a 2D patch across all calibration frames gives an estimate of the number of electron events (n c) at the central pixel of that patch. The average of these values across all randomly selected patches (ṉ c) is used as the estimated total dose per pixel in the calibration dataset. Here, a puddle is assumed to emanate from the pixel that has the maximum value in the puddle. Finally, using the per-pixel dark noise levels and gains the global threshold is adapted to compute each pixel's independent threshold. To compute a pixel's threshold the global threshold is first shifted such that the pixel's dark noise level matches the global mean dark noise level and then scaled such that the pixel's gain matches the global gain.

---

### Work-relatedness [^700e045f]. Journal of Occupational and Environmental Medicine (2018). Medium credibility.

Work-relatedness — Table 1 steps for evaluating epidemiological evidence list procedural actions and criteria: "Collect all epidemiological literature reported on that disorder", "Identify the design of each study", and "Assess each study's methods" including "Exposure assessment methods and potential biases", "Disease ascertainment methods and potential biases", "Absence of significant uncontrolled confounders; consideration of residual confounding", "Addressing of other potential biases", "Adequacy of biostatistical methods and analytical techniques", and "Ascertainment of statistical significance — degree to which chance may have produced those results". The table then directs to "Assess the studies using the Updated Hill's Criteria" including "Temporality", "Strength of association", "Dose–response", "Consistency", "Coherence", "Specificity", "Plausibility", "Reversibility", "Prevention/Elimination", "Experiment", and "Predictive Performance", and to make a "Conclusion regarding the degree to which such a causal association is/is not met".

---

### Finding a constrained number of predictor phenotypes for multiple outcome prediction [^46a488ae]. BMJ Health & Care Informatics (2025). High credibility.

The simplest-case LR and most-comprehensive-case LR designs have been used previously to provide relative bounds to put external validation performance into context. We chose the same designs to enable us to put the performance of the constrained models into context.

Models were developed using the standard PatientLevelPrediction process.75% of labelled data were used to learn the model with threefold cross-validation to pick the optimal hyper-parameters, and 25% of the labelled data were used to internally validate the models.

Performance was estimated using the commonly used discrimination metric, the area under the receiver operating characteristic curve (AUROC). AUROC is a measure that determines how well the models rank patients by order of risk. A value of 1 means perfect ranking and a value of 0.5 means random ranking. The AUROC can be thought of as the probability that a randomly selected patient who develops an outcome during the time at risk will be assigned a higher risk by the model compared with a randomly selected patient who does not develop the outcome during the time at risk.

Feature importance for the constrained set of candidate predictors

The coefficient for a specific predictor in a logistic regression model corresponds to how much the predictor impacts the prediction (when the predictors are normalised). To estimate the importance of each constrained predictor, we calculated the median and mean absolute value of a predictor's logistic regression coefficients across the models developed using different databases and for different prediction tasks. In addition, we calculate how often the predictor has a non-zero coefficient in the logistic regression models.

---

### Rapid and sensitive digital droplet PCR assays for detecting HPV16 DNA in liquid biopsies [^a2e8933f]. Journal of Medical Virology (2025). Medium credibility.

Figure 2
Detection of HPV16 DNA in purified cfDNA. (A, B) Purified cfDNA from paired plasma and serum samples showing the numbers of detected droplets (A) and detection of HPV16‐positive cases (B). Right panel selectively showed the droplet counts for paired cases with droplet counts less than 150 in the left panel (A). (C, D) Purified cfDNA from paired plasma and surgical drain fluid (SDF) samples showing the numbers of detected droplets (C) and detection of HPV16‐positive cases (D). Right panel selectively showed the droplet counts for paired cases with droplet counts less than 5000 droplets in the left panel (C). A sample is considered positive with the presence of a minimum of one HPV16 DNA‐positive droplet in a single ddPCR reaction (B, D) with black box represents HPV16 DNA‐positive sample and white box represents HPV16 DNA‐negative sample. p ‐values were assessed by Wilcoxon matched‐pairs signed rank test with p < 0.05 considered having a significant difference (A, C). p ‐values were assessed by Fisher's exact test with p < 0.05 considered having a significant difference (B, D). p ‐val: p ‐value; NS, not significant.

3.3 Direct Detection of HPV16 DNA in Unpurified Liquid Biopsy

Given that cfDNA purification is a relatively time‐consuming and costly process, we evaluated the feasibility of directly detecting HPV16 DNA in plasma, serum, and SDF without cfDNA extraction (unpurified cfDNA). To assess potential sample inhibition, we spiked HPV16 plasmid DNA into HPV16 DNA‐negative liquid biopsies and compared HPV16 DNA‐positive droplet counts between non‐heated unpurified specimens and unpurified specimens subjected to 3 min of heat treatment. In samples spiked with 100 HPV16 plasmids, heated plasma, serum, and SDF samples showed average droplet counts of 17, 9, and 19, respectively, and were substantially higher than their nonheated counterparts, which displayed average droplet counts of 5, 6, and 15, respectively (Figure 3A). Heat treatment did not affect the baseline fluorescence of plasma, serum, or SDF, as all tested specimens, including non‐heated samples and controls, showed base fluorescence below 0 (Figures S4A–C).

---

### A rational approach for selection of optimal covariate-based dosing strategies [^ff10e1ca]. Clinical Pharmacology and Therapeutics (2003). Low credibility.

Background

At present, there is no rational approach for choosing a dosing strategy for individualization based on a covariate. An approach to use in establishment of an a priori dosing strategy for individualization is presented. Factors influencing the choice of such a dosing strategy are identified.

Methods

The approach requires definition of the following: target variable, seriousness of deviations from the target (ie, risk function), population model, covariate distributions, and constraints. Minimizing the total risk yields an optimal dosing strategy, estimated as dose sizes for different subpopulations and covariate cutoff values at which doses are increased or decreased. The method was illustrated with the use of simulated and real drug examples for the situation in which clearance is related to creatinine clearance.

Results

The estimated optimal cutoff(s) paralleled the median creatinine clearance in the population. The extent of variability in clearance explained by creatinine clearance was the main factor influencing the optimal ratios between adjacent dose sizes. An optimal dosing strategy was possible to estimate for the real drug.

Conclusions

The method is simple to perform, although one difficulty lies in defining the target variable and risk function. Our results imply that commonly used constraints in dosing strategies based on renal function (ie, dose ratio of 2 and predetermined cutoffs) are nonoptimal in the sense we propose. Because an optimal dosing strategy may not be practical to use, the therapeutic cost that would result with any constraint can be assessed by comparison of the outcome after the desired and the optimal strategy.

---

### Exploiting common patterns in diverse cancer types via multi-task learning [^c2b31eec]. NPJ Precision Oncology (2024). Medium credibility.

Evaluation metrics

AUROC

The Area Under the Curve of the Receiver Operating Characteristic curve for a binary classification model calculates the proportion of all positive-negative pairs that are correctly classified. The ROC curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various thresholds. These rates are calculated as follows:where T P is the number of true positives, and F N is the number of false negatives.where F P is the number of false positives, and T N is the number of true negatives.

The AUROC can be interpreted as the probability that the classifier will rank a randomly chosen positive instance higher than a randomly chosen negative instance. A higher AUROC value indicates better model performance, with 1 being a perfect classifier and 0.5 indicating a performance no better than random chance. We calculate the AUROC using the following formula:hereandrepresent the negative and positive classes and ∣ D 0 ∣ and ∣ D 1 ∣ the number of samples in each class.represents the indicator function

AUPRC

The precision-recall curve plots the recall on the x axis and Precision on the y-axis for different threshold values. It measures how well a classifier identifies positive cases. As a result, the baseline for this metric is not fixed and depends on the proportion of positive instances in the dataset. The Precision (P) corresponds to the ratio of true positive predictions versus the total number of positive predictions (both true positives and false positives), and it is given by the following formula:

In contrast, the Recall (R) corresponds to the ratio of true positive predictions to the total number of actual positives in the data, calculated using:here T P is the number of true positives, F P is the number of false positives, and F N is the number of false negatives.

Finally, since we are in a discrete setting, the AUPRC is calculated by the following formula:where P n and R n are the Precision and Recall for a threshold with index n.

C-index

The C-index quantifies the correlation between observed survival times and predicted risk scores. A pair is defined as concordant if the patient with the shorter survival time also has a higher risk score. In this study, the C-index is calculated using:where i and j denote distinct patients, T i and T j represent their survival times, η i and η j are their corresponding model-predicted risk scores, andthe indicator function.

---

### Analysis of dose distribution in the 'rind' – a volume outside the PTV – in 3-dimensional conformal radiation therapy of non-small cell lung cancer [^a6ab95bc]. Radiotherapy and Oncology (2003). Low credibility.

Background and Purpose

Appropriate planning target volume (PTV) definition is critical for local disease eradication in the treatment of non-small cell lung cancer (NSCLC). When margins are added to the gross tumour volume (GTV) in the standard way, the PTV formed may be too large to facilitate dose escalation due to normal tissue tolerance. To increase the feasibility of dose escalation with 3-dimensional conformal radiotherapy (3DCRT), this study examines an alternative method for the formation of the PTV in NSCLC. This strategy is based on the reduced probability of tumour cells from the GTV outwards and on the associated lower dose requirements to eradicate such subclinical disease.

Materials and Methods

3DCRT plans were generated from the CT scans of 15 patients with NSCLC (stages Ib to IIIb). Each PTV was formed by adding a margin for geometric uncertainties directly onto the GTV. The success of this approach is dependent on the volume immediately outside this smaller PTV, the Rind volume, receiving 50 Gy, the minimum dose requirement that is considered sufficient for eradication of the reduced tumour cell density in this volume. While optimizing the treatment plans for each PTV to 70 Gy, the dose distribution in the Rind volume, and the factors affecting it, were assessed.

Results

One hundred percent of each PTV received a minimum of 95% of the prescribed dose. The percentage of the Rind volume receiving 50 Gy or more (V50) had a median value of 94%. The minimum dose in this volume, however, ranged from 5.6 to 32.1 Gy. The V50 was highest for apical tumours (96.1%) and lowest for peripheral tumours (86%) and correlated positively with the size of the PTV (Kendall's rank correlation (Kt) = +0.3, P = 0.05) and the number of beams used (Kt = +0.3, P = 0.03) but not with the conformity index. The average volume outside the Rind which still received ≥ 50 Gy (the Wasted 50 Gy) increased significantly with the V50 of the Rind volume and was inversely proportional to the Rind < 50 Gy, correlating significantly with the dose to the organs at risk.

Conclusions

Using this strategy with standard 3DCRT, all PTVs were irradiated to the required dose with this approach, but none of the corresponding Rind volumes had an acceptable dose distribution. The addition of dual volume planning or the use of intensity modulated radiation therapy may achieve an appropriate dose distribution in the Rind volume while not increasing the dose to the organs at risk and may thereby facilitate dose escalation.

---

### Optimal number of heartbeats required for representing left chamber volumes and function in patients with rate-controlled atrial fibrillation [^d04c7a9a]. Journal of the American Society of Echocardiography (2019). Medium credibility.

Background

The optimal number of heartbeats required for representing left heart chamber function in patients with atrial fibrillation (AFib) has not been extensively studied.

Methods

To determine the optimal number, we performed an automated quantification analysis of three-dimensional echocardiography (3DE) data sets in 93 patients with AFib for whom 10–20 consecutive one-beat full-volume 3DE data sets were acquired twice. We measured left ventricular end-diastolic volume (LVEDV), left ventricular ejection fraction (LVEF), and maximal left atrial volume (LAVmax) in each heartbeat; each parameter was averaged using a serial number of heartbeats randomly selected, and these values were compared with the averaged value obtained from the entire set of heartbeats. Coverage probability was determined using predefined cutoff values, the relative percentage differences in LVEDV and LAVmax of 5%, and the absolute percentage differences in LVEF of 5%. The optimal number of heartbeats was defined as the minimum number of heartbeats showing coverage probability ≥ 95%.

Results

Out of 93 patients, 73 patients had acceptable left ventricular contour casts (feasibility, 78%), and 79 patients had acceptable left atrial contour casts (feasibility, 85%). Using the aforementioned criteria, the minimum optimal number of heartbeats was nine for LVEDV and six for LAVmax. The corresponding minimum optimal number of heartbeats for LVEF was eight. However, the results varied as a function of the size of the chamber, the left ventricular function, and whether the AFib ventricular rate was controlled.

Conclusions

In patients with AFib, the optimal number of heartbeats required to obtain representative chamber volumes and function was six to nine heartbeats randomly selected using 3DE automated quantification software.

---

### Lymph node recovery from colorectal tumor specimens: recommendation for a minimum number of lymph nodes to be examined [^89822b52]. World Journal of Surgery (2002). Low credibility.

Lymph node involvement is the most important prognostic factor for patients who have undergone radical surgery for colorectal carcinoma. An accurate examination of the surgical specimens is mandatory for the correct assessment of the lymph node status of the tumor. The risk of understaging is particularly high for patients with tumors classified as Dukes B (TNM stage II). The aim of this study was to determine if a specified minimum number of lymph nodes examined per surgical specimen could have any effect on the prognosis of patients who had undergone radical surgery for Dukes B colorectal cancer. Between 1988 and 1995 a total of 140 patients underwent radical resection of Dukes B colorectal cancer by the same surgeon (C.C.). The relation between clinicopathologic variables and survival was estimated using the Kaplan-Meier method. The Cox proportional hazard regression model was used to identify the variables that can independently influence survival. A median of 12 lymph nodes (range 3–38) was examined per tumor specimen. The 5-year survival rate of Dukes B patients who had had eight or fewer lymph nodes examined after surgery was 54.9%, whereas the survival rate for those who had had nine or more lymph nodes examined was 79.9% (p < 0.001). Cox regression analysis identified the number of lymph nodes as the only independent prognostic factor (p = 0.01). Seventy patients with one to four metastatic lymph nodes (Dukes C patients) who had been operated on during the same period were included in the survival analysis for comparison. The 5-year survival rate of the Dukes B patients with eight or fewer lymph nodes examined was similar to that of the 70 Dukes C patients (54.9% and 51.8%, respectively). Examination of eight or fewer lymph nodes in Dukes B colorectal patients may be considered a high risk factor for missing positive lymph nodes in the surgical specimens. Our results suggest that harvesting and examining a minimum of nine lymph nodes per surgical specimen may be sufficient for reliable staging of lymph node-negative tumors.

---

### Chronic kidney disease diagnosis using decision tree algorithms [^8f646754]. BMC Nephrology (2021). Medium credibility.

Confusion matrix

The confusion matrix is a tabular representation of predictions made by a model. It shows a number of incorrect and correct predictions. These are calculated by comparing the classification results n-test data. The representation of the matrix is in the form of x-by-x, where, x is the number of classes in the dataset. Confusion matrix is a very strong tool to calculate the accuracy of a classifier.

In Table 3, TP A represents the true positive values, which means that they predicted values correctly predicted as actual positive values in class A. TP B represents that the predicted values correctly predicted as actual positive values in class B. TP C represents the true positive values, which means that predicted values correctly predicted as actual positive values in class C. E AB are the samples of class A which are misclassified as B. E AC are the samples of class A which are misclassified as C. E BA are the samples of class B which are misclassified as A. E BC are the samples of class B which are misclassified as C. E CA are the samples of class C which are misclassified as A. E CB are the samples of class C which are misclassified as B.

Table 3
Confusion Matrix for Multi-Class Classification

In Tables 4 and 5, a represents CKD Stage 2 (mildly reduced kidney function), b represents CKD Stage 1 (normal kidney function or structural abnormalities), C represents CKD stage 3B (moderately reduced kidney function), D represents CKD stage 4 (severely reduced kidney function), E represents CKD stage 3A (moderately reduced kidney function), F represents CKD Stage 5 (end stage kidney failure). FN A is False Negative in class A. FN A is calculated by using the formula FN A = E AB + E AC. FP A is False Positive in class A and calculated by using the formula FP A = E BA + E CA.

Table 4
Confusion Matrix for J48

Table 5
Confusion Matrix for Random Forest

---

### Primer on binary logistic regression [^73680247]. Family Medicine and Community Health (2021). Medium credibility.

Family medicine has traditionally prioritised patient care over research. However, recent recommendations to strengthen family medicine include calls to focus more on research including improving research methods used in the field. Binary logistic regression is one method frequently used in family medicine research to classify, explain or predict the values of some characteristic, behaviour or outcome. The binary logistic regression model relies on assumptions including independent observations, no perfect multicollinearity and linearity. The model produces ORs, which suggest increased, decreased or no change in odds of being in one category of the outcome with an increase in the value of the predictor. Model significance quantifies whether the model is better than the baseline value (ie, the percentage of people with the outcome) at explaining or predicting whether the observed cases in the data set have the outcome. One model fit measure is the count-, which is the percentage of observations where the model correctly predicted the outcome variable value. Related to the count-are model sensitivity — the percentage of those with the outcome who were correctly predicted to have the outcome — and specificity — the percentage of those without the outcome who were correctly predicted to not have the outcome. Complete model reporting for binary logistic regression includes descriptive statistics, a statement on whether assumptions were checked and met, ORs and CIs for each predictor, overall model significance and overall model fit.

---

### The interaction index: a measure of drug synergism [^ab8fcaf0]. Pain (2002). Low credibility.

Two drugs used in combination may produce enhanced or reduced effects. The degree of enhancement or reduction is measured from the interaction index (gamma), a quantity that indicates the changed potency of the combination. The index is therefore a quantitative marker for the drug combination and effect metric used. Methodology for measuring the interaction index utilizes the combination and individual drug dose-effect data suitably modeled by regression techniques that most often produce linear plots of effect on log dose from which isobolar analysis is employed. The isobologram provides a simple and convenient graphical assessment of the interaction index but an independent statistical analysis is needed to assess its precision. In some cases, the relative potency of the constituent drugs is the same at all effect levels. When this is so, it is shown that the interaction index can be measured by either an isobolar or an alternate method that is illustrated here. These calculations demonstrate that these different methods of analysis yield the same value of gamma, and do so with comparable precision.

---

### Summary benchmarks-full set – 2024 [^e2c40ddc]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — GRADE recommendation categories are specified as: "Strong recommendation (SR): Used when the desirable effects of an intervention clearly outweigh the undesirable effects or clearly do not" and "Discretionary recommendation (DR): Used when the trade-offs are less certain — either because of low-quality evidence or because evidence suggests that desirable and undesirable effects are closely balanced".

---

### Designing COVID-19 mortality predictions to advance clinical outcomes: evidence from the department of veterans affairs [^03f3df87]. BMJ Health & Care Informatics (2021). High credibility.

Consider, for example, the AUROC with only age vs the full model, which contains medical conditions, vital signs, and labs. While the AUROC between the two are nearly identical (0.84 vs 0.87), the full model has a substantially higher AUPRC, F1 score, and recall score. For example, the AUPRC and F1 score grow from 0.17 and 0.16 to 0.41 and 0.40, respectively, which is over a two-times order of magnitude increase. We focus on not only who dies (ie, sensitivity = true positives / (true positives+false negatives), but also who recovers (ie, true negatives = true negatives / (true negatives+false positives). The inclusion of chronic conditions, and to a larger extent acute conditions, helps increase the performance of the model, the inclusion of vital signs and labs are the features that improve the model the most. Given that many of the studies in this emerging literature on COVID-19 have focused on AUROC as a metric for evaluating model performance, we view our broader set of metrics as not only a form of model validation, but also a contribution in and of itself for obtaining more reliable predictions.

While there is no strict AUROC and AUPRC threshold for defining reliable models, it is important to focus on the AUPRC in settings with an imbalanced dataset. For example, here we have a small share of patients who died from COVID-19, which puts the AUPRC in perspective, since they show the number of true positives among positive predictions. In this sense, given a mortality rate of 0.043, the baseline AUPRC is 4.43%, so our actual AUPRC of 0.41 is well above what a classifier would predict randomly. Moreover, to better understand the quality of our predictions, figure 6 plots the distribution of the risk factors (eg, convalescence and mortality) across patients with the associated CI. Although we see significant dispersion in the risk factors, the CIs are still fairly narrow, suggesting that these predictions have been reliably estimated.

Figure 6
Department of Veterans Affairs. The figure reports the distribution of our predicted risk factor and convalescence with their associated confidence intervals.