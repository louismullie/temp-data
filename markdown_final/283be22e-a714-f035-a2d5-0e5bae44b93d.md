# Medical Question & Answer

**Sample ID**: 283be22e-a714-f035-a2d5-0e5bae44b93d
**Dataset Index**: 36594

---

## Question

Solve the following equation:

√[x − √(1 − x)] + √x = 1

---

## Answer

> Let's see… What do we have here? The user is asking me to solve the equation (√[x − √(1 − x)] + √x = 1). Let's break this down step-by-step. First, I need to think about the domain restrictions imposed by the square roots. Then, I should isolate one radical and square both sides to eliminate it. Next, I will simplify and isolate the remaining radical, square again, and solve the resulting polynomial. After that, I must verify each candidate solution in the original equation to catch extraneous roots. Finally, I will summarize the solution set and the reasoning steps with appropriate citations where applicable.

> Let me first confirm the domain constraints so I don't proceed with invalid inputs. For (√x) to be real, I need (x ≥ 0). For (√(1 − x)) to be real, I need (x ≤ 1). And for (√[x − √(1 − x)]) to be real, I need (x − √(1 − x) ≥ 0), which implies (x ≥ √(1 − x)). Squaring both sides of that last inequality gives (x² ≥ 1 − x), or (x² + x − 1 ≥ 0). Solving the quadratic (x² + x − 1 = 0) yields roots ([−1 ± √5]/2), so the inequality holds when (x ≤ [−1 − √5]/2) or (x ≥ [−1 + √5]/2). Given (0 ≤ x ≤ 1), the feasible domain is (x ≥ [−1 + √5]/2 ≈ 0.618), so I should only consider (x ∈ [[−1 + √5]/2, 1]) going forward [^notfound].

> Now, I will isolate one of the radicals to prepare for squaring. Let me rewrite the equation as (√[x − √(1 − x)] = 1 − √x). Before squaring, I should confirm that the right-hand side is non-negative, because squaring can introduce extraneous solutions if both sides are not non-negative. Since (x ≤ 1), (√x ≤ 1), so (1 − √x ≥ 0) is satisfied, which means squaring is valid here [^notfound].

> Next, I should square both sides carefully. Squaring gives (x − √(1 − x) = (1 − √x)² = 1 − 2√x + x). I can subtract (x) from both sides to obtain (−√(1 − x) = 1 − 2√x), and then multiply by (−1) to get (√(1 − x) = 2√x − 1). Wait, let me verify the sign change and the algebra; yes, that looks correct [^notfound].

> Hold on, I should verify the non-negativity condition for the new equation before squaring again. The left side (√(1 − x)) is non-negative by definition, so the right side must also be non-negative: (2√x − 1 ≥ 0), which implies (√x ≥ [1/2]), or (x ≥ [1/4]). Given our earlier domain restriction (x ≥ [−1 + √5]/2 ≈ 0.618), this new condition is automatically satisfied, so squaring again is safe [^notfound].

> I will now square both sides again to eliminate the remaining radical. Squaring (√(1 − x) = 2√x − 1) yields (1 − x = (2√x − 1)² = 4x − 4√x + 1). Simplifying, I get (1 − x = 4x − 4√x + 1), which reduces to (−x = 4x − 4√x), and then (5x = 4√x). Let me double-check that simplification; yes, that's correct [^notfound].

> Now, I need to solve (5x = 4√x). I can divide both sides by (√x) (since (x > 0) in our domain) to get (5√x = 4), so (√x = [4/5]), and thus (x = [16/25]). But wait, what if (x = 0)? I should confirm that (x = 0) is not a solution; plugging (x = 0) into the original equation gives (√[0 − √1] + √0 = √(−1) + 0), which is not real, so (x = 0) is excluded and division by (√x) is valid here [^notfound].

> I should confirm that ([16/25]) lies within the feasible domain. Since ([16/25] = 0.64) and ([−1 + √5]/2 ≈ 0.618), we have (0.64 ≥ 0.618), so the candidate is within the allowed interval. Next, I should verify the solution in the original equation to ensure it is not extraneous. Plugging (x = [16/25]) into the left side: (√[[16/25] − √(1 − [16/25])] + √[16/25] = √[[16/25] − √[9/25]] + [4/5] = √[[16/25] − [3/5]] + [4/5] = √[[16/25] − [15/25]] + [4/5] = √[1/25] + [4/5] = [1/5] + [4/5] = 1), which matches the right-hand side. Therefore, (x = [16/25]) is indeed a valid solution [^notfound].

> Let me reconsider whether there could be other solutions. The squaring steps can introduce extraneous roots, but in this case the domain restrictions and the algebraic flow led to a unique candidate that satisfied all constraints and the original equation. I should double-check that no other branch or case was overlooked; given the strict domain and the linear-in-(√x) form after simplification, uniqueness is reasonable here [^notfound].

> Final answer: The solution to the equation is ([16/25]) [^notfound].

---

The equation (√[x − √(1 − x)] + √x = 1) has a unique solution: **(x = [16/25])**. To solve it, isolate one radical, square both sides, and repeat until the radicals are eliminated. Then solve the resulting polynomial and verify the solution against the original equation and domain constraints. The domain requires (0 ≤ x ≤ 1) and (x ≥ √(1 − x)), which (x = [16/25]) satisfies.

---

## Step 1: Determine the domain

For the equation to be defined, the following must hold:

- (x ≥ 0) (from (√x))
- (1 − x ≥ 0 ⇒ x ≤ 1) (from (√(1 − x)))
- (x − √(1 − x) ≥ 0 ⇒ x ≥ √(1 − x))

Squaring the last inequality gives (x² ≥ 1 − x), or (x² + x − 1 ≥ 0). The roots of (x² + x − 1 = 0) are (x = [−1 ± √5]/2), so the inequality holds for (x ≤ [−1 − √5]/2) or (x ≥ [−1 + √5]/2). Combining with (0 ≤ x ≤ 1), the domain is (x ∈ [[−1 + √5]/2, 1]).

---

## Step 2: Isolate and square radicals

Isolate the inner radical and square both sides:

√[x − √(1 − x)] = 1 − √x

Square both sides:

x − √(1 − x) = (1 − √x)² = 1 − 2√x + x

Simplify:

−√(1 − x) = 1 − 2√x

Multiply by (-1):

√(1 − x) = 2√x − 1

---

## Step 3: Square again and solve

Square both sides again:

1 − x = (2√x − 1)² = 4x − 4√x + 1

Simplify:

1 − x = 4x − 4√x + 1

−5x + 4√x = 0

√x(4 − 5√x) = 0

This gives (√x = 0) or (√x = [4/5]), so (x = 0) or (x = [16/25]).

---

## Step 4: Verify solutions

Check (x = 0):

√(0 − √(1 − 0)) + √0 = √(−1) + 0

This is undefined, so (x = 0) is extraneous.

Check (x = [16/25]):

√([16/25] − √(1 − [16/25])) + √([16/25]) = √([16/25] − [3/5]) + [4/5] = √([1/25]) + [4/5] = [1/5] + [4/5] = 1

This satisfies the original equation.

---

## Final answer

The unique solution is **(x = [16/25])**.

---

## References

### An optimization framework to guide the choice of thresholds for risk-based cancer screening [^d35bc780]. NPJ Digital Medicine (2023). Medium credibility.

Optimization

The conditional model of advanced cancer risk may be used to estimate the best screening regimen j = 1,…, m for individual i = 1,…, n given constraints on resources through the following optimization model.

Denote the binary decision variable x i j, where x i j = 1 if screening regimen j is chosen for individual i, and x i j = 0 if not. Our objective is to minimize the expected advanced cancer detection ratefor the strategy defined by the n × m matrix X.

The constraints are:
The decision variable is binary: x i j ∈ (0, 1) for all i = 1,…, n; j = 1,…, m.
One screening regimen per person: ∑ j x i j = 1 for i = 1,…, n.
Resources are constrained: ∑ i ∑ j h i j x i j ≤ H where h i j is the cost associated with screening strategy j for individual i, and H is the total cost. For instance, h i j might be the number of screens done, and this is constrained to a fixed total number H.

Technically, this form of optimization problem is called an integer programme, and may be solved using standard algorithms.

---

### To infinity and some glimpses of beyond [^0f9a5f29]. Nature Communications (2017). Medium credibility.

Results

Ordinary differential equations

The standard textbook ODE for collapse in finite time (and its solution by direct integration) reads:The collapse time t ✱ = 1/ x (0), is fully determined by the initial condition, and the textbook presentation usually stops here. A numerical solver would overflow close to (but before reaching) t *; yet we can bypass this infinity by appropriately transforming the dependent variable x near the singularity. Indeed, the good quantit y y ≡ 1/ x ≡ x −1, satisfies the good differential equation d y /d t = −1; this equation will help cross the infinity (for x) by crossing zero and smoothly emerging on the other side (for y). Once infinity is crossed, we can revert to integrating the initial (bad, but now tame again) equation for x.

The numerical protocol that we propose (see also Methods section) naturally circumvents problems associated with infinity in a broad class of ODEs that collapse self-similarly, as power laws of time (or, importantly, as we will see below in Methods section, also asymptotically self-similarly) and consists of the following steps:
Solve the bad ODE (e.g. Eq. (1)) for a while, continuously monitoring, during the integration, its growth toward collapse.
If/when the approach to collapse is detected, estimate its (asymptotically) self-similar rate (the exponent of the associated power law, e.g. −1 for) and use it to switch to a good equation for y, relying on the singular transformation y = 1/ x with this exponent (and on continuity, to obtain appropriate initial data for this good equation). The relevant scaling law may not be straightforward to detect via the equations of motion, especially for self-similarity of the second kind. Nevertheless, a numerical identification utilizing, e.g. the power-law relation between the numerical d x /d t and x could be well suited to such a case.
Run this good equation for y until 0 (or ∞ for the former, bad equation) is safely crossed, computationally observing for x an (asymptotically) self-similar return from infinity.
Finally, transform back to the bad equation (now tamed, as infinity has been crossed) and march it further forward in time.

---

### Benefit of COVID-19 vaccination accounting for potential risk compensation [^653a8ecb]. NPJ Vaccines (2021). Medium credibility.

ARfs − (1 − E)(1 − (1 − A) X)Rfs = 0, and solving for X, this becomesFor the benefit to be halved because of risk compensation, one needs to have

2(ARfs − (1 − E)(1 − (1 − A)] X)Rfs) = ARfs − (1 − E)ARfs, and solving for X this becomesOf note, when H = 1, in the presented framework both the benefit for the index vaccinated person and for the other people are eliminated (or halved) with the same amount of risk compensation. In fact, in a setting where other people are also vaccinated, if they do become infected, they may have the same H as the vaccinated index person anyhow. To simplify matters, further calculations assume that H = 1 in the main analyses. Also of note, as more of the other people also get vaccinated, R would decrease accordingly.

Numerical examples

Based on these considerations, the results presented explore the required values of risk compensation X that would diminish the personal, other or total benefit to half or to zero, for a range of values for the other parameters. This includes different values of vaccine efficacy E (low 20%, moderate 60%, high 80%, very high 95%), and different values of A (0.001–50%). The impact of modification of infection fatality rate by vaccination H (none H = 1, doubled H = 2, halved H = 0.5) is also explored.

Reporting summary

Further information on research design is available in the Nature Research Reporting Summary linked to this article.

---

### Heritable epigenetic variation facilitates long-term maintenance of epigenetic and genetic variation [^26bb6531]. G3 (2024). Medium credibility.

Analysis methods

We used Mathematica version 12.2.0.0 to perform analysis of models and used R version 3.5.1 and Adobe Illustrator 2021 for data visualization. The Solve function was used to find solutions when a system could be solved analytically. For numerical solutions, equilibria were determined in Mathematica using the NSolve function by setting p x (t + 1) = p x (t) in Equations (1–3) above for a given parameter set, with p 4 = 1−(p 1 + p 2 + p 3). For a given solution, working precision was set to 10, and solutions were restricted to be bounded by 0 and 1, inclusive.

Stability of equilibrium numerical solutions was determined by calculating the Jacobian matrix of the system via approximating the recursion equations above by the following continuous differential equations:

---

### Applying network theory to epidemics: control measures for Mycoplasma pneumoniae outbreaks [^62c1d8db]. Emerging Infectious Diseases (2003). Low credibility.

Now, solving for Ф′ 1 (x), we find Ф′ 1 (x) = τ w ƒ 1 [1- τ c+ τ c g 1 (Ф 1 (x))] + τ w x ƒ′ 1 [1 - τ c + τ c g 1 (Ф 1 (x))]∙ τ c g′ 1 (Ф 1 (x))∙ Ф′ 1 (x). Hence. We thereby arrive at the following expression for average outbreak size:

Turning next to the size of the giant component, we know that 1 - Ѕ c = Ф 0 (1) = ƒ 0 (1 - τ c + τ c g 1 (Ф 1 (1). Hence Ѕ c = 1 – ƒ 0 (1 - τ c + τ c g 1 (Ф 1 (1). Likewise 1 - Ѕ w = Γ 0 (1) = g 0 (1- τ w + τ w ƒ 1 (Γ 1 (1)) implies Ѕ w = 1 – g 0 (1 - τ w + τ w ƒ 1 (Γ 1 (1)).

---

### SimOutbreakSelection: a simulation-based tool to optimise sampling design and analysis strategies for detecting epidemic-driven selection [^201f7579]. Nature Communications (2025). High credibility.

Calculating viabilities that give rise to a specified bottleneck size

Now assume we need to reduce the population size to X via viabilities while taking into account the genotype counts, i.e. sowhere X is a specific populations size.

Under recessive selection we want to find V a a and V A a and we know that V a a = V A a and therefore we can insert that into equation (1) and solve for V A a :which means we have an expression for V A a (and thus V a a) if we know X, N a a, N A a, N A A, and V A A.

For an additive model we know per definition that V A A - V a a = 2(V A A - V A a). This means thatwhich we can then plug into equation (1):

Similar to recessive selection, we know that in a dominant model, V A a = V A A as per definition and thus we can also use this in equation (1) to solve for V a a:

---

### A continuous-time maxSAT solver with high analog performance [^87d3d16d]. Nature Communications (2018). Medium credibility.

Algorithm description

Here, we give a simple, nonoptimized variant of the algorithm (see flowchart in Supplementary Fig. 2). Better implementations can be devised, for example with better fitting routines, however the description below is easier to follow and works well. Given a SAT problem, we first determine the b parameter as described previously. Step 1: initially we set, Γ min, = and t max. Unless specified otherwise, in our simulations we used Γ min = 100, Γ max = 2 × 10 6, t max = 50. Step 2: to initialize our statistics, we run Γ min trajectories up to t max, each from a random initial condition. For every such trajectory ω we update the p (E, t) distributions as function of the energies of the orthants visited by ω. We record the lowest energy value found. Step 3: starting from Γ = Γ min + 1 and up to Γ max, we continue running trajectories in the same way and for each one of them check: (a) If, set, update p (E, t) and go to Step 4. (b) If Γ just reached, go to Step 4. (c) If Γ = Γ max, output "Maximum number of steps reached, increase Γ max ", output the lowest energy value found, the predictedand the quality of fit for, then halt. Step 4: using the p (E, t) distributions, estimate the escape rates κ (E) as described in the corresponding Methods section. Step 5: the κ (E) curve is extrapolated to the E − 1 value obtaining κ (E − 1) and then using this we predict(as described in another Methods section). Further extrapolating the κ (E) curve to κ = 0 we obtain(see the corresponding Methods section). Step 6: we check the consistency of the prediction defined here as saturation of the predicted values. We call it consistent, ifhas not changed during the last 5 predictions. If it is not consistent yet, we continue running new trajectories (Step 4). If the prediction is consistent, we check for the following halting conditions: (i) Ifthen we decide the global optimum has been found:and skip to Step 7. (ii) If the fitting is consistently predicting(usually it is very close,) we check the number of trajectories that has attained states with, i.e. = . If it is large enough (e.g. > 100), we decide to stop running new trajectories and setand go to Step 7. (iii) Ifthen we most probably have not found the global optimum yet and we go to Step 4. We added additional stopping conditions that can shorten the algorithm in case of easy problems, see Methods corresponding section, but these are not so relevant. Step 7: the algorithm ends and outputs, values, the Boolean variables corresponding to the optimal state found, along with the quality of fit.

---

### Accurate and efficient estimation of local heritability using summary statistics and the linkage disequilibrium matrix [^97407ea9]. Nature Communications (2023). High credibility.

Results

Overview of HEELS

In a nutshell, HEELS uses variant-level statistics, including marginal association statistics and the LD matrix, to estimate SNP-heritability by iteratively solving the REML score equation. Because the score equations solved by HEELS are identical to those solved by GREML, HEELS yields heritability estimates that are as precise as those based on REML-based estimates which typically require individual-level data. We first introduce some notations and defineunder the linear mixed model, assuming individual-level data is available.

Let y be a length- n vector that denotes the phenotypes of n samples. Denote bythe genotype matrix of n individuals based on p markers or SNPs. We standardize X and y such that the variance of the phenotypes is 1 and the variance of each marker-specific genotype vector is 1/ p, or d i a g (X ⊤ X / n) = 1/ p. We use an additive genetic model for the phenotypes as y = X β + ϵ, where β is a p × 1 vector assumed to followand ϵ is a length- n vector distributed as. Under these assumptions, y ~ N (0, V), where the variance-covariance matrix is. SNP-heritability is defined as(Methods).

Now suppose we do not have access to the individual-level data, X and y, and are only provided with the marginal association statistics, S = X ⊤ y and the LD matrix R = X ⊤ X. (For the sake of simplicity of exposition, we omit the scaling byfor S or 1/ n for R, as the scaling does not affect the derivation of the HEELS estimator.) We show that the REML score equations forcan be solved using these summary-level statistics, i.e. S and R only, by applying the Sherman-Woodbury matrix identity to the Anderson's algorithm for solving the variance components (Supplementary Notes). HEELS iterates between updating the Best Linear Unbiased Predictor (BLUP) estimates of the joint effect sizesand updating the variance component estimatesuntil convergence. Let the superscript (t) denote the value of a variable or a parameter at iteration t. The HEELS estimation procedure is as follows:
Update the BLUP joint effect size estimates using:
Updateusing:
Updateusing:

---

### Combinatorial code governing cellular responses to complex stimuli [^f2481e0e]. Nature Communications (2015). Medium credibility.

Methods

Identification of the interaction profiles

The number of ways to rank the numbers e 0, e X, e Y and e X+Y was computed using the recurrence relation A n k = A n− 1 k +k A n− 1 k− 1 which, for n = 4 and summing up for k = 1, 2, 3 and 4 gives 75 possible rankings. Each ranking can be seen as a set of outcomes of the following qualitative pairwise comparisons:
e X versus e 0
e Y versus e 0
e X+Y versus e 0
e Y versus e X
e X+Y versus e X
e X+Y versus e Y

In our framework, the outcome of a qualitative comparison can take on the following three values: 0 (equal numbers), 1 (first larger than the second) and −1 (first smaller than the second). Each of 75 rankings was coded uniquely as a vector of six components describing the qualitative outcome of the comparisons 1–6 as listed above. For example, the vector (0,0,1,0,1,1) corresponds to:

e X = e 0, e Y = e, e X+Y > e 0, e Y = e X, e X+Y > e X, e X+Y > e Y

To identify which rankings were compatible with positive or negative interactions, we considered the equations 1–6 together with the inequalities that define positive and negative interactions:
Δ e X+Y > Δ e X + Δ e Y (positive interaction)
Δ e X+Y < Δ e X + Δ e Y (negative interaction)

which can be written as

(7a) e X+Y − e X − e Y + e 0 > 0

(8a) e X+Y − e X − e Y + e 0 < 0

If a ranking is consistent with a positive (or negative) interaction, the inequality constraints encoded in the corresponding vector can be solved simultaneously with 7a (or 8a). To verify this, we developed a constraint satisfaction that attempts to solve the six constraints of each ranking together with the inequality 7a (or 7b). The solution is searched numerically with a MATLAB linear solver. The variables e 0, e X, e Y and e X+Y were constrained to the interval 2–16, taken as an approximate of the range of expression values from Affymetrix chips in log2 scale. The method is implemented in the MATLAB code and is available upon request.

---

### Pointwise error estimates in localization microscopy [^0dea1e8f]. Nature Communications (2017). Medium credibility.

Covariance-based diffusion estimator

If x k (k = 0, 1,…) is the measured trajectory of a freely diffusing particle with diffusion constant D, the widely used model for camera-based tracking by Berglundpredicts that the measured step lengths Δ x k = x k +1 − x k are zero-mean Gaussian variables with covariances given by

and uncorrelated otherwise. Here, 0 ≤ R ≤ 1/4 is a blur coefficient that depends on how the images are acquired (for example, R = 1/6 for continuous illumination), Δ t is the measurement time-step, and ɛ 2 is the variance of the localization errors.

Substituting sample averages forandand solving for D yields a covariance-based estimator with good performance. If ɛ 2 is known or can be estimated independently, the first relation in equation (7) alone yields a further improved estimate of D. As we argue in Supplementary Note 9, these estimators apply also for variable localization errors if ɛ 2 is replaced by the average.

---

### Parametric and nonparametric statistical methods for genomic selection of traits with additive and epistatic genetic architectures [^292ae6f7]. G3 (2014). Low credibility.

Parametric methods in genome-wide selection

Linear least-squares regression model:

In GS, the main goal is to predict the individual's breeding value by modeling the relationship between the individual's genotype and phenotype. One of the simplest models is:where i = 1… n individual, j = 1… p marker position/segment, y i is the phenotypic value for individual i, μ is the overall mean, X ij is an element of the incidence matrix corresponding to marker j, individual i, m j is a random effect associated with marker j, and e i is a random residual. Typically, the residual term, e, is chosen to have a normal distribution with mean of 0 and variance of. The model for the data vector y can be written as:To estimate (μ, m), we can use least squares to minimize the sum of squared vertical distance between the observed response and the estimated response, which can be represented as | y − Xm | 2 (where | denotes the norm of a vector). The estimate of m obtained by solving the linear equations X ′ X m = X ′ y. Then, it is estimated as. For more details about linear models, the reader can refer to Linear Models in Statistics or Linear Models with R. The elements of the design matrix X depend on the number of different alleles present. For example, individuals having marker genotypes AA, Aa, aa have elements coded as −1, 0, and 1 in X ij, respectively.

---

### Benefit of COVID-19 vaccination accounting for potential risk compensation [^790bc75b]. NPJ Vaccines (2021). Medium credibility.

Methods

Personal benefit of vaccination

In the absence of vaccination, the index personal risk of death is given by the product of the probability of infection A and the infection fatality rate f of the index, i.e. With vaccination, the vaccine decreases the probability of infection to (1− E)-fold, where E is the efficacy of the vaccine. In the presence of risk compensation, the vaccination leads to X times higher exposure load. The increased exposure load aggregates the cumulative impact of meeting more people, for lengthier periods, and/or at higher risk circumstances, e.g. with less adherence to distancing, masks, and/or testing). The model assumes for simplicity that E percentage of indices would be immune and not possible to infect regardless of how much they are exposed, while the remaining 1 − E percentage (the infectable group) can be infected. It also assumes that each additional unit of exposure of equal magnitude to the baseline pre-vaccination exposure changes the chances of remaining uninfected in the infectable group by (1 − A)-fold. With X -fold exposure, the probability of not being infected for the infectable group thus becomes (1 − A) X, and the probability of being infected is 1 − (1 − A) X. It is unknown whether vaccination may affect also by H -fold the infection fatality rate for the index when/if that person does get infected despite having been vaccinated. Therefore, for an index person the personal risk of death with vaccination becomesThe net benefit for the index person isFor this benefit to be eliminated because of risk compensation, one needs to have

A f − (1 − E)(1 − (1 − A) X)f H = 0, and solving for X, this becomeswhile for H = 1, this is log[(1 − A − E)/(1 − E)]/log(1 − A)

For the benefit to be halved because of risk compensation, one needs to have

2(A f − (1 − E)(1 − (1 − A) X)f H) = A f − (1 − E)AfH, and solving for X this becomeswhile for H = 1, this is log[(2 + AE − 2 A − 2 E)/(2(1 − E))]/log(1 − A)

---

### A stochastic vs deterministic perspective on the timing of cellular events [^c3a6938f]. Nature Communications (2024). High credibility.

Case 1: the simple birth–death process

When r = 1, the model given in reaction scheme (8) coincides with a simple birth–death process. We are interested in the mean waiting time for the birth–death process to reach a fixed protein number N, given that the system is started from n initial proteins. Simulations of the simple birth–death process suggest the MFPT is smaller than the deterministic FPT (see Fig. 2 B, but note that only a small sample of trajectories are shown here for visualisation purposes). We now prove this analytically. To begin, we find an analytical solution for the MFPT of the stochastic description of the system, as given by the CME (see Equation (3) of the Supplementary Information).

Starting from Eq. (7), it can be shown (Supplementary Note 2), that the expected waiting time to reach N proteins given the system is started from n ≤ N, is given by, where for real number x and positive integer m, the notationabbreviates x (x − 1)… (x − (m − 1)), the falling factorial of x. In what follows for simplicity we shall set n = 0. The deterministic description of the system is given by the reaction rate equation d τ X = K − X, with X (0) = 0, and solving for X (τ) yields X (τ) = K (1 − e − τ). Solving for τ then gives, Consider some proportion 0 < ρ < 1, and consider the expected waiting time to reach ρ K, that is, some proportion ρ of the steady-state mean. Then Eq. (10) simplifies towhich we note is independent of K, and where the requirement of ρ < 1 enables this to be well defined. Now let us consider Eq. (9) for N = ⌊ ρ K ⌋. For notational simplicity only, we assume that ρ K is an integer, and again observe that the numeratoris bounded above by (ρ K) i +1. Replacing this in Eq. (9) givesRecognising the Taylor expansion ofaround 0 (for ρ < 1), we find that Eq. (12) is the truncated Taylor expansion of Eq. (11). Thus, as each term of the series is strictly positive, it follows that for any proportion ρ of the steady-state mean, the deterministic waiting time is a strict upper bound for the mean waiting times. An example is given in Fig. 2 C for ρ = 0.8. Here we show how the ratio of the stochastic mean waiting time to the deterministic waiting time, η, of the simple birth–death process scales with increasing K. Note that since δ = r = 1, K is equal to the steady-state mean number of molecules.

---

### Enabling 3D CT-scanning of cultural heritage objects using only in-house 2D X-ray equipment in museums [^34040c19]. Nature Communications (2024). High credibility.

The acquisition process can be modeled as a system of equations. The so-called forward operator A Θ contains all the geometric information on the scanning process and therefore depends on the vector Θ, which contains the unknown system parameters, such as the distances between the source, center of rotation, and detector; the projection angles; and the detector tilts. The vector x is the digital representation of the object and b is the projection data acquired. The goal is to find the representation of the image that leads to the acquired projection data, and in the process to minimize the difference between the forward projected image representation and the data:

We first computationally derive the system parameters Θ using the marker-based approach detailed in the next section and then solve equation (1) by using the algebraic SIRT algorithm, a standard iterative reconstruction method in the CT field. The SIRT algorithm operates by performing a gradient descent to minimize the residual, which is determined by forward-projecting the current estimate of the object representation and comparing it to the data.

---

### An analytical theory of balanced cellular growth [^98f39440]. Nature Communications (2020). High credibility.

Without dependent reactants, A is a square matrix with a unique inverse A −1, and x ≡ [P, a] T is the corresponding vector of independent concentrations. Multiplying both sides of the mass balance constraint (1) by A −1, we obtain (Theorem 5)The right-hand side of the mass balance constraint (1) quantifies how much of each component x i needs to be produced to offset the dilution that would otherwise occur through the exponential volume increase at rate μ.quantifies the proportion of flux v j invested into offsetting the dilution of component i, and we thus name A −1 the investment (or dilution) matrix; see Supplementary Fig. 1 for examples. In contrast to the mass-normalized stoichiometric matrix A, which describes local mass balances, A −1 describes the structural allocation of reaction fluxes into offsetting the dilution of all downstream cellular components, carrying global, systems-level information.

From the kinetic equation (Eq. (3)), p j = v j / k j (a), and inserting v j from the investment equation (Eq. (4)) giveswhere ∑ i sums over the total protein and individual reactant concentrations (Theorem 6). Substituting these expressions into the total protein sum (Eq. (2)) and solving for μ results in the growth equation (Theorem 7)

As detailed in "Methods" (Theorems 5–7), a corresponding result also holds for BGSs with dependent reactants. Thus, for any active matrix A with full column rank (in particular for all active matrices of EGSs) and for any corresponding concentration vector x, there are unique and explicit mathematical solutions for the fluxes v, individual protein concentrations p, and growth rate μ. If μ (Eq. (6)) and all individual protein concentrations p j (Eq. (5)) are positive, the cellular state is a BGS; otherwise, no balanced growth is possible at these concentrations.

---

### Using both qualitative and quantitative data in parameter identification for systems biology models [^fc71653e]. Nature Communications (2018). Medium credibility.

Results

An illustration of the potential value of qualitative data

To demonstrate the potential value of qualitative data, we consider a simple case of solving for the coefficients of polynomial functions.

We consider two polynomial functions: y 1 = ax 2 − bx + c and y 2 = dx + e. Suppose we want to solve for the coefficients a, b, c, d, and e, which we will take to be positive. As the ground truth coefficients to be determined, we choose (a, b, c, d, e) = (0.5, 3, 5, 1, 1.5).

Suppose that a limited amount of quantitative information is available. Namely, it is known that the parabola y 1 contains the points (2, 1) and (8, 13), and the line y 2 contains the point (3.5,5). This is not enough information to solve for any of the coefficients because three points are required to specify a parabola, and two points are required to specify a line (Fig. 1a).

Fig. 1
A simple illustration using polynomial functions. We use qualitative and quantitative information to determine the unknown coefficients. a Visualization of the problem. We seek to find the coefficients of equations for a parabola and a line, with the ground truth shown (blue solid curves). Two points on the parabola and one point on the line are known (black dots). These three points are consistent with infinitely many possible solutions (e.g. orange dashed curves). Qualitative information (colored circles, x -axis) specifies whether the parabola is above (+) or below (−) the line. This information limits the possible values of intersection points x 1 and x 2 to the green shaded segments of the x -axis. b Bounds on coefficient values as a function of the number of qualitative points known. Shaded areas indicate the range of possible values of each coefficient

---

### Prospective motion correction improves the sensitivity of fMRI pattern decoding [^05d1c169]. Human Brain Mapping (2018). Low credibility.

2.12 Simulations for comparison of MVPA methods

To allow for a more informed comparison between the two MVPA methods' sensitivity to PMC‐like effects, a set of computational simulations was also conducted and included in Supporting Information, Figure S4. This simulation was designed to closely resemble real dataset's design (alternating block design, 16 s per block, 8 blocks per subrun, 4 subruns per participant, 15 participants in each iteration), and analysis approach.

We first generated a design matrix with two regressors (one regressor per stimulus orientation). For each of 100 iterations, we multiplied this design matrix by a contrast vector to simulate the activation timecourse of a voxel. The activation response to each stimulus orientation was obtained by sampling from independent Gaussian distributions of with mean 0 and standard deviation 1. This generates a contrast vector centered around 0, with an expected mean absolute difference between conditions of 2/sqrt(pi). This expected mean is calculated by integrating abs(x 1 − x 2)P(x 1)P(x 2) across all possible values of x 1 and x 2. This was normalized to give a mean absolute contrast of 1 and repeated 500 times (average number of voxels at 3 mm) to create a simulated, noiseless fMRI dataset for 1 participant. We repeated this procedure separately for each of 15 simulated participants.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^4d58a8e5]. Nature Communications (2025). High credibility.

Algorithm 2

Hierarchical beta process

Given, c and, ▹ computed from data, ▹ number of refinements, ▹ 2-d distribution over end points

generate a discretized realisation.

Procedure:

▹ Initialise the procedure by drawing end point

1: repeat

2: draw

3: Until ▹ PPFs must be increasing

▹ Successively refine the interval

4: for n ∈ 1, 2,…, N do ▹ refinement levels

5: for do ▹ intermediate incrs.

6: compute r, v according to equations (73)

7: solve equations (75) to obtain α and β

8: draw x 1 ~ Beta(α, β)

9:

10: end for

11: end for

12: return

The quantities r and v computed in Algorithm 2 conceptually represent the r atio between two sucessive increments and the v ariance of those increments.

Relevant concepts of Wiener processes

Before introducing the HB process, let us first review a few key properties which stochastic processes must satisfy and which are covered in most standard introductions –. We use for this the well-known Wiener process, and also introduce notation which will become useful when we define the HB process. Since our goal is to define a process for PPFs, we use Φ to denote the independent "domain" variable and restrict ourselves to 1-d processes for which Φ ∈ [0, 1].

For the Wiener process, each realisation is a continuous function. One way to approximate a realisation ofis to first partition the interval into subintervals [0, Φ 1), [Φ 1, Φ 2),…, [Φ n, 1) with 0 < Φ 1 < Φ 2 < ⋯ < Φ n < 1; for simplicity we will only consider equal-sized subintervals, so that Φ k = k Δ Φ for some. We then generate a sequence of independent random increments (one for each subinterval) {Δ W Δ Φ (0), Δ W Δ Φ (Δ Φ), Δ W Δ Φ (2Δ Φ),…, Δ W Δ Φ (1 − Δ Φ)} and define the corresponding realisation as(Within each interval the function may be linearly interpolated, so that W is continuous.)

---

### Dynamics of collective cooperation under personalised strategy updates [^34fbf42c]. Nature Communications (2024). High credibility.

Methods

Evolutionary process

In each round of the game, individuals interact with their neighbours and accumulate the payoffs accordingly. The payoff matrix of the game is given byThe state of network at any given time can be encoded by a binary vector x ∈ {0, 1} N, where x i = 1 denotes that the player i chooses strategy C, otherwise x i = 0 indicates strategy D. Using this representation of the network state x, i 's average payoff is f i (x) = − c x i + b ∑ j p i j x j, where p i j = e i j / k i indicates the probability of a single step random walk from i to j on the network. For a node i with update rate λ i, the probability to be chosen for a strategy update is λ i /Λ, where Λ = ∑ i λ i defines the total rate of update events. It follows that at the end of each round, the probability for a player j to transmit its strategy to i is, where F j (x) = 1 + δ f j (x) indicates the fitness of individual j. Note that the fixation probability does not change when the rate of strategy updates for each individual is identical since the normalised update rates are the same.

Fixation probability

As shown in the Supplementary Note 1, the fixation probability of cooperation is derived by a first-order expression as the neutral fixation probability (1/ N) plus a correction term due to weak selection, namelywheredenotes the reproductive-value-weighted frequency change of cooperation, which is given byHereindicates the summation of the expectation of φ with φ (1) = φ (0) = 0 under neutral drift through time step t = 0 to infinity, namely, whereindicates the neutral probability of the system reaching state x at time step t starting from the initial state with a single uniformly selected cooperator in population with N − 1 defectors. Combining equations (5) and (6), the fixation probability can be expressed aswhere, andrepresents the reproductive-value-weighted frequency of cooperators, where π i is the reproductive value – uniquely solved by Supplementary equation (2), quantifying the expected contribution of site i to the future gene pool under neutral drift. Here η i j satisfies the recurrence relation ofBy letting ρ C > 1/ N, we obtain C * shown in equation (1).

---

### Data-driven control of complex networks [^85fc9bea]. Nature Communications (2021). High credibility.

Results

Network dynamics and optimal point-to-point control

We consider networks governed by linear time-invariant dynamicswhere, anddenote, respectively, the state, input, and output of the network at time t. The matrixdescribes the (directed and weighted) adjacency matrix of the network, and the matricesand, respectively, are typically chosen to single out prescribed sets of input and output nodes of the network.

In this work, we are interested in solving point-to-point control problems; that is, designing open-loop control policies that steer the network output y (t) from an initial value y (0) = y 0 to a desired one y (T) = y f in a finite number of steps T. If y f is output controllable in T steps (a standing assumption in this paper; we refer to Supplementary Note 1 for more details), then the latter problem admits a solution and, in fact, there are many ways to accomplish such a control task. Here, we assume that the network is initially relaxed (x (0) = 0), and we seek the control inputthat drives the output of the network to y f in T steps and, at the same time, minimizes a prescribed quadratic combination of the control effort and locality of the controlled trajectories.

Mathematically, we study and solve the following constrained minimization problem:where Q ≽ 0 and R ≻ 0 are tunable (positive semidefinite and positive definite, respectively) matrices that penalize output deviation and input usage, respectively, and y T = y (T). Problem (2) generalizes the classic (open-loop) linear–quadratic control framework by including the possibility of minimizing a linear function of the state (as opposed to the whole state) in addition to the control input. Further, we remark that increasing R in Eq. (2) leads to optimal control inputs that achieve the desired final state with increasingly smaller magnitudes. Similarly, the matrix Q in Eq. (2) weighs the norm of the output (state), so that increasing Q forces the optimization problem to generate inputs that limit the norm of the output (state), at the expenses of using a larger control input. In particular, if Q = 0 and R = I, thencoincides with the minimum-energy control to reach y f in T steps.

---

### Irreversible electroporation: just another form of thermal therapy? [^78c2155e]. The Prostate (2015). Low credibility.

IRE Case 2: Tissue With (Large) Blood Vessel

Two-needle IRE around a blood vessel can be simulated if blood flow removes the heated blood between pulses, keeping the intima at 37°C. An analytical solution to Equation (1) is available if radial cooling of the vessel wall is approximated by 1-D diffusion in the x-direction (intima at x = 0, Fig. 1), implying that the perivascular tissue becomes a 1-D semi-infinite medium. Using, Equation (1) of page 85, and that each pulse increases the whole perivascular tissue by ΔT 0 °C, solves Equation (1) as ΔT(x, t) = ΔT 0 erf(x/). Thus, as Equation (3), an approximate solution of Equation (1) following N consecutive pulses, at t = (N − 1)f −1 sec, is

Figure 1
IRE around a blood vessel and 1-D heat conduction in the x-direction. The needles are assumed to be placed at 5 mm from the "center of the blood vessel".

---

### Synthesizing theories of human language with Bayesian program induction [^c1292ede]. Nature Communications (2022). High credibility.

Methods

Program synthesis

We use the Sketchprogram synthesizer. Sketch can solve the following constrained optimization problem, which is equivalent to our goal of maximizing P (X ∣ T, L) P (T, L ∣UG):given observations X and bound on the number of rules K.

Sketch offers an exhaustive search strategy, but we use incremental solving in order to scale to large grammars. Mathematically this works as follows: we iteratively construct a sequence of theories T 0, T 1. alongside successively larger data sets X 0, X 1. converging to the full data set X, such that the t th theory T t explains data set X t, and successive theories are close to one another as measured by edit distance:where d (⋅, ⋅) measures edit distance between theories, D t +1 is the edit distance between the theory at iteration t + 1 and t, and we use the t = 0 base casesand T 0 is an empty theory containing no rules. We "minibatch" counterexamples to the current theory (in Eq. (4)) grouped by lexeme, and ordered by their occurrence in the data (e.g. if the theory fails to explain walk/walks/walked, and this is the next example in the data, then the surface forms of walk/walks/walked will be added to X t +1). See Supplementary Methods 3.3.

We implement all models as Python 2.7 scripts that invoke Sketch 1.7.5, and also use Python 2.7 for all data analysis.

---

### COOKIE-pro: covalent inhibitor binding kinetics profiling on the proteome scale [^ada42611]. Nature Communications (2025). High credibility.

Methods

Mathematics

Consider a 2-step irreversible inhibitorbinding to its target protein, the reaction can be described as:

The equilibrium constant(Eq. (2)) for the first step was defined based on the steady state approximation of(Eq. (5)):

The rate constant foradduct formation:

Mass law for total target proteinand non-covalent protein:

Assuming the reaction follows pseudo first-order reaction kinetics without inhibitor depletion, the unbound inhibitor concentrationis a constant value unaffected by enzyme binding.

Since proteomics experiment can only quantify, the goal is to define.

The formation rate ofequals the consumption rate of:

On the other hand, by rearranging Eq. (2) with (9), we solve for:

Therefore, the formation rate ofcan be expressed as:

Combining Eqs. (10) and (12), we get a differential equation for:

This equation can be solved by rearranging and integrating on both sides:

Here, we took advantage of the integration constant, let. After rearranging, we got:

Substitutewithand rearrange, we finally got the equation in Fig. 1A.

MATLAB simulation

MATLAB (version R2022b) and the SimBiology package (v6.4) were used to build and simulate the covalent inhibition model as shown in Eq. (1). The system was built as described in the following 4 ordinary differential equations (ODEs):

The initial value ofwas set at 1e−10 M, titrating from 1e−9 to 1e−5 M with 9 steps on the log scale. Bothandwere initially set as 0. The values of, andwere set as each condition in Fig. 1C. The simulation stop time was set at 86,400 s. ODE15s solver was used for all the simulation.

The simulated datapoints ofwere exported and used to fit forvalues using lsqcurvefit function in the MATLAB. The fitting model = @(params, x) params(1) + (params(2) − params(1)) * (1 − exp(-params(3) * x)), where params(1) = 0, params(2) = 1e−10, and params(3) isto be fitted. The fittedvalues and their correspondingwere used for the second round Michaelis–Menten fitting in GraphPad 10.2 to generate the individualandvalues.

---

### Bacterial motility can govern the dynamics of antibiotic resistance evolution [^c771c187]. Nature Communications (2023). High credibility.

Analytical and numerical techniques

In addition to computer simulations, we developed a combination of analytical and numerical techniques, which are used to compute the evolutionary time, ecological time and the adaptation rate. In contrast to simulations, these techniques are based on closed-form solutions of master equations, numerical methods for solving algebraic equations (such as Newton-Raphson method) and solving ODEs (such as Runge-Kutta method). To illustrate these techniques, we show how to compute the expected waiting times at a resistance state R and the adaptation rate a R in the original staircase model.
Find the wild-type profile N x at a resistance state R. Use Newton-Raphson method to solve the algebraic equations for the non-trivial fixed point of the mean-field theorywhereis the indicator function.
Compute the evolutionary time. A closed-form solution exists and follows from a modification of the theory in Hermsen et al. We consider three independent adaptation paths along which wild-type can produce a first mutant in the overlap region x = R + 1. On the position-genotype lattice (x, g), they are the division path D: (R, R) → (R, R + 1) → (R + 1, R + 1) (mutation at x = R followed by migration to the right); the overlap path O: (R + 1, R) → (R + 1, R + 1) (mutation at x = R + 1); and the no-division path N: (R + 2, R) → (R + 2, R + 1) → (R + 1, R + 1) (mutation at x = R + 2 followed by migration to the left). As the original theoryassumes low motility, it neglects the no-division path since there are no wild-type cells to mutate at position x = R + 2 in this regime (SI Theorem 3). Moreover, we assume that the flux of mutants between neighbouring compartments in the division region x ≤ R and the no-divison region x ≥ R + 2 is equilibrated, so that we can ignore the net probability flux of mutants that enter or exit the compartments x = R − 1, R, R + 1 and also ignore longer paths such as (R − 1, R) → (R − 1, R + 1) → (R, R + 1) → (R + 1, R + 1) (mutation at x = R − 1 followed by double migration to the right). This is a reasonable assumption as the flux of mutant-producing wild-type is equilibrated far from the population front. The computation ofhas the following structure. The waiting times till a mutant is produced in the overlap region along path i ∈ { D, O, N } are denoted by T i (Fig. 2 a). The probability that the waiting will be longer than t isand the probability density function of T i is given by F i (t) = −d S i /d t. Notice thatand that T i are independent. Thus. The probability density function ofis The expected timeis found by numerical integration. Therefore, the problem is solved by finding the functions S i (t) and differentiating them to produce F i (t). According to Hermsen et al. these functions are: For path D: whereandwhere 〈 n (t)〉 D is the average mutant population in compartment x = R at time t For path O: For path N: whereandwhere 〈 n (t)〉 N is the average mutant population in compartment x = R + 2 at time t Finally, if there is a nonnegligible chance that the first mutant in the overlap region dies before division, the evolutionary timemust be corrected to, where q is the probability that the first mutant in the overlap region divides before its death (Supplementary Note 8).
Compute the ecological time. The competition between wild-type and mutants can be described well by the mean-field equations:with initial conditions at time t = 0:In these equations w x and m x stand for the number of wild-type and mutant cells in spatial compartment x, respectively. The system is simulated from the end state of the previous stochastic process when a first mutant appears in the overlap region. This system can be simulated efficiently by a Runge-Kutta (RK4) method. The timeis reached when w R +1 < m R +1 for the first time, i.e. the mutants outcompete the wild-type in the overlap region.
Find the adaptation rate. The adaptation rate is found from

---

### Control of finite critical behaviour in a small-scale social system [^c81152c0]. Nature Communications (2017). Medium credibility.

Independent model inference

The independent model consists of individuals participating in conflict randomly, with the frequencies of individual appearance equal to their empirically measured, heterogeneous values f i = 〈 x i 〉. Naively, this can be written as a relative negative log likelihood, and is equivalent to a maximum entropy model that matches only the frequencies f i. (The relative negative log likelihood L (x) of state x is related to the likelihood p (x) by p (x) = exp(− L (x))/ Z, where Z is a normalization constant set by the constraint that the sum of likelihoods over all states is one. In statistical physics, Z is the partition function and L (x) is proportional to the free energy of state x.)

However, as detailed in 'Operational definitions' in the Methods section, a fight was operationalized for these analyses as involving two or more socially mature individuals. (Observed fights that involved only juveniles and 0 or 1 mature individuals are therefore excluded.) Correspondingly, we forbid our models from producing fights of size smaller than two. We treat this as an additional constraint on the model. The resulting maximum entropy model is then the one in which the likelihood of states with fewer than two individuals present is taken to zero. This corresponds to a relative negative log likelihood

where Θ(z) is 0 when z ≤ 0 and 1 when z > 0.

In the unconstrained case (α = 0), we can easily solve for h i:

We must now solve numerically for h i to match the empirically measured f i = 〈 x i 〉 = 〈 x i 〉 α →∞. To accomplish this, note that the unconstrained model will have modified statistics:

where f 0 is the frequency of size zero fights, f i 1 is the frequency of size one fights consisting solely of individual i, andis the overall frequency of size one fights (all measured in the unconstrained model). In terms of unconstrained individual frequencies, these are

We use an iterative procedure to solve equation (7) for 〈 x i 〉 α = 0, which are then used in equation (6) to find the fields. Finally, samples from the independent model (equation (4)) are produced by sampling using equation (5) and simply discarding samples in which fewer than two individuals appear. For our data, this results in discarding about 17% of samples produced with equation (5).

---

### Adherence to public institutions that foster cooperation [^193d8bdf]. Nature Communications (2021). High credibility.

In summary, for a population following the Stern Judging norm, the equilibrium frequencies of good individuals in each strategic type from the perspective of an arbitrary observer satisfy the following equations:An analogous derivation for Simple Standing yieldsLikewise, for Scoring we haveFinally, for Shunning we haveThese equations correspond to the expressions under private assessment with complete empathy from prior studies, with the exception that g = ∑ i f i g i, the proportion of the population with a good reputation in the eyes of an arbitrary observer, is replaced here by G = ∑ i f i G i, the proportion of the population with a good institutional reputation.

For each social norm, the system of equations above is closed once we specify how G depends on g X, g Y, g Z and on the size and strictness of the institution. We consider two limiting cases: very strict institutions that broadcast an individual as good only if all members agree she is good (q > (Q − 1)/ Q) and very tolerant institutions that broadcast an individual as good provided at least one member views her as good (q < 1/ Q). In these two respective cases we haveFor completeness' sake, we provide the expression for arbitrary q and Q :The resulting system of equations for g X, g Y, and g Z can be solved by radicals when Q ≤ 2. More generally, a unique feasible solution exists for any Q, and it can be computed numerically by iterating the above system of equations after choosing any initial value∈ (0, 1) 3. Successive iteratesremain in [0, 1] 3 and form a contraction: the equations for g X, g Y, g Z are each convex combinations of elements in (0, 1), provided e 2 > 0 and e 1 > 0. Solving this system determines the equilibrium frequency of good individuals of each strategic type from the eyes of an arbitrary observer (g i) and therefore also yields the institutional broadcast (G i, from Eq. (7), and G = ∑ i f i G i). Substituting these expressions into the replicator equation (Eqs. (1) and (2)) provides the dynamics of strategy frequencies f i, allowing us to compute selection gradients, strategic equilibria, and basins of attraction, as shown in Fig. 2.

---

### Optimal free descriptions of many-body theories [^f77fa88f]. Nature Communications (2017). Medium credibility.

Methods

Optimization

The optimization to find σ andin (3) is performed by a Monte Carlo basin-hopping strategyusing the Nelder–Mead simplex algorithm for local minimization within basins of the cost function. This global strategy was selected to counteract an observed tendency for local methods to get trapped in local minima. The initial guess for this search is found as follows. The normalization constant E 0 is the lowest entanglement energy of the input entanglement spectrum { E }. We iteratively construct an approximate set of single-particle entanglement energies starting from an empty set. First, we take the lowest remaining level in the spectrum and subtract E 0 to produce a new single-particle level k. Then we remove the many-body levels, which are closest to the new combinatorial levels generated according to (2) by the additional single-particle level. This process is repeated until the input spectrum is exhausted. We can also introduce a truncation of the entanglement spectrum cutting off high entanglement energies, making the construction of the initial guess terminate faster. The minimization of D (σ, σ f) to identify the optimal free model for the Ising Hamiltonian (5) is calculated using a local Nelder–Mead method.

Finite-size scaling

We perform the finite-size scaling according to an ansatz (4). The parameters of the collapse were estimated using the method of ref. From the scaling ansatz (4) and for a trial set of scaling parameters g c, ν and ζ, the scaled values x L = (g − g c) L 1/ ν andare calculated from each unscaled data point. From this collection of scaled data points (x L, y L) across all L, we implicitly define a so-called master curve that best represents them. This curve y (x) is defined around a point x as the linear regression calculated by taking the scaled data points immediately left and right of x for each system size L. We characterize the deviation of the scaled points (x L, y L) from the master curve y (x L) using the χ 2 statistic. This measure is used as the cost function for an optimization problem over the scaling parameters g c, ν, ζ and θ, which can be solved using the same techniques as the previous problems.

---

### Practical guide for interpreting and reporting cardiac PET measurements of myocardial blood flow: an information statement from the American Society of Nuclear Cardiology, and the Society of Nuclear Medicine and Molecular Imaging [^461b4a68]. Journal of Nuclear Cardiology (2021). High credibility.

Know your software (retention and compartment models) — compartment and retention model approaches for myocardial blood flow (MBF) are described as follows: The compartment model approach for calculating MBF requires a series of dynamic images that record tracer kinetics into and washout from the myocardium and uses a non-linear fit of the model to the acquired data to solve for the regional MBF, tracer washout, and partial volume artifacts. The retention model approach assumes tracer retention is determined by the blood pool concentration and irreversible extraction into the myocardium; the amount of radiotracer retained in the myocardium is corrected for or normalized to the amount of radiotracer delivered via arterial blood ("the arterial input function") and thus the integral of the arterial radiotracer concentration (assumed to be derived from the initial two-minute images), is corrected for the flow-dependent or K1-specific extraction fraction, and by assuming the tracer does not significantly washout determines blood flow based on tracer uptake; this model does not consider washout, assumes a partial volume correction factor, and its shortcoming can be somewhat mitigated by using a short dynamic scan acquisition time.

---

### Cooperative binding of T cell receptor and CD4 to peptide-MHC enhances antigen sensitivity [^00b448cd]. Nature Communications (2022). High credibility.

Two-step model for trimolecular interaction

We propose a two-step model for the formation of trimolecular bonds. Since the affinity and on-rate for the CD4–pMHC interaction are so much smaller than those of the TCR–pMHC interaction, it seems reasonable to assume that TCR interacts with pMHC at the same kinetic rates as bimolecular interaction in the first step as if CD4 does not interact with free pMHC. However, CD4 is able to interact with TCR-stabilized pMHC with a much higher affinity, thereby forming trimolecular bonds in the second step.

Let p m, n be the probability of having m TCR–pMHC bonds and n TCR–pMHC–CD4 bonds, which are governed by the following master equations:

We solved these master equations using a probability generating function, which converts Eq. 10 to a single first-order linear partial differential equation,

We found a general solution to Eq. 12 using the method of characteristics, which iswhere J is an arbitrary function of its two arguments. u 1 and u 2 are functions of x and y:

λ 1 and λ 2 are given by Eq. 2. They can be viewed as the fast (λ 1) and slow (λ 2) rates that control the two p44hases of the two-step interaction. To determine the function of integration J requires initial conditions. If there is no TCR–pMHC or TCR–pMHC–CD4 bonds at t = 0, the initial conditions on p m, n are:

The corresponding initial condition of g, obtained by substituting Eq. 15 into Eq. 11, is

Solving x and y in terms of u 1,2 from Eq. 14, substituting them into Eq. 16, and comparing the resulting equation to Eq. 13 evaluated at t = 0 allows us to solve for a particular solution for J:

---

### Demography and the emergence of universal patterns in urban systems [^36ce729d]. Nature Communications (2020). High credibility.

Methods

Probability solution for geometric random growth with boundary conditions

The Fokker-Planck equation for random geometric growth without drift iswhere P = P [x (t), t ∣ x (0), t 0] is the conditional probability of observing state x of the random variable at time t, given the initial state x (t 0) at time t 0. For simplicity of notation, we have dropped the i indices in x and σ and write x (t 0) as x 0.

Equation (17) has some similarities with the diffusion equation in physics and can analogously be solved exactly. First, it is useful to change variables so as to eliminate the non-linear term x 2. We setand. By changing the variables in Eq. (17) we findThis equation is now linear and can be solved in two ways. The first way is using factorization and solve it as a heat equationandThe second way is to solve it directly via a Fourier transform, so thatwhich leads toThis equation can be solved via a separation of variables, P [k, τ] = f (k) T (τ), which leads to an eigenvalue problem:Solving for k we obtain:withwhere P [y, 0] = f (k) T (0). In particular, there are two stationary solutions, for w = 0, with k = k 0 = 2 i and k = k 1 = i. Substituting k 0, we see that the solution corresponds to, which is Zipf's distribution. The other solution corresponds to the existence of a constant probability current up or down the urban hierarchy associated with different boundary conditions, see main text.

---

### The physical maps for sequencing human chromosomes 1, 6, 9, 10, 13, 20 and X [^214bc9a6]. Nature (2001). Excellent credibility.

We constructed maps for eight chromosomes (1, 6, 9, 10, 13, 20, X and (previously) 22), representing one-third of the genome, by building landmark maps, isolating bacterial clones and assembling contigs. By this approach, we could establish the long-range organization of the maps early in the project, and all contig extension, gap closure and problem-solving was simplified by containment within local regions. The maps currently represent more than 94% of the euchromatic (gene-containing) regions of these chromosomes in 176 contigs, and contain 96% of the chromosome-specific markers in the human gene map. By measuring the remaining gaps, we can assess chromosome length and coverage in sequenced clones.

---

### Vacuum laser acceleration of super-ponderomotive electrons using relativistic transparency injection [^d98e7d9f]. Nature Communications (2022). High credibility.

Test-particle simulation and Poincare-map analysis

The simulation solves the electron dynamics by directly integrating the relativistic equations of motion: where the vectors denote the three Cartesian components and the quantities are normalized as. A perfect light reflector is defined at x = 0 to mimic the foil before the onset of RT. As a result, the laser fields in front of the foil (x < 0) include both the incident laserand the reflected laser, wheredefine the temporal and spatial profiles, H is the Heaviside step function, H 2 = 1 – H, ω 2 is the angular frequency of the reflected laser normalized to that of the incident laser, and the initial phases satisfy. The pulse duration τ and spot radius σ follow that of the PIC simulation. The parameter t d introduces a time delay for the incident laser to arrive at the foil, thus t d < 0 means the laser has impinged on the foil for a period of |t d |. To obtain the Poincare map shown in Fig. 6c, we transform the independent variable of the above equations of motion from t to ξ = ξ 1 + ξ 2 where ξ 1 = t − x + ϕ 1 and ξ 2 = ω 2 (t + x) + ϕ 2 are the phase of the incident and reflected laser, respectively. The resulting equations of motion become, and are solved only at periodic ξ = 2Nπ where.

---

### Reconstructing propagation networks with natural diversity and identifying hidden sources [^8908aad3]. Nature Communications (2014). Medium credibility.

Results

Compressed sensing

The general problem that CST addresses is to reconstruct a vector X ε R N from linear measurements Y about X in the form

where Y ε R M and Φ is an M × N matrix. The striking feature of CS is that the number of measurements can be much less than the number of components of the unknown vector, that is, M < < N, insofar as X is sparse and the number of non-zero components in it is less than M. Accurate reconstruction can be achieved by solving the following convex-optimization problem:

whereis the L 1 norm of X and the matrix Φ satisfies the restricted isometry property. Solutions to the convex optimization are now standard. (More details of the CST can be found in Supplementary Note 1.) Our goal is to develop a framework to cast the problem of reconstructing propagation networks into form (1).

Reconstruction framework

To present our framework in a transparent manner, we first consider the relatively simple case where there is no hidden source. Further, we assume that the disease starts to propagate from a fraction of the infected nodes. As we will see, based on this framework, it is feasible to locate any hidden source based solely on time series after outbreak of infection. The state of an arbitrary node i is denoted as S i, where

Owing to the characteristic difference between the SIS dynamics and CP, we treat them separately (see Methods).

For the SIS dynamics, the probabilityof an arbitrary node i being infected by its neighbors at time t is

where λ i is the infection rate of i, a ij stands for the elements of the adjacency matrix (a ij = 1 if i connects to j and a ij = 0 otherwise), S j (t) is the state of node j at t, and the superscript 01 denotes the change from susceptible state (0) to infected state (1). At the same time, the recovery probability of i is, where δ i is the recovery rate of node i and the superscript 10 denotes the transition from infected state to susceptible state. Equation (4) can be rewritten as

---

### Deep learning for universal linear embeddings of nonlinear dynamics [^317523d7]. Nature Communications (2018). Medium credibility.

Methods

Creating the datasets

We create our datasets by solving the systems of differential equations in MATLAB using the ode45 solver.

For each dynamical system, we choose 5000 initial conditions for the test set, 5000 for the validation set, and 5000–20,000 for the training set (see Table 2). For each initial condition, we solve the differential equations for some time span. That time span is t = 0,.02,… ,1 for the discrete spectrum and pendulum datasets. Since the dynamics on the slow manifold for the fluid flow example are slower and more complicated, we increase the time span for that dataset to t = 0,.05,… ,6. However, when we include data off the slow manifold, we want to capture the fast dynamics as the trajectories are attracted to the slow manifold, so we change the time span to t = 0,.01,… ,1. Note that for the network to capture transient behavior as in the first and last example, it is important to include enough samples of transients in the training data.

Table 2
Dataset sizes

The discrete spectrum dataset is created from random initial conditions x where x 1, x 2 ∈[−0.5, 0.5], since this portion of phase space is sufficient to capture the dynamics.

The pendulum dataset is created from random initial conditions x, where x 1 ∈[−3.1,3.1] (just under), x 2 ∈[−2,2], and the potential function is under 0.99. The potential function for the pendulum is. These ranges are chosen to sample the pendulum in the full phase space where the pendulum approaches having an infinite period.

The fluid flow problem limited to the slow manifold is created from random initial conditions x on the bowl where r ∈[0, 1.1], θ ∈[0, 2 π], x 1 = r cos(θ), x 2 = r sin(θ),… This captures all of the dynamics on the slow manifold, which consists of trajectories that spiral toward the limit cycle at r = 1.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^bb9b7f12]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Δ Φ, the probability distribution at a point Φ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Φ ∈ {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters α and β, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since α and β are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Φ ∈ [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(α, β) and x 2 = 1 − x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Φ, it may seem natural to select α and β by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### The fast-recycling receptor megalin defines the apical recycling pathway of epithelial cells [^216fc71e]. Nature Communications (2016). Medium credibility.

Mathematical model of Megalin endosomal itinerary

To represent the endosomal pathway of Megalin in subconfluent MDCK cells (experiment in Fig. 3), the pools of Megalin within PM, SE Rab4, RE TfR and RE Rab11 are denoted X 0, X 1, X 2 and X 3, respectively. For a given experiment, the measure of Megalin (pixel density) within each compartment is denoted x (i, t), i = 0,3, in which t is experimental time. In this model, Megalin transition from a donor to an acceptor compartment is assumed to be proportional to its density within the donor compartment, so that one can write linear differential equations for mass transfer. Corresponding to the cartoon of Fig. 3c are the equations:

The constant coefficients, k 01, k 12, k 23 and k 30 denote transfer rates for sequential movement of Megalin through the compartments, while k 10 and k 20 correspond to short circuits from SE Rab4 and RE TfR back to the PM. The terms on the right, J ij, denote the unidirectional fluxes between compartments. The system is completed with the requirement that over the time course of the experiments, the total Megalin pool of mass, x T, remains constant:

When equation (4) is used to eliminate x (0) from equation (1), the resulting equation is

Equations (2), (3) and (5) are three linear equations with constant coefficients, which describe the model system. For the experiment of Fig. 3, it is assumed that all of the labelled Megalin starts within the PM, that is, for t = 0, x (1) = x (2) = x (3) = 0. Fitting this model to the data of Fig. 3 means solving for best values of the six model coefficients, k ij. For any set of choices of k ij, the model provides predictions for x (1, t), x (2, t) and x (3, t) for 0 ≤ t ≤ T, where the experiment runs for time, T.

---

### Observation and theory of X-ray mirages [^6bb03466]. Nature Communications (2013). Medium credibility.

Numerical simulations

As already noted, the elaborated theory of the mirage and the mirage equation itself are written for an arbitrary-extended optically inhomogeneous amplifying medium, in particular, for any spatial distribution of n e (x, y, z) and G (x, y, z) of the plasma. For the solution of the whole problem, that is, for calculation of mirage radiation at the output of the plasma and for finding the position and shape of the virtual source in a particular physical situation, it is enough to numerically integrate equations (2 and 3) for given n e (x, y) and G (x, y). To do this, we rewrite the original equation (2) as:

whereand the complex amplitude of an incident field A inc (x, y, z) is considered to be given and the complex amplitude of X-ray mirage A M (x, y, z) = 0 at z = 0. In our opinion, the most effective numerical method for solving such an equation is the split-step method, which we applied for our calculations. For numerical solution of this parabolic equation, we used a rectangular grid of N x × N y elements with the spatial step sizes Δ x and Δ y in the x and y directions correspondingly. Decomposition of the total field into two fields, A M (x, y, z) and A inc (x, y, z), has allowed us to have a limited area where A M (x, y, z) is non-zero and also allowed to use the periodic boundary conditions of the split-step method.

In numerical calculations, we used a diffraction limited Gaussian beam A G (x, y, z) = as an incident radiation A inc (x, y, z). As equation (3) is a special case of equation (2), we used the same numerical procedure for its solution.

The calculation scheme is graphically shown in Supplementary Fig. S3. For the main calculations, we used the computational volume of 65 × 68 × 6,000 μm 3 size with 640 × 670 elements in x − y computational grid. The number of steps along z axis was 670. For the calculations according to the simplified model, described in Supplementary Discussion, the numbers were 200 × 200 × 6,000, 1,024 × 1,024 and 1,024 correspondingly.

---

### A BaSiC tool for background and shading correction of optical microscopy images [^4cb277c4]. Nature Communications (2017). Medium credibility.

Results

BaSiC workflow

Inspired by Smith et al. we build our method on the shading model (equation (1)), which accounts for the effect of both S (x) and D (x). Such a full model is superior as compared to a partial model that considers S (x) only. As shown in the schematic plot of Fig. 1, BaSiC first constructs a measurement matrix I (step I), which is then decomposed into a low-rank matrix I B and a sparse residual matrix I R (step II). The low-rank matrix has a maximum rank of two as each column is the sum of a scaled version of S (x) (with a scaling factor B i) and D (x), which are all initialized with zeros (step III) and optimized by promoting the sparsity of the residual matrix with a reweighted L1-norm (step IV). In addition, smooth constraints are imposed on both S (x) and D (x) by regulating their sparsity in Fourier domain (Supplementary Fig. 1). The optimization is solved in an iterative fashion using the linearized augmented Lagrangian method, which is widely used in sparse matrix decomposition like Robust PCA (ref.) and RASL (ref.) (for a detailed description of the mathematical derivation and matrix updating see Methods and Supplementary Note 3). An automatic parameter setting strategy determines the smooth regularization parameters for S (x) and D (x), adaptive to different image contents, so that tedious manual parameter tuning is avoided (Supplementary Note 4). We provide a statistical interpretation of BaSiC in Supplementary Note 5. With the estimation of S (x) and D (x), we can correct the intensity profile of each image tile of a WSI by reversing the image formation process, equation (1), leading to a homogenous appearance and correct stitching (Fig. 1d versus a).

---

### Insights from a dynamical system approach into the history of atmospheric oxygenation [^2af2a9a7]. Nature Communications (2024). High credibility.

Here µ denotes the number of moles in 1 atmosphere. The oxidation parameter, Ψ, is parameterized as log Ψ = p 1 x 4 + p 2 x 3 + p 3 x 2 + p 4 x 1 + p 5, where x = log(µ p O 2) and coefficients p 1 = 0.00286, p 2 = −0.1608, p 3 = 3.216, p 4 = −26.53, p 5 = 76.04 are resulted from fitting a curve to the previously published data (Figure S1). The details of the goodness of the fit, along with the range of values and their confidence intervals are shown in Figure S1.

While our parameterization scheme for the rate of atmospheric methane oxidation is similar to previous studies, some assumptions in this parameterization can potentially cause uncertainty in the rate of methane oxidation. For instance, this parameterization makes assumptions about the age of the Sun as well as about the surface temperature, both of which effects can potentially influence the photochemistry of CH 4 and O 2. The assumption about the age of the sun can impact the Lyman alpha UV flux, an important factor for kinetics of CH 4 photolysis CH 4 and the Earth's surface temperature can impact the water vapor content of the troposphere with impacts on the abundance of OH and the rate of methane oxidation. To account for the uncertainty associated with these assumptions, we varied the kinetics of atmospheric methane oxidation in our stochastic simulation.

Finally, considering that the methane mass balance equation depends on p O 2, we first solved the methane mass balance equation and obtained the fixed points (steady states) of the methane mass balance equation, and then used those fixed points to solve the oxygen mass balance equation.

---

### Structure and inference in annotated networks [^535f823a]. Nature Communications (2016). Medium credibility.

Computationally, the most demanding part of the EM algorithm is calculating the sum in the denominator of equation (7), which has an exponentially large number of terms, making its direct evaluation intractable on all but the smallest of networks. Traditionally one gets around this problem by approximating the full distribution q (s) by Monte Carlo importance sampling. In our calculations, however, we instead use a recently proposed alternative method based on belief propagation, which is significantly faster, and fast enough in practice for applications to very large networks.

Final likelihood value

The EM algorithm always converges to a maximum of the likelihood but is not guaranteed to converge to the global maximum — it is possible for there to be one or more local maxima as well. To get around this problem we normally run the algorithm repeatedly with different random initial guesses for the parameters and from the results choose the one that finds the highest likelihood value. In the calculations presented in this paper we did at least 10 such 'random restarts' for each network. To determine which run has the highest final value of the likelihood we calculate the log-likelihood from the right-hand side of (6) using P (A | Θ, s) and P (s | Γ, x) as in equation (2), the final fitted values of the parameters Θ and Γ from the EM algorithm, and q (s) as in equation (7). (As we have said, the right-hand side of (6) becomes equal to the left, and hence equal to the true log-likelihood, when q (s) is given the value in equation (7).)

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^832976d6]. Nature Communications (2021). High credibility.

Results

Our results are divided up into 7 sections corresponding to 7 different regulatory networks of increasing complexity. An overall reference table (Table 2) is included in the discussion.

Motif 0: direct Hill regulation

We first describe how we use DDEs to model simple regulation of a gene Y by a gene X as the most basic motif, and then provide a simple yet unified mathematical framework for both activation and inhibition with delay.

Activation and inhibition can both be modeled with a single unified function

We consider a transcription factor x regulating the production of a protein y (Fig. 2). If x activates y, the production rate of y increases with increasing x, generally saturating at a maximum rate α. Often there is an additional cooperativity parameter n which determines how steeply y increases with x in the vicinity of the half-maximal x input value k. In this framework, n = 0 is constitutive production, n = 1 is a Michaelis-Menten regulation, 1 < n < 5 is a typical biological range of cooperativity, and n → ∞ is a step-function regulation. If x represses y, the same conditions hold except that the production rate of y then decreases with increasing x. A standard quantitative model for this behavior is called the Hill function, and serves as a good approximation for many regulatory phenotypes in biology, including transcription and translation rates, phosphorylation, enzymatic activity, neuronal firing, and so forth. In general there may also be a leakage rate α 0 that yields constant y production in the absence of x. The concentration of y is also generally removed at a rate β proportional to its concentration. This removal term can represent many biophysical processes, such as degradation, dilution, compartmentalization, or sequestration; for simplicity we mainly use the term "degradation".

---

### Reducing slab boundary artifacts in three-dimensional multislab diffusion MRI using nonlinear inversion for slab profile encoding (NPEN) [^fae07c39]. Magnetic Resonance in Medicine (2016). Low credibility.

As originally proposed, the PEN method relies on the assumption that slab profile estimates are accurate, which is increasingly difficult to ensure at short TR. To overcome this problem, we propose a new nonlinear reconstruction that jointly estimates the slab profile and the image. In this method, both slab profile and image are treated as unknowns x = [u, S] T, which should satisfywhereis a nonlinear operator mapping image, slab profile, and coil sensitivity to the acquired data. This poses the 3D multislab reconstruction as a nonlinear inversion problem, which we refer as nonlinear inversion for slab profile encoding (NPEN). As shown previously 41, 42, 43, 44, this problem can be solved using an iteratively regularized Gauss–Newton algorithm 45, 46, 47. In step n, Equation (3) is first linearized around x n :where E′ (x n) is the Fréchet derivative of E at the current guess x n, Δ x n is the update, and the new guess can be obtained by x n +1 = x n + Δ x n. The linear problem described by Equation (4) is then solved by the following Tikhonov regularized minimization:where the first term is the data fidelity term and the second term is the regularization term. x 0 is an initial guess and α n is a regularization parameter, which is reduced in each step to benefit from the robustness at the initial steps of iterations (i.e.g.radient–descent‐like) and the fast convergence when x is close to the solution (i.e. Gauss–Newton‐like). Equation (5) can be solved with the conjugate gradient algorithm.

---

### Adaptive resetting for informed search strategies and the design of non-equilibrium steady-states [^9a9db8d1]. Nature Communications (2025). High credibility.

The final value theorem for Z-transforms states that. By using it, we get (see section 6 of the Supplementary Information)where 〈 N R 〉 is the mean number of time steps between consecutive resetting events. Note that the steady-state in Equation (14) is well defined whenever 〈 N R 〉 is finite, regardless of whether or not the process without resetting has a steady-state. This is a generalization of a well-known result in the theory of standard resetting to state- and time-dependent resetting.

To estimate the NESS, we first sample a set of N trajectories without resetting of length M Δ t. We stress that M should be large enough such that, had we used resetting, the probability of surviving M steps without resetting would be negligible, i.e. Then, we use Equations (12) and (14), and the definition of the Z-transform, to obtainThis estimation results in an unnormalized distribution, which should be normalized. The normalization factor provides an estimate for the mean time between consecutive resetting events, 〈 N R 〉. Equation (15) shows that the estimation of the NESS with resetting, from trajectories without resetting, is done by averaging the histogram of positions over time and trajectories, but reweighing each trajectory, at every time step, by its survival probability.

Prediction and design of non-equilibrium steady-states

The above results can be used to predict and design NESS of spatially-dependent resetting protocols. We demonstrate this using two examples.

It is well known that for free diffusion with a constant resetting rate, a Laplace distributed NESS emerges. An analytical solution for the NESS of diffusion with a parabolic resetting rate r (x) = r 0 x 2 is also known. Interestingly, in both cases, the tails of the NESS decay as, with α = 1 for the constant resetting rate, and α = 2 for the parabolic resetting rate. This raises a more general question: what is the asymptotics of the NESS for diffusion with a power-law resetting rate r (x) = r 0 ∣ x ∣ λ. While there are currently no known closed-form solutions for the NESS with λ ≠ {0, 2}, we can easily estimate the resulting NESS using the procedure described in the previous section.

---

### Inferring time derivatives including cell growth rates using gaussian processes [^d99eaf3f]. Nature Communications (2016). Medium credibility.

Inferring the first and second time derivatives

To determine the time derivative of the data, we use that the derivative of a Gaussian process is another Gaussian process. We can therefore adapt standard techniques for Gaussian process to allow time derivatives to be sampled too.

Building on the work of Boyle, we let g (x) and h (x) be the first and second derivatives with respect to x of the latent function f (x). If f (x) is a Gaussian process then so are both g (x) and h (x). Writing ∂ 1 and ∂ 2 for the partial derivatives with respect to the first and second arguments of a bivariate function, we have

and that

as well as

following ref.

Consequently, the joint probability distribution for y and f *, g * and h * evaluated at points X * is again Gaussian (cf. equation (7)):

where we write K = K (X, X) and K ✱ = K (X *, X *) for clarity.

The covariance function is by definition symmetric: k (x i, x j) = k (x j, x i) from equation (1). Therefore, and so

for all positive integers k and. Consequently, the covariance matrix in equation (13) is also symmetric.

Conditioning on y now gives that the distribution P (f *, g *, h *| X, y, θ, X *) is Gaussian with mean

and covariance matrix

Equation (16) includes equation (9) and shows that

which gives the error in the estimate of the first derivative. Similarly,

is the error in estimating the second derivative.

Using an empirically estimated measurement noise

Although our derivation is given for a Gaussian process where the measurement errors in the data are independent and identically distributed with a Gaussian distribution of mean zero, the derivations are unchanged if the measurement noise has a different s.d. for each time point.

When the magnitude of the measurement noise appears to change with time, we first empirically estimate the relative magnitude of the measurement noise by the variance across all replicates at each time point. We then smooth this estimate over time (with a Gaussian filter with a width of 10% of the total time of the experiment, but the exact choice is not important) and replace the identity matrix, I, in equations (6), (15) and (16) by a diagonal matrix with the relative measurement noise on the diagonal in order to make predictions.

---

### Long-term memory induced correction to arrhenius law [^495130a6]. Nature Communications (2024). High credibility.

Fig. 4
Check of the approximations of the theory.

a Check of the stationary covariance approximation (i.e.): comparison between ψ π (t) = Var(x π (t)) measured in numerical simulations (symbols) and ψ (t) (dashed line: H = 3/8, full line H = 1/4). b Check of Eq. (9): comparison between the value m π (t) in simulations (symbols) and x 0 ϕ (t) (full line: x 0 = l /2 for H = 3/8; dashed line x 0 = l for H = 1/4). Note that m π (t) ≃ x 0 ϕ (t) is expected at large times only. c Check of the short-time scaling regime for H = 3/8. d Check of the long-time scaling regime (11) for H = 3/8. In a, c, d, the initial position is drawn from an equilibrium distribution, corresponding to our predictions for x 0 = 0. When present, error bars represent 68% confidence intervals.

General results

This approach first shows unambiguously that the mean FPT is finite. Indeed, we show in SI (Section B) that the solution to Eq. (8) satisfies at long timeswhich can be checked directly in numerical simulations, see figure 4b. This scaling, together with Eq. (6), shows that the mean FPT is finite. This contradicts the results obtained with the generalized Fokker–Planck equationor with the Wilemski-Fixman approximation. The latter amounts to assuming that the process is at all times in an equilibrium state, and would thus yield m π (t) ≃ L ϕ (t), leading to an infinite mean FPT when α < 1 (as noted earlier in a similar, but out of equilibrium, situation). Beyond this proof of finiteness, our approach yields a quantitative determination of 〈 T 〉 by solving numerically the integral equation (8) for m π (t) and next using Eq. (6); this shows quantitative agreement with numerical simulations in Fig. 3.

---

### Advanced statistics: linear regression, part II: multiple linear regression [^375ff623]. Academic Emergency Medicine (2004). Low credibility.

The applications of simple linear regression in medical research are limited, because in most situations, there are multiple relevant predictor variables. Univariate statistical techniques such as simple linear regression use a single predictor variable, and they often may be mathematically correct but clinically misleading. Multiple linear regression is a mathematical technique used to model the relationship between multiple independent predictor variables and a single dependent outcome variable. It is used in medical research to model observational data, as well as in diagnostic and therapeutic studies in which the outcome is dependent on more than one factor. Although the technique generally is limited to data that can be expressed with a linear function, it benefits from a well-developed mathematical framework that yields unique solutions and exact confidence intervals for regression coefficients. Building on Part I of this series, this article acquaints the reader with some of the important concepts in multiple regression analysis. These include multicollinearity, interaction effects, and an expansion of the discussion of inference testing, leverage, and variable transformations to multivariate models. Examples from the first article in this series are expanded on using a primarily graphic, rather than mathematical, approach. The importance of the relationships among the predictor variables and the dependence of the multivariate model coefficients on the choice of these variables are stressed. Finally, concepts in regression model building are discussed.

---

### Optimization by decoded quantum interferometry [^08dc0a17]. Nature (2025). Excellent credibility.

Achieving superpolynomial speed-ups for optimization has long been a central goal for quantum algorithms 1. Here we introduce decoded quantum interferometry (DQI), a quantum algorithm that uses the quantum Fourier transform to reduce optimization problems to decoding problems. When approximating optimal polynomial fits over finite fields, DQI achieves a superpolynomial speed-up over known classical algorithms. The speed-up arises because the algebraic structure of the problem is reflected in the decoding problem, which can be solved efficiently. We then investigate whether this approach can achieve a speed-up for optimization problems that lack an algebraic structure but have sparse clauses. These problems reduce to decoding low-density parity-check codes, for which powerful decoders are known 2,3. To test this, we construct a max-XORSAT instance for which DQI finds an approximate optimum substantially faster than general-purpose classical heuristics, such as simulated annealing. Although a tailored classical solver can outperform DQI on this instance, our results establish that combining quantum Fourier transforms with powerful decoding primitives provides a promising new path towards quantum speed-ups for hard optimization problems.

---

### Defining urge as an uncontrolled micturition explains pathogenesis, informs cure and helps solve the burgeoning OAB crisis [^80d1adbc]. Neurourology and Urodynamics (2022). Medium credibility.

3.8 Challenging validity of the binary model by a Chaos feedback equation

As the binary feedback model, Figure 1, is the very basis for challenging existing OAB concepts, it, too, was challenged for validity by the nonlinear Chaos Theory feedback equation X next = Xc(1 − X), Figure 6, where "c" inversely represents reflex cortical plus peripheral suppression of afferent urothelial impulses "X" from urothelial stretch receptors "N". X = fraction of possible nerve impulses emanating from "N" in the micturition circuit of the model. Numeral "1" = maximum possible number of impulses in the circuit; "c" represents the closure reflex's inhibition of micturition, central and peripheral, in the Chaos equation.

How "c" and "X" were calculated.

X was given a mean value of 0.5, being a sufficient number for input to the micturition centre from the urothelial stretch receptors "N", Figure 1. "c" is inversely proportional to the quantum of inhibition. The clinical conditions were fitted as estimates into known "c" values from the Chaos Equation: Fowler's Syndrome (retention) with a very low "c" 0.1, normal (normally suppressed) 0.2, low compliance (activated but controlled) 2.0, urge incontinence (uncontrolled unstable) 3.6, example below.

Note: The calculation figures are given only as an example. At 3 decimal places, they are inherently inaccurate, as Lorenz's original discovery was based on inclusion of the 5th decimal number. We have only calculated on three.

---

### The making of the standard model [^0fc9a4ce]. Nature (2007). Excellent credibility.

A seemingly temporary solution to almost a century of questions has become one of physics' greatest successes.

---

### Input-output maps are strongly biased towards simple outputs [^50fca0dd]. Nature Communications (2018). Medium credibility.

Many systems in nature can be described using discrete input-output maps. Without knowing details about a map, there may seem to be no a priori reason to expect that a randomly chosen input would be more likely to generate one output over another. Here, by extending fundamental results from algorithmic information theory, we show instead that for many real-world maps, the a priori probability P(x) that randomly sampled inputs generate a particular output x decays exponentially with the approximate Kolmogorov complexity [Formula: see text] of that output. These input-output maps are biased towards simplicity. We derive an upper bound P(x)≲[Formula: see text], which is tight for most inputs. The constants a and b, as well as many properties of P(x), can be predicted with minimal knowledge of the map. We explore this strong bias towards simple outputs in systems ranging from the folding of RNA secondary structures to systems of coupled ordinary differential equations to a stochastic financial trading model.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^7251e5e1]. Nature Communications (2021). High credibility.

Motif I: cascade (sequential regulation)

A common network motif in many biological networks is the cascade (Fig. 3), a series of regulatory steps (i.e. x regulates y, which regulates z, etc.). Since each regulator must reach the corresponding half-maximal input value k before significantly affecting the next item in the cascade, each step adds an effective delay. The following analysis falls within a range of methods used to replace intermediate steps with a delay.

Fig. 3
A delayed model of cascades recapitulates the implicit delays in ODE models.

a A cascade is a linear sequence of regulation steps, here X regulating Y regulating Z. b Standard models of cascades use ODEs (top), in which each step leads to a characteristic delay in the products Y and Z, which increases with each step based on the half-maximal inputs and degradation rates for each step. In an equivalent DDE model (middle), similar behavior is accomplished by replacing the explicit cascade of implicit delays with a single-step regulation including an explicit delay. A cascade in which each step contains an explicit delay (bottom) behaves analogously to delayed direct regulation (as in middle), with the final step delayed by the sum of delays in each step. η X = 1.5, η Y = η Z = 2.17, β X = β Y = 1, β Z = 0.667, n X = n Y = − 2. For the bottom graph in (b), each step is governed by Eq. (9) with identical parameters.

---

### Exploring replay [^e0cbceb2]. Nature Communications (2025). High credibility.

Model-free control

Several algorithmic approaches exist to solving the problem of optimal control in RL tasks. One popular example is Q -learning, which is an important and widely used algorithm for learning the optimal-function. It belongs to a more general class of model-free temporal difference algorithms which, after every experienced interaction with the environment, successively update their value function estimates based on the encountered reward prediction errors. Specifically for Q -learning, the update rule at iteration n is:

Here, the Q -value estimate is updated towards the difference (or prediction error) between the initial estimate, Q n (s, a), and the sum of the observed reward at the next state reached and the discounted maximal Q n -value at that state, weighted by the learning rate, α. Note that the action that optimisesat s might be different from the one used in equation (4) that optimised

The Q n +1 -values themselves can be used to determine a policy, for instance:where β > 0 is an inverse temperature parameter that controls how deterministic is π n +1. Since π n +1 (s, a) favours actions with higher Q n +1 -values, it tends to be better than π n (s, a) in terms of expected return. The remaining stochasticity is a crude method for arranging a mix of exploration and exploitation.

Model-based control

A different solution is to learn a model of the environment which can then be used to perform prospective planning of the actions to execute. Value functions can also be acquired using the recurrent Bellman equation, for instance:

Here, the recurrent relationship between the successive states allows the agent to make use of its knowledge of the transition structure of the environment (the model) to propagate the information about future rewards towards its current situation or state in the environment. If the agent does indeed know the model (also including), then various forms of planning can be used to compute the long-run consequences associated with the available actions at decision time and make a far-sighted and informed decision. Value iteration is one example planning algorithm which iteratively performs synchronous updates (for all states and actions in each sweep) specified by Equation (6). Such updates are also called Bellman backups because of the application of the Bellman equation. Given a perfect model of the environment, and, such procedure is guaranteed eventually to converge to the optimal value function.

---

### Multilayer block copolymer meshes by orthogonal self-assembly [^a9b06266]. Nature Communications (2016). Medium credibility.

The single chain partition function can be evaluated as follows

where q (r, s) is a restricted chain partition function (propagator) that could be calculated by solving a modified diffusion equation

subjected to the initial condition q (r, 0) = 1. As the two ends of the polymer are distinct, a complementary partition function q * (r, s) is defined similarly and satisfies the same modified diffusion equation with an initial condition q *(r, 1) = 1. Here, we utilized s as a chain contour variable in units of N. All lengths were expressed in units of the unperturbed radius-of-gyration of a polymer, R g = (Nb 2 /6) 1/2, where b is the statistical segment length. The solution to the modified diffusion equation was conducted by implementing the pseudo-spectral method. An iterative relaxation of the fields towards their saddle-point values was implemented following ref.

By evaluating q (r, s) and its complementary function, the segment volume fractions can be determined as follows

Here we considered a 2D rectangular domain of size (L x × L y = 24 × 14R g 2). The SCFT equations were solved on a grid that had an equal lattice spacing of 0.2R g in the x and y directions. The volume fraction of the polymer f A was set to 0.5, whereas the degree of incompatibility χN was 17. This resulted in a structure that had the form of stripes (2D projection of in-plane cylinders or out-of-plane lamellae). The sides of the computational cell had periodic boundary conditions simulating an unconfined system in the x direction. Confinement was applied to the top and bottom surfaces where both have equal affinity to block A.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^488abf14]. Nature Communications (2021). High credibility.

Chaotic behaviors are prevented in monotonic regulation with linear or cyclic network topology

Due to cyclic network topology (no more than one loop) and monotonic regulation, none of the motifs so far can yield chaotic dynamics. Either non-monotonic feedback (as in the Mackey-Glass equation) or non-cyclic topologyis required for chaos. Double feedback (Fig. 8 a) is a minimal motif fulfilling both requirements, although the non-monotonicity is sufficient (Supplementary Note 8).

The governing equation for such a double feedback motif is thus given by setting the output and both inputs of the logic equation (Eq. (19)) to X (and letting K = 1 for simplicity):

Chaotic behavior is possible for non-monotonic feedback with multiple feedback and disparate delays

Simulation of Eq. (37) demonstrates chaos for some parameters of positive/negative mixed feedback, as evidenced by sustained oscillations with variable maxima (Fig. 8 b), occupying an apparently fractal region in phase space (Fig. 8 c), and a large number of peaks in frequency space (Fig. 8 d). To confirm that the dynamics are in fact chaotic, we calculated the dominant Lyapunov exponent using the Wolf method(see "Methods"), finding a positive (i.e. chaotic) value of 0.0040 ± 0.00055 bits (mean ± standard deviation). We also calculated the box dimension of a reconstructed phase space with coordinates (X (T), X (T − 10), X (T − 20)) used by the Wolf method, yielding fractional dimension ~1.81, with 95% confidence interval (1.76, 1.87). These results indicate chaos (see Supplementary Fig. 4).

---

### Best (but oft-forgotten) practices: checking assumptions concerning regression residuals [^f2ebb771]. The American Journal of Clinical Nutrition (2015). Low credibility.

The residuals of a least squares regression model are defined as the observations minus the modeled values. For least squares regression to produce valid CIs and P values, the residuals must be independent, be normally distributed, and have a constant variance. If these assumptions are not satisfied, estimates can be biased and power can be reduced. However, there are ways to assess these assumptions and steps one can take if the assumptions are violated. Here, we discuss both assessment and appropriate responses to violation of assumptions.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^e2a04562]. Nature Communications (2021). High credibility.

With DDEs, multiple steps ("cascades") within a network can be rigorously simplified into a single step with delay (see below), an approach which has been explored in a variety of biological contexts. This makes interpretation of the phenomenology simpler than with ODEs and reduces the number of equations and parameters in the model. This idea can be visualized by depicting regulatory networks as directed graphs, with nodes representing biological species, and pointed vs. blunt arrows indicating activation vs. repression, respectively (Fig. 1). DDE models allow a single arrow to faithfully capture many biochemical steps, expanding the available dynamics in a model with a reduced set of equations and parameters. DDE regulatory models have in fact been used widely to model a range of biological phenomena such as development, and hematopoiesis. For instance, a 1-variable ODE such as Eq. (1) can only produce exponential growth or decay. A corresponding DDE, such as Eq. (2), can also oscillate (with amplitude approaching zero or infinity) and, if nonlinear, lead to stable oscillations, bistability and chaos.

A key challenge in using DDEs is their mathematical complexity relative to ODEs. For example, while Eq. (1) is 1-dimensional because one initial condition (x at t = 0) determines all future behavior, Eq. (2) is infinite-dimensional, because x must be specified at all times − τ ≤ t ≤ 0 to predict a unique solution. Concretely, the solution to Eq. (1) can be written as a single exponential, while Eq. (2) generally must be specified as a sum of an infinite number of exponential terms to satisfy initial conditions.

Despite the challenges, much progress has been made in analytical understanding of DDEs, and numerical methods exist for simulation. We thus see an opportunity to use DDEs to recapitulate dynamics found in ODE solutions of network motif behavior with fewer genes and thus fewer modeling parameters and equations (Fig. 1), a type of "modeling simplicity".

---

### Epidemic modelling suggests that in specific circumstances masks may become more effective when fewer contacts wear them [^c8fa1656]. Communications Medicine (2024). Medium credibility.

Parameters

Unless stated otherwise, the following parameter settings were used. Simulations were performed for N = 10 5 nodes and averaged over 20 independent iterations. The parameters were chosen such that k = 20, α = 0.02, γ = 0.1, q = 0.99, q r (0) = q s (0) = 1, η = 1, and ϵ = 0.1. For the initial condition we randomly choose ten nodes and set their X i (t = 0) = I; for all the others we set X i (t = 0) = S. The model halts once the outbreak has ended, i.e. X i (t) ≠ I for all i.

Reporting summary

Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.

---

### A space-time tradeoff for implementing a function with master equation dynamics [^57fa10b0]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### On the UNFOLD method [^2cbaf033]. Magnetic Resonance in Medicine (2002). Low credibility.

A graphical formalism is presented, showing that "UNaliasing by Fourier-encoding the Overlaps Using the temporaL Dimension" (UNFOLD) is equivalent to sampling k-t-space in a sheared grid pattern. Discrete regular sampling in k-t-space leads to periodic replication of the support region in x-f-space. Thus, the maximum acceleration achievable by UNFOLD is equivalent to the maximum packing of support regions in x-f-space. When the support region is separable along the x and f axes, the reconstruction can be performed separately for each k. UNFOLD can be combined with SiMultaneous Acquisition of Spatial Harmonics (SMASH) to further accelerate acquisition. However, a straightforward combination of the methods has been shown to result in a size restriction, which limits the portion of the field of view (FOV) with a larger temporal bandwidth to only a quarter of the FOV. Two solutions are presented to overcome this restriction.

---

### Abstract representations of events arise from mental errors in learning and memory [^748e9b84]. Nature Communications (2020). High credibility.

Estimating parameters and making quantitative predictions

Given an observed sequence of nodes x 1,…, x t −1, and given an inverse temperature β, our model predicts the anticipation, or expectation, of the subsequent node x t to be. In order to quantitatively describe the reactions of an individual subject, we must relate the expectations a (t) to predictions about a person's reaction timesand then calculate the model parameters that best fit the reactions of an individual subject. The simplest possible prediction is given by the linear relation, where the intercept r 0 represents a person's reaction time with zero anticipation and the slope r 1 quantifies the strength with which a person's reaction times depend on their internal expectations.

In total, our predictionscontain three parameters (β, r 0, and r 1), which must be estimated from the reaction time data for each subject. Before estimating these parameters, however, we first regress out the dependencies of each subject's reaction times on the button combinations, trial number, and recency using a mixed effects model of the form 'RT~log(Trial)*Stage+Target+Recency+(1+log(Trial)*Stage+Recency|ID)', where all variables were defined in the previous section. Then, to estimate the model parameters that best describe an individual's reactions, we minimize the RMS prediction error with respect to each subject's observed reaction times, where T is the number of trials. We note that, given a choice for the inverse temperature β, the linear parameters r 0 and r 1 can be calculated analytically using standard linear regression techniques. Thus, the problem of estimating the model parameters can be restated as a one-dimensional minimization problem; that is, minimizing RMSE with respect to the inverse temperature β. To find the global minimum, we began by calculating RMSE along 100 logarithmically spaced values for β between 10 −4 and 10. Then, starting at the minimum value of this search, we performed gradient descent until the gradient fell below an absolute value of 10 −6. For a derivation of the gradient of the RMSE with respect to the inverse temperature β, we point the reader to the Supplementary Discussion. Finally, in addition to the gradient descent procedure described above, for each subject we also manually checked the RMSE associated with the two limits β → 0 and β → ∞. The resulting model parameters are shown in Fig. 4 a, b for random walk sequences and Fig. 4 g, h for Hamiltonian walk sequences.

---

### Evolutionary dynamics of any multiplayer game on regular graphs [^916cb3ba]. Nature Communications (2024). High credibility.

General replicator equations

The evolution of frequencies x 1, x 2,…, x n can be deduced through the microscopic strategy update process. Specifically, in an infinite population, i.e. N → ∞, a single unit of time comprises N elementary steps, ensuring that each individual has an opportunity to update their strategy. During each elementary step, the frequency of i -players increases by 1/ N when a focal j -player (where j ≠ i) is chosen to update its strategy and is replaced by an i -player. Similarly, the frequency of i -players decreases by 1/ N when a focal i -player is selected to update its strategy and the player who takes the position is not an i -player. Based on this perception, we derive a simple form of the replicator equations for i = 1, 2,…, n in the weak selection limit (Supplementary Note 2) :We find that Eq. (4) offers an intuitive understanding, if we introduce the following two concepts: (1), the expected accumulated payoff of the i -player (zero steps away on the graph), and (2), the expected accumulated payoff of the i -player's neighbors (one step away on the graph). These concepts suggest that. Under pairwise comparison, the reproduction rate of i -players is dependent on how their accumulated payoff exceeds that of their neighbors. In essence, the evolution of x i is the competition between an individual and its first-order neighbors, which aligns with the results obtained by a different theoretical framework in two-strategy systems. We further extend it to n -strategy systems in the framework of pair approximation. We also verify that the death-birth rule is essentially the competition between an individual and its second-order neighbors for n -strategy systems (Supplementary Information).

---

### Saxagliptin and metformin hydrochloride (Kombiglyze) [^7cd34402]. FDA (2025). Medium credibility.

Labeled indications for Saxagliptin / metformin ER (also known as Kombiglyze XR) include:

- Treatment of diabetes mellitus type 2 in adults

---

### Mutualism supports biodiversity when the direct competition is weak [^1a78a1ee]. Nature Communications (2017). Medium credibility.

where we denote by x 1 the projection of vector x on the main eigenvector v 1. The resulting vulnerability η i is a linear function of Δ that depends on the random variables q i and q 1. The variance of the denominator of equation (21) is given by. It is easy to see thatand. They both share the main qualitative property of, that is, they are all sums of d i +1 positive terms. Thus, to simplify formulas, in the following we estimate the variance of the denominator of equation (21) simply through.

The feasibility condition depends on the most vulnerable species, which is different in the unperturbed system Δ = 0 (the species i that maximizes η i (p 0)) and for large Δ (the species v that maximizes η i (Δ)). If Δ is large enough, the most vulnerable species is the species that maximizes η i (q)− η i (p 0) and it does not change for larger Δ, so that the maximum value of η i (Δ) increases linearly as η (Δ)≡max i { η i (Δ)}≈ η v (p 0)+Δ η ′(p 0). η ′ must be computed for the species with maximum η i, leading to

where we estimate q i with its root mean square value multiplied times a constant, v i 1 ≈1/and, and we take into account that p 0 depends on the effective growth rates.

The above formula is complex, and we prefer to compute η ′ numerically with equation (14), but it makes clear two important qualitative points: (1) η i decreases with the number of links (that is, the number of non-zero components G ik), thus η ′ decreases with the connectance of the mutualistic network and (2) η ′ is larger for obligatory mutualism, in which the terms α and m have opposite sign.

---

### Adaptive resetting for informed search strategies and the design of non-equilibrium steady-states [^2c70ebfd]. Nature Communications (2025). High credibility.

Results

The MFPT with adaptive resetting

We begin by defining an adaptive, i.e. a state- and time-dependent, resetting rate r (X, t), with X being the state of the system and t representing time. Given a trajectory, we can define a random variable describing its resetting time R via its cumulative distribution functionNote that R distributes differently for different trajectories under the same functional choice of the resetting rate. We emphasize that any state- and time-dependent resetting strategy can be represented in this way. For example, for an exponential resetting time, one takes a constant resetting rate r (X, t) = r. More generally, to represent a general resetting time distribution whose survival function is Ψ(t) = 1 − Pr(R ≤ t), one takes, which does not depdend on X ﻿. Similarly, one may consider an arbitrary spatial dependence of the resetting rate, e.g. r (X, t) = r Θ (X), where Θ (X) is the Heaviside step function. More generally, arbitrary state-time couplings can also be captured as no restrictions are stipulated on r (X, t) other than it being non-negative. For simplicity of demonstration, in concrete examples given in this paper, we take resetting rates r (X) which are time-independent, although the theory developed below is completely general.

---

### Methane emissions from the nord stream subsea pipeline leaks [^c8a3187d]. Nature (2025). Excellent credibility.

Methods

Pre-rupture pipeline inventories

We estimate the mass of CH 4 contained in each pipeline before rupturing using equation (1):where M NS x is the mass of CH 4 contained within pipeline NS x (kt of CH 4), L NS x is the total pipeline length in kilometres (km), ρ NS x is the density of the natural gas mixture at the pressure and temperature within the pipeline (kg m −3), D NS x is the pipeline diameter (m), and C NS x is the percentage mass composition of CH 4 in the natural gas (Extended Data Tables 1 and 2). In equation (1), we use ρ NS x values from the gas mixture eventually used in the PBREAK simulations because our CATHARE simulations do not consider components in the mixture that make up less than 0.1% (Extended Data Table 2).

Pipeline emission simulation tools

We use PBREAK(version 2.28.3) and CATHARE (version CATHARE-3) to simulate pipeline rupture emission from NS1A, NS1B and NS2A. PBREAK is a mathematical model used to simulate the outflow of gas following the sudden puncture or rupture of a high-pressure transmission pipeline. The model is embedded in the PIPESAFE Quantitative Risk Assessment software. CATHARE is a tool developed by the French Alternative Energies and Atomic Energy Commission used for thermal hydraulic simulations of multiphase flows. Both model pipeline depressurization as the one-dimensional flow of natural gas in a pipeline and solve a system of three conservation equations for mass, momentum and energy. The models use an equation of state to compute fluid properties (Supplementary Discussion 3). Further descriptions of each model, including their suitability for modelling subsea pipeline ruptures, are provided in Supplementary Method 1.

PBREAK

Within the PBREAK workflow, each pipeline is divided in N segments, and the nonlinear equations for continuity and mass balance, momentum and energy solved at each time step using an implicit finite-difference scheme. These equations are solved every 1 s for the first 2 min, then every 30 s until the calculation finishes. The computation terminates when the internal pressure in all the lengths of each pipeline is equal to the external pressure. The model results are the variation with time of the outflow at the point of rupture coming from each branch of pipeline, and the cumulative flow from the beginning of the release.

---

### Implementing quantum dimensionality reduction for non-markovian stochastic simulation [^4ea471be]. Nature Communications (2023). High credibility.

Methods

Stochastic processes and minimal-memory classical modelling

A discrete-time stochastic processconsists of a sequence of random variables X t, corresponding to events drawn from a set, and indexed by a timestep. The process is defined by a joint distribution of these random variables across all timesteps, whererepresents the contiguous (across timesteps) series of events between timesteps t 1 and t 2. We consider stochastic processes that are bi-infinite, such thatand, and stationary (time-invariant), such that P (X 0: L) = P (X t: t + L) ∀ t, L ∈. Without loss of generality, we can take the present to be t = 0, such that the past is given by, and the future. Note that we use upper case for random variables and lower case for the corresponding variates.

A (causal) model of such a (bi-infinite and stationary) discrete-time stochastic process consists of an encoding functionthat maps from the set of possible past observationsto a set of memory states –. The model also requires an update rulethat produces the outputs and updates the memory state accordingly. We then designate the memory cost D f of the encoding as the logarithm of the dimension (i.e. the number of (qu)bits) of the smallest system into which these memory states can be embedded. For classical (i.e. mutually orthogonal) memory states, this corresponds to. For quantum memory states, which may, in general, be linearly dependent.

Let us, for now, restrict our attention to statistically-exact models, such that (f, Λ) must produce outputs with a distribution that is identical to the stochastic process being modelled. Under such a condition, the provably-memory minimal classical model of any given discrete-time stochastic process is known and can be systematically constructed. These models are referred to as the ε - machine of the process, which employs an encoding function f ε based on the causal states of the process. This encoding function satisfiesand given initial memory state, the evolution produces output x 0 with probabilityand updates the memory to state. The memory states are referred to as the causal states of the process, and the associated cost D μ is given by the logarithm of the number of causal states.

---

### SENSE phase-constrained magnitude reconstruction with iterative phase refinement [^9280ce41]. Magnetic Resonance in Medicine (2007). Low credibility.

Conventional sensitivity encoding (SENSE) reconstruction is based on equations in the complex domain. However, for many MRI applications only the magnitude is relevant. If there exists an estimate of the underlying phase information, a magnitude-only phase-constrained reconstruction can help to improve the conditioning of the SENSE reconstruction problem. Consequently, this reduces g-factor-related noise enhancement. In previous attempts at phase-constrained SENSE reconstruction, image quality was hampered by strong aliasing artifacts resulting from inadequate phase estimates and high sensitivity to phase errors. If a full-resolution phase image is used, a significant reduction in aliasing errors and better noise properties compared to SENSE can be obtained. An iterative scheme that improves the phase estimate to better approximate the phase is presented. The mathematical framework of the new approach is provided together with comparisons of conventional SENSE, phase-constrained SENSE, and the new phase-refinement method. Both theory and experimental verification demonstrate significantly better noise performance at high reduction factors, i.e., close to the theoretical limit. For applications that need only magnitude data, an iterative phase-constrained SENSE reconstruction can provide substantial SNR improvement over SENSE reconstruction and less artifacts than phase-constrained SENSE.

---

### Area under the expiratory flow-volume curve: predicted values by regression and deep learning methods and recommendations for clinical practice [^e6f4a561]. BMJ Open Respiratory Research (2021). High credibility.

Table 1
Simple linear regression model without interactions for predicting square root AEX-FV

Table 2
Linear regression model with interactions for predicting square root AEX-FV

Further attempts to improve the models' performance by using various response variable distributions (eg, gamma or lognormal transformations) and using optimisation (generalised regression) techniques such as ridge regression, elastic net, lasso and double lasso, with or without adaptive features, did not lead to major improvements in generalised R 2 of the validation set (table 3).

Table 3
Model Comparisons between linear regression (standard least squares) and various techniques of optimisation (generalised regression) for predicting square root AEX-FV

To counteract the issue of non-normal distributions and collinearities between variables (assumed by default in regression to be completely independent), we also developed a neural network model using the same variables (figure 3A–C). We ran several neural network architectures, and the optimised, fastest models were based on a deep learning architecture with one output (Sqrt AEX-FV, maintained as transformed for purpose of comparability with the other models), same five inputs (PEF, FVC, FEF 25, FEF 50 and FEF 75) and two hidden layers, each with three sigmoid [(e 2x -1)/(e 2x +1)], 3 gaussian (1/e x·x) and three linear activation function nodes. We used transformed covariates (with either Johnson Su or Johnson Sb distributions), a robust fit method that uses absolute deviations instead of least squares (in order to minimise the effects of outliers), 10 additional tours for the fitting process, and a squared penalty function to avoid overfitting (the latter being preferred when all X variables are expected to contribute to the predictive ability of the model). Generalised R 2 in this instance simplifies the traditional R 2 for continuous normal responses in the standard least squares setting, and is also known as the Nagelkerke or Craig-Uhler R 2, which is a normalised version of Cox and Snell's pseudo R 2. The root mean square error (RMSE) is equivalent to the SD, while Mean Absolute Deviance is represented by the average of the absolute differences between the predicted response and the actual Y variable (figure 3A). The SSE (figure 3A) represents the sum of squares error, while the main effects of the variables is based on the k nearest neighbours' method of resampling dependent variables. Overall, the R 2 was very high (0.995 in both the training and the validation set), while the RMSE was even further reduced to 0.070 (figure 3A). The prediction profiler in figure 3B shows the non-linear relationship between Sqrt AEX-FV and the X variables, while the contour profilers show the response surfaces for various combination of variables versus Sqrt AEX-FV (figure 3B). Another illustration of the neural network model's performance is shown in figure 3C, which showcases the high correlations between Sqrt AEX-FV observed and predicted values, and the residual values plotted against the predicted Sqrt AEX-FV in the training and validation sets, respectively. The figure 3C also shows that the residuals do not increase at higher response variable values (visual inspection confirming that the homoscedasticity condition is met).

---

### Mathematical modeling is more than fitting equations [^994275ea]. The American Psychologist (2014). Low credibility.

Comments on the comments made by Brown et al. (see record 2013-24609-001). The article by Brown et al. regarding the Fredrickson and Losada (see record 2005-11834-001) article discussed the use of differential equations in science and repeated our earlier observation (Luoma, Hämäläinen, & Saarinen, 2008) that there is lack of justification for the use of the Lorenz equations in the latter article. In this comment we want to point out that Brown et al. presented a very narrow view on mathematical modeling in behavioral research. We describe how the conceptual use of mathematical models is essential in many fields.

---

### Forecasting the outcome of spintronic experiments with neural ordinary differential equations [^248909db]. Nature Communications (2022). High credibility.

Mackey-Glass prediction task

In a chaotic system, small perturbations can result in radically different outcomes. The prediction of a chaotic system is thus a problematic task. As a chaotic system, the Mackey-Glass equation is generated from a delay differential equation (DDE), where x (t) is a dynamical variable, β and γ are constants. Chaotic time series can be achieved with β = 0.2, γ = 0.1 and τ = 17.

In the main paper, our goal is to predict the Mackey-Glass time series at a future time step: the preprocessed input signal at the current time is fed into the reservoir, which maps it non-linearly into higher-dimensional computational spaces (see Fig. 3 a), and a trained output matrix W out is used for reading out the reservoir states. The number of steps between the future time step and the current step is defined as prediction horizontal step H. W out is different for the different H values.

More precisely, the dataset for the prediction task is prepared in the following way. First, Eq.(2) is solved for 100,000 integration time steps with dt = 0.1. Before the data are processed by the reservoir, they are downsampled with a downsampling rate of 10 to remove the possible redundancy in the input data. Thus, we obtain 10,000 data points in total. The first 5000 + H data points are used for the training, and the rest 5000 − H are for testing. The first stage of the masking procedure is a matrix multiplication W i n ⋅ M o, whereis the mask matrix with data values drawn from a standard normal distribution and M o ∈ R 1× L is the original input data. Here L = 5000 + H is the number of the scalar input data points and N r is the reservoir size. We adopt N r = 50 in the main text. As a consequence of the masking, we obtain the data matrix. Then M e is column-wise flattened into a vectorand then fed into the reservoir of skyrmion systems.

---

### Advanced statistics: linear regression, part I: simple linear regression [^18239641]. Academic Emergency Medicine (2004). Low credibility.

Simple linear regression is a mathematical technique used to model the relationship between a single independent predictor variable and a single dependent outcome variable. In this, the first of a two-part series exploring concepts in linear regression analysis, the four fundamental assumptions and the mechanics of simple linear regression are reviewed. The most common technique used to derive the regression line, the method of least squares, is described. The reader will be acquainted with other important concepts in simple linear regression, including: variable transformations, dummy variables, relationship to inference testing, and leverage. Simplified clinical examples with small datasets and graphic models are used to illustrate the points. This will provide a foundation for the second article in this series: a discussion of multiple linear regression, in which there are multiple predictor variables.

---

### Why are we regressing? [^fd8ada5d]. The Journal of Foot and Ankle Surgery (2012). Low credibility.

In this first of a series of statistical methodology commentaries for the clinician, we discuss the use of multivariate linear regression.

---

### A mechanism for reversible mesoscopic aggregation in liquid solutions [^f7cd174c]. Nature Communications (2019). High credibility.

Additional computational difficulties are caused by the presence of the non-linear termin Eq. (2). We have numerically solved the resulting non-linear differential equations for several realizations of parameters — to be discussed in due time — however the majority of the calculations were performed for a linearized version of Eq. (2) so that the interconversion between the two species is effectively a first order reaction:wherein each phase. Note that if one considers Eq. (7) as a linearized version of Eq. (2), a variable change 2 n 2 → n 2 is implied. Equations (7) can also be considered on their own merit: They can approximate a physical situation where species 1 converts into species 2 by binding a third species that is part of the buffer. If the transport of this third species is fast compared with the transport of species 1 and 2, then the above equations apply. This said, we will continue to call species 1 and 2 "the monomer" and "the dimer", respectively.

The linearity of the reaction terms in Eq. (7) renders the problem linear within an individual phase. The chemical potentials and concentrations can be presented as linear combinations of Yukawa-like terms r −1 e ± qr while the differential equation is thus reduced to an algebraic characteristic equation for the lengths q −1 that can be solved much more readily than the original non-linear differential Eq. (2). This circumstance allows one to readily explore broad ranges of parameters. Once a non-trivial solution of the 1st order case (7) is found, one may then attempt to confirm whether a similar solution exists in the more complicated, 2nd order case from Eq. (2). Throughout, we consider exclusively the spherically symmetric geometry; such solutions are expected to minimize the surface tension between the two phases during phase coexistence.

---

### A general method for handling missing binary outcome data in randomized controlled trials [^9a6c6b42]. Addiction (2014). Low credibility.

Estimation

Estimating the intervention effect

The target parameter for inference is the intervention effect:which is the log odds ratio measuring the association between treatment group and smoking. X plays an important role as a predictor for any missing outcome data but does not contribute to the definition of the target parameter. Using the definition of Y adopted here, a negative α indicates a beneficial treatment and so the corresponding odds ratio of < 1 also indicates a treatment benefit.

The estimation procedure is conceptually simple. First we select the four values of the even numbered β sensitivity parameters that we wish to use. We then estimate the parameters β 1, β 3, β 5 and β 7. As an example, we focus on the 131 participants in the iQuit trial (see Example 2: the iQuit trial) who are in the treatment group (Z = 1) and are considered not smoking at baseline (X = 0). Of the 65 participants in this group who provide outcome data (R = 1), 41 (63%) were smoking at the end of the trial and so we estimate β 1 to be logit(41/65) = 0.536.

Having estimated all four odd-numbered β parameters, we then use equation 1 to estimate the probability of smoking for each combination of R, X and Z. In our example, suppose we choose β 2 = 1. Then we estimate the log odds of smoking among non-responders in the treatment group considered not smoking at baseline (Z = 1, X = 0, R = 0) as 0.536 + 1 = 1.536, so the proportion smoking in this group is 82%. Similarly, for the 746 participants who are in the treatment group and are considered smoking at baseline (Z = 1, X = 1), 286 provided outcome data of whom 230 (80%) were smoking at follow-up; and supposing β 4 = 1 we estimate that 92% of the 460 who did not provide outcome data were smoking at follow-up.

---

### Sequence anticipation and spike-timing-dependent plasticity emerge from a predictive learning rule [^00c42fd6]. Nature Communications (2023). High credibility.

Optimization and initialization scheme

Equation (19) defines a completely online optimization scheme that can be implemented locally by single neurons. In Fig. 3, Fig. S5, Fig. S6, Fig. S7, Fig. S8, and Fig. S10 we updated the synaptic weights following the online approximation of the gradient in Equation (19). For the results of Fig. 1, Fig. S1, Fig. 2 b, c, Fig. 4, Fig. S3, Fig. S4, Fig. S9 and Fig. S11 we added a scaling term to the predictive plasticity rule as, That is, we multiplied the learning rate with w t −1 to ensure non-negative values of the weights. Consequently, the weight update at time step t was proportional to the synaptic weight value at time step t − 1. In Fig. 2 d and Fig. S2 we did not use the online approximation of the gradient, and we optimized the full model (Equation (18)) using the Adam optimizer.

For the example in Fig. 1, for Fig. 2 b, c, Fig. 3 and for the spike-timing-dependent plasticity protocols in Fig. 4, the initial weights were assigned to fixed values for the different cases considered. In Fig. 2 d, the initial weights were randomly drawn from a truncated normal distribution. We bonded the truncated normal distribution to obtain positive values of the initial weights. The variance of the normal distribution was scaled by the squared root of the total input size N.

---

### Adaptive resetting for informed search strategies and the design of non-equilibrium steady-states [^3039a438]. Nature Communications (2025). High credibility.

We next construct the random variable describing the FPT under the resetting protocol r (X, t). We observe that a first-passage process with resetting can be described in the following way: First, a trajectory is sampled, and this determines T, i.e. the FPT without resetting. The trajectory also sets the distribution of R via Equation (1). We next sample R, and if T ≤ R, we conclude that the FPT is simply T. If, however, R < T, resetting occurs before first-passage, a new trajectory is sampled, and the procedure is repeated, tallying R. Overall, the random variable describing the FPT under resetting iswhereis an independent and identically distributed copy of T R. Using the total expectation theorem, and averaging over all the trajectories, and all realizations of R, we find that the MFPT under restart isFrom here, it is clear that the MFPT under restart can also be written as. We note that the same result was previously obtained for state-independent resetting using a similar technique. A detailed derivation of Equation (3) is given in section 1 of the Supplementary Information.

We can interpret Equation (3) as illustrated in Fig. 2. For a given trajectory with resetting, the first passage is composed of several failed attempts to complete the process, each ending in resetting. These are followed by one successful attempt, ending in first-passage, and completing the process. The total number of attempts, failed plus successful, is geometrically distributed. Namely, all attempts are statistically independent with a success probability Pr(T ≤ R). Therefore, the average number of attempts is 1/Pr(T ≤ R). The mean duration of an unsuccessful attempt is 〈 R ∣ R < T 〉, and the mean duration of a successful one is 〈 T ∣ T ≤ R 〉. The MFPT with resetting is just the sum of the mean duration of a failed attempt times the mean number of failed attempts, with the mean duration of the final successful attempt.

---

### Long-term memory induced correction to arrhenius law [^be18029d]. Nature Communications (2024). High credibility.

General non-Markovian analysis

We now proceed to the theoretical determination of the mean FPT to x = L, denoted 〈 T 〉, with fixed initial condition x (0) = x 0 [the case of stationary initial conditions can be obtained by averaging over p s (x 0)]. Our approach consists in generalizing the tools developed in refs., which, in the context of rare event kinetics, have been used so far only to analyze processes with a finite maximal relaxation time. We describe the main steps of the approach for completeness; details can be found in SI (Section B). We start with the following general exact expression of the mean FPT, derived in ref.:where we have introduced p π (x, t) as the pdf of the process x π (t) ≡ x (t + T), where T is the FPT; x π (t) is thus the process after a first-passage event. To characterize p π (x, t), we assume that the process x π (t) is Gaussian (as is x (t)), and thus fully characterized by its first moment m π (t) = 〈 x π (t)〉, and covariancethat is assumed to be identical to that of the unconditioned process x (t). The validity of these hypotheses has been checked numerically [Fig. 4 (a) and SI, Section D] and analytically for weakly non-Markovian processes (SI, Sections E). With these approximations, Eq. (6) becomesThe so far unknown quantity m π (t) can then be determined self-consistently by analyzing a generalized version of the renewal equation (see SI, Section B), leading toThis equation generalizes similar equations in refs. which were restricred on the determination of p π (L, t) at short times and thus did not enable the analysis of long-term memory effects. This integral equation, together with the condition m π (0) = L, allows to determine the only unknown m π (t): this finally gives access to 〈 T 〉 thanks to Eq. (7).

---

### Transition from light diffusion to localization in three-dimensional amorphous dielectric networks near the band edge [^f5bde5c4]. Nature Communications (2020). High credibility.

This result together with the self-consistent equation for the diffusion coefficientdetermines the transport properties of our system for all values of the parameters. In the above equation the cut off is given by. The latter depends on the chosen value (k l) c with (k l) = (k l) c at the mobility edge.

We solve the set of self-consistent equations by recursively solving Eq. (3) in a first step for all positions of the sourceand transversal wavenumbers q. The solution of the Green function is then plugged into Eq. (4) to correct for the z- dependent diffusion coefficientwhich is inserted back to Eq. (3) until convergence is reached. We choose a discretization scheme where all the considered positions z and wavenumbers squared q 2 are evenly spaced. Depending on the values of the parameters (k l), (k l) c, and total length L, we need to take a step Δ z ≡ h ranging from 0.02 to 0.2 and between 300 and 600 steps in q 2 to achieve a relative precision inof the order or 10 −4 in 5–50 recursion steps.

Taking the second order finite differences approximation for the derivatives in Eq. (3) leads to a tridiagonal system of equations which has to be solved for each value of the wavenumber q and position of the source. Obviously, changing the position of the source amounts to changing the independent term of the system of equations and hence all equations for a given value of q can be solved at once through the inverse of the corresponding tridiagonal matrix. We choose the Lapack function DGTSV to get the inverse since it is simple to use and universally accessible while efficient enough for our purposes. Specifically, if we take a discretization, for i = 1,⋯, n. And naming, the diagonal terms of the system of equations readthe subdiagonal terms areAnalogously, the superdiagonal terms areSince the sources are located in the interior of the slab, the points at which the source is located are. The independent term vector T i for the source at z i is.

---

### Standardizing 25-hydroxyvitamin D values from the Canadian health measures survey [^d6f537b6]. The American Journal of Clinical Nutrition (2015). Low credibility.

Background

The Canadian Health Measures Survey (CHMS) is an ongoing cross-sectional national survey that includes a measure of 25-hydroxyvitamin D [25(OH)D] by immunoassay. For cycles 1 and 2, the collection period occurred approximately every 2 y, with a new sample of ∼5600 individuals.

Objective

The goal was to standardize the original 25(OH)D CHMS values in cycles 1 and 2 to the internationally recognized reference measurement procedures (RMPs) developed by the US National Institute for Standards and Technology (NIST) and Ghent University, Belgium.

Design

Standardization was accomplished by using a 2-step procedure. First, serum samples corresponding to the original plasma samples were remeasured by using the currently available immunoassay method. Second, 50 serum samples with known 25(OH)D values assigned by the NIST and Ghent reference method laboratories were measured by using the currently available immunoassay method. The mathematical models for each step-i.e., 1) YCurrent = XOriginal and 2) YNIST-Ghent = XCurrent -were estimated by using Deming regression, and the 2 models were solved to obtain a single equation for converting the "original" values to NIST-Ghent RMP values.

Results

After standardization (cycles 1 and 2 combined), the percentage of Canadians with 25(OH)D values < 40 nmol/L increased from 16.4% (original) to 19.4% (standardized), and values < 50 nmol/L increased from 29.0% (original) to 36.8% (standardized). The 25(OH)D standardized distributions (cycles 1 and 2 analyzed separately) were similar across age and sex groups; slightly higher values were associated with cycle 2 in the young and old. This finding contrasts with the original data, which indicated that cycle 2 values were lower for all age groups.

Conclusion

The shifts in 25(OH)D distribution brought about by standardization indicate its importance in drawing correct conclusions about potential population deficiencies and insufficiencies and in permitting the comparison of distributions between national surveys.

---

### A general model for ontogenetic growth [^10660e44]. Nature (2001). Excellent credibility.

Several equations have been proposed to describe ontogenetic growth trajectories for organisms justified primarily on the goodness of fit rather than on any biological mechanism. Here, we derive a general quantitative model based on fundamental principles for the allocation of metabolic energy between maintenance of existing tissue and the production of new biomass. We thus predict the parameters governing growth curves from basic cellular properties and derive a single parameterless universal curve that describes the growth of many diverse species. The model provides the basis for deriving allometric relationships for growth rates and the timing of life history events.

---

### Adaptive resetting for informed search strategies and the design of non-equilibrium steady-states [^d30abff2]. Nature Communications (2025). High credibility.

A naive approach would be to simply apply a constant resetting rate within the desired shape and zero resetting rate outside (Equation (S15) in the Supplementary Information). However, using our approach to predict the resulting steady-state, we find that this naive guess leads to a very fuzzy distribution, in which the arrow is barely discernible (see Fig. 5 a). This problem cannot be solved by increasing the resetting rate. The reason is that, for every reasonably complicated NESS, there will be areas that are very hard to reach without crossing regions with a high resetting rate. In our case, the area engulfed by the arrow can only be approached from the right, which significantly lowers its occupation probability, leading to the fuzzy arrow.

Fig. 5
Non-equilibrium steady-state design.

Two-dimensional arrow-shaped non-equilibrium steady-state (NESS). a Predicted from simulations without resetting with a naive r (x, y). b Predicted from simulations without resetting with an improved r (x, y). c Obtained from brute-force simulations with the improved resetting protocol.

Previously, testing different resetting protocols, in a trial-and-error fashion, would have required running thousands of trajectories with resetting for every protocol until the desired steady-state would have been obtained. Instead, using our approach, we can design an improved adaptive resetting protocol (Equation (S16) in the Supplementary Information) that would lead to the desired shape (Fig. 5 b), using the same set of trajectories without resetting that is already available. In the improved resetting strategy, the resetting rate was increased quadratically in the distance from the center of the box, and to the right of the arrow, to prevent the accumulation of density in the box's corners and to the right of the arrow, respectively. Results from simulations with this resetting protocol are given in Fig. 5 c for comparison, showing excellent agreement with our prediction.

---

### Casein [^fd0595f4]. FDA (2009). Low credibility.

Volume desired x Concentration desired = Volume needed x Concentration available.

Example 1: If a 1:10 w/v extract is available and it is desired to use a 1:1,000 w/v extract substitute as follows:

Vd x Cd = Vn x Ca

10ml x 0.001 = Vn x 0.1

0.1 mL = Vn

Using a sterile technique, remove 0.10 mL of extract from the 1:10 vial and place it into a vial containing 9.90 mL of sterile diluent. The resulting ratio will be a 10 mL vial of 1:1,000 w/v.

Example 2: If a 10,000 pnu/ml extract is available and it is desired to use a 100 pnu/ml extract substitute as follows:

10ml x 100 = Vn x 10,000

0.1 mL = Vn

Using a sterile technique, remove 0.10 mL of extract from the 10,000 pnu/ml vial and place it into a vial containing 9.90 mL of sterile diluent. The resulting concentration will be a 10 mL vial of 100 pnu/ml.

Example 3: If a 10,000 AU/ml or BAU/ml extract is available and it is desired to use a 100 AU/ml or BAU/ml extract substitute as follows: Vd x Cd = Vn x Ca

---

### Adaptive resetting for informed search strategies and the design of non-equilibrium steady-states [^cfcebb66]. Nature Communications (2025). High credibility.

Fig. 3
Search with environmental information.

a The mean first-passage time to the origin as a function of r 0 and b for one-dimensional diffusion with diffusion coefficient D = 1. Motion starts at L = 1 and is conducted under position-dependent resetting of the form given in Equation (8). The dashed and dotted lines indicate the optimal rate for diffusion with a constant resetting rate, r 0 = 2.5, and the optimal rate for a parabolic resetting, r 0 = 5.6 b 2, respectively. b First-passage time distributionsfor the three marked points in the phase space of (a), estimated from simulations without resetting (solid lines), and brute-force simulations with resetting (markers). For simulations with resetting, we plot the meanstandard deviation of ten independent repetitions with 10 4 trajectories each (errors are smaller than the marker size in almost all cases).

We emphasize that all the results of Fig. 3 a were obtained using a single set of trajectories with no resetting and simply re-evaluating Equations (4–7) for every value of r 0 and b, leading to a different MFPT through Equation (3). Previously, obtaining the above results would have required performing an ensemble of brute-force simulations at every value of the parameters r 0 and b, to map out the MFPT phase space. This approach would be computationally prohibitive in many cases. Moreover, the strength of our approach becomes even clearer when considering other forms of r (x) apart from Equation (8). Tackling these using brute-force simulations, analytical methods, or experiments, would require a complete reanalysis of the problem. On the other hand, using our method, we can use the same initial ensemble of trajectories to generate the MFPT phase space for any resetting protocol, with minimal added cost. In section 4 of the Supplementary Information, we demonstrate this for several other sigmoidal-shaped resetting protocols (Supplementary Fig. S2).

---

### Improved machine learning algorithm for predicting ground state properties [^74810571]. Nature Communications (2024). High credibility.

Fig. 1
Overview of the proposed machine learning algorithm.

Given a vector x ∈ [−1, 1] m that parameterizes a quantum many-body Hamiltonian H (x), the algorithm uses a geometric structure to create a high-dimensional vector. The ML algorithm then predicts properties or a representation of the ground state ρ (x) of Hamiltonian H (x) using the m ϕ -dimensional vector ϕ (x).

The sample complexityof the proposed ML algorithm improves substantially over the sample complexity ofin the previously best-known classical ML algorithm, where c is a very large constant. The computational time of both the improved ML algorithm and the ML algorithm inis. Hence, the logarithmic sample complexity N immediately implies a nearly linear computational time. In addition to the reduced sample complexity and computational time, the proposed ML algorithm works for any distribution over x, while the best previously known algorithmworks only for the uniform distribution over [−1, 1] m. Furthermore, when we consider the scaling with the prediction error ϵ, the best known classical ML algorithm inhas a sample complexity of, which is exponential in 1/ ϵ. In contrast, the improved ML algorithm has a sample complexity of, which is quasi-polynomial in 1/ ϵ.

We also discuss a generalization of the proposed ML algorithm to predicting ground state representations when trained on classical shadow representations –. In this setting, the proposed ML algorithm yields the same reduction in sample and time complexity compared tofor predicting ground state representations.

---

### Optimal anticipatory control as a theory of motor preparation: a thalamo-cortical circuit model [^6506d4f0]. Neuron (2021). Medium credibility.

In order to derive the optimal control law, we further assume that the dynamics of the network remain approximately linear during both movement preparation and execution. This is a good approximation provided only few neurons become silent in either phase (the saturation at zero firing rate is the only source of nonlinearity in our model, c.f.in Equation 8; Figure S5). In this case, the prospective motor errorof Equation 22 affords a simpler, interpretable form, which we derive now. In the linear regime, Equation 8 becomeswith an effective state transition matrix. The network output at time, starting from stateat timeand with no control input thereafter, has an analytical form given byand similarly forwithreplaced by. The final termis a contribution from the external input: it does not depend on the initial condition, and is therefore the same in both cases. Thus, the prospective motor error (Equation 22) attached to a given preparatory stateisThe matrix integral on the r.h.s. of Equation 26 is known as the "observability Gramian"of the pair(;). It is found algebraically as the solution to the Lyapunov equation 1 Thus, under linearity assumptions, the prospective motor error is a quadratic function of the difference between the momentary preparatory stateand an optimal initial stateknown to elicit the right muscle outputs in open loop. The Gramian, a symmetric, positive-definite matrix, determines how preparatory deviations away fromgive rise to subsequent motor errors. Deviations along the few eigenmodes ofassociated with large eigenvalues will lead to large errors in muscle outputs. The optimal control inputwill need to work hard to minimize this type of 'prospectively potent' deviations – luckily, there are few (see Figure 3 D in the main text, where the top 20 eigenvalues ofare shown; see also Supplemental Math Note S1 for further dissection). In contrast, errors occurring along eigenmodes ofwith small eigenvalues – the vast majority – have almost no motor consequences ('prospectively null'). This large bottom subspace ofprovides a safe buffer in which preparatory activity is allowed to fluctuate without sacrificing control quality. It comprises both the "readout-null" and "dynamic-null" directions described in the main text (Figure 3). Geometrically, we can therefore think of the optimal preparatory subspace as a high-dimensional ellipsoid centered on, and whose small and (potentially inifinitely) large axes are given by the top and bottom eigenvectors of, respectively (small axes, steep directions, large eigenvalues; long axes, flat directions, small eigenvalues).

---

### A computational model for structural dynamics and reconfiguration of DNA assemblies [^4511a6dd]. Nature Communications (2023). High credibility.

In the hydrodynamic model, we assumed the Stokes flow of viscous, linear, and incompressible fluids, considering the length scale of structured DNA assemblies. The generalized Rotne-Prager-Yamakawa mobility matrixwas employed to construct the friction matrix where six translational and rotational degrees of freedom for each node were considered as in the structural model (Supplementary Note 3). This mobility matrix accounted for the damping of a DNA structure with a complex shape through the superposition of identical spherical particles with a hydrodynamic radius assigned to each node. Then, the inverse of the mobility matrix provided the friction matrix under the linearity of the Stokes flow, quantifying the effect of the viscosity and random thermal force acting on the structure in the solvent.

The governing equations for Langevin dynamics were solved to obtain the trajectories of the DNA structure by using a direct time integration algorithm that we developed. Half-time stepping and Simpson's rule were introduced in the calculation of the internal force vector based on the Grønbech-Jensen Farago scheme(Supplementary Note 4). Its application to representative linear problems (thermal diffusion in a flat potential and thermal harmonic oscillator) confirmed significantly improved numerical stability and performance compared to the original Grønbech-Jensen Farago scheme, enabling us to use a larger step size in time integration and hence to simulate more efficiently. To rapidly reach an equilibrium configuration, the dynamic analysis often began from a statically obtained configuration by minimizing the energy of structural and electrostatic potentials. The overall flow of the proposed analysis framework is described in Supplementary Fig. 1.

Global shape

To validate the proposed analysis framework, we first investigated the mean shape of ten DNA wireframe structures in equilibrium. They included six triangular, two square, and two hexagonal structures, categorized into two types of edges: two-helix bundle (DX)and 6-helix bundle (6HB). The dynamic simulations began from statically predicted energy-minimum configurations and were performed 500-ns-long simulations for them (Fig. 2). As expected, the structures with stiffer 6HB edges showed smaller means and variances in the root-mean-square deviation (RMSD) values than those with softer DX edges (Fig. 2a and Supplementary Table 1). The RMSD profiles further indicated that square and hexagonal structures with DX edges were flexible enough to reach another energy-minimum configuration more easily during the dynamic simulations, deviating slightly from the initial configuration obtained from static energy minimization (Supplementary Figs. 2 and 3).

---

### Exploration-based learning of a stabilizing controller predicts locomotor adaptation [^8f3a7424]. Nature Communications (2024). High credibility.

Asymptotic gradient estimate

Estimating the gradient of the performance objective J with respect to the parameters p is equivalent to building a local linear model relating changes in parameters p to changes in performance J. This can be understood by noting that a local linear model is the same as a first-order Taylor series, and the gradient ∇ x f of a function f (x) about x 0 appears as the coefficient of the variable x in this first-order Taylor series as follows:The performance J on a given stride will not only depend on the controller parameters p, but the entire system trajectory, which is uniquely determined by the initial system state and the subsequent control actions given by p. So, we posit a linear model that includes dependence on both s i and p i. On stride i, if the initial state is s i, the parameters are p i, and the performance over that stride is J i, a linear model relating these quantities is given by:Here, coefficient matrix G is the gradient of the performance J i on the current stride with respect to the learning parameters p i and the coefficient matrix F is the gradient with respect to initial state s i. Building a linear model of J i with respect to only p i, ignoring the dependence on state s i can lead to incorrect gradient estimates and unstable learning.

---

### ASN kidney health guidance on the management of obesity in persons living with kidney diseases [^07ab6ad4]. Journal of the American Society of Nephrology (2024). High credibility.

Behavioral modification core skills — Table 1 highlights components to facilitate behavior change: Self-monitoring (Daily recording of food intake; Daily or weekly exercise tracking; Regular self-weighing), Goal setting (Set specific, measurable, achievable, relevant, and time-bound goals in collaboration with a clinician; Progress assessment at follow-up visits), Stimulus control (Self-identification of environmental cues triggering unhealthy eating and low physical activity; Create an environment that makes healthy behaviors the default), and Problem solving (Identify a problem in detail; Create potential solutions; Consider the pros and cons of each option; Choose a solution; Develop an implementation plan; Evaluate the effectiveness of the chosen solution once the behavior has been implemented).

---

### Guiding principles for resident remediation: recommendations of the CORD remediation task force [^8e2c381e]. Academic Emergency Medicine (2010). Low credibility.

Remediation of residents is a common problem and requires organized, goal-directed efforts to solve. The Council of Emergency Medicine Residency Directors (CORD) has created a task force to identify best practices for remediation and to develop guidelines for resident remediation. Faculty members of CORD volunteered to participate in periodic meetings, organized discussions and literature reviews to develop overall guidelines for resident remediation and in a collaborative authorship of this article identifying best practices for remediation. The task force recommends that residency programs: 1. Make efforts to understand the challenges of remediation, and recognize that the goal is successful correction of deficits, but that some deficits are not remediable. 2. Make efforts aimed at early identification of residents requiring remediation. 3. Create objective, achievable goals for remediation and maintain strict adherence to the terms of those plans, including planning for resolution when setting goals for remediation. 4. Involve the institution's Graduate Medical Education Committee (GMEC) early in remediation to assist with planning, obtaining resources, and documentation. 5. Involve appropriate faculty and educate those faculty into the role and terms of the specific remediation plan. 6. Ensure appropriate documentation of all stages of remediation. Resident remediation is frequently necessary and specific steps may be taken to justify, document, facilitate, and objectify the remediation process. Best practices for each step are identified and reported by the task force.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^aeeaae45]. Nature Communications (2025). High credibility.

Fitting models to data is an important part of the practice of science. Advances in machine learning have made it possible to fit more-and more complex-models, but have also exacerbated a problem: when multiple models fit the data equally well, which one(s) should we pick? The answer depends entirely on the modelling goal. In the scientific context, the essential goal is replicability: if a model works well to describe one experiment, it should continue to do so when that experiment is replicated tomorrow, or in another laboratory. The selection criterion must therefore be robust to the variations inherent to the replication process. In this work we develop a nonparametric method for estimating uncertainty on a model's empirical risk when replications are non-stationary, thus ensuring that a model is only rejected when another is reproducibly better. We illustrate the method with two examples: one a more classical setting, where the models are structurally distinct, and a machine learning-inspired setting, where they differ only in the value of their parameters. We show how, in this context of replicability or "epistemic uncertainty", it compares favourably to existing model selection criteria, and has more satisfactory behaviour with large experimental datasets.

---

### Fast quantitative parameter maps without fitting: integration yields accurate mono-exponential signal decay rates [^02d449a6]. Magnetic Resonance in Medicine (2018). Low credibility.

Purpose

To develop a computationally fast and accurate algorithm for mono-exponential signal modelling and validate the new technique in the context of R2* mapping for iron overload assessment.

Methods

An algorithm is introduced that directly calculates R2* values from a series of images based on integration of the mono-exponential signal decay curve. The algorithm is fast, because fitting is avoided and only arithmetic computations without iterations are applied. Precision and accuracy of the method is determined in comparison to the conventional log-linear (LL), nonlinear least-squares-based Levenberg-Marquardt (NLM), and squared nonlinear Levenberg-Marquardt (SQNLM) methods, which rely on iterative curve fitting.

Results

In simulations, the signal integration based method consistently had the same or better accuracy than the LL, NLM, and SQNLM algorithms for R2* values ranging from 50 s -1 to 1200 s -1. In phantoms and in vivo (12 participants), this method was robust over a wide range of R2* values and signal-to-noise ratios. Computation times were approximately 100, 1460, and 930 times faster than those of the LL, NLM, and SQNLM methods, respectively.

Conclusions

The fast signal integration method accurately calculates R2* maps. It has the potential to replace conventional, mono-exponential fitting methods for quantitative MRI such as R2* parameter mapping. Magn Reson Med 79:2978–2985, 2018. © 2017 International Society for Magnetic Resonance in Medicine.

---

### A simple alpha / β-independent method to derive fully isoeffective schedules following changes in dose per fraction [^3a48e204]. International Journal of Radiation Oncology, Biology, Physics (2004). Low credibility.

Purpose

Dosimetric errors in delivering the prescribed dose per fraction made early in a treatment can be corrected by modifying the dose per fraction and total dose given subsequently to discovery of the error, using the linear-quadratic model to calculate the correcting doses which should be completed within the same overall time as originally prescribed. This study shows how these calculations can be carried out independently of any alpha/beta ratios to bring the treatment back exactly to planned tolerance simultaneously for all tissues and tumor involved.

Methods

Planned treatment is defined as p Gy per fraction to a total dose P Gy; the initial error is e Gy per fraction given to a total of E Gy. The linear-quadratic formula is assumed to describe all isoeffect relationships between total dose and dose per fraction.

Results and Conclusion

An exact solution is found that describes a compensating dose of d Gy per fraction to a total of D Gy. The formulae are: D = P-E d = Pp-Ee/P-E. Thus the total dose for the complete treatment (error plus compensation) remains as originally prescribed, with hyperfractionation being used to correct an initial hypofractionation error and hypofractionation being used to correct an initial hyperfractionation error. Incomplete repair is shown to perturb this exact solution. Thus compensating treatments calculated with these formulae should not be scheduled in such a manner that would introduce incomplete repair.

---

### High-order michaelis-menten equations allow inference of hidden kinetic parameters in enzyme catalysis [^21a9eeb4]. Nature Communications (2025). High credibility.

Single-molecule measurements provide a platform for investigating the dynamical properties of enzymatic reactions. To this end, the single-molecule Michaelis-Menten equation was instrumental as it asserts that the first moment of the enzymatic turnover time depends linearly on the reciprocal of the substrate concentration. This, in turn, provides robust and convenient means to determine the maximal turnover rate and the Michaelis-Menten constant. Yet, the information provided by these parameters is incomplete and does not allow access to key observables such as the lifetime of the enzyme-substrate complex, the rate of substrate-enzyme binding, and the probability of successful product formation. Here we show that these quantities and others can be inferred via a set of high-order Michaelis-Menten equations that we derive. These equations capture universal linear relations between the reciprocal of the substrate concentration and distinguished combinations of turnover time moments, essentially generalizing the Michaelis-Menten equation to moments of any order. We demonstrate how key observables such as the lifetime of the enzyme-substrate complex, the rate of substrate-enzyme binding, and the probability of successful product formation, can all be inferred using these high-order Michaelis-Menten equations. We test our inference procedure to show that it is robust, producing accurate results with only several thousand turnover events per substrate concentration.

---

### Inferring time derivatives including cell growth rates using gaussian processes [^b5444dae]. Nature Communications (2016). Medium credibility.

Methods

Using a Gaussian process to fit time-series data

In the following, we will denote a Gaussian distribution with mean μ and covariance matrix Σ as(μ, Σ) and use the notation of Rasmussen and Williamsas much as possible.

Prior probability

For n data points y i at inputs x i (each x i is a time for a growth curve), we denote the underlying latent function as f (x). We define a covariance matrix k (x, x ′), which has an explicit dependence on hyperparameters θ, and obeys

where the expectations are taken over the distribution of latent functions (samples of f (x)).

We interpret equation (1) as giving the prior probability distribution of the latent functions f (X), where were we use X to denote the inputs x i, such that

where K (X, X) is the n × n matrix with components k (x i, x j). With f denoting [f (x 1). f (x n)], this prior probability can be written as

noting the dependence of k (x, x ′; θ) on the hyperparameters θ.

---

### A blueprint for a synthetic genetic feedback optimizer [^0f7d579f]. Nature Communications (2023). High credibility.

Considering the dynamics (1)–(2), we seek a feedback optimizer (Fig. 1a) of the formwhere z and u are the internal state and the output of the optimizer, respectively, together with their time-dependent parameter vectors θ z and θ u, and ϵ z characterizes the timescale of the optimizer species. Although any one of ϵ x, ϵ y, and ϵ z can be eliminated by rescaling time (Supplementary Section 1.1), we keep them all to illustrate their effects throughout the paper.

Assuming that F (x, θ y) has a unique global time-dependent optimum at x * (if this is not the case, our approach only yields local optimality), we seek to design the optimizer to ensure that (i) trajectories of the closed loop system (1)–(3) converge towards the unique optimum x *; and (ii) the integrated system is biologically realizable and relies only on non-negative signals.

Gradient ascent and approximate gradient ascent

To illustrate the main idea underpinning the proposed optimizer module, we first consider a single species x that is produced and degraded according to(Supplementary Section 1.1), where u 1 and u 2 represent control signals regulating the production and degradation of x, respectively. In the absence of biological constraints, a potential choice for the optimizer is a gradient-based system with the feedback law u 1 = ∇ x F (x, θ y) and u 2 = − ∇ x F (x, θ y), where ∇ x F (x, θ y) denotes the gradient of F (x, θ y). The closed loop dynamicsrealizes gradient ascent by converging to x = x * where ∇ x F (x, θ y) = 0, thus tracking the time-varying optimum (Fig. 1b). Moreover, it is sufficient to simply swap the signs in the control laws above to implement minimization (gradient descent). Unfortunately, this approach has two fundamental limitations: (i) it requires explicit knowledge of the gradient ∇ x F (x, θ y); and (ii) u 1 and u 2 can become negative (Fig. 1b, gray regions).