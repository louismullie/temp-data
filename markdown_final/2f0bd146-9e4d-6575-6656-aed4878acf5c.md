# Medical Question & Answer

**Sample ID**: 2f0bd146-9e4d-6575-6656-aed4878acf5c
**Dataset Index**: 176161

---

## Question

2. Let p: (x² + x + 1) √(x + 3) ≥ 0, q: x ≥ −2.
Then p is a () condition for q.
(A) sufficient but not necessary
(B) necessary but not sufficient
(C) both sufficient and necessary
(D) neither sufficient nor necessary

---

## Answer

> Let's see… What do we have here? The user is asking whether the inequality ((x² + x + 1)√(x + 3) ≥ 0) is a sufficient, necessary, both, or neither condition for (x ≥ −2). Let's break this down step-by-step. First, I need to think about the domain where the expression is defined. Then, I should analyze when the expression is non-negative. Next, I will compare the solution set of p with the set defined by q to check sufficiency and necessity. Finally, I will verify edge cases and conclude with the correct option.

> Let me first confirm the domain of p, because the square root imposes a restriction. The expression (√(x + 3)) requires (x + 3 ≥ 0), so the domain of p is (x ≥ −3). Outside this domain, p is undefined, which already hints that p cannot be necessary for q if q allows values outside this domain, but I should verify that carefully before concluding anything.

> Now, I need to check the sign of the quadratic factor. The quadratic (x² + x + 1) has discriminant (1² − 4(1)(1) = −3 < 0), and since the leading coefficient is positive, the quadratic is always positive for all real x. Therefore, the sign of the entire expression is determined by the square root term, which is non-negative wherever it is defined. So, ((x² + x + 1)√(x + 3) ≥ 0) simplifies to (√(x + 3) ≥ 0), which is true for all (x ≥ −3). Thus, the solution set of p is exactly ([−3, ∞)).

> Next, I should review the statement q. The condition (x ≥ −2) defines the interval ([−2, ∞)). Hold on, let's not jump to conclusions; I need to compare the two sets precisely. The solution set of p is ([-3, ∞)), which strictly contains ([-2, ∞)), meaning every x that satisfies q also satisfies p, but not vice versa. For example, (x = −2.5) satisfies p but not q, which is a counterexample showing p is not necessary for q.

> I will now examine sufficiency. Because ([−2, ∞) ⊆ [−3, ∞)), whenever p is true, q could be either true or false; in other words, p does not guarantee q. Wait, let me verify the directionality here, because I initially thought this might imply sufficiency, but that would be backwards. Sufficiency would require that p implies q, which is not the case here; instead, q implies p, so p is necessary for q, but I already concluded p is not necessary. Hmm, wait a minute, I should correct that: since q implies p, p is necessary for q, but since p does not imply q, p is not sufficient for q. I almost misstated that, so I need to ensure I keep the implication directions straight.

> Let me reconsider the definitions to avoid confusion. Sufficiency of p for q means (p ⇒ q), which is false here because p can be true while q is false (e.g., (x = -2.5)). Necessity of p for q means (q ⇒ p), which is true here because if (x ≥ −2), then certainly (x ≥ −3), so q implies p. Therefore, p is necessary but not sufficient for q, which corresponds to option B.

> But wait, what if I overlooked an edge case at (x = −3)? I should double-check that the expression is indeed defined and non-negative there. At (x = -3), the square root is zero, so the product is zero, satisfying p, while q is false because (−3 ≱ −2). This confirms again that p can be true while q is false, reinforcing that p is not sufficient for q, and that p is not necessary because q can be true while p is also true, which doesn't violate necessity; rather, it shows q implies p, confirming necessity of p for q.

> Final answer: p is a necessary but not sufficient condition for q, so the correct choice is B.

---

The correct answer is **(B) necessary but not sufficient**.

- **Domain of p**: The expression ((x² + x + 1)√(x + 3)) is defined for (x ≥ −3) because of the square root, and since (x² + x + 1 > 0) for all real (x), (p) is true for all (x ≥ −3).

- **Relationship with q**: Since (q) is (x ≥ −2), every (x) satisfying (q) also satisfies (p) (because ([−2, ∞) ⊂ [−3, ∞)), so (p) is **necessary** for (q).

- **Not sufficient**: However, (p) is not **sufficient** because (p) is true for (x = -2.5), for example, but (q) is false there.

Thus, (p) is **necessary but not sufficient** for (q).

---

The correct answer is **(B) necessary but not sufficient**.

---

The inequality (p: (x² + x + 1)√(x+3) ≥ 0) is defined for (x ≥ −3) and is satisfied for all (x ≥ −3) because (x² + x + 1 > 0) for all real (x) and (√(x+3) ≥ 0) when defined. The condition (q: x ≥ −2) is a subset of the domain where (p) is true, so (p) is **necessary** for (q) (since (q) implies (p)), but (p) is not **sufficient** because (p) is true for (x = -3) and (x = -2.5), where (q) is false. Therefore, (p) is a **necessary but not sufficient** condition for (q).

---

## Domain and analysis of p

The expression ((x² + x + 1)√(x + 3)) is defined when (x + 3 ≥ 0), i.e. (x ≥ −3). Within this domain, (x² + x + 1) is always positive (its discriminant is negative and the leading coefficient is positive), so the sign of the product depends on (√(x+3)), which is non-negative. Thus, (p) is true for all (x ≥ −3).

---

## Relationship between p and q

The condition (q: x ≥ −2) is a subset of the domain where (p) is true (([−2, ∞) ⊂ [−3, ∞)). This means:

- **Necessity**: If (q) is true, then (p) must be true, so (p) is necessary for (q).
- **Not sufficiency**: (p) can be true while (q) is false, e.g. (x = -3) or (x = -2.5), so (p) is not sufficient for (q).

---

## Conclusion

(p) is a **necessary but not sufficient** condition for (q), so the correct answer is **(B)**.

---

## References

### Unbiased estimation of odds ratios: combining genomewide association scans with replication studies [^113Q77h1]. Genetic Epidemiology (2009). Low credibility.

Without loss of generality we assume that the event Q: X 1 ≥ X 2 ≥… ≥ X k has occurred, so that X i = X (i). Let. The pairand Z i = (σ 2, i /σ 1, i) X i)+(σ 1, i /σ 2, i)ϒ i are then sufficient and complete statistics for μ 1, …, μ k. The joint distribution of ϒ i and X 1, …, X k given Q, f (ϒ i, X | Q) is then transformed into f (X, Z i | Q) and. The joint densityis obtained from the integral

which enables the densityto be expressed as the ratio. This is greatly simplified due to numerous cancellations, in particular the selection probabilities which are analogous to those that feature in the denominator of, and cause problems for, formula (1). Using the Rao-Blackwell theorem formula (4), which is, is the uniformly minimum variance unbiased estimator for μ (i)′, conditional on Q (we call it the UMVCUE). This means that, given the ranking in stage 1, the estimator is unbiased for the corresponding effects and has minimum variance among all such unbiased estimators.

We now propose some modifications to this formula in order to apply the same estimation procedure to a genome scan followed by replication. Instead of the magnitude of the point estimates determining the rank order, it is more common in the genomewide setting to rank SNPs according to the statistical significance of their effects, and to restrict attention to those passing an initial P -value threshold p crit. For a one-sided Wald-type test, we can make this extension by conditioning instead on the event

This leads to slight changes in the proof since conditioning on Q * as opposed to Q changes the limits of integration for X i and Y i, with the result that W s, t in (4) becomes

For (8) to work generally we define X (k +1) /σ 1,(k +1) = Φ −1 (1 – p crit). This expression was noted in, though they did not allow for a P -value threshold. If SNPs that confer either an increased or decreased disease risk are of equal interest, as is usually the case, then the rank order of significance should be based on two-sided P -values. This now requires conditioning on the event

---

### Resolving discrepancies between chimeric and multiplicative measures of higher-order epistasis [^114eZs92]. Nature Communications (2025). High credibility.

Multivariate Bernoulli distribution

The three parametrizations we derived for the bivariate Bernoulli distribution extend to the multivariate Bernoulli distribution. Suppose that (X 1, …, X L) ∈ {0, 1} L is distributed according to a multivariate Bernoulli distribution. Then the distribution P (X) of the random variables X is uniquely specified by one of the three following parametrizations.
General parameters: These are 2 L non-negative valuessatisfyingFor example if L = 3, then p 010 = P (X 1 = 0, X 2 = 1, X 3 = 0) and p 110 = P (X 1 = 1, X 2 = 1, X 3 = 0). Note that since, only 2 L − 1 valuesare necessary to define the distribution.
Natural/canonical parameters: These are 2 L real numberssatisfyingSimilar to the general parameters p i, only 2 L − 1 values β S are necessary to uniquely define the distribution. Typically, the parameter, often called a normalizing constant or a partition function of the distribution, is left unspecified. As noted in the bivariate setting, equation (23) shows that the multivariate Bernoulli is an exponential family distribution with 2 L − 1 sufficient statistics of the form ∏ i ∈ S X i for subsets S with ∣ S ∣ > 0. Moreover, by rewriting (23) aswe observe that the natural parameters β correspond to interaction coefficients in a log-linear regression model with response variables p. For example, the natural parameter β 12 is the coefficient of the interaction term x 1 x 2.
Moments/mean parameters: These are 2 L real numberssatisfyingFor example if L = 3, then μ 13 = E [X 1 X 3] while μ 12 = E [X 1 X 2]. The mean parametersare sufficient statistics for the multivariate Bernoulli distribution, as seen in the exponential family form (23) of the multivariate Bernoulli distribution.

---

### Mind your p values [^116vsUkN]. The Journal of Foot and Ankle Surgery (2012). Low credibility.

In a medical study, the goal is to convince readers that a compelling argument has been made to prove the investigators' case. The logic of how this proof is provided in medical studies can be confusing, and is discussed here in simple terms.

---

### When to p and when not to p [^117HVYkz]. Neurogastroenterology and Motility (2023). Medium credibility.

The p-value was proposed in the early 20th century as a potentially useful metric for statistical inference and was defined as "the probability of the observed result, plus more extreme results, if the null hypothesis were true", in the context of a formal statistical testing process. A century later, the scientific community uses it extensively, mostly inappropriately, and often interprets it incorrectly. This editorial briefly reviews the history of the p-value, provides how to properly interpret it, and recommends when to use it and when not to use it.

---

### Machine learning the dimension of a fano variety [^111PBdve]. Nature Communications (2023). High credibility.

Data generation: weighted projective spaces

The following result characterises weighted projective spaces with terminal quotient singularities; this is [ref. Proposition 2.3].

Proposition 1

Letbe a weighted projective space of dimension at least three. Then X has terminal quotient singularities if and only iffor each k {2, …, a − 2}. Here a = a 1 + a 2 + ⋯ + a N and { q } denotes the fractional part q − ⌊ q ⌋ of.

A simpler necessary condition is given by [ref. Theorem 3.5]:

Proposition 2

Letbe a weighted projective space of dimension at least two, with weights ordered a 1 ≤ a 2 ≤… ≤ a N. If X has terminal quotient singularities then a i / a < 1/(N − i + 2) for each i {3, …, N }.

Weighted projective spaces with terminal quotient singularities have been classified in dimensions up to four. Classifications in higher dimensions are hindered by the lack of an effective upper bound on a.

We randomly generated 150,000 distinct weighted projective spaces with terminal quotient singularities, and with dimension up to 10, as follows. We generated random sequences of weights a 1 ≤ a 2 ≤… ≤ a N with a N ≤ 10 N and discarded them if they failed to satisfy any one of the following:
for each i {1, …, N }, whereindicates that a i is omitted;
a i / a < 1/(N − i + 2) for each i {3, …, N };
for each k {2, …, a − 2}.

Condition 1 here was part of our definition of weighted projective spaces above; it ensures that the set of singular points inhas dimension at most N − 2, and also that weighted projective spaces are isomorphic as algebraic varieties if and only if they have the same weights. Condition 2 is from Proposition 2; it efficiently rules out many non-terminal examples. Condition 3 is the necessary and sufficient condition from Proposition 1. We then deduplicated the sequences. The resulting sample sizes are summarised in Table 1.

Table 1
The distribution by dimension in our datasets

---

### Continuous growth reference from 24th week of gestation to 24 months by gender [^111Pr1ED]. BMC Pediatrics (2008). Low credibility.

Appendix

Assuming normality, to estimate the variance, skewness and kurtosis of the population based on the cases in the range of μ ± δσ in a sample for certain values of δ > 0.

Without lost of generality, let μ = 0. Let, where φ (x; σ) is the normal probability distribution function with mean 0 and variance σ 2. It can be deduced that

where Φ (x) is the standard normal cumulative distribution function. Letand. It can be shown that

Letand. It can be shown that

Let p = 2Φ(δ) - 1 be the proportion of observations in the range of μ ± δσ under a normal distribution. Considering a sample of n observations, x 1. x n. Let q 1 be the (1 - p)/2-th quantile and q 2 be the (1 - (1 - p)/2)-th quantile and X r = { x i: q 1 ≤ x i ≤ q 2 } be the reduced sample of size m. An empirical estimate of, j = 2, 3, 4, is given as

The general sample variance, the skewness and the kurtosis are given as

and

respectively. Let, Skew r and Kurt r be the corresponding statistics calculated based on the reduced sample. Substituting – into –, we have the statistics for the full dataset evaluated based on the reduced sample as

and

---

### Confidence reports in decision-making with multiple alternatives violate the Bayesian confidence hypothesis [^115ZJJ3X]. Nature Communications (2020). High credibility.

Inference on a given trial. We assume that the observer knows the mean and standard deviation of each category based on the exemplar dots, and that the observer assumes that the three categories have equal probabilities. The posterior probability of category C given the measurement x is then p (C | x) ∝ p (x | C) = N (x; m C, (σ s 2 + σ 2) I). Instead of the true posterior p (C | x), the observer makes the decisions based on q (C | x), a noisy version of the posterior probability. We obtain a noisy posterior q (C | x) by drawing from a Dirichlet distribution. The Dirichlet distribution is a generalization of the beta distribution. Just like the beta distribution is a continuous distribution over the probability parameter of a Bernoulli random variable, the Dirichlet distribution is a distribution over a vector that represents the probabilities of any number of categories. The Dirichlet distribution is parameterized as

Γ represents the gamma function. p is a vector consisting of the three posterior probabilities, p = (p 1, p 2, p 3) = (p (C = 1 | x), p (C = 2 | x), p (C = 3 | x)). q is a vector consisting of the three posterior probabilities perturbed by decision noise, q = (q 1, q 2, q 3) = (q (C = 1 | x), q (C = 2 | x), q (C = 3 | x)). The expected value of q is p. The concentration parameter α is a scalar whose inverse determines the magnitude of the decision noise; as α increases, the variance of q decreases. To make a category decision, the observer chooses the category that maximizes the posterior probability:

---

### Comparison of olympic-style weightlifting performances of elite athletes: scaling models account for body mass [^117SYPH1]. Medicine and Science in Sports and Exercise (2023). Medium credibility.

Fractional polynomials

As possible models of the relationship between weightlifting performance and body mass, we evaluated FP using quantile regression. Whereas polynomial functions of higher orders may have undesirable curvatures, polynomial functions of lower orders may not provide good fits. Splines that can handle local features in the data may be unstable and do not have a simple functional form. FP is an extended family of polynomial functions that allow modeling of a wide range of nonlinear relationships and may include logarithms, noninteger powers, or repeated powers. They are global functions with simple functional forms for positive continuous covariates such as body mass and can be viewed as a compromise between polynomial models and nonlinear curves estimated by splines. FP is of the form β 0 + β 1 x p 1 + β 2 x p 2, where p 1 and p 2 belong to a limited set P = {−2, −1, −0.5, 0, 0.5, 1, 2, 3} with the convention that x 0 denotes log(x). When p 1 = p 2, this transformation is defined as β 0 + β 1 x p 1 + β 2 x p 1 log (x). There are 8 possible transformations for one-degree FP (when β 2 = 0) and 36 for two-degree FP. For most practical settings, one or two degrees are sufficient, and we considered that sufficient for this setting. Quantile regression models with each of these 44 transformations were estimated to identify the optimal FP transformation using a goodness-of-fit criterion described in the Supplemental Digital Content (see Supplemental Digital Content). This was repeated for quantiles 0.9, 0.75, and 0.5, separately for males and females. The optimal FP transformation f (B) for body mass B was then used in the following formula to produce a scaled two-lift total T, here referred to as "Q-points":

---

### Do CIs give you confidence? [^111KP5zk]. Chest (2012). Low credibility.

This article describes the conceptual basis for the P value and the CI. We show that both are derived from the same underlying concepts and provide useful, but similar information.

---

### Cooperation in alternating interactions with memory constraints [^113WHi9q]. Nature Communications (2022). High credibility.

A recipe for identifying Nash equilibria for alternating games

To predict which memory-1 strategies evolve in the alternating game, we first characterize which of them are Nash equilibria. In the following, we refer to a strategy q as a Nash equilibrium if π (q, q) ≥ π (p, q) for all alternative memory-1 strategies p (for stronger results, see Supplementary Note 2). That is, against a co-player who adopts the Nash equilibrium strategy q, a player has no incentive to choose any different memory-1 strategy. The notion of Nash equilibrium is closely related to evolutionary robustness. In a population of size N, a resident strategy q is called evolutionary robust if no mutant strategy p has a fixation probability larger than neutral, 1/ N. When selection is sufficiently strong, strategies are evolutionary robust if and only if they are Nash equilibria.

Verifying that a given strategy q is a Nash equilibrium is not straightforward. In principle, this requires us to compare its payoff to the payoff of all possible mutant strategies p, taken from the uncountable set of all memory-1 strategies. However, for alternating games, it is possible to simplify the task in two steps (see Supplementary Note 2 for details). The first step is to show that it is sufficient to compare q to all reactive strategies, a strategy set of a lower dimension. The intuition for this result is as follows. Even if player 1 starts out with an arbitrary memory-1 strategy p, it is always possible to find an associated reactive strategythat yields the same stationary distribution and the same payoff against q (Fig. 3). That is, to find the best response to a strategy that remembers both players' last moves, it is sufficient to explore all strategies that only remember the co-player's last move. In particular, not only is there no advantage of having a strictly larger memory than the opponent, as shown by Press and Dyson for simultaneous games. A player can afford to remember strictly less in the alternating game.

---

### Positive and strongly relaxed purifying selection drive the evolution of repeats in proteins [^1114x5wn]. Nature Communications (2016). Medium credibility.

In the third step, the entire protein sequence from the beginning to the end is scanned to predict any additional repeats, following the Gibbs sampler procedure. Specifically, a probability position matrix is defined based on the seed, Q ij, which gives the probability of having in position i = 1-MFI the particular amino acid j = 1–20, and ranking each candidate MFI-mer relative to a background, B, according to: P = (1/MFI) x ∑ (Q ij − B j). Note that when the seed contains only one repeat Q i = 1, such that if B = 0, the maximum of P is 1. Repeats that belong to the periodic structure will have high P values and will often cluster together, separated from other MFI-mers that are unrelated to the periodic structure. Thus, the cluster of MFI-mers belonging to the periodic structure is easily identified by the maximum of ΔP (that is, a 'bending point', representing the large distance of the lowest ranked repeat in the cluster from other MFI-mers). If at least one member of the cluster exceeds an empirical upper threshold of 0.25, and the bending point exceeds a lower threshold of 1/MFI, then all repeats above the bending point are considered. Once repeats are added, the third step is reiterated, updating Q (and P), until no additional repeats are predicted. Note that in each such iteration, exceeding the upper threshold becomes harder. Obviously, in each step above, accumulated repeats must not overlap with each other.

---

### What MEG can reveal about inference making: the case of if… then sentences [^116bna7a]. Human Brain Mapping (2013). Low credibility.

Characterizing the neural substrate of reasoning has been investigated with regularity over the last 10 years or so while relying on measures that come primarily from positron emission tomography and functional magnetic resonance imaging. To some extent, these techniques — as well as those from electroencephalography — have shown that time course is equally worthwhile for revealing the way reasoning processes work in the brain. In this work, we employ magnetoencephalography while investigating Modus Ponens (If P then Q; P//Therefore, Q) in order to simultaneously derive time course and the source of this fundamental logical inference. The present results show that conditional reasoning involves several successive cognitive processes, each of which engages a distinct cerebral network over the course of inference making, and as soon as a conditional sentence is processed.

---

### K-bayes reconstruction for perfusion MRI II: modeling and technical development [^112LoDrY]. Journal of Digital Imaging (2010). Low credibility.

Derivation of K-Bayes E and M steps

For the purpose of clarity, the EM algorithm derivation is presented only for 2D K-Bayes. The adaptation to 3D is relatively straightforward.

Proposition: The j-th step of the K-Bayes EM algorithm (based on Eqs. (4), (5), and (6)) has E-step: where n = PQ is the number of voxels in the image, and M-step:where j * can be taken as j − 1, or for potentially improved performance, the latest available update (either j − 1 or j).

Proof

E-step

The E-step update for each missing data point z [k x, k y, p, q] is determined by its expectation given the observed data, d, and the perfusion map, i.e.

Bayes' Theorem givesFurthermore,

Therefore,

Completing the square, this leads to

Therefore, is complex Gaussian with meanand the j -th E-step update for z [k x, k y, p, q] is.

M-step

To generate the M-step, the posterior distribution, or equivalently the log-posterior distribution, is maximized with respect to each A [p, q]. Note that the posterior distribution is conditional on knowing the full set of latent (or missing) variables, i.e. z.

Differentiating the log-posterior distribution with respect to A [p, q] and setting equal to zero giveswhere δ (p, q) is the set of neighboring voxels to location (p, q). Rearranging Eq. (7) in terms of A [p, q], leads to

Differentiating Eq. (7) again we get that the second differential of the log-posterior is

Because the second differential is positive, this confirms that the solution given by Eq. (8) is a local maximum and therefore the j -th M-step update for A [p, q] is:□

---

### Quantum vertex model for reversible classical computing [^1171Q6bV]. Nature Communications (2017). Medium credibility.

Boundary conditions

Completing the description of the 2D model of universal classical computation requires a discussion of boundary conditions, which determine the type of computational problem one is addressing. For example, if the N -bit input is fully specified and one is interested in the output, all that is needed is to transfer the information encoded into the input left to right by applying sequentially the gates one column of tiles at a time. In this case, if the depth (that is, the number of steps) of the computation is a polynomial in N, this column–column computation reaches the output boundary, and thus solves the problem, in polynomial time.

As mentioned earlier, by using reversible gates one can also represent computational problems with mixed input–output boundary conditions for which only a fraction of the bits on the left (input) edge and a fraction of the bits on the right (output) edge are fixed. A concrete example is the integer factorization problem implemented in terms of a reversible integer multiplication circuit. A reversible circuit for multiplying two N -bit numbers p and q can be constructed using 5 N +1 bits in each column. One needs two N -bit registers for the two numbers p and q to be multiplied, one N -bit carry register c for the ripple-sums, a 2 N -bit register s for storing the answer p × q = s, and one ancilla bit b. For multiplication, one only fixes the boundary conditions on the input: p and q are the two numbers to be multiplied, and c, s and b are all 0's. For factorization we must impose mixed boundary conditions: on the input side the c, s and b registers are fixed to be all 0's; on the output side the s register is now fixed to the number to be factorized, and c and b are again all set to 0. Thus, 3 N +1 bits in the input and output are fixed, while 2 N bits are floating on both boundaries.

Boundary conditions on inputs, outputs, or both are imposed by inserting longitudinal fields at the appropriate bit sites, namely,

with. The sign of an individual h i field determines the value of the spin σ i and thus of the binary variable x i: For h i > 0, x i = 1, while for h i < 0, x i = 0. If no constraint is imposed on a binary variable x i, then h i = 0.

---

### Implementing quantum dimensionality reduction for non-markovian stochastic simulation [^1131bGcd]. Nature Communications (2023). High credibility.

Methods

Stochastic processes and minimal-memory classical modelling

A discrete-time stochastic processconsists of a sequence of random variables X t, corresponding to events drawn from a set, and indexed by a timestep. The process is defined by a joint distribution of these random variables across all timesteps, whererepresents the contiguous (across timesteps) series of events between timesteps t 1 and t 2. We consider stochastic processes that are bi-infinite, such thatand, and stationary (time-invariant), such that P (X 0: L) = P (X t: t + L) ∀ t, L ∈. Without loss of generality, we can take the present to be t = 0, such that the past is given by, and the future. Note that we use upper case for random variables and lower case for the corresponding variates.

A (causal) model of such a (bi-infinite and stationary) discrete-time stochastic process consists of an encoding functionthat maps from the set of possible past observationsto a set of memory states –. The model also requires an update rulethat produces the outputs and updates the memory state accordingly. We then designate the memory cost D f of the encoding as the logarithm of the dimension (i.e. the number of (qu)bits) of the smallest system into which these memory states can be embedded. For classical (i.e. mutually orthogonal) memory states, this corresponds to. For quantum memory states, which may, in general, be linearly dependent.

Let us, for now, restrict our attention to statistically-exact models, such that (f, Λ) must produce outputs with a distribution that is identical to the stochastic process being modelled. Under such a condition, the provably-memory minimal classical model of any given discrete-time stochastic process is known and can be systematically constructed. These models are referred to as the ε - machine of the process, which employs an encoding function f ε based on the causal states of the process. This encoding function satisfiesand given initial memory state, the evolution produces output x 0 with probabilityand updates the memory to state. The memory states are referred to as the causal states of the process, and the associated cost D μ is given by the logarithm of the number of causal states.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^116DSPos]. CDC (2011). Medium credibility.

Appendix G. Mathematical formula for sample size — sample size required to detect the difference of GM between two subpopulation groups defines a z test with null hypothesis GM1 = GM2 or GMmax/GMmin = 1 and alternative GM1 ≠ GM2 or GMmax / GMmin > 1, where GMmax = max(GM1, GM2) and GMmin = min(GM1, GM2). It considers power W (say 80% chance) to detect that one group's GM is at least k fold higher and gives the p-value as p = Prob(Z ≥ z) = 1 − φ−1(z) with Z ~ Normal (0,1). Given α (e.g., 0.05) and target ratio k, power is W = Prob (Z > φ−1(1 − α) − log10(k)/(S√(1/n1 + 1/n2))), where S is the expected standard deviation of log10(VL) in the population of interest; when n1 = n2 = n, W = Prob (Z > φ−1(1 − α) − log10(k)/(S√(2/n))). The required sample size is n = 2[(φ−1(1 − α) − φ−1(1 − W))S/log10(k)]^2, and tables are provided for W = 0.8 and W = 0.9 with α = 0.05.

---

### Scattering approach to diffusion quantifies axonal damage in brain injury [^113wmExd]. Nature Communications (2025). High credibility.

The scattering problem is solved in Methods in three steps: (i) coarse-graining of the 3D diffusion equation in a random tube over its cross-section to obtain the one-dimensional (1d) Fick-Jacobs (FJ) equationwith arbitrary stochastic A (x), valid for times t exceeding the time to traverse the cross-section (t ≳ 1 ms); (ii) finding the fundamental solution (Green's function) of Eq. (4) for a particular configuration of A (x); and (iii) disorder-averaging over the distribution of A (x). Step (iii) gives rise to the translation-invariant Green's function G (ω, q) = 1/[− i ω + D ∞ q 2 − Σ (ω, q)] in the Fourier domain of frequency ω and wavevector q. Steps (ii) and (iii) are fulfilled by summing Feynman diagrams (Fig. 5) representing individual "scattering events" off the cross-sectional variations, which after coarse-graining over sufficiently long ℓ (t) become small to yield the self-energy part Σ (ω, q) asymptotically exact in the limit ω, q → 0 with D ∞ q 2 / ω → 0. The dispersive diffusivity, follows from the pole of G (ω, q) upon expanding Σ (ω, q) up to q 2, yielding Eq. (1) via effective medium theory. In Methods, we also derive the power law tails ω ϑ ~ t − ϑ of diffusive metrics for other universality classes of structural fluctuations Γ η (q) ~ ∣ q ∣ p, relating the structural exponent p to the FJ dynamical exponent ϑ = (p + 1)/2, with p = 0 (short-range disorder) relevant for the axons.

---

### Three learning stages and accuracy-efficiency tradeoff of restricted boltzmann machines [^1129xFh8]. Nature Communications (2022). High credibility.

Our second example (cf. Fig. 3) is closer in spirit to traditional machine-learning applications and involves pattern recognition and artificial image generation. The target distribution p (x) generates 5 × 5 pixel images with a "hook" pattern comprised of 15 pixels (see Fig. 3 a) implanted at a random position in a background of noisy pixels that are independently activated (white, x i = 1) with probability q = 0.1 (see also Supplementary Note 2 B for more details). Periodic boundary conditions are assumed, meaning that p (x) is translationally invariant along the two image dimensions.

We also consider a one-dimensional variant of this example with only M = 4 (M = 5) visible units and an implanted "010" ("0110") pattern, cf. Fig. 3 d. In this case, we can solve the continuous-time learning dynamics (η → 0 limit of (4)) for the exact target and model distributions p (x) and, obviating artifacts caused by insufficient training data or biased gradient approximations, see also Supplementary Note 1.

Our third example (cf. Fig. 4 a–c) is a simplified digit reproduction task. Patterns of the ten digits 0 through 9 (see Fig. 4 a) are selected and inserted uniformly at random into image frames of 5 × 7 pixels, with the remaining pixels outside of the pattern again activated with probability q = 0.1 (see Supplementary Note 2 C for details). No periodic boundary conditions are imposed, i.e. the input comprises proper, ordinary images.

In our fourth example (cf. Fig. 4 d, e), we train RBMs on the MNIST dataset, which consists of 28 × 28-pixel grayscale images of handwritten digits. It comprises a training set of 60,000 and a test set of 10,000 images. We convert the grayscale images with pixel values between 0 and 255 to binary data by mapping values 0… 127 to 0 and 128… 255 to 1 (see also Supplementary Note 2 D).

---

### A practical guide for understanding confidence intervals and P values [^115vcziB]. Otolaryngology — Head and Neck Surgery (2009). Low credibility.

The 95 percent confidence interval about the mean demarcates the range of values in which the mean would fall if many samples from the universal parent population were taken. In other words, if the same observation, experiment, or trial were done over and over with a different sample of subjects, but with the same characteristics as the original sample, 95 percent of the means from those repeated measures would fall within this range. This gives a measure of how confident we are in the original mean. It tells us not only whether the results are statistically significant because the CI falls totally on one side or the other of the no difference marker (0 if continuous variables; 1 if proportions), but also the actual values so that we might determine if the data seem clinically important. In contrast, the P value tells us only whether the results are statistically significant, without translating that information into values relative to the variable that was measured. Consequently, the CI is a better choice to describe the results of observations, experiments, or trials.

---

### Structure and inference in annotated networks [^1138c43s]. Nature Communications (2016). Medium credibility.

Computationally, the most demanding part of the EM algorithm is calculating the sum in the denominator of equation (7), which has an exponentially large number of terms, making its direct evaluation intractable on all but the smallest of networks. Traditionally one gets around this problem by approximating the full distribution q (s) by Monte Carlo importance sampling. In our calculations, however, we instead use a recently proposed alternative method based on belief propagation, which is significantly faster, and fast enough in practice for applications to very large networks.

Final likelihood value

The EM algorithm always converges to a maximum of the likelihood but is not guaranteed to converge to the global maximum — it is possible for there to be one or more local maxima as well. To get around this problem we normally run the algorithm repeatedly with different random initial guesses for the parameters and from the results choose the one that finds the highest likelihood value. In the calculations presented in this paper we did at least 10 such 'random restarts' for each network. To determine which run has the highest final value of the likelihood we calculate the log-likelihood from the right-hand side of (6) using P (A | Θ, s) and P (s | Γ, x) as in equation (2), the final fitted values of the parameters Θ and Γ from the EM algorithm, and q (s) as in equation (7). (As we have said, the right-hand side of (6) becomes equal to the left, and hence equal to the true log-likelihood, when q (s) is given the value in equation (7).)

---

### Input-output maps are strongly biased towards simple outputs [^11354s9E]. Nature Communications (2018). Medium credibility.

Many systems in nature can be described using discrete input-output maps. Without knowing details about a map, there may seem to be no a priori reason to expect that a randomly chosen input would be more likely to generate one output over another. Here, by extending fundamental results from algorithmic information theory, we show instead that for many real-world maps, the a priori probability P(x) that randomly sampled inputs generate a particular output x decays exponentially with the approximate Kolmogorov complexity [Formula: see text] of that output. These input-output maps are biased towards simplicity. We derive an upper bound P(x)≲[Formula: see text], which is tight for most inputs. The constants a and b, as well as many properties of P(x), can be predicted with minimal knowledge of the map. We explore this strong bias towards simple outputs in systems ranging from the folding of RNA secondary structures to systems of coupled ordinary differential equations to a stochastic financial trading model.

---

### Quantifying randomness in real networks [^116kbmDa]. Nature Communications (2015). Medium credibility.

Results

General requirements to a systematic series of properties

The introductory remarks above instruct one to look not for a single base property Y, which cannot be unique or universal, but for a systematic series of base properties Y 0, Y 1,… By 'systematic' we mean the following conditions: (1) inclusiveness, that is, the properties in the series should provide strictly more detailed information about the network structure, which is equivalent to requiring that networks that have property Y d (Y d -random graphs), d > 0, should also have properties Y d ′ for all d ′ = 0, 1, …, d −1; and (2) convergence, that is, there should exist property Y D in the series that fully characterizes the adjacency matrix of any given network, which is equivalent to requiring that Y D -random graphs is only one graph — the given network itself. If these Y -series satisfy the conditions above, then whatever property X is deemed important now or later in whatever real network, we can always standardize the problem of explanation of X by reformulating it as the following question: what is the minimal value of d in the above Y -series such that property Y d explains X? By convergence, such d should exist; and by inclusiveness, networks that have property Y d ′ with any d ′ = d, d +1, …, D, also have property X. Assuming that properties Y d are once explained, the described procedure provides an explanation of any other property of interest X.

---

### Chiral phonons in quartz probed by X-rays [^111B5SSY]. Nature (2023). Excellent credibility.

Source data

Here, are the phonon eigenvectors of each of the n atoms in the unit cell (normalized such that), andandare eigenvectors corresponding to pure right- and left-handed rotations. The phonon angular momentum (L) is then given by L = ħ S. We also report the mode effective charges (Fig. 5b) as a metric of the strength of the interaction between the mode and light, calculated following the method of ref.

When we match the calculated and measured modes, we find that those with strong dichroic contrast are those calculated to have a large chirality. The peak with the largest contrast is at approximately 50 meV for all the reciprocal points we measured (Q 1 in Fig. 4c and Q 2 = (−0.29, 0.14, 0.32) and Q 3 = (−0.25, 0.25, 0.32) in Supplementary Fig. 2), suggesting that a mode that has large phonon circular polarization and energy around 50 meV dominates the contrast. The mode at the energy of approximately 47.6 meV at Q 1, which we refer to as mode X, matches the conditions (Supplementary Fig. 4 and Supplementary Table 1, which tabulates the energy and phonon circular polarization of all phonon modes at the measured Q points). Figure 5c and Supplementary Video 1 visualize mode X at Q 1 and show that it involves a circular motion of the atoms. Importantly, the mode satisfies the symmetry requirement for a chiral phonon mode.

For non-magnetic quartz, the RIXS spectra at the O K edge are mainly sensitive to the O 2 p orbital states. This means that phonon modes that significantly affect, for example, the orientation of the 2 p orbital states will create large scattering contrast in RIXS and will also be strongly X-ray polarization dependent. Figure 5d and Supplementary Video 2 visualize the evolution of the local charge quadrupoles at the O site when the chiral phonon mode is excited (Fig. 5c or Supplementary Video 1). These charge quadrupoles reflect the time evolution of the O 2 p orbitals, which shows that the dichroic RIXS signal is due to an evolution of the chiral stacking of the O 2 p orbital moments in the chiral phonon excitation as described in equations (1) and (2).

---

### Abstract representations of events arise from mental errors in learning and memory [^113CunEv]. Nature Communications (2020). High credibility.

Methods

Maximum entropy model and the infinite-sequence limit

Here we provide a more thorough derivation of our maximum entropy model of human expectations, with the goal of fostering intuition. Given a matrix of erroneous transition counts, our estimate of the transition structure is given by. When observing a sequence of nodes x 1, x 2, …, in order to construct the counts, we assume that humans use the following recursive rule: where B t (i) denotes the belief, or perceived probability, that node i occurred at the previous time t. This belief, in turn, can be written in terms of the probability P (Δ t) of accidentally recalling the node that occurred Δ t time steps from the desired node at time t:

In order to make quantitative predictions about people's estimates of a transition structure, we must choose a mathematical form for P (Δ t). To do so, we leverage the free energy principle: When building mental models, the brain is finely tuned to simultaneously minimize errors and computational complexity. The average error associated with a candidate distribution Q (Δ t) is assumed to be the average distance in time of the recalled node from the target node, denoted. Furthermore, Shannon famously proved that the only suitable choice for the computational cost of a candidate distribution is its negative entropy, denoted. Taken together, the total cost associated with a distribution Q (Δ t) is given by the free energy F (Q) = βE (Q) − S (Q), where β, referred to as the inverse temperature, parameterizes the relative importance of minimizing errors versus computational costs. By minimizing F with respect to Q, we arrive at the Boltzmann distribution p (Δ t) = e − β Δ t / Z, where Z is the normalizing partition function. We emphasize that this mathematical form for P (Δ t) followed directly from our free energy assumption about resource constraints in the brain.

---

### Estimating the success of re-identifications in incomplete datasets using generative models [^114zrNpX]. Nature Communications (2019). High credibility.

Theoretical and empirical population uniqueness

For n individuals x (1), x (2), …, x (n) drawn from X, the uniqueness Ξ X is the expected percentage of unique individuals. It can be estimated either (i) by computing the mean of individual uniqueness or (ii) by sampling a synthetic population of n individuals from the copula distribution. In the former case, we havewhere T x = [∃! i, x (i) = x] equals one if there exists a single individual i such as x (i) = x and zero otherwise. T x follows a binomial distribution B (p (x), n). Thereforeand

This requires iterating over all combinations of attributes, whose number grows exponentially as the number of attributes increases, and quickly becomes computationally intractable. The second method is therefore often more tractable and we use it to estimate population uniqueness in the paper.

For cumulative marginal distributions F 1, F 2, …, F d and copula correlation matrix Σ, the algorithm 1 (Supplementary Methods) samples n individuals from q (⋅|Σ,Ψ) using the latent copula distribution. From the n generated records (y (1), y (2), …, y (n)), we compute the empirical uniqueness

Individual likelihood of uniqueness and correctness

The probability distributioncan be computed by integrating over the latent copula density. Note that the marginal distributions X 1 to X d are discrete, causing the inversestoto have plateaus. When estimating p (x), we integrate over the latent copula distribution inside the hypercube:with ϕ Σ the density of a zero-mean multivariate normal (MVN) of correlation matrix Σ. Several methods have been proposed in the literature to estimate MVN rectangle probabilities. Genz and Bretz, proposed a randomized quasi Monte Carlo method which we use to estimate the discrete copula density.

The likelihood ξ x for an individual's record x to be unique in a population of n individuals can be derived from p X (X = x):

Similarly, the likelihoodfor an individual's record x to be correctly matched in a population of n individuals can be derived from. With, the number of potential false positives in the population, we have:

Note that, since records are independent, T follows a binomial distribution B (n − 1, p (x)).

We substitute the expression for ξ x in the last formula and obtain:

---

### The minimal work cost of information processing [^114WZWVs]. Nature Communications (2015). Medium credibility.

where the sum ranges only over those x that have a non-zero probability of occurring. In the case of deterministic mappings p (x ′| x)∈{0,1}, this corresponds to the maximum number of input states that map to a same output state. For the AND gate, provided all four states 00, 01, 10 and 11 have non-negligible probability of occurring, there are three input states mapping to the same output state, so (3) gives us simply. Also, in simple examples as considered here, the expression (3) is stable to considering an-approximation (Supplementary Note 4); this quantity is thus physically justified.

Crucially, our result reveals that the minimal work requirement in general depends on the specific logical process, and not only on the input and output states. This contrasts with traditional thermodynamics for large systems, where the minimal work requirement of a state transformation can always be written as a difference of a thermodynamical potential, such as the free energy. For example, the minimal work cost of performing specifically an AND gate may differ from that of another logical process mapping an input distribution (p 00, p 01, p 10, p 11) (with ∑ i p i = 1) to the distribution (p ′ 0, p ′ 1) = (p 00 + p 01 + p 10, p 11) (Recall that the classical counterpart of a quantum state is a probability distribution.). To see this, consider the XOR gate, which outputs a 1 exactly when both inputs are different (see Fig. 2b). The minimal work cost requirement of this gate, as given by (3), is now only kT ln 2, as in the worst case, only a single bit of information is erased (again supposing that all four input states have non-negligible probability of occurring). Now, suppose that, for some reason, the input distribution is such that p 01 + p 10 = p 11, that is, the input 11 occurs with the same probability as of either 01 or 10 appearing. Then, the XOR gate reproduces the exact same output distribution as the AND gate: in both cases, we have p ′ 0 = p 00 + p 10 + p 01 = p 00 + p 11 and p ′ 1 = p 11 = p 01 + p 10. In other words, both logical processes have the same input and output state, yet the XOR gate only requires work kT ln 2 compared with the AND gate, which requires 1.6 kT ln 2. Furthermore, we point out that this difference, which appears small in this case, may be arbitrarily large in certain scenarios (Supplementary Note 4).

---

### Radiation therapy for pancreatic cancer: executive summary of an ASTRO clinical practice guideline [^11515GMT]. Practical Radiation Oncology (2019). High credibility.

ASTRO pancreatic cancer radiation therapy — tumor motion with a gating technique (KQ4) reports motion magnitudes by axis and condition. In Huguet, 2015 (36) using single 4-D CT during simulated 40% duty cycle from EE, mean (SD) motion in cm was L-R 0.3 (0.1), A-P 0.2 (0.2), and S-I 0.5 (0.4), with maximums L-R 0.7, A-P 0.6, and S-I 1.3. In Shiinoki, 2011 (15 [60]) using 4-D CT, interfraction variation at EE, 2-sided showed mean (SD) L-R 0.0 (0.3), A-P 0.1 (0.4), S-I 0.0 (0.3) with max L-R 0.7, A-P 1.1, S-I 1.0; at El, 2-sided, mean (SD) was L-R 0.1 (0.5), A-P 0.2 (0.4), S-I 0.1 (0.7) with max L-R 1.6, A-P 1.3, S-I 1.5.

---

### Snapshotting quantum dynamics at multiple time points [^111gfkvm]. Nature Communications (2024). High credibility.

As a concrete example, we reconstruct the three-time QPDs following the quantum circuit of Fig. 2 b. The initial state of the system qubit (171 Yb + ion) is prepared to, whereas described in Fig. 3 a. At time t 1, the first ancilla-assisted measurement is performed. Then the system evolves from t 1 to t 2 under the unitary evolution. At time t 2, the second ancilla-assisted measurement is conducted. From time t 2 to t 3, the system evolves under the unitary evolutionand at time t 3, the system is directly measured (see Methods for further details).

Fig. 3
Experimental reconstruction of the three-time QPD and marginal distribution by the ancilla-assisted measurement.

a The system qubit's evolution is given by two successive rotationsandwith θ = 0.74 π. Note that the measurements are performed at three time points t 1, t 2, and t 3. b The normalized observed trajectories from the measurements at times t 1, t 2, and t 3 are represented by the 2D bar charts, where. The bar charts on the left and right represent the observed trajectories ofand, respectively, which are the measurement results of time t 3. c The three-time QPD p (x 1, x 2, x 3) was reconstructed from the observed trajectories by classical processing. (i) The left blue bars indicate the real parts of the reconstructed three-time QPD, and the negativity of the real QPD is verified for p (x 1 = 0, x 2 = 1, x 3 = 0) (in the blue-line box), which is − 0.123(± 0.060). (ii) The right red bars indicate the imaginary parts of the reconstructed three-time QPD. d The marginal distributions for times t 1, t 2, and t 3 under the unitary dynamicsandshow the snapshotting of the state evolution. The distributions are marginalized over all the other time points of the QPDs. For all figures, error bars indicate standard deviations (STDs).

---

### Quantum theory cannot consistently describe the use of itself [^1115ewML]. Nature Communications (2018). Medium credibility.

Implicit assumptions

Any no-go result, as for example Bell's theorem, is phrased within a particular framework that comes with a set of built-in assumptions. Hence it is always possible that a theory evades the conclusions of the no-go result by not fulfilling these implicit assumptions. Here we briefly discuss how Theorem 1 compares in this respect to other results in the literature.

Bell's original worktreats probabilities as a primitive notion. Similarly, many of the modern arguments in quantum foundations employ probabilistic frameworks –. In contrast, probabilities are not used in the argument presented here — although Assumption (Q) is of course motivated by the idea that a statement can be regarded as "certain" if the Born rule assigns probability-1 to it. In particular, Theorem 1 does not depend on how probabilities different from 1 are interpreted.

Another distinction is that the framework used here treats all statements about observations as subjective, i.e. they are always defined relative to an agent. This avoids the a priori assumption that measurement outcomes obtained by different agents simultaneously have definite values. (Consider for example Wigner's original setup described in section Introduction. Even when Assumptions (C) and (S) hold, agent W is not forced to assign a definite value to the outcome z observed by agent F.) The assumption of simultaneous definiteness is otherwise rather common. It not only enters the proof of Bell's theorembut also the aforementioned arguments based on probabilistic frameworks.

Nevertheless, in our considerations, we used concepts such as that of an "agent" or of "time". It is conceivable that the conclusions of Theorem 1 can be avoided by theories that provide a nonstandard understanding of these concepts. We are, however, not aware of any concrete examples of such theories.

---

### Integrating quality into the cycle of therapeutic development [^113hQZ11]. Journal of the American College of Cardiology (2002). Low credibility.

The quality of healthcare, particularly as reflected in current practice versus the available evidence, has become a major focus of national health policy discussions. Key components needed to provide quality care include: 1) development of quality indicators and performance measures from specific practice guidelines, 2) better ways to disseminate such guidelines and measures, and 3) development of support tools to promote standardized practice. Although rational decision-making and development of practice guidelines have relied upon results of randomized trials and outcomes studies, not all questions can be answered by randomized trials, and many treatment decisions necessarily reflect physiology, intuition, and experience when treating individuals. Debate about the role of "evidence-based medicine" also has raised questions about the value of applying trial results in practice, and some skepticism has arisen about whether advocated measures of clinical effectiveness, the basic definition of quality, truly reflect a worthwhile approach to improving medical practice. We provide a perspective on this issue by describing a model that integrates quantitative measurements of quality and performance into the development cycle of existing and future therapeutics. Such a model would serve as a basic approach to cardiovascular medicine that is necessary, but not sufficient, to those wishing to provide the best care for their patients.

---

### Steering is an essential feature of non-locality in quantum theory [^1146g9W2]. Nature Communications (2018). Medium credibility.

Results

Uncertainty principle — quantum game value correspondence

Let us first recall the precise correspondence between the fine-grained uncertainty relations and the strength of non-locality established in ref. Consider a two-player non-local game G, in which Alice and Bob receive questions x, y from respective input sets X, Y according to some input distribution π X, Y (x, y). They return answers a, b from some output sets A, B, respectively. The winning constraint is specified by a predicate V (a, b | x, y) ∈ {0, 1}. The success probability in the game ω s (G) is thus written aswhererefers to a set of conditional probability distributions (boxes) P A, B | X, Y. One considers boxes taken from sets C, Q, NS corresponding to the set of classical, boxes and general no-signaling boxes, with corresponding values ω c (G), ω q (G), and ω ns (G) respectively. One may also restrict attention to the free games for which the input distributions are independent, i.e. π X, Y (x, y) = π X (x) π Y (y).

We will in particular be interested in ω q (G), i.e. the value obtained from those boxes for which there exists a state ρ on a Hilbert spaceand sets of measurement operators (POVMs)such that P A, B | X, Y (a, b | x, y) = . The idea in ref.is to rewrite the game expression in Eq. (1) asLetbe Bob's marginal probability distribution when his state is steered by Alice to. Now, observe that for each (x, a), the expressionconstitutes a fine-grained uncertainty relation on Bob's system withdenoting the maximum over all possible statesof Bob's system. When the optimal valueequals unity, we refer to the corresponding uncertainty relation as trivial, i.e. while the probabilities are bounded below unity for some states, there exist states for which the outcomes (for each of Bob's inputs y) can be fixed with certainty. On the other hand, when, we infer that one cannot obtain a measurement outcome with certainty for all measurements simultaneously.

---

### Epidemic modelling suggests that in specific circumstances masks may become more effective when fewer contacts wear them [^1168xXTK]. Communications Medicine (2024). Medium credibility.

Parameters

Unless stated otherwise, the following parameter settings were used. Simulations were performed for N = 10 5 nodes and averaged over 20 independent iterations. The parameters were chosen such that k = 20, α = 0.02, γ = 0.1, q = 0.99, q r (0) = q s (0) = 1, η = 1, and ϵ = 0.1. For the initial condition we randomly choose ten nodes and set their X i (t = 0) = I; for all the others we set X i (t = 0) = S. The model halts once the outbreak has ended, i.e. X i (t) ≠ I for all i.

Reporting summary

Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.

---

### Are there circumstances in which phase 2 study results should be practice-changing? [^11248WV2]. Hematology: American Society of Hematology. Education Program (2007). Low credibility.

New pharmaceuticals, innovative combinations of approved agents, and novel treatment modalities have resulted in a marked increase in the need for clinical trials. Evidence for treatment efficacy is best derived from large phase 3 randomized, controlled clinical trials. However, phase 3 investigations are lengthy and expensive, and consume patient resources. Furthermore, some diseases and treatment indications are rare, and adequate numbers of patients for a definitive phase 3 trial do not exist. Consequently, it is imperative for clinicians to understand phase 2 trial design, since their interpretation is required to apply the findings in clinical practice appropriately. The complexity of phase 2 studies is explored, including unique designs, possible use of randomization, and other key elements necessary for interpretation of phase 2 trials. Specific examples and application of these concepts are discussed in this review.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^116hTmko]. CDC (2011). Medium credibility.

Community viral load sample size calculations — sample size to detect differences in geometric mean (GM) viral load depends on power and standard deviation (S). The table specifies "α = 0.05 and W = 0.8", with S columns 1 to 1.5; for k = 3 the minimum sample sizes by S are 54, 66, 78, 92, 106, and 122, and "1.2 is the standard deviation of national VL data", so at S = 1.2 the needed sample size is 78.

---

### Truncated product method for combining P-values [^117FbFjw]. Genetic Epidemiology (2002). Low credibility.

We present a new procedure for combining P-values from a set of L hypothesis tests. Our procedure is to take the product of only those P-values less than some specified cut-off value and to evaluate the probability of such a product, or a smaller value, under the overall hypothesis that all L hypotheses are true. We give an explicit formulation for this P-value, and find by simulation that it can provide high power for detecting departures from the overall hypothesis. We extend the procedure to situations when tests are not independent. We present both real and simulated examples where the method is especially useful. These include exploratory analyses when L is large, such as genome-wide scans for marker-trait associations and meta-analytic applications that combine information from published studies, with potential for dealing with the "publication bias" phenomenon. Once the overall hypothesis is rejected, an adjustment procedure with strong family-wise error protection is available for smaller subsets of hypotheses, down to the individual tests.

---

### Power of data in quantum machine learning [^114SGAen]. Nature Communications (2021). High credibility.

While this simple example makes the basic point that sufficient data can change complexity considerations, it perhaps opens more questions than it answers. For example, it uses a rather weak encoding into amplitudes and assumes one has access to an amount of data that is on par with the dimension of the model. The more interesting cases occur if we strengthen the data encoding, include modern classical ML models, and consider the number of data N much less than the dimension of the model. These more interesting cases are the ones we quantitatively answer.

Our primary interest will be ML algorithms that are much stronger than fitting a quadratic function and the input data are provided in more interesting ways than an amplitude encoding. In this work, we focus on both classical and quantum ML models based on kernel functions k (x i, x j). At a high level, a kernel function can be seen as a measure of similarity, if k (x i, x j) is large when x i and x j are close. When considered for finite input data, a kernel function may be represented as a matrix K i j = k (x i, x j) and the conditions required for kernel methods are satisfied when the matrix representation is Hermitian and positive semi-definite.

---

### A NIGT1-centred transcriptional cascade regulates nitrate signalling and incorporates phosphorus starvation signals in arabidopsis [^116rUBpB]. Nature Communications (2018). Medium credibility.

Fig. 8
The PHR1-NIGT1 cascade modulates nitrate uptake. a NIGT1-binding site-mediated P-starvation response of NRT2. 1 promoter activity. Transgenic Arabidopsis seedlings harbouring the LUC gene under the control of the wild-type or (mut1+2) mutant NRT2. 1 promoter were grown under P-sufficient (+P) or P-starved (−P) conditions for 3 days and then treated with 1 mM luciferin. Images of LUC activity in living seedlings were captured in two independent transgenic lines. Scale bar, 1 cm. b Nitrate uptake activity of nigt1 quadruple mutants (Q-1 and Q-2) during P starvation. 15 NO 3 − uptake by Arabidopsis Col and mutant seedlings grown hydroponically under P-sufficient (+P) or P-starved (−P) conditions for 5 days was measured at 0.2 mM K 15 NO 3. Values are means of five biological replicates ± s.d. p < 0.01 by two-tailed t -test

---

### SCENIC international consensus statement on surveillance and management of dysplasia in inflammatory bowel disease [^115LcrbT]. Gastrointestinal Endoscopy (2015). Medium credibility.

High definition white light surveillance colonoscopy in inflammatory bowel disease quantifies detection yields. Among all patients surveyed, high definition white light with targeted biopsy detected dysplasia in 15.4% (95% CI, 9.3%-24.5%; Cochran's Q 10.3 (P = 0.02), I^2 = 70%) versus random biopsy alone in 1.6% (95% CI, 0.7%-3.6%; Cochran's Q 1.26 (P = 0.73) and I^2 = 0%). Among patients with dysplasia, targeted biopsy identified 90.6% (95% CI, 80.1%-95.9%; Cochran's Q 0.3 (P = 0.96) and I^2 = 0%) versus 9.4% (95% CI, 4.1%-19.9%; Cochran's Q 0.3 (P = 0.96) and I^2 = 0%) with random biopsy alone. The studies included a total of 8739 random biopsy specimens in 426 IBD patients, and dysplasia was identified in 0.2% (95% CI, 0.0%-1.2%) of all random biopsy specimens taken; Heterogeneity: Cochran's Q 48.1 (P < .001) and I^2 = 91%. In the reduction of colorectal cancer incidence and mortality, random biopsies could not be adequately assessed due to insufficient power and/or longitudinal data.

---

### Brain tumor classification using the diffusion tensor image segmentation (D-SEG) technique [^117VvRFw]. Neuro-Oncology (2015). Low credibility.

The p and q maps are a set of observationswhere each observation is a 2D real vector in (p, q) space (Fig. 1 C). Clustering by k -means partitions the n data points into K disjoint subsets S j, whereby minimizing the within-cluster sum of squares objective function, where x n is a vector representing the n th data point, and μ j is the geometric centroid of the data points in S j. Centroids of the initial clusterswere selected by separating (p, q) space into K segments of roughly equal size according to median and quartile values of p and q (Fig. 1 D, Table 1). These initial conditions preserve the non-Gaussian structure of the p and q histograms in the cluster initialization. As the data are non-Gaussian, the centroid was defined to be the median (p, q) coordinate of each cluster. The following 2 steps of the algorithmwere repeated:

Table 1.
Number and percent of voxels in each D-SEG segment at initialization and termination of the k -means algorithm

Step 1. Assignment step: Assign each voxel to the cluster whose centroid is closest in (p, q) space, thus partitioning the voxels into K clusters, shown here at the t th iteration,

Step 2. Update step: Calculate the new centroids for each cluster, shown for iteration t + 1,

An iterative exponential decrease in the number of voxels changing cluster was observed. D-SEG was terminated after 250 iterations, after which convergence was achieved. Final segmentation of (p, q) space is displayed as a Voronoi tessellation(Fig. 1 E).

Selection of K

We tested our segmentation technique using a range of different K values (K = 4, 9, 16, and 25). K = 16 was selected because it provided the optimum computation time and allowed identification of our a priori postulated regions within a tumor-affected brain, namely: (i) healthy brain GM, (ii) heterogeneous WM, (iii) CSF, (iv) solid tumor, (v) regional necrosis, (vi) tumor-associated cystic regions, (vii) perilesional edema, (viii) perilesional tumor infiltration, and (ix) distant edema while also identifying differences among the 5 tumor types studied.

---

### Identifying domains of applicability of machine learning models for materials science [^114zq9ZY]. Nature Communications (2020). High credibility.

A predictive ML model is then a functionaiming to minimize the expected error (also called prediction risk)measured by some non-negative loss function l that quantifies the cost incurred by predicting the actual property value y with f (x). Examples for loss functions are the squared error, the absolute error, and, for non-zero properties, the relative error. Here P denotes some fixed probability distribution that captures how candidate materials are assumed to be sampled from the materials class (this concept, while commonly assumed in ML, is an unnecessary restriction for high-throughput screening as we discuss in more detail below). Since the true prediction risk is impossible to compute directly without perfect knowledge of the investigated materials class, models are evaluated by the test error (or empirical risk)defined as the average of the individual errors (losses) e i (f) = l (f (x i), y i) on some test set of m reference data points. The samples in this test set are drawn independently and identically distributed according to P and are also independent of the model — which means in practice that it is a random subset of all available reference data that has been withheld from the ML algorithm. In order to reduce the variance of this estimate, a common strategy is cross-validation, where this process is repeated multiple times based on partitioning the data into a number of non-overlapping "folds" and then to use each of these folds as test sets and the remaining data as a training set to fit the model.

This test error properly estimates the model performance globally over the whole representation space X (weighted by the distribution P used to generate the test points). This is an appropriate evaluation metric for selecting a model that is required to work well on average for arbitrary new input materials that are sampled according to the same distribution P. This is, however, not the condition of high-throughput screening. Here, rather than being presented with random inputs, we can decide which candidate materials to screen next. This observation leads to the central idea enabled by the DA analysis proposed in this work: if the employed model is particularly applicable in a specific subdomain of the materials class, and if that subdomain has a simple and interpretable shape that permits to generate new materials from it, then we can directly focus the screening there.

---

### Summary benchmarks-full set – 2024 [^1169zFGq]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — GRADE evidence quality ratings are defined for forming recommendations for care as follows: "Good quality (GQ): Further research is very unlikely to change our confidence in the estimate of effect", "Moderate quality (MQ): Further research is likely to have an important impact on our confidence in the estimate of effect and may change the estimate", and "Insufficient quality (IQ): Further research is very likely to have an important impact on our confidence in the estimate of effect and is likely to change the estimate; any estimate of effect is very uncertain".

---

### The dynamic relationships between the three events that release individual Na ⁺ ions from the Na ⁺ / K ⁺-ATPase [^115HczuR]. Nature Communications (2012). Medium credibility.

But, during the longer voltage steps (for example, the first in the sequence), the rapid initial charge movement at −120 mV was eventually followed by a much slower, and lower-amplitude, current decay representing the slow charge component (Q s) linked to the major occlusion transition P-E 2. Na 3 ↔(Na 3)E 1 -P (Fig. 1a). The longest steps to −120 mV allow the distribution of pumps to attain a new steady state, poised towards the occluded conformation (Na 3)E 1 -P. In those cases, return of the membrane potential to 0 mV elicited not only a rapidly decaying, though relatively low amplitude, outward Q f, reflecting release of bound Na + from P-E 2. Na 3, but also a subsequent slowly decaying Q s, reflecting deocclusion and release through the electric field of Na + that had become occluded in (Na 3)E 1 -P at −120 mV. Because, on return to 0 mV, Na + ions occluded at −120 mV must await deocclusion before they can be released to the bulk solution, the quantity of Na + released immediately (Q f) showed a strong inverse correlation with step duration; the longer the stay at −120 mV, the smaller the amplitude of the rapidly decaying current on return to 0 mV (Fig. 2a).

---

### Recovery after stroke: not so proportional after all? [^1129zvs9]. Brain (2019). Medium credibility.

Point A corresponds to the canonical example of spurious r(X,Δ), introduced in the last section: i.e. σ Y /σ X ≈ 1 and r(X, Y) ≈ 0, but r(X,Δ) ≈ −0.71 (Fig. 3 A). At Point B, σ Y /σ X ≈ 1 and r(X, Y) is strong, so recovery is approximately constant (Fig. 3 B) and r(X,Δ) ≈ 0, consistent with the view that strong r(X, Y) curtail spurious r(X,Δ). However, the situation is more complex when σ Y /σ X is more skewed.

When σ Y /σ X is large, Y contributes more variance to Y − X (Δ), and r(X,Δ) ≈ r(X, Y); this is Regime 1. Points C and D illustrate the convergence (Fig. 3 C and D). By contrast, when σ Y /σ X is small, X contributes more variance to Y − X, and r(X,Δ) ≈ r(X, −X): i.e. −1 (Supplementary material, Appendix A, theorem 2); this is Regime 2, where the confound emerges. Point E, near Regime 2, corresponds to data in which all patients recover proportionally (Δ = 70% of lost function; Fig. 2 E). Here, σ Y /σ X is already small enough (0.3) to be dangerous: after randomly shuffling Y, r(X, Y) ≈ 0, but r(X,Δ) is almost unaffected (Point F, and Fig. 3 F). In other words, if even the proportional recovery rule is approximately right, empirical data may enter territory, on the surface in Fig. 2, where over-optimistic r(X,Δ) are likely.

---

### Clinical utility of polygenic risk scores for embryo selection: a points to consider statement of the American College of Medical Genetics and genomics (ACMG) [^1151TykV]. Genetics in Medicine (2024). High credibility.

PGT-P results communication — Figure 3 illustrates using 3 components (percentile rank, relative risk, and absolute risk): families (a) and (b) have embryos at rank extremes while family (c) shows a broader range; two families display different relative risk ranges, with family x values 0.9, 1.0, 1.1, 1.2 and family y values 0.6, 0.9, 1.1, 1.4; example absolute risk increases span 1.0% - 1.4% versus 10.0% - 14.0% for diseases differing by 10x population prevalence.

---

### Methodologies for the development of CHEST guidelines and expert panel reports [^116vrcED]. Chest (2014). Medium credibility.

CHEST grading framework — dimensions and evidence adjustment are defined explicitly. The CHEST grading system defines the grades based on two key dimensions: (1) the balance of benefits of the proposed action as compared with the possible harms or risks and (2) the methodologic quality of the supporting evidence. Both factors are represented in the overall grade, with the former represented by 1 or 2 and the latter A, B, or C. The methodologic quality is initially based on the hierarchy of study design, but studies can be upgraded or downgraded based on specific criteria (eg, existence of methodologic flaws, directness, precision, consistency of results).

---

### Power of data in quantum machine learning [^115ZsrNe]. Nature Communications (2021). High credibility.

Ultimately, to see a prediction advantage in a particular dataset with specific function values/labels, we need a large separation between s C and s Q. This happens when the inputs x i, x j considered close in a quantum ML model are actually close in the target function f (x), but are far in classical ML. This is represented as the final test in Fig. 1 and the methodology here outlines how this result can be achieved in terms of its more essential components.

Projected quantum kernels

In addition to analyzing existing quantum models, the analysis approach introduced also provides suggestions for new quantum models with improved properties, which we now address here. For example, if we start with the original quantum kernel, when the effective dimension d is large, kernel Tr(ρ (x i) ρ (x j)), which is based on a fidelity-type metric, will regard all data to be far from each other and the kernel matrix K Q will be close to identity. This results in a small geometric difference g CQ leading to classical ML models being competitive or outperforming the quantum kernel method. In Supplementary Section 9, we present a simple quantum model that requires an exponential amount of samples to learn using the quantum kernel Tr(ρ (x i) ρ (x j)), but only needs a linear number of samples to learn using a classical ML model.

---

### ACOEM practice guidelines: elbow disorders [^1131WLjR]. Journal of Occupational and Environmental Medicine (2013). Medium credibility.

ACOEM elbow disorders — strength-of-evidence criteria define levels and thresholds as follows: levels are labeled "A = Strong evidence base", "B = Moderate evidence base", "C = Limited evidence base", and "I = Insufficient evidence†"; A requires "Two or more high-quality studies", B requires "At least one high-quality study or multiple moderate-quality studies", C requires "At least one study of moderate quality", and I applies when "Evidence is insufficient or irreconcilable". Study quality scoring specifies "High-quality studies are scored 8.0 to 11.0 points. Moderate-quality studies are scored 4.0 to 7.5 points". The guideline notes "Insufficient evidence recommendations are, by definition, consensus-based guidance".

---

### A co-design framework of neural networks and quantum circuits towards quantum advantage [^115MwAWr]. Nature Communications (2021). High credibility.

Neural computation P-LYR

An m -input neural computation component is illustrated in Fig. 4 c, where m -input data I 0, I 1, ⋯, I m −1 and m corresponding weights w 0, w 1, ⋯, w m −1 are given. Input data I i is a real number ranging from 0 to 1, while weight w i is a {−1, +1} binary number. Neural computation in P-LYR is composed of 4 operations: (i) R: this operation converts a real number p k of input I k to a two-point distributed random variable x k, where P { x k = −1} = p k and P { x k = +1} = 1 − p k, as shown in Fig. 4 b. For example, we treat the input I 0 's real value of p 0 as the probability of x 0 that outcomes −1 while q 0 = 1 − p 0 as the probability that outcomes +1. (ii) C: this operation calculates y as the average sum of weighted inputs, where the weighted input is the product of a converted input (say x k) and its corresponding weight (i.e. w k). Since x k is a two-point random variable, whose values are −1 and +1 and the weights are binary values of −1 and +1, if w k = − 1, w k ⋅ x k will lead to the swap of probabilities P { x k = −1} and P { x k = + 1} in x k. (iii) A: we consider the quadratic function as the nonlinear activation function in this work, and A operation outputs y 2 where y is a random variable. (iv) E: this operation converts the random variable y 2 to 0–1 real number by taking its expectation. It will be passed to batch normalization to be further used as the input to the next layer.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^115Xt6bN]. CDC (2011). Medium credibility.

Table 1b — Minimum sample size for detecting geometric mean (GM) ratio k with α = 0.05 and W = 0.9 — presents required sample sizes by standard deviation (S) strata and defines parameters; α is significance level, W is power, GM is geometric mean, and S is standard deviation, noting that 1.2 is the standard deviation of national viral load (VL) data. Each jurisdiction will need to assess the standard deviation of their local VL data and then determine the appropriate sample size needed to assess VL, and if the sample size is inadequate to meet the recommended case inclusion criterion, an alternate method may need to be used, such as combining multiple years of data. For k = 3, sample sizes across S = 1, 1.1, 1.2, 1.3, 1.4, 1.5 are 75, 91, 108, 127, 147, 169, and Table 1b uses power = 90%; jurisdictions may also explore differences in means of viral loads, including categorical differences in the proportion with undetectable or very low VL.

---

### LINS: a general medical Q&A framework for enhancing the quality and credibility of LLM-generated responses [^111WCRWP]. Nature Communications (2025). High credibility.

Large language models can lighten the workload of clinicians and patients, yet their responses often include fabricated evidence, outdated knowledge, and insufficient medical specificity. We introduce a general retrieval-augmented question-answering framework that continuously gathers up-to-date, high-quality medical knowledge and generates evidence-traceable responses. Here we show that this approach significantly improves the evidence validity, medical expertise, and timeliness of large language model outputs, thereby enhancing their overall quality and credibility. Evaluation against 15,530 objective questions, together with two physician-curated clinical test sets covering evidence-based medical practice and medical order explanation, confirms the improvements. In blinded trials, resident physicians indicate meaningful assistance in 87.00% of evidence-based medical scenarios, and lay users find it helpful in 90.09% of medical order explanations. These findings demonstrate a practical route to trustworthy, general-purpose language assistants for clinical applications.

---

### Quantum theory cannot consistently describe the use of itself [^115gPRu4]. Nature Communications (2018). Medium credibility.

Gambler: "The outcome w = ok proves, due to (4), that S was not prepared in state. This means that r = heads and hence the casino must pay me €1000".

Casino: "The outcomeimplies, due to (6), that our employee observed. This in turn proves that S was not prepared in state. But this means that r = tails, so the gambler must pay us €500".

How should the judge decide on this case? Could it even be that both assertions must be accepted as two "alternative facts" about what the value r was? We leave it as a task for further research to explore what the different interpretations of quantum mechanics have to say about this game.

Theorem 1 may be compared to earlier no-go results, such as. which also use assumptions similar to (Q) and (S) (although the latter is often implicit). These two assumptions are usually shown to be in conflict with additional assumptions about reality, locality, or freedom of choice. For example, the result of ref. which is as well based on an extension of Wigner's argument, asserts that no theory can fulfil all of the following properties: (i) be compatible with quantum theory on all scales, (ii) simultaneously assign definite truth values to measurement outcomes of all agents, (iii) allow agents to freely choose measurement settings, and (iv) be local. Here, we have shown that Assumptions (Q) and (S) are already problematic by themselves, in the sense that agents who use these assumptions to reason about each other as in Fig. 3 will arrive at inconsistent conclusions.

---

### Six steps in quality intervention development (6SQuID) [^111W4Ygh]. Journal of Epidemiology and Community Health (2016). Low credibility.

Improving the effectiveness of public health interventions relies as much on the attention paid to their design and feasibility as to their evaluation. Yet, compared to the vast literature on how to evaluate interventions, there is little to guide researchers or practitioners on how best to develop such interventions in practical, logical, evidence based ways to maximise likely effectiveness. Existing models for the development of public health interventions tend to have a strong social-psychological, individual behaviour change orientation and some take years to implement. This paper presents a pragmatic guide to six essential Steps for Quality Intervention Development (6SQuID). The focus is on public health interventions but the model should have wider applicability. Once a problem has been identified as needing intervention, the process of designing an intervention can be broken down into six crucial steps: (1) defining and understanding the problem and its causes; (2) identifying which causal or contextual factors are modifiable: which have the greatest scope for change and who would benefit most; (3) deciding on the mechanisms of change; (4) clarifying how these will be delivered; (5) testing and adapting the intervention; and (6) collecting sufficient evidence of effectiveness to proceed to a rigorous evaluation. If each of these steps is carefully addressed, better use will be made of scarce public resources by avoiding the costly evaluation, or implementation, of unpromising interventions.

---

### Identifying continuous quality improvement publications: what makes an improvement intervention' CQI'? [^112YF9SM]. BMJ Quality & Safety (2011). Medium credibility.

Methods

Overview

We first elicited a broad range of existing definitions for CQI, distilled them into candidate key features, and engaged an expert panel to rate and refine the features. We then used a previously published QII definitionas the basis for a QII screening form and applied it to articles from a broad electronic search. Finally, we operationalised the highest-scoring consensus-based CQI features as an assessment form and applied it to the QII article set.

Identification of potential key features of CQI

To identify key features of CQI, we conducted a simplified, sequential group consensus process, similar to a repeated focus group with feedback. We organised a 12-member expert panel, intentionally encompassing a diverse range of methodological perspectives representing both quality improvement and research expertise. Individual experts included process consultants, researchers and institutional decisionmakers from both the USA and the UK; several additionally serve as editors of clinical or quality improvement journals (see 'Acknowledgements' for complete list). To begin generating potentially definitional features of CQI, Robert Wood Johnson Foundation staff reviewed grant applications to the ASCQI Program and abstracted 48 phrases used by applicants to define 'CQI'. Two authors (LR, SH) independently reviewed these phrases to ascertain common themes, reconceptualised them as a list of unique, potentially definitional features, and then met to discuss and reach agreement on the list.

The expert panel then completed an online survey of the features, reviewed survey results, and discussed the results on two conference calls. The survey asked, for each feature: 'Is this feature necessary (definitional) for CQI?' (5 = definitely; 4 = probably; 3 = no difference; 2 = probably not; 1 = definitely not). The survey and discussion process enabled the addition of features to the original set from other sources, as suggested by the panel or research team. Table 1 lists 12 features (A–L) finalised when the process had ceased generating additional potential features. Panelists rated the 12 features again at a final in-person meeting, resulting in six features (A, C, D, E, G, K) rated as 'definitely' or 'probably' necessary (definitional) for CQI (median value ≥ 4.0). The final column in table 1 shows which items on the final CQI features assessment form reflected each 'definitely' or 'probably' definitional feature.

Table 1
Potentially definitional continuous quality improvement (CQI) features

---

### What matters matter? P values, H values, leadership, and us [^117SXmxb]. Obstetrics and Gynecology (2003). Low credibility.

Framed by the question "What matters matter?", this essay considers today's physicians' need for leadership, the principled road they embarked on, and the reasons to continue. Taken as a whole, the vast problems of health care seem unsolvable. Approached in small tangible steps, if not cure, could direction, even inspiration, appear? When people are sick, they look to physicians. By actually caring for patients, physicians have earned trust and learned, scientifically and artfully, about life in ways others cannot. Meaningful patient-centered care occurs at the junction of logical science and tenuous human needs: "p values" and "h values". Along with the privileged understanding gained from patients comes the responsibility to stand publicly for the rights of all patients to private moments. Standing up in these ways can never be easy; then again, it never was. It is the continued journey toward historic ideals. It is an imprecise place of struggle, where caring "leadership" has always been most needed, fulfilling and truly defining physicians. Despite today's seemingly insurmountable obstacles, in this place each physician can find ways to reenergize around what matters matter.

---

### Droperidol [^112Svbnh]. FDA (2023). Medium credibility.

DESCRIPTION

Droperidol Injection, USP, is a sterile, non-pyrogenic, aqueous solution for intravenous or intramuscular use only.

Each mL contains:

Droperidol… 2.5 mg

Water for Injection… \.q.s.

pH adjusted between 3.0 and 3.8 with lactic acid and, if necessary, with sodium hydroxide.

Droperidol is a neuroleptic (tranquilizer) agent. Chemically it is 1-(1-[3-(p-fluorobenzoyl)propyl]-1,2,3,6-tetrahydro-4-pyridyl)-2-benzimidazolinone and the structural formula is:

---

### Rotator cuff tendinopathy diagnosis, nonsurgical medical care, and rehabilitation: a clinical practice guideline [^115CBWnc]. The Journal of Orthopaedic and Sports Physical Therapy (2025). High credibility.

Grades of recommendations — This table defines grades A–F and links each to a strength-of-evidence statement and corresponding Level of Obligation. Grade A is defined as "A preponderance of level I and/or level II studies support the recommendation", with Level of Obligation (Based on Treatment Effects): "Must: benefits substantially outweigh harms; Should: benefits moderately outweigh harms; May: benefits minimally outweigh harms or benefit/harm ratio is value dependent; Should not: harms minimally or moderately outweigh benefits or evidence of no effect; Must not: harms largely outweigh benefits". Grade B is "A single high-quality randomized controlled trial or a preponderance of level II studies support the recommendation", with obligations "Should: benefits substantially outweigh harms; May: benefits moderately or minimally outweigh harms or benefit/harm ratio is value dependent; Should not: evidence that harms outweigh benefits or evidence of no effect". Grade C is "A single level II study or a preponderance of level III and IV studies, including statements of consensus by content experts, support the recommendation", with obligations "Should: benefits substantially outweigh harms; May: benefits moderately or minimally outweigh harms or benefit/harm ratio is value dependent; Should not: harms minimally or moderately outweigh benefits". Grade D is "Higher-quality studies conducted on this topic disagree with respect to their conclusions", with obligation "May: conflicting evidence, the benefit/harm ratio is value dependent". Grade E is "A preponderance of evidence from animal or cadaver studies, from conceptual models/principles, or from basic science/bench research support this conclusion", with obligations "May: in the absence of evidence from clinical studies, theoretical and/or foundational evidence supports benefit" and "Should not: in the absence of evidence from clinical studies, theoretical and/or foundational evidence suggests strong risk of harms". Grade F is "Best practice based on the clinical experience of the guideline development team supports this conclusion", and the table also specifies Level of Obligation (Based on Assessment/Diagnosis): "Must: strongly supported by consensus-based best practice/standard of care; Should: moderately supported by best practice/standard of care; May: supported by expert opinion in the absence of consensus; Should not: best practice/standard of care indicates potential harms; Must not: potential harms are strongly supported by consensus-based best practice/standard of care".

---

### Using Q-methodology to guide the implementation of new healthcare policies [^115go2DW]. BMJ Quality & Safety (2018). Medium credibility.

There are many challenges in the development, implementation and evaluation of healthcare policy. One challenge is understanding how different stakeholders view a particular policy and what impact these views may have during implementation. Q-methodology is one approach that can be used to help policy makers and researchers actively engage with those who are important in policy implementation, and anticipate their responses. Q-methodology combines qualitative and quantitative research methods to systematically explore and describe the range of viewpoints about a topic. Participants are required to rank a set of predefined statements relating to the topic, according to their own viewpoint. Factor analytic techniques then identify people who are like-minded in the way they view the topic and enable areas of consensus and divergence in viewpoint to be clearly defined. This mapping of viewpoints allows those working in policy implementation to anticipate likely barriers and levers in implementing new policies.

---

### Reyataz [^1145hggU]. FDA (2022). Medium credibility.

[Reyataz Figure L]

Figure L

Step 1. Prepare the infant formula according to the directions on the infant formula package.

Step 2. Pour 10 mL of infant formula into the medicine cup (see Figure M).

[Reyataz Figure M]

Figure M

Step 3. Tap the packet of REYATAZ oral powder to settle the contents to the bottom of the packet (see Figure N).

[Reyataz Figure N]

Figure N

Step 4. Using a clean pair of scissors, cut open the packet on the dotted line (see Figure O).

[Reyataz Figure O]

Figure O

Step 5. Empty the contents of the packet into the medicine cup (see Figure P).

[Reyataz Figure P]

Figure P

Repeat Steps 3 through 5 for each packet of REYATAZ oral powder needed for the total prescribed dose.

Step 6. Hold the medicine cup with one hand. With your other hand, use the small spoon to gently mix the powder and the infant formula (see Figure Q).

[Reyataz Figure Q]

Figure Q

Steps 7 through 9 must be completed within 1 hour of mixing the medicine.

Step 7. Draw up the powder and infant formula mixture into the oral dosing syringe as follows:

---

### A complete tool set for molecular QTL discovery and analysis [^114aia48]. Nature Communications (2017). Medium credibility.

Mapping proximal molQTLs using conditional analysis

The two mapping approaches above only report a single candidate QTL per phenotype or group of phenotypes. In some cases, this limitation may reduce significantly the number of discoveries. For example, it is relatively frequent that expression for a given gene is affected by multiple proximal eQTLs. A well-established approach to discover multiple QTLs with independent effects on a given phenotype relies on conditional analysis: new discoveries are made by conditioning on previous ones. In QTLtools, we implemented a conditional analysis scheme based on stepwise linear regression that is fast, accounts for multiple testing and automatically learns the number of independent signals per phenotype. Specifically, we implemented it as follows for both grouped and ungrouped phenotypes:
Initialization. We determine a nominal P value threshold of significance on a per-phenotype basis. To do so, we first perform a permutation pass as described above which gives us an adjusted P value per phenotype (or group of phenotypes) together with its most likely beta parameter values. Next, we determine the adjusted P value threshold corresponding to the targeted FDR level (for example, 5% FDR) and feed the beta quantile function (for example, R/q beta) with it to get a specific nominal P value threshold for each phenotype. Here, the beta quantile function allows us to use the Beta distribution in a reversed way: from adjusted P value to nominal P value. Note that the resulting nominal P value thresholds vary from one phenotype to the other depending on the complexity of the cis regions being tested and the effective number of independent tests they encapsulate.
Forward pass. We next learn the number of independent signals per phenotype using stepwise regressions with forward variable selection. More specifically, we start from the original phenotype quantifications and search for the variant in cis with the strongest association. When the corresponding nominal P value of association is below the threshold defined in step (1), we store the variant as additional and independent discovery and residualize its genotypes out from the phenotype quantifications. We then repeat these two steps until no more significant discovery is made: this immediately gives us the number of independent molQTLs together with a best candidate variant for each.
Backward pass. Finally, we try to assign nearby genetic variants to the various independent signals we discovered in step (2). To do so, we define a linear regression model that contains all candidate QTLs discovered so far in the forward pass: P = Q 1 +… + Q i +… + Q R where R is the number of independent signals and { Q 1, …, Q i, …, Q R } are the corresponding best molQTL candidates. Then, we test all possible hypotheses by fitting this model R x(L - R) times each time fixing { Q 1, …, Q i−1, Q i+1, …, Q R } and setting Q i as another variant in cis (L – R variants in cis not being a candidate molQTL times R independent signals). We then end up with a vector of R nominal P values for each variant in cis which allows us to determine the signal the variant belongs to by simply finding the smallest P value in this vector and comparing it to the significance threshold obtained in step (1).

---

### Different EEG brain activity in right and left handers during visually induced self-motion perception [^116qrjSL]. Journal of Neurology (2020). Medium credibility.

Vection strength

The median vection strength reported in the coherent condition was 5.00 for left handers and 5.50 for right handers, with no significant statistical difference between the two groups (Z = − 1.25, p = 0.21, r = − 0.25). In the incoherent condition the median vection strength was 0 for both left and right handers, with no significant difference between the two groups (Z = − 0.06, p = 0.96, r = − 0.01). The Spearman's rank-order correlation revealed a small negative correlation, with vection strength decreasing over trials for both the coherent (r s = − 0.08, p < 0.001) and incoherent conditions (r s = − 0.08, p < 0.001). This small negative correlation corresponds to a mean decrease of vection strength by 0.24 in the coherent condition and 0.35 in the incoherent condition, between the start and end of the experiment (mean of first 10 versus last 10 trials).

In summary, in both the coherent and incoherent conditions no statistically significant differences were observed between left and right handers on measures of vection presence, onset latency, duration and vection strength (Fig. 2).

Fig. 2
Behavioural data. a Vection presence, i.e. percentage of trials in which vection was reported as present. b Onset latency, i.e. time from motion onset to vection onset, in seconds. c Duration, i.e. length of vection period, in seconds. d Vection strength ratings on a scale of 0 ('no vection') to 10 ('I felt I was really moving'). Each panel presents a boxplot with the median group value for the coherent and incoherent conditions, for both left (white) and right (grey) handers. The box around the median represents the 25th and 75th percentile, with the whiskers extending to the most extreme scores. Crosses represent outliers, calculated as values greater than q 3 + w × (q 3 − q 1) or less than q 1 − w ×(q 3 − q 1), where w is the maximum whisker length and q 1 and q 3 are the 25th and 75th percentiles, respectively

EEG data

---

### Principles of analytic validation of immunohistochemical assays: guideline update [^116FtypX]. Archives of Pathology & Laboratory Medicine (2024). High credibility.

Supplemental Digital Content — AMSTAR (Assessing the Methodological Quality of Systematic Reviews) appraisal of methodological quality presents checklist items marked with "√" or "x": "Quality assessed & documented" is "√" and "x"; "Quality used appropriately for conclusion" is "√" and "x"; "Methods to combine used appropriately" is "√" and "√"; "Publication bias assessed" is "x" and "x"; and "COI (conflict of interest)" is "√" and "√". The AMSTAR SCORE /11 is 9 and 5.

---

### Clinical utility of polygenic risk scores for embryo selection: a points to consider statement of the American College of Medical Genetics and genomics (ACMG) [^111KfX3a]. Genetics in Medicine (2024). High credibility.

Application of PRSs for embryo selection — evaluation framework specifies that the application of PRS testing for embryo selection requires consideration of multiple factors in each step of the testing process, including: (1) the impact of the in vitro fertilization (IVF) process itself on clinical utility of preimplantation genetic testing for polygenic traits (PGT-P), (2) the clinical validity of PGT-P, (3) ethical issues from different clinical scenarios in which PGT-P testing could be requested, and (4) challenges in providing informed consent and communicating disease risk information; these factors inform whether PGT-P should be offered in clinical practice.

---

### Communication-efficient federated learning via knowledge distillation [^117QT8i9]. Nature Communications (2022). High credibility.

Dynamic gradients approximation

In our FedKD framework, although the size of mentee model updates is smaller than the mentor models, the communication cost can still be relatively high when the mentee model is not tiny. Thus, we explore to further compress the gradients exchanged among the server and clients to reduce computational cost. Motivated by the low-rank properties of model parameters, we use SVD to factorize the local gradients into smaller matrices before uploading them. The server reconstructs the local gradients by multiplying the factorized matrices before aggregation. The aggregated global gradients are further factorized, which are distributed to the clients for reconstruction and model update. More specifically, we denote the gradientas a matrix with P rows and Q columns (we assume P ≥ Q). It is approximately factorized into the multiplication of three matrix, i.e.g. i ≈ U i ∑ i V i, where, are factorized matrices and K is the number of retained singular values. If the value of K satisfies P K + K 2 + K Q < P Q, the size of the uploaded and downloaded gradients can be reduced. Note that we formulate g i as a single matrix for simplicity. In practice, different parameter matrices in the model are factorized independently, and the global gradients on the server are factorized in the same way. We denote the singular values of g i as [σ 1, σ 2. σ Q] (ordered by their absolute values). To control the approximation error, we use an energy threshold T to decide how many singular values are kept, which is formulated as follows:

To better help the model converge, we propose a dynamic gradient approximation strategy by using a dynamic value of T. The function between the threshold T and the percentage of training steps t is formulated as follows:where T start and T end are two hyperparameters that control the start and end values of T. In this way, the mentee model is learned on roughly approximated gradients at the beginning, while learned on more accurately approximated gradients when the model gets to convergence, which can help learn a more accurate mentee model.

To help readers better understand how FedKD works, we summarize the entire workflow of FedKD (Algorithm 1).

---

### Carp [^116YATQk]. FDA (2009). Low credibility.

Volume desired x Concentration desired = Volume needed x Concentration available.

Example 1: If a 1:10 w/v extract is available and it is desired to use a 1:1,000 w/v extract substitute as follows:

Vd x Cd = Vn x Ca

10ml x 0.001 = Vn x 0.1

0.1 mL = Vn

Using a sterile technique, remove 0.10 mL of extract from the 1:10 vial and place it into a vial containing 9.90 mL of sterile diluent. The resulting ratio will be a 10 mL vial of 1:1,000 w/v.

Example 2: If a 10,000 pnu/ml extract is available and it is desired to use a 100 pnu/ml extract substitute as follows:

10ml x 100 = Vn x 10,000

0.1 mL = Vn

Using a sterile technique, remove 0.10 mL of extract from the 10,000 pnu/ml vial and place it into a vial containing 9.90 mL of sterile diluent. The resulting concentration will be a 10 mL vial of 100 pnu/ml.

Example 3: If a 10,000 AU/ml or BAU/ml extract is available and it is desired to use a 100 AU/ml or BAU/ml extract substitute as follows: Vd x Cd = Vn x Ca

---

### The native ORAI channel trio underlies the diversity of casignaling events [^112GjSk8]. Nature Communications (2020). High credibility.

Fig. 1
SOCE but not ORAI1 is required for maintenance of Ca 2+ oscillations.

a – e Representative Ca 2+ oscillations in response to 10 µM carbachol (Cch) measured using Fura2 in wild-type HEK293 and ORAI-TKO cells. Cells were maintained in HBSS containing 2 mM Ca 2+ and stimulated with 10 µM Cch at 1 min (indicated by arrow in "a"). Representative traces from five cells/condition were chosen to represent the datasets as a whole. In c, d, cells were treated with 5 µM Gd 3+ to block SOCE. In e, ORAI-TKO cells were treated with 1 mM Gd 3+ (so-called Gd 3+ insulation to block both SOCE and Ca 2+ extrusion). f Quantification of total oscillations/14 min for all conditions from a – e (from left to right n = 80, 84, 180, 127, and 78; n -values correspond to individual cells). g – l Representative Ca 2+ oscillations in response to 10 µM Cch measured using Fura2 over the course of 15 min in ORAI single (SKO) and double (DKO) knockout cell clones. m – p Quantification of total oscillations/14 min (m) (from left to right n = 190, 111, 58, 29, 206, 214, and 67; n -values correspond to individual cells), % of oscillating cells (n), % of plateau cells (o), and % of non-responding cells (p) for data in g – l (for n – p, from left to right n = 12, 6, 6, 6, 9, 6, and 9; n -values correspond to independent experiments). q Direct ER Ca 2+ measurements using CEPIA1 er in parental HEK293 cells and ORAI-DKO cells. r – w Measurements of ER Ca 2+ release (in 0 mM Ca 2+) and SOCE (in 2 mM Ca 2+) on store depletion with 2 µM thapsigargin (Tg) in parental HEK293 cells and ORAI-SKO and DKO cells; all traces are plotted as mean ± SEM. x Quantification of SOCE magnitude for all conditions from r – w. Scatter plots (f, m, x) are represented as mean ± SEM and were statistically analyzed using a Kruskal–Wallis one-way ANOVA with multiple comparisons to WT HEK293 where (✱ p < 0.05, ✱✱ p < 0.0001) (from left to right n = 413, 244, 243, 217, 256, 179, and 435; n -values correspond to individual cells). Where indicated, an independent Kruskal–Wallis one-way test was performed comparing ORAI2-3-DKO cells to ORAI2-SKO and ORAI3-SKO cell lines. Scatter plots (n – p) are represented as mean ± SEM and were statistically analyzed using an ordinary one-way ANOVA with multiple comparisons to WT HEK293 (✱ p < 0.05).

---

### Size limits the sensitivity of kinetic schemes [^113ujoyg]. Nature Communications (2023). High credibility.

A different measure of sensitivity — the amplification of a fold-change in the input — provides a solution to this problem. Suppose that for some value x 0 of the input parameter x, scaling by a factor a scales the output by b, so f (a x 0) = b f (x 0). Then the quotientcan be thought of as a discrete approximation of the derivative defining the logarithmic sensitivity. And if f (x) is differentiable everywhere, then by the mean value theorem, there must be a value x * of x for whichThis means that careful measurement of any two points on the input-output curve (x versus f (x)) witnesses the (local, infinitesimal) logarithmic sensitivity somewhere. Importantly — unlike in the case of fitting to a Hill function — if error in the measurements is very low, then they are also telling us the derivative for some value of x very accurately.

Equation (25) leads us to another common definition of the effective Hill coefficient:where S 0.9 and S 0.1 are the values of the input variable (in our case, x) required to get 90% and 10% (respectively) of the maximum value of the output variable (in our case, f (x)). Note that (26) is like (25) with a = S 0.9 / S 0.1 and b = 9. It implies that somewhere between S 0.1 and S 0.9 there is a logarithmic sensitivity of H eff /2.

There is yet another common definition, specific to models of binding. Suppose x is the concentration of a ligand and 〈 n b 〉(x) is the expected number of sites bound by a ligand out of a total of n possible binding sites. It is common then, to takeor to report, as the Hill coefficient, the slope of a line fitted to x versusdata on a log-log plot.

---

### The POP-Q system: two decades of progress and debate [^113gM2GK]. International Urogynecology Journal (2014). Low credibility.

I appreciate the opportunity to provide my reflections on the POP-Q, now more than 20 years since it was first conceived and 17 years since its publication in the American Journal of Obstetrics and Gynecology. The invitation led me to dig into old files and recall — through a series of insightful emails, faxes, meeting summaries and draft documents — a document made possible by the combined efforts of an extraordinary group of individuals.

---

### Understanding statistical terms: 2 [^112Mjh3N]. Drug and Therapeutics Bulletin (2009). Low credibility.

An increasing number of statistical terms appear in journal articles and other medical information. A working knowledge of these is essential in assessing clinical evidence. With this in mind, we are producing a series of explanatory articles covering various statistical terms and their uses. This, the second article in the series, will focus on some of the most common terms used in reporting the results of randomised controlled trials.1.

---

### Correction [^111zWxo2]. Journal of Radiology Case Reports (2017). Low credibility.

[This corrects the article on p. 8 in vol. 10.].

---

### Evolution of reporting P values in the biomedical literature, 1990–2015 [^1131TBix]. JAMA (2016). Excellent credibility.

Importance

The use and misuse of P values has generated extensive debates.

Objective

To evaluate in large scale the P values reported in the abstracts and full text of biomedical research articles over the past 25 years and determine how frequently statistical information is presented in ways other than P values.

Design

Automated text-mining analysis was performed to extract data on P values reported in 12,821,790 MEDLINE abstracts and in 843,884 abstracts and full-text articles in PubMed Central (PMC) from 1990 to 2015. Reporting of P values in 151 English-language core clinical journals and specific article types as classified by PubMed also was evaluated. A random sample of 1000 MEDLINE abstracts was manually assessed for reporting of P values and other types of statistical information; of those abstracts reporting empirical data, 100 articles were also assessed in full text.

Main Outcomes and Measures

P values reported.

Results

Text mining identified 4,572,043 P values in 1,608,736 MEDLINE abstracts and 3,438,299 P values in 385,393 PMC full-text articles. Reporting of P values in abstracts increased from 7.3% in 1990 to 15.6% in 2014. In 2014, P values were reported in 33.0% of abstracts from the 151 core clinical journals (n = 29,725 abstracts), 35.7% of meta-analyses (n = 5620), 38.9% of clinical trials (n = 4624), 54.8% of randomized controlled trials (n = 13,544), and 2.4% of reviews (n = 71,529). The distribution of reported P values in abstracts and in full text showed strong clustering at P values of .05 and of .001 or smaller. Over time, the "best" (most statistically significant) reported P values were modestly smaller and the "worst" (least statistically significant) reported P values became modestly less significant. Among the MEDLINE abstracts and PMC full-text articles with P values, 96% reported at least 1 P value of .05 or lower, with the proportion remaining steady over time in PMC full-text articles. In 1000 abstracts that were manually reviewed, 796 were from articles reporting empirical data; P values were reported in 15.7% (125/796 [95% CI, 13.2%-18.4%]) of abstracts, confidence intervals in 2.3% (18/796 [95% CI, 1.3%-3.6%]), Bayes factors in 0% (0/796 [95% CI, 0%-0.5%]), effect sizes in 13.9% (111/796 [95% CI, 11.6%-16.5%]), other information that could lead to estimation of P values in 12.4% (99/796 [95% CI, 10.2%-14.9%]), and qualitative statements about significance in 18.1% (181/1000 [95% CI, 15.8%-20.6%]); only 1.8% (14/796 [95% CI, 1.0%-2.9%]) of abstracts reported at least 1 effect size and at least 1 confidence interval. Among 99 manually extracted full-text articles with data, 55 reported P values, 4 presented confidence intervals for all reported effect sizes, none used Bayesian methods, 1 used false-discovery rates, 3 used sample size/power calculations, and 5 specified the primary outcome.

Conclusions and Relevance

In this analysis of P values reported in MEDLINE abstracts and in PMC articles from 1990–2015, more MEDLINE abstracts and articles reported P values over time, almost all abstracts and articles with P values reported statistically significant results, and, in a subgroup analysis, few articles included confidence intervals, Bayes factors, or effect sizes. Rather than reporting isolated P values, articles should include effect sizes and uncertainty metrics.

---

### First and second stage labor management: ACOG clinical practice guideline no. 8 [^111w8hvQ]. Obstetrics and Gynecology (2024). High credibility.

Strength and quality labels — For STRONG recommendations, ACOG recommends and benefits clearly outweigh harms and burdens and most patients should receive the intervention, whereas ACOG recommends against when harms and burdens clearly outweigh the benefits and most patients should not receive the intervention; for CONDITIONAL recommendations, ACOG suggests and the balance of benefits and risks will vary depending on patient characteristics and their values and preferences with individualized, shared decision making recommended. Quality of evidence categories include HIGH, MODERATE, LOW, and VERY LOW, with HIGH supported by randomized controlled trials, systematic reviews, and meta-analyses without serious methodologic flaws or limitations and by very strong evidence from observational studies and with high confidence in the accuracy of the findings, while VERY LOW reflects unsystematic clinical observations or very indirect evidence from observational studies; intermediate categories have corresponding definitions. Good Practice Points are ungraded and are incorporated when clinical guidance is deemed necessary in the case of extremely limited or nonexistent evidence and are based on expert opinion as well as review of the available evidence.

---

### Menotropins (menopur) [^114iHxhn]. FDA (2018). Low credibility.

Step 2. Removing the Q•Cap and adding your needle for injection.

When you have finished mixing the last vial needed for your injection and have withdrawn all the medicine into the syringe, remove the syringe from the Q•Cap by twisting the syringe counter-clockwise while holding the Q•Cap steady. See Figure O. Throw away the Q•Cap with the attached vial into your household trash.

You are now ready to attach the needle to the syringe for your injection.

Your healthcare provider will tell you what needle you should use for your injection.
While holding the syringe with the syringe tip pointing up, place the needle on the top of the syringe. Gently push down on the needle and twist the needle onto the syringe in a clockwise direction until it is tight. See Figure P.

Do not remove the needle cap until you are ready for your injection. (See Step 4)
Carefully set the syringe with the needle down on the table. See Figure Q.

Step 3. Prepare Injection site for MENOPUR or MENOPUR mixed with BRAVELLE.

Select a site to inject MENOPUR or MENOPUR mixed with BRAVELLE on your stomach area (abdomen).
Pick a site on your lower abdomen, 1–2 inches below the navel, alternating between left and right sides.
Each day, inject in a different site to help reduce soreness and skin problems. For example, on day 1, inject yourself on the right side of your abdomen. The next day, inject yourself on the left side of your abdomen. Changing your injection sites every day will help reduce soreness and skin problems. See Figure R.

---

### CORRECTION [^1157KMU3]. Annals of Family Medicine (2017). Low credibility.

[This corrects the article on p. 14 in vol. 15.].

---

### Guidelines for parenteral nutrition in preterm infants: the American Society for Parenteral and Enteral Nutrition [^113ot3DV]. JPEN: Journal of Parenteral and Enteral Nutrition (2023). High credibility.

Parenteral nutrition in preterm infants — higher vs lower amino acid (AA) dose and head circumference standard deviation score (SDS) at 36 weeks' corrected age or discharge shows no clear difference overall, with a pooled mean difference of −0.05 [−0.19, 0.08] and low heterogeneity (τ^2 = 0.00, I^2 = 0.00%, H^2 = 1.00), with Q(3) = 2.12, P = 0.55 and overall effect z = −0.77, P = 0.44; representative study effects include Balakrishnan, 2018: −0.21 [−0.50, 0.08] and Bloomfield, 2022: 0.05 [−0.16, 0.26].

---

### A framework for evaluating clinical artificial intelligence systems without ground-truth annotations [^117ELR7e]. Nature Communications (2024). High credibility.

Evaluate classifier

After training g ϕ, we evaluated it on a held-out set of data comprising data points with ground-truth labels (from both class 0 and class 1) (Fig. 1 b, Step 5). The intuition here is that a classifier which can successfully distinguish between these two classes, by performing well on the held-out set of data, is indicative that the training data and the corresponding ground-truth labels are relatively reliable. Since the data points from class 1 are known to be correct (due to our use of ground-truth labels), then a highly-performing classifier would suggest that the class 0 pseudo-labels of the remaining data points are likely to be correct. In short, this step quantifies how plausible it is that the sampled unlabelled data points belong to class 0.

As presented, this approach determines how plausible it is that the sampled set of data points in some probability interval, s ∈ (s 1, s 2], belongs to class 0. It is entirely possible, however, that a fraction of these sampled data points belong to the opposite class (e.g. class 1). We refer to this mixture of data points from each class as class contamination. We hypothesised (and indeed showed) that the degree of this class contamination increases as the probability output, s, by an AI system steers away from the extremes (s ≈ 0 and s ≈ 1). To quantify the extent of this contamination, however, we also had to determine how plausible it was that the sampled set of data points belong to class 1, as we outline next.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^113JA7ko]. Annals of the American Thoracic Society (2023). High credibility.

Ventilation–perfusion (V A /Q) distribution modeling is binned into 50 equally spaced (log scale) compartments, with V A /Q ratios < 0.005 (shunt) and > 100 (dead space) separately calculated, and relative dispersion and gravitational gradients may also be assessed.

---

### Non-selective distribution of infectious disease prevention may outperform risk-based targeting [^114vPswe]. Nature Communications (2022). High credibility.

Derivation of F i n d i r

In the following, we implicitly assume that all should be evaluated at g = 0. At equilibrium, we perform the derivativeon both sides of the first line in Eq. (1):We compute the derivative of ξ from its definition in Eq. (1):and insert it into Eq. (12):This equation constitutes a linear system for the matrix J k m = d x k / d g m. Defining the auxiliary variables, v k = k p k, w k = k p k (y k − x k) and, we can rewrite Eq. (14) as

To get J, we note that the matrix u v T − D is a rank-1 perturbation of a diagonal matrix, and invert it by means of Ref.(Sherman–Morrison formula):By inserting the definitions of u, v, w and D into Eq. (16), and after some algebra, we get an explicit expression of J, and thus the derivative d x k / d g m.

Now, with d x k / d g m, y k, and x k at hand, and again after some algebra, we get to the final form of F i n d i r [Eq. (4)], and thus f (k):where we definedExpectation values are computed similarly to Eq. (11) (see also Supplementary Note 9).

---

### Radiation therapy for pancreatic cancer: executive summary of an ASTRO clinical practice guideline [^111gc13J]. Practical Radiation Oncology (2019). High credibility.

ASTRO pancreatic cancer radiation therapy — margins to account for tumor motion with a gating technique (KQ4) specifies necessary motion margins. Campbell, 2017 using CBCT projections targeted a geometric coverage goal of 95% from mean EE position with margins (cm): Left 0.2, Right 0.2, Ant 0.2, Post 0.2, Sup 0.3, Inf 0.3. Shiinoki, 2011 using 4-D CT and the 95th percentile of position variation reported El position margins Left 1.0, Right 0.9, Ant 0.9, Post 0.6, Sup 1.3, Inf 1.3 and EE position margins Left 0.5, Right 0.6, Ant 0.9, Post 0.7, Sup 0.5, Inf 0.6.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^114XJRus]. Annals of the American Thoracic Society (2023). High credibility.

V_A/Q distribution modeling — derived from cumulative plots of ventilation or perfusion versus V_A/Q — is binned into 50 equally spaced log-scale compartments, with shunt and dead space handled separately. Specifically, V_A/Q ratios < 0.005 (shunt) and > 100 (dead space) are calculated as separate components, and relative dispersion and gravitational gradients may also be assessed.

---

### Approaches to enhancing radiation safety in cardiovascular imaging: a scientific statement from the American Heart Association [^114ej6s6]. Circulation (2014). Medium credibility.

Table 1. Applying classification of recommendations and level of evidence — the schema defines directive wording by class and clarifies evidence levels. Class I uses "Procedure/Treatment SHOULD be performed/ administered", with suggested phrases including "should", "is recommended", "is indicated", and "is useful/effective/beneficial". Class IIa states "IT IS REASONABLE to perform procedure/administer treatment", with phrases such as "is reasonable" and "can be useful/effective/beneficial". Class IIb notes "Procedure/Treatment MAY BE CONSIDERED", with phrasing including "may/might be considered" and that "usefulness/efficacy is unknown/unclear/uncertain or not well established". Class III includes "COR III: No Benefit" with phrases such as "is not recommended" and "should not be performed/ administered", and "COR III: Harm" with phrases including "potentially harmful" and "causes harm". Evidence levels are defined as Level A "Sufficient evidence from multiple randomized trials or meta-analyses", Level B "Evidence from single randomized trial or nonrandomized studies", and Level C "Only expert opinion, case studies, or standard of care", with the note that "A recommendation with Level of Evidence B or C does not imply that the recommendation is weak".

---

### National consensus project clinical practice guidelines for quality palliative care guidelines, 4th edition [^114nxMFn]. Journal of Palliative Medicine (2018). Medium credibility.

Domain 1: Structure and Processes of Care — continuous quality improvement criteria state that the program measures and improves quality by systematically collecting and analyzing data on care processes and outcomes, setting improvement targets, and planning and implementing change, with an iterative, ongoing cycle until sustained improvement. The IDT considers the six domains of health care quality as defined in 2001 by the Institute of Medicine (safe, effective, patient-centered, timely, efficient and equitable), identifies care coordination measures for integration into CQI initiatives, uses validated and clinically relevant assessment instruments, quality measures, and experience of care surveys across settings or populations, ensures patients, families, clinicians, and other partners participate in evaluation of the IDT, and participates in quality reporting and accountability programs as required or necessary to maintain licensure or accreditation.

---

### Competition among networks highlights the power of the weak [^113krmHz]. Nature Communications (2016). Medium credibility.

Methods

Measuring the importance of nodes and networks

We use the eigenvector centrality x k for quantifying the importance of a node k in a network, which can be obtained as an iterative process that sums the centralities of all neighbours of k:

where λ is a constant, x k is the eigenvector centrality of node k and G kj are the components of the adjacency matrix, which could be both binary or weighted. In matrix notation equation (5) reads λ x = Gx so that x can be expressed as a linear combination of the eigenvectors u k of the adjacency matrix G, being λ k the set of the corresponding eigenvalues. Equation (5) can be regarded as an iterative process that begins at t = 0 with a set of initial conditions x 0. Regardless of the values of x 0, the value of x (t) at t →∞ will be proportional to the eigenvector u 1 associated with the largest eigenvalue λ 1. Therefore, the eigenvector centrality is directly obtained from the eigenvector u 1 of the adjacency matrix G, which also holds for weighted adjacency matrices. As explained in the main text, the centrality accumulated by each network is obtained as the fraction of centrality accumulated by their nodes. Finally, we use λ 1 as the measure of the network strength, as it is related to a series of dynamical properties of networks and, in turn, it increases with the number of nodes, links and the average degree of the network.

---

### Pegvaliase-pqpz (Palynziq) [^112TEi8p]. FDA (2025). Medium credibility.

You may see a drop of liquid on the tip of the needle. This is normal. Do not wipe it away. Throw the needle cap away in a puncture resistant or sharps disposal container.

Figure N

Step 13: Hold the body of the prefilled syringe in 1 hand between your thumb and index finger. Use your other hand to pinch up the skin around the injection site. Hold the skin firmly (See Figure O).

Do not touch the plunger head while inserting the needle into the skin.

Figure O

Step 14: Use a quick motion to fully insert the needle into the pinched skin at a 45 to 90 degree angle (See Figure P).

Release the pinch of skin. Use that hand to hold the bottom of the prefilled syringe steady. Place the thumb of your other hand on the plunger head and place your index and middle fingers under the finger grips (See Figure P).

Figure P

Step 15: Use your thumb to push in the plunger slowly and steadily as far as it will go to inject all the medicine (See Figure Q). More pressure on the plunger may be needed to inject all the medicine for the 10 mg and 20 mg strength prefilled syringes.

---

### Indication of measures of uncertainty for statistical significance in abstracts of published oncology trials: a systematic review and meta-analysis [^116N95ZR]. JAMA Network Open (2019). High credibility.

Importance

There is growing consensus that reliance on P values, particularly a cutoff level of .05 for statistical significance, is a factor in the challenges in scientific reproducibility. Despite this consensus, publications describing clinical trial results with P values near .05 anecdotally use declarative statements that do not express uncertainty.

Objectives

To quantify uncertainty expression in abstracts describing the results of cancer randomized clinical trials (RCTs) with P values between .01 and .10 and examine whether trial features are associated with uncertainty expression.

Data Sources

A total of 5777 prospective trials indexed on HemOnc.org, as of September 15, 2019.

Study Selection

Two-arm RCTs with a superiority end point with P values between .01 and .10.

Data Extraction and Synthesis

Abstracts were evaluated based on an uncertainty expression algorithm. Ordinal logistic regression modeling with multiple imputation was performed to identify whether characteristics of study design, results, trial authors, and context P values were normalized by dividing by prespecified α value.

Main Outcomes and Measures

Uncertainty expression in abstracts as determined by the algorithm and its association with trial and publication characteristics.

Results

Of 5777 trials screened, 556 met analysis criteria. Of these, 222 trials (39.9%) did not express uncertainty, 161 trials (29.0%) expressed some uncertainty, and 173 trials (31.1%) expressed full uncertainty. In ordinal logistic regression with multiple imputation, trial features with statistically significant associations with uncertainty expression included later year of publication (odds ratio [OR], 1.70; 95% CI, 1.24–2.32; P < .001), normalized P value (OR, 1.36; 95% CI, 1.11–1.67; P = 0.003), noncooperative group studies (OR, 1.72; 95% CI, 1.12–2.63; P = 0.01), and reporting an end point other than overall survival (OR, 1.41; 95% CI, 1.01–1.96; P = 0.047). Funding source, number of authors, journal impact tier, author nationality, study of unapproved drugs, abstract word count, whether the marginal end point was a primary or coprimary end point, and effect size (in subgroup analysis) did not have statistically significant associations with uncertainty expression.

Conclusions and Relevance

Published oncology articles with marginally significant results may often incompletely convey uncertainty. Although it appears that more uncertainty is expressed in recent abstracts, full uncertainty expression remains uncommon, and seemingly is less common when reporting overall survival, results with P values lower than α levels, and cooperative group studies.

---

### Methodologies for the development of CHEST guidelines and expert panel reports [^117WqPgE]. Chest (2014). Medium credibility.

CHEST guideline grading and evidence thresholds — use of Grading of Recommendations Assessment, Development, and Evaluation (GRADE) is described alongside interpretation of recommendation strength: The CHEST grading system was previously a modification of GRADE, and "In February 2014, the GOC made the decision to start using unmodified GRADE and discontinue use of the CHEST-modified version of GRADE". CHEST specifies evidence minimums: "CHEST has set a minimal threshold for evidence such that it must be published in peer-reviewed publications". Where evidence is sparse, "CHEST would label it as insufficient evidence and follow a consensus-based process or hybrid approach", whereas GRADE "would label it as very low evidence but permit the formulation of guideline recommendations". Methodologic changes have reduced top-tier ratings: "there are fewer IA-graded recommendations". Strength labeling is explicit: "Recommendation strength is reflected in the numerical grades (1 or 2)". Importantly, "Strong recommendations with low evidence (eg, 1C) are still highly recommended for the designated patient populations".

---

### Understanding statistical terms: 1 [^11352qqr]. Drug and Therapeutics Bulletin (2009). Low credibility.

An increasing number of statistical terms appear in journal articles and other medical information. A working knowledge of these is essential in assessing clinical evidence. With this in mind, we are producing a series of explanatory articles covering various statistical terms and their uses. This, the first article in the series, will focus on some of the most common terms used in randomised controlled trials.

---

### How to write a protocol: part 1 [^116mvUnH]. Journal of Nuclear Medicine Technology (2015). Low credibility.

Clinical imaging protocols play an important role in the provision of high-quality care in nuclear medicine. It is imperative that all nuclear medicine facilities have protocols for every procedure performed. However, creating protocols that are detailed, unambiguous, and consistent is often easier said than done. Properly written protocols help to ensure that nuclear medicine procedures are performed in a standardized, reproducible manner so that patients receive high-quality care. This 2-part article provides technologists with a framework for composing comprehensive protocols. Part 1 discusses the secrets to successfully composing protocols ensuring they are detailed and step-by-step along with the importance of basing protocols on evidence from published guidelines and peer-reviewed literature. The components and important aspects of clinical imaging protocols are detailed.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^112T2tTd]. Journal of the American College of Cardiology (2025). High credibility.

Selecting therapies for economic value statements in AHA/ACC guideline development is framed with explicit strength language: the authors state, "we suggest that clinical guidelines include economic value statements" for all interventions with adequate-quality economic evaluations; whether a value statement is appropriate "should rely on the judgment of clinical and economic experts". To the extent feasible, committees "should consider including economic value statements" for interventions with Class 1 and 2A recommendations and for interventions where understanding economic value is likely to influence adoption, with examples including high-cost therapies or those eligible to a large number of individuals. When high-quality evaluations are scarce, value statements "may highlight evidence gaps", and the first step in developing them is a literature review and consultation with subject matter experts to identify all relevant economic evaluations.

---

### Methodologies for the development of CHEST guidelines and expert panel reports [^1134vQLz]. Chest (2014). Medium credibility.

CHEST strength of recommendations — strong recommendations are graded 1A, 1B, and 1C with specified evidence and implications. Strong recommendation, high-quality evidence (1A): Benefits clearly outweigh risk and burden, or vice versa; Consistent evidence from randomized controlled trials without important limitations or exceptionally strong evidence from observational studies; Recommendation can apply to most patients in most circumstances. Further research is very unlikely to change our confidence in the estimate of effect. Strong recommendation, moderate-quality evidence (1B): Benefits clearly outweigh risk and burden, or vice versa; Evidence from randomized controlled trials with important limitations (inconsistent results, methodologic flaws, indirect or imprecise), or very strong evidence from observational studies; Recommendation can apply to most patients in most circumstances. Higher quality research may well have an important impact on our confidence in the estimate of effect and may change the estimate. Strong recommendation, low or very low-quality evidence (1C): Benefits clearly outweigh risk and burden, or vice versa; Evidence for at least one critical outcome from observational studies, case series, or from randomized, controlled trials with serious flaws or indirect evidence; Recommendation can apply to most patients in many circumstances. Higher quality research is likely to have an important impact on our confidence in the estimate of effect and may well change the estimate.

---

### 2019 ASCCP risk-based management consensus guidelines: methods for risk estimation, recommended management, and validation [^11622FFj]. Journal of Lower Genital Tract Disease (2020). High credibility.

Risk-based management uncertainty — confidence interval and decision confidence score: Confidence intervals for each risk estimate were calculated using a normal approximation or exact methods based on the binomial distribution, and a "decision confidence score" is defined that combines the uncertainty in the statistical precision and how close the risk estimates fall to the clinical action thresholds; mathematical details on estimating the recommendation confidence scores are presented in Appendix Part D (http://links.lww.com/LGT/A160).

---

### Defining the time-limited trial for patients with critical illness: an official American Thoracic Society workshop report [^116J4ouw]. Annals of the American Thoracic Society (2024). High credibility.

Time-limited trial in critical care — essential elements and phased process were derived through a Delphi process: investigators "identified 18 potential steps" and found first-round consensus that "11 of these were essential elements"; during the second round, they "identified seven additional essential elements". The committee then "combined 4 related items into two steps (to reach 16 essential elements)" and "organized steps into four phases of care: consider, plan, support, and reassess". Additional steps "may be helpful in some cases, but not necessary for all trials", and involvement of other disciplines is "highly dependent on a patient's specific situation and on the available hospital resources".

---

### Power of data in quantum machine learning [^115kMCsY]. Nature Communications (2021). High credibility.

A given kernel function corresponds to a nonlinear feature mapping ϕ (x) that maps x to a possibly infinite-dimensional feature space, such that. This is the basis of the so-called "kernel trick" where intricate and powerful maps ϕ (x i) can be implemented through the evaluation of relatively simple kernel functions k. As a simple case, in the example above, using a kernel of k (x i, x j) = ∣〈 x i ∣ x j 〉∣ 2 corresponds to a feature mapwhich is capable of learning quadratic functions in the amplitudes. In kernel based ML algorithms, the trained model can always be written as h (x) = w † ϕ (x) where w is a vector in the feature space defined by the kernel. For example, training a convolutional neural network with large hidden layers, is equivalent to using a corresponding neural tangent kernel k CNN. The feature map ϕ CNN for the kernel k CNN is a nonlinear mapping that extracts all local properties of x. In quantum mechanics, similarly a kernel function can be defined using the native geometry of the quantum state space. For example, we can define the kernel function as 〈 x i ∣ x j 〉 or ∣〈 x i ∣ x j 〉∣ 2. Using the output from this kernel in a method like a classical support vector machinedefines the quantum kernel method.

A wide class of functions can be learned with a sufficiently large amount of data by using the right kernel function k. For example, in contrast to the perhaps more natural kernel, 〈 x i ∣ x j 〉, the quantum kernel k Q (x i, x j) = ∣〈 x i ∣ x j 〉∣ 2 = Tr(ρ (x i) ρ (x j)) can learn arbitrarily deep quantum neural network U QNN that measures any observable O (shown in Supplementary Section 3), and the Gaussian kernel, with hyperparameter γ, can learn any continuous function in a compact space, which includes learning any QNN. Nevertheless, the required amount of data N to achieve a small prediction error could be very large in the worst case. Although we will work with other kernels defined through a quantum space, due both to this expressive property and terminology of past work, we will refer toas the quantum kernel method throughout this work, which is also the definition given in.

---

### The most important tasks for peer reviewers evaluating a randomized controlled trial are not congruent with the tasks most often requested by journal editors [^116GTTbL]. BMC Medicine (2015). Low credibility.

All tasks were secondarily combined to create principal task combinations (Fig. 1). In fact, some of the tasks collected were defined precisely and were related to the same domain. For example, evaluating the risk of bias could imply several tasks: evaluating the quality of the randomization procedure, blinding of patients, care providers, outcome assessors, and rating and handling of missing data. Similarly, assessing the quality of randomization procedure can be divided into different tasks such as evaluating the allocation sequence generation and allocation concealment, which imply checking who generated the random allocation sequence, who enrolled participants, and who assigned participants to interventions and evaluating the mechanism used to implement the random allocation sequence (such as sequentially numbered containers), assessing any steps taken to conceal the sequence until interventions were assigned. We did not want to go into such details for defining a task, so we condensed all the tasks related to the assessment of the risk of bias into a single task. We proceeded similarly for all tasks that needed some condensation.

Fig. 1
Flow chart of the survey of tasks expected of peer reviewers

Two researchers (AC and CB) independently combined the tasks, and consensus was reached by discussion in case of disagreement. If no consensus was reached, a third researcher helped resolve any disagreements (IB).

Survey of peer reviewers to sort the statements related to tasks by importance

To sort the tasks expected from peer reviewers, we used Q-sort, which asks participants to sort combined tasks in relation to each other according to a predetermined distribution.

---

### Arithmetic value representation for hierarchical behavior composition [^1132LeAP]. Nature Neuroscience (2023). High credibility.

Fig. 3
Transfer of Q Subtask representation to the composite task.

a, Left, schematic of the composite ANN architecture and the Q Composite function in action and state spaces in deep RL agents in the early learning phase of the composite task. Output scalar values (Q Task1 and Q Task2) are averaged to derive a scalar value (Q Composite (Q Comp.)). Right, same as left for mice with schematic of the imaging experiment. It is the same visualization as Fig. 2a, d. b, Q Subtask representations in the early phase of the composite task in deep RL agents (top) and mice (bottom). It is the same visualization as Fig. 2b. c, Learning curve in the composite task with ablation of Q Task1 - and Q Task2 -representing neurons in deep RL agents. Full ablation: 11.3 ± 0.3% (mean ± s.e.m.) for movement and 14.0 ± 0.3% for lick of all neurons ablated, ✱✱ p < 0.001 compared with control ablation; 50% ablation: 5.8 ± 0.1% for movement and 7.1 ± 0.2% for lick of all neurons ablated, ✱✱ p = 0.006 compared with control ablation; control ablation: same number as the full ablation ablated, P = 0.83 compared with no ablation, n = 6 agents, one-tailed bootstrap with Bonferroni's correction (mean ± s.e.m.). d, Persistent cortical distribution of Q Subtask -related movement (left) and lick (right) representations across the subtasks and composite (comp.) task in mice (Task 1: R 2 = 0.98, ✱✱ p < 0.001; Task 2: R 2 = 0.76, ✱✱ p < 0.001, one-tailed bootstrap for positive correlations, fraction ± s.d. > 1,000 samples with replacement). Direct comparisons in the absolute fractions of Q Subtask -related neurons across tasks are complicated due to different task predictors used in GLMs. e, Left, schematic of the Q manifold of population neural activity representing a Q function. Middle, Q manifold in an example of a deep RL agent embedded in the same principal component space. The dots correspond to activity at randomly sampled timepoints. Right, manifold overlaps. The dots indicate the mean KL divergence. The edges of the whiskers are maximum and minimum and the edges of the boxes are 75% and 25% of 1,000 shuffled mean KL divergence (P < 0.001 for all comparisons, n = 7 mice and n = 6 agents, one-tailed permutation).

---

### Validating whole slide imaging for diagnostic purposes in pathology: guideline from the college of American pathologists pathology and laboratory quality center [^116fpUAU]. Archives of Pathology & Laboratory Medicine (2013). Medium credibility.

Supplemental Table 12 — Evidence to Decision Summary of Recommendation 3 reports response distributions as follows: For "Is the problem a priority?", Yes was 70% (7/10) with 20% (2/10) Probably No, 10% (1/10) Probably Yes, and 0 for No, Varies, and Don't Know. For "How accurate is the test?", Very Inaccurate 0, Inaccurate 10% (1/10), Accurate 30% (3/10), Very Accurate 30% (3/10), Varies 20% (2/10), and Don't Know 10% (1/10). For "How substantial are the desirable anticipated effects?", Trivial 0, Small 0, Moderate 20% (2/10), Large 40% (4/10), Varies 20% (2/10), and Don't Know 20% (2/10). For "What is the overall certainty of the evidence of test accuracy?", Very Low 10% (1/10), Low 10% (1/10), Moderate 20% (2/10), High 40% (4/10), Very High 20% (2/10), and No Included Studies 0. For "What is the overall certainty of the evidence of effects of the management that is guided by the test results?", Very Low 10% (1/10), Low 20% (2/10), Moderate 30% (3/10), High 30% (3/10), Very High 10% (1/10), and No Included Studies 0. For "How certain is the link between test results and management decisions?", Very Low 10% (1/10), Low 20% (2/10), Moderate 10% (1/10), High 40% (4/10), Very High 10% (1/10), and No Included Studies 10% (1/10). For "What is the overall certainty of the evidence of effects of the test?", Very Low 10% (1/10), Low 0, Moderate 40% (4/10), High 40% (4/10), Very High 10% (1/10), and No Included Studies 0. For "What is the overall certainty of the evidence for any critical or important direct benefits, adverse effects or burden of the test?", Very Low 10% (1/10), Low 20% (2/10), Moderate 30% (3/10), High 40% (4/10), Very High 0, and No Included Studies 0.