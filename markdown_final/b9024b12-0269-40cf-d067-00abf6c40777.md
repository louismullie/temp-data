# Medical Question & Answer

**Sample ID**: b9024b12-0269-40cf-d067-00abf6c40777
**Dataset Index**: 182979

---

## Question

Example 2. Find lim x‚ÜíœÄ/3 [(1‚àí2 cos x)/(‚àö3‚àí2 sin x)] ¬∑ (0/0 indeterminate form)

---

## Answer

> Let's see‚Ä¶ What do we have here? The user is asking me to find the limit of (1 ‚àí 2 cos x)/(‚àö3 ‚àí 2 sin x) as x approaches œÄ/3, which is a 0/0 indeterminate form. Let's break this down step-by-step. First, I need to think about confirming the indeterminate form at x = œÄ/3. Then, I should verify whether L'H√¥pital's Rule applies and, if so, differentiate the numerator and denominator. Next, I will evaluate the resulting expression at x = œÄ/3. Finally, I should double-check the arithmetic and consider whether an alternative approach like Taylor expansion would yield the same result to ensure consistency.

> Let me first confirm the indeterminate form by substituting x = œÄ/3 into the numerator and denominator. The numerator becomes 1 ‚àí 2 cos(œÄ/3) = 1 ‚àí 2(1/2) = 0, and the denominator becomes ‚àö3 ‚àí 2 sin(œÄ/3) = ‚àö3 ‚àí 2(‚àö3/2) = ‚àö3 ‚àí ‚àö3 = 0, so we indeed have a 0/0 indeterminate form, which justifies using L'H√¥pital's Rule [^notfound].

> Wait, let me verify the conditions for L'H√¥pital's Rule before proceeding. The functions in the numerator and denominator are differentiable near x = œÄ/3, and the limit of their derivatives exists or is ¬± ‚àû, so L'H√¥pital's Rule is applicable here; I should proceed by differentiating the numerator and denominator separately and then taking the limit of the ratio of derivatives [^notfound].

> I will now differentiate the numerator and denominator. The derivative of the numerator, d/dx(1 ‚àí 2 cos x), is 2 sin x, and the derivative of the denominator, d/dx(‚àö3 ‚àí 2 sin x), is ‚àí2 cos x; so the limit becomes lim x‚ÜíœÄ/3 [2 sin x/‚àí2 cos x] = lim x‚ÜíœÄ/3 (‚àítan x) [^notfound].

> Hold on, I should verify the evaluation at x = œÄ/3. tan(œÄ/3) = ‚àö3, so ‚àítan(œÄ/3) = ‚àí‚àö3; therefore, the limit is ‚àí‚àö3. Let me double-check the sign and the exact value to avoid a careless mistake, and yes, that checks out [^notfound].

> But wait, what if I tried a Taylor expansion approach instead of L'H√¥pital's Rule to confirm consistency. Near x = œÄ/3, let h = x ‚àí œÄ/3; then cos x ‚âà 1/2 ‚àí (‚àö3/2)h and sin x ‚âà ‚àö3/2 + (1/2)h, so the numerator ‚âà 1 ‚àí 2(1/2 ‚àí ‚àö3/2 h) = ‚àö3 h and the denominator ‚âà ‚àö3 ‚àí 2(‚àö3/2 + 1/2 h) = ‚àíh; thus the ratio ‚âà [‚àö3 h/‚àíh] = ‚àí‚àö3, which matches the L'H√¥pital result and reassures me the answer is correct [^notfound].

---

The limit is **‚àí‚àö3**. This is a 0/0 indeterminate form, so we apply L'H√¥pital's Rule: differentiate the numerator and denominator, then evaluate at x = œÄ/3. The derivative of the numerator is 2 sin x, and the derivative of the denominator is ‚àí2 cos x. Evaluating these at x = œÄ/3 gives [2 sin(œÄ/3)/‚àí2 cos(œÄ/3)] = [2 ¬∑ ‚àö3/2/‚àí2 ¬∑ 1/2] = [‚àö3/‚àí1] = **‚àí‚àö3**.

---

## Step-by-step solution

### Step 1: Verify the indeterminate form

Substitute x = œÄ/3 into the expression:

- **Numerator**: 1 ‚àí 2 cos(œÄ/3) = 1 ‚àí 2(1/2) = 0
- **Denominator**: ‚àö3 ‚àí 2 sin(œÄ/3) = ‚àö3 ‚àí 2(‚àö3/2) = 0

Since both numerator and denominator are 0, the limit is of the **indeterminate form 0/0** [^notfound].

---

### Step 2: Apply L'H√¥pital's Rule

Differentiate the numerator and denominator:

- **Numerator derivative**: d/dx (1 ‚àí 2 cos x) = 2 sin x
- **Denominator derivative**: d/dx (‚àö3 ‚àí 2 sin x) = ‚àí2 cos x

Now the limit becomes:

lim x‚ÜíœÄ/3 [2 sin x/‚àí2 cos x] = lim x‚ÜíœÄ/3 (‚àítan x)

---

### Step 3: Evaluate the simplified limit

Substitute x = œÄ/3 into the simplified expression:

‚àítan(œÄ/3) = ‚àí‚àö3

---

## Final answer

lim x‚ÜíœÄ/3 [(1‚àí2 cos x)/(‚àö3 ‚àí 2 sin x)] = ‚àí‚àö3

---

## References

### Prospective motion correction improves the sensitivity of fMRI pattern decoding [^05d1c169]. Human Brain Mapping (2018). Low credibility.

2.12 Simulations for comparison of MVPA methods

To allow for a more informed comparison between the two MVPA methods' sensitivity to PMC‚Äêlike effects, a set of computational simulations was also conducted and included in Supporting Information, Figure S4. This simulation was designed to closely resemble real dataset's design (alternating block design, 16 s per block, 8 blocks per subrun, 4 subruns per participant, 15 participants in each iteration), and analysis approach.

We first generated a design matrix with two regressors (one regressor per stimulus orientation). For each of 100 iterations, we multiplied this design matrix by a contrast vector to simulate the activation timecourse of a voxel. The activation response to each stimulus orientation was obtained by sampling from independent Gaussian distributions of with mean 0 and standard deviation 1. This generates a contrast vector centered around 0, with an expected mean absolute difference between conditions of 2/‚àöœÄ. This expected mean is calculated by integrating abs(x 1 ‚àí x 2)P(x 1)P(x 2) across all possible values of x 1 and x 2. This was normalized to give a mean absolute contrast of 1 and repeated 500 times (average number of voxels at 3 mm) to create a simulated, noiseless fMRI dataset for 1 participant. We repeated this procedure separately for each of 15 simulated participants.

---

### Sandwiching intermetallic PtFe and ionomer with porous N-doped carbon layers for oxygen reduction reaction [^37032085]. Nature Communications (2025). High credibility.

O 2 transport resistance measurements

The investigation of O 2 transport resistance was performed by limiting current measurements at 353.15 K and 100% RH, using an H 2 flow rate of 1 L min -1 and an O 2 flow rate of 2 L min -1. The experimental setup included a dry oxygen mole fraction of 5% O 2 in N 2, with the cell potential controlled at 0.3, 0.15, and 0.1 V for a duration of 2 min, and the current density calculated based on the average of the last 30 s. To distinguish between pressure-independent and pressure-dependent oxygen transport resistance, the assessments were performed at pressures of 150, 175, 200, 225 KPa abs.

Here, the total O 2 transport resistance (R O2 total) can be calculated from the following Eqs. 2, 3 using i lim as the limiting current, P in as the total pressure, P H2O in as the partial pressure of H 2 O, and X dry, O2 as the dry oxygen mole fraction.

Notably, R O2 total can be divided into pressure dependent (R O2 PD) and pressure independent (R O2 PI) components. The former represents the resistance to intermolecular gas diffusion, while the latter represents the resistance to Knudsen diffusion and oxygen transport through the ionomer/liquid water layer. The pressure-independent resistance to O 2 transport (R O2 PI) could be determined by analyzing the Y-intercept of the linear relationship between R O2 total and the total pressure.

DFT calculation

All the calculations were assessed using the DFT with the projector augmented plane-wave method, which was implemented in the Vienna ab initio simulation package. Perdew-Burke-Ernzerhof (PBE) generalized gradient approximation was used to determine the exchange-correlation potential. A cut-off energy of 450 eV was set for the plane wave. The Kohn-Sham equation was solved iteratively with an energy criterion of 10‚Äì5 eV. The structures were relaxed until the residual forces on the atoms decreased to < 0.02 eV/√Ö. To prevent interlaminar interactions, a vacuum spacing of 20 √Ö was applied perpendicular to the slab. The formation energy (E form) was calculated using Eq. 4, which takes into account the total energy of the defect (E defect), the total energy of the perfect model (E perfect), and the chemical potential of the Fe atom (E Fe).

---

### How to evaluate agreement between quantitative measurements [^7316afc8]. Radiotherapy and Oncology (2019). Medium credibility.

When a method comparison study is performed, the aim is to evaluate the agreement of measurements of different methods. We present the Bland-Altman plot with limits of agreement as the correct analysis methodology. We also discuss other scaled and unscaled indices of agreement and commonly used inappropriate approaches.

---

### Structure and inference in annotated networks [^535f823a]. Nature Communications (2016). Medium credibility.

Computationally, the most demanding part of the EM algorithm is calculating the sum in the denominator of equation (7), which has an exponentially large number of terms, making its direct evaluation intractable on all but the smallest of networks. Traditionally one gets around this problem by approximating the full distribution q (s) by Monte Carlo importance sampling. In our calculations, however, we instead use a recently proposed alternative method based on belief propagation, which is significantly faster, and fast enough in practice for applications to very large networks.

Final likelihood value

The EM algorithm always converges to a maximum of the likelihood but is not guaranteed to converge to the global maximum ‚Äî it is possible for there to be one or more local maxima as well. To get around this problem we normally run the algorithm repeatedly with different random initial guesses for the parameters and from the results choose the one that finds the highest likelihood value. In the calculations presented in this paper we did at least 10 such 'random restarts' for each network. To determine which run has the highest final value of the likelihood we calculate the log-likelihood from the right-hand side of (6) using P (A | Œò, s) and P (s | Œì, x) as in equation (2), the final fitted values of the parameters Œò and Œì from the EM algorithm, and q (s) as in equation (7). (As we have said, the right-hand side of (6) becomes equal to the left, and hence equal to the true log-likelihood, when q (s) is given the value in equation (7).)

---

### Resolving discrepancies between chimeric and multiplicative measures of higher-order epistasis [^7747af71]. Nature Communications (2025). High credibility.

Equation (21) demonstrates that X = (X 1, X 2) follows an exponential family distribution, a wide class of distributions that includes many common distributions including normal distributions or Poisson distributions. In particular, using the terminology of exponential families, equation (21) shows that the sufficient statistics of X are X 1, X 2, and X 1 X 2, with corresponding canonical parameters Œ≤ 1, Œ≤ 2, and Œ≤ 12. As a result, the distribution P (X) is uniquely defined by the expected values E [X 1], E [X 2], E [X 1 X 2] of the sufficient statistics, sometimes called the moments or the mean parameters of the distribution. Thus, we obtain a third parametrization of the distribution P (X) using the moments Œº 0 = 1, Œº 1 = E [X 1], Œº 2 = E [X 2], Œº 12 = E [X 1 X 2]. The elements of the vector Œº = (1, Œº 1, Œº 2, Œº 12) of moments are sometimes called the mean parameters of the distribution.

---

### Do CIs give you confidence? [^0c65ec35]. Chest (2012). Low credibility.

This article describes the conceptual basis for the P value and the CI. We show that both are derived from the same underlying concepts and provide useful, but similar information.

---

### Evaluation of the evenness score in next-generation sequencing [^1f5f1936]. Journal of Human Genetics (2016). Low credibility.

The evenness score (E) in next-generation sequencing (NGS) quantifies the homogeneity in coverage of the NGS targets. Here I clarify the mathematical description of E, which is 1 minus the integral from 0 to 1 over the cumulative distribution function F(x) of the normalized coverage x, where normalization means division by the mean, and derive a computationally more efficient formula; that is, 1 minus the integral from 0 to 1 over the probability density distribution f(x) times 1-x. An analogous formula for empirical coverage data is provided as well as fast R command line scripts. This new formula allows for a general comparison of E with the coefficient of variation (= standard deviation œÉ of normalized data) which is the conventional measure of the relative width of a distribution. For symmetrical distributions, including the Gaussian, E can be predicted closely as 1-œÉ(2)/2‚©æE‚©æ1-œÉ/2 with œÉ ‚â§ 1 owing to normalization and symmetry. In case of the log-normal distribution as a typical representative of positively skewed biological data, the analysis yields E‚âàexp(-œÉ*/2) with œÉ*(2) = ln(œÉ(2)+1) up to large œÉ (‚â§ 3), and E‚âà1-F(exp(-1)) for very large œÉ (‚©æ2.5). In the latter kind of rather uneven coverage, E can provide direct information on the fraction of well-covered targets that is not immediately delivered by the normalized œÉ. Otherwise, E does not appear to have major advantages over œÉ or over a simple score exp(-œÉ) based on it. Actually, exp(-œÉ) exploits a much larger part of its range for the evaluation of realistic NGS outputs.

---

### Inferring time derivatives including cell growth rates using gaussian processes [^d99eaf3f]. Nature Communications (2016). Medium credibility.

Inferring the first and second time derivatives

To determine the time derivative of the data, we use that the derivative of a Gaussian process is another Gaussian process. We can therefore adapt standard techniques for Gaussian process to allow time derivatives to be sampled too.

Building on the work of Boyle, we let g (x) and h (x) be the first and second derivatives with respect to x of the latent function f (x). If f (x) is a Gaussian process then so are both g (x) and h (x). Writing ‚àÇ 1 and ‚àÇ 2 for the partial derivatives with respect to the first and second arguments of a bivariate function, we have

and that

as well as

following ref.

Consequently, the joint probability distribution for y and f *, g * and h * evaluated at points X * is again Gaussian (cf. equation (7)):

where we write K = K (X, X) and K ‚ú± = K (X *, X *) for clarity.

The covariance function is by definition symmetric: k (x i, x j) = k (x j, x i) from equation (1). Therefore, and so

for all positive integers k and. Consequently, the covariance matrix in equation (13) is also symmetric.

Conditioning on y now gives that the distribution P (f *, g *, h *| X, y, Œ∏, X *) is Gaussian with mean

and covariance matrix

Equation (16) includes equation (9) and shows that

which gives the error in the estimate of the first derivative. Similarly,

is the error in estimating the second derivative.

Using an empirically estimated measurement noise

Although our derivation is given for a Gaussian process where the measurement errors in the data are independent and identically distributed with a Gaussian distribution of mean zero, the derivations are unchanged if the measurement noise has a different s.d. for each time point.

When the magnitude of the measurement noise appears to change with time, we first empirically estimate the relative magnitude of the measurement noise by the variance across all replicates at each time point. We then smooth this estimate over time (with a Gaussian filter with a width of 10% of the total time of the experiment, but the exact choice is not important) and replace the identity matrix, I, in equations (6), (15) and (16) by a diagonal matrix with the relative measurement noise on the diagonal in order to make predictions.

---

### Learning shapes cortical dynamics to enhance integration of relevant sensory input [^5f42e6ef]. Neuron (2023). Medium credibility.

Non-normal dynamics (Figure S1)

We derived expressions relating linear Fisher Information to the dynamics of an arbitrary normal or non-normal network (subject to the same approximations described above). These expressions had a simple and interpretable form in three special cases: two-dimensional networks, normal networks, and non-normal networks with strong functionally-feedforward dynamics. Related findings have been presented previously.

To illustrate our analytical findings for the two-dimensional case, we constructed networks with modes m 1 = [cos Œ∏ 1; sin Œ∏ 1], m 2 = [cos Œ∏ 2; sin Œ∏ 2]. Figure S1A was constructed using the same procedure as for Figure 2, but this time with œÑ 1 = 10, œÑ 2 = 5. For Figure S1B we chose input with isotropic covariance Œ£ Œ∑ = I 2 (where I N is the N x N identity matrix) and Œî g = g (s 2) - g (s 1) = [1; 0]. These inputs were chosen in order to demonstrate the influence of non-normality as clearly as possible. We set œÑ 1 = 10, œÑ 2 = 1,5,7.5,9 and varied Œ∏ 1, Œ∏ 2 from ‚Äì œÄ /2 to œÄ /2 for each value. For each network (defined by the parameters Œ∏ 1, Œ∏ 2, œÑ 1, œÑ 2 using the procedure described for Figure 2), the Fisher Information of the stationary state network responsewas computed by substituting the long-run solution for the mean Œî r = ‚Äì A ‚Äì1 Œî g and the numerical solution to the Lyapunov equation for Œ£ (described above). We normalized this linear Fisher Information by the maximum achievable SNR in any normal network with the same time constants by defining. For each network, we computed the information-limiting correlations as œÅ ILC = Œî r T Œ£Œî r /(Œî r T Œî r Trace(Œ£)). For each choice of œÑ 2, we computed the Pearson correlation between the Fisher information and the information-limiting correlations corr(ùìò F, œÅ ILC), where the correlation was computed over a set of networks spanning the range of Œ∏ 1, Œ∏ 2 ‚àà [- œÄ /2, œÄ /2). We computed this correlation for various settings of Œ£ Œ∑ = [v 1, v 2] [Œª 1, 0; 0, Œª 2] [v 1, v 2] T, by varying the angle of its principal eigenvector v 1 from Œî g and the ratio of its two eigenvalues Œª 2 / Œª 1 with Œª 1 = 1 and Œª 2 ‚àà [0, 1].

---

### Estimation in medical imaging without a gold standard [^82dda7fe]. Academic Radiology (2002). Low credibility.

Rationale and Objectives

In medical imaging, physicians often estimate a parameter of interest (eg, cardiac ejection fraction) for a patient to assist in establishing a diagnosis. Many different estimation methods may exist, but rarely can one be considered a gold standard. Therefore, evaluation and comparison of different estimation methods are difficult. The purpose of this study was to examine a method of evaluating different estimation methods without use of a gold standard.

Materials and Methods

This method is equivalent to fitting regression lines without the x axis. To use this method, multiple estimates of the clinical parameter of interest for each patient of a given population were needed. The authors assumed the statistical distribution for the true values of the clinical parameter of interest was a member of a given family of parameterized distributions. Furthermore, they assumed a statistical model relating the clinical parameter to the estimates of its value. Using these assumptions and observed data, they estimated the model parameters and the parameters characterizing the distribution of the clinical parameter.

Results

The authors applied the method to simulated cardiac ejection fraction data with varying numbers of patients, numbers of modalities, and levels of noise. They also tested the method on both linear and nonlinear models and characterized the performance of this method compared to that of conventional regression analysis by using x-axis information. Results indicate that the method follows trends similar to that of conventional regression analysis as patients and noise vary, although conventional regression analysis outperforms the method presented because it uses the gold standard which the authors assume is unavailable.

Conclusion

The method accurately estimates model parameters. These estimates can be used to rank the systems for a given estimation task.

---

### Input-output maps are strongly biased towards simple outputs [^38aa07d9]. Nature Communications (2018). Medium credibility.

On its own, Eq. (2) may not be that useful, as K (x | f, n) can depend in a complex way on the details of the map f and the input space size n. To make progress towards map independent statements, we restrict the class of maps. The most important restriction is to consider only (1) limited complexity maps for whichin the asymptotic limit of large x (Supplementary Note 3). Using standard inequalities for conditional Kolmogorov complexity, such asand, it follows for limited complexity maps that. Thus, importantly, Eq. (2) becomes asymptotically independent of the map f, and only depends on the complexity of the output.

We include three further simple restrictions, namely (2) Redundancy: if N I and N O are the number of inputs and outputs respectively then we require, so that P (x) can in principle vary significantly, (3) Finite size: we imposeto avoid finite size effects, and (4) Nonlinearity: We require the map f to be a nonlinear function, as linear transformations of the inputs cannot show bias towards any outputs (Supplementary Note 4). These four conditions are not so onerous. We expect that many real-world maps will naturally satisfy them.

---

### Relative, local and global dimension in complex networks [^a94b7a43]. Nature Communications (2022). High credibility.

Results

Graph dimension from diffusion dynamics

We start with the Green's function of the diffusion equation in d dimensionswhich, together with an initial condition as a delta function at some position x 0, provides a solution of diffusion equation as p (x, t) = G t (x ‚àí x 0). From hereon, we refer to the time evolution of p (x, t) as the transient response. As already considered in our previous works, these solutions have a maxima in their transient response at any other location x, at timeand amplitudegiven aswhere, without loss of generality, x 0 = 0. Then, the dimension at any point x relative to x 0 can be evaluated to yield the definition of the relative dimensionClearly, on the Euclidean space, the relative dimension is always equal to d, independently of x and x 0. However, if we instead consider a compact subspace, the diffusion dynamics will deviate from those prescribed in Equation (1) due to the presence of boundaries relative to x and x 0.

The key property of Equation (3) that allows us to generalise it to graphs is that the positions x 0 and x are not explicit in the right-hand side but only used as labels to initialise the diffusion dynamics and measure the transient response. Consequently, the relative dimension can be seen as intrinsic as it does not rely on any Euclidean embedding, but only on the existence of a diffusion dynamics on the original space. In particular, on graphs we can use the standard diffusion processfor a time-dependent node vector p (t) with L the normalised graph Laplacian L = K ‚àí1 (K ‚àí A) (corresponding to Euclidean diffusion in the continuous limit), where K is the diagonal matrix of node degrees. Using a delta function at node i with mass m i, p (0) = (0, 0, ‚Ä¶, m i, ‚Ä¶, 0), as our initial condition, the j -th coordinate of the solution of Equation (4) (the so-called transient response of j) is given by the heat kernelBy numerically solving (5), we can measure the timeand amplitudeat which a maximum appears in the transient response peak (time evolution) of node j given a delta function initial condition at node i. In analogy to Equation (3), we can then compute the full N √ó N matrix of relative dimensions with elements

---

### Treatment of the X chromosome in mapping multiple quantitative trait loci [^6621f05b]. G3 (2021). Medium credibility.

Statistical methods to map quantitative trait loci (QTL) often neglect the X chromosome and may focus exclusively on autosomal loci. But the X chromosome often requires special treatment: sex and cross-direction covariates may need to be included to avoid spurious evidence of linkage, and the X chromosome may require a separate significance threshold. In multiple-QTL analyses, including the consideration of epistatic interactions, the X chromosome also requires special care and consideration. We extend a penalized likelihood method for multiple-QTL model selection, to appropriately handle the X chromosome. We examine its performance in simulation and by application to a large eQTL data set. The method has been implemented in the package R/qtl.

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^fe47f6c1]. Nature Communications (2025). High credibility.

Derivation of Œ¥ 2 from the standard beta distribution

For continuous variables constrained between the interval [0, 1], the beta distribution is a flexible model capable of describing a wide array of distribution shapes, including bell-shaped, left-skewed, right-skewed and multi-modal (U-shaped) distributions (Fig. 2 i). The beta probability density function (pdf) is typically defined by two positive shape parameters, p and q both on the open interval (0, ‚àû), with the function indexed by continuous values of x constrained between the closed interval [0, 1]. The pdf for the beta distribution iswhere the two exponential functions, x p ‚àí1 (1‚àí x) q ‚àí1, describe all distribution shapes modelled by the beta distribution (Fig. 2 i), Œì(.) is the gamma function, and Œì(p + q)/[Œì(p)Œì(q)] normalises the beta density function such that the area below the function is equal to 1. Notably, as p and q diverge from one another and their sum remains fixed (constant), the central tendency of the distribution shifts to the left (p < q) or to the right (p > q). To be exact, the mean (Œº) and 1 ‚àí Œº can be found withandwhere Œº is constrained between the open interval (0, 1). The interval constraining Œº is found with limits of the interval of p and q asand. Note Œº = 0.5 when p = q. Meanwhile, as the sum of p and q decreases the dispersion of the distribution increases, irrespective of Œº. Specifically, the dispersion can be described withwhere Œ¥ 2 is also constrained between the open interval (0, 1). The interval constraining Œ¥ 2 is found with limits of the interval of p and q asand. Note that Œ¥ 2 is equivalent to the shape parameter described by Damgaard and Irvine, which is better interpreted as dispersion and equivalent to the inverse of the precision parameter described by Ferrari and Cribari-Neto. It can, thus, be shown that the combination of Œº and Œ¥ 2 are capable of describing all possible values of p and q, and by extension all possible beta distributions, whereand

---

### Input-output maps are strongly biased towards simple outputs [^50fca0dd]. Nature Communications (2018). Medium credibility.

Many systems in nature can be described using discrete input-output maps. Without knowing details about a map, there may seem to be no a priori reason to expect that a randomly chosen input would be more likely to generate one output over another. Here, by extending fundamental results from algorithmic information theory, we show instead that for many real-world maps, the a priori probability P(x) that randomly sampled inputs generate a particular output x decays exponentially with the approximate Kolmogorov complexity [Formula: see text] of that output. These input-output maps are biased towards simplicity. We derive an upper bound P(x)‚â≤[Formula: see text], which is tight for most inputs. The constants a and b, as well as many properties of P(x), can be predicted with minimal knowledge of the map. We explore this strong bias towards simple outputs in systems ranging from the folding of RNA secondary structures to systems of coupled ordinary differential equations to a stochastic financial trading model.

---

### Continuous growth reference from 24th week of gestation to 24 months by gender [^0f68c406]. BMC Pediatrics (2008). Low credibility.

Appendix

Assuming normality, to estimate the variance, skewness and kurtosis of the population based on the cases in the range of Œº ¬± Œ¥œÉ in a sample for certain values of Œ¥ > 0.

Without lost of generality, let Œº = 0. Let, where œÜ (x; œÉ) is the normal probability distribution function with mean 0 and variance œÉ 2. It can be deduced that

where Œ¶ (x) is the standard normal cumulative distribution function. Letand. It can be shown that

Letand. It can be shown that

Let p = 2Œ¶(Œ¥) - 1 be the proportion of observations in the range of Œº ¬± Œ¥œÉ under a normal distribution. Considering a sample of n observations, x 1. x n. Let q 1 be the (1 - p)/2-th quantile and q 2 be the (1 - (1 - p)/2)-th quantile and X r = { x i: q 1 ‚â§ x i ‚â§ q 2 } be the reduced sample of size m. An empirical estimate of, j = 2, 3, 4, is given as

The general sample variance, the skewness and the kurtosis are given as

and

respectively. Let, Skew r and Kurt r be the corresponding statistics calculated based on the reduced sample. Substituting ‚Äì into ‚Äì, we have the statistics for the full dataset evaluated based on the reduced sample as

and

---

### Species interactions can explain Taylor's power law for ecological time series [^c24c43a3]. Nature (2003). Excellent credibility.

One of the few generalities in ecology, Taylor's power law, describes the species-specific relationship between the temporal or spatial variance of populations and their mean abundances. For populations experiencing constant per capita environmental variability, the regression of log variance versus log mean abundance gives a line with a slope of 2. Despite this expectation, most species have slopes of less than 2 (refs 2, 3‚Äì4), indicating that more abundant populations of a species are relatively less variable than expected on the basis of simple statistical grounds. What causes abundant populations to be less variable has received considerable attention, but an explanation for the generality of this pattern is still lacking. Here we suggest a novel explanation for the scaling of temporal variability in population abundances. Using stochastic simulation and analytical models, we demonstrate how negative interactions among species in a community can produce slopes of Taylor's power law of less than 2, like those observed in real data sets. This result provides an example in which the population dynamics of single species can be understood only in the context of interactions within an ecological community.

---

### Quantification of network structural dissimilarities [^3cf663e0]. Nature Communications (2017). Medium credibility.

Identifying and quantifying dissimilarities among graphs is a fundamental and challenging problem of practical importance in many fields of science. Current methods of network comparison are limited to extract only partial information or are computationally very demanding. Here we propose an efficient and precise measure for network comparison, which is based on quantifying differences among distance probability distributions extracted from the networks. Extensive experiments on synthetic and real-world networks show that this measure returns non-zero values only when the graphs are non-isomorphic. Most importantly, the measure proposed here can identify and quantify structural topological differences that have a practical impact on the information flow through the network, such as the presence or absence of critical links that connect or disconnect connected components.

---

### Evolution of reporting P values in the biomedical literature, 1990‚Äì2015 [^4e8c6039]. JAMA (2016). Excellent credibility.

Importance

The use and misuse of P values has generated extensive debates.

Objective

To evaluate in large scale the P values reported in the abstracts and full text of biomedical research articles over the past 25 years and determine how frequently statistical information is presented in ways other than P values.

Design

Automated text-mining analysis was performed to extract data on P values reported in 12,821,790 MEDLINE abstracts and in 843,884 abstracts and full-text articles in PubMed Central (PMC) from 1990 to 2015. Reporting of P values in 151 English-language core clinical journals and specific article types as classified by PubMed also was evaluated. A random sample of 1000 MEDLINE abstracts was manually assessed for reporting of P values and other types of statistical information; of those abstracts reporting empirical data, 100 articles were also assessed in full text.

Main Outcomes and Measures

P values reported.

Results

Text mining identified 4,572,043 P values in 1,608,736 MEDLINE abstracts and 3,438,299 P values in 385,393 PMC full-text articles. Reporting of P values in abstracts increased from 7.3% in 1990 to 15.6% in 2014. In 2014, P values were reported in 33.0% of abstracts from the 151 core clinical journals (n = 29,725 abstracts), 35.7% of meta-analyses (n = 5620), 38.9% of clinical trials (n = 4624), 54.8% of randomized controlled trials (n = 13,544), and 2.4% of reviews (n = 71,529). The distribution of reported P values in abstracts and in full text showed strong clustering at P values of .05 and of .001 or smaller. Over time, the "best" (most statistically significant) reported P values were modestly smaller and the "worst" (least statistically significant) reported P values became modestly less significant. Among the MEDLINE abstracts and PMC full-text articles with P values, 96% reported at least 1 P value of .05 or lower, with the proportion remaining steady over time in PMC full-text articles. In 1000 abstracts that were manually reviewed, 796 were from articles reporting empirical data; P values were reported in 15.7% (125/796 [95% CI, 13.2%-18.4%]) of abstracts, confidence intervals in 2.3% (18/796 [95% CI, 1.3%-3.6%]), Bayes factors in 0% (0/796 [95% CI, 0%-0.5%]), effect sizes in 13.9% (111/796 [95% CI, 11.6%-16.5%]), other information that could lead to estimation of P values in 12.4% (99/796 [95% CI, 10.2%-14.9%]), and qualitative statements about significance in 18.1% (181/1000 [95% CI, 15.8%-20.6%]); only 1.8% (14/796 [95% CI, 1.0%-2.9%]) of abstracts reported at least 1 effect size and at least 1 confidence interval. Among 99 manually extracted full-text articles with data, 55 reported P values, 4 presented confidence intervals for all reported effect sizes, none used Bayesian methods, 1 used false-discovery rates, 3 used sample size/power calculations, and 5 specified the primary outcome.

Conclusions and Relevance

In this analysis of P values reported in MEDLINE abstracts and in PMC articles from 1990‚Äì2015, more MEDLINE abstracts and articles reported P values over time, almost all abstracts and articles with P values reported statistically significant results, and, in a subgroup analysis, few articles included confidence intervals, Bayes factors, or effect sizes. Rather than reporting isolated P values, articles should include effect sizes and uncertainty metrics.

---

### A probabilistic Bayesian approach to recovermap and phase images for quantitative susceptibility mapping [^b146d367]. Magnetic Resonance in Medicine (2022). Medium credibility.

Purpose

Undersampling is used to reduce the scan time for high-resolution three-dimensional magnetic resonance imaging. In order to achieve better image quality and avoid manual parameter tuning, we propose a probabilistic Bayesian approach to recover R‚ÇÇ* map and phase images for quantitative susceptibility mapping (QSM), while allowing automatic parameter estimation from undersampled data.

Theory

Sparse prior on the wavelet coefficients of images is interpreted from a Bayesian perspective as sparsity-promoting distribution. A novel nonlinear approximate message passing (AMP) framework that incorporates a mono-exponential decay model is proposed. The parameters are treated as unknown variables and jointly estimated with image wavelet coefficients.

Methods

Undersampling takes place in the y-z plane of k-space according to the Poisson-disk pattern. Retrospective undersampling is performed to evaluate the performances of different reconstruction approaches, prospective undersampling is performed to demonstrate the feasibility of undersampling in practice.

Results

The proposed AMP with parameter estimation (AMP-PE) approach successfully recovers R‚ÇÇ* maps and phase images for QSM across various undersampling rates. It is more computationally efficient, and performs better than the state-of-the-art l‚ÇÅ -norm regularization (L1) approach in general, except a few cases where the L1 approach performs as well as AMP-PE.

Conclusion

AMP-PE achieves better performance by drawing information from both the sparse prior and the mono-exponential decay model. It does not require parameter tuning, and works with a clinical, prospective undersampling scheme where parameter tuning is often impossible or difficult due to the lack of ground-truth image.

---

### Fractional response analysis reveals logarithmic cytokine responses in cellular populations [^0d1364aa]. Nature Communications (2021). High credibility.

Calculation of typical fractions

The fractions of cells stimulated with dose i that have responses typical to dose j, v ij, can be easily calculated from data regardless of the number of doses and the type of experimental measurements. We have thatCalculation of typical fractions, v ij, with the above formula requires the possibility to examine the condition P (y | x j) > P (y | x k) for any experimentally observed response, y. The distributions P (y | x j) can be reconstructed from data using a variety of probability density estimators. The use of the available estimators, however, might be problematic for multivariate responses. We, therefore, propose a more convenient strategy. We replace the condition P (y | x j) > P (y | x k) with an equivalent condition that is computationally much simpler to evaluate. Precisely, we propose to use the Bayes formulaIf we set the equiprobable prior distribution, i.e. P (x j) = 1 /m, we have that P (y | x j) is proportional to P (x j | y) and the condition P (y | x j) > P (y | x k) is equivalent toThe above strategy allows avoiding estimation of the response distributions, P (y | x j), from data. For continuous and multivariate variable y the estimation of P (x j | y) is generally simpler than estimation of P (y | x j). Precisely, an estimatorof the distribution P (x j | y) can be built using a variety of Bayesian statistical learning methods. For simplicity and efficiency, here we propose to use logistic regression, which is known to work well in a range of applications. In principle, however, other classifiers could also be considered. The logistic regression estimators of P (x j | Y = y) arise from a simplifying assumption that log-ratio of probabilities, P (x j | Y = y) and P (x m | Y = y) is linear. Precisely, The above formulation allows fitting the logistic regression equations to experimental data, i.e. finding values of the parameters, Œ± j and Œ≤ j that best represent the data. The fitted logistic regression model allows assigning cellular responses to typical doses based on conditions given by Eq. 8. Formally, the fractions v ij defined by Eq. 6 are calculated aswhere n i is the number of cells measured for the dose x i, y l i denotes response of the l- th cell, andis equal 1 iffor anyand 0 otherwise.

---

### Computationally unmasking each fatty acyl C = C position in complex lipids by routine LC-MS / MS lipidomics [^95e75bf9]. Nature Communications (2025). High credibility.

Fig. 6
cPLA 2 specificity toward FAs at the sn -2 position depends on the C = C positions.

a Assay conditions are as follows (n = 3 biological replicates each): (blue) control including the cPLA 2 -specific inhibitor pyrrophenone (PYR) treatment, as prior work shows no significant difference from control lacking inhibitor; (orange) cPLA 2 activation via stimulation with KDO 2 Lipid A (KLA) ‚Äî the chemically defined version of the toll-like receptor-4 (TLR-4) activator lipopolysaccharide; (green) PYR pretreatment followed by 24 h KLA stimulation. Displayed values represent the combined changes in abundance (relative to +PYR) of all PI species with the respective FA at the sn ‚àí2 position, disregarding the FA at sn ‚àí1. Error bars represent the standard deviation. Shorthand notations represent the following significance thresholds for the two-sided statistical t -test: **< 0.001, **< 0.01, * < 0.05, ns ‚â• 0.05 (exact p -values from left to right, first +PYR compared to +KLA, then +PYR compared to +PYR +KLA for each analyte: 0.0079, 0.2267; 0.0125, 0.1311; 0.0050, 0.2963; 0.0008, 0.0605). Source data is provided as a Source Data file. b Representative chemical structures of PI x /20:4(n‚àí6), PI x /20:3(n‚àí9), PI x /20:3(n‚àí7), and PI x /20:3(n‚àí6), featuring homo-allylic FAs, which are predominantly observed in higher organisms, and verified by EAD. c Experimental chromatogram of œâ-positional isomers of PI 18:0/20:3, observed with a 30-min gradient. d Scheme of cPLA 2 's observed substrate specificity, including the previously discovered specificity towards AA and the C = C-position dependent specificity toward 20:3 isomers reported here. Increased amounts of downstream elongation and desaturation products further corroborate the finding about C = C position specificity of cPLA 2.

---

### Imputation of missing covariate values in epigenome-wide analysis of DNA methylation data [^204df57e]. Epigenetics (2016). Low credibility.

Figure 5.
Simulation models 1‚Äì3. X: covariate of interest; Y: DNA methylation level; Z: Covariate(s) that contain missing values.

In simulation model 1 and 2, the covariates Z 1, Z 2, and Z 3 were correlated with X, but they were independent of X in model 3. We masked a proportion of samples as missing in these 3 covariates, with equal missing rate at 0.3 or 0.9. In simulation model 1, we simulated methylation levels at 50 CpG sites as follows:where both Œ≤ and Œ≥ are positive (Fig. 5A). In model 2, we simulated methylation levels at 200 CpG sites using the same model above, with Œ≤ > 0 but Œ≥ = 0 (Fig. 5B). In these 2 models, the covariates Z confound the relationship between methylation level Y and variable X. We used the 50 CpG sites from model 1 to evaluate the statistical power of different imputation methods, and the 200 CpG sites from model 2 to evaluate the type 1 error in presence of confounding factors. In model 3, we simulated data at another 150 CpG sites where their methylation levels were correlated with covariates Z and X, but Z was independent of X (Fig. 5C). Under this simulation model, we evaluated the statistical power of imputation methods when the missing covariates are independent risk factors.

---

### Article 5. An introduction to estimation ‚Äì 2: from z to T [^fb82b806]. Emergency Medicine Journal (2001). Low credibility.

Provided the sample size is large enough (that is, n greater than 100), the z statistic can be used to determine the confidence interval estimation of the population mean even when the sigma is not known. In these cases the estimation of the standard error of the mean is used. The z statistic is also valid when determining the population's proportion based upon a large sample. However, when dealing with smaller samples, the z statistic is replaced by the t statistic. This makes it possible to estimate, in a population with an unknown standard deviation: The probability of getting a sample mean greater than or equal to a particular value The value of a sample mean with a particular probability of occurring The probability of getting a sample mean between two particular values The confidence interval for the estimation of the population mean can also be determined using the t statistic.

---

### Fundamental limits to learning closed-form mathematical models from data [^a345300f]. Nature Communications (2023). High credibility.

Given a finite and noisy dataset generated with a closed-form mathematical model, when is it possible to learn the true generating model from the data alone? This is the question we investigate here. We show that this model-learning problem displays a transition from a low-noise phase in which the true model can be learned, to a phase in which the observation noise is too high for the true model to be learned by any method. Both in the low-noise phase and in the high-noise phase, probabilistic model selection leads to optimal generalization to unseen data. This is in contrast to standard machine learning approaches, including artificial neural networks, which in this particular problem are limited, in the low-noise phase, by their ability to interpolate. In the transition region between the learnable and unlearnable phases, generalization is hard for all approaches including probabilistic model selection.

---

### The expected behaviour of random fields in high dimensions: contradictions in the results of bansal and peterson [^657f9d59]. Magnetic Resonance Imaging (2022). Medium credibility.

Bansal and Peterson (2018) found that in simple stationary Gaussian simulations Random Field Theory incorrectly estimates the number of clusters of a Gaussian field that lie above a threshold. Their results contradict the existing literature and appear to have arisen due to errors in their code. Using reproducible code we demonstrate that in their simulations Random Field Theory correctly predicts the expected number of clusters and therefore that many of their results are invalid.

---

### Strange data: when the numbers just arent't normal [^b856e8dd]. The Journal of Foot and Ankle Surgery (2015). Low credibility.

Many statistical tests assume that the populations from which we draw our data samples roughly follow a given probability distribution. Here, I review what these assumptions mean, why they are important, and how to deal with situations where the assumptions are not met.

---

### Clinical policy: critical issues in the evaluation and management of adult patients presenting to the emergency department with seizures [^3010d8a6]. Annals of Emergency Medicine (2014). Medium credibility.

Appendix B ‚Äî approach to downgrading strength of evidence maps design/class to evidence grades as follows: with "None" downgrading, classes "1", "2", and "3" correspond to "I", "II", and "III"; with "1 level", they correspond to "II", "III", and "X"; with "2 levels", they correspond to "III", "X", and "X"; and if "Fatally flawed", all are "X".

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^f37098d5]. Nature Communications (2021). High credibility.

Methods

Analytics and numerical simulation

Analytics were in general performed by hand, and checked for validity using Mathematica. Numerical simulations were run in Matlab using the dde23 delay differential equation solver for DDEs and ode45 for ODEs. Simulating activators as repressors with n < 0 technically fails when x is identically zero (Eq. (5)), since that would imply division by zero, but the limit as x goes to zero causes the regulation term to be zero, which is the same result as assumed by our notation. An initial value of exactly zero for x can thus lead to a divide-by-zero error in simulations, and so initial conditions of exactly zero were not used, as that case is an uninteresting fixed point for activators in any case. Note also that the consitutive case for Eq. (5) is degenerate, in that n = 0, Œ± ‚â† 0 is equivalent to n ‚â† 0, Œ± = 0 with Œ± 0 ‚Üí Œ± 0 + Œ± /2.

Phase plot simulations and analysis

For autoregulation phase plots, simulations were run with 100 constant-history initial conditions spread logarithmically between 10 ‚àí4 and 2 Œ∑ and run from T = 0 to T = 100(Œ≥ + 1). Solutions were considered stable if for all 100 simulations the maximum absolute value of the discrete derivative in the last three-quarters of the simulation time was less than 0.1. Stable solutions were sub-categorized as bistable if a histogram of final values over all 100 solutions had more than 1 peak. Solutions were considered oscillatory if the average Fourier transform of the last three-quarters of the simulation time for all 100 solutions had more than zero peaks with amplitude (square root of power) greater than 100. Solutions were considered spiral if this oscillation condition held for the first one-quarter of the simulation time only. For two-component loops, initial conditions were used that ranged between 0 and, for equal X and Y and for apposing X and Y. Bistability was determined as for autoregulation, and a cutoff of 0.05 was used to determine "low" values. All simulation histories were constant except where indicated in Supplementary Fig. 7. Specific parameter values and simulation details are given in the figures and/or made explicit in the MATLAB code in Supplementary Data 1.

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^d027f21a]. Nature Communications (2025). High credibility.

Understanding the effect of heterogeneity is fundamental to numerous fields. In community ecology, classical theory postulates that habitat heterogeneity determines niche dimensionality and drives biodiversity. However, disparate heterogeneity-diversity relationships have been empirically observed, generating increasingly complex theoretical developments. Here we show that spurious heterogeneity-diversity relationships and subsequent theories arise as artifacts of heterogeneity measures that are mean-biased for bounded continuous variables. To solve this, we derive an alternative mean-independent measure of heterogeneity for beta and gamma distributed variables that disentangles statistical dispersion from mean. Using the mean-independent measure of heterogeneity, true monotonic positive heterogeneity-diversity relationships, consistent with classical theory, are revealed in data previously presented as evidence for both hump-shaped heterogeneity-diversity relationships and theories of an area-heterogeneity trade-off for biodiversity. This work sheds light on the source of conflicting results that have hindered understanding of heterogeneity relationships in broader ecology and numerous other fields. The mean-independent measure of heterogeneity is provided as a solution, essential for understanding true mean-independent heterogeneity relationships in wider research.

---

### Recovery after stroke: not so proportional after all? [^d3259e7a]. Brain (2019). Medium credibility.

Spurious r(X,Œî) are likely when œÉ Y /œÉ X is small

For any X and Y, it can be shown that:

A formal proof of Equation 1 is provided in the Supplementary material, Appendix A [proposition 4 and theorem 1; also see]; its consequence is that r(X,Œî) is a function of r(X, Y) and œÉ Y /œÉ X. To illustrate that function, we performed a series of simulations (Supplementary material, Appendix B) in which r(X, Y) and œÉ Y /œÉ X were varied independently. Figure 2 illustrates the results: a surface relating r(X,Œî) to r(X, Y) and œÉ Y /œÉ X. Figure 3 shows example recovery data at six points of interest on that surface.

Figure 2
The relationship between r(X, Y), r(X,Œî) and œÉ Y /œÉ X. Note that the x -axis is log-transformed to ensure symmetry around 1; when X and Y are equally variable, log(œÉ Y /œÉ X) = 0. Supplementary material, proposition 7 in Appendix A, provides a justification for unambiguously using a ratio of standard deviations in this figure, rather than œÉ Y and œÉ X as separate axes. The two major regimes of Equation 1 are also marked in red. In Regime 1, Y is more variable than X, so contributes more variance to Œî, and r(X,Œî) ‚âà r(X, Y). In Regime 2, X is more variable than Y, so X contributes more variance to Œî, and r(X,Œî) ‚âà r(X,‚àíX) (i.e. ‚àí1). The transition between the two regimes, when the variability ratio is not dramatically skewed either way, also allows for spurious r(X,Œî). For the purposes of illustration, the figure also highlights six points of interest on the surface, marked A‚ÄìF; examples of simulated recovery data corresponding to these points are provided in Fig. 3.

---

### Parametric matrix models [^653168a8]. Nature Communications (2025). High credibility.

We present a general class of machine learning algorithms called parametric matrix models. In contrast with most existing machine learning models that imitate the biology of neurons, parametric matrix models use matrix equations that emulate physical systems. Similar to how physics problems are usually solved, parametric matrix models learn the governing equations that lead to the desired outputs. Parametric matrix models can be efficiently trained from empirical data, and the equations may use algebraic, differential, or integral relations. While originally designed for scientific computing, we prove that parametric matrix models are universal function approximators that can be applied to general machine learning problems. After introducing the underlying theory, we apply parametric matrix models to a series of different challenges that show their performance for a wide range of problems. For all the challenges tested here, parametric matrix models produce accurate results within an efficient and interpretable computational framework that allows for input feature extrapolation.

---

### A space-time tradeoff for implementing a function with master equation dynamics [^57fa10b0]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### Resolving discrepancies between chimeric and multiplicative measures of higher-order epistasis [^8eb47c09]. Nature Communications (2025). High credibility.

Multivariate Bernoulli distribution

The three parametrizations we derived for the bivariate Bernoulli distribution extend to the multivariate Bernoulli distribution. Suppose that (X 1, ‚Ä¶, X L) ‚àà {0, 1} L is distributed according to a multivariate Bernoulli distribution. Then the distribution P (X) of the random variables X is uniquely specified by one of the three following parametrizations.
General parameters: These are 2 L non-negative valuessatisfyingFor example if L = 3, then p 010 = P (X 1 = 0, X 2 = 1, X 3 = 0) and p 110 = P (X 1 = 1, X 2 = 1, X 3 = 0). Note that since, only 2 L ‚àí 1 valuesare necessary to define the distribution.
Natural/canonical parameters: These are 2 L real numberssatisfyingSimilar to the general parameters p i, only 2 L ‚àí 1 values Œ≤ S are necessary to uniquely define the distribution. Typically, the parameter, often called a normalizing constant or a partition function of the distribution, is left unspecified. As noted in the bivariate setting, equation (23) shows that the multivariate Bernoulli is an exponential family distribution with 2 L ‚àí 1 sufficient statistics of the form ‚àè i ‚àà S X i for subsets S with ‚à£ S ‚à£ > 0. Moreover, by rewriting (23) aswe observe that the natural parameters Œ≤ correspond to interaction coefficients in a log-linear regression model with response variables p. For example, the natural parameter Œ≤ 12 is the coefficient of the interaction term x 1 x 2.
Moments/mean parameters: These are 2 L real numberssatisfyingFor example if L = 3, then Œº 13 = E [X 1 X 3] while Œº 12 = E [X 1 X 2]. The mean parametersare sufficient statistics for the multivariate Bernoulli distribution, as seen in the exponential family form (23) of the multivariate Bernoulli distribution.

---

### X chromosome-inactivation patterns of 1, 005 phenotypically unaffected females [^2d21444c]. American Journal of Human Genetics (2006). Low credibility.

X-chromosome inactivation is widely believed to be random in early female development and to result in a mosaic distribution of cells, approximately half with the paternally derived X chromosome inactive and half with the maternally derived X chromosome inactive. Significant departures from such a random pattern are hallmarks of a variety of clinical states, including being carriers for severe X-linked diseases or X-chromosome cytogenetic abnormalities. To evaluate the significance of skewed patterns of X inactivation, we examined patterns of X inactivation in a population of > 1,000 phenotypically unaffected females. The data demonstrate that only a very small proportion of unaffected females show significantly skewed inactivation, especially during the neonatal period. By comparison with this data set, the degree of skewed inactivation in a given individual can now be quantified and evaluated for its potential clinical significance.

---

### The minimal work cost of information processing [^927873c9]. Nature Communications (2015). Medium credibility.

Classical mappings and dependence on the logical process

Our result, which is applicable to arbitrary quantum processes, applies to all classical computations as a special case. Classically, logical processes correspond to stochastic maps, of which deterministic functions are a special case. As a simple example, consider the AND gate. This is one of the elementary operations computing devices can perform, from which more complex circuits can be designed. The gate takes two bits as input, and outputs a single bit that is set to 1 exactly when both input bits are 1, as illustrated in Fig. 2a.

The logical process is manifestly irreversible, as the output alone does not allow to infer the input uniquely. If one of the inputs is zero, then the logical process effectively has to reset a three-level system to zero, forgetting which of the three possible inputs 00, 01 or 10 was given; this information can be viewed as being discarded, and hence dumped into the environment. We can confirm this intuition with our main result, using the fact that a general classical mapping is given by the specification of the conditional probability p (x ‚Ä≤| x) of observing x ‚Ä≤ at the output if the input was x. Embedding the classical probability distributions into the diagonals of quantum states, the infinity norm in expression (2) becomes simply

---

### An exact form for the magnetic field density of States for a dipole [^2c861285]. Magnetic Resonance Imaging (2001). Low credibility.

We present an analytical form for the density of states for a magnetic dipole in the center of a spherical voxel. This analytic form is then used to evaluate the signal decay as a function of echo time for different volume fractions and susceptibilities. The decay can be considered exponential only in a limited interval of time. Otherwise, it has a quadratic dependence on time for short echo times and an oscillatory decaying behavior for long echo times.

---

### Standardized incidence ratio and confidence levels‚Ä¶ [^590c438b]. seer.cancer.gov (2025). Medium credibility.

\. Exact Confidence Limits for the True SIR Sahai and Khurshid discuss the following method for obtaining the exact confidence limits, SIR and SIR L, for the true SIR, U œÜ. Assuming Dis Poisson distributed with mean Œº = E, confidence limits for Œºare obtained using the relationship between the Poisson distribution and the chi-square distribution. Then these limits are divided by the total number of expected events, E, to obtain the limits. and [œá¬≤(1‚àíŒ±/2)/(2E*)] where is the 100a percentile of the chi-square distribution with v degrees of freedom.

Approximate Confidence Limits for the True SIR The exact limits, SIR and SIR L, can be approximated by applying the Wilson and Hilferty approximation for chi-square percentiles, which is U ¬≥. The resulting approximate limits for the true SIR, œÜ*, are ¬≥ and. + [Z(1‚àíŒ±/2)/(3‚àö(D+1))]¬≥.

---

### Competition among networks highlights the power of the weak [^6bd3a4a1]. Nature Communications (2016). Medium credibility.

Methods

Measuring the importance of nodes and networks

We use the eigenvector centrality x k for quantifying the importance of a node k in a network, which can be obtained as an iterative process that sums the centralities of all neighbours of k:

where Œª is a constant, x k is the eigenvector centrality of node k and G kj are the components of the adjacency matrix, which could be both binary or weighted. In matrix notation equation (5) reads Œª x = Gx so that x can be expressed as a linear combination of the eigenvectors u k of the adjacency matrix G, being Œª k the set of the corresponding eigenvalues. Equation (5) can be regarded as an iterative process that begins at t = 0 with a set of initial conditions x 0. Regardless of the values of x 0, the value of x (t) at t ‚Üí‚àû will be proportional to the eigenvector u 1 associated with the largest eigenvalue Œª 1. Therefore, the eigenvector centrality is directly obtained from the eigenvector u 1 of the adjacency matrix G, which also holds for weighted adjacency matrices. As explained in the main text, the centrality accumulated by each network is obtained as the fraction of centrality accumulated by their nodes. Finally, we use Œª 1 as the measure of the network strength, as it is related to a series of dynamical properties of networks and, in turn, it increases with the number of nodes, links and the average degree of the network.

---

### Clinical policy: critical issues in the evaluation of adult patients presenting to the emergency department with acute blunt trauma [^38f47605]. Annals of Emergency Medicine (2024). High credibility.

Appendix E2 ‚Äî approach to downgrading strength of evidence ‚Äî maps design/class categories to evidence grades by level of downgrading: with "None" downgrading, classes 1, 2, and 3 correspond to I, II, and III; with "1 level" downgrading, they correspond to II, III, and X; with "2 levels" downgrading, they correspond to III, X, and X; if "Fatally flawed", all classes map to X.

---

### Clinical policy: critical issues in the evaluation and management of adult patients with suspected acute nontraumatic thoracic aortic dissection [^83f3c6fe]. Annals of Emergency Medicine (2015). Medium credibility.

Approach to downgrading strength of evidence ‚Äî under the header "Design/Class 1 2 3", the mapping shows that with "None" downgrading the levels are "I" for class 1, "II" for class 2, and "III" for class 3; with "1 level" downgrading they are "II", "III", and "X", respectively; with "2 levels" downgrading they are "III", "X", and "X", respectively; and when "Fatally flawed", all classes are "X".

---

### Clinical policy: critical issues in the evaluation and management of adult patients in the emergency department with asymptomatic elevated blood pressure [^0c674605]. Annals of Emergency Medicine (2013). Medium credibility.

Appendix B ‚Äî approach to downgrading strength of evidence shows that under "None" downgrading, Design/Class 1, 2, and 3 correspond to I, II, and III; with "1 level" downgrading they correspond to II, III, and X; with "2 levels" to III, X, and X; and when "Fatally flawed", all correspond to X.

---

### The challenge of detecting epistasis (G X G interactions): genetic analysis workshop 16 [^0abaeb2d]. Genetic Epidemiology (2009). Low credibility.

Interest is increasing in epistasis as a possible source of the unexplained variance missed by genome-wide association studies. The Genetic Analysis Workshop 16 Group 9 participants evaluated a wide variety of classical and novel analytical methods for detecting epistasis, in both the statistical and machine learning paradigms, applied to both real and simulated data. Because the magnitude of epistasis is clearly relative to scale of penetrance, and therefore to some extent, to the choice of model framework, it is not surprising that strong interactions under one model might be minimized or even disappear entirely under a different modeling framework.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^89cdccc2]. Annals of the American Thoracic Society (2023). High credibility.

V_A/Q distribution modeling ‚Äî derived from cumulative plots of ventilation or perfusion versus V_A/Q ‚Äî is binned into 50 equally spaced log-scale compartments, with shunt and dead space handled separately. Specifically, V_A/Q ratios < 0.005 (shunt) and > 100 (dead space) are calculated as separate components, and relative dispersion and gravitational gradients may also be assessed.

---

### Inferring time derivatives including cell growth rates using gaussian processes [^b5444dae]. Nature Communications (2016). Medium credibility.

Methods

Using a Gaussian process to fit time-series data

In the following, we will denote a Gaussian distribution with mean Œº and covariance matrix Œ£ as(Œº, Œ£) and use the notation of Rasmussen and Williamsas much as possible.

Prior probability

For n data points y i at inputs x i (each x i is a time for a growth curve), we denote the underlying latent function as f (x). We define a covariance matrix k (x, x ‚Ä≤), which has an explicit dependence on hyperparameters Œ∏, and obeys

where the expectations are taken over the distribution of latent functions (samples of f (x)).

We interpret equation (1) as giving the prior probability distribution of the latent functions f (X), where were we use X to denote the inputs x i, such that

where K (X, X) is the n √ó n matrix with components k (x i, x j). With f denoting [f (x 1). f (x n)], this prior probability can be written as

noting the dependence of k (x, x ‚Ä≤; Œ∏) on the hyperparameters Œ∏.

---

### Clinical policy: critical issues in the evaluation and management of adult patients presenting to the emergency department with acute headache [^999d0d3c]. Annals of Emergency Medicine (2019). High credibility.

Appendix B ‚Äî Approach to downgrading strength of evidence maps Design/Class 1, 2, and 3 to levels as follows: with none, levels are I, II, and III; with 1 level, they are II, III, and X; with 2 levels, they are III, X, and X; and when fatally flawed, all are X.

---

### Clinical decision rules: how to use them [^62ab325a]. Archives of Disease in Childhood: Education and Practice Edition (2010). Low credibility.

The first of this pair of papers outlined what a clinical decision rule is and how one should be created. This section examines how to use a rule, by checking that it is likely to work (examining how it has been validated), understanding what the various numbers that tell us about "accuracy" mean, and considers some practical aspects of how clinicians think and work in order to make that information usable on the front lines.

---

### State estimation of a physical system with unknown governing equations [^794152e0]. Nature (2023). Excellent credibility.

State estimation is concerned with reconciling noisy observations of a physical system with the mathematical model believed to predict its behaviour for the purpose of inferring unmeasurable states and denoising measurable ones 1,2. Traditional state-estimation techniques rely on strong assumptions about the form of uncertainty in mathematical models, typically that it manifests as an additive stochastic perturbation or is parametric in nature 3. Here we present a reparametrization trick for stochastic variational inference with Markov Gaussian processes that enables an approximate Bayesian approach for state estimation in which the equations governing how the system evolves over time are partially or completely unknown. In contrast to classical state-estimation techniques, our method learns the missing terms in the mathematical model and a state estimate simultaneously from an approximate Bayesian perspective. This development enables the application of state-estimation methods to problems that have so far proved to be beyond reach. Finally, although we focus on state estimation, the advancements to stochastic variational inference made here are applicable to a broader class of problems in machine learning.

---

### Spironolactone (Aldactone) [^e2a83be8]. FDA (2024). Medium credibility.

The dosage of spironolactone PO for treatment of ascites in adults with liver cirrhosis is:

- **Start at**: 100 mg PO daily
- **Maintenance**: 100‚Äì400 mg PO daily

---

### Using both qualitative and quantitative data in parameter identification for systems biology models [^e33a4be7]. Nature Communications (2018). Medium credibility.

Methods

Polynomial model

A system of linear equations for computing the polynomial coefficients was implemented with the Python Numpy module. We suppose we have n + 1 equally spaced qualitative data points, which divide the range x = [0, 10] into n equally-sized intervals. Exactly two of these intervals will be bounded on one end by a "‚àí" point and on the other by a "+" point; these two intervals must contain the intersection points x 1 and x 2.

To generate Fig. 1b, within the possible intervals for x 1 and x 2, we sampled all possible combinations of 100 (evenly spaced) values of x 1 and 100 values of x 2, and solved for the coefficients. Considering only the fits that yielded positive values for all coefficients, we report the minimum and maximum possible values for each coefficient.

Raf inhibitor model

The model shown in Fig. 2a was implemented in Python (Supplementary Software 1). We generated synthetic data points at various values of I and constant R tot = 50 ŒºM, and populated the other species concentrations based on the equations of the model. To generate quantitative data, we calculated = (RI + RIR + 2RIRI)/ R tot, and perturbed results by adding normally distributed noise with standard deviation 0.1. To generate qualitative data, we calculated A = RR + RIR, and compared the result to the value A (0) = 15.24. If the difference from A (0) was < 1.5, the data point was labeled as "0" (within error), otherwise it was labeled as "+" or "‚àí" appropriately.

For fitting, the objective function was created as described in Results. Minimization was performed with the Scipy function optimize.differential_evolution, with a maximum of 1000 iterations, strategy of "best1exp", and search range of [10 ‚àí4, 10 4] for each parameter.

To perform profile likelihood analysis, for each parameter, we considered 100 possible fixed values (log uniformly distributed in the range [10 2, 10 5] for K 3 and [10 ‚àí3, 10 0] for K 5). At each fixed value considered, minimization was performed by the same method as above, except that the parameter of interest was held at the fixed value. We report the resulting minimum objective function values in the profile likelihood plots (Fig. 2d, e).

---

### Statistics for nonparametric linkage analysis of X-linked traits in general pedigrees [^d18f287b]. American Journal of Human Genetics (2002). Low credibility.

We have compared the power of several allele-sharing statistics for "nonparametric" linkage analysis of X-linked traits in nuclear families and extended pedigrees. Our rationale was that, although several of these statistics have been implemented in popular software packages, there has been no formal evaluation of their relative power. Here, we evaluate the relative performance of five test statistics, including two new test statistics. We considered sibships of sizes two through four, four different extended pedigrees, 15 different genetic models (12 single-locus models and 3 two-locus models), and varying recombination fractions between the marker and the trait locus. We analytically estimated the sample sizes required for 80% power at a significance level of.001 and also used simulation methods to estimate power for a sample size of 10 families. We tried to identify statistics whose power was robust over a wide variety of models, with the idea that such statistics would be particularly useful for detection of X-linked loci associated with complex traits. We found that a commonly used statistic, S(all), generally performed well under various conditions and had close to the optimal sample sizes in most cases but that there were certain cases in which it performed quite poorly. Our two new statistics did not perform any better than those already in the literature. We also note that, under dominant and additive models, regardless of the statistic used, pedigrees with all-female siblings have very little power to detect X-linked loci.

---

### A genome-wide scan statistic framework for whole-genome sequence data analysis [^c7d73808]. Nature Communications (2019). High credibility.

Empirical power simulations

We also evaluate the empirical power of WGScan. For each replicate, we generate a 200 -kb region as simulated in the empirical family-wise error rate simulations. We consider two scenarios: high causal proportion with small effects, and low causal proportion with large effects. We set 2.5% or 0.5% variants in the 200 -kb region to be causal, all within a 10 -kb signal window in each scenario. Then we generated the quantitative/dichotomous phenotypes as follows:
Quantitative trait: Y i = X i 1 + Œ≤ 1 g 1 +‚Ä¶ + Œ≤ s g s + Œµ i
Dichotomous trait: logit pi = Œ± 0 + X i 1 + Œ≤ 1 g 1 +‚Ä¶ + Œ≤ s g s

where X i 1 ~ N (0, 1), Œµ i ~ N (0, 1) and they are all independent; (g 1, ‚Ä¶, g s) are selected risk variants. For the dichotomous trait, Œ± 0 is determined such that the prevalence is 1%. Then equal numbers of cases and controls were generated. The sequencing data were generated as described above. We set the effect, where m j is the MAF for the j th variant. For dichotomous trait, we set a = 0.5 when causal proportion is 2.5% (OR = 1.5, when MAF = 0.001), and a = 1.2 when causal proportion is 0.5% (OR = 3.6, when MAF = 0.001). For quantitative trait, we set a = 0.3 when causal proportion is 2.5%, and a = 1.8 when causal proportion is 0.5%.

---

### How accurately could we screen for individual risk? Using summary data to examine discriminatory accuracy of a risk marker [^516bcdac]. Preventive Medicine (2007). Low credibility.

Objective

While much has been written on the methodology of screening for presence of preclinical disease, correspondingly less has been written on screening for future risk of disease. Given the increasing attention paid to the concept of individualized prevention within the discipline of public health, this other type of screening warrants attention. Our aim is to demonstrate one way in which the potential accuracy of risk screening can be assessed.

Method

In this paper, we derive a simple computational formula for the concordance statistic, a measure of the ability to separate individuals into two groups (will get disease, will not get disease), based on the presence or absence of a dichotomous risk factor. This computational formula is based on summary data (prevalence, absolute risk) pertaining to the risk factor alone. We also present simple computational formulas for the true positive fraction (the sensitivity of the "high risk" label to actual disease development) and the false positive fraction (1-specificity of the "high risk" label.).

Conclusion

The above quantities are rarely presented when scientists make statements about the potential usefulness of a risk factor or genetic marker in screening for future disease risk. With the simple formulas offered here, readers will be better able to evaluate the accuracy of such statements.

---

### Clinical policy: critical issues in the evaluation and Management of adult patients presenting to the emergency department with suspected acute venous thromboembolic disease [^1849eb44]. Annals of Emergency Medicine (2018). Medium credibility.

Appendix B ‚Äî Approach to downgrading strength of evidence maps design classes to strength levels as follows: with none downgrading, classes 1, 2, and 3 correspond to I, II, and III; with 1 level downgrading, they correspond to II, III, and X; with 2 levels downgrading, they correspond to III, X, and X; and fatally flawed evidence is X across all classes.

---

### Identifying domains of applicability of machine learning models for materials science [^72c4ba76]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ŒΩ that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by œÉ lin (x 1, x 2) ‚â° ‚àí0.3 ‚â§ x 1 ‚â§ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: œÉ rbf ‚â° ‚àí3.3 ‚â§ x 11 ‚â§ 3.1 ‚àß x 2 ‚â§ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA œÉ poly (x 1, x 2) ‚â° ‚àí3.5 ‚â§ x 2 ‚â§ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### Scale-free networks are rare [^9c1c1630]. Nature Communications (2019). High credibility.

Real-world networks are often claimed to be scale free, meaning that the fraction of nodes with degree k follows a power law k -Œ±, a pattern with broad implications for the structure and dynamics of complex systems. However, the universality of scale-free networks remains controversial. Here, we organize different definitions of scale-free networks and construct a severe test of their empirical prevalence using state-of-the-art statistical tools applied to nearly 1000 social, biological, technological, transportation, and information networks. Across these networks, we find robust evidence that strongly scale-free structure is empirically rare, while for most networks, log-normal distributions fit the data as well or better than power laws. Furthermore, social networks are at best weakly scale free, while a handful of technological and biological networks appear strongly scale free. These findings highlight the structural diversity of real-world networks and the need for new theoretical explanations of these non-scale-free patterns.

---

### The minimal work cost of information processing [^894e044e]. Nature Communications (2015). Medium credibility.

where the sum ranges only over those x that have a non-zero probability of occurring. In the case of deterministic mappings p (x ‚Ä≤| x)‚àà{0,1}, this corresponds to the maximum number of input states that map to a same output state. For the AND gate, provided all four states 00, 01, 10 and 11 have non-negligible probability of occurring, there are three input states mapping to the same output state, so (3) gives us simply. Also, in simple examples as considered here, the expression (3) is stable to considering an-approximation (Supplementary Note 4); this quantity is thus physically justified.

Crucially, our result reveals that the minimal work requirement in general depends on the specific logical process, and not only on the input and output states. This contrasts with traditional thermodynamics for large systems, where the minimal work requirement of a state transformation can always be written as a difference of a thermodynamical potential, such as the free energy. For example, the minimal work cost of performing specifically an AND gate may differ from that of another logical process mapping an input distribution (p 00, p 01, p 10, p 11) (with ‚àë i p i = 1) to the distribution (p ‚Ä≤ 0, p ‚Ä≤ 1) = (p 00 + p 01 + p 10, p 11) (Recall that the classical counterpart of a quantum state is a probability distribution.). To see this, consider the XOR gate, which outputs a 1 exactly when both inputs are different (see Fig. 2b). The minimal work cost requirement of this gate, as given by (3), is now only kT ln 2, as in the worst case, only a single bit of information is erased (again supposing that all four input states have non-negligible probability of occurring). Now, suppose that, for some reason, the input distribution is such that p 01 + p 10 = p 11, that is, the input 11 occurs with the same probability as of either 01 or 10 appearing. Then, the XOR gate reproduces the exact same output distribution as the AND gate: in both cases, we have p ‚Ä≤ 0 = p 00 + p 10 + p 01 = p 00 + p 11 and p ‚Ä≤ 1 = p 11 = p 01 + p 10. In other words, both logical processes have the same input and output state, yet the XOR gate only requires work kT ln 2 compared with the AND gate, which requires 1.6 kT ln 2. Furthermore, we point out that this difference, which appears small in this case, may be arbitrarily large in certain scenarios (Supplementary Note 4).

---

### Nuclear microtubule filaments mediate non-linear directional motion of chromatin and promote DNA repair [^df119e73]. Nature Communications (2018). Medium credibility.

DCD error calculations

Let Œ∏ i, i = 1, ‚Ä¶, N, denote the calculated value of the angles determined from the position measurements (x j, y j), j = 1, ‚Ä¶, M. The uncertainty on each x j and y j is ¬± 200 nm and the corresponding uncertainty on Œ∏ i is ¬± Œµ i, where it is assumed that the angle is uniformly distributed on [Œ∏ i ‚àí Œµ i, Œ∏ i + Œµ i]. Let denote a range of angles for which a probability of inclusion has been calculated, that is, I (Œ±) = Pr[‚àí Œ± ‚â§ Œ∏ ‚â§ Œ±] = N Œ± / N, where N Œ± is the number of calculated angles fitting in the range and N is the total number of measurements. Now, let pi denote the probability that Œ∏ i is in the range (this is determined by integrating the uniform probability distribution between the appropriate limits corresponding to the overlap with [Œ∏ i ‚àí Œµ i, Œ∏ i + Œµ i]). Hence, pi = 1 corresponds to the case where Œ∏ i ¬± Œµ i is guaranteed to lie inside and pi corresponds to 0 when it is guaranteed to lie outside this range. The variance in N Œ± can be determined from that of the Bernoulli distribution and is given byThen, assuming that N Œ± has a uniform distribution, we can assume it lies in the range [N Œ± ‚àí ‚àö3 œÉ, N Œ± + ‚àö3 œÉ]. The uncertainty on the probability I Œ± is then ¬± ‚àö3 œÉ / N.

MSD curve fitting and MSD Œ± -value calculations

Let MSD = < (x (t + Œî t) ‚àí x (t)) 2 >, where x is the position of the focus and t is time. MSD fitting was achieved using MSD = Œìt Œ±, where Œì is a generalized coefficient and Œ± is a time-dependence coefficient reflecting the type of single particle mobility. Œ± ‚àº 1 reflects normal diffusion, Œ± < 1 reflects subdiffusion or anomalous diffusion, and Œ± ‚â• 2 reflects directed mobility.

---

### Clinical policy: critical issues in the evaluation and management of emergency department patients with suspected Non-ST-elevation acute coronary syndromes [^66e8b795]. Annals of Emergency Medicine (2018). Medium credibility.

Appendix B ‚Äî Approach to downgrading strength of evidence lists the mapping of design/class to evidence levels: With "None" downgrading the levels are "I" for class 1, "II" for class 2, and "III" for class 3; with "1 level" downgrading they become "II", "III", and "X"; with "2 levels" downgrading they are "III", "X", and "X"; and for "Fatally flawed" each class is "X".

---

### Learning non-stationary langevin dynamics from stochastic observations of latent trajectories [^8ffd6558]. Nature Communications (2021). High credibility.

Contributions of non-stationary components to the accurate inference

We found that accurate inference of non-stationary Langevin dynamics requires incorporating all three non-stationary components: the initial distribution p 0 (x), the boundary conditions, and the absorption operator. The necessity of all components for accurate inference is not obvious. Since spike trains generated from stationary versus non-stationary dynamics appear similar (Fig. 2), one could assume that omitting non-stationary components may affect the inference only insignificantly. To demonstrate how each component contributes to the accurate inference, we focus here on inferring the potential Œ¶(x) from synthetic data with known ground truth, assuming p 0 (x) and D are provided (we consider simultaneous inference of Œ¶(x), p 0 (x), D below). We use 200 trials of spike data generated from the model with a linear ground-truth potential and a narrow initial state distribution (full list of parameters in Supplementary Table 1). We simulated a reaction-time task, so that each trial terminates when the latent trajectory reaches one of the decision boundaries producing a non-stationary distribution of latent trajectories (Fig. 2 b).

The inference accurately recovers the Langevin dynamics from these non-stationary spike data when all non-stationary components are taken into account (Fig. 3 a). The GD algorithm iteratively increases the model likelihood (decreases the negative log-likelihood). Starting from an unspecific initial guess, the potential shape changes gradually over the GD iterations. After some iterations, the fitted potential closely matches the ground-truth shape while the log-likelihood of the fitted model approaches the log-likelihood of the ground-truth model. The concurrent agreement of the inferred potential and its likelihood with the ground truth indicates the accurate recovery of the Langevin dynamics. At later iterations, the potential shape can deteriorate due to overfitting, and model selection is required for identifying the model that accurately approximates dynamics in the data when the ground truth is not known(we consider model selection and uncertainty quantification below).

---

### Isolation by distance in populations with power-law dispersal [^209284ce]. G3 (2023). Medium credibility.

Intwo dimensions, it falls off logarithmically forand exponentially for:

Herewe generalize (2) and (3) to Œ± ‚â† 2, and find simple approximate expressions for œà in different parameter regimes, illustrated in Fig. 2. At long distances, we find that œà (x) has a universal form for all power-law dispersal kernels. Intuitively, power-law dispersal broadens the distribution of coalescence times for pairs at a given separation x, creating more overlap in the distributions for different x values (Fig. 3; see Appendix B for the equivalent schematic in one dimension).

Fig. 2.
For power-law dispersal, the form of isolation by distance is universal at long distances. Approximate form for the probability of identity as a function of distance, œà (x), for different dispersal kernel exponents Œ±. Left panel shows results for d = 2 spatial dimensions, right panel shows d = 1 dimension. Different regimes of parameter space are labeled by their qualitative dynamics. We use "‚àº" to denote proportionality in the limit of large population density where œà (0) ‚â™ 1. The key length scales are the characteristic length scale of identity, and the short distance Œ¥ at which coalescence can occur. Coalescence for distant pairs, typically occurs via one long jump, which leads to the universal power-law scaling at large distances predicted by (4) (green). The form of isolation by distance among nearby pairs, depends on d and Œ±. For sufficiently heavy-tailed dispersal (Œ± < d), nearby pairs typically either coalesce very quickly (at t ‚â™ 1/ Œº) or disperse far away from each other and mutate before coalescing, so the probability of identity is set by a competition between coalescence and dispersal and is nearly independent of the mutation rate ((6), blue). In this regime, identity still follows a power law with distance, although a shallower one than the power law at long distances, and with the opposite dependence on Œ±. Finite-variance dispersal with Œ± > 2 is effectively diffusive at short distances, with lineages typically reaching each other via many small jumps, so identity follows the classic diffusive predictions (2) or (3) (orange). In one dimension, there is an intermediate regime 1 < Œ± < 2, in which nearby pairs typically reach each other by many small jumps but the non-diffusive nature of the jumps is still apparent ((10), purple). For d = 2 and Œ± ‚â§ 1 in d = 1, the short-range details of coalescence become important for very close pairs ((3) or (7), brown). The solid vertical lines at Œ± = 1 and 2 indicate sharp transitions, while the dashed horizontal lines at x = Œ¥ andindicate smooth variation between asymptotic limits (see Figs. 4 and 5).

---

### Approximations to the expectations and variances of ratios of tree properties under the coalescent [^8355e038]. G3 (2022). Medium credibility.

Properties of gene genealogies such as tree height (H), total branch length (L), total lengths of external (E) and internal (I) branches, mean length of basal branches (B), and the underlying coalescence times (T) can be used to study population-genetic processes and to develop statistical tests of population-genetic models. Uses of tree features in statistical tests often rely on predictions that depend on pairwise relationships among such features. For genealogies under the coalescent, we provide exact expressions for Taylor approximations to expected values and variances of ratios Xn/Yn, for all 15 pairs among the variables {Hn, Ln, En, In, Bn, Tk}, considering n leaves and 2 ‚â§ k ‚â§ n. For expected values of the ratios, the approximations match closely with empirical simulation-based values. The approximations to the variances are not as accurate, but they generally match simulations in their trends as n increases. Although En has expectation 2 and Hn has expectation 2 in the limit as n‚Üí‚àû, the approximation to the limiting expectation for En/Hn is not 1, instead equaling œÄ¬≤/3 ‚àí 2 ‚âà 1.28987. The new approximations augment fundamental results in coalescent theory on the shapes of genealogical trees.

---

### Using synchronized oscillators to compute the maximum independent set [^c5524e12]. Nature Communications (2020). High credibility.

Not all computing problems are created equal. The inherent complexity of processing certain classes of problems using digital computers has inspired the exploration of alternate computing paradigms. Coupled oscillators exhibiting rich spatio-temporal dynamics have been proposed for solving hard optimization problems. However, the physical implementation of such systems has been constrained to small prototypes. Consequently, the computational properties of this paradigm remain inadequately explored. Here, we demonstrate an integrated circuit of thirty oscillators with highly reconfigurable coupling to compute optimal/near-optimal solutions to the archetypally hard Maximum Independent Set problem with over 90% accuracy. This platform uniquely enables us to characterize the dynamical and computational properties of this hardware approach. We show that the Maximum Independent Set is more challenging to compute in sparser graphs than in denser ones. Finally, using simulations we evaluate the scalability of the proposed approach. Our work marks an important step towards enabling application-specific analog computing platforms to solve computationally hard problems.

---

### Survival probability of stochastic processes beyond persistence exponents [^12e77d9f]. Nature Communications (2019). High credibility.

For many stochastic processes, the probability [Formula: see text] of not-having reached a target in unbounded space up to time [Formula: see text] follows a slow algebraic decay at long times, [Formula: see text]. This is typically the case of symmetric compact (i.e. recurrent) random walks. While the persistence exponent [Formula: see text] has been studied at length, the prefactor [Formula: see text], which is quantitatively essential, remains poorly characterized, especially for non-Markovian processes. Here we derive explicit expressions for [Formula: see text] for a compact random walk in unbounded space by establishing an analytic relation with the mean first-passage time of the same random walk in a large confining volume. Our analytical results for [Formula: see text] are in good agreement with numerical simulations, even for strongly correlated processes such as Fractional Brownian Motion, and thus provide a refined understanding of the statistics of longest first-passage events in unbounded space.

---

### The case for open computer programs [^0f6e8786]. Nature (2012). Excellent credibility.

Scientific communication relies on evidence that cannot be entirely included in publications, but the rise of computational science has added a new layer of inaccessibility. Although it is now accepted that data should be made available on request, the current regulations regarding the availability of software are inconsistent. We argue that, with some exceptions, anything less than the release of source programs is intolerable for results that depend on computation. The vagaries of hardware, software and natural language will always ensure that exact reproducibility remains uncertain, but withholding code increases the chances that efforts to reproduce results will fail.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^59d10d7c]. Annals of the American Thoracic Society (2023). High credibility.

Ventilation‚Äìperfusion (V A /Q) distribution modeling is binned into 50 equally spaced (log scale) compartments, with V A /Q ratios < 0.005 (shunt) and > 100 (dead space) separately calculated, and relative dispersion and gravitational gradients may also be assessed.

---

### Optimal profile limits for maternal mortality rate (MMR) in south Sudan [^2253d99b]. BMC Pregnancy and Childbirth (2018). Low credibility.

Linear profile limits

Profile monitoring systems assist and help to identify factors related to an observed phenomenon, assess the effect of changing any factor/s on the event and predict the behaviour of the phenomenon under different situations. In many situations the quality and performance of a process may be better characterized and summarized by relationship between the response (dependent) variable and one or more explanatory (independent) variables referred to as profile.

The general parametric linear profile model relating the explanatory variables X 1i, X 2i, X 3i. X pi to the response Y ij, is presented by.

where A 1j (l = 0,1,2. p) is the regression coefficient. The pair observation (X li, Y ij) is obtained in the jth random sample, where X li is the ith design point (i = 1,2,3. n) for the lth explanatory variable (l = 1.2,3. p). It is assumed that the errors …õ ijs are independent, identically distributed (i.i. d.) variables with mean zero and variance œÉ 2 j, when the process is in control.

Profile monitoring is used to understand and to check the stability of this relationship over time.

Recently many practitioners and researchers have used profile monitoring as a new sub-area of statistical process control exploring the application of profile monitoring in different disciplines and in real life. The application of profile monitoring is often focussed on processes with multiple quality characteristics and has also been extended to detect clusters of disease incident and used in public health surveillance.

In this study, profile monitoring will be used to monitor maternal mortality rate (MMR) in South Sudan and assess its variation influenced by SAB and GFR.

Development of profile limits

In this paper, MATLAB, Minitab, R and Excel Solver are used to obtain optimal values of Ln (SAB) and Ln (GFR) for a given value of Ln (MMR) while keeping Ln (GDP) constants at 7.480 (GDP = 1772) which is the average of Ln (GDP) over the period that the data were collected.

Furthermore, to generate the lower and upper profile control limits for Ln (MMR), the proposed predictive models presented in eq. (3) and the target minimum and maximum levels of MMR proposed by the UN agencies; MMR = 70 and MMR = 140 have been used. It was recommended that these limits should be achieved by 2030. The current MMR in South Sudan is about 730 deaths per 100,000.

---

### Fluctuation spectra of large random dynamical systems reveal hidden structure in ecological networks [^a92321c9]. Nature Communications (2021). High credibility.

Computing the power spectral density

In the following, we use features of the bipartite interaction network. For instance, all nodes that are connected to e.g. node x i, will be prey nodes y j, and thus are not connected with each other (see Fig. 6). This allows us to write the following recursion formulas for the mean power spectral densities according to Eq. (42),

Recall that the top left entries of Œ® x and Œ® y deliver the mean power spectral densities for predators œï x and prey œï y respectively. For the bipartite model, the helping matrices œá i, œá i j (as defined in Eq. (23)) are given by,

Inserting and writing out Eq. (59) gives, where c x, c y are the number of connections per predator and prey species respectively. Analogous to Eq. (38) we now derive a system of equations and solve for r x, r y and œï x, œï y. In the main text we describe the features of the power spectral density deduced from this system of equations.

Interpreting the power spectral density in the context of temporal stability

For orientation, we here provide some interpretation of the power spectral density in the context of temporal stability. Essentially when we talk about temporal stability, we can be referring to one of two measures. The first is how far stochastic trajectories tend to stray from their equilibrium value over long time horizons. We refer to this as 'variability'. The second is how quickly population abundances tend to change over finite time horizons. We will characterise this by the 'temporal autocorrelation'.

The variability can be characterised by the variance in time-averaged trajectories around the mean. For a system such as Eq. (2), which we recall can be a linear approximation for a nonlinear system such as Eq. (1), we find that Œæ is normally distributed with zero mean and a covariance matrix, Œ£, that solves the following Lyapunov equation;

---

### Implementing quantum dimensionality reduction for non-markovian stochastic simulation [^4ea471be]. Nature Communications (2023). High credibility.

Methods

Stochastic processes and minimal-memory classical modelling

A discrete-time stochastic processconsists of a sequence of random variables X t, corresponding to events drawn from a set, and indexed by a timestep. The process is defined by a joint distribution of these random variables across all timesteps, whererepresents the contiguous (across timesteps) series of events between timesteps t 1 and t 2. We consider stochastic processes that are bi-infinite, such thatand, and stationary (time-invariant), such that P (X 0: L) = P (X t: t + L) ‚àÄ t, L ‚àà. Without loss of generality, we can take the present to be t = 0, such that the past is given by, and the future. Note that we use upper case for random variables and lower case for the corresponding variates.

A (causal) model of such a (bi-infinite and stationary) discrete-time stochastic process consists of an encoding functionthat maps from the set of possible past observationsto a set of memory states ‚Äì. The model also requires an update rulethat produces the outputs and updates the memory state accordingly. We then designate the memory cost D f of the encoding as the logarithm of the dimension (i.e. the number of (qu)bits) of the smallest system into which these memory states can be embedded. For classical (i.e. mutually orthogonal) memory states, this corresponds to. For quantum memory states, which may, in general, be linearly dependent.

Let us, for now, restrict our attention to statistically-exact models, such that (f, Œõ) must produce outputs with a distribution that is identical to the stochastic process being modelled. Under such a condition, the provably-memory minimal classical model of any given discrete-time stochastic process is known and can be systematically constructed. These models are referred to as the Œµ - machine of the process, which employs an encoding function f Œµ based on the causal states of the process. This encoding function satisfiesand given initial memory state, the evolution produces output x 0 with probabilityand updates the memory to state. The memory states are referred to as the causal states of the process, and the associated cost D Œº is given by the logarithm of the number of causal states.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^22bd51a1]. Nature Communications (2021). High credibility.

Motif III: logic

A general class of functions used to describe natural,‚Äì and synthetic, biological networks are logic gates, which have two inputs regulating a single output (Fig. 5). Gates exhibit either high ("on") or low ("off") output depending on whether inputs are on or off. For example, the AND gate specifies high output only if both inputs are on. In this section we provide a specific DDE-based framework that covers 14 out of 16 possible 2-input logic operations, and show that these operations form a continuous 2D parameter space.

Fig. 5
A simple approximation for digital logic using a sum of Hill terms recapitulates all monotonic logic functions in a single parameter space.

a A prototypical regulatory network involving logic where X and Y both regulate Z, which must integrate the two signals using some logic before it can in turn activate a downstream reporter R. b Parameter space showing regions where regulation approximately follows 14 of the 16 possible 2-input logic functions depending on the strength of two single-variable Hill regulation terms (Œ∑ Z 1: regulation of Z by X, Œ∑ Z 2: regulation of Z by Y). Network logic can be smoothly altered by varying the parameters (Œ∑ Z 1, Œ∑ Z 2), with a change of sign in (n 1, n 2) required to switch quadrants. The bottom-left quadrant shows that very weak regulation in both terms leads to an always-off (FALSE) function, weak regulation in one arm only leads to single-input (X, Y) functions, strong regulation in both arms leads to an OR function, and regulation too weak in either arm alone to activate an output but strong enough in sum leads to an AND function. The other three quadrants are related by applying NOT to one or both inputs, with function names related by de Morgan's lawNOT(X OR Y) = NOT X AND NOT Y. In particular, X IMPLY Y = NOT(X) OR Y, X NIMPLY Y = X AND NOT(Y), X NOR Y = NOT X AND NOT Y, and X NAND Y = NOT X OR NOT Y. Truth tables for all 16 logic gates are provided in Supplementary Table 1 for reference. The two non-monotonic logic functions, X XOR Y and X XNOR Y, are those 2 of 16 not reproduced directly using this summing approximation. They can be produced by layering, e.g. NAND gates. c Representative time traces for AND (Œ∑ Z 1 = Œ∑ Z 2 = 0.9) and OR (Œ∑ Z 1 = Œ∑ Z 2 = 1.8) gates with n 1 = n 2 = ‚àí2, n 3 = ‚àí20, Œ∑ R = Œ∑ Z 1 + Œ∑ Z 2. The functionwhen n > 0, when n < 0.

---

### Spin dephasing under nonlinear gradients: implications for imaging and field mapping [^eaf2803c]. Magnetic Resonance in Medicine (2012). Low credibility.

This work examines the prototypical MR echo that would be expected for a voxel of spins evolving in a strong nonlinear field, specifically focusing on the quadratic z(2) - ¬Ω(x(2) + y(2)) field. Dephasing under nonlinear gradients is increasingly relevant given the growing interest in nonlinear imaging, and here, we report several notable differences from the linear case. Most notably, in addition to signal loss, intravoxel dephasing under gradients creating a wide and asymmetric frequency distribution across the voxel can cause skewed and nonlinear phase evolution. After presenting the qualitative and analytical origins of this difference, we experimentally demonstrate that neglecting these dynamics can lead to significant errors in sequences that assume phase evolution is proportional to voxel frequency, such as those used for field mapping. Finally, simplifying approximations to the signal equations are presented, which not only provide more intuitive forms of the exact expression but also result in simple rules to predict key features of the nonlinear evolution.

---

### Markerless real-time 3-dimensional kV tracking of lung tumors during free breathing stereotactic radiation therapy [^84aabecf]. Advances in Radiation Oncology (2021). Medium credibility.

Figure 1
(A) Experimental setup for the phantom study. (B) Transverse slice of the (planning) computed tomography scan of the phantom at the level of target 1, (C) target 2, and (D) target 3. The arrows indicate the tumor in the right upper lung (1, mean Hounsfield unit [HU] = ‚Äì130), the right middle lung (2, mean HU = ‚Äì130), and the left middle lung (3, mean HU = ‚Äì480).

During kV acquisition the phantom was either kept stationary or moved by programmed couch (table) motion in 3D using the TrueBeam Developer Mode (Varian Medical Systems). Regular and irregular breathing patterns were simulated to investigate the performance of the triangulation algorithm in an ideal situation (regular breathing) and a more realistic situation (irregular breathing). The regular breathing pattern had peak-to-peak amplitudes of 2/4.5/7.5 mm in lateral, longitudinal, and vertical directions, respectively. The irregular breathing pattern was simulated in the 3 directions with a direction-dependent multiplication factor combined with the equation: y = 0.25 * cos(2x) ‚Äì 0.75 * sin 2 (1.1 * x) ‚Äì sin 3 (x + 30) + (0.0003x)¬≤, where x denotes a time-factor (72/s) and y the position. It had maximum peak-to-peak amplitudes of ~3.8/~9.5/~17.2 mm, respectively, and a phase and amplitude drift. Planar kV images were acquired with 100 kV tube voltage, 45 mA current, and 32 ms pulse width for a full arc at 7 fps during 5.5 Gy/arc 10 MV flattening filter free VMAT delivery.

The 2D and 3D root mean square error of measured versus known positions was calculated, and the percentage of matched frames with deviation < 1, < 2, and > 5 mm from the expected location was determined. In addition, the influence of MS and PSR settings was evaluated. Both MS and PSR thresholds were set to zero for the remaining data.

---

### Clinical policy: critical issues in the evaluation of adult patients presenting to the emergency department with acute blunt abdominal trauma [^f1e59d16]. Annals of Emergency Medicine (2011). Medium credibility.

Appendix B ‚Äî Approach to downgrading strength of evidence maps downgrading levels to Design/Class columns "1", "2", and "3": with "None" downgrading the entries are "I", "II", and "III"; with "1 level" they are "II", "III", and "X"; with "2 levels" they are "III", "X", and "X"; and "Fatally flawed" corresponds to "X", "X", and "X".

---

### Using both qualitative and quantitative data in parameter identification for systems biology models [^fc71653e]. Nature Communications (2018). Medium credibility.

Results

An illustration of the potential value of qualitative data

To demonstrate the potential value of qualitative data, we consider a simple case of solving for the coefficients of polynomial functions.

We consider two polynomial functions: y‚ÇÅ = ax¬≤ ‚àí bx + c and y‚ÇÇ = dx + e. Suppose we want to solve for the coefficients a, b, c, d, and e, which we will take to be positive. As the ground truth coefficients to be determined, we choose (a, b, c, d, e) = (0.5, 3, 5, 1, 1.5).

Suppose that a limited amount of quantitative information is available. Namely, it is known that the parabola y 1 contains the points (2, 1) and (8, 13), and the line y 2 contains the point (3.5,5). This is not enough information to solve for any of the coefficients because three points are required to specify a parabola, and two points are required to specify a line (Fig. 1a).

Fig. 1
A simple illustration using polynomial functions. We use qualitative and quantitative information to determine the unknown coefficients. a Visualization of the problem. We seek to find the coefficients of equations for a parabola and a line, with the ground truth shown (blue solid curves). Two points on the parabola and one point on the line are known (black dots). These three points are consistent with infinitely many possible solutions (e.g. orange dashed curves). Qualitative information (colored circles, x -axis) specifies whether the parabola is above (+) or below (‚àí) the line. This information limits the possible values of intersection points x 1 and x 2 to the green shaded segments of the x -axis. b Bounds on coefficient values as a function of the number of qualitative points known. Shaded areas indicate the range of possible values of each coefficient

---

### Principles of analytic validation of immunohistochemical assays: guideline update [^cda8439c]. Archives of Pathology & Laboratory Medicine (2024). High credibility.

Supplemental Digital Content ‚Äî AMSTAR (Assessing the Methodological Quality of Systematic Reviews) appraisal of methodological quality presents checklist items marked with "‚àö" or "x": "Quality assessed & documented" is "‚àö" and "x"; "Quality used appropriately for conclusion" is "‚àö" and "x"; "Methods to combine used appropriately" is "‚àö" and "‚àö"; "Publication bias assessed" is "x" and "x"; and "COI (conflict of interest)" is "‚àö" and "‚àö". The AMSTAR SCORE /11 is 9 and 5.

---

### Correction [^26cbbc41]. Journal of Radiology Case Reports (2017). Low credibility.

[This corrects the article on p. 8 in vol. 10.].

---

### Principles of screening [^cba7419b]. Seminars in Perinatology (2005). Low credibility.

Screening and diagnostic testing are sequentially-oriented procedures for progressively analyzing risk. Screening is broadly based and aimed at identifying those at high risk of a disease or condition. Diagnostic testing is designed to more definitively answer the question of whether or not a person has a disease or condition. Understanding the differences among four key measures of evaluation for tests (sensitivity, specificity, positive predictive value and negative predictive value) are necessary for proper utilization of screening tests.

---

### Deep learning for universal linear embeddings of nonlinear dynamics [^317523d7]. Nature Communications (2018). Medium credibility.

Methods

Creating the datasets

We create our datasets by solving the systems of differential equations in MATLAB using the ode45 solver.

For each dynamical system, we choose 5000 initial conditions for the test set, 5000 for the validation set, and 5000‚Äì20,000 for the training set (see Table 2). For each initial condition, we solve the differential equations for some time span. That time span is t = 0,.02, ‚Ä¶,1 for the discrete spectrum and pendulum datasets. Since the dynamics on the slow manifold for the fluid flow example are slower and more complicated, we increase the time span for that dataset to t = 0,.05, ‚Ä¶,6. However, when we include data off the slow manifold, we want to capture the fast dynamics as the trajectories are attracted to the slow manifold, so we change the time span to t = 0,.01, ‚Ä¶,1. Note that for the network to capture transient behavior as in the first and last example, it is important to include enough samples of transients in the training data.

Table 2
Dataset sizes

The discrete spectrum dataset is created from random initial conditions x where x 1, x 2 ‚àà[‚àí0.5, 0.5], since this portion of phase space is sufficient to capture the dynamics.

The pendulum dataset is created from random initial conditions x, where x 1 ‚àà[‚àí3.1,3.1] (just under), x 2 ‚àà[‚àí2,2], and the potential function is under 0.99. The potential function for the pendulum is. These ranges are chosen to sample the pendulum in the full phase space where the pendulum approaches having an infinite period.

The fluid flow problem limited to the slow manifold is created from random initial conditions x on the bowl where r ‚àà[0, 1.1], Œ∏ ‚àà[0, 2 œÄ], x 1 = r cos(Œ∏), x 2 = r sin(Œ∏),‚Ä¶ This captures all of the dynamics on the slow manifold, which consists of trajectories that spiral toward the limit cycle at r = 1.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^bb9b7f12]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Œî Œ¶, the probability distribution at a point Œ¶ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Œ¶ ‚àà {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters Œ± and Œ≤, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since Œ± and Œ≤ are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Œ¶ ‚àà [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(Œ±, Œ≤) and x 2 = 1 ‚àí x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Œ¶, it may seem natural to select Œ± and Œ≤ by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### Estimating the success of re-identifications in incomplete datasets using generative models [^cfb6f216]. Nature Communications (2019). High credibility.

Results

Using Gaussian copulas to model uniqueness

We consider a dataset, released by an organization, and containing a sample ofindividuals extracted at random from a population of n individuals, e.g. the US population. Each row x (i) is an individual record, containing d nominal or ordinal attributes (e.g. demographic variables, survey responses) taking values in a discrete sample space. We consider the rows x (i) to be independent and identically distributed, drawn from the probability distribution X with, abbreviated p (x).

Our model quantifies, for any individual x, the likelihood Œæ x for this record to be unique in the complete population and therefore always successfully re-identified when matched. From Œæ x, we derive the likelihood Œ∫ x for x to be correctly re-identified when matched, which we call correctness. If Doe's record x (d) is unique in, he will always be correctly re-identified (and). However, if two other people share the same attribute (not unique,), Doe would still have one chance out of three to have been successfully re-identified. We modelas:and Œ∫ x as:with proofs in "Methods".

We model the joint distribution of X 1, X 2,‚Ä¶ X d using a latent Gaussian copula. Copulas have been used to study a wide range of dependence structures in finance, geology, and biomedicineand allow us to model the density of X by specifying separately the marginal distributions, easy to infer from limited samples, and the dependency structure. For a large sample spaceand a small numberof available records, Gaussian copulas provide a good approximation of the density using only d (d ‚àí 1)/2 parameters for the dependency structure and no hyperparameter.

The density of a Gaussian copula C Œ£ is expressed as:with a covariance matrix Œ£, u ‚àà [0, 1] d, and Œ¶ the cumulative distribution function (CDF) of a standard univariate normal distribution.

---

### Extraction of accurate cytoskeletal actin velocity distributions from noisy measurements [^e02eea03]. Nature Communications (2022). High credibility.

Consequently, for small displacements, the localization error overwhelms the measured distance. Figure 2 a, b shows simulations of measured distances between two positions with a true distance s = 5 œÉ x y, as well as for a true distance of s = 0 (a stationary point) with œÉ x y = 1. In both cases, the distribution of measured distances was in good agreement with the analytical solution given by a noncentral œá distribution. For the longer distance, the average measured distance is roughly correct, but for a stationary particle (s = 0), all measurements are greater than the true displacement of zero, giving a peaked distance distribution (with peak at). For both true distances, the distribution of measured distances is broader than the (Dirac delta-function) true distribution.

In order to quantitatively characterize the localization error, we treated chemically fixed cells with SiR-actin and tracked the actin fiducials through time in the same way as our live-cell measurements. The resulting distribution of measured displacements was peaked at non-zero values, as predicted (Fig. 2 c). However, this distribution is not well-fit by a single noncentral œá with s = 0, due to the varying localization errors of individual puncta. We are able to capture this variation by a mixture of two noncentral œá distributions, giving three fit parameters: œÉ x y 1 and œÉ x y 2, the œÉ s contributed from each of the two noncentral œá distributions, and f 1, the relative weighting of the noncentral chi with the smaller œÉ. Here and hereafter we report maximum likelihood estimates for fit parameters; MLE allows us to maximally leverage the information in tracks (n = ~70,000‚Äì360,000 displacements in fixed cells from each experimental dataset), rather than fitting to the binned histogram. On average across all experiments, the best fit parameters were œÉ x y 1 = 23 nm, œÉ x y 2 = 50 nm, and f 1 = 0.57, defining the localization error in our system (Fig. 2 c, Supplementary Fig. 1).

---

### Survival analysis part II: multivariate data analysis ‚Äì an introduction to concepts and methods [^ecee0901]. British Journal of Cancer (2003). Low credibility.

The Cox model is essentially a multiple linear regression of the logarithm of the hazard on the variables x i, with the baseline hazard being an 'intercept' term that varies with time. The covariates then act multiplicatively on the hazard at any point in time, and this provides us with the key assumption of the PH model: the hazard of the event in any group is a constant multiple of the hazard in any other. This assumption implies that the hazard curves for the groups should be proportional and cannot cross (see Figure 1

Figure 1
Example of (non-) proportional hazards (groups (c) and (d) only have proportional hazards) using the Weibull distribution. For the Weibull survival model, the hazard function h (t) = Œªs (Œªt) s ‚àí1 for Œª, s > 0: (a) increasing hazard (Œª = 0.5, s = 1.25); (b) decreasing hazard (Œª = 0.25, s = 0.75); (c) decreasing hazard (Œª = 0.5, s = 0.5); (d) decreasing hazard (Œª = 0.25, s = 0.5).

for examples of each). Proportionality implies that the quantities exp(b i) are called hazard ratios. A value of b i greater than zero, or equivalently a hazard ratio greater than one, indicates that as the value of the i th covariate increases, the event hazard increases and thus the length of survival decreases. Put another way, a hazard ratio above 1 indicates a covariate that is positively associated with the event probability, and thus negatively associated with the length of survival. This proportionality assumption is often appropriate for survival time data but it is important to verify that it holds. We discuss methods for assessing proportionality in the next paper in this series.

---

### Next generation reservoir computing [^398d0bfe]. Nature Communications (2021). High credibility.

Reservoir computing is a best-in-class machine learning algorithm for processing information generated by dynamical systems using observed time-series data. Importantly, it requires very small training data sets, uses linear optimization, and thus requires minimal computing resources. However, the algorithm uses randomly sampled matrices to define the underlying recurrent neural network and has a multitude of metaparameters that must be optimized. Recent results demonstrate the equivalence of reservoir computing to nonlinear vector autoregression, which requires no random matrices, fewer metaparameters, and provides interpretable results. Here, we demonstrate that nonlinear vector autoregression excels at reservoir computing benchmark tasks and requires even shorter training data sets and training time, heralding the next generation of reservoir computing.

---

### Nonlinear wave propagation governed by a fractional derivative [^db5b4cd7]. Nature Communications (2025). High credibility.

The idea of fractional derivatives has a long history that dates back centuries. Apart from their intriguing mathematical properties, fractional derivatives have been studied widely in physics, for example in quantum mechanics and generally in systems with nonlocal temporal or spatial interactions. However, systematic experiments have been rare because the physical implementation is challenging. Here we report the observation and full characterization of a family of temporal solitons that are governed by a fractional nonlinear wave equation. We demonstrate that these solitons have non-exponential tails, reflecting their nonlocal character, and have a very small time-bandwidth product.

---

### Triazolam [^59f91090]. FDA (2025). Medium credibility.

WARNING: RISKS FROM CONCOMITANT USE WITH OPIOIDS; ABUSE, MISUSE, AND ADDICTION; and DEPENDENCE AND WITHDRAWAL REACTIONS

Concomitant use of benzodiazepines and opioids may result in profound sedation, respiratory depression, coma, and death. Reserve concomitant prescribing of these drugs in patients for whom alternative treatment options are inadequate. Limit dosages and durations to the minimum required. Follow patients for signs and symptoms of respiratory depression and sedation [see Warnings and Precautions (5.1), Drug Interactions (7.1)].
The use of benzodiazepines, including triazolam, exposes users to risks of abuse, misuse, and addiction, which can lead to overdose or death. Abuse and misuse of benzodiazepines commonly involve concomitant use of other medications, alcohol, and/or illicit substances, which is associated with an increased frequency of serious adverse outcomes. Before prescribing triazolam and throughout treatment, assess each patient's risk for abuse, misuse, and addiction [see Warnings and Precautions (5.2)].
The continued use of benzodiazepines, including triazolam, may lead to clinically significant physical dependence. The risks of dependence and withdrawal increase with longer treatment duration and higher daily dose. Abrupt discontinuation or rapid dosage reduction of triazolam after continued use may precipitate acute withdrawal reactions, which can be life-threatening. To reduce the risk of withdrawal reactions, use a gradual taper to discontinue triazolam or reduce the dosage [see Dosage and Administration (2.3), Warnings and Precautions (5.3)].

WARNING: RISKS FROM CONCOMITANT USE WITH OPIOIDS; ABUSE, MISUSE, AND ADDICTION; and DEPENDENCE AND WITHDRAWAL REACTIONS

See full prescribing information for complete boxed warning.

Concomitant use of benzodiazepines and opioids may result in profound sedation, respiratory depression, coma, and death. Reserve concomitant prescribing of these drugs in patients for whom alternative treatment options are inadequate. Limit dosages and durations to the minimum required. Follow patients for signs and symptoms of respiratory depression and sedation (5.1, 7.1).
The use of benzodiazepines, including triazolam, exposes users to risks of abuse, misuse, and addiction, which can lead to overdose or death. Before prescribing triazolam and throughout treatment, assess each patient's risk for abuse, misuse, and addiction (5.2).
Abrupt discontinuation or rapid dosage reduction of triazolam after continued use may precipitate acute withdrawal reactions, which can be life-threatening. To reduce the risk of withdrawal reactions, use a gradual taper to discontinue triazolam or reduce the dosage (2.3, 5.3).

---

### Comparison of parameter optimization methods for quantitative susceptibility mapping [^26a0417b]. Magnetic Resonance in Medicine (2021). Medium credibility.

Purpose

Quantitative Susceptibility Mapping (QSM) is usually performed by minimizing a functional with data fidelity and regularization terms. A weighting parameter controls the balance between these terms. There is a need for techniques to find the proper balance that avoids artifact propagation and loss of details. Finding the point of maximum curvature in the L-curve is a popular choice, although it is slow, often unreliable when using variational penalties, and has a tendency to yield overregularized results.

Methods

We propose 2 alternative approaches to control the balance between the data fidelity and regularization terms: 1) searching for an inflection point in the log-log domain of the L-curve, and 2) comparing frequency components of QSM reconstructions. We compare these methods against the conventional L-curve and U-curve approaches.

Results

Our methods achieve predicted parameters that are better correlated with RMS error, high-frequency error norm, and structural similarity metric-based parameter optimizations than those obtained with traditional methods. The inflection point yields less overregularization and lower errors than traditional alternatives. The frequency analysis yields more visually appealing results, although with larger RMS error.

Conclusion

Our methods provide a robust parameter optimization framework for variational penalties in QSM reconstruction. The L-curve-based zero-curvature search produced almost optimal results for typical QSM acquisition settings. The frequency analysis method may use a 1.5 to 2.0 correction factor to apply it as a stand-alone method for a wider range of signal-to-noise-ratio settings. This approach may also benefit from fast search algorithms such as the binary search to speed up the process.

---

### Estimating the success of re-identifications in incomplete datasets using generative models [^9c6329d0]. Nature Communications (2019). High credibility.

Theoretical and empirical population uniqueness

For n individuals x (1), x (2), ‚Ä¶, x (n) drawn from X, the uniqueness Œû X is the expected percentage of unique individuals. It can be estimated either (i) by computing the mean of individual uniqueness or (ii) by sampling a synthetic population of n individuals from the copula distribution. In the former case, we havewhere T x = [‚àÉ! i, x (i) = x] equals one if there exists a single individual i such as x (i) = x and zero otherwise. T x follows a binomial distribution B (p (x), n). Thereforeand

This requires iterating over all combinations of attributes, whose number grows exponentially as the number of attributes increases, and quickly becomes computationally intractable. The second method is therefore often more tractable and we use it to estimate population uniqueness in the paper.

For cumulative marginal distributions F 1, F 2, ‚Ä¶, F d and copula correlation matrix Œ£, the algorithm 1 (Supplementary Methods) samples n individuals from q (‚ãÖ|Œ£,Œ®) using the latent copula distribution. From the n generated records (y (1), y (2), ‚Ä¶, y (n)), we compute the empirical uniqueness

Individual likelihood of uniqueness and correctness

The probability distributioncan be computed by integrating over the latent copula density. Note that the marginal distributions X 1 to X d are discrete, causing the inversestoto have plateaus. When estimating p (x), we integrate over the latent copula distribution inside the hypercube:with œï Œ£ the density of a zero-mean multivariate normal (MVN) of correlation matrix Œ£. Several methods have been proposed in the literature to estimate MVN rectangle probabilities. Genz and Bretz, proposed a randomized quasi Monte Carlo method which we use to estimate the discrete copula density.

The likelihood Œæ x for an individual's record x to be unique in a population of n individuals can be derived from p X (X = x):

Similarly, the likelihoodfor an individual's record x to be correctly matched in a population of n individuals can be derived from. With, the number of potential false positives in the population, we have:

Note that, since records are independent, T follows a binomial distribution B (n ‚àí 1, p (x)).

We substitute the expression for Œæ x in the last formula and obtain:

---

### Dynamical regimes of diffusion models [^a7936fcc]. Nature Communications (2024). High credibility.

Speciation time

In order to show the existence of regimes I and II, and compute the speciation time, we focus on the following protocol which consists of cloning trajectories. We consider a backward trajectory starting at time t f ‚â´ 1 from a point x f drawn from a random Gaussian distribution where all components are independent with mean zero and unit variance. This trajectory evolves backward in time, through the backward process until time t < t f. At this time the trajectory has reached the point y (t), at which cloning takes place. One generates for œÑ < t two clones, starting from the same x 1 (t) = x 2 (t) = y (t), and evolving as independent trajectories x 1 (œÑ) and x 2 (œÑ), i.e. with independent thermal noises. We compute the probability œï (t) that the two trajectories ending in x 1 (0) and x 2 (0) are in the same class. Defining P (x 1, 0‚à£ y, t) as the probability that the backward process ends in x 1, given that it was in y at time t, the joint probability of finding the trajectory in y at time t and the two clones in x 1 and x 2 at time 0 is obtained as Q (x 1, x 2, y, t) = P (x 1, 0‚à£ y, t) P (x 2, 0‚à£ y, t) P (y, t). Then œï (t) is the integral of Q over x 1, x 2, y with the constraint (x 1 ‚ãÖ m)(x 2 ‚ãÖ m) > 0. This simplifies into a one-dimensional integral (see SI Appendix):where G (y, u, v) is a Gaussian probability density function for the real variable y, with mean u and variance v, and, Œì t = Œî t + œÉ 2 e ‚àí2 t. The probability œï (t) that the two clones end up in the same cluster is a decreasing function of t, going from œï (0) = 1 to œï (‚àû) = 1/2. In the large d limit, the scaling variable controlling the change of œï iswhich can be rewritten asby using. This explicitly shows that speciation takes place at the time scale t S on a window of time of order one. As detailed in SI Appendix, this expression for t S coincides with the one obtained from the general criterion (5). We show in Fig. 2 the analytical result from (6) and direct numerical results obtained for increasingly larger dimensions. This comparison shows that our analysis is accurate already for moderately large dimensions. In the limit of infinite d, the analytical curve in Fig. 2 suddenly jumps from one to zero at t / t S = 1, corresponding to a symmetry-breaking phase transition (or a threshold phenomenon) on the time scale t S. In the numerics, following finite size scaling theory, we have defined the speciation time as the crossing point of the curves for different d, which corresponds approximatively to œï (t S) = 0.775 and indeed is of the orderfor d ‚Üí ‚àû. As it happens in mean-field theories of phase transitions, the large dimensional limit allows to obtain a useful limiting process. In our case, this leads to a full characterization of the asymptotic backward dynamics. At its beginning, i.e. in regime I, the overlap with the centers of the Gaussian model, ¬± m ‚ãÖ x (t), is of order. Defining, one can obtain a closed stochastic Langevin equation on q in a potential V (q, t) (see SI Appendix), where Œ∑ (t) is square root of two times a Brownian motion, andAt large d, this potential is quadratic at times, and it develops a double well structure, with a very large barrier, when. The trajectories of q (t) are subjected to a force that drives them toward plus and minus infinity. The barrier between positive and negative values of q becomes so large that trajectories commit to a definite sign of q: this is how the symmetry breaking takes place dynamically at the time scale t S, in agreement with the previous cloning results. Regime II corresponds to the scaling limit q ‚Üí ‚àû, where m ‚ãÖ x (t) becomes of order d. In this regime, the rescaled overlap m ‚ãÖ x (t)/ d concentrates, and its sign depends on the set of trajectories one is focusing on. Moreover, the stochastic dynamics of the x i correspond to the backward dynamics for a single Gaussian centered in ¬± m. This shows that the dynamics generalizes, see SI Appendix (and alsofor similar results).

---

### An efficient strategy for evaluating new non-invasive screening tests for colorectal cancer: the guiding principles [^9847c1bd]. Gut (2023). Medium credibility.

Comparing tests using either approach will identify if the new test has promise, and whether more rigorous and costly evaluation in the unbiased screening context would be worthwhile. Results are unlikely to be sufficient in themselves for acceptance by regulatory authorities, policy-makers, payers and professional guideline-making bodies.

Comparing test sensitivity for CRC and advanced precursor lesions infers mortality and incidence benefits of the new test relative to the known benefits of the comparator test, as sensitivity can be considered an intermediate/surrogate end point for mortality benefit.

Outcomes of importance in the typical screening context (see principle 6) in addition to specificity and sensitivity will be determined in larger-scale unbiased studies with longitudinal follow-up (see phases III and IV studies in principle 10).

10. Evaluation proceeds through increasingly complex phases

Evaluation of a new test should follow a four-phase (sequential) evaluation. This would start with limited-scale cohort or case-control studies in populations with and without neoplasia, possibly enriched for the neoplastic outcomes of interest. Initial estimates of diagnostic accuracy and test positivity threshold will be obtained in phases I and II studies. If results suggest that the test might achieve the desired diagnostic accuracy, evaluation should proceed to screening pathway evaluation requiring larger intended-use screening populations (phases III and IV). The latter studies should be prospective and will identify the most suitable threshold for test result positivity, among other important outcomes.

Explanatory text

Phased (ie, sequential) evaluation in a stepwise manner is an efficient way first to establish the potential value of a new test and then to subsequently gather the evidence that will lead to its acceptance by professionals, healthcare providers and regulatory bodies. There are four main phases (figure 3).

Figure 3
Goals, context and approach for each phase of evaluation, together with the hurdle identifying justification to advance to the next phase. Note that clinical status must be ascertained by colonoscopy. CRC, colorectal cancer; ROC, receiver operating characteristic.

---

### The snm procedure guideline for general imaging 6.0 [^85398c18]. SNMMI (2010). Medium credibility.

Nuclear medicine computer system components ‚Äî The camera head or associated image processing system performs functions including image size, position and zoom; energy correction; spatial distortion correction; other corrections (scatter correction, dead time correction, depth of interaction correction, sensitivity correction); and digital position computation; the interface handles the data in two basic modes: 1) Frame mode: complete images or matrices are available to the attached computer; and 2) List mode: data are passed on to the attached computer as a list of event x, y coordinates, to which time information and energy information may be also attached; for cardiac studies in particular, time lapse averaging is required, such that each image acquired at some specific time within the cardiac cycle is added to other acquired at similar times.

---

### Ozone trends and their sensitivity in global megacities under the warming climate [^5a370c7a]. Nature Communications (2024). High credibility.

Ozone in the troposphere is formed by a series of reactions driven by the emission of NO x and VOC induced by solar radiation. Owing to its secondary origins, monitoring and controlling the emissions of the main precursors, VOC and NO x, is crucial. The amount of O 3 formed depends on the relative ratio of the precursors. When NO x emissions are abundant, VOC are the limiting species for O 3 formation (VOC-limited regime). Meanwhile, NO x emissions govern O 3 formation (NO x -limited regime) at low NO x concentrations. However, the O 3 production process is influenced by several other factors, such as the competition between VOCs and NO x through HO x chemistry, resulting in a nonlinear relationship between O 3 formation and precursor emissions.

Formaldehyde (HCHO) is a secondary product of VOC oxidation, and nitrogen dioxide (NO 2) is closely related to NO x owing to the rapid oxidation of NO to NO 2. Therefore, HCHO and NO 2 have been widely used as proxies for VOC and NO x emissions, respectively, and the HCHO to NO 2 ratio (FNR) has been used as an indicator of O 3 formation sensitivity. Different FNR threshold values have been proposed to establish sensitivity regimes, and the methods presented by Duncan et al.are among the most commonly used; O 3 formation is VOC-limited at FNR < 1, transitional at 1 < FNR < 2, and NO x -limited at FNR > 2. However, the threshold values for regime classification are spatially and temporally dependent. Thus, other studies have proposed different threshold values for the transitional regime ranging from 1.5‚Äì2.3 to 3‚Äì4.

---

### Clinical policy: critical issues in the evaluation and management of adult patients presenting to the emergency department with acute heart failure syndromes: approved by ACEP board of directors, June 23, 2022 [^bbcf6bcb]. Annals of Emergency Medicine (2022). High credibility.

Appendix B ‚Äî approach to downgrading strength of evidence ‚Äî maps design/class to evidence levels based on downgrading: with none, class 1 maps to I, class 2 to II, and class 3 to III; with 1 level, class 1 maps to II, class 2 to III, and class 3 to X; with 2 levels, class 1 maps to III and classes 2 and 3 to X; and if fatally flawed, all classes map to X.

---

### Recommendations for cardiac chamber quantification by echocardiography in adults: an update from the American Society of Echocardiography and the European association of cardiovascular imaging [^7eb51193]. Journal of the American Society of Echocardiography (2015). Medium credibility.

Three-dimensional (3D) data sets ‚Äî key advantages and limitations are listed as no geometrical assumption, unaffected by foreshortening, and more accurate and reproducible compared to other imaging modalities, with limitations of lower temporal resolution, less published data on normal values, and image quality dependent.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^d6d8fcbc]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Spirometry reporting specifies that numerical values are given for the FEV1, the FVC, and the FEV1/FVC ratio; the latter should be reported as a decimal fraction and the space for percent predicted value left blank, and if bronchodilators are given the LLN column need not be repeated with absolute and percent change given only for FEV1 and FVC. Other numerical values such as the forced inspiratory flow at 75% of FVC (FEF75%) and FEF25‚Äì75% are not recommended for routine use. Graph requirements include that for the volume‚Äìtime curve the volume scale should be at least 10 mm/L, the time scale at least 20 mm/s, and 1 second prior to the start of expiration should be displayed; on the flow‚Äìvolume plot the flow display should be at least 5 l/min/L/s, and the ratio of flow to volume should be 2 L/s to 1 L, and linear and log scales where values are plotted as z-scores relative to the predicted value (z = 0) give an intuitive sense of severity.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^cbffc27e]. CDC (2011). Medium credibility.

Appendix G. Mathematical formula for sample size ‚Äî sample size required to detect the difference of GM between two subpopulation groups defines a z test with null hypothesis GM1 = GM2 or GMmax/GMmin = 1 and alternative GM1 ‚â† GM2 or GMmax / GMmin > 1, where GMmax = max(GM1, GM2) and GMmin = min(GM1, GM2). It considers power W (say 80% chance) to detect that one group's GM is at least k fold higher and gives the p-value as p = Prob(Z ‚â• z) = 1 ‚àí œÜ‚Åª¬π(z) with Z ~ Normal (0,1). Given Œ± (e.g., 0.05) and target ratio k, power is W = Prob (Z > œÜ‚Åª¬π(1 ‚àí Œ±) ‚àí log10(k)/(S‚àö(1/n‚ÇÅ + 1/n‚ÇÇ))), where S is the expected standard deviation of log10(VL) in the population of interest; when n‚ÇÅ = n‚ÇÇ = n, W = Prob (Z > œÜ‚Åª¬π(1 ‚àí Œ±) ‚àí log10(k)/(S‚àö(2/n))). The required sample size is n = 2[(œÜ‚Åª¬π(1 ‚àí Œ±) ‚àí œÜ‚Åª¬π(1 ‚àí W))S/log10(k)]¬≤, and tables are provided for W = 0.8 and W = 0.9 with Œ± = 0.05.

---

### Is the "biexponential diffusion" biexponential? [^f6e04ac3]. Magnetic Resonance in Medicine (2007). Low credibility.

Diffusion-weighted signal from the brain, S, deviates from monoexponential dependence on the b-factor. This property is often referred to as biexponential diffusion, since the corresponding model fits data well. The aim of this study is to examine the necessity of using the biexponential model in homogeneous voxels under isotropic diffusion weighting up to b = 2.5 ms/microm(2). The model is compared to the cumulant expansion of ln S in a power series in b, which takes its origin in fundamental properties of the diffusion-weighted signal, but diverges at large b. The absence of statistically significant evidences for the biexponential diffusion is demonstrated in gray matter. The cumulant expansion terminated after the term b(2) describes data equally well with fewer adjustable parameters. The biexponential model is preferable in voxels with a partial volume of CSF.