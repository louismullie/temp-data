# Medical Question & Answer

**Sample ID**: 4f08a62d-679c-c4b1-945f-b36b774e2077
**Dataset Index**: 158150

---

## Question

Example 17 Let the infinite sequence be defined by x₁ and xₙ = √[2xₙ₋₁ + 3], find lim n→∞ xₙ.

---

## Answer

> Let's see… What do we have here? The user is asking how to find the limit of the recursive sequence defined by (x_n = \\sqrt{2x_{n-1} + 3}) as (n \\to \\infty). Let's break this down step-by-step. First, I need to think about whether the sequence converges and under what conditions. Then, I should verify that if it converges, the limit must satisfy a fixed-point equation. Next, I will solve that fixed-point equation and check for extraneous solutions. After that, I should confirm that the sequence is indeed monotonic and bounded, which guarantees convergence. Finally, I will consider edge cases and summarize the result with a clear conclusion.

> Let me first confirm the core idea: if the sequence converges to some limit (L), then both (x_n) and (x_{n-1}) approach (L), so the recurrence becomes (L = \\sqrt{2L + 3}). I need to ensure I'm not jumping to conclusions here; this fixed-point reasoning is valid only if convergence is established, so I should verify convergence separately before accepting the fixed-point solution as the limit [^notfound].

> Now, I will solve the fixed-point equation (L = \\sqrt{2L + 3}). Squaring both sides gives (L^2 = 2L + 3), which rearranges to (L^2–2L - 3 = 0). Factoring yields ((L - 3)(L + 1) = 0), so the candidates are (L = 3) and (L = -1). Wait, let me verify the extraneous solution: since the square root is non-negative, (L) cannot be negative, so (L = -1) is extraneous and the only valid candidate is (L = 3) [^notfound].

> Hold on, let's not jump to conclusions about convergence yet. I need to check whether the sequence actually converges for a given starting value. Let me consider the behavior of the function (f(x) = \\sqrt{2x + 3}). I should confirm that this function is continuous and increasing for (x \\geq -\\frac{3}{2}), which it is, and that will help with monotonicity arguments. Next, I should analyze fixed points and intervals where the sequence is increasing or decreasing, because that will determine convergence and the direction of approach to the limit [^notfound].

> I will now examine monotonicity and boundedness. If (x_1 < 3), I can show by induction that the sequence is increasing and bounded above by 3, so it converges to 3. If (x_1 > 3), the sequence is decreasing and bounded below by 3, so it also converges to 3. If (x_1 = 3), the sequence is constant at 3. Let me double-check the inductive step: assuming (x_n < 3), then (x_{n+1} = \\sqrt{2x_n + 3} < \\sqrt{2 \\cdot 3 + 3} = 3), and similarly for the decreasing case, which confirms the bounds and monotonicity [^notfound].

> But wait, what if the starting value is too small? I should confirm the domain. The expression under the square root requires (2x_{n-1} + 3 \\geq 0), so (x_{n-1} \\geq -\\frac{3}{2}). If (x_1 < -\\frac{3}{2}), the first term (x_2) is undefined, so the sequence does not exist beyond the first term. Let me reconsider: if (x_1 = -\\frac{3}{2}), then (x_2 = 0), and from there the sequence increases toward 3, so even in that boundary case it converges to 3. Thus, the only problematic scenario is (x_1 < -\\frac{3}{2}), where the sequence is undefined after the first step [^notfound].

> I should double-check that the limit is indeed independent of the starting value, provided the sequence is defined. Since both increasing sequences bounded above and decreasing sequences bounded below converge, and the only possible limit is 3, the limit is 3 for any valid starting value (x_1 \\geq -\\frac{3}{2}). This aligns with the fixed-point analysis and the monotonicity arguments I just verified [^notfound].

> Final answer: The limit is 3 for any initial value (x_1 \\geq -\\frac{3}{2}). If (x_1 < -\\frac{3}{2}), the sequence is undefined after the first term, so no limit exists in that case [^notfound].

---

The limit of the sequence defined by (x_n = \\sqrt{2x_{n-1} + 3}) is **3** for any starting value (x_1 \\geq -\\frac{3}{2}). This is because the sequence is monotonic and bounded, so it converges to the fixed point (L = \\sqrt{2L + 3}), which has the unique solution (L = 3). If (x_1 < -\\frac{3}{2}), the sequence is undefined, so no limit exists.

---

## Step 1: Assume the limit exists

Let (L = \\lim_{n \\to \\infty} x_n). If the limit exists, then as (n \\to \\infty), both (x_n) and (x_{n-1}) approach (L), so the recurrence becomes:

L = √[2L + 3]

---

## Step 2: Solve the fixed-point equation

Square both sides to eliminate the square root:

L² = 2L + 3

Rearrange to a quadratic equation:

L² − 2L − 3 = 0

Factor:

(L − 3)(L + 1) = 0

So the solutions are (L = 3) or (L = -1). Since (x_n = \\sqrt{2x_{n-1} + 3} \\geq 0) for all defined terms, the limit must be non-negative; therefore, **(L = 3)** is the only valid candidate.

---

## Step 3: Prove convergence

We must show the sequence converges for (x_1 \\geq -\\frac{3}{2}). Consider the function (f(x) = \\sqrt{2x + 3}):

- **Domain**: (2x + 3 \\geq 0 \\Rightarrow x \\geq -\\frac{3}{2}).
- **Monotonicity**: (f'(x) = \\frac{1}{\\sqrt{2x + 3}} > 0), so (f) is strictly increasing.
- **Fixed point**: (f(3) = 3).

We analyze two cases:

- **Case 1**: (x_1 \\in [-\\frac{3}{2}, 3]). Then (x_1 \\leq 3 \\Rightarrow x_2 = f(x_1) \\leq f(3) = 3), and by induction (x_n \\leq 3) for all (n). Also, (x_{n+1} = f(x_n) \\geq x_n) because (f(x) \\geq x) on ([-\\frac{3}{2}, 3]). Thus, ({x_n}) is non-decreasing and bounded above by 3, so it converges to 3.
- **Case 2**: (x_1 > 3). Then (x_1 > 3 \\Rightarrow x_2 = f(x_1) < x_1) because (f(x) < x) for (x > 3), and by induction (x_n) is strictly decreasing and bounded below by 3, so it converges to 3.

---

## Step 4: Summary of results

| **Initial condition** | **Behavior of sequence** | **Limit** |
|-|-|-|
| (x_1 \\geq -\\frac{3}{2}) | Converges monotonically to 3 | 3 |
| (x_1 < -\\frac{3}{2}) | Undefined (square root of negative) | Does not exist |

---

The limit of the sequence is **3** for any starting value (x_1 \\geq -\\frac{3}{2}). If (x_1 < -\\frac{3}{2}), the sequence is undefined and no limit exists.

---

## References

### Modelling sequences and temporal networks with dynamic community structures [^4f105116]. Nature Communications (2017). Medium credibility.

Results

Inference of markov chains

Here we consider general time-series composed of a sequence of discrete observations { x t }, where x t is a single token from an alphabet of size N observed at discrete time t, and x t −1 = (x t −1,…, x t − n) is the memory of the previous n tokens at time t (Fig. 1). An n th-order Markov chain with transition probabilities p (x t | x t −1) generates such a sequence with probabilitywhere a x, x is the number of transitions x → x in { x t }. Given a specific sequence { x t }, we want to infer the transitions probabilities p (x | x). The simplest approach is to compute the maximum-likelihood estimate, that iswhere a x = ∑ x a x, x, which amounts simply to the frequency of observed transitions. Putting this back into the likelihood of eq. (1), we haveThis can be expressed through the total number of observed transitionsand the conditional entropyas. Hence, the maximisation of the likelihood in eq. (1) yields the transition probabilities that most compress the sequence. There is, however, an important caveat with this approach. It cannot be used when we are interested in determining the most appropriate Markov order n of the model, because the maximum likelihood in eq. (3) increases with n. In general, increasing number of memories at fixed number of transitions leads to decreased conditional entropy. Hence, for some large enough value of n there will be only one observed transition conditioned on every memory, yielding a zero conditional entropy and a maximum likelihood of 1. This would be an extreme case of overfitting, where by increasing the number of degrees of freedom of the model it is impossible to distinguish actual structure from stochastic fluctuations. Also, this approach does not yield true compression of the data, since it does not describe the increasing model complexity for larger values of n, and thus is crucially incomplete. To address this problem, we use a Bayesian formulation, and compute instead the complete evidencewhich is the sum of all possible models weighted according to prior probabilities P (p) that encode our a priori assumptions. This approach gives the correct model order for data sampled from Markov chains as long as there are enough statistics that balances the structure present in the data with its statistical weight, as well as meaningful values when this is not the case.

---

### Approximations to the expectations and variances of ratios of tree properties under the coalescent [^f384a1d5]. G3 (2022). Medium credibility.

Results

Expectations of the ratios

The approximate expected values in Table 4, as approximations of ratios, have the form of rational functions. As n grows, the approximate expectations of, andapproach 0. This behavior is sensible when considering the properties of the coalescent model: in the numerators, E n has expectation 2 andandhave bounded expectation in the limit as; in the denominators, L n and I n have expectations that grow without bound (Table 2). Similarly, approximate expectations of ratiosorwith L n and I n in the numerator and T k in the denominator grow to infinity as n increases. The approximation toapproaches 1 in the limit as: as the number of leaves in the tree grows, internal branches occupy an increasingly large fraction of the total branch length.

For pairs of variables that both have finite expectation, the approximate expectations of their associated ratios —, and — also approach finite values in the limit as. It is interesting to observe that although(Table 2). In other words, although expectations of the individual variables approach the same value, we expectto be somewhat larger than 1 on average.

For each of the 10 pairs of variables among, the approximate expectations from Table 4 are plotted in Fig. 2 together with the simulated values. Although some divergences are present for small n, the approximate and simulated values match closely.

The approximate ratios involving T k are shown in Fig. 3 as functions of k for each of three values of n. L n is the fastest-growing variable according to the expression for its expectation (Table 2), and the graph foris topmost in all three plots. As expectations of H n and E n are close (Table 2), the graphs forandare close in Fig. 3.

---

### Assessing limit of detection in clinical sequencing [^5fae7122]. The Journal of Molecular Diagnostics (2021). Medium credibility.

Clinical reporting of solid tumor sequencing requires reliable assessment of the accuracy and reproducibility of each assay. Somatic mutation variant allele fractions may be below 10% in many samples due to sample heterogeneity, tumor clonality, and/or sample degradation in fixatives such as formalin. The toolkits available to the clinical sequencing community for correlating assay design parameters with assay sensitivity remain limited, and large-scale empirical assessments are often relied upon due to the lack of clear theoretical grounding. To address this uncertainty, a theoretical model was developed for predicting the expected variant calling sensitivity for a given library complexity and sequencing depth. Binomial models were found to be appropriate when assay sensitivity was only limited by library complexity or sequencing depth, but functional scaling for library complexity was necessary when both library complexity and sequencing depth were co-limiting. This model was empirically validated with sequencing experiments by using a series of DNA input amounts and sequencing depths. Based on these findings, a workflow is proposed for determining the limiting factors to sensitivity in different assay designs, and the formulas for these scenarios are presented. The approach described here provides designers of clinical assays with the methods to theoretically predict assay design outcomes a priori, potentially reducing burden in clinical tumor assay design and validation efforts.

---

### Analysis of measurable residual disease by IG / TR gene rearrangements: quality assurance and updated euroMRD guidelines [^db4f1b1f]. Leukemia (2024). Medium credibility.

Task 2: IG/TR MRD target identification

Two points are awarded for each identified sequence (a sequence that has been detected by at least two other laboratories and conforms to the consensus interpretation) and correct nomenclature according to IMGT for a maximum of three sequences. A sequence is also scored if it was not identified by any other laboratory but was used as an MRD-PCR target in an RQ-PCR assay with a QR of at least 10–4.

The quantitative MRD levels are defined by the respective QA organizer by known dilutions of leukemic DNA into DNA of peripheral blood mononuclear cells of at least five healthy donors and are used as references for scoring. One point is awarded for the quantitative MRD level obtained for the two targets, as far as these levels were within a predefined upper and lower limit. The upper and lower limits were defined by a factor of 5 of the reference value for each marker and each time point (FU1-3): Limit 1 ≤ reference value ≤ Limit 2; Limit 1 = reference/sqrt(5), Limit 2 = reference*sqrt(5). The raw points of this task are multiplied with a weighting factor of 8 for the final score. Figure 3 shows an example of part of a certificate that the participant receives.

Fig. 3
Illustration of the results obtained with Task 2 as shown in the report and certificate.

The MRD levels of follow-up sample 1 were determined by the participants using different MRD-PCR targets. The names of the respective targets used for the MRD assay by the participant are colored green, the number of participants that used the respective target is written above the box plots (note: all participants will aim to analyze the FU sample using two MRD-PCR targets, therefore the sum of these numbers is about twice the number of participants). The blue line indicates the reference value.

Task 3: RQ-PCR-based MRD quantification

For task 3, the quantitative MRD levels are provided by the respective QA organizer and used as references for scoring, as already described for task 2. The raw points of this task are multiplied with a weighting factor of 10 for the final score.

---

### Determining sequencing depth in a single-cell RNA-seq experiment [^ce3ab87a]. Nature Communications (2020). High credibility.

An underlying question for virtually all single-cell RNA sequencing experiments is how to allocate the limited sequencing budget: deep sequencing of a few cells or shallow sequencing of many cells? Here we present a mathematical framework which reveals that, for estimating many important gene properties, the optimal allocation is to sequence at a depth of around one read per cell per gene. Interestingly, the corresponding optimal estimator is not the widely-used plug-in estimator, but one developed via empirical Bayes.

---

### PGSE, OGSE, and sensitivity to axon diameter in diffusion MRI: insight from a simulation study [^7ca4a558]. Magnetic Resonance in Medicine (2016). Low credibility.

Implications for Practical Applications

In practical applications, e.g. for a realistic tissue model with fiber dispersion or unknown orientation and realistic SNR, the choice of N impacts significantly the range of axon diameters that can be identified and distinguished from one another. For a fixed TE, sequences with N > 1 increase the detectable difference in the signal, and distinguish between axon diameters that N = 1 cannot. For example, for, 300 mT/m scanner's lower limit for SNR = 10 is approximately 4.3 µm for N = 1, and 3.1 µm for N = 4. In another example, Figure 7 middle row shows that forand G = 300 mT/m, N = 4 signal can with 1 µm resolution access axon diameters which are 1 µm lower than with N = 1. Hence, going from N = 1 to the best choice of N increases the overlap of the range of axon diameter sensitivity with naturally occurring axon diameters, which can potentially allow more accurate axon diameter imaging of in vivo human brain. These advantages are especially prominent on stronger gradient strengths.

It is also worth noting that moving from the traditional PGSE sequences to the OGSE sequences discussed here is simple and offers additional practical benefits. They have only one additional parameter, the number of lobes N, therefore, are easy to implement and run on standard clinical and preclinical scanners. Low‐frequency OGSE (N > 1) can also significantly reduce the gradient heating and deal better with eddy currents than PGSE sequences.

Model Considerations

In this work, we choose a prevailing simple model of white matter 11, 12. More complex models increase the difficulty of locating most sensitive sequence parameters and could potentially influence some of the results presented here, e.g. membranes with permeable walls could create an increase in signal loss for sequences with large b ‐value, hence OGSE sequences with N > 1 would have further advantage over PGSE sequences. Another simplification of our model is the representation of axons as a collection of cylinders with one diameter. It is known from histology 44 that in the brain, white matter axons have a diameter distribution which is often modelled as a Gamma function. Nevertheless, an understanding of which sequence is most sensitive to each particular axon diameter is very informative as it provides a deeper understanding of the relationship between individual restrictions and sequence parameters, and is a first step toward understanding sensitivity to the combinations of different diameters.

---

### Opportunities and challenges associated with clinical diagnostic genome sequencing: a report of the Association for Molecular Pathology [^b1741e3d]. The Journal of Molecular Diagnostics (2012). Medium credibility.

Common complex diseases — genome-wide association studies (GWAS) and single-nucleotide polymorphism (SNP) catalogs highlight scale and limits: 11 million SNPs and three million indels have been characterized, dense SNP chips have facilitated GWAS with many cases and controls (> 1000 per group), and genome.gov catalogues > 1200 polymorphisms that confer disease risk in > 165 common human diseases and traits, yet multiple risk alleles only explain approximately 5% of the variance in the population. At the individual level, the uncovering of three million SNPs within a genome requires interpretive knowledge, and the full potential can only be realized when the genome sequences of thousands of individuals are available and linked to corresponding phenotypes; data mining initiatives such as the U.K. 10,000 Genomes Project may refine disease pathways and clarify how specific alleles present as protective or deleterious.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^690140f5]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Next-generation sequencing (NGS) limits of detection (LOD) — the LOD for each type of genetic alteration is recommended to estimate during O&F phase using cell line mixing experiments, and the lower LOD (LLOD) could be defined as the minor allele fraction at which 95% of samples would reliably be detected. Although a laboratory director may choose to run validation samples to demonstrate the LLOD assuming that 19 or 20 correct results would indicate a reliability ≥ 95%, by testing just 20 samples the director could not be confident that the test would reliably detect 95% or more samples at that lower limit of detection; for example, the probability of getting 20 of 20 correct when the reliability is 90% would be 12% based on (0.90)20, and if 100 samples were all correct then (0.90)100 would equal 0.003%, which would be unlikely.

---

### Flexible and cost-effective genomic surveillance of P. falciparum malaria with targeted nanopore sequencing [^a75b070f]. Nature Communications (2024). High credibility.

Model

Each column of the read count matrix, x j = { x 1 j, x 2 j. x n j }, contains the read counts for a given target gene j across all n samples. We model x j with a Dirichlet-multinomial distribution:whereis the total read counts for target j across all samples; and α j = { α 1 j, α 2 j. α n j } is a vector of compound parameters, α i j, for the target gene j. These α i j determine the expected number of reads for each sample for a given target gene and are computed in three steps. First, we use the relative abundance of a sample, a i, and its deletion status for target gene j, c i j, to compute the expected proportion of reads generated for target gene j that should derive from sample i:

Note that p i j either equals zero, if target gene j is deleted in sample i; or the proportion of sample i in the sequencing library, but relative to only the samples where the gene is present. In the process of generating these reads, these expected proportions are altered through read misclassification and sample contamination, such that a different set of expected proportions, π i j, are reflected in the final data. Here, we make the assumption that read misclassification happens at a fixed rate, ϵ, and uniformly across samples, resulting in the expression:

Both p i j and π i j sum to one for a given j. Finally, we parameterise the Dirichlet-multinomial with α i j = ν π i j. The effect is that the expected read counts, x i j, equals the product of the total number of reads for target gene j, N j, multiplied by the deletion-status and error-adjusted sample proportion, π i j:

The variance of x i j equals:which increases as ν shrinks towards zero, or asymptotically approaches the variance of a binomial distribution, as ν grows towards infinity; ν controls read count overdispersion relative to a binomial distribution. In summary, the α i j incorporate information about the deletion status of the target gene, the relative abundance of each sample in the library, the rate of misclassification in the sequencing run, and the amount of overdispersion in read counts across samples.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^01148c3a]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Nonparametric tolerance intervals for non-normal distributions: The above estimate of the tolerance interval would only be applicable to a population that is normally distributed, but when the underlying population is often not normal (eg, when there is a natural boundary that the data cannot exceed (ie, 0% or 100%)) it is helpful to define tolerance intervals using nonparametric methods; the one-sided nonparametric tolerance interval can be determined by finding the value for k that satisfies the cumulative binomial equation, where CL is the confidence level (eg, 0.95), and by setting k = 0 (ie, 0 failures) the formula can be simplified.

---

### Input-output maps are strongly biased towards simple outputs [^50fca0dd]. Nature Communications (2018). Medium credibility.

Many systems in nature can be described using discrete input-output maps. Without knowing details about a map, there may seem to be no a priori reason to expect that a randomly chosen input would be more likely to generate one output over another. Here, by extending fundamental results from algorithmic information theory, we show instead that for many real-world maps, the a priori probability P(x) that randomly sampled inputs generate a particular output x decays exponentially with the approximate Kolmogorov complexity [Formula: see text] of that output. These input-output maps are biased towards simplicity. We derive an upper bound P(x)≲[Formula: see text], which is tight for most inputs. The constants a and b, as well as many properties of P(x), can be predicted with minimal knowledge of the map. We explore this strong bias towards simple outputs in systems ranging from the folding of RNA secondary structures to systems of coupled ordinary differential equations to a stochastic financial trading model.

---

### Accurately clustering biological sequences in linear time by relatedness sorting [^fdf1c40f]. Nature Communications (2024). High credibility.

Given that no sequences are clustered in phases 1 or 2, the objective of phase 3 is to establish linkage to a cluster representative as rapidly as possible. Each sequence only needs to be compared with a fixed number of sequences around it in the sequence ordering, resulting in a linear time clustering algorithm (Fig. 1L). If the sequence ordering is sufficiently out of order then potential links can be missed. To remedy these cases, Clusterize compares each sequence with sequences sharing the most rare k-mers, which are identified using the same process as in phase 1. This rare k-mer approach is analogous to that of Linclust except that sequences can be compared to multiple sequences per rare k-mer group rather than only one sequence per group. Calculation of k-mer similarity only needs to be performed on a fixed number of representatives (parameter C, by default 2000) corresponding to the clusters previously assigned to sequences that are nearby in the relatedness ordering or share rare k-mers. Clusterize keeps track of which approach (i.e. relatedness sorting or rare k-mers) results in the most clusters and proportionally draws more cluster representatives for comparison from the more rewarding approach.

Each phase requires up to a user-specified number of comparisons that each take linear time. In the end, a limited subset (parameter E, by default 200) of sequences with the highest k-mer similarity are aligned to obtain their percent identities. Therefore, the complete algorithm has time complexity O(A*N) + O(B*N) + O(C*N) + O(E*N) with respect to N. When A, B, C, and E are bounding constants, the asymptotic time complexity becomes O(N). The amount of memory required scales similarly, O(N), because only a fixed number of vectors of length N are required to store k-mers, distances, and orderings. As shown in Supplementary Fig. 1, parameters A, B, C, and E were optimized to minimize the number of clusters and time required using sequences from the RNAcentral database, which was chosen because of its large number of diverse sequences. Phase 3 often dominates clustering time and, accordingly, parameter C largely controls the trade-off between speed and accuracy. In contrast, parameters B and E typically have less effect because the algorithm converges prior to reaching the limit.

---

### Flexible use of limited resources for sequence working memory in macaque prefrontal cortex [^e30bd3e2]. Nature Communications (2025). High credibility.

We thus defined these vectorsas task variables, where = 1,2,3, = 1,2,…, and = 1,2,… ,6. In our model, the averaged neural response of the i th neuron in one trial during the late delay period (1000 ms before the onset of the "go" signal for M1 and M2, 500 ms before the onset of the "go" signal for M3 and M4) was assumed to be a linear combination of the task variables, such that,

are unknown regression coefficients andis the trial-by-trial noise. To account for the impact of varying trial numbers across sequence lengths, a weighted approach was incorporated into the loss function of Lassoglm algorithm for regression analysis. Specifically, for each distinct length s, where n s represents the number of trials in length s, individual trial weights w s were calculated as follows: Here, is the mean trial count of all lengths. The resulting w s values were then normalized across all trials so that they summed to 1. The optimization aimed to minimize this weighted loss while determining the regression coefficients. For the i th neuron, its neural responses with the same rank and length were centered to zero. A Lasso regularization term was added to the linear regression model to avoid overfitting, and the regularization amplitude with maximum likelihood was selected. We conducted this analyses using data from all correct trials. See the number of trials for each length in Table S1 - 2.

---

### Modelling sequences and temporal networks with dynamic community structures [^7129d625]. Nature Communications (2017). Medium credibility.

Bayesian Markov chains with communities

As described in the main text, a Bayesian formulation of the Markov model consists in specifying prior probabilities for the model parameters, and integrating over them. In doing so, we convert the problem from one of parametric inference where the model parameters need to be specified before inference, to a nonparametric one where no parameters need to be specified before inference. In this way, the approach possesses intrinsic regularisation, where the order of the model can be inferred from data alone, without overfitting.

To accomplish this, we rewrite the model likelihood, using eqs. (1) and (5), asand observe the normalisation constraints,… Since this is just a product of multinomials, we can choose conjugate Dirichlet priors probability densitiesand, which allows us to exactly compute the integrated likelihood, whereand. We recover the Bayesian version of the common Markov chain formulation (see ref.) if we put each memory and token in their own groups. This remains a parametric distribution, since we need to specify the hyperparameters. However, in the absence of prior information it is more appropriate to make a noninformative choice that encodes our a priori lack of knowledge or preference towards any particular model, which amounts to choosing α x = β rs = 1, making the prior distributions flat. If we substitute these values in eq. (19), and re-arrange the terms, we can show that it can be written as the following combination of conditional likelihoods, wherewithbeing the multiset coefficient, that counts the number of m -combinations with repetitions from a set of size n. The expression above has the following combinatorial interpretation: P ({ x t }| b, { e rs }, { k x }) corresponds to the likelihood of a microcanonical modelwhere a random sequence { x t } is produced with exactly e rs total transitions between groups r and s, and with each token x occurring exactly k x times. In order to see this, consider a chain where there are only e rs transitions in total between token group r and memory group s, and each token x occurs exactly k x times. For the first transition in the chain, from a memory x 0 in group s to a token x 1 in group r, we have the probabilityNow, for the second transition from memory x 1 in group t to a token x 2 in group u, we have the probabilityProceeding recursively, the final likelihood for the entire chain iswhich is identical to eq. (21).

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^59838d2f]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Sample size metrics — tolerance intervals versus confidence intervals: Although performance is often stated in terms of confidence intervals (CI), the CI of the mean only gives an estimate of the population mean with a stated level of confidence and does not give an indication of the performance of any given sample; to estimate the distribution of the underlying population and the performance of individual samples, the tolerance intervals should be used, with the lower tolerance interval for a normally distributed population determined as x̄ ± k × s, where s is the sample standard deviation (SD) and k is a correction factor, and for two-sided 95% confidence with n = 20 the k value is 2.75, approaching the z-score as the number of samples increases.

---

### The dynamics of higher-order novelties [^f6677587]. Nature Communications (2025). High credibility.

Here, we propose to go one step beyond and look at novelties as novel pairs, triplets, and higher-order combinations of consecutive symbols in a sequence. For instance, when exploring a network, a novel pair is represented by the first visit of a link. In order to measure the pace of discovery of these higher-order compounds starting from a sequence of events, we first create the surrogate sequence of overlapping pairs. Considering for example the sentence "One ring to rule them all", from the sequence of events{ one, ring, to, rule, them, all } we obtain the sequence of overlapping pairs{(one, ring), (ring, to), (to, rule), (rule, them), (them, all)}. Fromwe can then compute the number D 2 (t) of different pairs among the first t ones, with t ≤ T − 1. Notice that, in this manuscript, we consider the pairs (one, ring) and (ring, one) as two different pairs, i.e. order matters. By construction, we always have D 1 (t) ≤ D 2 (t) ≤ t, since, on the one hand, for each new element added tothere is a new pair in, and, on the other hand, there cannot be more than t different pairs among t items. From the power-law scaling, we can then extract the value of β 2, which we refer to as the 2nd- order Heaps' exponent. This definition can be naturally extended to any order n, considering the sequenceof consecutive overlapping n -tuples present in. Notice that, if, then. We can hence compute the number D n (t) of different tuples among the first t tuples in, and extract the n th- order Heaps' exponent β n ∈ [0, 1] from. Notice also that the n th-order Heaps' exponent can also be interpreted as the first order Heaps' exponent of a sequence whose events are the overlapping n -tuples of the original sequence. Finally, it is worth remarking that such an approach is close to the analysis of Zipf's law in linguistic data for n -grams or sentences. In this context, studies showed that as one moves from graphemes, to words, sentences, and n -grams, the Zipf's exponent (reciprocal of the Heaps' exponent for infinitely long sequences) gradually diminishes. This implies that n -grams or sentences are characterized by a larger novelty rate than words, a behavior analogous to what we have discussed above.

---

### Quantifying unobserved protein-coding variants in human populations provides a roadmap for large-scale sequencing projects [^3db7b451]. Nature Communications (2016). Medium credibility.

Methods

UnseenEst algorithm

UnseenEst uses the empirical SFS of a given class of variants in a cohort to estimate its frequency distribution in the population. The inputs into the algorithm are the number of alleles in the sample cohort, k, and the SFS, which is a set of counts, { F i }, where F i is the number of variants that are observed in exactly i out of the k alleles. A key challenge for the method is to accurately estimate the frequency distribution of variants that have empirical count of 0 (that is, they are not observed) in the cohort but are likely to have some small, non-zero frequency in the population.

More concretely, let X denote a discrete set of frequencies x in [0, 1] and let h (x) denote the fraction of all the variants with frequency x. UnseenEst estimates h (x) by finding the set of h (x) that jointly minimizes the value of the expression:

where bin(x, k, i) is the binomial probability of observing i heads in k independent flips of a coin with bias x. The intuition for minimizing this objective is as follows:is the number of variants that we expect to find in i out of k alleles in the cohort and F i is the empirical number of variants observed in i alleles. If h (x) is the true frequency distribution, then the expectation should be close to the empirical, which is why we want to find h (x) that minimizes the objective above. Given an estimate of the frequency distribution h (x), the expected number of unique variants in N alleles can be calculated by the formula.

The standard 3rd order jackknife estimator does not estimate the frequency distribution h (x). Instead, it directly estimates that the number of unique variants in N alleles is g 1 (N, k) F 1 +g 2 (N, k) F 2 +g 3 (N, k) F 3, where F 1, F 2, F 3 are the number of variants observed once, twice and three times in k alleles, and g 1, g 2, g 3 are specific functions of N and k derived from self-consistency requirements. Note that the confidence intervals of the jackknife in Supplementary Fig. 3 are very narrow because there is relatively little variation in the counts F 1, F 2, F 3, when the sample size is large.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^d3e55ccc]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Read depth planning example — confidence versus tolerance interpretation: For example, to determine the probability of getting a minimum of 250 reads for a given region, if a validation set shows a mean depth of coverage 275 reads and a standard deviation (SD) of 50 reads, after running 100 samples the 95% lower confidence limit indicates confidence that the average depth of coverage would be > 266, whereas the 95% lower tolerance interval indicates that for any given sample one could only be confident of reliably getting a read depth of 179 or greater.

---

### 2022 American society of anesthesiologists practice guidelines for management of the difficult airway [^7d4e88a3]. Anesthesiology (2022). High credibility.

ASA 2022 difficult airway expert consultant survey — unanticipated and emergency difficult airway management — documents agreement with decision points and safety measures, including waking/restoring spontaneous breathing (N 164; Strongly Agree (%) 64*, Agree (%) 23, Neutral (%) 10, Disagree (%) 2, Strongly Disagree (%) 1) and determining noninvasive versus invasive approaches (N 161; Strongly Agree (%) 62*, Agree (%) 30, Neutral (%) 5, Disagree (%) 2, Strongly Disagree (%) 1); if a noninvasive approach is selected, identifying a preferred sequence (N 164; Strongly Agree (%) 73*, Agree (%) 24, Neutral (%) 1, Disagree (%) 1, Strongly Disagree (%) 1) and, if difficulty occurs, using combination techniques (N 163; Strongly Agree (%) 66*, Agree (%) 26, Neutral (%) 6, Disagree (%) 1, Strongly Disagree (%) 1); being aware of passage of time, number of attempts, and oxygen saturation (N 162; Strongly Agree (%) 88*, Agree (%) 6, Neutral (%) 2, Disagree (%) 0, Strongly Disagree (%) 1); providing and testing mask ventilation between attempts (N 159; Strongly Agree (%) 59*, Agree (%) 25, Neutral (%) 9, Disagree (%) 7, Strongly Disagree (%) 1); limiting number of attempts (N 165; Strongly Agree (%) 83*, Agree (%) 12, Neutral (%) 4, Disagree (%) 1, Strongly Disagree (%) 1); when an invasive approach is necessary, identifying a preferred intervention (N 161; Strongly Agree (%) 76*, Agree (%) 20, Neutral (%) 2, Disagree (%) 1, Strongly Disagree (%) 1) and ensuring it is performed by an individual trained in invasive airway techniques (N 163; Strongly Agree (%) 83*, Agree (%) 14, Neutral (%) 2, Disagree (%) 1, Strongly Disagree (%) 1), with rapid performance (N 163; Strongly Agree (%) 67*, Agree (%) 23, Neutral (%) 7, Disagree (%) 2, Strongly Disagree (%) 1) and identification of an alternative intervention if the selected approach fails or is not feasible (N 163; Strongly Agree (%) 74*, Agree (%) 20, Neutral (%) 4, Disagree (%) 1, Strongly Disagree (%) 1).

---

### 2022 American society of anesthesiologists practice guidelines for management of the difficult airway [^c92561d2]. Anesthesiology (2022). High credibility.

ASA 2022 difficult airway expert consultant survey — anticipated difficult airway management — reports agreement with planning strategies, awake intubation triggers, noninvasive sequencing, and safety limits: strategy identification across awake, difficult ventilation, cannot ventilate or intubate, and alternatives had N 164 with Strongly Agree (%) 84*, Agree (%) 12, Neutral (%) 3, Disagree (%) 1, Strongly Disagree (%) 1; awake intubation for suspected difficult intubation with anticipated difficult ventilation had N 165 with Strongly Agree (%) 68*, Agree (%) 22, Neutral (%) 7, Disagree (%) 2, Strongly Disagree (%) 1; awake intubation when difficulty with emergency invasive airway rescue is anticipated had N 166 with Strongly Agree (%) 58*, Agree (%) 25, Neutral (%) 11, Disagree (%) 4, Strongly Disagree (%) 1; being aware of passage of time, number of attempts, and oxygen saturation had N 166 with Strongly Agree (%) 91*, Agree (%) 6, Neutral (%) 2, Disagree (%) 0, Strongly Disagree (%) 1; limiting the number of attempts at tracheal intubation or supraglottic airway placement had N 167 with Strongly Agree (%) 77*, Agree (%) 19, Neutral (%) 2, Disagree (%) 1, Strongly Disagree (%) 1; ensuring invasive airway procedures are performed by an individual trained in invasive airway techniques had N 166 with Strongly Agree (%) 83*, Agree (%) 15, Neutral (%) 2, Disagree (%) 0, Strongly Disagree (%) 1; and identifying an alternative invasive intervention if the selected approach fails or is not feasible had N 166 with Strongly Agree (%) 72*, Agree (%) 22, Neutral (%) 5, Disagree (%) 1, Strongly Disagree (%) 1.

---

### Standards and guidelines for validating next-generation sequencing bioinformatics pipelines: a joint recommendation of the Association for Molecular Pathology and the college of American pathologists [^101e0942]. The Journal of Molecular Diagnostics (2018). Medium credibility.

Minimum number of variants and calculation — For in-scope variants, they fall into one of three categories: SNVs, indels, and other; the last category includes variants that cannot be classified as SNVs or indels, such as vertically complex variants that are ≤ 21 bp in contiguous size or variants that require special algorithmic approaches; for each category in scope, the minimum number of variants to include in the validation sample set can be calculated as n = ln(1 – CI)/ln(P), where n is the number of variants to be included, CI is the confidence level of detection, and P is the probability of detection.

---

### Cutting edge or blunt instrument: how to decide if a stepped wedge design is right for you [^471f52f2]. BMJ Quality & Safety (2021). High credibility.

When might I consider doing a stepped wedge trial?

Research designs are shaped as much by practical constraints as by abstract schemes, and it is always a good idea to start with the constraints and work towards a design, rather than start with a design and try to fit it to constraints. These constraints will be unique to each research context, and box 1 lists some areas to think about. Still, there are some common features of settings where a stepped wedge trial might be considered as a possible design, and we now review these.

Box 1
Practical constraints on the design of a longitudinal cluster randomised trial

Are there limits on the time available to complete the evaluation, on the number of clusters, or on the number of participants (or the rate at which you can recruit participants) at each cluster? These constraints put limits on the overall scale of the evaluation, or force trade-offs between different design characteristics.
How will participants and their data be sampled in your study: as a series of cross-sectional surveys, as a continuous stream of incident cases, as a cohort followed over time, or some other way? Does the timescale divide into cycles, seasons or milestones that influence how you will sample participants and data?
Is there a limit on how many clusters can implement the intervention at the same time in the evaluation? If this is constrained by research resources (eg, if there are only enough trained research staff to implement the intervention one cluster at a time) then implementation must be staggered in some way.
If implementation is to be staggered, is there a minimum 'step length'? If the same team delivers the intervention in different clusters at different steps, then bear in mind it may take some time to get the intervention fully operational at a site, and the team will also need time to relocate from one cluster to the next.

---

### Discordant alternans as a mechanism for initiation of ventricular fibrillation in vitro [^975f6087]. Journal of the American Heart Association (2018). Low credibility.

In the first protocol, interstimulus intervals ending with premature stimuli were categorized as short, intermediate, or long (S, I, or L), as follows. The difference between cycle length (S n S n+1) and ERP (S n S n+1, min) of the interval between S n and S n+1 was defined as ΔS n S n+1 = S n S n+1 –S n S n+1, min, for n = 1, 2, 3, 4. Each cycle length (CL) that ended with a premature stimulus was classified as short (S), intermediate (I), or long (L), depending on the size of the ΔS n S n+1 value, that is, the amount by which the CL exceeded the ERP. The categories were defined as "S": 0 ≤ ΔS n S n+1 ≤ 5 ms, "I": 5 < ΔS n S n+1 ≤ 40 ms, and "L": ΔS n S n+1 > 40 ms. Hence, an S 2 –S 5 sequence with ΔS 1 S 2 = 5 ms, ΔS 2 S 3 = 50 ms, ΔS 3 S 4 = 0 ms, ΔS 4 S 5 = 2 ms, belongs to the category SLSS. "X" is a placeholder that refers to an interval of any length, thus XL includes all S 2 –S 3 sequences in categories SL, IL, and LL. Different combinations of premature stimulus sequences were applied to the tissue, and it was noted whether any of them induced sustained VF. For each preparation, we attempted to test examples from as many distinct sequence categories as possible, starting with SX and SLSX, and progressing through other sequence categories as time permitted. This sequence search was interwoven with the ERP identification protocol, both due to the need to reidentify ERPs when changes were made to the preceding intervals, and because searching for ERP values automatically led to applying different premature stimulus combinations to the tissue. The complete list of sequences that were tested, along with VF outcomes, is described in Data S1 and shown in Table S1.

---

### Optimal compressed representation of high throughput sequence data via light assembly [^664d3451]. Nature Communications (2018). Medium credibility.

Estimating

Our goal in this section is to justify the approximation in (1) for, the entropy of the set of reads sampled from a genome G, possibly containing read errors. To do this, we first make the assumption that the random process of sampling reads from G is independent of the read-error process that corrupts individual bases through substitutions, insertions, or deletions. In other words, ifdenotes the set of error-free reads (i.e. the reads incoincide with those in, but do not contain any read errors), then the genome G and the sampled reads (with errors)are conditionally independent given the corresponding error-free reads.

Using this assumption and elementary properties of entropy, we may writewhere H (⋅ | ⋅) denotes conditional entropy. As denoted, the first three terms in the expression forhave intuitive interpretations, and may be estimated in practice:
The first termis the entropy of the reads, given that the error-free readsare available to us. That is, is roughly equal to the number of bits needed to describe the read errors, if the error-free reads themselves were already known. For example, if the read-error process corresponds to corrupting each base independently with probabilityby replacing it randomly with one of the remaining three bases, we have, where N denotes the total number of reads, and L denotes their length (in bases).
The second termdenotes the entropy of the error-free reads, given that the sequence is known. In other words, this is the entropy of the random process of sampling reads from different locations in the genome. If we adopt the Poisson sampling model, in which reads are sampled uniformly at random from the genome, we have to first order the approximation

---

### Evaluation of the evenness score in next-generation sequencing [^1f5f1936]. Journal of Human Genetics (2016). Low credibility.

The evenness score (E) in next-generation sequencing (NGS) quantifies the homogeneity in coverage of the NGS targets. Here I clarify the mathematical description of E, which is 1 minus the integral from 0 to 1 over the cumulative distribution function F(x) of the normalized coverage x, where normalization means division by the mean, and derive a computationally more efficient formula; that is, 1 minus the integral from 0 to 1 over the probability density distribution f(x) times 1-x. An analogous formula for empirical coverage data is provided as well as fast R command line scripts. This new formula allows for a general comparison of E with the coefficient of variation (= standard deviation σ of normalized data) which is the conventional measure of the relative width of a distribution. For symmetrical distributions, including the Gaussian, E can be predicted closely as 1-σ(2)/2⩾E⩾1-σ/2 with σ ≤ 1 owing to normalization and symmetry. In case of the log-normal distribution as a typical representative of positively skewed biological data, the analysis yields E≈exp(-σ*/2) with σ*(2) = ln(σ(2)+1) up to large σ (≤ 3), and E≈1-F(exp(-1)) for very large σ (⩾2.5). In the latter kind of rather uneven coverage, E can provide direct information on the fraction of well-covered targets that is not immediately delivered by the normalized σ. Otherwise, E does not appear to have major advantages over σ or over a simple score exp(-σ) based on it. Actually, exp(-σ) exploits a much larger part of its range for the evaluation of realistic NGS outputs.

---

### Focal seizures induce spatiotemporally organized spiking activity in the human cortex [^3b3f592b]. Nature Communications (2024). High credibility.

We then created a matrix in rank space for each seizure in the following manner. First, we enumerated, in lexicographic order, the set of all 2-element subsets of micro-electrodes {1,…, n }, yielding the set of tuples p: = {(1, 2), (1, 3),…, (2, 3), (2, 4),…, (n − 1, n)}, of length. We then created a sequence matrix withcolumns, corresponding to the number of tuples in the set p, and k rows, corresponding to the number of combined bursts:Each column of this matrix, therefore, reflects a distinct pair of electrodes rather than a single electrode. We populated each value in each row with either a 0 or a 1, depending on whether the first member of the micro-electrode pair outranked the second in that particular burst. Hence, for each burst i containing a set of timestamps, and each pair of distinct electrode indices (j 0, j 1) ∈ p :This approach forces the dimensionality reduction procedure to consider relative rank information, rather than raw ordinal or continuous data. When considering null versions of sequences, we used the same procedure applied to a random version of a particular set of bursts. In these random sequences, we shuffled MUA timestamps at active electrodes were shuffled. We again in-filled missing values for normalized ranks with random values from the uniform random distribution on, prior to creation of A.

---

### Quantum behavior of the duffing oscillator at the dissipative phase transition [^8b22286e]. Nature Communications (2023). High credibility.

The first-order DPT

It is then natural to ask whether the Liouvillian gap can be closed at a particular parameter setting, where the system dynamics becomes infinitely slow and the two MSs become also SSs. However, this perception is in conflict with the uniqueness of the SS solution for the Duffing oscillator. Nevertheless, multiple SSs can exist in the driven-dissipative Bose-Hubbard model, where an infinite number of Duffing oscillators are coupled to each other and form a lattice. A bridge between the mean field description of an N -site Bose-Hubbard lattice and a single Duffing oscillator may be constructed by rescaling the nonlinearity and driving strength of the later as U → U 0 / N and. A thermodynamic limit of the Duffing oscillator is defined as N → ∞, where the Liouvillian gap is closed at a rescaled critical point, and results in a first-order DPT.

In our experiment, we in situ tune the scaling factor from approximately N = 2 to 11, and measure the average photon number of the SS for varying driving strength. Here, we define U 0 ≡ − γ for N = 1 and fix the detuning at Δ = 3 γ without loss of generality. As shown in Fig. 3 a, the transition happens at the same rescaled critical driving strength, for different N, and the photon density also saturates at a similar value of around 〈 a † a 〉/ N = 2.5. However, the transition process becomes sharper with increasing N. Fitting the data by a linear function, we obtain ∂(〈 a † a 〉/ N)/∂(ξ 0 /2 π) = 3.8, 5.8, and 8.5 MHz −1 for N = 6.1, 7.9, and 10.5, respectively, as shown in Fig. 3 b. This observed tendency indicates a sudden change of the photon density on the two sides ofin the thermodynamic limit, known as the first-order DPT. It implies a closed Liouvillian gap, equivalently, a diverging lifetime of the MSs, when approaching the critical point. This is consistent with the observed Liouvillian gap in Fig. 2 c.

---

### Strange data: when the numbers just arent't normal [^b856e8dd]. The Journal of Foot and Ankle Surgery (2015). Low credibility.

Many statistical tests assume that the populations from which we draw our data samples roughly follow a given probability distribution. Here, I review what these assumptions mean, why they are important, and how to deal with situations where the assumptions are not met.

---

### Opportunities and challenges associated with clinical diagnostic genome sequencing: a report of the Association for Molecular Pathology [^434309ee]. The Journal of Molecular Diagnostics (2012). Medium credibility.

Clinical diagnostic genome sequencing — consensus calling of heterozygous positions depends on sampling depth because each molecule is sequenced independently; the probability of detecting a second allele is dictated by the sampling probability, and if the second allele must be present in at least 20% of calls for a heterozygous consensus, the position needs to be sampled at least 18 times to be at least 99% confident of detecting both alleles.

---

### Mind your p values [^e7f228df]. The Journal of Foot and Ankle Surgery (2012). Low credibility.

In a medical study, the goal is to convince readers that a compelling argument has been made to prove the investigators' case. The logic of how this proof is provided in medical studies can be confusing, and is discussed here in simple terms.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^d82e5daf]. The Journal of Molecular Diagnostics (2017). Medium credibility.

NGS oncology panel validation — sample size calculations and rule of three: Required validation samples can be calculated from r^n = α with n = ln(α)/ln(r); for 95% confidence (α = 0.05) and at least 95% reliability (r = 0.95) the minimum number of samples is 59. For a 95% confidence level, the rule of three applies: counting 100 cells with no positives allows a claim that < 1% of cells have the event, but to be 95% confident that < 1% of cells have it, 300 cells with no positives are needed; to claim reliability ≥ 95% with 95% confidence requires 3 × 20 or 60 samples.

---

### Six steps in quality intervention development (6SQuID) [^221654f2]. Journal of Epidemiology and Community Health (2016). Low credibility.

Conclusion

In order to improve the effectiveness of public health interventions, a systematic approach to intervention development is required, as well as rigorous evaluation. However, little practical guidance exists for public health practitioners and researchers that explains the essential stages of intervention development. We argue that this process can be broken down into six key steps: defining and understanding the problem; identifying modifiable determinants; deciding on the mechanisms of change; clarifying how these will be delivered; testing and adapting the intervention; and collecting initial evidence of effectiveness. This model imposes somewhat arbitrary cut-offs in the process of intervention development and suggests a linear progression. In practice developers often return to an earlier step in the sequence before reaching step 6, and subsequently 'definitive trials' can lead to further revisions of the intervention. However, we hope that if each of these six steps is carefully addressed in the design of interventions better use will be made of scarce public resources by avoiding the costly evaluation, or implementation, of unpromising interventions.

What is already known on this subject?

There is little practical guidance for researchers or practitioners on how best to develop public health interventions. Existing models are generally orientated towards individual behaviour change and some are highly technical and take years to implement.

What this study adds?

This paper provides a pragmatic six-step guide to develop interventions in a logical, evidence-based way to maximise likely effectiveness. If each step is carefully addressed, better use will be made of scarce public resources by avoiding the costly evaluation, or implementation, of unpromising interventions.

---

### Beyond the P: II: precluding a puddle of P values [^1cd5f00e]. Journal of Cataract and Refractive Surgery (2004). Low credibility.

This is the second in a 4-part series discussing the proper use, interpretation, and limitations of P values.

---

### Do CIs give you confidence? [^0c65ec35]. Chest (2012). Low credibility.

This article describes the conceptual basis for the P value and the CI. We show that both are derived from the same underlying concepts and provide useful, but similar information.

---

### High-throughput sequencing in respiratory, critical care, and sleep medicine research. An official American Thoracic Society workshop report [^b782fe2f]. Annals of the American Thoracic Society (2019). High credibility.

Recommendations for design and analysis of next-generation sequencing studies — design scope and phenotyping — emphasize that general principles of genetic epidemiology study design are especially important in next-generation sequencing studies, with disease-specific studies potentially enabling more detailed phenotyping while population studies may allow larger sample sizes; investigators must clearly define the phenotypes of both cases and control subjects, including relevant exposures; because relying only on datasets largely composed of individuals of European ancestry will limit discoveries, we suggest expanding sequencing efforts in subjects of different ancestries; studies should consider broad consent for secondary data use; and researchers should use standardized methods of experimental design and data analysis for exome and whole-genome sequencing studies.

---

### A new next-generation sequencing approach in human cytomegalovirus for the identification of antiviral resistance mutations and genotypic classification [^6f5f5218]. Journal of Medical Virology (2025). Medium credibility.

2.10 Measurement of the Limit of Detection (LoD)

To determine the LoD of the sequencing technique, a 1:3 dilution series was prepared in triplicate from clinical plasma samples, covering viral loads between 600 and 4 000 000 copies/mL. The LoD was defined as the lowest viral load at which all target genes were successfully sequenced with predefined quality metrics: N content < 5%, coverage > 95%, and median depth ≥ 100×, following criteria adapted from Mallory et al.

Four clinical plasma samples with viral loads above 1 000 000 IU/mL were pooled after extraction, using 500 µL input per sample and obtaining 50 µl of eluate using EMAG platform (BioMérieux, France) to ensure sufficient volume. The resulting eluate was serially diluted 1:3 in triplicate and quantified with the AltoStar CMV PCR Kit 1.5 (Altona Diagnostics, Germany), calibrated to the 1st WHO International Standard for HCMV (NIBSC 09/162). Each dilution was sequenced in triplicate; replicate 1 was processed on 1 day, and replicates 2 and 3 on another.

Additionally, five clinical HCMV‐positive plasma samples (3000–310 000 IU/mL) collected in July 2024 were sequenced to validate the method's applicability to contemporary strains to confirm technique's robustness and relevance for real‐world clinical samples, addressing concerns about the use of older or long‐stored material in analytical validation studies.

---

### Opportunities and challenges associated with clinical diagnostic genome sequencing: a report of the Association for Molecular Pathology [^891b5a9a]. The Journal of Molecular Diagnostics (2012). Medium credibility.

Informed consent and ethical considerations in clinical genome sequencing emphasize making patients aware of potential benefits and harms, recognizing that incidental findings may suggest risk for unsuspected disease. The capacity to perform large-scale sequencing on the human genome presents unique challenges for informed consent, particularly in deciding the level of detail to share, and no specific guidance exists with institutions devising their own policies. Although genetic nondiscrimination laws exist at both state and federal levels, with genome testing by next-generation sequencing (NGS) the perceived and real potential risks are magnified compared with tests that target only one gene at a time. A patient undergoing genome sequencing will likely learn about sequence variants of unknown clinical significance or other sequence changes indicating disease risk unrelated to the original testing indication. With NGS the entire sequence is read and changes that are not clinically understood cannot be blocked out. Solutions highlighted include counseling and education before testing, informed consent with a clear explanation of current limits, maintenance of privacy and confidentiality, and sensitivity to culture within families, their heritage, and their communities.

---

### Detecting and phasing minor single-nucleotide variants from long-read sequencing data [^414c90f8]. Nature Communications (2021). High credibility.

Intuitively, in Eq. (5), the numerator is the number of reads containing substitution x k and the p other substitutions, and the denominator is the number of reads that contain the p other substitutions and cover locus k. The p loci, g 1, g 2. and g p are called dependent loci. As. and p in Eq. (5) are unknown, the estimated maximal conditional probability of event { X k = x k } given p other substitutions is used to detect SNVs and is formally defined by

The substitution x k is detected as a real SNV if H (x k) is larger than a threshold (0.65 in this study). H (x k) is also called maximal conditional substitution rate. To avoid high variance of the estimated(Eq. 5), we require that, and v min = 25 in this study. Sequencing errors at multiple loci that are very close to each other might induce slightly dependent substitutions. To avoid the impact of dependent substitutions induced by sequencing errors, we require that locus k and loci g 1, g 2. g p are not too close. Specifically, we require HD(k, g s) ≥ 15 for any g s ∈ { g 1, g 2. g p }. HD(k, g s) is the homopolymer distance between locus k and locus g s, and is defined as the number of homopolymers between the two loci. A homopolymer is a set of consecutive identical bases, and a base with no identical adjacent bases is also defined as a special homopolymer with size equal to 1.

It is computationally infeasible to enumerate all combinations of p loci to estimate H (x k) in Eq. (6). It is important to note that it is insufficient to detect SNVs accurately by restricting the number of dependent loci p to a certain number. In the example shown in Supplementary Fig. 1, H (x k) fails to detect the real SNVs if p is restricted to 1. Likewise, we can also have similar examples if p is restricted to another number greater than 1. In this work, we developed an algorithm called RSM that can estimate H (x k) efficiently without restricting p.

Detecting SNVs by RSM algorithm

---

### Detecting and phasing minor single-nucleotide variants from long-read sequencing data [^a63cace9]. Nature Communications (2021). High credibility.

Theoretical accuracy of RSM algorithm

Without the loss of generality, we denoteas the only set of substitutions that maximizes, and Ω as the set of subspaces containing. For a subspace C t ∈ Ω, the probability that the greedy algorithm findsis denoted as, where H k is defined by Eq. (6). The probability that RSM algorithm findsis

Assuming, and according to the chain rule of joint probability, whereif C t ∉ { C t −1, C t −2. C 1 }. As sequencing depth increases, ∣Ω∣ increases, andconverges to 0. Thus,(Eq. (11)) converges to 1 as sequencing depth increases. Intuitively, with infinite sequencing depth, RSM algorithm is guaranteed to detect real SNVs correctly if these SNVs have larger maximal conditional probabilities than sequencing errors.

Detecting orphan SNVs by correcting sequence-context effect

As RSM algorithm requires multiple real SNVs, it cannot detect orphan SNVs. An orphan SNV is the only SNV of the genome. We have to rely on the single-locus algorithm described in Eq. (4) to detect orphan SNVs. However, the substitution rate of a locus is not only affected by real SNVs but also affected by the sequence context of the locus. We built a gradient boostingmodel to learn the sequence-context effect and corrected it by the following likelihood ratio method (Fig. 2 d). For a substitution x k at locus k, its likelihood ratio iswhere Binomial(x; n, p) is the probability mass function of binomial distribution with parameters n and p, and

The substitution x k is detected as a SNV if L R (x k) is larger than a threshold. We used a threshold of 50 in this work. Calculation of p 0 is introduced in the next section. To reduce FDR, we also required a detected SNV has a substitution rate higher than 0.1 for PacBio data and 0.2 for ONT data respectively.

---

### Sequence-to-sequence models with attention mechanistically map to the architecture of human memory search [^3dec2ee1]. Communications Psychology (2025). Medium credibility.

The intermediate training evaluations of the seq2seq model exhibit similar qualitative patterns as are typically observed in human participants

Only a small proportion of top-performing human participants can demonstrate the exact behavior of the optimal policy. Averaged human recall behavior in free recall experiments (dashed line in Fig. 4 D–F; reproduced from Kahana et al.) also exhibits recency (enhanced end-of-list recall) and backward contiguity (tendency toward shorter than longer backward lags), in addition to what is amplified in the optimal behavior with primacy (enhanced beginning-of-list recall) and forward contiguity (tendency toward shorter than longer forward lags). To assess sub-optimal model behavior, we evaluate the recall behavior of several intermediate checkpoints taken of the model throughout the optimization process (Fig. 4 D–F). Characterizing these intermediate training stages provides insight into how recall strategies emerge and change as the model learns. We show that while the fully optimized seq2seq model with attention demonstrates the same behavior as the optimal policy of CMR, exhibiting both primacy and forward contiguity (Fig. 4 A–C), intermediate training evaluations of the model (lighter green lines) exhibit recency (higher recall probability at the end of list in Fig. 4 D in early Epochs 0–2) and backward contiguity (positive transition probabilities at negative lags in Fig. 4 F) similar to those observed in averaged human recall behavior. As training progresses, the model quickly loses recency and backward contiguity, increasingly depending on forward recalls that initiate recall from the beginning of the sequence. This trend is made more evident in Fig. 4 G–I, which displays the change in the model's tendency to initiate recall from the end (the last three items of the list; Fig. 4 G), to initiate recall from the beginning of the list (the first three items of the list; Fig. 4 H), and to recall items in the backward direction (conditional response probability with -1 lag; Fig. 4 I). Although model behavior during intermediate training evaluations is partially influenced by model initialization, we find that patterns such as recency and backward contiguity reliably appear across training runs and initializations, suggesting that these are stable features rather than idiosyncratic fluctuations.

---

### Privacy risks from genomic data-sharing beacons [^ac19735e]. American Journal of Human Genetics (2015). Low credibility.

Binomial Test

The null hypothesis is rejected if Λ < t for some threshold t. Let t α be such that P (Λ < t α | H 0) = α. This is equivalent to rejecting the null hypothesis if, where.

Because the x i are independent and identically distributed (i.i.d.) under both hypotheses,… Therefore, the power of the exact test can be calculated as, whereis chosen such that.

A sufficient statistic for the LRT is the number of "yes" responses from the beacon.

Relationship between the Number of Queries Required and Beacon Size

In the null and alternative hypotheses, x i is a Bernoulli random variable. Therefore, by the central limit theorem, the LRT statistic has a Gaussian distribution. We can therefore use the parameters of the Gaussian distribution to obtain a relationship between the number of queries (required for achieving a desired power and false-positive rate) and the number of individuals in the beacon.

Let μ 0 and σ 0 be the mean and SD, respectively, of the LRT statistic under the null hypothesis, and let μ 1 and σ 1 be the corresponding values under the alternative hypothesis.

For an LRT statistic with false-positive rate α, power 1 − β, and a normal distribution, we have thatwhere z y is the y quantile of the standard normal distribution.

For the LRT we describe, this relationship is equivalent to(see Gaussian LRT Power Approximation in Appendix A). The right-hand side of the equation is independent of both n and N for a specified false-positive rate α and power 1 − β. Thus, we have that.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^6dc7994e]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Library complexity in NGS oncology panel validation: At a specific depth of sequence coverage, the number of independent DNA template molecules has an independent impact on variant detection, and the number of unique genome equivalents sequenced by the assay is referred to as the library complexity; for example, the information content of 1000 sequence reads derived from only 10 genome equivalents is less than that from 100 different genome equivalents. Quantitation of input nucleic acid by mass can provide an unreliable estimate of library complexity; measurement is straightforward in a hybrid capture–based assay because sequence reads have different 5' and 3' termini, whereas in amplification-based methods all amplicons have identical 5' and 3' termini, and dilution experiments during the O&F phase can provide data. We recommend that the library complexity should be evaluated by dilution experiments using various amounts of DNA input during assay O&F phase or by other methods involving unique molecular identifiers as per discretion of the laboratory director.

---

### Quantifying unobserved protein-coding variants in human populations provides a roadmap for large-scale sequencing projects [^7150e901]. Nature Communications (2016). Medium credibility.

As new proposals aim to sequence ever larger collection of humans, it is critical to have a quantitative framework to evaluate the statistical power of these projects. We developed a new algorithm, UnseenEst, and applied it to the exomes of 60,706 individuals to estimate the frequency distribution of all protein-coding variants, including rare variants that have not been observed yet in the current cohorts. Our results quantified the number of new variants that we expect to identify as sequencing cohorts reach hundreds of thousands of individuals. With 500K individuals, we find that we expect to capture 7.5% of all possible loss-of-function variants and 12% of all possible missense variants. We also estimate that 2,900 genes have loss-of-function frequency of < 0.00001 in healthy humans, consistent with very strong intolerance to gene inactivation.

---

### Opportunities and challenges associated with clinical diagnostic genome sequencing: a report of the Association for Molecular Pathology [^05d2857b]. The Journal of Molecular Diagnostics (2012). Medium credibility.

Analytical NGS considerations — regulation and validation note that before any new laboratory-developed test is used for patient care, a clinical laboratory undertakes analytical validation, but Food and Drug Administration–approved/cleared tests using next-generation sequencing (NGS) do not exist, so laboratories will for the foreseeable future rely on Clinical Laboratory Improvement Amendment regulations. The purpose of assay validation for a laboratory-developed test is to document that the targeted analyte(s) can be detected in a robust and consistent manner, and in accordance with Clinical Laboratory Improvement Amendment regulations the following analytical characteristics must be documented: accuracy, precision, analytical sensitivity, analytical specificity, reportable range, reference intervals, and any other required performance characteristic. Sequences from NGS are not truly complete because of gaps, GC-rich regions, and bioinformatic limitations regarding indel calling. For nucleic acid sequencing, the standard comparator is traditional Sanger sequencing whose technical capabilities are dwarfed by NGS, and it is practically not feasible to perform Sanger sequencing of whole genomes for reasons of reagent costs, labor, and efficiency of both testing and analysis. All steps of NGS need to be evaluated, including sample library preparation, clonal fragment amplification, the sequencing itself, and data analysis, and quality control metrics need to be determined. Open questions include whether sequencing and documenting representative portions or comparison to online reference sequences would suffice, and what reference materials (RMs) are needed to make validation efficient and meaningful.

---

### A sample selection strategy for next-generation sequencing [^8ab2ae1a]. Genetic Epidemiology (2012). Low credibility.

Next-generation sequencing technology provides us with vast amounts of sequence data. It is efficient and cheaper than previous sequencing technologies, but deep resequencing of entire samples is still expensive. Therefore, sensible strategies for choosing subsets of samples to sequence are required. Here we describe an algorithm for selection of a sub-sample of an existing sample if one has either of two possible goals in mind: maximizing the number of new polymorphic sites that are detected, or improving the efficiency with which the remaining unsequenced individuals can have their types imputed at newly discovered polymorphisms. We then describe a variation on our algorithm that is more focused on detecting rarer variants. We demonstrate the performance of our algorithm using simulated data and data from the 1000 Genomes Project.

---

### A last-in first-out stack data structure implemented in DNA [^a022c217]. Nature Communications (2021). High credibility.

For the five 40-operation sequences tested, the model stipulates that a strict washing efficiency in the range 0 < μ < 0.03, 0 < ϕ 0 < 0.02 is required for storage and correct popping of all 20 signals in all cases (largely dictated by seq20). It should be highlighted that certain pathological sequences like seq1 (Supplementary Note 10.2) are insensitive to increasing ϕ 0: in this case, the non-specific leak of supernatant species through the wash actually helps produce the correct read out at the next step. To some extent, this is also true for the seq5 sequence used. We note that seq5 sequences with different arrangements of X 's and Y 's can be constructed and may perform differently to the seq5 sequence chosen. As a rough estimate, our estimated experimental washing efficiency W1 is predicted to achieve an average of 3.4 signals correctly popped per operations sequence, before output can no longer be discerned. This value generally agrees with our experimental observations of the two signal DNA stack system (see below).

---

### Spreading and scaling up innovation and improvement [^59491eef]. BMJ (2019). Excellent credibility.

Implementation science: spread and scale-up as structured improvement

Implementation science, defined as "the scientific study of methods to promote the systematic uptake of research findings and other evidence based practices into routine practice" (page 2), developed from the evidence based medicine movement in Europe and North America. It is perhaps best known for the sequential, structured (and somewhat top-down) method of spreading focused improvement techniques.

The first phase of this approach (after initial set-up and orientation) is the development of a clearly defined intervention, the components of which are optimised to reflect the evidence base (especially relating to how to change individual behaviour) (fig 1). There is then a small scale trial of this intervention in one or a few selected settings, followed by a systematic effort to replicate it in other settings, partly by identifying and dealing with barriers (which get in the way of the implementation effort) and facilitators (which potentially support it).

Fig 1
Rapid cycle test of change model of spread used in implementation science. Drawing on insights and a previous diagram in a review by Barker

Patient input can be harnessed very productively in this effort, though careful attention needs to be paid to power dynamics, the kinds of data that are collected, and how and by whom those data are analysed.

Although the sequence depicted in figure 1 is often promoted as the key to quality improvement, one systematic review showed that nearly half of all successful scale-up initiatives had not followed it.

Implementation science approaches tend to draw heavily on quality improvement methodology. Barker and colleagues describe this methodology as an "engine" that uses rapid cycle change to drive spread of an innovation, with some potential to adapt to different contexts.

In recent years, implementation science has matured as a field in a way that has paralleled developments in the Medical Research Council's guidance for developing and testing complex interventions. Both have shifted from a highly structured and narrowly experimental approach based on mechanical logic (which emphasised standardisation and replicability) to a more adaptive approach that recognises the need to think flexibly, understand and respond to local context, use qualitative methods to explore processes and mechanisms, and adapt the intervention to achieve best fit with different settings. This shift resonates with the complexity science approach described in the next section.

---

### ACOEM practice guidelines: elbow disorders [^51466a24]. Journal of Occupational and Environmental Medicine (2013). Medium credibility.

Summary of recommendations — basis and use: These recommendations are based on critically appraised higher quality research evidence and on expert consensus only when higher quality evidence was unavailable or inconsistent, and the more detailed indications, specific appropriate diagnoses, temporal sequencing, prior testing or treatment, and contraindications — which are elaborated in more detail for each test or treatment in the guideline — should be reviewed for recommendations in clinical practice or medical management; these recommendations are not simple "yes/no" criteria, and the evidence supporting them is in nearly all circumstances developed from typical patients, not unusual situations or exceptions.

---

### Neurocomputational dynamics of sequence learning [^56c2a1dd]. Neuron (2018). Low credibility.

The brain is often able to learn complex structures of the environment using a very limited amount of evidence, which is crucial for model-based planning and sequential prediction. However, little is known about the neurocomputational mechanisms of deterministic sequential prediction, as prior work has primarily focused on stochastic transition structures. Here we find that human subjects' beliefs about a sequence of states, captured by reaction times, are well explained by a Bayesian pattern-learning model that tracks beliefs about both the current state and the underlying structure of the environment, taking into account prior beliefs about possible patterns in the sequence. Using functional magnetic resonance imaging, we find distinct neural signatures of uncertainty computations on both levels. These results support the hypothesis that structure learning in the brain employs Bayesian inference.

---

### Opportunities and challenges associated with clinical diagnostic genome sequencing: a report of the Association for Molecular Pathology [^ddc6fcf3]. The Journal of Molecular Diagnostics (2012). Medium credibility.

Clinical diagnostic genome sequencing — alignment workflows often use a tiered approach: sequences aligning on a subset of the read are assigned a quality score based on mismatches to the reference, nonplaced reads undergo multistep gapped alignment, and probability values are assigned across solutions with the best alignment kept and scored on the Q scale; this may require use of more than one aligner, and for clinical applications, conservative approaches are most appropriate because high confidence should exist when making a call.

---

### A last-in first-out stack data structure implemented in DNA [^a299e2f0]. Nature Communications (2021). High credibility.

Model predictions of DNA stack with two signal types

Next, we extrapolated the parameterised model to exhaustively predict the (ideal) operation of the DNA stack with two signal types X and Y, this time over longer sequences of recording and popping operations.

The model was subject to five operation sequences each containing 20 record operations and 20 pop operations in total (Supplementary Note 10.1). In every case, approximately equal numbers of X and Y signals were recorded, and the stack always finished empty (under ideal operation). Four sequences, denoted seqN, were periodic, with N records always followed by N pops for N = 1, 5, 10, 20. One sequence, denoted seqR, arranged the 20 record and 20 pop operations randomly. Additionally, we tested the model with three increasingly stringent washing procedures. Procedure W1 was our experimental protocol (μ = 0.1, ϕ 0 = 0.33), procedure W2 was approximately twice as efficient (μ = 0.05, ϕ 0 = 0.15) and W3 was approximately twice as efficient again (μ = 0.02, ϕ 0 = 0.05).

For all operations sequences and all washing efficiencies, the model suggested that instead of detecting the most common stack species in the system, the most robust way to read out signals stored was to detect the majority signal popped into supernatant (Xr or Yr) following the addition of read strands (Supplementary Note 10.1). The model suggested that under imperfect washing, the stack population actually becomes de-synchronised from the target stack structure quickly (faster for W1 than for W3, as expected). However, despite this de-synchronisation of the stack population, the majority popped signal in supernatant remains correct for some time because the signals popped into supernatant derive from only the last signals stored on stacks in the population. That is, the stack population is simply required to have the majority of stacks terminating with the correct end signal, it is not required for all stacks in the population to be identical and synchronised (a much stricter condition). The fact that majority popped signal is the most robust system read out is indeed convenient, since the popped Xr and Yr complexes would be how the DNA stack is eventually linked to downstream nucleic acid circuits.

---

### A signal model for fat-suppressed T-mapping and dynamic contrast-enhanced MRI with interrupted spoiled gradient-echo readout [^5b4e8d3a]. NMR in Biomedicine (2025). Medium credibility.

5.2 Limitations

There were some limitations to this work. First, no correction for inhomogeneities in the B1 + ‐field was performed. Previous research has shown that this significantly improves the accuracy of VFA T 1 ‐mapping, especially at 3 T. Optimization of B1 + − field mapping proved a challenge on two scanners, which was also noted in other research. This was the main reason for not performing the correction. Additionally, the effect of B1 + ‐inhomogeneities is expected to be smaller in I‐SPGR‐type sequences compared to SPGR (Figure S2), so we do not expect a bias in our comparison of both models.

Secondly, it must be noted that our approach is only valid in sequences where the preparation pulses in the temporal gap do not affect the magnetization of the spins of interest. Thus, preparations such as inversion pulses, which would result in a more MP2RAGE‐type of sequence, are not included. In the future, other acquisition schemes may also be tested to estimate T 1, for example by varying parameters k, n, or the temporal gap length instead of the FA.

Thirdly, the signal model assumes that image contrast is only determined by data acquired in the center of the k‐space. In practice, however, image contrast is also determined by data acquired around the k‐space center. This implies that the effect of the temporal gap can be over‐ and underestimated, depending on which value for n is chosen. In the future, strategies for compensating for this effect can be investigated, for example by assuming an empirically determined "effective" n at which contrast is defined, instead.

Fourthly, we used the T 1 measured with a conventional VFA sequence as reference values in vivo, while inversion‐recovery‐based T 1 can also be used. We made this choice because other T 1 ‐mapping methods can introduce new biases that do not arise from the use of an incorrect signal model, but are inherent to the method. The proposed signal model should describe the I‐SPGR sequence in a similar fashion as the conventional signal model applies to an SPGR sequence, and hence we chose the conventional SPGR acquisition as a reference.

---

### Detecting and phasing minor single-nucleotide variants from long-read sequencing data [^09c64cc5]. Nature Communications (2021). High credibility.

Methods

Leveraging multiple loci to detect SNVs

For the i th aligned read, we encode its substitution at locus k of the reference genome by the following formula:where r i k is the base (short for nitrogenous base) of the i th aligned read at locus k, t k is the base at locus k of the reference genome and ϵ is an empty element, which is formally defined by. The first locus of the reference genome is 0 throughout this paper unless otherwise stated. The i th read is represented as a set of substitutions and its covering range (Fig. 2 a) and is denoted by b i and e i are the start and end loci of the region covered by the read, respectively, and S i is

The most intuitive way to detect SNVs is to use the substitution rate of each locus. Formally, we denote the encoded substitution at locus k as a random variable X k, and denote probability of the event { X k = x k } as P r (X k = x k), where x k ∈ {4 k, 4 k + 1, 4 k + 2, 4 k + 3}. Substitution rate is defined as the estimated P r (X k = x k), which iswhere {⋅} is a set and ∣⋅∣ is the number of elements in a set. Intuitively, in Eq. (4), the numerator is the number of reads with substitution x k at locus k, and the denominator is the number of reads covering locus k. Due to the high error rate of long-read sequencing data, it is inaccurate to detect minor variants using substitution rate alone (Fig. 1 b). Herein, we leverage the information of multiple loci to increase the detection accuracy. Assuming sequencing errors are independent with each other, real SNVs are likely to be present if there are multiple reads containing the same set of substitutions (Fig. 1 a). The conditional probability of { X k = x k } given other real SNVs of the same genome is therefore much larger than the marginal probability of { X k = x k } if x k is a real SNV, because these real SNVs are positively dependent (Fig. 1 a, c). Formally, the conditional probability of event { X k = x k } given p other substitutions is defined as, which is estimated by

---

### Technical update on HIV-1 / 2 differentiation assays [^50eb7933]. CDC (2016). Medium credibility.

HIV-1/HIV-2 differentiation testing — alternatives when the FDA-approved antibody differentiation immunoassay cannot be used states that there may be circumstances under which a laboratory is unable to adopt the FDA-approved HIV-1/HIV-2 antibody differentiation immunoassay, and in this situation a laboratory has alternatives for that step in the algorithm, some of which could delay turnaround time for test results. Send specimens to another laboratory that offers the FDA-approved supplemental HIV antibody differentiation assay. Refer to CDC/APHL laboratory testing guidance section I, "Alternative Testing Sequences When Tests in the Recommended Algorithm Cannot be Used".

---

### The case for open computer programs [^0f6e8786]. Nature (2012). Excellent credibility.

Scientific communication relies on evidence that cannot be entirely included in publications, but the rise of computational science has added a new layer of inaccessibility. Although it is now accepted that data should be made available on request, the current regulations regarding the availability of software are inconsistent. We argue that, with some exceptions, anything less than the release of source programs is intolerable for results that depend on computation. The vagaries of hardware, software and natural language will always ensure that exact reproducibility remains uncertain, but withholding code increases the chances that efforts to reproduce results will fail.

---

### Next-generation sequencing data interpretation: enhancing reproducibility and accessibility [^b2cc79ca]. Nature Reviews: Genetics (2012). Medium credibility.

Areas of life sciences research that were previously distant from each other in ideology, analysis practices and toolkits, such as microbial ecology and personalized medicine, have all embraced techniques that rely on next-generation sequencing instruments. Yet the capacity to generate the data greatly outpaces our ability to analyse it. Existing sequencing technologies are more mature and accessible than the methodologies that are available for individual researchers to move, store, analyse and present data in a fashion that is transparent and reproducible. Here we discuss currently pressing issues with analysis, interpretation, reproducibility and accessibility of these data, and we present promising solutions and venture into potential future developments.

---

### Opportunities and challenges associated with clinical diagnostic genome sequencing: a report of the Association for Molecular Pathology [^ac422a46]. The Journal of Molecular Diagnostics (2012). Medium credibility.

Clinical diagnostic genome sequencing — base-calling quality scoring uses a Phred score generated on a logarithmic scale (Q = − 10 log E) to estimate error probability, where a Phred of 10 corresponds to a 1/10 probability of error and a Phred of 20 corresponds to 1/100. For sequencing information, the Clinical Laboratory Standards Institute has recommended filtering on a Phred score of 20 to 30, resulting in an error rate of 1/100 to 1/1000, and when interrogating variants in population-level situations such as microbes or tumor samples, higher thresholds may be necessary.

---

### Two-phase stratified sampling designs for regional sequencing [^1a8d2da2]. Genetic Epidemiology (2012). Low credibility.

By systematic examination of common tag single-nucleotide polymorphisms (SNPs) across the genome, the genome-wide association study (GWAS) has proven to be a successful approach to identify genetic variants that are associated with complex diseases and traits. Although the per base pair cost of sequencing has dropped dramatically with the advent of the next-generation technologies, it may still only be feasible to obtain DNA sequence data for a portion of available study subjects due to financial constraints. Two-phase sampling designs have been used frequently in large-scale surveys and epidemiological studies where certain variables are too costly to be measured on all subjects. We consider two-phase stratified sampling designs for genetic association, in which tag SNPs for candidate genes or regions are genotyped on all subjects in phase 1, and a proportion of subjects are selected into phase 2 based on genotypes at one or more tag SNPs. Deep sequencing in the region is then applied to genotype phase 2 subjects at sequence SNPs. We investigate alternative sampling designs for selection of phase 2 subjects within strata defined by tag SNP genotypes and develop methods of inference for sequence SNP variant associations using data from both phases. In comparison to methods that use data from phase 2 alone, the combined analysis improves efficiency.

---

### Sequence-to-sequence models with attention mechanistically map to the architecture of human memory search [^4522bd39]. Communications Psychology (2025). Medium credibility.

Fig. 4
Optimized and intermediate training of the Seq2Seq model with attention.

The fully optimized seq2seq model with attention (128-Dim) shows behavioral patterns that closely align with those of the rational-CMR model across three sets of free recall patterns: serial position curve (A), probability of first recall (B), and conditional response probability (C). Rational-CMR results are reproduced from Zhang et al. D – F Recall characteristics are evaluated for intermediate epochs of training and compared qualitatively to the average participant recall from the PEERS dataset (N = 171). Intermediate model results exhibit recency effects and backward contiguity effects that are typical in human behavioral patterns. G – I The same results are plotted along the dimension of training epochs, summarizing the model's tendency to initiate recall from the end (the last three items of the list; G), to initiate recall from the beginning of the list (the first three items of the list; H), and to recall items in the backward direction (conditional response probability with −1 lag; I). J – L Actor and critic loss curves exhibit a consistent decrease over time, and the mean episode reward converges toward the maximum possible recall reward of 14 for our experiments. 5000 training steps correspond to 1 training epoch.

---

### Opportunities and challenges associated with clinical diagnostic genome sequencing: a report of the Association for Molecular Pathology [^87d769fc]. The Journal of Molecular Diagnostics (2012). Medium credibility.

Genome, exome, and gene panel sequencing coverage — Coverage is defined as the number of reads aligning to a reference region, and for the human haploid genome, which is approximately three billion bases, most investigators aim for 30- to 40-fold average coverage. For exome sequencing, the capture target varies from 30 to 50 Mb and typical average exon coverage would be 50- to 100-fold. Gene panel target sizes vary by the number of genes, with average coverage in the same range as for exome sequencing when analyzing constitutional variants. The page emphasizes balancing increased coverage for variant detection against sequencing costs, notes that lower genome coverage can miss variants observed in exome results from the same individual, highlights that panels achieve high coverage in defined targets, and cautions that adding new genes to a panel typically requires a redesign.

---

### Statistical modeling of RNA structure profiling experiments enables parsimonious reconstruction of structure landscapes [^f507fa1c]. Nature Communications (2018). Medium credibility.

Design matrix

The entries of the design matrix are expressed as functions of the unknown modification probability at unpaired sites (η) and noise parameters (γ). To estimate η, it is helpful to consider a related vector from the model by Aviran et al. namely, β. Recall that β l is the probability that site l is modified when considering all molecules in the sample, or in other words, it is a modification propensity that is aggregated over the many different structures in the ensemble. Importantly, while η is kept constant across all unpaired sites within an ensemble of structures, β l is both site-specific and an ensemble-based average measure. To illustrate the relationship between η and β, consider three sites with the following properties: i is unpaired in all structures, j is paired in all structures, and k is paired in some and unpaired in others. The respective ensemble-average probabilities would then be β i ≈ η, β j ≈ 0, and 0 < β k < η. Thus, estimating β at sites such as i allows one to infer η at reasonable accuracy. However, not knowing which candidate structures are highly abundant, it is not trivial to identify such sites. A conservative approach is to seek sites that are unpaired in all candidate structures, but it may be challenging in cases where structural diversity is high. Here, we estimate η fromempirically by sorting the's from high to low, calculating their mean, and settingas the median of those exceeding the mean. This approach is motivated by the expectation that β l would be large at sites that are primarily unpaired in the ensemble. At the same time, it considers a subset of reactive sites to confer robustness to outliers, which are commonly observed in SP data. If no sites that are unpaired across the ensemble exist, we are likely to underestimate η, which in turn will degrade prediction accuracy.

To estimate β and γ, we use maximum-likelihood estimates derived by Aviran et al.:where X l and Y l (l = 1, 2,… L) are counts of truncations/mutations at site l observed in treated and control samples, respectively, and C l + and C l − are local coverages in said samples. Local coverage at site l in a sample stands for the sequencing depth at l, which definition is straightforward in mutation mode. In truncation mode, local coverage is defined as the number of reads that either stop at or pass through l.

---

### Opportunities and challenges associated with clinical diagnostic genome sequencing: a report of the Association for Molecular Pathology [^cb811e9e]. The Journal of Molecular Diagnostics (2012). Medium credibility.

Bioinformatics requirements — NGS computational workflow and outputs — are outlined: Only three major computations are performed with NGS data: i) data assembly within base calling at the level of individual reads, ii) alignment of the assembled sequence to a reference sequence, and iii) and variant calling. Considering these steps in greater detail, they can be delineated as follows: i) image processing, ii) base calling and quality scoring, iii) purity filtering, iv) quality control checks, v) alignment to a reference sequence or de novo assembly, vi) SNP-calling or application-specific steps (eg, chromatin immunoprecipitation or RNA sequencing), vii) packaging, and viii) archiving. Typically, analysis begins with a large set of tiled fluorescence or luminescence images of a flow-cell surface recorded after each iterative round of sequencing; the next step, base calling, converts images to sequence reads by localizing clusters or beads and using parameters such as intensity, background, and noise via platform-specific algorithms to generate read sequences and per-base quality scores. As a result of base calling, FASTQ files are generated and are usually platform specific and contain sequencing read data that combine both the sequence and an associated per base quality score; the sequence reads are then aligned to a known reference sequence or assembled de novo, with the appropriate alignment method depending on the sequencing platform, data type, and computational resources. Limitations and error properties are noted: one limitation of short-reads alignment and assembly is the inability to uniquely align much of a read set; error rates for individual NGS reads are higher than for Sanger sequencing, whereas overall accuracy is achieved by sequencing a given region numerous times via the massively parallel process.

---

### Structure and inference in annotated networks [^535f823a]. Nature Communications (2016). Medium credibility.

Computationally, the most demanding part of the EM algorithm is calculating the sum in the denominator of equation (7), which has an exponentially large number of terms, making its direct evaluation intractable on all but the smallest of networks. Traditionally one gets around this problem by approximating the full distribution q (s) by Monte Carlo importance sampling. In our calculations, however, we instead use a recently proposed alternative method based on belief propagation, which is significantly faster, and fast enough in practice for applications to very large networks.

Final likelihood value

The EM algorithm always converges to a maximum of the likelihood but is not guaranteed to converge to the global maximum — it is possible for there to be one or more local maxima as well. To get around this problem we normally run the algorithm repeatedly with different random initial guesses for the parameters and from the results choose the one that finds the highest likelihood value. In the calculations presented in this paper we did at least 10 such 'random restarts' for each network. To determine which run has the highest final value of the likelihood we calculate the log-likelihood from the right-hand side of (6) using P (A | Θ, s) and P (s | Γ, x) as in equation (2), the final fitted values of the parameters Θ and Γ from the EM algorithm, and q (s) as in equation (7). (As we have said, the right-hand side of (6) becomes equal to the left, and hence equal to the true log-likelihood, when q (s) is given the value in equation (7).)

---

### Assumptions of statistical tests: what lies beneath [^d7ecccf4]. The Journal of Foot and Ankle Surgery (2017). Low credibility.

We have discussed many statistical tests and tools in this series of commentaries, and while we have mentioned the underlying assumptions of the tests, we have not explored them in detail. We stop to look at some of the assumptions of the t-test and linear regression, justify and explain them, mention what can go wrong when the assumptions are not met, and suggest some solutions in this case.

---

### Detecting and phasing minor single-nucleotide variants from long-read sequencing data [^f17888c8]. Nature Communications (2021). High credibility.

The greedy algorithm and its theoretical accuracy

We introduce a fast but inaccurate greedy algorithm to estimate H (x k) (Eq. 6), and then improve its accuracy by RSM in the next section. To estimate H (x k) for substitution x k at locus k, we only need to consider dependent loci in range [k − t l, k + t r], where

[b i, e i] is the covering range of read R i (equation (2)), and ∣{⋅}∣ is the number of elements in set {⋅}. Intuitive, [k − t l, k + t r] is the largest range where [k − t l, k] and [k, k + t r] are fully covered by at least one read. We estimate P r (X k = x k ∣ X g = x g) by Eq. (5) for each locus g ∈ [k − t l, k + t r] ∩ { k } c ({⋅} c is complement of a set), and sort the loci according to P r (X k = x k ∣ X g = x g) in descending order. The sorted loci are denoted as,… We keep adding locus s t to { s 1, s 2. s t −1 } ifand stop if otherwise.based on the final v selected loci { s 1, s 2. s v } is used to estimate H (x k).

The naive greedy algorithm described above avoids combinatorial explosion but might have low accuracy. We assumeare p + 1 real SNVs of the same genome, andare the only p substitutions that can maximize. Formally, andif. Assuming,.are the only loci with real SNVs in [k − t l, k + t r], we define signal-to-noise ratio bywhereand.is equivalent to. For any locus, the probability that it is selected by the greedy algorithm is denoted as P r (g s ∈ { s 1, s 2. s v }), where { s 1, s 2. s v }) is the v loci selected by the greedy algorithm. Without loss of generality, assuming v ≤ p and sequencing errors are independent

---

### Powerful eQTL mapping through low-coverage RNA sequencing [^1b75df85]. HGG Advances (2022). Medium credibility.

Mapping genetic variants that regulate gene expression (eQTL mapping) in large-scale RNA sequencing (RNA-seq) studies is often employed to understand functional consequences of regulatory variants. However, the high cost of RNA-seq limits sample size, sequencing depth, and, therefore, discovery power in eQTL studies. In this work, we demonstrate that, given a fixed budget, eQTL discovery power can be increased by lowering the sequencing depth per sample and increasing the number of individuals sequenced in the assay. We perform RNA-seq of whole-blood tissue across 1,490 individuals at low coverage (5.9 million reads/sample) and show that the effective power is higher than that of an RNA-seq study of 570 individuals at moderate coverage (13.9 million reads/sample). Next, we leverage synthetic datasets derived from real RNA-seq data (50 million reads/sample) to explore the interplay of coverage and number individuals in eQTL studies, and show that a 10-fold reduction in coverage leads to only a 2.5-fold reduction in statistical power to identify eQTLs. Our work suggests that lowering coverage while increasing the number of individuals in RNA-seq is an effective approach to increase discovery power in eQTL studies.

---

### When to p and when not to p [^f5db1073]. Neurogastroenterology and Motility (2023). Medium credibility.

The p-value was proposed in the early 20th century as a potentially useful metric for statistical inference and was defined as "the probability of the observed result, plus more extreme results, if the null hypothesis were true", in the context of a formal statistical testing process. A century later, the scientific community uses it extensively, mostly inappropriately, and often interprets it incorrectly. This editorial briefly reviews the history of the p-value, provides how to properly interpret it, and recommends when to use it and when not to use it.

---

### Large-scale DNA-based phenotypic recording and deep learning enable highly accurate sequence-function mapping [^645a0d4e]. Nature Communications (2020). High credibility.

Optimization of sampling time points

Sampling times were optimized using the high-quality kinetic profiles obtained from the proof-of-concept RBS library (see previous section for definition of high-quality profiles). To avoid biases towards the initial sampling schedule, we first represented the profile p of each RBS by an approximation with a logistic functionimputed at 5 min intervals, which was fixed as the minimal time difference between two samples (Supplementary Fig. 25a). In cases where logistic approximation was not possible (i.e. failed parameter optimization), an exponential decay function was used. Formally, RBS i is represented by its approximation. Afterwards, optimal sampling times were greedily selected while fixing the first and last sample at 0 and 720 min after induction. The goal of the optimization was to find a set S of optimal sampling times (initially S = (0, 720)) which allows to reconstructsuch that a linear approximation using time points in S is as close toas possible. Given the set of possible sampling times T = {0, 5, 10,…, 720} and the subset I of RBS profiles on which the sampling times should be inferred, the greedy optimization finds the next optimal sampling time point s * from T as follows:wherecorresponds to the linear approximation ofusing only sampling times in S (Supplementary Fig. 25b). In other words, s * is the time point that (i) is not part of the sampling schedule S yet and (ii) results in the smallest cumulative reconstruction error over all RBS profiles in I. Subsequently, S is augmented by s *, and Eq. (1) is evaluated to find the next optimal s *, until S contains the desired number of sampling times. Finally, the quality of the optimal sampling schedule for every RBS i is evaluated by computing the approximation error r i between the observed profile p i, and its linear interpolation at the optimal time points S, termed l i, S (Supplementary Fig. 25c):This optimization was performed for different sets of RBS profiles I: we first sorted RBSs by their observed strength (i.e. difference in the fraction of flipped discriminators between first and last sample) and optimized sampling for the top 5%, 10%, 25%, 50%, and 100%, respectively. This strategy was chosen to compensate for the strong bias towards weak RBSs in the initial library. Afterwards, we computed the cumulative approximation error on the entire library (top 100%) for the sampling schedules optimized on the different subsets. We found that for seven or more samples the difference in approximation error between subsets became indistinguishable and hence chose the sampling times inferred on the top 10% of profiles for the following experiments.

---

### Sequence-to-sequence models with attention mechanistically map to the architecture of human memory search [^eff97296]. Communications Psychology (2025). Medium credibility.

Putting everything together: mapping the seq2seq model with attention to CMR

Encoding phase

Both the encoding and decoding/recall phases of the seq2seq model with attention and CMR exhibit deep functional parallels. The correspondence between the two models during the encoding phase can be easily seen from Eqn. (1) and Eqn. (7). During each step of the encoding phase, both models update their current hidden state h i or context state c i by taking in an input embedding, x i, that incorporates pre-experimental information of the presented item f i, which is then combined with the hidden state h i −1 or context state c i −1 from the previous encoding step. In CMR, two parameters, ρ and β, control the mixing of the previous context and the new item embedding, while the seq2seq model accomplishes the same task through more complex parameter matrices W h and U h together with the bias vector b h and the activation function ϕ.

Decoding/recall phase

Compared with the encoding phase, the parallels between the seq2seq model and CMR for the decoding phase are less straightforward and require additional derivations, which we will demonstrate in detail below. The key function during the decoding phase for both models is to generate an output or recall at each timestep j. While the seq2seq model with attention uses the hidden stateto determine the output item f j (as shown in Eqn. (6) and Fig. 2 B), the CMR model utilizes its context state c j to determine the recalled item f j (as shown in Eqn. (16) and Fig. 2 D). Our primary goal here is to demonstrate the correspondence in the decoding phase between these two models, specifically by showing thatin the seq2seq model contains components equivalent to those of c j in CMR.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^d68b8e4b]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Validation sample size and tolerance interval — A one-sided tolerance interval with 95% confidence and 95% reliability could be determined by performance on a set of 59 or more samples, and testing a minimum of 59 samples during validation allows determination of tolerance intervals for performance characteristics. After performing the test on 59 representative samples, the highest false-positive rate is 1.9%, providing 95% confidence that 95% or more of samples will have a false-positive rate ≤ 1.9%.

---

### High-throughput sequencing in respiratory, critical care, and sleep medicine research. An official American Thoracic Society workshop report [^0fc33d36]. Annals of the American Thoracic Society (2019). High credibility.

Next-generation RNA sequencing (RNA-seq) study design — gene expression microarrays have been largely replaced by RNA-seq, which can assay a broader range of RNA types with increased sensitivity and lower costs; RNA-seq studies can be grouped into annotation studies versus quantitation or differential expression studies; for read depth, as low as 1 to 10 million reads per sample may be adequate for differential expression, whereas up to 200 million reads may be required to define all isoforms, and for most human disease studies the number of samples may be more important than the number of reads.

---

### Functional protein mining with conformal guarantees [^282fc321]. Nature Communications (2025). High credibility.

Our main tool will be to index some family of sets by a one-dimensional parameter. We will refer to this family of sets as. An example would bewhere f is a pre-trained machine learning model trained to predict whether x and y match. The methodologies exposed herein allow us to pick the parameter λ such that these sets have a small loss in a probabilistic sense, The parameteris picked based on a calibration procedure which uses a small dataset of proteins the model has not seen during training. We will call this calibration dataset X 1,…, X n, and we assume we can evaluate ℓ against any of the possible responses in. Then, we will deploy thethat is picked using this calibration data on a new, exchangeable protein X test.

The critical assumption in all the forthcoming techniques is the exchangeability of the calibration data and the test point. Exchangeability means that the joint distribution function of the calibration data and the test data point is invariant to permutations. As an example, i.i.d. data points are exchangeable; exchangeability is a weaker condition than this. Intuitively, this means that the calibration data must be representative of the test data, and not involve any deleterious distribution shifts. The particular technical condition required for our theory is that the vector of lossesis exchangeable for all λ — the exchangeability of the data points implies this fact, but it is technically weaker. In other words, exchangeability matters only insofar as the risk is concerned.

For clarity, we define some of the commonly utilized loss functions for our retrieved sets, false discovery rate (FDR) and false negative rate (FNR). Motivated by the desire to control against false significant hits, we define false discovery rate first. FDR measures the ratio between false positive hits (false discoveries) in our retrieved set of model-derived significant hits to the total number of hits (the size of our retrieved set). This is expressed as. The FNR, similarly, is the number of false negative significant hits (annotated hits not in the retrieval set) as a fraction of the total pool of possible hits, expressed as. For further literature relating to controlling FDR, we refer readers to.

---

### Identifying domains of applicability of machine learning models for materials science [^72c4ba76]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ν that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by σ lin (x 1, x 2) ≡ −0.3 ≤ x 1 ≤ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: σ rbf ≡ −3.3 ≤ x 11 ≤ 3.1 ∧ x 2 ≤ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA σ poly (x 1, x 2) ≡ −3.5 ≤ x 2 ≤ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### Considerations for severe acute respiratory syndrome coronavirus 2 genomic surveillance: a joint consensus recommendation of the Association for Molecular Pathology and association of public health laboratories [^8ef6216c]. The Journal of Molecular Diagnostics (2025). High credibility.

Sequencing read strategy for SARS-CoV-2 whole-genome sequencing contrasts paired-end and single-read approaches, noting that paired-end reads are sequenced in both directions (5' and 3' ends) to improve discrimination of artifacts from true variants but increase turnaround time, with batches taking approximately twice as long to complete. For discovery, evolution, or inter-isolate relationship studies, maximum genome coverage is desirable, whereas when the goal is identifying isolated strains, single-read sequencing could be adequate, providing sufficient high-level data and offering a more cost-effective and rapid approach.

---

### Accurate detection of identity-by-descent segments in human ancient DNA [^56d860ee]. Nature Genetics (2024). High credibility.

Single-locus emission probabilities

To define the emission model of the HMM, we need to specify P (D ∣ s), the likelihood of the genetic data for the five HMM states s = 0,1,… ,4 at one locus. Throughout, we denote reference and alternative alleles as 0 and 1, respectively, and the corresponding genotype as g ∈ {0,1}. The observed data D of our emission model will be the haploid dosage, which is the probability of a phased haplotype carrying an alternative allele, here denoted for each haplotype h as

First, we explain how we approximate the two haploid dosages for a single imputed diploid individual 1. We have to use an approximation as GLIMPSE only outputs the most likely phased diploid genotype GT ∈ {0∣0, 0∣1, 1∣0, 1∣1} as well as three posterior genotype probabilities GP for each of the unphased diploid genotypes, denoted by the number of alternative alleles as 0,1,2. We first approximate the posterior probabilities for the four phased states, here denoted as P 00, P 01, P 10 and P 11. The two homozygote probabilities P 00 and P 11 are obtained trivially from the corresponding unphased genotype probabilities GP, as no phase information is required for homozygotes. To obtain probabilities of the two phased heterozygotes states, P 01 and P 10, we use a simple approximation. Let p 0, p 1, p 2 denote the posterior probability for each of the three possible diploid genotypes. If the maximum-likelihood unphased genotype is heterozygote, that is max(p 0, p 1, p 2) = p 1, we set P 01 = p 1, P 10 = 0 if GT = 0∣1 and P 01 = 0, P 10 = p 1 if GT = 1∣0. If the maximum-likelihood unphased genotype is a homozygote, that is max(p 0, p 1, p 2) = p 0 or p 2 and thus there is no phase information for the heterozygote genotype available, we set P 01 = P 10 = p 1 /2. Having obtained the four probabilities for the possible phased genotypes, we can calculate the two haploid dosages as:When calling IBD segments between two individuals 1 and 2, we use this approach to obtain all four haploid dosages and denote them for haplotypes 1A, 1B, 2A, 2B as (x 1A, x 1B, x 2A, x 2B).

---

### Phase 0 / microdosing approaches: time for mainstream application in drug development? [^98d5620b]. Nature Reviews: Drug Discovery (2020). High credibility.

Phase 0 approaches - which include microdosing - evaluate subtherapeutic exposures of new drugs in first-in-human studies known as exploratory clinical trials. Recent progress extends phase 0 benefits beyond assessment of pharmacokinetics to include understanding of mechanism of action and pharmacodynamics. Phase 0 approaches have the potential to improve preclinical candidate selection and enable safer, cheaper, quicker and more informed developmental decisions. Here, we discuss phase 0 methods and applications, highlight their advantages over traditional strategies and address concerns related to extrapolation and developmental timelines. Although challenges remain, we propose that phase 0 approaches be at least considered for application in most drug development scenarios.

---

### Fast and sensitive mapping of nanopore sequencing reads with graphMap [^f4e7db26]. Nature Communications (2016). Medium credibility.

Refining alignments using L 1 linear regression. The alignments obtained using LCSk tend to be largely accurate but since its definition lacks constraints on the distance between substrings, the alignments obtained may include outlier matches and incorrect estimation of overall alignment length (Fig. 1e). These outliers are caused by repeats or sequencing errors, but they still satisfy the monotony condition. Similar to the observation presented for region selection, the LCSk list of anchors should ideally be collinear in the 2D query-target coordinate space, with a slope of 45°. All deviations from this line are caused by indel errors, and can be viewed as noise. The filtering of outlier anchors begins by fitting a 2D line with a 45° slope in the query-target space under the least absolute deviation criteria (L 1). Next, a subset of anchors which are located withinfrom either side of the L 1 line is selected, where e is the expected error rate (by default, conservatively set to 45%), T is the target (read) length and the factoris used to convert the distance from target coordinate space to a distance perpendicular to the L 1 line. A confidence intervalis calculated, where d i is the distance from a selected anchor i to the L 1 line (the constant 3 was chosen to mimic a 3σ rule). LCSk is then repeated once again but only on the anchors which are located within the distance ± c from the L 1 line to compensate for possible gaps caused by anchor filtering (Fig. 1e). The use of L 1 filtering was observed to improve the precision of alignment start and end coordinates for many reads, though the overall impact on performance was less significant in comparison to the LCSk stage (Supplementary Data 3).

After filtering, five empirically derived scores that describe the quality of the region are calculated. They include: the number of exact kmers covered by the anchors n kmers, the s.d. σ of anchors around the L 1 line, the length of the query sequence which matched the target (distance from the first to the last anchor) m len, the number of bases covered by anchors (includes only exact matching bases) n cb and the read length. The last four scores are normalized to the range [0,1] with the following equations (4, 5, 6, 7):

where Q is the length of the reference sequence (query in our previous definition). The overall quality of the alignment in a region is then calculated as the product of the normalized scores:

---

### Analytical validation of clinical whole-genome and transcriptome sequencing of patient-derived tumors for reporting targetable variants in cancer [^94613a8d]. The Journal of Molecular Diagnostics (2018). Low credibility.

We developed and validated a clinical whole-genome and transcriptome sequencing (WGTS) assay that provides a comprehensive genomic profile of a patient's tumor. The ability to fully capture the mappable genome with sufficient sequencing coverage to precisely call DNA somatic single nucleotide variants, insertions/deletions, copy number variants, structural variants, and RNA gene fusions was analyzed. New York State's Department of Health next-generation DNA sequencing guidelines were expanded for establishing performance validation applicable to whole-genome and transcriptome sequencing. Whole-genome sequencing laboratory protocols were validated for the Illumina HiSeq X Ten platform and RNA sequencing for Illumina HiSeq2500 platform for fresh or frozen and formalin-fixed, paraffin-embedded tumor samples. Various bioinformatics tools were also tested, and CIs for sensitivity and specificity thresholds in calling clinically significant somatic aberrations were determined. The validation was performed on a set of 125 tumor normal pairs. RNA sequencing was performed to call fusions and to confirm the DNA variants or exonic alterations. Here, we present our results and WGTS standards for variant allele frequency, reproducibility, analytical sensitivity, and present limit of detection analysis for single nucleotide variant calling, copy number identification, and structural variants. We show that The New York Genome Center WGTS clinical assay can provide a comprehensive patient variant discovery approach suitable for directed oncologic therapeutic applications.

---

### Sequence-to-sequence models with attention mechanistically map to the architecture of human memory search [^34dc3147]. Communications Psychology (2025). Medium credibility.

The seq2seq model can explain and predict recall behavior in memory search comparably to the CMR model

We begin by exploring the potential of the seq2seq model as a predictor of individual recall behavior and compare its explanation and predictive abilities to those of CMR. In order to assess the predictive ability of the two models, both CMR and the seq2seq model (128-Dim. with attention mechanism) were fit to a training split of each participant's recall data (rather than aggregated data across all participants) and then evaluated on a hold-out test set from the same participant. For the CMR model, fitting is accomplished by optimizing CMR parameters using Bayesian optimization to minimize the root-mean-square error between model-generated and human behavioral patterns. The seq2seq model is fit to human data using human participant-presented lists as model input and human participant recalled lists as ground-truth with a standard cross-entropy loss as the objective function. Figure 3 A–C depict the behavior patterns of the first 12 participants (for illustrative purposes) of the 171 participants taken from the PEERS dataset with both fitted CMR and seq2seq models overlaid. Additionally, we show the average recall behavioral patterns for all 171 participants in Fig. 3 D–F. While both CMR and the seq2seq model have qualitatively good fits to the human recall data, we compare their model fit quality in terms of the root-mean square error between the human recall behavior and corresponding model recall behavior across all 171 participants (Fig. 3 G–I). We performed a series of two-sided Wilcoxon signed-rank tests to compare each pair of distributions for the root-mean-square-error fit for both CMR and the seq2seq model. We find that the seq2seq model fits the corresponding human data curve with significantly smaller error for the serial position effects (Wilcoxon signed-rank test: two-sided, W = 0.0, n = 171, p = 8.2 × 10 −30, ranked biserial correlation = −0.995), the probability of first recall (Wilcoxon signed-rank test: two-sided, W = 2880.0, n = 171, p = 5.23 × 10 −12, ranked biserial correlation = − 0.608), and the conditional response probability (Wilcoxon signed-rank test: two-sided, W = 3822.0, n = 171, p = 5.51 × 10 −8, ranked biserial correlation = −0.480). These results are not affected by different methods of conducting the training and testing splits (see Supplementary Discussion S2.2). When the amount of training data is reduced, however, CMR is shown to give better predictions, highlighting a trade-off between model flexibility and data efficiency (see Supplementary Discussion S2.3). Together, these results support that the seq2seq model with attention can capture human recall patterns as effectively as CMR. This ability to capture human recall patterns is not a result of the neural network model being over-parameterized, as the trained model was able to predict behavioral patterns in the unseen portion of the recall data.

---

### Opportunities and challenges associated with clinical diagnostic genome sequencing: a report of the Association for Molecular Pathology [^b330d84f]. The Journal of Molecular Diagnostics (2012). Medium credibility.

Next-generation sequencing (NGS) data analysis and storage — key challenges include variable coverage, interpretation steps, somatic versus germline considerations, and data volume. Beyond sequence errors, inadequate coverage can cause failure to accurately detect nucleotide variation, leading to false-positive and false-negative results, and depending on the sequencing application, the next step may involve quantification of reads, mutation calling, copy number analysis, and/or analysis of structural variations. Today, most users align the NGS reads to a known reference using SNP databases, and in mendelian conditions, use of unaffected family members provides a useful background to identify novel disease-causing mutations, whereas the calling of somatic mutations in malignancies is much more complicated because of the variance in ploidy and purity of the tumor DNA and dependence on the allelic fraction; comparison to nontumor tissue provides an important tool, and elimination of false positives can largely be achieved through verification by conventional PCR and Sanger sequencing, although minimizing false negatives is less straightforward. With respect to data storage, typically tens or hundreds of giga bp of short reads can be generated during one NGS run and the average NGS experiment generates terabytes of raw data, yet the extent of data that need to be retained for NGS assays has not been established, the retention of the complete data is costly and, in many instances, cost prohibitive, and the use of cloud computing is proposed as a potential way to avoid obtaining expensive servers.

---

### An expanded sequence context model broadly explains variability in polymorphism levels across the human genome [^b8512e27]. Nature Genetics (2016). Medium credibility.

The rate of single-nucleotide polymorphism varies substantially across the human genome and fundamentally influences evolution and incidence of genetic disease. Previous studies have only considered the immediately flanking nucleotides around a polymorphic site — the site's trinucleotide sequence context — to study polymorphism levels across the genome. Moreover, the impact of larger sequence contexts has not been fully clarified, even though context substantially influences rates of polymorphism. Using a new statistical framework and data from the 1000 Genomes Project, we demonstrate that a heptanucleotide context explains > 81% of variability in substitution probabilities, highlighting new mutation-promoting motifs at ApT dinucleotide, CAAT and TACG sequences. Our approach also identifies previously undocumented variability in C-to-T substitutions at CpG sites, which is not immediately explained by differential methylation intensity. Using our model, we present informative substitution intolerance scores for genes and a new intolerance score for amino acids, and we demonstrate clinical use of the model in neuropsychiatric diseases.

---

### Defining the time-limited trial for patients with critical illness: an official American Thoracic Society workshop report [^cf1ee11e]. Annals of the American Thoracic Society (2024). High credibility.

Time-limited trial in critical care — essential elements and phased process were derived through a Delphi process: investigators "identified 18 potential steps" and found first-round consensus that "11 of these were essential elements"; during the second round, they "identified seven additional essential elements". The committee then "combined 4 related items into two steps (to reach 16 essential elements)" and "organized steps into four phases of care: consider, plan, support, and reassess". Additional steps "may be helpful in some cases, but not necessary for all trials", and involvement of other disciplines is "highly dependent on a patient's specific situation and on the available hospital resources".

---

### What is the value of a p value? [^dfc52b0a]. The Annals of Thoracic Surgery (2009). Low credibility.

Successful publication of a research study usually requires a small p value, typically p < 0.05. Many clinicians believe that a p value represents the probability that the null hypothesis is true, so that a small p value means the null hypothesis must be false. In fact, the p value provides very weak evidence against the null hypothesis, and the probability that the null hypothesis is true is usually much greater than the p value would suggest. Moreover, even considering "the probability that the null hypothesis is true" is not possible with the usual statistical setup and requires a different (Bayesian) statistical approach. We describe the Bayesian approach using a well-established diagnostic testing analogy. Then, as a practical example, we compare the p-value result of a study of aprotinin-associated operative mortality with the more illuminative interpretation of the same study data using a Bayesian approach.

---

### Towards a fully automated algorithm driven platform for biosystems design [^ed08b9a9]. Nature Communications (2019). High credibility.

We next sought to illustrate the optimization method with a similar 3-variable function with three inputs and one output to simulate a similar multi-dimensional optimization problem. It is noteworthy that Bayesian optimization has been used in numerous applications – and the purpose of this simulation is testing the algorithm on a simple but similar setting. The search perimeter was set to be 1–24 for each of the inputs and the maximum of the function was set to be 9 (y = f (x 1, x 2, x 3) | x i ɛ {1, 2,…, 24}, f max = 9). The Bayesian optimization algorithm was able to find the maximum value of this function by only evaluating 12 points out of all possible 24 3 = 13,824 points. These 12 evaluations were the result of 12 iterations of learning and testing, with each evaluation being followed by a learn step that produced the next point to evaluate. We then sought to compare this optimization strategy with baseline approach where randomly sampled points are evaluated and all of these points are used to train an Exterior Derivative Estimation (EDE)-based regression model described in previous publications. We found that although the EDE approach shows impressive predictive capability, especially given that all the data have been acquired at once and not through iterative sampling, even after sampling 192 random points, the maximum could not be found (Supplementary Table 1).

---

### Fast sequences of non-spatial state representations in humans [^22ae2b69]. Neuron (2016). Low credibility.

Sequences in Decoded Object Representations

We applied the trained regression models to MEG data collected in the planning phase of each trial. As already described, this planning phase was a period that varied in duration from 2 to 60 s (cf. Figure 1 D for distribution of times) during which no visual object was onscreen. Each 10 ms time bin of these data was independently input to each of the six regression models, yielding six time series of probabilities for each trial (example trial shown in Figure 3 A). The probability in time bin t from model k quantified the degree to which the spatial pattern of MEG activity at time t resembled the evoked neural response to visual object k.

We next asked whether these time series contained sequences that followed possible paths in the behavioral task based upon a measure we refer to as "sequenceness". For example, in the task there was a potential transition from state S 1 to state S 2. Sequenceness quantified whether a decoded neural representation of state S 1 was likely to be followed by a decoded representation of state S 2 or, in case of reverse sequences, whether S 2 would be followed by S 1. We operationalized sequenceness using a cross-correlation measure (see Experimental Procedures for details).

---

### Standards and guidelines for validating next-generation sequencing bioinformatics pipelines: a joint recommendation of the Association for Molecular Pathology and the college of American pathologists [^d47c4340]. The Journal of Molecular Diagnostics (2018). Medium credibility.

Recommendation 15 — bioinformatics pipeline validation and reporting metrics require representative variants, appropriate reference methods, indel-length coverage, and variant-type–specific performance reporting. Bioinformatics methods should be validated using a representative population of variants selected to match the expected prevalence of clinically significant variants in patients, and whenever possible the reference data should be generated using a high-quality clinically validated method, with orthogonal methods preferred. A medical molecular professional must assess the entire validation sample set to ensure samples with variants adequately represent the expected types and clinical significance. For short-read NGS (eg, < 150 bp), insertions and deletions of up to 21 bp may be difficult to detect, so laboratories must include indels of varying length up to and including the maximum they expect to reliably detect and should report only those indel lengths the assay can accurately and reproducibly detect; the maximum indel length must be included in performance descriptions. Accuracy metrics should always be reported separately for different variant types, and for each variant type the following must be reported using standard methods: true positives, true negatives, false positives, false negatives, analytic sensitivity (alias positive percentage agreement), and analytical positive predictive value; overall percentage accuracy (alias overall percentage agreement) should also be calculated. Sensitivity should be measured using reference variant calls produced on the same sample using an independently performed test.

---

### Clotrimazole [^b5a7188a]. FDA. Low credibility.

The dosage of clotrimazole OTIC for treatment of otomycosis in adults is 1 vial OTIC BID for 14 days (1%/0.17 mL)

---

### High-throughput sequencing in respiratory, critical care, and sleep medicine research. An official American Thoracic Society workshop report [^b5b2971a]. Annals of the American Thoracic Society (2019). High credibility.

Sequencing protocol selection and batch handling — key analytic considerations — include that "Single-end short reads (≤ 75 bp) are acceptable for standard differential expression (73)" whereas "Paired-end longer reads are preferable for annotating splicing events (73)". In larger studies, "samples should be randomized across batches to minimize confounding; analysis should account for batch effects". Biological signal can vary and may require cell-level adjustment, as "Gene expression in human tissues can be quite variable, depending on sampling, technical factors, and cellular heterogeneity (74). The latter can be addressed by deconvolution methods, which require additional information, either cell counts or cell-specific reference transcriptomes".

---

### Standards and guidelines for validating next-generation sequencing bioinformatics pipelines: a joint recommendation of the Association for Molecular Pathology and the college of American pathologists [^de5e263b]. The Journal of Molecular Diagnostics (2018). Medium credibility.

Recommended quality metrics for clinical bioinformatics pipelines — Table 4 outlines required (REQ) and optional (Opt) metrics with performance criteria across preanalytical, run metrics, mapping/read filters, per variant, and QC categories. Preanalytical metrics include "% of nucleated cells that are tumor cells" with "Min" for "Tumor samples". Run metrics include "% of reads passing a minimum Phred score criterion (eg, 99% of bases at Q30 or higher)" with "Min", and "Demultiplexing success (ie, all molecular identifiers present and no unexpected molecular identifiers detected)" with "Pass/fail" for "All sample types when multiplexing is used". Read filters/mapping include "Mapping quality" with "Min", "Mean on-target coverage of reads" with "Min", and "% of aligned bases exceeding the minimum Phred score that disagree with reference" with "Max" for "Samples for germline analysis only". Per-variant requirements include "Number of distinct vertical variants at the same position" with " ≤ 2" and "Number of distinct horizontal variants within a prescribed cluster window size (bp)" with " ≤ 1", both for "All sample types". QC measures include "Fingerprint genotypes match NGS results" with "Yes (no requires investigation/ explanation)" for "All sample types", and "Observed sex matches reported sex" with "Yes (no requires investigation/ explanation)" for "All sample types if X/Y chromosomes are included in assay". Cluster-level run control includes "Cluster density" with "Min, Max" for "All sample types on Illumina platforms that include this metric by default".

---

### Error correction enables use of oxford nanopore technology for reference-free transcriptome analysis [^c8d2602a]. Nature Communications (2021). High credibility.

Algorithm details

For a given cluster, we first generate all the paired positional minimizers StrW of the reads. Then for each read r we will construct a weighted interval scheduling instance (Step 1). Each positional minimizer pair ((m 1, p), (m 2, q)) ∈ W r defines an interval on r that is spanned by, but does not contain the minimizers, i.e. [p + k, q). The interval is given the weight a (q − p − k), where a denotes the support of the interval. We compute the support as the number of occurrences of (m 1, m 2) in StrW whose intervals have a similar sequence to the one spanned by (m 1, m 2) in r, and this is computed as follows. Let r ′ ≠ r be a read containing (m 1, m 2) with coordinates (p ′, q ′) in r ′ and let s = r [p: q + k] and s ′ = r ′ [p ′: q ′ + k] be substrings of r and r ′ spanned by, and including, the minimizer windows. We consider that s ′ has similar sequence to s if the edit distance is less than | s | (ϵ s + ϵ s ′), where ϵ s (respectively, ϵ s ′) is the average per-base error rate of s (respectively, s ′), inferred from the quality values. If a read has multiple (possibly disjoint) intervals matching a single interval of r, only the one with the smallest edit distance is considered.

---

### The significance of not finding a gene [^6e1e2b3b]. American Journal of Human Genetics (2001). Low credibility.

As more investigators conduct extensive whole-genome linkage scans for complex traits, interest is growing in meta-analysis as a way of integrating the weak or conflicting evidence from multiple studies. However, there is a bias in the most commonly used meta-analysis linkage technique (i.e. Fisher's [1925] method of combining of P values) when it is applied to many nonparametric (i.e., model free) linkage results. The bias arises in those methods (e.g., variance components, affected sib pair, extremely discordant sib pairs, etc.) that truncate all "negative evidence against linkage" into the single value of LOD = 0. If incorrectly handled, this bias can artificially inflate or deflate the combined meta-analysis linkage results for any given locus. This is an especially troublesome problem in the context of a genome scan, since LOD = 0 is expected to occur over half the unlinked genome. The bias can be overcome (nearly) completely by simply interpreting LOD = 0 as a P value of 1divided by 2ln(2) is approximately equal to .72 in Fisher's formula.

---

### Model selection: finding the right fit [^6940b900]. The Journal of Foot and Ankle Surgery (2019). Medium credibility.

There are numerous possible goals for building statistical models. Those statistical goals, the associated model types, and each statistical tool involved in model building come with its own assumptions and requirements. In turn, these requirements must be met if we are to ensure that our models produce meaningful, interpretable results. However, beyond these technical details is the intuition, and the additional set of tools and algorithms, used by the statistician, to build the contextually appropriate model: not only must we build an interpretable model, we must build a model that answers the particular question at hand and addresses the particular goal we have in mind. In this column we discuss the methods by which statisticians build models for description, risk factor identification, and prediction.

---

### New designs for phase 2 clinical trials [^a90544dc]. Blood (2003). Low credibility.

Conventional phase 2 clinical trials are typically single-arm experiments, with outcome characterized by one binary "response" variable. Clinical investigators are poorly served by such conventional methodology. We contend that phase 2 trials are inherently comparative, with the results of the comparison determining whether to conduct a subsequent phase 3 trial. When different treatments are studied in separate single-arm trials, actual differences between response rates associated with the treatments, "treatment effects", are confounded with differences between the trials, "trial effects". Thus, it is impossible to estimate either effect separately. Consequently, when the results of separate single-arm trials of different treatments are compared, an apparent treatment difference may be due to a trial effect. Conversely, the apparent absence of a treatment effect may be due to an actual treatment effect being cancelled out by a trial effect. Because selection involves comparison, single-arm phase 2 trials thus fail to provide a reliable means for selecting which therapies to investigate in phase 3. Moreover, reducing complex clinical phenomena, including both adverse and desirable events, to a single outcome wastes important information. Consequently, conventional phase 2 designs are inefficient and unreliable. Given the limited number of patients available for phase 2 trials and the increasing number of new therapies that must be evaluated, it is critically important to conduct these trials efficiently. These concerns motivated the development of a general paradigm for randomized selection trials evaluating several therapies based on multiple outcomes. Three illustrative applications of trials using this approach are presented.

---

### A comparison of phase II study strategies [^ebb760ae]. Clinical Cancer Research (2009). Low credibility.

The traditional oncology drug development paradigm of single arm phase II studies followed by a randomized phase III study has limitations for modern oncology drug development. Interpretation of single arm phase II study results is difficult when a new drug is used in combination with other agents or when progression-free survival is used as the endpoint rather than tumor shrinkage. Randomized phase II studies are more informative for these objectives but increase both the number of patients and time required to determine the value of a new experimental agent. In this article, we compare different phase II study strategies to determine the most efficient drug development path in terms of number of patients and length of time to conclusion of drug efficacy on overall survival.

---

### The p-value and model specification in statistics [^f5464b11]. General Psychiatry (2019). Medium credibility.

The p value has been widely used as a way to summarise the significance in data analysis. However, misuse and misinterpretation of the p value is common in practice. Our result shows that if the model specification is wrong, the distribution of the p value may be inappropriate, which makes the decision based on the p value invalid.