# Medical Question & Answer

**Sample ID**: d911f00a-a41f-6701-8ac1-3d046f6892e7
**Dataset Index**: 194224

---

## Question

4. Find the limit of the variable quantity $x = \frac{a z+1}{z}$ as $z \rightarrow \infty$.

---

## Answer

> Let's see… What do we have here? The user is asking about finding the limit of the variable quantity x = (az + 1)/z as z approaches infinity. Let's break this down step-by-step. First, I need to think about the algebraic structure of the expression and whether it can be simplified. Then, I should verify the limit behavior of each term as z grows without bound. Next, I will consider whether any special cases, like a = 0, change the outcome. Finally, I will state the limit and double-check that my reasoning aligns with standard limit properties and any relevant examples from the literature that illustrate similar asymptotic behavior.

> Let me first rewrite the expression to expose its asymptotic structure. I can split the numerator and divide each term by z, so x = (az + 1)/z = az/z + 1/z = a + 1/z. This simplification is valid for z ≠ 0, which is fine since we are taking the limit as z → ∞, where z will eventually be nonzero [^notfound].

> Now, I need to evaluate the limit of each term. As z → ∞, the term 1/z approaches 0, so the expression becomes a + 0, which equals a. Hold on, I should verify that this aligns with basic limit properties: the limit of a constant is the constant, and the limit of 1/z as z → ∞ is indeed 0, so the sum rule gives lim_{z→∞} (a + 1/z) = a + 0 = a [^notfound].

> Wait, let me consider whether the parameter a affects this conclusion. If a = 0, the original expression becomes x = 1/z, and the limit is still 0, which matches the general result a = 0. If a ≠ 0, the dominant term is az/z = a, and the 1/z term vanishes, so the limit remains a. I should confirm that this case-based reasoning is consistent with the algebraic simplification, and it is: the limit is a regardless of whether a is zero or nonzero [^notfound].

> I should double-check that there are no hidden subtleties, such as indeterminate forms or domain issues. The expression (az + 1)/z is a rational function where the degree of the numerator equals the degree of the denominator, so the limit is the ratio of leading coefficients, which is a/1 = a. This confirms the earlier result and reassures me that no additional constraints are needed [^notfound].

> Final answer: The limit of x = (az + 1)/z as z approaches infinity is a [^notfound].

---

The limit of (x = \\frac{az + 1}{z}) as (z \\to \\infty) is **a**. This is found by dividing the numerator and denominator by (z), giving (x = a + \\frac{1}{z}); as (z \\to \\infty), (\\frac{1}{z} \\to 0), so (x \\to a).

---

## Step-by-step solution

### Step 1: Rewrite the expression

Divide the numerator and denominator by (z):

[
x = \\frac{az + 1}{z} = \\frac{az}{z} + \\frac{1}{z} = a + \\frac{1}{z}
]

---

### Step 2: Evaluate the limit

As (z \\to \\infty), (\\frac{1}{z} \\to 0), so:

[
\\lim_{z \\to \\infty} x = \\lim_{z \\to \\infty} \\left(a + \\frac{1}{z} \\right) = a + 0 = a
]

---

## Conclusion

The limit of (x = \\frac{az + 1}{z}) as (z \\to \\infty) is **a**.

---

## References

### To infinity and some glimpses of beyond [^d42883f3]. Nature Communications (2017). Medium credibility.

Methods

Complexification of ordinary differential equations

The complexified version(z = x + iy) leads to the two-dimensional dynamical system:

The real axis is an invariant subspace, retrieving our real results; yet complexification endows the dynamics with an intriguing capability: as Fig. 4a, b illustrates through the (x, y) phase plane, collapse is avoided in the presence of a minuscule imaginary part. Large elliptical-looking trajectories are traced on the phase plane, eventually returning to the neighborhood of the sole fixed point of (0, 0) — which in the real case one would characterize as semi-stable. The system of Eq. (19) can be tackled in closed form since the ODEyields 1/ z = − t + 1/ z (0). For z = x + iy (z (0) = x 0 + iy 0) we obtain the explicit orbit formulaEliminating time by dividing the two ODEs within Eq. (19) directly yields an ODE for y = y (x) (rather than the parametric forms of Eqs. (20), (21)). From this ODE, one can obtain that the quantityis an invariant of the phase plane dynamics, and thus the latter can be written as x 2 + (y − R) 2 = R 2, where. That is, the trajectory evolves along circles of radius R in the upper (resp. lower) half plane if y 0 > 0 (resp. y 0 < 0.) Approaching the axis with y 0 → 0, the curvature of these circles tends to 0 and their radius to ∞ (retrieving the real dynamics as a special case). Figure 4 through its planar projections illustrates not only the radial projection of the dynamics in the x − y plane, but the x − t and y − t dependencies.

---

### Determination of X-ray detection limit and applications in perovskite X-ray detectors [^f4a0c352]. Nature Communications (2021). High credibility.

A simple modification to the IUPAC definition would not result in a correct equation to use for detection limit determination because of the different physics involved. IUPAC definition follows the Currie method that is applicable for γ-ray photon counting where the physics behind is radioactive decay. However, in X-ray detection, the large quantity of X-ray photons emitted from X-ray machine is not a decay phenomenon in nature, nor does the dark current I dark and the photocurrent under X-ray I X–ray of an X-ray detector. The statistical distribution applicable to photon counting is Poisson-Normal distribution where the sample size n that refers to number of counting events performed, is typically 1 or not far larger than 1. Contrarily, I dark and I X–ray for an X-ray detector follows Normal distribution, where the sample size n in the electric current measurement referred as the number of digitized current points is typically very large.

To establish a proper procedure for the detection limit determination of an X-ray detector working in current mode, we propose a method by comparing the means of two normally distributed samples (i.e. physical parameters) of unequal sample size (i.e. number of sampled data points), which considers the possible different standard deviations of the dark current I dark, the photocurrent under X-ray I X–ray, and the large number of digitized current data points. The model containing Eqs. (1) and (2) (named as Detection Limit (DL) equations thereafter in this paper) are a well-established model in statistical hypothesis testing theory. In DL equations, n 1, n 2 are the sample size of the Group 1 and Group 2, respectively. Δ = | μ 1 − μ 2 | is the theoretical detection limit. (μ 1,), (μ 2,) are the means and variances of the two respective groups following Normal distribution. k = n 2 / n 1 is the ratio of the two sample sizes. The z 1− α and z 1− β are the Z-score corresponding to type I error α and type II error β, respectively, for one-sided test. The relevant parameters are illustrated in Fig. 2c. The Eqs. (1) and (2) can be rearranged to cancel k, yielding Eq. (3), which is another form of the DL equations.

---

### Determination of X-ray detection limit and applications in perovskite X-ray detectors [^c02fe79a]. Nature Communications (2021). High credibility.

Assuming Group 1 and Group 2 represent the gross signal (photocurrent under X-ray I X–ray in the case of X-ray detector) and the blank signal (dark current I dark in the case of X-ray detector) throughout in this work (equivalently vice versa), respectively, then a prior detection limit Δ = | μ 1 − μ 2 | can be calculated with the measured blank signal of Group 2, i.e. (μ 2,) and n 2 from the DL equations, with some necessary pre-assumptions (we use the term " prior " because this method requires pre-assumptions from a statistical perspective). We make assumptions consistent with the Currie method and show that the DL equations is same as Currie formulars when sample sizes (i.e. number of sampled data points) are being reduced (Fig. 2c). Instead of setting α = 0.13% and β = 50% as in the IUPAC definition, we set more properly α = β = 0.05, corresponding to z 1− α = z 1− β = 1.645, which is consistent with Currie formulas. The standard deviation of the gross signal and the blank signal are assumed approximately equal as is in the Currie formulars, i.e. = , when the gross signal is small approaching the level of detection limit. The influence of sample size (i.e. number of sampled data points) on the detection limit is reflected by the value of k = n 2 / n 1. If we set n 2 = n 1 = 1, we have k = 1. Then the DL equations reduce to. With the assumptions of = and z 1− α = z 1− β = 1.645, we have, which is the detection limit L D in Paired observations case in Currie's classical paper(Table 1). If we set n 1 = 1 and n 2 > > n 1, that is, k = n 2 / n 1 > > 1, so that > > or > >, then the DL equations reduce to. With the same assumptions, we have Δ = | μ 1 − μ 2 | = 3.29 σ 2, which is the detection limit L D in "Well-known" blank case in Currie's paper(Table 1). The DL equations are based on Normal distribution, which makes it applicable to X-ray detectors working in a continuous current mode.

---

### Fundamental limits to learning closed-form mathematical models from data [^a345300f]. Nature Communications (2023). High credibility.

Given a finite and noisy dataset generated with a closed-form mathematical model, when is it possible to learn the true generating model from the data alone? This is the question we investigate here. We show that this model-learning problem displays a transition from a low-noise phase in which the true model can be learned, to a phase in which the observation noise is too high for the true model to be learned by any method. Both in the low-noise phase and in the high-noise phase, probabilistic model selection leads to optimal generalization to unseen data. This is in contrast to standard machine learning approaches, including artificial neural networks, which in this particular problem are limited, in the low-noise phase, by their ability to interpolate. In the transition region between the learnable and unlearnable phases, generalization is hard for all approaches including probabilistic model selection.

---

### Size limits the sensitivity of kinetic schemes [^71d1065f]. Nature Communications (2023). High credibility.

A different measure of sensitivity — the amplification of a fold-change in the input — provides a solution to this problem. Suppose that for some value x 0 of the input parameter x, scaling by a factor a scales the output by b, so f (a x 0) = b f (x 0). Then the quotientcan be thought of as a discrete approximation of the derivative defining the logarithmic sensitivity. And if f (x) is differentiable everywhere, then by the mean value theorem, there must be a value x * of x for whichThis means that careful measurement of any two points on the input-output curve (x versus f (x)) witnesses the (local, infinitesimal) logarithmic sensitivity somewhere. Importantly — unlike in the case of fitting to a Hill function — if error in the measurements is very low, then they are also telling us the derivative for some value of x very accurately.

Equation (25) leads us to another common definition of the effective Hill coefficient:where S 0.9 and S 0.1 are the values of the input variable (in our case, x) required to get 90% and 10% (respectively) of the maximum value of the output variable (in our case, f (x)). Note that (26) is like (25) with a = S 0.9 / S 0.1 and b = 9. It implies that somewhere between S 0.1 and S 0.9 there is a logarithmic sensitivity of H eff /2.

There is yet another common definition, specific to models of binding. Suppose x is the concentration of a ligand and 〈 n b 〉(x) is the expected number of sites bound by a ligand out of a total of n possible binding sites. It is common then, to takeor to report, as the Hill coefficient, the slope of a line fitted to x versusdata on a log-log plot.

---

### Fundamental limits to learning closed-form mathematical models from data [^5954e9c9]. Nature Communications (2023). High credibility.

Fig. 1
Probabilistic model selection makes quasi-optimal predictions about unobserved data.

We select two models m *, whose expressions are shown at the top of each column. a, b From each model, we generate synthetic datasets D with N points (shown, N = 100) and different levels of noise s ϵ (shown, s ϵ = 1). Here and throughout the article, the values of the independent variables x 1 and x 2 are generated uniformly at random in [− 2, 2]. Vertical lines show the observation error ϵ i for each point in D. For a model not drawn from the prior and data generated differently, see Supplementary Fig. S1. c, d For each dataset D (with dataset sizes N ∈ {25, 50, 100, 200, 400}), we sample models from p (m ∣ D) using the Bayesian machine scientist, select the MDL model (maximum p (m ∣ D)) among those sampled, and use this model to make predictions on a test dataset, generated exactly as D. We show the prediction root mean squared error (RMSE) of the MDL model onas a function of N and s ϵ. For comparison, we also show the predictions from an artificial neural network (ANN, dotted lines; Methods). Since s ϵ is the irreducible error, predictions on the diagonal RMSE = s ϵ are optimal. e, f We plot the prediction RMSE scaled by the irreducible error s ϵ; optimal predictions satisfy RMSE/ s ϵ = 1 (dashed line).

---

### The XZZX surface code [^e3f1dcc6]. Nature Communications (2021). High credibility.

Fig. 1
The XZZX surface code.

Qubits lie on the vertices of the square lattice. The codespace is the common +1 eigenspace of its stabilizers S f for all faces of the lattice f. a An example of a stabilizer S f associated with face f. We name the XZZX code according to its stabilizer operators that are the product of two Pauli-X terms and two Pauli-Z terms. Unlike the conventional surface code, the stabilizers are the same at every face. b A boundary stabilizer. c A logical operator that terminates at the boundary. d Pauli-Z errors give rise to string-like errors that align along a common direction, enabling a one-dimensional decoding strategy. e The product of stabilizer operators along a diagonal give rise to symmetries under an infinite bias dephasing noise model. f Pauli-X errors align along lines with an orthogonal orientation. At finite bias, errors in conjugate bases couple the lines. g Pauli-Y errors can be decoded as in ref. h A convenient choice of boundary conditions for the XZZX code are rectangular on a rotated lattice geometry. Changing the orientation of the lattice geometry means high-rate Pauli-Z errors only create strings oriented horizontally along the lattice. We can make a practical choice of lattice dimensions with d Z > d X to optimise the rates of logical failure caused by either low- or high-rate errors. Small-scale implementations of the XZZX code on rectangular lattices may be well suited for implementation with near-term devices. i In the limit where d X = 1 we find a repetition code. This may be a practical choice of code given a limited number of qubits that experience biased noise. j The next engineering challenge beyond a repetition code is an XZZX code on a rectangle with d X = 2. This code can detect a single low-rate error.

---

### Logistic model in clinical and health research-the elephant and blind men [^37757288]. American Journal of Clinical Oncology (2025). Medium credibility.

For OR < 1, such as BMI OR = 0.97, it says that relative to BMI < 30 (the reference group), the patients with BMI ≥ 30 will be less likely to experience the toxicity but since the 95% CI includes OR = 1.0 (this happens when no group difference at all!) so the P value will tell the same story — no statistical significance.

To make a forest plot, we put all the OR and their 95% CI in a graph. The OR in Figure 1 is adjusted OR, that is, it is the OR when there are other covariates (take one variable for consideration so we had call others as covariates) in the model; the raw OR can be obtained from the events/total events; for example, the OR of White versus non-White = (91/230) / (5/24) ∼1.899, and the adjusted OR is 1.98.

FIGURE 1
A forest plot of the logistic model on the outcome dose-limiting toxicity (odds ratios: yes vs. no) and the predictors. The solid vertical line is OR = 1.0.

In a logistic mode, the values of the predictors can be any values, from negative infinity to positive infinity, depending upon their type: categorical, continuous, nominal, or ordinal. Why? Because p (outcome Y = 1) = 1/[1+exp (− (β o + β 1 ×1 + β 2 ×2 + β 3 ×3 +…))], if any x value approaches positive infinity, then p (Y = 1) ∼1/ [1 + exp (-∞)] ∼1/ [1 + 0] = 1; if any x approaches negative infinity, then p (Y = 1) ∼1/[1 + ∞] ∼0. This is a remarkable advantage compared with other linear regressions, where we are not able to deal with a very large value of a variable. Here ∼reads approximately, to replace equal sign = for infinity things.

A hint — a forest plot may be equipped with other clinical measurements in daily research, to replace OR, such as risk ratio, relative risk, risk difference, effect size (for instance, Cohen d = mean difference/SD), Pearson coefficient r, and so on.

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^327fd22c]. Nature Communications (2023). High credibility.

Thresholding

The curve fold change for the i th curve (cfc i) is defined as the log 2 -ratio between the lowest and highest concentration using the regressed model and it quantifies the drug's effect size or efficacy (Eq. 11).

We transferred the SAM principle of differential T-statistics to the recalibrated F-statistic to obtain equivalent decision boundaries for the dose–response curve analyses. This is possible by recognizing that a dose–response curve converges in the limit to two groups (front plateau group = not affected data points, and back plateau group = affected data points), where the curve fold change is equivalent to a conventional SAM fold change between the two plateau groups. In this case, allowing for the conversion and application of the s 0 SAM principle. CurveCurator simplified this process by calculating the tuning parameter s 0 directly from the user-specified significance and fold change asymptotes (Eq. 12). Whereis the inverse cumulative density function of an F-distribution with degrees of freedom dfn and dfd as determined in the section above. This makes s 0 also a function of the number of data points, which is relevant when a curve has missing values.

The tuning parameter s 0, which defines the hyperbolic decision boundaries, can also be used to transform the curve's recalibrated F-value into the s 0 -adjusted F-value (F adj, i) based on the global s 0 value and the curve's measured fold change (cfc i) (Eq. 13).

We then transform this s 0 -adjusted F-value into a "relevance score" using the cumulative density function F X (x | dfn, dfd). For s 0 = 0.0, this simply corresponds to the p -value of the curve. For s 0 > 0.0, this can no longer be interpreted as a p -value, but it still provides an appropriate ordering of curves by both statistical and biological relevance. Additionally, a −log 10 transformation is applied for visualization purposes and to obtain an interpretable score that ranges from 0 to infinity, where 0 has no relevance (Eq. 13).

---

### Unbiased estimation of odds ratios: combining genomewide association scans with replication studies [^5dd42c88]. Genetic Epidemiology (2009). Low credibility.

Without loss of generality we assume that the event Q: X 1 ≥ X 2 ≥… ≥ X k has occurred, so that X i = X (i). Let. The pairand Z i = (σ 2, i /σ 1, i) X i)+(σ 1, i /σ 2, i)ϒ i are then sufficient and complete statistics for μ 1,…, μ k. The joint distribution of ϒ i and X 1,…, X k given Q, f (ϒ i, X | Q) is then transformed into f (X, Z i | Q) and. The joint densityis obtained from the integral

which enables the densityto be expressed as the ratio. This is greatly simplified due to numerous cancellations, in particular the selection probabilities which are analogous to those that feature in the denominator of, and cause problems for, formula (1). Using the Rao-Blackwell theorem formula (4), which is, is the uniformly minimum variance unbiased estimator for μ (i)′, conditional on Q (we call it the UMVCUE). This means that, given the ranking in stage 1, the estimator is unbiased for the corresponding effects and has minimum variance among all such unbiased estimators.

We now propose some modifications to this formula in order to apply the same estimation procedure to a genome scan followed by replication. Instead of the magnitude of the point estimates determining the rank order, it is more common in the genomewide setting to rank SNPs according to the statistical significance of their effects, and to restrict attention to those passing an initial P -value threshold p crit. For a one-sided Wald-type test, we can make this extension by conditioning instead on the event

This leads to slight changes in the proof since conditioning on Q * as opposed to Q changes the limits of integration for X i and Y i, with the result that W s, t in (4) becomes

For (8) to work generally we define X (k +1) /σ 1,(k +1) = Φ −1 (1 – p crit). This expression was noted in, though they did not allow for a P -value threshold. If SNPs that confer either an increased or decreased disease risk are of equal interest, as is usually the case, then the rank order of significance should be based on two-sided P -values. This now requires conditioning on the event

---

### Determination of X-ray detection limit and applications in perovskite X-ray detectors [^3389b814]. Nature Communications (2021). High credibility.

Fig. 4
Detection limit obtained by the dark current method for MAPbI 3 detectors working in different mode.

a Experimentally measured current response of reversely biased #2 Pb/Au photodiode to X-ray dose rates approaching the detection limit. Calculated detection limit of net current I limit and X-ray dose rate detection limitfor the b reversely biased #2 Pb / Au photodiode, and for the c #1 Au/Au photoconductor and forward biased #2 Pb/Au photodiode. d qualitative comparison of sensitivity (slope of the linear fitting) and dark current for photodiode under forward vs reverse bias mode. The subscripted F and R stands for forward and reverse bias mode, respectively.

To obtainby the X-ray photocurrent (I dark & I X–ray) method, we lowered the X-ray dose rate successively and check if each dose rate is detected. We show below that the lowest achievable 5 nGy air s −1 is detected under reverse 2 V of the #2 Pb/Au device. We collected a large number of digitized current data points (typically ~ 500) for the dark current to makevery large so thatis negligible. We have = 17 data points for a specific I X–ray measurement under 5 nGy air s −1. The standard deviation of I X–ray, i.e. is calculated to be ~ 0.468 pA. Then we can calculate a detection limit for this specific measurement to be = 0.37 pA with z 1− α = z 1− β = 1.645. Meanwhile, the measured difference of the mean dark current and the mean photocurrent under X-ray, i.e. is 5.9 pA. Asfor this specific measurement, the dose rate of 5 nGy air s −1 is considered as detected, satisfying z 1− α = z 1− β = 1.645.

---

### Evidence against a redshift z > 6 for the galaxy STIS123627 + 621755 [^27d24850]. Nature (2000). Excellent credibility.

The identification of galaxies at extreme distances provides the most direct information about the earliest phases of galaxy formation. But at redshifts z > 5 even the most luminous galaxies appear faint; the interpretation of low signal-to-noise ratio data is difficult and misidentifications do occur. Here we report optical and near-infrared observations of the source STIS123627+621755, which was previously suggested to be at a redshift of 6.68 (ref. 1). At that redshift, and with the reported spectral energy distribution, the galaxy should be essentially invisible at wavelengths less than 9,300 A, because the intervening intergalactic medium absorbs almost all light energetic enough to ionize neutral hydrogen — that is, with wavelengths less than the redshifted Lyman limit of lambda = (1 + z) x 912A. At near-infrared wavelengths, however, the galaxy should be relatively bright. Here we report a detection of the galaxy at 6,700 A and a non-detection at a wavelength of 1.2 microm, contrary to expectations for z approximately 6.68. The data conservatively require that STIS123627+621755 has a redshift z < 6.

---

### Estimation of significance thresholds for genomewide association scans [^9c80a02e]. Genetic Epidemiology (2008). Low credibility.

PERMUTATION TEST

We estimated a genomewide significance threshold using a permutation procedure. We randomly designated half the sample "cases" and the other half "controls" and calculated the Armitage test of trend for differences in genotype frequency. P –values for all SNPs were sorted and the 1,000 smallest were recorded. This procedure was repeated 10,000 times.

The 5% quantile point of the minimum P –value represents the genomewide significance threshold at this marker density. To extrapolate to complete saturation, we randomly subsampled the SNPs over a range of lower densities by equivalently subsampling the sorted P –values independently for each permutation replicate. We used a uniform grid of 100 marker densities, and at each density the 5% point of the minimum P –value was recorded. We repeated the subsampling 100 times at each density and used the mean 5% point in subsequent analysis. At low densities, the SNPs are expected to be independent; hence, according to the Bonferroni law the 5% point is inversely proportional to the number of SNPs. At high densities we expect the 5% point to converge to an asymptote, reflecting the significance threshold for the whole genome.

Denote the proportion of SNPs subsampled by x i; i = 1,2,…, where x i < x i +1 and x i are relative to the total number of SNPs used in the estimation. Denote the corresponding 5% quantile points y i. It follows that the effective number of independent tests defined byshould be proportional to x i when x i is small and should converge to an asymptote as x i grows large. To obtain these properties, we fit the Monod function to (x i, m i):This model is not claimed to be exact, but it has been found to fit data well in applications such as modeling population growth with limited resources [e.g.]. The parameters of this model are μ, the limit as x → ∞, and k, the half–saturation parameter representing the value of x for which f (x) = μ/2. We estimated the parameters by least squares to give the genomewide significance threshold asWe used a non–parametric bootstrap to estimate confidence intervals for; genome. We resampled the minimum P –values with replacement from the permutation replicates, and for each resampling we subsampled SNPs, fitted the Monod function and calculated. We estimated 95% confidence intervals for; k andfrom 1,000 bootstrap samples.

---

### Many-body quantum chaos and space-time translational invariance [^fa7791ef]. Nature Communications (2022). High credibility.

Floquet case

Before analyzing the effect of combining time-periodicity and translation invariance, we review the calculation of K F (t, L) for the Floquet RPM (Fig. 1 a). Here the single-site unitary gates are constant in time but are random in space u (r, t) = u (r). Thus, in the diagrammatic expansion of the SFF, we can choose any permutation σ ∈ S t to pair the t –copies of u (r) in the top layer with those of u † (r) in the bottom one. In the limit of large q, only time translations contribute, i.e.with t = 0,…, t − 1. Therefore, we get a many-body diagram by choosing a configuration t (r) (color) for each site. After averaging over the random phases, it was shown that K F (t, L) = Z Potts, with Z Potts the partition function of a t -state Potts model with a Boltzmann weight across all bonds. At large t, the partition function is dominated by the t ferromagnetic groundstates where all sites have the same color, leading to the RMT prediction K F ~ t. As t approaches t Th from above, excitations from the t ferromagnetic groundstates – the lowest-lying excitation being the domain wall states– become important. To access such corrections, in 1D, one makes use of the transfer matrix to write. Computing the spectrum ofand evaluating Z Potts in the scaling limit, this leads to the scaling form (see SI)with x = L / L Th (t) and L Th (t) = e ϵ t / t. In higher dimension, Z Potts cannot be easily computed for finite L and t. Nevertheless, in the scaling limit where L, t are both large, corrections to the RMT SFF are associated with diluted excitations where the color is changed with respect to the ground state's one. The position of the excitation can be chosen inways and each of them can be assigned any of the t − 1 remaining color, with a cost e −2 n ϵ d t. As a consequence, settingand, Intriguingly, note that, in contrast with Eqs. (10) and (12) which are divergent for any x ≥ 1, Eqs. (13) and (14) remain always smooth for finite x. This can be understood observing that infinite- q TI systems are mapped onto stat-mech models with non-local interactions, so that the scaling function is associated with the exchange of distant domains (d = 1) or defects (d > 1); on the contrary for Floquet systems, the resulting Potts model has purely local interactions.

---

### Supercritical fluids behave as complex networks [^770d11e8]. Nature Communications (2023). High credibility.

With equations (7) and (8) yielding a closed analytical solution for the total number of links in the network model, we proceed to connect this model to the MD system. The total number of links, M, is assumed as a known quantity and we numerically solve for the corresponding link density z that serves as input in generating an appropriate complex network. For the network to converge to a statistically steady state, we adopt the approach of Lin et al.: the presented data is averaged over 15 N iterations with the self-consistent histogram method. The steps for generating the network are summarized as follows.
Assign a fitness value, x i, to each node i using Equation (9): x i ~ g (x; α, β).
Calculate the probability, f (x i, x j), for a link between nodes i and j using Equation (6).
Create a symmetric adjacency matrix by comparing the probability, f (x i, x j), against a random number selected from a uniform distribution between (0, 1). No self-links are allowed.
From the adjacency matrix, calculate topological properties of the network model.
Repeat steps 1–4 for 15 N times.

Because we aim for the network model to depend only on experimentally measurable quantities, we seek to relate the link density, z, to the Widom self-similarity. Because it has been shown that, in the thermodynamic limit, the hidden-variable model maintains an invariant cluster size distribution in terms of z N, we propose the following expression:which is a regression (see Supplementary Fig. 7) of the calculated z values from the total number of links, M, in the MD data via Equations (7) and (8). Because the link density z in Equation (10) is directly calculated from experimentally measurable thermodynamic quantities through Equation (4), this relation allows for a self-contained network model to predict the microscopic structure of supercritical water. We note that the generalization of this representation to other fluids requires further investigation.

---

### Negative refraction of light in an atomic medium [^29de8bc5]. Nature Communications (2025). High credibility.

Our analysis of the lateral displacement also allows us to derive scaling behaviour of the optical response as a function of the sample thickness, achieving the macroscopic 'bulk' behaviour limit of light refraction. The emergence of the refractive index from the microscopic principles is illustrated in Fig. 4 b–d where the number of layers is varied from 25 to 100, effectively maintaining the macroscopic refractive response while the collective transverse dispersion band density increases. This scaling is possible because the collective linewidth υ ∝ 1/ N x precisely compensates for the increasing layer number N x, such that (N x − 1) υ remains finite for large N x. The finite limit of (N x − 1) υ for phase-matched resonances within the transmission band is consistent with the observed 1/ N x linewidth scaling of light-coupled resonant excitations in atomic arrays, and directly demonstrates the linear dependence of D on thickness in macroscopic media.

Our examination of negative refraction, employing stacked infinite layers, offers a computationally powerful approach even for large systems. However, in realistic finite arrays, edge effects may alter the excitations and lead to scattering off the sample boundaries. Consequently, we directly calculate the transmission of light, using Eq. (1), for a finite lattice of N x × N y × N z = 5 × 25 × 25 two-level atoms in Fig. 5 a. When compared with the case of infinite in-plane arrays (Fig. 2) we observe an excellent match in the exact scattering profile for z = 0, which exhibits a negative refraction predicted by the resonant polarisation Bloch bands (Fig. 5 b) at q z = 0. These results are consistent with experimental observations of light transmission through an atomic monolayerthat demonstrated coupling in a finite layer to collective modes resembling those of an infinite array.

---

### The minimal work cost of information processing [^927873c9]. Nature Communications (2015). Medium credibility.

Classical mappings and dependence on the logical process

Our result, which is applicable to arbitrary quantum processes, applies to all classical computations as a special case. Classically, logical processes correspond to stochastic maps, of which deterministic functions are a special case. As a simple example, consider the AND gate. This is one of the elementary operations computing devices can perform, from which more complex circuits can be designed. The gate takes two bits as input, and outputs a single bit that is set to 1 exactly when both input bits are 1, as illustrated in Fig. 2a.

The logical process is manifestly irreversible, as the output alone does not allow to infer the input uniquely. If one of the inputs is zero, then the logical process effectively has to reset a three-level system to zero, forgetting which of the three possible inputs 00, 01 or 10 was given; this information can be viewed as being discarded, and hence dumped into the environment. We can confirm this intuition with our main result, using the fact that a general classical mapping is given by the specification of the conditional probability p (x ′| x) of observing x ′ at the output if the input was x. Embedding the classical probability distributions into the diagonals of quantum states, the infinity norm in expression (2) becomes simply

---

### Significance, errors, power, and sample size: the blocking and tackling of statistics [^3bcc4e96]. Anesthesia and Analgesia (2018). Low credibility.

Inferential statistics relies heavily on the central limit theorem and the related law of large numbers. According to the central limit theorem, regardless of the distribution of the source population, a sample estimate of that population will have a normal distribution, but only if the sample is large enough. The related law of large numbers holds that the central limit theorem is valid as random samples become large enough, usually defined as an n ≥ 30. In research-related hypothesis testing, the term "statistically significant" is used to describe when an observed difference or association has met a certain threshold. This significance threshold or cut-point is denoted as alpha (α) and is typically set at .05. When the observed P value is less than α, one rejects the null hypothesis (Ho) and accepts the alternative. Clinical significance is even more important than statistical significance, so treatment effect estimates and confidence intervals should be regularly reported. A type I error occurs when the Ho of no difference or no association is rejected, when in fact the Ho is true. A type II error occurs when the Ho is not rejected, when in fact there is a true population effect. Power is the probability of detecting a true difference, effect, or association if it truly exists. Sample size justification and power analysis are key elements of a study design. Ethical concerns arise when studies are poorly planned or underpowered. When calculating sample size for comparing groups, 4 quantities are needed: α, type II error, the difference or effect of interest, and the estimated variability of the outcome variable. Sample size increases for increasing variability and power, and for decreasing α and decreasing difference to detect. Sample size for a given relative reduction in proportions depends heavily on the proportion in the control group itself, and increases as the proportion decreases. Sample size for single-group studies estimating an unknown parameter is based on the desired precision of the estimate. Interim analyses assessing for efficacy and/or futility are great tools to save time and money, as well as allow science to progress faster, but are only 1 component considered when a decision to stop or continue a trial is made.

---

### Anisotropic nanocrystal superlattices overcoming intrinsic light outcoupling efficiency limit in perovskite quantum dot light-emitting diodes [^2dbddefc]. Nature Communications (2022). High credibility.

It should be noted that controlling TDM orientation is not the only approach to induce emission directionality in individual QDs. Early findings from single-molecule spectroscopy have explored the effect of emission polarization in anisotropically dielectric-confined nanostructures. Specifically, consider an isolated NC surrounded by low-dielectric-constant ligand medium, with the dielectric constants of ϵ NC and ϵ m, respectively. The local electric field within NC, E loc, induced by an external field, E, would strongly depend on the NC shape and the dielectric contrast, = ϵ NC / ϵ m, characterized by the local field factors, f i = E i loc /E i, where subscript i correspond to x, y, and z coordinates in space (Supplementary Section 1.2). Figure 1a presents a set of calculated ratios of local field factor, f x / f z, as a function of the NC aspect ratio, AR, for spheroids and square cuboids, in which f x = f y, using the dielectric constants in our synthesized LHP NCs (ϵ NC = 4.7 and ϵ m = 2.129). The cubic and spherical NCs (AR = 1) are characterized by their isotropic polarization response, which then departs from unity as the shape evolves towards asymptotic rod (AR → 0) and disk. Specifically, the ANC has its in-plane (IP, x or y direction) local field factor increasing with AR, but the other way around for the out-of-plane (OP, z direction) component. When, f x = 1 and f z = 1/(see Supplementary Section 1.2), giving the theoretical upper limit for f x / f z = .

---

### Manipulating the symmetry of photon-dressed electronic States [^cafa6800]. Nature Communications (2024). High credibility.

We introduce another useful symmetry of the crystal structure here under the z -glide mirror S z: z → − z, x → x + a /2, y → y + b /2 where a / b is the lattice constant along the AC/ZZ direction. With the definition of the orbitals in Fig. 1 a, we find, meaning(the basis in Eq. (15)). If the probe field is in the ZZ direction and the incident plane is the z − y plane, its vector potential is. In the zero q z limit, the probe matrix elementdue tois zero because of the even parities of ψ 1/2 and f and the odd parity ofunder S z. As before, linear expansion in q z gives a nonzero quadruple probe matrix element: Applied to the conduction and valence band wave functions around the Z point, we getand. The matrix elements for all the probe configurations are summarized in Supplementary Table 2. For the cases relevant to our experiment (z − y incident plane), it is obvious that Eqs. (1) and (2) for the ARPES amplitude remain the same except for an overall factor.

---

### Correlations as a resource in quantum thermodynamics [^075da07c]. Nature Communications (2019). High credibility.

The presence of correlations in physical systems can be a valuable resource for many quantum information tasks. They are also relevant in thermodynamic transformations, and their creation is usually associated to some energetic cost. In this work, we study the role of correlations in the thermodynamic process of state formation in the single-shot regime, and find that correlations can also be viewed as a resource. First, we show that the energetic cost of creating multiple copies of a given state can be reduced by allowing correlations in the final state. We obtain the minimum cost for every finite number of subsystems, and then we show that this feature is not restricted to the case of copies. More generally, we demonstrate that in the asymptotic limit, by allowing a logarithmic amount of correlations, we can recover standard results where the free energy quantifies this minimum cost.

---

### Active carpets drive non-equilibrium diffusion and enhanced molecular fluxes [^0119199e]. Nature Communications (2021). High credibility.

The resulting velocity correlation function (VCF) and mean-squared displacement (MSD) are shown in Fig. 2 b and c, respectively. The tracer dynamics are ballistic at short times, t ≪ τ, called the Holtsmark regime. However, at long times they are diffusive with a linear relation 〈Δ x 2 〉 = 2 D x x t, and similarly in the other directions. Hence, we determine the components of the diffusion tensor as a function of z 0 (Fig. 2 d). Like the flow variances, the vertical component D z z is five times stronger than the horizontal components, leading to anisotropic diffusion, and they all decay asin all three directions.

This system can be solved analytically when considering the limit of small displacements, when the noise amplitude is small. This ensures that we determine the local diffusivity, D i j (z) with small variations in z. Using information from the variance of the active fluctuations and their temporal correlations, the motion can then be integrated (see 'Methods: Derivation of the mean-squared displacement and space-dependent diffusivity'). This gives the MSD, which captures both the short-term ballistic motion and the diffusivity after long times (Fig. 2 c, dashed red lines). Thus, for the vertical Stokeslets for example, we find the space-dependent diffusion, which is compared with the simulations in Fig. 2 d. Because our theoretical approximation is formulated for small amplitudes of active fluctuations, the expression only holds far from the surface, when(Eq. (54)). Beyond this distance we find a good agreement between the simulations and the theory.

Overall, we find that the diffusion driven by an active carpet can be much stronger than Brownian thermal diffusion. Considering a carpet of Vorticella with cell radius a ~25 μm, h ~150 μm, n ~1/(100 μm) 2, τ ~1 s and 8 π μ f ⊥ ~500 pN, the active diffusion can be dominant up to distances ofmm for small nutrient molecules of D th ~ 500 μm 2 /s. That is much larger than the cell size. Moreover, for micron-sized prey of D th ~0.5 μm 2 /s, we find z th ~7 mm, orders of magnitude larger than the organism itself. Hence, we expect these results to be highly relevant across the scales, for non-equilibrium transport from molecular to organismic sizes.

---

### Mind your p values [^e7f228df]. The Journal of Foot and Ankle Surgery (2012). Low credibility.

In a medical study, the goal is to convince readers that a compelling argument has been made to prove the investigators' case. The logic of how this proof is provided in medical studies can be confusing, and is discussed here in simple terms.

---

### Control of finite critical behaviour in a small-scale social system [^c81152c0]. Nature Communications (2017). Medium credibility.

Independent model inference

The independent model consists of individuals participating in conflict randomly, with the frequencies of individual appearance equal to their empirically measured, heterogeneous values f i = 〈 x i 〉. Naively, this can be written as a relative negative log likelihood, and is equivalent to a maximum entropy model that matches only the frequencies f i. (The relative negative log likelihood L (x) of state x is related to the likelihood p (x) by p (x) = exp(− L (x))/ Z, where Z is a normalization constant set by the constraint that the sum of likelihoods over all states is one. In statistical physics, Z is the partition function and L (x) is proportional to the free energy of state x.)

However, as detailed in 'Operational definitions' in the Methods section, a fight was operationalized for these analyses as involving two or more socially mature individuals. (Observed fights that involved only juveniles and 0 or 1 mature individuals are therefore excluded.) Correspondingly, we forbid our models from producing fights of size smaller than two. We treat this as an additional constraint on the model. The resulting maximum entropy model is then the one in which the likelihood of states with fewer than two individuals present is taken to zero. This corresponds to a relative negative log likelihood

where Θ(z) is 0 when z ≤ 0 and 1 when z > 0.

In the unconstrained case (α = 0), we can easily solve for h i:

We must now solve numerically for h i to match the empirically measured f i = 〈 x i 〉 = 〈 x i 〉 α →∞. To accomplish this, note that the unconstrained model will have modified statistics:

where f 0 is the frequency of size zero fights, f i 1 is the frequency of size one fights consisting solely of individual i, andis the overall frequency of size one fights (all measured in the unconstrained model). In terms of unconstrained individual frequencies, these are

We use an iterative procedure to solve equation (7) for 〈 x i 〉 α = 0, which are then used in equation (6) to find the fields. Finally, samples from the independent model (equation (4)) are produced by sampling using equation (5) and simply discarding samples in which fewer than two individuals appear. For our data, this results in discarding about 17% of samples produced with equation (5).

---

### Input-output maps are strongly biased towards simple outputs [^50fca0dd]. Nature Communications (2018). Medium credibility.

Many systems in nature can be described using discrete input-output maps. Without knowing details about a map, there may seem to be no a priori reason to expect that a randomly chosen input would be more likely to generate one output over another. Here, by extending fundamental results from algorithmic information theory, we show instead that for many real-world maps, the a priori probability P(x) that randomly sampled inputs generate a particular output x decays exponentially with the approximate Kolmogorov complexity [Formula: see text] of that output. These input-output maps are biased towards simplicity. We derive an upper bound P(x)≲[Formula: see text], which is tight for most inputs. The constants a and b, as well as many properties of P(x), can be predicted with minimal knowledge of the map. We explore this strong bias towards simple outputs in systems ranging from the folding of RNA secondary structures to systems of coupled ordinary differential equations to a stochastic financial trading model.

---

### An alternative to the breeder's and lande's equations [^1313df68]. G3 (2014). Low credibility.

Selection on multiple traits

The results of the aforementioned sections are naturally generalized to selection on multiple traits. Consider the vectors of parental breeding values y 0 = (y 1, y 2,…, y N), environmental effects e = (e 1,…, e N) and their phenotype z 0 = y 0 + e, to which a selection function W (z) is applied. Using the same notations as in the previous sections, we find without difficulty thatAs before, using FT, these relations transform intowhere ∇ is the gradient operator: ∇ f = (∂ f /∂ x 1,… ∂ f /∂ x N). We see again thatandare linearly related ifwhere A is a constant matrix. The linear relation is automatically satisfied if both p 0 and f follow a Gaussian distributionwhere G and E are the covariance matrices for the genotype and environmental effects. Defining P = G + E as the phenotype covariance matrix, it is straightforward to show that in this case A = EG −1 and therefore which is the usual breeder's equation for multiple traits. We stress that the limitation of this relation is the same as that of the scalar version: it relies on the normal distribution of the genotype. On the other hand, if the selection function W (z) is Gaussianthe arguments of the previous section 2 can be repeated and lead to the generalization of the alternative vectorial breeder's equation (21)which, in analogy with equation (21) we write as

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^59838d2f]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Sample size metrics — tolerance intervals versus confidence intervals: Although performance is often stated in terms of confidence intervals (CI), the CI of the mean only gives an estimate of the population mean with a stated level of confidence and does not give an indication of the performance of any given sample; to estimate the distribution of the underlying population and the performance of individual samples, the tolerance intervals should be used, with the lower tolerance interval for a normally distributed population determined as x̄ ± k × s, where s is the sample standard deviation (SD) and k is a correction factor, and for two-sided 95% confidence with n = 20 the k value is 2.75, approaching the z-score as the number of samples increases.

---

### Article 5. An introduction to estimation – 2: from z to T [^fb82b806]. Emergency Medicine Journal (2001). Low credibility.

Provided the sample size is large enough (that is, n greater than 100), the z statistic can be used to determine the confidence interval estimation of the population mean even when the sigma is not known. In these cases the estimation of the standard error of the mean is used. The z statistic is also valid when determining the population's proportion based upon a large sample. However, when dealing with smaller samples, the z statistic is replaced by the t statistic. This makes it possible to estimate, in a population with an unknown standard deviation: The probability of getting a sample mean greater than or equal to a particular value The value of a sample mean with a particular probability of occurring The probability of getting a sample mean between two particular values The confidence interval for the estimation of the population mean can also be determined using the t statistic.

---

### Correction to: a method to quantify the "cone of economy" [^c425b20a]. European Spine Journal (2018). Low credibility.

Unfortunately, in the abstract at the results section units have been published incorrectly.

---

### Assessing the agreement of biomarker data in the presence of left-censoring [^ec888b09]. BMC Nephrology (2014). Low credibility.

Methods

To find the optimal method to deal with left-censored data, we investigate how data deletion and simple data imputation methods compare to the ML approach in a computer simulation study. We adapt the framework from Barnhart et al. for our computer simulation studies. In all simulations described in the results, we generate bivariate normal data for paired data represented by the variables X and Y with a sample size of 100, 50, 25 for each of 1000 data sets using one of the following six combinations of parameter settings for the means, standard deviations, and correlation coefficient: μ x = 0, μ y = 0.2, σ x = 0.8, σ y = 1, ρ = 0.25, 0.50, 0.75, and left-censoring rates of (25% for X, 25% for Y) or (40% for X, 25% for Y). The selected values of the LLDs in the simulation study are determined by the censoring rates. All calculations are performed using SAS 9.3 statistical software. All estimated CCCs were obtained by maximizing the likelihood function with respect to each of the following five scenarios.

1. Deleting the pair method means that pairs with X, Y, or both X and Y below the detection limit are discarded before calculation of the CCC. The 95% confidence interval (CI) of this method is calculated by usingwhereis the estimated CCC, Z 0.025 is the critical value of the standard normal distribution, andis the standard error of the estimated CCC.

2. Replacing the left-censored data by the LLD method refers to the use of the CCC after replacing all non-detectable data by the applicable detection limit. The 95% confidence interval (CI) of this method is calculated by usingwhereis the estimated CCC, Z 0.025 is the critical value of the standard normal distribution, andis the standard error of the estimated CCC.

3. Replacing the left-censored data by one-half of the LLD method refers to the calculation of the CCC using all pairs after replacing the non-detectable data with 0.5 times the detection limit. The 95% confidence interval (CI) of this method is calculated by usingwhereis the estimated CCC, Z 0.025 is the critical value of the standard normal distribution, andis the standard error of the estimated CCC.

---

### In vitro implementation of robust gene regulation in a synthetic biomolecular integral controller [^deffcce2]. Nature Communications (2019). High credibility.

To determine an approximate analytical expression for the slope of deGFP, we used the simplified model to obtain an approximate analytical solution of Z (see Supplementary Note 1). We then analytically calculated the time derivative of G for the open and closed-loop cases (Fig. 4d). The analytical solution follows the response of the original model closely at different reaction conditions (Fig. 4e, f; Supplementary Fig. 15), and thus, validating our approach. Moreover, we found that the time derivative of G (same as slope of deGFP) follows the input P X linearly only in the closed-loop case as long as there are enough promoter sites are available on y and z genes. This is due to the fact that in the closed-loop configuration, Z is independent of the promoter activity of z gene as long as X ≫ ω/ν is not valid. This is considered as a fundamental limit of the controller to track the reference signal robustly. In contrast, in the open-loop case, as the sequestration reaction is absent, production of Z directly depends on the promoter activity of z gene which causes a nonlinear dependency of Z on X, thereby on P X (Fig. 4d). In the closed-loop case when X ≫ ω/ν, Z converges to a saturation. We verified these conclusions using the numerical simulations (see Supplementary Fig. 16). Moreover, the closed-loop controller capability to track the reference signal should be independent of the absolute value of the output when X ≫ ω/ν is not valid. To test this, we conducted numerical simulations at different initial DNA concentrations values for the open and closed-loop cases in such a manner that the controller's output remains in the proximity in these two cases. We observed linear relation between the input P X and the output only for the closed-loop case (Supplementary Fig. 17).

Further insight into the controller operation can be gained by analyzing the simplified model to determine how the error signal is processed in the closed-loop configuration of the controller. The analytical expression for the error signal clearly shows that it is integrated mathematically by the controller (see Supplementary Note 1). We then further validate the conditions under which Z faithfully tracks the plant signal, which ensures accurate reading of the controller dynamics (see Supplementary Note 1).

---

### Do CIs give you confidence? [^0c65ec35]. Chest (2012). Low credibility.

This article describes the conceptual basis for the P value and the CI. We show that both are derived from the same underlying concepts and provide useful, but similar information.

---

### Beyond the P: II: precluding a puddle of P values [^1cd5f00e]. Journal of Cataract and Refractive Surgery (2004). Low credibility.

This is the second in a 4-part series discussing the proper use, interpretation, and limitations of P values.

---

### Thinking outside the curve, part II: modeling fetal-infant mortality [^3a0b6eae]. BMC Pregnancy and Childbirth (2010). Low credibility.

A confidence interval for r j (x 0) is obtained by applying the inverse logit transformation to the confidence interval for θ. The above computations can be performed simultaneously at a series of birthweights. Connecting the resulting series of upper confidence limits produces an upper confidence bound for the risk function in component j, while connecting the resulting series of lower confidence limits produces a lower confidence bound.

As when constructing confidence intervals for p j, μ j, and σ j (1 ≤ j ≤ k), we can accommodate overlap in the N rep samples by choosing the value of C in Equation (6) according to the fraction of the underlying population that each of the N rep samples constitutes. Let C 0 denote the value of C that would be chosen if this fraction were negligibly small, and let C φ denote the value that would be chosen if this fraction were equal to φ, a positive number less than 1. In the previous paper, we established the relationship

---

### Quantifying randomness in real networks [^e1049c29]. Nature Communications (2015). Medium credibility.

Results

General requirements to a systematic series of properties

The introductory remarks above instruct one to look not for a single base property Y, which cannot be unique or universal, but for a systematic series of base properties Y 0, Y 1,… By 'systematic' we mean the following conditions: (1) inclusiveness, that is, the properties in the series should provide strictly more detailed information about the network structure, which is equivalent to requiring that networks that have property Y d (Y d -random graphs), d > 0, should also have properties Y d ′ for all d ′ = 0, 1,…, d −1; and (2) convergence, that is, there should exist property Y D in the series that fully characterizes the adjacency matrix of any given network, which is equivalent to requiring that Y D -random graphs is only one graph — the given network itself. If these Y -series satisfy the conditions above, then whatever property X is deemed important now or later in whatever real network, we can always standardize the problem of explanation of X by reformulating it as the following question: what is the minimal value of d in the above Y -series such that property Y d explains X? By convergence, such d should exist; and by inclusiveness, networks that have property Y d ′ with any d ′ = d, d +1,…, D, also have property X. Assuming that properties Y d are once explained, the described procedure provides an explanation of any other property of interest X.

---

### Constrained instruments and their application to mendelian randomization with pleiotropy [^1b99dfc2]. Genetic Epidemiology (2019). Medium credibility.

In the presence of pleiotropic phenotypes Z (α z ≠ 0 in Figure 2), any method that conditions on Z would induce collider bias. The main advantage of CIV_smooth is to propose a selection of valid instruments G ∗ that are meant to approach, as closely as possible, the ideal situation, α z ∗ = 0. Nevertheless, collider bias in CIV_smooth will still be induced when the pleiotropic correlation between G ∗ and Z is high in absolute values. However, Simulation Series I and II show that CIV_smooth is more robust than sisVIVE and 2SLS methods even though spurious association may have been introduced by the constrained projections (see Figures 7, 8, 9 and 13, 14, 15). We plan additional investigation in future work.

In this paper we did not consider scenario (iii) of Section 2.1 corresponding to γ xz ≠ 0, in which the total causal effect of X on Y includes a contribution through Z. In this scenario, we do have a true X → Z relationship with total causal effect of β + γ xz η. Therefore sisVIVE is unlikely to perform well if there are pleiotropic genotypes since by its definition all genotypes are invalid for X (a valid genotype only impacts Y through X). Also, neither CIV_naive nor CIV_smooth estimate β + γ xz η, and generalizations do not seem to be easy. Although some preliminary simulation results including an X → Z causal relationship did show that CIV_smooth may have potential in this scenario, in that the direct causal effect β of X on Y can be estimated adequately, a good estimate of the total causal effect requires extensive changes to the present methodology, and is beyond the scope of this paper. Leaving out scenario (iii) is certainly a limitation of this study that will require further careful research. On the other hand, even with this limitation the results of this paper have practical applications. Indeed, it may be plausible that any correlation between Z and X is due to a "common cause" and not to any causal relationship X → Z or Z → X, in which case this "common cause" would be absorbed by U and fall under the case we consider here (both γ xz and γ zx are zero). We note further that conditioning on Z when γ xz ≠ 0 may exacerbate collider bias.

---

### Cutting edge or blunt instrument: how to decide if a stepped wedge design is right for you [^471f52f2]. BMJ Quality & Safety (2021). High credibility.

When might I consider doing a stepped wedge trial?

Research designs are shaped as much by practical constraints as by abstract schemes, and it is always a good idea to start with the constraints and work towards a design, rather than start with a design and try to fit it to constraints. These constraints will be unique to each research context, and box 1 lists some areas to think about. Still, there are some common features of settings where a stepped wedge trial might be considered as a possible design, and we now review these.

Box 1
Practical constraints on the design of a longitudinal cluster randomised trial

Are there limits on the time available to complete the evaluation, on the number of clusters, or on the number of participants (or the rate at which you can recruit participants) at each cluster? These constraints put limits on the overall scale of the evaluation, or force trade-offs between different design characteristics.
How will participants and their data be sampled in your study: as a series of cross-sectional surveys, as a continuous stream of incident cases, as a cohort followed over time, or some other way? Does the timescale divide into cycles, seasons or milestones that influence how you will sample participants and data?
Is there a limit on how many clusters can implement the intervention at the same time in the evaluation? If this is constrained by research resources (eg, if there are only enough trained research staff to implement the intervention one cluster at a time) then implementation must be staggered in some way.
If implementation is to be staggered, is there a minimum 'step length'? If the same team delivers the intervention in different clusters at different steps, then bear in mind it may take some time to get the intervention fully operational at a site, and the team will also need time to relocate from one cluster to the next.

---

### A nearby long gamma-ray burst from a merger of compact objects [^1503897e]. Nature (2022). Excellent credibility.

GRB distance scale

We investigate the joint X-ray/UV/optical SED at 1 h to place a direct upper limit on the GRB distance scale. UVOT spectra were created with the tool uvot2pha using the same source and background regions selected for photometry. We adopt a power-law model and include the effects of absorption, dust reddening and intergalactic medium attenuation as implemented in the XSPEC models zphabs, zdust and zigm. The Galactic absorption was fixed to N H = 1.76 × 10 20 cm −2 and the reddening at E (B − V) = 0.015 mag. All other parameters were left free to vary. We increase the redshift from 0 to 2.5 in steps of 0.1 and find the best-fit model by minimizing the Cash statistics, recording its value at each step. On the basis of the variations of the test statistics, we derive an upper limit of z < 2.3 (99.9% CL) from the UV/optical data, and z < 1.5 (99.9% CL) from the joint X-ray/UV/optical fit. By imposing the redshift of the putative host galaxy, z ≈ 0.0762, we find no evidence for any dust extinction or absorption at the GRB site with 3 σ upper limits of E (B − V) z < 0.005 mag and N H, z < 9 × 10 19 cm −2, respectively. This is consistent with the location of the GRB, well outside the galaxy's light.

---

### Thinking outside the curve, part II: modeling fetal-infant mortality [^27ea1e69]. BMC Pregnancy and Childbirth (2010). Low credibility.

a. Simulation study to calibrate confidence intervals

We simulated 25 overlapping data sets of size 50,000, the degree of overlap consistent with a population of 200,000, based on the specifications in Table 2. The mixture density and the risk functions were chosen to mimic the patterns actually observed for white singletons born to heavily-smoking mothers; see panel b of Figure One from the previous paper and Figure 1d of the present paper. For each of various C between 2.0 and 5.0, we used Equation (6) to form confidence intervals for mortality risks at selected birthweights, namely r 1 (μ 1 - σ 1), r 1 (μ 1), r 1 (μ 1 + σ 1), r 2 (μ 2 - σ 2), r 2 (μ 2), r 2 (μ 2 + σ 2), r 3 (μ 3 - σ 3), r 3 (μ 3), r 3 (μ 3 + σ 3), r 4 (μ 4 - σ 4), r 4 (μ 4), and r 4 (μ 4 + σ 4). Above, μ j and σ j denote the mean and standard deviation of the birthweights in component j (1 ≤ j ≤ 4). This was repeated nine more times, and we tabulated how many of the 120 = 12 × 10 confidence intervals contained their targets. Confidence intervals were also formed using Equation (5) for comparative purposes. The above steps were repeated with overlapping data sets consistent with a population of 1,000,000 and with nonoverlapping data sets consistent with an effectively infinite population.

Table 2
Mixture Model and Mortality Functions for Simulation Study

The probability density for the mixture model used in our simulation study is specified, as are the mortality risk functions associated with the mixture model components. Above, z is defined as (x - 3000)/1000, where x is birthweight in grams.

The results are summarized in Table 3. With an effectively infinite population, only 75.0% of the confidence intervals formed using Equation (5) contained their targets at C = 5.0. On the other hand, the confidence intervals formed using Equation (6) contained their targets 95.0% of the time at C = 4.0. The latter finding provided the rationale for taking C 0 = 4.0 when constructing confidence intervals for mortality risks in our examples with real data.

---

### The need for data harmonization – a response to boden and ozonoff [^68de19d2]. American Journal of Industrial Medicine (2010). Low credibility.

Boden and Ozonoff's undercount estimates in their recent Commentary rely on three assumptions for which no quantitative literature references are provided. Alternatively, we show that findings in both studies and published data indicate lower upper-bound estimates for the undercount range. Am. J. Ind. Med. 53:854–855, 2010. (c) 2010 Wiley-Liss, Inc.

---

### Roflumilast (Zoryve) [^e2de6cd7]. FDA (2024). Medium credibility.

The dosage of roflumilast TOP for treatment of plaque psoriasis in adults is 1 application(s) TOP daily (0.3% cream or foam)

---

### Estimation of significance thresholds for genomewide association scans [^72c784bb]. Genetic Epidemiology (2008). Low credibility.

The question of what significance threshold is appropriate for genomewide association studies is somewhat unresolved. Previous theoretical suggestions have yet to be validated in practice, whereas permutation testing does not resolve a discrepancy between the genomewide multiplicity of the experiment and the subset of markers actually tested. We used genotypes from the Wellcome Trust Case-Control Consortium to estimate a genomewide significance threshold for the UK Caucasian population. We subsampled the genotypes at increasing densities, using permutation to estimate the nominal P-value for 5% family-wise error. By extrapolating to infinite density, we estimated the genomewide significance threshold to be about 7.2 x 10(-8). To reduce the computation time, we considered Patterson's eigenvalue estimator of the effective number of tests, but found it to be an order of magnitude too low for multiplicity correction. However, by fitting a Beta distribution to the minimum P-value from permutation replicates, we showed that the effective number is a useful heuristic and suggest that its estimation in this context is an open problem. We conclude that permutation is still needed to obtain genomewide significance thresholds, but with subsampling, extrapolation and estimation of an effective number of tests, the threshold can be standardized for all studies of the same population.

---

### Clotrimazole [^b5a7188a]. FDA. Low credibility.

The dosage of clotrimazole OTIC for treatment of otomycosis in adults is 1 vial OTIC BID for 14 days (1%/0.17 mL)

---

### What is the value of a p value? [^dfc52b0a]. The Annals of Thoracic Surgery (2009). Low credibility.

Successful publication of a research study usually requires a small p value, typically p < 0.05. Many clinicians believe that a p value represents the probability that the null hypothesis is true, so that a small p value means the null hypothesis must be false. In fact, the p value provides very weak evidence against the null hypothesis, and the probability that the null hypothesis is true is usually much greater than the p value would suggest. Moreover, even considering "the probability that the null hypothesis is true" is not possible with the usual statistical setup and requires a different (Bayesian) statistical approach. We describe the Bayesian approach using a well-established diagnostic testing analogy. Then, as a practical example, we compare the p-value result of a study of aprotinin-associated operative mortality with the more illuminative interpretation of the same study data using a Bayesian approach.

---

### Statistical laws of stick-slip friction at mesoscale [^23513423]. Nature Communications (2023). High credibility.

Figure 2 shows the measured probability density function (PDF) P (f c) of the normalized maximal force, where 〈 F c 〉 andare, respectively, the mean and standard deviation of F c. It is seen that the measured PDFs P (f c) for the quasi-1D probe (black circles) and 2D probe (red triangles) overlap with each other, suggesting that the measured P (f c) is an intrinsic property of the stick-slip friction on the sandpaper and is not sensitive to the shape and dimensions of the scanning probe used. The two sets of data can be well described by the generalized extreme value (GEV) distribution, where z = (f c − μ)/ β is the standardized variable withand μ = β [1 − Γ(1 − ξ)]/ ξ being, respectively, the scale and location parameters of the distribution function, and Γ(x) is the gamma function. Equation (1) is used to model the distribution of extreme values in a sequence of independent and identically distributed random variables with zero mean and unity variance so that the normalized maximal force f c is used as the random variable in Eq. (1) (see SI Section II.D.1 for more details). The solid lines in Fig. 2 show the fits of Eq. (1) to the data points with only one fitting parameter ξ. For the data obtained with the quasi-1D probe (black circles), we find ξ = − 0.13 ± 0.06. For the data obtained with the 2D probe (red triangles), we find ξ = − 0.03 ± 0.05. For ξ = 0, Eq. (1) is reduced to the Gumbel distribution, which has an exponential tail at large values of z without an upper bound for the local maximal force. Evidently, the data shown in Fig. 2 are not far from this limit. Figure 2 thus demonstrates that the maximal force F c is a depinning force for local slips, which follows the extreme value statistics.

---

### Ultra-wideband optical coherence elastography from acoustic to ultrasonic frequencies [^b0673957]. Nature Communications (2023). High credibility.

Noises and time jitter correction for ultrasonic frequencies

Various noises ultimately limit the system's sensitivity to vibration and the accuracy of wave velocity measurement. The fundamental limit comes from optical SNR, X, in A-lines. The length of the modulation-induced phasor is r (z 0) k 0 δ (z 0). The minimum detectable amplitude δ min is then given by r (z 0) k 0 δ min (z 0) = r min (z 0 − z m), where r min (z 0 − z m) = r min (z 0) = r (z 0) X (z 0) −0.5 is the minimum detectable reflection coefficient by definition. So, k 0 δ min = X −0.5, same for non-aliased and aliased regimes. We note that although δ (z 0) is obtained from sidebands at z 0 ± z m, its error is governed by the SNR of the main peak at z 0. In OCE, δ is typically extracted from a set of M-mode data consisting of N A-lines. Then, the sensitivity is improved by, and k 0 δ min = X −0.5 N −0.5. For the above experiment, where N = 108 and X = 10 4, we find δ min = 195 pm. Another fundamental noise source is 1/f electrical noise. SNR at low frequencies may be limited by the 1/f noise rather than the reflectivity-limited phase noise. Mechanical vibrations of optical components add noises. In our system, the mechanical jitters of beam scanners and rotating polygon filters were suppressed to practically acceptable levels.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^ed0b69c2]. The Journal of Molecular Diagnostics (2017). Medium credibility.

NGS somatic variant detection — At a sequence quality equivalent to a Phred Q score of 20, the probability of getting five or more errors at a particular base would be 10.78%, and the probability that five or more random errors would all have the same nucleotide change is 0.01%. However, not all errors are random and platform-specific systemic errors do occur, so estimation of needed depth of coverage using the binomial distribution is only an estimate and determination of false-positive and false-negative rates for a given depth and threshold must be validated. We recommend a minimal depth of coverage > 250 reads per tested amplicon or target for somatic variant detection, and in certain limited circumstances, minimal depth < 250 reads may be acceptable but the appropriateness should be justified based on intended limit of detection, the quality of the reads, and tolerance for false-positive or false-negative results.

---

### Statistical analysis of high-dimensional biomedical data: a gentle introduction to analytical goals, common approaches and challenges [^4b7b59f5]. BMC Medicine (2023). Medium credibility.

Hypothesis testing is often operationalized by calculation of a p -value from the observed data, which estimates the probability of observing a value of the test statistic that is at least as extreme as that observed, assuming that the null hypothesis was true. (Note the correct definition of a p -value stated here, in contrast to the common misinterpretation of a p -value as the probability that H 0 is true). A significance test is performed by comparing the computed p -value to the prespecified α level. When the p -value is less than or equal to α (e.g. 0.05 in the conventional setting), the null hypothesis is rejected; otherwise, it cannot be rejected.

It should be mentioned that sometimes the goal of a scientific study is to estimate certain parameters of interest, for example means or correlations, rather than to test hypotheses. In estimation settings, it is generally desired to provide intervals of uncertainty, such as confidence intervals, to accompany parameter estimates. Although errors in hypothesis testing have some relation to confidence interval coverage probabilities, most of the multiple testing procedures discussed in this section are not readily applicable to multiple estimation. Multiple estimation procedures are beyond the scope of the present discussion.

---

### Improved figure of merit (z) at low temperatures for superior thermoelectric cooling in Mg (Bi, Sb) [^3cd8d9d4]. Nature Communications (2023). High credibility.

Grain growth usually performs at high temperatures, posing difficulties in deliberate control of the content of volatile reactants and meanwhile the raw materials could react with the crucible, leading to deviation from the stoichiometry of the target composition. To solve this problem, we hereby developed a universal approach to synthesize a variety of coarse-grained alloys or compounds containing active elements, e.g. Mg and Sb, represented by the Zintl phase Mg 3 (Bi, Sb) 2. Following this method, we have prepared bulk polycrystal n-type Bi-rich Mg 3 Bi 2- x Sb x (x = 0.5, 0.75) with grain size up to 1.0 mm. Such grain size encompasses almost two-third of the length of a thermoelectric leg in the devices. The chemical composition, defects and grain size of Mg 3 Bi 2- x Sb x materials were successfully regulated to approach the limit of carrier mobility for a single crystal. Combined with the intrinsically low lattice thermal conductivity, the z values of both Mg 3 Bi 1.5 Sb 0.5 and Mg 3 Bi 1.25 Sb 0.75 increased significantly below and at room temperatures, which is beneficial for the cooling purpose. Moreover, the as-grown coarse-grained bulk crystals (with a dimension of) also exhibit excellent uniformity and mechanical performance, bringing great advantages for subsequent thermoelectric device fabrication in this work. This technique is also applicable for the crystal growth of MgAgSb, Zintl phase, and other Mg-based alloys, which contain reactive and volatile elements (such as alkaline metals, and rare earth elements).

---

### Sticky collisions of ultracold RbCs molecules [^555689ec]. Nature Communications (2019). High credibility.

Measurement of trap frequencies

We measure the trap frequencies experienced by the molecules by observing centre-of-mass oscillations in the optical potential. We also compare the oscillation frequencies for the molecules to those of atoms in the same potential. For the results shown in Fig. 1, we find (ω x, ω y, ω z) = 2 π × (181(2), 44(1), 178(1)) Hz, where z is in the direction of gravity.

Measurement of temperatures

The initial temperature of the molecules is measured by ballistic expansion in free space. Due to the small number of molecules, we can only image the cloud over an expansion time of ~2 ms. For comparison, we also measure the temperature of atoms by the same method in similar trapping conditions. We find good agreement between the temperature of the molecules and that of the atoms. We do not measure the variation of temperature as a function of time during the loss measurement, as the loss of molecules further limits the maximum expansion time available leading to unreliable temperature measurements.

The rate Eq. (2), which we use to model the loss, depends on both N mol (t) and T (t). As described in the main text, we fit N mol (t) for a fixed initial T, allowing the temperature to evolve as a function of time within the constraints of the model. We have also fitted our results assuming the molecules remain at their initial temperature throughout the measurement. In this limit, we find that our results are still consistent with a two-body process. For the results in Fig. 1, we extract k 2 = 3.8(5) × 10 −11 cm 3 s −1.

---

### Bayesian prediction intervals for assessing P-value variability in prospective replication studies [^ee55d352]. Translational Psychiatry (2017). Low credibility.

Table 1
Binomial probabilities for 80% prediction intervals, using a two-sample Z -test

The table illustrates the effect of thresholding, applied to observed P -values, e.g. selection of statistically significant P -values at 5% level, on binomial probabilities

Mixture Bayes intervals were included in these simulations to check how well they approximate a continuous prior distribution assumed by the conjugate intervals. We used mixture components with the length σ 0 /8 for every component and truncated the normal prior at 10 −6 and 1−10 −6 quantiles. This provided us with sufficient accuracy and resulted in the number of mixture components, B, equal to 76 for all values of.

Table 1 clearly indicates that all three construction methods have the correct coverage (~80%) if a prediction interval is calculated for a randomly observed P -value ∈[0,1]. However, selection and small prior variance both impair performance of P -intervals. For instance, if an interval is constructed for a P -value < 0.001 and, the coverage of the traditional non-Bayesian P -interval may be as low as 17%. This poor coverage is due to a combination of both the selection bias and the implicit assumption that prior variance ofranges from negative infinity to positive infinity, which leads to the left-side P -interval endpoint being too close to zero. However, even for large values of prior, the P -interval has poor coverage when constructed for P -values around genome-wide significance levels (e.g. P -value < 1.5 × 10 −7). On the other hand, for large P -values the coverage of P -intervals becomes greater than the nominal (1 − α)% value. This is a consequence of the fact that P -interval's width depends on the magnitude of P -values and as P -values become larger, the width of the interval increases as well. For example, given the prior variance, the width of P -intervals and the Bayesian intervals coincides at P -value = 0.446 (hence, the Bayesian intervals are wider than P -intervals at values smaller than 0.446). The P -interval around = 0.446 is: 0.147 ≤ P ≤ 0.995, while the Bayesian interval is 0.110 ≤ P ≤ 0.958.

---

### Foundations of the minimal clinically important difference for imaging [^72f785d9]. The Journal of Rheumatology (2001). Low credibility.

This article develops a generic conceptual framework for defining and validating the concept of minimal clinically important difference. We propose 3 approaches. The first uses statistical descriptions of the population ("distribution based"), the second relies on experts ("opinion based"), and a third is based on sequential hypothesis formation and testing ("predictive/data driven based"). The first 2 approaches serve as proxies for the third, which is an experimentally driven approach, asking such questions as "What carries the least penalty?" or "What imparts the greatest gain?" As an experimental approach, it has the expected drawbacks, including the need for greater resources, and the need to tolerate trial and error en route, compared to the other 2 models.

---

### Elacestrant (Orserdu) [^25e5bc7d]. FDA (2024). Medium credibility.

The dosage of elacestrant PO for treatment of breast cancer in postmenopausal female adults with ESR1 mutation (advanced or metastatic, ER-positive, HER2-negative, progression after ≥ 1 line of endocrine therapy) is 345 mg PO daily until disease progression or unacceptable toxicity

---

### Why we need confidence intervals [^d6fe7644]. World Journal of Surgery (2005). Low credibility.

The estimation approach to statistical analysis aims to quantify the effect of interest as an "estimate" of a clinically relevant quantity and to quantify the uncertainty in this estimate by means of a confidence interval (CI). As such, results expressed in this form are much more informative than results presented just as p values. This article focuses on the principles rather than the mathematics of CIs and discusses interpretation of CIs and some common misuses. CIs can be constructed for almost all analyses. They are especially useful for avoiding misinterpretation of nonsignificant results of small studies. CIs should be provided routinely for the main results of trials and observational studies.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^d3e55ccc]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Read depth planning example — confidence versus tolerance interpretation: For example, to determine the probability of getting a minimum of 250 reads for a given region, if a validation set shows a mean depth of coverage 275 reads and a standard deviation (SD) of 50 reads, after running 100 samples the 95% lower confidence limit indicates confidence that the average depth of coverage would be > 266, whereas the 95% lower tolerance interval indicates that for any given sample one could only be confident of reliably getting a read depth of 179 or greater.

---

### Controlling light in complex media beyond the acoustic diffraction-limit using the acousto-optic transmission matrix [^62dff92e]. Nature Communications (2019). High credibility.

AOTM using two ultrasound beams

The first singular vector of a single AOTM (Figs. 1–2) allows focusing only at a single point located at the center of the ultrasonic focus. Given the large number of measurements required for such focusing, this forms a limitation for the use of this approach for applications such as imaging. However, as was recently shown by Judkewitz et al. in their TROVE work, a joint analysis of several matrices of acousto-optically modulated output modes, measured for different ultrasound foci positions, can allow focus scanning. Here, we apply the same mathematical approach to jointly decompose multiple AOTMs measured for two different ultrasound focus position. Specifically, we exploit the joint analysis of two AOTMs to perform a scan of a tight optical focus over multiple positions. For this purpose, we consider the separate measurement of two AOTMs, T 1 and T 2, using two different ultrasound focused beam that are spatially shifted such that the two ultrasound focal spots, P 1 (x, y) and P 2 (x, y), partially overlap (Fig. 3 a–d). Such ultrasound focal spots are easily obtained in the same experimental setup presented in Fig. 2 by changing the time-delay, Δτ = z/v US, between the ultrasound pulse and the optical pulses (Fig. 3a), where z denotes the axial distance of the acoustic focal spot from the ultrasound transducer, and v US is the speed of sound in the medium. Injecting the first singular vector of T 1 or T 2 would only form a tight optical focus at the center of each of the two ultrasound foci. However, the joint information in the two matrices can be exploited to scan even a sharper focus along the axis connecting the centers of the two acoustic foci (Fig 3i–k).

---

### When to p and when not to p [^f5db1073]. Neurogastroenterology and Motility (2023). Medium credibility.

The p-value was proposed in the early 20th century as a potentially useful metric for statistical inference and was defined as "the probability of the observed result, plus more extreme results, if the null hypothesis were true", in the context of a formal statistical testing process. A century later, the scientific community uses it extensively, mostly inappropriately, and often interprets it incorrectly. This editorial briefly reviews the history of the p-value, provides how to properly interpret it, and recommends when to use it and when not to use it.

---

### Multisociety guideline on reprocessing flexible GI endoscopes and accessories [^33ef4f8a]. Gastrointestinal Endoscopy (2021). High credibility.

Endoscope storage time and contamination — Figure 4 — compares " < 7 and ≥ 7 days" of storage with respect to bacterial contamination for pathogenic organisms and reports study-level and pooled statistics: Heroux 2017 shows odds ratio 0.657 with lower limit 0.145 and upper limit 2.970, Z-Value -0.546, P-Value .585; Osborne 2007 shows odds ratio 0.137 with lower limit 0.005 and upper limit 3.630, Z-Value -1.188, P-Value .235; and a random-effects model shows odds ratio 0.500 with lower limit 0.127 and upper limit 1.966, Z-Value -0.993, P-Value .321.

---

### Correction [^26cbbc41]. Journal of Radiology Case Reports (2017). Low credibility.

[This corrects the article on p. 8 in vol. 10.].

---

### Synthetic physical interactions with the yeast centrosome [^0f1ca5ec]. G3 (2019). Medium credibility.

Tools based on mixture models

Having determined that mixture models are a more appropriate statistical model than a normal distribution, we developed metrics to determine the significance of individual results and cutoffs to distinguish hits from non-hits. A typical approach in genome-wide screens is to calculate p-values based on a null model of the data. In the case of mixture models, identifying Component 1 as an empirical null model for the data allows for calculation of p-values, which may be adjusted for multiple hypothesis testing, for example by calculating FDR q-values. However, in this context a more natural approach is to calculate the conditional probability of inclusion in Component 2. We defineto be the probability of inclusion in Component 2 given a measured LGR of x. The point whereis the point where a strain with measured LGR x is equally likely to be in Component 1 or 2, and is therefore a logical point to place a cutoff. We define this point as, while the point where a Z-transformation of the data has value 2 is. We found thatalways sat below thebut this effect was more pronounced in screens with more hits. Notably using Z-score as a cutoff limited the range of numbers of hits to 100–250. In contrast, usingas a cutoff has a dynamic range of 100–700 hits (Figure 3A, Supplementary Table S1). This makes the mixture model approach a more effective tool than Z-score to distinguish between screens with many or few hits. We compiled a list of hits for each screen based either onor, depending on the success of the empirical Bayes approach (Supplementary File S4).

---

### Magnetic hyperbolic optical metamaterials [^3d496bee]. Nature Communications (2016). Medium credibility.

At 1,450 nm wavelength, the parameters ɛ x, ɛ z and μ y are vanishing simultaneously, representing the regime of optical topological transition. Around the topological transition, ɛ x, ɛ z and μ y change their signs due to the resonant nature of the metamaterial's response. This results in an increase of the phase velocity of light towards infinity inside the structure. Importantly, at this wavelength, the structure supports propagating waves with k -vectors substantially smaller than the k -vectors in air, while all conventional optical materials support k -vectors larger than those in air. As local material parameters become close to zero, we expect to see strong contributions from nonlocal response of the metamaterials. This implies that the permittivity coefficients ɛ x and ɛ z become functions of the wave vector k (see details in Supplementary Note 4). We find that for the magnetic (TE) dispersion of the fishnet metamaterials, both local and nonlocal models result in the same dispersion relation. Therefore, we consider the electric (TM) dispersion equation only, which in the nonlocal case takes the form:

We notice that(ref.). We further neglect the nonlocal parameter, as we find it to be of a minor importance. Thus, for the case of 1,450 nm wavelength, near the point of the optical topological transition of the metamaterial, we introduce two extra spatially dispersive termsandto describe the experimental dispersion. The values of the material parameters for the 1,450 nm wavelength are also given in Table 1. Our results suggest that a wide range of nontrivial isofrequency dispersion contours can be realized by an appropriate tuning of material's loss, gain and spatial dispersion. While all possible types of isofrequency contours for local media without loss/gain are limited to the second-order geometrical curves(such as an ellipse or hyperbola), the presence of loss, gain and spatial dispersion extends the possible cases of isofrequency contours to the fourth-order curves. This leads to new topologies of the metamaterial dispersion.

In addition, we use full-wave numerical simulations to calculate material's isofrequency contours (see details in Supplementary Note 5 and Supplementary Fig. 1). We find that numerical results are in a good agreement with both experimental measurements and analytical calculations.

---

### Learning properties of quantum States without the IID assumption [^d59de50a]. Nature Communications (2024). High credibility.

We develop a framework for learning properties of quantum states beyond the assumption of independent and identically distributed (i.i.d.) input states. We prove that, given any learning problem (under reasonable assumptions), an algorithm designed for i.i.d. input states can be adapted to handle input states of any nature, albeit at the expense of a polynomial increase in training data size (aka sample complexity). Importantly, this polynomial increase in sample complexity can be substantially improved to polylogarithmic if the learning algorithm in question only requires non-adaptive, single-copy measurements. Among other applications, this allows us to generalize the classical shadow framework to the non-i.i.d. setting while only incurring a comparatively small loss in sample efficiency. We leverage permutation invariance and randomized single-copy measurements to derive a new quantum de Finetti theorem that mainly addresses measurement outcome statistics and, in turn, scales much more favorably in Hilbert space dimension.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^01148c3a]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Nonparametric tolerance intervals for non-normal distributions: The above estimate of the tolerance interval would only be applicable to a population that is normally distributed, but when the underlying population is often not normal (eg, when there is a natural boundary that the data cannot exceed (ie, 0% or 100%)) it is helpful to define tolerance intervals using nonparametric methods; the one-sided nonparametric tolerance interval can be determined by finding the value for k that satisfies the cumulative binomial equation, where CL is the confidence level (eg, 0.95), and by setting k = 0 (ie, 0 failures) the formula can be simplified.

---

### Variability in forced expiratory volume in 1 s in children with symptomatically well-controlled asthma [^2853207f]. Thorax (2024). Medium credibility.

Aims

Spirometry is used by many clinicians to monitor asthma in children but relatively little is understood about its variability over time. The aim of this study was to determine the variability of forced expiratory volume in 1 s (FEV 1) in children with symptomatically well-controlled asthma by applying three different methods of expressing change in FEV 1 over 3-month intervals.

Methods

Data from five longitudinal studies of children with asthma which measured FEV 1 at 3-month intervals over 6 or 12 months were used. We analysed paired FEV 1 measurements when asthma symptoms were controlled. The variability of FEV 1% predicted (FEV 1%), FEV 1 z-score (FEV 1 z) and conditional z score for change (Zc) in FEV 1 was expressed as limits of agreement.

Results

A total of 881 children had 3338 FEV 1 measurements on occasions when asthma was controlled; 5184 pairs of FEV 1 measurements made at 3-month intervals were available. Each unit change in FEV 1 z score was equivalent to a Zc 1.45 and an absolute change in FEV 1% of 11.6%. The limits of agreement for change in FEV 1% were -20 and +21, absolute change in FEV 1 z were -1.7 and +1.7and Zc were -2.6 and +2.1. Regression to the mean and increased variability in younger children were present for change in FEV 1% and FEV 1 z comparisons, but not Zc.

Conclusion

Given the wide limits of agreement of paired FEV 1 measurements in symptomatically well-controlled children, asthma treatment should primarily be guided by symptoms and not by a change in spirometry.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^bb9b7f12]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Δ Φ, the probability distribution at a point Φ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Φ ∈ {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters α and β, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since α and β are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Φ ∈ [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(α, β) and x 2 = 1 − x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Φ, it may seem natural to select α and β by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^aeeaae45]. Nature Communications (2025). High credibility.

Fitting models to data is an important part of the practice of science. Advances in machine learning have made it possible to fit more-and more complex-models, but have also exacerbated a problem: when multiple models fit the data equally well, which one(s) should we pick? The answer depends entirely on the modelling goal. In the scientific context, the essential goal is replicability: if a model works well to describe one experiment, it should continue to do so when that experiment is replicated tomorrow, or in another laboratory. The selection criterion must therefore be robust to the variations inherent to the replication process. In this work we develop a nonparametric method for estimating uncertainty on a model's empirical risk when replications are non-stationary, thus ensuring that a model is only rejected when another is reproducibly better. We illustrate the method with two examples: one a more classical setting, where the models are structurally distinct, and a machine learning-inspired setting, where they differ only in the value of their parameters. We show how, in this context of replicability or "epistemic uncertainty", it compares favourably to existing model selection criteria, and has more satisfactory behaviour with large experimental datasets.

---

### Recommendations for a standardized pulmonary function report. An official American Thoracic Society technical statement [^d6d8fcbc]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Spirometry reporting specifies that numerical values are given for the FEV1, the FVC, and the FEV1/FVC ratio; the latter should be reported as a decimal fraction and the space for percent predicted value left blank, and if bronchodilators are given the LLN column need not be repeated with absolute and percent change given only for FEV1 and FVC. Other numerical values such as the forced inspiratory flow at 75% of FVC (FEF75%) and FEF25–75% are not recommended for routine use. Graph requirements include that for the volume–time curve the volume scale should be at least 10 mm/L, the time scale at least 20 mm/s, and 1 second prior to the start of expiration should be displayed; on the flow–volume plot the flow display should be at least 5 l/min/L/s, and the ratio of flow to volume should be 2 L/s to 1 L, and linear and log scales where values are plotted as z-scores relative to the predicted value (z = 0) give an intuitive sense of severity.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^cbffc27e]. CDC (2011). Medium credibility.

Appendix G. Mathematical formula for sample size — sample size required to detect the difference of GM between two subpopulation groups defines a z test with null hypothesis GM1 = GM2 or GMmax/GMmin = 1 and alternative GM1 ≠ GM2 or GMmax / GMmin > 1, where GMmax = max(GM1, GM2) and GMmin = min(GM1, GM2). It considers power W (say 80% chance) to detect that one group's GM is at least k fold higher and gives the p-value as p = Prob(Z ≥ z) = 1 − φ−1(z) with Z ~ Normal (0,1). Given α (e.g., 0.05) and target ratio k, power is W = Prob (Z > φ−1(1 − α) − log10(k)/(S√(1/n1 + 1/n2))), where S is the expected standard deviation of log10(VL) in the population of interest; when n1 = n2 = n, W = Prob (Z > φ−1(1 − α) − log10(k)/(S√(2/n))). The required sample size is n = 2[(φ−1(1 − α) − φ−1(1 − W))S/log10(k)]^2, and tables are provided for W = 0.8 and W = 0.9 with α = 0.05.

---

### Phase 0 / microdosing approaches: time for mainstream application in drug development? [^98d5620b]. Nature Reviews: Drug Discovery (2020). High credibility.

Phase 0 approaches - which include microdosing - evaluate subtherapeutic exposures of new drugs in first-in-human studies known as exploratory clinical trials. Recent progress extends phase 0 benefits beyond assessment of pharmacokinetics to include understanding of mechanism of action and pharmacodynamics. Phase 0 approaches have the potential to improve preclinical candidate selection and enable safer, cheaper, quicker and more informed developmental decisions. Here, we discuss phase 0 methods and applications, highlight their advantages over traditional strategies and address concerns related to extrapolation and developmental timelines. Although challenges remain, we propose that phase 0 approaches be at least considered for application in most drug development scenarios.

---

### Clinical policy: critical issues in the management of patients presenting to the emergency department with acetaminophen overdose [^c54b8b7b]. Annals of Emergency Medicine (2007). Medium credibility.

Appendix B — approach to downgrading strength of evidence: For Design/Class 1, 2, and 3, with none downgrading the levels are I, II, and III; with 1 level downgrading they are II, III, and X; with 2 levels downgrading they are III, X, and X; and fatally flawed maps to X, X, and X.

---

### The minimal work cost of information processing [^894e044e]. Nature Communications (2015). Medium credibility.

where the sum ranges only over those x that have a non-zero probability of occurring. In the case of deterministic mappings p (x ′| x)∈{0,1}, this corresponds to the maximum number of input states that map to a same output state. For the AND gate, provided all four states 00, 01, 10 and 11 have non-negligible probability of occurring, there are three input states mapping to the same output state, so (3) gives us simply. Also, in simple examples as considered here, the expression (3) is stable to considering an-approximation (Supplementary Note 4); this quantity is thus physically justified.

Crucially, our result reveals that the minimal work requirement in general depends on the specific logical process, and not only on the input and output states. This contrasts with traditional thermodynamics for large systems, where the minimal work requirement of a state transformation can always be written as a difference of a thermodynamical potential, such as the free energy. For example, the minimal work cost of performing specifically an AND gate may differ from that of another logical process mapping an input distribution (p 00, p 01, p 10, p 11) (with ∑ i p i = 1) to the distribution (p ′ 0, p ′ 1) = (p 00 + p 01 + p 10, p 11) (Recall that the classical counterpart of a quantum state is a probability distribution.). To see this, consider the XOR gate, which outputs a 1 exactly when both inputs are different (see Fig. 2b). The minimal work cost requirement of this gate, as given by (3), is now only kT ln 2, as in the worst case, only a single bit of information is erased (again supposing that all four input states have non-negligible probability of occurring). Now, suppose that, for some reason, the input distribution is such that p 01 + p 10 = p 11, that is, the input 11 occurs with the same probability as of either 01 or 10 appearing. Then, the XOR gate reproduces the exact same output distribution as the AND gate: in both cases, we have p ′ 0 = p 00 + p 10 + p 01 = p 00 + p 11 and p ′ 1 = p 11 = p 01 + p 10. In other words, both logical processes have the same input and output state, yet the XOR gate only requires work kT ln 2 compared with the AND gate, which requires 1.6 kT ln 2. Furthermore, we point out that this difference, which appears small in this case, may be arbitrarily large in certain scenarios (Supplementary Note 4).

---

### A blueprint for a synthetic genetic feedback optimizer [^0f7d579f]. Nature Communications (2023). High credibility.

Considering the dynamics (1)–(2), we seek a feedback optimizer (Fig. 1a) of the formwhere z and u are the internal state and the output of the optimizer, respectively, together with their time-dependent parameter vectors θ z and θ u, and ϵ z characterizes the timescale of the optimizer species. Although any one of ϵ x, ϵ y, and ϵ z can be eliminated by rescaling time (Supplementary Section 1.1), we keep them all to illustrate their effects throughout the paper.

Assuming that F (x, θ y) has a unique global time-dependent optimum at x * (if this is not the case, our approach only yields local optimality), we seek to design the optimizer to ensure that (i) trajectories of the closed loop system (1)–(3) converge towards the unique optimum x *; and (ii) the integrated system is biologically realizable and relies only on non-negative signals.

Gradient ascent and approximate gradient ascent

To illustrate the main idea underpinning the proposed optimizer module, we first consider a single species x that is produced and degraded according to(Supplementary Section 1.1), where u 1 and u 2 represent control signals regulating the production and degradation of x, respectively. In the absence of biological constraints, a potential choice for the optimizer is a gradient-based system with the feedback law u 1 = ∇ x F (x, θ y) and u 2 = − ∇ x F (x, θ y), where ∇ x F (x, θ y) denotes the gradient of F (x, θ y). The closed loop dynamicsrealizes gradient ascent by converging to x = x * where ∇ x F (x, θ y) = 0, thus tracking the time-varying optimum (Fig. 1b). Moreover, it is sufficient to simply swap the signs in the control laws above to implement minimization (gradient descent). Unfortunately, this approach has two fundamental limitations: (i) it requires explicit knowledge of the gradient ∇ x F (x, θ y); and (ii) u 1 and u 2 can become negative (Fig. 1b, gray regions).

---

### A practical guide for understanding confidence intervals and P values [^c0a7d4a0]. Otolaryngology — Head and Neck Surgery (2009). Low credibility.

The 95 percent confidence interval about the mean demarcates the range of values in which the mean would fall if many samples from the universal parent population were taken. In other words, if the same observation, experiment, or trial were done over and over with a different sample of subjects, but with the same characteristics as the original sample, 95 percent of the means from those repeated measures would fall within this range. This gives a measure of how confident we are in the original mean. It tells us not only whether the results are statistically significant because the CI falls totally on one side or the other of the no difference marker (0 if continuous variables; 1 if proportions), but also the actual values so that we might determine if the data seem clinically important. In contrast, the P value tells us only whether the results are statistically significant, without translating that information into values relative to the variable that was measured. Consequently, the CI is a better choice to describe the results of observations, experiments, or trials.

---

### MRNAs, proteins and the emerging principles of gene expression control [^2d1f7bae]. Nature Reviews: Genetics (2020). High credibility.

Gene expression involves transcription, translation and the turnover of mRNAs and proteins. The degree to which protein abundances scale with mRNA levels and the implications in cases where this dependency breaks down remain an intensely debated topic. Here we review recent mRNA-protein correlation studies in the light of the quantitative parameters of the gene expression pathway, contextual confounders and buffering mechanisms. Although protein and mRNA levels typically show reasonable correlation, we describe how transcriptomics and proteomics provide useful non-redundant readouts. Integrating both types of data can reveal exciting biology and is an essential step in refining our understanding of the principles of gene expression control.

---

### Hydrocodone bitartrate (hysingla ER) [^0c03a0bc]. FDA (2023). Medium credibility.

Warnings and precautions regarding the use of hydrocodone bitartrate ER PO (also known as Hysingla ER):
- **Adrenal insufficiency**: use caution in patients taking the drug for a prolonged period (> 1 month).
- **Central sleep apnea**: use caution in patients taking higher doses.
- **Decreased serum hydrocodone level**: use caution in patients taking CYP3A4 inducers (such as rifampin, carbamazepine, or phenytoin) or discontinuing CYP3A4 inhibitors (such as macrolide antibiotics, azole antifungals, or protease inhibitors).
- **Erectile dysfunction, infertility**: use caution in patients taking the drug for a prolonged period.
- **Exacerbation of increased ICP**: use caution in patients with increased ICP, brain tumor, or head injury.
- **Growth suppression**: use caution in patients with chronic corticosteroid therapy. Monitor growth regularly in pediatric patients receiving chronic corticosteroid therapy. Reassess the need for hydrocodone regularly and adjust the corticosteroid therapy as appropriate.
- **Hypotension, syncope**: use caution in patients with reduced blood volume or taking other CNS depressants. Monitor BP after initiation and dose titration. Avoid use in patients with circulatory shock.
- **Mask symptoms of head injury**: use caution in patients with head injury. Avoid use in patients with impaired consciousness or coma.
- **Opioid overdose**: use caution in patients taking CNS depressants or with a history of opioid use disorder or prior opioid overdose. Consider prescribing naloxone based on the patient's risk factors for overdose.
- **Opioid withdrawal syndrome**: do not discontinue abruptly in patients physically dependent on opioids.
- **Opioid withdrawal syndrome**: use caution in patients taking CYP3A4 inducers (such as rifampin, carbamazepine, or phenytoin) or discontinuing CYP3A4 inhibitors (such as macrolide antibiotics, azole antifungals, or protease inhibitors).
- **Opioid withdrawal syndrome**: use extreme caution in patients taking mixed agonist/antagonist analgesics (such as pentazocine, nalbuphine, or butorphanol) or partial agonist analgesics (such as buprenorphine).
- **Prolonged QT interval**: use caution in patients with congestive HF, bradyarrhythmia, electrolyte abnormalities, or taking drugs prolonging QT interval. Avoid use in patients with congenital long QT syndrome. Do not exceed 90 mg BID in patients developing QT prolongation.
- **Seizure**: use caution in patients with seizure disorder.
- **Serotonin syndrome**: use caution in patients taking serotonergic drugs.
- **Serotonin syndrome**: use extreme caution in patients taking MAOIs or within 14 days of stopping treatment.
- **Somnolence**: use extreme caution in patients performing activities requiring mental alertness, such as driving or operating machinery.
- **Sphincter of Oddi dysfunction**: use caution in patients with biliary tract disease and acute pancreatitis.

---

### Dimensionality-tailored pure organic semiconductor with high hole mobility for low-dose X-ray imaging [^857fb066]. Nature Communications (2025). High credibility.

However, the practical deployment of direct-type organic X-ray detectors remains hindered by their inherently low detection sensitivity (S) and poor limit of detection (LoD). These shortcomings stem both from weak X-ray attenuation and limited charge carrier mobility, which are intrinsic to their low-Z molecular composition and weak intermolecular interactions such as hydrogen bonding and van der Waals forces –. As a result, current organic detectors exhibit low responsivity, posing risks of inaccurate radiation dose monitoring and potential safety hazards during X-ray imaging.

Efforts over the past decade have focused on doping strategies that incorporate high-Z elements to enhance both X-ray absorption and S. For example, molecular engineering of TIPGe-pentacene has demonstrated a 22-fold increase in carrier mobility (from 0.018 to 0.4 cm 2 V −1 s −1) –. High-Z nanoparticle doping strategies, such as Bi 2 O 3 nanoparticle incorporation into P3HT:PCBM bulk-heterojunction films, have boosted the S up to 600 nC Gy air −1 cm −2 through enhanced X-ray scattering and absorption –. Furthermore, molecular tailoring approaches like replacing silicon with germanium in TIPS-pentacenehave increased the S from 2.6 × 10 5 to 9.0 × 10 5 µC Gy air −1 cm −3. However, high-Z doping in organic semiconductors triggers three interlinked failure modes: interface defect accumulation, charge transport degradation, and temporal resolution collapse, collectively impairing detector reliability and temporal response. This multi-mechanism deterioration ultimately constrains radiation detection fidelity under dynamic operating, necessitating integrated organic detector designs that enhance S and spatial resolution while maintaining charge transport efficiency and medical regulatory compliance.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^dee6cacc]. CDC (2011). Medium credibility.

Community viral load sample size calculations — sample size to detect differences in geometric mean (GM) viral load depends on power and standard deviation (S). The table specifies "α = 0.05 and W = 0.8", with S columns 1 to 1.5; for k = 3 the minimum sample sizes by S are 54, 66, 78, 92, 106, and 122, and "1.2 is the standard deviation of national VL data", so at S = 1.2 the needed sample size is 78.

---

### Guidelines for handling decedents contaminated with radioactive materials [^fd43bf99]. CDC (2007). Medium credibility.

Table 3 — examples of radiation doses (millirad) for comparison include: 1 chest x-ray — 20; 1 year exposure to background radiation (all sources) — 300; 1 year regulated limit to a member of the public — 500; 1 year regulated occupational limit — 5,000; Threshold for acute radiation syndrome — 50,000; and 50% Fatality dose — 500,000.

---

### Clinical policy: critical issues in the evaluation and management of adult patients with suspected acute nontraumatic thoracic aortic dissection [^83f3c6fe]. Annals of Emergency Medicine (2015). Medium credibility.

Approach to downgrading strength of evidence — under the header "Design/Class 1 2 3", the mapping shows that with "None" downgrading the levels are "I" for class 1, "II" for class 2, and "III" for class 3; with "1 level" downgrading they are "II", "III", and "X", respectively; with "2 levels" downgrading they are "III", "X", and "X", respectively; and when "Fatally flawed", all classes are "X".

---

### The certainty behind reporting a significance result: what the clinician should know [^c7e6f766]. American Journal of Physical Medicine & Rehabilitation (2019). Medium credibility.

The P value is the most common method used in medical literature for the result of a statistical test. It is the probability of the data with a true null hypothesis and is calculated using a formal statistical test after the appropriate model has been determined to analyze study data. The P value is dependent on the effect size, sample size, and a measure of variability within the outcomes. For many years, the P value has been set at 0.05, which is an arbitrary cutoff. It is important to understand that setting the cutoff at 0.05 may be correct for some study designs but not in others. Therefore, we recommend that in addition to the P value, another metric should be reported that specifies the magnitude of the effect such as effect size, confidence interval of the effect size, or fragility index.

---

### Bayesian prediction intervals for assessing P-value variability in prospective replication studies [^6f64c058]. Translational Psychiatry (2017). Low credibility.

Derived as a Bayesian prediction interval through the conjugate model, the endpoints of a P -interval in Eq. (5) can now be interpreted as bounds of the supposed likely range of replication P -values within a given probability (e.g. 80%). However, the conjugate model is restrictive in that a specific prior distribution has to be assumed, which may not provide an adequate representation of external knowledge about the effect size distribution. It also limits construction of the intervals to P -values derived from statistics for which there are known conjugate priors. Here, we introduce a more flexible approach, the Mixture Bayes, without these restrictions. The Mixture Bayes intervals can be constructed for P -values derived from statistics whose distribution is governed by a parameter γ that captures deviation from the usual point null hypothesis, H 0, and has the formor its square, N × δ 2. This includes normal, chi-squared, Student's t and F-statistics. We partition the prior distribution ofinto a finite mixture of values δ 1, δ 2,…, δ B with the corresponding prior probabilities, Pr (δ i). As an example, let P -value be derived from an F-test for comparison of two-sample means, with the corresponding sample sizes n 1 and n 2. Let N = 1/(1/ n 1 + 1/ n 2). For i -th prior value of effect, a statistic based on sampling values ofhas a noncentral F-distribution, with the noncentralityand the degrees of freedom df 1 = 1, df 2 = n 1 + n 2 − 2:where f is the density of the noncentral F-distribution. The posterior distribution is a mixture, with the posterior mean

---

### Clinical policy: critical issues in the evaluation and management of emergency department patients with suspected appendicitis: approved by ACEP board of directors February 1, 2023 [^fae36370]. Annals of Emergency Medicine (2023). High credibility.

Appendix B — approach to downgrading strength of evidence maps the three design/class columns such that with "None" downgrading the labels are I, II, III; with "1 level" they are II, III, X; with "2 levels" they are III, X, X; and if "Fatally flawed" they are X, X, X.

---

### Rank truncated product of P-values, with application to genomewide association scans [^8df60c5a]. Genetic Epidemiology (2003). Low credibility.

Large exploratory studies are often characterized by a preponderance of true null hypotheses, with a small though multiple number of false hypotheses. Traditional multiple-test adjustments consider either each hypothesis separately, or all hypotheses simultaneously, but it may be more desirable to consider the combined evidence for subsets of hypotheses, in order to reduce the number of hypotheses to a manageable size. Previously, Zaykin et al. ([2002] Genet. Epidemiol. 22:170–185) proposed forming the product of all P-values at less than a preset threshold, in order to combine evidence from all significant tests. Here we consider a complementary strategy: form the product of the K most significant P-values. This has certain advantages for genomewide association scans: K can be chosen on the basis of a hypothesised disease model, and is independent of sample size. Furthermore, the alternative hypothesis corresponds more closely to the experimental situation where all loci have fixed effects. We give the distribution of the rank truncated product and suggest some methods to account for correlated tests in genomewide scans. We show that, under realistic scenarios, it provides increased power to detect genomewide association, while identifying a candidate set of good quality and fixed size for follow-up studies.

---

### A new look at P values for randomized clinical trials [^2a9dcae0]. NEJM Evidence (2024). Medium credibility.

BACKGROUND: We have examined the primary efficacy results of 23,551 randomized clinical trials from the Cochrane Database of Systematic Reviews. METHODS: We estimate that the great majority of trials have much lower statistical power for actual effects than the 80 or 90% for the stated effect sizes. Consequently, "statistically significant" estimates tend to seriously overestimate actual treatment effects, "nonsignificant" results often correspond to important effects, and efforts to replicate often fail to achieve "significance" and may even appear to contradict initial results. To address these issues, we reinterpret the P value in terms of a reference population of studies that are, or could have been, in the Cochrane Database. RESULTS: This leads to an empirical guide for the interpretation of an observed P value from a "typical" clinical trial in terms of the degree of overestimation of the reported effect, the probability of the effect's sign being wrong, and the predictive power of the trial. CONCLUSIONS: Such an interpretation provides additional insight about the effect under study and can guard medical researchers against naive interpretations of the P value and overoptimistic effect sizes. Because many research fields suffer from low power, our results are also relevant outside the medical domain. (Funded by the U.S. Office of Naval Research.)

---

### Stability of synchronization in simplicial complexes [^14660083]. Nature Communications (2021). High credibility.

Fig. 1
Synchronization in simplicial complexes of Rössler oscillators.

Contour plots of the time averaged (over an observation time T = 500) synchronization error E (see "Methods" for definition and the vertical bars of each panel for the color code) in the plane (σ 1, σ 2) for some examples of simplicial complexes (whose sketches are reported in the top left of each panel). Simulations refer to coupled Rössler oscillators (x = (x, y, z) T and f = (− y − z, x + a y, b + z (x − c)) T) with parameters fixed in the chaotic regime (a = b = 0.2, c = 9). In a – d, while in (e). As for the other coupling function, one hasin (d) andin all other panels. The blue continuous lines are the theoretical predictions of the synchronization thresholds obtained from Eq. (3). a, b, and c are examples of class III problems, whereas panels d and e are examples of class II problems.

Far from being limited to the case of D = 2, our approach can be extended straightforwardly to simplicial complexes of any order D. Each term on the right hand side of Eq. (1) can, indeed, be manipulated following exactly the same three conceptual steps described in the "Methods". Once again, one is entitled to select the eigenvector set which diagonalizes, to introduce the new variables η = (V −1 ⊗ I m) δ x. Following the very same steps which led us to write Eq. (3), one then obtainswhere JG (d) = J 1 g (d) (x s,…, x s) + J 2 g (d) (x s,…, x s) +… + J d g (d) (x s,…, x s) and the coefficientsresult from transformingwith the matrix that diagonalizes. As a result, one has conceptually the same reduction of the problem to a single, uncoupled, nonlinear system, plus a system of N − 1 coupled linear equations, from which the maximum Lyapunov exponent Λ max = can be extracted and monitored (for each simplicial complex) in the D -dimensional hyper-space of the coupling strength parameters.

---

### Data-driven control of complex networks [^85fc9bea]. Nature Communications (2021). High credibility.

Results

Network dynamics and optimal point-to-point control

We consider networks governed by linear time-invariant dynamicswhere, anddenote, respectively, the state, input, and output of the network at time t. The matrixdescribes the (directed and weighted) adjacency matrix of the network, and the matricesand, respectively, are typically chosen to single out prescribed sets of input and output nodes of the network.

In this work, we are interested in solving point-to-point control problems; that is, designing open-loop control policies that steer the network output y (t) from an initial value y (0) = y 0 to a desired one y (T) = y f in a finite number of steps T. If y f is output controllable in T steps (a standing assumption in this paper; we refer to Supplementary Note 1 for more details), then the latter problem admits a solution and, in fact, there are many ways to accomplish such a control task. Here, we assume that the network is initially relaxed (x (0) = 0), and we seek the control inputthat drives the output of the network to y f in T steps and, at the same time, minimizes a prescribed quadratic combination of the control effort and locality of the controlled trajectories.

Mathematically, we study and solve the following constrained minimization problem:where Q ≽ 0 and R ≻ 0 are tunable (positive semidefinite and positive definite, respectively) matrices that penalize output deviation and input usage, respectively, and y T = y (T). Problem (2) generalizes the classic (open-loop) linear–quadratic control framework by including the possibility of minimizing a linear function of the state (as opposed to the whole state) in addition to the control input. Further, we remark that increasing R in Eq. (2) leads to optimal control inputs that achieve the desired final state with increasingly smaller magnitudes. Similarly, the matrix Q in Eq. (2) weighs the norm of the output (state), so that increasing Q forces the optimization problem to generate inputs that limit the norm of the output (state), at the expenses of using a larger control input. In particular, if Q = 0 and R = I, thencoincides with the minimum-energy control to reach y f in T steps.

---

### Research techniques made simple: sample size Estimation and power calculation [^b2b6b836]. The Journal of Investigative Dermatology (2018). Low credibility.

Sample size and power calculations help determine if a study is feasible based on a priori assumptions about the study results and available resources. Trade-offs must be made between the probability of observing the true effect and the probability of type I errors (α, false positive) and type II errors (β, false negative). Calculations require specification of the null hypothesis, the alternative hypothesis, type of outcome measure and statistical test, α level, β, effect size, and variability (if applicable). Because the choice of these parameters may be quite arbitrary in some cases, one approach is to calculate the sample size or power over a range of plausible parameters before selecting the final sample size or power. Considerations that should be taken into account could include correction for nonadherence of the participants, adjustment for multiple comparisons, or innovative study designs.

---

### Implementing quantum dimensionality reduction for non-markovian stochastic simulation [^4ea471be]. Nature Communications (2023). High credibility.

Methods

Stochastic processes and minimal-memory classical modelling

A discrete-time stochastic processconsists of a sequence of random variables X t, corresponding to events drawn from a set, and indexed by a timestep. The process is defined by a joint distribution of these random variables across all timesteps, whererepresents the contiguous (across timesteps) series of events between timesteps t 1 and t 2. We consider stochastic processes that are bi-infinite, such thatand, and stationary (time-invariant), such that P (X 0: L) = P (X t: t + L) ∀ t, L ∈. Without loss of generality, we can take the present to be t = 0, such that the past is given by, and the future. Note that we use upper case for random variables and lower case for the corresponding variates.

A (causal) model of such a (bi-infinite and stationary) discrete-time stochastic process consists of an encoding functionthat maps from the set of possible past observationsto a set of memory states –. The model also requires an update rulethat produces the outputs and updates the memory state accordingly. We then designate the memory cost D f of the encoding as the logarithm of the dimension (i.e. the number of (qu)bits) of the smallest system into which these memory states can be embedded. For classical (i.e. mutually orthogonal) memory states, this corresponds to. For quantum memory states, which may, in general, be linearly dependent.

Let us, for now, restrict our attention to statistically-exact models, such that (f, Λ) must produce outputs with a distribution that is identical to the stochastic process being modelled. Under such a condition, the provably-memory minimal classical model of any given discrete-time stochastic process is known and can be systematically constructed. These models are referred to as the ε - machine of the process, which employs an encoding function f ε based on the causal states of the process. This encoding function satisfiesand given initial memory state, the evolution produces output x 0 with probabilityand updates the memory to state. The memory states are referred to as the causal states of the process, and the associated cost D μ is given by the logarithm of the number of causal states.

---

### Cartography of genomic interactions enables deep analysis of single-cell expression data [^f3c91bd8]. Nature Communications (2023). High credibility.

Efficient computation of the interaction matrix

Computation of the pairwise interaction matrix (eq. (3)) can be intensive because of the inversion of the covariance matrix. Interestingly, the formulation of the interaction strength between two genes (eq. (3)) is same as that of their partial correlation. An efficient way to compute the partial correlation matrix (and thus interaction matrix) is to 1) solve the two associated linear regression problems (shown below), 2) get the residuals from the regression problems, and 3) calculate the correlation between the residuals. Let us assume that X and Y are random variables taking real values (denoting expression levels of two genes), and let Z be the (n − 2)-dimensional vector-valued random variable (denoting expression levels of all other genes). Let us also assume that x i, y i and z i, i = 1,…, m denotes the independent and identically distributed m observations from some joint probability distribution of the random variables X, Y and Z. If we want to find the relationship between the random variables through regression, we have to find the regression coefficient vectorsandsuch thatwith 〈 w, v 〉 the scalar product between the vectors w and v. The residuals can then be computed asThe partial correlation between X and Y can then be expressed as:For independent 2-way (pairwise) interactions between X and Y (Z has no effect on the interaction of X and Y), eq. (16) can be simplified as

---

### Clinical policy: critical issues in the evaluation of adult patients presenting to the emergency department with acute blunt abdominal trauma [^f1e59d16]. Annals of Emergency Medicine (2011). Medium credibility.

Appendix B — Approach to downgrading strength of evidence maps downgrading levels to Design/Class columns "1", "2", and "3": with "None" downgrading the entries are "I", "II", and "III"; with "1 level" they are "II", "III", and "X"; with "2 levels" they are "III", "X", and "X"; and "Fatally flawed" corresponds to "X", "X", and "X".

---

### Death by p-value: the overreliance on p-values in critical care research [^bfbf92b9]. Critical Care (2025). Medium credibility.

The p-value has changed from a versatile tool for scientific reasoning to a strict judge of medical information, with the usual 0.05 cutoff frequently deciding a study's significance and subsequent clinical use. Through an examination of five critical care interventions that demonstrated meaningful treatment effects yet narrowly missed conventional statistical significance, this paper illustrates how rigid adherence to p-value thresholds may obscure therapeutically beneficial findings. By providing a clear, step-by-step illustration of a basic Bayesian calculation, we demonstrate that clinical importance can remain undetected when relying solely on p-values. These observations challenge current statistical paradigms and advocate for hybrid approaches-including both frequentist and Bayesian methodologies-to provide a more comprehensive understanding of clinical data, ultimately leading to better-informed medical decisions.

---

### Fundamentals of research data and variables: the devil is in the details [^07fcd2d5]. Anesthesia and Analgesia (2017). Low credibility.

Designing, conducting, analyzing, reporting, and interpreting the findings of a research study require an understanding of the types and characteristics of data and variables. Descriptive statistics are typically used simply to calculate, describe, and summarize the collected research data in a logical, meaningful, and efficient way. Inferential statistics allow researchers to make a valid estimate of the association between an intervention and the treatment effect in a specific population, based upon their randomly collected, representative sample data. Categorical data can be either dichotomous or polytomous. Dichotomous data have only 2 categories, and thus are considered binary. Polytomous data have more than 2 categories. Unlike dichotomous and polytomous data, ordinal data are rank ordered, typically based on a numerical scale that is comprised of a small set of discrete classes or integers. Continuous data are measured on a continuum and can have any numeric value over this continuous range. Continuous data can be meaningfully divided into smaller and smaller or finer and finer increments, depending upon the precision of the measurement instrument. Interval data are a form of continuous data in which equal intervals represent equal differences in the property being measured. Ratio data are another form of continuous data, which have the same properties as interval data, plus a true definition of an absolute zero point, and the ratios of the values on the measurement scale make sense. The normal (Gaussian) distribution ("bell-shaped curve") is of the most common statistical distributions. Many applied inferential statistical tests are predicated on the assumption that the analyzed data follow a normal distribution. The histogram and the Q-Q plot are 2 graphical methods to assess if a set of data have a normal distribution (display "normality"). The Shapiro-Wilk test and the Kolmogorov-Smirnov test are 2 well-known and historically widely applied quantitative methods to assess for data normality. Parametric statistical tests make certain assumptions about the characteristics and/or parameters of the underlying population distribution upon which the test is based, whereas nonparametric tests make fewer or less rigorous assumptions. If the normality test concludes that the study data deviate significantly from a Gaussian distribution, rather than applying a less robust nonparametric test, the problem can potentially be remedied by judiciously and openly: (1) performing a data transformation of all the data values; or (2) eliminating any obvious data outlier(s).

---

### Truncated product method for combining P-values [^f492b3a2]. Genetic Epidemiology (2002). Low credibility.

We present a new procedure for combining P-values from a set of L hypothesis tests. Our procedure is to take the product of only those P-values less than some specified cut-off value and to evaluate the probability of such a product, or a smaller value, under the overall hypothesis that all L hypotheses are true. We give an explicit formulation for this P-value, and find by simulation that it can provide high power for detecting departures from the overall hypothesis. We extend the procedure to situations when tests are not independent. We present both real and simulated examples where the method is especially useful. These include exploratory analyses when L is large, such as genome-wide scans for marker-trait associations and meta-analytic applications that combine information from published studies, with potential for dealing with the "publication bias" phenomenon. Once the overall hypothesis is rejected, an adjustment procedure with strong family-wise error protection is available for smaller subsets of hypotheses, down to the individual tests.

---

### Fast and covariate-adaptive method amplifies detection power in large-scale multiple hypothesis testing [^1e4f8215]. Nature Communications (2019). High credibility.

Introduction

Multiple hypothesis testing (or multiple testing correction) is an essential component in many modern data analysis workflows. A common objective is to maximize the number of discoveries while controlling the fraction of false discoveries (FD). For example, we may want to identify as many genes as possible that are differentially expressed between two populations such that less than, say, 10% of these identified genes are false positives.

In the standard setting, the data for each hypothesis is summarized by a p -value, with a smaller value presenting stronger evidence against the null hypothesis that there is no association. Commonly-used procedures such as the Benjamini-Hochberg procedure (BH)works solely with this list of p -values –. Despite being widely used, these multiple testing procedures fail to utilize additional information that is often available in modern applications but not directly captured by the p -value.

---

### Standards of care in diabetes – 2025 [^5eb43e21]. Diabetes Care (2025). High credibility.

Regarding follow-up and surveillance for diabetes mellitus type 1, more specifically with respect to continuous glucose monitoring, ADA 2025 guidelines recommend to implement early continuous glucose monitoring in adult patients with T1DM to improve glycemic outcomes and QoL and minimize hypoglycemia.

---