# Medical Question & Answer

**Sample ID**: 81f20572-9de5-a614-33e2-eddb58795eb2
**Dataset Index**: 39121

---

## Question

6. 100 n positive numbers x₁, x₂, ⋯, xₙ sum up to 1. Let S be the largest of the following numbers:
[x₁/(1 + x₁)], [x₂/(1 + x₁ + x₂)], ⋯, [xₙ/(1 + x₁ + x₂ + ⋯ + xₙ)].

Find the smallest possible value of S. For what values of x₁, x₂, ⋯, xₙ is this minimum value achieved?

---

## Answer

> Let's see… What do we have here? The user is asking for the smallest possible value of the largest of the sequence of fractions (\\frac{x_k}{1 + \\sum_{i = 1}^k x_i}) for (k = 1, \\dots, n), given that all (x_i) are positive and sum to 1, and to determine the values of (x_i) that achieve this minimum. Let's break this down step-by-step. First, I need to think about how to define the objective function and constraints precisely. Then, I should verify the monotonicity properties of the denominators and how that affects the sequence of fractions. Next, I will consider a candidate strategy where all fractions are equal and check whether that is feasible and optimal. After that, I should prove optimality using an inequality argument. Finally, I will solve for the exact values of (x_i) and confirm the minimal value of (S).

> Let me first confirm the setup. We have (x_1, x_2, \\dots, x_n > 0) with (\\sum_{i = 1}^n x_i = 1), and we define (S = \\max_{1 \\le k \\le n} \\frac{x_k}{1 + \\sum_{i = 1}^k x_i}). The goal is to minimize (S) over all such sequences, and to find the minimizing sequence if it exists.

> Wait, let me verify the behavior of the denominators. Let (D_k = 1 + \\sum_{i = 1}^k x_i). Since all (x_i > 0), the sequence (D_k) is strictly increasing with (k), which means the fractions (\\frac{x_k}{D_k}) tend to decrease as (k) increases for a fixed (x_k), though (x_k) itself can vary. This monotonicity in the denominator will matter when I try to balance the fractions across (k).

> I will now examine the candidate strategy where all fractions are equal to some constant (c), i.e., (\\frac{x_k}{1 + \\sum_{i = 1}^k x_i} = c) for all (k = 1, \\dots, n). If this is achievable, then (S = c) and we would have minimized the maximum because any deviation would force at least one fraction to be larger than (c), increasing the maximum. Let me first confirm feasibility and then prove optimality rigorously.

> Let me derive the recurrence implied by equal fractions. If (\\frac{x_k}{D_k} = c), then (x_k = c D_k). But (D_k = D_{k-1} + x_k), so substituting gives (D_k = D_{k-1} + c D_k), which rearranges to (D_k (1 - c) = D_{k-1}), hence (D_k = \\frac{D_{k-1}}{1 - c}). Starting from (D_0 = 1), this yields (D_k = \\frac{1}{(1 - c)^k}). Then (x_k = c D_k = \\frac{c}{(1 - c)^k}).

> Hold on, I should verify the total sum constraint. Summing (x_k) from (k = 1) to (n) gives (\\sum_{k = 1}^n \\frac{c}{(1 - c)^k} = 1). This is a geometric series with first term (\\frac{c}{1 - c}) and ratio (\\frac{1}{1 - c}), so the sum equals (\\frac{c}{1 - c} \\cdot \\frac{(1 - c)^{-n} - 1}{(1 - c)^{-1} - 1} = \\frac{c}{1 - c} \\cdot \\frac{(1 - c)^{-n} - 1}{\\frac{c}{1 - c}} = (1 - c)^{-n} - 1). Setting this equal to 1, I get ((1 − c)^(−n) − 1 = 1), so ((1 − c)^(−n) = 2), which implies (1 − c = 2^(−1/n)) and therefore (c = 1 − 2^(−1/n)).

> Hmm, wait a minute, I initially thought the minimal (S) might be (\\frac{1}{n+1}) based on a rough averaging intuition, but that would be incorrect for (n > 1). Let me reconsider and stick to the algebra: the achievable equal-fraction value is (c = 1–2^{-1/n}), and this will turn out to be the minimum. I should double-check that this value is indeed less than 1 and positive, which it is for (n \\ge 1).

> Next, I should review why this is optimal. Suppose there exists a sequence with (\\max_k \\frac{x_k}{1 + \\sum_{i = 1}^k x_i} < c = 1–2^{-1/n}). Then for all (k), (\\frac{x_k}{D_k} < c), so (x_k < c D_k). This implies (D_k = D_{k-1} + x_k < D_{k-1} + c D_k), which gives (D_k < \\frac{D_{k-1}}{1 - c}). Iterating, (D_n < \\frac{1}{(1 - c)^n}). But (D_n = 1 + \\sum_{i = 1}^n x_i = 2), so we would have (2 < \\frac{1}{(1 - c)^n}), i.e., ((1 - c)^n < \\frac{1}{2}), or (c > 1–2^{-1/n}), contradicting the assumption that (c) was larger than the supposed maximum. Therefore, the minimal possible value of (S) is exactly (1–2^{-1/n}).

> I will now solve for the optimal sequence explicitly. From earlier, (x_k = \\frac{c}{(1 - c)^k}) with (c = 1–2^{-1/n}), so (1 - c = 2^{-1/n}). Substituting, (x_k = (1–2^{-1/n}) \\cdot 2^{k/n}). Let me verify the sum: (\\sum_{k = 1}^n (1–2^{-1/n}) 2^{k/n} = (1–2^{-1/n}) \\sum_{k = 1}^n 2^{k/n} = (1–2^{-1/n}) \\cdot 2^{1/n} \\frac{2–1}{2^{1/n} - 1} = (2^{1/n} - 1) \\frac{1}{2^{1/n} - 1} = 1), which checks out.

> Final answer: The smallest possible value of (S) is (1 − 2^(−1/n)), and this minimum is achieved when (x_k = (1 − 2^(−1/n)) × 2^(k/n)) for (k = 1, 2, …, n). Let me just double-check that all (x_k) are positive and sum to 1, which they are and do, confirming the solution is valid.

---

The smallest possible value of (S) is (1 − 2^(−1/n)), achieved when (x_k = 2^((k−1)/n) (2^(1/n) − 1)) for (k = 1, 2, …, n). This configuration equalizes all the fractions ([x_k/(1 + ∑_{i = 1}^k x_i)]) to (1 − 2^(−1/n)), which is the minimal achievable maximum.

---

## Proof of minimality

To prove that (1–2^{-1/n}) is the minimal value, we use the method of Lagrange multipliers and induction.

---

### Base case (n = 1)

For (n = 1), we have (x₁ = 1), so (S = [x₁/(1 + x₁)] = [1/2] = 1 − 2^(−1/1)). The formula holds.

---

### Inductive step

Assume the formula holds for (n = m), i.e. the minimal (S) is (1–2^{-1/m}) with (x_k = 2^{(k-1)/m} (2^{1/m} - 1)). For (n = m + 1), we need to show the minimal (S) is (1–2^{-1/(m+1)}) with (x_k = 2^{(k-1)/(m+1)} (2^{1/(m+1)} - 1)).

---

### Lagrange multipliers

We minimize (S = \\max_{1 \\leq k \\leq n} \\frac{x_k}{1 + \\sum_{i = 1}^k x_i}) subject to (\\sum_{i = 1}^n x_i = 1) and (x_i > 0). The Lagrangian is:

L = max_{1 ≤ k ≤ n} [x_k/(1 + ∑_{i = 1}^k x_i)] − λ (∑_{i = 1}^n x_i − 1)

At the optimum, all fractions are equal to some constant (c):

[x_k/(1 + ∑_{i = 1}^k x_i)] = c for all k

This gives the recurrence:

x_k = c (1 + ∑_{i = 1}^k x_i)

Solving this recurrence yields:

x_k = c (1 − c)^(−k)

---

### Sum constraint

Imposing (\\sum_{k = 1}^n x_k = 1):

∑_{k = 1}^n c (1 − c)^(−k) = 1

This simplifies to:

c × [((1 − c)^(−n) − 1)/c] = 1 ⇒ (1 − c)^(−n) = 2

Thus:

c = 1 − 2^(−1/n)

---

### Optimal values

Substituting (c = 1–2^{-1/n}) back into (x_k = c (1 - c)^{-k}):

x_k = (1 − 2^(−1/n)) × 2^(k/n) = 2^((k−1)/n)(2^(1/n) − 1)

---

## Verification

We verify that these (x_k) satisfy the sum constraint and yield the claimed minimal (S):

∑_{k = 1}^n x_k = (2^(1/n) − 1) ∑_{k = 1}^n 2^((k−1)/n) = (2^(1/n) − 1) × [(2 − 1)/(2^(1/n) − 1)] = 1

And for each (k):

[x_k/(1 + ∑_{i = 1}^k x_i)] = [2^((k−1)/n)(2^(1/n) − 1)/(1 + (2^(k/n) − 1))] = 1 − 2^(−1/n)

---

Thus, the smallest possible value of (S) is (1 − 2^(−1/n)), achieved when (x_k = 2^((k−1)/n) (2^(1/n) − 1)) for (k = 1, 2, …, n).

---

## References

### Power of data in quantum machine learning [^03099894]. Nature Communications (2021). High credibility.

Given some set of data, if s K (N) is found to be small relative to N after training for a classical ML model, this quantum model f (x) can be predicted accurately even if f (x) is hard to compute classically for any given x. In order to formally evaluate the potential for quantum prediction advantage generally, one must take s K (N) to be the minimal over efficient classical models. However, we will be more focused on minimally attainable values over a reasonable set of classical methods with tuned hyperparameters. This prescribes an effective method for evaluating potential quantum advantage in practice, and already rules out a considerable number of examples from the literature.

From the bound, we can see that the potential advantage for one ML algorithm defined by K 1 to predict better than another ML algorithm defined by K 2 depends on the largest possible separation betweenandfor a dataset. The separation can be characterized by defining an asymmetric geometric difference that depends on the dataset, but is independent of the function values or labels. Hence evaluating this quantity is a good first step in understanding if there is a potential for quantum advantage, as shown in Fig. 1. This quantity is defined bywhere ∣∣. ∣∣ ∞ is the spectral norm of the resulting matrix and we assume Tr(K 1) = Tr(K 2) = N. One can show that, which implies the prediction error bound. A detailed derivation is given in Supplementary Section C and an illustration of g 12 can be found in Fig. 2. The geometric difference g (K 1 ∣∣ K 2) can be computed on a classical computer by performing a singular value decomposition of the N × N matrices K 1 and K 2. Standard numerical analysis packagesprovide highly efficient computation of a singular value decomposition in time at most order N 3. Intuitively, if K 1 (x i, x j) is small/large when K 2 (x i, x j) is small/large, then the geometric difference g 12 is a small value ~1, where g 12 grows as the kernels deviate.

---

### Stability criteria for complex microbial communities [^a42f68e1]. Nature Communications (2018). Medium credibility.

Unequal numbers of consumers and resources

For any solution forandwith positive consumer and resource densities where the number of consumers is N S and the number of resources is N R, with N S not necessarily equal to N R, we can identify positive-valued influx and mortality rates that will lead to these solutions as follows:

Because C is positive, any positivewill result in. And ifis positive. Thus for appropriate choice of parameters, any positivecan be found as a solution to Eq. (1) in the main text.

Let the number of consumers be N S, and the number of resources be N R. Assume an equilibrium solution such that all resources and all consumer species have positive density. The Jacobian matrix corresponding to any such equilibrium solution is

If all the real parts of the eigenvalues of this Jacobian are negative, then this equilibrium is locally stable. To compute these eigenvalues, we first note that the eigenvalue equation det(L − λI) = 0 is given by

Next, we note thatis invertible, so long as λ is not an eigenvalue of. If it is an eigenvalue, then the entries ofare nonnegative and so λ ≤ 0. However, if for some j, then from Eq. (1), there is no nontrivial, finite equilibrium value for the corresponding resource, R j. Thus, we consider this a degenerate situation and assume the entries ofto be strictly positive. And so for λ an eigenvalue of, we have λ < 0. On the other hand, if λ is not an eigenvalue ofthenNote that. Thus, the above equation impliesDefining D 1 = and D 2 = then

---

### A solution to the collective action problem in between-group conflict with within-group inequality [^a7f2643e]. Nature Communications (2014). Medium credibility.

Hierarchical groups

Assume that group members differ in their valuations v i and that each group is characterized by exactly the same set of v i values. To find the invasion fitness w (y i | x i) for the effort x i at rank i, we need to set x ij = y i, f ij = 1+ bGP j nv i − cy i, and

in equation (10). Here X = ∑ j x j and X − i = X − x i.

Then, in the large G limit, the selection gradientat y = x for x i is proportional to

This equation is linear in x i. It follows that effort x i at each rank i evolves either to a positive value or to zero.

Assume first that the effort at each rank evolves to a positive value. Summing up equation (16) over all i with account that ∑ v i = 1 and ∑ x i = X, one finds that X satisfies to a quadratic equation

Therefore, at equilibrium

(The other solution, X ✱ = n (1+ b)/ c, results in zero average fertility and thus is not biologically relevant). That is, the total group effort does not depend on the group size or on the distribution of the valuations. The equilibrium individual efforts are

This equilibrium is feasible (that is, all x i * > 0) if for all i

Note that increasing the value of the resource b or the group size n moves v crit closer to 1/ n, so that individuals with a valuation smaller than the average will not contribute but will free ride.

---

### First-in-human validation of a DROP-IN β-probe for robotic radioguided surgery: defining optimal signal-to-background discrimination algorithm [^d560c412]. European Journal of Nuclear Medicine and Molecular Imaging (2024). Medium credibility.

To find the best possible settings of this approach, thus delivering the best sensitivity and specificity as compared to pathological results, the combination of these 3 parameters was optimized:
The number of sigmas above the mean value we set the cutoff to,(see Eq. 1) that we varied to assume the values {0.5, 1, 1.5, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 3.75, 4, 4.5, 5, 6, 8, 10, 30}
The minimum fraction of countings above a signal in a sample(see Eq. 2) that we varied in the ensemble {5%, 10%, 20%, 50%} (The minimum number of consecutive countings above a threshold(see Eq. 3) that we allowed to assume the values {2, 3, 4, 6, 8, 10, 20} (All things considered, for each sample, the same data were analyzed with a combination of 504 different parameters, for each of which the sample could be defined as "Probe-Positive" or "Probe-Negative". This result was then compared with pathological analysis, considered as the standard reference, defining each sample as Path.-Positive or Path.-Negative and thus allowing to classify it as true positive (TP), true negative (TN), false positive (FP), or false negative (FN).

Therefore, for each chosen value N σ, sensitivity and specificity can be calculated as.

And, for a given pair of ϵ Frac and ϵ Num, a ROC curve can be constructed by plotting the variation of sensitivity and specificity with N σ.

To strengthen the analysis, the whole dataset was split into two parts: one to be used for training the algorithm (training dataset) and the other to test it (test dataset). To this aim, data were randomly split on a district basis and assigned to either the training or test dataset, in order to find a distribution of districts between the two samples that contain, according to pathology, a similar number of healthy and diseased areas.

---

### A solution to the collective action problem in between-group conflict with within-group inequality [^1e238285]. Nature Communications (2014). Medium credibility.

Equation (19) shows that individual effortsincrease with valuation v i. In contrast, individual reproductive success at equilibrium is

and thus decreases with valuation. That is, the higher costs paid by high-rank individuals negate their larger shares of the reward.

If not all group members make a positive effort, then the derivations become more complex. Let there be n e contributors and v be their total valuation (v = ∑ v i where the sum is over n e contributors). Then, summing up equation (16) over all contributors, at equilibrium their total effort X satisfies to a quadratic equation

Of the two solutions of this quadratic, only the smallest one, X *, is relevant biologically (the other solution leads to negative fertilities). Note that cX * ≤ n e + bn Σ v i < n (1+ b). (The first inequality follows from the requirement that all f i values are non-negative: which one then sums up over all contributing individuals).

The individual effortscan be found from equation (16) by substituting X * into

This equilibrium is biologically feasible (that is, all), if

Equation (23) shows that individual effort increases with valuation. However, within-group reproductive success

and is decreasing with valuation v i because cX * ≤ n (b +1).

By differentiating the expression for X *, one can show that X * always increases with v and generally decreases with n e. (The latter requires that v exceeds a certain threshold that depends on b and n but is relatively small). That is, decreasing the number of contributors and/or increasing their total share of the reward results in increasing group effort. Moreover, analytical derivation and numerical simulations strongly suggest that the derivative of v crit with respect to X is positive, so that the threshold valuation increases with decreasing the number of contributors. This leads to a recursive procedure for finding the equilibrium: (i) setfor all ranks for which inequality (20) is not satisfied; (ii) using appropriate values of n e and v, find X * from equation (22); (iii) if there are ranks for which inequality (24) is not satisfied, set their contributionsto zero, recompute n e and v, and return to step (ii).

---

### Understanding quantum machine learning also requires rethinking generalization [^0feeb9f5]. Nature Communications (2024). High credibility.

Theorem 3

(Conditioning as a convex program 1). Let ρ 1, …, ρ N be unknown, linearly-independent quantum states on n qubits, with. For any i ∈ [N], letbe approximations of ρ i, each of which can be efficiently prepared using a PQC. Assume the computation ofin polynomial time for any choice of i, j and k. Call σ = (σ 1, …, σ N). The real numbersdefine the auxiliary statesasand the matrix of inner productswith entriesThen. Further, one can then decide in polynomial time whether, given ρ 1, …, ρ N, σ, and, there exists a specification ofsuch thatis well-conditioned in the sense that. And, if there exists such a specification, a convex semi-definite problem (SDP) outputs an instance of α ← SDP(ρ, σ, κ) for whichis well-conditioned. If it exists, one can also find in polynomial time the α with the smallestornorm.

Proof

The inequalityfollows from Gershgorin's circle theorem, given that all entries ofare bounded between [0, 1]. In particular, the largest singular value of the matrixreaches the value N when all entries are 1.

The expressionis a linear constraint on α and, for i, j ∈ [N], whilein matrix ordering is a positive semi-definite constraint.is equivalent with, whilemeans that the smallest singular value ofis lower bounded by κ, being equivalent withfor an invertible. The test whether such ais well-conditioned hence takes the form of a semi-definite feasibility problem. One can additionally minimize the objective functionsandboth again as linear or convex quadratic and hence semi-definite problems. Overall, the problem can be solved as a semi-definite problem, that can be solved in a run-time with low-order polynomial effort with interior point methods. Duality theory readily provides a rigorous certificate for the solution.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^dee6cacc]. CDC (2011). Medium credibility.

Community viral load sample size calculations — sample size to detect differences in geometric mean (GM) viral load depends on power and standard deviation (S). The table specifies "α = 0.05 and W = 0.8", with S columns 1 to 1.5; for k = 3 the minimum sample sizes by S are 54, 66, 78, 92, 106, and 122, and "1.2 is the standard deviation of national VL data", so at S = 1.2 the needed sample size is 78.

---

### Input-output maps are strongly biased towards simple outputs [^659947d0]. Nature Communications (2018). Medium credibility.

Random matrix map with bias but not simplicity bias

Finally, we provide an example of a map that exhibits strong bias that is not simplicity bias. We define the matrix map by having binary input vectors p of length n that map to binary output vectors x of same length though x i = Θ ((M ⋅ p) i), where M is a matrix and the Heaviside thresholding function Θ (y) = 0 if y < 0 and Θ (y) = 1 if y ≥ 0. This last nonlinear step, which resembles the thresholding function in simple neural networks, is important since linear maps do not show simplicity bias. In Fig. 1e, we illustrate this map for a matrix made with entries randomly chosen to be −1 or +1. The rank plot (Supplementary Fig. 17) shows strong bias, but in marked contrast to the other maps, the probability P (x) does not correlate with the complexity of the outputs. Simplicity bias does not occur because the n × n independent matrix elements mean that the mapping's complexity grows rapidly with increasing n so that the map violates our limited complexity condition (1). Intuitively: the pattern of 1s and 0s in output x is strongly determined by the particular details of the map M, and so does not correlate with the complexity of the output.

To explore in more detail how simplicity bias develops or disappears with limited complexity condition (1), we also constructed a circulant matrix where we can systematically vary the complexity of the map. It starts with a row of p positive 1 s and q − 1 s, randomly placed. The next row has the same sequence, but permuted by one. This permutation process is repeated to create a square n × n matrix. Thus the map is completely specified by defining the first row together with the procedure to fill out the rest of the matrix. In Fig. 2, we plot the ratio of the mean complexity sampled over outputs divided by the mean complexity sampled over all inputs, as a function of the complexity of the first row that that defines the matrix. If the ratiois significantly larger than one, then the map shows simplicity bias. Simplicity bias only occurs whenis very small, i.e. for relatively simple maps that respect condition (1). The output of one such simple matrix maps is shown in Fig. 1f. In Supplementary Note 12, we investigate these trends in more detail as a function of matrix type, size n, and also investigate the role of matrix rank and sparsity.

---

### Learning the pattern of epistasis linking genotype and phenotype in a protein [^6d4da130]. Nature Communications (2019). High credibility.

Functional trajectories and genotypic connectivity

To obtain the number of functional single-step trajectories, we compute an adjacency matrix between functional genotypes by binarization of the full genotype adjacency matrix above a select threshold brightness (for Fig. 6, y = 0.73, the value for the red parental variant). From the binarized adjacency matrix, the (i, j)-th element of the m th power of the matrix gives the number of functional m -step trajectories that exist between genotypes i and j. Summing over the powers of this matrix to any order M gives all viable trajectories consisting of M or fewer steps in the sequence space. Conversion of the resulting summed matrix to block-diagonal form produces a "genotypic connectogram" — a graph that directly reveals the connectivity and topology of viable genotypes (Supplementary Fig. 12C).

Sparse optimization and phenotype reconstruction

CS is performed by finding a sparse representation for the data on the basis of a (small) subset of measurements, and subsequent reconstruction the full dataset by inverse transformation from the sparse representation back to the data domain (see ref.for an excellent presentation of the theory). Finding the optimal sparse distribution of epistatic terms is achieved by L1-norm minimization and was performed in this study inusing the1 solver, version 1.4. The performance for mutant prediction is scored by the Goodness of Prediction (GoP) parameter, where SSE is the sum of squared errors between reconstruction phenotypes and the measured values, and SST is the total sum of squares.scripts are provided as Supplementary Software 1.

Alignment epistasis

First-order alignment epistasis is calculated according to, where φ i is the average valueof the two states of amino acid x at position i represented as −1 and +1, over the n functional sequences in the alignment.and N tot are the number of functional sequences and the total number of sequences in the combinatorial dataset (see ref.for extended theory). Second order alignment epistasis is calculated according to, where φ ij is the joint expectation between pairs of positions i and j. In the current dataset we have explicit numbers forand N tot and therefore we can numerically compare this with calculated background-averaged epistatic terms (Fig. 5 and Supplementary Fig. 10).

Reporting summary

Further information on research design is available in the Nature Research Reporting Summary linked to this article.

---

### Fundamental limits to learning closed-form mathematical models from data [^5954e9c9]. Nature Communications (2023). High credibility.

Fig. 1
Probabilistic model selection makes quasi-optimal predictions about unobserved data.

We select two models m *, whose expressions are shown at the top of each column. a, b From each model, we generate synthetic datasets D with N points (shown, N = 100) and different levels of noise s ϵ (shown, s ϵ = 1). Here and throughout the article, the values of the independent variables x 1 and x 2 are generated uniformly at random in [− 2, 2]. Vertical lines show the observation error ϵ i for each point in D. For a model not drawn from the prior and data generated differently, see Supplementary Fig. S1. c, d For each dataset D (with dataset sizes N ∈ {25, 50, 100, 200, 400}), we sample models from p (m ∣ D) using the Bayesian machine scientist, select the MDL model (maximum p (m ∣ D)) among those sampled, and use this model to make predictions on a test dataset, generated exactly as D. We show the prediction root mean squared error (RMSE) of the MDL model onas a function of N and s ϵ. For comparison, we also show the predictions from an artificial neural network (ANN, dotted lines; Methods). Since s ϵ is the irreducible error, predictions on the diagonal RMSE = s ϵ are optimal. e, f We plot the prediction RMSE scaled by the irreducible error s ϵ; optimal predictions satisfy RMSE/ s ϵ = 1 (dashed line).

---

### Dynamics of specialization in neural modules under resource constraints [^ada676da]. Nature Communications (2025). High credibility.

For each input sample, the module with the highest overall "certainty" (largest value) takes the decision. More precisely, the global output layer of the network r o u t is defined to be the output layer of the sub-network with the largest maximum value. In practice, that means that only the module having produced the largest overall value is being used in computing the resulting loss value, and that gradients are back-propagated to the other (non-deciding) module only via the (sparse) communication weights. This choice was made to minimize interactions between the modules other than the explicit sparse connectivity, ensuring the necessity of training the sparse inter-module weights.

Structural modularity

We define the fraction of connections between two modules of size n as p ∈ [1/ n 2, 1]. The same fraction of connections is used in each direction. The smallest value is p = 1/ n 2 corresponding to a single connection in each direction, and the largest fraction p = 1 corresponds to n 2 connections in each direction (all-to-all connectivity).

We adapt the Q metric for structural modularityto a directed graph:where M is the total number of edges in the network, A is the adjacency matrix, δ is the Kronecker delta, g i is the group index of node i (which module the node is in), and P i j is the probability of a connection between nodes i and j if we randomized the edges respecting node degrees. For our network, we can analytically compute (Supplements 1.1):This varies from Q = 0 when p = 1 (all nodes connected, so no modularity) to Q = 1/2 for p = 0 (no connections between the sub-networks, so perfect modularity). Note that for g equally sized groups in a network, the maximum value that Q can attain is 1–1/ g.

---

### Automated acquisition of explainable knowledge from unannotated histopathology images [^12b08a4f]. Nature Communications (2019). High credibility.

Defining the total number of small images belonging to the positive and negative groups as n' positive and n' negative, we defined the positive and negative degrees r' positive, k' and r' negative, k' for the k' th feature aswhere the sums Σ + and Σ −, analogously to those in Step 1, are over all i, j, j' such that the image S ' i, j, j' belonged to the positive and negative groups, respectively. For this step, we defined the impact score I' i, j aswhere m denotes the total number of small images S 'i, j, j' used for S i, j.

In Step 3, results of Step 2 complementarily corrected those of Step 1. Images that were frequently in the positive and negative groups had impact scores above and below 0.5, respectively, so we defined images with impact scores above and below 0.5 as having positive and negative characteristics, respectively. We then removed images whose characters, based on the impact scores in Steps 1 and 2, did not match. Finally, we used the total numbers of each feature type for the subsequent predictions.

Comparison of BCR predictions

To evaluate our approach, we predicted cancer recurrence using deep learning-generated 100 features. We used AUC and pseudo R -squared for comparison of deep learning-generated features and Gleason score. AUC is the most frequently used metric to compare classifier performance, taking values ranging from 0 to 1. Higher the AUC, better the model is at classification. The pseudo R -squared valueis a goodness-of-fit metric for regression models with a categorical response variable. It is an analogue to the R -squared for the ordinary least-squares regression and takes a value between 0 and 1. Higher the pseudo R -squared value, the better the model is at classification. We conducted two evaluations: BCR predictions based on cross validation using data at NMSH excluding 100 cases that were used for key feature generation, and external validation of BCR predictions based on data at SMH and AMH using the prediction model only trained by data at NMSH.

---

### Restricting datasets to classifiable samples augments discovery of immune disease biomarkers [^5ba7613e]. Nature Communications (2024). High credibility.

Standardising each rAUC according to sample size gave the restricted standardised AUC (rzAUC). The maximum absolute value of rzAUC defined the optimal restriction value (Fig. 2c). In our example, rzAUC was maximal at FPR = 0.258, which corresponded to an optimal biomarker restriction value of 6.8. Consequently, biomarker HIGH samples should be kept and biomarker LOW samples should be discarded – that is to say, biomarker HIGH samples are classifiable, whereas biomarker LOW samples are unclassifiable. In other situations, the positive class may have higher or lower biomarker values, potentially leading to an AUC < 0.5 and accordingly, a negative rzAUC. In Supplementary Fig 4, we show that regardless of which class is labelled positive or negative, our method identifies the same restriction value. In such cases, biomarker HIGH and biomarker LOW rzAUCs are mirrored, meaning the restriction at the optimal absolute rzAUC remains identical. Critically, regardless of biomarker distribution, because areas under ROC curves are independent of class size, it follows that restriction values are also independent of class size.

Box 1 Restriction

Input: Biomarker values Y i for i in 1, …, N samples of diseased (positive, D i = 1) or non-diseased (negative, D i = 0) class.

Output: Optimal restriction (r), informative range (info. range), restricted AUC (rAUC), restricted standardised AUC (rzAUC) and permutation p-value.

Algorithm:
Calculate ROC curve with true positive rateand false positive rate,
For every restriction r: Calculate the partial area under the ROC curve i. up to the false positive ratewith ii. starting from a true positive ratewith Calculate restricted AUCs Calculate approximate statistic for X = 'high' or X = 'low' where m X is the number of positive and n X is the number of negative samples defined as
Find the optimal restriction value r opt and informative part X opt by
Report
Calculate permutation p-values. Repeat steps 1–3 for permuted class labels (D = 1, D = 0) n total times. Count. Calculate p-value (see methods).

---

### How do you design randomised trials for smaller populations? A framework [^9ac3f0db]. BMC Medicine (2016). Low credibility.

Exploring less common approaches to reducing sample size

We now consider some less standard approaches to bringing the sample size requirements closer to the numbers it is feasible to recruit in a reasonable time frame.

Step 3: Relaxing α by a small amount, beyond traditional values

The much-criticised 5% significance level is used widely in much applied scientific research, but is an arbitrary figure. It is extremely rare for clinical trials to use any other level. It may be argued that this convention has been adopted as a compromise between erroneously concluding a new treatment is more efficacious and undertaking a trial of an achievable size and length. Settings where traditionally sized trials are not possible may be just the area where researchers start to break this convention, for good reason.

In considering the type I error, it is critical to consider the question: 'What are the consequences of erroneously deciding to use a new treatment routinely if it is truly not better?'

Taking the societal perspective as before, we might consider the probability of making a type I error, thus erroneously burdening patients with treatments that do not improve outcomes, or even worsen them, while potentially imposing unnecessary toxicity.

First, for conditions where there are only enough patients available to run one modestly sized randomised trial in a reasonable time frame, research progress will be relatively slow, and making a type I error may be less of a concern than a type II error. In contrast, making several type I errors in a common disease could lead in practice to patients taking several ineffective treatments; for a disease area where only one trial can run at any given time, the overall burden on patients is potentially taking one ineffective treatment that does not work.

Thus, if we take the societal perspective with the trials in Table 1 then, if each trial was analysed with α = 0.05 and we see (hypothetically) 40% positive results, then the expected number of false positive trials is given in the final column. We also assumed 10% and 70% positive results, with qualitatively similar conclusions.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^cbffc27e]. CDC (2011). Medium credibility.

Appendix G. Mathematical formula for sample size — sample size required to detect the difference of GM between two subpopulation groups defines a z test with null hypothesis GM1 = GM2 or GMmax/GMmin = 1 and alternative GM1 ≠ GM2 or GMmax / GMmin > 1, where GMmax = max(GM1, GM2) and GMmin = min(GM1, GM2). It considers power W (say 80% chance) to detect that one group's GM is at least k fold higher and gives the p-value as p = Prob(Z ≥ z) = 1 − φ−1(z) with Z ~ Normal (0,1). Given α (e.g., 0.05) and target ratio k, power is W = Prob (Z > φ−1(1 − α) − log10(k)/(S√(1/n1 + 1/n2))), where S is the expected standard deviation of log10(VL) in the population of interest; when n1 = n2 = n, W = Prob (Z > φ−1(1 − α) − log10(k)/(S√(2/n))). The required sample size is n = 2[(φ−1(1 − α) − φ−1(1 − W))S/log10(k)]^2, and tables are provided for W = 0.8 and W = 0.9 with α = 0.05.

---

### Balancing false positives and false negatives for the detection of differential expression in malignancies [^26b6ac47]. British Journal of Cancer (2004). Low credibility.

Calculation of the number of genes that are and are not actually differentially expressed

Call N 0 the number of genes that are not actually differentially expressed (i.e. for which the null hypothesis is true) and N 1 the number of genes that are actually differentially expressed (i.e. for which the null hypothesis is false) – also see Table 1. Of course, these numbers are not known in advance and have to be estimated from the data. We use a recently introduced method for the estimation of N 0, which essentially consists of an evaluation of the following formula:

After N 0 is derived, N 1 can easily be estimated by N − N 0.

To test whether this approach results in reliable estimates for N 0 and N 1, we applied this method on five synthetic data sets generated by the model introduced by(also see) with increasing values for the standard deviations of the additive and multiplicative errors. These data sets contained 100 experiments each and were designed such that the values for N 1 and N 0 were known beforehand. For each data set, 8000 (N 0) genes had a constant true expression level over the 100 experiments, while 2000 (N 1) genes had a different true expression level in the first 50 experiments compared with the last 50 experiments. The five estimates of N 1 varied between 1939 and 1866, dependent on the settings for the additive and multiplicative error. Note that using a two-sample t -test instead of the Wilcoxon test did not result in a significantly different result (estimates varied between 1909 and 1847).

Estimation of the number of true positive, true negative, false-positive and false-negative genes

Suppose that we declare the genes with a P -value smaller than or equal to a certain rejection level α = p i (P -value of the i th gene) as differentially expressed (i.e. the null hypotheses for these genes are rejected) and the genes with a P -value larger than this rejection level as not differentially expressed (i.e. the null hypotheses for these genes are not rejected). When the declared status of differential expression is compared with the actual status, four categories of genes (true positive, true negative, false-positive and false-negative genes) emerge that are defined in Table 1. Using the value of N 1 and N 0, derived in the previous section, we can calculate the number of genes in each category using the formulas from Table 1.

---

### Simulation of pool testing to identify patients with coronavirus disease 2019 under conditions of limited test availability [^b8aca0ce]. JAMA Network Open (2020). High credibility.

Methods

This decision analytical model study did not require institutional review board review because it used simulation-based research, per Common Rule 45 CFR §102 (e). This study followed the Strengthening the Reporting of Empirical Simulation Studies reporting guideline.

We considered a 2-stage pool testing, in the presence of imperfect testing, in which p is prevalence, S e is test sensitivity, and S p is specificity. We assumed that the probability of a true-positive result pool test equals S e, the probability of a false-positive result equals 1 − S p, test S e and S p would be unaffected by the number of patients in a pool (k), and all tests are identically distributed. Let Z be a random variable denoting the number of tests needed to complete the pooling strategy. Let N be the total number of patients, then there are N/ k subgroups, each with k members. Assuming independent Bernoulli trials, let P k be the probability of having at least 1 positive test in k patients. If 1 patient of the subgroup has RT-PCR results positive for SARS-CoV-2, then there will be (k + 1) tests necessary; otherwise, only 1 test will be needed. The expected number of tests for each subgroup with k patients is (k + 1)P k + (1 − P k). Then, for N/ k subgroups, the expected number of tests needed is E(Z) = (N/ k)[(k + 1)P k + (1 − P k)] = (N/ k)[kP k + 1], in which P k = (1 − S p)(1 − p) k + S e (1 − (1 − p) k), which incorporates the effects of test sensitivity and specificity. The optimal pool size is achieved for k that minimizes E (Z). To characterize the pooled test strategy, we define efficiency of the pool as E f = E (Z)/ N = [kP k + 1]/ k and expected number of false negatives (ENFN) as kp (1 − S 2 e) This method does not include a dilution effect on the pooling strategy sensitivity, PS = S 2 e, as a function of pool size, where S 2 e provides the upper bound of the pooling sensitivity.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^aeeaae45]. Nature Communications (2025). High credibility.

Fitting models to data is an important part of the practice of science. Advances in machine learning have made it possible to fit more-and more complex-models, but have also exacerbated a problem: when multiple models fit the data equally well, which one(s) should we pick? The answer depends entirely on the modelling goal. In the scientific context, the essential goal is replicability: if a model works well to describe one experiment, it should continue to do so when that experiment is replicated tomorrow, or in another laboratory. The selection criterion must therefore be robust to the variations inherent to the replication process. In this work we develop a nonparametric method for estimating uncertainty on a model's empirical risk when replications are non-stationary, thus ensuring that a model is only rejected when another is reproducibly better. We illustrate the method with two examples: one a more classical setting, where the models are structurally distinct, and a machine learning-inspired setting, where they differ only in the value of their parameters. We show how, in this context of replicability or "epistemic uncertainty", it compares favourably to existing model selection criteria, and has more satisfactory behaviour with large experimental datasets.

---

### A faster and more accurate algorithm for calculating population genetics statistics requiring sums of stirling numbers of the first kind [^7a69c229]. G3 (2020). Medium credibility.

Ewen's sampling formula is a foundational theoretical result that connects probability and number theory with molecular genetics and molecular evolution; it was the analytical result required for testing the neutral theory of evolution, and has since been directly or indirectly utilized in a number of population genetics statistics. Ewen's sampling formula, in turn, is deeply connected to Stirling numbers of the first kind. Here, we explore the cumulative distribution function of these Stirling numbers, which enables a single direct estimate of the sum, using representations in terms of the incomplete beta function. This estimator enables an improved method for calculating an asymptotic estimate for one useful statistic, Fu's [Formula: see text] By reducing the calculation from a sum of terms involving Stirling numbers to a single estimate, we simultaneously improve accuracy and dramatically increase speed.

---

### Timing of seasonal influenza epidemics for 25 countries in Africa during 2010–19: a retrospective analysis [^37d8d3e9]. The Lancet: Global Health (2023). High credibility.

The aggregate average method generates a summary curve by summing weekly data across years without requiring manual assessment and alignment of yearly peak influenza activity. With this method, we generated summary epidemic curves of all years with surveillance data for each country by first calculating the sum of influenza virus-positive specimens and the sum of total specimens tested for each epidemiological week, using the following formulas:andwhere n is the epidemiological week, x is the first year with available influenza surveillance data, y is the last year with available influenza surveillance data, p [n] is the number of influenza virus-positive specimens in week n, p n is the total number of influenza virus-positive specimens for week n in years x–y of influenza surveillance, s [n] is the number of specimens tested for influenza virus in week n, and s n is the total number of specimens tested for influenza virus for week n in years x–y of influenza surveillance.

We then calculated a centre-aligned 3-week moving influenza proportional positivity using the following formula:where m n was the 3-week moving proportional positivity for week n = 1… 52. We defined p 0 = p 52, s 0 = s 52, p 53 = p 1, and s 53 = s 1 to account for the temporal continuity between years. We calculated 95% binomial proportion CIs for the 3-week moving proportional positivity using the Wilson method. Weeks were excluded from smoothed analyses if the number of specimens tested for that week represented less than 0·1% of the total specimens tested during all 52 weeks, because of low confidence in proportional positivity data. We calculated the epidemic threshold as the mean of the 3-week moving proportional positivity across all 52 epidemiological weeks. We defined the onset of an epidemic or season as the first week when the weekly proportional positivity exceeded the annual mean influenza proportional positivity for at least 3 consecutive weeks. The end of an epidemic or season was the first week when the weekly proportional positivity fell below the annual mean influenza proportional positivity and stayed below the annual mean for at least 3 consecutive weeks. Influenza transmission peaks during a year were defined as the highest proportional positivity during the periods of time when proportional positivity was above the epidemic threshold.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^b1513dd5]. CDC (2011). Medium credibility.

Table 1b — Minimum sample size for detecting geometric mean (GM) ratio k with α = 0.05 and W = 0.9 — presents required sample sizes by standard deviation (S) strata and defines parameters; α is significance level, W is power, GM is geometric mean, and S is standard deviation, noting that 1.2 is the standard deviation of national viral load (VL) data. Each jurisdiction will need to assess the standard deviation of their local VL data and then determine the appropriate sample size needed to assess VL, and if the sample size is inadequate to meet the recommended case inclusion criterion, an alternate method may need to be used, such as combining multiple years of data. For k = 3, sample sizes across S = 1, 1.1, 1.2, 1.3, 1.4, 1.5 are 75, 91, 108, 127, 147, 169, and Table 1b uses power = 90%; jurisdictions may also explore differences in means of viral loads, including categorical differences in the proportion with undetectable or very low VL.

---

### Estimation of significance thresholds for genomewide association scans [^1d5c925a]. Genetic Epidemiology (2008). Low credibility.

EFFECTIVE NUMBER OF TESTS

Permutation procedures are time consuming, and an attractive alternative is to estimate an effective number of independent tests directly from the genotype correlation matrix. A moment–based estimator was recently proposed bybased on the eigenvalues of the correlation matrix. This estimator has good properties in the context of detecting population structure, but we wished to see whether it is equally useful for correcting multiplicity. Letwhere n is the number of markers and M is a normalized matrix of genotypes with one row per subject and one column per marker [for details, see]. Denoting the eigenvalues of X by λ 1,… ,λ m, where m is the number of subjects, the effective number of tests is estimated byAs for the permutation test, we estimated n P for a grid of subsampling densities. We calculated n P for each chromosome and summed to obtain a genomewide estimate.

BETA DISTRIBUTION

If there really is an effective number of independent tests n E, then the minimum P –value should follow a Beta distribution with parameters (1, n E), as We fitted the Beta distribution to the minimum P –value of the permutation replicates, with the first parameter set to 1 and also with both parameters free. This would allow us to test whether the minimum P –value is consistent with an effective number of independent tests, by testing whether the first parameter is 1, and whether Patterson's estimator is accurate, by testing whether n p = n E. The moment estimators for the parameters of the Beta (a, b) distribution arewhereand s 2 are the sample mean and variance of observations, respectively. When a is set to 1, the moment estimate of b isWe used the moment estimates as starting points for numerical maximum likelihood estimation, using the optim function in R.

---

### Estimation of significance thresholds for genomewide association scans [^9c80a02e]. Genetic Epidemiology (2008). Low credibility.

PERMUTATION TEST

We estimated a genomewide significance threshold using a permutation procedure. We randomly designated half the sample "cases" and the other half "controls" and calculated the Armitage test of trend for differences in genotype frequency. P –values for all SNPs were sorted and the 1,000 smallest were recorded. This procedure was repeated 10,000 times.

The 5% quantile point of the minimum P –value represents the genomewide significance threshold at this marker density. To extrapolate to complete saturation, we randomly subsampled the SNPs over a range of lower densities by equivalently subsampling the sorted P –values independently for each permutation replicate. We used a uniform grid of 100 marker densities, and at each density the 5% point of the minimum P –value was recorded. We repeated the subsampling 100 times at each density and used the mean 5% point in subsequent analysis. At low densities, the SNPs are expected to be independent; hence, according to the Bonferroni law the 5% point is inversely proportional to the number of SNPs. At high densities we expect the 5% point to converge to an asymptote, reflecting the significance threshold for the whole genome.

Denote the proportion of SNPs subsampled by x i; i = 1,2, …, where x i < x i +1 and x i are relative to the total number of SNPs used in the estimation. Denote the corresponding 5% quantile points y i. It follows that the effective number of independent tests defined byshould be proportional to x i when x i is small and should converge to an asymptote as x i grows large. To obtain these properties, we fit the Monod function to (x i, m i):This model is not claimed to be exact, but it has been found to fit data well in applications such as modeling population growth with limited resources [e.g.]. The parameters of this model are μ, the limit as x → ∞, and k, the half–saturation parameter representing the value of x for which f (x) = μ/2. We estimated the parameters by least squares to give the genomewide significance threshold asWe used a non–parametric bootstrap to estimate confidence intervals for; genome. We resampled the minimum P –values with replacement from the permutation replicates, and for each resampling we subsampled SNPs, fitted the Monod function and calculated. We estimated 95% confidence intervals for; k andfrom 1,000 bootstrap samples.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^658be462]. Nature Communications (2025). High credibility.

Interpreting the realisationsas PPFs also imposes a third constraint, more subtle but equally important:
The processmust be non-accumulating.

A process which is accumulating would start at one end of the domain, say Φ = 0, and sequentially accumulate increments until it reaches the other end. Brownian motion over the interval [0, T] is an example of such a process. In contrast, consider the process of constructing a PPF for the data in Fig. 2: initially we have few data points and the PPF of their loss is very coarse. As the number of points increases, the PPF gets refined, but since loss values occur in no particular order, this happens simultaneously across the entire interval.

The accumulation of increments strongly influences the statistics of a process; most notably, the variance is usually larger further along the domain. This would not make sense for a PPF: ifis smaller than, that should be a consequence of δ EMD (Φ) being smaller than — not of Φ occurring "before".

This idea that a realisationis generated simultaneously across the interval led us to defineas a sequence of refinements: starting from an initial incrementfor the entire Φ interval [0, 1], we partition [0, 1] into n subintervals, and sample a set of n subincrements in such a way that they sum to. This type of distribution, where n random variables are drawn under the constraint of a fixed sum, is called a compositional distribution. Note that the constraint reduces the number of dimensions by one, so a pair of increments would be drawn from a 1-d compositional distribution. A typical 1-d example is the beta distribution for x 1 ∈ [0, 1], with x 2 = (1 − x 1) and α, β > 0:

---

### Quantification of network structural dissimilarities [^3cf663e0]. Nature Communications (2017). Medium credibility.

Identifying and quantifying dissimilarities among graphs is a fundamental and challenging problem of practical importance in many fields of science. Current methods of network comparison are limited to extract only partial information or are computationally very demanding. Here we propose an efficient and precise measure for network comparison, which is based on quantifying differences among distance probability distributions extracted from the networks. Extensive experiments on synthetic and real-world networks show that this measure returns non-zero values only when the graphs are non-isomorphic. Most importantly, the measure proposed here can identify and quantify structural topological differences that have a practical impact on the information flow through the network, such as the presence or absence of critical links that connect or disconnect connected components.

---

### Evolutionary game theory of growth factor production: implications for tumour heterogeneity and resistance to therapies [^c0051ab8]. British Journal of Cancer (2013). Low credibility.

Results

It follows that x = 0 is a stable rest point of (1) if and only if Δ b 0 < c, and that x = 1 is a stable rest point of (1) if and only if Δ b n −1 ⩾ c. In addition, any interior stable rest point x s must satisfy equation (3) and β ′(x s) < 0. It follows that there is at most one interior stable rest point x s and that, if such a rest point exists, it satisfies x * < x s < 1. These conclusions define, for any sigmoid benefit function, the following five types of dynamics (Figure 1), where we simplify notation by defining β ✱ = β (x *):
If c > β *, then β (x) < c x, and x = 0 is the only stable equilibrium.
If c < min[Δ b 0, Δ b n −1], then β (x) > c x, and x = 1 is the only stable equilibrium.
If min[Δ b 0,Δ b n −1] < c < max[Δ b 0,Δ b n −1], and Δ b 0 < Δ b n −1, then β (x) > c for x > x u and β (x) < c for x < x u; therefore, the unique interior unstable equilibrium x u divides the basin of attraction of the two stable equilibria x = 1 and x = 0.
If min[Δ b 0,Δ b n −1] < c < max[Δ b 0, Δ b n −1], and Δ b 0 > Δ b n −1, then β (x) > c for x < x s and β (x) < c for x > x s; therefore, the unique interior stable equilibrium x s divides the basin of attraction of the two unstable equilibria x = 1 and x = 0.
If max[Δ b 0, Δ b n −1] < c < β *, then β (x) > c for x u < x < x s, whereas β (x) < c for x < x u and for x > x s; therefore, the interior unstable equilibrium x u divides the basins of attraction of the two stable equilibria x = 0 and x = x s.

---

### Detecting bioterror attack [^30f1fb2d]. Emerging Infectious Diseases (2004). Low credibility.

Appendix

Following Kaplan et al. the probability D (τ) of finding at least one positive blood donation and detecting the attack within time τ, after a single bioterror attack initially infecting a proportion P of an exposed population of size N, is given by

When R 0 > 1, this logistic function increases, remains constant or decreases from the initial value I(0) = I 0 towards the steady state I (t →∞) = I ∞ = (R 0–1) N / R 0 for

I 0 < 1 ∞, I 0 - I ∞, or I 0 > I ∞, respectively. In particular, in the limit of P < < (R 0–1) / R 0, this expression reduces to the early approximation solution, I es (t) – I 0 exp {-(R 0–1) rt }, and the resulting probability D es (τ)of attack detection is instead,

D es (τ) = 1 – exp{-1I 0 [a ƒ(τ /∞) − b ƒ[(R 0–1) rτ]]}

[2]

where the function, ƒ(x) = x – 1 + exp (-x), and the constants are, a = k ∞ [1 - r ∞(2 R 0–1)] / [1-r∞(R 0–1)] and b = kR 0 / { r (2R 0–1) 2 { 1-r∞ (R 0–1)]}. This D es (τ) increases when any of the parameters increase.

---

### Detecting the ultra low dimensionality of real networks [^baa84bdd]. Nature Communications (2022). High credibility.

Methods

Estimation of hidden degrees of real networks

Given a real network, our goal is to generate networks with themodel but preserving the degree distribution of the real network as much as possible. In themodel, hidden degrees are given fixed values but nodes' degrees are random variables that depend on the hidden degrees. Therefore, to reproduce the degree distribution, we have to find the sequence of hidden degrees that better reproduces the sequence of observed degrees. To do so, we generalize the method into arbitrary dimensions. Given a set of parameters β and D, for each observed degree class k in a real network, we infer the corresponding hidden degree κ so that the ensemble average degree of themodel of a node with hidden degree κ is equal to its observed degree, that is.

After this procedure, the degree distribution of synthetic networks generated by themodel with the inferred sequence of hidden degrees is very similar to the one from the real network. Specifically,
Initially set κ i = k i ∀ i = 1, N, where k i is the observed degree of node i in the real network.
Compute the expected degree for each node i according to themodel as whereand.
Correct hidden degrees: Letbe the maximal deviation between actual degrees and expected degrees. If ϵ m a x > ϵ, the set of hidden degrees needs to be corrected. Then, for every class of degree k i, we set, where u is a random variable drawn from U (0, 1). The random variable u prevents the process from getting trapped in a local minimum. Next, go to step 2 to compute the expected degrees corresponding to the new set of hidden degrees. Otherwise, if ϵ m a x ≤ ϵ, hidden degrees have been correctly inferred for the current global parameters.

Following this algorithm, we can generate surrogates of a given network G with different D and β values without modifying the degree distribution. The tolerance value of ϵ used in this work is ϵ = 1.

---

### Optimization of b-value schemes for estimation of the diffusion coefficient and the perfusion fraction with segmented intravoxel incoherent motion model fitting [^b1e55efe]. Magnetic Resonance in Medicine (2019). Medium credibility.

Optimization of b‐values and number of acquisitions for a finite total number of acquisitions was based on CRLB (Equation 9 and 10). The objective function used in the optimization was the average error over a range of values of D and f (D ∊ [1.0 1.2] µm 2 /ms, f ∊ [0.15 0.30]), with error defined as the sum of the coefficients of variation as follows:where (F −1) DD and (F −1) ff are obtained from Equations 9 and 10, respectively. The average error was calculated by summing CV tot over 100 × 100 linearly distributed values of D and f on the specified range, which were based on previously reported parameter values in healthy liver. 27

Optimal sets of b‐values were found through constrained minimization using the MATLAB function fmincon. The adjustable parameters in the optimization were the b‐value of each signal acquisition, b i. To avoid local minima, 10 random initializations were used. The optimization procedure was performed for 3 to 12 signal acquisitions, with the requirements of at least one acquisition with b = 0 and at least two acquisitions in the interval 200 to 800 s/mm 2. Note that the number of acquisitions of each b‐value was not explicitly included in the optimization, i.e. n i = 1 in Equations 9 and 10. Instead, it was obtained as the number of times the same b‐value repeated in the solution. Since SNR 0 in Equation 11 is a positive constant, which can be chosen arbitrarily without affecting the SNR dependence in the optimization, it was omitted in the expression used in the optimization.

To study the behavior of the optimization at a large number of acquisitions, a similar optimization procedure was set up in the limit of infinitely many acquisitions. Since the total number of acquisitions (n tot = Σ i n i) can be considered a constant in the optimization, Equation 8 can be divided by n tot without affecting the solution. The resulting ratios a i = n i / n tot are the proportion of acquisitions acquired at each b‐value. These were added as adjustable parameters in the optimization with the constraints that a i was allowed to vary between 0 and 1 and that they summed up to 1.

The MATLAB code for optimization of b‐value schemes is available at.

---

### SARS-CoV-2 gene content and COVID-19 mutation impact by comparing 44 sarbecovirus genomes [^41ae8b01]. Nature Communications (2021). High credibility.

To find regions that were significantly enriched for missense mutations in conserved amino acids, we first defined a null model as follows. For each mature protein, we counted the number of missense mutations and the number of conserved amino acids and randomly assigned each SNV to a conserved amino acid in the same mature protein (using Python's random.randint function), allowing multiplicity. For any positive integer n, we found the largest number of mutations that had been assigned to any set of n consecutive conserved amino acids within the same mature protein across the whole genome. Doing this 100,000 times gave us a distribution of the number of missense mutations in the most enriched set of n consecutive conserved amino acids in the genome. Comparing the number of actual missense mutations in any particular set of n consecutive conserved amino acids to this distribution gave us a nominal p -value for that n. We applied this procedure for each n from 1 to 100 and multiplied the resulting p -values by a Bonferroni correction of 100 to calculate a corrected p -value for a particular region to be significantly enriched. We note that these 100 hypotheses are correlated because enriched regions of different lengths can overlap, so a Bonferroni correction is overly conservative and our reported p -value of 0.012 understates the level of statistical significance. To find significantly depleted regions we applied a similar procedure with every n from 1 to 1000, but did not find any depleted regions with nominal P -value < 0.05 even without multiple hypothesis correction.

---

### The minimal work cost of information processing [^894e044e]. Nature Communications (2015). Medium credibility.

where the sum ranges only over those x that have a non-zero probability of occurring. In the case of deterministic mappings p (x ′| x)∈{0,1}, this corresponds to the maximum number of input states that map to a same output state. For the AND gate, provided all four states 00, 01, 10 and 11 have non-negligible probability of occurring, there are three input states mapping to the same output state, so (3) gives us simply. Also, in simple examples as considered here, the expression (3) is stable to considering an-approximation (Supplementary Note 4); this quantity is thus physically justified.

Crucially, our result reveals that the minimal work requirement in general depends on the specific logical process, and not only on the input and output states. This contrasts with traditional thermodynamics for large systems, where the minimal work requirement of a state transformation can always be written as a difference of a thermodynamical potential, such as the free energy. For example, the minimal work cost of performing specifically an AND gate may differ from that of another logical process mapping an input distribution (p 00, p 01, p 10, p 11) (with ∑ i p i = 1) to the distribution (p ′ 0, p ′ 1) = (p 00 + p 01 + p 10, p 11) (Recall that the classical counterpart of a quantum state is a probability distribution.). To see this, consider the XOR gate, which outputs a 1 exactly when both inputs are different (see Fig. 2b). The minimal work cost requirement of this gate, as given by (3), is now only kT ln 2, as in the worst case, only a single bit of information is erased (again supposing that all four input states have non-negligible probability of occurring). Now, suppose that, for some reason, the input distribution is such that p 01 + p 10 = p 11, that is, the input 11 occurs with the same probability as of either 01 or 10 appearing. Then, the XOR gate reproduces the exact same output distribution as the AND gate: in both cases, we have p ′ 0 = p 00 + p 10 + p 01 = p 00 + p 11 and p ′ 1 = p 11 = p 01 + p 10. In other words, both logical processes have the same input and output state, yet the XOR gate only requires work kT ln 2 compared with the AND gate, which requires 1.6 kT ln 2. Furthermore, we point out that this difference, which appears small in this case, may be arbitrarily large in certain scenarios (Supplementary Note 4).

---

### DrFARM: identification of pleiotropic genetic variants in genome-wide association studies [^a2a4ca5b]. Nature Communications (2025). High credibility.

Simulation II

For this simulation, all settings are kept the same as Simulation I except x n i Bin(2, p i) independently for all n = 1, …, N and Z k ~ MVN N (0, K) independently for k = 1, …, K, where Z = [Z 1, …, Z K]. To mimic common variants in GWAS, p i ~ Unif(0.05, 0.95) independently for all i = 1, …, P. We generated kinship K using the standardized X * X * T (i.e. cov2cor in R) wherehas its entriesfor n = 1, …, N and i = 1, …, P so that the off-diagonal entries of K has a mean of 0.25 to simulate a third-degree relationship (2 × 0.125) between individuals on average.

Performance metrics

We used true positive rate (TPR), true negative rate (TNR), FDR, and Matthew's correlation coefficient (MCC)to compare the performance of different approaches in simulations I and II, at both the individual-level and group (SNP) level. In particular, for methods that do not provide p values (i.e. without debiasing or with inner-debiasing only), the number of true positive (TP) is the number of nonzero elements in the selectedin the signal set for signal-level result and the number of pleiotropic variants with at least one nonzero association for the group (SNP) level result. The number of true negatives (TN) is the number of zeros in the selectedin the non-signal set for signal-level result and the number of the non-pleiotropic variant with no association for the group-level result. Then, the number of false positives (FP) and the number of false negatives (FN) are simply given by the number of positive (nonzero coefficients) minus TP, and the number of negatives (zero coefficients) minus TN, respectively. For methods that provide p values (i.e. outer-debiasing or double debiasing), we applied Benjamini–Hochberg procedureto both the signal-level and group-level p values at 5% level. To calculate TP, TN, FP and FN, instead of evaluating whether the coefficients are nonzero, we consider whether the adjusted p values are smaller than 0.05.

---

### Group testing via hypergraph factorization applied to COVID-19 [^8ca17f9d]. Nature Communications (2022). High credibility.

Simulation under the COVID-19 model

We performed simulations studies using the COVID-19 model of Cleary and Hay et al. The model first simulates viral loads for a large population of n pop = 12,500,000 individuals across d pop = 357 days during which the epidemic grows then declines. It captures the evolution of both: (a) viral loads within each individual, i.e. within-host viral kinetics, and (b) infection prevalence in the overall population. See Cleary and Hay et al.for a detailed description. The main output we use is a matrixof the population viral loads, whereis the viral load of individual i on day d.

Next, the model simulates pooled testing to determine the average efficiency (relative to individual testing) and average sensitivity for each day. For the reader's benefit, we detail the process here. For HYPER designs, i.e. H n, m, q, the simulation proceeds for each trial r of day d as follows:
Draw n individuals uniformly at random from the population. Let z 1, …, z n be their viral loads that day. That is, draw n indices k 1, …, k n uniformly at random from the set {1, …, n pop } (with replacement), and let. Put another way. Individuals with nonzero viral load are positive/infected.
Generate the sampled viral load for each of the m poolsas follows:whereis the size of pool j, i.e. the number of individuals assigned to it.
Compute stage 1 pooled testing results:

- if v j > LOD then pool j tests positive, where the LOD (limit of detection) we use is 100.
- otherwise, pool j tests negative with probability 0.99 (i.e. the false-positive rate of PCR results is 1%).
Select putative positives as those individuals that are not in any negative pools.
Compute stage 2 individual testing results for the putative positives: putative positive individual j tests positive if z j > LOD and tests negative otherwise.
Declare individuals identified by HYPER as those that tested positive in stage 2.
Record the following for the current trial r and day d:

- the number of true positive individuals identified by HYPER:
- the number of tests expended: T (r) (d) = m + number of tests used in stage 2,
- the number of true positive individuals seen: = number of individuals with viral load > 0.

---

### Universality of quantum phase transitions in the integer and fractional quantum hall regimes [^2d7a66a5]. Nature Communications (2024). High credibility.

Fig. 4
Scaling exponent in the ES regime for ν = 3 + 2/5 to 3 + 3/7 transition.

a Plots of the T -dependence of G x x versus filling factor ν for two FQH states between ν = 3 and ν = 4. The black box marks the region where the ES analysis was carried out. b Fit of ES Eq. (4) (dotted lines) to the G x x data for the transition from ν = 3 + 2/5 and ν = 3 + 3/7. Each set of data points is for a given value of δ ν = ∣ ν – ν c ∣ with ν c = 3.416. The plots deviate from the expected ES behavior at high T (the region is marked with an ellipse). c Plots of T 0 versus δ ν CF. The dotted line is a linear fit to the data (see Eq. (5)). The slope yields the value of γ. The error bars are determined from the least square fits to the data in (b). d Plot of scaled longitudinal conductance G x x / s as a function of scaling parameter s = ∣ δ ν ∣ γ / T for PT between ν = 17/5 and ν = 24/7. The scatter points of different colors are for different values of ∣ δ ν ∣, and the solid black line is fit to Eq. (6).

An independent estimate of γ is obtained by casting Eq. (4) into a single-parameter scaling form:with the the scaling parameter s = ∣ δ ν CF ∣ γ / T. Figure 4 d shows the scaling plots of G x x / s versus s 1/2 for the PT in ES regime from ν = 3 + 2/5 to ν = 3 + 3/7. We find a near-perfect data collapse for all values of δ ν CF in the localized regime with γ ≈ 2.3, providing an independent validation of the universality of γ.

---

### Efficient classical sampling from gaussian boson sampling distributions on unweighted graphs [^bb10c3c1]. Nature Communications (2025). High credibility.

Theorem 4

Given a bipartite graph G = (V 1, V 2, E) with ∣ V 1 ∣ = m, ∣ V 2 ∣ = n and m ≥ n. If the minimum degree of vertices in V 1 satisfies δ 1 (G) ≥ n − ξ, and the minimum degree of vertices in V 2 satisfies δ 2 (G) ≥ m − ξ for some constant ξ, then for, given error ϵ, we can achieve a sampling in timesuch that the total variation distance between the sampling distribution and the ideal stationary distribution is at most ϵ.

On dense non-bipartite graphs, we have:

Theorem 5

Given a non-bipartite graph G = (V, E) with ∣ V ∣ = 2 n, If the minimum degree of vertices in V satisfies δ 1 (G) ≥ 2 n − 1 − ξ, for some constant ξ, then for, given error ϵ, we can achieve a sampling in timesuch that the total variation distance between the sampling distribution and the ideal stationary distribution is at most ϵ.

We note that our theoretical performance bound in Theorem 4 is stronger for bipartite graphs. This advantage stems from the fact that the subroutine of uniformly sampling perfect matchings is known to be classically more efficient for bipartite graphs. For context, the BipartiteGBS technique also leverages the unique properties of bipartite graphs, though for the different goal of encoding arbitrary matrices for hardness proofs.

Numerical experiments

We conduct experiments to compare our algorithms with prior approaches. All results and plots are obtained by numerical simulations on a 10-core Apple M2 Pro chip with 16 GB memory and an NVIDIA L4 chip with 24 GB GPU memory via python 3.11.0. We use "networkx" libraryto store and manipulate graphs and "thewalrus" libraryto calculate Hafnian from adjacency matrix. Specifically, the graph problems we aim to solve are defined as follows:
Max-Hafnian: Given an undirected graph with non-negative adjacent matrix and target subgraph size k, find the subgraph of size k with the maximum Hafnian value defined as Eq. (2).
Densest k -subgraph: Given an undirected graph with non-negative adjacent matrix and target subgraph size k, find the subgraph of size k with the maximum density value. Density denotes the number of edges divided by the number of vertices.

---

### Anti-correlated feature selection prevents false discovery of subpopulations in scRNAseq [^b5424c1a]. Nature Communications (2024). High credibility.

Definitions

E o: the original expression matrix

rand: an integer vector of the length 5000 for the random samples within the space of 1.n, where n is the number of genes

E r: The random subset matrix that is permuted as defined below:

For i. N (rand):Where E r provides a N(cell) x N(rand) matrix, which is a within-gene bootstrap shuffled version of a subset of the transcriptome, therefore unpairing the gene-gene pairs for measuring the null background of Spearman correlations.

In our testing, using a greater number of randomly selected genes, N (rand), for the permutation based null-background did alter the null-distributions, as these distributions were stable at this sampling depth, and did not notably change the selected cutoffs. Note that the method of rank transformation for Spearman correlation effects the outcome; here we perform dense-rank transformation. Non-dense rank transformations frequently result in large gaps within the distributions because of ties. This is particularly important with count-based datasets where ties are frequent.

The null Spearman background matrix (B) was the symmetric 5000 × 5000 comparison of this sample (5000 choose 2 combinations).

For i = 1. N (rand) and j = 1. N (rand):

Next, this B background matrix, of null Spearman rho values, is filtered for only values B i, j < 0, thus creating a negative correlation null-background; this is needed because the null background for values B i, j > 0 and values B i, j < 0 follow different distributions (Supplementary Fig. 2c), indicating the necessity to measure them independently. Self-comparisons and duplicate comparisons were also removed.

For i = 1. N (rand), and j = i + 1. N (rand):

Conceptually, this filtering is also important because the estimated number of FP for a given gene i is dependent on the number of genes that are actually randomly distributed, or truly correlated. For example, gene X is co-regulated within a module of 2000 genes, while gene Y is not genuinely correlated with any other genes. Given that the number of genes is static and zero sum, this true positive co-regulation removes those genes from possible FP negatively correlated genes.

---

### Skewed X-inactivation is common in the general female population [^3c74d6bf]. European Journal of Human Genetics (2019). Medium credibility.

Analysis of individual genes

To determine whether a gene escapes X-inactivation, we selected skewed individuals with a median paternal ratio across the entire X-chromosome (see previous paragraph) of ≤ 0.35 or ≥ 0.65. We used the following procedure for the analysis of skewing status of individual genes:

Let Mx = median (paternal ratio[i, k]), where i = 1, …, m, with m being the number of SNVs (covered by ≥ 10 reads) on the X-chromosome in a sample and Mg = median (paternal ratio[j, k]), where j = 1, …, n, with n being the number of SNVs in a sample that are mapped to the gene g and where k = 1, …, p, with p being the number of samples with SNVs in the gene g. Note that m and n differ per sample and that we analyze only the genes with p ≥ 5. Further, let Sx = | Mx − 0.5| and Sg = | Mg − 0.5| be the skew factors (the distance from 0.5, where 0.5 reflects balanced expression of the paternal and maternal alleles), then for each gene we have two possible situations:
X-chromosome and gene g agree distance direction(Mg > 0.5 and Mx > 0.5)|(Mg < 0.5 and Mx < 0.5) = > We perform the paired t test: t.test(Sx [k], Sg [k], alternative = "less").
X-chromosome and gene g disagree on distance direction (Mg < 0.5 and Mx > 0.5)|(Mg > 0.5 and Mx < 0.5) = > We perform the paired t test: t.test(Sx [k], − Sg [k], alternative = "less").

The null hypothesis in the test is that the median paternal ratio of the gene is not different from the overall median paternal ratio for that individual, consistent with absence of escapee behavior. The alternative hypothesis is that the median paternal ratio of the gene is closer to 0.5 than the overall median paternal ratio for that individual, consistent with escapee behavior.

---

### Protein degradation by human 20S proteasomes elucidates the interplay between peptide hydrolysis and splicing [^26c1c0a0]. Nature Communications (2024). High credibility.

Computation of the theoretical sequence space size

The number of possible unmodified spliced and non-spliced peptides that could be derived from a protein sequence in sequence-agnostic fashion constituted the size of the theoretical peptide sequence space. The number X of non-spliced peptides of length N that could theoretically arise from a substrate of length L was:

To derive the positions of all spliced peptides, we defined four indices i, j, k and n that denoted the first (i, j) and second (k, n) splice-reactant, respectively. The corresponding number of peptides was calculated via summing over interval ranges that form valid spliced peptides. Cis -spliced peptides can be formed via forward or reverse ligation. The number of all forward cis -spliced peptides of length N that could theoretically arise from a substrate of length L was:

L ext denoted the minimal splice-reactant length and was set to 1 per default. Analogously, the number of theoretically possible reverse cis -spliced peptides was calculated as:

To calculate the number of theoretical homologous trans -spliced peptides in an in vitro scenario where a single protein was digested with purified proteasomes, the following formula was used:

Computation of substrate-specific cleavage and splicing strengths

SCS-P 1 and PSP-P 1 were calculated using the MS1 intensities (quantities) of peptides after four hours of digestion. For α-Synuclein, the MS1 intensity after three hours of digestion was taken. In the case of a multi-mapper peptide, i.e. a peptide that maps to several substrate locations (see above), the intensity value of this peptides was adjusted by the number of possible peptide origins.

For each amino acid residue of a given substrate sequence, the summed quantity of all non-spliced peptides containing this residue at their P 1 position was denoted as site-specific cleavage strength (SCS-P 1). Similarly, the summed intensity of all spliced peptides carrying this residue at the C-terminus of their first splice-reactant (P1) was denoted as site-specific splicing strength (PSP-P 1) (Supplementary Fig. 15). Cleavage and splicing strength values for a given residue were then normalized by their respective sums in a local, 29 amino acid long, window surrounding the current residue, resulting in percentage distributions of SCS-P 1 and PSP-P 1 over the substrate sequence.

---

### A genetically informed prediction model for suicidal and aggressive behaviour in teens [^1272b3bd]. Translational Psychiatry (2022). Medium credibility.

Missing predictor data was imputed using K-nearest neighbours. A value of 6 was chosen for k, i.e. the number of data points within a cluster, was chosen by finding the square root of the number of columns in our dataset. To avoid bias during this step, the test set and the external validation data were imputed separately. The outcome variable was not included as an informative imputation variable. We then completed a final check for outliers in the imputed datasets using the steps described above. This led to a total sample size of 8,676 participants (N training = 4773; N tuning = 603; N test = 598; N NTR = 2702) (Fig. 1).

Statistical analysis

Main analysis

A PCA in the final data sets was completed to determine clustering of the four outcomes (suicidal behaviours, aggressive behaviours, both, or neither). R package H2O was used for all supervised machine learning analyses. The performance of the models was determined by macro area under the receiver operating characteristic curve (AUC), derived from averaging the AUC for each class. The AUC for each class was determined using a one-versus-all approach that collapses each class into a binary outcome, e.g. having only suicidal behaviours vs all other outcomes combined. Positive predictive values (PPV), negative predictive values (NPV), sensitivity, and specificity were determined based on Youden's J statistic. As the model merely gives probabilities of an individual belonging to the classes rather than giving a specific class prediction, a threshold must be selected at which a participant is classified into a specific class. Youden's J is a way to determine this threshold by finding the maximum of the "sensitivity plus specificity minus 1", associated with each of the possible thresholds; the values for J range from 0 to 1.

---

### Article 5. An introduction to estimation – 2: from z to T [^fb82b806]. Emergency Medicine Journal (2001). Low credibility.

Provided the sample size is large enough (that is, n greater than 100), the z statistic can be used to determine the confidence interval estimation of the population mean even when the sigma is not known. In these cases the estimation of the standard error of the mean is used. The z statistic is also valid when determining the population's proportion based upon a large sample. However, when dealing with smaller samples, the z statistic is replaced by the t statistic. This makes it possible to estimate, in a population with an unknown standard deviation: The probability of getting a sample mean greater than or equal to a particular value The value of a sample mean with a particular probability of occurring The probability of getting a sample mean between two particular values The confidence interval for the estimation of the population mean can also be determined using the t statistic.

---

### Data-driven recombination detection in viral genomes [^4c88d7e9]. Nature Communications (2024). High credibility.

Specificity

We analyzed the false positive rate by evaluating a collection of 3500 randomly selected sequences assigned to the non-recombinant SARS-CoV-2 lineage BA.2. The set was partitioned into seven groups of 500 sequences, and added -respectively to each group- 1, 3, 5, 10, 15, 20, or 30 mutations out of 24,277 that are non-characteristic and with frequency ≥ 1/10 5 in BA.2, at random genomic positions. RecombinHunt classified as non-recombinant -and assigned the correct lineage to- the great majority of the sequences. False positives' ratios ranged from 0.6% to 1.2% to 8.8% for 1, 10 or 30 added mutations; see Table 1 b.

Minimum requirements

Ideally, RecombinHunt can be applied to any virus for which an adequately large collection of viral genome sequences and a structured classification system (clades or lineages) are available; in these settings, we denote as characteristic mutations of each class those showing a frequency above a given threshold. Here, we estimate the minimum number of sequences necessary to derive a stable set of characteristic mutations. To be characteristic of a lineage L, in our model, a mutation M must have a frequency f ≥ threshold above a user-defined value. The minimum number n of genomic sequences required to determine whether the observed frequency of M is above f with a certain level of confidence can be approximated by a Fisher test of a stochastic variable X ~ B i n (n, f) describing the number of independent observations of M in a set of n genome sequences of L; our null hypothesis is then H 0 = X ≥ n *threshold = X ≥ K. Thus, M is characteristic of L if observed at least K times. Based on these assumptions, depending on the threshold and the frequency of M in L, we can compute the minimum value of n required to identify a characteristic mutation with a level of confidence of choice (p-value or test acceptance threshold).

---

### Comprehensive characterization of 536 patient-derived xenograft models prioritizes candidatesfor targeted treatment [^7e9ccb24]. Nature Communications (2021). High credibility.

Define a study arm match score for detecting positive signals

To characterize the ability of PDX models to a satisfy study arm target criteria throughout their passages, we define a study arm match score, S arm, as the fraction of unique passages across the cohort that displays a positive signal for the target. Here, the unique passages are determined by binning the cohort's PDX samples into passages according to the passage number and counting the number of bins, N pb. This "collapse" of passages is designed to avoid a type of overrepresenation that would result from model expansion. A score of S arm = 1 indicates that for each passage bin N pb, there is a PDX sample (possibly more than 1) with the represented passage number that is a positive match for the study arm target. For S arm < 1, there will be a passage number across all of the cohort's PDX model lines for which no samples match the study arm target. S arm and N pb together provide a measure of the depth of targetable passages in a PDX model cohort. The significance of this combination is to indicate which cohorts may be more amenable to drug studies across multiple passage numbers (S arm ~ 1, especially with large N pb) versus those that may not (S arm ≪ 1 or small N pb). We calculated the percentage of unique passages that display positive signals for the target arms and show 19 arms with S arm = 1 (Supplementary Fig. 6b) that may be conducive to drug trials.

Reporting summary

Further information on research design is available in the Nature Research Reporting Summary linked to this article.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^bb9b7f12]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Δ Φ, the probability distribution at a point Φ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Φ ∈ {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters α and β, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since α and β are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Φ ∈ [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(α, β) and x 2 = 1 − x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Φ, it may seem natural to select α and β by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### Validation of monte carlo estimates of three-class ideal observer operating points for normal data [^26b6e5d9]. Academic Radiology (2013). Low credibility.

Rationale and Objectives

Traditional two-class receiver operating characteristic (ROC) analysis is inadequate for the complete evaluation of observer performance in tasks with more than two classes.

Materials and Methods

Here, a Monte Carlo estimation method for operating point coordinates on a three-class ROC surface is developed and compared with analytically calculated coordinates in two special cases: (1) univariate and (2) restricted bivariate trinormal underlying data.

Results

In both cases, the statistical estimates were found to be good in the sense that the analytical values lay within the 95% confidence interval of the estimated values about 95% of the time.

Conclusions

The statistical estimation method should be key in the development of a pragmatic performance metric for evaluation of observers in classification tasks with three or more classes.

---

### Improved machine learning algorithm for predicting ground state properties [^cea9d142]. Nature Communications (2024). High credibility.

Proposition 1

(A variant of Proposition 1 in). Consider a randomized polynomial-time classical algorithmthat does not learn from data. Suppose for any smooth family of gapped 2D Hamiltoniansand any single-qubit observablecan compute ground state propertiesup to a constant error averaged over x ∈ [−1, 1] m uniformly. Then, NP-complete problems can be solved in randomized polynomial time.

This proposition states that even under the restricted settings of considering only 2D Hamiltonians and single-qubit observables, predicting ground state properties is a hard problem for non-ML algorithms. When one consider higher-dimensional Hamiltonians and multi-qubit observables, the problem only becomes harder because one can embed low-dimensional Hamiltonians in higher-dimensional spaces.

Numerical experiments

We present numerical experiments to assess the performance of the classical ML algorithm in practice. The results illustrate the improvement of the algorithm presented in this work compared to those considered in, the mild dependence of the sample complexity on the system size n, and the inherent geometry exploited by the ML models. We consider the classical ML models previously described, utilizing a random Fourier feature map. While the indicator function feature map was a useful tool to obtain our rigorous guarantees, random Fourier features are more robust and commonly used in practice. Moreover, we still expect our rigorous guarantees to hold with this change because Fourier features can approximate any function, which is the central property of the indicator functions used in our proofs. Furthermore, we determine the optimal hyperparameters using cross-validation to minimize the root-mean-square error (RMSE) and then evaluate the performance of the chosen ML model using a test set. The models and hyperparameters are further detailed in Supplementary Section 4.

---

### Genome-wide detection of cytosine methylations in plant from nanopore data using deep learning [^01e5b211]. Nature Communications (2021). High credibility.

Fig. 1
DeepSignal-plant for 5mC detection using Nanopore sequencing.

a Architecture of DeepSignal-plant. BiLSTM: a sequence processing network that uses long short-term memory layer to take the input from forward and backward direction to learn order dependence; Full Connection: a fully connected layer that connects all the inputs from the former layer to every activation unit of the next layer; Softmax: an activation function which normalizes a vector of real numbers into a vector of probabilities that sum to 1. b Schema of denoise training samples in DeepSignal-plant. c Signal comparison of different kinds of samples of a k -mer after denoising training. positive_kept: positive samples kept by the denoising step; positive_removed: positive samples removed by the denoising step; negative: negative samples; n = number of signal values for each base; Boxplots indicate 50th percentile (middle line), 25th and 75th percentile (box), the smallest value within 1.5 times interquatile range below 25th percentile and largest value within 1.5 times interquatile range above 75th percentile (whiskers), and outliers (dots). d Effectiveness of training samples selection on 5mC detection. The training samples were extracted from ~500× Nanopore reads of A. thaliana. Pearson correlations were calculated using the results from ~20× Nanopore reads and three bisulfite replicates of A. thaliana.

---

### An additive selection of markers to improve diagnostic accuracy based on a discriminatory measure [^96f44714]. Academic Radiology (2013). Low credibility.

Rationale and Objectives

The estimation of the area under the receiver operating characteristic (ROC) curve (AUC) often relies on the assumption that the truly positive population tends to have higher marker results than the truly negative population. The authors propose a discriminatory measure to relax such an assumption and apply the measure to identify the appropriate set of markers for combination.

Materials and Methods

The proposed measure is based on the maximum of the AUC and 1-AUC. The existing methods are applied to estimate the measure. The subset of markers is selected using a combination method that maximizes a function of the proposed discriminatory score with the number of markers as a penalty in the function.

Results

The properties of the estimators for the proposed measure were studied through large-scale simulation studies. The application was illustrated through a real example to identify the set of markers to combine.

Conclusion

Simulation results showed excellent small-sample performance of the estimators for the proposed measure. The application in the example yielded a reasonable subset of markers for combination.

---

### West nile virus in the United States: guidelines for surveillance, prevention, and control. 4th revision [^b3fb8cd7]. CDC (2013). Medium credibility.

Vector index (VI) for West Nile virus mosquito surveillance combines "vector species presence, vector species density, vector species infection rate" into "a single value", and "the VI is simply the estimated average number of infected mosquitoes collected per trap night summed for the key vector species in the area". For calculation, N̄ is defined as "Average Density; (number per trap night for a given species)" and P̂ as "EstimatedinfectionRate (proportion of the mosquito population WNV positive)". An example dataset for two primary vectors shows columns "Cx. tarsalis" and "Cx. pipiens", with "Total 442 233", "Average per Trap Night 74 39", and "Standard Deviation 41 21" as the density inputs for "Step 1: Calculate mosquito density".

---

### Foundations of the minimal clinically important difference for imaging [^72f785d9]. The Journal of Rheumatology (2001). Low credibility.

This article develops a generic conceptual framework for defining and validating the concept of minimal clinically important difference. We propose 3 approaches. The first uses statistical descriptions of the population ("distribution based"), the second relies on experts ("opinion based"), and a third is based on sequential hypothesis formation and testing ("predictive/data driven based"). The first 2 approaches serve as proxies for the third, which is an experimentally driven approach, asking such questions as "What carries the least penalty?" or "What imparts the greatest gain?" As an experimental approach, it has the expected drawbacks, including the need for greater resources, and the need to tolerate trial and error en route, compared to the other 2 models.

---

### The X factor: a robust and powerful approach to X-chromosome-inclusive whole-genome association studies [^df818ada]. Genetic Epidemiology (2021). Medium credibility.

The X-chromosome is often excluded from genome-wide association studies because of analytical challenges. Some of the problems, such as the random, skewed, or no X-inactivation model uncertainty, have been investigated. Other considerations have received little to no attention, such as the value in considering nonadditive and gene-sex interaction effects, and the inferential consequence of choosing different baseline alleles (i.e., the reference vs. the alternative allele). Here we propose a unified and flexible regression-based association test for X-chromosomal variants. We provide theoretical justifications for its robustness in the presence of various model uncertainties, as well as for its improved power when compared with the existing approaches under certain scenarios. For completeness, we also revisit the autosomes and show that the proposed framework leads to a more robust approach than the standard method. Finally, we provide supporting evidence by revisiting several published association studies. Supporting Information for this article are available online.

---

### Minimal number of gradient directions for robust measurement of spherical mean diffusion weighted signal [^ae2ac478]. Magnetic Resonance Imaging (2018). Low credibility.

Purpose

Determination of the minimum number of gradient directions (N min) for robust measurement of spherical mean diffusion weighted signal (S¯).

Methods

Computer simulations were employed to characterize the relative standard deviation (RSD) of the measured spherical mean signal as a function of the number of gradient directions (N). The effects of diffusion weighting b-value and signal-to-noise ratio (SNR) were investigated. Multi-shell high angular resolution Human Connectome Project diffusion data were analyzed to support the simulation results.

Results

RSD decreases with increasing N, and the minimum number of N needed for RSD ≤ 5% is referred to as N min. At high SNRs, N min increases with increasing b-value to achieve sufficient sampling. Simulations showed that N min is linearly dependent on the b-value. At low SNRs, N min increases with increasing b-value to reduce the noise. RSD can be estimated as σS¯N, where σ = 1/SNR is the noise level. The experimental results were in good agreement with the simulation results. The spherical mean signal can be measured accurately with a subset of gradient directions.

Conclusion

As N min is affected by b-value and SNR, we recommend using 10 × b / b 1 (b 1 = 1 ms/μm 2) uniformly distributed gradient directions for typical human diffusion studies with SNR ~ 20 for robust spherical mean signal measurement.

---

### How I read an article that uses machine learning methods [^0f2f5676]. Blood Advances (2023). Medium credibility.

Step 1: Understand the problem being addressed. The first step in reading an ML paper is to understand the problem that the authors are trying to solve and, more importantly, understand the clinical or scientific impact of solving this problem. In other words, if the aim of the study is to solve a clinical problem, how does the answer or the recommendation provided by the algorithm help physicians or researchers in their day-to-day practice, and is this solution mature enough to be implemented in clinical workflows? Major clinical problems in health care can mainly affect either patient outcomes or operations (can I make the process easier and faster for the patient and the health care system?).

Step 2: Assess the quality of the data. The quality of the data used to build the ML model is crucial for the validity of the results. Following are some questions that can be used to evaluate the data:
1 Sample size: Is the size of the training, validation, and test sets enough to build a reproducible and generalizable ML model? Is this size of the data appropriate for the chosen methods (ie, some methods are "data-hungry" and understanding which methods require larger datasets is key)? However, different algorithms require different data types (image, tabular, text, or others) and sizes, and there are no rules of thumb or formulas that can estimate the perfect data.
2 Relevance: Are the data appropriate and relevant to the problem that the model is trying to solve?
3 Accuracy: How are the data collected and annotated (human vs natural language process). How are the data transformed to make it ready for ML use, etc.
4 Consistency: Are the data consistent? Do they have any missing values and how the authors dealt with this?
5 Representativeness: The data should be representative of the population being studied.
6 Balance: The data should be balanced, with roughly equal representation of all relevant classes or groups. However, most health care data are unbalanced. It is critical to understand how the authors dealt with unbalanced data.
7 Bias: To evaluate bias in data, it is important to look at the distribution of certain characteristics, such as race, gender, or socioeconomic status, among the samples in the data set, and how the data were collected. This will help to identify any disparities or overrepresentation of certain groups, which can indicate the presence of bias in the data. It is critical to evaluate bias at this stage because if this is not addressed properly, it could produce a biased model.

---

### Should asymptomatic and low-risk individuals be tested for SARS-CoV-2? [^6e9e2983]. Journal of General Internal Medicine (2020). Medium credibility.

METHODS

The impact of testing was modeled based on the following parameters: 1% prevalence of active infection, test sensitivity 90% and specificity 95%, reproduction number 0.9, and 5 contacts per case. – Because performance of SARS-CoV-2 tests is not well established, the values that correspond to PCR-based tests for other respiratory viral infections were used. The reproduction number R is the median value in the USA. Because individuals with positive test results will be instructed to isolate, R following a positive test was assumed to decrease to 0.1. The number of people subsequently infected by each infected case disease was calculated using the formula R + R 2 + R 3 +… = 1/(1− R)−1 when R < 1. For R = 0.9, this sum of 9.0 represents the number of new infections in a transmission chain and was applied to individuals with false negative results and to infected individuals who were not tested. The corresponding sum for a true positive case is 1.0, which assumes a reproduction number of 0.1 for the first transmission and 0.9 for subsequent transmissions.

---

### DNA synthesis for true random number generation [^4f3c367b]. Nature Communications (2020). High credibility.

Nucleotide-binding prevalence

For every position × along the sequence data, the relative presence of two nucleotides following each other (e.g. AT) is normalized by the relative presence of the single nucleotides (e.g. A and T) at the corresponding position. (e.g. for AT at position x: f x norm (AT) = ((f x (AT)/ f x (A))/(f x +1 (T)). This gives the general formula with nucleotides N 1 and N 2: f x norm (N 1 N 2) = ((f x (N 1 N 2)/ f x (N 1))/(f x +1 (N 2)).

NIST evaluation parameters

NIST evaluation tests were chosen such that for each test, 56 bit streams containing 1096 bits were tested (with the exception of the rank test, where 56 bit streams containing 100,000 bits each were tested). This variant of statistical analyses was chosen and explained by Rojas et al. The tests were applied with the standard parameters as given by the NIST statistical test suite, with the following parameters differing from set values: (1) frequency test, no parameter adjustments; (2) block frequency test, no parameter adjustments (block length, M = 128); (3) cumulative sums test, no parameter adjustments; (4) runs test, no parameter adjustments; (5) longest runs of ones test, no parameter adjustments; (6) rank test, no parameter adjustments; (7) discrete Fourier transform test, no parameter adjustments; (8) approximate entropy test parameter adjustment: block length, m = 5; 9) serial test no parameter adjustments (block length, m = 16). For each statistical test, the NIST software computes a P -value, which gives the probability that the sequence tested is more random than the sequence a perfect RNG would have produced. Thereby, a P -value of 1 indicates perfect randomness, whereas a P -value of 0 indicated complete non-randomness. More specifically, for a P -value ≥ 0.001: sequence can be considered random with a confidence of 99.9%, and for a P -value < 0.001 sequence can be considered non-random with a confidence of 99.9%.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^d82e5daf]. The Journal of Molecular Diagnostics (2017). Medium credibility.

NGS oncology panel validation — sample size calculations and rule of three: Required validation samples can be calculated from r^n = α with n = ln(α)/ln(r); for 95% confidence (α = 0.05) and at least 95% reliability (r = 0.95) the minimum number of samples is 59. For a 95% confidence level, the rule of three applies: counting 100 cells with no positives allows a claim that < 1% of cells have the event, but to be 95% confident that < 1% of cells have it, 300 cells with no positives are needed; to claim reliability ≥ 95% with 95% confidence requires 3 × 20 or 60 samples.

---

### Assessing the dilution effect of specimen pooling on the sensitivity of SARS-CoV-2 PCR tests [^8d446189]. Journal of Medical Virology (2021). Medium credibility.

The SARS-CoV-2 pandemic has led to an unprecedented demand for diagnostic tests. Many studies have modeled the efficiency gains of specimen pooling, but few have systematically evaluated the dilution effect of pooling on the sensitivity of tests. Using the frequency distribution of cycle threshold (C t) values of our first 838 SARS-CoV-2 positive specimens, we modeled 100 specimens on the same frequency distribution. Given this distribution, we then tested dilutions of 1:5, 1:10, and 1:50 to find the percentage of specimens positive at each C t value with each pool size. Using the frequency distribution and the percentage of specimens positive at each C t value, we estimate that pools of 5 lead to 93% sensitivity, pools of 10 lead to 91% sensitivity, and pools of 50 lead to 81% sensitivity. Pools of 5 and 10 lead to some specimens with C t values of ≥ 32 becoming negative, while pools of 50 lead to some specimens with C t values of ≥ 28 becoming negative. These sensitivity estimates can inform laboratories seeking to implement pooling approaches as they seek to balance test efficiency with sensitivity.

---

### Dynamical regimes of diffusion models [^a7936fcc]. Nature Communications (2024). High credibility.

Speciation time

In order to show the existence of regimes I and II, and compute the speciation time, we focus on the following protocol which consists of cloning trajectories. We consider a backward trajectory starting at time t f ≫ 1 from a point x f drawn from a random Gaussian distribution where all components are independent with mean zero and unit variance. This trajectory evolves backward in time, through the backward process until time t < t f. At this time the trajectory has reached the point y (t), at which cloning takes place. One generates for τ < t two clones, starting from the same x 1 (t) = x 2 (t) = y (t), and evolving as independent trajectories x 1 (τ) and x 2 (τ), i.e. with independent thermal noises. We compute the probability ϕ (t) that the two trajectories ending in x 1 (0) and x 2 (0) are in the same class. Defining P (x 1, 0∣ y, t) as the probability that the backward process ends in x 1, given that it was in y at time t, the joint probability of finding the trajectory in y at time t and the two clones in x 1 and x 2 at time 0 is obtained as Q (x 1, x 2, y, t) = P (x 1, 0∣ y, t) P (x 2, 0∣ y, t) P (y, t). Then ϕ (t) is the integral of Q over x 1, x 2, y with the constraint (x 1 ⋅ m)(x 2 ⋅ m) > 0. This simplifies into a one-dimensional integral (see SI Appendix):where G (y, u, v) is a Gaussian probability density function for the real variable y, with mean u and variance v, and, Γ t = Δ t + σ 2 e −2 t. The probability ϕ (t) that the two clones end up in the same cluster is a decreasing function of t, going from ϕ (0) = 1 to ϕ (∞) = 1/2. In the large d limit, the scaling variable controlling the change of ϕ iswhich can be rewritten asby using. This explicitly shows that speciation takes place at the time scale t S on a window of time of order one. As detailed in SI Appendix, this expression for t S coincides with the one obtained from the general criterion (5). We show in Fig. 2 the analytical result from (6) and direct numerical results obtained for increasingly larger dimensions. This comparison shows that our analysis is accurate already for moderately large dimensions. In the limit of infinite d, the analytical curve in Fig. 2 suddenly jumps from one to zero at t / t S = 1, corresponding to a symmetry-breaking phase transition (or a threshold phenomenon) on the time scale t S. In the numerics, following finite size scaling theory, we have defined the speciation time as the crossing point of the curves for different d, which corresponds approximatively to ϕ (t S) = 0.775 and indeed is of the orderfor d → ∞. As it happens in mean-field theories of phase transitions, the large dimensional limit allows to obtain a useful limiting process. In our case, this leads to a full characterization of the asymptotic backward dynamics. At its beginning, i.e. in regime I, the overlap with the centers of the Gaussian model, ± m ⋅ x (t), is of order. Defining, one can obtain a closed stochastic Langevin equation on q in a potential V (q, t) (see SI Appendix), where η (t) is square root of two times a Brownian motion, andAt large d, this potential is quadratic at times, and it develops a double well structure, with a very large barrier, when. The trajectories of q (t) are subjected to a force that drives them toward plus and minus infinity. The barrier between positive and negative values of q becomes so large that trajectories commit to a definite sign of q: this is how the symmetry breaking takes place dynamically at the time scale t S, in agreement with the previous cloning results. Regime II corresponds to the scaling limit q → ∞, where m ⋅ x (t) becomes of order d. In this regime, the rescaled overlap m ⋅ x (t)/ d concentrates, and its sign depends on the set of trajectories one is focusing on. Moreover, the stochastic dynamics of the x i correspond to the backward dynamics for a single Gaussian centered in ± m. This shows that the dynamics generalizes, see SI Appendix (and alsofor similar results).

---

### Estimating the effectiveness of routine asymptomatic PCR testing at different frequencies for the detection of SARS-CoV-2 infections [^5fe80f00]. BMC Medicine (2021). Medium credibility.

Fig. 1
Testing and symptom data for the 27 individuals used in the analysis. Each point represents a symptom report and PCR test result. The border of the point is green if the PCR test result was positive and purple if it was negative. The inside of the point is red if the individual reported symptoms and white if they did not. Black crosses show the date of the initial negative serological test. Points are aligned along the x -axis by the timing of each participant's last asymptomatic report

We developed a Bayesian model to jointly infer both the likely infection time for each individual and the probability of a positive PCR test depending on the time since infection across all individuals. We used a likelihood function specifically for inferring parameters from censored data to derive a posterior distribution for the time of infection. This accounts for the fact that the true onset time is censored, i.e. symptom onset for each individual could have occurred anywhere between their last asymptomatic report and their first symptomatic report. Specifically, individual i has their likely infection time, T i, inferred based on the interval between their last asymptomatic report, and their first symptomatic report. The log-likelihood for the infection time for person i is as follows:

where F is the cumulative density function of the lognormal distribution for the incubation period of COVID-19 as estimated in Lauer et al. For a detailed description of the procedure used to arrive at the onset times from the censored data and list of the sources of uncertainty in our model, see Additional file 1: Section D.

For a given inferred infection time for person i, the relationship between the time since infection and a positive PCR test on person i, administered at time t n, i is given by a piecewise logistic regression model with a single breakpoint:

where C is the time of the breakpoint, x is the amount of time between infection and testing minus the value of the breakpoint, I (x) is a step function that equals 0 if x < 0 or equals 1 if x > 0, and the β terms define the regression coefficients fit across all tests and people (see Table 1 for parameter details).

---

### Evolution of dosage compensation under sexual selection differs between X and Z chromosomes [^a1882357]. Nature Communications (2015). Medium credibility.

The probability of fixation determines the expected change in expression over one particular substitution in each sex. If the mutant is lost, expression does not change, if the mutant is fixed, male and female expression become z m + δ m and z f +2 δ f, respectively (Table 2). To calculate the expected change in expression, we consider all possible mutational sizes and their frequencies,

where 3 N is the total number of genes in the population, μ is the mutation rate and f (δ m, δ f) is the probability density function for the effects of mutants on male and female expression. We assume that f (δ m, δ f) is a bivariate normal distribution with mean (0, 0) and covariance matrix

In this case, equations (6) and (7) read

and the second moments of the change in male and female expressions are approximately E[(Δ z m) 2 | z m, z f] = E[(Δ z f) 2 | z m, z f] = σ 2, and E[(Δ z m)(Δ z f)| z m, z f] = ρ σ 2. For simplicity, we set σ 2 = 0.1. Setting different values for σ 2 changes the impact of mutations, but since it does so with the same intensity in males and females, it does not change our results. However, increasing σ 2 increases the overall rate of evolution.

---

### A novel oppositional binary crow search algorithm with optimal machine learning based postpartum hemorrhage prediction model [^f6de23e1]. BMC Pregnancy and Childbirth (2022). Medium credibility.

Whereasrepresent the opposite number and x ∈ R denotes a real number determined on range of x ∈[a, b]. While a = 0 and b = 1 Eq. (3) becomes

While there is a point P (x 1, x 2,… x n) in n dimension coordinate and x 1, x 2, …, x n ∈ R later, the opposite pointis determined as its coordinates:

In such cases, have 2 values, x represent initial arbitrary value in [a, b] anddenotes the opposite values of x. They calculate f (x)&in all the iterations of OBCSA, later, employ on the evaluation function g ifselect x or else selectConsequently, the f l would be in range: f l ∈[f l min, f l max]. The opposite numbercan be determined by:

Later, evaluate the fitness for the first f l value and the fitness forin all the iterations. When, they select f l, or elsewould be selected. The stages of presented method can be given in the following.

Step1: The count of crows is n c = 25, f l min = 0.1, f l max = 1.8, A P = 0.3, and the maximal number of iterations is t max = 100.

Step2: The position that represent the features are made by U (0, 1).

Step3: The fitness function (FF) can be determined by

Whereas C represent the classification performance, W represent the weighted factors in the range of zero and one, F all represent the overall amount of features and F sub signifies the length of elected feature.

Step4: The position of the crows are upgraded as Eq. (2)

Step5: Steps 3 & 4 are repetitive till a t max is attained.

---

### Solving the diagnostic challenge: a patient-centered approach [^df1cd6b6]. Annals of Family Medicine (2018). Low credibility.

Arriving at an agreed-on and valid explanation for a clinical problem is important to patients as well as to clinicians. Current theories of how clinicians arrive at diagnoses, such as the threshold approach and the hypothetico-deductive model, do not accurately describe the diagnostic process in general practice. The problem space in general practice is so large and the prior probability of each disease being present is so small that it is not realistic to limit the diagnostic process to testing specific diagnoses on the clinician's list of possibilities. Here, new evidence is discussed about how patients and clinicians collaborate in specific ways, in particular, via a process that can be termed inductive foraging, which may lead to information that triggers a diagnostic routine. Navigating the diagnostic challenge and using patient-centered consulting are not separate tasks but rather synergistic.

---

### Identifying domains of applicability of machine learning models for materials science [^72c4ba76]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ν that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by σ lin (x 1, x 2) ≡ −0.3 ≤ x 1 ≤ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: σ rbf ≡ −3.3 ≤ x 11 ≤ 3.1 ∧ x 2 ≤ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA σ poly (x 1, x 2) ≡ −3.5 ≤ x 2 ≤ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### Evaluating molecular biomarkers for the early detection of lung cancer: when is a biomarker ready for clinical use? An official American Thoracic Society policy statement [^1c6ced37]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Accuracy guidance for biomarker clinical validation — formulas specify minimal accuracy thresholds tied to action after positive versus negative results. The committee "agreed that it would be helpful to provide guidance about the minimal accuracy, as assessed in the clinical validation phase, that could lead to a positive clinical impact". For tests where "a positive biomarker result leads to an action where a negative biomarker result is associated with standard of care for the population", the threshold is "sensitivity/(1 − specificity) ≥ [(1 − prevalence)/prevalence] × harm/benefit", where harm/benefit is "the ratio of the net harm of a falsely positive test result to the net benefit of a true-positive test result". For tests where "a negative test leads to an action other than standard of care for the population", the threshold is "specificity/(1 − sensitivity) ≥ [prevalence/(1 − prevalence)] × harm/benefit", where harm/benefit is "the ratio of the net harm of a falsely negative test result to the net benefit of a true-negative test result". "Sensitivity/(1 − specificity) is known as the positive likelihood ratio, and specificity/(1 − sensitivity) is 1 divided by the negative likelihood ratio", and "Prevalence refers to the percentage of cases in the intended use population". The harm/benefit term "can be articulated in one of two ways: 1/N, where in the first scenario N is the maximum number of control subjects testing positive that is tolerated to benefit one case subject testing positive".

---

### The contribution of X-linked coding variation to severe developmental disorders [^9603b3c0]. Nature Communications (2021). High credibility.

Over 130 X-linked genes have been robustly associated with developmental disorders, and X-linked causes have been hypothesised to underlie the higher developmental disorder rates in males. Here, we evaluate the burden of X-linked coding variation in 11,044 developmental disorder patients, and find a similar rate of X-linked causes in males and females (6.0% and 6.9%, respectively), indicating that such variants do not account for the 1.4-fold male bias. We develop an improved strategy to detect X-linked developmental disorders and identify 23 significant genes, all of which were previously known, consistent with our inference that the vast majority of the X-linked burden is in known developmental disorder-associated genes. Importantly, we estimate that, in male probands, only 13% of inherited rare missense variants in known developmental disorder-associated genes are likely to be pathogenic. Our results demonstrate that statistical analysis of large datasets can refine our understanding of modes of inheritance for individual X-linked disorders.

---

### Direct measurement of large-scale quantum States via expectation values of non-hermitian matrices [^f8699c5e]. Nature Communications (2016). Medium credibility.

In quantum mechanics, predictions are made by way of calculating expectation values of observables, which take the form of Hermitian operators. Non-Hermitian operators, however, are not necessarily devoid of physical significance, and they can play a crucial role in the characterization of quantum states. Here we show that the expectation values of a particular set of non-Hermitian matrices, which we call column operators, directly yield the complex coefficients of a quantum state vector. We provide a definition of the state vector in terms of measurable quantities by decomposing these column operators into observables. The technique we propose renders very-large-scale quantum states significantly more accessible in the laboratory, as we demonstrate by experimentally characterizing a 100,000-dimensional entangled state. This represents an improvement of two orders of magnitude with respect to previous phase-and-amplitude characterizations of discrete entangled states.

---

### Deep learning-based ranking method for subgroup and predictive biomarker identification in patients [^768bc590]. Communications Medicine (2025). Medium credibility.

Null scenarios

To assess the model performance in null scenarios where no predictive biomarkers exist (β = 0), we have evaluated the AUC for subgroup identification across three simulation settings (I-III) for both continuous and binary outcomes with N = 1000. Specifically, we have created a ground truth by labeling a sample as positive (labe l i = 1) if Y i (1) > Y i (0) and negative labe l i = 0 otherwise. Both Y i (1) and Y i (0) include the same prognostic term plus independent, is purely random. These results are detailed in Supplementary Fig. 21–23. Consistent with expectations, the subgroups identified by all methods resembled random selections, with each method achieving an average AUC of approximately 50%. This result confirms the hypothesis that, in the absence of true predictive effects, subgroup classification is equivalent to random chance.

Detection rates for identifying biomarkers are presented in Supplementary Fig. 24–26. Whenall models exhibit similar detection rates for biomarkers, approximately 20%. For continuous outcomes, despite varying prognostic effects, the detection rates for all models remain near the theoretical threshold of 20%, reflecting the inclusion of 10 covariates and indicating that all models effectively guard against overfitting in scenarios without true predictive effects.

However, in the case of binary outcomes, methods such as CF and XGboostML show a higher detection rate for prognostic biomarkersandwhen the prognostic effect is strong. This result is unexpected, as these models are designed to detect predictive, rather than prognostic, biomarkers. Taken together, the results indicate that the selection rate for all biomarkers, including non-informative ones, remains approximately equal, suggesting that DeepRAB does not favor any particular biomarker in the absence of an underlying signal. This pattern reflects random feature selection under the null scenarios. The results forshow a similar pattern.

---

### Leveraging polygenic functional enrichment to improve GWAS power [^515f8f41]. American Journal of Human Genetics (2019). Medium credibility.

Functional genomics data has the potential to increase GWAS power by identifying SNPs that have a higher prior probability of association. Here, we introduce a method that leverages polygenic functional enrichment to incorporate coding, conserved, regulatory, and LD-related genomic annotations into association analyses. We show via simulations with real genotypes that the method, functionally informed novel discovery of risk loci (FINDOR), correctly controls the false-positive rate at null loci and attains a 9%-38% increase in the number of independent associations detected at causal loci, depending on trait polygenicity and sample size. We applied FINDOR to 27 independent complex traits and diseases from the interim UK Biobank release (average N = 130K). Averaged across traits, we attained a 13% increase in genome-wide significant loci detected (including a 20% increase for disease traits) compared to unweighted raw p values that do not use functional data. We replicated the additional loci in independent UK Biobank and non-UK Biobank data, yielding a highly statistically significant replication slope (0.66–0.69) in each case. Finally, we applied FINDOR to the full UK Biobank release (average N = 416K), attaining smaller relative improvements (consistent with simulations) but larger absolute improvements, detecting an additional 583 GWAS loci. In conclusion, leveraging functional enrichment using our method robustly increases GWAS power.

---

### Determination of X-ray detection limit and applications in perovskite X-ray detectors [^c02fe79a]. Nature Communications (2021). High credibility.

Assuming Group 1 and Group 2 represent the gross signal (photocurrent under X-ray I X–ray in the case of X-ray detector) and the blank signal (dark current I dark in the case of X-ray detector) throughout in this work (equivalently vice versa), respectively, then a prior detection limit Δ = | μ 1 − μ 2 | can be calculated with the measured blank signal of Group 2, i.e. (μ 2,) and n 2 from the DL equations, with some necessary pre-assumptions (we use the term " prior " because this method requires pre-assumptions from a statistical perspective). We make assumptions consistent with the Currie method and show that the DL equations is same as Currie formulars when sample sizes (i.e. number of sampled data points) are being reduced (Fig. 2c). Instead of setting α = 0.13% and β = 50% as in the IUPAC definition, we set more properly α = β = 0.05, corresponding to z 1− α = z 1− β = 1.645, which is consistent with Currie formulas. The standard deviation of the gross signal and the blank signal are assumed approximately equal as is in the Currie formulars, i.e. = , when the gross signal is small approaching the level of detection limit. The influence of sample size (i.e. number of sampled data points) on the detection limit is reflected by the value of k = n 2 / n 1. If we set n 2 = n 1 = 1, we have k = 1. Then the DL equations reduce to. With the assumptions of = and z 1− α = z 1− β = 1.645, we have, which is the detection limit L D in Paired observations case in Currie's classical paper(Table 1). If we set n 1 = 1 and n 2 > > n 1, that is, k = n 2 / n 1 > > 1, so that > > or > >, then the DL equations reduce to. With the same assumptions, we have Δ = | μ 1 − μ 2 | = 3.29 σ 2, which is the detection limit L D in "Well-known" blank case in Currie's paper(Table 1). The DL equations are based on Normal distribution, which makes it applicable to X-ray detectors working in a continuous current mode.

---

### Improved machine learning algorithm for predicting ground state properties [^0c6418ab]. Nature Communications (2024). High credibility.

Theorem 1

(Sample and computational complexity). Givenand a training data setof sizewhere x ℓ is sampled from an unknown distributionandfor any observable O with eigenvalues between −1 and 1 that can be written as a sum of geometrically local observables. With a proper choice of the efficiently computable hyperparameters δ 1, δ 2, and B, the learned function h * (x) = w * ⋅ ϕ (x) satisfieswith probability at least 1 − δ. The training and prediction time of the classical ML model are bounded by.

The output y ℓ in the training data can be obtained by measuringfor the same observable O multiple times and averaging the outcomes. Alternatively, we can use the classical shadow formalism –, that performs randomized Pauli measurements on ρ (x ℓ) to predictfor a wide range of observables O. We can also combine Theorem 1 and the classical shadow formalism to use our ML algorithm to predict ground state representations, as seen in the following corollary. This allows one to predict ground state propertiesfor a large number of observables O rather than just a single one. We present the proof of Corollary 1 in Supplementary Section 3B.

Corollary 1

Givenand a training data setof sizewhere x ℓ is sampled from an unknown distributionand σ T (ρ (x ℓ)) is the classical shadow representation of the ground state ρ (x ℓ) using T randomized Pauli measurements. For, then the proposed ML algorithm can learn a ground state representationthat achievesfor any observable O with eigenvalues between −1 and 1 that can be written as a sum of geometrically local observables with probability at least 1 − δ.

We can also show that the problem of estimating ground state properties for the class of parameterized Hamiltoniansconsidered in this work is hard for non-ML algorithms that cannot learn from data, assuming the widely believed conjecture that NP-complete problems cannot be solved in randomized polynomial time. This is a manifestation of the computational power of data studied in. The proof of Proposition 1 inconstructs a parameterized Hamiltonian H (x) that belongs to the family of parameterized Hamiltonians considered in this work and hence establishes the following.

---

### Quantifying the dynamics of failure across science, startups and security [^6acaa77f]. Nature (2019). Excellent credibility.

Human achievements are often preceded by repeated attempts that fail, but little is known about the mechanisms that govern the dynamics of failure. Here, building on previous research relating to innovation 1–7, human dynamics 8–11 and learning 12–17, we develop a simple one-parameter model that mimics how successful future attempts build on past efforts. Solving this model analytically suggests that a phase transition separates the dynamics of failure into regions of progression or stagnation and predicts that, near the critical threshold, agents who share similar characteristics and learning strategies may experience fundamentally different outcomes following failures. Above the critical point, agents exploit incremental refinements to systematically advance towards success, whereas below it, they explore disjoint opportunities without a pattern of improvement. The model makes several empirically testable predictions, demonstrating that those who eventually succeed and those who do not may initially appear similar, but can be characterized by fundamentally distinct failure dynamics in terms of the efficiency and quality associated with each subsequent attempt. We collected large-scale data from three disparate domains and traced repeated attempts by investigators to obtain National Institutes of Health (NIH) grants to fund their research, innovators to successfully exit their startup ventures, and terrorist organizations to claim casualties in violent attacks. We find broadly consistent empirical support across all three domains, which systematically verifies each prediction of our model. Together, our findings unveil detectable yet previously unknown early signals that enable us to identify failure dynamics that will lead to ultimate success or failure. Given the ubiquitous nature of failure and the paucity of quantitative approaches to understand it, these results represent an initial step towards the deeper understanding of the complex dynamics underlying failure.

---

### How I read an article that uses machine learning methods [^d784cc52]. Blood Advances (2023). Medium credibility.

Step 3: Familiarize yourself with the ML methods used. The next step is to understand the ML methods that the authors have used to solve the problem. Many papers will provide a brief overview of the methods used (in clinical or applied journals), but it is important to have a good understanding of the underlying concepts. It is critical to familiarize yourself with some of these terminologies presented in Table 1. There are many papers that explain these terminologies in a very simple manner. It is also important to understand the key issues in building ML (Figure 1) models and what the authors did to address these at each step.

Table 1.
ML terminologies

Figure 1.
Steps to build a machine learning model. Problem formulation: The first step is to clearly define the problem that you want to solve. This involves defining the inputs and outputs of your model, as well as the type of problem you are trying to solve (classification, regression, clustering, etc). It is important to have a clear understanding of the problem you are trying to solve before you start building a model. Data collection: Once you have formulated the problem, the next step is to collect the relevant data. This may involve scraping data from websites, downloading data sets from public repositories, or collecting data through surveys or experiments. It is important to collect enough data to train your model and validate its performance. Data preparation: After collecting the data, you will need to clean and preprocess it. This involves removing any irrelevant data, dealing with missing values, and transforming the data into a suitable format for ML algorithms. It also includes dividing the data set into training, validation, and test cohorts. This step can take a lot of time and effort, but it is essential for building an accurate and effective model. Feature engineering: Feature engineering is the process of selecting and transforming the input variables (features) in a way that will improve the performance of the model. This may involve selecting the most relevant features, transforming them into a different representation (eg, using one-hot encoding), or creating new features based on existing ones. Feature engineering can have a significant impact on the performance of the model. Model selection: Once you have prepared the data and engineered the features, the next step is to select a suitable ML algorithm. This involves choosing the type of algorithm (eg, decision trees, neural networks, support vector machines) and the specific parameters of the algorithm. This step requires some knowledge of ML and experience with different algorithms. Model training: After selecting the algorithm, the next step is to train the model on the prepared data. This involves feeding the input data into the algorithm and adjusting the model parameters to optimize its performance. This step can take a lot of time and computational resources, especially for large data sets and complex models. Model evaluation: Once the model has been trained, the next step is to evaluate its performance on a separate test set of data. This involves measuring metrics, such as accuracy, precision, recall, and F1 score, to assess the performance of the model. It is important to test the model on data that it has not seen before to ensure that it can be generalized to new data. Model optimization: If the model performance is not satisfactory, then the next step is to optimize the model. This involves tweaking the model parameters, changing the algorithm, or modifying the feature engineering process to improve the model's performance. This step may require several iterations until the desired level of performance is achieved. Model deployment: Once you have built a satisfactory model, the final step is to deploy it in a production environment. This may involve integrating the model into a web application, creating an application programming interface for other developers to use, or deploying it as a stand-alone application. It is important to ensure that the model is well documented and tested thoroughly before it is deployed.

---

### Sparse haplotype-based fine-scale local ancestry inference at scale reveals recent selection on immune responses [^655b111a]. Nature Communications (2025). High credibility.

Hidden Markov model in vector form

Let N be the number of haplotypes in the reference panel K be the number of SNPs, and μ be the mutation probability per SNP. λ is a recombination scaling constant, proportional to effective population size in simple demographies and called N e in Lawson et al. The reference panel X is an N by K matrix, and a target haplotype y is an K -vector, all taking values of either 0 or 1 corresponding to whether the reference allele is present or not. However, we can simplify this into a match matrix M of dimension N × K which also takes values of either 0 or 1, with M i j = 1 if X i j = y j and 0 otherwise. We will refer to the row vectors m j = M ⋅ j and use the shorthand D (x) = Diag(x) as the matrix with the vector x on the diagonal. We will refer to D N (x) as an N × N matrix with the scalar x on the diagonal.

SparsePainter implements the Li and Stephens modelin the form of ChromoPainterin a sparse setting. We define V as the emission matrix, and the column vectors are v j = V ⋅ j The observation matrix is an N × N matrix:The transition matrix from position j to position j + 1 is an N × N matrix:wherewith g j being the genetic distance between position j and position j + 1 in Morgans.

Let f 0 = 1/ N be the prior probabilities for the matches. We can write the forward probabilities for j = 1, …, K aswhere f j are row vectors (1 × N). With b K = 1 N where 1 N is an 1 × N row vector, the backward probabilities for j = 1, …, K − 1 areHowever, Equation (4) and (5) can be significantly simplified due to the special form of the output and transition matrices. We can arrive at a vector form for which computations are O (N) instead of O (N 2).

---

### Using qualitative comparative analysis and theory of change to unravel the effects of a mental health intervention on service utilisation in Nepal [^9ba2f03b]. BMJ Global Health (2018). Medium credibility.

During the process of applying QCA together with ToC, we learnt several lessons. First, using a conceptual framework, such as ToC, to guide QCA is important to identify the outcome and condition sets as well as providing a theoretical basis for how to treat logical remainders. Second, the number of conditions needs to be kept to around 5–7 to avoid the 'limited diversity problem'which occurs when there are too many conditions in relation to the number of cases which prevent Boolean minimisation of the truth tables and result in causal pathways with many conditions and low coverage. This is similar to the problem in frequentist statistics of including too many explanatory variables in a multivariate linear regression without an adequate sample size. To avoid this problem, we aggregated similar indicators prior to data analysis, for example, proportion of trained prescribers and non-prescribers and the number of trained home-based care workers and FCHVs. Other approaches to avoid limited diversity include (1) a comprehensive approach where all conditions are included iteratively in the analysis until the most parsimonious solution is reached or (2) a two-step approach where an analysis of ths distal conditions is conducted first, followed by the proximal conditions. Third, data calibration is one of the key steps in QCA and should be conducted with a theoretical understanding of the values chosen, that is, that 1 is fully in the set, 0 is fully out of the set and that 0.5 is neither in nor out of the set. The calibration of data should be conducted using thresholds that are meaningful to the programme. Although tools exist in STATA and other software to calibrate continuous variables, the distribution of the resulting calibrated data should be checked to ensure that the interpretation of the data is in line with the theoretical understanding. Because of the iterative nature of QCA, it is possible and may be necessary to recalibrate the data following the analysis.

---

### Integration of machine learning and genome-scale metabolic modeling identifies multi-omics biomarkers for radiation resistance [^efedb6c5]. Nature Communications (2021). High credibility.

Fig. 4
Multi-omics classifier integrating clinical, gene expression, mutation, and FBA-predicted metabolite features for prediction of radiation response.

a ROC curve for multi-omics classifier, with points representing 50% threshold value and optimal Youden's J statistic shown. Blue line: mean across n = 20 training+validation/testing splits. Blue error band: ± 1 standard deviation. b (Left, black) Mean absolute SHAP values (mean |ΔP|) for individual features. (Right, gray) Cumulative mean |ΔP| values. c Top 50 features with largest mean |ΔP| values, colored based on original dataset. (Inset, Left) Number of significant features from each dataset. (Inset, Right) Relative contribution of features from each dataset to sum of total absolute SHAP values, averaged across all samples. n = 904 samples. Error bars: mean ± 1 standard error. d Relative contribution of features from each dataset to sum of total absolute SHAP values, for each individual sample. n = 904 samples. e Clustering of samples into "Low/Medium/High" clinical groups based on relative contribution of clinical dataset. Optimal number of clusters calculated based on maximizing gap statistic from k -means clustering (Supplementary Fig. 13a). f Top 50 features with largest mean |ΔP| scores among samples within "Low Clinical" cluster. (Inset) Relative contribution of features from each dataset to sum of total mean |ΔP| scores, averaged across all samples within "Low Clinical" cluster. n = 249 samples. Error bars: mean ± 1 standard error. g Statistical significance of patient clustering into "Low/Medium/High" clinical groups based on clinical factors, calculated by χ 2 test with Yates' correction. Only factors with p ≤ 0.05 are shown. h Clinical cluster and dataset contribution of samples within different cancer types. Numbers of samples in each group are provided in Supplementary Data 1. i Clinical cluster and dataset contribution of BRCA samples with different histological subtypes. Number of samples in each group are provided in Supplementary Data 1. j Prediction of clinical cluster based on meta-learner weight for the clinical dataset. Dotted line: threshold maximizing accuracy in separating "Low Clinical" from "Medium/High Clinical" clusters. n = 904 samples. All boxplots: box = 25th, 50th, and 75th percentiles, whiskers = 1.5 times the interquartile range.

---

### Exploiting redundancy in large materials datasets for efficient machine learning with less data [^ba2f4c45]. Nature Communications (2023). High credibility.

Extensive efforts to gather materials data have largely overlooked potential data redundancy. In this study, we present evidence of a significant degree of redundancy across multiple large datasets for various material properties, by revealing that up to 95% of data can be safely removed from machine learning training with little impact on in-distribution prediction performance. The redundant data is related to over-represented material types and does not mitigate the severe performance degradation on out-of-distribution samples. In addition, we show that uncertainty-based active learning algorithms can construct much smaller but equally informative datasets. We discuss the effectiveness of informative data in improving prediction performance and robustness and provide insights into efficient data acquisition and machine learning training. This work challenges the "bigger is better" mentality and calls for attention to the information richness of materials data rather than a narrow emphasis on data volume.

---

### Colorectal cancer screening and prevention [^390b568a]. American Family Physician (2025). High credibility.

Regarding screening and diagnosis for colon cancer, more specifically with respect to indications for screening, high-risk individuals, family history, AAFP 2025 guidelines recommend to obtain CRC screening in patients with ≥ 1 first-degree relatives with CRC or adenomatous polyps, starting at 40 years of age or 10 years before the age of the youngest relative at the time of their diagnosis.

---

### Path sampling of recurrent neural networks by incorporating known physics [^17765f03]. Nature Communications (2022). High credibility.

Three-state Markovian dynamics

For the first illustrative example, we apply LSTM to a 3 state model system following Markovian dynamics for moving between the 3 states. This system, comprising states labeled 0, 1 and 2 is illustrated in Fig. 2 (a). Figure 2 (a) also shows the state-to-state transition rates for the unconstrained system. We then seek to constrain the average number of transitions per unit time between states 0,1 and 1,2 as defined belowwhere L traj is the length of trajectory and N 0↔1 and N 1↔2 are the number of times a transition occurs between states 0 and 1 or states 1 and 2 respectively. This example can then be directly compared with the analytical result Eq. (18) derived in the Appendix, thereby validating the findings from ps-LSTM.

Fig. 2
3 state Markovian system: LSTM, ps-LSTM and analytical predictions.

Here we show results of applying ps-LSTM to the 3 state Markovian system where we constrain 〈 N 〉. In (a), we provide the input transition kernel without constraints. In (b), we show the transition kernel obtained from ps-LSTM generated time-series via direct counting, where we achieve a 〈 N 〉 close to the target 〈 N 〉 = 0.13. The calculated values for 〈 N 〉 are shown in (c) for LSTM as the average of 100 predictions and for ps-LSTM as the average of 200 predictions. The error represents "error percentage" which is defined as the difference between ps-LSTM result and target value 〈 N 〉 = 0.13 divided by the target value.

---

### Data-driven recombination detection in viral genomes [^4d5f2070]. Nature Communications (2024). High credibility.

Sensitivity, specificity, minimum requirements

Extensive simulations (see simulated sequences in Supplementary Data 1) were performed to measure the sensitivity and specificity of RecombinHunt and the minimum requirements for its application.

Sensitivity

For testing the sensitivity, we considered two SARS-CoV-2 lineages (BA.2 and AY.45) and generated recombinant sequences with one or two breakpoints. Similarly to Turakhia et al. we simulated two sets of 3500 recombinant sequences (1BP and 2BP cases) each partitioned into seven groups of 500 sequences, and injected increasing levels of noise, by adding -respectively to each group- 0, 3, 5, 10, 15, 20, or 30 mutations out of 4,983 that are non-characteristic and with frequency ≥ 1/10 5 in both parent lineages, at random genomic positions. Note that the generated SARS-CoV-2 genome sequences carry about 60 mutations compared to the reference Wuhan1 genome; hence, adding 3 mutations corresponds to inserting 5% noise, whereas 30 mutations to inserting 50% noise. RecombinHunt achieved an almost perfect sensitivity when the number of added mutations was ≤ 10 (100%-99.4% for 1BP and 99%-95.6% for 2BP). By adding up to 30 mutations, performance slightly decreases as expected; see Table 1 a. The breakpoint position was identified correctly (i.e. within a single mutation range) in 99.4% of the simulated 1BP cases and in 97.6% of the simulated 2BP cases.

Table 1
a Sensitivity analysis

Number (and percentage, out of a total of 500) of sequences detected as 1BP/2BP recombinants or non-recombinants by RecombinHunt (RH). b Specificity analysis. Number (and percentage, out of a total of 500) of sequences detected as 1BP/2BP recombinants when they instead were non-recombinants. c Minimum number of sequences to correctly characterize a real lineage. Cell values represent the median valuesuch that the median number of characteristic mutations is correct for all the subsequent values of. The median values have been computed by sampling N sequences 100 times for increasing values of N. Each mutations-noise level indicates the maximum number of mutations that differ from the characterization of the lineage, present in the sequences of the sampled dataset.

---

### European Stroke Organisation (ESO) and European Association of Neurosurgical Societies (EANS) guideline on stroke due to spontaneous intracerebral haemorrhage [^a24b784e]. European Stroke Journal (2025). High credibility.

Regarding diagnostic investigations for intracerebral hemorrhage, more specifically with respect to initial evaluation, EANS/ESO 2025 guidelines recommend to consider using algorithms such as the DIAGRAM for targeted investigation of the cause of spontaneous ICH to improve the performance of prediction regarding the underlying cause, compared to standard care.

---

### Defining the time-limited trial for patients with critical illness: an official American Thoracic Society workshop report [^cf1ee11e]. Annals of the American Thoracic Society (2024). High credibility.

Time-limited trial in critical care — essential elements and phased process were derived through a Delphi process: investigators "identified 18 potential steps" and found first-round consensus that "11 of these were essential elements"; during the second round, they "identified seven additional essential elements". The committee then "combined 4 related items into two steps (to reach 16 essential elements)" and "organized steps into four phases of care: consider, plan, support, and reassess". Additional steps "may be helpful in some cases, but not necessary for all trials", and involvement of other disciplines is "highly dependent on a patient's specific situation and on the available hospital resources".

---

### Charting the landscape of priority problems in psychiatry, part 2: pathogenesis and aetiology [^a29b4ed3]. The Lancet: Psychiatry (2016). Medium credibility.

This is the second of two companion papers proposing priority problems for research on mental disorders. Whereas the first paper focuses on questions of nosology and diagnosis, this Personal View concerns pathogenesis and aetiology of psychiatric diseases. We hope that this (non-exhaustive and subjective) list of problems, nominated by scientists and clinicians from different fields and institutions, provides guidance and perspectives for choosing future directions in psychiatric science.

---

### Standards of care in diabetes – 2025 [^cb4a5970]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for diabetes mellitus type 1, more specifically with respect to general principles, ADA 2025 guidelines recommend to obtain a complete medical evaluation at the initial visit and follow-up, as appropriate, to:

- confirm the diagnosis and classify diabetes

- ssess glycemic status and previous treatment

- evaluate for diabetes complications, potential comorbid conditions, and overall health status

- identify care partners and support system

- assess social determinants of health and structural barriers to optimal health and health care

- review risk factor management in the patient with diabetes

- begin engagement with the patient with diabetes in the formulation of a care management plan including initial goals of care

- develop a plan for continuing care.

---

### Colorectal cancer screening and prevention [^2277b19f]. American Family Physician (2025). High credibility.

Regarding screening and diagnosis for colon cancer, more specifically with respect to indications for screening, general population, aged 50–75 years, AAFP 2025 guidelines recommend to obtain periodic screening for CRC in adults aged 50–75 years at average risk with no signs or symptoms of the condition.

---

### Clinical policy: critical issues in the initial evaluation and management of patients presenting to the emergency department in early pregnancy [^24639a9a]. Annals of Emergency Medicine (2012). Medium credibility.

Appendix C — Likelihood ratios (LR) and number needed to treat (NNT) — categorizes LR thresholds with paired LR(+) and LR(−) values and interpretations as follows: "1.0 1.0 Useless"; "1–5 0.5–1 Rarely of value, only minimally changes pretest probability"; "10 0.1 Worthwhile test, may be diagnostic if the result is concordant with pretest probability"; "20 0.05 Strong test, usually diagnostic"; and "100 0.01 Very accurate test, almost always diagnostic even in the setting of low or high pretest probability". NNT is defined as "Number needed to treat (NNT): number of patients who need to be treated to achieve 1 additional good outcome; NNT = 1/absolute risk reduction×100, where absolute risk reduction is the risk difference between 2 event rates (ie, experimental and control groups)".

---

### Cohesin depleted cells rebuild functional nuclear compartments after endomitosis [^2595e904]. Nature Communications (2020). High credibility.

Nuclear volume measurements

Volume measurements were done with the Volocity software (Version 6.1.2.). RGB image stacks were separated in their respective channels and then nuclei structures were obtained and segmented for volume measurements by using the following commands: (1) "Find Objects" (Threshold using: Automatic, Minimum object size: 200 µm³), (2) "Dilate" (number of iterations: 15), (3) "Fill Holes in Objects" and (4) "Erode" (number of iterations: 15). In ≈5% of cases these settings had to be adjusted for the challenging task of nuclei segmentation. To confirm statistical significance of volume differences the Mann-Whitney test was applied.

Segmentation and quantification of RD signals

Aligned 3D SIM image stacks were used as RGB for object counting and volume measurements in the Volocity software (Version 6.1.2.). For each series between n = 7 and n = 11 nuclei were measured resulting in 31,000–55,000 single values for each series. Image stacks were separated into their respective channels. The segmentation of RD structures was performed with the following software commands: (1) "Find Objects" (Threshold using: Intensity, Lower: 32, Upper: 255), (2) "Separate Touching Objects" (Object size guide of 0.002 µm³) and (3) "Exclude Objects by Size", excluding structures < 0.005 µm³. This cut-off level largely corresponds to the resolution limit of 3D-SIM (~120 nm lateral/300 nm axial). Exclusion of signals outside a selected nucleus was achieved by the commands "Intersect" and "Compartmentalize". Segmentation of nuclei was realized by the following commands: (1) "Find Objects" (Threshold using: Intensity), (2) "Dilate", (3) "Fill Holes in Objects" and (4) "Erode". Measured values of individual object counts and segmented RD volumes were displayed as boxplots indicating the median with 25–75% quartiles. Plots were generated using R Studio (version 1.0.143).

---

### Evaluation and referral for developmental dysplasia of the hip in infants [^931f6720]. Pediatrics (2016). Medium credibility.

World Health Organization screening criteria and their application to developmental dysplasia of the hip (DDH) are outlined. The page lists "TABLE 1 World Health Organization Criteria for Screening for Health Problems", including items 1–5 ("1. The condition should be an important health problem" through "5. There should be a suitable test or examination for the condition") and items 6–10 ("6. The test should be acceptable to the population" through "10. Case finding should be a continuous process"). The narrative states, "The AAP believes DDH fulfills most of these screening criteria (Table 1), except for an understanding of the natural history of hip dysplasia and an agreed-on policy of whom to treat", and reiterates, "A reasonable goal for screening is to prevent the late presentation of DDH after 6 months of age".

---

### Charting the landscape of priority problems in psychiatry, part 1: classification and diagnosis [^033871c3]. The Lancet: Psychiatry (2016). Medium credibility.

Contemporary psychiatry faces major challenges. Its syndrome-based disease classification is not based on mechanisms and does not guide treatment, which largely depends on trial and error. The development of therapies is hindered by ignorance of potential beneficiary patient subgroups. Neuroscientific and genetics research have yet to affect disease definitions or contribute to clinical decision making. In this challenging setting, what should psychiatric research focus on? In two companion papers, we present a list of problems nominated by clinicians and researchers from different disciplines as candidates for future scientific investigation of mental disorders. These problems are loosely grouped into challenges concerning nosology and diagnosis (this Personal View) and problems related to pathogenesis and aetiology (in the companion Personal View). Motivated by successful examples in other disciplines, particularly the list of Hilbert's problems in mathematics, this subjective and eclectic list of priority problems is intended for psychiatric researchers, helping to re-focus existing research and providing perspectives for future psychiatric science.

---

### Learning interpretable network dynamics via universal neural symbolic regression [^9f547ef7]. Nature Communications (2025). High credibility.

Performance measures

The performance measures for evaluating the methods in this work are as follows:

Adjusted R 2 score

Adjusted R 2 offsets the impact of the number of nodes N on R 2, so that the true value range is (0, 1), and the larger the better. The score can be calculated as:where. X i (t) andare the true and predictive states of node i at time t, respectively.is the average of X i (t) over all node and N is the number of nodes.

NED

We adopt the normalized Euclidean distance (NED) to finely evaluate the difference between the predictive states generated by the inferred equation and the true states for each node i, defined as follows:whererepresents the maximum Euclidean distance between pairs of true states over the time interval [0, T]. X i (t) andare the true and predictive states of node i at time t, respectively.andare their respective derivatives.

Recall

From the perspective of the symbolic form of equations, we use recall to evaluate the accuracy of the discovered equations, specifically whether a certain function term is present or not. Let ξ true and ξ pre denote the true and inferred coefficients under a candidate library of function terms. For example, a library is [1, x, x 2, x 3, x 4], If a true equation is y = 1 + x, then ξ true = [1, 1, 0, 0, 0]. And if an inferred equation is y = 1 + x 2, then ξ pre = [1, 0, 1, 0, 0]. Recall measures the percentage of coefficients successfully identified among the true coefficients and is defined as:where ⊙ represents the element-by-element product of two vectors, ∣ ⋅ ∣ 0 denotes the number of non-zero elements in the vector.

More performance measures, including MRE (Mean Relative Error), MAE (Mean Absolute Error), L2 error, and Precision, can be found in Supplementary Section IV.

---

### Do we need to improve the reporting of evidence in tendinopathy management? A critical appraisal of systematic reviews with recommendations on strength of evidence assessment [^9708e49f]. BMJ Open Sport & Exercise Medicine (2021). High credibility.

Pooling of results

Item 3: The principal summary measures for each included outcome measure need to be stated in the methods. For quantitative analyses, where outcome measures are continuous, a justification needs to be provided for the use of (raw) mean differences (MD) and not standardised MD (SMD) and vice versa (ie, identical or different outcome measure tools used across studies, respectively). Similarly, for dichotomous outcomes, the authors should state in the methods whether OR or relative risks (RR) were used.

Item 4: An SR without a meta-analysis should ideally have some quantitative summary measures (ie, MD, SMD, OR or RR with accompanying CI) to demonstrate the treatment effect of the assessed intervention over the comparator. Where results are pooled only based on direction of effect (ie, increased, decreased or unchanged), this needs to be stated in the methods with a justification. If pooling is not possible and results are only described narratively, the reason should be stated (usually substantial clinical heterogeneity); in such cases, we question whether the article should be identified as an SR.

Item 5: When meta-analyses are performed, the model used needs to be stated (ie, fixed effects or random effects) with a justification and the statistical heterogeneity assessment (usually χ 2 and/or I 2 test) and how it was used in data syntheses. Sensitivity and subgroup analyses should also be pre-defined in the methods.

Items 6 and 7: Results should only be pooled for similar follow-up time periods which should be prespecified in the methods. Traditionally these are short term, mid term and long term. The range of these should be defined by the authors in the methods based on the population, outcome measures and interventions. Equally, assessed outcome measures should be predefined (usually as part of PICOS).

Item 8: A statement needs to be included in the section explaining how the authors dealt with missing data, which are usually methodological details of the included studies needed for the RoB assessment and descriptive statistics for quantitative analyses. For both, ideally attempts should be made to contact the authors of the RCTs for retrieval of the missing data and this should be stated. If significant statistical data of included studies cannot be retrieved (ie, sample sizes and means) these studies should be excluded from quantitative analyses; if variability statistics (SD) are missing, the authors have the option to impute these using data from other RCTs and the imputation method used should be described.

---

### The initial management of chronic pelvic pain (green-top guideline no. 41) [^f7192d27]. RCOG (2012). Medium credibility.

Regarding diagnostic investigations for chronic pelvic pain in women, more specifically with respect to initial evaluation, RCOG 2012 guidelines recommend to recognize that there is frequently more than one component to chronic pelvic pain. Aim to identify contributory factors rather than assign causality to a single pathology. Recognize that it may not be possible to identify confidently the cause of the pain at the initial assessment.

---

### Population variability in X-chromosome inactivation across 10 mammalian species [^49b6ef93]. Nature Communications (2024). High credibility.

One of the two X-chromosomes in female mammals is epigenetically silenced in embryonic stem cells by X-chromosome inactivation. This creates a mosaic of cells expressing either the maternal or the paternal X allele. The X-chromosome inactivation ratio, the proportion of inactivated parental alleles, varies widely among individuals, representing the largest instance of epigenetic variability within mammalian populations. While various contributing factors to X-chromosome inactivation variability are recognized, namely stochastic and/or genetic effects, their relative contributions are poorly understood. This is due in part to limited cross-species analysis, making it difficult to distinguish between generalizable or species-specific mechanisms for X-chromosome inactivation ratio variability. To address this gap, we measure X-chromosome inactivation ratios in ten mammalian species (9531 individual samples), ranging from rodents to primates, and compare the strength of stochastic models or genetic factors for explaining X-chromosome inactivation variability. Our results demonstrate the embryonic stochasticity of X-chromosome inactivation is a general explanatory model for population X-chromosome inactivation variability in mammals, while genetic factors play a minor role.

---

### What are small, medium and large effect sizes for exercise treatments of tendinopathy? A systematic review and meta-analysis [^6092a8e2]. BMJ Open Sport & Exercise Medicine (2023). High credibility.

Description of effect size distributions

From the initial 1454 outcomes extracted, a total of 38 outliers were removed from the analysis with a lower bound threshold of –0.82 (6 effect sizes below) and an upper bound threshold of 7.0 (32 effect sizes above). Across all outcomes and tendinopathy locations, direct calculation of the 0.25 (small), 0.5 (medium) and 0.75 quantiles (large) from the complete empirical data returned the following SMD pre values: 0.37, 0.77 and 1.31, respectively. Application of the meta-analysis model across the data with borrowing of information across studies resulted in similar but shrunken estimates (0.25 quantile 0.5 = 0.34 (95% CrI: 0.31 to 0.37); 0.5 quantile 0.5 = 0.73 (95% CrI: 0.70 to 0.77) and 0.75-quantile 0.5 = 1.21 (95% CrI: 1.17 to 1.27)). A forest plot of effect sizes illustrating effect sizes across studies is presented in online supplemental file 12. Sensitivity analyses checking the potential influences of study type and study quality are presented in online supplemental file 13. No evidence was obtained of greater effect sizes with quasi-experimental designs or with studies identified as high risk of bias.

Analyses of effect size distributions across the different tendinopathy locations are illustrated in figure 2 with numerical values presented in table 3. Analyses were pooled across all outcome domains as a means to compare tendinopathy locations with the largest amount of data possible. Analysis of the modelled small, medium and large thresholds showed considerable overlap in small and medium thresholds across all tendinopathy locations (0.25 quantile 0.5 ranged from 0.28 to 0.38; 0.5 quantile 0.5 ranged from 0.70 to 0.82). However, greater divergence was identified for large threshold estimates, with the greatest values estimated for the elbow (0.75 quantile 0.5 ranged from 1.18 to 1.49).

Figure 2
Effect size distributions across tendinopathy locations with identification of small, medium and large thresholds. black curve represents density plot of empirical effect size distribution. Diamonds with intervals represent small, medium and large thresholds with credible intervals (black: all outcomes; red: tendinopathy specific). RCRSP, rotator cuff related shoulder pain.

---

### Standards of care in diabetes – 2025 [^ff82c0b4]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for obesity, more specifically with respect to patients with diabetes mellitus, lifestyle changes, ADA 2025 guidelines recommend to offer weight management as the primary goal of treatment along with glycemic management in patients with T2DM and overweight or obesity.

---

### Clinical policy: critical issues in the evaluation and management of adult patients in the emergency department with asymptomatic elevated blood pressure [^56a1a4b3]. Annals of Emergency Medicine (2013). Medium credibility.

Appendix C — likelihood ratios and number needed to treat (NNT) interprets likelihood ratio (LR) values as follows: "1.0 1.0 Useless"; "1–5 0.5–1 Rarely of value, only minimally changes pretest probability"; "10 0.1 Worthwhile test, may be diagnostic if the result is concordant with pretest probability"; "20 0.05 Strong test, usually diagnostic"; and "100 0.01 Very accurate test, almost always diagnostic even in the setting of low or high pretest probability"; it also defines NNT as "number of patients who need to be treated to achieve 1 additional good outcome; NNT = 1/absolute risk reduction×100", with absolute risk reduction as "the risk difference between 2 event rates (ie, experimental and control groups)".

---

### Applying design-thinking principles to practice-based pharmacy research [^5b8f2382]. The Annals of Pharmacotherapy (2023). Medium credibility.

Design thinking is an approach to problem solving that focuses on a solution to a problem. This systematic approach can be applied to practice-based research or implementation projects in your practice setting. It may be useful for starting new projects as well as revisiting past projects that may not have yielded meaningful results. The design-thinking process begins with identifying a problem or knowledge gap and then the steps include: (1) understanding the problem, (2) observing the problem, (3) defining the problem, (4) brainstorming possible solutions, (5) prototyping the best solution, and (6) testing the solution.