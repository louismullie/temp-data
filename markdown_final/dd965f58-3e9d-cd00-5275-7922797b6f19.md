# Medical Question & Answer

**Sample ID**: dd965f58-3e9d-cd00-5275-7922797b6f19
**Dataset Index**: 121760

---

## Question

4. Consider the set $M$ of integers $n \in[-30 ; 100]$, for which the expression $A=n^{3}+4 n^{2}+n-6$ is divisible by 5. How many integers are contained in $M$? Find the largest and smallest of them.

---

## Answer

> Let's see… What do we have here? The user is asking how many integers in the interval [-30, 100] make the expression A = n³ + 4n² + n - 6 divisible by 5, and to identify the smallest and largest such integers. Let's break this down step-by-step. First, I need to think about reducing the divisibility condition modulo 5. Then, I should verify the periodicity of the cubic modulo 5 and enumerate the residue classes that satisfy the congruence. Next, I will count how many integers in [-30, 100] fall into those residue classes. Finally, I will identify the smallest and largest qualifying integers within the interval and double-check the arithmetic.

> Let me first confirm the core condition: we need A ≡ 0 (mod 5), which is equivalent to n³ + 4n² + n - 6 ≡ 0 (mod 5). I should simplify this congruence by replacing coefficients with their residues mod 5, so 4 ≡ -1 and -6 ≡ -1, giving n³ - n² + n - 1 ≡ 0 (mod 5). Wait, let me verify that simplification: yes, 4 ≡ -1 and -6 ≡ -1 mod 5, so the expression becomes n³ - n² + n - 1 ≡ 0 (mod 5).

> Next, I should review the periodicity. Because we are working modulo 5, the behavior of any polynomial in n repeats every 5 integers, so I only need to check n ≡ 0, 1, 2, 3, 4 (mod 5) to determine which residue classes satisfy the congruence. Hold on, I should verify that claim: for prime modulus, polynomials exhibit period equal to the modulus, so checking 0 through 4 suffices to characterize all solutions mod 5.

> I will now examine each residue class. For n ≡ 0: 0–0 + 0–1 ≡ -1 ≢ 0. For n ≡ 1: 1–1 + 1–1 ≡ 0, so 1 works. For n ≡ 2: 8–4 + 2–1 ≡ 5 ≡ 0, so 2 works. For n ≡ 3: 27–9 + 3–1 ≡ 20 ≡ 0, so 3 works. For n ≡ 4: 64–16 + 4–1 ≡ 51 ≡ 1 ≢ 0. Let me double-check the arithmetic: 64 mod 5 is 4, 16 mod 5 is 1, so 4–1 + 4–1 = 6 ≡ 1 mod 5, which is not 0. Thus, the solutions mod 5 are n ≡ 1, 2, 3.

> Now I need to count how many integers in [-30, 100] are congruent to 1, 2, or 3 mod 5. The total number of integers in the interval is 100 - (-30) + 1 = 131. Since the residues 1, 2, 3 occur with equal frequency over complete blocks of 5, I should expect roughly 3/5 of the integers to qualify, but I need to check the partial block at the beginning and end. Let me think about this carefully: in any complete set of 5 consecutive integers, exactly 3 satisfy the condition, so I should divide 131 by 5 to get 26 complete blocks with 3 solutions each, and then handle the remainder of 1 integer separately.

> Wait, let me verify the remainder calculation. 131 divided by 5 is 26 with remainder 1, so there are 26 full blocks contributing 26 × 3 = 78 solutions, and 1 extra integer to consider. The extra integer is 100 itself, and 100 ≡ 0 (mod 5), which does not satisfy the condition, so the total remains 78. Hold on, I should double-check the endpoints: -30 ≡ 0 (mod 5), so the sequence of residues starts at 0 and ends at 0, which aligns with 100 ≡ 0 and supports that the partial block at the end does not add a qualifying integer.

> I will now identify the smallest and largest qualifying integers. The smallest integer in [-30, 100] congruent to 1, 2, or 3 mod 5 is -29, because -30 ≡ 0, so the next integers are -29 ≡ 1, -28 ≡ 2, and -27 ≡ 3. Let me verify: -29 mod 5 is indeed 1, so -29 is the smallest. The largest integer in the interval congruent to 1, 2, or 3 mod 5 is 98, because 100 ≡ 0, 99 ≡ 4, and 98 ≡ 3. I should confirm: 98 mod 5 is 3, so 98 qualifies and is the largest.

> Final answer: There are 78 integers in [-30, 100] for which A is divisible by 5, the smallest is -29, and the largest is 98.

---

The expression (A = n^3 + 4n^2 + n - 6) is divisible by 5 when (n \\equiv 1, 2, \\text{ or } 3 \\pmod{5}). In the interval ([-30, 100]), there are **78 integers** satisfying this condition. The smallest is **-29** and the largest is **98**.

---

## Step 1: Simplify the expression modulo 5

We need to find when (A \\equiv 0 \\pmod{5}). First, reduce the coefficients modulo 5:

[
A = n^3 + 4n^2 + n - 6 \\equiv n^3 - n^2 + n - 1 \\pmod{5}
]

Factor the expression:

[
A \\equiv (n - 1)(n^2 + 1) \\pmod{5}
]

So (A \\equiv 0 \\pmod{5}) when (n - 1 \\equiv 0 \\pmod{5}) or (n^2 + 1 \\equiv 0 \\pmod{5}).

---

## Step 2: Solve the congruences

- **Case 1**: (n - 1 \\equiv 0 \\pmod{5} \\Rightarrow n \\equiv 1 \\pmod{5}).

- **Case 2**: (n^2 + 1 \\equiv 0 \\pmod{5} \\Rightarrow n^2 \\equiv -1 \\equiv 4 \\pmod{5}). The squares modulo 5 are (0^2 \\equiv 0), (1^2 \\equiv 1), (2^2 \\equiv 4), (3^2 \\equiv 4), (4^2 \\equiv 1), so (n \\equiv 2) or (3 \\pmod{5}).

Thus, (A \\equiv 0 \\pmod{5}) when (n \\equiv 1, 2, \\text{ or } 3 \\pmod{5}).

---

## Step 3: Count integers in ([-30, 100]) satisfying the congruences

The interval ([-30, 100]) contains 131 integers. In any complete residue system modulo 5, exactly 3 of 5 residues satisfy the condition, so we expect (\\frac{3}{5} \\times 131 = 78.6). Since the count must be an integer, we confirm by enumeration:

- **Residue 1**: (-29, -24, \\dots, 96) → 26 numbers.
- **Residue 2**: (-28, -23, \\dots, 97) → 26 numbers.
- **Residue 3**: (-27, -22, \\dots, 98) → 26 numbers.

Total: (26 + 26 + 26 = 78).

---

## Step 4: Identify the smallest and largest integers

The smallest integer in ([-30, 100]) congruent to 1, 2, or 3 modulo 5 is **-29** (≡ 1 mod 5). The largest is **98** (≡ 3 mod 5).

---

## Final answer

There are **78 integers** in ([-30, 100]) for which (A = n^3 + 4n^2 + n - 6) is divisible by 5. The smallest is **-29** and the largest is **98**.

---

## References

### Current sample size conventions: flaws, harms, and alternatives [^f34a2bb9]. BMC Medicine (2010). Low credibility.

Value of information methods

Many methods have already been described in the statistical literature for choosing the sample size that maximizes the expected value of the information produced minus the total cost of the study. See for an early discussion, for recent examples, and the introduction of for additional references. These require projecting both value and cost at various different sample sizes, including quantifying cost and value on the same scale (note, however, that this could be avoided by instead maximizing value divided by total cost). They also require formally specifying uncertainty about the state of nature; although this can be criticized as being subjective, it improves vastly on the usual conventional approach of assuming that one particular guess is accurate. These methods can require considerable effort and technical expertise, but they can also produce the sort of thorough and directly meaningful assessment that should be required to justify studies that are very expensive or that put many people at risk.

Simple choices based on cost or feasibility

Recent work has justified two simple choices that are based only on costs, with no need to quantify projected value or current uncertainty about the topic being studied. Because costs can generally be more accurately projected than the inputs for conventional calculations, this avoids the inherent inaccuracy that besets the conventional approach. One choice, called n min, is the sample size that minimizes the total cost per subject studied. This is guaranteed to be more cost-efficient (produce a better ratio of projected value to cost) than any larger sample size. It therefore cannot be validly criticized as inadequate. The other, called n root, is the sample size that minimizes the total cost divided by the square root of sample size. This is smaller than n min and is most justifiable for innovative studies where very little is already known about the issue to be studied, in which case it is also guaranteed to be more cost efficient than any larger sample size. An interactive spreadsheet that facilitates identification of n min and n root is provided as Additional file 2.

A common pragmatic strategy is to use the maximum sample size that is reasonably feasible. When sample size is constrained by cost barriers, such as exhausting the pool of the most easily studied subjects, this strategy may closely approximate use of n min and therefore share its justification. When constraints imposed by funders determine feasibility, doing the maximum possible within those constraints is a sensible choice.

---

### SARS-CoV-2 gene content and COVID-19 mutation impact by comparing 44 sarbecovirus genomes [^41ae8b01]. Nature Communications (2021). High credibility.

To find regions that were significantly enriched for missense mutations in conserved amino acids, we first defined a null model as follows. For each mature protein, we counted the number of missense mutations and the number of conserved amino acids and randomly assigned each SNV to a conserved amino acid in the same mature protein (using Python’s random.randint function), allowing multiplicity. For any positive integer n, we found the largest number of mutations that had been assigned to any set of n consecutive conserved amino acids within the same mature protein across the whole genome. Doing this 100,000 times gave us a distribution of the number of missense mutations in the most enriched set of n consecutive conserved amino acids in the genome. Comparing the number of actual missense mutations in any particular set of n consecutive conserved amino acids to this distribution gave us a nominal p -value for that n. We applied this procedure for each n from 1 to 100 and multiplied the resulting p -values by a Bonferroni correction of 100 to calculate a corrected p -value for a particular region to be significantly enriched. We note that these 100 hypotheses are correlated because enriched regions of different lengths can overlap, so a Bonferroni correction is overly conservative and our reported p -value of 0.012 understates the level of statistical significance. To find significantly depleted regions we applied a similar procedure with every n from 1 to 1000, but did not find any depleted regions with nominal P -value <0.05 even without multiple hypothesis correction.

---

### Integrating genomics and metabolomics for scalable non-ribosomal peptide discovery [^f956b07f]. Nature Communications (2021). High credibility.

For many organisms, the total number of possible core NRPs is prohibitively large, making it infeasible to conduct search against massive spectral repositories. Currently, even the fastest state-of-the-art spectral search methods are slow for searching millions of input spectra against databases with over 10 5 peptides in a modification-tolerant manner as the runtime grows exceedingly large when the database size grows. Supplementary Tables S2 and S7 shows that for 24 (22) out for 27 organisms in XPF dataset and 9 (7) out of 20 organisms in SoilActi dataset, the total number of core NRPs exceed 10 5 (10 6). Therefore, to enable scalable peptidogenomics for NRP discovery, for each constructed assembly line NRPminer, selects a set of candidate core NRPs. To do so, NRPminer starts by finding the number of core NRPs of A according to their adenylation scores (Problem 1) and then it uses these numbers for generating all core NRPs of A with adenylation scores higher than a threshold (Problem 2).

Problem 1. Given A = A 1,…,A n and a positive integer s, find the number of all core NRPs of A with adenylation score equal to s.

Letwhere || shows the number of amino acids in A i. For any positive integers i and s satisfying,1 ≤ i ≤ n and s ≤ maxScore A, let numCoreNRPs A (i, s) denote the number of core NRPs, of assembly line A 1,.A i withequal to s. Let numCoreNRPs A (0, s) = 0 for any positive integer s, and numCoreNRPs A (i, s) = 0 for any integer s < 0, across all possible values of i. Then, for any positive integers i and s satisfying 1 ≤ i ≤ n and 0 < s ≤ maxScore A, we have

---

### Statistics review 4: sample size calculations [^eadbdd64]. Critical Care (2002). Low credibility.

The present review introduces the notion of statistical power and the hazard of under-powered studies. The problem of how to calculate an ideal sample size is also discussed within the context of factors that affect power, and specific methods for the calculation of sample size are presented for two common scenarios, along with extensions to the simplest case.

---

### Dynamics of ranking [^58e0eeab]. Nature Communications (2022). High credibility.

Virtually anything can be and is ranked; people, institutions, countries, words, genes. Rankings reduce complex systems to ordered lists, reflecting the ability of their elements to perform relevant functions, and are being used from socioeconomic policy to knowledge extraction. A century of research has found regularities when temporal rank data is aggregated. Far less is known, however, about how rankings change in time. Here we explore the dynamics of 30 rankings in natural, social, economic, and infrastructural systems, comprising millions of elements and timescales from minutes to centuries. We find that the flux of new elements determines the stability of a ranking: for high flux only the top of the list is stable, otherwise top and bottom are equally stable. We show that two basic mechanisms - displacement and replacement of elements - capture empirical ranking dynamics. The model uncovers two regimes of behavior; fast and large rank changes, or slow diffusion. Our results indicate that the balance between robustness and adaptability in ranked systems might be governed by simple random processes irrespective of system details.

---

### Eighty-six billion and counting: do we know the number of neurons in the human brain? [^75dfd524]. Brain (2025). Medium credibility.

The problem with the 2009 paper

Before we proceed, let us look at the data and main claim of the 2009 paper.Following the text, we know that ‘brains from 50-, 51-, 54-, and 71- year-old males, deceased from nonneurological causes and without cognitive impairment (CDR 0, IQCODE 3.0), were analyzed’. The average weight of the four brains is 1508.91 g, with a standard deviation of 299.14 g. The weight of each brain or the number of neurons is not given. However, according to the paper, the average number of neurons in the four brains is 86.06 bn, with a standard deviation of 8.12 bn. We are further provided with a range of 78.82 to 95.40 bn. We can then combine these data to reconstitute the original dataset and find that the numbers of neurons are: {78.82, 79.72, 90.30, 95.40} bn. From this small dataset, the authors make the most important claim that: ‘We find that the adult male human brain contains on average 86.1 ± 8.1 billion NeuN-positive cells (“neurons”)’. With over 3000 citations, it is probably the single most cited number about the brain.

It is quite an achievement, and the low number of data-points in the study demonstrates how difficult the process must have been. But before we close the matter, let us reflect on what can really be concluded from four data-points, as we know that statistical statements depend strongly on the number of data points. If we have one or two data-points, we would gladly agree that a mean would not be representative of the entire population. How about four?

Statisticians have studied this type of problem in great detail: if we sample n ≥ 2 points from a distribution, it is easy to take the sample mean (µ) and standard deviation (δ) of n samples. We want to know to what extent these values are representative of the actual (but unknown) population mean that we are interested in. In our case, the question is: to what extent is µ = 86.1 bn with n = 4 a valid estimate of the number of neurons in the human brain?

---

### Current sample size conventions: flaws, harms, and alternatives [^2ab4a8e1]. BMC Medicine (2010). Low credibility.

Sensitivity analysis

Sample size planning involves considerable uncertainty, and a simple and familiar way of assessing uncertainty is with sensitivity analyses: examining how results change under different assumptions. I propose a framework, illustrated in Table 1, for presenting a useful array of possibilities for a study's most important products, the estimated effect and its confidence interval. This is based on varying 1) assumptions that determine the precision of estimates and 2) the observed effect size. Together with discussion of the potential value of each resulting outcome, this provides an informal assessment of the value of the information that may result. This is less systematic than the value of information methods mentioned above, but it covers a range of likely scenarios, avoids technical difficulties of the mathematically formalized methods, and focuses on particular concrete results, which allows reviewers to easily assess the claimed potential value. Because the table entries show the information that would be used in a systematic review, the potential value can be discussed in terms of how it would modify a recent review or contribute to future ones, if those are deemed to be the most important considerations.

Table 1 
Sample layout of sensitivity analysis.

Shown are possible study results with a given sample size (935 per group, based on the vitamin study discussed above), for a yes or no outcome. Rows have differing assumptions concerning precision of the estimates, ranging from high precision (top row) to low precision (bottom row). For a continuous outcome, the rows would instead be for optimistic (small), expected, and pessimistic (large) standard deviations.

The entries in the table are exactly the key results that interpretation should focus on when the study is completed, so this properly aligns planning with eventual use. The middle row can be a best guess such as would be used for conventional calculations; the other rows should reflect a reasonable range of uncertainty, which will depend on what is already known about the topic being studied. For the columns, inclusion of the intermediate case is important, because this will often include the most problematic or disappointing potential results. The vitamin study paired a safe and inexpensive intervention with a severe outcome, so even results in the middle column would be regarded as encouraging; the actual completed study landed essentially in Box 7, which should have been interpreted as very encouraging even though not definitive. Boxes 8 and 9 will usually be the least useful, but as noted above (False assurance), the risk of disappointing results is always present and should not be considered a flaw in study design.

---

### Integrating genomics and metabolomics for scalable non-ribosomal peptide discovery [^32f62782]. Nature Communications (2021). High credibility.

Table 3 
Number of core NRPs of SurugamideAL (assembly line corresponding to cyclic surugamides A–D) according to their adenylation scores.

Only values of s with non-zero number of cores and corresponding to the top 1000 high-scoring core NRPs are shown.

Problem 2. Given an assembly line A and a positive integer N, generate candidateCoreNRPs A (N), defined as all core NRPs of A with adenylation scores at least thresholdScore A (N).

NRPminer follows a graph-theoretic approach to quickly generate candidateCoreNRPs A (N) by using the computed values of numCoreNRPs. Let G (A) be the acyclic directed graph with nodes corresponding to pairs of positive integers i ≤ n and s ≤ maxScore A, such that numCoreNRPs A (i,s) > 0, denoted by. For every node(i = 1,…, n) and everysuch that numCoreNRPs A (i−1,s−S A (i,a)) > 0, there exists a directed edge fromto. Let Source beand let Sink be the set of all nodessuch that thresholdScore A (N). We call each directed path in G (A) from Source to the nodes in Sink as a candidate path of G(A).

Each candidate path of G (A) corresponds to a distinct core NRP of A with adenylation score at least thresholdScore A (N) and vice versa. Therefore, the problem of finding all core NRPs of A with adenylation score at least thresholdScore A (N) corresponds to the problem of finding all candidate paths of G (A). While enumerating all paths with n nodes in a directed acyclic graph can grow exponentially large (as there can be exponentially many such paths), but due to our choice of thresholdScore A (N), the number of candidate paths of G (A) is bound by 10 5 (or N if). NRPminer uses the default value N = 1000. Moreover, n ≤ 20 (only assembly lines made up of up to 20 A-domains are considered) and k 1.200000000000000e+01

---

### Article 5. An introduction to estimation – 2: from z to T [^fb82b806]. Emergency Medicine Journal (2001). Low credibility.

Provided the sample size is large enough (that is, n greater than 100), the z statistic can be used to determine the confidence interval estimation of the population mean even when the sigma is not known. In these cases the estimation of the standard error of the mean is used. The z statistic is also valid when determining the population's proportion based upon a large sample. However, when dealing with smaller samples, the z statistic is replaced by the t statistic. This makes it possible to estimate, in a population with an unknown standard deviation: The probability of getting a sample mean greater than or equal to a particular value The value of a sample mean with a particular probability of occurring The probability of getting a sample mean between two particular values The confidence interval for the estimation of the population mean can also be determined using the t statistic.

---

### Pattern of mutation rates in the germline of drosophila melanogaster males from a large-scale mutation screening experiment [^e169f0a0]. G3 (2014). Low credibility.

Estimates of mutation rates

In the previous section, we established that N (5) = 5 ~ 6 and N (8) = 7 ~ 9 is the best assumption for the germline population dynamics; therefore, we will use this assumption for subsequent analysis. Because we investigate the mutation rates for different lethality levels, it is desirable to know whether a slight deviation of the optimal parameters will lead to a significant difference in the subsequent mutation rate estimation and hypothesis testing. We performed full likelihood analysis for several lethalities around N (5) = 5 − 6 and N (8) = 7 − 9 and found that indeed the impact is rather marginal unless the deviation leads to a substantially smaller likelihood value. Table 5 lists, as examples, the maximum likelihood estimates of mutation rates for combinations of N (5) and N (8) that differs no more than one step from the optimal. As can be seen, overall the estimates are rather similar to those under the optimal, but there are some notable differences in u 2 for cases two steps away from the optimal; but then the difference of likelihood to that of the optimal is substantial. Therefore, we are confident that conclusions based on the optimal N (5) and N (8) will be robust.

Table 5 
Full maximum likelihood estimates of u × 10 3 for lethality ≥ 97% and N (5) = 5 ~ 6 and N (8) = 7 ~ 9 with intervals [1,1], [2,2],[3,3], [4 14], [15 -6], and [-5 38]

Table 6 gives the estimates of u for three recessive lethalities for 38 cell divisions and the optimal combination of N (5) and N (8). The most striking pattern is that the mutation rate for the first division varies considerably, but regardless of how the data are examined, it is markedly larger than those for subsequent divisions. Similar to the pattern observed in, the spermatogenesis has a relatively higher mutation rate than the interval divisions. With data aggregation, there appears to be a trend of appreciable mutation rate for the second division, and a similar pattern is observed for the stem cell stage although the magnitude is less appreciable. The cleavage stage, excluding the first (and perhaps also the second), harbors the smallest mutation rate. The ratio of the first division mutation rate to the mean mutation rate of the internal divisions are all larger than 100, with the highest ratio of 620 for lethality [97%, 98%).

---

### How do you design randomised trials for smaller populations? A framework [^9ac3f0db]. BMC Medicine (2016). Low credibility.

Exploring less common approaches to reducing sample size

We now consider some less standard approaches to bringing the sample size requirements closer to the numbers it is feasible to recruit in a reasonable time frame.

Step 3: Relaxing α by a small amount, beyond traditional values

The much-criticised 5 % significance level is used widely in much applied scientific research, but is an arbitrary figure. It is extremely rare for clinical trials to use any other level. It may be argued that this convention has been adopted as a compromise between erroneously concluding a new treatment is more efficacious and undertaking a trial of an achievable size and length. Settings where traditionally sized trials are not possible may be just the area where researchers start to break this convention, for good reason.

In considering the type I error, it is critical to consider the question: ‘What are the consequences of erroneously deciding to use a new treatment routinely if it is truly not better?’

Taking the societal perspective as before, we might consider the probability of making a type I error, thus erroneously burdening patients with treatments that do not improve outcomes, or even worsen them, while potentially imposing unnecessary toxicity.

First, for conditions where there are only enough patients available to run one modestly sized randomised trial in a reasonable time frame, research progress will be relatively slow, and making a type I error may be less of a concern than a type II error. In contrast, making several type I errors in a common disease could lead in practice to patients taking several ineffective treatments; for a disease area where only one trial can run at any given time, the overall burden on patients is potentially taking one ineffective treatment that does not work.

Thus, if we take the societal perspective with the trials in Table 1 then, if each trial was analysed with α =0.05 and we see (hypothetically) 40 % positive results, then the expected number of false positive trials is given in the final column. We also assumed 10 % and 70 % positive results, with qualitatively similar conclusions.

---

### The three numbers you need to know about healthcare: the 60-30-10 challenge [^d6cb22ab]. BMC Medicine (2020). Medium credibility.

Background

Healthcare represents a paradox. While change is everywhere, performance has flatlined: 60% of care on average is in line with evidence- or consensus-based guidelines, 30% is some form of waste or of low value, and 10% is harm. The 60-30-10 Challenge has persisted for three decades.

Main Body

Current top-down or chain-logic strategies to address this problem, based essentially on linear models of change and relying on policies, hierarchies, and standardisation, have proven insufficient. Instead, we need to marry ideas drawn from complexity science and continuous improvement with proposals for creating a deep learning health system. This dynamic learning model has the potential to assemble relevant information including patients' histories, and clinical, patient, laboratory, and cost data for improved decision-making in real time, or close to real time. If we get it right, the learning health system will contribute to care being more evidence-based and less wasteful and harmful. It will need a purpose-designed digital backbone and infrastructure, apply artificial intelligence to support diagnosis and treatment options, harness genomic and other new data types, and create informed discussions of options between patients, families, and clinicians. While there will be many variants of the model, learning health systems will need to spread, and be encouraged to do so, principally through diffusion of innovation models and local adaptations.

Conclusion

Deep learning systems can enable us to better exploit expanding health datasets including traditional and newer forms of big and smaller-scale data, e.g. genomics and cost information, and incorporate patient preferences into decision-making. As we envisage it, a deep learning system will support healthcare's desire to continually improve, and make gains on the 60-30-10 dimensions. All modern health systems are awash with data, but it is only recently that we have been able to bring this together, operationalised, and turned into useful information by which to make more intelligent, timely decisions than in the past.

---

### Limits on fundamental limits to computation [^98c96d2d]. Nature (2014). Excellent credibility.

An indispensable part of our personal and working lives, computing has also become essential to industries and governments. Steady improvements in computer hardware have been supported by periodic doubling of transistor densities in integrated circuits over the past fifty years. Such Moore scaling now requires ever-increasing efforts, stimulating research in alternative hardware and stirring controversy. To help evaluate emerging technologies and increase our understanding of integrated-circuit scaling, here I review fundamental limits to computation in the areas of manufacturing, energy, physical space, design and verification effort, and algorithms. To outline what is achievable in principle and in practice, I recapitulate how some limits were circumvented, and compare loose and tight limits. Engineering difficulties encountered by emerging technologies may indicate yet unknown limits.

---

### Expert panel on integrated guidelines for cardiovascular health and risk reduction in children and adolescents: summary report [^4c1faf55]. PES (2012). Medium credibility.

Quality criteria for assessment of individual randomized controlled trials—The table lists “Criteria Needed for ‘Y’ Selection” including study selection elements “Inclusion/exclusion criteria specified,” “Criteria applied equally to study arms,” and “Comparable patient characteristics,” with details that “Health, demographics, and other characteristics of subjects/patients [are] described” and that the “Distribution of health, demographics, and other characteristics [is] similar across study arms at baseline.” Methodologic safeguards include “Appropriate randomization” with the “Method of randomizing subjects to arms described” and “free from bias (e.g., random number generator),” “Allocation concealment” to prevent “foreknowledge of treatment assignment,” and blinding of participants and assessors, including that “Patients or subjects [are] blinded to treatment as appropriate,” “Provider or other treatment administrator [is] blinded,” and “Data collectors/analysts or others with ability to affect results [are] blinded as appropriate.” Additional items are “Low attrition rates” with a “Low rate of attrition for each arm” and “Withdrawals and reasons for withdrawal similar across arms,” plus transparency items “Conflicts of interest” with “Sources of funding and investigators’ affiliations described,” and “Industry sponsorship beyond provision of drug or placebo,” with notes that “Selection does not affect quality grade” and that “an asterisk will appear by quality grade if selection is ‘Y’.” Data value options shown include “• Y • N • NR/unknown.”

---

### Summary benchmarks-full set – 2024 [^76ffec10]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines—scope and principles emphasize that Preferred Practice Patterns provide guidance for the pattern of practice, not for the care of a particular individual, and that the Preferred Practice Pattern guidelines are not medical standards to be adhered to in all individual situations. The series is based on three principles: each Preferred Practice Pattern should be clinically relevant and specific enough to provide useful information to practitioners; each recommendation that is made should be given an explicit rating that shows its importance to the care process; and each recommendation should also be given an explicit rating that shows the strength of evidence that supports the recommendation and reflects the best evidence available. Adherence to these Preferred Practice Patterns will not ensure a successful outcome in every situation, and these practice patterns should not be deemed inclusive of all proper methods of care or exclusive of other methods of care reasonably directed at obtaining the best results.

---

### Subsampling scaling [^8e2e9a81]. Nature Communications (2017). Medium credibility.

As each event is observed independently, the probability of X sub = s is the sum over probabilities of observing clusters of X = s + k events, where k denotes the missed events and s the sampled ones (binomial sampling):

This equation holds for any discrete P (s) defined on, the set of non-negative integers. To infer P (s) from P sub (s), we develop in the following a novel ‘subsampling scaling' that allows to parcel out the changes in P (s) originating from spatial subsampling. A correct scaling ansatz collapses the P sub (s) for any sampling probability p.

In the following, we focus on subsampling from two specific families of distributions that are of particular importance in the context of neuroscience, namely exponential distributions P (s)= C λ e − λs with λ >0, and power laws P (s)= C γ s − γ with γ >1. These two families are known to show different behaviours under subsampling:
For exponential distributions, P (s) and P sub (s) belong to the same class of distributions, only their parameters change under subsampling. Notably, this result generalizes to positive and negative binomial distributions, which include Poisson distributions.
Power-laws or scale-free distributions, despite their name, are not invariant under subsampling. Namely, if P (s) follows a power-law distribution, then P sub (s) is not a power law but only approaching it in the limit of large cluster size (s →∞).

---

### Dissection of gene expression datasets into clinically relevant interaction signatures via high-dimensional correlation maximization [^2aa45772]. Nature Communications (2019). High credibility.

Signature functional

To detect and unambiguously dissect a signature, first its optimal linear directions along which it extends in gene and sample space have to be determined. The goal of the signature functional is to score candidate directions during the initial search (Methods/Step 1) and during optimization (Methods/Step 2).

The signature functionalassigns to every possible axes paira scalar.is the larger, the more genes or samples are correlated to these axes, and the higher these correlations are. The selection of an initial representative with help of this functional (step 1) and maximizing this functional (step 2) guides SDCM to one of the largest and most consistent signatures in the current signal. (Thus, in all versatility tests, the large signature #1 was always detected first, and narrow or weak signatures #5, #6, and #7 were detected last; see detection ranks in Supplementary Fig. 1.)

The signature size, i.e. the number of participating genes or samples, is estimated by summing gene weightsrespectively sample weightsof the extended signature focus:

Average absolute correlations for all genes and all samples in the extended signature focus are computed as weighted means:

Next, these separate scores for genes and samples need to be combined.

As m k n k corresponds to the number of measurement values supporting the signature, a natural choice for the combined signature size would be their geometric average. However, signatures affecting the same number m k n k of measured (gene, sample)-values in M 0 may be caused by noise or by artifacts with different probabilities. For example, a narrow signature concerning 10,000 genes in two samples is more likely a measurement artefact than a signature representing an interaction that affects 100 genes in 200 samples. To detect and dissect broad and robust signatures first, we therefore optimized the usual geometric average by putting 90% weight on the order dimension with lower size. Hence, the combined signature size is defined as

---

### KDOQI US commentary on the KDIGO 2024 clinical practice guideline for the evaluation and management of CKD [^7b494d73]. American Journal of Kidney Diseases (2025). High credibility.

KDIGO 2024 CKD albuminuria categories—description and numeric ranges—define A1 as normal to mildly increased <30 mg/g <3 mg/mmol, A2 as moderately increased 30–299 mg/g 3–29 mg/mmol, and A3 as severely increased ≥300 mg/g ≥30 mg/mmol; in the risk grid, the numbers in the boxes are a guide to the frequency of monitoring (number of times per year).

---

### Sample sizes based on three popular indices of risks [^4a61a58f]. General Psychiatry (2018). Low credibility.

Sample size justification is a very crucial part in the design of clinical trials. In this paper, the authors derive a new formula to calculate the sample size for a binary outcome given one of the three popular indices of risk difference. The sample size based on the absolute difference is the fundamental one, which can be easily used to derive sample size given the risk ratio or OR.

---

### Current sample size conventions: flaws, harms, and alternatives [^a12f302b]. BMC Medicine (2010). Low credibility.

Inherent inaccuracy

Although precise mathematical formulas are available for calculating sample sizes, these depend on specifying exact values for inputs, and changes in these inputs produce magnified changes in the resulting sample size. In particular, studies with a continuous primary outcome measure, such as quality of life score, change in telomere length, weight loss, and so on, must specify its standard deviation. This is difficult to do accurately, unless there is so much preliminary data that the study isn't really needed, but it has a big influence on sample size: a two-fold increase in the assumed standard deviation produces a four-fold increase in sample size. A four-fold larger sample size also results from halving the difference of interest (the alternative hypothesis). This is particularly hard to specify because even the theoretical principles for choosing it are unclear. Some assert that it should be the smallest difference that would be important to patients or for scientific or public health purposes, but this is often subjective and difficult to specify (what difference in survival rates is unimportant), importance is rarely all-or-nothing, and very small differences may be important, leading to impractical sample sizes. Investigators frequently use the difference suggested by preliminary data, but this is unreliable and has greater risk of inconclusive results for differences that are smaller but still interesting. A case has even been made for using the difference hoped for by patients.

Inaccuracy of sample size calculations is not only theoretically inevitable but also empirically verified. One careful study of assumed standard deviations in a seemingly best-case scenario, randomized trials published in four leading general medical journals, found that about a quarter had more than five-fold inaccuracy in sample size and more than half had more than two-fold inaccuracy. Another study recently found high rates of inaccuracy along with many other problems. This problem cannot be solved by simply trying harder to pinpoint where 80% power will be achieved, because inaccuracy is inherent in the conventional framework.

---

### Quantum algorithmic measurement [^618274ff]. Nature Communications (2022). High credibility.

QUALM complexity

Having defined tasks and QUALMs, we now turn to defining QUALM complexity.

Definition 10 (Gate complexity, query complexity, and QUALM complexity). The gate complexity of a given QUALM over the admissible set of gatesis the length (i.e. the number of symbols from) of the sequence, minus the number of □ symbols. We denote this by GateComplexity[QUALM], and call this the QUALM gate complexity. Similarly, the query complexity is the number of □’s appearing in, and this is denoted by QueryComplexity[QUALM]. This is called the QUALM query complexity. We call the sumthe QUALM complexity.

The exact (respectively, approximate) QUALM complexity of a task is given by the QUALM with least QUALM complexity, which achieves the task exactly (respectively, approximately).

We also note that it might be relevant, in various situations, to weight gates versus query calls differently, namely to consider the QUALM complexity to befor some suitable penalty factor λ.

As usual in computational complexity, one is interested in families of tasks and QUALMs, where some parameter dictating the size of the problem grows to infinity, and we ask how the complexity grows as a function of that parameter.

---

### Massively parallel reporter perturbation assays uncover temporal regulatory architecture during neural differentiation [^f6420dcd]. Nature Communications (2022). High credibility.

Altogether our graph now has 1547 region nodes, 4393 motif nodes and 68 property nodes. These nodes are connected by a total of 99,165 edges. Our goal now becomes to find the minimum number of [region × motif] combinations (each representing a specific motif instance, or—equivalently—an edge in our graph) that will guarantee a sufficient coverage of each property. In other words, we want to select a minimal number of motif-region pairs such that every “property node” in our third layer is connected by an edge to a sufficient number of motifs and regions (as detailed below). Having staged our data in a tripartite graph allowed us to re-state our goal as a constrained optimization problem-guaranteeing minimal level of connectivity for the third layer, while minimizing the number of selected nodes and edges in the first two layers. Since this problem is NP- hard, we followed the common practice and formulated it as an integer linear program (ILP), which can be solved efficiently through a range of heuristics with available solvers. With this ILP, we were able to select 591 regulatory regions and 255 motifs that are organized into 2144 region-motif pairs. Below, we provide a more in-depth description of this process.

---

### Summary benchmarks-full set – 2024 [^e2c40ddc]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines—GRADE recommendation categories are specified as: “Strong recommendation (SR): Used when the desirable effects of an intervention clearly outweigh the undesirable effects or clearly do not” and “Discretionary recommendation (DR): Used when the trade-offs are less certain—either because of low-quality evidence or because evidence suggests that desirable and undesirable effects are closely balanced.”

---

### Separating measurement and expression models clarifies confusion in single-cell RNA sequencing analysis [^510a7011]. Nature Genetics (2021). High credibility.

The high proportion of zeros in typical single-cell RNA sequencing datasets has led to widespread but inconsistent use of terminology such as dropout and missing data. Here, we argue that much of this terminology is unhelpful and confusing, and outline simple ideas to help to reduce confusion. These include: (1) observed single-cell RNA sequencing counts reflect both true gene expression levels and measurement error, and carefully distinguishing between these contributions helps to clarify thinking; and (2) method development should start with a Poisson measurement model, rather than more complex models, because it is simple and generally consistent with existing data. We outline how several existing methods can be viewed within this framework and highlight how these methods differ in their assumptions about expression variation. We also illustrate how our perspective helps to address questions of biological interest, such as whether messenger RNA expression levels are multimodal among cells.

---

### Approaches to working in high-dimensional data spaces: gene expression microarrays [^4f0907bf]. British Journal of Cancer (2008). Low credibility.

Although feature selection is integral to each of these analytical tasks, an exhaustive search of all 2 n −1 possible feature subsets is prohibitive for large n. Thus, practical feature selection techniques are of necessity heuristic, with an inherent accuracy/complexity tradeoff. Moreover, while multivariate analysis methods based on complex criterion functions may reveal subtle joint marker effects, they are also prone to overfitting. Additionally, high dimensionality compromises the ability to validate marker discovery, which requires accurately measuring true and false discovery rates. These issues have prompted the development of a variety of novel statistical methods for estimating (or controlling for) false discoveries.

---

### Thermodynamics of quantum systems with multiple conserved quantities [^9299e804]. Nature Communications (2016). Medium credibility.

Recently, there has been much progress in understanding the thermodynamics of quantum systems, even for small individual systems. Most of this work has focused on the standard case where energy is the only conserved quantity. Here we consider a generalization of this work to deal with multiple conserved quantities. Each conserved quantity, which, importantly, need not commute with the rest, can be extracted and stored in its own battery. Unlike the standard case, in which the amount of extractable energy is constrained, here there is no limit on how much of any individual conserved quantity can be extracted. However, other conserved quantities must be supplied, and the second law constrains the combination of extractable quantities and the trade-offs between them. We present explicit protocols that allow us to perform arbitrarily good trade-offs and extract arbitrarily good combinations of conserved quantities from individual quantum systems.

---

### Summary benchmarks-full set – 2024 [^6f68d3ad]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines—use and ethics note states that “The PPPs are intended to serve as guides in patient care, with greatest emphasis on technical aspects,” that “true medical excellence is achieved only when skills are applied in a such a manner that the patients’ needs are the foremost consideration,” and that “The Academy is available to assist members in resolving ethical dilemmas that arise in the course of practice.”

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^59838d2f]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Sample size metrics—tolerance intervals versus confidence intervals: Although performance is often stated in terms of confidence intervals (CI), the CI of the mean only gives an estimate of the population mean with a stated level of confidence and does not give an indication of the performance of any given sample; to estimate the distribution of the underlying population and the performance of individual samples, the tolerance intervals should be used, with the lower tolerance interval for a normally distributed population determined as x̄ ± k × s, where s is the sample standard deviation (SD) and k is a correction factor, and for two-sided 95% confidence with n = 20 the k value is 2.75, approaching the z-score as the number of samples increases.

---

### Number needed to treat and number needed to harm are not the best way to report and assess the results of randomised clinical trials [^f243221a]. British Journal of Haematology (2009). Low credibility.

The inverse of the difference between rates, called the 'number needed to treat' (NNT), was suggested 20 years ago as a good way to present the results of comparisons of success or failure under different therapies. Such comparisons usually arise in randomised controlled trials and meta-analysis. This article reviews the claims made about this statistic, and the problems associated with it. Methods that have been proposed for confidence intervals are evaluated, and shown to be erroneous. We suggest that giving the baseline risk, and the difference in success or event rates, the 'absolute risk reduction', is preferable to the number needed to treat, for both theoretical and practical reasons.

---

### Evaluating molecular biomarkers for the early detection of lung cancer: when is a biomarker ready for clinical use? An official American Thoracic Society policy statement [^1c6ced37]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Accuracy guidance for biomarker clinical validation—formulas specify minimal accuracy thresholds tied to action after positive versus negative results. The committee “agreed that it would be helpful to provide guidance about the minimal accuracy, as assessed in the clinical validation phase, that could lead to a positive clinical impact.” For tests where “a positive biomarker result leads to an action where a negative biomarker result is associated with standard of care for the population,” the threshold is “sensitivity/(1 − specificity) ≥ [(1 − prevalence)/prevalence] × harm/benefit,” where harm/benefit is “the ratio of the net harm of a falsely positive test result to the net benefit of a true-positive test result.” For tests where “a negative test leads to an action other than standard of care for the population,” the threshold is “specificity/(1 − sensitivity) ≥ [prevalence/(1 − prevalence)] × harm/benefit,” where harm/benefit is “the ratio of the net harm of a falsely negative test result to the net benefit of a true-negative test result.” “Sensitivity/(1 − specificity) is known as the positive likelihood ratio, and specificity/(1 − sensitivity) is 1 divided by the negative likelihood ratio,” and “Prevalence refers to the percentage of cases in the intended use population.” The harm/benefit term “can be articulated in one of two ways: 1/N, where in the first scenario N is the maximum number of control subjects testing positive that is tolerated to benefit one case subject testing positive.”

---

### How many days was that? We' re still not sure, but we' re asking the question better! [^b41c7375]. Medicine and Science in Sports and Exercise (2008). Low credibility.

Unreliable measures limit the ability to detect relationships with other variables. Day-to-day variability in measurement is a source of unreliability. Studies vary substantially in numbers of days needed to reliably assess physical activity. The required numbers of days has probably been underestimated due to violations of the assumption of compound symmetry in using the intraclass correlation. Collecting many days of data become unfeasible in real-world situations. The current dilemma could be solved by adopting distribution correction techniques from nutrition or gaining more information on the measurement model with generalizability studies. This would partition the variance into sources of error that could be minimized. More precise estimates of numbers of days to reliably assess physical activity will likely vary by purpose of the study, type of instrument, and characteristics of the sample. This work remains to be done.

---

### The minimal work cost of information processing [^927873c9]. Nature Communications (2015). Medium credibility.

Classical mappings and dependence on the logical process

Our result, which is applicable to arbitrary quantum processes, applies to all classical computations as a special case. Classically, logical processes correspond to stochastic maps, of which deterministic functions are a special case. As a simple example, consider the AND gate. This is one of the elementary operations computing devices can perform, from which more complex circuits can be designed. The gate takes two bits as input, and outputs a single bit that is set to 1 exactly when both input bits are 1, as illustrated in Fig. 2a.

The logical process is manifestly irreversible, as the output alone does not allow to infer the input uniquely. If one of the inputs is zero, then the logical process effectively has to reset a three-level system to zero, forgetting which of the three possible inputs 00, 01 or 10 was given; this information can be viewed as being discarded, and hence dumped into the environment. We can confirm this intuition with our main result, using the fact that a general classical mapping is given by the specification of the conditional probability p (x ′| x) of observing x ′ at the output if the input was x. Embedding the classical probability distributions into the diagonals of quantum states, the infinity norm in expression (2) becomes simply

---

### Estimating the statistical significance of gene expression changes observed with oligonucleotide arrays [^ba8afa58]. Human Molecular Genetics (2002). Low credibility.

We present a simple method to assign approximate P-values to gene expression changes detected with Affymetrix oligonucleotide arrays and software. The method pools data for groups of genes and a small number of like-to-like comparisons in order to estimate the significance of changes observed for single genes in comparisons of experimental interest. Statistical significance levels are based on the observed variability in the fractional majority of probe pairs that indicate increasing or decreasing differential expression in comparisons of technical replicates. From this reference distribution or error model, we compute the expected frequency for fractional majorities in comparisons for N > or = 2. These computed distributions are the source of P-value estimates for changes seen in the experimental comparisons. The method is intended to complement the Affymetrix software and to rationalize gene selection for experimental designs involving limited replication.

---

### Incorporating economic evidence in clinical guidelines: a framework from the clinical guidelines committee of the American College of Physicians [^e53f1cf5]. Annals of Internal Medicine (2025). High credibility.

GRADE Evidence-to-Decision summary of judgments—A sample blank framework outlines judgment scale options, including “No clinically meaningful,” “Small,” “Medium,” “Large,” “Varies,” “Uncertain,” and “No included studies.” Type of recommendation categories listed are “Strong recommendation against the intervention,” “Conditional recommendation against the intervention,” “Conditional recommendation for either the intervention or the comparator,” “Conditional recommendation for the intervention,” and “Strong recommendation for the intervention.” The figure clarifies “GRADE = Grading of Recommendations Assessment, Development and Evaluation” and notes it was “Prepared with GRADEpro (https://gradepro.org) and modified by the authors.”

---

### Tradict enables accurate prediction of eukaryotic transcriptional States from 100 marker genes [^b83a72c5]. Nature Communications (2017). Medium credibility.

Transcript levels are a critical determinant of the proteome and hence cellular function. Because the transcriptome is an outcome of the interactions between genes and their products, it may be accurately represented by a subset of transcript abundances. We develop a method, Tradict (transcriptome predict), capable of learning and using the expression measurements of a small subset of 100 marker genes to predict transcriptome-wide gene abundances and the expression of a comprehensive, but interpretable list of transcriptional programs that represent the major biological processes and pathways of the cell. By analyzing over 23,000 publicly available RNA-Seq data sets, we show that Tradict is robust to noise and accurate. Coupled with targeted RNA sequencing, Tradict may therefore enable simultaneous transcriptome-wide screening and mechanistic investigation at large scales.

---

### A space-time tradeoff for implementing a function with master equation dynamics [^57fa10b0]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### Machine learning in spectral domain [^b6a6f35e]. Nature Communications (2021). High credibility.

In Fig. 6, we report the results of the tests performed when operating under the deep linear configuration. Symbols are analogous to those employed in Fig. 5. In all inspected cases, the entry layer is made of N 1 = 784 elements and the output one has N ℓ = 10 nodes. The first five points, from left to right, refer to a three layers (linear) neural network. Hence, ℓ = 3 and the size of the intermediate layer is progressively increased, N 2 = 20, 80, 100, 500, 800. The total number of trained eigenvalues is N 2 + N 3, and gets therefore larger as the size of the intermediate layer grows. The successive four points of the collections are obtained by setting ℓ = 4. Here, N 2 = 800 while N 3 is varied (=100, 200, 400, 600). The training impacts on N 2 + N 3 + N 4 parameters. Finally, the last point in each displayed curve is obtained by working with a five layers deep neural network, ℓ = 5. In particular, N 2 = 800, N 3 = 600 and N 4 = 500, for a total of N 2 + N 3 + N 4 + N 5 tunable parameters. Also in this case, the spectral algorithm performs better than conventional learning schemes constrained to operate with an identical number of free parameters. Similarly, the distribution of the weights of an equivalent perceptron trained in reciprocal space matches that obtained when operating in the space of the nodes and resting on a considerably larger number of training parameters. To sum up, eigenvalues are parameters of key importance for neural networks training, way more strategic than any other set of equivalent cardinality in the space of the nodes. As such, they allow for a global approach to the learning, with significant reflexes of fundamental and applied interest. In all cases here considered, the learning can extend to the eigenvectors: an optimised indentation of the eigen-directions contribute to enhance the overall performance of the trained device.

---

### An introduction to machine learning [^ad10d73c]. Clinical Pharmacology and Therapeutics (2020). Medium credibility.

DATA AND FEATURES

In ML, we deal with data and datasets. A dataset is composed of multiple data points (sometimes also called samples), where each data point represents an entity we want to analyze. Therefore, a data point can represent anything like a patient or a sample taken from a cancer tissue. Many of the issues related to data are universal and affect not only ML approaches but any quantitative discipline, including pharmacometrics.

To compile the dataset, one has measured and collected a number of features (i.e. data that describe properties of the data points). Those features can be categorical (predefined values of no particular order like male and female), ordinal (predefined values that have an intrinsic order to them like a disease stage), or numerical (e.g. real values). For a patient in a clinical setting, these could be (combinations of) the patient’s demographics, disease history, results of blood tests, or more complex and high dimensional measures, like gene expression profiles in a particular tissue or all single nucleotide polymorphisms that represent the patient’s unique genome.

---

### A general framework for optimizing arterial spin labeling MRI experiments [^5e2edb0e]. Magnetic Resonance in Medicine (2019). Medium credibility.

Figure 2 
Pseudocode outlining the PLD optimization algorithm used in this study. The optimal number of PLDs, N, can be found by running this algorithm for a range of N and finding the design which minimizes Equation 9

An ATT range ofs was assumed for GM in healthy volunteers. 4, 20 A uniform prior probability distribution for this range was used to ensure equal weighting of all ATTs. To avoid edge effects, the ATT distribution was extended on either side by 0.3 s with linearly decreasing probability. The CBF point prior was set to 50 mL/100 g/min. The readout duration was assumed to be 1.275 s to give realistic scan times. The total allowable scan time was set to 5 min. Variable TR was assumed such that a short PLD is acquired with a correspondingly short TR to minimize dead time in the sequence. 21 The number of averages in Equation 2 was calculated as:, where the floor function rounds down to the nearest integer. The noise variance used in the optimization and subsequent simulations was derived from preliminary in vivo data. All other model parameters are given in Table 1.

Table 1 
Parameters used for optimizations, simulations, and in vivo experiments

For this proof of principle study, the protocols were optimized so that the mean of Equation 9 across 5 slices was minimized. The number of slices was kept small to minimize the range of PLDs and BGS across slices. For a given slice, if all effective PLDs exceed the ATT then the FIM becomes severely ill‐conditioned because of a lack of ATT information. This is an inherent problem for CASL methods because the inflow of signal is more difficult to sample than for PASL. To avoid ill‐conditioned matrices, the ATT probability distribution was truncated for each slice based on the shortest attainable PLD. The sum in Equation 9 was then additionally weighted by the number of contributing slices. Analytical inversion of the FIM was performed to improve algorithm speed.

---

### Dextrose 5% [^9a9aba0e]. FDA (2025). Medium credibility.

SPECIFICATION

（1）50ml:2.5g（2）50ml:5g（3）100ml：5g（4）100ml:10g（5）250ml:12.5g（6）250ml:25g（7）500ml:25g（8）500ml:50g（9）1000ml:50g（10）1000ml:100g

---

### A continuous-time maxSAT solver with high analog performance [^d9759cd1]. Nature Communications (2018). Medium credibility.

Fig. 6 
Finding the Ramsey number R (4, 4). The E (κ) relationship for the 6-SAT problems corresponding to the K N complete graph colorings with two colors. E 0 is the extrapolated value based on the fit from Eq. (5) (dashed lines). The long vertical bars indicate the lower end of the fitting range. Note that for N = 16, 17, E 0 is a negative value indicating full colorability (the corresponding 6-SAT problem is fully satisfiable), whereas for N ≥ 18, E 0 > 0, and thus the 6-SAT problem becomes MaxSAT

Searching for the value of R (5, 5) one can relatively easily find coloring solutions without 5-cliques up to N = 35 for which the number of variables is 595 and the number of clauses 649,264, already huge for a 10-SAT problem for other types of SAT and MaxSAT solvers. To find solutions faster for N ≥ 36, however, we employ a strategy based on circulant matriceshelping us find solutions (proper colorings) up to and including N = 42 in a relatively short time (on the order of hours), see the description in the Methods. This approach places the trajectories relatively close to the solution and a proper coloring can be found in hours even for N = 42, (see Fig. 7a, b, and Supplementary Data 3 for an easily readable list of edge colorings), for which other heuristic algorithms take many days of computational time, even with the circulant matrix strategy. Applying the same strategy for N = 43 we did not find any complete coloring solutions, however, we did find a coloring that creates only two (out of= 962,598 possible) monochromatic 5-cliques, see Fig. 7c, d, and the specific coloring provided in Supplementary Data 4.

---

### Experimental noise cutoff boosts inferability of transcriptional networks in large-scale gene-deletion studies [^981a9ed7]. Nature Communications (2018). Medium credibility.

Case N 0 > 1

If more than one node are not perturbed we get from Eq. (11)Non-unique solutions of Eq. (15) can arise if a given fraction of the variance of the receiver node i can be explained by more than one sender node, for example, when a perturbed node j targets two nodes with index i and l. In this case it is unclear from the node activity data whether i is affected directly by j or indirectly through l, or by a combination of both routes. If node l is not perturbed or only weakly perturbed, a statistical criterion is needed to decide about inferability or identifyability of the link j → i, which can be computed numerically as follows: To find out whether j transmits a significant amount of information to i that is not passing through l, we first remove node j from the observable nodes of the network but keep its perturbative effect on other nodes in the data set. We then determine the link strengths A ′ for the remaining network of size N − 1. To construct a possible realisation of A ′ we set in Eq. (15) the non-zero values of Σ 0 to unity and use W = U to arrive at the expressionwith U ′ determined from the sample covariance matrix with the j -th column and j -th row removed. Fixing W and Σ 0 to seemingly arbitrary values does not affect the result we are after. If l is the only unperturbed node besides i, then in the A ′ system l can now be treated as perturbed—as it may receive perturbations from the unobserved node j —and thus Eq. (14) applies. If l is part of many unperturbed nodes that are affected by j, then the knowledge how much each of these nodes contributes to the variance of the target node i (which is determined by W and Σ 0) is irrelevant as we are only interested in the total effect of the alternative routes on node i. Using the inferred link strength from Eq. (16) we can rewrite Eq. (2) as a two-node residual inference problem between j and i, where we obtain a lower bound for link strength from node j to i by using the variation of i that could not be explained by A ′. This concept is similar to computing partial correlations. Defining by,andthe 2 × 2 analogues to the full problem we obtainwiththe covariance matrix of the vectorand,, using the scaled variances. Note that A ii < 0 for all i as these elements represent sufficiently strong restoring forces that ensure negative definiteness of A and that we havefrom Eq. (1) in the stationary case. An estimate for the minimum relative link strength from node j to node i can be calculated from Eq. (13) and is given byEq. (18) can be considered as an asymptotically unbiased response coefficient between node 1 as target node and node 2 as source node, as again any dependency on σ 2 has dropped out. An estimate for the maximum relative link strength from node j to node i follows from Eq. (18) with the off-diagonal elements of A ′ set to zero. We classify a link as non-inferable if there exists (i) a significant difference between the minimum und maximum estimated link strength and (ii) a minimum link strength that is not significantly different from noise.

---

### Experimental noise cutoff boosts inferability of transcriptional networks in large-scale gene-deletion studies [^061df17c]. Nature Communications (2018). Medium credibility.

To further improve the predictive power of our method we included the prior knowledge that transcriptional networks are highly sparse. Sparsity constraints are typically realised by penalising either the existence of links or the link strengths by adding appropriate cost functions, such as L 1 -norm regularised regression (Lasso). Adding a cost function to the main objective comes with the problem to trade-off the log-likelihood against the number of links in the network whose strength is allowed to be non-zero. In the absence of experimentally verified interactions there is no obvious way how to determine a suitable regularisation parameter that weights the likelihood against the cost function, which is one of the great weaknesses of such methods.

In our approach we reduce network complexity by assuming that functionally relevant information in molecular networks can only pass through nodes whose response to perturbations is significantly above the base line that is given by the variability among biological replicates. The individual noise levels can be estimated from natural variations among wild-type experimental replicates (Fig. 3a). The significance level that removes nodes from the network with low signal-to-noise ratio can be set to a desired false discovery rate. It can be shown that removal of noisy nodes imposes a sparsity constraint on the inference problem (Online Methods). The different steps required to arrive at a list of significant links are illustrated in Fig. 4a. In the first step, genes are grouped in clusters that are co-expressed under all perturbations. These clusters are treated as single network nodes in the subsequent steps. In the second step, only those samples are extracted from the data set that correspond to a perturbation of a chosen gene—the source node—with no other genes perturbed (node 5 in Fig. 4a). From this reduced data set, we identify all nodes in the network that change expression above a given significance level upon perturbing the source node. These significantly responding nodes define a subnetwork for each source node, which is typically much smaller in size than the complete network. In the third step, we collect all perturbation data from the complete data set for all nodes that are part of the subnetwork. Before inferring a direct interaction that points from the source node to a given target node in the subnetwork (green arrows in Fig. 4a), we remove all experiments from the data set where the target node is perturbed. The second and third steps essentially realise the counting procedure of inferable links as illustrated in Fig. 2a, with the difference that significant links are identified by PRCs in combination with residual bootstrapping over replicates (Online Methods, Supplementary Note 3). In the fourth step, we collect all clusters of co-expressed genes that contain exactly two nodes, with one of the nodes perturbed and check statistical significance of the directed link between them. In the fifth step, all significant links are collected in an edge list. We refer to these five steps as the clustering method. If we remove all links from the edge list that have more than one node in a source cluster or more than one node in a target cluster, we obtain an edge list that corresponds to links between single genes. This reduced edge list would also arise by skipping the clustering step and we refer to the remaining inference steps that compute links between single genes as subnetwork method.

---

### Six persistent research misconceptions [^f5bb0d85]. Journal of General Internal Medicine (2014). Low credibility.

Scientific knowledge changes rapidly, but the concepts and methods of the conduct of research change more slowly. To stimulate discussion of outmoded thinking regarding the conduct of research, I list six misconceptions about research that persist long after their flaws have become apparent. The misconceptions are: 1) There is a hierarchy of study designs; randomized trials provide the greatest validity, followed by cohort studies, with case-control studies being least reliable. 2) An essential element for valid generalization is that the study subjects constitute a representative sample of a target population. 3) If a term that denotes the product of two factors in a regression model is not statistically significant, then there is no biologic interaction between those factors. 4) When categorizing a continuous variable, a reasonable scheme for choosing category cut-points is to use percentile-defined boundaries, such as quartiles or quintiles of the distribution. 5) One should always report P values or confidence intervals that have been adjusted for multiple comparisons. 6) Significance testing is useful and important for the interpretation of data. These misconceptions have been perpetuated in journals, classrooms and textbooks. They persist because they represent intellectual shortcuts that avoid more thoughtful approaches to research problems. I hope that calling attention to these misconceptions will spark the debates needed to shelve these outmoded ideas for good.

---

### How to get better care with lower costs? See the person, not the patient [^8db91cf6]. Journal of the American Geriatrics Society (2016). Medium credibility.

Medicare spending and functional impairment in older adults—Among Medicare beneficiaries, "15% of Medicare beneficiaries who have chronic health conditions and functional impairment account for 32% of Medicare spending." Cost rises with functional limitations; for example, "average annual Medicare spending per person in 2006 with any chronic condition was $6,274, a figure that jumps to $15,383 when the same person also has a functional limitation," and the page notes the burden of cost is "roughly doubling or tripling or even more" with concurrent functional limitation.

---

### Foundations of the minimal clinically important difference for imaging [^72f785d9]. The Journal of Rheumatology (2001). Low credibility.

This article develops a generic conceptual framework for defining and validating the concept of minimal clinically important difference. We propose 3 approaches. The first uses statistical descriptions of the population ("distribution based"), the second relies on experts ("opinion based"), and a third is based on sequential hypothesis formation and testing ("predictive/data driven based"). The first 2 approaches serve as proxies for the third, which is an experimentally driven approach, asking such questions as "What carries the least penalty?" or "What imparts the greatest gain?" As an experimental approach, it has the expected drawbacks, including the need for greater resources, and the need to tolerate trial and error en route, compared to the other 2 models.

---

### Validating whole slide imaging for diagnostic purposes in pathology: guideline from the college of American pathologists pathology and laboratory quality center [^ddcb0ec9]. Archives of Pathology & Laboratory Medicine (2013). Medium credibility.

Supplemental Table 12—Evidence to Decision Summary of Recommendation 3 reports response distributions as follows: For “Is the problem a priority?”, Yes was 70% (7/10) with 20% (2/10) Probably No, 10% (1/10) Probably Yes, and 0 for No, Varies, and Don’t Know. For “How accurate is the test?”, Very Inaccurate 0, Inaccurate 10% (1/10), Accurate 30% (3/10), Very Accurate 30% (3/10), Varies 20% (2/10), and Don’t Know 10% (1/10). For “How substantial are the desirable anticipated effects?”, Trivial 0, Small 0, Moderate 20% (2/10), Large 40% (4/10), Varies 20% (2/10), and Don’t Know 20% (2/10). For “What is the overall certainty of the evidence of test accuracy?”, Very Low 10% (1/10), Low 10% (1/10), Moderate 20% (2/10), High 40% (4/10), Very High 20% (2/10), and No Included Studies 0. For “What is the overall certainty of the evidence of effects of the management that is guided by the test results?”, Very Low 10% (1/10), Low 20% (2/10), Moderate 30% (3/10), High 30% (3/10), Very High 10% (1/10), and No Included Studies 0. For “How certain is the link between test results and management decisions?”, Very Low 10% (1/10), Low 20% (2/10), Moderate 10% (1/10), High 40% (4/10), Very High 10% (1/10), and No Included Studies 10% (1/10). For “What is the overall certainty of the evidence of effects of the test?”, Very Low 10% (1/10), Low 0, Moderate 40% (4/10), High 40% (4/10), Very High 10% (1/10), and No Included Studies 0. For “What is the overall certainty of the evidence for any critical or important direct benefits, adverse effects or burden of the test?”, Very Low 10% (1/10), Low 20% (2/10), Moderate 30% (3/10), High 40% (4/10), Very High 0, and No Included Studies 0.

---

### Three learning stages and accuracy-efficiency tradeoff of restricted boltzmann machines [^87550a89]. Nature Communications (2022). High credibility.

In practical applications, one does not have access to p (x), but only to a collection of samples(training and/or test data). The empirical counterpart of Δ θ for such a dataset S issee also below Eq. (4). The critical part is again the partition function Z θ. Due to the aforementioned factorization (cf. Eq. (12)), evaluating (13) remains feasible as long as the number of hidden units N is sufficiently small, even if M is large. Similarly, for small N, we can draw independent samples from, without reverting to Markov chains and Gibbs sampling: We first generate independent samplesof the hidden units, using the fact thatremains accessible for small N. Subsequently, we sample configurations of the visible units using. This scheme was utilized to obtain the model test samplesfor the N ≤ 32 examples in Fig. 4. For the examples with N > 32, the samples inwere instead generated via Gibbs sampling according to (5), using 10 parallel chains and storing every τ θ -th sample after 2 × 10 6 burn-in steps.

The accuracy measuresandinvolve empirical distributions of coarse-grained visible-unit samples. These reduced samples are obtained by using a weighted majority rule: For a partition { L 1, …, L L } of the visible-unit indices {1, …, M } and a threshold r ∈ [0, 1], we defineFor every samplein a given multiset S, the associated coarse-grained sample iswith.

---

### Summary benchmarks-full set – 2024 [^c9abe316]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines—GRADE evidence quality ratings are defined for forming recommendations for care as follows: “Good quality (GQ): Further research is very unlikely to change our confidence in the estimate of effect,” “Moderate quality (MQ): Further research is likely to have an important impact on our confidence in the estimate of effect and may change the estimate,” and “Insufficient quality (IQ): Further research is very likely to have an important impact on our confidence in the estimate of effect and is likely to change the estimate; any estimate of effect is very uncertain.”

---

### Separability and geometry of object manifolds in deep neural networks [^2028230a]. Nature Communications (2020). High credibility.

Measuring capacity numerically from samples

Classification capacity can be measured numerically by directly performing linear classification of the manifolds. Consider a population of N neurons which represents P objects through their population responses to samples of those objects. Assuming the objects are linearly separable using the entire population of N neurons, we seek the typical sub-population size n where those P objects are no longer separable. For a given sub-population size n we first project the N dimensional response to the lower dimension n using random projections; using sub-sampling rather than random projections provide very similar results but breaks down for very sparse population responses (Supplementary Fig. 16). Second, we estimate the fraction of linearly separable dichotomies by randomly sampling binary labels for the object manifolds and checking if the sub-population data is linearly separable using those labels. Testing for linearly separability of manifold can be done using regular optimization procedures (i.e. using quadratic optimization), or using efficient algorithms developed specifically for the task of manifold classification. As n increase the fraction of separable dichotomies goes from 0 to 1 and we numerically measure classification capacity as α c = P ∕ n c where the fraction surpasses 50%; a binary search for the exact value n c is used to find this transition. The full procedure is described in Supplementary Methods 2.2.

When numerically measuring the capacity of balls with geometric properties derived from the data (as in Fig. 10 d) the centers of the balls (and thus their center correlation structure) is taken from the data, as well as the direction of the manifold axes. The number of axes is set by rounding D M to the nearest integer and manifold radius is R M in units of the corresponding center norm. Then a specialized method the for measuring linear separability of balls is used.

---

### Transient power-law behaviour following induction distinguishes between competing models of stochastic gene expression [^6513fe13]. Nature Communications (2025). High credibility.

What features of transcription can be learnt by fitting mathematical models of gene expression to mRNA count data? Given a suite of models, fitting to data selects an optimal one, thus identifying a probable transcriptional mechanism. Whilst attractive, the utility of this methodology remains unclear. Here, we sample steady-state, single-cell mRNA count distributions from parameters in the physiological range, and show they cannot be used to confidently estimate the number of inactive gene states, i.e. the number of rate-limiting steps in transcriptional initiation. Distributions from over 99% of the parameter space generated using models with 2, 3, or 4 inactive states can be well fit by one with a single inactive state. However, we show that for many minutes following induction, eukaryotic cells show an increase in the mean mRNA count that obeys a power law whose exponent equals the sum of the number of states visited from the initial inactive to the active state and the number of rate-limiting post-transcriptional processing steps. Our study shows that estimation of the exponent from eukaryotic data can be sufficient to determine a lower bound on the total number of regulatory steps in transcription initiation, splicing, and nuclear export.

---

### Using GNN property predictors as molecule generators [^2bb7babf]. Nature Communications (2025). High credibility.

Methods

Training of the property predictors

Since optimizing the inputs (generating molecules) will require an explicit representation of the adjacency matrix, sparse matrices and lists of edge indices cannot be used during training of the property predictor GNN. Mini-batching using a single adjacency matrix per batch thus uses a large amount of memory that scales quadratically with the total number of atoms in a batch. To avoid that issue, we use a fixed graph size (N) instead which allows us to store the adjacency matrices as a B × N × N array where B is the batch size. Empty rows of the adjacency matrix are associated with atoms of type “no-atom” in the feature vector one-hot encoding. This allows for molecules of any size smaller than N.

Although sloped rounding and maximum functions guarantee that the adjacency matrix and the feature vector are populated with near-integer values, these values are not exactly integers and these variations around integer values can have a significant impact on the property predicated by the GNN. To make the GNN less sensitive to these variations, we add a small amount of random noise around integer values of A and F during training.

For the energy gap task, we used a simple graph convolutional network (GCN) with an adjustable number of convolutions, a pooling layer and an adjustable number of linear layers. We trained this model on the QM9 dataset containing approximately 130000 small molecules for which the energy gap was calculated with density functional theory (DFT). With manual hyperparameter tuning, we obtain a mean absolute error of about 0.12 eV on the test set which is on par with other models using the same amount of information (2D graphs, no coordinates). More details on this model can be found in the SI.

---

### Laboratory perspectives in the development of polygenic risk scores for disease: a points to consider statement of the American College of Medical Genetics and genomics (ACMG) [^50389303]. Genetics in Medicine (2023). High credibility.

Bioinformatic and statistical algorithms for PRS—early models “focused on sites whose disease association passed a threshold for genome-wide significance, often limiting the PRS inputs to few sites, which may be more easily and directly genotyped,” whereas more recent models “incorporate information from hundreds of thousands to millions of sites across the genome,” and “require either genome sequence or imputation to determine variant status.”

---

### How I read an article that uses machine learning methods [^0f2f5676]. Blood Advances (2023). Medium credibility.

Step 1: Understand the problem being addressed. The first step in reading an ML paper is to understand the problem that the authors are trying to solve and, more importantly, understand the clinical or scientific impact of solving this problem.In other words, if the aim of the study is to solve a clinical problem, how does the answer or the recommendation provided by the algorithm help physicians or researchers in their day-to-day practice, and is this solution mature enough to be implemented in clinical workflows? Major clinical problems in health care can mainly affect either patient outcomes or operations (can I make the process easier and faster for the patient and the health care system?).

Step 2: Assess the quality of the data. The quality of the data used to build the ML model is crucial for the validity of the results. Following are some questions that can be used to evaluate the data:
1 Sample size: Is the size of the training, validation, and test sets enough to build a reproducible and generalizable ML model? Is this size of the data appropriate for the chosen methods (ie, some methods are “data-hungry” and understanding which methods require larger datasets is key)? However, different algorithms require different data types (image, tabular, text, or others) and sizes, and there are no rules of thumb or formulas that can estimate the perfect data.
2 Relevance: Are the data appropriate and relevant to the problem that the model is trying to solve?
3 Accuracy: How are the data collected and annotated (human vs natural language process). How are the data transformed to make it ready for ML use, etc.
4 Consistency: Are the data consistent? Do they have any missing values and how the authors dealt with this?
5 Representativeness: The data should be representative of the population being studied.
6 Balance: The data should be balanced, with roughly equal representation of all relevant classes or groups. However, most health care data are unbalanced. It is critical to understand how the authors dealt with unbalanced data.
7 Bias: To evaluate bias in data, it is important to look at the distribution of certain characteristics, such as race, gender, or socioeconomic status, among the samples in the data set,and how the data were collected. This will help to identify any disparities or overrepresentation of certain groups, which can indicate the presence of bias in the data. It is critical to evaluate bias at this stage because if this is not addressed properly, it could produce a biased model.

---

### The clinical application of polygenic risk scores: a points to consider statement of the American College of Medical Genetics and genomics (ACMG) [^e6fe44ea]. Genetics in Medicine (2023). High credibility.

Polygenic risk scores (PRS) interpretation—low scores and absolute risk is summarized as follows: A low PRS does not rule out significant risk for the disease or condition in question. PRS outputs usually reflect relative ranking rather than absolute lifetime risk, because PRS results are typically given in the unit of the effect size and can be converted to a score between the first to 99th percentiles; a low percentile score reflects lower predicted risk versus a reference population and does not typically report the absolute lifetime risk. Potential sources of unidentified risk and methodological limitations also constrain PRS, absolute risk from the original genome-wide association studies (GWAS) or reference population is not always available, and area under the curve may not be as high as lifestyle-based prediction models, although some studies have shown PRS may be at least comparable or stronger risk predictors than traditional tools.

---

### How to avoid common problems when using clinicalTrials.gov in research: 10 issues to consider [^7380ceb5]. BMJ (2018). Excellent credibility.

Dos and don’ts of using ClinicalTrials.gov data for research

1 Don’t assume that studies registered on ClinicalTrials.gov are an unbiased reflection of the clinical research enterprise

Studies reported to ClinicalTrials.gov are an incomplete sample of the clinical research enterprise. The degree of bias introduced by a sample of records varies according to the strength of the incentives and norms for reporting studies with certain characteristics. Key factors include intervention type (eg, drugs versus behavioural interventions), funding source, date of study initiation or completion, geographical location, and regulatory jurisdiction.
Researchers conducting analyses aimed at showing differences or changes in the clinical research enterprise need to consider carefully how reliance on the samples of studies available from ClinicalTrials.gov might introduce biases in the analyses.

2 Don’t use the wrong data elements to operationally define a sample

Many concepts used in analyses of the clinical research enterprise can be addressed by several different data elements in ClinicalTrials.gov. The choice of data elements will affect the meaning and the interpretation of an analysis—sometimes substantially.
Before establishing an operational definition for a concept, researchers should fully review the definitions of the data elements, and understand the consequences of all possible options; examples of several common concepts are shown in table 3.

Table 3 
Common concepts and implications of selecting different data elements for a concept

3 Think carefully when selecting an analysis population

Many analyses can be summarised as ratios using the number of studies with a certain feature as the numerator, and the overall number of studies of interest (analysis population) as the denominator. The selection of an analysis population influences the validity of any conclusions and their relevance to the research question. For example, when determining the percentage of studies that have results posted on ClinicalTrials.gov, choosing all registered studies as the analysis population provides a methodologically straightforward finding but does not account for important factors. Selection of all studies completed since the start of the results database, or those legally required to report results to ClinicalTrials.gov as the analysis population (and comparable numerators) would estimate the usefulness of the results database in providing summary results and compliance with the law, respectively.
The choice of analysis population is also important when using a data element that is missing from many studies (eg, a new or optional data element). A denominator of all studies versus a denominator of only those studies that have a value entered for that data element will provide different results.

---

### Impact of hierarchical water dipole orderings on the dynamics of aqueous salt solutions [^4ec7ef24]. Nature Communications (2023). High credibility.

Ion solvation kinetics determined by bond-orientational ordering

Furthermore, for d < λ HB (q), we find a nontrivial non-monotonic dependence of the dynamic properties on q and d. The ultralong residence time in specific regions of the q − d plane (Fig. 2 a) suggests the formation of ultrastable hydration shells. Figure 3 a shows the coordination number n for our model ions on the q − d plane, which is well supported by experimental data for main-group ions (see Supplementary Table 1 for the reference). The hydration shell stability can be described by the susceptibility of n to a small change in d :. In Fig. 3 a, b, we can see that the ultrastable regions (χ ≃ 0) have a one-to-one correspondence to the plateaus of the composite coordination number, i.e. n = 4, 6, 8, 9, 10, and 12, whereas the hydration structure of prime n is unstable or even hard to form (large χ). An ultrastable hydration shell with composite n is featured by a sharp first peak and a deep first minimum in g (r), reflecting high translational order (Fig. 3 c and Supplementary Figs. 5 b and 11 b). Moreover, we find that in the n -plateau regions, the characteristic oxygen-ion-oxygen angle ϕ ion takes specific values that precisely follow the solution ϕ T of the Thomson problem, explaining the results of scattering experiments and ab initio MD simulations (Fig. 3 e). Such a composite-number effect has originally been reported in the Thomson problem that considers the most stable configuration of n point charges on a sphere with an opposite charge. For example, Glasser and Every reported that in the range of n ⩽ 30, configurations with prime n show instability, whereas those with composite n are stable. The similarity between the water arrangement in the hydration shell and the Thomson problem for the electron arrangement in a classical atom can be understood by the similarity between dipolar repulsion between water molecules constrained radially and translationally in the hydration shell for d < λ HB (Fig. 2 e) and the electrostatic repulsion between electrons translationally constrained on a spherical surface. Although the underlying selection rule for the stability of electron configurations in the Thomson problem has remained unclear, it has been suggested that the composite n that permits high bond-orientational order gives rise to high stability.

---

### Application of transcriptional gene modules to analysis of' gene expression data [^62a65936]. G3 (2020). Medium credibility.

To enable functional evaluation of the modules, it is useful to summarize them as discrete gene sets. To this end, we partitioned each column of the S matrix into three sets of genes: one set consisting of genes excluded from the module, and two other sets consisting of genes implied to be regulated in opposite directions. We refer to these latter two sets as “hemi-modules” “a” and “b”, one set consisting of genes with highly positive weights and the other consisting of genes with highly negative weights (signs assigned based on skewness) in the independent component. Thus, module genes regulated in one direction (up or down) are part of one hemi-module and genes regulated in the opposite direction (down or up) are part of the other hemi-module. The “a” hemi-module is derived from the more highly skewed side of the independent component, and is usually larger (i.e. contains more genes). While others have used a fixed-threshold approach to component partitioning (;;); for example, defining genes with weights exceeding +/− 3 standard deviations from the component mean to be “in” each hemi-module, we found that different individual modules showed maximum annotation enrichments at different thresholds, suggesting that a ‘one-size-fits-all’ approach to partitioning was sub-optimal. An alternative approach to partitioning that we attempted (described in) failed to converge in many cases (data not shown). Therefore, to increase partitioning accuracy, we trained a function to predict partitioning thresholds from the shape of component distributions. Because plots of training data revealed a complex solution surface, we decided to use an artificial neural network (ANN) to predict partitioning thresholds for each component from the skewness and kurtosis of its distribution. The output of the ANN, i.e. predicted partitioning thresholds, is shown in Figure S2a. Although using a fixed-threshold approach to module partitioning produced similar results qualitatively (Figures S2b-d), it resulted in fewer significant annotations across the range of parameters tested than did ANN-based partitioning (P <2.2E-16, Figure S2e). Because the mean optimum number of extracted components (dashed vertical lines in Figure 2a-c and Figures S2b-d) was similar for both threshold and ANN partitioning (209, and 209.33, respectively), we chose 209 as the final number of components to extract from the compendium.

---

### Exploiting redundancy in large materials datasets for efficient machine learning with less data [^ba2f4c45]. Nature Communications (2023). High credibility.

Extensive efforts to gather materials data have largely overlooked potential data redundancy. In this study, we present evidence of a significant degree of redundancy across multiple large datasets for various material properties, by revealing that up to 95% of data can be safely removed from machine learning training with little impact on in-distribution prediction performance. The redundant data is related to over-represented material types and does not mitigate the severe performance degradation on out-of-distribution samples. In addition, we show that uncertainty-based active learning algorithms can construct much smaller but equally informative datasets. We discuss the effectiveness of informative data in improving prediction performance and robustness and provide insights into efficient data acquisition and machine learning training. This work challenges the "bigger is better" mentality and calls for attention to the information richness of materials data rather than a narrow emphasis on data volume.

---

### A primer to gene therapy: progress, prospects, and problems [^4faec44d]. Journal of Inherited Metabolic Disease (2021). Medium credibility.

1 RECOMBINANT RETROVIRUSES FOR THE STABLE GENETIC MODIFICATION OF HUMAN CELLS

Research findings on virology and genetics provided biomedical scientists with an extensive toolbox that has permitted to identify and characterize genes and gene mutations that are causative of specific diseases. In the early 1970s, it was realized that it might be feasible to develop a dedicated gene transfer toolbox for ferrying functional genes into patient cells to remedy the consequences of genetic diseases. 1 This was and so remains the main concept of gene therapy. The identification of genes linked to human disorders and their smaller coding sequences started to become available in rapid succession from the 1980s onwards owing to steep developments in recombinant DNA techniques. 2 In parallel, these developments also contributed decisively to the making of efficient gene transfer vehicles that were primarily generated for stable genetic modification of human cells. Initially these vectors were mostly derived from retroviruses. 3 Simple and complex retroviruses, such as the gamma‐retrovirus named Moloney murine leukemia virus (M‐MLV) and the lentivirus named human immunodeficiency virus type 1 (HIV‐1), respectively, are particularly attractive for gene therapy in that, once engineered as vectors, they stably integrate their transgenic cargo into host cell chromosomes and, in doing so, provide a basis for long‐term therapeutic gene expression (Figure 1). During the process of generating a retroviral vector, all viral protein‐coding sequences are removed from the retrovirus genome and replaced by the coding sequence of a gene‐of‐interest (transgene). In addition to the transgene, the retroviral vector genome contains viral non‐coding sequences (cis ‐acting elements) necessary for its replication and encapsidation into newly formed enveloped vector particles (Figure 2). These elements are the viral long terminal repeats (LTRs) and packaging signal (Ψ), respectively. Functional gamma‐retroviral vectors are made using dedicated packaging cell lines (Figure 2). These so‐called helper packaging cell lines provide in trans the products encoded by the genes that were deleted from the parental virus. The structural and some catalytic retroviral proteins assemble, together with viral vector genome transcripts (two RNA genomic copies per particle), to form fully mature vector particles that carry the transgene sequence and, importantly, no viral genes. Such viral vector particles are therefore capable of only a single round of infection (transduction) once target cells are exposed to them. Once inside the cell, the retroviral vector genome transcripts are reverse transcribed into double‐stranded complementary DNA (cDNA) copies that ultimately integrate into target‐cell chromosomes. The reverse transcription and chromosomal integration steps leading to the stable genetic modification of transduced cells are catalyzed by the vector particle‐associated reverse transcriptase and integrase proteins, respectively (Figure 1). Retroviral vectors (both simple and complex) are, therefore, particularly suitable for modifying stem cells since the integrated viral vector genome is not only stably maintained in the stem cells, but can also be passed on to all their daughter cells.

---

### An official American Thoracic Society research statement: current challenges facing research and therapeutic advances in airway remodeling [^920847d8]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Airway remodeling (AR) research barriers and solutions—Table 2—details methodological obstacles such as “Lack of consensus regarding physiologically or clinically relevant features/aspects of AR” and “Difficulties in accessing or measuring critical indices,” with approaches including “Agreement on definitions and indices for AR,” “Assessment of importance of indices in asthma pathophysiology and symptoms (and, conversely, responses to therapy),” and “Identification of biomarkers or indicators of AR that correlate with disease phenotype, severity, responsiveness to therapy, and other relevant factors.” Modeling gaps are addressed by “Identification of preclinical models that most closely approximate the human condition for both AHR and AR.” For the “Costs and logistics given longitudinal and slow-developing nature of AR,” proposed actions include “Attention to balance between reasonable timeframes for AR development or intervention in preclinical models vs. much slower AR development in humans” and “Exploration of AR prevention/prophylaxis in asthma, thus identifying “at-risk” populations (aided by research using genetic and phenotypic markers), overall reducing timeframe of studies.” To improve funding prospects, the table notes “Perceived limited impact of AR research and associated difficulties and increased competitiveness for funding” and calls for “Education, education, education, regarding importance of AR and the many limitations that render AR research and advancement of AR therapeutics difficult.” Economic/regulatory obstacles—“Jaundiced perspective of regulatory bodies (lack of FDA regulatory indicators for asthma medications that include changes in airway histopathology)” and “Difficulties of proposing deliverables with any anti-AR drug leading to limited interest by pharmaceutical industry (lack of regulatory indications, proof of concept, and unclear commercialization strategy)”—are paired with solutions to “Encourage preclinical and clinical studies establishing that AR features cause or exacerbate regulatory indicators of asthma (airflow obstruction, AHR, resolution of inflammation, and inflammatory markers)” and “Establish regulatory precedent that includes AR features,” plus to “Assess drugs targeting AR in the context of current regulatory indicators, given the history of difficulty in establishing a new regulatory indicator for most diseases” and to “Initiate and maintain dialogue among investigators, funding agencies, and regulatory agencies with frequent informational meetings, preferably at large scientific/clinical gatherings, to provide updates on research and drug development in AR,” citing examples including “the NIH Small Business Innovation Research and Small Business Technology Transfer grants, R33 and other grants for drug repurposing, and larger clinical/translational grants and contracts.”

---

### Translating metastasis-related biomarkers to the clinic – progress and pitfalls [^904df733]. Nature Reviews: Clinical Oncology (2013). Medium credibility.

In the context of metastatic disease, preclinical models have been used primarily to decipher different steps of the metastatic cascade. Numerous molecular processes operate in these model systems, but none of these has been successfully translated to the clinic. We discuss some of the successes and failures of preclinical models in metastasis research and suggest some of the clues for more clinically relevant research. These potential avenues of research include: the use of adequate statistical methods and well-annotated cohorts in biomarker discovery; an objective assessment of the level of evidence provided by each biomarker; the development of robust molecular or cellular surrogates of metastasis in patients; and original designs for clinical trials.

---

### Evaluating molecular biomarkers for the early detection of lung cancer: when is a biomarker ready for clinical use? An official American Thoracic Society policy statement [^9255594f]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

American Thoracic Society policy—reporting requirements across biomarker evaluation phases: For clinical validation, report “The sensitivity and specificity of a technically validated biomarker, with fixed interpretation of the biomarker result, applied to the intended use population, as compared with the reference standard for the clinical application,” “The clinical features of the cancer and control groups in clinical validation studies of the biomarker compared with the intended use population,” “Biomarker results for relevant clinical subgroups,” and “Biomarker performance compared with and combined with clinical calculators, standard practice, and/or clinician judgment.” For clinical utility, report “The frequency with which the biomarker result impacts a clinical decision” and “The impact (benefit and harm) of patient management decisions on patient outcomes when the biomarker is used.” For cost-effectiveness, report “The cost-effectiveness of the biomarker compared with the currently accepted standards for the clinical application.” Regarding economic evaluation, “It is important that cost-effectiveness analysis be performed and reported within each clinical application” and “Ideally, a third-party independent analysis would be performed to avoid the potential for bias,” while “The steering committee members decided that defining a threshold of cost-effectiveness that is acceptable to society is beyond the scope of this statement.” The committee asked, “Should we include guidance about the components of study design and details of study results that should be reported for all phases of biomarker development?” and “agreed that we should suggest components of the study design and details of the study results that should be reported for the phases of biomarker development that most directly influence the interpretation of clinical utility. This includes clinical validation, clinical utility, and cost-effectiveness (Table 3).”

---

### Early and locally advanced non-small-cell lung cancer (NSCLC): ESMO clinical practice guidelines for diagnosis, treatment and follow-up [^ce624940]. Annals of Oncology (2017). Medium credibility.

Regarding classification and risk stratification for non-small cell lung cancer, more specifically with respect to staging, ESMO 2017 guidelines recommend to use the size of the invasive component of part-solid tumors to assign the T category for clinical staging. Evaluate two lung lesions fulfilling the criteria for two primaries accordingly.

---

### Diagnosis, management and treatment of the Alport syndrome-2024 guideline on behalf of ERKNet, ERA and ESPN [^45eede36]. Nephrology, Dialysis, Transplantation (2025). High credibility.

Regarding diagnostic investigations for Alport syndrome, more specifically with respect to hearing assessment, ERA/ERN ERKNet/ESPN 2025 guidelines recommend to consider obtaining a hearing evaluation at diagnosis or upon reaching adulthood in female patients with X-linked alport syndrome, and then every 5 years in the absence of hearing loss symptoms.

---

### NCCN guidelines® insights: breast cancer, version 5.2025 [^edae72db]. Journal of the National Comprehensive Cancer Network (2025). High credibility.

Category notation for these recommendations: “Note: All recommendations are category 2A unless otherwise indicated.”

---

### 2014 ESC guidelines on diagnosis and management of hypertrophic cardiomyopathy: the task force for the diagnosis and management of hypertrophic cardiomyopathy of the European Society of Cardiology (ESC) [^11206c8f]. European Heart Journal (2014). Medium credibility.

Regarding diagnostic investigations for syncope, more specifically with respect to initial ECG, ESC 2014 guidelines recommend to obtain a 12-lead ECG to identify the cause of symptoms in patients with unexplained syncope.

---

### How can clinicians, specialty societies and others evaluate and improve the quality of apps for patient use? [^e5852707]. BMC Medicine (2018). Low credibility.

A common-sense app evaluation framework

The next stage is to test the accuracy of any advice or risks calculated. The methods are well established for decision support systems, predictive models and more generally. To summarise, investigators need to:
Define the exact question; for example, “how accurately does the app predict stroke risk in people with cardiovascular disease aged 60–85?”
Assemble a sufficiently large, representative test set of patients who meet the inclusion criteria, including the ‘gold standard’ for each. This gold standard can be based on follow-up data or on expert consensus for questions about the appropriateness of advice, using the Delphi technique.
Enter the data (ideally, recruit typical app users for this), recording the app’s output and any problems; for example, cases in which the app is unable to produce an answer.
Compare the app’s results against the gold standard using two-by-two tables, receiver operating characteristic (ROC) curves and a calibration curve to measure the accuracy of any probability statements. For details of these methods, see Friedman and Wyatt.

Assuming accurate results in laboratory tests, the next question is: “does the app influence users’ decisions in a helpful way?” This is important because poor wording of advice or presentation of risk, inconsistent data entry, or variable results when used offline may reduce its utility in practice. To answer this question, we can use the same test data but instead examine how the app’s output influences simulated decisions in a within-participant before/after experiment. Here, members of a group of typical users review each scenario and record their decisions without the app, then enter the scenario data into the app and record their decision after consulting it. This low cost study design is faster than a randomised clinical trial (RCT) and estimates the likely impact of the app on users’ decisions if they use it routinely. It also allows us to estimate the size of any ‘automation bias’; i.e. the increase in error rate caused by users mistakenly following incorrect app advice when they would have made the correct decision without it.

The most rigorous app evaluation is an RCT of the app’s impact on real (as opposed to simulated) user decisions and on the health problem it is intended to alleviate. Some app developers complain that they lack the funds or that their software changes too frequently to allow an RCT to be conducted. However, at least 57 app RCTs have been conducted and there are variant RCT designs that may be more efficient.

---

### Evaluation after a first seizure in adults [^b94f7748]. American Family Physician (2022). High credibility.

Regarding diagnostic investigations for first seizure in adults, more specifically with respect to initial assessment, AAFP 2022 guidelines recommend to evaluate for provoking factors after a first seizure, such as inflammatory, infectious, structural, toxic, or metabolic causes.

---

### Massive computational acceleration by using neural networks to emulate mechanism-based biological models [^2c01f625]. Nature Communications (2019). High credibility.

To identify the minimum size of dataset needed for accurately making predictions, we trained deep LSTM network on different training dataset sizes. The RMSEs are calculated based on predictions of a fixed test dataset, which contains 20,000 samples. Figure 2c demonstrates how the RMSEs of distributional data and peak values decrease with the increase of training data size. Since the x -axis is log-scaled, when the dataset size is beyond 10 4, the rate of error reduction becomes asymptotically smaller. When the data size is 10 5, the RMSE value decreased below a preset threshold (0.3), when we deemed a prediction to be accurate. In general, depending on specific models and the acceptable tolerance of errors, the threshold can be set differently, which could require different data sizes for effective training. This training dataset size is manageable and results in sufficient accuracy for our analysis. Based on error tolerance and numerical data generation efficiency, one can choose the desired dataset size for training the neural network. With an ensemble of deep neural networks, which will be described in the next section, the errors can be further reduced without increasing the dataset size.

---

### Opportunities and challenges associated with clinical diagnostic genome sequencing: a report of the Association for Molecular Pathology [^e8738f75]. The Journal of Molecular Diagnostics (2012). Medium credibility.

Clinical diagnostic genome sequencing—platform-specific error profiles and data representations require algorithmic handling: platforms may have insertion or deletion errors during homopolymer runs or increased sequence errors toward the end of the read; representations can be in color space (AA, AC, CC, GG, or TT) versus base space (A, C, G, T), and these differences must be addressed at the algorithmic level because some sequence errors are correctable.

---

### Incorporating economic evidence in clinical guidelines: a framework from the clinical guidelines committee of the American College of Physicians [^4483eea4]. Annals of Internal Medicine (2025). High credibility.

Figure 1—comparative value of interventions—categorizes value by clinical net benefit and incremental cost for quality-adjusted life-year (QALY) with intervention versus comparator, where the incremental cost dimension reflects the incremental cost-effectiveness ratio per QALY gained. For favorable clinical net benefit, higher cost corresponds to high value (<$100 000), intermediate value ($100 000 to $200 000), or low value (>$200 000) with the intervention more effective and more costly; lower cost corresponds to high value with the intervention more effective and less costly (cost-saving). For unfavorable clinical net benefit, higher cost indicates no value with the intervention less effective and more costly (cost-dominated), and lower cost indicates no value with the intervention less effective and less costly.

---

### How to avoid common problems when using clinicalTrials.gov in research: 10 issues to consider [^ffbcd9dd]. BMJ (2018). Excellent credibility.

8 ClinicalTrials.gov data can be accessed in several ways

There are three ways to identify and retrieve a set of study records directly from ClinicalTrials.gov:

Basic fielded search: specify term(s) by condition/disease, facility location, or other terms (fig S4).
Advanced search: enter search terms in any of over 20 structured fields, including study type (eg, “interventional”, “observational”), recruitment status (eg, “recruiting”, “completed”), intervention/treatment, and has posted results (fig S5).
Download the search results: registration and results information from study records can be downloaded in a spreadsheet format, or as a single zip file containing the study records formatted using XML that researchers can later search with their own system and tools (fig S6).

Alternatively, researchers may prefer to use the Clinical Trials Transformation Initiative’s database for aggregate analysis of ClinicalTrials.gov.This database contains the full set of registration and results information from ClinicalTrials.gov; researchers should note the update schedule of the database to determine its appropriateness for their particular analysis.

9 Defining a sample of records to answer a specific question

Sample selection is integrally related to the analytical goals, and any intended inferences. Many analyses rely on ClinicalTrials.gov data to reach conclusions about the practices and characteristics of the clinical research enterprise, but the degree to which a sample of ClinicalTrials.gov records accurately represents the full population of studies within the clinical research enterprise varies by the prevailing reporting incentives (eg, by geographical region, over time, and the type and characteristics of the study). For example, a recent press release announced that “Korea ranks 6th in world in number of clinical trials” based on information from ClinicalTrials.gov.The analysis apparently did not account for the fact that ClinicalTrials.gov does not include all trials (issue 7), incentives for reporting by geographical location vary and change over time (issue 3), or the existence of a primary registry in the WHO ICPTR registry network “for clinical trials (researches) to be conducted in Korea”.

In contrast, an investigation of publication rates among NIH funded trials used a sample of ClinicalTrials.gov records for the analysis.Given that multiple reporting incentives apply to such trials (see issue 3), including FDAAA, ICMJE policy, and NIH’s recommendation that grantees register their trials, the evidence suggested that the sample from ClinicalTrials.gov was largely representative of NIH funded trials.

---

### Opportunities and challenges associated with clinical diagnostic genome sequencing: a report of the Association for Molecular Pathology [^b1741e3d]. The Journal of Molecular Diagnostics (2012). Medium credibility.

Common complex diseases—genome-wide association studies (GWAS) and single-nucleotide polymorphism (SNP) catalogs highlight scale and limits: 11 million SNPs and three million indels have been characterized, dense SNP chips have facilitated GWAS with many cases and controls (>1000 per group), and genome.gov catalogues >1200 polymorphisms that confer disease risk in >165 common human diseases and traits, yet multiple risk alleles only explain approximately 5% of the variance in the population. At the individual level, the uncovering of three million SNPs within a genome requires interpretive knowledge, and the full potential can only be realized when the genome sequences of thousands of individuals are available and linked to corresponding phenotypes; data mining initiatives such as the U.K. 10,000 Genomes Project may refine disease pathways and clarify how specific alleles present as protective or deleterious.

---

### Gene expression data analysis [^8f3a0bd1]. Microbes and Infection (2001). Low credibility.

Microarrays are one of the latest breakthroughs in experimental molecular biology, which allow monitoring of gene expression for tens of thousands of genes in parallel and are already producing huge amounts of valuable data. Analysis and handling of such data is becoming one of the major bottlenecks in the utilization of the technology. The raw microarray data are images, which have to be transformed into gene expression matrices, tables where rows represent genes, columns represent various samples such as tissues or experimental conditions, and numbers in each cell characterize the expression level of the particular gene in the particular sample. These matrices have to be analyzed further if any knowledge about the underlying biological processes is to be extracted. In this paper we concentrate on discussing bioinformatics methods used for such analysis. We briefly discuss supervised and unsupervised data analysis and its applications, such as predicting gene function classes and cancer classification as well as some possible future directions.

---

### Clinical genetic counseling and translation considerations for polygenic scores in personalized risk assessments: a practice resource from the National Society of Genetic Counselors [^28d8464d]. Journal of Genetic Counseling (2023). High credibility.

Polygenic score (PGS) development and validation for clinical use—Ordering providers should have a basic understanding of PGS test development and downstream testing restrictions in order to support appropriate test utilization, and there are no best practices or standards for developing a PGS, such that PGS for the same condition are often not the same. Regulatory requirements for genetic tests to make it to market often require demonstrated analytic and clinical validity, and for PGS and integrated PGS risk models analytic and clinical validity may be restricted to the model parameters used during development. From genome-wide association studies (GWAS) to PGS, the basic steps are to (1) conduct a GWAS to identify variants in a training/source data set, (2) select disease-associated variants that meet prespecified thresholds and combine these to create a PGS, and (3) validate the PGS using an independent dataset; GWAS evaluates associations using “common” variants with “common” population prevalence (≥1% frequency in a population).

---

### Improved measures for evolutionary conservation that exploit taxonomy distances [^96ebed2c]. Nature Communications (2019). High credibility.

Optimizing LIST hierarchical structure

We made the following two assumptions:

First, we assumed that, once compensated for alignment depth, each module’s scores can be loosely considered probabilities after being rescaled to the range [0+ C, 1− C] and then weighted:where min and max are the minimum and maximum scores for each module observed in the optimization set 2. C = 0.2 was used to prevent extreme values from dominating the final outcome. A weight (ω ∈ [0.1,1]) is used to account for the relative prediction accuracy of each module, and the weighted scores were calculated as:Weights ω were learned using a grid search on optimization set 2 to maximize LIST’s AUC, such that modules with higher accuracy are assigned higher ω values. Low ω values produce weighted scores that reflect high uncertainty, i.e. probabilities near 50%, whereas weighted scores resulting from higher ω values have more impact on the final score that is generated by combining weighted scores using Bayes rule,.

Second, the output scores that are generated when combining weighted scores using Bayes rule are likely to be skewed away from the center because the different input scores are not completely independent. Thus, in order to use it as an input probability to the next hierarchical level, these scores are redistributed to fit a normal distribution centered at Bayes rule identity element 0.5, N (μ = 0.5, σ 2 = 0.01) and bounded by the range [0+ C, 1− C] (Supplementary Note 5, Supplementary Figs. 9 and 10).

We redistribute LIST’s output scores to fit a uniform distribution (i.e. rank score), which, we believe, makes the interpretation of these scores simpler. We learned the redistribution function from optimization set 2. The final ROC curves representing the performances of each of LIST three sub-modules are shown in Supplementary Fig. 11.

---

### Advancing value-based medicine: why integrating functional outcomes with clinical measures is critical to our health care future [^c5702769]. Journal of Occupational and Environmental Medicine (2017). Medium credibility.

US health care spending and functional outcomes—economic context and potential impact are summarized as follows: In 2014, US national health expenditures grew 5.3% to $3 trillion, or roughly $9523 per person, and accounted for 17.5% of the gross domestic product (GDP), with 20% attributable to Medicare and about 16%, or $496 billion, to Medicaid; from 2014 to 2024, expenditures are projected to rise at an average annual rate of 5.8%. Research indicates that by focusing on functional outcomes in patient treatment, medical costs can be decreased and the quality of medical outcomes increased.

---

### European Respiratory Society statement: diagnosis and treatment of pulmonary disease in α-antitrypsin deficiency [^a734a41a]. The European Respiratory Journal (2017). Medium credibility.

Regarding diagnostic investigations for alpha-1 antitrypsin deficiency, more specifically with respect to protein electrophoresis, ERS 2017 guidelines recommend to obtain protein phenotyping by isoelectric focusing to identify variants where α-1 antitrypsin is present in the sample including the rarer variants F, I, and P.

---

### Optimal gene expression analysis by microarrays [^15b18110]. Cancer Cell (2002). Low credibility.

DNA microarrays make possible the rapid and comprehensive assessment of the transcriptional activity of a cell, and as such have proven valuable in assessing the molecular contributors to biological processes and in the classification of human cancers. The major challenge in using this technology is the analysis of its massive data output, which requires computational means for interpretation and a heightened need for quality data. The optimal analysis requires an accounting and control of the many sources of variance within the system, an understanding of the limitations of the statistical approaches, and the ability to make sense of the results through intelligent database interrogation.

---

### Brief resolved unexplained events (formerly apparent life-threatening events) and evaluation of lower-risk infants [^d837207b]. Pediatrics (2016). Medium credibility.

AAP rating of evidence and recommendations—aggregate evidence quality is categorized as LEVEL A, LEVEL B, LEVEL C, LEVEL D, and LEVEL X, and the framework uses the columns “BENEFIT OR HARM PREDOMINATES” and “BENEFIT AND HARM BALANCED” to label guidance as “STRONG RECOMMENDATION,” “MODERATE RECOMMENDATION,” “WEAK RECOMMENDATION (based on balance of benefit and harm),” “WEAK RECOMMENDATION (based on low-quality evidence),” or “No recommendation may be made.” LEVEL A emphasizes randomized/meta-analytic support as stated in “Intervention: Well-designed and conducted trials, meta-analyses on applicable populations,” whereas LEVEL X covers “Exceptional situations where validating studies cannot be performed and there is a clear preponderance of benefit or harm.”

---

### Statistical analysis of high-dimensional biomedical data: a gentle introduction to analytical goals, common approaches and challenges [^552ae458]. BMC Medicine (2023). Medium credibility.

PRED1: Construct prediction models

Researchers developing a prediction model primarily focus on how well the model predicts the outcome of interest, especially for new observations, e.g. for patients whose data were not used to build the prediction model. While this is the main concern, often the researchers are also interested in the interpretation of the model, for example identifying which variables contribute most to the prediction and in what way. From this perspective, models involving only a limited number of predictor variables (denoted as “sparse models”), which clearly distinguish informative variables from non-informative variables, may be preferred to models making use of all variables measured for all observations. This is a particularly big challenge in the HDD setting, where many candidate variables are available. Beyond the issue of interpretability, sparse models may be easier to apply in clinical practice, because fewer variables have to be measured or determined to use them than for non-sparse models. In the case of gene expression, for example, the measurement of, say, 10 genes can be easily performed in any lab using PCR techniques, while the measurement of genome-wide expression requires the use of high-throughput methods (see, e.g.).

A model is said to be “complex” if it reflects many patterns present in the available data, for example, by considering many predictor variables or capturing non-linear effects. Overly complex models risk overfitting the data, i.e. adhere too specifically to the data at hand and identify spurious patterns randomly present in the data used for model development that will not be present in independent data (see, e.g.). An overfitted model usually exhibits suboptimal prediction performance when subjected to appropriate unbiased evaluation methods, and interpreting such models can be misleading. In contrast, a model that is not complex enough underfits the data. It misses important patterns that might have been useful for the purpose of prediction. When fitting prediction models, in particular (but not only) in the HDD setting, the challenge thus is to identify the optimal level of model complexity that will yield interpretable models with good prediction performance on independent data (see, e.g.).

---

### Quantification of network structural dissimilarities [^3cf663e0]. Nature Communications (2017). Medium credibility.

Identifying and quantifying dissimilarities among graphs is a fundamental and challenging problem of practical importance in many fields of science. Current methods of network comparison are limited to extract only partial information or are computationally very demanding. Here we propose an efficient and precise measure for network comparison, which is based on quantifying differences among distance probability distributions extracted from the networks. Extensive experiments on synthetic and real-world networks show that this measure returns non-zero values only when the graphs are non-isomorphic. Most importantly, the measure proposed here can identify and quantify structural topological differences that have a practical impact on the information flow through the network, such as the presence or absence of critical links that connect or disconnect connected components.

---

### Supervised dimensionality reduction for big data [^b6768b12]. Nature Communications (2021). High credibility.

To solve key biomedical problems, experimentalists now routinely measure millions or billions of features (dimensions) per sample, with the hope that data science techniques will be able to build accurate data-driven inferences. Because sample sizes are typically orders of magnitude smaller than the dimensionality of these data, valid inferences require finding a low-dimensional representation that preserves the discriminating information (e.g., whether the individual suffers from a particular disease). There is a lack of interpretable supervised dimensionality reduction methods that scale to millions of dimensions with strong statistical theoretical guarantees. We introduce an approach to extending principal components analysis by incorporating class-conditional moment estimates into the low-dimensional projection. The simplest version, Linear Optimal Low-rank projection, incorporates the class-conditional means. We prove, and substantiate with both synthetic and real data benchmarks, that Linear Optimal Low-Rank Projection and its generalizations lead to improved data representations for subsequent classification, while maintaining computational efficiency and scalability. Using multiple brain imaging datasets consisting of more than 150 million features, and several genomics datasets with more than 500,000 features, Linear Optimal Low-Rank Projection outperforms other scalable linear dimensionality reduction techniques in terms of accuracy, while only requiring a few minutes on a standard desktop computer.

---

### Validating whole slide imaging for diagnostic purposes in pathology: guideline from the college of American pathologists pathology and laboratory quality center [^19dfc59e]. Archives of Pathology & Laboratory Medicine (2013). Medium credibility.

Supplemental Table 5. Evidence to Decision Summary for Recommendation 1—For the question “Is the problem a priority?”, responses were 0 No, 0 Probably No, 30% (3/10) Probably Yes, 70% (7/10) Yes, 0 Varies, and 0 Don’t Know. For “How accurate is the test?”, selections were 0 Very Inaccurate, 0 Inaccurate, 70% (7/10) Accurate, 20% (2/10) Very Accurate, 10% (1/10) Varies, and 0 Don’t Know. For “How substantial are the desirable anticipated effects?”, selections were 0 Trivial, 0 Small, 30% (3/10) Moderate, 60% (6/10) Large, 10% (1/10) Varies, and 0 Don’t Know. For “How substantial are the undesirable anticipated effects?”, selections were 50% (5/10) Large, 10% (1/10) Moderate, 20% (2/10) Small, 20% (2/10) Trivial, and 0 Varies.

---

### Small cell lung cancer, version 2.2022, NCCN clinical practice guidelines in oncology [^f6bb383e]. Journal of the National Comprehensive Cancer Network (2021). High credibility.

Regarding medical management for small cell lung cancer - NCCN, more specifically with respect to management of metastatic disease (extracranial metastases), NCCN 2021 guidelines recommend to offer the following common radiation dose-fractionation regimens for palliation of small cell lung cancer metastases in most patients:

- 30 gray in 10 fractions

- 20 gray in 5 fractions

- 8 gray in 1 fraction.

---

### How to avoid common problems when using clinicalTrials.gov in research: 10 issues to consider [^4895e52c]. BMJ (2018). Excellent credibility.

4 ClinicalTrials.gov includes mandatory and optional data elements

Each record contains both mandatory and optional structured data elements. Mandatory data elements comprise the minimum amount of information needed to understand the study and its findings (table S7) and are annotated with red asterisks in the documents listing data element definitions.Automated validation rules prevent the submission of records with missing entries for data elements that were mandatory when registered, and identify obviously inconsistent information at the time of submission (eg, recruitment status set to “recruiting,” but listed study start date is in the future). Optional data elements are not reported in all posted records.

Data elements, subelements, and submission requirements have evolved (table S8). For example, the primary outcome measure data element, introduced as an optional free text field in 2005, was restructured in 2007 to include three separate optional subelements for greater precision that paralleled the top three levels of the specification framework for reporting outcome measures: title, including domain (eg, anxiety) and specific measurement (eg, Hamilton anxiety rating scale); description, including specific metric (eg, change from baseline); and timeframe for the time point(s) at which the measurement is assessed for the specific metric.All three subelements became mandatory for study records submitted after 1 December 2012.

---

### Pruritus: diagnosis and management [^81a93f0a]. American Family Physician (2022). High credibility.

Regarding diagnostic investigations for pruritus, more specifically with respect to history and physical examination, AAFP 2022 guidelines recommend to differentiate lesions as primary to pruritus or secondary (such as excoriations and scarring), recognizing that primary skin lesions indicate skin disease.

---

### Gene-expression measurement: variance-modeling considerations for robust data analysis [^4e2bc0ca]. Nature Immunology (2012). Medium credibility.

System-wide measurements of gene expression by DNA microarray and, more recently, RNA-sequencing strategies have become de facto tools of modern biology and have led to deep understanding of biological mechanisms and pathways. However, analyses of the measurements have often ignored statistically robust methods that account for variance, resulting in misleading biological interpretations.

---

### Summary benchmarks-full set – 2024 [^db7367b2]. AAO (2024). High credibility.

Primary open-angle glaucoma—consensus-based follow-up intervals: When target IOP is achieved with no progression and Duration of Control (months) is ≤6, the Approximate Follow-up Interval (months)* is 6; if target IOP is achieved with no progression and Duration of Control (months) is >6, the interval is 12; if target IOP is achieved with progression, the interval is 1–2; if target IOP is not achieved with progression, the interval is 1–2; and if target IOP is not achieved without progression, the interval is 3–6; IOP is defined and These intervals are the maximum recommended time between evaluations.

---

### Behavioral weight loss interventions to prevent obesity-related morbidity and mortality in adults: US preventive services task force recommendation statement [^4add68cb]. JAMA (2018). Excellent credibility.

Regarding nonpharmacologic interventions for obesity, more specifically with respect to multicomponent lifestyle/behavioral interventions, USPSTF 2018 guidelines recommend to offer intensive multicomponent behavioral interventions in adult patients with a BMI ≥ 30.

---

### Learning properties of quantum States without the IID assumption [^d59de50a]. Nature Communications (2024). High credibility.

We develop a framework for learning properties of quantum states beyond the assumption of independent and identically distributed (i.i.d.) input states. We prove that, given any learning problem (under reasonable assumptions), an algorithm designed for i.i.d. input states can be adapted to handle input states of any nature, albeit at the expense of a polynomial increase in training data size (aka sample complexity). Importantly, this polynomial increase in sample complexity can be substantially improved to polylogarithmic if the learning algorithm in question only requires non-adaptive, single-copy measurements. Among other applications, this allows us to generalize the classical shadow framework to the non-i.i.d. setting while only incurring a comparatively small loss in sample efficiency. We leverage permutation invariance and randomized single-copy measurements to derive a new quantum de Finetti theorem that mainly addresses measurement outcome statistics and, in turn, scales much more favorably in Hilbert space dimension.

---

### An official American Thoracic Society workshop report: developing performance measures from clinical practice guidelines [^53d0f733]. Annals of the American Thoracic Society (2014). Medium credibility.

American Thoracic Society workshop—limitations and cautions include that excluding weak recommendations can miss improvement opportunities, since “it is possible that some weak recommendations could improve patient outcomes if broadly applied” and “weakly recommended treatments may improve outcomes on average, even if they harm some patients”; there may be value in reducing variation, but “this approach carries the risk that meaningful opportunities for performance improvement are missed,” and codifying weak recommendations “carries greater risks, including wasted resources” and punishing “legitimate practice variation.” The approach applies primarily to broad-use measures, as local tracking “may not need to follow such stringent criteria,” though developers “should be cognizant of opportunity costs” when considering uniform implementation of weak recommendations. Although GRADE-based recommendations are “subjective and conditional on the varying judgments of the developers,” the system documents judgments and values that developers can consider when selecting recommendations to turn into measures.

---

### Diagnosis and management of aspergillus diseases: executive summary of the 2017 ESCMID-ECMM-ERS guideline [^958b69dd]. Clinical Microbiology and Infection (2018). Medium credibility.

Regarding diagnostic investigations for invasive aspergillosis, more specifically with respect to species identification, ECMM/ERS/ESCMID 2018 guidelines recommend to obtain species identification to the complex level for all clinically significant isolates in clinical laboratories.

---