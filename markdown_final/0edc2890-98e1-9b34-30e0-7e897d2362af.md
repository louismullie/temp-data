# Medical Question & Answer

**Sample ID**: 0edc2890-98e1-9b34-30e0-7e897d2362af
**Dataset Index**: 221757

---

## Question

9. Let x, y be real numbers, the algebraic expression
5 x² + 4 y² − 8 x y + 2 x + 4

has a minimum value of

---

## Answer

> Let's see… What do we have here? The user is asking for the minimum value of the expression (5x² + 4y² − 8xy + 2x + 4). Let's break this down step-by-step. First, I need to think about whether this is a quadratic form and how to handle the mixed (xy) term. Then, I should complete the square to eliminate the cross term and rewrite the expression in a simpler form. Next, I will identify the critical point by setting partial derivatives to zero and confirm it is a minimum. Finally, I will substitute back to compute the minimum value and double-check the arithmetic.

> Let me first confirm the structure. The expression is a quadratic in two variables with a mixed term, so I should expect a single global extremum if the quadratic form is positive definite, which I will verify as I proceed [^notfound].

> I will now examine the quadratic part (5x² − 8xy + 4y²). Wait, let me verify whether this is positive definite. The discriminant for a quadratic form (ax² + 2bxy + cy²) is (ac − b²), so here that is (5 × 4 − (−4)² = 20 − 16 = 4 > 0), and since the leading coefficient is positive, the form is positive definite, which implies the function has a unique global minimum [^notfound].

> Next, I should complete the square to simplify. Let me group terms carefully: (5x² − 8xy + 4y² + 2x + 4). I can treat the quadratic part as ((√5x − [4/√5]y)²) plus a remainder, but wait, I should double-check the coefficients to avoid an error. Alternatively, I can complete the square in (x) while treating (y) as constant, which is safer. Let me rewrite the quadratic part as (5(x² − [8/5]xy) + 4y²) and then complete the square in (x): (5(x − [4/5]y)² − 5([4/5]y)² + 4y² = 5(x − [4/5]y)² − [16/5]y² + 4y² = 5(x − [4/5]y)² + [4/5]y²). So the expression becomes (5(x − [4/5]y)² + [4/5]y² + 2x + 4). Hold on, I should verify the linear term handling; I still have (2x) mixed in, so I need to substitute (u = x − [4/5]y) to separate variables cleanly [^notfound].

> Let me substitute (u = x − [4/5]y) so that (x = u + [4/5]y). Then the expression becomes (5u² + [4/5]y² + 2(u + [4/5]y) + 4 = 5u² + [4/5]y² + 2u + [8/5]y + 4). Now I can complete the square in (u) and in (y) separately. For the (u) terms: (5u² + 2u = 5(u² + [2/5]u) = 5(u + [1/5])² − [1/5]). For the (y) terms: ([4/5]y² + [8/5]y = [4/5](y² + 2y) = [4/5](y + 1)² − [4/5]). Putting these together, the expression is (5(u + [1/5])² + [4/5](y + 1)² − [1/5] − [4/5] + 4 = 5(u + [1/5])² + [4/5](y + 1)² + 3). I should double-check the constants: ([−1/5] − [4/5] = −1), and (−1 + 4 = 3), so yes, the constant is 3 [^notfound].

> Since both squared terms are non-negative and the coefficients are positive, the minimum occurs when each squared term is zero. That gives (u + [1/5] = 0) and (y + 1 = 0), so (y = −1) and (u = −[1/5]). Substituting back, (x = u + [4/5]y = −[1/5] + [4/5](−1) = −[1/5] − [4/5] = −1). I should confirm the critical point: ((x, y) = (−1, −1)) [^notfound].

> Now I will substitute ((x, y) = (−1, −1)) into the original expression to verify the minimum value. (5(−1)² + 4(−1)² − 8(−1)(−1) + 2(−1) + 4 = 5 + 4 − 8 − 2 + 4 = 3). Alternatively, from the completed-square form, the constant term is 3, so the minimum is 3. Both approaches agree, which gives me confidence in the result [^notfound].

> But wait, what if I made an arithmetic mistake in the constants during completion of the square? Let me reconsider the final constant: after completing the squares, I had (−[1/5] − [4/5] + 4 = −1 + 4 = 3), which matches the direct substitution, so the minimum value is indeed 3 [^notfound].

---

The minimum value of (5x² + 4y² − 8xy + 2x + 4) is **3**, achieved at ((x, y) = (−1, −1)). This is found by completing the square to rewrite the expression as a sum of squares plus a constant, which reveals the minimum directly.

---

## Step 1: Complete the square

Group terms to complete the square:

5x² + 4y² − 8xy + 2x + 4 = (5x² − 8xy + 4y²) + 2x + 4

Treat the quadratic part as a quadratic in (x):

5x² − 8xy + 2x = 5(x² − [(8y − 2)/5]x)

Complete the square inside the parentheses:

x² − [(8y − 2)/5]x = (x − [(4y − 1)/5])² − ([(4y − 1)/5])²

Substitute back:

5[(x − [(4y − 1)/5])² − ([(4y − 1)/5])²] + 4y² + 4

Simplify:

5(x − [(4y − 1)/5])² − [(4y − 1)²/5] + 4y² + 4

Expand and combine like terms:

5(x − [(4y − 1)/5])² + [(4y² + 8y + 19)/5]

---

## Step 2: Complete the square for the remaining quadratic in y

Now complete the square for the (y) terms:

[(4y² + 8y + 19)/5] = [(4(y² + 2y) + 19)/5] = [(4(y + 1)² + 15)/5] = [4(y + 1)²/5] + 3

---

## Step 3: Rewrite the expression as a sum of squares

Substitute back to obtain:

5(x − [(4y − 1)/5])² + [4(y + 1)²/5] + 3

---

## Step 4: Determine the minimum value

Since squares are non-negative, the minimum occurs when both squared terms are zero:

x − [(4y − 1)/5] = 0 and y + 1 = 0

Solve: (y = −1), then (x = [(4(−1) − 1)/5] = −1). Thus, the minimum value is **3** at ((x, y) = (-1, -1)).

---

The minimum value of the expression is **3**.

---

## References

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^c734ee06]. Journal of the American College of Cardiology (2025). High credibility.

ACC/AHA economic value statement templates — three formats and threshold — are specified as follows: The examples define key terms and use a cost-effectiveness threshold of $120 000 per QALY gained; Format 1 asks "What is the cost-effectiveness of the intervention at its current cost?" and may include "an ICER of $i per QALY gained (< $120 000 per QALY gained in p% of probabilistic simulations)"; Format 2 asks "What would the cost of the intervention have to be in order for the intervention to meet the cost-effectiveness threshold?" and states the strategy is cost-effective "at a threshold of $120 000 per QALY gained if the cost of the (intervention) is less than $t"; Format 3 combines both questions; ICER (incremental cost-effectiveness ratio) and QALY (quality-adjusted life year) abbreviations are defined on-page.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^68cfa501]. Journal of the American College of Cardiology (2025). High credibility.

Cost-effectiveness thresholds — framework and updates explain that the 2014 ACC/AHA statement provided per–capita GDP–based thresholds such that an intervention with an ICER < $50 000 per QALY gained should be considered high value, $50 000 to < $150 000 (1-3x per capita GDP) per QALY gained should be considered intermediate value, and ≥ $150 000 per QALY gained should be considered low value. The World Health Organization's program suggested that costs less than 3 times the national annual GDP per capita per disability-adjusted life year averted should be considered cost-effective, whereas costs less than 1 times should be considered highly cost-effective; however, experts have argued this approach lacks theoretical justification, and World Health Organization experts now also advise against the use of per capita GDP-based cost effectiveness thresholds and instead suggest considering cost-effectiveness information alongside other contextual information in a transparent multicriterion decision process. The writing committee reviewed alternative approaches, including relying on US-based studies; for example, a health opportunity cost analysis estimated that at 2019 prices, for every $1000000 increase in health care expenditures 1860 persons were projected to become uninsured with 81 QALYs lost due to mortality and 15 QALYs lost due to morbidity, implying a cost-effectiveness threshold of $104 000 per QALY ($51 000 to $209 000 per QALY) in 2019 US dollars, and in the probabilistic analysis 40% of the simulations suggested the threshold was less than $100 000 per QALY.

---

### Bayesian tomography of high-dimensional on-chip biphoton frequency combs with randomized measurements [^b26a4537]. Nature Communications (2022). High credibility.

In addition to the parameters forming ρ, the scale factor K must also be suitably parameterized. Following ref. we find it convenient to write K = K 0 (1 + σ z), where K 0 and σ are hyperparameters defined separate of the inference process, and z is taken to follow a standard normal distribution, leading to a normal prior on K of mean K 0 and standard deviation K 0 σ. We take σ = 0.1 and K 0 equal to the sum of the counts in all d 2 bins for the first JSI measurement (r = 1), where the absence of modulation ensures that all initial photon flux remains in measured bins, i.e. This provides an effectively uniform prior, since a fractional deviation of 0.1 is much larger than the maximum amount of fractional uncertaintyexpected from statistical noise at our total count numbers; the use of a normal distribution simplifies the sampling process.

The total parameter set can therefore be expressed as the vector, with the prior distribution

We note that this parameterization entails a total of 4d⁴ + 1 independent real numbers (2d⁴ complex parameters for ρ, one real parameter for K) — noticeably higher than the minimum of d⁴ − 1 required to uniquely describe a density matrix. Nevertheless, this ρ (y) parameterization is to our knowledge the only existing constructive method to produce Bures-distributed states, and is straightforward to implement given its reliance on independent normal parameters only.

Following Bayes' rule, the posterior distribution becomeswhereis a constant such that ∫ d x π (x) = 1. We have adopted this notation for Bayes' theorem — rather than the more traditional — to emphasize the functional dependencies on x, which are all that must be accounted for in the sampling algorithm below. From π (x), the Bayesian mean estimator f B of any quantity (scalar, vector, or matrix) expressible as a function of x can be estimated aswhere, in lieu of direct integration, S samples { x (1), …, x (S) } are obtained from the distribution π (x) through Markov chain Monte Carlo (MCMC) techniques, as described below.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^943345bf]. Journal of the American College of Cardiology (2025). High credibility.

Figure 3 — health opportunity cost approach to US cost-effectiveness thresholds reports 50000 simulations with varying input values, with 23902/50000 (48%) thresholds < $100000/QALY and 7006/50000 (14%) thresholds > $150000/QALY; the base-case estimate is $104,000/QALY in 2019 US dollars with a (95% UI, $51,000–$209,000/QALY), and updating to 2024 US dollars suggests a cost-effectiveness threshold of $117000 per QALY.

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^ade8746b]. Nature Communications (2025). High credibility.

Derivation of δ 2 from the general beta distribution

Given an observation (v) of a random variable (V) with any real number upper (U) and lower (L) bound, x can be retrieved for indexing the beta pdf and cdf (Equation (6); Equation (18)) withEssentially, v is rescaled to a value between 0 and 1 based on its possible minimum and maximum, such that it can be used to index the beta pdf in Equation (6). Likewise, the expected value or mean of the variable, E(V), can also be scaled to a value between 0 and 1 in the same way:where E(X) is equivalent to μ for the beta distribution (defined in Equation (7)). Similarly, 1 − E(X), can be retrieved for V withThe variance of the variable, Var(V), scales based on the squared range between the L and U :where Var(X) is equivalent to σ 2 for the beta distribution (defined in Equation (12)). As a result, the mean-independent measure of dispersion, δ 2, can be retrieved for a double-bounded variable withThus, Equation (33) is a general version of Equation (13) that can be calculated for variables with any real number lower and upper bound. In the main text Var(V) is denoted σ 2 and E(V) is denoted μ for simplicity.

The general beta pdf can be expressed using the standard beta pdf (f (x); Equation (6)) as, where the denominator normalises the pdf to adjust for the modified scale when modelling V, x for indexing the pdf is found for V with Equation (29), and the parameters p and q can be found for V with Equations (30), (33), (10), and (11).

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^a83ec36b]. Journal of the American College of Cardiology (2025). High credibility.

Health opportunity cost approach — United States cost-effectiveness threshold estimation indicates that, across 50000 simulations, 23902/50000 (48%) of thresholds were < $100000/QALY and 7006/50000 (14%) were > $150000/QALY; the base-case estimate was $104,000/QALY in 2019 US dollars with a 95% uncertainty interval of $51,000–$209,000/QALY, and updating to 2024 US dollars suggests a threshold of $117000 per QALY.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^9d0e2219]. Journal of the American College of Cardiology (2025). High credibility.

Supporting text requirements for economic value statements specify that supporting text should include key elements: a summary of the high-quality CEAs used to generate the statement with rationale for which studies were included and details such as study population, intervention, comparator, clinical outcomes, sources of key effectiveness and safety parameters, perspective, analytic horizon, base-case treatment cost, and source of funding; point estimates and the proportion of simulations that were cost-effective at a specified threshold; and results of key sensitivity analyses, noting that a 1-way sensitivity analysis of intervention cost variation can be helpful, that additional information about sensitivity to intervention cost should be provided, and that inclusion of a figure similar to Figure 4 is encouraged if available.

---

### Clinical utility of polygenic risk scores for embryo selection: a points to consider statement of the American College of Medical Genetics and genomics (ACMG) [^9ccd284a]. Genetics in Medicine (2024). High credibility.

PGT-P results communication — Figure 3 illustrates using 3 components (percentile rank, relative risk, and absolute risk): families (a) and (b) have embryos at rank extremes while family (c) shows a broader range; two families display different relative risk ranges, with family x values 0.9, 1.0, 1.1, 1.2 and family y values 0.6, 0.9, 1.1, 1.4; example absolute risk increases span 1.0% - 1.4% versus 10.0% - 14.0% for diseases differing by 10x population prevalence.

---

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^5c74e315]. American Journal of Kidney Diseases (2006). Medium credibility.

Hemodialysis adequacy — minimum single‑pool Kt/V (spKt/V) targets per treatment to achieve a weekly standard Kt/V (stdKt/V) of approximately 2.0 are stratified by residual urea clearance (K_r): for 2x/wk dialysis with K_r < 2 mL/min/1.73 m2 it is Not recommended, whereas with K_r ≥ 2 mL/min/1.73 m2 it is 2.0*; for 3x/wk the minima are 1.2 (K_r < 2) and 0.9 (K_r ≥ 2); for 4x/wk 0.8 and 0.6; and for 6x/wk (short daily) 0.5 and 0.4. Table 13 states these sessional targets correspond to "a weekly stdKt/V value of 2.0" in patients "undergoing 2 to 6 treatments per week" with adjustment for "a weekly K_r of 2 mL/min"; a "urea clearance of 2 mL/min is approximately 20 L/wk" and with "V = 30 L, it represents about a 0.67 weekly Kt/V unit". It is noted that "the minimum values for spKt/V" do not account for outcome improvements when frequency "is increase to more than 3/wk". The Work Group "recommended targeting an spKt/V value that is about 15% higher than the recommended minimum targets", developed a scheme that "limited the downward adjustment in spKt/V for K_r to 2 mL/min", and stated that "Maintaining a minimum 'total Kt/V' value of 1.2… would allow reduction of the dialysis dose down to near zero".

---

### Simultaneous and stoichiometric purification of hundreds of oligonucleotides [^8a1b4a8d]. Nature Communications (2018). Medium credibility.

Tunable increase of oligo concentrations via degenerate precursors

Designing multiple precursors with the same oligo product is a simple way to increase the relative concentration of that species in the SNOP product mixture. This can be desirable for high-value targets where the user strongly wishes to avoid low relative concentrations. The length B of the barcode region controls the total number of different different precursors available (B = 6 for 2⁶ = 64-plex SNOP, B = 8 for 2⁸ = 256-plex SNOP). Subject to the above limitation on the total number of precursors, the number of precursors that can used to each oligo product species can be arbitrarily decided.

Individual synthesis of many different precursor molecules to generate the same product oligo is not cost-efficient. A more economical way to achieve higher relative concentrations of a set of desired oligos is to use degenerate randomer barcodes in the precursors (Fig. 5e). In this way, the number of precursor molecules synthesized scales only with the number of distinct product molecules. The tradeoff is that the number of barcodes assigned to each oligo product must a power of 2 (e.g. 1, 2, 4, 8, etc.). Given the distribution of relative concentrations for SNOP products, we do not feel that finer control on number of barcodes assigned would be necessary.

Figure 5f shows NGS results for a 64-plex SNOP with 36 distinct oligo products; 32 of these have a single barcode, and four of these each have eight barcodes (3 nt randomer region). The four oligos with eight barcodes (green dots) show significantly higher relative concentrations than the other 32, though unexpectedly the concentrations are roughly 4-fold higher than median rather than 8-fold as designed. To facilitate comparison, the relative concentrations of a standard 64-plex SNOP with 64 product species are also plotted with the same X indices. Oligo #4, which had a low (0.5x median) concentration in the standard SNOP, exhibits a high (2x median) concentration in the degenerate barcode SNOP. This demonstrates that low concentration oligo products can be "rescued" through the use of degenerate barcode precursors.

---

### Methodologies for the development of CHEST guidelines and expert panel reports [^380f4748]. Chest (2014). Medium credibility.

Finalizing recommendations — recommendations are formulated by content experts informed by the evidence; cost considerations may be included only if published formal cost-benefit analyses are available, with resource consultants or health economists conducting analyses as needed, and when cost constraints exist implementers are encouraged to use ADAPTE strategies; patient values and preferences are reflected in recommendations or remarks and are especially pertinent in weaker recommendations when wide variability in choices is anticipated; recommendations must be specific and actionable with as much detail as evidence allows, and measure developers are cautioned not to convert lower-evidence recommendations into performance measures.

---

### Experimental demonstration of quantum advantage for NP verification with limited information [^776b5b5c]. Nature Communications (2021). High credibility.

Box 1 Protocol for NP verification

Input: Instance of the NP-complete problem and all its relevant parameters: N, δ, etc. after the reduction to a 2-out-of-4 SAT;

Goal: Verification of the solution;

1 Merlin and Arthur jointly perform a pre-calibration of the optical setup, finding the values of the visibility ν N and the transmittivity η;

2 Arthur computes the minimum value of the mean photon number number μ N in order to satisfy the quantum advantage conditions 1–3 and communicates it to Merlin in order to tune the amplitude of his pulses; he also computes the threshold T for accepting a proof;

3 Arthur sends a signal to Merlin to trigger the protocol;

4 Merlin encodes his proof in the phases of the pulses which are then sent to Arthur;

5 Arthur interferes Merlin's pulses with his own and assigns a value x k each time he registers a measurement in the k th pulse:

5.1 x k = 0 for a click in detector D 0 and no click in D 1;

5.2 x k = 1 for a click in detector D 1 and no click in D 0;

5.3 x k is randomly assigned if both detectors click.

6 For all the measured bits that form a clause, Arthur checks the satisfiability;

7 If the number of satisfied clauses is greater than T, Arthur accepts the proof, otherwise he rejects.

Experimental results

We now have all the ingredients to describe the experimental implementation of our verification test and the assessment of the quantum advantage for this task. As we defined previously, we need to satisfy three conditions to show quantum advantage. We need the verification procedure to take time linear in N, to have completeness and soundness such thatand, and that the number of clicks Arthur registers is much smaller than the input size N.

First, as we will see, in our experiment we use indeed a train of coherent pulses of size N and some simple classical post-processing of the measurement results, so our test satisfies condition 1. In fact, the real time to run the verification procedure for N between 5000 and 14,000 was a fraction of a second for the quantum part, a few seconds for the classical post-processing and a couple of minutes for the calibration procedure for each run.

---

### Towards a fully automated algorithm driven platform for biosystems design [^ed08b9a9]. Nature Communications (2019). High credibility.

We next sought to illustrate the optimization method with a similar 3-variable function with three inputs and one output to simulate a similar multi-dimensional optimization problem. It is noteworthy that Bayesian optimization has been used in numerous applications – and the purpose of this simulation is testing the algorithm on a simple but similar setting. The search perimeter was set to be 1–24 for each of the inputs and the maximum of the function was set to be 9 (y = f (x 1, x 2, x 3) | x i ɛ {1, 2, …, 24}, f max = 9). The Bayesian optimization algorithm was able to find the maximum value of this function by only evaluating 12 points out of all possible 24 3 = 13,824 points. These 12 evaluations were the result of 12 iterations of learning and testing, with each evaluation being followed by a learn step that produced the next point to evaluate. We then sought to compare this optimization strategy with baseline approach where randomly sampled points are evaluated and all of these points are used to train an Exterior Derivative Estimation (EDE)-based regression model described in previous publications. We found that although the EDE approach shows impressive predictive capability, especially given that all the data have been acquired at once and not through iterative sampling, even after sampling 192 random points, the maximum could not be found (Supplementary Table 1).

---

### A co-design framework of neural networks and quantum circuits towards quantum advantage [^aa9b9d37]. Nature Communications (2021). High credibility.

Neural computation P-LYR

An m -input neural computation component is illustrated in Fig. 4 c, where m -input data I 0, I 1, ⋯, I m −1 and m corresponding weights w 0, w 1, ⋯, w m −1 are given. Input data I i is a real number ranging from 0 to 1, while weight w i is a {−1, +1} binary number. Neural computation in P-LYR is composed of 4 operations: (i) R: this operation converts a real number p k of input I k to a two-point distributed random variable x k, where P { x k = −1} = p k and P { x k = +1} = 1 − p k, as shown in Fig. 4 b. For example, we treat the input I 0 's real value of p 0 as the probability of x 0 that outcomes −1 while q 0 = 1 − p 0 as the probability that outcomes +1. (ii) C: this operation calculates y as the average sum of weighted inputs, where the weighted input is the product of a converted input (say x k) and its corresponding weight (i.e. w k). Since x k is a two-point random variable, whose values are −1 and +1 and the weights are binary values of −1 and +1, if w k = − 1, w k ⋅ x k will lead to the swap of probabilities P { x k = −1} and P { x k = + 1} in x k. (iii) A: we consider the quadratic function as the nonlinear activation function in this work, and A operation outputs y 2 where y is a random variable. (iv) E: this operation converts the random variable y 2 to 0–1 real number by taking its expectation. It will be passed to batch normalization to be further used as the input to the next layer.

---

### Pain and symptoms of depression: international comparative study on selected factors affecting the quality of life of elderly people residing in institutions in Europe [^1ed9d0d0]. BMC Geriatrics (2019). Medium credibility.

Data analysis

The results concerning women residing in Polish, French and German nursing homes underwent statistical analysis including three respective groups; the values of the parameters measured on the nominal scale were expressed as absolute values (N) and percentages (%). The impact of the independent variables on the subjective assessment of the quality of life was expressed in points (for the BDI and DOLOPLUS scales), and was analyzed using multiple regression. The quality of life model included analyses of multiple factors, among them: X 1 (age), X 2 (place of residence), X 3 (marital status), X 4 (living conditions), X 5 (educational level), X 6 (independence in performing basic activities), X 7 (dependence from other person's care). General model of multiple and linear regression was used:

where: X means vector of independent variables; Y means dependent variable; β means model coefficient; f (X, β) means regression function of real numbers values; F means statistics F value in regression; df means degree of freedom; p means statistical significance; R² means coefficient of determination; E means random error; the least squares method algorithm was applied. The accepted models of analysis can be considered trustworthy because for the BDI scale data R² = 0,7308 and for the DOLOPLUS scale data R² = 0,7449; and for both datasets the variance analysis points at the error value close to the null (Tables 1 and 2). We took statistically significant data to have a level of p < 0.05. We also calculated the arithmetical mean, standard deviation (SD), median, and the minimum and maximum values for all parameters. Statistical analysis was performed using Microsoft Excel spreadsheet and Statistica 8.0 software.

Table 1
Variance analysis for the variable: results of Beck Depression Inventory (BDI) scale examination of the elderly living in nursing homes

Table 2
Variance analysis for the variable: results of DOLOPLUS scale examination of the elderly living in nursing homes

---

### Complex modeling with detailed temporal predictors does not improve health records-based suicide risk prediction [^ac54580a]. NPJ Digital Medicine (2023). Medium credibility.

Variables describing trend in X over time:
D18 (# of months with X) × [(difference between the earliest month and most recent month with X] + 1). Those who do not have X observed (ever) or only have one occurrence, set value to 0.
D19 Maximum # of days with X in any month minus the minimum count of days with X in any month.
D20 Maximum # of days with X in any month.
D21 Number of months in which days with X exceeds Y, where Y is the entire visit sample's average monthly days with X. Calculate Y by averaging over all months with at least one X.
D22 Number of months in which days with X exceeds Y, where Y is person's average monthly days with X as of this visit. Only consider X that occurred while person was enrolled. If X not observed while enrolled, set to −5.
D23 Number of months in which days with X exceeds Y, where Y is person's average monthly days with X as of this visit. Only consider X that occurred while person was enrolled prior to visit. If X was not observed during that time, set to 65.
D24 Proportion of months enrolled in which days with X exceeds Y, where Y is entire visit sample's average monthly days with X. Calculate Y by averaging over all months with at least one X.
D25 Proportion of months enrolled in which days with X exceeds Y, where Y is person's average monthly days with X as of this visit. Only consider X that occurred while person was enrolled prior to visit up to the full past 60 months.
D26 Total days with X in last month minus monthly average for prior 2–12 months. Only consider X that occurred while person was enrolled prior to visit. If not enrolled ≥ 2 months, set to −5.
D27 Monthly average of days with X in last 2 months minus monthly average over prior 3–12 months. Only consider X that occurred while person was enrolled prior to visit. If not enrolled ≥ 3 months, set to −5.
D28 Monthly average of days with X in last 3 months minus monthly average over prior 4–12 months. Only consider X that occurred while person was enrolled prior to visit. If not enrolled ≥ 4 months, set to −5.

---

### An analysis of multirules for monitoring assay quality control [^b37bd68a]. Laboratory Medicine (2020). Medium credibility.

Background

Multirules are often employed to monitor quality control (QC). The performance of multirules is usually determined by simulation and is difficult to predict. Previous studies have not provided computer code that would enable one to experiment with multirules. It would be helpful for analysts to have computer code to analyze rule performance.

Objective

To provide code to calculate power curves and to investigate certain properties of multirule QC.

Methods

We developed computer code in the R language to simulate multirule performance. Using simulation, we studied the incremental performance of each rule and determined the average run length and time to signal.

Results

We provide R code for simulating multirule performance. We also provide a Microsoft Excel spreadsheet with a tabulation of results that can be used to create power curves. We found that the R4S and 10x rules add very little power to a multirule set designed to detect shifts in the mean.

Conclusion

QC analysts should consider using a limited-rule set.

---

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^4ba28381]. American Journal of Kidney Diseases (2006). Medium credibility.

Hemodialysis adequacy — minimum single-pool Kt/V (spKt/V) to achieve a standard Kt/V (stdKt/V) of 2.0 per week is tabulated by treatment frequency, session duration (Td), and residual renal urea clearance (Kr). For Kr = 0, 2 times/week at Td 8.0 hr is 3.00 (with " — " at 2.0 and 3.5 hr); 3 times/week is 1.22 at 3.5 hr and 1.06 at 8.0 hr; 4 times/week is 0.87 at 2.0 hr, 0.77 at 3.5 hr, and 0.68 at 8.0 hr; and 7 times/week is 0.42, 0.38, and 0.34 at Td 2.0, 3.5, and 8.0 hr. For Kr = 2 mL/min/1.73 m², 2 times/week is 1.93 at 3.5 hr and 1.68 at 8.0 hr; 3 times/week is 0.94, 0.85, and 0.77 for Td 2.0, 3.5, and 8.0 hr; and 7 times/week is 0.31, 0.28, and 0.26 at Td 2.0, 3.5, and 8.0 hr. Footnotes specify "Dialyzer clearance only, expressed per dialysis", and that values were "Calculated using a 2-compartment mathematical model" with assumptions including "V = 35 L", "ultrafiltration rate is 7 L/wk", "nPCR is 1 g/kg/d", and "K:V(urea) is 0 or 2 mL/min"; the table notes these minima "do not take into account reported improvements in outcome from increasing Kt/V when dialysis frequency is increase to more than 3x/week".

---

### Incorporating economic evidence in clinical guidelines: a framework from the clinical guidelines committee of the American College of Physicians [^4483eea4]. Annals of Internal Medicine (2025). High credibility.

Figure 1 — comparative value of interventions — categorizes value by clinical net benefit and incremental cost for quality-adjusted life-year (QALY) with intervention versus comparator, where the incremental cost dimension reflects the incremental cost-effectiveness ratio per QALY gained. For favorable clinical net benefit, higher cost corresponds to high value (< $100 000), intermediate value ($100 000 to $200 000), or low value (> $200 000) with the intervention more effective and more costly; lower cost corresponds to high value with the intervention more effective and less costly (cost-saving). For unfavorable clinical net benefit, higher cost indicates no value with the intervention less effective and more costly (cost-dominated), and lower cost indicates no value with the intervention less effective and less costly.

---

### A space-time tradeoff for implementing a function with master equation dynamics [^57fa10b0]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### Combinatorial code governing cellular responses to complex stimuli [^f2481e0e]. Nature Communications (2015). Medium credibility.

Methods

Identification of the interaction profiles

The number of ways to rank the numbers e 0, e X, e Y and e X+Y was computed using the recurrence relation A n k = A n− 1 k +k A n− 1 k− 1 which, for n = 4 and summing up for k = 1, 2, 3 and 4 gives 75 possible rankings. Each ranking can be seen as a set of outcomes of the following qualitative pairwise comparisons:
e X versus e 0
e Y versus e 0
e X+Y versus e 0
e Y versus e X
e X+Y versus e X
e X+Y versus e Y

In our framework, the outcome of a qualitative comparison can take on the following three values: 0 (equal numbers), 1 (first larger than the second) and −1 (first smaller than the second). Each of 75 rankings was coded uniquely as a vector of six components describing the qualitative outcome of the comparisons 1–6 as listed above. For example, the vector (0,0,1,0,1,1) corresponds to:

e X = e 0, e Y = e, e X+Y > e 0, e Y = e X, e X+Y > e X, e X+Y > e Y

To identify which rankings were compatible with positive or negative interactions, we considered the equations 1–6 together with the inequalities that define positive and negative interactions:
Δ e X+Y > Δ e X + Δ e Y (positive interaction)
Δ e X+Y < Δ e X + Δ e Y (negative interaction)

which can be written as

(7a) e X+Y − e X − e Y + e 0 > 0

(8a) e X+Y − e X − e Y + e 0 < 0

If a ranking is consistent with a positive (or negative) interaction, the inequality constraints encoded in the corresponding vector can be solved simultaneously with 7a (or 8a). To verify this, we developed a constraint satisfaction that attempts to solve the six constraints of each ranking together with the inequality 7a (or 7b). The solution is searched numerically with a MATLAB linear solver. The variables e 0, e X, e Y and e X+Y were constrained to the interval 2–16, taken as an approximate of the range of expression values from Affymetrix chips in log2 scale. The method is implemented in the MATLAB code and is available upon request.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^59838d2f]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Sample size metrics — tolerance intervals versus confidence intervals: Although performance is often stated in terms of confidence intervals (CI), the CI of the mean only gives an estimate of the population mean with a stated level of confidence and does not give an indication of the performance of any given sample; to estimate the distribution of the underlying population and the performance of individual samples, the tolerance intervals should be used, with the lower tolerance interval for a normally distributed population determined as x̄ ± k × s, where s is the sample standard deviation (SD) and k is a correction factor, and for two-sided 95% confidence with n = 20 the k value is 2.75, approaching the z-score as the number of samples increases.

---

### Data-driven control of complex networks [^4869b5fd]. Nature Communications (2021). High credibility.

Discussion

In this paper, we present a framework to control complex dynamical networks from data generated by non-optimal (and possibly random) experiments. We show that optimal point-to-point controls to reach the desired target state, including the widely used minimum-energy control input, can be determined exactly from data. We provide closed-form and approximate data-based expressions of these control inputs and characterize the minimum number of samples needed to compute them. Further, we show by means of numerical simulations that data-driven inputs present some numerical advantages with respect to classic model-based approaches, and can be used to analyze and manipulate the controllability properties of real networks.

The data-driven expressions derived in this paper are not only theoretically intriguing and practically relevant but they may also provide an alternative set of tools to investigate how different network properties, such as dimension, heterogeneity, and structure, affect controllability. These questions, which are currently being asked in a model-based setting, may find an easier answer in a data-driven framework due to the simplified mathematical expressions of optimal controls. More generally, our framework and results suggest that many network control problems may be solved by simply relying on experimental data, thus promoting a new, exciting, and practical line of research in the field of complex networks. Because of the abundance of data in modern applications and the computationally appealing properties of data-driven controls, we expect that this new line of research will benefit a broad range of research communities, spanning from engineering to biology, which employs control-theoretic methods and tools to comprehend and manipulate complex networked phenomena.

---

### Summary benchmarks-full set – 2024 [^c9abe316]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — GRADE evidence quality ratings are defined for forming recommendations for care as follows: "Good quality (GQ): Further research is very unlikely to change our confidence in the estimate of effect", "Moderate quality (MQ): Further research is likely to have an important impact on our confidence in the estimate of effect and may change the estimate", and "Insufficient quality (IQ): Further research is very likely to have an important impact on our confidence in the estimate of effect and is likely to change the estimate; any estimate of effect is very uncertain".

---

### Data-driven recombination detection in viral genomes [^4d5f2070]. Nature Communications (2024). High credibility.

Sensitivity, specificity, minimum requirements

Extensive simulations (see simulated sequences in Supplementary Data 1) were performed to measure the sensitivity and specificity of RecombinHunt and the minimum requirements for its application.

Sensitivity

For testing the sensitivity, we considered two SARS-CoV-2 lineages (BA.2 and AY.45) and generated recombinant sequences with one or two breakpoints. Similarly to Turakhia et al. we simulated two sets of 3500 recombinant sequences (1BP and 2BP cases) each partitioned into seven groups of 500 sequences, and injected increasing levels of noise, by adding -respectively to each group- 0, 3, 5, 10, 15, 20, or 30 mutations out of 4,983 that are non-characteristic and with frequency ≥ [1/10⁵] in both parent lineages, at random genomic positions. Note that the generated SARS-CoV-2 genome sequences carry about 60 mutations compared to the reference Wuhan1 genome; hence, adding 3 mutations corresponds to inserting 5% noise, whereas 30 mutations to inserting 50% noise. RecombinHunt achieved an almost perfect sensitivity when the number of added mutations was ≤ 10 (100%-99.4% for 1BP and 99%-95.6% for 2BP). By adding up to 30 mutations, performance slightly decreases as expected; see Table 1 a. The breakpoint position was identified correctly (i.e. within a single mutation range) in 99.4% of the simulated 1BP cases and in 97.6% of the simulated 2BP cases.

Table 1
a Sensitivity analysis

Number (and percentage, out of a total of 500) of sequences detected as 1BP/2BP recombinants or non-recombinants by RecombinHunt (RH). b Specificity analysis. Number (and percentage, out of a total of 500) of sequences detected as 1BP/2BP recombinants when they instead were non-recombinants. c Minimum number of sequences to correctly characterize a real lineage. Cell values represent the median valuesuch that the median number of characteristic mutations is correct for all the subsequent values of. The median values have been computed by sampling N sequences 100 times for increasing values of N. Each mutations-noise level indicates the maximum number of mutations that differ from the characterization of the lineage, present in the sequences of the sampled dataset.

---

### Summary benchmarks-full set – 2024 [^6f68d3ad]. AAO (2024). High credibility.

Preferred Practice Pattern (PPP) guidelines — use and ethics note states that "The PPPs are intended to serve as guides in patient care, with greatest emphasis on technical aspects", that "true medical excellence is achieved only when skills are applied in a such a manner that the patients' needs are the foremost consideration", and that "The Academy is available to assist members in resolving ethical dilemmas that arise in the course of practice".

---

### Quantum machine learning for electronic structure calculations [^f4bc05c9]. Nature Communications (2018). Medium credibility.

Methods

Preparation of the Hamiltonian of H 2, LiH and H 2 O

We treat H 2 molecule with 2-electrons in a minimal basis STO-3G and use the Jordan-Wigner transformation. The final Hamiltonian is of 4 qubits. We treat LiH molecule with 4-electrons in a minimal basis STO-3G and use the Jordan-Wigner transformation. We assumed the first two lowest orbitals are occupied by electrons and the the final Hamiltonian is of 4 qubits. We treat H 2 O molecule with 10-electrons in a minimal basis STO-3G, we use Jordan-Wigner transformation. We assume the first four lowest energy orbitals are occupied by electrons and first two highest energy orbitals are not occupied all time. We also use the spin symmetry in, to reduce another two qubits. With the reduction of the number of qubits, finally we have 6 qubits Hamiltonian. All calculations of integrals in second quantization and transformations of electronic structure are done by OpenFermionand Psi4.

Gradient estimation

The two functions ϕ (x) and s (x) are both real function. Thus, the gradient for parameter p k can be estimated aswhereis so called local energy. 〈.〉 represents the expectation value of joint distribution determined by ϕ (x) and s (x) (details in Supplementary Note 1).

Implementation details

In our simulation we choose small constant learning rate 0.01 to avoid trapping in local minimum. All parameter are initialized as a random number between (−0.02,0.02). The range of initial random parameter is to avoid gradient vanishing of tanh. For each calculation we just need 1 reusing ancilla qubit all the time. Thus, in the simulation, the number of required qubits is m + n + 1. All calculations do not consider the noise and system error (details in Supplementary Note 5).

---

### Dynamics of ranking [^67f65495]. Nature Communications (2022). High credibility.

Fig. 2
Model of rank dynamics reproduces features of real-world ranking lists.

a Model of rank dynamics in a system of N elements and ranking list size N 0. At time t, a random element is moved to a random rank with probability τ. A random element is also replaced by a new element with probability ν. b Probability P x, t that element in rank r = R / N moves to x = X / N after time t (uppercase/lowercase symbols are integer/normalized ranks). Elements not replaced diffuse around x = r (with probability D x, t) or perform Lévy walks(with probability L t). Eq. (2) recovers simulation results, shown here for τ = 0.1, ν = 0.2, N = 100, and N 0 = 80 at times t = 1, 5 (left/right plots), averaged over 10⁵ realizations. c Time series of rank flux F t over observation period T for data (lines), and mean flux F from the fitted model (dashes) (all datasets in SI Fig. S3; for fitting see SI Section S5). d Probability P x, t for t = 1 and varying r (left) and rank change C (right), shown for selected datasets (lines) and fitted model (dashes; τ and ν in the plot) (empirical P x, t is passed through a Savitzky–Golay smoothing filter; see SI Figs. S6 – S9 and SI Table S2). As systems become more open, we lose symmetry in the rank dependence of both C and the height of the diffusion peaks of P x, t (signaled by curved arrows). Data and model have similar qualitative behavior in all rank measures (for a systematic comparison see SI Fig. S19).

---

### Evaluating molecular biomarkers for the early detection of lung cancer: when is a biomarker ready for clinical use? An official American Thoracic Society policy statement [^1f4d7938]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Representative biomarker accuracies meeting a required threshold — Table 6 lists "Representative Biomarker Accuracies That Would Satisfy the Calculation Yielding a Sensitivity/(1 − Specificity) ≥ 4.18", with examples including Sensitivity (%) 40 with Specificity (%) 90.4, Sensitivity (%) 60 with Specificity (%) 85.6, Sensitivity (%) 70 with Specificity (%) 83.4, and Sensitivity (%) 80 with Specificity (%) 80.9.

---

### Article 5. An introduction to estimation – 2: from z to T [^fb82b806]. Emergency Medicine Journal (2001). Low credibility.

Provided the sample size is large enough (that is, n greater than 100), the z statistic can be used to determine the confidence interval estimation of the population mean even when the sigma is not known. In these cases the estimation of the standard error of the mean is used. The z statistic is also valid when determining the population's proportion based upon a large sample. However, when dealing with smaller samples, the z statistic is replaced by the t statistic. This makes it possible to estimate, in a population with an unknown standard deviation: The probability of getting a sample mean greater than or equal to a particular value The value of a sample mean with a particular probability of occurring The probability of getting a sample mean between two particular values The confidence interval for the estimation of the population mean can also be determined using the t statistic.

---

### Evaluating molecular biomarkers for the early detection of lung cancer: when is a biomarker ready for clinical use? An official American Thoracic Society policy statement [^1c6ced37]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Accuracy guidance for biomarker clinical validation — formulas specify minimal accuracy thresholds tied to action after positive versus negative results. The committee "agreed that it would be helpful to provide guidance about the minimal accuracy, as assessed in the clinical validation phase, that could lead to a positive clinical impact". For tests where "a positive biomarker result leads to an action where a negative biomarker result is associated with standard of care for the population", the threshold is "sensitivity/(1 − specificity) ≥ [(1 − prevalence)/prevalence] × harm/benefit", where harm/benefit is "the ratio of the net harm of a falsely positive test result to the net benefit of a true-positive test result". For tests where "a negative test leads to an action other than standard of care for the population", the threshold is "specificity/(1 − sensitivity) ≥ [prevalence/(1 − prevalence)] × harm/benefit", where harm/benefit is "the ratio of the net harm of a falsely negative test result to the net benefit of a true-negative test result". "Sensitivity/(1 − specificity) is known as the positive likelihood ratio, and specificity/(1 − sensitivity) is 1 divided by the negative likelihood ratio", and "Prevalence refers to the percentage of cases in the intended use population". The harm/benefit term "can be articulated in one of two ways: 1/N, where in the first scenario N is the maximum number of control subjects testing positive that is tolerated to benefit one case subject testing positive".

---

### A faster and more accurate algorithm for calculating population genetics statistics requiring sums of stirling numbers of the first kind [^b9d5300d]. G3 (2020). Medium credibility.

Figure 3
(A) Comparison of relative error of the estimator from and the single term asymptotic estimator in (35). Relative error for each is calculated against the arbitrary precision implementation described in. In total, 10,000 calculations were performed with n randomly sampled from a uniform distribution between 50 and 500; m between 2 and n; and θ between 1 and 50. A solid diagonal line is drawn at. Dotted lines are drawn at a relative error of 0.001. Numbers within each quadrant defined by the dotted lines indicate the number of points in each quadrant. The red dot indicates the one case where the relative error wasand the error of (35) was greater than the estimator from. (B) Comparison of mollified error (34) as a function of m. For this plot, we fixed(solid lines) or 500 (dotted lines) and(as indicated by different line colors).

The fewer calculations led to a clear improvement in calculation speed (median 54.6x faster; Figure 4). The speedup also depends on the parameter choices; in general, the speed advantage is greater when the hybrid calculator requires many calculations (namely, when m is small relative to n, as the hybrid calculator performs the sum in (2)) (Figure 4B).

Figure 4
(A) Comparison of run times between the hybrid algorithm from and the single term asymptotic estimator in (35). 100 iterations were run, each with 10,000 calculations; the time elapsed for each set of 10,000 calculations was recorded and plotted here. The same set of parameters were used for each algorithm. The order of running the algorithms was alternated with each iteration. The dark horizontal line indicates the median, the box indicates the first and third quartiles, the whiskers are drawn at 1.5x the interquartile range, and outliers are represented by open circles. The median for the hybrid algorithm is 62.64 s; the median for the asymptotic algorithm is 1.17 s. (B) Detailed benchmarking for(open violins) or 500 (gray violins),… Fold speedup (ratio of the time taken for the hybrid calculator to that taken for the aysmptotic estimator) is plotted on the y-axis. Each dot represents one set of parameters; the violin plots summarize the density of points on the y-axis. Times were calculated for 100 iterations of each estimator for the same parameter values.

---

### A novel oppositional binary crow search algorithm with optimal machine learning based postpartum hemorrhage prediction model [^f6de23e1]. BMC Pregnancy and Childbirth (2022). Medium credibility.

Whereasrepresent the opposite number and x ∈ R denotes a real number determined on range of x ∈[a, b]. While a = 0 and b = 1 Eq. (3) becomes

While there is a point P (x 1, x 2,… x n) in n dimension coordinate and x 1, x 2, …, x n ∈ R later, the opposite pointis determined as its coordinates:

In such cases, have 2 values, x represent initial arbitrary value in [a, b] anddenotes the opposite values of x. They calculate f (x)&in all the iterations of OBCSA, later, employ on the evaluation function g ifselect x or else selectConsequently, the f l would be in range: f l ∈[f l min, f l max]. The opposite numbercan be determined by:

Later, evaluate the fitness for the first f l value and the fitness forin all the iterations. When, they select f l, or elsewould be selected. The stages of presented method can be given in the following.

Step1: The count of crows is n c = 25, f l min = 0.1, f l max = 1.8, A P = 0.3, and the maximal number of iterations is t max = 100.

Step2: The position that represent the features are made by U (0, 1).

Step3: The fitness function (FF) can be determined by

Whereas C represent the classification performance, W represent the weighted factors in the range of zero and one, F all represent the overall amount of features and F sub signifies the length of elected feature.

Step4: The position of the crows are upgraded as Eq. (2)

Step5: Steps 3 & 4 are repetitive till a t max is attained.

---

### The minimal work cost of information processing [^894e044e]. Nature Communications (2015). Medium credibility.

where the sum ranges only over those x that have a non-zero probability of occurring. In the case of deterministic mappings p (x ′| x)∈{0,1}, this corresponds to the maximum number of input states that map to a same output state. For the AND gate, provided all four states 00, 01, 10 and 11 have non-negligible probability of occurring, there are three input states mapping to the same output state, so (3) gives us simply. Also, in simple examples as considered here, the expression (3) is stable to considering an-approximation (Supplementary Note 4); this quantity is thus physically justified.

Crucially, our result reveals that the minimal work requirement in general depends on the specific logical process, and not only on the input and output states. This contrasts with traditional thermodynamics for large systems, where the minimal work requirement of a state transformation can always be written as a difference of a thermodynamical potential, such as the free energy. For example, the minimal work cost of performing specifically an AND gate may differ from that of another logical process mapping an input distribution (p 00, p 01, p 10, p 11) (with ∑ i p i = 1) to the distribution (p ′ 0, p ′ 1) = (p 00 + p 01 + p 10, p 11) (Recall that the classical counterpart of a quantum state is a probability distribution.). To see this, consider the XOR gate, which outputs a 1 exactly when both inputs are different (see Fig. 2b). The minimal work cost requirement of this gate, as given by (3), is now only kT ln 2, as in the worst case, only a single bit of information is erased (again supposing that all four input states have non-negligible probability of occurring). Now, suppose that, for some reason, the input distribution is such that p 01 + p 10 = p 11, that is, the input 11 occurs with the same probability as of either 01 or 10 appearing. Then, the XOR gate reproduces the exact same output distribution as the AND gate: in both cases, we have p ′ 0 = p 00 + p 10 + p 01 = p 00 + p 11 and p ′ 1 = p 11 = p 01 + p 10. In other words, both logical processes have the same input and output state, yet the XOR gate only requires work kT ln 2 compared with the AND gate, which requires 1.6 kT ln 2. Furthermore, we point out that this difference, which appears small in this case, may be arbitrarily large in certain scenarios (Supplementary Note 4).

---

### Can a minimum rate of investigation of measleslike illnesses serve as a standard for evaluating measles surveillance? [^a976a98b]. The Journal of Infectious Diseases (2004). Low credibility.

To determine whether measles case finding is sensitive, we developed a standard by which to evaluate measles surveillance. We compiled data on the incidence of measleslike illnesses (MLIs) from multiple, diverse sources and used the distribution of these values to determine the minimum level of measles case-finding activity that could be expected in a given region. Among surveillance programs in the United States, other countries in the Americas, and other World Health Organization regions, the median annual rates for rash investigations that were ruled out for measles were 4.3, 4.1, and 1.8/100000 population, respectively. The annual rates of measles IgM testing in the United States in public laboratories and commercial laboratories were 1.6 and 9.2/100000 population, respectively. In total, we collected data on annual MLI incidence from > 80 sources. Values ranged from 0.1 to 22.6 cases of MLI per 100000 population, and 90% of values were ≥ 1.0/100000 population. On the basis of these findings, we propose that programs attempting measles elimination consider evaluating surveillance by comparing the annual rate of suspected measles investigations against a minimum standard of 1/100000 population.

---

### Clinical pharmacogenomic testing and reporting: a technical standard of the American College of Medical Genetics and genomics (ACMG) [^0f896bc7]. Genetics in Medicine (2022). High credibility.

CYP2D6 structural variation and copy number variant notation — "Although currently there is no standardized method to annotate CYP2D6 structural variants, CNVs are commonly indicated by placing a multiplication sign and the number of gene copies after the affected allele". Examples illustrate the expected counts: "CYP2D6✱1/✱2x2 indicates that there are 2 copies of the *2 allele on the same chromosome for 3 total gene copies", and "CYP2D6✱1/✱36+*10 indicates that 1 chromosome has a copy of the CYP2D6✱36 allele in tandem with a CYP2D6✱10 allele for a total of 3 gene copies".

---

### Biased expectations about future choice options predict sequential economic decisions [^9b0d2bac]. Communications Psychology (2024). Medium credibility.

Next, the model works backwards through the sequence, iteratively using the aforementioned formula forwhen computing each respective action value Q for taking the option and declining the option for each t. Whenever the reward value of taking the current option is considered, the reward function R assigns reward values to options based on their ranks. h represents the relative rank of the current option.

In contrast, the reward value of sampling again is simply the cost to sample C.

This customisable R function allowed us to examine how the Ideal Observer changes its sampling strategy under the different reward payoff schemes used in our studies. Pilot full, Study 1 full, Study 2 and both conditions in Study 3 all involved instructing participants to try to choose the best price possible. In study conditions using these instructions, we implemented a continuous payoff function (resembling that of the classic Gilbert & Mosteller formulation), in which the relative rank of each choice would be rewarded commensurate with the value of its associated option. In Pilot baseline and the baseline, squares, timing, and prior conditions of Study 1, we adapted the payoff scheme to match participants' instructions that they would be paid £0.12 for the best rank, £0.08 for the second-best rank, £0.04 for the third best rank and £0 for any other ranks. Lastly, in the payoff condition of Study 1, we programmed the reward payoff function to match participants' reward of 5 stars for the best rank, 3 stars for the second-best rank, one star for the third-best rank and zero stars for any other ranks.

Another feature added to our implementation of the Ideal Observer, compared to the Gilbert & Mosteller base model, is the ability to update the model's generating distribution from its experience with new samples in a Bayesian fashion, instead of this generating distribution being specified in advance and then fixed throughout the paradigm. This Bayesian version of the optimality model treats option values as samples from a Gaussian distribution with a normal-inverse- χ² prior. The prior distribution is initialised before experiencing any options with four parameters: the prior mean μ 0, the degrees of freedom of the prior mean κ, the prior variance σ 2 0 and the degrees of freedom of the prior variance ν. The μ 0 and σ 2 0 parameters of this prior distribution are then updated by the model following presentation of each newly sampled option value as each sequence progresses.

---

### Optimal diffusion MRI acquisition for fiber orientation density estimation: an analytic approach [^f3f4dff6]. Human Brain Mapping (2009). Low credibility.

An important challenge in the design of diffusion MRI experiments is how to optimize statistical efficiency, i.e., the accuracy with which parameters can be estimated from the diffusion data in a given amount of imaging time. In model-based spherical deconvolution analysis, the quantity of interest is the fiber orientation density (FOD). Here, we demonstrate how the spherical harmonics (SH) can be used to form an explicit analytic expression for the efficiency of the minimum variance (maximally efficient) linear unbiased estimator of the FOD. Using this expression, we calculate optimal b-values for maximum FOD estimation efficiency with SH expansion orders of L = 2, 4, 6, and 8 to be approximately b = 1,500, 3,000, 4,600, and 6,200 s/mm(2), respectively. However, the arrangement of diffusion directions and scanner-specific hardware limitations also play a role in determining the realizable efficiency of the FOD estimator that can be achieved in practice. We show how some commonly used methods for selecting diffusion directions are sometimes inefficient, and propose a new method for selecting diffusion directions in MRI based on maximizing the statistical efficiency. We further demonstrate how scanner-specific hardware limitations generally lead to optimal b-values that are slightly lower than the ideal b-values. In summary, the analytic expression for the statistical efficiency of the unbiased FOD estimator provides important insight into the fundamental tradeoff between angular resolution, b-value, and FOD estimation accuracy.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^01148c3a]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Nonparametric tolerance intervals for non-normal distributions: The above estimate of the tolerance interval would only be applicable to a population that is normally distributed, but when the underlying population is often not normal (eg, when there is a natural boundary that the data cannot exceed (ie, 0% or 100%)) it is helpful to define tolerance intervals using nonparametric methods; the one-sided nonparametric tolerance interval can be determined by finding the value for k that satisfies the cumulative binomial equation, where CL is the confidence level (eg, 0.95), and by setting k = 0 (ie, 0 failures) the formula can be simplified.

---

### The snm procedure guideline for general imaging 6.0 [^638aaec4]. SNMMI (2010). Medium credibility.

SNM Procedure Guideline for General Imaging — matrix size and pixel depth in nuclear medicine acquisitions notes that matrix size is almost always a power of 2 with typical values 64 × 64, 128 × 128, 256 × 256 and 512 × 512, and that non-square matrix sizes also exist for whole-body studies. Each pixel can be represented with a single byte (pixel values ranging from 0 to 255 counts) or with 16 bit words (pixel values ranging up to a maximum of 32k or 64k). Overflow occurs when the number of counts recorded at some given position exceeds the maximum number of counts, and overflow is more likely to occur when using a byte matrix.

---

### Uniform labeling of blocks and slides in surgical pathology: guideline from the college of American pathologists pathology and laboratory quality center and the national society for histotechnology [^e6d14a42]. Archives of Pathology & Laboratory Medicine (2015). Medium credibility.

Supplemental Table 4 — Grades for strength of evidence defines Convincing, Adequate, and Inadequate criteria based on study design/execution, challenges, consistency, and generalizability. Convincing includes either two or more Level 1 a or 2 studies (study design and execution) that had an appropriate number and distribution of challenges and reported consistent and generalizable results, or one Level 1 or 2 study that had an appropriate number and distribution of challenges and reported generalizable results. Adequate is defined as two or more Level 1 or 2 studies that lacked the appropriate number and distribution of challenges OR were consistent but not generalizable. Inadequate is defined as combinations of Level 1 or 2 studies that show unexplained inconsistencies OR combinations of one or more lower quality studies (Level 3 or 4) OR expert opinion. Footnotes clarify that challenges are based on number of possible response categories and required confidence in results, that consistency can be assessed formally by testing for homogeneity or less formally using central estimates and range of values when data are limited, and that generalizability is the extension of findings and conclusions from one study to other settings.

---

### Allergen immunotherapy: a practice parameter third update [^924f1626]. The Journal of Allergy and Clinical Immunology (2011). Medium credibility.

Allergen extract dilution calculations — use V₁ × C₁ = V₂ × C₂ to prepare target concentrations and express wt/vol as percentages; for example, preparing V₁ = 5 mL at C₁ = 1:200 from C₂ = 1:10 uses V₂ = [(V₁ × C₁)/C₂], yielding V₂ = [0.025/0.1] = 0.25, and percent expressions include 1:10 = 10%, 1:20 = 5%, and 1:40 = 2.5%.

---

### Surge capacity logistics: care of the critically ill and injured during pandemics and disasters: CHEST consensus statement [^0dde908f]. Chest (2014). Medium credibility.

Surge capacity logistics — Table 4 (continued) specifies Minimum Number per 10 Treatment Spaces for 10 d for critical care devices with reuse/consumable status and duration of use. Chest drains (chlorhexidine, Y-connectors, insertion set, tubing, chamber, sterile water, negative pressure valve and connectors) are Consumable with Duration of need, minimum 6, with the comment Assumption: some patients may require more than one drain, and others may require none. For patient warming/cooling: Regular blankets are Reusable for 10 d with 13 units; Insulating blankets are Consumable with Duration of need and 13 units; Bair Hugger (3M) is Reusable with Duration of need and 2 units; Bair Hugger blankets are Consumable with Duration of need and 13 units; HOTLINE (Smiths Medical) is Reusable with Duration of need and 2 units; High-flow IV lines, high-flow three-way connectors, hot line sets are Reusable with Duration of need and 26 units, and availability highly depends on the type of event anticipated; Pressure bags (for blood/fluid) are Reusable with Duration of need and 10 units. Miscellaneous equipment: a Disposable bath package is Consumable for 2–3 d with 35 units; Nasogastric and orogastric tubes are Consumable with Duration of need and 13 units, noted as a route for enteral nutrition and medications in ventilated patients, and if there are insufficient enteral feeding pumps, bolus feeding by gravity is an acceptable alternative. Nasogastric and orogastric tube ancillary supplies (eg, securing tape, syringe, feeding bags, draining bags) are Consumable with Per institutional preference, with minimums defined as Sustained-use equipment: 13 x units of equipment per patient x 10/duration of use (d) and Daily consumable equipment: 13 x units of equipment per patient per day x 10 d. Optional equipment includes a Continuous heart rate and rhythm monitor, Reusable with Duration of need and 10 units, with the note May consider at least one device capable of cardioversion (for nonpulseless but unstable arrhythmias).

---

### The problem with dichotomizing quality improvement measures [^547f33ef]. BMC Anesthesiology (2022). Medium credibility.

Expressing binary data with continuous variables

Alternatively, it is possible to express compliance with QI measures as the number of cases recorded before a case fails to meet the compliance criteria, thus creating a continuous dataset. For example: the NACOR benchmark for a given measure is 97.58% compliance. Expressed another way, 2.42% of cases (2.42 cases per 100) can be defective ("performance not met") and still satisfy the benchmark. The following expression can be used to determine the minimum number of cases between consecutive noncompliant cases, where X = # defects / month (where defect = case recorded as "performance not met"):

This expression can be further simplified:

Analyzing the number of cases between consecutive defects still employs dichotomization of neuromuscular function to either "performance met" or "performance not met". However, improved characterization of these data can be accomplished with the continuous dataset.

Advantages of continuous data

Continuous data allow for distribution analyses, including mean and variability of a process. Consider the following example. Two anesthesiologists (A and B) demonstrate identical compliance (97.5%) with a QI measure and complete 100 cases each month. Before one case fails compliance, anesthesiologist A and B each complete an average of 40 cases. Although the two anesthesiologists demonstrate similar compliances, there can be differences among the distribution of cases that fail compliance. Knowledge of dataset distribution parameters can better guide the QI processes, especially in conjunction with the appropriate SPC chart.

---

### Principles of analytic validation of immunohistochemical assays: guideline update [^cda8439c]. Archives of Pathology & Laboratory Medicine (2024). High credibility.

Supplemental Digital Content — AMSTAR (Assessing the Methodological Quality of Systematic Reviews) appraisal of methodological quality presents checklist items marked with "√" or "x": "Quality assessed & documented" is "√" and "x"; "Quality used appropriately for conclusion" is "√" and "x"; "Methods to combine used appropriately" is "√" and "√"; "Publication bias assessed" is "x" and "x"; and "COI (conflict of interest)" is "√" and "√". The AMSTAR SCORE /11 is 9 and 5.

---

### COCATS 4 task force 6: training in nuclear cardiology [^84bb05e1]. Journal of the American College of Cardiology (2015). Medium credibility.

Nuclear cardiology training requirements — Table 3 specifies Level I and Level II minimums: Level I requires a minimum duration of training of 2 months with a minimum number of examinations of 100, and Level II requires 4 months with 300; these are approximate cumulative numbers of examinations, and at least 30 cases with hands-on experience must be performed and interpreted under supervision.

---

### Dimension-agnostic and granularity-based spatially variable gene identification using BSP [^7b9ac05c]. Nature Communications (2023). High credibility.

3D simulation on FISH data with discrete spatial patterns

To further evaluate the model's performance on data with locally influential discrete patterns (Fig. 4D), we designed a set of 3D simulation scenarios within the ranges of x- and y- coordinates from 0 to 30. The local discrete spatial patterns were constructed using solid spheres with a center-to-center distance of 8 units. Specifically, sixteen center points were selected, with fixed z-coordinates of 5.5, and the x- and y-coordinates were generated from a sequence of numbers from 3 to 27 with an interval of 8. To introduce randomness into spatial patterns, we incorporated a uniformly distributed random variable ranging from −2 to 2 and added it to the coordinates of each center point. The cells within the spheres were marked, and expression values were assigned as Section 4.3.1.

In these simulations, SVGs were generated using the Irregular lump pattern described in Section 4.3.1. We created 500 SVGs with locally influential discrete patterns and an additional 500 SVGs with globally influential continuous patterns, along with 9000 non-SVGs as permutated genes without any spatial pattern. We considered three scenarios to compare the effects of pattern sizes, signal strengths, and noise levels, respectively. We adjusted radius values from 1.5 to 2.0 (small and moderate), expected expression fold changes from 2 to 2.5 (small and moderate), and noise levels from 2.0 to 3.0 (moderate and high). To ensure robustness, ten replicates were generated to perform the power analysis.

---

### Nonlinear topological symmetry protection in a dissipative system [^31b67b9b]. Nature Communications (2024). High credibility.

For this experiment, we have accumulated a sequence of 2.4 million individual events and expressed them as zero and one binary values. Due to the limitations of our oscilloscope, this data was assembled from four different batches acquired in quick succession over a few seconds without adjusting any parameters. The randomness of the acquired sequence was then rigorously assessed using the NIST Statistical Test Suite for random number generation (NIST STS-2.1.2), which consists of a series of statistical tests used to detect non-randomness within a given data set. In each test, a p -value is generated, which gives the probability of this result occurring under the null hypothesis. We consider a p -value of 0.01 or lower to be evidence of non-randomness or failure. NIST recommends that the minimum pass rate for each statistical test should be 96% (the whole sequence is partitioned into 100 individual sub-sequences for testing). The results are presented in Fig. 4, which shows that all tests are passed with a proportion well over 96%, highlighting that we have no evidence of any bias in the SSB state selection process. Additional data gathered as the phase defect is varied around π further support this finding (see Supplementary Section IV and Supplementary Fig. S4). This confirms the robustness of the exchange symmetry provided by the Möbius topology in our experiment.

Fig. 4
Test results for randomness of the P2 SSB state selection process using the NIST statistical test suite.

The bars indicate the percentage of sequences (a total of 2.4 million events partitioned into 100 sub-sequences) that pass the tests with a significance level of 0.01. The recommended minimum pass rate of 96% (black dotted line) is satisfied for all tests. The normalized driving power was set to X ≃ 12.

---

### Defining the time-limited trial for patients with critical illness: an official American Thoracic Society workshop report [^d28d917f]. Annals of the American Thoracic Society (2024). High credibility.

Time-limited trials — framing to promote collaboration outlines in Table 3 that they "Should Be" "Framed by uncertainty", "Transparent", "Individualized and adaptable", "Iterative", "Oriented to patients' goals and priorities", "Trials of high-quality, standard-of-care therapies", and "A collaborative process between patients, surrogates, and clinicians", and "Should Not Be" "Framed by notions of "poor prognosis", "Coercive", "Prescriptive or binding", "Time pressured or finite", "Oriented to one specific outcome", "Lower quality care", or "Determined exclusively by clinicians".

---

### Guidelines for handling decedents contaminated with radioactive materials [^3c500666]. CDC (2007). Medium credibility.

Rule of Seven for radiation dose rates — fallout dose rate after a detonation can be calculated and estimated as follows: "Dose rate = Initial dose rate × t⁻¹.²", and "the dose rate decreases by a factor of 10 for each sevenfold increase in time following a fission detonation"; this rule "is valid for only the first 2 weeks after a detonation", and "rain accelerates the decrease". Table values indicate percentage of initial dose rate remaining of 100% at 0 hour, 10% at 7 hours, 1% at 49 hours (2 days), and 0.1% at 343 hours (14 days).

---

### Estimating the statistical significance of gene expression changes observed with oligonucleotide arrays [^ba8afa58]. Human Molecular Genetics (2002). Low credibility.

We present a simple method to assign approximate P-values to gene expression changes detected with Affymetrix oligonucleotide arrays and software. The method pools data for groups of genes and a small number of like-to-like comparisons in order to estimate the significance of changes observed for single genes in comparisons of experimental interest. Statistical significance levels are based on the observed variability in the fractional majority of probe pairs that indicate increasing or decreasing differential expression in comparisons of technical replicates. From this reference distribution or error model, we compute the expected frequency for fractional majorities in comparisons for N ≥ 2. These computed distributions are the source of P-value estimates for changes seen in the experimental comparisons. The method is intended to complement the Affymetrix software and to rationalize gene selection for experimental designs involving limited replication.

---

### Fundamental limits to learning closed-form mathematical models from data [^bfc95d4c]. Nature Communications (2023). High credibility.

Sampling models with the Bayesian machine scientist

For a given dataset D, each model m has a description lengthgiven by Eq. (4). The Bayesian machine scientistgenerates a Markov chain of models { m 0, m 1, …, m T } using the Metropolis algorithm as described next.

Each model is represented as an expression tree, and each new model m t +1 in the Markov chain is proposed from the previous one m t by changing the corresponding expression tree: changing an operation or a variable in the expression tree (for example, proposing a new model m t +1 = θ 0 + x 1 from m t = θ 0 * x 1); adding a new term to the tree (for example, m t +1 = θ 0 * x 1 + x 2 from m t = θ 0 * x 1); or replacing one block of the tree (for example, from m t = θ 0 * x 1) (see ref.for details). Once a new model m t +1 is proposed from m t, the new model is accepted using the Metropolis rule.

Note that the only input to the Bayesian machine is the observed data D. In particular, the observational noise s ϵ is unknown and must be estimated, via maximum likelihood, to calculate the Bayesian information criterion B (m) of each model m.

Artificial neural network benchmarks

For the analysis of predictions on unobserved data, we use as benchmarks the following artificial neural network architectures and training procedures. The networks consist of: an input layer with two inputs corresponding to x 1 and x 2; (ii) four hidden fully connected feed-forward layers, with 10 units each and ReLU activation functions; and (iii) a linear output layer with a single output y.

Each network was trained with a dataset D containing N points, just as in the probabilistic model selection experiments. Training errors and validation errors (computed on an independent set) were calculated, and the training process stopped when the validation error increased, on average, for 100 epochs; this typically entailed training for 1000–2000 epochs. This procedure was repeated three times, and the model with the overall lowest validation error was kept for making predictions on a final test set. The predictions onare those reported in Fig. 1.

---

### Evaluating molecular biomarkers for the early detection of lung cancer: when is a biomarker ready for clinical use? An official American Thoracic Society policy statement [^4009df9d]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

American Thoracic Society policy statement — analytic performance characteristics and result types are defined. Limit of detection is "The lowest assay level at which the presence of the analyte is detected with reliability in repeated measurement". Linearity is "The ability to provide results directly proportional to the amount of analyte in the test sample within a given measuring range". Measurement accuracy is "The closeness of agreement between a measurement result and an accepted reference value; an aggregate of trueness and precision". Performance around the cutoff is "The measurement accuracy of an assay at biomarker levels near the threshold chosen to distinguish a positive and negative result for the intended use of the test". Precision is "The closeness of agreement of replicate test results under stipulated conditions", with repeatability defined as "Precision when replicate measurements are taken under the same conditions (within a run)" and reproducibility as "Precision when one of the conditions being varied is the laboratory for an in vitro diagnostic measurement". A qualitative result is "A biomarker result consisting of a set number of possible responses (often two)", whereas a quantitative result is "A biomarker result that is numerical in amount or level of a physical quantity", and semiquantitative denotes "Results of a test that fall into an approximate range of values". Signature refers to "Multiple variables combined to provide a single result", and medical tests are "Results of a clinical, imaging, or laboratory-based assay that are used alone or with other information to help assess a subject's health condition of interest, or target condition".

---

### Insights from a real-world experience with inclisiran at a large United States lipid clinic [^481f6a58]. Journal of Clinical Lipidology (2025). High credibility.

Table 2 — absolute and percent changes in lipid measures (mean ± SD) with inclisiran show baseline (n = 60), first follow up (n = 60), and last follow-up (n = 30) values: LDL-C 107 ± 47.1 to 66 ± 41.2 (— 38%) to 65 ± 38.6 (— 39%); non-HDL-C 130 ± 50.5 to 92 ± 49.4 (— 29%) to 88 ± 42.6 (— 32%); HDL-C 56 ± 18.2 to 57 ± 16.6 (+1%) to 58 ± 16.4 (+2%); total cholesterol 187 ± 51.4 to 146 ± 47.5 (— 22%) to 146 ± 44.7 (— 22%); and triglycerides 116 ± 61.5 to 116 ± 67.6 (0%) to 116 ± 68.6 (0%). Timepoints were defined as first follow-up at least 30 days after the first dose (134 ± 85 days) and last follow-up at data collection (280 ± 116 days), with values in mg/dL.

---

### Clinical practice guidelines for hemodialysis adequacy, update 2006 [^6b37ae06]. American Journal of Kidney Diseases (2006). Medium credibility.

Minimally adequate hemodialysis — dose targets and frequency considerations for more-frequent schedules: In developing this CPR, the Work Group decided to target a minimum dialysis dose equivalent to an stdKt/V level of 2.0 per week, and this is the level obtained when one analyzes using a thrice-weekly schedule to an spKt/V of approximately 1.2 per treatment over 3.5 hours. In the absence of RKF, it is not possible to reach an stdKt/V of 2.0 by using a twice-weekly schedule, and kinetic modeling examined spKt/V per treatment needed to reach a weekly stdKt/V value of 2.0 for twice-weekly to 7-times-weekly schedules with dialysis treatment times ranging from 2 to 8 hours. These spKt/V values should be considered minimum values, not target values, and extending dialysis time is much more effective for controlling solute levels when frequency is increased to 4 to 7 treatments per week. For nocturnal HD, similar spKt/V values can be determined for 8-hour treatments and the Kt/V for an 8-hour treatment will be greater than 1.0; hence, the Work Group did not believe that adequacy determined by predialysis or postdialysis BUN monitoring is appropriate for nocturnal HD schedules. Given limited research maturity, the Work Group decided to refrain from proposing guidelines regarding minimally adequate therapy given more frequently than 3 times per week.

---

### Substantial underestimation of SARS-CoV-2 infection in the United States [^653b4163]. Nature Communications (2020). High credibility.

Accurate estimates of the burden of SARS-CoV-2 infection are critical to informing pandemic response. Confirmed COVID-19 case counts in the U.S. do not capture the total burden of the pandemic because testing has been primarily restricted to individuals with moderate to severe symptoms due to limited test availability. Here, we use a semi-Bayesian probabilistic bias analysis to account for incomplete testing and imperfect diagnostic accuracy. We estimate 6,454,951 cumulative infections compared to 721,245 confirmed cases (1.9% vs. 0.2% of the population) in the United States as of April 18, 2020. Accounting for uncertainty, the number of infections during this period was 3 to 20 times higher than the number of confirmed cases. 86% (simulation interval: 64–99%) of this difference is due to incomplete testing, while 14% (0.3–36%) is due to imperfect test accuracy. The approach can readily be applied in future studies in other locations or at finer spatial scale to correct for biased testing and imperfect diagnostic accuracy to provide a more realistic assessment of COVID-19 burden.

---

### Experimental demonstration of quantum advantage for one-way communication complexity surpassing best-known classical protocol [^2c59f7d2]. Nature Communications (2019). High credibility.

Fig. 1
The Sampling Matching problem. Alice gets an input x ∈ {0, 1} n and sends a message m (x) to Bob who outputs the pair 〈(k, l) ∈ σ i, b = x k ⊕ x l 〉 for a matching σ i, whose distribution is uniform in, even conditioned on m (x). The parity should be correct with high probability for all choices of the matching

Fig. 2
Illustration of a set of edge-disjoint perfect matchings for size n = 4. The matching sethas three edge-disjoint perfect matchings: [σ 1 :{(1, 2), (3, 4)}; σ 2 :{(1, 3), (2, 4)}; σ 3 :{(1, 4), (2, 3}]

It is also important to note that in our one-way model the communication cost of a protocol is the number of bits Alice has to send to Bob in order to solve the problem, while the transmitted information, instead of the number of bits sent, calculates the real bits of information about the inputs that the messages carry. For example, if Alice always sends the same, long message, independent of her input, then the communication cost will be large, while the transmitted information will be zero, since no information about her input has been transmitted. Transmitted information is a resource that is important for privacy, when on top of having an efficient protocol, we want the players to solve the task without learning much about the other player's input. One can define the transmitted information as the mutual information between the messages and the inputs and can upper bound it with the logarithm of the number of different messages. The transmitted information is always at most the communication cost, since one bit carries at most one bit of information, and hence the bottleneck is always the time. Last, we remark that we define our problem in the randomized setting where Bob is allowed to use random coins and output the correct value with high probability.

---

### Surge capacity logistics: care of the critically ill and injured during pandemics and disasters: CHEST consensus statement [^973cab0b]. Chest (2014). Medium credibility.

Suggested medical equipment for critical care surge — IV fluids and peripheral IV supplies: IV crystalloid solution is listed with 4–5 L, day 1 2–3 L, days 2 and 3 1–2 L/d thereafter and a minimum number of 200 L per 10 treatment spaces for 10 d; peripheral IV (alcohol, swabs, catheter, tape, regular and blood administration sets, needleless caps) is consumable used for 4 d with a minimum number of 65 per 10 treatment spaces for 10 d.

---

### 2022 American society of anesthesiologists practice guidelines for management of the difficult airway [^527ec6e6]. Anesthesiology (2022). High credibility.

Practice Guidelines for Difficult Airway Management — evidence grading and study selection are specified, with literature classifications reported for each intervention as follows: Category A, level 1, meta-analysis of randomized controlled trials; Category A, level 2, multiple randomized controlled trials; Category A, level 3, a single randomized controlled trial; Category B, level 1, nonrandomized studies with group comparisons; Category B, level 2, nonrandomized studies with associative findings; Category B, level 3, nonrandomized studies with descriptive findings; and Category B, level 4, case series or case reports. Statistically significant outcomes (P < 0.01) are designated as either beneficial (B) or harmful (H) to the patient; statistically nonsignificant findings are designated as equivocal (E), and when available, Category A evidence is given precedence over Category B evidence for any particular outcome. The lack of sufficient scientific evidence in the literature is reported in the text of the guidelines as "insufficient evidence". Criteria for literature acceptance included randomized controlled trials, prospective nonrandomized comparative studies (e.g., quasiexperimental, cohort), retrospective comparative studies (e.g., case control), observational studies (e.g., correlational or descriptive statistics), and case reports or case series from peer-reviewed journals, and exclusion criteria encompassed items such as articles with no original data and retracted publications as listed. All meta-analyses are conducted by the ASA methodology group and meta-analyses from other sources are reviewed but not included as evidence in this document; a minimum of five independent randomized controlled trials (i.e., sufficient for fitting a random-effects model) is required for meta-analysis. Survey findings from task force–appointed expert consultants and samples of the memberships of ASA and participating organizations are reported in appendix 2, and survey responses for each recommendation are reported using a five-point scale based on median values from strongly agree to strongly disagree.

---

### An expectation-maximization algorithm for the analysis of allelic expression imbalance [^80a63f76]. American Journal of Human Genetics (2006). Low credibility.

A significant proportion of the variation between individuals in gene expression levels is genetic, and it is likely that these differences correlate with phenotypic differences or with risk of disease. Cis-acting polymorphisms are important in determining interindividual differences in gene expression that lead to allelic expression imbalance, which is the unequal expression of homologous alleles in individuals heterozygous for such a polymorphism. This expression imbalance can be detected using a transcribed polymorphism, and, once it is established, the next step is to identify the polymorphisms that are responsible for or predictive of allelic expression levels. We present an expectation-maximization algorithm for such analyses, providing a formal statistical framework to test whether a candidate polymorphism is associated with allelic expression differences.

---

### Using both qualitative and quantitative data in parameter identification for systems biology models [^e33a4be7]. Nature Communications (2018). Medium credibility.

Methods

Polynomial model

A system of linear equations for computing the polynomial coefficients was implemented with the Python Numpy module. We suppose we have n + 1 equally spaced qualitative data points, which divide the range x = [0, 10] into n equally-sized intervals. Exactly two of these intervals will be bounded on one end by a "−" point and on the other by a "+" point; these two intervals must contain the intersection points x 1 and x 2.

To generate Fig. 1b, within the possible intervals for x 1 and x 2, we sampled all possible combinations of 100 (evenly spaced) values of x 1 and 100 values of x 2, and solved for the coefficients. Considering only the fits that yielded positive values for all coefficients, we report the minimum and maximum possible values for each coefficient.

Raf inhibitor model

The model shown in Fig. 2a was implemented in Python (Supplementary Software 1). We generated synthetic data points at various values of I and constant R tot = 50 μM, and populated the other species concentrations based on the equations of the model. To generate quantitative data, we calculated = (RI + RIR + 2RIRI)/ R tot, and perturbed results by adding normally distributed noise with standard deviation 0.1. To generate qualitative data, we calculated A = RR + RIR, and compared the result to the value A (0) = 15.24. If the difference from A (0) was < 1.5, the data point was labeled as "0" (within error), otherwise it was labeled as "+" or "−" appropriately.

For fitting, the objective function was created as described in Results. Minimization was performed with the Scipy function optimize.differential_evolution, with a maximum of 1000 iterations, strategy of "best1exp", and search range of [10⁻⁴, 10⁴] for each parameter.

To perform profile likelihood analysis, for each parameter, we considered 100 possible fixed values (log uniformly distributed in the range [10², 10⁵] for K₃ and [10⁻³, 10⁰] for K₅). At each fixed value considered, minimization was performed by the same method as above, except that the parameter of interest was held at the fixed value. We report the resulting minimum objective function values in the profile likelihood plots (Fig. 2d, e).

---

### KDOQI US commentary on the 2017 KDIGO clinical practice guideline on the evaluation and care of living kidney donors [^bc193d94]. American Journal of Kidney Diseases (2020). High credibility.

KDOQI commentary — living kidney donor risk prediction tool limitations and related consent guidance are described as follows: The tool "does not include family history of kidney disease or donation to an immediate relative with kidney failure" and "does not include other important clinical variables", such that "for donor candidates with these risk factors, the tool may underestimate risk". It "was constructed using a life-table approach" with follow-up "from only 4 to 16 years", whereas donor risk spans "multiple decades", and "The lifetime estimates generated are therefore inherently imprecise approximations". The website "(http://www.transplantmodels.com/esrdrisk) does not readily acknowledge most of these limitations", and outcomes compared were "only to 15 years postdonation". Accordingly, "The tool should be used only with a clear understanding that it may well generate incorrect information about long-term outcomes" and "should be considered as a possibly valuable step". An "additional predictive tool by Massie et al" exists, but "Both the pre- and postdonation tools require additional validation and study before being considered standard of care". Given current constraints, "the lack of tools to estimate future risk with precision makes the goal unreachable at this time", and programs "currently lack both the data and the experience to be able to follow the KDIGO recommendation to set their own acceptable absolute thresholds for long-term risks of kidney failure". The commentary "question[s] the prescription in recommendation 1.11 to express risks 'as absolute as well as relative risks'", agreeing the absolute risk is key because the increase "will usually be relatively low", while stating informed consent would include "the disclosure of the best-available estimates of relative risk of kidney failure due to kidney donation", and it notes recommendation "1.17" "is a UNOS requirement to provide what is termed the 'Independent Living Donor Advocate'.

---

### 2022 American society of anesthesiologists practice guidelines for management of the difficult airway [^4991ac84]. Anesthesiology (2022). High credibility.

American Society of Anesthesiologists difficult airway guidelines — evidence grading and significance reporting are specified as: Category A, level 1, meta-analysis of randomized controlled trials; Category A, level 2, multiple randomized controlled trials; Category A, level 3, a single randomized controlled trial; Category B, level 1, nonrandomized studies with control group comparisons; Category B, level 2, nonrandomized studies with associative findings; Category B, level 3, nonrandomized studies with descriptive findings; and Category B, level 4, case series or case reports. Statistically significant outcomes (P < 0.01) are designated as beneficial (B) or harmful (H), with statistically nonsignificant findings designated as equivocal (E). When available, Category A evidence is given precedence over Category B evidence for any particular outcome, and lack of sufficient scientific evidence is reported as "insufficient evidence". All meta-analyses are conducted by the ASA methodology group, and a minimum of five independent randomized controlled trials is required for meta-analysis. Survey responses for each recommendation are reported using a five-point scale based on median values from strongly agree to strongly disagree.

---

### Gene-expression measurement: variance-modeling considerations for robust data analysis [^4e2bc0ca]. Nature Immunology (2012). Medium credibility.

System-wide measurements of gene expression by DNA microarray and, more recently, RNA-sequencing strategies have become de facto tools of modern biology and have led to deep understanding of biological mechanisms and pathways. However, analyses of the measurements have often ignored statistically robust methods that account for variance, resulting in misleading biological interpretations.

---

### Principles of analytic validation of immunohistochemical assays: guideline update [^0cfec0c6]. Archives of Pathology & Laboratory Medicine (2024). High credibility.

Supplemental Table 3 — certainty of evidence — defines categories with verbatim descriptions derived from GRADE. High: "There is high confidence that available evidence reflects true effect. Further research is very unlikely to change the confidence in the estimate of effect. Included studies will be of high or intermediate quality". Moderate: "There is moderate confidence that available evidence reflects true effect. Further research is likely to have an important impact on the confidence in estimate of effect and may change the estimate. Included studies will be of intermediate or low quality". Low: "There is limited confidence in the estimate of effect. The true effect may be substantially different from the estimate of the effect. Included studies will be of low quality". Very Low: "There is very little confidence in the estimate of effect. The true effect is likely to be substantially different from the estimate of effect. Any estimate of effect is very uncertain. Included studies will be of low or very low quality".

---

### Healthy housing reference manual [^d4c68057]. CDC (2006). Medium credibility.

Reducing heat loss and condensation — window energy efficiency metrics are defined. The energy efficiency of windows is measured in terms of their U-values (measure of the conductance of heat) or their R-values.

---

### Summary benchmarks-full set – 2024 [^ac55ce29]. AAO (2024). High credibility.

Idiopathic macular hole management — stages 3 or 4: For stage 3 or 4, vitreoretinal surgery is performed usually within a few weeks to a few months of diagnosis, with postoperative follow-up at 1–2 days, then 1–2 weeks during which time strict face down positioning is advised; frequency and timing of subsequent postoperative visits varies depending on the outcome of surgery and the patient's clinical course.

---

### SNMMI procedure standard / EANM practice guideline on pediatricF-FDG PET / CT for oncology 1.0 [^aa5441aa]. Journal of Nuclear Medicine (2021). High credibility.

Pediatric FDG PET/CT CT technique ranges — CTDIvol (mGy) guidance for chest+abdomen+pelvis acquisitions by size is summarized: for Weight 2.5–12.2 kg, Lateral distance 7–11 cm, Average age < 1 y, CTAC 0.6–0.8, CTAC+CTL 1.1–1.5, CTAC+CTL+Dx 1.9–2.5; for Weight 14.7–45.6 kg, Lateral distance 19–23 cm, Average age 5–10 y, CTAC 0.8–1.3, CTAC+CTL 1.6–2.6, CTAC+CTL+Dx 2.7–4.3; for Weight 40.5–95.7 kg, Lateral distance 29–33 cm, Average age > 15 y, CTAC 1.6–2.6, CTAC+CTL 3.1–5.3, CTAC+CTL+Dx 5.2–8.8. The footnote notes, "To convert the above CTDIvol 32-cm phantom measurements to the 16-cm phantom, multiply these values by 2".

---

### Modeling and forecasting the total number of cases and deaths due to pandemic [^4243a391]. Journal of Medical Virology (2022). Medium credibility.

The COVID-19 pandemic has appeared as the predominant disease of the 21st century at the end of 2019 and was a drastic start with thousands of casualties and the COVID-19 victims in 2020. Due to the drastic effect, COVID-19 scientists are trying to work on pandemic diseases and Governments are interested in the development of methodologies that will minimize the losses and speed up the process of cure by providing vaccines and treatment for such pandemics. The development of a new vaccine for any pandemic requires long in vitro and in vivo trials to use. Thus the strategies require understanding how the pandemic is spreading in terms of affected cases and casualties occurring from this disease, here we developed a forecasting model that can predict the no of cases and deaths due to pandemic and that can help the researcher, government, and other stakeholders to devise their strategies so that the damages can be minimized. This model can also be used for the judicial distribution of resources as it provides the estimates of the number of casualties and number of deaths with high accuracy, Government and policymakers on the basis of forecasted value can plan in a better way. The model efficiency is discussed on the basis of the available dataset of John Hopkins University repository in the period when the disease was first reported in the six countries till the mid of May 2020, the model was developed on the basis of this data, and then it is tested by forecasting the no of deaths and cases for next 7 days, where the proposed strategy provided excellent forecasting. The forecast models are developed for six countries including Pakistan, India, Afghanistan, Iran, Italy, and China using polynomial regression of degrees 3–5. But the models are analyzed up to the 6th-degree and the suitable models are selected based on higher adjusted R-square (R 2) and lower root-mean-square error and the mean absolute percentage error (MAPE). The values of R 2 are greater than 99% for all countries other than China whereas for China this R 2 was 97%. The high values of R 2 and Low value of MAPE statistics increase the validity of proposed models to forecast the total no cases and total no of deaths in all countries. Iran, Italy, and Afghanistan also show a mild decreasing trend but the number of cases is far higher than the decrease percentage. Although India is expected to have a consistent result, more or less it depicts some other biasing factors which should be figured out in separate research.

---

### Validating whole slide imaging for diagnostic purposes in pathology: guideline from the college of American pathologists pathology and laboratory quality center [^19dfc59e]. Archives of Pathology & Laboratory Medicine (2013). Medium credibility.

Supplemental Table 5. Evidence to Decision Summary for Recommendation 1 — For the question "Is the problem a priority?", responses were 0 No, 0 Probably No, 30% (3/10) Probably Yes, 70% (7/10) Yes, 0 Varies, and 0 Don't Know. For "How accurate is the test?", selections were 0 Very Inaccurate, 0 Inaccurate, 70% (7/10) Accurate, 20% (2/10) Very Accurate, 10% (1/10) Varies, and 0 Don't Know. For "How substantial are the desirable anticipated effects?", selections were 0 Trivial, 0 Small, 30% (3/10) Moderate, 60% (6/10) Large, 10% (1/10) Varies, and 0 Don't Know. For "How substantial are the undesirable anticipated effects?", selections were 50% (5/10) Large, 10% (1/10) Moderate, 20% (2/10) Small, 20% (2/10) Trivial, and 0 Varies.

---

### Principles of analytic validation of immunohistochemical assays: guideline update [^02cc989d]. Archives of Pathology & Laboratory Medicine (2024). High credibility.

Supplemental Table 4 — AMSTAR risk of bias assessment of included systematic reviews — lists two reviews (Girolami et al, 2022 and Voidazan et al, 2022) with the following item ratings: "A priori design √ √"; "Duplicate study selection & data extraction √ √"; "Comprehensive lit search performed √ x"; "Grey lit used √ x"; "List included & excluded studies x x"; and "Characteristics of included studies provided √ √".

---

### MRNAs, proteins and the emerging principles of gene expression control [^2d1f7bae]. Nature Reviews: Genetics (2020). High credibility.

Gene expression involves transcription, translation and the turnover of mRNAs and proteins. The degree to which protein abundances scale with mRNA levels and the implications in cases where this dependency breaks down remain an intensely debated topic. Here we review recent mRNA-protein correlation studies in the light of the quantitative parameters of the gene expression pathway, contextual confounders and buffering mechanisms. Although protein and mRNA levels typically show reasonable correlation, we describe how transcriptomics and proteomics provide useful non-redundant readouts. Integrating both types of data can reveal exciting biology and is an essential step in refining our understanding of the principles of gene expression control.

---

### Evaluation after a first seizure in adults [^b94f7748]. American Family Physician (2022). High credibility.

Regarding diagnostic investigations for first seizure in adults, more specifically with respect to initial assessment, AAFP 2022 guidelines recommend to evaluate for provoking factors after a first seizure, such as inflammatory, infectious, structural, toxic, or metabolic causes.

---

### A stochastic vs deterministic perspective on the timing of cellular events [^139cb843]. Nature Communications (2024). High credibility.

Cells are the fundamental units of life, and like all life forms, they change over time. Changes in cell state are driven by molecular processes; of these many are initiated when molecule numbers reach and exceed specific thresholds, a characteristic that can be described as "digital cellular logic". Here we show how molecular and cellular noise profoundly influence the time to cross a critical threshold-the first-passage time-and map out scenarios in which stochastic dynamics result in shorter or longer average first-passage times compared to noise-less dynamics. We illustrate the dependence of the mean first-passage time on noise for a set of exemplar models of gene expression, auto-regulatory feedback control, and enzyme-mediated catalysis. Our theory provides intuitive insight into the origin of these effects and underscores two important insights: (i) deterministic predictions for cellular event timing can be highly inaccurate when molecule numbers are within the range known for many cells; (ii) molecular noise can significantly shift mean first-passage times, particularly within auto-regulatory genetic feedback circuits.

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^675fa3aa]. Nature Communications (2025). High credibility.

Additionally, we adopted here the common convention used in machine learning of one-hot encoding to specify classification outcomes, but biologically, it may be more realistic to specify whole profiles of chemical concentrations as computational outputs. In previous workwe showed analytically that the ratiois independent of F i j for any k and, which can be shown to imply that the monotonicity constraint holds under any linear mapping. Thus, the expressivity limitations identified in this work should at least hold for output profiles that are arbitrary linear transformations of one-hot encoded outputs. By considering the encoding-agnostic objective of maximizing the mutual information between the output distribution π and the input distribution, we have provided preliminary evidence that the expressive capacity of the system increases with M, independent of specific encoding schemes.

Finally, an additional aspect of physics that deserves attention in the future is that chemical dynamics are inherently stochastic, and fluctuations about the steady-state mean are important, especially when copy numbers are small. Decision-making under fluctuations has often been treated using the framework of information theory. A general trend from this line of research is that maximizing information flow requires reducing fluctuations, which in turn requires greater expenditure of chemical free energy. On the other hand, some forms of chemical computation harness stochasticity to generatively model probability distributions. Combining insights from these works with our results on classification expressivity can help paint a full picture of how biochemical systems use fuzzy logic to make decisions.

---

### Lessons from applied large-scale pooling of 133, 816 SARS-CoV-2 RT-PCR tests [^eea0cc9e]. Science Translational Medicine (2021). Medium credibility.

Pooling multiple swab samples before RNA extraction and real-time reverse transcription polymerase chain reaction (RT-PCR) analysis has been proposed as a strategy to reduce costs and increase throughput of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) tests. However, reports on practical large-scale group testing for SARS-CoV-2 have been scant. Key open questions concern reduced sensitivity due to sample dilution, the rate of false positives, the actual efficiency (number of tests saved by pooling), and the impact of infection rate in the population on assay performance. Here, we report an analysis of 133,816 samples collected between April and September 2020 and tested by Dorfman pooling for the presence of SARS-CoV-2. We spared 76% of RNA extraction and RT-PCR tests, despite the frequently changing prevalence (0.5 to 6%). We observed pooling efficiency and sensitivity that exceeded theoretical predictions, which resulted from the nonrandom distribution of positive samples in pools. Overall, our findings support the use of pooling for efficient large-scale SARS-CoV-2 testing.

---

### Defining the time-limited trial for patients with critical illness: an official American Thoracic Society workshop report [^f52940f6]. Annals of the American Thoracic Society (2024). High credibility.

Table 2: Major themes in defining time-limited trials in critical care — Healthcare literature reflects operational approaches (essential elements and pragmatic steps) and conceptual approaches (objectives and rationales). Operational themes include Interventions (life-sustaining care in general, specific life-sustaining therapies) and Engagement with patients or surrogates (agreement, planning, standardization). Conceptual themes include Prognosis (uncertain, poor) and Decision making (uncertainty, readiness, disagreement).

---

### Standards of care in diabetes – 2025 [^1462a151]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for hypoglycemia, more specifically with respect to pregnant patients, ADA 2025 guidelines recommend to offer continuous glucose monitoring to pregnant patients with T1DM. Consider obtaining real-time continuous glucose monitoring to reduce the risk for large-for-gestational-age infants and neonatal hypoglycemia in pregnancy complicated by T1DM, in conjunction with aims to achieve traditional pre- and postprandial glycemic goals.

---

### Individualized prescriptive inference in ischaemic stroke [^5e9fe0d5]. Nature Communications (2025). High credibility.

The gold standard in the treatment of ischaemic stroke is set by evidence from randomized controlled trials, typically using simple estimands of presumptively homogeneous populations. Yet the manifest complexity of the brain's functional, connective, and vascular architectures introduces heterogeneities that violate the underlying statistical premisses, potentially leading to substantial errors at both individual and population levels. The counterfactual nature of interventional inference renders quantifying the impact of this defect difficult. Here we conduct a comprehensive series of semi-synthetic, biologically plausible, virtual interventional trials across 100M+ distinct simulations. We generate empirically grounded virtual trial data from large-scale meta-analytic connective, functional, genetic expression, and receptor distribution data, with high-resolution maps of 4K+ acute ischaemic lesions. Within each trial, we estimate treatment effects using models varying in complexity, in the presence of increasingly confounded outcomes and noisy treatment responses. Individualized prescriptions inferred from simple models, fitted to unconfounded data, are less accurate than those from complex models, even when fitted to confounded data. Our results indicate that complex modelling with richly represented lesion data may substantively enhance individualized prescriptive inference in ischaemic stroke.

---

### A tweezer array with 6, 100 highly coherent atomic qubits [^da43db31]. Nature (2025). Excellent credibility.

The second method we use to characterize imaging fidelity and survival requires no assumption for the photon distribution but considers that the imaging survival and fidelity is identical for three successive images. We start by estimating the probabilityof the presence of an atom in three images being x 1 x 2 x 3, in which x i is a Boolean, equal to 1 if there is an atom and 0 if there is none,

Here S is the survival probability during imaging and F is the initial filling fraction. From this, we can estimate the probability of detecting y 1 y 2 y 3 as. The conditional probabilities on the detection categorization given the true atomic presence are, and.

We use the method of least squares to minimize the difference between the experimental frequencies of bitstrings y 1 y 2 y 3 and theby tuning the four parameters F, S,… The imaging fidelity is then defined as. The array-wide binarization threshold is chosen to maximize the imaging fidelity (Extended Data Fig. 4c). Using this method, we find an imaging fidelity, with a false-positive probabilityand a false-negative probability; we find the survival to be S = 0.999864(2), slightly lower than the steady-state imaging survival probability measured by repeated imaging. Finally, we can inject the model-free survival probability into the lossy Poisson model to increase its accuracy (trying to extract the loss directly from the lossy Poisson model would indeed be inaccurate, because losses appear as a small tail feature between the two peaks of the imaging histogram). Using this approach, and fitting each site independently, we find an average imaging fidelity of 99.992(1)%, in reasonable agreement with the model-free imaging fidelity. By setting the atom loss to zero while keeping the other five fit parameters constant for each site, we can estimate a hypothetical imaging fidelity in the absence of atomic loss of 99.999(1)%. This analysis also illustrates that fitting the imaging histogram with a Gaussian or Poissonian model without including losses leads to overestimating imaging fidelities.

---

### Pediatric eye evaluations preferred practice pattern [^381bfb19]. Ophthalmology (2023). High credibility.

Pediatric refractive error thresholds for isometropia and anisometropia — Values were generated by consensus and are presented as general guidelines that may differ among age groups; for isometropia, myopia thresholds are 5.00 or more, 4.00 or more, 3.00 or more, and 2.50 or more; hyperopia (no manifest deviation) thresholds are 5.00 or more, 5.00 or more, 4.50 or more, and 3.50 or more; hyperopia with esotropia thresholds are 1.50 or more, 1.00 or more, 1.00 or more, and 1.00 or more; and astigmatism thresholds are 3.00 or more, 2.50 or more, 2.00 or more, and 1.50 or more. For anisometropia (without strabismus), the values represent the minimum difference in the magnitude of refractive error between eyes that would prompt refractive correction, with myopia 4.00 or more, 3.00 or more, 3.00 or more, and 2.50 or more; hyperopia 2.50 or more, 2.00 or more, 1.50 or more, and 1.50 or more; and astigmatism 2.50 or more, 2.00 or more, 2.00 or more, and 1.50 or more. These values were generated by consensus and are based solely on professional experience and clinical impressions, the exact values are unknown and may differ among age groups, for the youngest children an evaluation every 3 to 6 months may be needed to evaluate progression, and threshold for correction of anisometropia should be lower if the child has strabismus or amblyopia.

---

### Acid-base interpretation: a practical approach [^abc8e566]. American Family Physician (2025). High credibility.

Regarding diagnostic investigations for blood gas analysis, more specifically with respect to general principles, AAFP 2025 guidelines recommend to obtain pH and pCO2 on blood gas to determine the primary process of an acid-base disorder. Recognize that abnormal values increasing or decreasing in the same direction indicate a metabolic process, while those shifting in opposite directions indicate a respiratory process.

---

### Optimal gene expression analysis by microarrays [^15b18110]. Cancer Cell (2002). Low credibility.

DNA microarrays make possible the rapid and comprehensive assessment of the transcriptional activity of a cell, and as such have proven valuable in assessing the molecular contributors to biological processes and in the classification of human cancers. The major challenge in using this technology is the analysis of its massive data output, which requires computational means for interpretation and a heightened need for quality data. The optimal analysis requires an accounting and control of the many sources of variance within the system, an understanding of the limitations of the statistical approaches, and the ability to make sense of the results through intelligent database interrogation.

---

### Diagnostic sweat testing: the Cystic Fibrosis Foundation guidelines [^9394b93d]. The Journal of Pediatrics (2007). Medium credibility.

Cystic Fibrosis Foundation diagnostic sweat testing — collection materials, stimulation parameters, and minimum sample amounts are defined: sweat must be collected on gauze or filter paper or in a Macroduct coil after iontophoresis; for gauze or filter paper, the stimulated area must be 2 × 2 inches (total area, 4 square inches) using a slightly smaller electrode (eg, 1 1/2 × 1 1/2 inches) or other electrode sizes that cover greater than 50% of the 2 × 2 inch area, iontophoresis should be carried out using USP grade pilocarpine for 5 minutes, collection is from a single site using 2 × 2 inch gauze or filter paper, and the minimum sample weight is 75 mg in 30 minutes; if a Macroduct coil is used, sweat must be stimulated with a disposable Pilogel electrode using the Webster Sweat Inducer for 5 minutes, and after a 30-minute collection the minimum acceptable sample is 15 μL; to ensure a valid result, the average sweat rate should exceed 1 g/m² per minute and the area of stimulation and collection must be of similar size.

---

### Exploring the phenotypic consequences of tissue specific gene expression variation inferred from GWAS summary statistics [^ea276dad]. Nature Communications (2018). Medium credibility.

Calculation of Z -score

To assess the significance of the association, we need to compute the ratio of the estimated effect sizeand standard error se, or Z -score,

with which we can compute the p -value as p = 2Φ(−|Z g |) where Φ(.) is the normal CDF function. Thuswhere we used Eqs. (5) and (6) in the second line and Eq. (8) in the last step. So

Based on results with actual and simulated data for realistic effect size ranges, we have found that the last approximation does not affect our ability to identify the association. The approximation becomes inaccurate only when the effect sizes are very large. But in these cases, the small decrease in statistical efficiency induced by the approximation is compensated by the large power to detect the larger effect sizes.

Calculation of σ g in reference set

The variance of predicted expression is computed using Eq. (4) which takes weights for each SNP in the prediction model and the correlation (LD) between the SNPs. The correlation is computed in a reference set such as 1000G or in the training set.

---

### Principles of analytic validation of immunohistochemical assays: guideline update [^542e7126]. Archives of Pathology & Laboratory Medicine (2024). High credibility.

Supplemental Table 11 — Concordance outcome GRADE assessment reports 5 studies with design "4 NRS, 1 CS", with aggregate risk of bias "Serious", inconsistency "Not serious", indirectness "Not serious", imprecision "Not serious", other "None", importance "Critical", certainty of evidence grade for outcome "Moderate", and overall certainty of evidence grade for statement "Moderate".

---

### Rotator cuff tendinopathy diagnosis, nonsurgical medical care, and rehabilitation: a clinical practice guideline [^a4079329]. The Journal of Orthopaedic and Sports Physical Therapy (2025). High credibility.

Grades of recommendations — This table defines grades A–F and links each to a strength-of-evidence statement and corresponding Level of Obligation. Grade A is defined as "A preponderance of level I and/or level II studies support the recommendation", with Level of Obligation (Based on Treatment Effects): "Must: benefits substantially outweigh harms; Should: benefits moderately outweigh harms; May: benefits minimally outweigh harms or benefit/harm ratio is value dependent; Should not: harms minimally or moderately outweigh benefits or evidence of no effect; Must not: harms largely outweigh benefits". Grade B is "A single high-quality randomized controlled trial or a preponderance of level II studies support the recommendation", with obligations "Should: benefits substantially outweigh harms; May: benefits moderately or minimally outweigh harms or benefit/harm ratio is value dependent; Should not: evidence that harms outweigh benefits or evidence of no effect". Grade C is "A single level II study or a preponderance of level III and IV studies, including statements of consensus by content experts, support the recommendation", with obligations "Should: benefits substantially outweigh harms; May: benefits moderately or minimally outweigh harms or benefit/harm ratio is value dependent; Should not: harms minimally or moderately outweigh benefits". Grade D is "Higher-quality studies conducted on this topic disagree with respect to their conclusions", with obligation "May: conflicting evidence, the benefit/harm ratio is value dependent". Grade E is "A preponderance of evidence from animal or cadaver studies, from conceptual models/principles, or from basic science/bench research support this conclusion", with obligations "May: in the absence of evidence from clinical studies, theoretical and/or foundational evidence supports benefit" and "Should not: in the absence of evidence from clinical studies, theoretical and/or foundational evidence suggests strong risk of harms". Grade F is "Best practice based on the clinical experience of the guideline development team supports this conclusion", and the table also specifies Level of Obligation (Based on Assessment/Diagnosis): "Must: strongly supported by consensus-based best practice/standard of care; Should: moderately supported by best practice/standard of care; May: supported by expert opinion in the absence of consensus; Should not: best practice/standard of care indicates potential harms; Must not: potential harms are strongly supported by consensus-based best practice/standard of care".

---

### Validating whole slide imaging for diagnostic purposes in pathology: guideline from the college of American pathologists pathology and laboratory quality center [^ddcb0ec9]. Archives of Pathology & Laboratory Medicine (2013). Medium credibility.

Supplemental Table 12 — Evidence to Decision Summary of Recommendation 3 reports response distributions as follows: For "Is the problem a priority?", Yes was 70% (7/10) with 20% (2/10) Probably No, 10% (1/10) Probably Yes, and 0 for No, Varies, and Don't Know. For "How accurate is the test?", Very Inaccurate 0, Inaccurate 10% (1/10), Accurate 30% (3/10), Very Accurate 30% (3/10), Varies 20% (2/10), and Don't Know 10% (1/10). For "How substantial are the desirable anticipated effects?", Trivial 0, Small 0, Moderate 20% (2/10), Large 40% (4/10), Varies 20% (2/10), and Don't Know 20% (2/10). For "What is the overall certainty of the evidence of test accuracy?", Very Low 10% (1/10), Low 10% (1/10), Moderate 20% (2/10), High 40% (4/10), Very High 20% (2/10), and No Included Studies 0. For "What is the overall certainty of the evidence of effects of the management that is guided by the test results?", Very Low 10% (1/10), Low 20% (2/10), Moderate 30% (3/10), High 30% (3/10), Very High 10% (1/10), and No Included Studies 0. For "How certain is the link between test results and management decisions?", Very Low 10% (1/10), Low 20% (2/10), Moderate 10% (1/10), High 40% (4/10), Very High 10% (1/10), and No Included Studies 10% (1/10). For "What is the overall certainty of the evidence of effects of the test?", Very Low 10% (1/10), Low 0, Moderate 40% (4/10), High 40% (4/10), Very High 10% (1/10), and No Included Studies 0. For "What is the overall certainty of the evidence for any critical or important direct benefits, adverse effects or burden of the test?", Very Low 10% (1/10), Low 20% (2/10), Moderate 30% (3/10), High 40% (4/10), Very High 0, and No Included Studies 0.

---

### Standards of care in diabetes – 2025 [^31be65a6]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for hypoglycemia, more specifically with respect to screening for cognitive impairment, ADA 2025 guidelines recommend to simplify diabetes treatment plans as much as possible for patients with cognitive impairment and tailor to minimize the risk of hypoglycemia.

---

### 2022 American society of anesthesiologists practice guidelines for management of the difficult airway [^5298d89c]. Anesthesiology (2022). High credibility.

Difficult airway — consensus-based evidence and survey scoring indicates validation by consensus from survey opinion from expert consultants, survey opinions from randomly selected samples of active members of the American Society of Anesthesiologists (ASA) and participating organizations, and internet commentary, with only findings from formal surveys reported. Survey responses were recorded using a five-point scale and summarized based on median values, with categories defined as: strongly agree median score of 5 (at least 50% of the responses are 5), agree median score of 4 (at least 50% of the responses are 4 or 4 and 5), equivocal median score of 3 (at least 50% of the responses are 3, or no other response category or combination of similar categories contains at least 50% of the responses), disagree median score of 2 (at least 50% of responses are 2 or 1 and 2), and strongly disagree median score of 1 (at least 50% of responses are 1). When an equal number of categorically distinct responses are obtained, the median value is determined by calculating the arithmetic mean of the two middle values, and ties are calculated by a predetermined formula.

---

### Short-term local predictions of COVID-19 in the United Kingdom using dynamic supervised machine learning algorithms [^c3cf9dd8]. Communications Medicine (2022). Medium credibility.

Fig. 1
Schematic figure showing model selection and assessment.

SE squared error, MSE mean squared error. In each of the assessment steps, the optimal model had the smallest MSE. X m 1(t) to X m 6(t): mobility metrics at six locations. X s 1(t) to X s 8(t): search metrics of the eight base symptoms. X v 1(t) and X v 2(t): COVID-19 vaccination coverage for the first and second dose. Details are in Supplementary Method.

At each of the steps, model performance was assessed through calculating an average mean squared error (MSE) of the predictions over the previous four weeks, i.e. 4-week MSE, with the MSE for each week being evaluated separately by fitting the same candidate model (Fig. 1 and Supplementary Methods). The calculated 4-week MSE reflected the average predictability of candidate models over the previous four weeks (referred to as retrospective 4-week MSE). Models with minimum 4-week MSE were considered for inclusion in each step. Separate model selection processes were conducted for each of the prediction targets.

In addition, we considered naïve models as alternative model candidates for selection; naïve models (which assumed no changes in the growth rate) carried forward the last available observation for each of the outcomes as the prediction. Similar to the full models (i.e. models with predictors), we considered a time lag between zero and three weeks, and used the 4-week MSE for naïve models (Supplementary Table 2).

---

### Do we need to improve the reporting of evidence in tendinopathy management? A critical appraisal of systematic reviews with recommendations on strength of evidence assessment [^9708e49f]. BMJ Open Sport & Exercise Medicine (2021). High credibility.

Pooling of results

Item 3: The principal summary measures for each included outcome measure need to be stated in the methods. For quantitative analyses, where outcome measures are continuous, a justification needs to be provided for the use of (raw) mean differences (MD) and not standardised MD (SMD) and vice versa (ie, identical or different outcome measure tools used across studies, respectively). Similarly, for dichotomous outcomes, the authors should state in the methods whether OR or relative risks (RR) were used.

Item 4: An SR without a meta-analysis should ideally have some quantitative summary measures (ie, MD, SMD, OR or RR with accompanying CI) to demonstrate the treatment effect of the assessed intervention over the comparator. Where results are pooled only based on direction of effect (ie, increased, decreased or unchanged), this needs to be stated in the methods with a justification. If pooling is not possible and results are only described narratively, the reason should be stated (usually substantial clinical heterogeneity); in such cases, we question whether the article should be identified as an SR.

Item 5: When meta-analyses are performed, the model used needs to be stated (ie, fixed effects or random effects) with a justification and the statistical heterogeneity assessment (usually χ² and/or I² test) and how it was used in data syntheses. Sensitivity and subgroup analyses should also be pre-defined in the methods.

Items 6 and 7: Results should only be pooled for similar follow-up time periods which should be prespecified in the methods. Traditionally these are short term, mid term and long term. The range of these should be defined by the authors in the methods based on the population, outcome measures and interventions. Equally, assessed outcome measures should be predefined (usually as part of PICOS).

Item 8: A statement needs to be included in the section explaining how the authors dealt with missing data, which are usually methodological details of the included studies needed for the RoB assessment and descriptive statistics for quantitative analyses. For both, ideally attempts should be made to contact the authors of the RCTs for retrieval of the missing data and this should be stated. If significant statistical data of included studies cannot be retrieved (ie, sample sizes and means) these studies should be excluded from quantitative analyses; if variability statistics (SD) are missing, the authors have the option to impute these using data from other RCTs and the imputation method used should be described.

---

### ACOEM practice guidelines: elbow disorders [^b1b81aa8]. Journal of Occupational and Environmental Medicine (2013). Medium credibility.

ACOEM elbow disorders — activity and return-to-work principles emphasize that "Numerous randomized trials addressed other parts of the body have provided universal findings of superior results of early activity, ambulation, exercise, and return to work, although poorer outcomes result from immobilization and rest".

---

### Identifying domains of applicability of machine learning models for materials science [^72c4ba76]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ν that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by σ lin (x 1, x 2) ≡ −0.3 ≤ x 1 ≤ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: σ rbf ≡ −3.3 ≤ x 11 ≤ 3.1 ∧ x 2 ≤ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA σ poly (x 1, x 2) ≡ −3.5 ≤ x 2 ≤ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### Real-time reverse transcription-polymerase chain reaction assay for SARS-associated coronavirus [^19fa224e]. Emerging Infectious Diseases (2004). Low credibility.

Table 3
Efficiency of real-time PCR assays a

a PCR, polymerase chain reaction; CT, threshold cycle number. b Values shown are mean of triplicate samples ± standard deviations. c Slope determined from the formula: Y = Y intercept – slope log 1.000000000000000e+01 Slopes calculated for SARS1 (7.5 × 10⁶ to 7.5 × 10¹); SARS2 (2 × 10⁶ to 2 × 10¹); SARS3 (2 × 10⁶ to 2 × 10⁰). d Efficiency = [10 (–1/slope)] – 1. e Reactions performed in presence of pooled total nucleic acid extract from 20 human respiratory specimens.

The real-time RT-PCR assay was compared with a previously described conventional RT-PCR for SARS-CoV by using fluorescent dye-labeled primers and GeneScan amplicon analysis. Tenfold serial dilutions of a pretitrated SARS-CoV stock adjusted to 1 × 10⁷ PFUs/mL were prepared in triplicate and tested by all assays (Table 4). The real-time RT-PCR assays were positive with 100% frequency at a 10⁻⁸ dilution. Accordingly, the lowest virus quantity detected was 0.01 PFU/100 μL of specimen extract. The conventional RT-PCR assay was at least 10-fold less sensitive in repeat comparisons.

Table 4
Comparison of real-time RT-PCR assays with culture and conventional RT-PCR a

a RT-PCR, reverse transcription–polymerase chain reaction; SARS-CoV, severe acute respiratory syndrome–associated coronavirus. b Serial 10-fold dilution of SARS-CoV stock culture containing 1 x 10 7 PFUs/mL. c Number of positive results divided by the number of replicates tested.