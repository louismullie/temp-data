# Medical Question & Answer

**Sample ID**: 8bf06f9f-2023-9034-44a0-5fd871655b16
**Dataset Index**: 208623

---

## Question

Task B-4.2. The general term of the sequence $a_{n}$ is given by the formula $a_{n} = \frac{1}{(n+1) \sqrt{n}+n \sqrt{n+1}}$. Determine the sum of the first 9999 terms of this sequence.

---

## Answer

> Let's see… What do we have here? The user is asking for the sum of the first 9999 terms of the sequence defined by (a_n = \\frac{1}{(n+1)\\sqrt{n} + n\\sqrt{n+1}}). Let's break this down step-by-step. First, I need to think about simplifying the general term algebraically. Then, I should verify whether the sequence telescopes and identify the cancellation pattern. Next, I will write out the partial sum explicitly and check the boundary terms. Finally, I will compute the exact value for 9999 terms and confirm the result.

> Let me first simplify the general term. The denominator looks like it might rationalize nicely; multiplying numerator and denominator by the conjugate ((n+1)\\sqrt{n} - n\\sqrt{n+1}) gives me a difference of squares in the denominator, which should collapse the expression into something telescoping. Wait, let me verify the algebra before I proceed: the denominator becomes ((n+1)^2 n - n^2 (n+1) = n(n+1)[(n+1) - n] = n(n+1)), so the term simplifies to (\\frac{(n+1)\\sqrt{n} - n\\sqrt{n+1}}{n(n+1)} = \\frac{\\sqrt{n}}{n} - \\frac{\\sqrt{n+1}}{n+1} = \\frac{1}{\\sqrt{n}} - \\frac{1}{\\sqrt{n+1}}). Yes, that checks out, and now the telescoping structure is evident [^notfound].

> Hold on, let's not jump to conclusions; I should confirm the telescoping pattern explicitly. The partial sum (S_N = \\sum_{n = 1}^{N} a_n) becomes (\\sum_{n = 1}^{N} \\left(\\frac{1}{\\sqrt{n}} - \\frac{1}{\\sqrt{n+1}} \\right)), which is a telescoping series where intermediate terms cancel, leaving (1 - \\frac{1}{\\sqrt{N+1}}). I should double-check that the first term is indeed 1 and the last term is (\\frac{1}{\\sqrt{N+1}}), which matches the pattern for (N = 1, 2, 3) if I test small cases, so the closed form is correct [^notfound].

> Now, I will apply this to (N = 9999). Plugging in, the sum is (1 - \\frac{1}{\\sqrt{9999 + 1}} = 1 - \\frac{1}{\\sqrt{10000}} = 1 - \\frac{1}{100} = \\frac{99}{100}). Wait, let me verify the arithmetic: (\\sqrt{10000} = 100), so (1–0.01 = 0.99 = \\frac{99}{100}), which is correct. Therefore, the sum of the first 9999 terms is (\\frac{99}{100}) [^notfound].

---

The sum of the first 9999 terms of the sequence (a_n = \\frac{1}{(n+1)\\sqrt{n} + n\\sqrt{n+1}}) is **(\\frac{99}{100})**.

Key steps: rationalize the denominator to get (a_n = \\frac{1}{\\sqrt{n}} - \\frac{1}{\\sqrt{n+1}}), which yields a telescoping series; the sum then collapses to (1 - \\frac{1}{\\sqrt{10000}} = 1 - \\frac{1}{100} = \\frac{99}{100}).

---

## Step 1: Simplify the general term

Rationalize the denominator by multiplying numerator and denominator by the conjugate:

[
a_n = \\frac{1}{(n+1)\\sqrt{n} + n\\sqrt{n+1}} \\cdot \\frac{(n+1)\\sqrt{n} - n\\sqrt{n+1}}{(n+1)\\sqrt{n} - n\\sqrt{n+1}}
]

Simplify the denominator using the difference of squares:

[
[(n+1)\\sqrt{n}]^2 - [n\\sqrt{n+1}]^2 = (n+1)^2 n - n^2 (n+1) = n(n+1)
]

Thus,

[
a_n = \\frac{(n+1)\\sqrt{n} - n\\sqrt{n+1}}{n(n+1)} = \\frac{\\sqrt{n}}{n} - \\frac{\\sqrt{n+1}}{n+1} = \\frac{1}{\\sqrt{n}} - \\frac{1}{\\sqrt{n+1}}
]

---

## Step 2: Recognize the telescoping series

The simplified form shows that each term is the difference of consecutive reciprocals of square roots, so the partial sum telescopes:

[
\\sum_{n = 1}^{N} a_n = \\sum_{n = 1}^{N} \\left(\\frac{1}{\\sqrt{n}} - \\frac{1}{\\sqrt{n+1}} \\right) = 1 - \\frac{1}{\\sqrt{N+1}}
]

---

## Step 3: Compute the sum for N = 9999

Substitute (N = 9999):

[
\\sum_{n = 1}^{9999} a_n = 1 - \\frac{1}{\\sqrt{9999 + 1}} = 1 - \\frac{1}{\\sqrt{10000}} = 1 - \\frac{1}{100} = \\frac{99}{100}
]

---

The sum of the first 9999 terms is **(\\frac{99}{100})**.

---

## References

### The simplicity of protein sequence-function relationships [^541b5dea]. Nature Communications (2024). High credibility.

RFA is more interpretable and robust than other global formalisms

Like RFA, Fourier analysis (FA) and background-averaging (BA) take a bird's-eye view of genetic architecture, but RFA has a more interpretable structure and is more robust to missing data. In RFA, each model term directly expresses the global phenotypic effect of an amino acid or combination, and a variant's phenotype is the sum of the effects of only the states in its sequence (Fig. 3a). In FA, each sequence state is recoded as a series of (1, –1) coordinates over (q – 1) Fourier dimensions, where q is the number of states (Supplementary Section 1.2). Each first-order Fourier term represents the effect of having a positive or negative coordinate along one of these dimensions. If more than two amino acid states are present, Fourier terms have no straightforward genetic or biochemical meaning. With 20 states, for example, the effect of each amino acid at a site is a uniquely signed sum over 19 first-order Fourier terms, each pairwise amino acid interaction is a signed sum over 361 second-order Fourier terms, and so on. The phenotype of any variant is therefore a sum over every term in the entire model (Fig. 3a). This complex mapping makes it difficult to understand how a variant's phenotype arises from its sequence.

---

### Modelling sequences and temporal networks with dynamic community structures [^4f105116]. Nature Communications (2017). Medium credibility.

Results

Inference of markov chains

Here we consider general time-series composed of a sequence of discrete observations { x t }, where x t is a single token from an alphabet of size N observed at discrete time t, and x t −1 = (x t −1,…, x t − n) is the memory of the previous n tokens at time t (Fig. 1). An n th-order Markov chain with transition probabilities p (x t | x t −1) generates such a sequence with probabilitywhere a x, x is the number of transitions x → x in { x t }. Given a specific sequence { x t }, we want to infer the transitions probabilities p (x | x). The simplest approach is to compute the maximum-likelihood estimate, that iswhere a x = ∑ x a x, x, which amounts simply to the frequency of observed transitions. Putting this back into the likelihood of eq. (1), we haveThis can be expressed through the total number of observed transitionsand the conditional entropyas. Hence, the maximisation of the likelihood in eq. (1) yields the transition probabilities that most compress the sequence. There is, however, an important caveat with this approach. It cannot be used when we are interested in determining the most appropriate Markov order n of the model, because the maximum likelihood in eq. (3) increases with n. In general, increasing number of memories at fixed number of transitions leads to decreased conditional entropy. Hence, for some large enough value of n there will be only one observed transition conditioned on every memory, yielding a zero conditional entropy and a maximum likelihood of 1. This would be an extreme case of overfitting, where by increasing the number of degrees of freedom of the model it is impossible to distinguish actual structure from stochastic fluctuations. Also, this approach does not yield true compression of the data, since it does not describe the increasing model complexity for larger values of n, and thus is crucially incomplete. To address this problem, we use a Bayesian formulation, and compute instead the complete evidencewhich is the sum of all possible models weighted according to prior probabilities P (p) that encode our a priori assumptions. This approach gives the correct model order for data sampled from Markov chains as long as there are enough statistics that balances the structure present in the data with its statistical weight, as well as meaningful values when this is not the case.

---

### Accurately clustering biological sequences in linear time by relatedness sorting [^120d0aa3]. Nature Communications (2024). High credibility.

Two different values of k-mer length (k) are computed: one for use with k-mer matching between sequences and the other for use with rare k-mers. The goal of selecting k for k-mer matching is to ensure that k-mer matches between two sequences happen infrequently by chance, while using only a single value of k across all sequences. Hence, k is calculated such that one in every 100 k-mers is expected to be found by chance in sequences at the 99th percentile of input sequence lengths using an alphabet with a given number of letters (x):

The reasoning behind this formula for k was previously described. In contrast, k for rare k-mers is set such that at most 10% of the selected 20,000 (i.e. parameter A) sequences are expected to share a rare k-mer due to chance. Each of N sequences contributes 50 (by default) rare k-mers, and the objective is to find k such that each rare k-mer will be found by chance in less than 40 (i.e. 20,000/50 * 10%) sequences on average. Thus, k for rare k-mers is calculated as:

---

### Learning the pattern of epistasis linking genotype and phenotype in a protein [^6d4da130]. Nature Communications (2019). High credibility.

Functional trajectories and genotypic connectivity

To obtain the number of functional single-step trajectories, we compute an adjacency matrix between functional genotypes by binarization of the full genotype adjacency matrix above a select threshold brightness (for Fig. 6, y = 0.73, the value for the red parental variant). From the binarized adjacency matrix, the (i, j)-th element of the m th power of the matrix gives the number of functional m -step trajectories that exist between genotypes i and j. Summing over the powers of this matrix to any order M gives all viable trajectories consisting of M or fewer steps in the sequence space. Conversion of the resulting summed matrix to block-diagonal form produces a "genotypic connectogram" — a graph that directly reveals the connectivity and topology of viable genotypes (Supplementary Fig. 12C).

Sparse optimization and phenotype reconstruction

CS is performed by finding a sparse representation for the data on the basis of a (small) subset of measurements, and subsequent reconstruction the full dataset by inverse transformation from the sparse representation back to the data domain (see ref.for an excellent presentation of the theory). Finding the optimal sparse distribution of epistatic terms is achieved by L1-norm minimization and was performed in this study inusing the1 solver, version 1.4. The performance for mutant prediction is scored by the Goodness of Prediction (GoP) parameter, where SSE is the sum of squared errors between reconstruction phenotypes and the measured values, and SST is the total sum of squares.scripts are provided as Supplementary Software 1.

Alignment epistasis

First-order alignment epistasis is calculated according to, where φ i is the average valueof the two states of amino acid x at position i represented as −1 and +1, over the n functional sequences in the alignment.and N tot are the number of functional sequences and the total number of sequences in the combinatorial dataset (see ref.for extended theory). Second order alignment epistasis is calculated according to, where φ ij is the joint expectation between pairs of positions i and j. In the current dataset we have explicit numbers forand N tot and therefore we can numerically compare this with calculated background-averaged epistatic terms (Fig. 5 and Supplementary Fig. 10).

Reporting summary

Further information on research design is available in the Nature Research Reporting Summary linked to this article.

---

### Combinatorial summation of feynman diagrams [^86070c11]. Nature Communications (2024). High credibility.

Combinatorial summation for the determinant

A good starting point is the algorithm for division-free calculation of the determinantbased on its permutation cycle decomposition. In terms of the cycle covers, representing an ordered sequence of matrix indices (called elements) grouped into m cycles by the parenthesis, the determinant becomeswhereand. For instance, the cycle coverhasand. In this form, one easily recognises Feynman's rules for constructing the diagrams, with the cycles corresponding to fermionic loops. It is useful to view building each, one element at a time, by an ordered walk of 2 n steps, where at each step l the current element is e and the new elementis selected according to some rules, while the currentis multiplied by, as well as by an additional − 1 when the cycle is closed. An expression like Eq. (2) is then evaluated as a sum over all such walks. The central observationis that, when different walks are executed in parallel, there will be many for which the step l is identical. Thus, before step l the weights of all such walks constructed up to this point can be combined, and the multiplication byapplied to the sum. This suggests linking all walks in a graph, such as that in Fig. 1 c, where the result of the summation before each step is stored in the nodes and the steps are the edges. An optimal structure of the graph minimises the number of times the multiplication byneeds to be performed, and finding it is the task of dynamic programming. In the case of the determinant, the total number of edges can be made only polynomial in n.

---

### Fragmentation of pooled PCR products for highly multiplexed TILLING [^1203a751]. G3 (2019). Medium credibility.

GC percentage for each sequence was extracted using the Emboss infoseq tool. Minimum and maximum GC percentage was extracted using the following command: awk ′BEGIN{first = 1;} {if (first) {max = min = $3; first = 0; next;} if (max < $3) max = $3; if (min > $3) min = $3;} END {print min, max}′. Average GC was calculated using the following command: awk ′{sum = sum+$3; sumX2+ = (($3)^2)} END { printf "Average: %f. Standard Deviation: %f \n", sum/NR, sqrt(sumX2/(NR) - ((sum/NR)^2))}′.

Sonication of PCR products

Sonication of PCR products was performed using a Covaris M220 ultrasonicator with microTUBE AFA bead split tubes (coffee amplicons) or microTUBE AFA FiberPre-slit (barley amplicons) in 60 μl volumes except where indicated with parameters adjusted according to Tables 1, S2 and S3. Fragmentation of PCR products was assayed using a Fragment Analyzer with the low sensitivity 1kb separation matrix with 30 cm capillaries (cat #DNF935). Analysis of data were performed using the PROSize Data Analysis Software. Performance of sonication of pooled PCR products was independently tested in 48 pools. Pooling of genomic DNA for each of the 48 pools was done such that each individual genomic DNA is represented in triplicate in the experiment (in three different pools). A total of 4096 DNAs from unique mutant lines are represented in the 48 pools.

Table 1
Test parameters for sonication of a 964 bp PCR product

---

### DNA synthesis for true random number generation [^4f3c367b]. Nature Communications (2020). High credibility.

Nucleotide-binding prevalence

For every position × along the sequence data, the relative presence of two nucleotides following each other (e.g. AT) is normalized by the relative presence of the single nucleotides (e.g. A and T) at the corresponding position. (e.g. for AT at position x: f x norm (AT) = ((f x (AT)/ f x (A))/(f x +1 (T)). This gives the general formula with nucleotides N 1 and N 2: f x norm (N 1 N 2) = ((f x (N 1 N 2)/ f x (N 1))/(f x +1 (N 2)).

NIST evaluation parameters

NIST evaluation tests were chosen such that for each test, 56 bit streams containing 1096 bits were tested (with the exception of the rank test, where 56 bit streams containing 100,000 bits each were tested). This variant of statistical analyses was chosen and explained by Rojas et al. The tests were applied with the standard parameters as given by the NIST statistical test suite, with the following parameters differing from set values: (1) frequency test, no parameter adjustments; (2) block frequency test, no parameter adjustments (block length, M = 128); (3) cumulative sums test, no parameter adjustments; (4) runs test, no parameter adjustments; (5) longest runs of ones test, no parameter adjustments; (6) rank test, no parameter adjustments; (7) discrete Fourier transform test, no parameter adjustments; (8) approximate entropy test parameter adjustment: block length, m = 5; 9) serial test no parameter adjustments (block length, m = 16). For each statistical test, the NIST software computes a P -value, which gives the probability that the sequence tested is more random than the sequence a perfect RNG would have produced. Thereby, a P -value of 1 indicates perfect randomness, whereas a P -value of 0 indicated complete non-randomness. More specifically, for a P -value ≥ 0.001: sequence can be considered random with a confidence of 99.9%, and for a P -value < 0.001 sequence can be considered non-random with a confidence of 99.9%.

---

### Backbone spiking sequence as a basis for preplay, replay, and default States in human cortex [^240af284]. Nature Communications (2023). High credibility.

Quantification of sequence similarity

We quantified sequence similarity using the matching index, (Fig. S 17). For a given sequence of length N, a total of N (N −1)/2 pairs can be assessed between neurons occurring in different positions within the sequence. For a second sequence that is to be matched to the first, we define m as the number of pairs of neurons that fire in the same order as in the first sequence, and n as the number of pairs of neurons that fire in the opposite order. The matching index (MI) is then defined as:

MI is bounded between −1 and 1, indicating exactly reverse and forward replay, respectively, of the second sequence of interest relative to the original sequence. We determined the significance for how similar a given sequence pair is by randomly rearranging the temporal positions of units in each sequence 1000 times, calculating the MI for each permutation, and comparing the true MI to the shuffled distribution.

---

### High-order species interactions shape ecosystem diversity [^681f395c]. Nature Communications (2016). Medium credibility.

Results

Modelling communities with pairwise and high-order interactions

We use a replicator dynamics modelto simulate communities with interactions of different orders. Our model assumes N species with abundances x i, and whose per-capita growth ratesare determined by the sum of pairwise interactions A ij x j (where A ij determines the effect of species j on species i), of three-way interactions B ijk x j x k (where species j and k have a joint effect on species i) and so on. This model therefore represents the Taylor expansion of a general system that includes trait-mediated effects as well as pairwise interactions:

The model assumes random interactions perturbing an inherently stable community. The stability is provided by the first term (− x i), which reflects the assumption that species are self-limiting in high concentrations. The pairwise interactions matrix is given by, whereis a random matrix whose elements are drawn from a Gaussian distribution with mean 0 and variance 1, and α determines the strength (variance) of the pairwise interactions. The three-way interactions are set by the tensor, whereis a random three-dimensional (N × N × N) tensor whose elements are drawn from a Gaussian distribution with mean 0 and variance 1, and β represents the strength of three-way interactions; and similarly with four-way interactions whose strength scales with a parameter γ. As the total biomass is often determined by external resources, such as nutrients, energy and space, we keep the sum of all species abundances constant by subtracting from the individual growth rate of each species the average growth rate of the species in the community.

---

### Modelling sequences and temporal networks with dynamic community structures [^e2cbc28f]. Nature Communications (2017). Medium credibility.

Fig. 2
Schematic representation of the Markov model with communities. The token sequence { x t } = 'It was the best of times' represented with nodes for memories (top row) and tokens (bottom row), and with directed edges for transitions in different variations of the model. a A partition of the tokens and memories for an n = 1 model. b A unified formulation of an n = 1 model, where the tokens and memories have the same partition, and hence can be represented as a single set of nodes. c A partition of the tokens and memories for an n = 2 model

As before, this maximum likelihood approach cannot be used if we do not know the order of the Markov chain, otherwise it will overfit. In fact, this problem is now aggravated by the larger number of model parameters. Therefore, we employ a Bayesian formulation and construct a generative process for the model parameters themselves. We do this by introducing prior probability densities for the parametersandfor tokens and memories, respectively, with hyperparameter sets α and β, and computing the integrated likelihoodwhere we used b as a shorthand for { b x } and { b x }. Now, instead of inferring the hyperparameters, we can make a noninformative choice for α and β that reflects our a priori lack of preference towards any particular model. Doing so in this case yields a likelihood (for details, see Methods section Bayesian Markov chains with communities), where P ({ x t }| b, { e rs }, { k x }) corresponds to the likelihood of the sequence { x t } conditioned on the transitions counts { e rs } and token frequencies { k x }, and the remaining terms are the prior probabilities on the discrete parameters { e rs } and { k x }. Since the likelihood above still is conditioned on the partitions { b x } and { b x }, as well as the memory group counts { e s }, we need to include prior probabilities on these as well to make the approach fully nonparametric. Doing so yields a joint likelihood for both the sequence and the model parameters, It is now possible to understand why maximising this joint likelihood will prevent overfitting the data. If we take its negative logarithm, it can be written asThe quantity Σ is called the description length of the data. It corresponds to the amount of information necessary to describe both the data and the model simultaneously, corresponding to the first and second terms in eq. (12), respectively. As the model becomes more complex — either by increasing the number of groups or the order of the Markov chain — this will decrease the first term as the data likelihood increases, but it will simultaneously increase the second term, as the model likelihood decreases. The second term then acts as a penalty to the model likelihood, forcing a balance between model complexity and quality of fit. Unlike approximative penalty approaches based solely on the number of free parameters such as BICand AIC, which are not to valid for network models, the description length of the model is exact and fully captures its flexibility. Because of the complete character of the description length, minimising it indeed amounts to achieving true compression of data, differently from the parametric maximum likelihood approach mentioned earlier. Because the whole process is functionally equivalent to inferring the SBM for networks, we can use the same algorithms(for a details about the inference method, see Methods section 'Bayesian Markov chains with communities').

---

### Functional regimes define soil microbiome response to environmental change [^a79690cc]. Nature (2025). Excellent credibility.

To identify the ASVs enriched for each perturbed pH level, it was necessary to determine what change in recorded abundance constitutes a significant change, relative to what might be expected for purely stochastic reasons. The relevant null model would combine sampling and sequencing noise with the stochasticity of ecological dynamics over a four-day incubation, and cannot be derived from first principles. However, since all measurements were performed in triplicate with independent incubations, the relevant null model can be determined empirically. The deviations of replicate–replicate comparisons from 1:1 line were well-described by an effective model combining two independent contributions, a Gaussian noise of fractional magnitude c frac and a constant Gaussian noise of magnitude c 0 reads, such that repeated measurements (over biological replicates) of an ASV with mean abundance n counts are approximately Gaussian-distributed with a standard deviation ofcounts. In this expression, c frac was estimated from moderate-abundance ASVs (> 50 counts) for which the other noise term is negligible; and c 0 was then determined as the value for which 67% of replicate–replicate comparisons are within ± σ (c 0, c frac) of each other, as expected for 1-sigma deviations. This noise model was inferred separately for each soil and each perturbed pH level, as the corresponding samples were processed independently in different sequencing runs. For example, the parameters in Soil 11 were c frac = 0.21 ± 0.04 and c 0 = 4.5 ± 0.7 counts (Extended Data Fig. 9i).

---

### Modelling sequences and temporal networks with dynamic community structures [^7129d625]. Nature Communications (2017). Medium credibility.

Bayesian Markov chains with communities

As described in the main text, a Bayesian formulation of the Markov model consists in specifying prior probabilities for the model parameters, and integrating over them. In doing so, we convert the problem from one of parametric inference where the model parameters need to be specified before inference, to a nonparametric one where no parameters need to be specified before inference. In this way, the approach possesses intrinsic regularisation, where the order of the model can be inferred from data alone, without overfitting.

To accomplish this, we rewrite the model likelihood, using eqs. (1) and (5), asand observe the normalisation constraints,… Since this is just a product of multinomials, we can choose conjugate Dirichlet priors probability densitiesand, which allows us to exactly compute the integrated likelihood, whereand. We recover the Bayesian version of the common Markov chain formulation (see ref.) if we put each memory and token in their own groups. This remains a parametric distribution, since we need to specify the hyperparameters. However, in the absence of prior information it is more appropriate to make a noninformative choice that encodes our a priori lack of knowledge or preference towards any particular model, which amounts to choosing α x = β rs = 1, making the prior distributions flat. If we substitute these values in eq. (19), and re-arrange the terms, we can show that it can be written as the following combination of conditional likelihoods, wherewithbeing the multiset coefficient, that counts the number of m -combinations with repetitions from a set of size n. The expression above has the following combinatorial interpretation: P ({ x t }| b, { e rs }, { k x }) corresponds to the likelihood of a microcanonical modelwhere a random sequence { x t } is produced with exactly e rs total transitions between groups r and s, and with each token x occurring exactly k x times. In order to see this, consider a chain where there are only e rs transitions in total between token group r and memory group s, and each token x occurs exactly k x times. For the first transition in the chain, from a memory x 0 in group s to a token x 1 in group r, we have the probabilityNow, for the second transition from memory x 1 in group t to a token x 2 in group u, we have the probabilityProceeding recursively, the final likelihood for the entire chain iswhich is identical to eq. (21).

---

### Inferring selection intensity and allele age from multilocus haplotype structure [^83bf13b9]. G3 (2013). Low credibility.

Next we explain how we derive the recursive formula in Equation 6. Starting from time v back from the present, there are four possible paths to arrive at the sample configuration (, n) at time v: (1) a coalescent event occurred at time v, and a possible sample configuration prior to time v was (, n − e k); (2) a mutation occurred on a haplotype h k that had only single multiplicity, or n k = 1, at time v, and a new mutation coordinate was added to h k; (3) a mutation occurred on a haplotype h j that had multiplicity greater than 1, or n j > 1, at time v, and a new haplotype h k with n k = 1 was generated by adding the new mutation coordinate to h j; (4) a recombination event occurred, and altered one of the recombination coordinates of h j by that of h i. Note that a recombination event not only changes the recombination coordinate, it also changes the mutation coordinates: after the recombination coordinates are changed by a recombination event, all the mutation coordinates of that haplotype are checked, and only those located within the interregion between the two new recombination coordinates are kept. The four terms on the RHS of Equation 6 correspond to the above four paths respectively, and the derivation of the first three terms follows. In the first path, the sample configuration at time v compatible with the occurrence of coalescence is (, n − n k), the probability that the event occurred at time v is a coalescent event is. And whenstarting from the configuration (, n − n k) and going forward in time, the probability that one of the n k − 1 haplotype e k is chosen to duplicate is. The one-step transition probability of p ((, n) v |(, n − e k) v) is then. Note that a restriction for haplotype group h k is that there must be more than one lineage in group h k at time v. Summing over all possible haplotype groups that have multiplicity n k ≥ 2 at time v, and are compatible for coalescent events to occur, we obtain the first term of Equation 6. The second and the third path correspond to the cases when the event occurring at time v is a mutation. Under the assumption of the infinitely many-sites model, if a mutation event occurs, it can result only in one of the single-multiplicity haplotype groups at time v (n k = 1) and the mutation coordinate must be a singleton in the sample configuration at time v. In both the second and the third paths, the chance that a mutation occurred at time v is. The configuration at time v compatible with the occurrence of second path is (k, n), and the probability for the mutation to happen to haplotype h k is. Summing over all haplotypes satisfying n k = 1 and h ≠ h j for all j yields the second term in Equation 6. In the third path, the probability for the mutation to happen to haplotype h k iswith the sample configuration prior to the event beingfor all k with n k = 1 at time v. In the fourth path, recombination occurs with a probability of. If recombination causes a haplotype h j to become h i, the haplotype configuration prior to the event is (h i, (n − e i + e j)) and the probability for a haplotype h j changing into h i is, with n j ≥ 0. The transition probabilitybetween different haplotypes follows the multilocus haplotype model presented in the section A simplified multi-locus model for haplotype structure, where h i and h j correspond to one of the distinct haplotypes defined by the "recombination coordinates". Combining these possibilities and averaging over the time to the first event more ancient than t, the sampling distribution of haplotype configuration, (, n), is analogous to Equation 2:

---

### Survival of the simplest in microbial evolution [^e9f464b5]. Nature Communications (2019). High credibility.

Evolutionary equilibria for individual traits

We now derive the equilibrium conditions of the model given by Eqs. 15, 16, which are used in the main text. This involves three steps. First, the deterministic term in Eq. 16 determines the average trait diversity Δ G as given in Eq. 1, if we neglect the selection component (this will be justified in step three below). That is, Δ G follows from a mutation-coalescence balance: the trait gains a heritable variance Δ G by new mutations at a speed, and it loses variation by coalescence at a rate. Equation 1 is consistent with well-known results for the average sequence diversity Δ, indicating that diversity expectation values do not depend on details of the coalescence process. These results include the relationin the standard theory of neutral evolution, where N e is proportional to the actual population size. The same relation is obtained for the sequence diversity of neutral genomic sites in models of genetic draftand in fitness wave models, whereis determined by selection. To obtain the equivalent form for a quantitative trait G, we simply rescale the sequence diversity by the mean square effect, which leads to Eq. 1.

---

### The learning curve of TaTME for mid-low rectal cancer: a comprehensive analysis from a five-year institutional experience [^64590494]. Surgical Endoscopy (2021). Medium credibility.

Statistical analysis

The learning curve analysis was performed according to the cumulative sum CUSUM method to explore the relationship between the primary outcomes and the sequence number of the TaTME procedure. The simple CUSUM series was defined for every case as CUSUM n = ∑(X n – X 0) + CUSUM n-1, where X n represents the individual measurement, and X 0 is a predetermined reference level. X 0 was set as the mean for all cases in the first analysis. The CUSUM score was plotted against the sequence of operations. The inflection point of the learning curve was defined as the point where the curve started to descend gradually and was considered the end of the learning curve.

As a complement to the CUSUM series, a two-sided Bernoulli CUSUM chart was plotted to detect consistency with the mean or "alarm signals" in the surgical performance. The point at which the OT became consistent with the mean, without further significant changes in terms of the mOT, was defined as the point of "mastery" of the technique. The control limits for the "alarm signals" were set at ± 4* σD (σD = standard deviation). The Bernoulli CUSUM max and CUSUM min were defined as CUSUM maxn = max (0, X n – i 0 + σD /2 + CUSUM maxn-1), where the max (0,) yields 0 when a negative value is calculated, and CUSUM minn = min (0, X n – X 0 — σD /2 + CUSUM maxn-1), where the min (0,) yields 0 when a positive value is calculated.

---

### Genes associated with anhedonia: a new analysis in a large clinical trial (GENDEP) [^078ba282]. Translational Psychiatry (2018). Low credibility.

GWAS using a linear mixed model (LMM)

In order to test for genotype–phenotype association while controlling for potential confounding factors such as population structure, family structure, and cryptic relatedness simultaneously, we used factored spectrally transformed LMM (FaST-LMM) for our association study. In brief, the LMM log likelihood of the phenotype data, y (dimension n × 1; n denoting the cohort size), given fixed effects X (dimension n × d; d denoting the number of fixed effects in a single model, including the offset, the covariates, and the SNP to be tested), can be written as

where N (r|m; Σ) denotes a normal distribution of variable r with mean m and covariance matrix Σ; K (dimension n × n) is the genetic similarity matrix; I is the identity matrix; is the magnitude of the residual variance; is the magnitude of the genetic variance; and β (dimension d × 1) denotes the weight of the fixed effects.

The "Fa" in FaST-LMM stands for factorization. Let S be genetic similarity matrix, as the covariance matrix of the normal distribution becomes a diagonal matrix S + δ I (spectral decomposition), the log likelihood can be rewritten as the sum over n terms. Factorization dramatically increases the size of datasets that can be analyzed with LMM, and additionally enhances the speed and feasibility of the analysis.

In our analysis, we chose the continuous interest-activity score as our outcome measure, controlling for gender, age, years of education, recruitment centres and the first 20 PCs from EIGENSTRAT as covariates.

Replication analysis using STAR*D

Following our initial findings, we used data from the Sequenced Treatment Alternatives to Relieve Depression Study (STAR*D) to replicate our primary results. Detailed information about the STAR*D including its demographic characteristics and genomic profile have been previously described –. In brief, 1351 patients with MDD were recruited with the phenotype being defined as the sum of items with corresponding content in baseline HAMD-17, QIDS-SR, QIDS-C and the research outcome assessor-rated 30-item Inventory for Depression Symptomatology, with genomic profile including 7405247 SNPs after quality control and imputation. Further, a linear model using PLINK 1.09 was chosen with age, gender, years of education, recruitment centre, and the first four population PCs being included as covariates.

---

### Scale-free networks are rare [^0cde0f97]. Nature Communications (2019). High credibility.

Robustness analysis

In order to assess the dependence of these results on the evaluation scheme itself, we conduct a series of robustness tests.

Specifically, we test whether the above results hold qualitatively when (i) we consider only network data sets that are naturally simple (unweighted, undirected, monoplex, and no multi-edges); (ii) we remove the power-law with cutoff from the set of alternative distributions; (iii) we lower the percentage thresholds for all categories to allow admission if any one constituent simple graph satisfies the requirements; and (iv) we analyze the scaling behavior of the degree distribution's first and second moment ratio. Details for each of these tests, and two others, are given in Supplementary Note 5. We also test whether the evaluation scheme correctly classifies four different types of synthetic networks with known structure, both scale free and non-scale free. Details and results for these tests are given in Supplementary Note 6.

The first test evaluates whether the extension of the scale-free hypothesis to non-simple networks and the corresponding graph-simplification procedure biases the results. The second evaluates whether the presence of finite-size effects drives the lack of evidence for scale-free distributions. Applied to the corpus, each test produces qualitatively similar results as the primary evaluation scheme (see Supplementary Note 5, and Supplementary Fig. 4), indicating that the lack of empirical evidence for scale-free networks is not driven by these particular choices in the evaluation scheme itself.

The third considers a "most permissive" parameterization, which evaluates the impact of our requirements that a minimum percentage of degree sequences satisfy the constraints of a category. Under this test, we specifically examine how the evidence changes if we instead require that only one degree sequence satisfies the given requirements. That is, this test lowers the threshold for each category to be maximally permissive: if scale-free structure exists in any associated degree sequence, the network data set is counted as falling into the corresponding category.

---

### Protein degradation by human 20S proteasomes elucidates the interplay between peptide hydrolysis and splicing [^26c1c0a0]. Nature Communications (2024). High credibility.

Computation of the theoretical sequence space size

The number of possible unmodified spliced and non-spliced peptides that could be derived from a protein sequence in sequence-agnostic fashion constituted the size of the theoretical peptide sequence space. The number X of non-spliced peptides of length N that could theoretically arise from a substrate of length L was:

To derive the positions of all spliced peptides, we defined four indices i, j, k and n that denoted the first (i, j) and second (k, n) splice-reactant, respectively. The corresponding number of peptides was calculated via summing over interval ranges that form valid spliced peptides. Cis -spliced peptides can be formed via forward or reverse ligation. The number of all forward cis -spliced peptides of length N that could theoretically arise from a substrate of length L was:

L ext denoted the minimal splice-reactant length and was set to 1 per default. Analogously, the number of theoretically possible reverse cis -spliced peptides was calculated as:

To calculate the number of theoretical homologous trans -spliced peptides in an in vitro scenario where a single protein was digested with purified proteasomes, the following formula was used:

Computation of substrate-specific cleavage and splicing strengths

SCS-P 1 and PSP-P 1 were calculated using the MS1 intensities (quantities) of peptides after four hours of digestion. For α-Synuclein, the MS1 intensity after three hours of digestion was taken. In the case of a multi-mapper peptide, i.e. a peptide that maps to several substrate locations (see above), the intensity value of this peptides was adjusted by the number of possible peptide origins.

For each amino acid residue of a given substrate sequence, the summed quantity of all non-spliced peptides containing this residue at their P 1 position was denoted as site-specific cleavage strength (SCS-P 1). Similarly, the summed intensity of all spliced peptides carrying this residue at the C-terminus of their first splice-reactant (P1) was denoted as site-specific splicing strength (PSP-P 1) (Supplementary Fig. 15). Cleavage and splicing strength values for a given residue were then normalized by their respective sums in a local, 29 amino acid long, window surrounding the current residue, resulting in percentage distributions of SCS-P 1 and PSP-P 1 over the substrate sequence.

---

### Rhinovirus dynamics across different social structures [^d139759f]. Npj Viruses (2023). Medium credibility.

Intra-type and (type-specific) inter-wave diversity

The genetic diversity of viruses belonging to the same type (mean pairwise genetic distances) was calculated for each social structure using MEGA X.

We further investigated the genetic diversity across different mini-epidemic waves of the same type in the same spatial frame. Starting with the assertion that the basic unit of transmission is an epidemic wave comprising a variant of a single type that enters and spreads within a local population and fades out, then multiple waves/peaks of the same type in the same location are because of separate introductions and should be identifiable as genetically different. This way, observed patterns for a single type are composites of multiple individual introductions, each spreading independently within the local community.

We performed this analysis on purposively select types with multiple identifiable mini-epidemic waves in the Kilifi HDSS (A15, C1, C11) and countrywide studies (A22, A34, A49). We used a method described by Konishi et al.that directly applies principal component analysis (PCA) to a sequence alignment. First, the difference between any two samples was calculated using Euclidean distances. PCA then summarized the distance matrix to identify the principal components and record distances between each combination of samples. Finally, the highest-contributing principal components were subjected to k-means clustering. The optimal number of clusters was determined using the within-cluster sum of squares (wss) index.

Definition of terms

We adopted a flexible definition of the term social structures as differing arrangement of institutions where people live or interact with each other. We defined a phylogenetic cluster as a group of sequences collected either from the same administrative location or a similar timeframe, supported by a branching posterior probability of > 0.95. These are labelled K 1 -K n (Fig. 3) and are independent from genetic clusters identified using machine learning (unsupervised learning using k-means clustering), subsequently labelled Cluster 1-Cluster n. Unless otherwise stated, the term (genetic) clusters refers to machine learning clusters.

---

### Modelling sequences and temporal networks with dynamic community structures [^5eb6ea13]. Nature Communications (2017). Medium credibility.

Before we continue, we point out that the selection of priors in eq. (9) needs to be done carefully to avoid underfitting the data. This happens when strong prior assumptions obscure structures in the data. We tackle this by using hierarchical priors, where the parameter themselves are modelled by parametric distributions, which in turn contain more parameters, and so on. Besides alleviating the underfitting problem, this allows us to represent the data in multiple scales by a hierarchical partition of the token and memories. We describe this in more detail in the Methods section Bayesian Markov chains with communities.

This Markov chain model with communities succeeds in providing a better description for a variety of empirical sequences when compared with the common Markov chain parametrisation (see Table 1). Not only do we systematically observe a smaller description length, but we also find evidence for higher-order memory in all examples. We emphasise that we are protected against overfitting: If we randomly shuffle the order of the tokens in each data set, with dominating probability we infer a fully random model with n = 1 and B N = B M = 1, which is equivalent to an n = 0 memoryless model. We have verified that we infer this model for all analysed data sets. Accordingly, we are not susceptible to the spurious results of nonstatistical methods.

---

### Eukaryotic transcription factors can track and control their target genes using DNA antennas [^f839247c]. Nature Communications (2020). High credibility.

Statistical mechanical model for EngHD binding to DNA

The binding of EngHD to a DNA molecule of N base pairs contains a total of 2 · (N −5) potential binding sites (binding to all possible 6 bp sites in either strand). Defining the unbound state as reference (statistical weight, w free = 1), the partition function for EngHD binding to DNA is thus:where i and j are dummy indexes that indicate the position in the DNA sequence of the first base of each 6-bp binding site on the coding and complementary strands, respectively, both running from the 5′–3′ end. The statistical weight for EngHD bound to DNA site x (whether on the coding or the complementary strand) is defined by the following equation:where w 0 is a proportionality constant that represents the ratio between the diffusion controlled rate constant for association (in M −1 s −1) and the dissociation rate constant in the absence of interactions between EngHD and DNA (in s −1).is the binding free energy between EngHD and site x. Here we set w 0 to 5 · 10 −4 M −1, consistently with EngHD's diffusion coefficient (determined by FCS as ~122 μm 2 s −1, see results). However, its exact value is of no practical consequence since it just scales. To defineas a function of the site's sequence we developed two complementary interaction models inspired by the PWM and the 3D structure of the EngHD-DNA complex. Both models follow the general formula:where δ is a Kronecker delta that takes a value of 1 for any binding site in a central position of the DNA (x ≥ 10 and x ≤ N −10) or 0 for any binding site located less than 1 turn from either DNA end (x ≤ 10 or x ≥ N −10) to account for end effects on the electrostatic interactions.is the free energy associated with each of the six possible electrostatic interactions between EngHD and the phosphate backbone, and depends on the ionic strength as(Debye-Hückel treatment). The electrostatic term is identical for both models, and it is parameterized using the ionic strength dependence data (e.g. Fig. 2c).is the free energy of the EngHD specific interactions with each base on site x. In the PWM-based model, is directly defined according to the 6 × 4 position weight matrix (Fig. 2b) as, whereis the probability of base B to be found in position k of the site, andThis is a model with 24 predetermined and one free fitting parameter. In the structure-based model, is defined by two types of specific interactions: one interaction for each consensus base in the core tetrad (A 2, A 3, T 4, T 5) and one interaction for any other T or A present in the site (A 1 /T 1, T 2, T 3, A 4, A 5, and A 6 /T 6) (asterisk in Fig. 3b). The degenerate A/T interaction represents a structural preference of EngHD for the narrower minor groove and increased flexibility (shorter persistence length) of AT rich regions. δ 16, δ 15, and δ 26 are Kronecker delta that take a value of 1 when the binding site contains the full consensus, the first five or the last five consensus bases, respectively, and 0 otherwise.is the cooperative binding free energy associated to formation of the specific consensus binding site.accounts for: (a) the entropy loss of forming the specific complex, which is nearly complete when the protein locks onto the consensus core tetrad, and thus is not additive; (b) the extra hydrogen bonds that T 1 and A 6 can make with EngHD's residues Q50 and N51, respectively (Fig. 3a), once the protein is forming the specific complex. As guidance, the two examples of binding sites shown in Fig. 3b lead to the following calculations for the binding free energy in the structured-based model:; and. In both models the probability of EngHD binding to site x is simply, and the overall probability of finding EngHD bound to DNA and unbound (free) are, respectively

---

### Complex systems and the technology of variability analysis [^e26f6d93]. Critical Care (2004). Low credibility.

F(n) = v (1/N S k = 1 N [y(k) 2 - y n (k) 2])

By performing this analysis for all values of n, it is possible to calculate the relationship between F(n) and n. Scaling or fractal correlation is present if the data is linear on a graph of log F(n) versus log(n). The slope of the graph has been termed a, the scaling exponent, which will vary from 0.5 (white noise or uncorrelated random data) to 1.5 (Brownian noise or integrated white noise or random walk). When a = 1, behaviour corresponds to the 1/f noise. As a increases above 1 to 1.5, behaviour is no longer determined by a power law. Because the linear relationship between log F(n) and log(n) appears to have two distinct linear segments, one for small (n < 11) and large n (n > 11), the slopes of both lines are calculated separately and termed a 1 and a 2, respectively; repeatedly, a 1 has proven superior to a 2 in terms of prognostic ability.

Approximate entropy

ApEn is a measure of 'irregularity'; smaller values indicate a greater chance that a set of data will be followed by similar data (regularity), and a greater value for ApEn signifies a lesser chance of similar data being repeated (irregularity). To calculate ApEn of a series of data, the data series is evaluated for patterns that recur. This is performed by evaluating data sequences or runs of length m, and determining the likelihood that other runs of length m are similar, within a tolerance r. Thus, two parameters, m and r, must be fixed to calculate ApEn. Increased regularity is associated with illness.

The following is a description of the calculation of ApEn. Given any sequence of data points u(i) from i = 1 to N, it is possible to define vector sequences x(i), which consist of length m and are made up of consecutive u(i), specifically defined by the following:

---

### Clotrimazole [^b5a7188a]. FDA. Low credibility.

The dosage of clotrimazole OTIC for treatment of otomycosis in adults is 1 vial OTIC BID for 14 days (1%/0.17 mL)

---

### Optical imaging of single-protein size, charge, mobility, and binding [^b22fd451]. Nature Communications (2020). High credibility.

Equation of motion of tethered protein molecules

The motion of a protein tethered to a surface by PEG is determined by, where m, z, c, and q are the mass, displacement, damping coefficient, and charge of the protein, k PEG is the spring constant of the PEG tether, arising from the entropic force, E is the applied electric field. F r is the stochastic force on the PEG and the protein due to thermal fluctuation, which has an average of zero. For a protein with molecular weight of 100 kDa (m = 1.7 × 10 −19 g) under an electric field oscillating at 80 Hz, the first and the second terms are ~10 −1 and 10 −3 pN, respectively, which are much smaller than the entropic force estimated below.

To estimate the entropic force associated with the conformation change of the PEG tether, we use the FJC model, which leads to an entropic force ofwhere k B is the Boltzmann constant, T is temperature, b is the Kuhn length of PEG, and n is the number of segments with length of b. For PEG10k, b = 0.55 nm, n = 113,… The entropic force is 22.8 pN when the PEG is stretched to 63 nm, which is many orders of magnitude greater than the first and second terms of Eq. (4). This simplifies Eq. (4) to, which is Eq. (2). For a sinusoidal electrical field, where E 0 is the field amplitude andis the angular frequency. The sinusoidal oscillation of the displacement with amplitude Δ z 0 takes the form of, where θ is the phase shift of the oscillation with respect to the field. Both the oscillation amplitude and phase shift can be determined by performing FFT on the image sequence.

---

### Determining sequencing depth in a single-cell RNA-seq experiment [^ce3ab87a]. Nature Communications (2020). High credibility.

An underlying question for virtually all single-cell RNA sequencing experiments is how to allocate the limited sequencing budget: deep sequencing of a few cells or shallow sequencing of many cells? Here we present a mathematical framework which reveals that, for estimating many important gene properties, the optimal allocation is to sequence at a depth of around one read per cell per gene. Interestingly, the corresponding optimal estimator is not the widely-used plug-in estimator, but one developed via empirical Bayes.

---

### Understanding statistical terms: 2 [^351bd399]. Drug and Therapeutics Bulletin (2009). Low credibility.

An increasing number of statistical terms appear in journal articles and other medical information. A working knowledge of these is essential in assessing clinical evidence. With this in mind, we are producing a series of explanatory articles covering various statistical terms and their uses. This, the second article in the series, will focus on some of the most common terms used in reporting the results of randomised controlled trials.1.

---

### Barren plateaus in quantum neural network training landscapes [^ae50fda4]. Nature Communications (2018). Medium credibility.

The objective operator H is chosen to be a single Pauli ZZ operator acting on the first and second qubits, H = Z 1 Z 2. The gradient is evaluated with respect to the first parameter, θ 1,1. This simple choice helps to extract the exponential scaling. As complex objectives can be written as sums of these operators, the results for large objectives can be inferred from these numbers. Moreover, it is clear that for any polynomial sum of these operators, the exponential decay of the signal in the gradient will not be circumvented.

From Fig. 3 we see that for a single 2-local Pauli term, both the expected value of the gradient and its spread decay exponentially as a function of the number of qubits even when the number of layers is a modest linear function. Empirically for our linear connectivity, we see that value is about 10 n where n is the number of qubits, following the expected scaling of O (n 1/ d) where d is the dimension of the connectivity. For empirical reference, the expected gate depth in a chemistry ansatz such as unitary coupled cluster is at least O (n 3), meaning that if the initial parameters were randomized, this effect could be expected on less than 10 orbitals, a truly small problem in chemical terms. We also observe in Fig. 4 that as the number of layers increases, there is a transition to a 2-design where the variance converges. This leads to a distinct plateau as the circuit length increases, where the height of the plateau is determined by the number of qubits. An additional example with an objective function defined by projection on a target state is provided as Supplementary Figures 1 and 2, showing the rapid decay of variance and similar plateaus as a function of circuit length. These results substantiate our conclusion that gradients in modest-sized random circuits tend to vanish without additional mitigating steps.

---

### Uncovering the mechanism for aggregation in repeat expanded RNA reveals a reentrant transition [^5ddf4c35]. Nature Communications (2023). High credibility.

Methods

Partition functions determine equilibrium behavior

We consider a nucleic acid sequence comprised of n stickers separated by n −1 linkers (Fig. 1 a). Stickers are self-complementary and bind through base pairing interactions, such that each sticker can be bound to at most one other sticker. The strength of the sticker interactions, F b, is determined by the sequence of the stickers; for example, an RNA GC sticker with A nucleotide linkers in standard conditions has F b = −6.4 kcal/mol (or, for DNA, −1.4), while a GCGC sticker has F b = −12.2 kcal/mol (−5.8 for DNA). These are calculated using the classic nearest-neighbor model for RNA or DNA base-pairing interactions. The linkers, each of which is of length l, are inert.

We seek to predict how frequently multimers comprised of m strands form, and how this frequency changes with m. Aggregation occurs in the parameter regime where the concentration of multimers comprised of m strands, c m, increases with m. c m is defined as the sum of all structures that have m strands connected by base pairing interactions. In equilibrium, c m is proportional to the partition function of m -mers, Z m :Here, σ m is a structure comprised of m strands linked by base pairing, including potential intramolecular bonds; and β = 1/ k B T where k B is Boltzmann's constant and T is the temperature measured in Kelvin. F (σ m) is the free energy of the structure, given bywhere N b (σ m) is the number of bonds in the structure, and Δ G assoc is the hybridization penalty associated with intermolecular binding (discussed below). Each closed loop of length l loop leads to an entropic penalty of Δ S loop (l loop), associated with the decrease in three-dimensional configurations of the single-stranded region of the loop compared to a free chain, given by, where v s is the volume within which two nucleotides can bind, and b is the persistence length of single-stranded regions. This equation treats the single-stranded loop as an ideal chain. An excluded volume term v m 2 can be added to Eq. 4 but we assume v is small enough that this term is negligible except for very large m (see Supplementary Note 4 for further discussion).

---

### Sodium fluoride1.1%, potassium nitrate 5% (sodium fluoride 5000 ppm sensitive) [^0c434f29]. FDA (2020). Medium credibility.

Active Ingredients:

Sodium Fluoride 1.1% (w/w)

Potassium Nitrate 5%

---

### The effects of structural complexity on age-related deficits in implicit probabilistic sequence learning [^3228c747]. The Journals of Gerontology: Series B, Psychological Sciences and Social Sciences (2016). Low credibility.

Objective

The primary objective was to determine whether age deficits in implicit sequence learning occur not only for second-order probabilistic regularities (event n - 2 predicts n), as reported earlier, but also for first-order regularities (event n - 1 predicts event n). A secondary goal was to determine whether age differences in learning vary with level of structure.

Method

Younger and older adults completed a nonmotor sequence learning task containing either a first- or second-order structure. Learning scores were calculated for each subject and compared to address our research objectives.

Results

Age deficits in implicit learning emerged not only for second-order probabilistic structure, but also for simple, first-order structure. In addition, age differences did not vary significantly with structure; both first and second order yielded similar age deficits.

Discussion

These findings are consistent with the view that there is an associative binding deficit in aging and that this deficit occurs for implicit as well as explicit learning and across simple and more complex sequence structures.

---

### The early days of DNA sequences [^c6984123]. Nature Medicine (2001). Excellent credibility.

Fred Sanger was awarded the rare distinction of two Nobel Prizes for Chemistry, in 1958 and 1980. The first was for his work on the structure of proteins, particularly that of insulin, the latter for his contribution concerning the determination of nucleic acid sequences, the foundation work that ultimately led to The Human Genome Project.

---

### Early detection of emerging viral variants through analysis of community structure of coordinated substitution networks [^2accb953]. Nature Communications (2024). High credibility.

Theorem 1 Suppose that the 2-haplotype (1, 1) is not viable, i.e. f 11 = 0. Then

Proof The proof follows the same lines as the proof in ref. Given that f 11 = 0, we haveandIt is easy to see that the terms in Eqs. (7) and (8) except for the last ones are equal. Thus we havewhere the last inequality follows from (5). Thus, the inequality (6) holds.

Using Theorem 1, we can evaluate the likelihood of the event that a large number of genomes contain 2-haplotype (1, 1) given that this 2-haplotype is not viable. Considering the density of sampling and the number of SARS-CoV-2 genomes analyzed in this study, we assume that observed and expected numbers of 2-haplotypes are close to each other. Let q is the probability of observing a genome containing 2-haplotypes (1, 1) among N genomes given that f 11 = 0. Following ref. we model the count of such genomes, X, with a binomial distribution B (N, q). The probability thatis:where F X is the binomial cumulative distribution function. Theorem 1 implies an upper bound for q: ThereforeWe consider SAVs at positions u and v linked when the probabilityis sufficiently low, which is guaranteed when its upper bound in Eq. (11) is sufficiently low, i.e.where ρ is a chosen significance level, and the denominatoris a Bonferroni correction. The latter is used to account for multiple comparisons betweenpairs of SAVs being tested for linkage. This leads to the formula (10).

---

### Hourly step recommendations to achieve daily goals for working and older adults [^45deacdb]. Communications Medicine (2024). Medium credibility.

Discussion

Studies have shown that prompts are an effective intervention in changing step behaviour for most participants. As different demographic groups exercise at different timings (e.g. working versus older adults), personalised prompts may be more beneficial at specific times convenient for individuals. We provide the step counts that participants should achieve in the evenings to ensure they achieve the daily step goals (5000, 7500, or 10,000 steps) by the end of the day (Table 3 and Supplementary Tables S3 and S4). These recommendations could then be experimentally tested in future studies.

The results suggest that the daily step goals (5000, 7500, or 10,000 steps) influenced the steps behaviour of the participants. The number of participant days rose sharply from 1090 (2.08%) with 9500–9999 steps to 7507 (14.3%) with 10,000 to 10,499 steps (Fig. 1a). The increase in the HealthPoints earned (from 25 (7500–9999 steps) to 40 (10,000 steps and above) HealthPoints) would reduce the days required to complete the first tier from 30 to 19 days (Supplementary Table S1). By setting a 10,000 daily step goal, participants whose daily step counts were just below 10,000 could have been motivated by the incentives to put in extra effort to achieve the goal, leading to the observed spike. However, we also observed a decreasing number of participant days after each goal, which may indicate that participants lose motivation once the goal has been achieved.

---

### Transformer-based long-term predictor of subthalamic beta activity in Parkinson's disease [^d4cea312]. NPJ Parkinson's Disease (2025). Medium credibility.

Architecture

The proposed architecture aims to improve long-term DBS treatment for PD. Our aim is to predict a certain daily distribution of beta oscillatory activity based on recordings from previous days. This has very important clinical implications because it represents the first step toward IPGs that can automatically adjust stimulation parameters and set them based not on the immediate readout of the brain signal but on its evolution over time (see Discussion).

We used a Transformer model (3.6 M parameters) to address our time series forecasting problem, due to the multi-head attention (MHA) core, based on a general notion of sparsity of interactions within the sequence (Fig. 2 b). The input 'x' to the Transformer (see previous subsection) is a tensor of size [N, n _ b i n s] where N is the number of consecutive distributions used as input, n _ b i n s is considered as the embedding size. This means that each distribution in the sequence would represent a token, for which we can perform the one-step-ahead and the multi-step-ahead prediction. Given the input sequence, the Transformer processes it through three encoder layers and six decoder layers, followed by an FC layer with a sigmoid function as output non-linearity. The output of each layer for each position in the input sequence is a representation of that position after processing. When using Transformers for sequence-to-sequence tasks, the choice of x [−1] as input for the FC layer stands for using the representation of the last position in the sequence as the final representation for the entire sequence. Since we are interested in predicting the distribution for the (N + K)th day, with K ∈ [1, 6], based on the information from the previous six days, using x [−1] as input to the FC layer captures the model's understanding of the entire sequence. The FC layer then projects this representation to the output space, with size n _ b i n s, to achieve the final prediction.

We need to set specific hyperparameters for our framework. We distinguish between dataset-related, architecture-related, and training-related hyperparameters, all listed in Supplementary Table 2 and Supplementary Table 3.

---

### The making of the standard model [^0fc9a4ce]. Nature (2007). Excellent credibility.

A seemingly temporary solution to almost a century of questions has become one of physics' greatest successes.

---

### Data-driven insights to inform splice-altering variant assessment [^90c9d08f]. American Journal of Human Genetics (2025). Medium credibility.

First base of the exon (E+1)

The exonic bases adjacent to the splice site play less of a role in 3′SS selection than 5′SS selection; however, there is greater weight on the first base of the exon (E+1) for determining the location of the 3′SS. We determined the preference order at the first base to be G > A = C > T, where A or C was interchangeable in terms of resultant splice site strength. Following the acceptor flowchart, a variant with the reference E+1G is classified as "DA6 standard" with 66.7% [58.1–75.3] spliceogenicity (n = 96). However, recognition of the acceptor motif also depends on the −3 position. The spliceogenicity of an E+1G variant may be upgraded by considering the surrounding sequence context. If the nucleotide at −3 is not a pyrimidine, then "DA3 standard" is applicable with 99.2% [91.6–100] spliceogenicity (n = 124). We note that variants at the first base of the exon are highly relevant for an established splicing theory of AG-dependent exons, which have a G as the first exonic base. This theory proposed that AG-dependent exons do not require a long PPT to recognize the 3′SS. However, in our analyses, including PPT length or strength as a contextual modifier did not assist in the classification of E+1 variants.

---

### Reconstruction of stochastic temporal networks through diffusive arrival times [^3c835452]. Nature Communications (2017). Medium credibility.

Temporal networks have opened a new dimension in defining and quantification of complex interacting systems. Our ability to identify and reproduce time-resolved interaction patterns is, however, limited by the restricted access to empirical individual-level data. Here we propose an inverse modelling method based on first-arrival observations of the diffusion process taking place on temporal networks. We describe an efficient coordinate-ascent implementation for inferring stochastic temporal networks that builds in particular but not exclusively on the null model assumption of mutually independent interaction sequences at the dyadic level. The results of benchmark tests applied on both synthesized and empirical network data sets confirm the validity of our algorithm, showing the feasibility of statistically accurate inference of temporal networks only from moderate-sized samples of diffusion cascades. Our approach provides an effective and flexible scheme for the temporally augmented inverse problems of network reconstruction and has potential in a broad variety of applications.

---

### Abstract representations of events arise from mental errors in learning and memory [^748e9b84]. Nature Communications (2020). High credibility.

Estimating parameters and making quantitative predictions

Given an observed sequence of nodes x 1,…, x t −1, and given an inverse temperature β, our model predicts the anticipation, or expectation, of the subsequent node x t to be. In order to quantitatively describe the reactions of an individual subject, we must relate the expectations a (t) to predictions about a person's reaction timesand then calculate the model parameters that best fit the reactions of an individual subject. The simplest possible prediction is given by the linear relation, where the intercept r 0 represents a person's reaction time with zero anticipation and the slope r 1 quantifies the strength with which a person's reaction times depend on their internal expectations.

In total, our predictionscontain three parameters (β, r 0, and r 1), which must be estimated from the reaction time data for each subject. Before estimating these parameters, however, we first regress out the dependencies of each subject's reaction times on the button combinations, trial number, and recency using a mixed effects model of the form 'RT~log(Trial)*Stage+Target+Recency+(1+log(Trial)*Stage+Recency|ID)', where all variables were defined in the previous section. Then, to estimate the model parameters that best describe an individual's reactions, we minimize the RMS prediction error with respect to each subject's observed reaction times, where T is the number of trials. We note that, given a choice for the inverse temperature β, the linear parameters r 0 and r 1 can be calculated analytically using standard linear regression techniques. Thus, the problem of estimating the model parameters can be restated as a one-dimensional minimization problem; that is, minimizing RMSE with respect to the inverse temperature β. To find the global minimum, we began by calculating RMSE along 100 logarithmically spaced values for β between 10 −4 and 10. Then, starting at the minimum value of this search, we performed gradient descent until the gradient fell below an absolute value of 10 −6. For a derivation of the gradient of the RMSE with respect to the inverse temperature β, we point the reader to the Supplementary Discussion. Finally, in addition to the gradient descent procedure described above, for each subject we also manually checked the RMSE associated with the two limits β → 0 and β → ∞. The resulting model parameters are shown in Fig. 4 a, b for random walk sequences and Fig. 4 g, h for Hamiltonian walk sequences.

---

### Modelling sequences and temporal networks with dynamic community structures [^a14be7f3]. Nature Communications (2017). Medium credibility.

Temporal networks

A general model for temporal networks treats the edge sequence as a time series. We can in principle use the present model without any modification by considering the observed edges as tokens in the Markov chain, that is, x t = (i, j) t, where i and j are the endpoints of the edge at time t (see Fig. 1b). However, this can be suboptimal if the networks are sparse, that is, if only a relatively small subset of all possible edges occur, and thus there are insufficient data to reliably fit the model. Therefore, we adapt the model above by including an additional generative layer between the Markov chain and the observed edges. We do so by partitioning the nodes of the network into groups, that is, c i ∈ [1, C] determines the membership of node i in one of C groups, such that each edge (i, j) is associated with a label (c i, c j). Then we define a Markov chain for the sequence of edge labels and sample the actual edges conditioned only on the labels. Since this reduces the number of possible tokens from O (N 2) to O (C 2), it has a more controllable number of parameters that can better match the sparsity of the data. We further assume that, given the node partitions, the edges themselves are sampled in a degree-corrected manner, conditioned on the edge labels, where κ i is the probability of a node being selected inside a group, with. The total likelihood conditioned on the label sequence becomesSince we want to avoid overfitting the model, we once more use noninformative priors, but this time on { κ i }, and integrate over them

---

### Accurately clustering biological sequences in linear time by relatedness sorting [^6aa50ac0]. Nature Communications (2024). High credibility.

Phase 2: pre-sorting sequences based on k-mer similarity in linear time

The key innovation underlying the Clusterize algorithm is to pre-sort sequences such that accurate clustering can be performed in linear time. Pre-sorting is conducted within each partition until reaching an iteration limit (i.e. parameter B) or the rank ordering of sequences stabilizes. First, a sequence is randomly selected from each partition with a probability proportional to its average variability in rank order (v) multiplied by its length (L): This formula encourages the selection of longer sequences that still have not stably sorted. Then, the k-mer distance (1 - k-mer similarity) is calculated between the sequence and all others, resulting in a vector of distances (d 1). Second, another sequence is randomly selected with probability proportional to d 1 and inversely proportional to the non-overlapping difference in length (Δov) with the first randomly selected sequence: This prioritizes sequences that overlap completely with the first sequence but are dissimilar. Again, the k-mer distance is calculated between the second sequence and all others (d 2). The relative difference between these two vectors (d 1 − d 2) becomes the initial vector (D) upon which the rank order of sequences is determined.

The process of generating new relative distance vectors (D) is repeated up to 2000 iterations (i.e. parameter B) within each partition. I termed this process relatedness sorting because more similar sequences become closer together with additional iterations. Only a single vector is kept after each iteration to limit the memory overhead of phase 2. There are multiple ways in which to combine D i and D i-1 into a new unified vector, D. I chose to project both vectors onto their axis of maximum shared variance after centering both vectors to have a mean of zero (Fig. 1I). This is equivalent to performing principal component analysis on the two column matrix formed by combining D i and D i-1 and retaining only the first principal component (as D). Each vector is ordered with radix sorting to define the rank order of sequences. Clusterize keeps track of the number of positions each sequence moves in the relatedness ordering (v) using an exponentially weighted moving average with smoothing parameter α (0.05): Note that the vector v 0 is initialized with values of 2000 (i.e. parameter C) and values of v are capped at 2000 in each iteration as an upper limit for practicality.

---

### Optimal echo times for multi-gradient echo-based bfield-mapping [^4eadb218]. NMR in Biomedicine (2020). Medium credibility.

B 0 field maps are used ubiquitously in neuroimaging, in disciplines ranging from magnetic resonance spectroscopy to temperature mapping and susceptibility-weighted imaging. Most B 0 maps are acquired using standard gradient-echo-based vendor-provided sequences, often comprised of two echoes spaced a few milliseconds apart. Herein, we analyze the optimal spacing of echo times, defined as those maximizing precision-minimizing the standard deviation-for a fixed total acquisition time. Field estimation is carried out using a weighted least squares estimator. The standard deviation is shown to be approximately inversely proportional to the total acquisition time, suggesting a law of diminishing returns, whereby substantial gains are obtained up to a certain point, with little improvement beyond that point. Validations are provided in a phantom and a group of volunteers. Multi-gradient echo sequences are readily available on all manufacturer platforms, making our recommendations straightforward to implement on any modern scanner.

---

### A last-in first-out stack data structure implemented in DNA [^a022c217]. Nature Communications (2021). High credibility.

For the five 40-operation sequences tested, the model stipulates that a strict washing efficiency in the range 0 < μ < 0.03, 0 < ϕ 0 < 0.02 is required for storage and correct popping of all 20 signals in all cases (largely dictated by seq20). It should be highlighted that certain pathological sequences like seq1 (Supplementary Note 10.2) are insensitive to increasing ϕ 0: in this case, the non-specific leak of supernatant species through the wash actually helps produce the correct read out at the next step. To some extent, this is also true for the seq5 sequence used. We note that seq5 sequences with different arrangements of X 's and Y 's can be constructed and may perform differently to the seq5 sequence chosen. As a rough estimate, our estimated experimental washing efficiency W1 is predicted to achieve an average of 3.4 signals correctly popped per operations sequence, before output can no longer be discerned. This value generally agrees with our experimental observations of the two signal DNA stack system (see below).

---

### Six steps in quality intervention development (6SQuID) [^221654f2]. Journal of Epidemiology and Community Health (2016). Low credibility.

Conclusion

In order to improve the effectiveness of public health interventions, a systematic approach to intervention development is required, as well as rigorous evaluation. However, little practical guidance exists for public health practitioners and researchers that explains the essential stages of intervention development. We argue that this process can be broken down into six key steps: defining and understanding the problem; identifying modifiable determinants; deciding on the mechanisms of change; clarifying how these will be delivered; testing and adapting the intervention; and collecting initial evidence of effectiveness. This model imposes somewhat arbitrary cut-offs in the process of intervention development and suggests a linear progression. In practice developers often return to an earlier step in the sequence before reaching step 6, and subsequently 'definitive trials' can lead to further revisions of the intervention. However, we hope that if each of these six steps is carefully addressed in the design of interventions better use will be made of scarce public resources by avoiding the costly evaluation, or implementation, of unpromising interventions.

What is already known on this subject?

There is little practical guidance for researchers or practitioners on how best to develop public health interventions. Existing models are generally orientated towards individual behaviour change and some are highly technical and take years to implement.

What this study adds?

This paper provides a pragmatic six-step guide to develop interventions in a logical, evidence-based way to maximise likely effectiveness. If each step is carefully addressed, better use will be made of scarce public resources by avoiding the costly evaluation, or implementation, of unpromising interventions.

---

### Opportunities and challenges associated with clinical diagnostic genome sequencing: a report of the Association for Molecular Pathology [^434309ee]. The Journal of Molecular Diagnostics (2012). Medium credibility.

Clinical diagnostic genome sequencing — consensus calling of heterozygous positions depends on sampling depth because each molecule is sequenced independently; the probability of detecting a second allele is dictated by the sampling probability, and if the second allele must be present in at least 20% of calls for a heterozygous consensus, the position needs to be sampled at least 18 times to be at least 99% confident of detecting both alleles.

---

### Inferring experimental procedures from text-based representations of chemical reactions [^7cb71d05]. Nature Communications (2021). High credibility.

Action sequence filtering

Some action sequences are not adequate for training and were removed from the data set. This is the case for reactions with incomplete experimental procedures (such as the ones shortening the description by referring to other procedures), or for unsuccessful or incomplete action sequence extraction.

We ignored experimental procedures when their processing with the Paragraph2Actions modelcontained any InvalidAction or FollowOtherProcedure action. We filtered out any experimental procedure containing fewer than five actions because such short sequences are not likely to describe a chemical reaction appropriately. We also ignored experimental procedures that likely describe multiple reaction steps. Therefore, any experimental paragraph whose action sequence contains multiple Yield actions interlaid with actions other than purification or work-up were rejected.

Action sequence postprocessing

The action sequence extraction model operates sentence by sentence. When combining the actions for a full experimental procedure, it is crucial to provide a consistent record, guaranteeing correct relative dependencies between instructions. To do so, the action sequence extracted from the experimental procedure text underwent a series of postprocessing steps.

We removed all NoAction actions, because these instructions usually arise from sentences that are not relevant to the actual synthesis and can safely be ignored.

Whenever possible, we merged Wait actions with previous actions. This provides a more concise representation when a sentence specifies the duration of an action only in the next sentence. For instance, "The mixture was brought to reflux. After 2 hours". is equivalent to a Reflux action with a time parameter of 2 h.

Often, experimental procedures do not explicitly state whether the precipitate or the filtrate should be kept upon filtering. We therefore inferred this information from the previous or subsequent actions.

The use of preceding actions is also important to complete instructions containing information related to previous actions. For instance, an action specifying the temperature parameter with the "same temperature" value, requires the inspection of the preceding actions to identify the latest set temperature.

Finally, we replaced MakeSolution actions appearing as first instruction in a procedure by a series of Add actions. This reduces the differences between equivalent ways of formulating action sequences while keeping an identical logic in terms of experimental operations.

---

### Early nasal microbiota and subsequent respiratory tract infections in infants with cystic fibrosis [^fa0fb4b5]. Communications Medicine (2024). Medium credibility.

Effect of a "first hit" on the microbiota in infants with CF

We investigated the role of first antibiotic treatment and the role of number of antibiotic treatments in the first year of life analogous to 1.−4. of the first part of the study. Infants with CF were divided into three groups depending on the number of antibiotic treatments: "Never" (0 antibiotic treatments, n = 17), " < 75th Percentile" (1 antibiotic treatment, n = 25), and " ≥ 75th Percentile" (2 or more antibiotic treatments, n = 8). We investigated the role of first RTI in the first year of life analogous to 1.−4. of the first part of the study.

Statistics and reproducibility

Statistics were performed with statistical software R (version 4.1.2). For normal distributed microbiota exposures, generalized additive mixed models (gamm function, mgcv package) were performed with autoregressive structure (lag 1) to account for temporal correlation within subjects (random effect) and corrected for before named possible confounders (fixed effect). The age of the infant in weeks was added in a smooth term. β-diversity was tested with permutational multivariate analysis of variance (PERMANOVA). Terms were added with the option "marginal" to account for all predictor variables equally (adonis2 function, vegan package). Differential abundance analysis of bacterial families was calculated with MaAsLin2 after total sum scaling normalization. We focused on the nine most abundant families for the main manuscript and summarized the rest ("others") due to 16S rRNA sequencing resolution. Reported q -values were corrected for multiple testing with Benjamini-Hochberg procedure as recommended in MaAsLin2 default. To control for confounding, we corrected for sweat chloride levels in infants with CF, mode of delivery, feeding type (breastfeeding vs formula fed), parental smoking, siblings, season, and antibiotic treatment. Due to only two infants with CF attending childcare in the first year of life, we matched control cases that did not attend childcare either without correcting in the further analyses. Results with p < 0.05, q < 0.05 respectively, were considered significant.

Reporting summary

Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^01148c3a]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Nonparametric tolerance intervals for non-normal distributions: The above estimate of the tolerance interval would only be applicable to a population that is normally distributed, but when the underlying population is often not normal (eg, when there is a natural boundary that the data cannot exceed (ie, 0% or 100%)) it is helpful to define tolerance intervals using nonparametric methods; the one-sided nonparametric tolerance interval can be determined by finding the value for k that satisfies the cumulative binomial equation, where CL is the confidence level (eg, 0.95), and by setting k = 0 (ie, 0 failures) the formula can be simplified.

---

### Roflumilast (Zoryve) [^e2de6cd7]. FDA (2024). Medium credibility.

The dosage of roflumilast TOP for treatment of plaque psoriasis in adults is 1 application(s) TOP daily (0.3% cream or foam)

---

### Understanding systematic reviews and meta-analyses [formula: see text] [^a0b57635]. JPEN: Journal of Parenteral and Enteral Nutrition (2017). Low credibility.

Systematic reviews should be distinguished from narrative reviews. In the latter, an editor asks an expert to sum up all of the information that is known about a particular topic. However, the expert is under no constraints regarding what he or she does, or does not, choose to include in the review. As a result, his or her bias can influence the final message. A systematic review, which may or may not be written by experts, typically asks a narrower question, and then answers it using the entirety of the medical literature. The systematic review process includes computer searches to identify the pertinent literature, a statement of the inclusion and exclusion criteria for identified studies, a list of items of interest to extract from each study, a method to assess the quality of each study, a summary of the evidence that has been found (which may or may not involve attempts to combine data), a discussion of the evidence and the limitations of the conclusions, and suggestions for future research efforts. If the data are combined, that process is called meta-analysis. In meta-analysis, an estimate of the reliability of each study is made, and those that appear to be more reliable are weighed more heavily when the data are combined. While systematic reviews depend on a more preplanned method and thus, unlike narrative reviews, contain sections on method, they can be easily read once the reader becomes familiar with the vocabulary.

---

### PRISMA 2020 explanation and elaboration: updated guidance and exemplars for reporting systematic reviews [^a6667a54]. BMJ (2021). Excellent credibility.

Essential elements

Provide the full line by line search strategy as run in each database with a sophisticated interface (such as Ovid), or the sequence of terms that were used to search simpler interfaces, such as search engines or websites.
Describe any limits applied to the search strategy (such as date or language) and justify these by linking back to the review's eligibility criteria.
If published approaches such as search filters designed to retrieve specific types of records (for example, filter for randomised trials)or search strategies from other systematic reviews, were used, cite them. If published approaches were adapted — for example, if existing search filters were amended — note the changes made.
If natural language processing or text frequency analysis tools were used to identify or refine keywords, synonyms, or subject indexing terms to use in the search strategy, specify the tool(s) used.
If a tool was used to automatically translate search strings for one database to another, specify the tool used.
If the search strategy was validated — for example, by evaluating whether it could identify a set of clearly eligible studies — report the validation process used and specify which studies were included in the validation set.
If the search strategy was peer reviewed, report the peer review process used and specify any tool used, such as the Peer Review of Electronic Search Strategies (PRESS) checklist.
If the search strategy structure adopted was not based on a PICO-style approach, describe the final conceptual structure and any explorations that were undertaken to achieve it (for example, use of a multi-faceted approach that uses a series of searches, with different combinations of concepts, to capture a complex research question, or use of a variety of different search approaches to compensate for when a specific concept is difficult to define).

---

### Graphs [^c6c6d5a7]. Chest (2006). Low credibility.

Two rules of good graphs are presented and explicated.

---

### Alignment-free population genomics: an efficient estimator of sequence diversity [^6c3f2b55]. G3 (2012). Low credibility.

Derivation of

We wish to know the distribution of shustring lengths as a function of the average number of differences per site. For the derivation of the shustring length distribution, we use the well-kown fact from coalescent theory that the time to the most recent common ancestor of two lineages is approximately exponential with expectation N e, where N e is the effective (haploid) population size.

First, we compute the distribution of X i, i, where we assume that position i in Q and i in S are homologous. The fact that our data are unaligned does not invalidate this assumption for the purpose of deriving. We further assume that no recombination event falls between i and i + X i, i until Q and S coalesce in this genomic region. This is equivalent to assuming that mutation is more frequent than recombination. Moreover, we take π / N e as a proxy for the mutation probability per generation per site. Then we obtain for an exponentially distributed random variable T with expectation N e Second, we compute the distribution of X i, i ′ for i ≠ i ′. We assume that the two subsequences starting at i in Q and at i ′ in S are random words with GC -content 2 p and AT -content 1–2 p. We know from equation 1 inand equation 4 inthatwhich for equiprobable nucleotidessimplifies toIn this case, the distribution of max i ≠ i ′ X i, i ′ is concentrated around x ∼ log 4 (2ℓ S). By combining Equations 1 and 2, we obtainandAs explained in the previous section, we can observewhere f (x) is the absolute number of shustrings of length x for a pair of sequences and ξ is the length of the longest shustring. Now we assume that f (x) is the realization of a Poisson-distributed random variable with parameterand that f (1), f (2),… are independent. We can then readily compute the log-likelihoodfor some C, which does not depend on π. Hence, the maximum-likelihood estimator for π, is given by maximizingOne often needs to computerepeatedly during a sliding window analysis, where an interval Q [i. j] is fixed with, say, j – i = 10 5, i.e. extends over 100 kb. Then, using Q [i,…, j] as the query and S as the subject, the above computations work as well, as we can still assume that every position in Q [i,…, j] has a homolog in S. Here, we observe f (1), f (2),… for the specific window Q [i,…, j], and maximize the likelihood as given in Equation 6.

---

### Two types of motifs enhance human recall and generalization of long sequences [^fb9b3ea3]. Communications Psychology (2025). Medium credibility.

Model simulation

We compared the behavioral results with the model predictions. We used the same sequences instructed to the participants to train the motif learning model, which creates memory representations of sequence motifs from the observational sequences in an abstract space. We then generated sequences based on the representations learned by the model up to the current time point. We came up with generative accuracy as a surrogate for sequence recall accuracy. The generative accuracy was the edit distance between a generative sequence sampled from the model and the instruction sequence in a particular trial. Figure 3 b shows the average generative accuracy of the model. We observed a significant effect of group (χ 2 (2) = 216.23, p < 0.001, Conditional R 2 = 0.13), suggesting that participants in the Motif 1 group (, 95% CI = 0.16 to 0.20) and the Motif 2 group (, 95% CI = 0.15 to 0.18) recalled sequences more accurately during the training blocks than the independent group. Similar to participants, the model remembered sequences with underlying motifs more accurately.

---

### Data storage using peptide sequences [^6d026dfa]. Nature Communications (2021). High credibility.

Erasure based on order checking

Based on the orders of the estimated symbol pairs { S 1, S 2 } and { S 15, S 16 } for 40 sequence set, 2 bits are generated according to the order-checking rule, which will be compared with the first bits of the estimated symbols S 3 and S 7, respectively. Similarly, 3 bits are generated based on the orders of the estimated symbol pairs { S 1, S 2 }, { S 2, S 3 }, and { S 15, S 16 } for 511 sequence set, which will be compared with the 3 bits of the estimated symbol S 4. If any one of the generated order-checking bits does not match the corresponding bit in an estimated sequence, the estimated sequence will be erased.

Selection for each address

According to the address represented by the first two and three elements of a sequence, the sequences are divided into 40 and 511 address groups, respectively. Then for each group, there are following cases possible:

Case 1: There is only one sequence.

Case 2: There are two or more sequences, some of which are the same, where:

2a. there is only one result with two or more sequences; or

2b. there are at least two different results, each with two or more sequences.

Case 3: All sequences in the group are different, where:

3a. different sequences belong to the same spectrum; or

3b. different sequences belong to different spectra.

For Case 1, the only sequence is recovered for the group. For Case 2a, the result with two or more sequences is selected. For Case 2b, the results with the largest number of sequences are first selected. Among the sequences corresponding to these results, the sequence with the highest score according to Steps 3.1–3.5, 4.2, and 4.3 of the two-stage sequencing method is further selected. For Case 3a, the sequence with the highest score according to Steps 4.2 and 4.3 of the two-stage sequencing method is selected. For Case 3b, the sequence with the highest score according to Steps 3.1–3.5, 4.2, and 4.3 of the two-stage sequencing method is selected.

---

### Data storage using peptide sequences [^f0dce428]. Nature Communications (2021). High credibility.

At step 8, the candidate paths could be constructed by combining the three parts: N-terminus, tag, and C-terminus. At step 9, one could follow steps 3 and 4 of the two-stage sequencing method to select and refine the sequences. Note that a larger value for W sometimes introduced one or more wrong amino acids in the head and/or tail parts of a tag, while a smaller value for W may give more reliable tag but the length of the tag may be limited. Therefore, after step 6, if no valid candidate could be found, one may attempt to reduce the value of W with W = w i by increasing i by 1, i.e. i = i + 1, and repeat the tag, N-terminus and C-terminus finding procedure until the candidate sequence could be found or i = V (where V is the maximum number of iterations).

For the special case when the experimental mass with the highest intensity gave an unreliable message due to noise and uncertainty, a highest-intensity-based tag or a valid path with the highest-intensity-based tag could not be found. In this case, the mass with the second highest intensity was used by setting J = J + 1 and i = 1 to find the second highest-intensity-based tag and the candidates. This process would continue until the sequence could be found or J = J max (where J max is the maximum number of higher-ranking-intensity masses allowed to be the start point to find the tag).

---

### Emergence and expansion of SARS-CoV-2 B.1.526 after identification in New York [^db7d3bc4]. Nature (2021). Excellent credibility.

Analysis of this genomic collection (Fig. 1b) showed that by May 2021, SARS-CoV-2 variants (including B.1.526, B.1.1.7 and, more recently, P.1) comprised two-thirds of all sequenced isolates, replacing the vast majority of earlier lineages (Fig. 1b). The proportion of infections caused by B.1.526 rose rapidly from late 2020 to February 2021, and remained at approximately 40–50% of all sequenced cases from March to May 2021, despite a concurrent increase in B.1.1.7. Indeed, during December and January, when the prevalence of B.1.1.7 was still negligible (Fig. 1b), the frequency of all viruses in the B.1.526 lineage increased from less than 5% to 50%, while the frequency of other lineages declined from more than 95% to 50% (Fig. 1b, where white blank space represents other lineages). Calculations using these numbers in a head-to-head comparison and an established mathematical methodindicate that B.1.526 has a growth advantage of approximately 5% per day. Similarly, fitting a logistic regression model to 478 individual observations from the extended timeframe of November 2020 to January 2021 shows that B.1.526 had a similar growth advantage of 4.6% per day (95% confidence interval 2.8–6.5% per day). Given that the serial interval for SARS-CoV-2 transmission is about 7 days in the absence of any intervention, these results suggest that B.1.526 is about 35% more transmissible than non-variant viruses.

---

### Cosyntropin [^de9ad5c2]. FDA (2024). Medium credibility.

DESCRIPTION

Cosyntropin for Injection is a sterile lyophilized powder in vials containing 0.25 mg of Cosyntropin and 10 mg of mannitol to be reconstituted with 1 mL of 0.9% Sodium Chloride Injection, USP. Administration is by intravenous or intramuscular injection. Cosyntropin is a 1 to 24 corticotropin, a synthetic subunit of ACTH. It is an open chain polypeptide containing, from the N terminus, the first 24 of the 39 amino acids of natural ACTH. The sequence of amino acids in the 1 to 24 compounds is as follows:

---

### Bayes' theorem, COVID19, and screening tests [^fd5f2e1a]. The American Journal of Emergency Medicine (2020). Medium credibility.

The COVID19 crisis has provided a portal to revisit and understand qualities of screening tests and the importance of Bayes' theorem in understanding how to interpret results and implications of next actions.

---

### Technical update on HIV-1 / 2 differentiation assays [^50eb7933]. CDC (2016). Medium credibility.

HIV-1/HIV-2 differentiation testing — alternatives when the FDA-approved antibody differentiation immunoassay cannot be used states that there may be circumstances under which a laboratory is unable to adopt the FDA-approved HIV-1/HIV-2 antibody differentiation immunoassay, and in this situation a laboratory has alternatives for that step in the algorithm, some of which could delay turnaround time for test results. Send specimens to another laboratory that offers the FDA-approved supplemental HIV antibody differentiation assay. Refer to CDC/APHL laboratory testing guidance section I, "Alternative Testing Sequences When Tests in the Recommended Algorithm Cannot be Used".

---

### Protein sequence modelling with Bayesian flow networks [^57c9aab3]. Nature Communications (2025). High credibility.

In practice, it is possible to derive a continuous-time loss L ∞ where N → ∞. Under the assumption of a uniform prior θ (0), the loss becomes remarkably simple and easy to approximate through Monte Carlo integration,

This loss function reflects the transition from an N -step process to a continuous-time process, where t moves from 0 to 1. The sequence of accuracies α 1… α N is replaced with the monotonically increasing accuracy schedule, whereis the final accuracy at t = 1, and its derivative scales the loss through time,

The continuous-time loss corresponds to a negative variational lower bound, and therefore by optimising the parameters of ϕ to minimise it, we arrive at an approximation of the true distribution p (x) from which x was drawn. To train our models, we follow the general procedure outlined in ref. Specifically, the computation of the loss is described in Box 1.

Entropy encoding

In ref. the current time t is presented to the output network alongside the input distribution θ (t). During initial sampling experiments, we discovered that, during the sampling process, the entropy of the input was noticeably higher at a given time t in comparison to that observed during training. We believe that this phenomenon occurs as the input distribution θ (t) contains additional entropy from uncertainty in the output distribution. When time t is presented as an additional input to the network, this mismatch is, essentially, out of distribution for the output network, hampering performance. To resolve this, we replace the conventional Fourier encoding of time t used inwith a Fourier encoding of the entropy of each variable, appended to its corresponding input distribution before being passed into the network. This effectively makes the model invariant to t, and such an approach mirrors similar techniques used in diffusion models. In general, the use of entropy encoding is a design choice, and alternative representations of progress through the sampling process could suffice.

---

### The DNA sequence and biological annotation of human chromosome 1 [^b256cb0d]. Nature (2006). Excellent credibility.

The reference sequence for each human chromosome provides the framework for understanding genome function, variation and evolution. Here we report the finished sequence and biological annotation of human chromosome 1. Chromosome 1 is gene-dense, with 3,141 genes and 991 pseudogenes, and many coding sequences overlap. Rearrangements and mutations of chromosome 1 are prevalent in cancer and many other diseases. Patterns of sequence variation reveal signals of recent selection in specific genes that may contribute to human fitness, and also in regions where no function is evident. Fine-scale recombination occurs in hotspots of varying intensity along the sequence, and is enriched near genes. These and other studies of human biology and disease encoded within chromosome 1 are made possible with the highly accurate annotated sequence, as part of the completed set of chromosome sequences that comprise the reference human genome.

---

### Standards and guidelines for validating next-generation sequencing bioinformatics pipelines: a joint recommendation of the Association for Molecular Pathology and the college of American pathologists [^101e0942]. The Journal of Molecular Diagnostics (2018). Medium credibility.

Minimum number of variants and calculation — For in-scope variants, they fall into one of three categories: SNVs, indels, and other; the last category includes variants that cannot be classified as SNVs or indels, such as vertically complex variants that are ≤ 21 bp in contiguous size or variants that require special algorithmic approaches; for each category in scope, the minimum number of variants to include in the validation sample set can be calculated as n = ln(1 – CI)/ln(P), where n is the number of variants to be included, CI is the confidence level of detection, and P is the probability of detection.

---

### A last-in first-out stack data structure implemented in DNA [^a299e2f0]. Nature Communications (2021). High credibility.

Model predictions of DNA stack with two signal types

Next, we extrapolated the parameterised model to exhaustively predict the (ideal) operation of the DNA stack with two signal types X and Y, this time over longer sequences of recording and popping operations.

The model was subject to five operation sequences each containing 20 record operations and 20 pop operations in total (Supplementary Note 10.1). In every case, approximately equal numbers of X and Y signals were recorded, and the stack always finished empty (under ideal operation). Four sequences, denoted seqN, were periodic, with N records always followed by N pops for N = 1, 5, 10, 20. One sequence, denoted seqR, arranged the 20 record and 20 pop operations randomly. Additionally, we tested the model with three increasingly stringent washing procedures. Procedure W1 was our experimental protocol (μ = 0.1, ϕ 0 = 0.33), procedure W2 was approximately twice as efficient (μ = 0.05, ϕ 0 = 0.15) and W3 was approximately twice as efficient again (μ = 0.02, ϕ 0 = 0.05).

For all operations sequences and all washing efficiencies, the model suggested that instead of detecting the most common stack species in the system, the most robust way to read out signals stored was to detect the majority signal popped into supernatant (Xr or Yr) following the addition of read strands (Supplementary Note 10.1). The model suggested that under imperfect washing, the stack population actually becomes de-synchronised from the target stack structure quickly (faster for W1 than for W3, as expected). However, despite this de-synchronisation of the stack population, the majority popped signal in supernatant remains correct for some time because the signals popped into supernatant derive from only the last signals stored on stacks in the population. That is, the stack population is simply required to have the majority of stacks terminating with the correct end signal, it is not required for all stacks in the population to be identical and synchronised (a much stricter condition). The fact that majority popped signal is the most robust system read out is indeed convenient, since the popped Xr and Yr complexes would be how the DNA stack is eventually linked to downstream nucleic acid circuits.

---

### Data storage using peptide sequences [^bf7b4c65]. Nature Communications (2021). High credibility.

Details of the two-stage sequencing method

Figure 5 shows a flowchart illustrating a method of two-stage sequencing. Four steps are involved in the two-stage sequencing method: (1) preprocessing, (2) candidate sequence generation, (3) sequence selection, and (4) candidate refining. As shown in Fig. 5, Steps 1–3 belong to the first stage (Stage 1), while Step 4 is processed in the second stage (Stage 2). In Stage 1 of the two-stage sequencing method, partial sequence is inferred using the preprocessed data after Step 1. In Stage 2, the remaining part of the sequence is determined using the raw data.

Fig. 5
A flowchart illustrating the method of two-stage sequencing.

AAC stands for amino acid combinations.

At Step 1, preprocessing is performed. At Step 2, the preprocessed data from step 1 is used to find the valid paths (sequences), and the number n of candidate sequences is counted. At Step 3, the effects of the following five factors are jointly considered when arriving at the score of a sequence candidate from Step 3.1 to Step 3.5: length of consecutive amino acids retrieved, number of amino acids retrieved, match error, average intensity of amino acids retrieved, and number of occurrences for different ion types with different offsets. The sequences with the longest length of consecutive amino acids retrieved are first selected (Step 3.1). Among the selected sequences, the sequences with the largest number of amino acids retrieved are then selected (Step 3.2). For the sequences with equal length of consecutive amino acids retrieved together with equal number of amino acids retrieved, the match error is evaluated, which is the mean error between the observed mass values for the amino acids retrieved from the experimental spectrum and the actual mass values of the amino acids normalized by the corresponding observed mass values (Step 3.3). If there is more than one sequence with identical match errors, the average intensity of amino acids retrieved is further calculated and a higher score is given to a sequence with a larger average intensity value (Step 3.4). In addition, multiple ion types are usually considered as the important factors in inferring an amino acid, which means that a mass value may correspond to different types of ions in the spectrum. Generally, the more the number of occurrences for different ion types of an amino acid is, the more likely the amino acid is correct. Therefore, for the sequences with equal score after the aforementioned evaluations of Steps 3.1–3.4, the number of occurrences for different ion types is counted to determine the sequence (Step 3.5). The mass offset sets for the N-terminal a-ion, b-ion, and c-ion type sets, i.e. {a, a-H 2 O, a-NH 3, a-NH 3 -H 2 O}, {b, b-H 2 O, b-H 2 O-H 2 O, b-NH 3, b-NH 3 -H 2 O}, and {c, c-H 2 O, c-H 2 O-H 2 O, c-NH 3, c-NH 3 -H 2 O} are {−27, −45, −44, −62}, {+1, −17, −35, −16, −34}, and {+18, 0, −18, +1, −17}, respectively. According to the fragmentation method and the property of the data, all or some of the above ion types can be used flexibly.

---

### Hydrocodone bitartrate (hysingla ER) [^0c03a0bc]. FDA (2023). Medium credibility.

Warnings and precautions regarding the use of hydrocodone bitartrate ER PO (also known as Hysingla ER):
- **Adrenal insufficiency**: use caution in patients taking the drug for a prolonged period (> 1 month).
- **Central sleep apnea**: use caution in patients taking higher doses.
- **Decreased serum hydrocodone level**: use caution in patients taking CYP3A4 inducers (such as rifampin, carbamazepine, or phenytoin) or discontinuing CYP3A4 inhibitors (such as macrolide antibiotics, azole antifungals, or protease inhibitors).
- **Erectile dysfunction, infertility**: use caution in patients taking the drug for a prolonged period.
- **Exacerbation of increased ICP**: use caution in patients with increased ICP, brain tumor, or head injury.
- **Growth suppression**: use caution in patients with chronic corticosteroid therapy. Monitor growth regularly in pediatric patients receiving chronic corticosteroid therapy. Reassess the need for hydrocodone regularly and adjust the corticosteroid therapy as appropriate.
- **Hypotension, syncope**: use caution in patients with reduced blood volume or taking other CNS depressants. Monitor BP after initiation and dose titration. Avoid use in patients with circulatory shock.
- **Mask symptoms of head injury**: use caution in patients with head injury. Avoid use in patients with impaired consciousness or coma.
- **Opioid overdose**: use caution in patients taking CNS depressants or with a history of opioid use disorder or prior opioid overdose. Consider prescribing naloxone based on the patient's risk factors for overdose.
- **Opioid withdrawal syndrome**: do not discontinue abruptly in patients physically dependent on opioids.
- **Opioid withdrawal syndrome**: use caution in patients taking CYP3A4 inducers (such as rifampin, carbamazepine, or phenytoin) or discontinuing CYP3A4 inhibitors (such as macrolide antibiotics, azole antifungals, or protease inhibitors).
- **Opioid withdrawal syndrome**: use extreme caution in patients taking mixed agonist/antagonist analgesics (such as pentazocine, nalbuphine, or butorphanol) or partial agonist analgesics (such as buprenorphine).
- **Prolonged QT interval**: use caution in patients with congestive HF, bradyarrhythmia, electrolyte abnormalities, or taking drugs prolonging QT interval. Avoid use in patients with congenital long QT syndrome. Do not exceed 90 mg BID in patients developing QT prolongation.
- **Seizure**: use caution in patients with seizure disorder.
- **Serotonin syndrome**: use caution in patients taking serotonergic drugs.
- **Serotonin syndrome**: use extreme caution in patients taking MAOIs or within 14 days of stopping treatment.
- **Somnolence**: use extreme caution in patients performing activities requiring mental alertness, such as driving or operating machinery.
- **Sphincter of Oddi dysfunction**: use caution in patients with biliary tract disease and acute pancreatitis.

---

### A very simple model to account for the rapid rise of the alpha variant of SARS-CoV-2 in several countries and the world [^fbef93b4]. Virus Research (2021). Medium credibility.

Since its first detection in the UK in September 2020, a highly contagious version of the coronavirus, the alpha or British variant a.k.a. B.1.1.7 SARS-CoV-2 virus lineage, rapidly spread across several countries and became the dominant strain in the outbreak. Here it is shown that a very simple evolutionary model can fit the observed change in frequency of B.1.1.7 for several countries, regions of countries and the whole world with a single parameter, its relative fitness f, which is almost universal f ≈ 1.5. This is consistent with a 50% higher transmissibility than the local wild type and with the fact that the period in which this variant takes over has been in all the studied cases around 22 weeks.

---

### Neurocomputational dynamics of sequence learning [^56c2a1dd]. Neuron (2018). Low credibility.

The brain is often able to learn complex structures of the environment using a very limited amount of evidence, which is crucial for model-based planning and sequential prediction. However, little is known about the neurocomputational mechanisms of deterministic sequential prediction, as prior work has primarily focused on stochastic transition structures. Here we find that human subjects' beliefs about a sequence of states, captured by reaction times, are well explained by a Bayesian pattern-learning model that tracks beliefs about both the current state and the underlying structure of the environment, taking into account prior beliefs about possible patterns in the sequence. Using functional magnetic resonance imaging, we find distinct neural signatures of uncertainty computations on both levels. These results support the hypothesis that structure learning in the brain employs Bayesian inference.

---

### Fast and accurate modeling of transient-state, gradient-spoiled sequences by recurrent neural networks [^de525b7e]. NMR in Biomedicine (2021). Medium credibility.

3.2.1 Network structure specification

The RNN architecture described in Section 2.2 was selected for modeling gradient‐spoiled sequence responses. Specifically, at the n ‐th time step, the inputs of the network were tissue parameters in logarithmic scale θ = (log T 1, log T 2) T and time‐dependent sequence parameters β = (T R (n), T E (n), α (n)) T, and the outputs were (M xy (n), ∂M xy (n)/ ∂ θ) T, that is, both the magnetization and derivatives. At the first time step (n = 0), the input for the initial linear layer was the initial magnetization vector M 0 = (M x (0), M y (0), M z (0)) T. Each layer of the GRUs had 32 hidden states, and the whole network has in total 16,643 trainable parameters.

---

### Dissecting splicing decisions and cell-to-cell variability with designed sequence libraries [^92c9c73e]. Nature Communications (2019). High credibility.

Most human genes are alternatively spliced, allowing for a large expansion of the proteome. The multitude of regulatory inputs to splicing limits the potential to infer general principles from investigating native sequences. Here, we create a rationally designed library of > 32,000 splicing events to dissect the complexity of splicing regulation through systematic sequence alterations. Measuring RNA and protein splice isoforms allows us to investigate both cause and effect of splicing decisions, quantify diverse regulatory inputs and accurately predict (R 2 = 0.73–0.85) isoform ratios from sequence and secondary structure. By profiling individual cells, we measure the cell-to-cell variability of splicing decisions and show that it can be encoded in the DNA and influenced by regulatory inputs, opening the door for a novel, single-cell perspective on splicing regulation.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^dee6cacc]. CDC (2011). Medium credibility.

Community viral load sample size calculations — sample size to detect differences in geometric mean (GM) viral load depends on power and standard deviation (S). The table specifies "α = 0.05 and W = 0.8", with S columns 1 to 1.5; for k = 3 the minimum sample sizes by S are 54, 66, 78, 92, 106, and 122, and "1.2 is the standard deviation of national VL data", so at S = 1.2 the needed sample size is 78.

---

### Taxometer: improving taxonomic classification of metagenomics contigs [^6504675f]. Nature Communications (2024). High credibility.

Data preprocessing

For short-read benchmarking, we used sample-specific assemblies for the seven CAMI2 datasets: Airways (10 samples), Oral (10 samples), Skin (10 samples), Gastrointestinal (10 samples), Urogenital (9 samples), Marine (10 samples), Rhizosphere (21 samples), and the ZymoBIOMICS Microbial Community Standard with 1 sample. For each dataset we aligned the synthetic short paired-end reads from each sample using bwa-mem (v.0.7.15)to the concatenation of per-sample contigs from the particular dataset. BAM files were sorted using samtools (v.1.14). For all datasets, we used only contigs ≥ 2000 base pairs (bp). For long-read benchmarking we used ZymoBIOMICS Gut Microbiome Standard with 1 sample, a human gut microbiome dataset with 4 samples and a dataset from anaerobic digester sludge with 3 samples, both sequenced using Pacific Biosciences HiFi technology. We assembled each sample using metaMDBG (v. b55df39), mapped reads using minimap2 (v.2.24)with the'-ax map-hifi' setting, and from there proceeded as with the short reads.

Abundances and TNFs

Computation of abundances and TNFs was done using the VAMB metagenome binning tool. To determine TNFs, tetramer frequencies of non-ambiguous bases were calculated for each contig, projected into a 103-dimensional orthonormal space and normalized by z-scaling each tetranucleotide across the contigs. To determine the abundances of each sample, we used pycoverm (v.0.6.0). The abundances were first normalized within sample by total number of mapped reads, then across samples to sum to 1. To determine sample-relative abundance, the sum of abundances for a contig was taken before the normalization across samples. The dimensionality of the feature table was then N c × (103+ N s + 1) where N c was the number of contigs, N s was the number of samples.

---

### Model selection: finding the right fit [^6940b900]. The Journal of Foot and Ankle Surgery (2019). Medium credibility.

There are numerous possible goals for building statistical models. Those statistical goals, the associated model types, and each statistical tool involved in model building come with its own assumptions and requirements. In turn, these requirements must be met if we are to ensure that our models produce meaningful, interpretable results. However, beyond these technical details is the intuition, and the additional set of tools and algorithms, used by the statistician, to build the contextually appropriate model: not only must we build an interpretable model, we must build a model that answers the particular question at hand and addresses the particular goal we have in mind. In this column we discuss the methods by which statisticians build models for description, risk factor identification, and prediction.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^d82e5daf]. The Journal of Molecular Diagnostics (2017). Medium credibility.

NGS oncology panel validation — sample size calculations and rule of three: Required validation samples can be calculated from r^n = α with n = ln(α)/ln(r); for 95% confidence (α = 0.05) and at least 95% reliability (r = 0.95) the minimum number of samples is 59. For a 95% confidence level, the rule of three applies: counting 100 cells with no positives allows a claim that < 1% of cells have the event, but to be 95% confident that < 1% of cells have it, 300 cells with no positives are needed; to claim reliability ≥ 95% with 95% confidence requires 3 × 20 or 60 samples.

---

### Detecting and phasing minor single-nucleotide variants from long-read sequencing data [^09c64cc5]. Nature Communications (2021). High credibility.

Methods

Leveraging multiple loci to detect SNVs

For the i th aligned read, we encode its substitution at locus k of the reference genome by the following formula:where r i k is the base (short for nitrogenous base) of the i th aligned read at locus k, t k is the base at locus k of the reference genome and ϵ is an empty element, which is formally defined by. The first locus of the reference genome is 0 throughout this paper unless otherwise stated. The i th read is represented as a set of substitutions and its covering range (Fig. 2 a) and is denoted by b i and e i are the start and end loci of the region covered by the read, respectively, and S i is

The most intuitive way to detect SNVs is to use the substitution rate of each locus. Formally, we denote the encoded substitution at locus k as a random variable X k, and denote probability of the event { X k = x k } as P r (X k = x k), where x k ∈ {4 k, 4 k + 1, 4 k + 2, 4 k + 3}. Substitution rate is defined as the estimated P r (X k = x k), which iswhere {⋅} is a set and ∣⋅∣ is the number of elements in a set. Intuitively, in Eq. (4), the numerator is the number of reads with substitution x k at locus k, and the denominator is the number of reads covering locus k. Due to the high error rate of long-read sequencing data, it is inaccurate to detect minor variants using substitution rate alone (Fig. 1 b). Herein, we leverage the information of multiple loci to increase the detection accuracy. Assuming sequencing errors are independent with each other, real SNVs are likely to be present if there are multiple reads containing the same set of substitutions (Fig. 1 a). The conditional probability of { X k = x k } given other real SNVs of the same genome is therefore much larger than the marginal probability of { X k = x k } if x k is a real SNV, because these real SNVs are positively dependent (Fig. 1 a, c). Formally, the conditional probability of event { X k = x k } given p other substitutions is defined as, which is estimated by

---

### Epidural interventions in the management of chronic spinal pain: American Society of Interventional Pain Physicians (ASIPP) comprehensive evidence-based guidelines [^93148fe8]. Pain Physician (2021). High credibility.

Appendix Table 1 — sources of risk of bias from Cochrane Review collaboration outlines criteria to assess studies for randomization methods, allocation concealment, blinding, attrition, and reporting. For randomization adequacy, "A random (unpredictable) assignment sequence" with methods such as coin toss, rolling a dice, computer-generated random sequence, and preordered sealed envelopes is considered adequate, whereas "Examples of inadequate methods are alternation, birth date, social insurance/security number, date in which they are invited to participate in the study, and hospital registration number". For allocation concealment, adequacy requires "Assignment generated by an independent person not responsible for determining the eligibility of the patients" and that "This person has no information about the persons included in the trial and has no influence on the assignment sequence or on the decision about eligibility of the patient". Blinding criteria include that "Index and control groups are indistinguishable for the patients" and for care providers, and for outcome assessment, "Adequacy of blinding should be assessed for each primary outcome separately" and "This item should be scored 'yes' if the success of blinding was tested among the outcome assessors and it was successful". For attrition, "The number of participants who were included in the study but did not complete the observation period or were not included in the analysis must be described and reasons given", and "If the percentage of withdrawals and drop-outs does not exceed 20% for short-term follow-up and 30% for long-term follow-up and does not lead to substantial bias a 'yes' is scored (N.B. these percentages are arbitrary, not supported by literature)". Analysis quality is supported if "All randomized patients are reported/analyzed in the group they were allocated to", and selective reporting is minimized when "All the results from all pre-specified outcomes have been adequately reported in the published report of the trial" and "This information is either obtained by comparing the protocol and the report, or in the absence of the protocol, assessing that the published report includes enough information to make this judgment".

---

### Data storage using peptide sequences [^9400a285]. Nature Communications (2021). High credibility.

When this scheme was used on dataset B, a 511 × 16 block of symbols was constructed (Table S5), which comprises 511 × 16 × 3 = 24528 bits. The three-symbol sets A i,1 A i,2 A i,3 (i = 1, 2,…, 511) were used for addressing, with Symbols S 1 to S 3 having the values of 000, 001, 002,…, 775, 776. The three bits of Symbol S 4 were the three order-checking bits used to protect the order of Symbols S 1 and S 2, the order of Symbols S 2 and S 3, and the order of Symbols S 15 and S 16, respectively. Then there were 511 × 12 × 3 = 18396 bit positions in Symbols S 5 to S 16 of the block to store the information and parity bits for the RS codes. Due to the different protection requirements for the partial sequences { S 5 S 6 S 7 S 8 S 9 S 10 } and { S 11 S 12 S 13 S 14 S 15 S 16 }, two (511, 409) RS codes were used for partial sequences { S 5 S 6 S 7 } (RS1) and { S 8 S 9 S 10 } (RS2), another two (511, 357) RS codes were used for partial sequences { S 11 S 12 S 13 } (RS3) and { S 14 S 15 S 16 } (RS4). Each symbol in RS code comprised 9 bits. The (511, 409) RS and (511, 357) RS codes could correct up to 51 and 77 9-bit symbol errors, respectively (Table S5). Moreover, the numbers of total information bits and total parity bits of all four RS codes were given by (409 + 357) × 2 × 9 = 13788 and (102 + 154) × 2 × 9 = 4608, respectively. The maximum overall code rate R of the block is given by 13788/(511 × 16 × 3) = 0.562. As dataset B only had 13752 bits, zeros were appended to the end such that the block could be fully filled. This code rate is comparable to that of DNA (varied from 0.17 for repetition encodingto 0.785 for fountain encoding, assuming a maximum capacity of 2 bits per nucleotide). Improvement of code rate is possible if longer peptides could be used, if the peptide design could be improved to reduce error, and if the coding scheme could be more focused on the error-prone amino acid positions to reduce unnecessary redundancy.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^729957be]. CDC (2011). Medium credibility.

Community viral load (VL) data reporting example — Figure 4 labels an example scenario "When VL LLD† = 50 copies/mL (standard undetectable)" and denotes the example cohort as "Community VL (N = 1000)", illustrating the "Examples for Data Reporting (bottom up)" approach.

---

### Antisymmetric linear magnetoresistance and the planar hall effect [^6352851f]. Nature Communications (2020). High credibility.

The final term, is the second term in Eq. 1. It provides a general mechanism for obtaining both antisymmetric linear MR and the planar Hall effect, which originates from the same anomalous electron velocity that underlies the ferromagnetism-induced anomalous Hall effect (the third term in Eq. 3). Closely related conclusions have been reached by considering the Boltzmann equation in ref.and transport equation in ref.

We further note that the assumption of a single τ and n in Eq. 3 breaks down in a ferromagnet, which leads to an additional mechanism for inducing antisymmetric linear MR. Microscopically, electrons of spin parallel (up) and antiparallel (down) to M have different scattering times τ up and τ dn, with the carrier densities n up and n dn tuned linearly by the applied field through a Zeeman energy shift at the Fermi surface. To first order in H, this brings a correction to nτ inwhere nτ is replaced bywithAfter projection along M, the first term in Eq. 3 gives a field-dependent MR that is negatively sloped and proportional to (M · H) E (the first term of Eq. 1). This linear form was observed in systems of saturated moments at high field, and over a very narrow field range of 1–2 Oe in the hysteresis region of Ni–Fe magnets. Since the carrier density difference is linear in H, this argument would introduce a second order in H correction to the last term (M × E) × H in Eq. 3 and hence is neglected here. In sum, Eq. 3 leads to the full general form given in Eq. 1.

---

### Roflumilast (Zoryve) [^8f91de64]. FDA (2024). Medium credibility.

The dosage of roflumilast TOP for treatment of seborrheic dermatitis in both children (in patients ≥ 9 years) is 1 thin layer TOP daily (0.3% foam)

---

### Spreading and scaling up innovation and improvement [^59491eef]. BMJ (2019). Excellent credibility.

Implementation science: spread and scale-up as structured improvement

Implementation science, defined as "the scientific study of methods to promote the systematic uptake of research findings and other evidence based practices into routine practice" (page 2), developed from the evidence based medicine movement in Europe and North America. It is perhaps best known for the sequential, structured (and somewhat top-down) method of spreading focused improvement techniques.

The first phase of this approach (after initial set-up and orientation) is the development of a clearly defined intervention, the components of which are optimised to reflect the evidence base (especially relating to how to change individual behaviour) (fig 1). There is then a small scale trial of this intervention in one or a few selected settings, followed by a systematic effort to replicate it in other settings, partly by identifying and dealing with barriers (which get in the way of the implementation effort) and facilitators (which potentially support it).

Fig 1
Rapid cycle test of change model of spread used in implementation science. Drawing on insights and a previous diagram in a review by Barker

Patient input can be harnessed very productively in this effort, though careful attention needs to be paid to power dynamics, the kinds of data that are collected, and how and by whom those data are analysed.

Although the sequence depicted in figure 1 is often promoted as the key to quality improvement, one systematic review showed that nearly half of all successful scale-up initiatives had not followed it.

Implementation science approaches tend to draw heavily on quality improvement methodology. Barker and colleagues describe this methodology as an "engine" that uses rapid cycle change to drive spread of an innovation, with some potential to adapt to different contexts.

In recent years, implementation science has matured as a field in a way that has paralleled developments in the Medical Research Council's guidance for developing and testing complex interventions. Both have shifted from a highly structured and narrowly experimental approach based on mechanical logic (which emphasised standardisation and replicability) to a more adaptive approach that recognises the need to think flexibly, understand and respond to local context, use qualitative methods to explore processes and mechanisms, and adapt the intervention to achieve best fit with different settings. This shift resonates with the complexity science approach described in the next section.

---

### A general derivation and quantification of the third law of thermodynamics [^314e2820]. Nature Communications (2017). Medium credibility.

Discussion

We hope the present work puts the third law on a footing more in line with those of the other laws of thermodynamics. These have already been long established, although they've recently been reformulated within the context of other resource theories. Namely, as described in ref. the first law (energy conservation), and unitarity (or microscopic reversibility) describe the class of operations which are allowed within thermodynamics. The zeroth law, is the fact that the only state which one can add to the theory without making it trivial, are the equivalence class of thermal states at temperature T. This allows the temperature to emerge naturally. The second law(s), tells us which state transformations are allowed under the class of operations. For macroscopic systems with short-range interactions, there is only one function, the entropy, which tells you whether you can go from one state to another, but in general there are many constraints. The third law quantifies how long it takes to cool a system. We propose to generalize it further: While the second laws tell us which thermodynamical transitions are possible, generalized third laws quantify the time of these transitions. In this context, it would be interesting to explore the time and resource costs of other thermodynamical transitions. It would also be interesting to explore the third law in more restricted physical settings, as well as to other resource theory frameworks, in particular, those discussed in ref.

It is worth noting that scaling we find, for example, the inverse polynomial of equation (1), is more benign than what one might have feared, and does not exclude obtaining lower temperatures with a modest increase of resources. However, it is also stronger than that envisioned when the third law was original formulated. Consider for example, the cooling protocol of Fig. 1 proposed by Nernst. It is unphysical, since it requires an infinite number of heat baths, each one at a lower and lower temperature, and with the final heat baths at close to zero temperature. However, it allows the temperature of the system to decrease exponentially in the number of steps, something which we are able to rule out when one doesn't have colder and colder reservoirs.

---

### Evaluation of the evenness score in next-generation sequencing [^1f5f1936]. Journal of Human Genetics (2016). Low credibility.

The evenness score (E) in next-generation sequencing (NGS) quantifies the homogeneity in coverage of the NGS targets. Here I clarify the mathematical description of E, which is 1 minus the integral from 0 to 1 over the cumulative distribution function F(x) of the normalized coverage x, where normalization means division by the mean, and derive a computationally more efficient formula; that is, 1 minus the integral from 0 to 1 over the probability density distribution f(x) times 1-x. An analogous formula for empirical coverage data is provided as well as fast R command line scripts. This new formula allows for a general comparison of E with the coefficient of variation (= standard deviation σ of normalized data) which is the conventional measure of the relative width of a distribution. For symmetrical distributions, including the Gaussian, E can be predicted closely as 1-σ(2)/2⩾E⩾1-σ/2 with σ ≤ 1 owing to normalization and symmetry. In case of the log-normal distribution as a typical representative of positively skewed biological data, the analysis yields E≈exp(-σ*/2) with σ*(2) = ln(σ(2)+1) up to large σ (≤ 3), and E≈1-F(exp(-1)) for very large σ (⩾2.5). In the latter kind of rather uneven coverage, E can provide direct information on the fraction of well-covered targets that is not immediately delivered by the normalized σ. Otherwise, E does not appear to have major advantages over σ or over a simple score exp(-σ) based on it. Actually, exp(-σ) exploits a much larger part of its range for the evaluation of realistic NGS outputs.

---

### A general derivation and quantification of the third law of thermodynamics [^6246fb55]. Nature Communications (2017). Medium credibility.

A number of recent works analyse a process closely related to cooling to absolute zero: erasing information or generating pure states. In refs, it is shown that, regardless of the amount of time invested, these processes are strictly impossible if the reservoir is finite-dimensional. However, strict unattainability in the sense of Nernst is not really a physically meaningful statement. Rather, one wants to obtain a finite bound to how close one can get to the desired state with a finite amount of resources or within a given time. Some interesting steps in this direction are taken in ref. where they obtain a bound in terms of the dimension of the reservoir, but not one that can be translated into time. It also requires the dimension of the reservoir to be finite, an assumption that is not needed to derive our unattainability result here, and something which rarely holds in real setups. In fact, we shall see that the physical mechanism which enforces our third law is not dimension, but the profile of the density of states of the reservoir. On the other hand, argues that for a qubit, one can produce a pure state to arbitrary accuracy as the time invested increases. This, however, requires that the work injected in the bath fluctuates around its mean value by an unbounded amount (this is also necessary in ref.). A fact that becomes more relevant when cooling systems much larger than a qubit.

In the present article, we bound the achievable temperature by resources such as the volume of the reservoir and the largest value by which the work can fluctuate. This in itself can be said to constitute a version of the third law. However, we also argue that, in any process implemented in finite time, these two resources must remain finite too. When the scaling of these resources with time has a standard form (explained below), and the heat bath consists of radiation, our third law provides the following relation between the lowest achievable temperatureand time t

We believe this to be the first derivation of a quantitative lower bound on the temperature in terms of cooling resources, confirming the general validity of the models and conjecture in ref.(although we do not require the system to be continually thermal, we are able to get a bound which is more general than the differential equation postulated there).

---

### Evaluating molecular biomarkers for the early detection of lung cancer: when is a biomarker ready for clinical use? An official American Thoracic Society policy statement [^1c6ced37]. American Journal of Respiratory and Critical Care Medicine (2017). Medium credibility.

Accuracy guidance for biomarker clinical validation — formulas specify minimal accuracy thresholds tied to action after positive versus negative results. The committee "agreed that it would be helpful to provide guidance about the minimal accuracy, as assessed in the clinical validation phase, that could lead to a positive clinical impact". For tests where "a positive biomarker result leads to an action where a negative biomarker result is associated with standard of care for the population", the threshold is "sensitivity/(1 − specificity) ≥ [(1 − prevalence)/prevalence] × harm/benefit", where harm/benefit is "the ratio of the net harm of a falsely positive test result to the net benefit of a true-positive test result". For tests where "a negative test leads to an action other than standard of care for the population", the threshold is "specificity/(1 − sensitivity) ≥ [prevalence/(1 − prevalence)] × harm/benefit", where harm/benefit is "the ratio of the net harm of a falsely negative test result to the net benefit of a true-negative test result". "Sensitivity/(1 − specificity) is known as the positive likelihood ratio, and specificity/(1 − sensitivity) is 1 divided by the negative likelihood ratio", and "Prevalence refers to the percentage of cases in the intended use population". The harm/benefit term "can be articulated in one of two ways: 1/N, where in the first scenario N is the maximum number of control subjects testing positive that is tolerated to benefit one case subject testing positive".

---

### Probing the physical limits of reliable DNA data retrieval [^e55ae166]. Nature Communications (2020). High credibility.

Synthetic DNA is gaining momentum as a potential storage medium for archival data storage. In this process, digital information is translated into sequences of nucleotides and the resulting synthetic DNA strands are then stored for later retrieval. Here, we demonstrate reliable file recovery with PCR-based random access when as few as ten copies per sequence are stored, on average. This results in density of about 17 exabytes/gram, nearly two orders of magnitude greater than prior work has shown. We successfully retrieve the same data in a complex pool of over 10 10 unique sequences per microliter with no evidence that we have begun to approach complexity limits. Finally, we also investigate the effects of file size and sequencing coverage on successful file retrieval and look for systematic DNA strand drop out. These findings substantiate the robustness and high data density of the process examined here.

---

### Considerations for severe acute respiratory syndrome coronavirus 2 genomic surveillance: a joint consensus recommendation of the Association for Molecular Pathology and association of public health laboratories [^4c2f9915]. The Journal of Molecular Diagnostics (2025). High credibility.

Wet laboratory pre-analytical elements for SARS-CoV-2 sequencing — the workflow begins with identification of clinical samples positive for SARS-CoV-2 using antigen testing, nucleic acid amplification tests, or viral culture; most clinical samples selected for sequencing were already manipulated before starting the sequencing workflow, and several pre-analytical elements impact sequencing data quality, including the sample used and viral concentration (density) in the original sample.

---

### Chromosome-scale and haplotype-resolved genome assembly of a tetraploid potato cultivar [^73462c60]. Nature Genetics (2022). High credibility.

Linkage-based grouping of contigs

The read count (denoted by r) at each coverage marker of size W (in bp) for each of the selected 717 pollen genomes (Supplementary Fig. 2), which were collected using bedtools (v.2.29.0), was firstly normalized using n r = r × (10 4 / W) × (10 6 / N), where N was the total number of reads aligned to the assembly of 6,366 contigs. Meanwhile, the average read count, m r, for all coverage markers with more than 7 × (N /10 6) reads was calculated for each pollen genome. The coverage (genotype) at a marker for each pollen was set to n r / m r (which would be rounded to 0, 1 or 2). In general, coverages across 717 pollen read sets at the same window marker were used to build up a PAP: X = x 1 × 2… x 717, where x i was in {0, 1}, {0, 1, 2}, {1, 2} or {2}, depending on whether the marker type was haplotig, diplotig, triplotig or tetraplotig. The correlation between two PAPs X and Y was calculated as cor XY = sum[(x i - m x) × (y i - m y)] / [sqrt(sum(x i - m x) 2) × sqrt(sum(y i - m y) 2)], where m x and m y were the respective average values. Initially, any pair of haplotigs/vertices (with sizes ≥ 100 kb) was connected by an edge, if the highest correlation value between the PAPs of the markers at two ends of the two haplotigs was > 0.55 (Supplementary Fig. 3). This graph-based clusteringled to 48 groups representing the 48 haplotypes (Extended Data Fig. 2). If the lowest correlation value between any pair of the markers of any two groups was less than –0.25, they could be determined as homologous linkage groups (same chromosome, different haplotypes). With this, the 48 groups were clustered into 12 chromosomes, each with four different haplotypes.

---

### Considerations for severe acute respiratory syndrome coronavirus 2 genomic surveillance: a joint consensus recommendation of the Association for Molecular Pathology and association of public health laboratories [^2f4e8c55]. The Journal of Molecular Diagnostics (2025). High credibility.

SARS-CoV-2 genomic surveillance — sequencing platform considerations note that using the short-read sequencing method, the user can perform either single- or paired-end sequencing depending on the overall objective of sequencing.

---

### Tracing household transmission of SARS-CoV-2 in New Zealand using genomics [^7c4c32fb]. Npj Viruses (2024). Medium credibility.

Genome sequence analysis

High-quality genomes were first designated lineages using Pangolin v4.0.6. Sublineages were grouped by parent lineage due to sequence ambiguities in some genomes. Household cohort viral genomes were first aligned together with SARS-CoV-2 genomes sampled during 2022 from New Zealand (n = 2029) and the rest of the world (n = 1967) selected at random from GISAID, as well as 408 global genomes sampled during 2020–2021 (see Supplementary Table 1 for all genome accession numbers). Genomes were aligned using NextAlign, using Wuhan-Hu-1 (NC_045512.2) as a reference. A maximum likelihood time-calibrated phylogenetic tree was estimated using IQ-TREE2, using the Hasegawa-Kishino-Yano (HKY + Γ) nucleotide substitution model(the best-fit model was determined by ModelFinder), and branch support assessment using the ultrafast bootstrap method.

SNP analysis for households was performed by first aligning genomes for a given household using NextAlign and summing the number of nucleotide differences. Households that showed two separate transmission events, or two different subvariants that circulated concurrently were aligned separately.

---

### A summary of the methods that the national clinical guideline centre uses to produce clinical guidelines for the national institute for health and clinical excellence [^17347588]. Annals of Internal Medicine (2011). Medium credibility.

NICE study search and retrieval workflow — Search results undergo a First sift: titles (done by information scientist or systematic reviewer) to exclude studies outside the topic; a Second sift: abstracts (done by systematic reviewer) to exclude studies that are not relevant to the review questions; and Assessment of full articles (done by systematic reviewer) to exclude studies on limits set by the GDG (for example, study design or outcomes), after which studies included proceed for data extraction; because of potential bias or error, a second reviewer performs sampling checks, and usually several thousand titles are sifted at the first stage.

---

### Imlunestrant [^7b5c5001]. FDA. Low credibility.

Labeled indications for Imlunestrant include:

- Treatment of breast cancer in female adults with ESR1 mutation (advanced or metastatic, ER-positive, HER2-negative, progression after ≥ 1 line of endocrine therapy)

---

### Thermodynamic control of-1 programmed ribosomal frameshifting [^c1d7e574]. Nature Communications (2019). High credibility.

mRNA contexts containing a 'slippery' sequence and a downstream secondary structure element stall the progression of the ribosome along the mRNA and induce its movement into the -1 reading frame. In this study we build a thermodynamic model based on Bayesian statistics to explain how -1 programmed ribosome frameshifting can work. As training sets for the model, we measured frameshifting efficiencies on 64 dnaX mRNA sequence variants in vitro and also used 21 published in vivo efficiencies. With the obtained free-energy difference between mRNA-tRNA base pairs in the 0 and -1 frames, the frameshifting efficiency of a given sequence can be reproduced and predicted from the tRNA-mRNA base pairing in the two frames. Our results further explain how modifications in the tRNA anticodon modulate frameshifting and show how the ribosome tunes the strength of the base-pair interactions.

---

### Coherent two-dimensional electronic mass spectrometry [^78041777]. Nature Communications (2018). Medium credibility.

In conventional pump–probe experiments the delay Δ between UV pump and visible probe pulse is scanned from −5 to −500 fs in 46 steps (100 fs), from −500 to 750 fs in 126 steps (10 fs), from 750 to 5 ps in 43 steps (101.2 fs) and additionally from 5 to 100 ps in 41 steps with exponentially increasing step size. Here, Δ > 0 corresponds to the UV pulse acting as pump and the visible pulse as probe pulse. For each time delay the signal is integrated over 2000 laser shots and averaged over 40 complete scans. Since pump and probe beam on their own already generate ions, we use a chopping sequence at each delay position in order to measure the pump-only and the probe-only mass spectrum and thus to extract the pump–probe signal by subtracting both from the mass spectrum when both pulses are present.

In all experiments employing a visible four-pulse sequence, a phase factor ϕ = (1 − γ 0) ω 0 t ′ is added to the probe pulses two, three, and four in order to shift their carrier phases with respect to the envelope depending on the respective delay t ′ to the first probe pulse. This moves coherent oscillations from optical frequencies (laboratory frame, γ 0 = 1) to a region close to the origin in frequency space (rotating frame, γ 0 = 0). Here we use γ 0 = 0 and set ω 0 at the center of the probe-pulse spectrum, ω 0 = 3.49 rad fs −1, to perform the experiments in the rotating frame. Despite this, we plot all 2D spectra with laboratory-frame frequency axes by adding the rotating-frame center frequency to the measured frequencies.

---

### AGA clinical practice guideline on management of gastroparesis [^864a37c2]. Gastroenterology (2025). High credibility.

Gastroparesis — Botulinum toxin injection (BTI) cross-over randomized controlled trial describes 4-quadrant BTI of 1 mL (total 100 units diluted into 4 mL saline) at first endoscopy with saline at repeat endoscopy 1 month later, with the reverse sequence also used and no washout period; exclusions included esophagitis, gastric atrophy or erosive gastroduodenal lesions, lesions on small bowel X-ray, major abdominal surgery, underlying psychiatric illness, and use of NSAIDs, steroids or drugs affecting gastric acid secretion; gastric emptying was Solid 121 min (SE 14) and Liquid 107 min (SE 11); symptom severity used the Gastroparesis Cardinal Symptom Index (GCSI), nine individual symptoms graded 0–5 then summed (range 0–45), with n: 12 vs 11 and mean change 21.3 (SE 2.8) to 15.7 (SE 3.2) vs 32.1 (SE 1.7) to 26.3 (SE 2.2).

---

### Simultaneous and stoichiometric purification of hundreds of oligonucleotides [^33a1be9e]. Nature Communications (2018). Medium credibility.

Purification of oligonucleotides has traditionally relied on mobility-based separation methods. However, these are imperfect, biased, and difficult to scale high multiplex. Here, we present a method for simultaneous purification of many oligonucleotides that also normalizes concentrations. The method uses a rationally designed randomer capture probe to enrich for oligos with perfect 5' sequences, based on the observation that synthesis errors are correlated: product molecules with one or more deletions in one region are also more likely to have deletions in other regions. Next-generation sequencing analysis of 64-plex 70nt purification products show a median 78% purity, a significant improvement over polyacrylamide gel electrophoresis and high pressure liquid chromatography (60% median purity). Additionally, 89% of the oligo products are within a factor of 2 of the median concentration.