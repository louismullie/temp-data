# Medical Question & Answer

**Sample ID**: 9c064be1-c580-fa23-66fe-f4ba414dc217
**Dataset Index**: 201778

---

## Question

13.3.6 Given P(3,4) is a point inside the circle x² + y² = 64, and two moving points A and B on the circumference of the circle satisfy ∠A P B − π/2. A rectangle A P B Q is constructed with A P and B P as adjacent sides. Find the equation of the trajectory of point Q.

---

## Answer

> Let's see… What do we have here? The user is asking for the equation of the trajectory of point Q, given that P(3,4) is inside the circle x² + y² = 64, A and B move on the circumference such that ∠APB = π/2, and rectangle APBQ is constructed with AP and BP as adjacent sides. Let's break this down step-by-step. First, I need to think about the geometric constraints and what they imply for points A and B. Then, I should verify the rectangle construction and how Q relates to A, B, and P. Next, I will translate the geometric conditions into vector and coordinate equations. After that, I need to eliminate parameters to find the locus of Q. Finally, I will simplify and interpret the resulting equation, making sure it makes geometric sense and checking for any special cases or errors along the way.

> Let me first confirm the setup and constraints. The circle has center O at the origin and radius 8, and P(3,4) lies inside it since 3² + 4² = 25 < 64. Points A and B are on the circumference, and the angle condition ∠APB = π/2 means that triangle APB is right-angled at P, so AB must be a diameter of the circumcircle of triangle APB; wait, let me verify that, because I initially thought AB might be a diameter of the original circle, but that would only be true if P lay on the original circle, which it does not. Hold on, I should verify the Thales' theorem logic: if ∠APB = 90°, then P lies on the circle with diameter AB, so AB is a diameter of the circumcircle of triangle APB, not necessarily of the original circle x² + y² = 64; I need to keep that distinction in mind as I proceed.

> Next, I should review the rectangle construction. Rectangle APBQ implies that vectors AP and BP are adjacent sides, so the diagonals bisect each other and PQ should share the same midpoint as AB; moreover, the vector from P to Q should equal the vector from A to B, because in a rectangle the opposite sides are equal and parallel. Let me write that explicitly: if M is the midpoint of AB, then M is also the midpoint of PQ, and vector PQ equals vector AB, which gives Q = P + AB; I should double-check that this respects the orientation and indeed yields a rectangle, and yes, with AP and BP as adjacent sides, the fourth vertex Q is determined by translating P by the vector AB.

> I will now translate this into coordinates. Let A = (x₁, y₁) and B = (x₂, y₂) be on the original circle, so x₁² + y₁² = 64 and x₂² + y₂² = 64. The midpoint of AB is ((x₁ + x₂)/2, (y₁ + y₂)/2), and since M is also the midpoint of PQ, we have (P + Q)/2 = (A + B)/2, which simplifies to Q = A + B − P. Substituting P = (3,4), this gives Q = (x₁ + x₂ − 3, y₁ + y₂ − 4). I should confirm that this aligns with the vector statement Q = P + AB; yes, AB = (x₂ − x₁, y₂ − y₁), so P + AB = (3 + x₂ − x₁, 4 + y₂ − y₁), which is not the same as A + B − P; wait, I made an error here, let me reconsider. The correct vector relation is that PQ equals AB, so Q − P = B − A, hence Q = P + B − A, not P + AB; I initially wrote P + AB, but that would be P plus the vector AB, which is not the endpoint Q. Hold on, I should verify the rectangle property again: in rectangle APBQ, the diagonal vector from P to Q equals the diagonal vector from A to B, so Q − P = B − A, which gives Q = P + B − A; that is the correct expression I should use.

> Now, I need to incorporate the angle condition. Since ∠APB = π/2, the vectors PA and PB are perpendicular, so their dot product is zero. PA = (x₁ − 3, y₁ − 4) and PB = (x₂ − 3, y₂ − 4), so (x₁ − 3)(x₂ − 3) + (y₁ − 4)(y₂ − 4) = 0. Expanding this, I get x₁x₂ − 3x₁ − 3x₂ + 9 + y₁y₂ − 4y₁ − 4y₂ + 16 = 0, which simplifies to x₁x₂ + y₁y₂ − 3(x₁ + x₂) − 4(y₁ + y₂) + 25 = 0. I should keep this equation handy because it links x₁, y₁, x₂, y₂ and will help me eliminate variables when I express the locus of Q.

> Let me express the coordinates of Q in terms of sums to simplify. Set Sx = x₁ + x₂ and Sy = y₁ + y₂; then Q = (Sx − 3, Sy − 4). I need to find a relationship involving Sx and Sy that uses the angle condition and the circle constraints. From the angle condition, I have x₁x₂ + y₁y₂ − 3Sx − 4Sy + 25 = 0, so x₁x₂ + y₁y₂ = 3Sx + 4Sy − 25. Next, I should consider the circle equations: x₁² + y₁² = 64 and x₂² + y₂² = 64. Adding these gives x₁² + x₂² + y₁² + y₂² = 128, and I can rewrite that as (x₁ + x₂)² − 2x₁x₂ + (y₁ + y₂)² − 2y₁y₂ = 128, which is Sx² + Sy² − 2(x₁x₂ + y₁y₂) = 128. Substituting the expression for x₁x₂ + y₁y₂ from above, I get Sx² + Sy² − 2(3Sx + 4Sy − 25) = 128, which simplifies to Sx² + Sy² − 6Sx − 8Sy + 50 = 128, and then to Sx² + Sy² − 6Sx − 8Sy − 78 = 0. I should double-check the arithmetic: 2 times 25 is 50, and 128 minus 50 is 78, so yes, that is correct.

> Now I substitute Q's coordinates in terms of Sx and Sy. Since Qx = Sx − 3 and Qy = Sy − 4, I have Sx = Qx + 3 and Sy = Qy + 4. Plugging these into the equation Sx² + Sy² − 6Sx − 8Sy − 78 = 0 yields (Qx + 3)² + (Qy + 4)² − 6(Qx + 3) − 8(Qy + 4) − 78 = 0. Expanding and simplifying, I get Qx² + 6Qx + 9 + Qy² + 8Qy + 16 − 6Qx − 18 − 8Qy − 32 − 78 = 0, which reduces to Qx² + Qy² − 103 = 0, or Qx² + Qy² = 103. Wait, let me verify the constant: 9 + 16 − 18 − 32 − 78 equals 25 − 18 − 32 − 78 = 7 − 32 − 78 = −25 − 78 = −103, so the equation is indeed Qx² + Qy² = 103. That means the trajectory of Q is a circle centered at the origin with radius √103.

> Hold on, let's not jump to conclusions; I should confirm that this locus is actually attainable and that the construction is consistent. The derivation shows that for any pair A, B on the original circle satisfying ∠APB = π/2, the corresponding Q lies on x² + y² = 103, and conversely, any point on that circle can be realized by choosing appropriate A and B; geometrically, as A and B vary on the original circle maintaining the right angle at P, the vector Q = P + B − A traces a circle whose radius exceeds the original radius because the construction effectively adds a vector of length up to 16 (the diameter) to P, and the algebra shows the exact radius is √103 ≈ 10.15, which is greater than 8, consistent with the locus lying outside the original circle. I should also confirm that the center remains at the origin; yes, the final equation is centered at (0,0), so the trajectory is a concentric circle.

> Final answer: The trajectory of point Q is the circle x² + y² = 103.

---

The trajectory of point (Q) is a circle centered at the origin with radius √103. This is derived by noting that (∠APB = π/2) implies (AB) is a diameter of the circumcircle of (△APB), and that the rectangle condition gives (Q = A + B - P). Using the circle equation and the right-angle condition, we eliminate (A) and (B) to find (x_Q² + y_Q² = 103).

---

## Geometric setup and conditions

- **Circle**: (x² + y² = 64), center (O(0, 0)), radius 8.
- **Point P**: ((3, 4)), inside the circle.
- **Points A and B**: on the circumference, with (∠APB = [π/2]).
- **Rectangle APBQ**: (AP) and (BP) are adjacent sides, so (Q) is the fourth vertex.

---

## Key geometric insights

- **Right angle condition**: (∠APB = π/2) implies (AB) is a diameter of the circumcircle of (△APB).
- **Rectangle property**: Diagonals bisect each other, so the midpoint of (AB) coincides with the midpoint of (PQ).
- **Vector relation**: (Q = A + B - P), since (PQ = AB).

---

## Algebraic derivation

Let (A = (x₁, y₁)) and (B = (x₂, y₂)). The right-angle condition gives:

(x₁ − 3)(x₂ − 3) + (y₁ − 4)(y₂ − 4) = 0

Expanding and rearranging:

x₁x₂ + y₁y₂ − 3(x₁ + x₂) − 4(y₁ + y₂) + 25 = 0

From the rectangle property, (Q = (x_Q, y_Q) = (x₁ + x₂–3, y₁ + y₂–4)), so:

x₁ + x₂ = x_Q + 3, y₁ + y₂ = y_Q + 4

Substitute into the expanded right-angle equation:

x₁x₂ + y₁y₂ − 3(x_Q + 3) − 4(y_Q + 4) + 25 = 0

x₁x₂ + y₁y₂ = 3x_Q + 4y_Q + 25 − 9 − 16 = 3x_Q + 4y_Q

Next, use the circle equations (x₁² + y₁² = 64) and (x₂² + y₂² = 64). Add them:

x₁² + x₂² + y₁² + y₂² = 128

Rewrite using sums and products:

(x₁ + x₂)² − 2x₁x₂ + (y₁ + y₂)² − 2y₁y₂ = 128

Substitute (x₁ + x₂ = x_Q + 3), (y₁ + y₂ = y_Q + 4), and (x₁ x₂ + y₁ y₂ = 3x_Q + 4y_Q):

(x_Q + 3)² − 2(3x_Q + 4y_Q) + (y_Q + 4)² = 128

Expand and simplify:

x_Q² + 6x_Q + 9 + y_Q² + 8y_Q + 16 − 6x_Q − 8y_Q = 128

x_Q² + y_Q² + 25 = 128

x_Q² + y_Q² = 103

---

## Final result

The trajectory of (Q) is the circle:

x² + y² = 103

This is a circle centered at the origin with radius √103.

---

## References

### The great oxygenation event as a consequence of ecological dynamics modulated by planetary change [^6a693f56]. Nature Communications (2021). High credibility.

Methods

We elaborate our mathematical model of ecological dynamics, its analytical solutions, the stability properties of its fixed points, and important considerations behind the GOE in Supplementary Note 1. Here, we provide an abbreviated account of our analysis and key findings.

Ecological dynamics and fixed points

Equations (1) specify the ecological dynamics of APB and cyanobacteria. They set the foundation for our understanding of the GOE. To make progress analytically, we make two simplifying assumptions. First, we assume that, at any given time, Equations (1) are approximately in steady state. This is because f 1, f 2, c, a, and b change very slowly relative to the typical reproductive lifetimes of the bacteria. Second, we assume that u 1 and u 2 are both small. Equations (1) becomeEquations (5) admit a solution for which the equilibrium abundance of cyanobacteria is zero:Equations (5) admit another solution for which the equilibrium abundance of APB is zero:Equations (5) also admit a solution for which the equilibrium abundances of cyanobacteria and APB are both nonzero:Here, we have set

Dynamical stability

Equilibrium E 1, given by Eqs. (6), is stable if p < 0 and unstable if p > 0. From Eq. (9), setting p < 0 and rearranging, we obtain Condition (2). Equilibrium E 2, given by Eqs. (7), is stable if q < 0 and unstable if q > 0. From Eq. (10), setting q < 0 and rearranging, we obtain Condition (3). Equilibrium, given by Eqs. (8), is stable if r > 0 and unstable if r < 0. From Eq. (11), setting r > 0 and rearranging, we obtain Condition (4).

---

### Multidimensional cerebellar computations for flexible kinematic control of movements [^5eee6bc7]. Nature Communications (2023). High credibility.

Analysis of manifolds

Given a manifold of MFs or PCs for a given kinematic variable, we computed the manifold size and rotation speed in 2D (Figs. 4–5). We defined the manifold size as the area enclosed within the 2D circular trajectory, computed by numerically integrating the areas of triangles defined by two neighboring data points and the origin (0,0). For the rotation speed, we first computed the phase of rotation θ at each data point (x, y) by θ = tan −1 (∆ y /∆ x) where (∆ x, ∆ y) = (x−x 0, y − y 0) and (x 0, y 0) is a reference point defined by [(maximum of x coordinate data)/2, 0]. Then, we estimated the time T 3/4 from the trial beginning t = −250 ms, where θ ≈ −180° (by definition), to the point θ = 90° (rotation of 3/4 cycles), finally finding the average rotation speed by 270°/ T 3/4. We summarized how the manifold size and rotation speed vary with the kinematic parameters by computing the normalized slope angle in the manifold size and rotation speed plane (Figs. 4p, q and 5c). To do so, we first normalized the manifold size and rotation speed data for all cases by the standard deviations of the control case, which was the correlated variation in Fig. 4p, q and post-no-CS case in Fig. 5c. The slope angle was computed in each case in the normalized coordinates. We also performed the comparison/alignment analysis of multiple manifolds using the canonical correlation analysis. See Supplementary Methods for details.

---

### Topological phase locking in stochastic oscillators [^bed95573]. Nature Communications (2025). High credibility.

A number of phase portraits for different points on the phase-locking diagram is shown in the insets of Fig. 5, and more examples are shown in Supplementary Figs. 2 and 3. In particular, a number of phase portraits displaying TPL are included. Just like the (2, 3) ∞ trajectory in Fig. 4, periodic trajectories inside these running bands form torus knots. Such knots are defined by a tuple (q, p) where q, p are coprime to each other and characterize the winding along the two axis of the torus. For all (m, n) ∞ topologies that we have observed, m and n were indeed coprime, suggesting that the TPL states correspond to various torus knots. Torus-knot trajectories have been found in the past in soliton equations, for instance in the non-linear Schrödinger equation. Our system provides a new example of a non-linear dynamical system that can give rise to such mathematical structures.

The phase-locking diagram in Fig. 5 bears some resemblance to the well-known Arnold tongues describing phase locking in a number of other systems. However, the resemblance is only superficial: in fact, while in the case of Arnold tongues the key parameter controlling phase-locking ratios is the frequency asymmetry and the coupling merely acts to broaden the phase-locking regions, here the opposite is true. The main parameter controlling the phase-locking ratios is the coupling h, and the very limited amount of broadening of the phase-locking regions originates from the driving asymmetry E *2 / E *1. Besides this clear operational difference, the context here is entirely different, as we are still dealing with noise-activated dynamics — although the coexistence of a running band and a stable fixed point leads to a coexistence of dissipative and effectively conservative/deterministic dynamics.

---

### Long-term memory induced correction to arrhenius law [^2938019f]. Nature Communications (2024). High credibility.

METHODS

Numerical methods

To generate stochastic trajectories x (t n) satisfying the GLE (2) at sampling times t n = n × d t, a modified version of the circulant matrix algorithmdescribed in ref.was used. This algorithm enables one to generate trajectories of N points with a computational complexity. The value of the maximal time of the simulations and the time steps were systematically varied to check that the measured properties of the FPT do not depend on such values. Next, to determine the FPT, we have used a theory relying on the analysis of trajectories after the FPT, assumed to display Gaussian statistics with the same covariance function as the original stochastic process. The control of such approximations can be found in SI (Sections D for numerical check and E for a perturbative analysis for weakly non-Markovian processes). To obtain numerical predictions for the mean FPT, equations (7) and (8) were integrated numerically by evaluating the integrals over a non-uniform mesh (with more concentrated points near the origin) using the trapeze method; the number of points in the mesh and the maximal time were systematically varied to make sure that predictions do not depend on the properties of the mesh.

---

### Exact, time-dependent analytical equations for spiral trajectories and matching gradient and density-correction waveforms [^ced202c6]. Magnetic Resonance in Medicine (2026). Medium credibility.

2.4 Inner arc

To define the spiral trajectory for r <, a quarter‐circle k‐space arc of radiusis used to connect the center of k‐space to the circle involute described above, as illustrated in Figure 1A–C in blue. This inner arc, which itself has a fixed radius of curvature R C = , is designed to meet the WHIRL trajectory at a k‐space radius of r = , and = 1. Knowing the relevant variables for the WHIRL trajectory at this point allows one to create an appropriate gradient waveform that matches the gradient, slew, and k‐space waveforms smoothly between the arc and the WHIRL trajectory. There are many gradient solutions to achieve this inner‐arc trajectory; for this work the arc is defined in three stages played sequentially, From Equation (10), G is the integral of, is defined to maintain the circular trajectory on the arc, and the variables C, t are local to the stage, and TA and TB are chosen to smoothly match the WHIRL trajectory at the point of intersection, including all the parameters shown in the gray inset of Figure 1A. This smooth inner arc trajectory mitigates gradient and slew rate performance requirements near the k‐space center to increase trajectory fidelity; however, the total duration of this arc is typically a few hundred μs.

2.5 Outer arc

We have extended the previous WHIRLED PEAS to include a quarter‐circle k‐space arc at the outer edge of the involute as well, such that the trajectory ends in a purely angular direction (= 0), as seen in red in Figure 1A–C. This outer arc with a radius of curvaturejoins smoothly with the outer edge of the involute and allows mirror images of the trajectory to be pieced together for seamless spiral out/spiral in acquisitions with no additional trajectory calculations. The addition of this outer arc increases the required readout timeby a few microseconds and decreases the total gradient time by a few microseconds due to a slight reduction in required refocusing areas; neither change is substantial.

---

### Model-free tracking control of complex dynamical trajectories with machine learning [^0d79344b]. Nature Communications (2023). High credibility.

The Cartesian coordinates of the end effector arewhich give the angular positions of the two arms asFor any end-effector position, there are two admissible solutions for the angular variables. We select the pair of angles that result in a continuous trajectory. In addition, the end effector may end up in any of the four quadrants, so the range of q 1 is [0, 2 π]. The range of q 2 is, since the second joint can be above or below the first joint. In our simulations, we ensure that the solutions are continuous and thus are physically meaningful, as demonstrated in Fig. 8 b.

Noises and unpredictable disturbances are constantly present in real-world applications, making it crucial to ensure that the control strategy is robust and operational in their presence. In fact, a model is always inaccurate compared with the actual physical system because of factors such as change of parameters, unknown time delays, measurement noise, and input disturbances. The goal of the robustness test is to maintain an acceptable level of performance under these circumstances. In our study, we treat disturbances and measurement noise as external inputs, where the former are added to the control signal and the latter is present in the sensor measurements. In particular, the disturbances are modeled as an additive stochastic process ξ to the data:For measurement noise, we use multiplicative noise ξ in the formBoth stochastic processes ξ d and ξ m follow a normal distribution of zero mean and with standard deviation σ d and σ m, respectively.

---

### Degenerate boundaries for multiple-alternative decisions [^a8d28202]. Nature Communications (2022). High credibility.

Rewards are generated according to the Bayes risk represented over single trials (equation 12), when stochastic decision trajectories encounter a decision boundary. A reward landscape can be generated by systematically varying decision boundary parameters and averaging the rewards generated over a large number of simulated trajectories for each set of parameters. Averaging over many reward outcomes reduces variance producing a smooth surface and simulates the expected reward, which coincides with sampling rewards to learn the decision boundary under equation (11). So each point in the reward landscape is the mean reward obtained from a distribution of rewards for that boundary shape (typically from 100,000 samples across the parameter ranges); differing landscapes with c / W discretized in 1000 steps across its range from 0 to 0.2 were then considered. Each landscape has a point of mean reward that is an absolute maximum, which we will denote by; however, the landscape is very noisy even after this averaging, with many other points with similar mean rewards. Therefore, we consider a set of parameters corresponding to multiple points x ∈ X on the landscape, with mean rewards r x and standard deviations σ x according to the distribution of rewards for those parameters. This set of parameters is selected such that the maximum mean reward value of the landscape falls within a δ σ x of the point, so that. Using some example reward landscapes, we found that as δ decreases, the area designated as maximum also decreases as expected. However, below δ = 0.02, the area of the maximal region does not appear to change, but the points selected became more sparse, so we use this value.

Reporting summary

Further information on research design is available in the Nature Research Reporting Summary linked to this article.

---

### Damage dynamics and the role of chance in the timing of E. coli cell death [^b53c548b]. Nature Communications (2023). High credibility.

To define the production and removal terms that make up G (X, t), we used timescale separation, by assuming that at each time point the damage distribution among cells P (X, t) is a steady-state solution of the equation. The analytical solution for the steady-state is P (X, t) = e − U (X, t)/ σ, where U (X, t) is a potential function defined by ∂ U /∂ X = − G (X, t). This is analogous to the Boltzmann distribution in statistical mechanics.

Using the measured distribution of damage at different timepoints, P (X, t), we estimated U, differentiated it to provide G (X, t) and hence the production-removal terms in the model.

To facilitate this process, we characterized the experimental damage distributions P (X, t) by comparing them to 15 commonly-used distribution functions with 3–4 parameters (Supplementary Fig. 4, Supplementary Data 1). The best fit for the PI uptake distribution was a type-2 generalized beta distributionwith shape parameters whose ratio, p/(p + q), rises approximately linearly with age (Fig. 1h). The stochastic process which gives rise to this distribution is (see Methods):

In this inferred mechanism (Fig. 4a) damage production rises linearly with age as ηt, and damage removal is a saturating function of damage,(Fig. 4b, c). The parameters are a production slopeand removal parameters, andThe white noise amplitude is.

Fig. 4
The saturating-removal model captures damage dynamics.

a Schematic of the MP-SR model. b Rate plot showing linearly increasing production with age and a removal rate that saturates with damage, causing the fixed point to accelerate to high damage levels. c The potential function of the MP-SR model and its evolution with age. Simulations of the MP-SR model for PI uptake rate show rising mean (d) and standard deviation (e), reducing CV (f) and reducing skewness (g). The model provides a death hazard that rises exponentially with age (h) and a Weibull-like survival funct i on (i). Blue lines and regions represent the means and 95% confidence intervals respectively from subsampling n = 6200 simulated cells. Source data (both simulated trajectories and statistics) are provided in the Source Data file.

---

### Quantum measurement arrow of time and fluctuation relations for measuring spin of ultracold atoms [^3e23b120]. Nature Communications (2021). High credibility.

The distributions have been successfully reconstructed in a quantum trajectory analysis previously. Experimentally constructed probability distributions of the arrow of time are shown in Fig. 2 a, c. The green shaded area represents statistically rare events where the arrow of time points backward.

Fig. 2
Arrow of time distributions and fluctuation relations for uniform initialization of the ultracold cloud.

a Arrow of time distribution, p (Q), for, and σ 2 = 0.39, where σ encodes the measurement strength. The inset shows the corresponding probability distribution, p F (r), of the readouts, r. p (Q) is the distribution of the arrow of time measure Q, estimated from the experimental data using Eq. (5) and the form of the measurement operatorobtained from the fit to the experimentally determined forward probability distribution. The red profile is the theoretical distribution p (Q) for z = 0 [see Eq. (6)]. b Absolute irreversibility for z ≈ 0 for different σ 2. The black dots with error bars represent prediction for 〈 e − Q 〉 by estimating Q from the experimental data using Eq. (5), for single-shot realizations of the experiment. The red dashed line is the theory prediction for z = 0. c Arrow of time distribution for z = 0.92, σ 2 = 0.34. The negative Q axis is shaded in green, and the red profile indicates the corresponding numerical simulation of p (Q). d Absolute irreversibility for different z, same σ 2, for single-shot realizations of the experiment. The black dots with error bars represent prediction for 〈e − Q 〉, the left-hand side of the fluctuation theorem, by estimating Q from the experimental data using Eq. (5). The red dashed line is the theory prediction for 〈 σ 2 〉 = 0.35. In both b and d, the center of the blue diamonds (the shape is not intended to provide an error region) indicate the model prediction for 〈e − Q 〉 ≡ 1 − μ obtained by assuming the corresponding fit parameters for each individual realizations in an ideal simulation of the experiment. The error bars shown in b, d account for systematic errors in the experiment and data processing: the omitted background noise (appearing as negative column densities recorded by our imaging process, see above), the error from renormalizing this column density, and the error in the fit assumed for the experimental data (see Supplementary Note 5).

---

### Accelerating compressed sensing in parallel imaging reconstructions using an efficient circulant preconditioner for cartesian trajectories [^38854bb4]. Magnetic Resonance in Medicine (2019). Medium credibility.

2.3 Split Bregman iterations

Solving Equation 3 is not straightforward as the partial derivatives of the‐norm terms are not well‐defined around 0. Instead, the problem is transformed into one that can be solved easily. In this work, we use Split Bregman to convert Equation 3 into multiple minimization problems in which the‐norm terms have been decoupled from the‐norm term, as discussed in detail in Refs. 14 and 24. For convenience, the Split Bregman method is shown in Algorithm 1. The Bregman parametersare introduced by the Bregman scheme and auxiliary variablesare introduced by writing the constrained problem as an unconstrained problem. The algorithm consists of two loops: an outer loop and an inner loop. In the inner loop (steps 4–11), we first compute the vector b that serves as a right‐hand side in the system of equations of step 5. Subsequently, the‐norm subproblems are solved using the shrink function in steps 6–8. Hereafter, the residuals for the regularization terms are computed in steps 9–11 and are subsequently fed back into the system by updating the right‐hand side vector b in step 5. Steps 4–11 can be repeated several times, but one or two inner iterations are normally sufficient for convergence. Similarly, the outer loop feeds the residual encountered in the data fidelity term back into the system, after which the inner loop is executed again.

The system of linear equations, in line 5 of the algorithm follows from a standard least squares problem, where the system matrix is given bywith right‐hand side

In this work we focus on solving Equation 4, which is computationally the most expensive part of Algorithm 1. It is important to note that the system matrixremains constant throughout the algorithm and only the right‐hand side vector b changes, which allows us to efficiently solve Equation 4 by using preconditioning techniques.

Algorithm 1
Split Bregman Iteration

---

### Design of a logarithmic k-space spiral trajectory [^3d2eeeb3]. Magnetic Resonance in Medicine (2001). Low credibility.

Spiral acquisitions are used in fast cardiac imaging because they traverse k-space efficiently and minimize flow artifacts. A variable pitch logarithmic spiral trajectory is designed to critically sample the low-frequency region in k-space and gradually undersample the high-frequency region. An approximate analytical expression for the trajectory provides a fast means to calculate the gradient waveforms and the sampled data points. A numerical method is introduced based on the trajectory curvature and the rate of change in the gradient magnitude with time for the composite Archimedean-logarithmic trajectory. The pulse sequence is implemented and images are acquired on phantoms and human hearts. The images show improved image resolution and some improvement in image quality as a result of increased extent in k-space and reduction in aliasing artifacts, respectively.

---

### A topological fluctuation theorem [^0a82bf95]. Nature Communications (2022). High credibility.

Let us now consider trajectories for which the initial and final points share the same radial and axial coordinates: Δ r = Δ z = 0, while the total winding angle Δ ϕ remains arbitrary. In case I where the potential is isotropic (α = 1) and its ground state coincides with the vortex core (x c = 0), Δ U is independent of the winding angle Δ ϕ such that the probability ratio ϑ averaged over trajectory time and initial positions follows the predicted exponential scaling for all winding angles (see orange line in Fig. 2 f). In general, however, Γ is affected by the symmetries of the trap such that ϑ does not decay exponentially with the winding angle Δ ϕ. Setting α ≠ 1 (case II, Fig. 2 d), Δ U depends explicitly on Δ ϕ and vanishes only for Δ ϕ = n π. Consequently, the averaged winding angle probability ratio departs from the exponential scaling for all Δ ϕ not multiple of π (see red line in Fig. 2 f). Moreover, in the third case of an isotropic harmonic potential, but whose minimum is shifted of x c ≠ 0 (Fig. 2 e), the winding angle probability ratio shows similar oscillations as in case II, except that due to the lack of reflection symmetry of U those are 2 π -periodic (blue line in Fig. 2 f). In all cases, thanks to the topological protection offered by closed loops the data shown in Fig. 2 f systematically falls on the predicted exponential curve for Δ ϕ integer factor of 2 π.

The multi-vortex case

To simplify the following analysis and allow for the study of fixed time trajectories, we now consider the case where the potential U (r) acts as a stiff wall confining the particles inside a cylinder of radius R and such that U = 0 inside of the cylinder. For such confinement and a single vortex line located at r = 0, we find that for t ≪ R 2 / D the distribution p (ϕ, t) exhibits exponential tails, while for t ≫ R 2 / D the effect of confining boundaries leads to p (ϕ, t) being Gaussian (see Fig 2 c). As expected from Eq. (13), in this case one can set Δ U = 0 and the fluctuation theorem is found to hold at winding angles for all observation times, independently of the particular shape of the distribution.

---

### Density-weighted concentric rings k-space trajectory forH magnetic resonance spectroscopic imaging at 7 T [^844b243b]. NMR in Biomedicine (2018). Low credibility.

2 METHODS

2.1 k ‐space trajectory designs for MRSI

2.1.1 EPSI

The echo‐planar k ‐space trajectory for MRSI (EPSI) (Figure 1 A) was generated with phase encoding along one spatial dimension (k y) and an oscillating readout gradient along the other (k x ‐ t plane). 20 The EPSI k ‐space data are scanned in a zigzag k x ‐ t trajectory and collected using both odd and even echoes.

Figure 1
A, sampling densities for EPSI, RE and DW trajectories. B, k ‐space trajectories of EPSI, RE and DW concentric ring MRSI for 32 × 32 pixel extent in (k x, k y) space of over 240 mm × 240 mm FOV. C, simulation comparison of different k ‐space trajectories' 2D SRFs (top) and 1D SRF profiles along the x ‐axis (bottom). Simulations were performed by reconstructing a constant object with a Gaussian noise at the origin in a grid of 32 × 32 (FOV = 240 mm × 240 mm with a nominal voxel dimension of 0.56 mL) using NUFFT

2.1.2 RE CRT

A RE‐CRT for MRSI was derived similarly to Furuyama et al. 18 The following equations were used to generate the gradient waveforms along the x (G x) and y (G y) directions:where T represents the duration of the circle, and k n = (n + 1/2)/FOV represents the radius of the circle with n = 0, 1, 2, …, n max (Figure 1 A).

---

### Optimal experimental design and estimation for q-space trajectory imaging [^0add04d1]. Human Brain Mapping (2023). Medium credibility.

For MD, IWLLS and CIWLLS2 and CIWLLS3 are least biased, followed by CIWLLS1 (Figure 3a). For FA and OP, CIWLLS3 exhibits the lowest bias, followed by CIWLLS1, CIWLLS2, and IWLLS (Figure 3d, p). For μFA, IWLLS, CIWLLS1, and CIWLLS2 are least biased, whereas CIWLLS3 exhibits a slight overestimation (Figure 3g). For MK i, CIWLLS1 is the least biased, followed by IWLLS and CIWLLS2, and lastly CIWLLS3 (Figure 3j). For MK a, IWLLS, CIWLLS1, and CIWLLS2 are least biased, whereas CIWLLS3 is more biased (Figure 3m).

The standard deviation in the estimation of MD is highest for IWLLS, followed by CIWLLS1 and CIWLLS2, with CIWLLS3 having the lowest standard deviation (Figure 3b). For FA and OP, IWLLS has the highest standard deviation, whereas the standard deviation becomes progressively lower going from CIWLLS1 to CIWLLS3 (Figure 3e, q). For μFA, MK i, MK a, IWLLS, CIWLLS1, and CIWLLS2 have a higher and comparable standard deviation, whereas CIWLLS3 has the lowest standard deviation (Figure 3h, k, n). In terms of RMSE, similar trends can be observed as those seen for the standard deviation.

In summary, in terms of bias, there is no clear best estimator, as this varies across the different metrics considered, but its magnitude is small compared to that of the unconstrained estimators. For the standard deviation, the CIWLLS3 estimator shows the best performance, and IWLLS the worst. In terms of RMSE, the CIWLLS3 estimator performs best.

---

### A topological fluctuation theorem [^134082e7]. Nature Communications (2022). High credibility.

Computational verification of the theorem

For more complicated force profiles, solving the Fokker–Planck equation may not be possible, and one needs to resort to Brownian dynamics simulations. Below we address two such non-trivial cases in order to illustrate the quantization of the medium entropy production in presence of an external potential, as well as the application of the theorem to the multi-vortex case.

As the data shown in Fig. 2 b suggests, a numerical verification of the theorem in free space would be very demanding due to the need for excessive sampling. On the contrary, constraining the stochastic trajectories inside a finite volume with an externally applied potential U (r) significantly speeds up the winding number statistics convergence. In practice, however, such confinement also imposes to make sure that stochastic trajectories cannot reach the vortex cores where the winding angle is ill-defined; which we achieve by means of a stiff potential barrier (see Methods for details about numerical simulations). For simplicity, we moreover consider the case of an effectively two-dimensional vortex force such that all vortex lines are along the third spatial direction, as described by Eq. (11). For such force fields, the dissipated heat (7) readswhere, as before, γ i denotes the i th vortex strength, while Δ ϕ i and Δ U are the corresponding total winding angle and potential energy difference between the final and initial positions. Note that Eq. (13) holds irrespectively of the observation time, such that the numerical verification of the theorem can be carried out either looking at finite time distributions, or after time averaging at fixed initial conditions.

---

### Observation of quantum criticality of a four-dimensional phase transition [^e523c441]. Nature Communications (2025). High credibility.

Mapping on a disordered 4D model

Following refs., the dynamics of the 4D-QpQKR can be mapped onto an effectively periodic one by introducing three extra dimensions, represented by canonically conjugated position and momentum operators x i and p i, i = 2, 3, 4. In this respect, we define the effective Hamiltonian, which corresponds to a time-periodic four-dimensional system, with linear kinetic energy for the extra dimensions. Furthermore, one can showthat the corresponding four-dimensional wavefunction Ψ t (x, x 2, x 3, x 4) with a "plane source" initial condition Ψ t = 0 (x, x 2, x 3, x 4) ≡ ψ t = 0 (x) δ (x 2) δ (x 3) δ (x 4) has the same dynamics along x as that of the physically one-dimensional 4D-QpQKR with initial condition ψ t = 0 (x). Moreover, the Floquet eigenstates Φ of the one-period evolution operator associated withwith eigenvaluecan be mapped onto eigenstates of a 4D tight binding model ϵ p Φ p + ∑ q t q Φ p − q = 0, where p = (p, p 2, p 3, p 4) and q are the four-dimensional momenta on a hypercube. Here, the "on-site energies" areand the short-range "hopping amplitudes" t q are given by the four-fold Fourier transform of. The hopping t q is not limited just to nearest neighbors, but it is short-range and anisotropic, reduced by a factor ε along the synthetic dimensions p i compared to the 'physical' one p. Furthermore, if are incommensurate, ϵ p is a pseudo-random sequence, playing the role of a (correlated) disorder. The corresponding tight-binding model is therefore a (pseudo-)disordered Anderson model in momentum space.

---

### The hammam effect or how a warm ocean enhances large scale atmospheric predictability [^382ddf87]. Nature Communications (2019). High credibility.

Local dimension and persistence

To determine d, we exploit recent results from the application of extreme value theory to Poincaré recurrences in dynamical systems. This approach considers long trajectories of a system — in our case successions of daily SLP latitude–longitude maps — corresponding to a sequence of states on the attractor. For a given point ζ in phase space (e.g. a given SLP map), we compute the probability that the system returns within a ball of radiuscentred on the point ζ. The Freitas et al.theorem, modified by Lucarini et al. states that logarithmic returns:

yield a probability distribution such that:

where z = g (x (t)) and s is a high threshold associated to a quantile q of the series g (x (t)). Requiring that the orbit falls within a ball of radiusaround the point ζ is equivalent to asking that the series g (x (t)) is over the threshold s; therefore, the ball radiusis simply e − s(q). The resulting distribution is the exponential member of the Generalised Pareto Distribution family. The parameters μ and σ, namely the location and the scale parameter of the distribution, depend on the point ζ in phase space. μ (ζ) corresponds to the threshold s (q), while the local dimension d (ζ) can be obtained via the relation.

When x (t) contains all the variables of the system, the estimation of d based on extreme value theory has a number of advantages over traditional methods (e.g. the box counting algorithm). First, it does not require to estimate the volume of different sets in scale space: the selection of s (q) based on the quantile provides a selection of different scales s, which depends on the recurrence rate around the point ζ. Moreover, it does not require the a priori selection of the maximum embedding dimension as the observable g is always a univariate time-series.

The persistence of the state ζ is measured via the extremal index, an adimensional parameter, from which we extract. θ (ζ) is therefore the inverse of the average residence time of trajectories aroundand it has unit of a frequency (in this study 1/days). If ζ is a fixed point of the attractor. For a trajectory that leaves the neighbourhood of ζ at the next time iteration, θ = 1. To estimate θ, we adopt the Süveges estimator. For further details on the extremal index, see ref.

---

### Ultra-high dynamic range quantum measurement retaining its sensitivity [^6f9c90ae]. Nature Communications (2021). High credibility.

To improve the sensitivity for a large range, initially, a number of measurements with different areas are combined to uniquely define the magnetic field amplitude in a range limited by the measurement with the smallest area (Fig. 1d). Consequently, only part of the measurement time is spent on the largest area, which has the best sensitivity (but a small range), while the remainder of the time is spent on areas with a worse sensitivity. Therefore, the sensitivity of the combined measurement is strictly worse than this best sensitivity. Using halved areas (hence requiring at leastadditional areas) and the same number of iterations for each area and using no optimisations, for roughly the same σ B, the measurement time for the combined sequence. Thus, the sensitivity would becometimes worse, which is already a significant improvement compared to the straightforward case in the last paragraph.

The measurements resulting from different areas are combined via Bayes' theorem. For area A n, the measurement gives signal S n (for example the crosses/circles/triangles on the sinusoids in Fig. 1a, d for three areas). The posterior probability distribution for the magnetic field B given measured signal S n iswiththe prior distribution, independent of B, andwiththe relation between the signal S and the applied field B (the sinusoids in Fig. 1a, d, e). Fig. 1e visualises these equations.is a Poisson distribution (counting photons), but it can be approximated by a normal distribution (green line along y -axis in Fig. 1e) when more than ~10 photons arrive (with continuity correction). This is generally the case when the uncertainty is below the maximum uncertainty, as described later. For the first measurement, the prior distribution is flat since there is no initial knowledge about the field, and for the remainder of the measurements, the previous posterior is the new prior distribution. This results in a combined distribution as demonstrated in Fig. 1f.

---

### Insight into human pubertal growth by applying the QEPS growth model [^0637eb71]. BMC Pediatrics (2017). Low credibility.

The first model to describe individual longitudinal human growth and its different phases from fetal life until adult height is the QEPS-model by Nierop et al. The model was constructed with a combination of four distinct shape-invariant growth functions: Quadratic (Q), Exponential (E), Pubertal (P) and Stop (S), (Fig. 1). All four functions have an individual height-scale parameter, and the E- and P-functions also have individual time-scale parameters; giving six modifying parameters in total to describe individual growth. Basic features of the Q and E functions have been used in previous prediction models.

Fig. 1
The QEPS-model. Left panel: The total height (QEPS) is the sum of four growth functions: a quadratic growth function (Q), a negative exponential growth function (E), a pubertal growth function (P) and a stop function (S) modelling the end of growth for function Q. B = birth, t 0 = about 6 weeks after conception. Birth is marked with a vertical line. Age scale below 3 years is stretched out. Right panel: QEPS model for total height, T(age), = E(age) + QS(age) + P(age), with QS(age) = Q(age) – S(age). T max = E max + QS max + P max. AgeP5 and AgeP50 mark the ages where 5% and 50% of P max are reached, AgeS 0 marks the age where the S -function is starting. Originally published in Journal of theoretical biology 2016;406:143–65, Nierop AF et al. used by permission of Elsevier journals

In the present study we implement novel estimates of pubertal growth from the QEPS mathematical growth model in cm and SD-scores both at the individual and the group level. The model calculates both the specific pubertal P- function during puberty and the from prepubertal period ongoing QES- functions, as well as the combined total growth. Moreover, the model provides confidence intervals (CIs) of the different growth estimates that can be used to assess the quality of growth data at the individual level. Basic features of the QEPS-model have been presented at meetings.

---

### K-space trajectory mapping and its application for ultrashort echo time imaging [^22f6c80c]. Magnetic Resonance Imaging (2017). Low credibility.

MR images are affected by system delays and gradient field imperfections which induce discrepancies between prescribed and actual k-space trajectories. This could be even more critical for non-Cartesian data acquisitions where even a small deviation from the assumed k-space trajectory results in severe image degradation and artifacts. Knowledge of the actual k-space trajectories is therefore crucial and can be incorporated in the reconstruction of high quality non-Cartesian images. A novel MR method for the calibration of actual gradient waveforms was developed using a combination of phase encoding increments and subsequent detection of the exact time point at which the corresponding trajectory is crossing the k-space origin. The measured sets of points were fitted to a parametrical model to calculate the complete actual acquisition trajectory. Measurements performed on phantoms and volunteers, positioned both in- and off-isocenter of the magnet, clearly demonstrate the improvement in reconstructed ultrashort echo time (UTE) images, when information from calibration of k-space sampling trajectories is employed in the MR image reconstruction procedure. The unique feature of the proposed method is its robustness and simple experimental setup, making it suitable for quick acquisition trajectory calibration procedures e.g. for non-Cartesian radial fast imaging.

---

### Distributed spirals: a new class of three-dimensional k-space trajectories [^5f354c75]. Magnetic Resonance in Medicine (2013). Low credibility.

This work presents a new class of three-dimensional spiral based-trajectories for sampling magnetic resonance data. The distributed spirals trajectory efficiently traverses a cylinder or sphere or intermediate shape in k-space. The trajectory is shown to be nearly as efficient as a conventional stack of spirals trajectory in terms of scan time and signal-to-noise ratio, while reducing coherent aliasing in all three spatial directions and reducing Gibbs ringing due to the nature of collecting data from a sphere in k-space. The trajectory uses a single two-dimensional spiral waveform with the addition of a single orthogonal waveform which is scaled with each repetition, making it relatively easy to implement. Blurring from off-resonance only occurs in two dimensions due to the temporal nature of the sampling.

---

### Simple method for MR gradient system characterization and k-space trajectory estimation [^d0e10c31]. Magnetic Resonance in Medicine (2012). Low credibility.

Fast imaging trajectories are used in MRI to speed up the acquisition process, but imperfections in the gradient system create artifacts in the reconstructed images. Artifacts result from the deviation between k-space trajectories achieved on the scanner and their original prescription. Measuring or approximating actual k-space trajectories with predetermined gradient timing delays reduces the artifacts, but are generally based on a specific trajectory and scan orientation. A single linear time-invariant characterization of the gradient system provides a method to predict k-space trajectories scanned in arbitrary orientations through convolution. This is done efficiently, by comparing the Fourier transforms of the input and measured waveforms of a single high-bandwidth test gradient waveform. This new method is tested for spiral, interleaved echo-planar, and three-dimensional cones imaging, demonstrating its ability to reduce reconstructed image artifacts for various k-space trajectories.

---

### Improved k-space trajectory measurement with signal shifting [^b4e8c9a2]. Magnetic Resonance in Medicine (2007). Low credibility.

Various techniques for k-space trajectory measurement have been described in the literature. Self-encoding gradient techniques are time-consuming due to the high number of phase-encoding steps needed. The approach with localized slices is faster, but its use apparently has not been reported in the context of high spatial resolution experiments. Signals associated with high k-space frequencies may then reach low or even zero values, and this may result in errors in the estimate of the trajectories at the k-space periphery. To overcome this problem without increasing the measurement duration of the localized slice method too much, a new approach is proposed in which an addition dephasing gradient applied prior to the gradient to be measured shifts the signal maximum.

---

### An optical neural chip for implementing complex-valued neural network [^64866747]. Nature Communications (2021). High credibility.

In the binary classification, the decision boundary partitions the underlying vector space into two regions, one for each class. The decision boundaries of the simulated real-valued model, the complex-valued model and the on-chip implemented complex model when classifying the Circle and the Spiral are shown in Fig. 5b, c, respectively. As shown, the decision boundaries of the real model are formed by straight lines, while the decision boundaries of the complex model are nonlinear curves that perfectly match the entangled shape of the datasets. The complex model is also appreciably superior in classification accuracy. The complex model achieves simulated accuracy of 100% on both datasets, far exceeding the 55% (on Circle) and 89% (on Spiral) achieved by the real model. In chip implementation, we scan both inputs x 1 and x 2 from −1 to 1 by a step size of 0.1. A total 441 input sets are tested. In experiment results, the black wires are our expected decision boundaries while the white wires are experimental ones, from which we can easily figure out which points are incorrectly classified. The chip testing accuracies are 98% for the Circle and 95% for the Spiral. The theoretical decision boundaries of the complex model are smooth, while the visualized resolution is reduced by the input interval in experiment.

The theoretical decision boundaries can be derived. Suppose we have a single layer with two neurons, the outputs of the real model arewhere the w 11,12,21,22 and b 1,2 are real-valued, x 1,2 are real inputs and y 1,2 are the outputs. The decision boundary is derived by solving the equation || y 1 || = || y 2 ||. Therefore, the decision surface is formed by two straight lines:

In the complex model, we replace the weight matrices by w jk = p jk + iq jk, and b j = m j + in j, where j, k = 1, 2. By solving || y 1 || = || y 2 ||, a nonlinear decision boundary is formed bywhich can be simplified to a binary quadratic equation:

Equation (5) can form various curves, such as parabola, circle, ellipse, and hyperbola, with different parameters A–F, which can be learned from training data. For nonlinear dataset, the complex-valued network shows strong learning capability, by forming nonlinear decision boundaries and achieving high classification accuracy.

---

### Early life growth is related to pubertal growth and adult height-a QEPS-model analysis [^8a406c1a]. Pediatric Research (2025). Medium credibility.

For participants born in 1974, the birth weights were ranging from 1530 g to 5420 g (mean 3524 g) among males and from 1130 g to 5670 g (mean 3395) among females. For participants born in 1990, the birth weighs were ranging from 785 g to 5310 g (mean 3595 g) among males and from 1045 g to 5430 g (mean 3501 g) among females. In 1974, 1810 participants (82%) were born at term (37 + 0 to 42 + 0 weeks/days of gestation), 315 (14%) post-term (> 42 + 0 weeks/days of gestation) and 76 (3%) preterm (< 37 + 0 weeks/days of gestation), of which 2 were born very preterm (28 + 0 to 32 + 0 weeks/days of gestation) and 0 extremely preterm (< 28 + 0 weeks/days of gestation). The figures for the participants born in 1990 were 2198 (88%) term, 181 (7%) post-term, 105 (4%) preterm, 14 very preterm, and one extremely preterm.

Study design

Individual, longitudinal length/height curves from birth to adult height were obtained using the longitudinal height measurements and the growth functions of the QEPS -model. Analyses were based on the QEPS -model derived for each growth period [early life (fetal–infancy, including birth), childhood, and puberty] (Fig. 1). The importance of these growth periods for the studied outcomes were assessed in univariate and multivariate linear regression models, separately per sex.

Fig. 1
Design of the study.

The total height (QEPS) is the sum of four growth functions: a quadratic growth function (Q), a negative exponential growth function (E), a pubertal growth function (P) and a stop function (S) modeling the end of growth for function Q. B = birth, t 0 = about 6 weeks after conception. Birth is marked with a vertical line. Age scale below 3 years is stretched out. The growth periods (early life: fetal-infancy, childhood, and puberty) are defined by the QEPS estimates Age 40w, Age E99, Age P5, Age TPHV, and Age P95. The contribution of each growth function (Q max, E max, P max, and S max) to the total height, T max, is illustrated to the right.

---

### Learning emergent partial differential equations in a learned emergent space [^a1e6d5de]. Nature Communications (2022). High credibility.

Results

Learning partial differential equations in emergent coordinates

For an illustrative caricature, we use a PDE with a known independent space variable, before returning to our coupled agent example. In this case we do have a known independent spatial coordinate, x, but we will randomly scramble it ourselves, to validate that our algorithms can, in a meaningful way, recover it. Consider the 1D complex Ginzburg-Landau equation, a PDE for the evolution of a complex field W (x, t) in one spatial dimension, defined bywith real parameters c 1 = 0, c 2 = −3, L = 80, and, here, periodic boundary conditions. We integrate this system using a pseudo-spectral method with exponential time stepping. This results in spatiotemporal chaotic dynamics, so called spatiotemporal intermittency, with the spatiotemproal evolution shown in Fig. 2 (a). See section Methods for an additional example with c 1 = 1, c 2 = 2 and no-flux (Neumann) boundary conditions showing periodic dynamics.

Fig. 2
Data-driven PDE for the chaotic dynamics in the complex Ginzburg-Landau equation.

a The real part of the complex field W (x, t) obtained from simulating Eq. (2) with N = 256 mesh points after initial transients have decayed. b Removing the spatial label yields a collection of N time series plotted here in random sequence. c Using manifold learning (here diffusion maps), one finds that there exists two modes ϕ 1 and ϕ 2 parametrizing these time series. Each point corresponds to one of the N time series, and is colored by its scrambled spatial location x. d Having obtained the embedding, we can introduce an emergent coordinateparametrizing the circle spanned by ϕ 1 and ϕ 2. e The real parts of the time series parametrized by. f Real part of simulation predictions for the complex variable W starting from an initial condition in our test set, using the partial differential equation model learned withas the spatial variable and a periodic domain.

---

### Exploring replay [^e0cbceb2]. Nature Communications (2025). High credibility.

Model-free control

Several algorithmic approaches exist to solving the problem of optimal control in RL tasks. One popular example is Q -learning, which is an important and widely used algorithm for learning the optimal-function. It belongs to a more general class of model-free temporal difference algorithms which, after every experienced interaction with the environment, successively update their value function estimates based on the encountered reward prediction errors. Specifically for Q -learning, the update rule at iteration n is:

Here, the Q -value estimate is updated towards the difference (or prediction error) between the initial estimate, Q n (s, a), and the sum of the observed reward at the next state reached and the discounted maximal Q n -value at that state, weighted by the learning rate, α. Note that the action that optimisesat s might be different from the one used in equation (4) that optimised

The Q n +1 -values themselves can be used to determine a policy, for instance:where β > 0 is an inverse temperature parameter that controls how deterministic is π n +1. Since π n +1 (s, a) favours actions with higher Q n +1 -values, it tends to be better than π n (s, a) in terms of expected return. The remaining stochasticity is a crude method for arranging a mix of exploration and exploitation.

Model-based control

A different solution is to learn a model of the environment which can then be used to perform prospective planning of the actions to execute. Value functions can also be acquired using the recurrent Bellman equation, for instance:

Here, the recurrent relationship between the successive states allows the agent to make use of its knowledge of the transition structure of the environment (the model) to propagate the information about future rewards towards its current situation or state in the environment. If the agent does indeed know the model (also including), then various forms of planning can be used to compute the long-run consequences associated with the available actions at decision time and make a far-sighted and informed decision. Value iteration is one example planning algorithm which iteratively performs synchronous updates (for all states and actions in each sweep) specified by Equation (6). Such updates are also called Bellman backups because of the application of the Bellman equation. Given a perfect model of the environment, and, such procedure is guaranteed eventually to converge to the optimal value function.

---

### Centering the projection reconstruction trajectory: reducing gradient delay errors [^4b3d64dc]. Magnetic Resonance in Medicine (2003). Low credibility.

The projection reconstruction (PR) trajectory was investigated for the effect of gradient timing delays between the actual and requested start time of each physical gradient. Radial trajectories constructed with delayed gradients miss the center of k-space in an angularly dependent manner, causing effective echo times to vary with projection angle. The gradient timing delays were measured in phantoms, revealing delays on the x, y, and z gradients which differed by as much as 5 micro sec. Using this one-time calibration measurement, the trajectories were corrected for gradient delays by addition of compensatory gradient areas to the prephasers of the logical x and y readout gradients. Effective projection-to-projection echo time variability was reduced to less than 1 micro sec for all imaging orientations. Using corrected trajectories, artifacts were reduced in phantom images and in volunteer studies. This correction should potentiate greater clinical use of the PR trajectory.

---

### Long-term memory induced correction to arrhenius law [^495130a6]. Nature Communications (2024). High credibility.

Fig. 4
Check of the approximations of the theory.

a Check of the stationary covariance approximation (i.e.): comparison between ψ π (t) = Var(x π (t)) measured in numerical simulations (symbols) and ψ (t) (dashed line: H = 3/8, full line H = 1/4). b Check of Eq. (9): comparison between the value m π (t) in simulations (symbols) and x 0 ϕ (t) (full line: x 0 = l /2 for H = 3/8; dashed line x 0 = l for H = 1/4). Note that m π (t) ≃ x 0 ϕ (t) is expected at large times only. c Check of the short-time scaling regime for H = 3/8. d Check of the long-time scaling regime (11) for H = 3/8. In a, c, d, the initial position is drawn from an equilibrium distribution, corresponding to our predictions for x 0 = 0. When present, error bars represent 68% confidence intervals.

General results

This approach first shows unambiguously that the mean FPT is finite. Indeed, we show in SI (Section B) that the solution to Eq. (8) satisfies at long timeswhich can be checked directly in numerical simulations, see figure 4b. This scaling, together with Eq. (6), shows that the mean FPT is finite. This contradicts the results obtained with the generalized Fokker–Planck equationor with the Wilemski-Fixman approximation. The latter amounts to assuming that the process is at all times in an equilibrium state, and would thus yield m π (t) ≃ L ϕ (t), leading to an infinite mean FPT when α < 1 (as noted earlier in a similar, but out of equilibrium, situation). Beyond this proof of finiteness, our approach yields a quantitative determination of 〈 T 〉 by solving numerically the integral equation (8) for m π (t) and next using Eq. (6); this shows quantitative agreement with numerical simulations in Fig. 3.

---

### Quantitative evaluation of methods to analyze motion changes in single-particle experiments [^d992a033]. Nature Communications (2025). High credibility.

CP detection metrics

Following Ref. given a ground-truth CP at locations t (GT), i, and a predicted CP at locations t (P), j, we defined the gated absolute distance:where ε CP was used as a fixed maximum penalty for CPs located more than ε CP apart. For a set of M GT ground-truth CPs and M P predicted CPs, we solved a rectangular assignment problem using the Hungarian algorithmby minimizing the sum of distances between paired CPs:The distance d CP allows to define a pairing metric:whereis the distance associated with having all predicted CPs unpaired or at a distance larger than ε CP from all ground-truth CPs. The metric α CP is bound in [0, 1], taking a value of 1 if all ground-truth and predicted CPs are matching exactly. Similarly, we define a CP localization metric:whereis the distance associated with having all unassigned predicted CPs at a distance larger than ε CP from all ground-truth CPs. This metric measures the presence of spurious CPs and is bound in [0, α CP], taking value α CP if no spurious CPs are present. We also calculate the number of true positives (TP), i.e. the paired true and predicted CPs with a distance smaller than ε CP. Spurious predictions, i.e. not associated with any ground truth or having a distance larger than ε CP were counted as false positives (FP). Ground truth CPs not having an associated prediction at a distance shorter than ε CP were considered false negatives (FN). Given an experiment containing N trajectories, we computed the overall number of TP, FP, and FN. We then used these values to calculate the JSC over the whole experiment as:For the predicted CPs classified as TP, we also computed the root mean square error (RMSE), defined as:

---

### Abstract representations of events arise from mental errors in learning and memory [^9c8f0a05]. Nature Communications (2020). High credibility.

Network structure guides reactions to novel transitions

Given a model of human behavior, it is ultimately interesting to make testable predictions. Thus far, in keeping with the majority of existing research. we have focused on static transition graphs, wherein the probability A ij of transitioning from state i to state j remains constant over time. However, the statistical structures governing human life are continually shifting, and people are often forced to respond to rare or novel transitions. Here we show that, when confronted with a novel transition — or a violation of the preexisting transition network — not only are people surprised, but the magnitude of their surprise depends critically on the topology of the underlying network.

We consider a ring graph where each node is connected to its nearest and next-nearest neighbors (Fig. 6 a). We asked subjects to respond to sequences of 1500 nodes drawn as random walks on the ring graph, but with 50 violations randomly interspersed. These violations were divided into two categories: short violations of topological distance two and long violations of topological distances three and four (Fig. 6 a). Using maximum likelihood estimation (Eq. (1)) as a guide, one would naively expect people to be equally surprised by all violations — indeed, each violation has never been seen before. In contrast, our model predicts that that surprise should depend crucially on the topological distance of a violation in the underlying graph, with topologically longer violations inducing increased surprise over short violations (Fig. 6 b).

---

### Global strategy for the diagnosis, management, and prevention of chronic obstructive pulmonary disease (2025 report) [^2f98094c]. GOLD (2025). High credibility.

Trajectories of lung function: development and aging — At birth, the lungs are not fully developed, and they "grow and mature until about 20–25 years of age (earlier in females), when lung function reaches its peak (Figure 1.1)". This is "followed by a short plateau and a final phase of mild lung function decline due to physiological lung aging". The "normal lung function trajectory can be altered by processes occurring during gestation, birth, childhood, and adolescence that affect lung growth… and/or processes shortening the plateau phase and/or accelerating the aging phase".

---

### Multiple tipping points and optimal repairing in interacting networks [^d95be1ee]. Nature Communications (2016). Medium credibility.

Despite the seeming complexity of equations (1) and (2), it is noteworthy that there are only two unknown variables, a A and a B, and that all other parameters are fixed. These two equations define two curves in the (a A, a B) plane.

Figure 1a shows a graphical representation of the curves for a random regularnetwork (in which all the nodes have the same degree) with degree of k = 16 and threshold m = 8, for the symmetric parameter values, r A = r B = 0.60 and r d = 0.15. The size of each network is N = 2 × 10 4. The blue curve is a graphical representation of equation (1) and the brown curve is defined by equation (2). The curves, similar to two 'ropes', create a 'knot' that can have up to nine intersections, representing mathematical solutions of the system of equations. However, not all of these solutions represent observable and stable physical states. To see that, observe one of the curves in Fig. 1a, for example, the blue curve described by equation (1). If we increase damage done to network B (that is, we increase a B) and keep everything else constant, some damage will undoubtedly spread to network A. Thus, we expect that when a B is increased, a A must also increase (it would be very unusual if one network improves its activity as a result of damaging the other network). We conclude that the parts of the blue and brown curve that produce physical solutions are only those where a A and a B increase together or decrease together along the curve. This elimination leaves only four states in Fig. 1a that are stable (green circles), whereas the other five states are unstable (red crosses), for this particular choice of parameters. In simulated finite networks, when the network system evolves according to the rules of the model, at t = 0 we have a freedom to set initial conditions for the activities. Systems initially prepared to have a pair of values (a A, a B) corresponding to an unstable solution of equations (1) and (2) will be disturbed by a small fluctuation of a A or a B, owing to the system dynamics, and the values of a A or a B will rapidly change until one of the stable states is reached. Systems that are initially prepared to have values of a A or a B corresponding to a stable solution will fluctuate around these values, until perhaps a large finite fluctuation occurs and the system 'jumps' to another stable state. In general, for any choice of parameters, we have between one and four stable (physical) states. Figure 1b shows the scenario for the same network system when, r A = r B = 0.60 and r d = 0.15. In this case we have two stable states and one unstable state.

---

### Geographic population structure analysis of worldwide human populations infers their biogeographical origins [^529852ec]. Nature Communications (2014). Medium credibility.

Calculating the biogeographical origin of a test sample

Given nine admixture proportions for a sample of unknown geographic origin obtained using ADMIXTURE's supervised approach with the nine putative ancestral populations, we calculated the Euclidean distance between its admixture proportions and the N reference populations (GEN). All reference populations were sorted in an ascending order according to their genetic distance from the sample. The smallest distancerepresented the sample's deviation from the best matching population. This distance in genetic space was converted into geographic distanceusing Equation 1. The contribution of other reference populations m = 2… N to the samples' genetic make-up is represented using the weight, whereis the distance to the m th reference population.

To refine the positioning of the sample, we used M closest reference populations (default value is M = 10). Because GPS relies on the linear relationship between genetic and geographic coordinates, which holds for populations that are less than 4,000 miles apart, the value of parameter M depends on the density of the reference population data set. A data set with few populations that are geographically sparse would require a smaller M. We determined the value of M empirically by increasing the number of nearest populations (M) and observing changes of geographic coordinates. We found that the addition of extra populations after M reaches 10 does not significantly affect the prediction. The predicted position of the sample on the map is computed using Equation 2 below which is a linear combination of vectors with the origin at the geographic centre of the best matching population (Position best), weighted by w and scaled to fit on the circle with a radius:

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^bb9b7f12]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Δ Φ, the probability distribution at a point Φ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Φ ∈ {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters α and β, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since α and β are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Φ ∈ [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(α, β) and x 2 = 1 − x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Φ, it may seem natural to select α and β by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### Triazolam [^59f91090]. FDA (2025). Medium credibility.

WARNING: RISKS FROM CONCOMITANT USE WITH OPIOIDS; ABUSE, MISUSE, AND ADDICTION; and DEPENDENCE AND WITHDRAWAL REACTIONS

Concomitant use of benzodiazepines and opioids may result in profound sedation, respiratory depression, coma, and death. Reserve concomitant prescribing of these drugs in patients for whom alternative treatment options are inadequate. Limit dosages and durations to the minimum required. Follow patients for signs and symptoms of respiratory depression and sedation [see Warnings and Precautions (5.1), Drug Interactions (7.1)].
The use of benzodiazepines, including triazolam, exposes users to risks of abuse, misuse, and addiction, which can lead to overdose or death. Abuse and misuse of benzodiazepines commonly involve concomitant use of other medications, alcohol, and/or illicit substances, which is associated with an increased frequency of serious adverse outcomes. Before prescribing triazolam and throughout treatment, assess each patient's risk for abuse, misuse, and addiction [see Warnings and Precautions (5.2)].
The continued use of benzodiazepines, including triazolam, may lead to clinically significant physical dependence. The risks of dependence and withdrawal increase with longer treatment duration and higher daily dose. Abrupt discontinuation or rapid dosage reduction of triazolam after continued use may precipitate acute withdrawal reactions, which can be life-threatening. To reduce the risk of withdrawal reactions, use a gradual taper to discontinue triazolam or reduce the dosage [see Dosage and Administration (2.3), Warnings and Precautions (5.3)].

WARNING: RISKS FROM CONCOMITANT USE WITH OPIOIDS; ABUSE, MISUSE, AND ADDICTION; and DEPENDENCE AND WITHDRAWAL REACTIONS

See full prescribing information for complete boxed warning.

Concomitant use of benzodiazepines and opioids may result in profound sedation, respiratory depression, coma, and death. Reserve concomitant prescribing of these drugs in patients for whom alternative treatment options are inadequate. Limit dosages and durations to the minimum required. Follow patients for signs and symptoms of respiratory depression and sedation (5.1, 7.1).
The use of benzodiazepines, including triazolam, exposes users to risks of abuse, misuse, and addiction, which can lead to overdose or death. Before prescribing triazolam and throughout treatment, assess each patient's risk for abuse, misuse, and addiction (5.2).
Abrupt discontinuation or rapid dosage reduction of triazolam after continued use may precipitate acute withdrawal reactions, which can be life-threatening. To reduce the risk of withdrawal reactions, use a gradual taper to discontinue triazolam or reduce the dosage (2.3, 5.3).

---

### Single-shot magnetic field mapping embedded in echo-planar time-course imaging [^7ac850af]. Magnetic Resonance in Medicine (2003). Low credibility.

A technique for acquiring magnetic field maps simultaneously with gradient-recalled echo-planar time-course data is described. This technique uses a trajectory in which the central part of k-space is collected twice. For a 64 x 64 image acquired with a 125-kHz bandwidth, a field map suitable for geometric correction can be collected simultaneously with the echo-planar time-course data in < 70 ms. The field maps generated by this technique are registered with the magnitude images because they are calculated using the same data. They do not suffer from errors due to subject motion, or from different geometric distortions that can result from using different pulse sequences. In addition to correcting geometric distortions that resulted from dynamic magnetic field perturbations, this method was used to measure field shifts arising from respiration and jaw motion across five subjects. Values ranged from 0.035 to 0.165 parts per million (ppm).

---

### Insights from a dynamical system approach into the history of atmospheric oxygenation [^2af2a9a7]. Nature Communications (2024). High credibility.

Here µ denotes the number of moles in 1 atmosphere. The oxidation parameter, Ψ, is parameterized as log Ψ = p 1 x 4 + p 2 x 3 + p 3 x 2 + p 4 x 1 + p 5, where x = log(µ p O 2) and coefficients p 1 = 0.00286, p 2 = −0.1608, p 3 = 3.216, p 4 = −26.53, p 5 = 76.04 are resulted from fitting a curve to the previously published data (Figure S1). The details of the goodness of the fit, along with the range of values and their confidence intervals are shown in Figure S1.

While our parameterization scheme for the rate of atmospheric methane oxidation is similar to previous studies, some assumptions in this parameterization can potentially cause uncertainty in the rate of methane oxidation. For instance, this parameterization makes assumptions about the age of the Sun as well as about the surface temperature, both of which effects can potentially influence the photochemistry of CH 4 and O 2. The assumption about the age of the sun can impact the Lyman alpha UV flux, an important factor for kinetics of CH 4 photolysis CH 4 and the Earth's surface temperature can impact the water vapor content of the troposphere with impacts on the abundance of OH and the rate of methane oxidation. To account for the uncertainty associated with these assumptions, we varied the kinetics of atmospheric methane oxidation in our stochastic simulation.

Finally, considering that the methane mass balance equation depends on p O 2, we first solved the methane mass balance equation and obtained the fixed points (steady states) of the methane mass balance equation, and then used those fixed points to solve the oxygen mass balance equation.

---

### Interfering trajectories in experimental quantum-enhanced stochastic simulation [^0151bfa3]. Nature Communications (2019). High credibility.

Results

Framework and tools

In this work, we study a simple stochastic process known as the perturbed coin. It consists of a binary random variable that represents the state of a coin (0 corresponds to heads and 1 to tails) inside a box. At each time step, the box is perturbed, causing the coin to flip with some probability. Afterwards, the state of the coin is emitted. In general the coin may be biased, so the probability of remaining in heads, l, can differ from the probability of remaining in tails, m, as presented in Fig. 1. Repetition of this procedure generates a string of 0s and 1s, whose statistics define the perturbed coin process.

Fig. 1
Perturbed coin. The process that we study here is a coin with two outcomes, 0 and 1. The transition probabilities, T ij, between different outcomes are determined by l and m for i, j ∈ {0, 1}. The optimal classical model uses the causal states ({ S i }) depicted in the circles. There is a simple mapping from the past of the process to the relevant causal state: the last outcome from the coin determines the input causal state. Arrows, with the associated expressions j | T ij, represent the transitions from causal states S i to S j with probability T ij, emitting the classical outcome j. In the quantum model, the causal states become quantum states, {| S i 〉}

Any device that seeks to replicate correct future statistics must retain relevant past information in a memory. This involves a prescription for configuring its memory in an appropriate state for each possible observed past, such that systematic actions on this memory recover a sequence of future outputs that are faithful to conditional future statistics. The amount of past information stored in memory is quantified by the Shannon entropy, where d s is the probability that the memory is in state s and the logarithm is in base 2. The minimal possible memory required, C μ, is known as the statistical complexity, and is an important measure of structure in complexity science –. For the perturbed coin (Fig. 1), the minimal information required about the past is the current state of the coin. This induces a statistical complexity of C μ = − q log q − (1 − q) log (1 − q), where q represents the probability that the last outcome was heads (see Eq. (1) in Methods).

---

### Phase informed model for motion and susceptibility [^b1900879]. Human Brain Mapping (2013). Low credibility.

Performing the Dynamic Distortion Correction

From Eq. (5), the resulting map of local phase changes associated with each time point i can be scaled from radians to a change in B 0 field, ΔΒ 0 i in Hz by dividing by the echo time, TE, of the EPI volume, and a value of 2π since the phases are given in radians (i.e. ΔΒ 0 i = Δ ϕ correction i /(2π TE)). ΔΒ 0 i must be divided by the bandwidth per pixel in the phase‐encode direction, BW PE, to give the relative distortion in voxels, resulting in a voxel displacement map (vdm):Each vdm i describes the voxel displacement in the phase encode direction caused by the change in field at each voxel in the distorted image i with respect to the space of the first image. To calculate the field required to correct for the relative distortion between each image and the first in the time series, the vdm i must be inverted. For this inversion procedure, the forward mapping gives us x′ = f (x) for each value x on a regular grid, where x is a 1‐dimensional column in the phase encode direction. The inverse mapping yields x = f −1 (x ′) for each value x ′ on a regular grid. It is calculated by, for each value x ′, finding the first x i so that f (x i) > x ′ and then finding x by linear interpolation between f (x i −1) and f (x i). This is a valid procedure as long as f (x) is monotonously increasing, which is the case in general. When it is not, i.e. when the off‐resonance field changes very rapidly over space (compared with the effective encoding gradient strength), there will be almost total signal loss in the gradient echo EPI images, so the displacement of the nonexistent signal will not matter. Finally, at each time point i, the original distorted image can be distortion corrected to the space of the first image by resampling distorted image voxels at new locations along the phase encode direction according to the values in the vdm i [Hutton et al. 2002; Jezzard and Balaban, 1995].

---

### Mean first-passage times of non-markovian random walkers in confinement [^37f88f96]. Nature (2016). Excellent credibility.

The first-passage time, defined as the time a random walker takes to reach a target point in a confining domain, is a key quantity in the theory of stochastic processes. Its importance comes from its crucial role in quantifying the efficiency of processes as varied as diffusion-limited reactions, target search processes or the spread of diseases. Most methods of determining the properties of first-passage time in confined domains have been limited to Markovian (memoryless) processes. However, as soon as the random walker interacts with its environment, memory effects cannot be neglected: that is, the future motion of the random walker does not depend only on its current position, but also on its past trajectory. Examples of non-Markovian dynamics include single-file diffusion in narrow channels, or the motion of a tracer particle either attached to a polymeric chain or diffusing in simple or complex fluids such as nematics, dense soft colloids or viscoelastic solutions. Here we introduce an analytical approach to calculate, in the limit of a large confining volume, the mean first-passage time of a Gaussian non-Markovian random walker to a target. The non-Markovian features of the dynamics are encompassed by determining the statistical properties of the fictitious trajectory that the random walker would follow after the first-passage event takes place, which are shown to govern the first-passage time kinetics. This analysis is applicable to a broad range of stochastic processes, which may be correlated at long times. Our theoretical predictions are confirmed by numerical simulations for several examples of non-Markovian processes, including the case of fractional Brownian motion in one and higher dimensions. These results reveal, on the basis of Gaussian processes, the importance of memory effects in first-passage statistics of non-Markovian random walkers in confinement.

---

### Scalable spatiotemporal prediction with Bayesian neural fields [^fea8687d]. Nature Communications (2024). High credibility.

Prediction queries

We can approximate the posterior (13) using a set of samples, which may be obtained from either MAP ensemble estimation or stochastic variational inference (by sampling from the ensemble of M variational distributions). We can then approximate the posterior-predictive distribution(which marginalizes out the parameters Θ) of Y (r *) at a novel field index r ✱ = (s *, t *) by a mixture model with M equally weighted components:Equipped with Eq. (23), we can directly compute predictive probabilities of events { Y (r *) ≤ y }, predictive probability densities { Y (r *) = y }, or conditional expectationsfor a probe function. Prediction intervals around Y (r *) are estimated by computing the α -quantile y α (r *), which satisfiesFor example, the median estimate is y 0.50 (s *, t *) and 95% prediction interval is [y 0.025 (s *, t *), y 0.975 (s *, t *)]. The quantiles (24) are estimated numerically using Chandrupatla's root finding algorithmon the cumulative distribution function of the mixture (23).

Temporal seasonal features

Including seasonal features (c.f. Eq. (8)), where possible, is often essential for accurate prediction. Example periodic multiples p for datasets with a variety of time units and seasonal components are listed below (Y = Yearly; Q = Quarterly; Mo = Monthly; W = Weekly; D = Daily; H; Hourly; Mi = Minutely; S = Secondly):
Q: Y = 4
Mo: Q = 3, Y = 12
W: Mo = 4.35, Q = 13.045, Y = 52.18
D: W = 7, Mo = 30.44, Q = 91.32, Y = 365.25
H: D = 24, W = 168, Mo = 730.5, Q = 2191.5, Y = 8766
Mi: H = 60, D = 1440, W = 10080, Mo = 43830, Q = 131490, Y = 525960
S: Mi = 60, H = 3600, D = 86400, W = 604800, Mo = 2629800, Q = 7889400, Y = 31557600

---

### Recovery after stroke: not so proportional after all? [^d3259e7a]. Brain (2019). Medium credibility.

Spurious r(X,Δ) are likely when σ Y /σ X is small

For any X and Y, it can be shown that:

A formal proof of Equation 1 is provided in the Supplementary material, Appendix A [proposition 4 and theorem 1; also see]; its consequence is that r(X,Δ) is a function of r(X, Y) and σ Y /σ X. To illustrate that function, we performed a series of simulations (Supplementary material, Appendix B) in which r(X, Y) and σ Y /σ X were varied independently. Figure 2 illustrates the results: a surface relating r(X,Δ) to r(X, Y) and σ Y /σ X. Figure 3 shows example recovery data at six points of interest on that surface.

Figure 2
The relationship between r(X, Y), r(X,Δ) and σ Y /σ X. Note that the x -axis is log-transformed to ensure symmetry around 1; when X and Y are equally variable, log(σ Y /σ X) = 0. Supplementary material, proposition 7 in Appendix A, provides a justification for unambiguously using a ratio of standard deviations in this figure, rather than σ Y and σ X as separate axes. The two major regimes of Equation 1 are also marked in red. In Regime 1, Y is more variable than X, so contributes more variance to Δ, and r(X,Δ) ≈ r(X, Y). In Regime 2, X is more variable than Y, so X contributes more variance to Δ, and r(X,Δ) ≈ r(X,−X) (i.e. −1). The transition between the two regimes, when the variability ratio is not dramatically skewed either way, also allows for spurious r(X,Δ). For the purposes of illustration, the figure also highlights six points of interest on the surface, marked A–F; examples of simulated recovery data corresponding to these points are provided in Fig. 3.

---

### Thinking outside the curve, part I: modeling birthweight distribution [^b60d1510]. BMC Pregnancy and Childbirth (2010). Low credibility.

b. Simulation study on calibrating confidence intervals

For our second simulation study we generated 25 overlapping data sets of size 50,000 from design C in Table 3, the degree of overlap consistent with a population of 200,000. For each of various C between 2.0 and 5.0, we used Equation (7) to form confidence intervals for the mixture parameters p 1, p 2, p 3, p 4, μ 1, μ 2, μ 3, μ 4, σ 1, σ 2, σ 3, σ 4. We recorded how many of the mixture parameters were contained in their respective confidence intervals. This was repeated nine more times, and we tabulated how many of the 120 = 12 × 10 confidence intervals contained their targets. Confidence intervals were also formed using Equation (6) for comparative purposes. The above steps were repeated with overlapping data sets consistent with a population of 1,000,000 and with nonoverlapping data sets consistent with an effectively infinite population.

The results are summarized in Table 5. With an effectively infinite population, only 81.7% of the confidence intervals formed using Equation (6) contained their targets at C = 5.0. The confidence intervals formed using Equation (7) contained their targets 95.0% of the time at C = 2.5. The adjustment suggested by Equation (8) appears reasonable: φ = .05 = 50,000/1,000,000 and N rep = 25 yield C φ = 1.315 C 0, which accords with the 95.8% capture of mixture parameters at C = 3.5 ≈ 1.315 × 2.5 with a population of 1,000,000.

Table 5
Confidence Interval Coverage Probabilities in Simulation Studies

The row with " C " = 2 and "Population size" = 200,000 identifies the numbers and percentages of confidence intervals containing their targets of mixture parameters, based on 10 repetitions in each of which 25 samples of size 50000 were simulated from a 4-component normal mixture with 12 parameters; results under the heading of "Bias adjustment included" are based on Equation (7) with C = 2, results under the heading of "Bias adjustment omitted" are based on Equation (6) with C = 2, and the 25 samples of size 50000 had overlap consistent with a population size of 200,000. Other rows correspond to different choices of C and/or population sizes.

---

### Meta-reinforcement learning via orbitofrontal cortex [^36797795]. Nature Neuroscience (2023). High credibility.

Fig. 7
Dynamics and stabilization of OFC value coding during meta-learning.

a, Longitudinal tracking of neural populations across sessions (5 deep RL models, 14 OFC populations). Scale bar = 100 µm. This figure focuses on the activity at postchoice period (0–1 s for mice, the time point after choice for deep RL). Analyses at other trial periods are shown in Extended Data Figs. 9 and 10. b, Decoding accuracy of value-related signals increases during training. All 100 recurrent units were used for deep RL models, and OFC neurons were subsampled (55 cells per population). Shadings indicate s.e.m. Statistics are from mixed effects models with session as the fixed effect and neural population as the random intercept. Deep RL (Δ Q, P = 7.08 × 10 −202; Q ch, P = 1.72 × 10 −99; ∑ Q, P = 1.21 × 10 −110), mouse (Δ Q, P = 0.22; Q ch, P = 8.53 × 10 −19; ∑ Q, P = 3.24 × 10 −14). c, Relationships between the decoding accuracy and the strength of behavioral dependence on reward history (sum of unsigned regression weights). Kernel density estimation of the distributions (deep RL), and scatterplots with different colors for 14 different OFC populations. For deep RL, early sessions (< 100th) were excluded due to their unstable decoding accuracy. Regression lines and statistics are from mixed effects models (accuracy as the fixed effect, neural population as the random intercept). Deep RL (Δ Q, P = 3.92 × 10 −73; Q ch, P = 1.76 × 10 −63; ∑ Q, P = 6.72 × 10 −58), mouse (Δ Q, P = 0.13, Q ch, P = 6.75 × 10 −16; ∑ Q, P = 1.26 ×;10 −6). d, Angle between coding axes for shared neurons from adjacent sessions (1 session apart for deep RL, 2 d apart for OFC) was measured to quantify the similarity of population coding for value-related signals. Cosine similarity of the coding axes increases during training in both deep RL and mouse OFC. Shadings indicate s.e.m. Statistics are from mixed effects models with session pair as the fixed effect and neural population as the random intercept. Deep RL (Δ Q, P = 2.18 × 10 −39; Q ch, P = 6.14 × 10 −140; ∑ Q, P = 1.53 × 10 −154), mouse (Δ Q, P = 2.55 × 10 −3; Q ch, P = 7.53 × 10 −11; ∑ Q, P = 3.13 × 10 −7). e, Relationships between the angle of coding axes for values and the angle of action policy axes for reward history in pairs of sessions. The similarity in coding axes correlates with the similarity in behavioral action policies. Deep RL (Δ Q, P = 3.60 × 10 −39; Q ch, P = 4.35 × 10 −113; ∑ Q, P = 3.78 × 10 −102), mouse (Δ Q, P = 9.83 × 10 −4; Q ch, P = 3.56 × 10 −10; ∑ Q, P = 3.02 × 10 −6). Statistics are from mixed effects models with coding axis angle as the fixed effect and neural population as the random intercept. NS P > 0.05, ✱✱ p < 0.01, ✱✱ p < 0.001, ✱✱ p < 0.0001. All tests are two-sided.

---

### Versatile whole-organ / body staining and imaging based on electrolyte-gel properties of biological tissues [^fe2ca42b]. Nature Communications (2020). High credibility.

Numerical simulation of 3D staining patterns inside the gel

Numerical integration of systems of partial differential equations was carried out by using Mathematica software version 11.3 (Wolfram Research) with the built-in function "NDSolveValue".

The reaction-diffusion process of the reversible binding and unbinding scheme of the reactive solute (i.e. dyes and antibodies) and its binding target in 2D space is described below:

[S], [T] and [ST] denote the concentrations of the reactive solute, the binding target and the solute-target complex, respectively. D diff, k on, and k off denote the diffusion coefficient, the binding rate, and the unbinding rate, respectively. The simulation space was set as a 20 × 20 square. A circle with a 1.0 radius, namely, the gel space, was placed at the center of the square. The simulation was started with the following parameters and initial conditions unless otherwise stated: D diff = 1; k on = 1, k off = 0.01; initial [S] outside of the circle = 100; initial [S] within the circle = 0; initial [T] outside of the circle = 0; initial [T] within the circle = 100; initial [ST] for the entire simulation space = 0. The numerical integration was continued until time = 1. All parameters and time scales were unitless and chosen such that the simulation with initial default values listed above resulted in an almost homogenous distribution of [ST] inside the circle at time = 1. The time step of numerical integration was determined by the build-in Mathematica function "NDSloveValue", with the option MaxStepFraction = 0.003. This option constrains the maximum fraction of the total simulation range used for numerical integration.

Note that the binding target and the solute-target complex ([T] and [ST]) do not diffuse outside of the circular gel. All numbers in this simulation are unitless because the goal of the simulation was not to elucidate the exact physical parameters needed to recapitulate the experimental results accurately.

---

### Physical activity and exercise during pregnancy and the postpartum period: ACOG committee opinion, number 804 [^7aa306cd]. Obstetrics and Gynecology (2020). High credibility.

Recommended weight limits for lifting at work during pregnancy — an on-page algorithm and stepwise method describe how to determine a recommended weight limit (RWL). The algorithm asks, "At work do you perform lifting tasks more than once every 5 minutes?" and, if yes, "Do you lift more than three times per minute?", then queries "How many hours per day do you spend lifting at work?" with categories "Less than 1 continuous hour" or "1 or more hours per day", corresponding to infrequent, repetitive short duration, or repetitive long duration lifting panels. Steps instruct to "select the one graphic (A, B, or C) that best describes the lifting frequency or frequency/duration pattern", then "When less than 20 weeks pregnant, select the image on the left of the graphic; when pregnant for 20 weeks or more, select the image on the right", to underline all relevant numerical values along the lift path and circle the lowest, and that "The number circled in step 3 is the RWL (in pounds)" with a proviso that the short-duration category can include multiple hours per day provided each continuous bout is less than 1 hour and followed by at least 1 hour of nonlifting before resuming; the figure presents "Provisional recommended weight limits for lifting at work during pregnancy".

---

### Free-hand insertion of S2AI screw: what can we learn from O-arm navigation? [^91f85ead]. European Spine Journal (2025). Medium credibility.

Objective

To characterize the actual entry point and trajectory of the S2AI screw following O-arm navigation from the postoperative CT scans.

Methods

Postoperative CT scans from 66 patients of adult degenerative scoliosis who received S2AI fixation under O-arm navigation were investigated. 3D image processing was applied to analyze and compare two entry points of the S2AI screw: point A was the reference point which was located at 1 mm lateral and 1 mm distal to the S1 dorsal sacral foramen; point B was the actual entry point, and the relative positions of point A and point B were compared with the S1 entry point. The following trajectory of the screw between two points was compared: the screw length (SL), the transverse angle (TA), the sagittal angle (SA) and the skin distance (SD). For point B, compared the difference of the entry point and the trajectory of screws between the two groups: the low PT group (Low PT) was defined as having a baseline PT/PI < 25th percentile, and the high PT group (High PT) was defined as having a PT/PI > 25th percentile.

Results

On the left side point A was located at the lateral (5.26 ± 0.84) mm and the caudal (22.10 ± 1.26) mm of the S1 entry point and the right side was (5.23 ± 0.88) mm lateral and (22.13 ± 1.66) mm caudal of the S1 entry point. For point B, the location was at the lateral (8.08 ± 1.39) mm and the caudal (24.47 ± 2.20) mm of the S1 entry point, and the location of the right side is symmetrical to the left side. The actual entry point was 3 mm more lateral and 2 mm more caudal than the reference one. The TA of the actual point is smaller than the reference one and the SL of the actual point is longer than the reference one, whilst the rest of the trajectory parameters are comparable. The 25th PT/PI percentile of all patients was 0.5, 17 patients were divided into the Low PT group and 49 patients were divided into the High PT group. There was no significant difference between the two groups for the entry point and the trajectory of S2AI screws.

Conclusions

This study used O-arm navigation to determine the optimal entry point for S2AI screws, which is located at 3 mm caudal and 4 mm lateral to the S1 dorsal foramen. The optimal transverse angle is 46°, and the sagittal angle is 39°. These parameters with low dispersion, offer a reliable reference for the free hand to insert the S2AI screws.

---

### Simple spatial scaling rules behind complex cities [^322659c8]. Nature Communications (2017). Medium credibility.

Fig. 2
Population density distribution by the model and validation with empirical data. a The relationship between C and β for different sizes (P (t) = 10 4, 5 × 10 4, 10 5), which can be well fitted by a power law with an exponential cutoff (, see Supplementary Fig. 1 and Supplementary Note 1 for more details). β is estimated by the non-linear least squares (NLS) fitting of the simulation. Error bars mean ± sd, obtained from ten different realisations. b Simulated AP density on a log-log plot along the distance for various system sizes with C = 0.002 and P (t) = 10 4, 5 × 10 4, 10 5. The absolute density increases as the system grows larger (see Supplementary Fig. 2 and Supplementary Note 1 for the re-scaled population density). c The empirical AP density along the distance to the central areas of London (β = 0.296) and Beijing (Inset, β = 0.0888) (see Table S1 for further details on the central areas of cities)

Fig. 3
The spatial scaling sresults for two cities. Results for a London and b Beijing. The exponents of AP in London and Beijing are 1.70 and 1.91, respectively, which are estimated by linear regression in a, b. Then, β = 0.30 for London and 0.09 for Beijing can be estimated according to Eq. (5). According to our theory, the exponents of road length and socioeconomic interactions captured by nighttime lights can be predicted to be 2 − β /2 = 1.85(1.95) and 2 − 3 β /2 = 1.55(1.86) for London (Beijing), which are very close to the empirical results — by linear regression, the exponents and standard deviations for road length and socioeconomic interactions of London are 1.85 (sd 0.00800) and 1.66 (sd 0.0118). For Beijing, they are 1.95 (sd 0.0164) and 1.74 (sd 0.0105), respectively

---

### The anti-distortive polaron as an alternative mechanism for lattice-mediated charge trapping [^9583ff9e]. Nature Communications (2025). High credibility.

Fig. 2
Electronic and phonon properties of WO 3 (W atoms in green and O atoms in yellow).

a Comparison of the electronic bandgap amplitudes (in eV) and position of the valence band (VB) and conduction band (CB) edges in distinct phases of WO 3. b Simple sketch of the valence and conduction bands in the P 2 1 / c (left) and P 4/ n m m (right) phases of WO 3, with yellow and green area highlighting respectively the positions of O 2 p and W 5 d states. The change of hybridization between O 2 p and W 5 d x y orbitals, yielding the gap opening under atomic distortions from P 4/ n m m to P 2 1 / c, are illustrated from the evolution of wavefunctions valence and conduction states at Γ. c Evolution of the electronic band structure of the P 4/ n m m phase of WO 3 when condensing thein-plane antipolar motion, andoxygen rotation motions, bringing the system toward the P 2 1 / c phase. d Contributions to the polaron atomic distortion of phonon modes at distinct (q x, q y, 0) points of first Brillouin zone of the P 2 1 / c phase (using a 6 × 6 × 1 supercell). e Contributions to the polaron atomic distortion of distinct phonon modes from Γ and Γ − C and Γ − Y lines of first Brillouin zone of the P 2 1 / c phase. Amplitudes of respective contributions are proportional to the red circles' radii.

---

### Nonlinear wave propagation governed by a fractional derivative [^db5b4cd7]. Nature Communications (2025). High credibility.

The idea of fractional derivatives has a long history that dates back centuries. Apart from their intriguing mathematical properties, fractional derivatives have been studied widely in physics, for example in quantum mechanics and generally in systems with nonlocal temporal or spatial interactions. However, systematic experiments have been rare because the physical implementation is challenging. Here we report the observation and full characterization of a family of temporal solitons that are governed by a fractional nonlinear wave equation. We demonstrate that these solitons have non-exponential tails, reflecting their nonlocal character, and have a very small time-bandwidth product.

---

### The 30-degree angle revisited [^eec48357]. Journal of the American Academy of Dermatology (2005). Low credibility.

The standard surgical ellipse, with 30 degrees apical angles and a length-to-width ratio of 2 or 3 to 1, works optimally on a flat surface. The same pattern, when used for excisions on strongly convex or concave surfaces, leads to distortions which may require significant revisions. The reason for these discrepancies is explainable by the mathematical differences between flat Euclidian geometry and curved non-Euclidian geometry. Understanding these basic mathematical principles as applied to cutaneous surgery should lead to better preoperative planning, fewer intraoperative surprises, and more pleasing results.

---

### Dimensionless learning based on information [^90f574e3]. Nature Communications (2025). High credibility.

The applications of dimensional analysis extend beyond the construction of dimensionally consistent physical laws. It provides the conditions under which two systems share identical behavior (dynamic similarity), allowing predictions from laboratory experiments to be extended to real-world applications. Dimensional analysis also facilitates dimensionality reduction, i.e. simplifying physical problems to their most fundamental forms, thus decreasing the amount of data required for analysis. Another application is the discovery of self-similar variables, for which systems exhibit invariant solutions under appropriate scaling. Finally, dimensional analysis can reveal the physical regimes in which different phenomena dominate (e.g. incompressible versus compressible flow), enabling researchers to identify the most influential physical mechanisms in a given context.

A key landmark of dimensional analysis is the Buckingham- π theorem, which offers a systematic framework for deriving dimensionless variables. However, the solution is not unique, as there are infinitely many possible ways to construct these variables. To address this limitation, recent studies have developed data-driven tools to identify unique dimensionless variables that minimize the error for a given model structure, particularly in the data-rich context enabled by modern simulations and experiments. These methods combine dimensional analysis with machine learning techniques to identify dimensionless variables using multivariate linear regression, polynomial regression –, ridge regression, hypothesis testing, Gaussian process regression, neural networks, sparse identification of nonlinear dynamics, clustering, symbolic regression, and entropic principles. Table 1 offers a non-exhaustive comparative overview of data-driven methods for discovering dimensionless variables, highlighting their respective capabilities. These include applicability to ordinary and partial differential equations (ODEs/PDEs), the ability to rank dimensionless variables by predictability (and thus relative importance), identification of distinct physical regimes, detection of self-similar behavior, and extraction of characteristic scales (e.g. length and time scales of the system). Another key capability is whether the method can determine a bound on the minimum possible error across all models–which in turn enables the definition of a model efficiency. The comparison in Table 1 indicates that, although many approaches incorporate several of these properties, no single method currently supports all of these capabilities simultaneously. One notable shortcoming of previous data-driven methods is that they are not model-free; i.e. the discovery of dimensionless variables relies on a predefined model structure (e.g. linear regressions, neural networks,.). This can lead to potentially biased results, as the dimensionless variables identified may not be optimal for other models. A detailed account of the challenges in dimensionless learning can be found in the Supplementary Materials.

---

### Abstract representations of events arise from mental errors in learning and memory [^5bb6e0c1]. Nature Communications (2020). High credibility.

Predicting the behavior of individual humans

To model the behavior of individual subjects, we relate the transition estimates in Eqs. (3)–(5) to predictions about people's reaction times. Given a sequence of nodes x 1, …, x t −1, we note that the reaction to the next node x t is determined by the expected probability of transitioning from x t −1 to x t calculated at time t − 1, which we denote by. From this internal anticipation a (t), the simplest possible predictionfor a person's reaction time is given by the linear relationship, where the intercept r 0 represents a person's reaction time with zero anticipation and the slope r 1 quantifies the strength of the relationship between a person's reactions and their anticipation in our model.

To estimate the parameters β, r 0, and r 1 that best describe a given individual, we minimize the root mean squared error (RMSE) between their predicted and observed reaction times after regressing out the dependencies on button combination, trial number, and recency (Fig. 1 e, f; Methods). The distributions of the estimated parameters are shown in Fig. 4a–b for random walks and in Fig. 4g–h for Hamiltonian walks. Among the 358 random walk sequences in the modular and lattice graphs (across 286 subjects; Methods), 40 were best described as performing maximum likelihood estimation (β → ∞) and 73 seemed to lack any notion of the transition structure whatsoever (β → 0), while among the remaining 245 sequences, the average inverse temperature was β = 0.30. Meanwhile, among the 120 subjects that responded to Hamiltonian walk sequences, 81 appeared to have a non-trivial value of β, with an average of β = 0.61. Interestingly, these estimates of β roughly correspond to the values for which our model predicts the strongest network effects (Fig. 3 c, d). In the following section, we will compare these values of β, which are estimated indirectly from people's reaction times, with direct measurements of β in an independent memory experiment.

---

### Optimal free descriptions of many-body theories [^f77fa88f]. Nature Communications (2017). Medium credibility.

Methods

Optimization

The optimization to find σ andin (3) is performed by a Monte Carlo basin-hopping strategyusing the Nelder–Mead simplex algorithm for local minimization within basins of the cost function. This global strategy was selected to counteract an observed tendency for local methods to get trapped in local minima. The initial guess for this search is found as follows. The normalization constant E 0 is the lowest entanglement energy of the input entanglement spectrum { E }. We iteratively construct an approximate set of single-particle entanglement energies starting from an empty set. First, we take the lowest remaining level in the spectrum and subtract E 0 to produce a new single-particle level k. Then we remove the many-body levels, which are closest to the new combinatorial levels generated according to (2) by the additional single-particle level. This process is repeated until the input spectrum is exhausted. We can also introduce a truncation of the entanglement spectrum cutting off high entanglement energies, making the construction of the initial guess terminate faster. The minimization of D (σ, σ f) to identify the optimal free model for the Ising Hamiltonian (5) is calculated using a local Nelder–Mead method.

Finite-size scaling

We perform the finite-size scaling according to an ansatz (4). The parameters of the collapse were estimated using the method of ref. From the scaling ansatz (4) and for a trial set of scaling parameters g c, ν and ζ, the scaled values x L = (g − g c) L 1/ ν andare calculated from each unscaled data point. From this collection of scaled data points (x L, y L) across all L, we implicitly define a so-called master curve that best represents them. This curve y (x) is defined around a point x as the linear regression calculated by taking the scaled data points immediately left and right of x for each system size L. We characterize the deviation of the scaled points (x L, y L) from the master curve y (x L) using the χ 2 statistic. This measure is used as the cost function for an optimization problem over the scaling parameters g c, ν, ζ and θ, which can be solved using the same techniques as the previous problems.

---

### Mathematical description of q-space in spherical coordinates: exact q-ball imaging [^267854bb]. Magnetic Resonance in Medicine (2009). Low credibility.

Novel methodologies have been recently developed to characterize the microgeometry of neural tissues and porous structures via diffusion MRI data. In line with these previous works, this article provides a detailed mathematical description of q-space in spherical coordinates that helps to highlight the differences and similarities between various related q-space methodologies proposed to date such as q-ball imaging (QBI), diffusion spectrum imaging (DSI), and diffusion orientation transform imaging (DOT). This formulation provides a direct relationship between the orientation distribution function (ODF) and the diffusion data without using any approximation. Under this relationship, the exact ODF can be computed by means of the Radon transform of the radial projection (in q-space) of the diffusion MRI signal. This new methodology, termed exact q-ball imaging (EQBI), was put into practice using an analytical ODF estimation in terms of spherical harmonics that allows obtaining model-free and model-based reconstructions. This work provides a new framework for combining information coming from diffusion data recorded on multiple spherical shells in q-space (hybrid diffusion imaging encoding scheme), which is capable of mapping ODF to a high accuracy. This represents a step toward a more efficient development of diffusion MRI experiments for obtaining better ODF estimates.

---

### Minimum electric-field gradient coil design: theoretical limits and practical guidelines [^070d751c]. Magnetic Resonance in Medicine (2021). Medium credibility.

Force and torque constraints can be added using the known main B 0 field distribution, integrating over the current distribution for each basis function and entering the results into matrix H or C. One row is added for each direction of force or torque, to obtain an overall magnitude constraint. Eddy currents are computed in a similar manner; the step response for time‐dependent eddy currents on the surface of a conducting cylinder — typically the thermal shield in the magnet — is computed at multiple points in space and time. Each point in space and time is entered as a separate inequality constraint equation. It is usually sufficient to constrain eddy currents at time = 0 following a step change in gradient current, reducing the size of the problem.

Finally, and of primary significance to the present work, E‐field constraints can be handled in a similar manner to those for force and torque. At gradient frequencies of interest, the time evolution of E‐fields in human tissues is short, and therefore proportional to instantaneous slew rate and not the detailed waveform shape in time. Thus, given a body model, the E‐field is computed from each gradient coil basis function for unit slew rate at a series of points over the body surface, such as using the methods we have described recently. 17 Calculation at points on the interior of the model is not required, as the maximum magnitude always occurs on the surface of a uniform body model (see Supporting Information). A magnitude constraint is created on the surface by placing a series of rotated inequality constraints on the vector E‐field tangent to the surface. For example, we choose 32 directions at each point spaced over 360º, which constrains the magnitude to within 0.5% of the desired value.

---

### Scalable multiple whole-genome alignment and locally collinear block construction with sibeliaZ [^83b6db3d]. Nature Communications (2020). High credibility.

Box 2 Update-collinear-walks

Input: A sorted set of collinear walks P, a vertex w

Output: Updated set P

1: for edges x ∈ E (G) ending at w not marked as used do

2: Let p ∈ P be a walk such that its b -extension q contains x and pos(end(p)) is maximized ⊳ Find a walk extendable with x

3: if such p exists then

4: Truncate q so that end(q) = x

5: Append p with q ⊳ Lengthen the chain that p forms with p a

6: else

7: Add a new walk consisting of the edge x to P

8: return P

Other considerations

For simplicity of presentation, we have described the algorithm in terms of the ordinary de Bruijn graph; however, it is crucial for running time and memory usage that the graph is compacted first. Informally, the compacted de Bruijn graph replaces each non-branching path with a single edge. Formally, the vertex set of the compacted graph consists of vertices of the regular de Bruijn graph that have at least two outgoing (or ingoing) edges pointing at (incoming from) different vertices. Such vertices are called junctions. Let ℓ = v 1, …, v n be the list of k -mers corresponding to junctions, in the order, they appear in the underlying string s. The edge set of the compacted graph consists of edges { v 1 → v 2, v 2 → v 3, …, v n −1 → v n }. We efficiently construct the compacted graph using our previously published algorithm TwoPaCo.

---

### Measurements of the gravitational constant using two independent methods [^a70ae1bd]. Nature (2018). Excellent credibility.

The Newtonian gravitational constant, G, is one of the most fundamental constants of nature, but we still do not have an accurate value for it. Despite two centuries of experimental effort, the value of G remains the least precisely known of the fundamental constants. A discrepancy of up to 0.05 per cent in recent determinations of G suggests that there may be undiscovered systematic errors in the various existing methods. One way to resolve this issue is to measure G using a number of methods that are unlikely to involve the same systematic effects. Here we report two independent determinations of G using torsion pendulum experiments with the time-of-swing method and the angular-acceleration-feedback method. We obtain G values of 6.674184×10–11 and 6.674484×10–11 cubic metres per kilogram per second squared, with relative standard uncertainties of 11.64 and 11.61 parts per million, respectively. These values have the smallest uncertainties reported until now, and both agree with the latest recommended value within two standard deviations.

---

### Localization by nonlinear phase preparation and k-space trajectory design [^5f825e7a]. Magnetic Resonance in Medicine (2012). Low credibility.

A technique is described to localize MR signals from a target volume using nonlinear pulsed magnetic fields and spatial encoding trajectories designed using local k-space theory. The concept of local k-space is outlined theoretically, and this principle is applied to simulated phantom and cardiac MRI data in the presence of surface and quadrupolar gradient coil phase modulation. Phantom and in vivo human brain images are obtained using a custom, high-performance quadrupolar gradient coil integrated with a whole-body 3-T MRI system to demonstrate target localization using three-dimensional T 2*-weighted spoiled gradient echo, two-dimensional segmented, multiple gradient encoded spin echo, and three-dimensional balanced steady-state free precession acquisitions. This method may provide a practical alternative to selective radiofrequency excitation at ultra-high-field, particularly for steady-state applications where repetition time (TR) must be minimized and when the amount of energy deposited in human tissues is prohibitive. There are several limitations to the approach including the spatial variation in resolution, high frequency aliasing artifacts, and spatial variation in echo times and contrast.

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^37daae40]. Nature Communications (2025). High credibility.

Fig. 2
The matrix-tree theorem.

A Computing the steady-state occupancy π 1 by summing weights over directed spanning trees. Directed spanning trees are subgraphs containing all graph nodes but no cycles, with edges oriented toward a root node. In each directed spanning tree, the input forces make a positive, negative, or zero contribution to the tree weight. The structural vectorsare shown below each tree; these quantities enter into Equation (3) below. B Schematic illustration of the high-dimensional space of feature vectors ψ (i; θ) and χ (i, F). The depicted arrangement of vectors could solve a binary classification problem.

We define the input multiplicityas the number of edges affected per input variable, which we assume to be the same for each input. To focus on the functional way in which the input driving enters the steady-state probabilities, the driving contributions can be factored out in the algebraic expressions for the numerator and denominator of Equation (1). This has been previously been used to make analytical progress for M = D = 1 in, for example, refs. –. This equivalent formulation of Eq. (1) suggests that steady states of Markov jump processes implement a rational polynomial function of exponentiated input variables. Defining, we rewrite the matrix-tree expression for π i for general D and M We use the multi-index, whereis the set of D input labels and each componentof the multi-index runs over the values, to enumerate themonomials. These monomials y μ (F) in Equation (2) combinatorially depend on the different mixtures μ of input driving, representing a net total μ a of signed contributions from the input force F a, μ b such contributions for F b, and so on for each input. The coefficients, which are functions of the parameters θ, are the sums of weights over all directed spanning trees rooted at node i which have the corresponding mixture μ of signed input contributions. The monomial coefficientsthus represent learnable amplitudes of each polynomial basis function y μ (F). The coefficients in the denominator are defined as. Classification will be successful if, for F ρ drawn from class ρ, the coefficientsand monomials y μ (F ρ) are large for the same μ. In the subsequent sections of the paper and in the Supplementary Information we use the formulation in Equation (2) to show how the classification ability of a non-equilibrium Markov processes may be systematically modulated.

---

### Phase informed model for motion and susceptibility [^83489a19]. Human Brain Mapping (2013). Low credibility.

Figure 2
Fit of the PIMMS model to the change of phase data acquired in Experiment 1 for three subjects. Top rows (a, d, g), from left: Example slices through brain of rate of change of phase per degree of rotation about the x and y axes, ∂ϕ/∂θ x and ∂ϕ/∂θ y in radians per degree, rate of change of phase per unit time, ∂ϕ/∂τ in radians per second and the mean phase change C in radians. Middle rows (b, e, h): Head rotation estimated from realignment about the x ‐axis (green solid line) and y ‐axis (pink dashed line) in degrees versus image volume number. Bottom rows (c, f, i): Average phase change (blue dotted line) and the average model fit (red solid line) over all voxels in the brain (in radians) versus image volume number. [Color figure can be viewed in the online issue, which is available at.]

The motion parameters in the middle rows (b, e, h) demonstrate that estimated rotations were between ± 2 and 3° around the x ‐axis and up to ± 0.5° about the y ‐axis. Multiplying these values by the rate of change of phase estimates and scaling using Eq. (6) results in voxel shifts of up to 0.5 voxels (i.e. 1.5 mm) associated with the corresponding head rotations. The rate of change of phase per unit time, ∂ϕ/∂τ is scaled between ± 0.003 radians which corresponds to a voxel shift of around 0.5 voxels (1.5 mm) over the experimental run. One would expect this effect to be spatially homogeneous but some structure can be observed in the rate of change of phase with time map. The mean phase change, C corresponds to a static distortion over the time series of between ± 0.2 voxels. From the bottom plots (c, f, and i) it can be observed that on average over the brain, the estimated change of phase effects range from 0.2 to 0.6 radians which correspond to voxel shifts of 0.1–0.3 mm. The correspondence between the blue dotted and red solid lines illustrate the fit of the PIMMS model to the change of phase averaged over the whole brain.

---

### Confidence reports in decision-making with multiple alternatives violate the Bayesian confidence hypothesis [^b245f51e]. Nature Communications (2020). High credibility.

Inference on a given trial. We assume that the observer knows the mean and standard deviation of each category based on the exemplar dots, and that the observer assumes that the three categories have equal probabilities. The posterior probability of category C given the measurement x is then p (C | x) ∝ p (x | C) = N (x; m C, (σ s 2 + σ 2) I). Instead of the true posterior p (C | x), the observer makes the decisions based on q (C | x), a noisy version of the posterior probability. We obtain a noisy posterior q (C | x) by drawing from a Dirichlet distribution. The Dirichlet distribution is a generalization of the beta distribution. Just like the beta distribution is a continuous distribution over the probability parameter of a Bernoulli random variable, the Dirichlet distribution is a distribution over a vector that represents the probabilities of any number of categories. The Dirichlet distribution is parameterized as

Γ represents the gamma function. p is a vector consisting of the three posterior probabilities, p = (p 1, p 2, p 3) = (p (C = 1 | x), p (C = 2 | x), p (C = 3 | x)). q is a vector consisting of the three posterior probabilities perturbed by decision noise, q = (q 1, q 2, q 3) = (q (C = 1 | x), q (C = 2 | x), q (C = 3 | x)). The expected value of q is p. The concentration parameter α is a scalar whose inverse determines the magnitude of the decision noise; as α increases, the variance of q decreases. To make a category decision, the observer chooses the category that maximizes the posterior probability:

---

### Haplotype probabilities in advanced intercross populations [^b1573efc]. G3 (2012). Low credibility.

The recurrence relation for the p s is like that in equation (1): p s +1 = (1 − r) p s + r /64. The solution is

Note that the recombinant haplotypes are all equally likely, due to the random order of the initial crosses, and so each has probability (1 − 8 p s)/56.

HS corresponds to the DO with α 1 = 1 (that is, k ≡ 1), in which case p 1 = q 2 = 7 − 24 r + 24 r 2 − 8 r 3.

I now turn to the X-chromosome. Let m s and f s denote the frequency of the AA haplotype on the X chromosome in males and females in the DO at generation s. Assuming random orders of crosses to generate the pre-CC progenitors, whereandare the frequencies of the AA and CC haplotypes, respectively, on the X-chromosome in females at generation G 1: F k +1 in the construction of four-way RIL by sibling mating (see, Table 4). m 1 is calculated in the same way. The recurrence relations are much like equation (3):

The solutions are the following:where w, y, and z are as in equation (4).

Again, HS corresponds to DO with α 1 = 1, in which case f 1 = (4 − 5 r + r 2)/32 and, m 1 = (2 − 3 r + r 2)/16.

In Figure 1, the probabilities of recombinant two-locus haplotypes are displayed for the different populations. For the DO, I used the distribution of k as in Figure 1 ofand s = 5. For HS and AIL, I used s = 10 and 12, respectively, to match the total number of generations with recombination — the average k inwas six. Recombinant haplotypes are more frequent on the autosome, and are more frequent in HS than in the DO; inbreeding in the pre-CC progenitors of the DO is accompanied by a loss of recombinants.

---

### Characteristic rotational behaviors of rod-shaped cargo revealed by automated five-dimensional single particle tracking [^ec77546c]. Nature Communications (2017). Medium credibility.

Fig. 1
Principle and localization precision of the parallax-DIC microscope. a Schematic illustration of a gold nanorod placed in the 3D coordinate system. The x - and y -axes are set according to the polarization directions of the illumination light. The dipole shown here corresponds to the long axis of the gold nanorod, whose centroid locates at (x, y, z). The azimuthal angle and the elevation angle of the longitudinal axis of the gold nanorod are presented as ϕ and ψ, respectively. b Bright (left), dark (middle), and half-bright/half-dark (right) images of a gold nanorod (40 × 80 nm) at different orientations and vertical positions. The half-plane images are aligned by the nanorod's center of mass at the focal plane (z = 0 μm). The yellow dashed lines indicate the z -position of the center of the gold nanorod in focus. Scale bar is 1 μm. c Calibration of the 5D-SPT technique. The distance between the two half-plane images changes linearly with the z -position of the gold nanorod within ± 0.5 μm of the focal plane. The distance for an in-focus gold nanorod is defined as d 0. The error bars reflect the standard deviations of the distances measured when the nanorod's azimuthal angle ϕ changes in the range of 0–180° with 5° steps (error bar ± s.d. n = 36). The calibration curve returned a slope of +0.626 (d /Δ z) and R 2 of 0.997. d Three-dimensional localization distribution of the gold nanorod with standard deviations of 11 nm in x, 14 nm in y, and 17 nm in z. e The in-focus half-plane image patterns of a gold nanorod at different azimuthal angles when the sample slide is rotated in 5° steps for 180°. The scale bar represents 1 μm. f The normalized bright part (blue) and dark part (brown) intensities of the half-plane images on the top in each pair in E

---

### Non-canonical wnt signaling promotes directed migration of intestinal stem cells to sites of injury [^3206081a]. Nature Communications (2021). High credibility.

Analysis of migration directionality

To assay ISC accumulation around the periphery of the wound in fixed, ablated intestines (Supplementary 2c), the ISC numbers within a 40 µm radius of the ablation site were counted and compared with the ISC numbers within the same area of the contralateral side of the intestine. ISCs within the ablation site area on the contralateral side were not counted. For undamaged controls, ISCs were counted on contralateral sides of the intestine in a 200 µm × 200 µm area. To quantify the angle of ISC protrusions in relation to the wound of an ablated gut (Supplementary Fig 2a), the angle was measured between the vector of the shortest distance from the center of the cell body to the periphery of the wound and the vector from the center of the cell body to the center of the leading edge. To plot ISC position in ablated tissue (Supplementary 2b), the periphery of the wound closest to a given ISC was designated as the center of the Cartesian plane (0,0). The starting position of the ISC was the x, y coordinate of the cell cortex closest to the wound immediately after ablation, and normalized to the x, y coordinate of the wound periphery closest to the ISC. The ending position of the ISC was the x, y coordinate of the cell cortex closest to the wound 4.5 hrs post-ablation, and normalized to the x, y coordinate of the wound periphery closest to the ISC. For example, if the x, y coordinate of an ISC was measured at 108, 148 and the wound was measured at 133, 148, the wound would be set at 0,0 and the normalized x, y coordinate of the ISC would be −25, 0. In control, unablated tissue, a single arbitrary point in the middle of the gut was designated as the center of the Cartesian plane (0,0). ISC position was determined by the x, y coordinate of the cell cortex closest to the designated point, and normalized to the x, y coordinate of the designated point. Change in displacement was calculated from the difference in distance between the starting ISC position and the Cartesian origin, and the ending ISC position and the Cartesian origin.

---

### Inflection points in magnetic resonance imaging technology-35 years of collaborative research and development [^f69d5e02]. Investigative Radiology (2015). Low credibility.

The technology for clinical magnetic resonance imaging (MRI) has advanced with remarkable speed and in such a manner reflecting the influence of 3 forces-collaboration between disciplines, collaboration between academia and industry, and the enabling of software applications by hardware. The forces are evident in the key developments from the past and emerging trends for the future highlighted in this review article. These developments are associated with MRI system attributes, such as wider, shorter, and stronger magnets; specialty magnets and hybrid devices; k space; and the notion that magnetic field gradients perform a Fourier transform on the spatial distribution of magnetization, phased-array coils and parallel imaging, the user interface, the wide range of contrast possible, and applications that exploit motion-induced phase shifts. An attempt is made to show connections between these developments and how the 3 forces mentioned previously will continue to shape the technology used so productively in clinical MRI.

---

### The evolution of the temporal program of genome replication [^c3a4a83b]. Nature Communications (2018). Medium credibility.

Identification of replication origins

The mean replication times and MFA profiles were plotted as a function of the chromosomal coordinates. The first derivative of these curves was estimated by calculating the slope of each coordinate x of the window i along the genome, using the following formula (y i – y i − 1)/(x i – x i − 1), as described in ref. The second derivative was then estimated from the first derivative values using the same method and plotted as a function of the chromosomal coordinates. Both peaks and shoulders from the original mean replication time and MFA curves appear as clear peaks on the second derivative plot. The chromosomal location of these peaks was determined where the slope curves corresponding to the third derivatives intersect 0 from negative to positive value as described in ref. We defined peak coordinates as the location of active replication origins only when peaks were co-detected in both the mean replication time and the MFA second derivatives profiles. Then, for each replication origin location, we compared its two chromosomal coordinates issued from the two curves to estimate the precision of our experiments in calling origin locations. We found a median and a mean difference between the two peak coordinates of 4.0 and 4.9 kb, respectively. All origin locations are available in the Supplementary Data 1.

---

### Electrocardiographic repolarization-related variables as predictors of coronary heart disease death in the women's health initiative study [^ee62cdf9]. Journal of the American Heart Association (2014). Low credibility.

The conceptual model used to derive RT subintervals and other model parameters for the present study has been described in detail in previous publications. A simplified summary description of the main model variables in nonstatistical terms is contained in Table 6. In more basic terms, RT of the subepicardial myocyte layers (RT epi), 1 of the main repolarization model parameters, is considered to represent RT of left ventricular (LV) myocytes at the time of global T‐wave peak (T p) when the majority of LV lateral wall myocytes are at some point of phase 3 of their action potential. RT epi is computed as a function of QT pa whereby RT epi = QT pa −(1−CosѲ(T p |T ref)×(T p T xd))/2, where Ѳ(T p |T ref) is the spatial angle between the T p vector and T ref is the reference normal T p vector with xyz components (0.75, 0.57, −0.33). T p T xd, in turn, is the interval from T p to T xd, where T xd is the inflexion point (the steepest negative slope) at global T wave downstroke. Thus, RT epi is obtained from QT pa by modifying it by the degree of deviation of direction of the initial repolarization from the direction of normal repolarization. RT at time point T xd (RT xd) is obtained with an algorithm similar to that for RT epi, whereby RT xd = QT pa −(1+CosѲ(T p |T ref)×(T p T xd))/2. In addition to Ѳ(T p |T ref) noted above, a number of other spatial angles between various QRS and T vectors and other interval and amplitude variables were used in various phases of the study.

Ascertainment of Outcome Events

After baseline, deaths and hospitalization events were ascertained in each clinical center by annual follow‐up calls, review of vital records, and community surveillance of hospitalized and fatal events. Detailed definitions for criteria for CHD death classification have been published previously. Briefly, CHD deaths included death with no known non‐CHD death and either a history of chest pain within 72 hours before death or a history of chronic ischemic heart disease in the absence of valvular heart disease. The average follow‐up was 15.9 years (up to 17 years).

---

### Rigorous location of phase transitions in hard optimization problems [^d98ebea5]. Nature (2005). Excellent credibility.

It is widely believed that for many optimization problems, no algorithm is substantially more efficient than exhaustive search. This means that finding optimal solutions for many practical problems is completely beyond any current or projected computational capacity. To understand the origin of this extreme 'hardness', computer scientists, mathematicians and physicists have been investigating for two decades a connection between computational complexity and phase transitions in random instances of constraint satisfaction problems. Here we present a mathematically rigorous method for locating such phase transitions. Our method works by analysing the distribution of distances between pairs of solutions as constraints are added. By identifying critical behaviour in the evolution of this distribution, we can pinpoint the threshold location for a number of problems, including the two most-studied ones: random k-SAT and random graph colouring. Our results prove that the heuristic predictions of statistical physics in this context are essentially correct. Moreover, we establish that random instances of constraint satisfaction problems have solutions well beyond the reach of any analysed algorithm.

---

### Abstract representations of events arise from mental errors in learning and memory [^12f8f0f1]. Nature Communications (2020). High credibility.

In combination, our experimental observations indicate that people are sensitive to the higher-order architecture of transition networks. But how do people infer abstract features like community structure from sequences of stimuli? In what follows, we turn to the free energy principle to show that a possible answer lies in understanding the subtle role of mental errors.

Network effects reveal errors in graph learning

As humans observe a sequence of stimuli or events, they construct an internal representationof the transition structure, whererepresents the expected probability of transitioning from node i to node j. Given a running tally n ij of the number of times each transition has occurred, one might naively expect that the human brain is optimized to learn the true transition structure as accurately as possible. This common hypothesis is represented by the maximum likelihood estimate, taking the simple form

To see that human behavior does not reflect maximum likelihood estimation, we note that Eq. (1) provides an unbiased estimate of the transition structure; that is, the estimated transition probabilities inare evenly distributed about their true value 0.25, independent of the higher-order transition structure. Thus, the fact that people's reaction times depend systematically on abstract features of the network marks a clear deviation from maximum likelihood estimation. To understand how higher-order network structure impacts people's internal representations, we must delve deeper into the learning process itself.

Consider a sequence of nodes (x 1, x 2), where x t ∈ {1, …, N } represents the node observed at time t and N is the size of the network (here N = 15 for all graphs). To update the maximum likelihood estimate of the transition structure at time t + 1, one increments the counts n ij using the following recursive rule, where the Iverson bracketif its argument is true and 0 otherwise. Importantly, we note that at each time t + 1, a person must recall the previous node that occurred at time t; in other words, they must associate a cause x t to each effect x t +1 that they observe. Although maximum likelihood estimation requires perfect recollection of the previous node at each step, human errors in perception and recall are inevitable –. A more plausible scenario is that, when attempting to recall the node at time t, a person instead remembers the node at time t − Δ t with some decreasing probability P (Δ t), where Δ t ≥ 0. This memory distribution, in turn, generates an internal belief about which node occurred at time t

---

### KDIGO 2024 clinical practice guideline for the evaluation and management of chronic kidney disease [^34c1b44c]. Kidney International (2024). High credibility.

Early-stage chronic kidney disease risk prediction applicability and progression example — Practice Point 2.2.4 notes that risk prediction equations developed for use in people with CKD G3–G5, may not be valid for use in those with CKD G1–G2. The Work Group recognizes that in earlier stages (G1–G3), large declines in eGFR can occur in 2- to 5-year time frames without reaching kidney failure. Figure 16 illustrates this with a CKD G1–G2 profile in which kidney failure risk is 0.07% over 2 years and 0.32% over 5 years versus CKD progression risk of 10.4% over 3 years, for a 50-year-old male with diabetes with eGFR 80 ml/min per 1.73 m 2 and urine ACR 1 g/g.

---

### European Stroke Organisation (ESO) and European Association of Neurosurgical Societies (EANS) guideline on stroke due to spontaneous intracerebral haemorrhage [^a24b784e]. European Stroke Journal (2025). High credibility.

Regarding diagnostic investigations for intracerebral hemorrhage, more specifically with respect to initial evaluation, EANS/ESO 2025 guidelines recommend to consider using algorithms such as the DIAGRAM for targeted investigation of the cause of spontaneous ICH to improve the performance of prediction regarding the underlying cause, compared to standard care.

---

### Predicting the direction of phenotypic difference [^5dd2569a]. Nature Communications (2025). High credibility.

Fig. 2
Evaluating prediction accuracy using the known-to-total ratio (κ).

a Simulated prediction accuracies for various κ values (grouped into equally spaced bins), for different proportions of the known vs. unknown effects (10%, 50%, and 90% of effects known). Effect sizes were drawn from a normal distribution. In gray is the theoretical expectation from Eq. (4). The light blue line shows values only for κ ≤ 0.6 because there were insufficient simulations with higher κ values (< 50 for each data point) to determine prediction accuracy. Error bars show 95% confidence interval, and only data points with error bars > 0.01 are shown. b The distribution of κ values for the case where the known effects are randomly sampled. The vertical line denotes the κ values required for prediction accuracy of P > 0.95 (κ = 0.62; Eq. 2). c The effect of predicting phenotypic direction above a threshold c of phenotypic difference (Eq. (7)). For each curve, We simulated 10 7 pairs of polygenic scores, each distributed normally with zero mean and variance of r 2 = 0.1. The y-axis shows the probability that y 1 > y 2 + c (y 1 is the trait value of the individual with the higher score) vs the score difference d (x-axis). Circles show simulation results and lines show theory based on Eq. (7). Each curve (different colors) corresponds to a different value of the gap c (legend). Units are in standard deviation of the phenotypic values. d The distribution of κ values for the case where the known effects are those with the largest effects. The vertical line denote the κ values required for prediction accuracy of P > 0.95. In panels (a, b, d) 10,000 effect sizes were drawn from a standard normal distribution to represent the known and unknown effects on the phenotype.

---

### The force-velocity profile for jumping: what It is and what It is not [^c97562eb]. Medicine and Science in Sports and Exercise (2023). Medium credibility.

Model A2: a mass projected by a nonlinearly damped force

Model A2 (Fig. 4 A) was similar to model A1 but had a hyperbolic force–velocity relationship rather than a linear one. The relationship is shown in Figure 4 B, together with the force–velocity profile. For the five jumps that were used to construct the profile, we took the effective work and determined the isokinetic velocity v* at which the same amount of effective work could be produced; these are the data points shown on the intrinsic force–velocity relationship in Figure 4 B. Because the intrinsic force–velocity relationship of the model is nonlinear, v* is no longer the average of velocity over distance, as it was in model A1. Just as the data points of the force–velocity profile, the v* data points seem to fall on a straight line that can be extrapolated to calculate values forand, but this is a meaningless operation because the intrinsic relationship is nonlinear. The trueandcan only be found by fitting a hyperbola to the data points, which can successfully be done if we know the underlying relationship, but is doomed to fail if we need to determine and extrapolate the relationship using noisy experimental data. Because the straight line is fitted to only a small section of the nonlinear relationship, andare no longer independent; if we reduce the force of the intrinsic relationship and keep the maximal velocity constant, decreases where it should not (lowermost dashed lines in Fig. 4 B).

FIGURE 4
Model A2 and its force–velocity profile. A, Model A2 consists of a mass projected by a nonlinearly damped force (inset). Abbreviations as in Figure 2. The intrinsic force–velocity relationship of the actuator was (F + a) (v + b) = b (F 0 + a), where F 0 is isometric force (4000 N), and a (1600 N) and b (2.2 m·s −1) are constants. B, Force–velocity profile of the model obtained by plotting effective work per unit push-off distance as a function of half the takeoff velocity (see Fig. 1). The force–velocity profile differs from the intrinsic relationship of the model. The latter is obtained by having the model extend isokinetically at different velocities. If the maximal force of the intrinsic relationship is reduced but the maximal velocity is kept constant, the maximal velocity of the force–velocity profile is reduced (lowermost dashed lines).

---

### Electrocardiographic predictors of coronary heart disease and sudden cardiac deaths in men and women free from cardiovascular disease in the atherosclerosis risk in communities study [^9015ae5b]. Journal of the American Heart Association (2013). Low credibility.

Figure 1.
ST‐T vector magnitude curve with temporal landmarks and measurement points for key intervals and amplitudes in the repolarization model. The peak of the global T wave (T p) coincides with the rate‐adjusted QTpeak (QT pa). The time point of Tonset (T o) corresponding to the rate‐adjusted QTonset interval (QT oa) is obtained by extrapolating the line from the time of maximum slope at T‐wave upstroke (T xc) to the intersection with the horizontal line drawn from the minimum ST after the J‐point. The time of the minimum slope at the T‐wave downstroke (T xd) defines the end point of the T p T xd interval conceived in the repolarization model to represent initial left ventricular repolarization time (RT) dispersion. Double arrow signifies ST segment convexity as the difference of the ST segment from the gradient line from the ST onset to T p.

In addition to θ(T p |T ref), a number of other spatial angles reflecting deviations of the direction of repolarization from the reference normal direction during various repolarization subintervals and other repolarization‐related interval and amplitude variables were used in various phases of the study. Their definitions are listed in the footnotes of the corresponding tables.

---

### Diagnosis, management and treatment of the Alport syndrome-2024 guideline on behalf of ERKNet, ERA and ESPN [^45eede36]. Nephrology, Dialysis, Transplantation (2025). High credibility.

Regarding diagnostic investigations for Alport syndrome, more specifically with respect to hearing assessment, ERA/ERN ERKNet/ESPN 2025 guidelines recommend to consider obtaining a hearing evaluation at diagnosis or upon reaching adulthood in female patients with X-linked alport syndrome, and then every 5 years in the absence of hearing loss symptoms.

---

### A continuous-time maxSAT solver with high analog performance [^3d122302]. Nature Communications (2018). Medium credibility.

Many real-life optimization problems can be formulated in Boolean logic as MaxSAT, a class of problems where the task is finding Boolean assignments to variables satisfying the maximum number of logical constraints. Since MaxSAT is NP-hard, no algorithm is known to efficiently solve these problems. Here we present a continuous-time analog solver for MaxSAT and show that the scaling of the escape rate, an invariant of the solver's dynamics, can predict the maximum number of satisfiable constraints, often well before finding the optimal assignment. Simulating the solver, we illustrate its performance on MaxSAT competition problems, then apply it to two-color Ramsey number R(m, m) problems. Although it finds colorings without monochromatic 5-cliques of complete graphs on N ≤ 42 vertices, the best coloring for N = 43 has two monochromatic 5-cliques, supporting the conjecture that R(5, 5) = 43. This approach shows the potential of continuous-time analog dynamical systems as algorithms for discrete optimization.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^7e0b2d1d]. Annals of the American Thoracic Society (2023). High credibility.

Basic stereological measurements for lung imaging — first-order parameters, probes, and reference space are specified. First-order parameters are volume (three-dimensional [3D]), surface (two-dimensional [2D]), length (one-dimensional [1D]), and number (zero-dimensional [0D]). Measurements are performed using geometric test probes, such as points (0D), lines (1D), planes (2D), or volumes (3D), with probe–voxel interactions generating countable events; raw counts yield ratios that are multiplied by the reference space volume to obtain absolute lung or subcompartment measures. It is crucial to define and measure a biologically meaningful reference space for analysis and reporting, because measurements expressed only as ratios are subject to "the reference trap", where changes can arise from the numerator, denominator, or both; an efficient volume measurement method, point counting, is used for acinar components by micro-CT and for extrapulmonary organ volume by CT.

---

### DNA replication origins retain mobile licensing proteins [^a956c16f]. Nature Communications (2021). High credibility.

Localization distributions and offset correction

The HtH origin is located 6.7 kbp from one end of the 21.2 kbp DNA tether, but the tether can be oriented in either of two ways between the beads. To determine the occupancy of the original sequence, we used a radial localization histogram that plots the particle positions as a function of distance from the center of the DNA. We performed this mapping in two steps: first, subtraction of the horizontal offset in DNA position owing to the beads at the edges of the image, and averaging of histogram bins at equal distances from the center on opposite sides.

To determine the offset, which is caused by both the variability of the microsphere diameter (± 0.2 µm) and slight variations in the position of the scan region relative to the tracked microspheres, we first plotted the raw localization distribution by taking the mean of the first five position measurements of each particle on each DNA molecule in the data set. In data sets with a symmetrical two-peaked localization distribution, corresponding to an enriched population at the origin (right of center on 50% of DNA molecules, and left of center on the other 50% of DNA molecules), we selected the offset that maximizes the autocorrelation of the two peaks. In data sets for which there is no clear bimodal distribution, we used microsphere position metadata exported directly from the instrument. Since the position value exported by the system depends on a user-defined tracking region, there is a possibility of systematic bias, hence our preference for the autocorrelation method when possible. However, in virtually all cases the offset needed to align the 2D scans, including those taken months apart, was in a range of 0.4 ± 0.3 µm. In the data sets collected on a mutated HtH origin in Fig. 1 and Supplementary Fig. 5, no metadata was exported, so we performed a manual offset correction using offset value 0.4 µm.

---

### Interpreting change in quantitative imaging biomarkers [^20a80f73]. Academic Radiology (2018). Low credibility.

Rationale and Objectives

Quantitative imaging biomarkers (QIBs) are becoming increasingly adopted into clinical practice to monitor changes in patients' conditions. The repeatability coefficient (RC) is the clinical cut-point used to discern between changes in a biomarker's measurements due to measurement error and changes that exceed measurement error, thus indicating real change in the patient. Imaging biomarkers have characteristics that make them difficult for estimating the repeatability coefficient, including nonconstant error, non-Gaussian distributions, and measurement error that must be estimated from small studies.

Methods

We conducted a Monte Carlo simulation study to investigate how well three statistical methods for estimating the repeatability coefficient perform under five settings common for QIBs.

Results

When the measurement error is constant and replicates are normally distributed, all of the statistical methods perform well. When the measurement error is proportional to the true value, approaches that use the log transformation or coefficient of variation perform similarly. For other common settings, none of the methods for estimating the repeatability coefficient perform adequately.

Conclusion

Many of the common approaches to estimating the repeatability coefficient perform well for only limited scenarios. The optimal approach depends strongly on the pattern of the within-subject variability; thus, a precision profile is critical in evaluating the technical performance of QIBs. Asymmetric bounds for detecting regression vs progression can be implemented and should be used when clinically appropriate.

---

### Folic acid [^98c37733]. FDA (2025). Medium credibility.

The dosage of folic acid PO for treatment of anemia of nutritional origin in adults is 0.4–1 mg PO daily

---

### Use of endocrine therapy for breast cancer risk reduction: ASCO clinical practice guideline update [^247375b3]. Journal of Clinical Oncology (2019). High credibility.

Regarding preventative measures for breast cancer, more specifically with respect to chemoprophylaxis, ASCO 2019 guidelines recommend to offer exemestane as an alternative to tamoxifen and/or raloxifene to reduce the risk of invasive breast cancer, specifically ER+ breast cancer, in postmenopausal women ≥ 35 years of age with a 5-year projected absolute breast cancer risk ≥ 1.66% or with lobular carcinoma in situ or atypical hyperplasia.

---

### Obesity trajectories and risk of dementia: 28 years of follow-up in the whitehall II study [^ca992594]. Alzheimer's & Dementia (2018). Low credibility.

3.2 Trajectories of adiposity before dementia

The trajectory of BMI for more than 28 years was different (P < .0001) in those with dementia compared with all nondemented participants (Fig. 1 A), the differences in BMI between these two groups are presented in Table 3. These results, using a backward timescale, showed that those with dementia had higher BMI in midlife and accelerated decline in the decade before dementia diagnosis. Analyses with the backward timescale were repeated using a case-control design (Fig. 1 B). These analyses also show BMI trajectories to be different in cases and control subjects (P < .0001). In cases BMI was higher in midlife and they experienced accelerated decline in BMI in the years before dementia. BMI in cases was significantly higher from year −28 (0.79, P = 0.001) to year −16 (0.47 kg/m 2, P = 0.05), starting from year −8 BMI was lower in cases than in control subjects (Table 3).

Fig. 1
Trajectories of BMI in the 28 years before dementia. (1) The figure represents marginal effects of dementia on trajectories of BMI, adjusted for age, sex, and education and 5-year birth cohort in panel (A). Mean number of observations per participant is 3.3 for dementia cases and 4.1 for those dementia free for approach (A) and 4.4 for control subjects for approach (B). (2) The 95% confidence intervals are presented for alternate time points for ease of presentation.

Table 3
Differences in BMI between cases and the others and between cases and control subjects for more than the 28 years before dementia ∗

Fig. 2 (Panel A) shows waist circumference trajectories for more than the 22 years to differ in cases and control subjects (P < .0001); yearly differences in waist circumference are tabulated in Supplementary Table 4. Waist circumference in those with dementia was higher from year −22 (1.64 cm, P = 0.04) to year −16 (1.46, P = 0.04) and lower starting 8 years before dementia. Fig. 2 (Panel B) shows a similar pattern of results for waist-to-hip ratio (Supplementary Table 4 provides yearly differences), with the trajectories in those with dementia being different from the control subjects (P = 0.0009) and rapid decline close to dementia diagnosis.

---

### Cryptic transmission of SARS-CoV-2 and the first COVID-19 wave [^159f016b]. Nature (2021). Excellent credibility.

SARS-CoV-2 introductions

As the model allows the recording of the origin and destination of travellers carrying SARS-CoV-2 at the global scale, we can study the possible sources of SARS-CoV-2 introductions for each US state and European country. More specifically, we record the cumulative number of introductions in each stochastic realization of the model until 30 April 2020. In Fig. 3 we visualize the origin of the introductions considering some key geographical regions (for example, Europe and Asia) while keeping the USA and China separate and aggregating all of the other countries (Others). For both the USA and Europe, the contribution from mainland China is barely visible and the local share (that is, sources within Europe and the USA) becomes significantly higher across the board. Hence, while introduction events in the early phases of the outbreak were key to start local spreading (see details in the Supplementary Information), the cryptic transmission phase was sustained largely by internal flows. Domestic SARS-CoV-2 introductions to 30 April 2020 account for 69% (interquartile range (IQR) 60%−81%) of the introductions in CA, 78% (IQR 71%−87%) in TX and 69% (IQR 60%−80%) in MA, which is supported by phylogenetic analysis. European origins account for 69% (IQR 60%−80%), 84% (IQR 79%−91%) and 58% (IQR 48%−68%) of the introductions in Italy, Spain and the UK, respectively. In the Supplementary Information, we report the full breakdown for all states and countries.

---

### Standards of care in diabetes – 2025 [^ef149bc3]. Diabetes Care (2025). High credibility.

Regarding specific circumstances for diabetic nephropathy, more specifically with respect to pediatric patients, ADA 2025 guidelines recommend to determine the eGFR at the time of diagnosis and annually thereafter.

---

### Modelling sequences and temporal networks with dynamic community structures [^7129d625]. Nature Communications (2017). Medium credibility.

Bayesian Markov chains with communities

As described in the main text, a Bayesian formulation of the Markov model consists in specifying prior probabilities for the model parameters, and integrating over them. In doing so, we convert the problem from one of parametric inference where the model parameters need to be specified before inference, to a nonparametric one where no parameters need to be specified before inference. In this way, the approach possesses intrinsic regularisation, where the order of the model can be inferred from data alone, without overfitting.

To accomplish this, we rewrite the model likelihood, using eqs. (1) and (5), asand observe the normalisation constraints,… Since this is just a product of multinomials, we can choose conjugate Dirichlet priors probability densitiesand, which allows us to exactly compute the integrated likelihood, whereand. We recover the Bayesian version of the common Markov chain formulation (see ref.) if we put each memory and token in their own groups. This remains a parametric distribution, since we need to specify the hyperparameters. However, in the absence of prior information it is more appropriate to make a noninformative choice that encodes our a priori lack of knowledge or preference towards any particular model, which amounts to choosing α x = β rs = 1, making the prior distributions flat. If we substitute these values in eq. (19), and re-arrange the terms, we can show that it can be written as the following combination of conditional likelihoods, wherewithbeing the multiset coefficient, that counts the number of m -combinations with repetitions from a set of size n. The expression above has the following combinatorial interpretation: P ({ x t }| b, { e rs }, { k x }) corresponds to the likelihood of a microcanonical modelwhere a random sequence { x t } is produced with exactly e rs total transitions between groups r and s, and with each token x occurring exactly k x times. In order to see this, consider a chain where there are only e rs transitions in total between token group r and memory group s, and each token x occurs exactly k x times. For the first transition in the chain, from a memory x 0 in group s to a token x 1 in group r, we have the probabilityNow, for the second transition from memory x 1 in group t to a token x 2 in group u, we have the probabilityProceeding recursively, the final likelihood for the entire chain iswhich is identical to eq. (21).

---

### The long lives of primates and the 'invariant rate of ageing' hypothesis [^69b8ebf4]. Nature Communications (2021). High credibility.

Fig. 1
The life expectancy–lifespan equality landscape for seven genera of primates for for both sexes.

a Life expectancy and lifespan equality regression lines for females; each species is represented by a different colour. b Life expectancy and lifespan equality regression lines for males. Each genus is characterised by a relatively constrained relationship between life expectancy and lifespan equality, and thus a distinct regression line; colours as in a. The central lines are the predicted fitted values of the regression and the type of line (e.g. continuous, dashed, or dotted) depicts three levels for the p values of the slopes (how significantly different from 0 they are, two-sided t test, H 0: β 1 = 0, Supplementary Table 1), while the shaded polygons show the 95% confidence intervals of the regressions. c The relationship between the Siler mortality parameters and the resulting mortality function, given by the equation μ (x) = exp(a 0 – a 1 x) + c + exp(b 0 + b 1 x), where infant and juvenile mortality (blue) are controlled by parameters a 0 and a 1, age-independent mortality (orange) is captured by c, and senescent mortality (green) is captured by b 0 (initial adult mortality) and b 1 (rate of ageing). d Each box shows how gradual changes in each Siler mortality parameter modify the life expectancy and lifespan equality values (thick purple lines). The green line in each box corresponds to the regression line for female chimpanzees, shown for reference to illustrate the general trends among all genus lines. The purple curves show the changes in life expectancy and lifespan equality after varying individual Siler parameters while holding the other parameters constant. Note the striking change in life expectancy and lifespan equality that would result from changes in the ageing parameters, particularly b 1. See Supplementary Fig. S3 for plots that include individual points for each population. Source data to generate the regression lines are available in Supplementary Data 3.

---

### Evaluating the evidence for exponential quantum advantage in ground-state quantum chemistry [^03db0a64]. Nature Communications (2023). High credibility.

Due to intense interest in the potential applications of quantum computing, it is critical to understand the basis for potential exponential quantum advantage in quantum chemistry. Here we gather the evidence for this case in the most common task in quantum chemistry, namely, ground-state energy estimation, for generic chemical problems where heuristic quantum state preparation might be assumed to be efficient. The availability of exponential quantum advantage then centers on whether features of the physical problem that enable efficient heuristic quantum state preparation also enable efficient solution by classical heuristics. Through numerical studies of quantum state preparation and empirical complexity analysis (including the error scaling) of classical heuristics, in both ab initio and model Hamiltonian settings, we conclude that evidence for such an exponential advantage across chemical space has yet to be found. While quantum computers may still prove useful for ground-state quantum chemistry through polynomial speedups, it may be prudent to assume exponential speedups are not generically available for this problem.

---

### Syrian national growth references for children and adolescents aged 2–20 years [^d07042f4]. BMC Pediatrics (2022). Medium credibility.

Background

During the past three decades, growth charts have become one of the principal tools for monitoring anthropometric development in individuals and populations as well. Growth references by the CDC and other countries have been widely used in our hospitals and healthcare units for clinical assessment of children's development. The apparent overestimation and underestimation of many children's anthropometrics indicated the need to construct our own references. The objective of this study is to establish the national growth references for the Syrian population 2-20-year-old.

Methods

A multicenter cross-sectional sample of 13,548 subjects, aged 2–20 years, were recruited from various kindergartens, schools, and universities across the Syrian Arab Republic between February and May-2019. Response variables (stature, weight, and BMI) were fitted against age using P-splines and three empirical distributions: Box-Cox T, Box-Cox Power Exponential, and Box-Cox Cole and Green. Residuals diagnostic Q-tests and worm plots were used to check the validity of fitted models.

Results

Box-Cox T provided the best fit for stature-for-age, whereas Box-Cox Power Exponential provided the best fit for weight-for-age and BMI-for-age. Residuals diagnostics revealed adequate models fitting. BMI cutoffs revealed an increased prevalence of obesity (4.5% and 3.66%) and overweight (20.1% and 19.54%), for boys and girls respectively, in our population.

Conclusions

Growth charts are available for use now in our hospitals and healthcare units. For 0-2-year-old children, we recommend using the World Health Organization's standards.

---

### Standards of care in diabetes – 2025 [^31be65a6]. Diabetes Care (2025). High credibility.

Regarding diagnostic investigations for hypoglycemia, more specifically with respect to screening for cognitive impairment, ADA 2025 guidelines recommend to simplify diabetes treatment plans as much as possible for patients with cognitive impairment and tailor to minimize the risk of hypoglycemia.

---

### Hippocampal firing fields anchored to a moving object predict homing direction during path-integration-based behavior [^6b5ff16b]. Nature Communications (2023). High credibility.

Analysis of cell activity around the lever box

The distance from the lever was defined as the distance between the center of the animal's head to the closest wall of the lever box. To analyze the firing rate of the neurons around the variably located lever box, we transformed the animal position into a lever-centered coordinate system by subtracting the coordinates of the lever box from the position of the mouse. Each position of the mouse can be seen as a 2D vector originating at 0,0 and pointing towards the direction of the mouse around the lever. Data from when the animal was more than 12 cm away from the lever box were excluded from the analysis.

We considered three possible directional reference frames in which the neurons could encode directional information (Fig. 5b). In the Cardinal reference frame, the direction of the animal around the lever box was measured from a vector pointing south. The original lever-centered data were in the Cardinal reference frame. In the Bridge reference frame, the direction of the mouse was measured from a vector originating at the center of the lever box (0,0) and pointing toward the center of the bridge. To transform the data from the Cardinal to the Bridge reference frame, we calculated the angle of the vector originating at the center of the lever box (0,0) and pointing toward the bridge relative to a vector pointing south (0,-1). We then rotated the Cardinal lever-centered mouse position data by this angle. In the Lever reference frame, the direction of the mouse is measured from a vector originating at the lever box center and pointing towards the lever. To transform the lever-centered data from the Cardinal to the Lever reference frame, we calculated the angle of the vector originating at the center of the lever box (0,0) and pointing towards the lever relative to a vector pointing south. We then rotated the Cardinal lever-centered mouse position data by this angle.

---

### Systematic errors in connectivity inferred from activity in strongly recurrent networks [^f2b0544e]. Nature Neuroscience (2020). High credibility.

Understanding the mechanisms of neural computation and learning will require knowledge of the underlying circuitry. Because it is difficult to directly measure the wiring diagrams of neural circuits, there has long been an interest in estimating them algorithmically from multicell activity recordings. We show that even sophisticated methods, applied to unlimited data from every cell in the circuit, are biased toward inferring connections between unconnected but highly correlated neurons. This failure to 'explain away' connections occurs when there is a mismatch between the true network dynamics and the model used for inference, which is inevitable when modeling the real world. Thus, causal inference suffers when variables are highly correlated, and activity-based estimates of connectivity should be treated with special caution in strongly connected networks. Finally, performing inference on the activity of circuits pushed far out of equilibrium by a simple low-dimensional suppressive drive might ameliorate inference bias.

---

### Phase informed model for motion and susceptibility [^3e94039e]. Human Brain Mapping (2013). Low credibility.

The fit of the PIMMS model to the phase data acquired in Experiment 1 is illustrated in Figure 2 for each subject. The top rows (a, d, g) show two slices (one inferior and one more superior) through each of the four rate of change of phase maps. From the left these are the rate of change of phase per degree of rotation about the x and y axes, ∂ϕ/∂θ x and ∂ϕ/∂θ y in radians per degree, rate of change of phase per unit time, ∂ϕ/∂τ in radians per second and the constant term C representing the mean phase change over the time series with respect to the first image. The middle rows (b, e, h) show a plot of the motion parameters estimated from the realignment for head rotation about the x ‐axis (green solid line) and the y ‐axis (pink dashed line) in degrees versus image volume number. The bottom rows (c, f, i) show the average phase change (blue dotted line) and the average model fit (red solid line) over all voxels in the brain in radians versus volume number. The impact of head rotation around the x ‐axis is visible in the ∂ϕ/∂θ x maps which show an increased field at the front of the head and a decreased field at the back of the head. This means that head rotation about the x ‐axis will lead to stretching of the front of the image in the anterior direction and of the back of the head in the posterior direction. For the ∂ϕ/∂θ y maps, there is left right asymmetry indicating the impact on the field of a head rotation about the y ‐axis. The ∂ϕ/∂θ x and the ∂ϕ/∂θ y maps are shown at the same scale of ± 0.5 radians per degree of rotation but it is apparent that the relative effect of rotation about the y ‐axis is greater and the ∂ϕ/∂θ y map contains more spatial structure. This is likely to be a result of there being very little head motion about the y ‐axis resulting in a noisier estimate for this model parameter.

---

### Guidelines for the early management of patients with acute ischemic stroke: 2019 update to the 2018 guidelines for the early management of acute ischemic stroke: a guideline for healthcare professionals from the American Heart Association / American stroke association [^a19800a8]. Stroke (2019). High credibility.

Regarding quality improvement for acute ischemic stroke, more specifically with respect to stroke centers/units, AHA/ASA 2019 guidelines recommend to certify stroke centers by an independent external body.