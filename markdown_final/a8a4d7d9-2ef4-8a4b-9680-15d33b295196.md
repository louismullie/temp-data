# Medical Question & Answer

**Sample ID**: a8a4d7d9-2ef4-8a4b-9680-15d33b295196
**Dataset Index**: 217683

---

## Question

4. According to the problem, the random variable ξ = [CM/CB] is uniformly distributed on the interval [0; 1]. If x is the value of the random variable ξ, then

s(x) = S_PQM = S_ABC·(1 − [1/6] − [3x/4] − [(1 − x)/3]) = [S_ABC/12](6 − 5x) → [S_PQM/S_ABC] ≥ 1/3

Then the condition [S_PQM/S_ABC] ≥ 1/3 takes the form

[(6 − 5x)/12] ≥ 1/3

From this, we find x ≤ 2/5. Considering the uniform distribution, P(A) = 2/5.

Let's find the expected value of the random variable X = [S_PQM/S_ABC]

Mₓ = ∫₀¹ [(6 − 5x)/12] dx = −[1/120](6 − 5x)²|₀¹ = 7/24

---

## Answer

> Let's see… What do we have here? The user is asking about a geometric probability problem involving a uniformly distributed ratio and the expected value of an area ratio. Let's break this down step-by-step. First, I need to think about the setup and the uniform distribution assumption. Then, I should verify the algebraic simplification of the area ratio expression. Next, I will solve the inequality to find the probability that the area ratio is at least 1/3. After that, I should compute the expected value of the area ratio and double-check the integration. Finally, I will summarize the results and ensure the reasoning is consistent with the uniform distribution properties.

> Let me first confirm the problem setup. The random variable ξ = CM/CB is uniformly distributed on [0, 1], and the area ratio S_PQM/S_ABC is given as (6 − 5x)/12, where x is a realization of ξ. I need to ensure that this expression is indeed a function of x and that the uniform distribution applies directly to x, which it does here [^notfound].

> Wait, let me verify the algebraic simplification of the area ratio. The original expression is 1 − 1/6 − 3x/4 − (1 − x)/3. Let's see… 1 − 1/6 is 5/6. Then −3x/4 − (1 − x)/3 becomes −3x/4 − 1/3 + x/3, which is −3x/4 + x/3 − 1/3. Combining the x terms gives (−9x + 4x)/12 = −5x/12, so the total is 5/6 − 5x/12. Converting 5/6 to 10/12, I get (10 − 5x)/12 = (6 − 5x)/12, which matches the simplified form. Good, the algebra checks out [^notfound].

> Next, I should review the inequality to find when the area ratio is at least 1/3. Setting (6 − 5x)/12 ≥ 1/3, I multiply both sides by 12 to get 6 − 5x ≥ 4, then subtract 6 to get −5x ≥ −2, and divide by −5 (reversing the inequality) to obtain x ≤ 2/5. Since x ~ Uniform(0, 1), the probability P(x ≤ 2/5) is simply 2/5, which is 0.4. Hold on, I should verify that I'm not misapplying the uniform CDF; yes, for U(0,1), P(X ≤ a) = a when 0 ≤ a ≤ 1, so 2/5 is correct [^notfound].

> I will now examine the expected value calculation. The expected value of the area ratio X = (6 − 5x)/12 is E[X] = ∫₀¹ (6 − 5x)/12 dx. Let me integrate carefully: the antiderivative of (6 − 5x)/12 is (6x − 5x²/2)/12, so evaluating from 0 to 1 gives (6 − 5/2)/12 − 0 = (12/2 − 5/2)/12 = (7/2)/12 = 7/24. Hmm, wait a minute, the user's working shows a negative sign and a squared term, which looks like a u-substitution artifact; I should double-check that. The direct integration I just did is correct and avoids sign confusion, so E[X] = 7/24 ≈ 0.2917, which is less than 1/3, consistent with the probability result that only 40% of the distribution exceeds 1/3 [^notfound].

> Let me reconsider the user's integration approach to ensure I'm not missing something. The user's expression −(1/120)(6 − 5x)²|₀¹ implies a substitution u = 6 − 5x, du = −5dx, so dx = −du/5, and the integral becomes −(1/60)∫ u du = −(1/120)u². Evaluating −(1/120)[(6 − 5)² − (6)²] = −(1/120)(1 − 36) = −(1/120)(−35) = 35/120 = 7/24, which matches my direct result. So the user's method is actually correct; I initially thought the negative sign was a mistake, but it cancels out properly in the definite integral, yielding the same expected value of 7/24 [^notfound].

> Final answer summary: The probability that the area ratio S_PQM/S_ABC is at least 1/3 is 2/5, and the expected value of the area ratio is 7/24. I should confirm that both results are dimensionless and consistent with the uniform distribution assumption, which they are, and that the expected value sits below the 1/3 threshold, aligning with the probability calculation that only 40% of cases meet or exceed 1/3 [^notfound].

---

The probability that the area ratio ([S_PQM/S_ABC]) is at least (1/3) is (2/5), and the expected value of the area ratio is (7/24). The key steps are: (1) simplify the area ratio to ([(6−5x)/12]), (2) solve the inequality ([(6−5x)/12] ≥ 1/3) to get (x ≤ 2/5), and (3) compute the expected value as (∫₀¹ [(6−5x)/12] dx = 7/24).

---

## Probability calculation

Given the area ratio:

[S_PQM/S_ABC] = [(6−5x)/12]

we need to find the probability that this ratio is at least ([1/3]):

[(6−5x)/12] ≥ 1/3

Multiply both sides by 12:

6−5x ≥ 4

Subtract 6 from both sides:

−5x ≥ −2

Divide by -5 (reverse the inequality):

x ≤ 2/5

Since (x) is uniformly distributed on ([0, 1]), the probability is:

P(x ≤ 2/5) = 2/5

---

## Expected value calculation

The expected value of the area ratio is:

E[[S_PQM/S_ABC]] = ∫₀¹ [(6−5x)/12] dx

Compute the integral:

∫₀¹ [(6−5x)/12] dx = [1/12] ∫₀¹ (6−5x) dx = [1/12] [6x − 5x²/2]₀¹ = [1/12](6 − 5/2) = [1/12]·[7/2] = 7/24

---

## Final results

- **Probability that ([S_PQM/S_ABC] ≥ 1/3)**: (2/5)
- **Expected value of ([S_PQM/S_ABC])**: (7/24)

---

## References

### Fundamentals of research data and variables: the devil is in the details [^07fcd2d5]. Anesthesia and Analgesia (2017). Low credibility.

Designing, conducting, analyzing, reporting, and interpreting the findings of a research study require an understanding of the types and characteristics of data and variables. Descriptive statistics are typically used simply to calculate, describe, and summarize the collected research data in a logical, meaningful, and efficient way. Inferential statistics allow researchers to make a valid estimate of the association between an intervention and the treatment effect in a specific population, based upon their randomly collected, representative sample data. Categorical data can be either dichotomous or polytomous. Dichotomous data have only 2 categories, and thus are considered binary. Polytomous data have more than 2 categories. Unlike dichotomous and polytomous data, ordinal data are rank ordered, typically based on a numerical scale that is comprised of a small set of discrete classes or integers. Continuous data are measured on a continuum and can have any numeric value over this continuous range. Continuous data can be meaningfully divided into smaller and smaller or finer and finer increments, depending upon the precision of the measurement instrument. Interval data are a form of continuous data in which equal intervals represent equal differences in the property being measured. Ratio data are another form of continuous data, which have the same properties as interval data, plus a true definition of an absolute zero point, and the ratios of the values on the measurement scale make sense. The normal (Gaussian) distribution ("bell-shaped curve") is of the most common statistical distributions. Many applied inferential statistical tests are predicated on the assumption that the analyzed data follow a normal distribution. The histogram and the Q-Q plot are 2 graphical methods to assess if a set of data have a normal distribution (display "normality"). The Shapiro-Wilk test and the Kolmogorov-Smirnov test are 2 well-known and historically widely applied quantitative methods to assess for data normality. Parametric statistical tests make certain assumptions about the characteristics and/or parameters of the underlying population distribution upon which the test is based, whereas nonparametric tests make fewer or less rigorous assumptions. If the normality test concludes that the study data deviate significantly from a Gaussian distribution, rather than applying a less robust nonparametric test, the problem can potentially be remedied by judiciously and openly: (1) performing a data transformation of all the data values; or (2) eliminating any obvious data outlier(s).

---

### Bayesian prediction intervals for assessing P-value variability in prospective replication studies [^00250a77]. Translational Psychiatry (2017). Low credibility.

Further, once a P -interval is constructed for a particular P obt, how can one interpret its bounds? If it is constructed based on an 80% classical prediction interval for a normal test statistic as originally suggested by Cumming, with no regard to prior distribution assumptions, by definition it guarantees that 80% of P -intervals will capture a replication P -value. That is, the lower and the upper bounds of a P -interval do not provide bounds on the range of possible values of a replication P -value.

Our main goal in this work is to derive prediction intervals for P -values based on the Mixture Bayes approach whose endpoints have a clear probability interpretation for any specific interval constructed based on a given data set. Unlike the classical coverage property, Bayesian intervals based on a posterior P -value distribution have the interval endpoints that are directly interpreted as defining a target range to contain a replication P -value with probability (1 − α).

P -values can be viewed as random variables, reflecting variability due to random sampling. This notion goes back to Fisher, whose method of aggregating information from several independent P -values is based on recognizing the fact that their product is itself a random variable (and twice the negative logarithm of that product has a chi-square distribution). The distribution of P -value and thus its variability are easily characterized analytically for the basic test statistics and depend on a measure of effect size, such as the value of odds ratio (OR) of disease given exposure vs. nonexposure to a pollutant. The nature of P -value randomness may be viewed from a number of angles -, but the randomness of P -value reflects randomness of the respective test statistic. When the effect size is zero, P -value of a continuous test statistic is uniformly distributed between zero and one, and as the departure from the null hypothesis increases, the shape of the P -value distribution becomes increasingly skewed toward zero (suppl. section S1.) Furthermore, effect sizes can also be thought of as arising from a distribution, (e.g. Equation 11 in Kuo et al.) in which case the P -value distribution becomes a weighted average, i.e. a marginal distribution over all possible values of the effect with their respective probabilities as weights.

---

### Experimental nonclassicality in a causal network without assuming freedom of choice [^e83d6c77]. Nature Communications (2023). High credibility.

Violation of a causal compatibility inequality

In order to demonstrate the nonclassicality of the experimental data relative to the triangle causal network, we seek to identify some causal inequalities, which must be satisfied by all distributions compatible with the classical triangle network but which are violated by our experimental statistics. To this end, we turn to the inflation technique for causal inference introduced in ref.

As detailed in the Methods, the inflation technique relates compatibility with a given causal structure to feasibility of a linear program (LP). If the LP related to an inflation of(see Fig. 5 (a)) is found to be infeasible, then evidently p is incompatible with. In our case, is taken to be the classical triangle scenario causal structure depicted in Fig. 1 (b).

Fig. 5
Inflation technique for the triangle network.

a The second order inflation graph of the triangle network. Such an inflation doubles the number of latent variables relative to the triangle scenario, having six latent variables. The inflation quadruples the number of observable random variables of the triangle scenario, having twelve observable random variables { a (1), b (1), c (1), a (2), b (2), c (2), a (3), b (3), c (3), a (4), b (4), c (4) }. Distributions compatible with this inflated structure satisfy symmetry properties, and have marginals corresponding to products of triangle-compatible distribution. This can be exploited to derive suitable causal compatibility inequalities that are violated by the experimental data. b This plot depicts the 64 × 64 coefficientsfor a quadratic inequality of the form of Eq. (7) such that the left-hand side is nonnegative on all distributions compatible with the classical triangle scenario, but which evaluates to the negative number V e x p = − 0.02436 ± 0.00016 on our experimental data. The x -axis ranges over the values of (a 1, b 1, c 1) while the y -axis ranges over the values of (a 2, b 2, c 2), and the color at a given point denotes the value ofaccording to the mapping set out in the legend.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^443e12f1]. Annals of the American Thoracic Society (2023). High credibility.

Unbiased stereological sampling — principles and methods note that "Unbiased sampling designs guarantee statistically representative samples and generally must be spatially randomized", and for anisotropic structures some parameters "require spatial orientation randomization". Named approaches include "systematic uniform random sampling", "isotropic uniform random sampling", "stratified sampling", "fractionator sampling", and "proportionator sampling". Importantly, "Voxel-by-voxel automated image analysis ensures neither accuracy nor precision of the results, nor does it obviate the need to strictly follow stereological sampling principles".

---

### Article 5. An introduction to estimation – 2: from z to T [^fb82b806]. Emergency Medicine Journal (2001). Low credibility.

Provided the sample size is large enough (that is, n greater than 100), the z statistic can be used to determine the confidence interval estimation of the population mean even when the sigma is not known. In these cases the estimation of the standard error of the mean is used. The z statistic is also valid when determining the population's proportion based upon a large sample. However, when dealing with smaller samples, the z statistic is replaced by the t statistic. This makes it possible to estimate, in a population with an unknown standard deviation: The probability of getting a sample mean greater than or equal to a particular value The value of a sample mean with a particular probability of occurring The probability of getting a sample mean between two particular values The confidence interval for the estimation of the population mean can also be determined using the t statistic.

---

### Confidence reports in decision-making with multiple alternatives violate the Bayesian confidence hypothesis [^6173346e]. Nature Communications (2020). High credibility.

Fig. 2
Models.

a Generative model. Target position is represented by s. Two sources of variability are considered in the model: First, observers have access to noisy measurement x, a Gaussian distribution centered at s with a standard deviation σ. Second, given the same measurement x, the posterior distribution varies across trials due to decision noise, modeled by Dirichlet distribution, of which spread (represented by the shade of the ternary plot) is controlled by a parameter α (see Methods). On each trial, a decisionand a confidence c are read out from the posterior distribution of that trial. b We use ternary plots to represent all possible posterior distributions. For example, a point at the center represents a uniform posterior distribution; at the corners of the ternary plot, the posterior probability of one category is one while the posterior for the other two categories are zeros. c The bar graphs illustrate how confidence is read out from posterior probabilities in each model. For the purposes of these plots, we did not include decision noise here. The color of each ternary plot represents the confidence as a function of posterior distribution for each model. The color is scaled for each ternary plot (independently) to take the whole range of the color bar.

We introduce three models of confidence reports: the Max model, the Entropy model and the Difference model. Each of these models contains two steps: (a) mapping the posterior distribution (q) to a real-valued internal confidence variable; (b) applying three criteria to this confidence variable to divide its space into four regions, which, when ordered, map to the four confidence ratings. The second step accounts for every possible monotonic mapping from the internal confidence variable to the four-point confidence rating. The three models differ only in the first step.

The Max model corresponds to the Bayesian confidence hypothesis. In this model, the confidence variable is the probability that the chosen category is correct, or in other words, it is the highest of the three posterior probabilities (Fig. 2c). In this model, the observer is least confident when the posterior distribution is uniform. Importantly, after the posterior distribution is computed, the posterior probability of the unchosen options does not further contribute to the computation of confidence.

---

### Between-batch pharmacokinetic variability inflates type I error rate in conventional bioequivalence trials: a randomized advair diskus clinical trial [^b7751d35]. Clinical Pharmacology and Therapeutics (2017). Low credibility.

Figure 3
Distributions of the test/reference ratio estimate from a two‐way crossover bioequivalence study design in which a single randomly selected test batch is compared to a single randomly selected reference batch in 26 subjects. On the logarithmic scale, the within‐subject residual error variance is assumed to be 0.04. On the natural scale, the true test/reference ratio is assumed to be 1.05. Specific distributions are shown for between‐batch variance values) on the log‐scale of zero (blue), 0.01 (red), 0.03 (green), and 0.06 (gray). The expected range of the 90% confidence interval of the test/reference ratio assumingis shown as a shaded area to illustrate the coverage of a 90% confidence interval derived from a two‐way crossover design. For non‐zero, the two‐way crossover design 90% confidence interval provides only a fraction of the coverage provided for.

This interpretation of the standard test/reference confidence interval, however, fails for a two‐way (i.e. two‐period, two‐treatment) crossover design in the presence of between‐batch variability. When the true, underlying mean and within‐subject variance component values are test/reference, and, the expected 90% confidence interval for the design can be shown to be 0.955–1.155, indicated by the filled portion of the " distribution in Figure 3. Due to widening of the test/reference ratio distribution by the addition of between‐batch variability, this (alleged) "90%" confidence interval covers the true test/reference mean ratio with a probability of only 46%, 29%, or 21% when between‐batch variance is 0.01, 0.03, or 0.06 (indicated by the red, green, and gray distributions in Figure 3, respectively). Thus, the two‐way crossover design confidence interval (that cannot estimate between‐batch variability and therefore ignores it in confidence interval construction (i.e. underestimates true total within‐subject variability)) can no longer be interpreted as providing a good interval estimate of the test/reference ratio. The two‐way crossover design confidence interval is, simply, incorrectly narrow.

---

### Evolving use of ancestry, ethnicity, and race in genetics research – A survey spanning seven decades [^7e9b81de]. American Journal of Human Genetics (2021). Medium credibility.

Figure 1
Visualization of parameters of random variable y, n t, n st, and n s

Visualization of parameters of random variable y, n t, n st, and n s, where t is the target term, s is the co-occurring word of interest, and N is the total number of 200-token segments in a given range of years. n t represents number of segments containing t, n s represents number of segments containing s, and n st represents number of segments containing both.

The mean of this distribution is the expected co-occurrence on a random basis and is given byWe compute the p value, i.e. the probability of the observed or a greater co-occurrence frequency arising by chance as the following:This calculation is equivalent to representing the presence or absence of two terms in a two-by-two contingency table and conducting a one-sided Fisher's exact test.

We measured the "effect size" as the odds ratio (OR) — the ratio of the odds of term t occurring in a segment where term s is present — to its odds of being present in the absence of s. The OR is given by, and N represents the total number of even or odd number of segments in a given decade and n the number of segments with s, t, or both (Figure 1). 95% confidence intervals (CIs) for the ORs are given by a well-known formula. As p values and CIs were calculated with different distributions, ORs whose CIs include "1" can be statistically significant.

---

### Estimating with confidence the risk of rare adverse events, including those with observed rates of zero [^766792a7]. Regional Anesthesia and Pain Medicine (2002). Low credibility.

Omission of a confidence interval (CI) associated with the risk of a serious complication can lead to inaccurate interpretation of risk data. The calculation of a CI for a risk or a single proportion typically uses the familiar Gaussian (normal) approximation. However, when the risk is small, "exact" methods or other special techniques should be used to avoid overshooting (risks that include values outside of [0,1]) and zero width interval degeneration. Computer programs and simple equations are available to construct CIs reasonably accurately. In the special case in which the complication has not occurred, the risk estimated with 95% confidence is no worse than 3/n, where n is the number of trials.

---

### Confidence reports in decision-making with multiple alternatives violate the Bayesian confidence hypothesis [^b245f51e]. Nature Communications (2020). High credibility.

Inference on a given trial. We assume that the observer knows the mean and standard deviation of each category based on the exemplar dots, and that the observer assumes that the three categories have equal probabilities. The posterior probability of category C given the measurement x is then p (C | x) ∝ p (x | C) = N (x; m C, (σ s 2 + σ 2) I). Instead of the true posterior p (C | x), the observer makes the decisions based on q (C | x), a noisy version of the posterior probability. We obtain a noisy posterior q (C | x) by drawing from a Dirichlet distribution. The Dirichlet distribution is a generalization of the beta distribution. Just like the beta distribution is a continuous distribution over the probability parameter of a Bernoulli random variable, the Dirichlet distribution is a distribution over a vector that represents the probabilities of any number of categories. The Dirichlet distribution is parameterized as

Γ represents the gamma function. p is a vector consisting of the three posterior probabilities, p = (p 1, p 2, p 3) = (p (C = 1 | x), p (C = 2 | x), p (C = 3 | x)). q is a vector consisting of the three posterior probabilities perturbed by decision noise, q = (q 1, q 2, q 3) = (q (C = 1 | x), q (C = 2 | x), q (C = 3 | x)). The expected value of q is p. The concentration parameter α is a scalar whose inverse determines the magnitude of the decision noise; as α increases, the variance of q decreases. To make a category decision, the observer chooses the category that maximizes the posterior probability:

---

### Intermodal consistency of whole-brain connectivity and signal propagation delays [^46ca680f]. Human Brain Mapping (2025). Medium credibility.

4.1.3 Transition Matrices

Each source‐reconstructed signal was binned (such as to obtain a branching ratio~1) and then z ‐scored and binarized, such that, at any time bin, a z ‐score exceeding ± 3 was set to 1 (active); all other time bins were set to 0 (inactive). An avalanche was defined as starting when any region is above threshold, and finishing when no region is active, as in our earlier work (Sorrentino, Rucco et al.). The results reported refer to binning = 3, corresponding to a branching ratio of 1. An avalanche‐specific transition matrix (ATM) was calculated, where the element (i, j) represented the probability that region j was active at time t + ẟ, given that region i was active at time t, where ẟ ~ 3 ms.

4.1.4 Construction of Random Surrogates

Randomized transition matrices were generated to ensure that associations between transition probabilities and structural connectivity could not be attributed to chance. Avalanches were randomized across time, without changing the order of avalanches at each time step. We generated a total of 1000 randomized transition matrices, and the Pearson's correlation coefficient was computed between each randomized matrix and the matrices derived from the F‐TRACT dataset. This yielded a distribution of correlation coefficients under randomization. The proportion of correlation coefficients that were greater than, or equal to, the observed correlation coefficient provided a p value for the null hypothesis that the structure of the avalanches is not related to the spreading of the cortico‐cortical evoked potential as recorded in F‐TRACT.

---

### Bayesian prediction intervals for assessing P-value variability in prospective replication studies [^ee55d352]. Translational Psychiatry (2017). Low credibility.

Table 1
Binomial probabilities for 80% prediction intervals, using a two-sample Z -test

The table illustrates the effect of thresholding, applied to observed P -values, e.g. selection of statistically significant P -values at 5% level, on binomial probabilities

Mixture Bayes intervals were included in these simulations to check how well they approximate a continuous prior distribution assumed by the conjugate intervals. We used mixture components with the length σ 0 /8 for every component and truncated the normal prior at 10 −6 and 1−10 −6 quantiles. This provided us with sufficient accuracy and resulted in the number of mixture components, B, equal to 76 for all values of.

Table 1 clearly indicates that all three construction methods have the correct coverage (~80%) if a prediction interval is calculated for a randomly observed P -value ∈[0,1]. However, selection and small prior variance both impair performance of P -intervals. For instance, if an interval is constructed for a P -value < 0.001 and, the coverage of the traditional non-Bayesian P -interval may be as low as 17%. This poor coverage is due to a combination of both the selection bias and the implicit assumption that prior variance ofranges from negative infinity to positive infinity, which leads to the left-side P -interval endpoint being too close to zero. However, even for large values of prior, the P -interval has poor coverage when constructed for P -values around genome-wide significance levels (e.g. P -value < 1.5 × 10 −7). On the other hand, for large P -values the coverage of P -intervals becomes greater than the nominal (1 − α)% value. This is a consequence of the fact that P -interval's width depends on the magnitude of P -values and as P -values become larger, the width of the interval increases as well. For example, given the prior variance, the width of P -intervals and the Bayesian intervals coincides at P -value = 0.446 (hence, the Bayesian intervals are wider than P -intervals at values smaller than 0.446). The P -interval around = 0.446 is: 0.147 ≤ P ≤ 0.995, while the Bayesian interval is 0.110 ≤ P ≤ 0.958.

---

### Uncertainty analysis: an example of its application to estimating a survey proportion [^2fb1ea6c]. Journal of Epidemiology and Community Health (2007). Low credibility.

Uncertainty analysis is a method, established in engineering and policy analysis but relatively new to epidemiology, for the quantitative assessment of biases in the results of epidemiological studies. Each uncertainty analysis is situation specific, but usually involves four main steps: (1) specify the target parameter of interest and an equation for its estimator; (2) specify the equation for random and bias effects on the estimator; (3) specify prior probability distributions for the bias parameters; and (4) use Monte-Carlo or analytic techniques to propagate the uncertainty about the bias parameters through the equation, to obtain an approximate posterior probability distribution for the parameter of interest. A basic example is presented illustrating uncertainty analyses for four proportions estimated from a survey of the epidemiological literature.

---

### Crisis + emergency risk communication. 2012 edition [^45b9a99d]. CDC (2012). Medium credibility.

Epidemiology terms — Population-based is defined as "Pertaining to a general population defined by geopolitical boundaries" and Prevalence as "The number of instances of a given disease or other condition in a given population at a designated time". Prevention states "The goals of public health and medicine are to promote health, to preserve health, to restore health when it is impaired, and to minimize suffering and distress". Primary case is "The individual who introduces the disease into the family or group under study", and Quantitative data are "Data in numerical quantities, such as continuous measurements or counts". Quarantine is "The limitation of freedom of movement of well persons or animals exposed to a communicable disease, for a period of time not longer than the longest usual incubation period of the disease". Random "Occurs by chance", and Rate is a "Ratio whose essential characteristic is that time is an element of the denominator and in which there is a distinct relationship between numerator and denominator". Relative risk is "The ratio of the risk of disease or death among the exposed to the risk among the unexposed", and Reservoir is "The natural habitat of the infectious agent". Risk is "A probability that an event will occur", and a Risk factor is "An attribute of exposure that is associated with an increased probability of a specified outcome, such as the occurrence of a disease". Sample is "A selected subset of a population", and Screening is "The use of tests or examinations to identify unrecognized disease". Seroepidemiology is "Epidemiologic study or activity based on the detection on serological testing of characteristic change in the serum level of specific antibodies", and Statistical significance indicates that "Statistical methods allow an estimate to be made of the probability for the observed or greater degree of association between independent and dependent variables under the null hypothesis".

---

### Prediction of unfavorable outcome of acute decompensation of diabetes mellitus [^60d16c66]. Diabetology & Metabolic Syndrome (2025). Medium credibility.

To solve the problem, we will apply a solution similar to that indicated in works and based on the assumption that any considered set of indicators of the human body's performance, where, can be represented in the form

In this case, is background noise, which is a consequence of the action of a complex of phenomena and processes associated with the normal (not associated with serious pathologies) physiological functioning of the patient's health. At the same time, the termdescribes the deviation of a set of laboratory indicators or parameters, which is a consequence of the launch of a pathological process. The ensemble of these parameters is considered as a sequence of complexly interconnected random variables.

In this case, in addition to the assumption used in about the weak dependence of the two sequences of random variables (RV), the hypothesis about significantly different properties of the statistics of the RV setfor patients with subsequent recovery or, conversely, a fatal outcome of the disease is allowed for consideration. In other words, we believe that for the corresponding patients, with a high probability, there is a difference in the properties of the experimental probability density corresponding to the set of results of the initial medical tests at the time of admission to the hospital. Further, this assumption is verified using the example of a favorable and unfavorable course of acute decompensation of diabetes in patients admitted to the hospital due to acute decompensation of diabetes mellitus.

In an effort to clarify the calculations made, we present Fig. 1 below. In Fig. 1 a, the wide "rough" curvecorresponds to the probability density of the complex of values ​​of the RV. According to the relation (Eq. 3) written below, the dependenceis discrete, but in Fig. 1, for greater clarity, it is schematically presented as a continuous function. The small-scale roughnesses of the curveshown in Fig. 1 a are associated with a deviation from some unknown "ideal" distribution and are explained by the limited number of measurements taken. The effective period of such small-scale roughnesses of the functionis equal to the discretization interval specified when dividing the range of values ​​of the results of the medical tests into separate intervals. This interval can be chosen to be very small, if necessary. In Fig. 1 a also schematically demonstrates the "narrow" curvewhich corresponds to the probability density for the complex of RV, associated with the action of pathology. Just likefor greater clarity, the discrete dependenceis schematically depicted as a continuous curve.

---

### 2019 ASCCP risk-based management consensus guidelines: methods for risk estimation, recommended management, and validation [^c44c6ea6]. Journal of Lower Genital Tract Disease (2020). High credibility.

Risk-based management uncertainty — confidence interval and decision confidence score: Confidence intervals for each risk estimate were calculated using a normal approximation or exact methods based on the binomial distribution, and a "decision confidence score" is defined that combines the uncertainty in the statistical precision and how close the risk estimates fall to the clinical action thresholds; mathematical details on estimating the recommendation confidence scores are presented in Appendix Part D (http://links.lww.com/LGT/A160).

---

### The airborne transmission of viruses causes tight transmission bottlenecks [^8bb3e96b]. Nature Communications (2024). High credibility.

Calculating individual exposures

We generated values c (x, r, t) at time intervals of one second for a period of one hour following each emission event. The initial values c (x, r,0) were scaled so that the total volume, summed across particle sizes, was equal to the desired volume of the emission. In order to calculate the total exposure of person i at the location x i = { x i, y i }, we generated values c (x, r, t) at positions in the square grid centred on x i, with dimension 40 cm, and containing points at resolution 2 cm, finding the mean value of c in this grid. The volume of the space represented by this box is 0.16 Z, where Z is the height of the room so that we can calculate the density of viral particles of radius r in the box. Given a parameter A, describing the rate of air inhalation by a person, we calculated the expected number of particles of radius r inhaled within a 1 second interval at time t. Summing these values over times t, we obtained an expected number of particles of radius r inhaled in a 1 hour period following a single emission event. Summing these values over multiple emission events, we obtained an expected number of particles of radius r inhaled over the entire model period. We denote this number as P i (r).

For each uninfected individual in our model, we generated a Poisson random variable

describing the number of particles of radius r inhaled by that person. This number was converted into a number of effective viruses: For each such particle, the expected number of effective viruses is given bywhere k b is the effective viral load of particles at the point of emission. To calculate the effective number of viruses inhaled via particles of radius r, we calculated a second Poisson random variable

The transmission bottleneck related to the person i was finally calculated as the sum of these values:

Person i was considered to have been infected if and only if N i > 0. Statistics of bottlenecks were calculated across cases of infection.

For each scenario considered, we calculated 10 6 independent simulations, generating N i for each individual in each simulation. Statistics were collated across simulations.

Calculation of k b

By default the effective viral load was calculated using an epidemiological model. Details are given in the Supplementary Information. A broad range of values of k b were considered.

---

### A practical guide for understanding confidence intervals and P values [^c0a7d4a0]. Otolaryngology — Head and Neck Surgery (2009). Low credibility.

The 95 percent confidence interval about the mean demarcates the range of values in which the mean would fall if many samples from the universal parent population were taken. In other words, if the same observation, experiment, or trial were done over and over with a different sample of subjects, but with the same characteristics as the original sample, 95 percent of the means from those repeated measures would fall within this range. This gives a measure of how confident we are in the original mean. It tells us not only whether the results are statistically significant because the CI falls totally on one side or the other of the no difference marker (0 if continuous variables; 1 if proportions), but also the actual values so that we might determine if the data seem clinically important. In contrast, the P value tells us only whether the results are statistically significant, without translating that information into values relative to the variable that was measured. Consequently, the CI is a better choice to describe the results of observations, experiments, or trials.

---

### Improving the sensitivity of cluster-based statistics for functional magnetic resonance imaging data [^cf541549]. Human Brain Mapping (2021). Medium credibility.

1.4 How to combine different test statistics?

A crucial advantage of the randomization framework is that it allows to combine cluster statistics with different sensitivity profiles. A researcher may not know whether to expect small or large clusters. In that case, it is possible to analyze the data using different cluster definitions, for example, by varying the CDT, the neighbor definition, and/or the number of required neighboring voxels. Within the randomization framework, the results can be combined over these different cluster definitions. For each of the different cluster definitions (CDT, neighbor definition, etc.), the randomization step results in a distribution of optimum (i.e. maximum or minimum) cluster magnitudes (size or sum). These randomized optimum cluster magnitudes can each be transformed into p‐values by comparing them to their corresponding randomization distribution (see Figure 3). By definition, and separately for each of the cluster definitions, the probability distribution of these p‐values is uniform (see Figure 3, Step 2). Similarly, each observed cluster magnitude can also be transformed into a p‐value by comparing it to its corresponding randomization distribution. After transforming the cluster‐definition‐specific magnitudes into p‐values, these transformed magnitudes can be meaningfully combined in a single randomization distribution. This is realized by taking the minimum p‐value over all cluster definitions. This min(p) randomization distribution is constructed in a loop over draws from the randomization distribution: for every draw, evaluate which of the cluster definitions (statistics) has the smallest p ‐value, and use the resulting value as a realization of the min(p) randomization distribution. This min(p) randomization distribution is the final distribution that is used for decision‐making: if the observed min(p)‐value is less than theth percentile of the min(p) randomization distribution, then we reject the null hypothesis of statistical independence between biological data and the explanatory variable. By using the min(p) randomization distribution for decision‐making (instead of the cluster‐definition‐specific randomization distributions), we correct for multiple testing (one test per cluster definition).

---

### Drawing statistical conclusions from experiments with multiple quantitative measurements per subject [^70dff523]. Radiotherapy and Oncology (2020). Medium credibility.

In experiments with multiple quantitative measurements per subject, for example measurements on multiple lesions per patient, the additional measurements on the same patient provide limited additional information. Treating these measurements as independent observations will produce biased estimators for standard deviations and confidence intervals, and increases the risk of false positives in statistical tests. The problem can be remedied in a simple way by first taking the average of all observations of each specific patient, and then doing all further calculations only on the list of these patient means. A more sophisticated statistical modeling of the experiment, for example in a linear mixed model, is only required if (i) there is a large imbalance in the number of observations per patient or (ii) there is a specific interest in actually identifying the various sources of variation in the experiment.

---

### Bayesian prediction intervals for assessing P-value variability in prospective replication studies [^d0d55973]. Translational Psychiatry (2017). Low credibility.

Neyman's description excludes any probabilistic meaning attached to the endpoints of a particular interval: " after observing the values of the x's we may decide to behave as if we actually knew that the true value [of the parameter μ] were between [L (x) and U (x)]. This is done as a result of our decision and has nothing to do with "reasoning" or "conclusion" The above process is also devoid of any "belief" concerning the [true value of μ] "'.

An important point in the preceding discussion of confidence and prediction intervals is that their coverage property is defined as a long-run average of zeros and ones, where "1" indicates an event that a random interval covers the quantity of interest, i.e. a replication P -value in the case of P -intervals, and "0" indicates that the replication P -value is outside that interval. Properly constructed intervals applied repeatedly to independent data sets will result in 1's occurring with (1 − α) frequency. Although it is desirable to have the shortest possible intervals with this property, there is generally no information provided by the interval endpoints about a possible spread of replication P -values. However, interpretation of the classical interval endpoints in a meaningful way is warranted from a Bayesian viewpoint. A Bayesian derivation of a classical interval may reveal the tacitly assumed data generating mechanism. We will refer to that mechanism conventionally as an implicit prior distribution. It allows one to interpret the endpoints of a replication P -interval as (1 − α) probability of capturing the replication P -value. The endpoints of a P -interval are typically interpreted in a probabilistic fashion without specifying implicit prior assumptions. The following quote from Cumminggets to the heart of the matter succinctly: " This article shows that, if an initial experiment results in two-tailed P = 0.05, there is an 80% chance the one-tailed P-value from a replication will fall in the interval (0.00008, 0.44) Remarkably, the interval — termed a P interval — is this wide however large the sample size". An equivalent statement appears in a Nature Methods letter by Halsey and colleagues: " regardless of the statistical power of an experiment, if a single replicate returns a P-value of 0.05, there is an 80% chance that a repeat experiment would return a P-value between 0 and 0.44". Both statements make use of a specific value, = 0.05, for which the interval is constructed and the endpoints of that interval are described explicitly as probability bounds for possible values of replication P -values.

---

### Bayesian prediction intervals for assessing P-value variability in prospective replication studies [^fb7fd2e4]. Translational Psychiatry (2017). Low credibility.

Results

Table 1 summarizes empirical binomial probabilities of the 80% prediction intervals for a standardized effect sizeunder different types of P -value selection (simulation study set-up is detailed in Supplementary Information). The observed P -value was based on a two-sample Z -test and was thresholded according to the following selection rules: (i) no selection, i.e. a prediction interval is constructed for a randomly observed P -value; (ii) selection of P -values around a value, e.g. i.e. prediction intervals are constructed only for P -values that were close to the 5% significance level; (iii) selection of P -values that are smaller than a threshold, e.g. Empirical binomial probabilities were calculated based on 50,000 simulations, using three different methods: (a) a conjugate Bayesian model assuming normal prior distribution for the observed value of a test statistic, where; (b) our Mixture Bayes approach with the same prior as for the conjugate model; and (c) the original P -interval proposed by Cumming.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^0b6d9d81]. Nature Communications (2025). High credibility.

Key to our approach is a novel method for assigning to each model a risk- or R - distribution representing epistemic uncertainties due either to finite samples, model misspecification or non-stationarity. Only when two models have sufficiently non-overlapping R -distributions do we reject the one with higher R. This approach allows us to compare any number of candidate models, by reducing the problem to a sequence of pairwise comparisons. We make no assumption on the structure of those models: they may be given by two completely different sets of equations, or they may have the same equations but differ only in their parameters, as long as they can be cast in the form of equation (1).

In many cases, models will contain a "physical" component — the process we want to describe — and an "observation" component — the unavoidable experimental noise. Distinguishing these components is often useful, but it makes no difference from the point of view of our method: only the combined "physical + observation" model matters. In their simplest forms, the physical component may be deterministic and the observation component may be additive noise, so that the model is written as a deterministic function f plus a random variable ξ affecting the observation:In such a case, the probability in equation (1) reduces to; if ξ i is Gaussian with variance σ 2, this further reduces to. Of course, in many cases the model itself is stochastic, or the noise is neither additive nor Gaussian. Moreover, even when such assumptions seem justified, we should be able to test them against alternatives.

---

### Bayesian prediction intervals for assessing P-value variability in prospective replication studies [^64d5cedb]. Translational Psychiatry (2017). Low credibility.

To further illustrate possible issues with P -interval coverage due to restrictions on the P -value range, consider the following example. Suppose one performs a test for the mean difference between two populations and predicts variability of P -values in a replication study by constructing the corresponding 80% P -interval. If multiple samples are drawn from these populations and 80% P -intervals are constructed each time regardless of whether the observed P-value was significant or not, the results of these multiple experiments can be summarized graphically by Fig. 1. Each dot in Fig. 1 represents a value of the test statistic from a replication study and error bars show prediction intervals based on the observed P -value in the original study. The underlying effect size and sample sizes across studies are kept constant so replication values of the test statistic ranges from about negative two to positive two across all simulated experiments. Pink color highlights P -intervals that failed to capture the future value of statistic. Given these results, one can calculate an empirical binomial probability, i.e. the proportion of times a parameter was captured by the interval, which should be close to the stated nominal level. For instance, in Fig. 1, the binomial probability is 84% (16 out of 100 intervals did not capture the future value) — very close to 80% nominal level, given a small number of repetitions.

Fig. 1
Randomly simulated Z -statistics (dots) with the corresponding 80% prediction intervals (vertical error bars). Tests were performed based on two samples (n 1 = n 2 = 50) from two different populations. The difference between population means was a random draw from the standard normal distribution. Pink color highlights intervals that did not capture the value of the future test statistic

Now, consider a bit different scenario, in which P -intervals are constructed only if the experiment returns a P -value close to 0.05, P obt ≈ 0.05. That is, all experiments with P -values that did not reach statistical significance are discarded and a particular P -interval is constructed only if the obtained P -value is close to 0.05. Would about 80% of the P -intervals constructed around the respective P obt still capture the future value of statistic? An intuitive way to think about this scenario is in connection to the publication bias phenomenon, where the actual relationships tend to be weaker in reality than what was claimed in publications, and we may suspect that P -intervals should be similarly biased when constructed around non-random, selected subsets of P -values.

---

### Bayesian prediction intervals for assessing P-value variability in prospective replication studies [^c0c043e4]. Translational Psychiatry (2017). Low credibility.

Another difficulty with P -interval interpretation arises when it is constructed for a specific P -value. The coverage property of P -intervals as a long-run average is well-defined for random P -values, and the resulting intervals are themselves random. On the other hand, a P -interval constructed for a given P -value, has specific, fixed endpoints. One way to interpret the endpoints of a particular interval and to relate them to the long-run average definition is to restrict the range of random P -values (0 to 1) to a narrow interval around, i.e. for some small ε. We can think of a process that generates these P -values as being the same as in the unrestricted case, but then we would discard any P -value outside the ± ε interval and evaluate coverage only for the intervals around P -values that are similar to. In general, such selection can lead to bias in coverage of the classical interval. For example, Lazzeroni and colleagues reported that the coverage for P -values restricted to a specific range could be much smaller than the nominal (1 − α)% level expected across all possible values of the P -value. Thus, the endpoints of any particular P -interval constructed around an obtained P -value are not readily interpreted in terms related to the P -value at hand or any future values in replication studies.

---

### Statistical significance versus clinical importance of observed effect sizes: what Do P values and confidence intervals really represent? [^4356ec74]. Anesthesia and Analgesia (2018). Low credibility.

Figure 1.
Ninety-five percent confidence intervals (vertical lines) and means (dots) calculated from a simulation of 25 samples (sample size of 30 each) drawn from a normally distributed population with a mean of 30 and a standard deviation of 10. One could think of this as a population of patients with a mean age of 30 y and a standard deviation of 10 y, from which we sample n = 30 patients to estimate the mean age in the population, and we repeat this experiment 25 times. Note that 23 of the confidence intervals (23/25 = 92%) cover the "true" population mean of 30. If we (infinitely) keep repeating this simulation, we would expect that 95% of the confidence intervals contain the true population parameter value.

A common misinterpretation is that there is a 95% probability that a given 95% CI contains the true population parameter. The parameter is a fixed albeit unknown value. The 95% CI either contains the parameter or does not, and the probability is either 100% or 0%. This becomes clear when looking at an example with actual numbers. In Figure 1, the first 95% CI ranges from 29.4 to 34.8. While the true population parameter is usually unknown, we know that it is 30 in this simulated example. Now, it does not make any sense to say that the probability is 95% that 30 falls within the range between 29.4 and 34.8. While in this particular case we definitely know that the CI contains the true population parameter value, there is no way to know whether any CI estimated from a sample contains the parameter.

---

### Sample sizes based on three popular indices of risks [^4a61a58f]. General Psychiatry (2018). Low credibility.

Sample size justification is a very crucial part in the design of clinical trials. In this paper, the authors derive a new formula to calculate the sample size for a binary outcome given one of the three popular indices of risk difference. The sample size based on the absolute difference is the fundamental one, which can be easily used to derive sample size given the risk ratio or OR.

---

### Random laser ablated tags for anticounterfeiting purposes and towards physically unclonable functions [^e1701254]. Nature Communications (2024). High credibility.

Analysis of pattern randomness through RMSE value

Analysis of the random outline circle patterns was accomplished using mathematical models in MATLAB software. First, to evaluate the crater patterns created under a variety of conditions, such as 0.3 mm, 0.5 mm, and 0.7 mm H t Ds, with 13°, 33°, 53°, 73°, and 93° RAs, the number of outline circles produced per unit area was analyzed, as shown in Fig. 3a. Approximately 800 segments were tested in each condition. The smallest number of 0.588 (per unit area) was obtained for 0.7 mm H t D and 93° RA, and the largest number of 6.539 for 0.3 mm H t D and 13° RA. In addition, to analyze whether the distribution was uniform, the root mean square error (RMSE) value of the distribution was calculated using the following Eq. (3) (Fig. 3b):where, and n are the variable probability of analyzed outline circles patterns, variable probability of uniform distribution, and variable number of distributions, respectively. A low RMSE value indicates that the distribution is very uniform. The RMSE analysis yielded a lowest value of 0.013 for 0.3 mm H t D and 13° RA, and a highest value of 0.068 for 0.7 mm H t D and 93° RA. This analysis confirmed that the outline circles number per unit area is more uniform when more outline circles are present. When the outline circles were close to each other, they were either separated or overlapped with other outline circles to form objects (Fig. 3c). An object composed of an isolated single outline circle does not have a specific direction; therefore, the object's orientation value is zero. However, for an object composed of multiple overlapping outline circles, the orientation has a specific value. If the number of outline circles per unit area is large, such as for 0.3 mm H t D, several overlapped outline circles exist, so the probability of uniform distribution of object orientation is relatively low. Figure 3c shows the analysis of the object orientation distribution for 0.3 mm H t D with various RA. The number of elements of the orientation distribution was set to six (with a step size of 30°). The RMSE values of the outline circles patterns created under 0.1 mm, 0.3 mm, 0.5 mm, and 0.7 mm H t D and 13°, 33°, 53°, 73°, and 93° RA are shown in Fig. 3e. Analysis of the randomly distributed outline circles for 0.3 mm H t D confirmed that the distribution had a very low RMSE value of approximately 0.004, regardless of the RA. Most of the objects obtained with 93° RA were separated individually; thus, the RMSE value was expected to be low. Even at 13° RA, in which several outline circles overlapped, the RMSE value was equally low. These results show that the orientation of an object is random even if outline circles overlap, confirmed by the low RMSE values of approximately 0.004 obtained for all analyzed conditions. Consequently, object orientation can always be considered random. The ratio of objects to the total outline circles was analyzed. In general, the smaller the H t D and RA, the larger the number of overlapping outline circles and, consequently, the smaller the object/outline circles ratio. From this analysis, a low objects/outline circles ratio of 61% for 0.3 mm H t D and 13° RA and a high ratio of 88% for 0.7 mm H t D and 93° RA was obtained (Supplementary Fig. S7).

---

### New evidence for positive selection helps explain the paternal age effect observed in achondroplasia [^4485ebed]. Human Molecular Genetics (2013). Low credibility.

Figure 2.
Assay validation. Mutation frequencies (Fx) estimated in a dilution series of standards derived from genomic DNA of a heterozygote ACH patient mixed at different proportions with 1 µg of normal blood DNA (1:10, 1:100, 1:1000; n = 3) or 330 000 plasmid DNA molecules with a wild-type FGFR3 insert plus 1 µg of E. coli DNA (1:10000; n = 2). The measured ratios shown with 95% confidence intervals match the known input ratios as represented by the hand-drawn diagonal.

We assayed the spatial distribution of the c.1138G > A mutation in the testis of an 80-year-old man (ID 57650). As described previously, the testis was cut into six slices and each slice was further divided into 32 pieces of approximately equal size. In Figure 3, we show the spatial distribution of the ACH mutation frequency with different colors representing the different mutation frequencies: the highest frequency is colored dark red and the lowest frequency is colored light gray. The mutation frequencies and confidence intervals for each individual testis piece are listed in. The mutations are not uniformly distributed throughout the testis but are highly clustered. In fact, there are a small number of pieces with mutation frequencies that are orders of magnitude greater than the remaining pieces. The average mutation frequency (Av) for the whole testis is 122 mutants pmg. The piece with the maximum mutation frequency (Mx) is 3072 pmg (0.31%). This piece is adjacent to a piece with no mutants. Next, we introduce several summary statistics to quantify the amount of mutation clustering. The ratio of the maximum piece frequency to the testis average frequency (Mx/Av) is 25. If the mutations were uniformly distributed and there were no clusters, this ratio would be closer to 1. Most of the pieces have low mutation frequencies: the fraction of the pieces with mutation frequencies < 50 pmg (F < 50) is 76% (these pieces are colored light or dark gray in Fig. 3). If the mutations were uniformly distributed, none or very few of the pieces would be expected to have frequencies < 50 pmg, given that the Av is 122 pmg. Finally, the fraction of the testis necessary to include 95% of the mutants (N95) is only 27%. If the mutations were uniformly distributed, one would expect this fraction to be near 95%.

---

### Statistics 101 for radiologists [^ebd40439]. Radiographics (2015). Low credibility.

Diagnostic tests have wide clinical applications, including screening, diagnosis, measuring treatment effect, and determining prognosis. Interpreting diagnostic test results requires an understanding of key statistical concepts used to evaluate test efficacy. This review explains descriptive statistics and discusses probability, including mutually exclusive and independent events and conditional probability. In the inferential statistics section, a statistical perspective on study design is provided, together with an explanation of how to select appropriate statistical tests. Key concepts in recruiting study samples are discussed, including representativeness and random sampling. Variable types are defined, including predictor, outcome, and covariate variables, and the relationship of these variables to one another. In the hypothesis testing section, we explain how to determine if observed differences between groups are likely to be due to chance. We explain type I and II errors, statistical significance, and study power, followed by an explanation of effect sizes and how confidence intervals can be used to generalize observed effect sizes to the larger population. Statistical tests are explained in four categories: t tests and analysis of variance, proportion analysis tests, nonparametric tests, and regression techniques. We discuss sensitivity, specificity, accuracy, receiver operating characteristic analysis, and likelihood ratios. Measures of reliability and agreement, including κ statistics, intraclass correlation coefficients, and Bland-Altman graphs and analysis, are introduced.

---

### Thinking outside the curve, part I: modeling birthweight distribution [^b60d1510]. BMC Pregnancy and Childbirth (2010). Low credibility.

b. Simulation study on calibrating confidence intervals

For our second simulation study we generated 25 overlapping data sets of size 50,000 from design C in Table 3, the degree of overlap consistent with a population of 200,000. For each of various C between 2.0 and 5.0, we used Equation (7) to form confidence intervals for the mixture parameters p 1, p 2, p 3, p 4, μ 1, μ 2, μ 3, μ 4, σ 1, σ 2, σ 3, σ 4. We recorded how many of the mixture parameters were contained in their respective confidence intervals. This was repeated nine more times, and we tabulated how many of the 120 = 12 × 10 confidence intervals contained their targets. Confidence intervals were also formed using Equation (6) for comparative purposes. The above steps were repeated with overlapping data sets consistent with a population of 1,000,000 and with nonoverlapping data sets consistent with an effectively infinite population.

The results are summarized in Table 5. With an effectively infinite population, only 81.7% of the confidence intervals formed using Equation (6) contained their targets at C = 5.0. The confidence intervals formed using Equation (7) contained their targets 95.0% of the time at C = 2.5. The adjustment suggested by Equation (8) appears reasonable: φ = .05 = 50,000/1,000,000 and N rep = 25 yield C φ = 1.315 C 0, which accords with the 95.8% capture of mixture parameters at C = 3.5 ≈ 1.315 × 2.5 with a population of 1,000,000.

Table 5
Confidence Interval Coverage Probabilities in Simulation Studies

The row with " C " = 2 and "Population size" = 200,000 identifies the numbers and percentages of confidence intervals containing their targets of mixture parameters, based on 10 repetitions in each of which 25 samples of size 50000 were simulated from a 4-component normal mixture with 12 parameters; results under the heading of "Bias adjustment included" are based on Equation (7) with C = 2, results under the heading of "Bias adjustment omitted" are based on Equation (6) with C = 2, and the 25 samples of size 50000 had overlap consistent with a population size of 200,000. Other rows correspond to different choices of C and/or population sizes.

---

### Microbial coexistence through chemical-mediated interactions [^3f7ed7ef]. Nature Communications (2019). High credibility.

Network architecture of initial species pool

In binomial networks, the presence or absence of c-links and f-links each is determined by a fixed probability. A randomly sampled consumption factor or production rate (see simulation parameters below) is assigned to each consumption or production link, respectively. The basal growth rate values of species in the initial pool of species are picked randomly from a uniform distribution (r 0 ~ U (0.08, 0.12) per hour, except in Supplementary Fig. 9 as noted). The exact value of basal growth rate is inconsequential as all other growth rate values (e.g. the influence of chemicals) and time-scales can be scaled accordingly without any loss of generality. For influence strengths, the values are picked randomly from a uniform distribution (see simulation parameters below) when the fraction of positive to negative interactions is 1:1. In cases where either positive or negative interactions are more likely, the absolute values of influence strengths within positive or negative interactions still follow a uniform distribution, but the sign will be positive or negative based on a binomial distribution. The only exception is Supplementary Fig. 7, in which other distributions of influence strengths are used.

Calculating mean excess richness (MER)

To quantify how much richness is supported in a given setting, we define MER aswhere i is the richness (i.e. the number of coexisting species) in the derived community and p i is the probability of achieving a richness of i. This measure quantifies the level of coexistence beyond a single-species domination expected from competitive exclusion. To calculate the confidence intervals for MER, we used bootstrap (using bootci routine in Matlab), typically with 3000 samples with substitution.

---

### Experimental demonstration of quantum advantage for NP verification with limited information [^1b700755]. Nature Communications (2021). High credibility.

Second, we will show that our verification procedure has high completeness, i.e. when the instance is satisfiable and Merlin sends to Arthur a satisfying assignment encoded in the coherent states, then Arthur accepts with high probability. For the same experimental parameters we will then use our theoretical analysis that upper bounds the maximum soundness of our protocol for any strategy of Merlin, and ensure that the soundness is much lower than the experimentally demonstrated completeness, thus proving condition 2 of quantum advantage.

In fact, to simplify the classical pre- and post-processing, we experimentally perform a modified version of the test, where we do not sample balanced and probabilistically checkable YES instances with planted satisfying assignments (this is far from being straightforward), but we generate uniformly random N -bit strings (for several values of N) that correspond to satisfying assignments. Note that a uniform distribution of the satisfying assignments is the hardest case for the problem, since with any other distribution, Arthur would already have some information about the possible solutions to the problem. After that, we check the number of the variables for which Arthur obtains the correct value, the number of wrong values, and the number of undefined variables. From these numbers we compute the expected number of satisfied and unsatisfied clauses Arthur will get on a random YES instance, and using the threshold that has been defined in the calibration phase of the experiment described below, we conclude whether Arthur would accept or reject the instance, thus estimating the completeness of our protocol.

---

### Statistical significance versus clinical importance of observed effect sizes: what Do P values and confidence intervals really represent? [^9df65ff6]. Anesthesia and Analgesia (2018). Low credibility.

PART 2 FROM SAMPLE TO POPULATION: CIs

Frey et alobserved a difference in temperature of 0.6°C between their sampled study groups. However, this provides only limited information on how large the actual effect in the population might be. In this context, a CI provides a range of plausible values for the estimate. Actually, a CI can not only be calculated for an effect size (eg, mean difference, risk ratio, or odds ratio) but also for a wide range of estimates of population parameters, including means and proportions.

Formally, a CI is an interval that contains the true population parameter in a fixed percentage of samples with repeated sampling. The fixed percentage is termed the confidence level, which is often (though again arbitrarily) chosen as 95%. This indicates that when samples are repeatedly taken over and over again from the same population, and if we would calculate the 95% CI for each sample, about 95% of them will contain the true population parameter (Figure 1).

---

### Confidence bounds when the estimated ROC area is 1.01 [^28dc467d]. Academic Radiology (2002). Low credibility.

Rationale and Objectives

In studies with small samples, the authors often encounter data sets in which the estimated area under the receiver operating characteristic (ROC) curve is 1.0. In such cases, neither asymptotic nor resampling methods provide a means of estimating the standard error or constructing a lower confidence bound. The purpose of this study was to develop tables for determining the approximate 95% lower confidence bound when the estimated ROC area is 1.0.

Materials and Methods

Using Monte Carlo simulation, the authors generated 10,000 data sets for each specification of sample sizes, ROC curve shape, and data format (continuous and ordinal scale). For each of these combinations the authors determined the 95% lower confidence bound.

Results

When the estimated ROC area is 1.0, the 95% lower confidence bounds differ dramatically depending on the shape of the ROC curve and on whether the test results are ordinal or continuous. Four tables of 95% lower confidence bounds are provided, along with guidelines for their use.

Conclusion

Given the different shapes of ROC curves and the different formats in which ROC data are collected, it is not feasible to offer one simple method of constructing confidence bounds that works for all ROC curves. The tables provided in this article are useful for interpreting studies with estimated ROC areas of 1.0.

---

### Joint, multifaceted genomic analysis enables diagnosis of diverse, ultra-rare monogenic presentations [^f14dc9f0]. Nature Communications (2025). High credibility.

Analytical test for de novo cohort-level recurrence

Basic statistic definition

We define a cohort as a set of N genomes (i.e. collections of genes) each with sets of de novo variants arising independently but based on the same background de novo mutation rate. Letdenote the de novo mutation rate of a specific variant i. The mutational target of a gene g isThe mutational target of a variantin gene g iswhereis the deleteriousness score of variant i. Intuitively, the more surprising and/or deleterious a variant, the smaller its mutational target. By definition, variant mutational targets are uniformly distributed from 0 to, soppose there are K de novo variants falling within gene g across the cohort, where. We define a statistic y asNote that Y is a sum of K uniformly distributed variables on [0,1] under the null. The distribution of Y given parameter K can thus be modeled by the Irwin-Hall (IH) "sum of uniforms" distribution, which has a closed form for its cumulative density function (CDF) and thus also for its survival function (SF), where. This enables us to replace permutation-based significance evaluations and instead analytically compute the probability of achieving a Y as high or higher than observed with K variants using the IH survival function as. We note that there are many other constructions over a set of uniformly-distributed random variables (such as P values). We further note that as the cohort size dramatically increases, the Irwin-Hall distribution can be replaced with the normal distribution.

Finally, we also model Pr (K), the probability of K independent de novo variants to land in gene g given this cohort of size N, to assign an overall significance value to our statistic y as

Because neither y nor Pr (K) are defined for, we do not expect Pr (y) to be uniformly distributed. Instead, onlyis expected to be uniformly distributed (Supplementary Fig. 9).

In a single genome withtotal observed de novo variants, the number of de novo variants to land in a particular gene g, given that, is Poisson distributed, parameterized by the expected number of de novo s. In a cohort of N genomes, the number of de novo variants to land in gene g is therefore a sum of N Poisson-distributed random variables, which itself is also Poisson distributed. We thus compute, where λ is given by

---

### Time esophageal pH [^b60228f3]. BMC Gastroenterology (2008). Low credibility.

Table 2
Bayesian Posterior Probabilities that the Prevalence of Pathologic Esophageal Reflux During PPI Treatment is Greater than the Prevalence of False-Positive Values for Pathologic Integrated Esophageal Acidity and Time Esophageal pH < 4.

Bayesian posterior probabilities were calculated using the values for the prevalence of false-positive values given in Table 1, the combined prevalence of pathologic esophageal reflux calculated from the index and 3rd studies, and a flat prior probability [8]. The calculations were for values of prevalence from 0 to 1.0 in steps of 0.05. The flat prior probability considered all prevalence values from 0 to 1.0 to be equally probable.

Table 3 gives the values for the area under the receiver operator characteristic (ROC) curves for analyses of gastric acidity stratified in terms of pathologic esophageal acid exposure. The area under the ROC curve is conceptually equivalent to the Wilcoxon statistic and is a quantitative measure of the extent to which integrated gastric acidity differs between records with pathologic esophageal reflux and those with normal esophageal reflux. That is, the ROC area represents the probability that a subject selected randomly from the pathologic reflux group will have a higher value for gastric acidity than a subject selected randomly from the normal reflux group. If there is no overlap in gastric acidity between the 2 groups, the area will be 1.00 and the probability will be 100%. If the distributions are identical, the area will be 0.50 and the probability will be 50%, i.e. the same as chance alone.

Table 3
Parameter Values for the Relationship Between Measures of Gastric Acidity and the Odds Pathologic Esophageal Reflux.

Parameter values were calculated from data illustrated in Figures 1–4 using values for gastric acidity stratified on the basis of pathologic esophageal acidity. Numbers in parentheses for ROC area give the 95% confidence interval.

In the index study, the ROC areas for integrated acidity were all higher than 0.7, and each curve was significantly different from 0.5. In contrast, for the data for time pH < 4, the ROC areas were all lower than 0.7, and the area for nocturnal gastric acidity in the index study was not significantly different from 0.5 (Table 3). The overall odds of pathologic time esophageal pH < 4 were higher than corresponding values for pathologic integrated esophageal acidity over 24 hours and during the nocturnal period (Table 3), because in GERD subjects treated with a PPI, measuring time esophageal pH < 4 significantly overestimates the prevalence of pathologic esophageal acid exposure during these periods.

---

### Three learning stages and accuracy-efficiency tradeoff of restricted boltzmann machines [^efe3f1d0]. Nature Communications (2022). High credibility.

Restricted Boltzmann Machines (RBMs) offer a versatile architecture for unsupervised machine learning that can in principle approximate any target probability distribution with arbitrary accuracy. However, the RBM model is usually not directly accessible due to its computational complexity, and Markov-chain sampling is invoked to analyze the learned probability distribution. For training and eventual applications, it is thus desirable to have a sampler that is both accurate and efficient. We highlight that these two goals generally compete with each other and cannot be achieved simultaneously. More specifically, we identify and quantitatively characterize three regimes of RBM learning: independent learning, where the accuracy improves without losing efficiency; correlation learning, where higher accuracy entails lower efficiency; and degradation, where both accuracy and efficiency no longer improve or even deteriorate. These findings are based on numerical experiments and heuristic arguments.

---

### Serotonin signaling regulates actomyosin contractility during morphogenesis in evolutionarily divergent lineages [^acd77af9]. Nature Communications (2023). High credibility.

Fig. 4
5HT2A activates junctional MyoII through Gβ13f/Gγ1, Dp114RhoGEF and Rho1.

a Schematic showing GPCR signaling modules that activate junctional and medial Rho1/MyoII. b – e Rho1 activity in 5HT2A loss-of-function (5HT2A−/ −). Rho1-GTP snapshots in control (left panel) and 5HT2A−/− (right panel) (b). Quantification of Rho-GTP sensor signal over-time in DV and AP oriented junctions (c), medial-apically (d), and amplitude of polarity (e) in different conditions. f – h Rho1 activity in 5HT2A overexpression (5HT2A + +). Images of Rho1-GTP at different time points (left to right) and different genotypes (top to bottom): control (top panel), 5HT2A + + (bottom panel) (f). Quantification of Rho1-GTP sensor signal in DV and AP oriented junctions (g), and amplitude of polarity (h) over-time. i – l Dp114RhoGEF levels in 5HT2A loss and gain-of-function. Snapshot of endogenous distribution of Dp114RhoGEF::mNeonGreen in control (left panel) and 5HT2A−/− (right panel) (i); and quantification of signal in the respective genotypes (j). Snapshots of sqh -Dp114RhoGEF::GFP in control (left panel), and 5HT2A + +(right panel) (k); and quantification of the junctional signals (l) in the respective genotypes. m – o MyoII distribution in 5HT2A overexpression and Dp114RhoGEF knockdown (Dp114RhoGEF-KD). Still images of MyoII in control (left), Dp114RhoGEF-KD (middle), and 5HT2A + + Dp114RhoGEF-KD (right) (m). Quantification of junctional MyoII intensities (n) and amplitude of polarity (o) for the above genotypes. p, q Gβ13f/Gγ1 overexpression (Gβ13f/Gγ1 + +) in 5HT2A−/ −. MyoII images in control (top-left), Gβ13f/Gγ1 + + (top-right), 5HT2A−/− (bottom-left) and 5HT2A-/- Gβ13f/Gγ1 + + (bottom-right) (p). Quantification of junctional MyoII in different genotypes (q). In the box plots in (c, d, e, g, h, j, l, n, o, q) the line in the middle is plotted at the median. The box extends from the 25th to 75th percentiles. The whiskers indicate minimum and maximum values. Statistics: ns P > 0.05, ✱ p ≤ 0.05, ✱✱ p ≤ 0.005, ✱✱ p ≤ 0.0005 from two-tailed Mann–Whitney test. P values in (c): 1 (P = 0.026), 2 (P = 5.83 × 10 −4), 3 (P = 5.83 × 10 −4), 4 (P = 5.83 × 10 −4), 5 (P = 0.038), 6 (P = 5.83 × 10 −4), 7 (P = 5.83 × 10 −4), 8 (P = 5.83 × 10 −4). P values in (d): 1 (P = 5.83 × 10 −4), 2 (P = 0.001), 3 (P = 5.83 × 10 −4), 4 (P = 5.83 × 10 −4). P values in (e): 1 (P = 0.259), 2 (P = 0.456), 3 (P = 0.097), 4 (P = 0.456). P values in (g): 1 (P = 0.129), 2 (P = 0.492), 3 (P = 0.007), 4 (P = 5.3 × 10 −5), 5 (P = 0.84), 6 (P = 0.206), 7 (P = 0.152), 8 (P = 1.06 × 10 −4). P values in (h): 1 (P = 0.968), 2 (P = 0.351), 3 (P = 0.442), 4 (P = 0.272). P values in (j): 1 (P = 0.02), 2 (P = 0.152), 3 (P = 0.003), 4 (P = 0.001), 5 (P = 0.001), 6 (P = 1.08 × 10 −4). P values in (l): 1 (P = 0.004), 2 (P = 0.008), 3 (P = 0.004), 4 (P = 0.003), 5 (P = 0.126), 6 (P = 0.008). P values in (n): 1 (P = 0.005), 2 (P = 0.018), 3 (P = 0.018), 4 (P = 0.005), 5 (P = 0.005), 6 (P = 0.003), 7 (P = 0.004), 8 (P = 0.004), 9 (P = 0.009), 10 (P = 0.004), 11 (P = 0.009), 12 (P = 0.004). P values in (o): 1 (P = 0.003), 2 (P = 0.004), 3 (P = 0.945). P values in (q): 1 (P = 7.99 × 10 −4), 2 (P = 0.018), 3 (P = 0.008), 4 (P = 0.008), 5 (P = 0.036), 6 (P = 0.005), 7 (P = 0.836), 8 (P = 0.181), 9 (P = 0.138), 10 (P = 0.295), 11 (P = 0.295), 12 (P = 0.005), 13 (P = 0.022), 14 (P = 0.056), 15 (P = 0.635), 16 (P = 0.313), 17 (P = 0.147), 18 (P = 0.635), 19 (P = 0.002), 20 (P = 0.002), 21 (P = 0.033), 22 (P = 0.033), 23 (P = 0.025), 24 (P = 0.001). n = number of embryos. Scale bars 5 µm. Source data are provided as a Source Data file.

---

### Mapping the probability of schistosomiasis and associated uncertainty, west Africa [^da5be341]. Emerging Infectious Diseases (2008). Low credibility.

Table
Bayesian logistic regression model of prevalence of infection with Schistosoma haematobium in children in 418 schools in Burkina Faso, Mali, and Niger, 2004–2006*

*CrI, Bayesian credible interval. Values for the fixed effects are odds ratios; note the odds ratios for the climate variables are on a common scale, where the variables were standardized to have a mean = 0 and SD = 1. The reference group for sex was boys and for age was 6–8 y. The number of children found to be infected with S. haematobium was modeled by using a binomial distribution described by the proportion infected and the total number sampled in each survey location. The proportion infected was modeled by using logistic regression with an intercept, covariates (sex, age, distance to perennial water body, land surface temperature, and a quadratic term for land surface temperature), and a random effect that described spatial correlation (i.e. clustering). Model outputs were distributions (termed posterior distributions) that can be summarized by using the mean, SD, and 95% CrI (representing the range of values that contains the true value with a probability of 95%). More details on the model are presented in the Technical Appendix.

A prevalence map for the study area was constructed, using the model, by predicting infection prevalence at the centroids of cells of a 0.15 × 0.15 decimal degree (≈18 km × 18 km) grid. This model was implemented with the spatial.unipred command of WinBUGS (details are provided in the Technical Appendix). Estimates from Bayesian models are distributions (termed posterior distributions) that represent the probability of each of a range of plausible values being true for the variable being modeled. To quantify the uncertainties surrounding the model predictions, we plotted the probability of each prediction location having a prevalence > 50%, rather than mean predicted prevalence at each location. The probabilities were calculated from the posterior distributions of predicted prevalence at each location (i.e. if 95% of the posterior distribution of predicted prevalence was > 0.5, the probability of prevalence > 50% at that location was 95%).

---

### Population coding of conditional probability distributions in dorsal premotor cortex [^3f3e65a0]. Nature Communications (2018). Medium credibility.

Next, for each neuron, we found the likelihood of the number of spikes given all possible movement directions (in 1° increments). This was done by assuming the number of spikes during the time period is a Poisson random variable with a mean rate determined by the value of the tuning curve at the direction being tested. If r i is the number of spikes during the interval for neuron i, s is the direction, and f i (s) is the value of the tuning curve (the expected number of spikes) for neuron i at direction s:

We assumed that neurons' activities were conditionally independent given the direction (a naïve Bayes decoder), and thus multiplied their probability distributions:

We can use Bayes rule to determine the likelihood of all the movement directions given the number of spikes of all neurons. Assuming a uniform prior, by Bayes rule:

Finally, we normalized(so it was a probability distribution), and this term was the decoded distribution.

The decoded direction was the direction corresponding to the peak of the distribution (the maximum likelihood decoded direction). The width of the decoded distribution was the full width half maximum (FWHM) of the decoded distribution.

To calculate the width of the decoded distribution over time (Fig. 5g), we decoded using a 50 ms sliding window of neural activity. All methods were the same as above, just replacing the 100 ms of activity prior to target onset with the given 50 ms of activity. This choice allowed us a better temporal resolution.

As we did decoding separately for each session, to do significance testing, we used a simple multilevel model analysis — specifically, a random intercepts model, where the baseline (intercept) can be different for every session. Thus, if there were 4 sessions, when looking at the width of the decoded distribution (w) as a function of the distance from center (d), we wrote the model as, wherethroughare indicator variables for whether the values are from a given session. We looked at whetherwas significantly different from 0 using a two-sided one-sample t -test. We used an equivalent approach to determine the significance of the relationship between latency and the difference between the decoded direction and the target direction. We also used an equivalent approach to determine the significance of the relationship between latency and the width of the distribution for expected reaches (expected reaches are those where the difference between the actual and expected target direction is less than 60°).

---

### Essential information on surgical heart valve characteristics for optimal valve prosthesis selection: expert consensus document from the European Association for Cardio-thoracic Surgery (EACTS)-the Society of Thoracic Surgeons (STS)-American Association for Thoracic Surgery (AATS) valve labelling task force [^7fcbdf9e]. The Journal of Thoracic and Cardiovascular Surgery (2021). High credibility.

Prosthesis–patient mismatch (PPM) risk estimation — standardized indexed effective orifice area (iEOA) cut-offs and probability method: In standardized PPM charts, the following PPM cut-off values should be used for severe PPM — non-obese (BMI < 30kg/m^2): indexed EOA ≤ 0.65 cm^2/m^2; obese (BMI ≥ 30kg/m^2): indexed EOA ≤ 0.55 cm^2/m^2. Expected indexed EOA values follow the same distribution as reference EOA values, and the exact probability of PPM can be calculated by defining the area under the curve below the PPM limit relative to the area under the curve of the whole expected indexed EOA distribution; in standardized PPM charts, the probability of PPM should be provided using this method, which is more useful for clinical decision-making than a 'yes/no' classification.

---

### Data-sharing and trustworthiness of trials evaluating cervical ripening in induction of labour: a meta-epidemiological study of randomised controlled trials [^2b90eeb4]. EClinicalMedicine (2025). Medium credibility.

Probability of random sampling according to baseline characteristics

Monte Carlo simulations (computational algorithms that use random sampling to generate numerical results) were used to generate p-values for excessive similarity or differences in baseline continuous variables between randomised groups. This method has been used previously by Carlisle et al. 2015 and similar works by our team. According to Carlisle's works, the resulting distribution of p-values for individual baseline variables closely resembles the distribution expected from randomly sampled, normally distributed, continuous variables, resulting in a uniform [0, 1] distribution. A dominance of simulation-generated p-values close to either 0 or 1 would cast doubt on the assumption that the randomisation was appropriately performed. Using the Kolmogorov–Smirnov test, we compared the distribution of simulation-generated p-values of continuous baseline variables against the expected uniform distribution in the shared and non-shared groups separately. We also plotted the cumulative distribution of simulation-generated p-values for baseline variables. Analysis was further stratified by trustworthiness categorisation. Whenever possible, subgroup analyses were performed in the non-shared group according to the reasons for not sharing IPD (data loss, not willing, and other reasons).

Statistical analysis results that cannot be reproduced with summary data

We attempted to reproduce univariable statistical analyses using the summary statistics and methods provided in publications. For comparing the means of continuous baseline variables, we used independent-sample t-tests and one-way analysis of variance (ANOVA), reproducing the p values with the corresponding means, standard deviations (with or without equal variances), and the number of participants reported in the tables and texts. We also reproduced analyses using the absolute numbers in tables presenting the cross-tabulation of categorical data, using Pearson's Chi-squared or Fisher's Exact tests, and calculating unadjusted odds ratios and risk ratios. We evaluated the consistency between the computed results and those published, reporting inconsistencies only when calculated p-values deviated from the reported values by more than 0.03 to consider the possible rounding of the reported summary data in the publications. Additionally, we plotted the cumulative distribution of a ratio (calculated p-value/reported p-value) for these inconsistencies (p-values that cannot be reproduced with summary data), separating baseline characteristics and outcomes in the shared and non-shared groups, as well as trustworthiness assessment groups.

---

### Individual heritable differences result in unique cell lymphocyte receptor repertoires of naïve and antigen-experienced cells [^7e15a435]. Nature Communications (2016). Medium credibility.

Estimating noise

Although equations 5, 6, 7 can be used to calculate the expected ratios for a particular gene, the observed ratios will also be affected by noise because of sampling of the repertoire. For a single gene X, the total number of reads that come from the primary allele (X 1) can be modelled using a binomial distribution:

where N is the total number of reads observed for gene X, and P is the probability of observing the major allele, given by:

with R (X) being the ratio defined from one of the equations 5, 6, 7. The resulting allele ratio for the gene X is then:

To estimate the noise in our data set assuming the null model of no chromosomal bias, we set R C = 1 and randomly sample from the binomial distribution using a range of read counts and R X values. The read counts used are drawn from the actual read counts of heterozygous alleles in the data set, and the distribution is sampled twice for each R X value; once with N = total number of reads, and again with N = number of non-productive reads. These two ratios can then be directly compared to estimate the expected variance between the non-productive and total read group.

To calculate a P -value for the observed difference between a total ratio and a non-productive ratio in a single allele, a total of 250,000 ratios were randomly generated for both the total read count and the non-productive read count, using R X values ranging from 0.2 to 5, and the absolute log difference between the two ratios was calculated (because the allele ratios will be linearly correlated with the R X values in both sequence sets, the R X values should not affect the final distribution). The ratio differences were used to create a sample distribution, and the P -value was calculated as the total number of repetitions that were greater than the observed ratio. An overall P -value was then calculated by combining together the P -values of the individual alleles using Fisher's method.

Estimating chromosomal bias parameter

To estimate the true value of the chromosomal bias parameter, a modified gradient descent algorithm was used. The genetic parameter, R X, was optimized for each allele individually, whereas the chromosomal bias parameter, R C, was assumed to be constant across all alleles. Because we do not know which of the two alleles is on the major chromosome, we consider both R C and 1/ R C, and define the error as the smallest difference between the observed ratio and the model prediction using either parameter.

---

### A computational method for cell type-specific expression quantitative trait loci mapping using bulk RNA-seq data [^5e62b991]. Nature Communications (2023). High credibility.

Simulation setup

We describe how the cell type proportions are simulated. In the first scenario, let X ~ U (a, b) denote a random variable X sampled from a continuous uniform distribution ranging from a to b. Specificallyand U i q ~ U (−4, 4). In the second scenario, to allow cell types to reflect observed proportions with wide and narrow ranges of proportions, we simulated ρ i 1 from a beta distribution with shape parameters 10 and 24 (values derived based on maximum likelihood estimates from fitting a beta distribution to CMC's astrocyte cell type proportions), where ϵ i was sampled from a centered normal distribution with standard deviation 0.02, and ρ i 3 = 1 − ρ i 1 − ρ i 2. If ρ i 3 < 0, we set it to zero and normalize the proportions across cell types. For the third scenario, proportions are first simulated under the second scenario. Next, for each cell type, the initial proportions greater than the 99% quantile were replaced by values sampled from U (0.7, 0.9) while initial proportions less than the 1% quantile were replaced by values sampled from U (0, 0.1). These final values are re-normalized across cell types to sum to one.

For n = 300, we simulate p = 4 baseline covariates. The first covariate is X i 1, which represents read-depth, is simulated by a gamma distribution with shape parameter set to 600 and rate parameter set to 100, based on empirical MLE estimates from CMC samples. X i 2, which represent sex, is generated by a Bernoulli distribution with success probability of 0.5. X i 3 is generated by a continuous uniform distribution ranging from −1 to 1. X i 4 is simulated by a standard normal distribution. These latter two variables represent arbitrarily distributed continuous covariates. Continuous covariates X i 1, X i 3, and X i 4 are centered and scaled with zero mean and unit variance. Assuming Hardy Weinberg equilibrium, genotypes were generated using a categorical distribution with probabilities, m A (1 − m A), m A (1 − m A), for A A, A B, B A, B B, respectively, where m A denotes the minor allele frequency. We set m A = 0.2.

---

### Extensive benchmarking of a method that estimates external model performance from limited statistical characteristics [^cae2d43b]. NPJ Digital Medicine (2025). Medium credibility.

Efficient approximate weighting

As in this benchmark we deal with larger scale problems than those tested in ref. we developed a more efficient algorithm by approximating the problem represented in Equation (5). First, to satisfy equality constraints, we formulate the following optimization problem:Second, to maintain weights that are as close to uniform as possible, we use an exponentiated gradient algorithmand initialize the algorithm with uniform weights. At every iteration t of the gradient descent algorithm, it attempts to decrease the objective while maintaining the normalization constraints using the following updates:where ∇ f (w) is the gradient of f, specifically, ∇ f (w) = Φ (Φ ⊤ w t − μ ext), and α is a pre-specified optimization rate. As every iteration locally minimizes f (w t +1) + α ⋅ K L (w t +1 ∥ w t), it tends to reduce the objective while attempting to maintain proximity between the weights in consecutive steps. Therefore, we use the heuristic of initializing the search with uniform weights.

Assumptions and conditions for running the algorithm

We pose two assumptions about the underlying data distributions of the internal and external cohorts that should be satisfied to give accurate estimations. First, for all the features contributing to the prediction model, whenever the external joint probability of a given feature and outcome values is greater than zero, so is their internal one. We call this assumption one-sided positivity, as it is analogous to the positivity, also known as overlap, assumption in causal inference. Second, we assume that the external distribution is relatively close to the internal one among the set of distributions that have expectations μ ext. Intuitively, the plausibility of this assumption increases with the richness of the transformation ϕ of which we share external statistics. In other words, the assumption states that the statistics shared between the external and internal systems are sufficient to give a good approximation of the data shift between them.

Estimation pipeline

We implemented an estimation pipeline that combines the weighting algorithm and various tests to assess the feasibility of accurate estimations. Specifically, it includes the following steps:
Assess one-sided positivity for each element in the external statistics
If one-sided positivity holds: run the optimization algorithm to solve the problem represented in Equation (6) and obtain weights.
If external statistics are reproduced: estimate external performance using the weighted dataset.

Confidence intervals can be computed using repeated re-sampling and weighting of the internal cohort.

---

### Evaluating the utility of identity-by-descent segment numbers for relatedness inference via information theory and classification [^1841600c]. G3 (2022). Medium credibility.

Methods

We analyzed the potential for using coefficients of relatedness r (defined below) either alone or both r and n, the number of IBD segments a pair of relatives share, to infer the pair's degrees of relatedness D.

Mutual information discrete definition and binning approaches

MI is difficult to calculate for continuously valued variables without a known distribution and whose distribution must therefore be estimated from finite data. Moreover, estimating the MI between one continuous and one discrete random variable is in general nontrivial and multiple approaches exist for this estimation, such as nearest-neighbor and binning methods. To enable our MI calculations [such as I (r; D)], we used a procedure that bins data points of r and avoids biased MI estimates in our finite but large sample size. In computing MI, we treated the binned feature vector(wherehas the possibility of being 1D when representing r or n) and the degree of relatedness D as two discrete random variables with realizations f and, respectively. If we know the probability mass functions (pmfs) of the discrete random variables X and Y with realizations x and y, we can calculate MI using its definition aswhereis the joint pmf of X and Y and p X, p Y are the marginal pmfs of X and Y, respectively.

Binning a continuous variable in order to use Equation (1) introduces the difficulty of picking the right bin size. It has been shown that MI is sensitive to bin size and that its stability with respect to this variable is dependent on the sample distribution. Our distributions and sample sizes of r yielded a large range of bin sizes that have stable MI estimates (see the flat regions of each curve in Supplementary Fig. 1). Because the normalized MI gain G N is a fraction composed of MI values (Results), its correct calculation relies on the unbiasedness of the various MI quantities that form it. At bin sizes smaller than 150 pairs per bin (ppb), both the means and standard deviations (Supplementary Fig. 2) of our MI quantities increase rapidly. Given this, in our calculations of MI, we binned r at 150 ppb. Our binning converts a continuous value of r to its nearest bin-value inevenly spaced numbers from, where N represents all sampled values for the desired feature r, n, or (r, n). Here and belowrepresents all sampled training data points r.

---

### Quantum-secure covert communication on bosonic channels [^29b3267c]. Nature Communications (2015). Medium credibility.

Willie's detector

Willie's detection problem can be reduced to a test between two simple hypotheses where the log-likelihood ratio test minimizes(ref.). The test statistic for the log-likelihood ratio test is derived in the Supplementary Note 5 and is simply the total number of clicks Y observed by Willie. Willie compares Y with a threshold S, accusing Alice if Y ≥ S. Willie chooses the value of S that minimizes Willie's detection error probability.

For each pair of parametersas well as Alice's transmission state, we perform m experiments, recording the observed number of clicks Y. We denote byandthe sequences of experimentally observed click counts when Alice does not transmit and transmits, respectively. To estimate Willie's detection error probability, we construct empirical distribution functionsand, wheredenotes the indicator function. The estimated probability of error is then

We perform a Monte-Carlo study using 10 5 simulations perpair. We generate, encode and detect the messages as in the physical experiment and use equation (2) to estimate Willie's probability of error, but simulate the optical channel induced by our choice of a laser-light transmitter and an SPD using its estimated characteristics reported in Table 1. Similarly, we use the values in Table 1 for our analytical Gaussian approximation ofdescribed in the Supplementary Note 6.

We compute the confidence intervals for the estimate in equation (2) using Dvoretzky–Keifer–Wolfowitz inequality, which relates the distribution function F X (x) of random variable X to the empirical distribution functionassociated with a sequenceof m i.i.d. draws of the random variable X as follows:

where. For x 0, the (1− α) confidence interval for the empirical estimate of F (x 0) is given bywhere. Thus, is used for reporting the confidence intervals in Fig. 4.

---

### Variation in mutational (co) variances [^b81af1f4]. G3 (2023). Medium credibility.

Comparing M matrices

To compare the M matrices of founder genotypes, we performed eigendecomposition of each N2 or PB306 M matrix independently. The resulting first eigenvectors usually contain most of the genetic variance due to mutation, the genetic variance that is measured by the eigenvalue, and can thus be called. Random sampling expectations for the six, given the number of measured MA lines, were computed once by shuffling block and MA line identity per N2 or PB306 genotype. To obtain a null value along each eigentrait measuring noise, we rotated the 1,000 random matrices along the six eigentraits of the observed dataset corresponding to each genotype (see the previous section).

We then compared the relative direction of the phenotypic dimension with more genetic variance between N2 and PB306 by computing the angle between their respective:

As bothandare the first eigenvector of the M matrix and their eigenvaluea single (positive) scalar, values between 90° and 180° were transformed so thatalways remains between 0° and 90° (Θ′ = 180∘ − Θ, which results from usinginstead ofin equation (5)). The null expectation forwhen using random vectors of a matrix with dimension two is then 45°. For matrices of higher dimension (with a higher number of traits), such as our M matrices, the angle will be higher than 45°, increasing asymptotically towards 90° as the number of dimensions goes to infinity (simulations not shown). Credible intervals for the null angle ofwere obtained by sampling 1,000 pairs of random vectors from a uniform distribution.

Additionally, we used a metric introduced byto estimate the amount of genetic variance in one M matrix along theof a second matrix. For this, we first computed the amount of genetic variance of backgroundalong theof the background(noted):

Then we definedas the ratio between the genetic variance alongand the maximum amount of variance in any phenotypic direction in the second M matrix (noted here, the first eigenvalue of thematrix):

values are comprised between 0 (no genetic variance along thephenotypic dimension) and 1 (when theof the two matrices can be said to be aligned). The null expectation ofis:

whereis the mean of all six eigenvalues of the projected, the average genetic variance across any phenotypic dimensions. Credible intervals ofandwere obtained by sampling 1,000 times the posterior distributions of the M matrices.

---

### Reciprocity of social influence [^7bc552ea]. Nature Communications (2018). Medium credibility.

The second choice of the partner was computed differently for susceptible and insusceptible partners. For experiment 1, the influence that the insusceptible partner took from the participants in each trial was chosen with a probability of 0.65 from a uniform distribution on the interval [0, 0.2], with a probability of 0.2 randomly from a uniform distribution on the interval [0.3, 0.7], and with a probability of 0.15 randomly from a uniform distribution on the interval [0.7, 0.9]. For experiment 2, the influence that the insusceptible partner took from the participants was chosen randomly from a uniform distribution on the interval [0, 0.2]. For the susceptible partner, in all experiments, the influence was chosen with a probability of 0.5 randomly from a uniform distribution on the interval [0.7, 1], with a probability of 0.2 randomly from a uniform distribution on the interval [0.3, 0.7], and with a probability of 0.3 randomly from a uniform distribution on the interval [0, 0.3]. In the transition block, the influence of the partner was a linear interpolation between the susceptible and the insusceptible partner:where inf s and inf ins were the influences of the susceptible and insusceptible partners, respectively (as explained above). λ gradually increased with time from 0 at the beginning to 1 at the end of the transition block for a transition from the susceptible to the insusceptible condition. For the transition from the insusceptible to the susceptible condition, λ decreased gradually from 1 to 0.

In experiment 1, on average, the advice that the partners took from the participants was 0.3 and 0.55 in the insusceptible and susceptible conditions, respectively. In experiment 2A, on average, the advice that the partner took from the participants was 0.07 and 0.5 in the insusceptible and susceptible conditions, respectively. The second choice of the partners in experiment 2B was designed exactly the same as in experiment 2A and the average advice that the partner took in the insusceptible and the susceptible condition was identical to experiment 2A.

---

### Significance, errors, power, and sample size: the blocking and tackling of statistics [^3bcc4e96]. Anesthesia and Analgesia (2018). Low credibility.

Inferential statistics relies heavily on the central limit theorem and the related law of large numbers. According to the central limit theorem, regardless of the distribution of the source population, a sample estimate of that population will have a normal distribution, but only if the sample is large enough. The related law of large numbers holds that the central limit theorem is valid as random samples become large enough, usually defined as an n ≥ 30. In research-related hypothesis testing, the term "statistically significant" is used to describe when an observed difference or association has met a certain threshold. This significance threshold or cut-point is denoted as alpha (α) and is typically set at .05. When the observed P value is less than α, one rejects the null hypothesis (Ho) and accepts the alternative. Clinical significance is even more important than statistical significance, so treatment effect estimates and confidence intervals should be regularly reported. A type I error occurs when the Ho of no difference or no association is rejected, when in fact the Ho is true. A type II error occurs when the Ho is not rejected, when in fact there is a true population effect. Power is the probability of detecting a true difference, effect, or association if it truly exists. Sample size justification and power analysis are key elements of a study design. Ethical concerns arise when studies are poorly planned or underpowered. When calculating sample size for comparing groups, 4 quantities are needed: α, type II error, the difference or effect of interest, and the estimated variability of the outcome variable. Sample size increases for increasing variability and power, and for decreasing α and decreasing difference to detect. Sample size for a given relative reduction in proportions depends heavily on the proportion in the control group itself, and increases as the proportion decreases. Sample size for single-group studies estimating an unknown parameter is based on the desired precision of the estimate. Interim analyses assessing for efficacy and/or futility are great tools to save time and money, as well as allow science to progress faster, but are only 1 component considered when a decision to stop or continue a trial is made.

---

### Validation of monte carlo estimates of three-class ideal observer operating points for normal data [^26b6e5d9]. Academic Radiology (2013). Low credibility.

Rationale and Objectives

Traditional two-class receiver operating characteristic (ROC) analysis is inadequate for the complete evaluation of observer performance in tasks with more than two classes.

Materials and Methods

Here, a Monte Carlo estimation method for operating point coordinates on a three-class ROC surface is developed and compared with analytically calculated coordinates in two special cases: (1) univariate and (2) restricted bivariate trinormal underlying data.

Results

In both cases, the statistical estimates were found to be good in the sense that the analytical values lay within the 95% confidence interval of the estimated values about 95% of the time.

Conclusions

The statistical estimation method should be key in the development of a pragmatic performance metric for evaluation of observers in classification tasks with three or more classes.

---

### Estimation-uncertainty affects decisions with and without learning opportunities [^9fad6fc9]. Nature Communications (2025). High credibility.

Value updating is similar to Q-learning, with the exception of a dynamic learning rate which is specific for each object and depends on its current level of estimation-uncertainty:

The prediction erroris defined as above (Eq. 2).

The estimation-uncertainty of object i is estimated by the standard deviation σ(t) i, which is updated according to:

The learning rate α(t) i for object i in trial t is given by:

Q-values were initialized as the expected value of a randomly selected option given three possible outcomes r, their respective magnitudes R(r) and probabilities p(r): where p(r) = 1/3). The constant σ 0 was calculated as the standard deviation of the categorical distribution given R(r) and p(r) = 1/3:

For the Kalman-filter learning models, the initial estimation-uncertainty was set to σ 0.

All learning rates (i.e. all α) were constrained between 0 and 1, while all decision weights (i.e. all β) were constrained between −20 and 20. To ensure that the results are not caused by using too strict priors, we allowed the fitted parameters to vary widely across both individual and group-level parameter-fits (i.e. for individual fits, the prior means for all α and all β were set to 0.5 and 0, respectively, while the variance for all fits was set to 16.25).

---

### Mortality rates among hospitalized patients with COVID-19 infection treated with tocilizumab and corticosteroids: a Bayesian reanalysis of a previous meta-analysis [^f72c5729]. JAMA Network Open (2022). High credibility.

Posterior Predictive Distributions

As shown in previous subsections, tocilizumab's association with mortality benefit in hospitalized patients with COVID-19 receiving IMV is uncertain, as is the probability of clinically meaningful benefits in other respiratory subgroups (Figure 1). Figure 2 A and eTable 4 in the Supplement show the predictive distributions for future trial populations for each subgroup while assuming weakly informative priors. The simple oxygen only subgroup showed large benefit (OR, 0.69; 95% CrI, 0.40–1.09) but now acknowledges that the next trial has a small possibility of showing harm (posterior probability of 5.6% for OR > 1). In patients receiving NIV, the median OR was 0.80 (95% CrI, 0.46–1.25) along with a posterior probability of 12.4% for harm. The median predictive OR was 0.88 (95% CrI, 0.47–1.40) in the IMV group, and 27.9% of the area under the curve is to the right of 1, representing the probability of potential harm.

Figure 2.
Posterior Predictive Distributions and Probabilities Assuming Weakly Informative Priors

Posterior predictive distributions and probabilities of each subgroup. A, Posterior predictive distributions, in which point estimates (black solid-circle data markers) depict the median, and interval bars represent the 95% credible (highest-density) intervals (CrIs). B, Cumulative posterior probabilities, which correspond to the probabilities that future studies (specific to each subgroup) will find a point estimate lower than or equal to the effect size on the x-axis (X). Underlying weakly informative priors are normal, mean (SD) of 0 (0.82) for coefficients and half-normal of 0.5 for the between-study standard deviation. OR indicates odds ratio.

In a bayesian framework, one can also calculate the probability that future study populations will find a mean effect size less than a specific threshold (Figure 2 B; eTable 4 in the Supplement). Regarding future study populations that exclusively include patients receiving simple oxygen only, there is a 94.2% probability they will find any benefit (< 1 OR) and an 88.9% probability for greater benefit (OR < 0.9). In contrast, regarding future studies that exclusively include patients receiving IMV, these probabilities were lower (72.1% and 53.3%, respectively).

---

### Fundamental limits to learning closed-form mathematical models from data [^a345300f]. Nature Communications (2023). High credibility.

Given a finite and noisy dataset generated with a closed-form mathematical model, when is it possible to learn the true generating model from the data alone? This is the question we investigate here. We show that this model-learning problem displays a transition from a low-noise phase in which the true model can be learned, to a phase in which the observation noise is too high for the true model to be learned by any method. Both in the low-noise phase and in the high-noise phase, probabilistic model selection leads to optimal generalization to unseen data. This is in contrast to standard machine learning approaches, including artificial neural networks, which in this particular problem are limited, in the low-noise phase, by their ability to interpolate. In the transition region between the learnable and unlearnable phases, generalization is hard for all approaches including probabilistic model selection.

---

### Research techniques made simple: sample size Estimation and power calculation [^b2b6b836]. The Journal of Investigative Dermatology (2018). Low credibility.

Sample size and power calculations help determine if a study is feasible based on a priori assumptions about the study results and available resources. Trade-offs must be made between the probability of observing the true effect and the probability of type I errors (α, false positive) and type II errors (β, false negative). Calculations require specification of the null hypothesis, the alternative hypothesis, type of outcome measure and statistical test, α level, β, effect size, and variability (if applicable). Because the choice of these parameters may be quite arbitrary in some cases, one approach is to calculate the sample size or power over a range of plausible parameters before selecting the final sample size or power. Considerations that should be taken into account could include correction for nonadherence of the participants, adjustment for multiple comparisons, or innovative study designs.

---

### Determining clinical course of diffuse large B-cell lymphoma using targeted transcriptome and machine learning algorithms [^49098776]. Blood Cancer Journal (2022). Medium credibility.

Theorem

Assume that the probabilities in the likelihood are independent, uniformly distributed random variables. Then, the expected value of the likelihood is

Proof

By the previous lemma and the independence of the random variables, The limit of the expected value is

Therefore, as the dimension increases, the likelihood will never approach 0 uniformly. Applying the function h to the likelihood does not change the relative order of the probability estimates of the classes. However, the probabilities will have more reasonable values than 0 and 1.

We can also show that the functionis unique under certain conditions.

Lemma

Letbe a positive continuous function of positive real numbers. If f is multiplicative, thenfor some constant a.

In the case of the functional transform on the likelihood, the assumption of the multiplicative property on the function h is a natural extension of the naïve Bayesian assumption.

If we require that the likelihood approaches a non-zero limit as d approaches infinity, then the function could have the formfor a constant c.

Theorem

If h is multiplicative andthen, where.

Proof

The previous lemma shows thatSimilar to the previous proof, the expectation isBy the assumption, we haveLettingand, thenFurthermore, andTherefore,

When the dimension d is high, the independence assumption of the naïve Bayesian classifier is unlikely to be true in most applications. Consequently, the probability estimates are unrealistic. Our proposed extension can solve this problem.

Example

Consider a two-class problem with d-dimensional Gaussian distributions, with means of

andand the same covariance matrixthe inverse matrix isConsider the probability estimations for the point. The true probability for class 1 isFor the original naïve Bayesian classifier, and for our proposed classifier,

Figure S3 shows the three probability estimates for d = 10 and r = 0.5. The naïve Bayesian probability estimates change steeply around the boundary owing to the independence assumption. In contrast, our proposed method closely approximates the true probabilities.

---

### Essential information on surgical heart valve characteristics for optimal valve prosthesis selection: expert consensus document from the European Association for Cardio-thoracic Surgery (EACTS)-the Society of Thoracic Surgeons (STS)-American Association for Thoracic Surgery (AATS) valve labelling task force [^7d805c8b]. The Journal of Thoracic and Cardiovascular Surgery (2021). High credibility.

Aortic surgical heart valve prosthesis–patient mismatch (PPM) probability — this standardized chart provides the percentage probability of severe PPM after implantation of an aortic surgical heart valve into a specific patient and calculates this probability using the distribution of reference effective orifice areas (EOAs), patient body surface area (BSA), and the body mass index (BMI)-adjusted severe PPM cut-off. Severe PPM is defined as iEOA: ≤ 0.65 cm^2/m^2 in patients with BMI < 30 kg/m^2 and as iEOA: ≤ 0.55 cm^2/m^2 in patients with BMI ≥ 30 kg/m^2. Study details show EOA, mean ± SD [cm^2] by labelled valve size 19/21/23/25/27/29 as 1.1 ± 0.2, 1.3 ± 0.3, 1.6 ± 0.4, 1.8 ± 0.4, 2.2 ± 0.5, and 2.8 ± 0.3, with patients per valve size (n) 25, 52, 79, 65, 43, 32; the design is prospective, centers per valve size (n) 3, pooled data no, regulatory no, echo core lab yes, the period of data collection 2012–2014, and timing of postop echo 3 months. A disclaimer states the chart is a support tool to estimate PPM probability and that actual risk further depends on specific patient characteristics and operative technique.

---

### The expected behaviour of random fields in high dimensions: contradictions in the results of bansal and peterson [^657f9d59]. Magnetic Resonance Imaging (2022). Medium credibility.

Bansal and Peterson (2018) found that in simple stationary Gaussian simulations Random Field Theory incorrectly estimates the number of clusters of a Gaussian field that lie above a threshold. Their results contradict the existing literature and appear to have arisen due to errors in their code. Using reproducible code we demonstrate that in their simulations Random Field Theory correctly predicts the expected number of clusters and therefore that many of their results are invalid.

---

### Consistent estimation in mendelian randomization with some invalid instruments using a weighted median estimator [^ffbe4726]. Genetic Epidemiology (2016). Low credibility.

As an illustration, two sets of weights are given in Table 1, and percentiles are calculated for each set of weights as well as for the simple median (equal weights). As the first set of weights are symmetric, the weighted median in this case equals the simple median. However, less weight is given to outlying estimates, and the empirical distribution function (Fig. 3, red line) is close to the median value across a wider range of the distribution. Confidence intervals for the weighted median, which can be obtained by a parametric bootstrap method, should therefore be narrower. In the second set of weights, smaller estimates happen to receive more weight (Fig. 3, blue line). The weighted median estimate will be interpolated between ratio estimatesand, but will be closer toas the percentile p 4 is closest to 50%. The exact weighted median estimate in this case will beThe weighted median can also be thought of as the simple median from a set of values (a pseudopopulation) in which the ratio estimatefor variant 1 appearstimes, ratio estimatefor variant 2 appearstimes, and so on. R code to calculate weighted median estimates, confidence intervals, standard errors and P ‐values is provided in Supporting Information Appendix 2.

Figure 3
Empirical distribution functions of ordered ratio instrumental variable estimates used for calculation of the simple median estimate (black) and two weighted median estimates (shown in red and blue) using the weights given in Table 1.

Table 1
Weights and percentiles of weighted median function

Analogously to the IVW method, we suggest using the inverse of the variance of the ratio estimates as weights:These weights are derived from the delta method for the variance of the ratio of two random variables, and represent the reciprocal of the variance of the ratio estimates (the inverse‐variance weights) (Thomas et al. 2007). Standardized weights are. The unstandardized weights are identical to those used in the IVW estimator. Only the first‐order term from the delta expansion is used here; further terms could be considered, although we found that they did not affect estimates or standard errors from the weighted median method substantially.

---

### Characteristics of baseline frequency data in spinal RCTs do not suggest widespread non-random allocation [^49894cfb]. European Spine Journal (2023). Medium credibility.

Results

Dataset

A total of 228 articles were first identified in the database search (Fig. 1). Sixty-one of those were excluded for reasons including not containing categorical data, not presenting baseline data separately by arm or having been misclassified as an RCT, among others. One hundred sixty-seven articles were retained for analysis, containing a total of 921 categorical variables, for an average 5.5 variables per study. Figure 2 shows the distribution of the number of variables per study.

Fig. 2
Distribution of the number of baseline frequency variables reported by articles

Observed p values

A supplementary file containing all variable-wise and study-wise p values is available as Appendix B. Figure 3 A and B shows, respectively, all study-wise and variable-wise p values, ordered and plotted according to their percentile. Both sets of values show a near-uniform distribution, as would be expected in the setting of genuine random allocation without data fabrication. Only a small tail towards p values of 1 (Fig. 3 B) was observed. The mean variable-wise p values were 0.52, and the mean study-wise p value was 0.58.

Fig. 3
A The observed distribution of 921 variable-wise p values across 167 trials, plotted by percentile rank. B The observed distribution of study-wise p values in 167 trials, plotted by percentile rank. The distributions observed are close to the flat distributions expected if all studies report data from genuine randomized experiments. In the setting of systemic fabricated data or non-random allocation, an S shaped curve would be expected

Statistical findings

The number of trials with study-wise p values less than 0.01, less than 0.05, and more than 0.95 were not greater than expected by random chance. Only in the p > 0.99 category did we observe a significant difference between the observed and expected number of studies: only two were expected, but five were identified (p = 0.03) (see Table 1).

Table 1
Observed and expected number of studies for each p value range

The group " p < 0.05" includes those studies in the group " p < 0.01", likewise the group " p > 0.95" includes those studies in the group " p > 0.99"

* Exact binomial probability of n observed events or greater

 Statistically significant (p < 0.05)

---

### Daily forecasting of regional epidemics of coronavirus disease with Bayesian uncertainty quantification, United States [^737b3281]. Emerging Infectious Diseases (2021). Medium credibility.

Statistical Model for Noisy Case Reporting

We used a deterministic compartmental model to predict the expected number of new confirmed COVID-19 cases reported daily. In other words, we assumed that the number of new cases reported over a 1-day period was a random variable and that the expected value would follow a deterministic trajectory. We further assumed that day-to-day fluctuations in the random variable were independent and characterized by a negative binomial distribution, denoted as NB(r, p). We used NB(r, p) to model noise in reporting and case detection. The support of this distribution is the nonnegative integers, which is natural for populations. Furthermore, the shape of NB(r, p) is flexible enough to recapitulate an array of unimodal empirical distributions. With these assumptions, we obtained a likelihood function (Appendix 1 Equation 27) in the form of a product of probability mass functions of NB(r, p). Formulation of a likelihood is a prerequisite for standard Bayesian inference; however, some related methods, such as approximate Bayesian computation, do not rely on a likelihood function.

Online Learning of Model Parameter Values through Bayesian Inference

We used Bayesian inference to identify adjustable model parameter values for each MSA of interest. In each inference, we assumed a uniform prior and used an adaptive Markov chain Monte Carlo algorithm to generate samples of the posterior distribution for the adjustable parameters (Appendix 1).

The maximum a posteriori (MAP) estimate of a parameter is the value corresponding to the mode of its marginal posterior, where probability mass is highest. Because we assumed a uniform prior, our MAP estimates were maximum-likelihood estimates.

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^98feb86b]. Annals of the American Thoracic Society (2023). High credibility.

Quantitative imaging study design — precision versus bias and measurement efficiency emphasize that "Bias" refers to systematic errors leading to inaccuracy and "Precision" refers to measurement reproducibility; bias "can be neither detected in the data nor decreased by making more measurements" and is avoided a priori by rigorous design, whereas precision can be checked and adjusted by increasing sample size or number of measurements. Because "more measurements do not increase accuracy if bias exists at higher levels of study design", automated exhaustive measurements "could aggravate inaccuracy and reduce efficiency", and it is "more efficient to invest extra effort into the higher levels of study design"; with unbiased sampling "it is not always necessary to exhaustively analyze each voxel". In a well-designed study, precision is tuned so the within-subject coefficient of error is "significantly smaller than" between-subject variation; "repetitive imaging can often be made to assess response to intervention", and baseline between-subject variability is less important in longitudinal designs in which subjects act as their own controls.

---

### A recurrent network model of planning explains hippocampal replay and human behavior [^11787816]. Nature Neuroscience (2024). High credibility.

In more detail, we took p t during nonguided trials to be uniform between 0 and 7 s — the maximum response time allowed, beyond which subjects were considered disengaged, and the trial was discarded and reset. For p d (t d), we assumed a shifted log-normal distribution, where parameters μ, σ and δ were obtained from a maximum-likelihood estimation based on the collection of response times t r ≡ t d observed during guided trials. For a given δ, the maximum-likelihood values of μ and σ are simply given by the mean and s.d. of the logarithm of the shifted observations. Thus, to fit this shifted log-normal model, we performed a grid search overat 1-ms resolution and selected the value under which the optimal (μ, σ) gave the largest likelihood. This range of δ was chosen to ensure that (1) only positive values ofhad positive probability and (2) all observedhad nonzero probability. We then retained the optimal μ, σ and δ to define the prior over p d (t d) on nonguided trials for each subject.

According to Bayes' rule, the posterior is proportional towhereTherefore, the posterior is given byresulting in the following posterior mean:Here, p d (t d ∣ t d < t r) denotes p d (t d) renormalized over the interval t d < t r and the condition (t d < t r) is equivalent to (t t > 0). We note that the integral runs from δ to t r because p d (t d) = 0 for t d < δ. Because δ simply shifts the distribution over t d, we can rewrite this asThis is useful because the conditional expectation of a log-normally distributed random variable with δ = 0 is given in closed form bywhere Φ(⋅) is the cumulative density function of the standard Gaussian. This allows us to compute the posterior mean thinking time for an observed response time t r in closed form as

---

### Three learning stages and accuracy-efficiency tradeoff of restricted boltzmann machines [^2d291dc7]. Nature Communications (2022). High credibility.

Our second example (cf. Fig. 3) is closer in spirit to traditional machine-learning applications and involves pattern recognition and artificial image generation. The target distribution p (x) generates 5 × 5 pixel images with a "hook" pattern comprised of 15 pixels (see Fig. 3 a) implanted at a random position in a background of noisy pixels that are independently activated (white, x i = 1) with probability q = 0.1 (see also Supplementary Note 2 B for more details). Periodic boundary conditions are assumed, meaning that p (x) is translationally invariant along the two image dimensions.

We also consider a one-dimensional variant of this example with only M = 4 (M = 5) visible units and an implanted "010" ("0110") pattern, cf. Fig. 3 d. In this case, we can solve the continuous-time learning dynamics (η → 0 limit of (4)) for the exact target and model distributions p (x) and, obviating artifacts caused by insufficient training data or biased gradient approximations, see also Supplementary Note 1.

Our third example (cf. Fig. 4 a–c) is a simplified digit reproduction task. Patterns of the ten digits 0 through 9 (see Fig. 4 a) are selected and inserted uniformly at random into image frames of 5 × 7 pixels, with the remaining pixels outside of the pattern again activated with probability q = 0.1 (see Supplementary Note 2 C for details). No periodic boundary conditions are imposed, i.e. the input comprises proper, ordinary images.

In our fourth example (cf. Fig. 4 d, e), we train RBMs on the MNIST dataset, which consists of 28 × 28-pixel grayscale images of handwritten digits. It comprises a training set of 60,000 and a test set of 10,000 images. We convert the grayscale images with pixel values between 0 and 255 to binary data by mapping values 0… 127 to 0 and 128… 255 to 1 (see also Supplementary Note 2 D).

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^b1513dd5]. CDC (2011). Medium credibility.

Table 1b — Minimum sample size for detecting geometric mean (GM) ratio k with α = 0.05 and W = 0.9 — presents required sample sizes by standard deviation (S) strata and defines parameters; α is significance level, W is power, GM is geometric mean, and S is standard deviation, noting that 1.2 is the standard deviation of national viral load (VL) data. Each jurisdiction will need to assess the standard deviation of their local VL data and then determine the appropriate sample size needed to assess VL, and if the sample size is inadequate to meet the recommended case inclusion criterion, an alternate method may need to be used, such as combining multiple years of data. For k = 3, sample sizes across S = 1, 1.1, 1.2, 1.3, 1.4, 1.5 are 75, 91, 108, 127, 147, 169, and Table 1b uses power = 90%; jurisdictions may also explore differences in means of viral loads, including categorical differences in the proportion with undetectable or very low VL.

---

### Venous thromboembolism: diagnosis and treatment [^bb2a0bb8]. American Family Physician (2025). High credibility.

Regarding classification and risk stratification for pulmonary embolism, more specifically with respect to risk stratification, AAFP 2025 guidelines recommend to use validated clinical prediction rules to estimate the pretest probability of VTE and guide further evaluation.

---

### Identifying domains of applicability of machine learning models for materials science [^9c5f816f]. Nature Communications (2020). High credibility.

A predictive ML model is then a functionaiming to minimize the expected error (also called prediction risk)measured by some non-negative loss function l that quantifies the cost incurred by predicting the actual property value y with f (x). Examples for loss functions are the squared error, the absolute error, and, for non-zero properties, the relative error. Here P denotes some fixed probability distribution that captures how candidate materials are assumed to be sampled from the materials class (this concept, while commonly assumed in ML, is an unnecessary restriction for high-throughput screening as we discuss in more detail below). Since the true prediction risk is impossible to compute directly without perfect knowledge of the investigated materials class, models are evaluated by the test error (or empirical risk)defined as the average of the individual errors (losses) e i (f) = l (f (x i), y i) on some test set of m reference data points. The samples in this test set are drawn independently and identically distributed according to P and are also independent of the model — which means in practice that it is a random subset of all available reference data that has been withheld from the ML algorithm. In order to reduce the variance of this estimate, a common strategy is cross-validation, where this process is repeated multiple times based on partitioning the data into a number of non-overlapping "folds" and then to use each of these folds as test sets and the remaining data as a training set to fit the model.

This test error properly estimates the model performance globally over the whole representation space X (weighted by the distribution P used to generate the test points). This is an appropriate evaluation metric for selecting a model that is required to work well on average for arbitrary new input materials that are sampled according to the same distribution P. This is, however, not the condition of high-throughput screening. Here, rather than being presented with random inputs, we can decide which candidate materials to screen next. This observation leads to the central idea enabled by the DA analysis proposed in this work: if the employed model is particularly applicable in a specific subdomain of the materials class, and if that subdomain has a simple and interpretable shape that permits to generate new materials from it, then we can directly focus the screening there.

---

### Development of the probability of return of spontaneous circulation in intervals without chest compressions during out-of-hospital cardiac arrest: an observational study [^9477aec3]. BMC Medicine (2009). Low credibility.

Appendix 2

If we choose value x at t = 0 for the regression to logslope, for a hypothetical interval h by choosing U h 0 = x - β 0, we can estimate the development of the mean P ROSC for intervals with a specific starting value. In this way the estimated development is not influenced by the random short time variation in logslope at t = 0, represented by ε h 0. By analogy, this was the problem with the analysis performed by Eftestol et al. With the above choice of U h 0 the marginal distribution of the other random terms is given by:

the model for the logslope at t by:

and, using Equation (2) for the logslope to P ROSC mapping, the model for P ROSC at t is given by:

Then the expected P ROSC development given the chosen initial value can be calculated by:

By choosing different values for x we can calculate the expected P ROSC development for cases with different starting values. For a given starting value of P ROSC, the corresponding logslope value x is found using Equation (2). If we make no specific choice of logslope at t = 0, but modify Equation (8) to integrate out the random intercept U h0 as well as the other random terms, we can obtain an estimate for how the mean P ROSC develops with time, pooling together cases at all P ROSC levels.

---

### Investigating suspected cancer clusters and responding to community concerns: guidelines from CDC and the council of state and territorial epidemiologists [^b59fe385]. MMWR: Recommendations and Reports (2013). Medium credibility.

CDC/CSTE suspected cancer clusters — alpha level choices for interpreting SIRs: Responders often use an alpha value of 0.05 (or equivalently a 95% confidence interval); using a larger alpha such as 0.10 (90% confidence interval) will increase the risk of false positive results, whereas a smaller alpha such as 0.01 (99% confidence interval) may be considered when many SIRs are computed, noting that with a 95% confidence interval, one expects to see five statistically significant results in a group of 100 results.

---

### Calculating the probability of random sampling for continuous variables in submitted or published randomised controlled trials [^0eb96628]. Anaesthesia (2015). Low credibility.

In a previous paper, one of the authors (JBC) used a chi-squared method to analyse the means (SD) of baseline variables, such as height or weight, from randomised controlled trials by Fujii et al., concluding that the probabilities that the reported distributions arose by chance were infinitesimally small. Subsequent testing of that chi-squared method, using simulation, suggested that the method was incorrect. This paper corrects the chi-squared method and tests its performance and the performance of Monte Carlo simulations and ANOVA to analyse the probability of random sampling. The corrected chi-squared method and ANOVA method became inaccurate when applied to means that were reported imprecisely. Monte Carlo simulations confirmed that baseline data from 158 randomised controlled trials by Fujii et al. were different to those from 329 trials published by other authors and that the distribution of Fujii et al.'s data were different to the expected distribution, both p < 10(-16). The number of Fujii randomised controlled trials with unlikely distributions was less with Monte Carlo simulation than with the 2012 chi-squared method: 102 vs 117 trials with p < 0.05; 60 vs 86 for p < 0.01; 30 vs 56 for p < 0.001; and 12 vs 24 for p < 0.00001, respectively. The Monte Carlo analysis nevertheless confirmed the original conclusion that the distribution of the data presented by Fujii et al. was extremely unlikely to have arisen from observed data. The Monte Carlo analysis may be an appropriate screening tool to check for non-random (i.e. unreliable) data in randomised controlled trials submitted to journals.

---

### The net chance of a longer survival as a patient-oriented measure of treatment benefit in randomized clinical trials [^dedfe16d]. JAMA Oncology (2016). Medium credibility.

Importance

Time to events, or survival end points, are common end points in randomized clinical trials. They are usually analyzed under the assumption of proportional hazards, and the treatment effect is reported as a hazard ratio, which is neither an intuitive measure nor a meaningful one if the assumption of proportional hazards is not met.

Objective

To demonstrate that a different measure of treatment effect, called the net chance of a longer survival, is a meaningful measure of treatment effect in clinical trials whether or not the assumption of proportional hazards is met.

Design

In this simulation study, the net chance of a longer survival by at least m months, where m months is considered clinically worthwhile and relevant to the patient, was calculated as the probability that a random patient in the treatment group has a longer survival by at least m months than does a random patient in the control group minus the probability of the opposite situation. The net chance of a longer survival is equal to zero if treatment does not differ from control and ranges from -100% if all patients in the control group fare better than all patients in the treatment group up to 100% in the opposite situation. We simulated data sets for realistic trials under various scenarios of proportional and nonproportional survival hazards and plotted the Kaplan-Meier survival curves as well as the net chance of a longer survival as a function of m. Data analysis was performed from August 14 to 18, 2015.

Main Outcomes and Measures

The net chance of a longer survival calculated for values of m ranging from 0 to 40 months.

Results

When hazards are proportional, the net chance of a longer survival approaches zero as m increases. The net chance of a longer survival (Δ) was 13% (95% CI, 6.5%-19.4%; P < .001) when any survival difference was considered clinically relevant (m = 0 months). When survival differences larger than 20 months were considered relevant (m = 20), the net chance of a longer survival was very close to zero (Δ[20] = 0.5%; 95% CI, -0.1% to 1.1%; P = 0.09). In contrast, when treatment effects are delayed or when some patients are cured by treatment, the net chance of a longer survival benefit remains high and tends to the cure rate. For crossing hazards, the Δ was negative (Δ = -6.9%; 95% CI, -14.0% to -0.5%; P = 0.047). However when large survival differences were considered (m = 20), the Δ(m) was positive (Δ[20] = 8.9%; 95% CI, 6.7%-11.1%; P < .001).

Conclusions and Relevance

The net chance of a longer survival is useful whether or not the assumption of proportional hazards is met in the analysis of survival end points and may be helpful as a measure of treatment benefit that has direct relevance to patients and health care professionals.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^7f98c702]. Nature Communications (2025). High credibility.

Fitting models to data is an important part of the practice of science. Advances in machine learning have made it possible to fit more — and more complex — models, but have also exacerbated a problem: when multiple models fit the data equally well, which one(s) should we pick? The answer depends entirely on the modelling goal. In the scientific context, the essential goal is replicability: if a model works well to describe one experiment, it should continue to do so when that experiment is replicated tomorrow, or in another laboratory. The selection criterion must therefore be robust to the variations inherent to the replication process. In this work we develop a nonparametric method for estimating uncertainty on a model's empirical risk when replications are non-stationary, thus ensuring that a model is only rejected when another is reproducibly better. We illustrate the method with two examples: one a more classical setting, where the models are structurally distinct, and a machine learning-inspired setting, where they differ only in the value of their parameters. We show how, in this context of replicability or "epistemic uncertainty", it compares favourably to existing model selection criteria, and has more satisfactory behaviour with large experimental datasets.

---

### Interpreting trial results-time for confidence and magnitude and not P values please [^f6868fcd]. Kidney International (2019). Medium credibility.

The problems with the P value as the single metric to summarize the results of a study are being recognized. It captures a single domain-random error-but it is relatively uninformative about more critical domains for deciding whether the results should be applied to clinical care and policy. Alternatives include the components of the outcomes reported (relevance, magnitude, frailty, and net benefit) and confidence (risk of bias and directness).

---

### Uncertainty of risk estimates from clinical prediction models: rationale, challenges, and approaches [^fc3badf0]. BMJ (2025). Excellent credibility.

Reasons why presenting uncertainty of risk estimates is important

We outline five key reasons why presenting the uncertainty of risk (probability) estimates can be beneficial. The reasons are interrelated but mentioned separately to help reinforce the argument.

Firstly, and fundamentally, expressing uncertainty of an individual's outcome risk provides a more complete picture than just a point estimate, as the sampling variability (or stability) of a model's prediction is shown. A point estimate of an individual's risk is a single value (best guess); for example, an individual's point estimate might be calculated as the average value of the model's uncertainty distribution for their risk. However, providing the entire uncertainty distribution shows other plausible values, potentially indicating a wide range of possible risks for an individual. Sampling variability and model instability generally increase with smaller model development sample sizes (lower numbers of participants and outcome events), larger numbers of candidate predictors for inclusion in the model, and low signal to noise situations (ie, smaller R 2). Variability is hidden when only a point estimate of risk is reported. Figure 2 illustrates this variability for models based on simulated data, with the uncertainty of individual risk often spanning the entire range of 0 to 1 in smaller sample sizes. Therefore, quantifying uncertainty in model predictions provides a useful model performance metric to be presented alongside other aspects, including whether risk estimates are well calibrated in the overall population and key subgroups. Expressing uncertainty is consistent with other areas of medical research, for example, in randomised trials, where uncertainty of estimates are expected to be presented (eg, 95% confidence intervals around treatment effect estimates).

Fig 2
1000 risk estimates ("predictions", y axis) sampled from the uncertainty distribution for nine individuals (with true risks (P), x axis, between 0.1 and 0.9), across six different models developed in sample sizes (n D) of 50, 100, 385, 500, 1000, and 5000 participants. Each model was produced by fitting a lasso logistic regression to a different random sample of individuals simulated from the same population with a true overall risk of 0.5, considering one genuine predictor (X∼N(0,4)) and 10 noise variables (Z1, …, Z10∼N(0,1)). Figure adapted from Riley and Collins with permission. The smaller the sample size, the wider the uncertainty distribution, even spanning the entire range of 0 to 1 in small samples.

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^d027f21a]. Nature Communications (2025). High credibility.

Understanding the effect of heterogeneity is fundamental to numerous fields. In community ecology, classical theory postulates that habitat heterogeneity determines niche dimensionality and drives biodiversity. However, disparate heterogeneity-diversity relationships have been empirically observed, generating increasingly complex theoretical developments. Here we show that spurious heterogeneity-diversity relationships and subsequent theories arise as artifacts of heterogeneity measures that are mean-biased for bounded continuous variables. To solve this, we derive an alternative mean-independent measure of heterogeneity for beta and gamma distributed variables that disentangles statistical dispersion from mean. Using the mean-independent measure of heterogeneity, true monotonic positive heterogeneity-diversity relationships, consistent with classical theory, are revealed in data previously presented as evidence for both hump-shaped heterogeneity-diversity relationships and theories of an area-heterogeneity trade-off for biodiversity. This work sheds light on the source of conflicting results that have hindered understanding of heterogeneity relationships in broader ecology and numerous other fields. The mean-independent measure of heterogeneity is provided as a solution, essential for understanding true mean-independent heterogeneity relationships in wider research.

---

### Effect sizes in ongoing randomized controlled critical care trials [^0585241b]. Critical Care (2017). Low credibility.

Statistical analysis

Continuous variables are reported as median and IQR or mean ± SD, and categorical variables are reported as counts and percents. Clinician-perceived prior probability for each trial was used to derive an estimate that a statistically significant result at the P = 0.05 level would represent a true-positive using the method described in Fig. 1. Specifically, as outlined in Additional file 2, the chance of a true-positive was calculated as follows:

Fig. 1
Graphical representation of the method of estimation of the chances that a statistically significant result represents a "true–positive" based on 100 hypothetical trials where there is a 10% chance the hypothesis is correct and experiments are conducted with 90% power at an α of 0.05. In this example, where there is a 10% prior probability that the hypothesis is correct, each box represents a hypothetical trial. The top row of boxes (surrounded by a green line) represent the 10 occasions where the hypothesis is correct; the remaining 90 boxes represent the occasions where the null hypothesis is correct. In an experiment with 90% power, one would expect to correctly identify nine of ten correct hypotheses (the area shaded red). Because the α value is defined as the probability of rejecting the null hypothesis when the null hypothesis is correct, one would also expect to incorrectly reject the null hypothesis on 4.5 of 90 occasions (the area shaded blue). As a result, with a 10% prior probability in an experiment with 90% power, a true-positive result is expected 67% of the time when the P value is 0.05

The sample size that each trial would require to detect the median largest effect size considered plausible by clinicians was calculated using standard methods for trials designed to compare two binomial proportions. We used the same β for these calculations as investigators had used in their initial sample size calculations and assumed an α of 0.05. Analysis of variance was used to analyze differences in survey results by location and specialty. A Mann-Whitney U test was used to compare clinicians' estimates of effect size, with treatment effect sizes used to inform sample size calculations. A P value of < 0.05 was considered to indicate statistical significance. Statistical analysis was performed using Real Statistics Resource Pack release 3.8 software (London, UK).

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^ade8746b]. Nature Communications (2025). High credibility.

Derivation of δ 2 from the general beta distribution

Given an observation (v) of a random variable (V) with any real number upper (U) and lower (L) bound, x can be retrieved for indexing the beta pdf and cdf (Equation (6); Equation (18)) withEssentially, v is rescaled to a value between 0 and 1 based on its possible minimum and maximum, such that it can be used to index the beta pdf in Equation (6). Likewise, the expected value or mean of the variable, E(V), can also be scaled to a value between 0 and 1 in the same way:where E(X) is equivalent to μ for the beta distribution (defined in Equation (7)). Similarly, 1 − E(X), can be retrieved for V withThe variance of the variable, Var(V), scales based on the squared range between the L and U :where Var(X) is equivalent to σ 2 for the beta distribution (defined in Equation (12)). As a result, the mean-independent measure of dispersion, δ 2, can be retrieved for a double-bounded variable withThus, Equation (33) is a general version of Equation (13) that can be calculated for variables with any real number lower and upper bound. In the main text Var(V) is denoted σ 2 and E(V) is denoted μ for simplicity.

The general beta pdf can be expressed using the standard beta pdf (f (x); Equation (6)) as, where the denominator normalises the pdf to adjust for the modified scale when modelling V, x for indexing the pdf is found for V with Equation (29), and the parameters p and q can be found for V with Equations (30), (33), (10), and (11).

---

### Descriptive statistics: reporting the answers to the 5 basic questions of who, what, why, when, where, and a sixth, so what? [^85777ffe]. Anesthesia and Analgesia (2017). Low credibility.

Descriptive statistics are specific methods basically used to calculate, describe, and summarize collected research data in a logical, meaningful, and efficient way. Descriptive statistics are reported numerically in the manuscript text and/or in its tables, or graphically in its figures. This basic statistical tutorial discusses a series of fundamental concepts about descriptive statistics and their reporting. The mean, median, and mode are 3 measures of the center or central tendency of a set of data. In addition to a measure of its central tendency (mean, median, or mode), another important characteristic of a research data set is its variability or dispersion (ie, spread). In simplest terms, variability is how much the individual recorded scores or observed values differ from one another. The range, standard deviation, and interquartile range are 3 measures of variability or dispersion. The standard deviation is typically reported for a mean, and the interquartile range for a median. Testing for statistical significance, along with calculating the observed treatment effect (or the strength of the association between an exposure and an outcome), and generating a corresponding confidence interval are 3 tools commonly used by researchers (and their collaborating biostatistician or epidemiologist) to validly make inferences and more generalized conclusions from their collected data and descriptive statistics. A number of journals, including Anesthesia & Analgesia, strongly encourage or require the reporting of pertinent confidence intervals. A confidence interval can be calculated for virtually any variable or outcome measure in an experimental, quasi-experimental, or observational research study design. Generally speaking, in a clinical trial, the confidence interval is the range of values within which the true treatment effect in the population likely resides. In an observational study, the confidence interval is the range of values within which the true strength of the association between the exposure and the outcome (eg, the risk ratio or odds ratio) in the population likely resides. There are many possible ways to graphically display or illustrate different types of data. While there is often latitude as to the choice of format, ultimately, the simplest and most comprehensible format is preferred. Common examples include a histogram, bar chart, line chart or line graph, pie chart, scatterplot, and box-and-whisker plot. Valid and reliable descriptive statistics can answer basic yet important questions about a research data set, namely: "Who, What, Why, When, Where, How, How Much?"

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^dee6cacc]. CDC (2011). Medium credibility.

Community viral load sample size calculations — sample size to detect differences in geometric mean (GM) viral load depends on power and standard deviation (S). The table specifies "α = 0.05 and W = 0.8", with S columns 1 to 1.5; for k = 3 the minimum sample sizes by S are 54, 66, 78, 92, 106, and 122, and "1.2 is the standard deviation of national VL data", so at S = 1.2 the needed sample size is 78.

---

### Future increase in compound soil drought-heat extremes exacerbated by vegetation greening [^3e9a24d7]. Nature Communications (2024). High credibility.

A framework for quantifying the likelihood of compound soil drought-heat events from vegetation biophysics

The total LMF

Compound soil drought-heat extremes are expected to have large impacts on natural ecosystems and human society, especially for those happen during the warm season. Additionally, the warm season also overlaps with the main growing season for most regions, during which vegetation change through modifying α and Tr can substantially regulate SM and Ta. We thus focus on global vegetated regions during the warm season. Following ref. the warm season is defined as the average hottest three consecutive months during study periods in each model grid cell.

Following ref. we used the likelihood multiplication factor (LMF) to quantify the change in the frequency of compound soil drought-heat events driven by Ta and SM (hereafter referred to as LMF Tot). We first employed the commonly used bivariate copulas to calculate the joint probability of a compound soil drought-heat event. Accordingly, our two variables are monthly Ta and the opposite of SM (-SM) over the warm season. Given two random variables X (i.e. Ta) and Y (i.e. -SM), the joint probability distribution function of X and Y with a copula C can be expressed asWhere P is the joint cumulative probability; u and v are the uniform distribution of X and Y, respectively. The joint probability of a compound soil drought-heat event during the historical period, defined by both variables exceeding their specific thresholds, can be expressed aswhere C his is the copula during the historical period. u his and v his are Ta and SM thresholds, respectively, and both are set at 0.9, which is a common threshold to define climate extremes.

Subsequently, we calculated the future thresholds of X and Y, corresponding to the 90 th quantile values during the historical period. For example, future thresholds of X (u fut) can be expressed asWhere X 90-his is the 90 th percentile value of X during the historical period, is the normal cumulative distribution function (CDF) of X during the future period. We then recomputed a new copula driven by future X and Y, and used Eq. (2) to calculate the future joint probability. The LMF Tot is defined as the ratio oftoAn increase in likelihood is indicated by a value greater than one. We used the function "BiCopCDF" in R VineCopula packageto calculate the joint probability.

---

### Gaze-centered gating, reactivation, and reevaluation of economic value in orbitofrontal cortex [^5459dbf1]. Nature Communications (2024). High credibility.

The sequential presentation of offers in our task design has several important advantages over simultaneous offer presentations. First, if we employed simultaneous presentation of the two offers, the gaze patterns might have been much more variable, and therefore the dynamical analysis of neural firing aligned with gaze could have been more difficult to perform. Second, given our interest in studying the reactivation of first value encoding in the delay 2 epoch, we needed such epoch to happen enough time after offer 1, eventually followed by a second offer (distractor), so that looking back to first offer side and encoding of the first offer value could not be purely explained by sensory encoding, and required gaze-dependent reactivation.

Visual stimuli were presented on a 24" monitor with a resolution of 1024 pixels × 768 pixels, a physical width and 48.8 cm and a height of 36.6 cm, positioned at 57 cm from the eyes of the subjects. The visual offers consisted of two vertical rectangles 300 pixels tall and 80 pixels wide (14.3 cm height and 3.8 cm width). Their centers were displaced right or left from the center of the screen by 256 pixels (12.2 cm, 1/4 of the total screen width). The presentation sites were randomized so that the first offer could be presented with equal probability on either side of the screen and the second on the opposite side (the final count was that in 50.01% of the trials, the first offer was presented on the left side of the screen). The colors and height of the rectangles respectively indicated the magnitudeand probabilityof the offered liquid reward. Reward magnitudes were pseudo-randomized across trials. Reward probabilities of the medium and large offers were independently drawn from a uniform probability distribution. The magnitude of the reward was indicated by the color of the bottom part of the vertical bar stimuli (gray: small, 125 μL; blue: medium, 165 μL; green: large, 240 μL). In all analyses, reward magnitudesare reported and used in nominal units (1 = small, 2 = medium, 3 = large reward). The height of the bottom part of the bar stimuli indicated the (success) probabilityof obtaining the liquid reward (with magnitudeas indicated by its color) if the offer was chosen, in a way that if the whole bar is of one single color, the probability of the reward is equal to one. Wheneverwas < 1, the stimuli top fraction was colored in red, with its height indicating the complementary probabilityfor unsuccessful outcome. We define the expected value of an offer as the product of its probability times its magnitude: To consider the trial-to-trial variability of offer value, we also defined the variance, also referred to as offer risk. The definition of variance follows the intuition that risk can be modeled as the variance ofindependent Bernoulli variables (sinceis normalized to). We checked that the definition of variance as for a single Bernoulli variable would not have qualitatively affected the results of our main behavioral and neural analyses. Offers having small rewards were always sure,(safe option). Reward probabilities for the medium and large magnitude offers were randomly and independently drawn from a uniform distribution in the unit interval, whose presentation was limited in size only to the resolution of screen pixel size. Reward magnitudes (small, medium, or large) of the first and second offers were randomized across trials so that 1/8 were safe, and for the remainder, 7/8, offers could be either medium or large in equal fractions (yielding a total of: 12.15% small, safe offers; 43.93% medium-sized gamble offers, 43.92% large sized gamble offers). Trials were interleaved with an 800 ms idle inter-trial interval when subjects were not given instructions and the screen was left blank.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^aeeaae45]. Nature Communications (2025). High credibility.

Fitting models to data is an important part of the practice of science. Advances in machine learning have made it possible to fit more-and more complex-models, but have also exacerbated a problem: when multiple models fit the data equally well, which one(s) should we pick? The answer depends entirely on the modelling goal. In the scientific context, the essential goal is replicability: if a model works well to describe one experiment, it should continue to do so when that experiment is replicated tomorrow, or in another laboratory. The selection criterion must therefore be robust to the variations inherent to the replication process. In this work we develop a nonparametric method for estimating uncertainty on a model's empirical risk when replications are non-stationary, thus ensuring that a model is only rejected when another is reproducibly better. We illustrate the method with two examples: one a more classical setting, where the models are structurally distinct, and a machine learning-inspired setting, where they differ only in the value of their parameters. We show how, in this context of replicability or "epistemic uncertainty", it compares favourably to existing model selection criteria, and has more satisfactory behaviour with large experimental datasets.

---

### Evaluation of clinical prediction models (part 3): calculating the sample size required for an external validation study [^4dbddaa9]. BMJ (2024). Excellent credibility.

Fig 4
Comparison of histogram (grey bars) of predicted values (estimated event probabilities) in the validation population of Gupta et alwith our assumed beta distribution (curved line) used

We targeted a confidence interval width of 0.22 for the observed/expected statistic (which corresponds to a small absolute error of about 0.05 compared with the assumed overall outcome event proportion of 0.43; see calculations elsewhere), 0.3 for the calibration slope, 0.1 for the c statistic, and 0.2 for the standardised net benefit. Applying the sample size calculations, the corresponding Stata code is:

This calculation gives a minimum required sample size of 423 (182 events) for the observed/expected statistic, 949 (408 events) for calibration slope, 347 (149 events) for c statistic, and 38 (16 events) for standardised net benefit at a threshold of 0.1; and 407 (175 events) for standardised net benefit at a threshold of 0.3. Hence, at least 949 participants (408 events) are required for the external validation study to target precise estimates of all four measures and, in particular, to ensure calibration is properly evaluated. This sample size is much larger than the rule of thumb of 100 (or 200) events and 100 (or 200) non-events.

Additional calculations were done to see how the required sample size changed when our assumptions changed. For example, when assuming the model has the same distribution of estimated probabilities but with worse performance of either a calibration slope of 0.9 or a c statistic of 0.72, the sample size required was fewer than the 949 participants originally identified. However, if we assumed the external validation population had a narrower case mix distribution, and so used a tighter distribution of predicted values than the previous beta distribution, a sample size larger than 949 participants was required for precise estimation of the calibration slope. This change in target sample size emphasises the importance of understanding the target population and its likely distribution of predicted values. In the absence of any information, a pilot study might be useful to help gauge this distribution.

---

### A note on genetic variance components in mixed models [^621f7d16]. Genetic Epidemiology (2003). Low credibility.

Burton et al. ([1999] Genet. Epidemiol. 17:118–140) proposed a series of generalized linear mixed models for pedigree data that account for residual correlation between related individuals. These models may be fitted using Markov chain Monte Carlo methods, but the posterior mean for small variance components can exhibit marked positive bias. Burton et al. ([1999] Genet. Epidemiol. 17:118–140) suggested that this problem could be overcome by allowing the variance components to take negative values. We examine this idea in depth, and show that it can be interpreted as a computational device for locating the posterior mode without necessarily implying that the original random effects structure is incorrect. We illustrate the application of this technique to mixed models for familial data.

---

### Sources of confidence in value-based choice [^4f84ca43]. Nature Communications (2021). High credibility.

Fig. 5
The efficient coding model.

a The decision process with three distinct process stages, color-coded to match the graphs. The prior matches the distribution of subjective values v of supermarket products. When choosing between two items, subjects look repeatedly at them, spending unequal time on the two options. The subjective values are internally encoded, the corresponding likelihood functionis constrained by the prior p (v) via efficient coding. Lastly, noise that occurs after the decoding is taken into account. b Standardized posterior estimates of the relationship between confidence and variance in the encoding process, the variance in the comparison process, and attentional factors (β θ). is not significantly different from 0 (P = 0.39), both and (β θ) are significantly bigger than zero with P < 0.001. The effect size of is not significantly different from 0. Both the effect sizes of and (β θ) are significantly bigger than zero with P < 0.001. Error bars indicate the mean posterior estimate of the standard deviation. P -values are based on the highest density interval of the posterior estimates. c Left column: the empirical probabilities of choosing the upper item; up: as a function of value difference; down: as a function of the difference in dwell time. Right column: the same as left but for the predicted probabilities by the efficient coding model. The trials are median split in high/low confidence. Value difference and dwell time difference are split into eight groups of equal size. Data are presented as mean values ± SEM. Source data are provided as a Source Data file. d Comparison of parameter estimates of two alternative efficient coding models: a model with agent-specific estimates of σ enc and a model that allows for trial-to-trial fluctuations of σ enc. The median of the posterior estimate of σ enc of the agent-specific model is indicated by the horizontal green line, the shaded grey area indicates the 95% confidence interval. The diagonal green lines represent 100 random samples of the posterior distribution of how σ enc changes with confidence in the model allowing for trial-to-trial fluctuations. e, f Same as (e) but for σ comp and θ. g Comparison of the effect sizes of the posterior estimates of σ enc and σ comp. Vertical red dashed line indicates the median, black lines indicate the 95% confidence interval. h Comparison of the posterior estimates of the intercept of θ in the efficient coding model and the RUM. i Comparison of the posterior estimates of the slope of θ in the efficient coding model and the RUM. For the whole figure n = 33 independent participants.

---

### Validating whole slide imaging for diagnostic purposes in pathology: guideline from the college of American pathologists pathology and laboratory quality center [^8c342dfb]. Archives of Pathology & Laboratory Medicine (2013). Medium credibility.

College of American Pathologists whole slide imaging (WSI) guideline — evidence-to-decision panel voting distributions are reported across certainty, values, balance of effects, and resource use. For "overall certainty of the evidence of test accuracy", votes were 0 0 30% (3/10) 60% (6/10) 10% (1/10) 0 across columns labeled Very Low, Low, Moderate, High, Very High, and No Included Studies. For "overall certainty of the evidence of effects of the management that is guided by the test results", votes were 0 20% (2/10) 20% (2/10) 60% (6/10) 0 0. For "How certain is the link between test results and management decisions?" the distribution was 0 10% (1/10) 40% (4/10) 20% (2/10) 30% (3/10) 0. For "overall certainty of the evidence of effects of the test", votes were 0 0 30% (3/10) 40% (4/10) 30% (3/10) 0, and for "overall certainty of the evidence for any critical or important direct benefits, adverse effects or burden of the test?" votes were 0 0 30% (3/10) 50% (5/10) 20% (2/10) 0. Regarding values, responses to "Is there important uncertainty about or variability in how much people value the main outcomes?" were 10% (1/10) 50% (5/10) 30% (3/10) 10% (1/10) across, respectively, Important uncertainty or variability, Possibly important uncertainty or variability, Probably no important uncertainty or variability, and No important uncertainty or variability. For "Does the balance between desirable and undesirable effects favor the intervention or the comparison?" ratings were 30% (3/10) 30% (3/10) 0 0 40% (4/10) 0 0 across Favors the comparison, Probably favors the comparison, Does not favor either the intervention or comparison, Probably favors the intervention, Favors the intervention, Varies, and Don't Know. For "How large are the resource requirements (costs)?", votes were 10% (1/10) 60% (6/10) 30% (3/10) 0 0 0 0 across Large costs, Moderate costs, Negligible costs and savings, Moderate savings, Large savings, Varies, and Don't Know.

---

### Designing accurate emulators for scientific processes using calibration-driven deep models [^a9a8917a]. Nature Communications (2020). High credibility.

In this paper, we present Learn-by-Calibrating (LbC), a nonparametric approach based on interval calibration for building emulators in scientific applications that are effective even with heterogeneous data and are robust to outliers. The notion of interval calibration comes from the uncertainty quantification literature, and can be formally defined as follows: let us assume that the model f is designed to produce prediction intervals, in lieu of simple point estimates, for the response y, i.e. Suppose that the likelihood for the true response y to be contained in the prediction interval is, the intervals are considered to be well-calibrated if the likelihood matches the expected confidence level. For a confidence level α, we expect the interval to contain the true response for 100 × α % of realizations from p (x). Though calibration has been conventionally used for evaluating and correcting uncertainty estimators, this paper advocates for utilizing calibration as a training objective in regression models. More specifically, LbC uses two separate modules, implemented as neural networks, to produce point estimates and intervals, respectively, for the response variable, and poses a bilevel optimization problem to solve for the parameters of both the networks. This eliminates the need to construct priors on the expected residual structure and makes it applicable to both homogeneous and heterogeneous data. Furthermore, by effectively recovering the inherent noise structure, LbC leads to highly robust models.

---

### Computational processes of simultaneous learning of stochasticity and volatility in humans [^b0ca0704]. Nature Communications (2024). High credibility.

Model-agnostic analysis of trial-trial learning rate dynamics

For each participant, we used the bucket position to define the trial-wise update (change in prediction) and prediction error (outcome minus prediction) values. The per-trial learning rate was then calculated as the update divided by the prediction error, bounded between 0 and 1 (set to 0 if negative and 1 if greater than 1). We also divided trials into two clusters based on the direction of sample autocorrelation, specifically the sign of the product of prediction errors on the two recent trials. This allowed us to examine how learning rate changes were modulated by the local outcome autocorrelation structure. The changes in learning rate were calculated separately for each cluster.

All reported statistical tests are two-sided. We performed parametric tests (e.g. t -test), unless the data were expected to not be normally distributed, in which case appropriate nonparametric rank tests were conducted. When analyzing the response time data, we first excluded trials where the response time was identified as an outlier. To detect outliers, we used MATLAB's is outlier routine, which flags data points greater than three scaled median absolute deviations as outliers. In the corresponding regression analysis, we regressed the response time on any trial against |AC| and AC on the previous trial. An intercept was included in all regression analyses.

Reporting summary

Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.

---

### Designing accurate emulators for scientific processes using calibration-driven deep models [^5e03881e]. Nature Communications (2020). High credibility.

Predictive models that accurately emulate complex scientific processes can achieve speed-ups over numerical simulators or experiments and at the same time provide surrogates for improving the subsequent analysis. Consequently, there is a recent surge in utilizing modern machine learning methods to build data-driven emulators. In this work, we study an often overlooked, yet important, problem of choosing loss functions while designing such emulators. Popular choices such as the mean squared error or the mean absolute error are based on a symmetric noise assumption and can be unsuitable for heterogeneous data or asymmetric noise distributions. We propose Learn-by-Calibrating, a novel deep learning approach based on interval calibration for designing emulators that can effectively recover the inherent noise structure without any explicit priors. Using a large suite of use-cases, we demonstrate the efficacy of our approach in providing high-quality emulators, when compared to widely-adopted loss function choices, even in small-data regimes.

---

### Venous thromboembolism: diagnosis and treatment [^72348313]. American Family Physician (2025). High credibility.

Regarding diagnostic investigations for deep vein thrombosis, more specifically with respect to assessment of pretest probability, AAFP 2025 guidelines recommend to use validated clinical prediction rules to estimate the pretest probability of VTE and guide further evaluation.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^bfe9469b]. CDC (2011). Medium credibility.

Multiple imputation for surveillance viral load (VL) measures — cautions: Jurisdictions should carefully weigh the benefits and limitations of multiple imputation before embarking on such analyses because methods like multiple imputation that extrapolate from available surveillance VL data to the remainder of the population may not be appropriate for surveillance data, there is no clear cut-off for how much information may be imputed (e.g., the recommendation of 75% as the minimum threshold for data completeness), and jurisdictions are urged to consult with statisticians familiar with the methodology when planning analyses that involve imputation.

---

### Work-relatedness [^700e045f]. Journal of Occupational and Environmental Medicine (2018). Medium credibility.

Work-relatedness — Table 1 steps for evaluating epidemiological evidence list procedural actions and criteria: "Collect all epidemiological literature reported on that disorder", "Identify the design of each study", and "Assess each study's methods" including "Exposure assessment methods and potential biases", "Disease ascertainment methods and potential biases", "Absence of significant uncontrolled confounders; consideration of residual confounding", "Addressing of other potential biases", "Adequacy of biostatistical methods and analytical techniques", and "Ascertainment of statistical significance — degree to which chance may have produced those results". The table then directs to "Assess the studies using the Updated Hill's Criteria" including "Temporality", "Strength of association", "Dose–response", "Consistency", "Coherence", "Specificity", "Plausibility", "Reversibility", "Prevention/Elimination", "Experiment", and "Predictive Performance", and to make a "Conclusion regarding the degree to which such a causal association is/is not met".

---

### Executive summary: diagnosis and management of lung cancer, 3rd ed: American College of Chest Physicians evidence-based clinical practice guidelines [^d4e76b05]. Chest (2013). Medium credibility.

Regarding classification and risk stratification for pulmonary nodules, more specifically with respect to malignancy risk assessment, ACCP 2013 guidelines recommend to consider estimating the pretest probability of malignancy either qualitatively by using clinical judgment and/or quantitatively using a validated model in patients with a solid, indeterminate nodule > 8 mm in diameter.

---

### Number needed to treat (or harm) [^d0b683ae]. World Journal of Surgery (2005). Low credibility.

The effect of a treatment versus controls may be expressed in relative or absolute terms. For rational decision-making, absolute measures are more meaningful. The number needed to treat, the reciprocal of the absolute risk reduction, is a powerful estimate of the effect of a treatment. It is particularly useful because it takes into account the underlying risk (what would happen without the intervention?). The number needed to treat tells us not only whether a treatment works but how well it works. Thus, it informs health care professionals about the effort needed to achieve a particular outcome. A number needed to treat should be accompanied by information about the experimental intervention, the control intervention against which the experimental intervention has been tested, the length of the observation period, the underlying risk of the study population, and an exact definition of the endpoint. A 95% confidence interval around the point estimate should be calculated. An isolated number needed to treat is rarely appropriate to summarize the usefulness of an intervention; multiple numbers needed to treat for benefit and harm are more helpful. Absolute risk reduction and number needed to treat should become standard summary estimates in randomized controlled trials.