# Medical Question & Answer

**Sample ID**: 6d1e88fa-aee6-cf11-2072-fac4b9d009c7
**Dataset Index**: 345761

---

## Question

4. Find all four-digit numbers that, when divided by 197, give a remainder of 47, and when divided by 198, give a remainder of 37.

---

## Answer

> Let's see… What do we have here? The user is asking to find all four-digit numbers that satisfy two remainder conditions: when divided by 197 the remainder is 47, and when divided by 198 the remainder is 37. Let's break this down step-by-step. First, I need to think about translating the remainder conditions into modular congruences. Then, I should verify how to combine these into a single congruence using the Chinese Remainder Theorem. Next, I will calculate the modulus and the particular solution, and then express the general solution. After that, I need to determine the range of four-digit numbers and solve for the valid values of the parameter. Finally, I will list the exact four-digit numbers and double-check a couple of them against the original conditions to ensure correctness.

> Let me first translate the problem into congruences. If a number N leaves remainder 47 when divided by 197, that means N ≡ 47 (mod 197). Similarly, if N leaves remainder 37 when divided by 198, that means N ≡ 37 (mod 198). So I have the system N ≡ 47 (mod 197) and N ≡ 37 (mod 198), and I need to find all four-digit solutions.

> Wait, let me verify the approach to combine these. The Chinese Remainder Theorem applies when the moduli are coprime. Here, 197 and 198 are consecutive integers, so gcd(197, 198) = 1, which means the CRT guarantees a unique solution modulo 197 × 198. I should confirm that before proceeding, and yes, 197 and 198 are indeed coprime, so I can proceed with CRT.

> I will now examine how to express the general solution. The CRT tells me there exists a unique solution modulo 197 × 198 = 39006. Let me denote that solution as N ≡ a (mod 39006), so the general solution will be N = a + 39006k for integer k. I need to find the particular value of a that satisfies both congruences, and then determine which values of k yield four-digit numbers.

> Next, I should review how to find the particular solution a. One way is to substitute the first congruence into the second: since N = 197m + 47 for some integer m, I can plug this into the second congruence to get 197m + 47 ≡ 37 (mod 198). Simplifying, 197m ≡ −10 (mod 198), which is equivalent to 197m ≡ 188 (mod 198). But wait, 197 ≡ −1 (mod 198), so this becomes −m ≡ 188 (mod 198), which implies m ≡ −188 ≡ 10 (mod 198). Therefore, m = 198t + 10 for integer t, and substituting back gives N = 197(198t + 10) + 47 = 197 × 198t + 1970 + 47 = 39006t + 2017. So the particular solution is a = 2017, and the general solution is N = 2017 + 39006k.

> Hold on, I should verify the four-digit constraint. Four-digit numbers range from 1000 to 9999, so I need 1000 ≤ 2017 + 39006k ≤ 9999. Solving for k, I get (1000 − 2017)/39006 ≤ k ≤ (9999 − 2017)/39006, which is approximately −0.026 ≤ k ≤ 0.204. Since k must be an integer, the only valid value is k = 0. Therefore, the only four-digit solution is N = 2017 + 39006 × 0 = 2017.

> Hmm, wait a minute, I initially thought there might be more than one four-digit solution because 39006 is larger than 9000, but I should double-check the arithmetic. With k = 0, N = 2017, which is indeed a four-digit number. With k = 1, N = 2017 + 39006 = 41023, which is a five-digit number, so it exceeds the upper bound. With k = −1, N = 2017 − 39006 = −36989, which is negative and far below the lower bound. So yes, k = 0 is the only valid integer in the range, confirming there is exactly one four-digit solution.

> I should confirm the answer satisfies both original conditions. Let me check 2017 ÷ 197: 197 × 10 = 1970, and 2017 − 1970 = 47, so the remainder is 47, which matches the first condition. Now, 2017 ÷ 198: 198 × 10 = 1980, and 2017 − 1980 = 37, so the remainder is 37, which matches the second condition. Both checks pass, so the solution is correct.

> Final answer: There is exactly one four-digit number that satisfies the given conditions, and that number is 2017.

---

The only four-digit number that leaves a remainder of 47 when divided by 197 and a remainder of 37 when divided by 198 is **2017**. This is found by solving the system of congruences (N ≡ 47 (mod 197)) and (N ≡ 37 (mod 198)) using the Chinese Remainder Theorem, which yields a unique solution modulo 39006. Within the four-digit range (1000–9999), the only valid solution is 2017.

---

## Step 1: Set up the congruences

We are given:

N ≡ 47 (mod 197)

N ≡ 37 (mod 198)

---

## Step 2: Apply the Chinese Remainder Theorem

Since 197 and 198 are coprime, the Chinese Remainder Theorem guarantees a unique solution modulo (197 × 198 = 39006). We express (N) as:

N = 197k + 47

Substitute into the second congruence:

197k + 47 ≡ 37 (mod 198)

197k ≡ −10 (mod 198)

−k ≡ −10 (mod 198) (since 197 ≡ −1 (mod 198))

k ≡ 10 (mod 198)

Thus, (k = 198m + 10) for integer (m), and:

N = 197(198m + 10) + 47 = 39006m + 2017

---

## Step 3: Find four-digit solutions

We need (1000 ≤ N ≤ 9999):

1000 ≤ 39006m + 2017 ≤ 9999

−1017 ≤ 39006m ≤ 7982

−0.026 ≤ m ≤ 0.204

The only integer (m) in this range is **0**, giving (N = 2017).

---

## Step 4: Verify the solution

Check the remainders:

2017 ÷ 197 = 10 remainder 47

2017 ÷ 198 = 10 remainder 37

Both conditions are satisfied.

---

## Conclusion

The only four-digit number meeting both conditions is **2017**.

---

## References

### Too many digits: the presentation of numerical data [^115CFoHw]. Archives of Disease in Childhood (2015). Low credibility.

Thus a decimal places rule that ignores significant digits does not work. But equally, and perhaps surprisingly, a significant digits rule that ignores decimal places does not always work either. Reporting risk ratios to three significant digits for example leads to the largest ratio below 1 being reported as 0.999 and the smallest above 1 as 1.01, with three and two decimal places, respectively. This is clearly unsatisfactory as they differ in precision by a factor of ten. In this instance a combination of significant digits and decimal places, the rule of four, works best: round the risk ratio to two significant digits if the leading non-zero digit is four or more, otherwise round to three.

The rule of four gives three decimal places for risk ratios from 0.040 to 0.399, two from 0.40 to 3.99 and one from 4.0 to 39.9. Applying it to the example of 22.68 above gives 22.7 (95% CI 7.5 to 74). Alternatively one can apply the rule with one less significant digit, giving 23 with CI 8 to 70.

Another example is the reporting of test statistics such as t or F. Specifying one decimal place would permit say t = 30.1, where 30 is clearly sufficient as it is so highly significant. Conversely specifying two significant digits would permit t = −0.13, where again the extra precision is irrelevant as it is far from significant. A suitable rule specifies up to one decimal place and up to two significant digits.

When comparing group means or percentages in tables, rounding should not blur the differences between them. This is the basis for the Hopkins two digits rule, whereby the mean has enough decimal places to ensure two significant digits for the SD. An analogous rule for percentages might be to use enough decimal places to ensure two significant digits for the range of values across groups, eg, if the range is 10% or more use whole numbers, if less than 1% use two decimal places, and otherwise one. In practice percentages are usually given along with their corresponding frequencies, so precision is less critical as the exact values can be calculated.

---

### [^114BdPoa]. Developmental Medicine and Child Neurology (2019). Medium credibility.

5.3 干预方式：（组）场景设置

5.3.1 个人因素

自2011年以来，训练方式发生了转折性改变，以小组为基础的干预被加入治疗领域中。总而言之，小组干预对运动表现产生了很大的影响。

虽然尚不能通过数据分析结果来决定最佳的小组规模，但由一名治疗师引导下，4～6名儿童的小组规模的干预是易于管理和有效的模式，必要时可增加一名助理。 268, 273, 280, 281 限制规模的小组训练可以方便指导者在参与者间走动，以此监控小组动态和个人进步情况。一项研究发现， 280 运动能力较差的儿童在大组活动中会感到更焦虑。然而，这些儿童在大组中处理同伴问题的能力确实得到了提高，这是非常有用的生活技能。因此，应该仔细考虑小组规模的设置，但也要根据年龄、疾病的严重程度、小组成员和干预的目标谨慎设定。

---

### CLIMB: high-dimensional association detection in large scale genomic data [^1162temj]. Nature Communications (2022). High credibility.

These association vectors can be appreciated as an alternative to binarization or ternarization of genomic signals, since they assign binary or ternary labels to the data. A label directly reflects the pattern of condition specificity of the observations in its associated cluster. Further, as a mixture modeling approach, these labels naturally allow for heterogeneity in signals, resulting in greater model flexibility.

Yet, a remaining challenge is that models that leverage these association vectors suffer from computational intractability for even a modest number of conditions. To understand this issue, consider D conditions: Letbe the set of all 3ᴰ possible configurations of association vector H, such that an observation described by an association vector with h [i] = 1(h [i] = − 1) has a positive (negative) association in condition i. It is clear that this model formulation becomes computationally prohibitive even for single-digit D because the total number of possible association vectors grows exponentially with D, possibly resulting in the number of model parameters exceeding the number of observations. In response to this, several restrictive assumptions are imposed. For example, Amar et al.somewhat alleviate computational burden by assuming all associations must be positive, and estimate partial latent associations for subgroups of conditions with a heuristic approach. This heuristic reduces statistical power and resolution when testing for consistent findings, and cannot provide a single unified clustering of observations since it is not a true joint analysis. Moreover, this approach does not distinguish an observation that is significant in opposite directions in two conditions from an observation that exhibits consistent direction of association across conditions. Alternatively, Urbut et al.make computational gains by assuming all observations come from a uni-modal distribution centered over zero, but this restriction does not always hold in practice.

---

### Making sense of number words and Arabic digits: does order count more? [^111RupdB]. Child Development (2020). Medium credibility.

The ability to choose the larger between two numbers reflects a mature understanding of the magnitude associated with numerical symbols. The present study explores how the knowledge of the number sequence and memory capacity (verbal and visuospatial) relate to number comparison skills while controlling for cardinal knowledge. Preschool children's (N = 140, M age-in-months = 58.9, range = 41–75) knowledge of the directional property of the counting list as well as the spatial mapping of digits on the visual line were assessed. The ability to order digits on the visual line mediated the relation between memory capacity and number comparison skills while controlling for cardinal knowledge. Beyond cardinality, the knowledge of the (spatial) order of numbers marks the understanding of the magnitude associated with numbers.

---

### Symbolic metaprogram search improves learning efficiency and explains rule learning in humans [^1153LGyR]. Nature Communications (2024). High credibility.

Experimental procedure

We report the results of a behavioral experiment involving human participants. Our procedure complies with all relevant ethical regulations and was approved by the Institutional Review Board at Massachusetts Institute of Technology where the study was conducted. Participants provided informed consent and received a flat fee of $7.50 for participating plus a $0.01 bonus for each correct response. This study was not preregistered.

Supplementary Fig. 1 shows a representative display from the behavioral paradigm. Participants agreed to play a guessing game with the computer and began by reviewing the game's instructions. After a short comprehension check, participants completed 110 trials — 10 rounds of 11 trials each, with the current round clearly indicated onscreen. In each round, the computer selected one of the 250 list functions as a rule for transforming input lists into output lists. Functions were selected uniformly at random for each participant; neither the experimenter nor the participant knew the functions being tested at the time of the experiment. Each function took a list of natural numbers as input and returned a list of natural numbers as output. Lists could include the numbers 0–99 as elements and contain 0–15 elements. To help participants learn the rule, the computer presented a series of trials. To begin each trial, the computer would show a novel input list and ask the participant to predict the output associated with the input by typing their predicted response into the text box. Participants were told that their job was to guess the rule and use it to correctly respond to as many of the computer's queries as possible. Participants were required to type in the entire list and had to do so without typos for their response to be considered correct. After each prediction, the computer revealed the correct output, ending the trial. The input, output, and participant prediction remained on screen for the rest of the experiment to reduce working memory load; participants could review it on any future trial, including those in subsequent rounds. The paradigm thus encouraged online learning in an attempt to reduce long-term memory demand and more accurately measure trial-by-trial generalization. Progress indicators at the bottom of the screen informed participants of their performance and the number of remaining trials. At the end of each round, the computer asked participants to enter a natural language description of the rule they thought the computer had been using. The experiment ended with a brief demographical survey. No statistical methods were used to pre-determine sample sizes but our sample sizes are similar to those reported in previous publications.

---

### Atmospheric source of mercury to the ocean constrained by isotopic model [^112wNzSa]. Nature Communications (2025). High credibility.

Isotope notion and fractionation

We follow the notation established by Blum and Bergquistto express the modeled isotope signatures. These notations are defined as the isotopic ratio difference simulated species and the NIST-3133 standard, measured in permil (‰):

MIF is calcuated as below:

In our calculations, the xxx / 198 Hg NIST3133 ratios are treated as constants, derived from the atomic abundances of Hg isotopes in the NIST-3133 standard,: 196 Hg = 0.155%, 198 Hg = 10.04%, 199 Hg = 16.94%, 200 Hg = 23.14%, 201 Hg = 13.17%, 202 Hg = 29.73%, and 204 Hg = 6.83%. For the Hg isotope inventories, we synthesize the Hg isotopic composition of each source and calculate its isotopic ratio (xxx/198 Hg) using Eqs. (3)–(7) (Table 1). These ratios allow us to divide total Hg emissions into seven isotopes, which can be input into the model as individual tracers. After running the model, it outputs the concentrations and fluxes of these isotopes, enabling us to calculate the isotope signatures using Eqs. (3)–(7).

---

### Machine learning applied to enzyme turnover numbers reveals protein structural correlates and improves metabolic models [^114ZfMUk]. Nature Communications (2018). Medium credibility.

It would thus be desirable to understand the underlying genome-scale patterns of catalytic enzyme turnover rates — a major part of the kinetome — and thus protein efficiency. For in vitro k cat, global trends were found in relation to the basic biochemical mechanism of the reaction, measured as the first digit of the respective EC numbers. In addition to EC numbers, enzyme molecular weight and reaction flux were shown to correlate with k cat in vitro, indicating that differential selection pressure explains variance in turnover numbers. It is unclear how these features act together to explain variance in k cat. Machine learning (ML) methods for the development of complex statistical models have been successfully applied to modelling bacterial physiology –, enzyme specificity, and enzyme affinity, with applications in metabolic engineering and synthetic biology. Here, we combine known correlates of k cat with novel features for enzyme structure, biochemical mechanism, network context, and assay conditions to build ML models of k cat in vitro and k app, max that can predict these parameters at the genome scale. Application of these ML models to the parameterization of mechanistic GEMs enables improved predictions of proteome allocation.

---

### Number processing induces spatial performance biases [^111bo32A]. Neurology (2001). Low credibility.

Background

Response speed in parity judgments is faster with the left hand for small numbers (e.g., 1 or 2) and faster with the right hand for larger numbers (e.g., 8 or 9). This effect suggests that number processing can induce systematic spatial biases in bisection tasks.

Method

Neurologically healthy participants bisected visually presented stimuli with a pencil. Stimuli were long strings of uniform digits (experiment 1) or lines with single digit flankers (experiment 2).

Results

Bisection performance was biased to the left of center for strings made of digits 1 or 2 and to the right of center for strings made of digits 8 or 9. Line bisection was biased toward the flanker representing the larger magnitude, regardless of its position.

Conclusions

These results extend previous findings and support the notion of an automatic association of number magnitudes with spatial response codes. The effect may be useful for an assessment of semantic number processing in special populations.

---

###: vol. 66, no. 47 [^114eErkH]. MMWR: Morbidity and Mortality Weekly Report (2017). Low credibility.

[This corrects the article DOI: 10.15585/mmwr.mm6647a3.].

---

### [^111mo49R]. Developmental Medicine and Child Neurology (2019). Medium credibility.

英国一项大型流行病学研究发现，患有严重DCD的儿童更容易发生视觉异常。在近距离（OR = 1.98 [95%CI：1.13–3.48]）和远距离（2.59[1.16–5.79]）异常视知觉融合的风险升高；出现运动融合（1.74 [1.07–2.84]）；立体视觉降低（2.75 [1.78–4.23]）；远视（2.29 [1.1–4.57]）和屈光参差（2.27 [1.13–4.60]）。 195 该并发共患情况显示了此类眼部异常和DCD可能存在共同机制通路。 196

当DCD儿童有读写问题时，必须认识到其并发共患视觉异常的风险增加。DCD并发共患阅读和/或书写障碍是已经被长期公认的并发共患情况。 27, 184, 197, 198

然而，一项台湾研究表明，DCD儿童与正常发育儿童相比，在汉语阅读能力测试中的得分以及在基本阅读和写作测试中的阅读综合得分，没有显著差异。这些结果与英语国家儿童的结果形成了有趣的对比：在英语国家中DCD患儿相比正常儿童，阅读能力和写作能力较差。 199 这表明DCD患者有特定的知觉问题。最近，在DCD患儿中也发现了特定的数学问题。特别在非符号和符号数字比较任务中DCD患儿的表现要比正常发育的儿童弱。与同年龄儿童相比，DCD患儿在数字检索和程序计算方面的表现明显较差。 28, 200

---

### Effect of timeframes to define long term conditions and sociodemographic factors on prevalence of multimorbidity using disease code frequency in primary care electronic health records: retrospective study [^114k3paV]. BMJ Medicine (2024). High credibility.

Disease definitions

The number of chronic conditions for each person was calculated as of the 1 January 2020 under each of the following five definitions (figure 2). These definitions were selected to reflect the variety used in previous multimorbidity research and definitions of chronic conditions:

Any disease code in the record for all 212 conditions, which was used as the reference definition (single code);
two codes at least three months apart for the 41 conditions requiring multiple codes, or a single code for the remaining 171 conditions;
two codes at least 12 months apart for the 41 conditions requiring multiple codes, or a single code for the remaining 171 conditions;
three codes within any 12 month period for the 41 conditions requiring multiple codes, or a single code for the remaining 171 conditions; and
any code appearing within the past 12 months for the 41 conditions requiring multiple codes, or a single code for the remaining 171 conditions.

Figure 2
Schematic diagram of assignment of a long term condition for a hypothetical patient under each alternative definition. Each cross represents a code for a condition in a single patient s electronic health record timeline from January 2018 to April 2019, with the colour representing the disease. The single code means a single code was sufficient for diagnosis results in all four long term conditions being counted. Two codes over three months and two codes over 12 months required at least two codes separated by at least three months and 12 months, respectively. Three within 12 months' requires three codes occurring within a 12 month time window. Any in past 12 months requires at least one code occurring in the past 12 months (measured in the 12 months from 1 January 2019 to 1 January 2020. Diseases defined by problems not included here

We also compared a definition of multimorbidity only including conditions defined as active problems (referred to here as problems). Diagnostic codes entered into the record can be marked by clinicians as problems, which then appear on a patient's summary during a consultation. These may be more visible to the clinician than historical diagnostic codes, which would require manual searching through the electronic notes and are also pulled across onto referral letters. Problems are recorded as either active or past problems and can be marked as ended if no longer active, with the end date recorded. We included only those problems that were active, that is, those that did not have an associated end date before 1 January 2020 and were marked as active. The problem definition was applied to all 212 long term conditions and not only the 41 requiring multiple codes. Further details are given in the online supplemental appendix page 10.

---

### The NASSS framework for ex post theorisation of technology-supported change in healthcare: worked example of the TORPEDO programme [^114jix1u]. BMC Medicine (2019). Medium credibility.

Results

The analysis identified interacting complexities in the TORPEDO programme which played out differently in different sites and settings. These are presented under the NASSS domains below. Direct quotes from the new primary dataset of ex post interviews are labelled "ex post interview [number]"; quotes from the original TORPEDO dataset are labelled with the original coding notation (e.g. 2282–005, with the first four digits indicating the original study site number and the last three digits indicating the participant number).

The condition

HealthTracker was designed for use in two kinds of patient: those who already had cardiovascular disease and those (usually asymptomatic) who were potentially at high risk of developing it. Established cardiovascular disease is well characterised, and guidelines for its management are relatively uncontested and widely accepted. The evidence base on managing cardiovascular risk is more complex. It is skewed towards a white European and North American population (especially the US Framingham study, on which the HealthTracker algorithm was partially based). Furthermore, since cardiovascular risk is a continuous variable influenced by multiple risk factors such as blood pressure and cholesterol levels, HealthTracker could not offer an unambiguous binary categorisation of patients into "high risk" or "low risk".

In Australia (as elsewhere), cardiovascular disease is strongly patterned by socio-demographic factors: it is commoner in those who are poor, those with low health literacy, and in Aboriginal people. Such individuals are more likely to have comorbidities such as diabetes or mental health conditions ("High risk is a sort of multifarious set of component conditions" — ex post interview 2). They may also have cultural beliefs and practices that affect their ability and willingness to understand the risk communication and comply with preventive treatment.

---

### Vernier frequency division with dual-microresonator solitons [^1111jcNW]. Nature Communications (2020). High credibility.

Fig. 3
Summary of experimental data.

a Optical spectra of main solitons (red) and Vernier solitons (blue) with sech² envelopes (dashed lines). The 9-th and 11-th pairs of comb lines are shown in the zoomed-in panel. The pump laser is suppressed by Bragg-grating filters. b ESA spectra of dual-comb beat notes. Δ₁, Δ₉, Δ₁₀, and Δ₁₁ are apparent. The strong VCO₁ beat note is derived from the pump laser unit, and can be filtered out optically or electronically. ESA spectrum of: c Δ₉ divided by 36, d Δ₁₁ divided by 44, e fᵥ = [fᵣ₁/198] as the sum of [Δ₉/36] and [Δ₁₁/44], and f beat note fₑ from out-of-loop EOM method. g Phase noise measurement of fᵥ (red) and fₑ (blue). The phase noise of fᵥ multiplied by 198² matches that of fᵣ₁ measured by out-of-loop EOM method. h Rep-rate of the main solitons measured by Vernier method (orange) and EOM method (blue). Both main and Vernier solitons are free-running. The gate time is 10 ms. i The frequency difference between rep-rate measured with Vernier and EOM methods in panel h. Mean value is concluded with a 95% confidence interval under normal distribution. j Allan deviation of the frequency difference. The frequency difference agrees with the counter resolution limit for the Vernier method.

---

### A summary of the methods that the national clinical guideline centre uses to produce clinical guidelines for the national institute for health and clinical excellence [^111bQLU7]. Annals of Internal Medicine (2011). Medium credibility.

NICE study search and retrieval workflow — Search results undergo a First sift: titles (done by information scientist or systematic reviewer) to exclude studies outside the topic; a Second sift: abstracts (done by systematic reviewer) to exclude studies that are not relevant to the review questions; and Assessment of full articles (done by systematic reviewer) to exclude studies on limits set by the GDG (for example, study design or outcomes), after which studies included proceed for data extraction; because of potential bias or error, a second reviewer performs sampling checks, and usually several thousand titles are sifted at the first stage.

---

### Good laboratory practices for biochemical genetic testing and newborn screening for inherited metabolic disorders [^116hxiHD]. MMWR: Recommendations and Reports (2012). Medium credibility.

Postanalytic testing phase — test reports must comply with the Clinical Laboratory Improvement Amendments (CLIA) general test report requirements (42 CFR §493.1291) and should include the recommended additional information that follows to ensure accurate understanding and interpretation of test results. CLIA requires that test reports for nonwaived testing include the following information: patient name and identification number or a unique patient identifier and identification number; name and address of the laboratory where the test was performed, test report date, test performed, specimen source (when appropriate), and test result and (if applicable) units of measurement or interpretation; and information regarding the condition and disposition of specimens that did not meet laboratory criteria for acceptability. For laboratory-developed tests using analyte-specific reagents, test reports must include the statement, "This test was developed and its performance characteristics determined by (Laboratory Name). It has not been cleared or approved by the U.S. Food and Drug Administration" (21 CFR 809.30(e)). In addition, reports should include the reference intervals or normal range and other performance specifications and limitations that affect the understanding and clinical use of the test results, test results in appropriate measurement units and current recommended standard nomenclature, interpretation of results for complex tests and profiles linked to the reasons for testing and communicated in a timely and clinically relevant manner, the name of the laboratory personnel providing the interpretation, notation to indicate whether the report is preliminary, final, amended, or corrected, and the date and, when appropriate, time the test report is released.

---

### How to find answers to clinical questions [^114BuGKA]. American Family Physician (2009). Low credibility.

Many barriers exist to finding answers to physicians' clinical questions. Lack of time, resources, and computer skills, as well as physicians' environment and attitudes about problem solving, all contribute to unanswered questions. Making use of computer-based information resources can give physicians a framework for answering questions and keeping their practice consistent with the best available evidence.

---

### Naming and counting disorders (conditions) included in newborn screening panels [^114qLhr6]. Pediatrics (2006). Low credibility.

The rapid introduction of new technologies for newborn screening is affecting decisions about the disorders (conditions) that are required or offered as an option through public and private newborn screening. An American College of Medical Genetics report to the Health Resources and Services Administration summarized an extensive effort by a group of experts, with diverse expertise within the newborn screening system, to determine a process for selecting a uniform panel of newborn screening disorders. The expert panel did not propose a mechanism for counting or naming conditions. Differences in the nomenclature used to identify disorders have resulted in difficulties in developing a consensus listing and counting scheme for the disorders in the recommended uniform panel. We suggest a system of nomenclature that correlates the screening panel of disorders recommended in the American College of Medical Genetics report with the screening analyte and accepted standardized nomenclature. This nomenclature system is proposed to remove ambiguity and to increase national uniformity in naming and counting screening disorders.

---

### A summary of the methods that the national clinical guideline centre uses to produce clinical guidelines for the national institute for health and clinical excellence [^111Ew2rg]. Annals of Internal Medicine (2011). Medium credibility.

NICE evidence searching — Information scientists search core databases (for example, the Cochrane Database of Systematic Reviews, DARE, CENTRAL, MEDLINE, EMBASE, CINAHL, HEED, and NHS EED) and, as needed, subject-specific databases; search strategies may limit by date, language, study design, or participant age, questions may be grouped, terms are agreed with the program and modified if needed, and searches are rerun 6 to 8 weeks before the first draft is finalized to include studies published during development.

---

### Sample sizes based on three popular indices of risks [^112uGtAa]. General Psychiatry (2018). Low credibility.

Sample size justification is a very crucial part in the design of clinical trials. In this paper, the authors derive a new formula to calculate the sample size for a binary outcome given one of the three popular indices of risk difference. The sample size based on the absolute difference is the fundamental one, which can be easily used to derive sample size given the risk ratio or OR.

---

### Context-dependent limb movement encoding in neuronal populations of motor cortex [^113QbkFu]. Nature Communications (2019). High credibility.

Twin grasp analysis

Similarity of grasp pairs across the two conditions (regular and irregular) was quantified as described above using Euclidean distance of normalized vectors in the 12-dimensional space (four joint angles, four joint angle speeds, four joint angle accelerations, Eqs. (2) and (3)). Starting with the standard grasp cluster, we first selected the most similar grasp pair across conditions. The respective standard grasps were then no longer available for further selections. From the remainder of grasps in the standard grasp clusters, we again selected the most similar pair and repeated this procedure until all grasps in the standard grasp cluster of one condition were consumed. The same procedure for twin grasp selection was performed on the corrective and digit-tip grasp clusters. After this selection procedure, each twin cluster in the regular condition featured the same number of grasps as its counterpart in the irregular condition. The mean deviation of all joint angles in twin grasp pairs over the grasp duration was 14.03° ± 1.37° across animals (mean ± s.d. see Supplementary Fig. 7a for values with regard to individual neuronal networks and individual joints). In the twin grasp analysis, the grasp-to-grasp variability of each joint angle was calculated as before for all grasps in the regular or irregular condition (not only for twin grasps) because we regarded this measure as a general feature of the entire movement sequence and its environmental context.

---

### Vernier frequency division with dual-microresonator solitons [^1122NmHG]. Nature Communications (2020). High credibility.

Beat frequencies Δ₉ and Δ₁₁ are selected for the main soliton rep-rate division. Δ₉ (Δ₁₁) is the beat frequency between the 9 (11)-th Vernier soliton comb line and the 10 (12)-th main soliton comb line, where Δ₉ = 10fᵣ₁ − 9fᵣ₂, and Δ₁₁ = 11fᵣ₂ − 12fᵣ₁. In the measurement, after combining the main and Vernier solitons with a fiber coupler, a bandpass filter is used to pass the comb lines associated with Δ₉, Δ₁₀, and Δ₁₁ for optical amplification. Then a second fiber coupler splits the power into two optical paths, where in each path a bandpass filter is used to select the comb lines of Δ₉ or Δ₁₁, and the corresponding beat note is created on a photodiode. To divide the main soliton rep-rate, Δ₉ and Δ₁₁ are divided by 36 and 44 in frequency, respectively, and sent to a RF mixer to produce their sum frequency, fᵥ = [Δ₉/36] + [Δ₁₁/44] = [fᵣ₁/198], which is the main soliton repetition rate divided by 198. The electrical spectra of [Δ₉/36], [Δ₁₁/44] and their sum fᵥ are shown in Fig. 3 c–e. The complete experimental setup is shown in Fig. 2. More experimental details are included in Methods section. In principle, one can use the configuration in Fig. 1, where Δ₁ is mixed with [Δₙ/N] to generate [fᵣ₁/N]. However, limited by the selection of electrical mixers in our lab, we do not have the capability to mix Δ₁ (~20 GHz) and [Δₙ/N] (~2 GHz for N = 9, 11), and thus we select Δ₉ and Δ₁₁ instead.

---

### The PRISMA statement for reporting systematic reviews and meta-analyses of studies that evaluate healthcare interventions: explanation and elaboration [^1118RpwT]. BMJ (2009). Excellent credibility.

Some studies are published more than once. Duplicate publications may be difficult to ascertain, and their inclusion may introduce bias. We advise authors to describe any steps they used to avoid double counting and piece together data from multiple reports of the same study (such as juxtaposing author names, treatment comparisons, sample sizes, or outcomes). We also advise authors to indicate whether all reports on a study were considered, as inconsistencies may reveal important limitations. For example, a review of multiple publications of drug trials showed that reported study characteristics may differ from report to report, including the description of the design, number of patients analysed, chosen significance level, and outcomes. Authors ideally should present any algorithm that they used to select data from overlapping reports and any efforts they used to solve logical inconsistencies across reports.

Item 11: Data items

List and define all variables for which data were sought (such as PICOS, funding sources) and any assumptions and simplifications made.

Examples "Information was extracted from each included trial on: (1) characteristics of trial participants (including age, stage and severity of disease, and method of diagnosis), and the trial's inclusion and exclusion criteria; (2) type of intervention (including type, dose, duration and frequency of the NSAID [non-steroidal anti-inflammatory drug]; versus placebo or versus the type, dose, duration and frequency of another NSAID; or versus another pain management drug; or versus no treatment); (3) type of outcome measure (including the level of pain reduction, improvement in quality of life score (using a validated scale), effect on daily activities, absence from work or school, length of follow up, unintended effects of treatment, number of women requiring more invasive treatment)".

---

### Checking reference lists to find additional studies for systematic reviews [^116UerSy]. The Cochrane Database of Systematic Reviews (2011). Low credibility.

Background

Checking reference lists to identify relevant studies for systematic reviews is frequently recommended by systematic review manuals and is often undertaken by review authors. To date, no systematic review has explicitly examined the effectiveness of checking reference lists as a method to supplement electronic searching.

Objectives

To investigate the effectiveness of checking reference lists for the identification of additional, relevant studies for systematic reviews. Effectiveness is defined as the proportion of relevant studies identified by review authors solely by checking reference lists.

Search Strategy

We searched the databases of The Cochrane Library (Issue 3, 2008), Library and Information Science abstracts (LISA) (1969 to July 2008) and MEDLINE (1966 to July 2008). We contacted experts in systematic review methods and examined reference lists of articles.

Selection Criteria

Studies of any design which examined checking reference lists as a search method for systematic reviews in any area. The primary outcome was the additional yield of relevant studies (i.e. studies not found through any other search methodologies); other outcomes were publication types identified and data pertaining to the costs (e.g. cost-effectiveness, cost-efficiency) of checking reference lists.

Data Collection and Analysis

We summarized data descriptively.

Main Results

We included 12 studies (in 13 publications) in this review, but interpretability and generalizability of these studies is difficult and the study designs used were at high risk of bias. The additional yield (calculated by dividing the additional 'unique' yield identified by checking reference lists by the total number of studies found to be eligible within the study) of relevant studies identified through checking reference lists ranged from 2.5% to 42.7%. Only two studies reported yield information by publication type (dissertations and systematic reviews). No cost data were reported although one study commented that it was impossible to isolate the time spent on reference tracking since this was done in parallel with the critical appraisal of each paper, and for that particular study costs were not specifically estimated.

Authors' Conclusions

There is some evidence to support the use of checking reference lists for locating studies in systematic reviews. However, this evidence is derived from weak study designs. In situations where the identification of all relevant studies through handsearching and database searching is difficult, it would seem prudent that authors of reviews check reference lists to supplement their searching. The challenge, therefore, is for review authors to recognize those situations.

---

### Simple approaches to characterising multiple long-term conditions (multimorbidity) and rates of emergency hospital admission: findings from 495, 465 UK biobank participants [^1166np9r]. Journal of Internal Medicine (2023). Medium credibility.

Assessment of covariates

Covariates that are widely available in population‐based studies, assessed during primary care health checks and may potentially confound the main associations of interest were selected a priori. These factors were all assessed during the baseline assessment. We described socioeconomic position using the Townsend deprivation index. This is a commonly used area‐based measure of socioeconomic position derived from postcodes, which was categorised into fifths from the least deprived to the most deprived. We calculated body mass index (BMI) using measured height and weight, categorised as normal (< 25 kg/m²), overweight (≥ 25 and < 30 kg/m²) and obese (≥ 30 kg/m²). We categorised self‐reported smoking status and alcohol status as never, previous and current.

Assessment of emergency hospital admission (or death)

Participants were linked to National Health Service (NHS) central registers for information on death and to country‐specific hospital admission records (Hospital Episode Statistics, Scottish Morbidity Records and the Patient Episode Database for Wales). To minimise error, participants were linked using their 10‐digit NHS number, postcode, sex and age. Notifications of deaths were recorded in the UK Biobank via regular updates from NHS Digital for participants in England and Wales and from the NHS Central Register for participants in Scotland. We calculated time to event in days from the date of baseline assessment to whichever of emergency hospital admission or death occurred first over 5 years of follow‐up, which was taken as 1826 days. Participants who withdrew consent for future linkage of data during this time were censored at the interval between baseline assessment and the date recorded in the UK Biobank as lost to follow‐up. Participants not experiencing emergency admission or death were censored at 1826 days.

---

### Vol. 66, no. 21 [^114dx2Uf]. MMWR: Morbidity and Mortality Weekly Report (2017). Low credibility.

In "Notifiable Diseases and Mortality Tables", on pages ND403–22, weekly case counts were inadvertently omitted from the tables "Provisional cases of selected* infrequently reported notifiable diseases (< 1,000 cases reported during the preceding year) — United States, week ending May 27, 2017 (21st week) † " and "Provisional cases of selected notifiable diseases (≥ 1,000 cases reported during the preceding year), and selected* low frequency diseases, United States and U.S. territories, weeks ending May 27, 2017, and May 28, 2016 (21st week). † " The updated Week 21 tables can be found atand at.

---

### Expert panel on integrated guidelines for cardiovascular health and risk reduction in children and adolescents: summary report [^112wrhgx]. PES (2012). Medium credibility.

Quality criteria for assessment of individual randomized controlled trials — The table lists "Criteria Needed for 'Y' Selection" including study selection elements "Inclusion/exclusion criteria specified", "Criteria applied equally to study arms", and "Comparable patient characteristics", with details that "Health, demographics, and other characteristics of subjects/patients [are] described" and that the "Distribution of health, demographics, and other characteristics [is] similar across study arms at baseline". Methodologic safeguards include "Appropriate randomization" with the "Method of randomizing subjects to arms described" and "free from bias (e.g., random number generator)", "Allocation concealment" to prevent "foreknowledge of treatment assignment", and blinding of participants and assessors, including that "Patients or subjects [are] blinded to treatment as appropriate", "Provider or other treatment administrator [is] blinded", and "Data collectors/analysts or others with ability to affect results [are] blinded as appropriate". Additional items are "Low attrition rates" with a "Low rate of attrition for each arm" and "Withdrawals and reasons for withdrawal similar across arms", plus transparency items "Conflicts of interest" with "Sources of funding and investigators' affiliations described", and "Industry sponsorship beyond provision of drug or placebo", with notes that "Selection does not affect quality grade" and that "an asterisk will appear by quality grade if selection is 'Y'". Data value options shown include "• Y
- N
- NR/unknown".

---

### Search strategies to identify diagnostic accuracy studies in MEDLINE and EMBASE [^114vQmk1]. The Cochrane Database of Systematic Reviews (2013). Low credibility.

Background

A systematic and extensive search for as many eligible studies as possible is essential in any systematic review. When searching for diagnostic test accuracy (DTA) studies in bibliographic databases, it is recommended that terms for disease (target condition) are combined with terms for the diagnostic test (index test). Researchers have developed methodological filters to try to increase the precision of these searches. These consist of text words and database indexing terms and would be added to the target condition and index test searches. Efficiently identifying reports of DTA studies presents challenges because the methods are often not well reported in their titles and abstracts, suitable indexing terms may not be available and relevant indexing terms do not seem to be consistently assigned. A consequence of using search filters to identify records for diagnostic reviews is that relevant studies might be missed, while the number of irrelevant studies that need to be assessed may not be reduced. The current guidance for Cochrane DTA reviews recommends against the addition of a methodological search filter to target condition and index test search, as the only search approach.

Objectives

To systematically review empirical studies that report the development or evaluation, or both, of methodological search filters designed to retrieve DTA studies in MEDLINE and EMBASE.

Search Methods

We searched MEDLINE (1950 to week 1 November 2012); EMBASE (1980 to 2012 Week 48); the Cochrane Methodology Register (Issue 3, 2012); ISI Web of Science (11 January 2013); PsycINFO (13 March 2013); Library and Information Science Abstracts (LISA) (31 May 2010); and Library, Information Science & Technology Abstracts (LISTA) (13 March 2013). We undertook citation searches on Web of Science, checked the reference lists of relevant studies, and searched the Search Filters Resource website of the InterTASC Information Specialists' Sub-Group (ISSG).

Selection Criteria

Studies reporting the development or evaluation, or both, of a MEDLINE or EMBASE search filter aimed at retrieving DTA studies, which reported a measure of the filter's performance were eligible.

Data Collection and Analysis

The main outcome was a measure of filter performance, such as sensitivity or precision. We extracted data on the identification of the reference set (including the gold standard and, if used, the non-gold standard records), how the reference set was used and any limitations, the identification and combination of the search terms in the filters, internal and external validity testing, the number of filters evaluated, the date the study was conducted, the date the searches were completed, and the databases and search interfaces used. Where 2 × 2 data were available on filter performance, we used these to calculate sensitivity, specificity, precision and Number Needed to Read (NNR), and 95% confidence intervals (CIs). We compared the performance of a filter as reported by the original development study and any subsequent studies that evaluated the same filter.

Main Results

Ninteen studies were included, reporting on 57 MEDLINE filters and 13 EMBASE filters. Thirty MEDLINE and four EMBASE filters were tested in an evaluation study where the performance of one or more filters was tested against one or more gold standards. The reported outcome measures varied. Some studies reported specificity as well as sensitivity if a reference set containing non-gold standard records in addition to gold standard records was used. In some cases, the original development study did not report any performance data on the filters. Original performance from the development study was not available for 17 filters that were subsequently tested in evaluation studies. All 19 studies reported the sensitivity of the filters that they developed or evaluated, nine studies reported the specificities and 14 studies reported the precision. No filter which had original performance data from its development study, and was subsequently tested in an evaluation study, had what we defined a priori as acceptable sensitivity (> 90%) and precision (> 10%). In studies that developed MEDLINE filters that were evaluated in another study (n = 13), the sensitivity ranged from 55% to 100% (median 86%) and specificity from 73% to 98% (median 95%). Estimates of performance were lower in eight studies that evaluated the same 13 MEDLINE filters, with sensitivities ranging from 14% to 100% (median 73%) and specificities ranging from 15% to 96% (median 81%). Precision ranged from 1.1% to 40% (median 9.5%) in studies that developed MEDLINE filters and from 0.2% to 16.7% (median 4%) in studies that evaluated these filters. A similar range of specificities and precision were reported amongst the evaluation studies for MEDLINE filters without an original performance measure. Sensitivities ranged from 31% to 100% (median 71%), specificity ranged from 13% to 90% (median 55.5%) and precision from 1.0% to 11.0% (median 3.35%). For the EMBASE filters, the original sensitivities reported in two development studies ranged from 74% to 100% (median 90%) for three filters, and precision ranged from 1.2% to 17.6% (median 3.7%). Evaluation studies of these filters had sensitivities from 72% to 97% (median 86%) and precision from 1.2% to 9% (median 3.7%). The performance of EMBASE search filters in development and evaluation studies were more alike than the performance of MEDLINE filters in development and evaluation studies. None of the EMBASE filters in either type of study had a sensitivity above 90% and precision above 10%.

Authors' Conclusions

None of the current methodological filters designed to identify reports of primary DTA studies in MEDLINE or EMBASE combine sufficiently high sensitivity, required for systematic reviews, with a reasonable degree of precision. This finding supports the current recommendation in the Cochrane Handbook for Systematic Reviews of Diagnostic Test Accuracy that the combination of methodological filter search terms with terms for the index test and target condition should not be used as the only approach when conducting formal searches to inform systematic reviews of DTA.

---

### Methodologies for the development of CHEST guidelines and expert panel reports [^115yEiB6]. Chest (2014). Medium credibility.

CHEST consensus achievement process — Delphi-based voting parameters are specified: "An explicit consensus achievement process based on the Delphi technique develops suggestions… for voting by all eligible panelists", and "Up to three rounds of voting may occur using the GRADE grid… until consensus is achieved". The voting thresholds require "The minimal response rate for each suggestion is 75% of the panel, with 80% of respondents voting to agree or strongly agree… to obtain consensus", with revisions permitted "not meeting the 80% threshold". "Suggestions not achieving consensus in three rounds are dropped", and "All voting is anonymous, and final tallies are available upon request".

---

### Optimal resources for children's surgical care: from the committee on children's surgery American college of surgeons [^115EHGAn]. APSA (2014). Medium credibility.

Children's surgical care — patient record and personal effects for transfer — specifies that each Institution agrees to provide information to accompany the patient from one Institution to the other, to include when available: patient identifiers and a decision‑maker contact ('Patient's name, address, patient identification number, age and the name, address and telephone number of at least one of the following (in the order of priority): the patient's legal guardian or other person authorized to make medical decisions for the patient;'), pertinent administrative and social information, third party billing data if any (including managed care participation and prior authorizations, provided in emergencies only if it does not delay treatment), and clinical documentation ('All medical records… including available history, records relating to the patient's emergency medical condition, observation of signs or symptoms, preliminary diagnosis').

---

### Onabotulinumtoxina (Botox) [^113iFGtm]. FDA (2023). Medium credibility.

[a]Confidence intervals are constructed from the analysis of covariance table with treatment and investigational site as main effects, and baseline CDSS as a covariate.

[b]These values represent the prospectively planned method for missing data imputation and statistical test. Sensitivity analyses indicated that the 95% confidence interval excluded the value of no difference between groups and the p-value was less than 0.05. These analyses included several alternative missing data imputation methods and non-parametric statistical tests.

[c]Confidence intervals are based on the t-distribution.

Exploratory analyses of this study suggested that the majority of patients who had shown a beneficial response by week 6 had returned to their baseline status by 3 months after treatment. Exploratory analyses of subsets by patient sex and age suggest that both sexes receive benefit, although female patients may receive somewhat greater amounts than male patients. There is a consistent treatment-associated effect between subsets greater than and less than age 65. There were too few non-Caucasian patients enrolled to draw any conclusions regarding relative efficacy in racial subsets.

In this study the median total BOTOX dose in patients randomized to receive BOTOX (N = 88) was 236 Units, with 25th to 75th percentile ranges of 198 Units to 300 Units. Of these 88 patients, most received injections to 3 or 4 muscles; 38 received injections to 3 muscles, 28 to 4 muscles, 5 to 5 muscles, and 5 to 2 muscles. The dose was divided amongst the affected muscles in quantities shown in Table 49. The total dose and muscles selected were tailored to meet individual patient needs.

* The mid-range of dose is calculated as the 25th to 75th percentiles.

There were several randomized studies conducted prior to the double-blind, placebo-controlled study, which were supportive but not adequately designed to assess or quantitatively estimate the efficacy of BOTOX.

---

### Defining the time-limited trial for patients with critical illness: an official American Thoracic Society workshop report [^116J4ouw]. Annals of the American Thoracic Society (2024). High credibility.

Time-limited trial in critical care — essential elements and phased process were derived through a Delphi process: investigators "identified 18 potential steps" and found first-round consensus that "11 of these were essential elements"; during the second round, they "identified seven additional essential elements". The committee then "combined 4 related items into two steps (to reach 16 essential elements)" and "organized steps into four phases of care: consider, plan, support, and reassess". Additional steps "may be helpful in some cases, but not necessary for all trials", and involvement of other disciplines is "highly dependent on a patient's specific situation and on the available hospital resources".

---

### Pediatric and adult brain death / death by neurologic criteria consensus guideline [^113YXeFt]. Neurology (2023). High credibility.

Regarding diagnostic investigations for brain death, more specifically with respect to neurological evaluation (numbers of examinations), AAN/AAP/CNS/SCCM 2023 guidelines recommend to assess all elements of the neurologic examination for BD/death by neurologic criteria evaluation that can be assessed and ensure that findings are consistent with BD/death by neurologic criteria.
Recognize that the patient does not meet the criteria for BD/death by neurologic criteria if any components of the neurologic examination are inconsistent with BD/death by neurologic criteria.

---

### Preparing raw clinical data for publication: guidance for journal editors, authors, and peer reviewers [^112g4HBs]. BMJ (2010). Excellent credibility.

Data preparation guidance

What is the dataset?

For the purposes of this guidance, the dataset is the aggregated collection of patient observations (including sociodemographic and clinical information) used for the purposes of producing the summary statistical findings presented in the main report of the research project, whether previously published or not.

Data are almost always collected at a greater level of detail than are reported in a journal article. For example, each participant in a pain study may complete a pain diary twice a day for 30 days, with the authors reporting "mean post-treatment pain" for one or more groups of participants. Similarly, a quality of life questionnaire may include a large number of questions divided into domains such as physical, mental, emotional, and social wellbeing.

Here we define a dataset as that containing the minimum level of detail necessary to reproduce all numbers reported in the paper. The dataset for the pain trial, for example, might therefore consist of one value per individual for mean post-treatment pain rather than 30 values for pain levels on each day. However, if more detailed, underlying data are available and can be shared then that should also be encouraged, provided the data conform to the same standards — as proposed in this article — as the main dataset. If possible, authors should present all outcomes and variables, regardless of significance.

Anonymisation

A list of 28 patient identifiers has been formulated, based on information aggregated from policy documents and research guidance from major UK and US funding agencies, governmental health departments and statutes, and three internationally recognised publication ethics resources for editors of biomedical journals. This list is provided in the table.

list of potential patient identifiers in datasets

Types of identifying information have been classified as either direct or indirect. Publication of any direct identifiers places individuals in the dataset at risk of being identified. Although none of the indirect identifiers on its own would point to an individual, a dataset with several indirect identifiers, especially those relating to attributes, might do. The consensus of the authors, and working group members acknowledged in the current manuscript, is that a dataset including three or more indirect identifiers should be assessed by an independent researcher or ethics committee to evaluate the risk that individuals might be identifiable. If the risk of identification is considered non-negligible, before publication can proceed approval should be sought from a relevant advisory body (see below). An explicit justification for publication of a dataset with three or more indirect identifiers should be given by the researcher — as an annotation to the dataset and in any accompanying articles. This should include the name of any oversight bodies consulted.

---

### The independent contribution of executive functions to health related quality of life in older women [^113eeF2v]. BMC Geriatrics (2010). Low credibility.

Verbal Digits Backward

We used the Verbal Digits Backward test to assess working memory. Working memory (updating) refers to an individuals ability filter incoming information for relevance to the current task and subsequently update informational content replacing old non-relevant information with new relevant incoming information. Seven pairs of random number sequences were read aloud by an assessor at one number per second. The first sequence of numbers is three and the sequence was increased by one number up to a length of nine digits. Participants repeated each sequence in exactly the reverse order until they failed two attempts of the same sequence length. It was scored on a 14-point scale with higher scores indicating a better performance. For the verbal digits forward test, the participant's task is to repeat each sequence exactly as it is given. The difference between the verbal digits forward test score and the verbal digits backward test score was used as an index of the central executive component of working memory. Smaller difference scores indicate better working memory.

Stroop Test

The Stroop Test, assessed response inhibition including deliberate inhibition of automatic, dominant or routine responses. For the primary test condition, participants were presented colour-words printed in incongruent coloured inks (e.g. the word "BLUE" printed in red ink) and were required name the ink colour that the words were printed while ignoring the word itself. We recorded the time participants took to read each condition. The ability to selectively attend and control response output was calculated as the time difference between the test condition and the priming condition (e.g. coloured X's). Smaller time differences indicate better selective attention and conflict resolution.

---

### Neural representations of the perception of handwritten digits and visual objects from a convolutional neural network compared to humans [^116UaUza]. Human Brain Mapping (2023). Medium credibility.

Numeral digits are fundamental symbolic visual stimuli for humans that encode the numerical concept of abstract numbers (Dehaene; Nieder). The neural underpinnings of numerical digit recognition in the human brain have long been the focus of research (Anobile et al; Ansari et al; Bulthé et al; Dehaene; Dehaene & Cohen; Nieder; Yeo et al.). Bulthé and colleagues demonstrated that multivoxel pattern analysis (MVPA) of fMRI data can be used to decode the numerical magnitude of Arabic digits from the localized regions. More recently, Yeo and colleagues reported the categorical distinction of numbers versus other symbols in the number‐preferring region in the posterior inferior temporal gyrus (pITG). However, very few studies have investigated the visual perception of symbolic digits in conjunction with that of objects. Investigating neural representations for visual object recognition simultaneously with symbolic digit recognition is fundamental to understanding human visual perception. We believe that brain‐encoding research on the visual perception of concrete objects and symbolic digits is urgently needed.

In the present study, we were motivated to investigate the neural representation of digit and object perception from a CNN model and humans using naturalistic visual stimuli. To this end, we used the ten handwritten digits available in the MNIST dataset and six objects in the ImageNet dataset as visual stimuli. The neural activations for each of the stimuli were acquired from fMRI data. The features from the CNN model trained to classify the 10 digits and six objects were represented as an RDM across all 16 classes. Consequently, we conducted RSA to obtain the neural representation of the human brain from a trained CNN model perspective. We also designed RDM codes that encode several hypothetical human perceptions across our digit and object categories in terms of the numerical magnitude of the digits (i.e. small vs. large) and animacy of the objects (i.e. animate vs. inanimate) in addition to distinguishing between digits, between objects, and digits versus objects conditions. We believed that the systematic comparison of the neural representations from the CNN model and from human perception would illustrate their similarities and differences, providing an insight into the development of CNN‐based computational models that imitate the visual perception of humans.

---

### Medication abortion up to 70 days of gestation: ACOG practice bulletin, number 225 [^114wevsA]. Obstetrics and Gynecology (2020). High credibility.

U.S. Preventive Services Task Force evidence hierarchy and recommendation grading for this ACOG Practice Bulletin are specified as follows: the MEDLINE database, the Cochrane Library, and internal American College of Obstetricians and Gynecologists resources were searched for articles "published between January 2000 and February 2020", with the search "restricted to articles published in the English language" and priority to original research; studies were "reviewed and evaluated for quality according to the method outlined by the U.S. Preventive Services Task Force", with evidence levels I (evidence obtained from at least one properly designed randomized controlled trial), II-1 (evidence obtained from well-designed controlled trials without randomization), II-2 (evidence obtained from well-designed cohort or case–control analytic studies, preferably from more than one center or research group), II-3 (evidence obtained from multiple time series with or without the intervention; dramatic results in uncontrolled experiments also could be regarded as this type of evidence), and III (opinions of respected authorities, based on clinical experience, descriptive studies, or reports of expert committees); based on the highest level found, recommendation grades are Level A (recommendations are based on good and consistent scientific evidence), Level B (recommendations are based on limited or inconsistent scientific evidence), and Level C (recommendations are based primarily on consensus and expert opinion).

---

### Rigorous location of phase transitions in hard optimization problems [^116ZYG3W]. Nature (2005). Excellent credibility.

It is widely believed that for many optimization problems, no algorithm is substantially more efficient than exhaustive search. This means that finding optimal solutions for many practical problems is completely beyond any current or projected computational capacity. To understand the origin of this extreme 'hardness', computer scientists, mathematicians and physicists have been investigating for two decades a connection between computational complexity and phase transitions in random instances of constraint satisfaction problems. Here we present a mathematically rigorous method for locating such phase transitions. Our method works by analysing the distribution of distances between pairs of solutions as constraints are added. By identifying critical behaviour in the evolution of this distribution, we can pinpoint the threshold location for a number of problems, including the two most-studied ones: random k-SAT and random graph colouring. Our results prove that the heuristic predictions of statistical physics in this context are essentially correct. Moreover, we establish that random instances of constraint satisfaction problems have solutions well beyond the reach of any analysed algorithm.

---

### Erratum: vol. 69, no. 31 [^11489pVV]. MMWR: Morbidity and Mortality Weekly Report (2020). Medium credibility.

In the report "Alcohol Use and Co-Use of Other Substances Among Pregnant Females Aged 12–44 Years — United States, 2015–2018", on p. 1011, errors occurred in the "Race/Ethnicity" section of Table 1. In the row for "Black, non-Hispanic", the numbers under "Past 30 days binge drinking" should have read 7.2 (4.6–10.9) § and in the row for "Other", the numbers under "Past 30 days drinking" should have read 8.4 (4.8–14.4) §.

---

### As-3 [^117FFFuc]. FDA (2025). Medium credibility.

Storage and handling

Store at room temperature (25 °C/77 °F). Avoid excessive heat.

Protect from freezing

---

### Changes in semantic memory structure support successful problem-solving and analogical transfer [^1136rTFc]. Communications Psychology (2024). Medium credibility.

Introduction

In our daily life, we constantly deal with problems, ranging from the most mundane (e.g. what to cook for dinner given the ingredients at our disposal), to professional activities (e.g. how to reorganize our current plans to meet a new deadline), up to major societal challenges (e.g. how to find innovative solutions against global warming). How do we find new solutions to problems? While the ability to solve problems is a critical skill for adapting to new situations and innovating, the mechanisms underlying the problem-solving process remain largely unknown.

Among the new problems we face each day, some are well-defined (e.g. playing a jigsaw puzzle). The initial state (i.e. the number of independent pieces) and goal state (i.e. assembling the pieces so it looks like the picture model) are clear, and the solver can apply a set of operations (i.e. interlocking the pieces as a function of their shape) to reach the goal. However, for many of our problems (e.g. organizing work activities during the COVID-19 pandemic), the problem space is ambiguous. No heuristics or existing rules could be applied to transform the initial state into the goal state. Such "ill-defined" problemsthus require additional mental processes, which have been tightly linked to creative thinking –. Ill-defined problem-solving (or creative problem-solving) is often referred to as insight solving, where the solution comes to mind suddenly and effortlessly, with a "Eureka" phenomenon –. According to the Representational Change Theory, solving such problems involves restructuring the initial problem mental representational space, which presumably entails combining elements related to the problem in a new way. In theory, restructuring allows one to change perspective, reframe the problem, or escape its implicitly imposed constraints, leading to creative associations. For instance, consider the following problem: "A man walks into a bar and asks for a glass of water. The bartender points a shotgun at the man. The man says, 'Thank you', and walks out". The problem is ill-defined because the path to finding the solution is to be discovered, and the goal state is vague. Solving this problem first requires asking the right question: in which context would a shotgun and a glass of water help somebody? Rather than relying on obvious associations (e.g. a glass of water is related to thirst), solvers must fill the missing link between the relevant elements of the problem (a shotgun induces fear, and fear can be a remedy for hiccups, as can drinking a glass of water). Hence, restructuring the initial representation of a given problem would allow one to see this link and find its solution.

---

### Exenatide [^115Cb4T6]. FDA (2025). Medium credibility.

Section 2 Read and follow the directions in this section only after you've read Section 1 — What You Need To Know About Your Exenatide Injection Pen.

GETTING STARTED

Set up your new pen just before you use it the first time. For routine use, do not repeat this one-time-only new pen setup. If you do, you will run out of Exenatide Injection before 30 days of use.

ONE-TIME-ONLY NEW PEN SETUP

STEP 1 Check the Pen

STEP 2 Attach the Needle

STEP 3 Dial the Dose

Note: If you cannot turn the dose knob away from you to the, see Commonly Asked Questions, number 7, in Section 4 of these Instructions for Use.

STEP 4 Prepare the Pen

Note: If you do not see liquid after 4 times, see Commonly Asked Questions, number 3, in Section 4 of these Instructions for Use.

STEP 5 Complete New Pen Setup

Section 3 Now that you have done the one-time-only new pen setup, follow Section 3 for all of your injections.

ROUTINE USE

STEP 1 Check the Pen

STEP 2 Attach the Needle

STEP 3 Dial the Dose

Note: If you cannot turn the dose knob away from you to the, see Commonly Asked Questions, number 7, in Section 4 of these Instructions for Use.

---

### Does antenatal micronutrient supplementation improve children's cognitive function? Evidence from the follow-up of a double-blind randomised controlled trial in Nepal [^112bgsBY]. BMJ Global Health (2018). Medium credibility.

We used a counting Stroop test to assess children's executive function. The number–quantity Stroop task involved three conditions. In each condition, the children were asked to name the quantity of items in rows of one to seven identical items. In the baseline condition, children were asked to name the quantity of X in a row (eg, the correct response to XX is 'two'). Second, in the congruent condition, they were asked to name the quantity of digits in a number with the equivalent digits (eg, 55555 = 'five'). Finally, in the third, incongruent condition, they were asked to name the quantity of digits in a number with the non-equivalent digits (eg, 1111 = 'four'). We recorded the time that children took to correctly name all items for each condition. We then derived two scores from these totals. An interference score was calculated by subtracting the baseline from the incongruent condition. This represented the potentially disruptive effect of automatic reading of the incongruous digits on quantity naming. A facilitation score was calculated by subtracting results for the congruent condition from those for the baseline, representing the enhancing effect of congruent digits.

Training of the testers was comparable to that for the US standardisation of the UNIT. The principal investigator (SD) was trained by two experienced UK-based psychologists (one clinical, one experimental, AK and FL) in the general principles of psychometric assessment and procedures specific to the UNIT and the counting Stroop test. She recruited two testers from Dhanusha district who were fluent in the local language (Maithili) and had a background in education and social science. The testers were trained for 15 days to administer the tests and practised under supervision for a week with children aged 10–13. We video-recorded the testers' practice administrations of the UNIT. These were checked by the two experienced psychologists to ensure adherence to standardised testing procedures.

SD and the two other testers administered the UNIT and Stroop tests in an office with minimal distractions located in Janakpur. The testers recorded the children's UNIT raw and scaled scores on a form with a unique child identification number. We checked each form individually and confirmed the scores using the proprietary CompuScore (V.1.1) UNIT software. These audited raw, scaled and standardised scores were double-entered into a Microsoft Access database. We identified discrepancies between first and second entries and resolved them by consulting the original forms.

---

### The impact of alternative arrangements, contingent jobs, and work secured through an app on the well-being of working age adults: results from the California work and health survey [^113cqHwb]. American Journal of Industrial Medicine (2024). Medium credibility.

Background:

There is recognition of the growing prevalence of alternative work arrangements, contingent jobs, and work secured through an app. However, there have been few systematic efforts to understand the impact of these forms of work on individuals and households.

Methods:

The data derive from the California Work and Health Survey administered to a sample of the working age population of the state solicited through random-digit dialing of cell phone numbers. 4014 individuals completed the survey, 26% of those with an in-service cell phone number. We present odds ratios and 95% confidence intervals from logistic regression estimating the impact of being an independent contractor, in other forms of alternative work arrangements, in contingent jobs, and in work secured through an app, on economic and health status and working conditions in main jobs, with and without adjustment for covariates.

Results:

Several of the forms of work analyzed are associated with lower earnings and higher rates of wage theft, household poverty, benefit recipiency, and expectation of hardships in food, housing, and medical care in the immediate future. Association between the forms of work and current health status is less consistent. However, several forms of work are associated with working conditions known to be risk factors for subsequent health problems.

Conclusions:

Public policy to mitigate the adverse impacts of work, largely developed in the 20th Century when there was an identified workplace, may be insufficient to protect workers' well-being for alternative work arrangements, contingent jobs, and work secured through an app.

---

### Effective use of tables and figures in abstracts, presentations, and papers [^1152FgyH]. Respiratory Care (2004). Low credibility.

In some situations, tables, graphs, and figures can present certain types of information (including complicated relationships and sequences of events) more clearly and in less space than the same information would require in sentence form. However, do not use tables, graphs, and figures for small amounts of data that could be conveyed clearly and succinctly in a sentence. Also, do not reiterate in sentences the data that are shown in a table, graph, or figure: the point of creating a table or graph or figure is to eliminate that type of sentence from your manuscript. In building a data table you must balance the necessity that the table be complete with the equally important necessity that it not be too complex. Sometimes it is helpful to break a large table into several smaller ones to allow the reader to identify important information easily, but, conversely, it is a common mistake of novice authors to split up into several tables data that belong in one table. In almost all cases, only one table or graph or figure should be included in an abstract, and then only if it can convey essential information in less space and in a more easily interpretable way than the sentence form. For a poster, in almost all instances you should use only one typeface and one font in a table, graph, or figure. In general, do not use bold, italics, or color unless you are presenting a great deal of data and you need to highlight certain data values and you are certain that using bold, italics, or color will improve readability, which is rare. Do not include identical information in a table and a graph/figure. In reporting a clinical trial you will need to include a patient flow chart that identifies the number of patients initially screened for the study, the number of patients who were excluded (and why) after initial screening or in the final analysis, and how many patients entered, exited early, and completed each arm of the study. A treatment protocol should also be described with a flow chart. In preparing a graph the most common error is to include a line that suggests an unsubstantiated extrapolation between or beyond the data points. In selecting the graph's axes, avoid truncating, enlarging, or compressing the axes in ways that might make the graph confusing or misleading. To prepare clear, accurate, easily interpretable tables, graphs, and figures, rely on the rules described in authoritative guides such as the Council of Science Editors' Scientific Style and Format and the American Medical Association's Manual of Style.

---

### Committee opinion no. 676: health literacy to promote quality of care [^117VrXUE]. Obstetrics and Gynecology (2016). Medium credibility.

Patient tasks and system demands related to health literacy — When "patients encounter the health care system, they are expected to understand and apply verbal information pertaining to consent, diagnosis, medical advice, and treatment; have access to and use various forms of technology, including computers and smartphones; calculate and interpret numerical data; and comprehend graphs and visual information". Furthermore, "patients are expected to articulate accurate information about their symptoms, concerns, and medical history". The report cautions that "The current health care system asks consumers to make sophisticated health-related decisions using information relayed in encounters that often are brief and may not consider the individual's distinct information needs".

---

### Acute lymphoblastic leukaemia (ALL) things come to those who wait: 60 years of progress in the treatment of adult ALL [^117JCKBQ]. British Journal of Haematology (2020). Medium credibility.

To date, UKALL12 has contributed to 53 publications. UCL has thankfully agreed to retain sponsorship of UK ALL trials and has also taken on the paediatric trials. Amy Kirkwood has supported the statistical handover from Oxford to London and UK adult ALL trials continue to benefit from her first‐class input. The contributions and support of critical trial personnel also cannot be underestimated, we are grateful to numerous staff at CRUK and UCL Cancer Trial Centre for their enormous contribution to adult ALL trials in the UK.

---

### Prebiotic chemistry and human intervention [^116gF8vf]. Nature Communications (2018). Medium credibility.

It is not easy to compute a score for how prebiotically plausible an experiments is. Often, if a reagent has already been found somewhere in the geosphere, it is considered plausible, and if it was taken from the catalog of a chemical supplier, it is not. Is this reasonable? Further, how likely it is that a series of transformations occurred that require vastly different conditions in a specific sequence, is not easy to gauge, even for the specialist. Still, this parameter may be more important than the source of the chemicals. Temperature changes are plausible, based on seasonal changes and day/night cycles, hydration and dehydration are plausible, based on rainfall, tides and other processes in the hydrosphere, but going well beyond a range of, say, −20 to 100 °C for aqueous media or assuming that arctic conditions change to volcanic conditions within hours or days seems unreasonable to me. For processes requiring base pairing outside the active site of enzymes, temperatures above 85°C seem unrealistic, based on the known stabilities of duplexes. For base pairing involving single nucleotides, as in genetic copying, we do not see significant template effects above body temperature (37 °C) in primer extension assays, suggesting that these temperatures are too strongly denaturing. Also, how concentrated specific organic compounds may have become in one specific location on the early Earth, based on temperature gradients and diffusion, is unclear. Not every geochemist may agree with specific assumptions, and as a consequence of such issues, scholarly debates over what is prebiotically plausible and what is not, have been controversial.

Plausibility is important. So, perhaps it is time to think about ways out of the "Hand of God" dilemma. Three things come to mind.

First of all, I feel is it reasonable to report the number of manual interventions during an assay explicitly. This number can be quite high, as in the case of enzyme-free replication from activated nucleotides reported by us, where washing and deprotection steps were necessary to be able to measure the level of misincorporation of nucleotides mass spectrometrically. It can also be high for multistep syntheses, mimicking entire biochemical pathways. Understandably so, as self-organizing biochemical cycles are difficult to demonstrate experimentally. Usually, one tries to keep the number of steps in the single digit range. When it becomes unavoidable to intervene as experimentalist, just state the number of discontinuities in the experimental conditions or human interventions!

---

### An event-based architecture for solving constraint satisfaction problems [^117D7bYD]. Nature Communications (2015). Medium credibility.

Constraint satisfaction problems (CSPs) are a fundamental class of problems in computer science with wide applicability in areas such as channel coding, circuit optimizationand scheduling. Algorithms for solving CSPs are typically run on classical von Neumann computing platforms that were not explicitly designed for these types of problems. This paper addresses the question: how can we implement a more efficient computing substrate whose architecture and dynamics better reflect the distributed nature of CSPs?

Many dynamical systems that have been proposed for solving CSPs violate the 'physical implementability' condition. Non-physicality arises from the use of variables that can grow without bounds as the system is searching for solutions. On the other hand, there is a long, well-established tradition of studying physically realizable dynamical systems, for example, in the form of artificial neural networks, to solve CSPs or 'best-match problems'. Early attempts in this field used attractor networks, such as Hopfield networks, to solve NP hard (non-deterministic polynomial-time hard) problems such as the travelling salesman problem. These attractor networks, however, would often get stuck at locally optimal solutions. To overcome this problem, stochastic mechanisms were proposed, which require explicit sources of noise to force the network to continuously explore the solution space. While noise is an inextricable part of any physical system, dynamically controlling its power to balance 'exploratory' versus 'greedy' search, or to move the network from an exploratory phase to a greedy one according to an annealing schedule, is not a trivial operation and puts an additional overhead on the physical implementation.

---

### Association of constipation with risk of end-stage renal disease in patients with chronic kidney disease [^111eMFZZ]. BMC Nephrology (2019). Medium credibility.

Exposures to constipation

We defined constipation group as patients who had at least two claims for constipation (ICD-9-CM 564.0x) or Laxatives prescription (ACT code A06AA, A06AB, A06AC, A06AD, A06AG, A06AH or A06AX) over 60 days apart within one year before their index date. The date of their second visit of a constipation outpatient clinic was the index date. In the cohort, those with constipation were 4035 and without constipation was 18797. The constipation group and non-constipation group, each consisting of 3909 subjects, were formed after matching by propensity score according to age, region, insurance salary, CKD-year, index year, concomitant diseases, and related medications (Fig. 1). Among constipation group, we further divided them into subgroups according to the duration of laxative use. For there were no insights or references about the relationships between the duration of constipation and ESRD, we thus divided the constipation group into 3 equal subgroups by the numbers of the subjects, which may represent mild, moderate, and severe constipation respectively, were done. The cut-off time was the lower third: < 33, the middle third: 33–197 and the upper third: ≥ 198 days per year. Besides, the different numbers of types (less than or equal to one type (≤ 1 type) and more than one type (> 1 type)) of laxatives used were compared too.

---

### Unequal burden of COVID-19 in Hungary: a geographical and socioeconomic analysis of the second wave of the pandemic [^112yCt16]. BMJ Global Health (2021). High credibility.

Methods

Data collection

Data on COVID-19 cases were obtained from the Hungarian Notifiable Disease Surveillance System, operated by the National Public Health Center (NPHC). The national surveillance protocol for COVID-19, which includes case definitions, was updated several times in line with changing European Centre for Disease Prevention and Control and WHO guidance. Under the Hungarian protocol, suspected cases of COVID-19 (who met the clinical and epidemiological criteria or at the discretion of the physician) were reported to NPHC by the healthcare provider. A person with laboratory confirmation (detection of SARS-CoV-2 by PCR and, since 7 November 2020, SARS-CoV-2 antigen detection by a lateral flow test) of COVID-19 infection, irrespective of clinical signs and symptoms, was considered a confirmed case.

The first and last days of the second pandemic wave in Hungary were defined as the days before and after the peak of the second wave when the lowest daily number of cases was recorded. These days were 22 June 2020 and 24 January 2021.

The epidemic curve was constructed using daily confirmed COVID-19 cases reported to NPHC from various data sources. Official COVID-19 data by age, gender and municipality were obtained from NPHC. Morbidity was defined as the incidence of COVID-19 cases confirmed during the second wave.

A brief explanation of the subnational divisions of Hungary is necessary. There are three territories at Nomenclature of National Units for Statistics (NUTS) 1 level, Central Hungary, which includes the capital, Budapest, Transdanubia, in the West, and the Great Plain and North, in the East. Each of these is divided into 2–3 NUTS2 territories (total 8) and these are further divided into 20 NUTS3 territories, comprising the capital and 19 counties. These are further divided, with Budapest comprising 23 districts and the counties, collectively, in 197 districts (the Hungarian names of these two types of districts differ but for the present purposes can be considered the same). The districts in counties, but not in the capital, are further divided into between 60 and 358 municipalities, a total of 3155. In this study, data on the share of Roma in the population were available only at district level, while all other data were available at municipality level (except in Budapest where the lowest administrative level is the district).

---

### The genetic relationship between educational attainment and cognitive performance in major psychiatric disorders [^117TALXB]. Translational Psychiatry (2019). Medium credibility.

Trail-Making-Test (TMT)

The TMT is a measure of visual attention and task switching and is one of several executive functioning measures. The test consists of two parts, part A assesses psychomotor speed of the participant, and part B assesses switching between two automated tasks (counting and reciting the alphabet). The time taken to complete each part of the test was measured and the difference in time needed (part B-part A) was used, as it is considered a more accurate measure of the divided attention and alternating sequencing tasks tested in part B –. In this case, a higher score meant worse cognitive performance.

Verbal digit span

The verbal digit span, from the Wechsler Adult Intelligence Scale, assesses short-term (forward digit-span) and working memory capacity (backward digit-span). Briefly, participants were asked to recall verbally a sequence of digits, with increasingly longer sequences in each trial. For each correctly recalled string of digits, one point was given. The test was ended when the participant was unable to correctly repeat two presented strings of the same length. The difference between the forward and backward task is that the latter involves mental manipulation as the participant is required to repeat the digits in backward order. A score for each task was considered.

Digit-Symbol-Test (DST)

The DST is a subset of the Wechsler Adult Intelligence Scaleand measures processing speed, working memory, visuospatial processing and attention. In this test, the participant was asked to use a key of numbers 1–9 with coinciding symbols to draw the appropriate symbol that matched the number given. The participant was given 120 s to fill in as many corresponding symbols as possible. In the end, the correct number of symbols drawn was totaled to get an overall score.

Verbal Learning and Memory Test (VLMT)

The VLMT is the German version of the Auditory Verbal Learning Test. This word-list learning paradigm assesses several memory parameters through serial list learning with subsequent distraction, retrieval after distraction and half-hour time delay, and through a recognition task. The test consists of two different word lists which are each 15 independent words and a recognition list which includes 30 words from the two lists and 20 similar distractor words. Four VLMT scores were rated, the first for the number of correctly recalled words from the first list, a second score for the number of words lost after distraction, a third score of words lost after a time interval, and a fourth score of correctly recalled words from the recognition list.

---

### National Lipid Association recommendations for patient-centered management of dyslipidemia: part 1 – full report [^112zYqKy]. Journal of Clinical Lipidology (2015). Medium credibility.

Table 1 — classifications of cholesterol and triglyceride levels in mg/dL define thresholds for non–HDL-C, LDL-C, HDL-C, and triglycerides as follows: non–HDL-C desirable < 130, above desirable 130–159, borderline high 160–189, high 190–219, very high ≥ 220; LDL-C desirable < 100, above desirable 100–129, borderline high 130–159, high 160–189, very high ≥ 190; HDL-C is low at < 40 (men) and < 50 (women); triglycerides are normal < 150, borderline high 150–199, high 200–499, and very high ≥ 500. Non–HDL-C equals total cholesterol minus HDL-C, and HDL-C and LDL-C abbreviations are defined on-page.

---

### Defining documentation requirements for coding quality care in workers' compensation [^112prcdu]. Journal of Occupational and Environmental Medicine (2016). Medium credibility.

Workers' compensation documentation — Under current CMS CPT evaluation and management (E&M) rules, level selection relies on defined elements of the encounter, and the history component emphasizes specified items while work-focused details are not weighted. The appropriate level of billing is determined by "four factors" (setting, history, examination, and complexity of medical decision making), and for the history element providers are asked to document items such as "the four to six specific details in the history of present illness". In this framework, "any mention of 'occupational history', whether cursory or thorough, will count toward a 'detailed' history under the CMS coding rules and will merit the same level of billing", leaving key information for claim adjudication and recovery "essentially unrecognized and unrewarded under these current coding rules". If causation details are sought, "Medical providers may also be required to submit follow-up documentation…; usually, there is no additional fee offered for such additional documentation", and quality practice also involves tasks "that currently have no relevant CPT codes at all".

---

### PRISMA 2020 explanation and elaboration: updated guidance and exemplars for reporting systematic reviews [^117ERkng]. BMJ (2021). Excellent credibility.

Box 3
Study selection methods

Several approaches to selecting studies exist. Here we comment on the advantages and disadvantages of each.

Assessment of each record by one reviewer — Single screening is an efficient use of time and resources, but there is a higher risk of missing relevant studies
Assessment of records by more than one reviewer — Double screening can vary from duplicate checking of all records (by two or more reviewers independently) to a second reviewer checking a sample only (for example, a random sample of screened records, or all excluded records). This approach may be more reliable than single screening but at the expense of increased reviewer time, given the time needed to resolve discrepancies
Priority screening to focus early screening effort on most relevant records — Instead of screening records in year, title, author or random order, machine learning is used to identify relevant studies earlier in the screening process than would otherwise be the case. Priority screening is an iterative process in which the machine continually reassesses unscreened records for relevance. This approach can increase review efficiency by enabling the review team to start on subsequent steps of the review while less relevant records are still being screened. Both single and multiple reviewer assessments can be combined with priority screening
Priority screening with the automatic elimination of less relevant records — Once the most relevant records have been identified using priority screening, teams may choose to stop screening based on the assumption that the remaining records are unlikely to be relevant. However, there is a risk of erroneously excluding relevant studies because of uncertainty about when it is safe to stop screening; the balance between efficiency gains and risk tolerance will be review-specific
Machine learning classifiers — Machine learning classifiers are statistical models that use training data to rank records according to their relevance. They can be calibrated to achieve a given level of recall, thus enabling reviewers to implement screening rules, such as eliminating records or replacing double with single screening. Because the performance of classifiers is highly dependent on the data used to build them, classifiers should only be used to classify records for which they are designed
Previous "known" assessments — Screening decisions for records that have already been manually checked can be reused to exclude the same records from being reassessed, provided the eligibility criteria are the same. For example, groups that maintain registers of controlled trials to facilitate systematic reviews can avoid continually rescreening the same records by matching and then including/excluding those records from further consideration.
Crowdsourcing — Crowdsourcing involves recruiting (usually via the internet) a large group of individuals to contribute to a task or project, such as screening records. If crowdsourcing is integrated with other study selection approaches, the specific platforms used should have well established and documented agreement algorithms, and data on crowd accuracy and reliability

---

### Professional use of digital and social media: ACOG committee opinion, number 791 [^115B9RzF]. Obstetrics and Gynecology (2019). High credibility.

ACOG Committee Opinion — online ratings and reviews: It is imperative for physicians to understand and comply with the terms and conditions of the user agreement for these sites because failure to comply with these terms and conditions, specifically regarding solicitation of reviews, can be the source of litigation for any business, including physician group and individual practices. Physicians should be prepared to handle negative online ratings or reviews, and current literature reveals that most online physician ratings are positive; one physician group found that patient satisfaction and recruitment improved as a result of online physician ratings, including negative reviews. Regardless of the content of the review, physicians are best served by monitoring, rather than ignoring, these online ratings; inaccurate information in reviews should be presented to the host site promptly to allow for investigation and removal of inaccuracies, where appropriate, and physicians should abide by the same code of professionalism and conduct that applies to other offline behavior.

---

### Clustering knowledge and dispersing abilities enhances collective problem solving in a network [^1143zBgC]. Nature Communications (2019). High credibility.

Here, however, we find quite different results: intermixing hampers systemic performance. Furthermore, there is a tradeoff in systemic performance over time, as seen in Fig. 6b. Networks with some form of intermixing perform better in the short-run. However, the network with the minimal possible intermixing performs worse in the short-run but best in the long-run. In other words, this suggests an "all or nothing" trade-off over time: the configuration with the least amount of intermixing possible (minimal) performs worse in the short-run, but better in the long-run, while any gradation of increased intermixing yields the same rank-ordering in performance as their counterpart networks in the diversity of ability simulations.

Agents in the intermixed network setups explore for solutions early on in the simulation, which is a double-edged sword. In the short-run, with more intermixing, agents quickly turn to exploitation, as they merely take solutions that are marginally better relative to other setups but are still mediocre in absolute terms. Said differently, agents in setups with at least some intermixing quickly coalesce to whatever the few agents that did explore the problem space found, which are often not the best solutions possible. So, exploring the problem space using worse solutions often leads to modest gains. However, the agents in setups with minimal intermixing are more commonly exploring, rather than exploiting, because their neighbors' initial solutions are less optimal than their counterparts in intermixed networks. This is because there is minimal exposure to diversity in these minimal setups, which inadvertently produces poor solutions in the short-run but allows for more exploration to find better solutions in the long-run. In other words, by finding better solutions through exploration, agents more often uncover pathways to better solutions earlier on. The end result is that the long-run performance of setups with minimal intermixing is best.

---

### Hyperdiploidy: the longest known, most prevalent, and most enigmatic form of acute lymphoblastic leukemia in children [^116KAEvD]. Leukemia (2022). Medium credibility.

Genomic features of HD ALL

Chromosome copy number abnormalities

Childhood ALL cases with gross ploidy changes are formally based on their DNA content and the overall number of chromosomes in their karyotypes. They comprise seven biologically related yet distinct categories whose common feature is a ploidy-related overrepresentation of chromosome 21, respectively (Fig. 1A). Although their categorization is primarily based on karyotype patterns, it takes increasingly also selected genomic features into account. Some of these new parameters are largely corroborating, whereas others uncover glitches and discrepancies in the current classification system, insights that justify a careful modification of the taxonomy as well as the terminology of these diseases that should take these novel findings into account. Because the ploidy-related overrepresentation of chromosomes 21 serves as their essential and overarching hallmark, one can use the definitions of the "International System of Cytogenetic Nomenclature", namely hyperhaploid (25–34 chromosomes), hypodiploid (35–45 chromosomes), hyperdiploid (47–58 chromosomes), hypotriploid (59–68 chromosomes) and hypertriploid (70–80 chromosomes) for their further subclassification without obscuring any otherwise pertinent information. Just for convenience's sake and practical reasons, the hypo- and hypertriploid may still be merged into a "near-triploid" group (59–80 chromosomes). In our review we will avoid the term "duplicated or doubled up haploids" because it insinuates a presumed but never proven mode that is supposed to generate this specific karyotype pattern. Since the more appropriate yet still not entirely correct descriptive term "hyperdiploidy due to a genome-wide loss of heterozygosity" would be rather impracticable to use, we opted to introduce the neutral terms "classical" and "nonclassical" for the two dissimilar but nevertheless closely related forms of genuine hyperdiploid forms that are the specific focus of our review. Both these types are defined by an agreed-upon chromosome number that lies on the lower side between 50 and 52 and on the upper side between 58 and 67, cutoffs that vary slightly in different studies (Fig. 1B, C). However, given that more than 80% of HD cases fall into the range of 52–58 with a modal peak of 55–56, we propose that an upper limit of 58 would be more appropriate, especially because such a cutoff eases the delineation of genuine HD forms from near-triploid ones (Fig. 1A).

---

### Programmed cell death ligand 1 expression on monocytes is inversely correlated with tumour response to preoperative chemoradiotherapy for locally advanced rectal cancer [^115Vj6dr]. Colorectal Disease (2022). Medium credibility.

Leucocyte subpopulations in circulating blood before and after chemoradiotherapy

The numbers of white blood cells and their subpopulations in peripheral blood were investigated from the medical records and flowcytometric analysis. As shown in Figure 1, the numbers of total leucocytes as well as neutrophils and lymphocytes were significantly reduced after CRT. The reduction rates were more prominent for lymphocytes than neutrophils (neutrophils; 4072 ± 197 to 3207 ± 197, p = 0.0027, lymphocytes; 1536 ± 69.2 to 791 ± 70.0, p < 0.0001). In contrast, the number of circulating monocytes was not significantly decreased after CRT (Figure 1C). As a result, the neutrophil‐lymphocyte and lymphocyte‐monocyte ratios were increased and decreased after CRT, respectively (Figure S1). Among the lymphocyte subpopulations, the reduction rate was most remarkable in CD4(+) T cells (511 ± 30.2 to 199 ± 30.9, p < 0.0001), while the number of CD3(−) CD16(+) NK cells was not changed by CRT (Figures 1E–H). As shown in Figure 2A, circulating monocytes were phenotypically divided into three subtypes, CD14 high (+) CD16(−) classical, CD14 med (+) CD16(+) intermediate and CD14 dim (+)CD16(+) nonclassical (patrolling) monocytes. Although the numbers of classical and intermediate monocytes were not changed, the numbers of patrolling monocytes were increased with marginal significance (61 ± 5.4 to74 ± 5.4, p = 0.050).

FIGURE 1
The number of peripheral white blood cells (A), neutrophils (B), monocytes (C), lymphocytes (D) and lymphocyte subsets (E–H) obtained before and after chemoradiotherapy (CRT). ✱✱ p < 0.001, ✱✱ p < 0.0001 by student t ‐test

FIGURE 2
Circulating monocytes were divided into three subsets by these gating strategy (A). The number of classical (B), intermediate (C) and patrolling (D) monocytes in peripheral blood obtained before and after chemoradiotherapy (CRT). ✱ p < 0.05 by Student's t ‐test

---

### Technical update on HIV-1 / 2 differentiation assays [^1134y6wL]. CDC (2016). Medium credibility.

HIV-1/HIV-2 differentiation testing — alternatives when the FDA-approved antibody differentiation immunoassay cannot be used states that there may be circumstances under which a laboratory is unable to adopt the FDA-approved HIV-1/HIV-2 antibody differentiation immunoassay, and in this situation a laboratory has alternatives for that step in the algorithm, some of which could delay turnaround time for test results. Send specimens to another laboratory that offers the FDA-approved supplemental HIV antibody differentiation assay. Refer to CDC/APHL laboratory testing guidance section I, "Alternative Testing Sequences When Tests in the Recommended Algorithm Cannot be Used".

---

### A chronological map of 308 physical and mental health conditions from 4 million individuals in the english national health service [^116oRfSd]. The Lancet: Digital Health (2019). High credibility.

Introduction

A chronological map of human health from birth to death depicting the most common conditions by age and marking the median age at diagnosis is fundamental to understanding who gets which conditions when, on a population level. This understanding can inform clinicians about the frequency, and hence the prior probability, of a range of conditions on the basis of the age of presentation. This knowledge could also allow policy makers to consider common conditions at different ages or in different groups when allocating training and resources, and researchers and their funders to prioritise prevalent conditions.

Research in context

Evidence before this study

We did two English language searches in MEDLINE for studies published in the past 10 years: one search for studies describing disease prevalence for multiple diseases, using the keywords "diseases" OR "disorders", "epidemiology" OR "prevalence", AND "comorbidity" OR "multimorbidity"; and the other for studies reporting mean or median age of disease onset or diagnosis using the keywords "diseases" OR "disorders", AND "age of onset" OR "age at diagnosis" OR "age of diagnosis", AND "mean" OR "median".

Consented cohort studies investigating multiple health conditions were limited in age range and in ascertainment of conditions diagnosed in primary care. Many had too few participants to reliably estimate disease distribution by age, sex, and ethnicity. Studies based on electronic health records (EHRs) surmounted these limitations, but the manual curation required for developing case definitions and phenotype algorithms from EHR data restricted the number of conditions analysed within a single study to fewer than 100. Many studies reported prevalence estimates for comorbid conditions relative to an index disease, such as heart failure. Some were confined to either primary or secondary care. The Global Burden of Disease initiative inferred disease prevalence estimates from mathematical models based on empirical frequency data. The US National Cancer Institute's Surveillance, Epidemiology, and End Results cancer statistics review reported age at diagnosis by sex and ethnicity for primary cancer sites. Most other studies reported age at diagnosis for a single disease from small, sometimes unrepresentative sample sets. We did not find any studies that described the age distribution and age at diagnosis stratified by sex and ethnicity from birth to death for several hundred diseases contemporaneously with a single linked clinical dataset obtained from primary-care and secondary-care settings within a universal health-care system.

---

### Digitally generated trail making test data: analysis using hidden Markov modeling [^116fKbwu]. Alzheimer's & Dementia (2022). Medium credibility.

Our findings of associations between digital metrics and neuropsychological test performance are also in line with those from versions of the TMT administered on a tablet. Fellows et al.found that digital metrics from the TMT‐A were associated with performance on the Symbol Digital Modalities Test, a processing speed test similar to the DSST which in our study was associated with several TMT metrics including drawing time and number/length of HMM segments on both conditions, as well as thinking time on TMT‐B. Fellows et al.also found that digital metrics on TMT‐B were associated with tests of executive function including inhibitory control and visual working memory, a finding supported in our study wherein TMT‐B thinking time was associated with number span backward. Despite broad similarities in results across studies, there are some differences in findings in our study (e.g. the association of processing speed with TMT‐B metrics), likely reflecting differences in the algorithms used to create each digital metric and the cognitive processes that each metric captures (ie. pen lift duration vs. thinking time). Further research is needed to identify the underlying cognitive process(es) associated with each metric and to determine which algorithms are the most reliable. Additionally, there may be other digital metrics beyond those in the existing studies that relate to cognitive and personality outcome measures, such as thinking time after an error as an indication of ability to incorporate feedback and reestablish set, or changes in performance as the number of processed targets increases, which may relate to visual scanning abilities and susceptibility to interfering targets.

There may be clinical utility to the digital metrics derived in this study. The Boston Process Approach to neuropsychological assessment stresses the decomposition of cognitive test performance into its multifactorial components to better understand the underlying cognitive constructs and improve clinical decision making. In an effort to isolate the executive component of performance on TMT‐B from the visual scanning and graphomotor speed components, clinicians often subtract completion time on TMT‐A from completion time on TMT‐B. However, there are some biases in this method as the spatial array of dots on each test are not directly comparable; connections on TMT‐B are longer and have more interfering stimuli than TMT‐A. In contrast, the digital metrics allow us to separate aspects of cognitive processing and motor function within each task, thereby reducing the effects of variations in the task stimuli and potential differences in testing conditions and test engagement between TMT‐A and TMT‐B, and offering the potential to eliminate administration of TMT‐A.

---

### Outcomes of treated hypertension at age 80 and older: cohort analysis of 79, 376 individuals [^112uqv4M]. Journal of the American Geriatrics Society (2017). Low credibility.

Results

Data were from 79,376 individuals aged 80 and older who met inclusion criteria for the analysis. Mean age was 82.1 ± 3.3, and 30.5% were men (Table 1). The average number of BP measurements according to SBP category varied from 7.2 for SBP less than 125 mmHg to 13.4 for SBP of 165 to 174 mmHg (Table S3). There were trends in covariates across SBP (e.g. current or recent smoking 24% at SBP < 125 mmHg and 18% at SBP > 185 mmHg) (Table 1). Maximum follow‐up was 11.9 years (overall mean 4.4 ± 2.9 years), with mean follow‐up times somewhat shorter in the low SBP groups.

Table 1
Cohort Characteristics and Frequency of Outcomes According to Achieved Median Systolic Blood Pressure

---

### Indications and management of preimplantation genetic testing for monogenic conditions: a committee opinion [^114v5AZ5]. Fertility and Sterility (2023). High credibility.

Pretest counseling for PGT-M — In vitro fertilization providers should ensure that their patients have a thorough understanding of the PGT-M indication and process and set expectations accordingly, and the following aspects of PGT-M should be addressed with patients before beginning the process: the natural history of the condition and any known genotype/phenotype associations for the specific genetic variants of interest; the timeline for test development and need to postpone an IVF cycle until it has been completed or the risks of proceeding with an IVF cycle before completion; the potential need to involve relatives, to varying degrees, in the test development process and recognition that this may conflict with the patient or couple's preferences; the possibility of detecting an incidental finding during test development (which may or may not prevent successful PGT-M testing), such as a copy number variant, a discrepant finding, or uncovering nonparentage, consanguinity or other misattributed biologic relationships; the unlikely but possible scenario that PGT-M test development cannot be completed despite initial acceptance by the laboratory; the types of results that may be reported and what information can and cannot be conveyed by a positive or negative result; and the potential for inconclusive or reduced accuracy results.

---

### Practice bulletin no. 162: prenatal diagnostic testing for genetic disorders [^116ifKb3]. Obstetrics and Gynecology (2016). Medium credibility.

Previous fetus or child with autosomal trisomy or sex chromosome aneuploidy — the recurrence risk after one affected pregnancy is 1.6–8.2 times the maternal age risk of autosomal trisomies; risk for a second autosomal trisomy can pertain to any chromosome, is elevated for 47, XXX and 47, XXY, and does not appear to be increased for 45, X or 47, XYY.

---

### Natural language generation for electronic health records [^115bHVg7]. NPJ Digital Medicine (2018). Low credibility.

Table 4
Discrete variables in our dataset

The first column shows the variable names, the second column a description of their unique original values, and the third column a bracketed set indicating the range of those values after being recoded during preprocessing

To preprocess the chief complaints, we begin by removing records containing words with a frequency of less than 10 (n = 240,343); this yields a vocabulary V of 23,091, and is primarily a way to reduce the size of the word embedding matrix in our Long Short Term Memory (LSTM) network, reducing the model's computational complexity, but is also serves to remove infrequent abbreviations and misspellings. Similarly, we also remove records with chief complaints longer than 18 words (n = 490,958), the 95th percentile for the corpus in terms of length, which prevents the LSTM from having to keep track of especially long-range dependencies in the training sequences. Following these 2 steps, we convert all the text to lowercase, and we append special start-of-sequence and end-of-sequence strings to each chief complaint to facilitate text generation during inference. Finally, we vectorize the text, convert each sentence to a sequence of vocabulary indices, and pad sequences shorter than 18 words with zeros (these are masked by the embedding layer of our LSTM).

After preprocessing, we are left with ~4.8 million record-sentence pairs, where each record is a 403-dimensional binary vector R, and each chief complaint is a 18-dimensional integer vector S. Using a random 75–25 split, we divide these into 3.6 million training pairs and 1.2 million validation pairs. To reduce the time needed to evaluate different sampling procedures during inference, we save only 50,000 of the validation pairs for final testing.

Modeling

---

### Cutting edge or blunt instrument: how to decide if a stepped wedge design is right for you [^112pSeFT]. BMJ Quality & Safety (2021). High credibility.

When might I consider doing a stepped wedge trial?

Research designs are shaped as much by practical constraints as by abstract schemes, and it is always a good idea to start with the constraints and work towards a design, rather than start with a design and try to fit it to constraints. These constraints will be unique to each research context, and box 1 lists some areas to think about. Still, there are some common features of settings where a stepped wedge trial might be considered as a possible design, and we now review these.

Box 1
Practical constraints on the design of a longitudinal cluster randomised trial

Are there limits on the time available to complete the evaluation, on the number of clusters, or on the number of participants (or the rate at which you can recruit participants) at each cluster? These constraints put limits on the overall scale of the evaluation, or force trade-offs between different design characteristics.
How will participants and their data be sampled in your study: as a series of cross-sectional surveys, as a continuous stream of incident cases, as a cohort followed over time, or some other way? Does the timescale divide into cycles, seasons or milestones that influence how you will sample participants and data?
Is there a limit on how many clusters can implement the intervention at the same time in the evaluation? If this is constrained by research resources (eg, if there are only enough trained research staff to implement the intervention one cluster at a time) then implementation must be staggered in some way.
If implementation is to be staggered, is there a minimum 'step length'? If the same team delivers the intervention in different clusters at different steps, then bear in mind it may take some time to get the intervention fully operational at a site, and the team will also need time to relocate from one cluster to the next.

---

### Assessing the reporting quality of early phase dose-finding trial protocols: a methodological review [^116WdkFK]. EClinicalMedicine (2023). Medium credibility.

Methods

Study design

This methodological cross-sectional study evaluated the reporting quality of early phase dose-finding protocols. The study was registered in the international prospective register of systematic reviews PROSPERO (registration no: CRD42022314572).

Included protocols

To be included in this study, protocols had to meet the following criteria: 1) phase I or phase I/II trials that include a dose (de-)escalation component, 2) treated living humans, 3) had their full protocols in English, and 4) posted between 01/01/2017 and 08/02/2023 in. No restrictions were applied to limit types of intervention or setting. To identify the potential full protocols, an electronic systematic search was performed via. Details of the systematic search can be found in Fig. 1 and Supplementary materials. Examination of potentially eligible protocols was performed independently by two authors (GV and DP) based on titles and a first screen of the protocol; potential disagreements were resolved by a third author (CY). A random sample of 80% of all eligible trial protocols were selected for complete revision and 106 full protocols were finally included in the study.

Fig. 1
Flow diagram with the identification of early phase dose-finding protocols via.

Variables

A standardised Excel spreadsheet was created to extract data from the included protocols. The following variables were collected for all protocols if available:identifier, study title, first author (or principal investigator), study sample size (of the dose-escalation component only), area of research, protocol year, research funding, multi-centre or single-centre study and type of the dose-escalation design.

Additionally, to evaluate the reporting quality of the protocols, a checklist of items was created as a combination of 1) original items from SPIRIT 2013 and modified items tailored to dose-finding trials and 2) additional items unique to dose-finding trials considered useful by the review team. The complete list of all 68 items can be found in Supplementary Table S1. The selected items were evaluated in all included studies. Each item was assessed as " Yes " or " No ". For some items, there was the option to select " Not applicable " and these items were not evaluated. For composite items, the option " Partially " was also available (examples of partially reported items can be found in the Supplementary materials).

---

### Relation of completeness of reporting of health research to journals' endorsement of reporting guidelines: systematic review [^115wGFXG]. BMJ (2014). Excellent credibility.

Identifying evaluations of reporting guidelines

Many developers of reporting guidelines have devised acronyms for their guidelines for simplicity of naming (for example, CONSORT, PRISMA, STARD). Some acronyms, however, refer to words with other meanings (for example, STROBE). For this reason, we used a dual approach to searching for evaluations of relevant reporting guidelines.

We searched for reporting guidelines with unique acronyms cited in bibliographic records in Ovid Medline (1990 to October 2011), Embase (1990 to 2011 week 41), and the Cochrane Methodology Register (2011, issue 4); we searched Scopus (October 2011) for evaluations of all other guidelines (that is, ones with alternate meanings or without an acronym). We did addendum searches in January 2012. Details are provided in appendix 3. In addition, we contacted the corresponding authors of reporting guidelines, scanned bibliographies of related systematic reviews, and consulted with members of our research team for other potential evaluations.

We included English or French language evaluations if they assessed the completeness of reporting as a primary intent and included studies enabling the comparisons of interest (after versus before journal endorsement and/or endorsing versus non-endorsing journals). Choice of language for inclusion was based on expertise within our research team; owing to budget constraints, we could not seek translations of potential evaluations in other languages.

After removing any duplicate results from the search yield, we uploaded records to Distiller SR. We first screened records by title and abstract (one person to include, two people to exclude a record) and then in two rounds for the full reports (two reviewers, independently) owing to the complexity of assessing screening criteria and using a team of reviewers. Disagreements were resolved by consensus or a third person. Where needed, we contacted authors of evaluations (n = 66) or journal editors (n = 48) for additional information. One person (from among a smaller working group of the team) processed evaluations with responses to queries to authors and journal editors and collated multiple reports for evaluations.

We first assessed each published study from within an included evaluation according to the journal in which it was published (fig 1). We collected information on endorsement from evaluations or journal websites. If the journal's "Instruction to authors" section (or similar) specifically listed the guideline, we considered the journal to be an "endorser".

Fig 1 Schematic depicting relation among evaluation of reporting guideline, studies contained within it, and determination of comparison groups according to journal endorsement status

---

### Identifying clusters of people with multiple long-term conditions using large language models: a population-based study [^113K4VEe]. NPJ Digital Medicine (2025). Medium credibility.

Longitudinal patient sequences of electronic health records

The study outline for generating a patient embedding, which is the high-dimensional vector that represents a patient's medical history and MLTC patterns, is shown in Fig. 1. For each patient, a chronological sequence of their medical history was constructed using clinical diagnoses, symptoms, medication prescriptions and laboratory test results. We utilised a total of 3776 different diagnostic codes (ICD-10 codes and medication groups such as ACE inhibitors) to create the patient sequences. With each diagnosis or medication event, we pair the event with additional patient information including the age of the patient when the event occurs, the calendar year the event occurs, the gender of the patient and a visitation number, which is a counter for the number of unique healthcare utilisations each individual has recorded in the chronological sequence based on pooled 1-month period intervals.

Clinical diagnoses included all conditions encoded at the highest level of the ICD-10 hierarchy across all diseases or health conditions chapters (Chapters A to S), and all SNOMED-CT codes which mapped to an ICD-10 code within these chapters. Primary care records of interest were mapped to ICD-10 codes prior to inclusion chronologically in each patient sequence and records with no mapping to an ICD-10 code were excluded. ICD-10 codes that occurred in less than 100 individuals within the entire study were excluded to reduce vocabulary size. Symptoms and laboratory tests were included chronologically using ICD-10 Section R and a manually curated list of SNOMED-CT codes which encode indicators of abnormal test results. Products within the prescription records were first mapped to medication groups based on the primary function of the product (Supplementary Table 3) and subsequently included chronologically in each patient sequence. To reduce the chance of continuous prescriptions dominating each patient sequence, only the first instance of mapped medications prescribed monthly were included in the chronological sequences with a 6-month gap in prescription required before the re-addition of the medication code to the sequence. For mapped medications not prescribed monthly, all prescription events were included chronologically in each patient sequence.

---

### Clinical profile of acute myocardial infarction patients included in the hospital readmissions reduction program [^114v1EKe]. Journal of the American Heart Association (2018). Low credibility.

Results

Of 200 patients, 3 were excluded (1.5%) as 1 was readmitted to the emergency department observation unit rather than inpatient care, 1 was unable to be matched to a medical record number, and 1 did not meet criteria for AMI during the index hospitalization. The 197 patients remaining were included in the analytic cohort. The average age was 77.7 years (± 7.3), 40.6% (80/197) of the study population were female, and 90.4% (178/197) were white (Table). On average, patients had smoked 37 (± 23) pack years, had an average low‐density lipoprotein cholesterol of 78.2 (± 34.5) mg/dL, average creatinine of 1.30 (± 0.62) mg/dL, and average hemoglobin A1C of 6.4% (± 1.19). On index admission, 82.7% (163/197) had no restrictions on goals of care at beginning of index hospitalization (Table). Of all included patients, 148/197 (75.1%) had Type I NSTEMIs, 29/197 patients (14.7%) had STEMIs, and 20/197 (10.2%) had Type II NSTEMIs. The median length of stay for this population was 7.0 days on index hospitalization (interquartile range, 4–11 days) and the mean Charlson Comorbidity Index was 5.3 ± 3.1. Demographic and clinical characteristics of these patients, divided by type of AMI, are described in Table.

Table 1
Demographics and Clinical Characteristics of Patients Readmitted to Index Hospital Within 30 Days Post AMI

---

### Standards of care in diabetes – 2025 [^113RQNuE]. Diabetes Care (2025). High credibility.

Regarding follow-up and surveillance for diabetes mellitus type 1, more specifically with respect to continuous glucose monitoring, ADA 2025 guidelines recommend to implement early continuous glucose monitoring in adult patients with T1DM to improve glycemic outcomes and QoL and minimize hypoglycemia.

---

### Defining documentation requirements for coding quality care in workers' compensation [^117QaPH4]. Journal of Occupational and Environmental Medicine (2016). Medium credibility.

An improved system for better outcomes and lower costs — workers' compensation documentation needs include knowledge of workplace factors, focus on minimizing avoidable work disability, analysis of activity tolerance and functionality, evidence-based medicine considerations in diagnosis, treatment, and disability determinations with some jurisdictions requiring such reference, reports completed for regulatory reporting requirements, and specialized OEM knowledge for specific cases (for example, causation analysis, analysis of activities and functionality, ergonomics, as well as more arena areas such as toxicology and epidemiology). Other factors counted in current payment systems are not relevant and could be eliminated; in particular, history and physical examination of irrelevant body parts and systems should not be encouraged, health status of near relatives is often not relevant, and inquiries about hereditary diseases may be prohibited by law. Injured workers themselves are likely to benefit from a coding system that encourages functional assessments and attention to other indicators of delayed recovery.

---

### Summary benchmarks-full set – 2024 [^116Qm9yC]. AAO (2024). High credibility.

Amblyopia — initial exam history key elements include demographic data (sex, date of birth, and identity of parent/caregiver), the identity of the historian and relationship to the patient and any language barriers, the identity of health care providers involved in the child's care, the chief complaint and reason for the eye evaluation, current eye problems, ocular history including prior eye problems, diseases, diagnoses, and treatments, systemic history including birth weight, gestational age, prenatal and perinatal history with examples such as alcohol, tobacco, and drug use during pregnancy plus past hospitalizations and operations and general health and development including presence of developmental delay, current medications and allergies, and family history of ocular conditions and relevant systemic diseases.

---

### The impact of alternative arrangements, contingent jobs, and work secured through an app on the well-being of working age adults: results from the California work and health survey [^112NgXWo]. American Journal of Industrial Medicine (2024). Medium credibility.

Background

There is recognition of the growing prevalence of alternative work arrangements, contingent jobs, and work secured through an app. However, there have been few systematic efforts to understand the impact of these forms of work on individuals and households.

Methods

The data derive from the California Work and Health Survey administered to a sample of the working age population of the state solicited through random-digit dialing of cell phone numbers. 4014 individuals completed the survey, 26% of those with an in-service cell phone number. We present odds ratios and 95% confidence intervals from logistic regression estimating the impact of being an independent contractor, in other forms of alternative work arrangements, in contingent jobs, and in work secured through an app, on economic and health status and working conditions in main jobs, with and without adjustment for covariates.

Results

Several of the forms of work analyzed are associated with lower earnings and higher rates of wage theft, household poverty, benefit recipiency, and expectation of hardships in food, housing, and medical care in the immediate future. Association between the forms of work and current health status is less consistent. However, several forms of work are associated with working conditions known to be risk factors for subsequent health problems.

Conclusions

Public policy to mitigate the adverse impacts of work, largely developed in the 20th Century when there was an identified workplace, may be insufficient to protect workers' well-being for alternative work arrangements, contingent jobs, and work secured through an app.

---

### Practice bulletin no. 162: prenatal diagnostic testing for genetic disorders [^111u6Hqg]. Obstetrics and Gynecology (2016). Medium credibility.

Parental aneuploidy or aneuploidy mosaicism — women with trisomy 21 have an increased risk of having offspring with a trisomy, whereas women with 47, XXX and men with 47, XYY are usually fertile and are not known to have a discernible increased risk; limited data on men with Klinefelter syndrome (47, XXY) using intracytoplasmic sperm injection do not indicate increased offspring aneuploidy.

---

### Summary benchmarks-full set – 2024 [^113q9NSQ]. AAO (2024). High credibility.

Exotropia — initial exam history key elements emphasize collecting demographic data; documenting the identity/relationship of the historian and other care providers; detailing the chief complaint with onset, frequency, direction/eye of deviation, and associated symptoms; and reviewing photographs/videos when helpful, plus ocular, systemic, family, and social history relevant to strabismus and amblyopia.

---

### Clinical consensus methodology [^114rMeFM]. ACOG (2021). High credibility.

ACOG Clinical Consensus — committee voting and approval procedures specify that after a comprehensive discussion of each recommendation and supporting evidence, a formal committee vote is held for approval of each recommendation statement, with a quorum of two thirds of eligible committee members needed to hold a vote. Eligible voting members include full committee members, young physician members, public members, and ex officio members, while liaisons are excluded from voting, and any member with a conflict of interest that prevents them from voting on the topic will be subtracted from the total number of eligible voting participants. For passage, a 75% approval threshold is needed to move a final recommendation forward, proposed recommendations not meeting this threshold can be revised and reconsidered, and proposed recommendations that do not meet the threshold after a second vote are not incorporated into the Clinical Consensus.

---

### Acute lymphoblastic leukemia, version 2.2024, NCCN clinical practice guidelines in oncology [^115bvUc9]. Journal of the National Comprehensive Cancer Network (2024). High credibility.

Regarding diagnostic investigations for acute lymphoblastic leukemia - NCCN, more specifically with respect to molecular testing, NCCN 2024 guidelines recommend to recognize that there are other results that are not < 44 chromosomes that may be equivalent to hypodiploidy and have the same implications. It is important to distinguish true hypodiploidy from masked hypodiploidy, which results from the doubling of hypodiploid clones.

---

### Clustering knowledge and dispersing abilities enhances collective problem solving in a network [^1155DKsT]. Nature Communications (2019). High credibility.

The severity of this problem's complexity is not additive or linear in nature, but interdependent on all of the sub-problems that need to be solved for this to work. Furthermore, adopting a solution might have unintended, if not non-linear, consequences to the system's overall outcome that were not obvious or foreseeable. For instance, allowing students to use their mobile phones in schools might seem like a good idea to improve class engagement through other media, but an unintended consequence might be distraction or discouraging deeper critical thinking. The objective of the NK space is to capture the complex interaction among activities that yield performance. The NK problem space's popularity in modeling human decision-making stems from its verisimilitude with the complex and multidimensional problems that face problem-solving tasks, and because researchers can easily generate a large number of statistically similar problem spaces for robustness checks. (We note, however, in order to make sure our results are not an artifact of the idiosyncrasies of the NK problem space, we replicated all of our results in another rugged problem space, the Traveling Salesperson Problem — findings available in Supplemental Methods in Supplementary Fig. 2 through 9 and 11).

---

### Contribution of health care factors to the burden of skin disease in the United States [^1175VwQ8]. Journal of the American Academy of Dermatology (2017). Medium credibility.

Supplemental Table I — HCPCS E and M visit codes are organized with "Code description" and "Family description" columns; the code description reads "Refer to an AMA-licensed code list", and family categories include "Consults", "ER visits and observation care", "IP visits - critical care services", "IP visits - nursing facility services", and "Office/home visits" subtypes "rest home", "care plan oversight services", and "home visits".

---

### Observing the overall rocking motion of a protein in a crystal [^114tg8F6]. Nature Communications (2015). Medium credibility.

The same approach was employed to record the cubic-PEG-ub trajectory. In this case the initial coordinates were derived from the crystallographic structure 3N30. The periodic boundary box was modelled after a single crystal unit cell, containing 48 ubiquitin molecules (equally divided between chains A and B) and 23,419 water molecules. The net length of the trajectory was 1 μs. The volume of the simulation box remains stable throughout the simulation within 0.7% of its target value (on average, there is a slight uniform contraction as described by linear factor 0.9986). Note that the statistical sampling for both chain A and chain B is the same as for the single ubiquitin chain in the MPD-ub trajectory. Finally, the rod-PEG-ub trajectory was designed based on the crystallographic coordinates 3EHV. The periodic boundary box was defined as a block of two crystal unit cells, containing 24 ubiquitin molecules (equally divided between chains A, B and C, which comprise the asymmetric unit) and 6,198 water molecules, for the total of 48,234 atoms.

---

### Investigating suspected cancer clusters and responding to community concerns: guidelines from CDC and the council of state and territorial epidemiologists [^114nvBwd]. MMWR: Recommendations and Reports (2013). Medium credibility.

Cancer cluster investigation — Step 2 decision to close or proceed to Step 3 is based on multiple factors, and to interpret the standardized incidence ratio (SIR) the health agency must assess whether there are enough cases and population for statistical stability and, in general, that the population size of a typical census tract is the smallest denominator that will allow reliable results; if the numerator allows stability, agencies should consider whether the 95% CI exclude 1.0, along with environmental contaminants, population changes, and other information beyond the SIR to estimate whether cancers represent an excess and share a common etiology. A SIR of limited magnitude that is not statistically significant with lack of known contaminant association and no increasing trend justifies closing at Step 2, whereas a statistically significant SIR of great magnitude with an increasing trend and a known contaminant argues for continuing to Step 3; example thresholds indicate that an SIR of < 2.0 with CIs surrounding or overlapping 1.0 and < 10 cases might justify closing, while an SIR of > 4 with CIs not overlapping 1.0 and ≥ 10 etiologically linked cases should encourage advancing to Step 3.

---

### How deep learning solved my seizure detection problems [^112neYgf]. Epilepsy Currents (2020). Medium credibility.

[Box: see text].

---

### Rationale and design of the phase 3a development programme (ONWARDS 1–6 trials) investigating once-weekly insulin icodec in diabetes [^1122vnRB]. Diabetes, Obesity & Metabolism (2023). Medium credibility.

2.2 Trials in insulin‐naive people with

For ONWARDS 1, 3 and 5, only individuals who are insulin‐naive are eligible for inclusion (Table 1). These trials represent individuals with T2D who have inadequate glycaemic control with their current non‐insulin glucose‐lowering agents (including glucagon‐like peptide‐1 receptor agonists) and are therefore eligible to begin insulin.

2.2.1 ONWARDS 1

ONWARDS 1 (NCT04460885) is a treat‐to‐target trial, in which participants are randomized to treatment with once‐weekly icodec or once‐daily glargine U100, both in combination with non‐insulin glucose‐lowering drugs. Participants may continue such pretrial non‐insulin background drugs, except for sulphonylureas and glinides, which are discontinued at randomization (Table 1).

With 78 weeks of randomized treatment, ONWARDS 1 will have the longest randomized treatment period of all the ONWARDS trials. The effect of icodec will be evaluated after a 52‐week main phase followed by a 26‐week extension phase to evaluate efficacy and long‐term safety (Figure 1A).

2.2.2 ONWARDS 3

In the ONWARDS 3 (NCT04795531) double‐blinded, double‐dummy, treat‐to‐target trial, participants are randomized to receive once‐weekly icodec or once‐daily insulin degludec (degludec), both in combination with non‐insulin glucose‐lowering agents (with appropriate placebos administered in both treatment arms) for 26 weeks. Participants can continue treatment with these pretrial medications; however, sulphonylureas or glinides will be administered at a reduced (50%) dose at the discretion of the investigator (Table 1; Figure 1B).

---

### September 19, 1966 issue of JAMA… [^114kQwMV]. JAMA Network (2025). Excellent credibility.

195: 9–984 March 21 195: 9–1088 March 28 195: 9–1168 April 4 196: 1–328 April 11 196: 9–270 April 18. 196: 9–750 May 30 196: 9–814 June 6 196: 9–926 June 13 196: 9–1038 June 20 196: 9–1106 June
27. 196: 9–1170 July 4 197: 1–206 July 11 197: 9–276 July 18 197: 9–236 July 25 197: 9–308 August 1. 197: 9–384 August 8 197: 9–524 August 15 197: 9–604 August 22 197: 9–674 August 29 197: 9–744 September
5. 197: 9–834 September 12 197: 9–942 September 19 197: 9–1058 September 26 197: 9–1120 October 3 198: 1–302 October 10. 198: 9–314 October 17 198: 9–342 October 24 198: 9–498 October 31 198: 9–574 November 7 198: 9–692 November
14. 198: 9–804 November 21 198: 9–964 November 28 198: 9–1046 December 5 198: 9–1142 December 12 198: 9–1236 December 19. 198: 9–1324 December 26 198: 9–1384 September 19, 1966 Vol 197, No. 12, Pages 9.

---

### 2013 ACC / AHA guideline on the assessment of cardiovascular risk: a report of the American college of cardiology / American Heart Association task force on practice guidelines [^116Lg68Y]. Journal of the American College of Cardiology (2014). Medium credibility.

Table 5 — Distribution of estimated 10-year risk of a first hard atherosclerotic cardiovascular disease (ASCVD) event in the CVD-free, nonpregnant US population aged 40 to 79 years is organized by predicted 10-year risk categories < 2.5%, 2.5%–4.9%, 5.0%–7.4%, 7.5%–9.9%, 10.0%–14.9%, 15.0%–19.9%, and ≥ 20.0%. In the overall population, the percentages (95% CI) are 33.4 (31.2–35.5), 21.0 (19.4–22.7), 12.7 (11.4–14.0), 7.4 (6.5–8.3), 8.9 (8.1–9.6), 6.3 (5.6–7.1), and 10.2 (9.5–11.0), with corresponding n of 33 534 000, 21 151 000, 12 766 000, 7 470 000, 8 940 000, 6 380 000, and 10 300 000. Among men, the percentages (95% CI) are 17.4 (15.2–19.7), 22.7 (20.3–25.1), 15.6 (13.8–17.4), 10.1 (8.5–11.6), 12.1 (10.7–13.5), 8.8 (7.4–10.2), and 13.3 (12.1–14.4), with n of 8 386 000, 10 950 000, 7 511 000, 4 847 000, 5 849 000, 4 248 000, and 6 388 000. Among women, the percentages (95% CI) are 48.0 (44.8–51.3), 19.5 (17.3–21.6), 10.0 (8.3–11.8), 5.0 (3.8–6.2), 5.9 (5.1–6.7), 4.1 (3.4–4.7), and 7.5 (6.5–8.4), with n of 25 148 000, 10 200 000, 5 256 000, 2 622 000, 3 091 000, 2 131 000, and 3 912 000. These estimates were derived by applying the Pooled Cohort Equations to NHANES 2007–2010 with N = 5367 and weighting to 100 542 000 US population.

---

### Teasing out missing reactions in genome-scale metabolic networks through hypergraph learning [^111uFJXT]. Nature Communications (2023). High credibility.

Discussion

Optimization-based GEM gap-filling has been long considered as a process of fitting a GEM to observed data. This problem is typically formulated by a mixed-integer linear programming that minimizes the number of added reactions under the constraint that the observed phenotypes are satisfied. Therefore, the majority of GEM gap-filling methods falls short of predicting metabolic gaps in both network connections and functions without knowing experimental phenotypes a priori. FastGapFill, as one of a few exceptions, fits a specific task of gap-filling to resolve dead-ends and blocked reactions. Previous studies – have shown that FastGapFill exhibits a poor performance in filling artificially introduced gaps. Although gap-filling with experimental data is critically important, it is limited to understanding the gene-reaction-phenotype mappings in conditions where the data was collected. The environmental conditions are combinatorially complex; as a theoretical tool, the primary purpose of GEMs is to rapidly offer theoretical predictions of metabolic activities over a large array of environmental conditions where data has not been collected.

---

### The problem with using patient complaints for improvement [^117HSD4D]. BMJ Quality & Safety (2018). Medium credibility.

'The Problem with… ' series covers controversial topics related to efforts to improve healthcare quality, including widely recommended, but deceptively difficult strategies for improvement and pervasive problems that seem to resist solution.

---

### Effective patient-physician communication [^115kWD9S]. ACOG (2025). High credibility.

Best practices for communication in medical encounters — Providing information lists roles and responsibilities to Seek to understand patient's informational needs, Share information, Overcome barriers to patient understanding (language, health literacy, hearing, numeracy), Facilitate understanding, and Provide information resources and help patient evaluate and use them; skills include Explain nature of problem and approach to diagnosis, treatment, Give uncomplicated explanations and instructions, Avoid jargon and complexity, Encourage questions and check understanding, and Emphasize key messages.

---

### Necessity and implications of ICD-10: facts and fallacies [^11523574]. Pain Physician (2011). Low credibility.

The International Classification of Diseases-10 (ICD-10 is a new system that is expected to be implemented effective on October 1, 2013. This new system is a federally mandated change affecting all payers and providers, and is expected to exceed both the Health Insurance Portability and Accountability Act (HIPAA) and Y2K in terms of costs and risks. However, the Administration is poised to implement these changes at a rapid pace which could be problematic for health care in the United States. In 2003, HIPAA named ICD-9 as the code set for supporting diagnoses and procedures in electronic administrative transactions. However, on January 16, 2009, the Department of Health and Human Services (HHS) published a regulation requiring the replacement of ICD-9 with ICD-10 as of October 1, 2013. While ICD-9 and 10 have a similar type of hierarchy in their structures, the ICD-10 is more complex and incorporates numerous changes. Overall, ICD-10 contains over 141,000 codes, a whopping 712% increase over the less than 20,000 codes in ICD-9, creating enormous complexities, confusion, and expense. Multiple published statistics illustrate that there are approximately 119 instances where a single ICD-9 code can map to more than 100 distinct ICD-10 codes, whereas there are 255 instances where a single ICD-9 code can map to more than 50 ICD-10 codes. To add to the confusion, there are 3,684 instances in the mapping for diseases where a single ICD-10 code can map to more than one ICD-9 code. Proponents of the new ICD-10 system argue that the granularity should lead to improvements in the quality of health care, since more precise coding that more accurately reflects actual patient conditions will permit smarter and more effective disease management in pay-for-performance programs. This, in essence, encapsulates the benefits that supporters of this new system believe will be realized, even though many of these experts may not be involved in actual day-to-day medical practices. Detractors of the system see the same granularity as burdensome. The estimated cost per physician is projected to range from $25,000 to $50,000. Further, they argue that the ICD-10 classification is extremely complicated, and expensive. Concerns exist that it is being implemented without establishing either the necessity or thinking through the unintended consequences. Opponents also argue that beyond financial expense, it is also costly in terms of human toll, hardware and software expenses and has the potential to delay reimbursement. There is also concern that an unintended consequence of granularity would be the potential for enhanced and unnecessary fraud and abuse investigations. The authors of this article favor postponing the implementation of the ICD-10 until such time as its necessity is proven and implications are understood.

---

### Defining the time-limited trial for patients with critical illness: an official American Thoracic Society workshop report [^112btxG5]. Annals of the American Thoracic Society (2024). High credibility.

Table 1: Modified Delphi participant characteristics — Among n = 100 participants, geographical area included North America 81 (87), Europe 8 (9), and Asia 4 (4). Gender included Female 48 (51) and Male 46 (49). Roles included Physician 78 (82) and Researcher/scientist 22 (23). Years in practice included 0–5 13 (13) and 11–20 36 (37). Percentages may total greater than 100% because some participants had more than one role.

---

### Treosulfan-fludarabine-thiotepa-based conditioning treatment before allogeneic hematopoietic stem cell transplantation for pediatric patients with hematological malignancies [^112xD1Rb]. Bone Marrow Transplantation (2020). Medium credibility.

Patients and methods

Patient eligibility

Between November 2014 and July 2015, 70 children (aged 28 days to 17 years) with acute lymphoblastic leukemia (ALL), acute myeloid leukemia (AML), myelodysplastic syndrome (MDS), or juvenile myelomonocytic leukemia (JMML) were enrolled in this prospective non-randomized phase II trial at 18 transplantation sites in five European countries. Written informed consent on all aspects of the study was obtained from all children and/or their legal guardians before enrollment. The study was approved by the responsible independent ethics committees and competent authorities and was performed in accordance with the Declaration of Helsinki, as well as Good Clinical Practice guideline. Safety and outcome parameters of all surviving patients were analyzed after a 36 months follow-up period focusing on the 65 patients who were additionally treated with thiotepa. The 5 out of 70 patients, who received treosulfan and fludarabine only, were excluded from the statistical analysis reported here. However, the key findings for all 70 trial patients (including the five patients conditioned only with treosulfan/fludarabine) are provided in Fig. S1 in the online supplementary.

Donors and grafts

Either human leucocyte antigens (HLA)-identical siblings (MSD), matched family donors, or matched unrelated donors were eligible donors (Table 1). An HLA match was defined as at least a 9/10 allele-matched after high-resolution four-digit typing in HLA-A*, B*, C* and DRB1* and DQB1*.

Table 1
Summary of demographic data and transplant characteristics by disease.

ALL acute lymphoblastic leukaemia, AML acute myeloid leukaemia, MDS myelodysplastic syndrome, JMML juvenile myelomonocytic leukaemias, ICH International Council of Harmonization, Max. maximum, Min. minimum, N number of subjects, n number of subjects in category, SD standard deviation, na not applicable.

a For ALL and AML subjects only.

---

### Somatropin (Humatrope) [^116x2CBh]. FDA (2025). Medium credibility.

Step 3E – Store Pen and Cartridge for next use

See, " How do I store HumatroPen 6 mg?" in Section 1 of this Instructions for Use for more information.
When it is time for the next scheduled dose, go to Section 3, and repeat Steps 3A through 3D.
When using the Hidden Needle Cover with HumatroPen 6 mg, repeat steps 1 to 10 of the Hidden Needle Cover Instructions for Use.

Commonly asked questions:

Do I need to perform the New Cartridge Setup before every dose?
No. You only need to perform the New Cartridge Set-up 1 time for each Cartridge, just before a new Cartridge is used for the first time.
The purpose of the New Cartridge Set-up is to make sure the HumatroPen 6 mg with an attached HUMATROPE 6 mg Cartridge are ready to use.
If you repeat the New Cartridge Set-up before each routine dose, you may run out of HUMATROPE early. The small amount of medicine used in the New Cartridge Set-up will not affect the amount of HUMATROPE in the Cartridge.
What should I do if the Cartridge label and Pen do not match?
Do not use the Pen if the Cartridge strength on the HUMATROPE Cartridge label does not match the number on the Pen's front housing. This is important to make sure the correct dose of HUMATROPE is given.
Contact your healthcare provider for a replacement.
What should I do if the HUMATROPE is not clear?
Do not use the Pen if the liquid is cloudy or contains particles. Contact your healthcare provider or call Lilly at 1-800-545-5979 or go to www.humatrope.com.
Why are there air bubbles in the Cartridge?
Air bubbles may remain in the Cartridge after mixing.
If the Pen is stored with a Needle attached, air bubbles may form in the Cartridge.

---

### Erratum: vol. 69, no. 27 [^112r61Lf]. MMWR: Morbidity and Mortality Weekly Report (2023). Medium credibility.

In the report "Trends in Nonfatal Falls and Fall-Related Injuries Among Adults Aged ≥ 65 Years — United States 2012–2018", on page 876, in Table 1, for the category "Total, all aged ≥ 65 years", among Black persons, the number reporting a fall should have read 957,303, and among American Indian or Alaska Native persons, should have read 104,915. On page 878, in Table 2, for the category "65–74 years", among American Indian or Alaska Native persons, the percentage of fall-related injuries and corresponding 95% CI should have read 16.9 (11.9–23.3).

---

### Concerns regarding… [^113tKZEQ]. EClinicalMedicine (2024). Medium credibility.

Controversy about hyperparameter optimization of various machine learning models

In this study, the process of hyperparameter optimization is not described, even though different combinations of hyperparameters are crucial for model building and validation.

In the field of machine learning, using default hyperparameters may not be suitable for specific datasets. Lack of hyperparameter optimization can prevent the model from fully exploiting feature information, leading to less stable performance, high heterogeneity, and insufficient robustness, which negatively affects model building and generalization. Different combinations of hyperparameters significantly impact the predictive efficacy of the model. Failure to adequately perform hyperparameter optimization may result in models that do not achieve optimal predictive efficacy. This causes selection bias in the model and ultimately leads to incorrect decisions.

Machine learning is prone to model overfitting and poor generalization if the built-in hyperparameters are not properly set. Therefore, hyperparameter optimization in the field of machine learning is a scientific, rigorous, and reliable practice.

---

### Global strategy for asthma management and prevention [^113dGUfW]. GINA (2024). High credibility.

Asthma strategy implementation — essential elements required to implement a health-related strategy enumerate "Steps in implementing an asthma strategy into a health system" that include "Develop a multidisciplinary working group", "Assess the current status of asthma care delivery, outcomes e.g., exacerbations, admissions, deaths, care gaps and current needs", "Select the material to be implemented, agree on main goals, identify key recommendations for diagnosis and treatment, and adapt them to the local context or environment", "In treatment recommendations, consider environmental issues (planetary health) in addition to patient health", "Identify barriers to, and facilitators of, implementation", and "Select an implementation framework and its component strategies". The plan continues with "Develop a step-by-step implementation plan:" to "Select target populations and evaluable outcomes, and specify data coding requirements (if relevant)", "Identify local resources to support implementation", "Set timelines", "Distribute tasks to members", "Evaluate outcomes", and "Continually review progress and results to determine if the strategy requires modification".