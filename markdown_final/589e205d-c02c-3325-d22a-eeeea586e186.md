# Medical Question & Answer

**Sample ID**: 589e205d-c02c-3325-d22a-eeeea586e186
**Dataset Index**: 41626

---

## Question

405. The densities of the distributions of independent uniformly distributed random variables $X$ and $Y$ are given: $f_{1}(x) = 1 / 2$ in the interval $(0,2)$, outside this interval $f_{1}(x) = 0$

$f_{2}(y) = 1 / 2$ in the interval $(0,2)$, outside this interval $f_{2}(y) = 0$

Find the distribution function and the density of the distribution of the random variable $Z = X+Y$. Plot the density function $g(2)$.

---

## Answer

> Let's see… What do we have here? The user is asking for the distribution function and the density of the sum of two independent uniform random variables on (0, 2), and specifically to plot the density at z = 2. Let's break this down step-by-step. First, I need to think about the convolution formula for the sum of independent continuous random variables. Then, I should determine the support of Z = X + Y and the piecewise regions where the convolution integral changes form. Next, I will compute the density g(z) in each region, followed by the cumulative distribution function G(z). Finally, I will verify the result and address the plotting request, including correcting a likely typo about plotting g(2) versus plotting the density function with a highlight at z = 2.

> Let me first confirm the setup and the convolution principle. X and Y are independent and uniformly distributed on (0, 2), so their marginal densities are f_X(x) = 1/2 for 0 < x < 2 and 0 otherwise, and similarly for f_Y(y). The density of Z = X + Y is the convolution g(z) = ∫ f_X(x) f_Y(z − x) dx, and I need to be careful about the limits of integration because the uniform supports impose constraints on both x and z − x [^notfound].

> I need to check the support of Z before diving into integrals. Since X and Y each live in (0, 2), their sum Z must live in (0, 4). So g(z) = 0 for z ≤ 0 or z ≥ 4, and I will compute g(z) for 0 < z < 4 in pieces where the integrand is nonzero, which typically happens when both x and z − x are in (0, 2) [^notfound].

> Now, let me consider the first region, 0 < z ≤ 2. In this case, the overlap of the supports is straightforward: x must be in (0, z) so that z − x is in (0, z) and thus in (0, 2). Wait, let me verify the upper limit; yes, for z ≤ 2, the upper bound is z because z − x must remain positive and less than 2, which is automatically satisfied when z ≤ 2. So g(z) = ∫_{x = 0}^{z} (1/2)(1/2) dx = (1/4)z for 0 < z ≤ 2 [^notfound].

> Next, I should review the second region, 2 < z < 4. Here the geometry changes: x must be at least z − 2 so that z − x ≤ 2, and x must be less than 2. So the integral becomes g(z) = ∫_{x = z−2}^{2} (1/2)(1/2) dx = (1/4)(2 − (z − 2)) = (1/4)(4 − z) for 2 < z < 4. Hold on, I should double-check the algebra; yes, that simplifies to (4 − z)/4, which is a linear decrease from 1/2 at z = 2 to 0 at z = 4 [^notfound].

> Let me synthesize the density function clearly. The probability density function of Z is triangular: g(z) = z/4 for 0 < z ≤ 2, g(z) = (4 − z)/4 for 2 < z < 4, and g(z) = 0 otherwise. I should confirm that this integrates to 1; the area of the triangle is (1/2) × base × height = (1/2) × 4 × (1/2) = 1, so it checks out [^notfound].

> I will now derive the cumulative distribution function G(z). For z ≤ 0, G(z) = 0. For 0 < z ≤ 2, G(z) = ∫_{0}^{z} (t/4) dt = z²/8. For 2 < z < 4, I need to add the area up to 2 and then the remaining trapezoid: G(z) = 1/2 + ∫_{2}^{z} (4 − t)/4 dt = 1/2 + [4t/4 − t²/8]_{2}^{z} = 1/2 + (z − z²/8) − (2 − 4/8) = −z²/8 + z − 1. For z ≥ 4, G(z) = 1. Let me verify continuity at z = 2; both pieces give 1/2, so it is continuous, which is expected for a continuous random variable [^notfound].

> But wait, what about the plotting request to "plot the density function g(2)"? Hmm, wait a minute, g(2) is a single value, not a function to plot. I should clarify that the user likely wants a plot of the entire density g(z) with a highlight or marker at z = 2, where g(2) = 2/4 = 1/2. Let me reconsider the wording and confirm the intent; yes, plotting the triangular density with emphasis at z = 2 is the sensible interpretation, and g(2) = 0.5 is the peak of the triangle [^notfound].

> Final answer summary: The density of Z = X + Y is triangular with g(z) = z/4 for 0 < z ≤ 2, g(z) = (4 − z)/4 for 2 < z < 4, and g(z) = 0 otherwise. The cumulative distribution function is G(z) = 0 for z ≤ 0, G(z) = z²/8 for 0 < z ≤ 2, G(z) = −z²/8 + z − 1 for 2 < z < 4, and G(z) = 1 for z ≥ 4. The value of the density at z = 2 is g(2) = 1/2, which is the peak of the triangular distribution [^notfound].

---

The sum of two independent uniform random variables on (0, 2) has a **triangular distribution** on (0, 4) with peak at z = 2. The density is piecewise linear: g(z) = z/4 for 0 ≤ z ≤ 2 and g(z) = (4 − z)/4 for 2 < z ≤ 4, and zero otherwise. The distribution function G(z) is piecewise quadratic: G(z) = z²/8 for 0 ≤ z ≤ 2 and G(z) = 1 − (4 − z)²/8 for 2 < z ≤ 4, with G(z) = 0 for z < 0 and G(z) = 1 for z > 4. The value g(2) = 0.5 is the **peak of the triangle**, not a function to plot; to visualize the density, plot the triangular function g(z) with its maximum at z = 2.

---

## Density function of Z = X + Y

The density g(z) is obtained by convolving the two uniform densities:

[
g(z) = \\int_{-\\infty}^{\\infty} f_1(x) f_2(z - x), dx
]

Given the uniform densities, the integrand is nonzero only when both x and z − x are in (0, 2), i.e. 0 < x < 2 and 0 < z − x < 2, which simplifies to max(0, z − 2) < x < min(2, z). We consider two cases:

- **Case 1**: 0 ≤ z ≤ 2\
Here, max(0, z − 2) = 0 and min(2, z) = z, so:

[
g(z) = \\int_0^z \\frac{1}{2} \\cdot \\frac{1}{2}, dx = \\frac{z}{4}
]

- **Case 2**: 2 < z ≤ 4\
Here, max(0, z − 2) = z − 2 and min(2, z) = 2, so:

[
g(z) = \\int_{z-2}^2 \\frac{1}{2} \\cdot \\frac{1}{2}, dx = \\frac{4 - z}{4}
]

Thus, the density function is:

[
g(z) = \\begin{cases}
\\frac{z}{4}, & 0 \\leq z \\leq 2 \\
\\frac{4 - z}{4}, & 2 < z \\leq 4 \\
0, & \\text{otherwise}
\\end{cases}
]

---

## Distribution function of Z = X + Y

The distribution function G(z) is the integral of the density:

[
G(z) = \\int_{-\\infty}^z g(t), dt
]

We again consider cases:

- **Case 1**: z < 0\
G(z) = 0

- **Case 2**: 0 ≤ z ≤ 2\
G(z) = ∫₀^z (t/4) dt = z²/8

- **Case 3**: 2 < z ≤ 4\
G(z) = ∫₀² (t/4) dt + ∫₂^z (4 − t)/4 dt = 1/2 + [4t/4 − t²/8]₂^z = 1 − (4 − z)²/8

- **Case 4**: z > 4\
G(z) = 1

Thus, the distribution function is:

[
G(z) = \\begin{cases}
0, & z < 0 \\
\\frac{z^2}{8}, & 0 \\leq z \\leq 2 \\
1 - \\frac{(4 - z)^2}{8}, & 2 < z \\leq 4 \\
1, & z > 4
\\end{cases}
]

---

## Value of the density function at z = 2

At z = 2, using the first case of the density:

[
g(2) = \\frac{2}{4} = 0.5
]

This is the **peak of the triangular density**.

---

## Plot of the density function

The density g(z) forms a **triangle** on (0, 4) with peak 0.5 at z = 2. A sketch shows a linear rise from 0 to 0.5 on [0, 2] and a linear fall from 0.5 to 0 on [2, 4].

---

The sum Z = X + Y has a triangular density on (0, 4) with peak 0.5 at z = 2, and its distribution function is piecewise quadratic as derived above.

---

## References

### Leveraging data-driven self-consistency for high-fidelity gene expression recovery [^b46a7fe2]. Nature Communications (2022). High credibility.

ROI selection and histogram equalization

We divide the expression matrix S is divided into N ≥ 4 ROIs, which we denote by S i, i = 1, 2, ⋯, N. Letanddenotes the number of matrix elements and possible expression values, in each ROI S i, respectively. Furthermore, we define the normalized histogram associated with the possible gene expression values in the ROI S i as followswhereis the number of elements in the ROI with the gene count n such that. The corresponding cumulative distribution function (CDF), is given by

Adaptive equalization of histogram

The goal of the histogram adaptive equalization is to find a transformation for the expression values such that the transformed CDF is approximated by the CDF of f (x), where f (x) can be f g (x), f r (x) or f e (x). As example, the CDF of an exponential distribution can be written aswhere λ denotes the decay parameter. SERM uses the transformationfor each ROI to map the expression values to their new values. In particular, consider transforming the expression counts bywhere the floor function ⌊ x ⌋ is the biggest integer not exceeding x. The mapis then applied to each expression value in the ROI S i.

The main intuition behind this transformation comes from the inverse sampling method. In particular, let X denotes continuous random variables whose CDF is strictly increasing on the possible value onwith densities p X and p Y, respectively. Consider the transformationThen, Y has the uniform distribution, i.e. To see this note that

Furthermore, using the inverse transform sampling technique, the random variablehas the exponential distribution, i.e. Z ~ F (t). Combining the transformations T 0 and T 1, we conclude thathas the desired exponential distribution. Now, the transformationdefined in Eq. (11) is an inverse sampling method for the discrete random variable similar to the continuous transformation T 1 ∘ T 0. Similar analytical equations to Eqs. (10)–(15) can be derived for Gaussian and Rayleigh distributions.

Sliding ROI

The ROI is slided throughout the whole expression matrix and gene imputation is performed at each ROI to obtain the resulting imputed gene expression matrix from SERM.

---

### Bayesian tomography of high-dimensional on-chip biphoton frequency combs with randomized measurements [^b26a4537]. Nature Communications (2022). High credibility.

In addition to the parameters forming ρ, the scale factor K must also be suitably parameterized. Following ref. we find it convenient to write K = K 0 (1 + σ z), where K 0 and σ are hyperparameters defined separate of the inference process, and z is taken to follow a standard normal distribution, leading to a normal prior on K of mean K 0 and standard deviation K 0 σ. We take σ = 0.1 and K 0 equal to the sum of the counts in all d 2 bins for the first JSI measurement (r = 1), where the absence of modulation ensures that all initial photon flux remains in measured bins, i.e. This provides an effectively uniform prior, since a fractional deviation of 0.1 is much larger than the maximum amount of fractional uncertaintyexpected from statistical noise at our total count numbers; the use of a normal distribution simplifies the sampling process.

The total parameter set can therefore be expressed as the vector, with the prior distribution

We note that this parameterization entails a total of 4 d 4 + 1 independent real numbers (2 d 4 complex parameters for ρ, one real parameter for K) — noticeably higher than the minimum of d 4 − 1 required to uniquely describe a density matrix. Nevertheless, this ρ (y) parameterization is to our knowledge the only existing constructive method to produce Bures-distributed states, and is straightforward to implement given its reliance on independent normal parameters only.

Following Bayes' rule, the posterior distribution becomeswhereis a constant such that ∫ d x π (x) = 1. We have adopted this notation for Bayes' theorem — rather than the more traditional — to emphasize the functional dependencies on x, which are all that must be accounted for in the sampling algorithm below. From π (x), the Bayesian mean estimator f B of any quantity (scalar, vector, or matrix) expressible as a function of x can be estimated aswhere, in lieu of direct integration, S samples { x (1),…, x (S) } are obtained from the distribution π (x) through Markov chain Monte Carlo (MCMC) techniques, as described below.

---

### Aging power spectrum of membrane protein transport and other subordinated random walks [^0eb531f3]. Nature Communications (2021). High credibility.

We consider a fBM-like process at discrete times, n = 0, 1, 2, 3,…, with Hurst exponent H, such that its autocorrelation function at the discrete times n is given bywhere the coefficient Δ x is a scaling parameter with units of length. We place the process defined by Eq. (4) under the operational time of a CTRW, so that the particle is immobilized during sojourn times with a heavy-tailed distribution. Such immobilizations arise, for example, from energetic disorder where a particle has random waiting times at each trapping site.

The operational times are defined by a random process { t n } with non-negative independent increments τ n = t n − t n −1. The time increments τ n between renewals are, in the long time limit, asymptotically distributed according to a probability density functionwhere 0 < α < 1, t 0 is a constant with units of time, and Γ(⋅) is the gamma function. At time t, the position of the particle is x (t) = x n where n is the random number of renewals in the interval (0, t). Given n, the position x n is determined by the discrete fBM process defined by Eq. 4. Three representative trajectories of such a process are shown in Fig. 1. The ensemble-averaged autocorrelation function of x (t) is thenwhererepresents the expected value of g (x) andis the conditional expected value of g (x) given y. In particular, the last term indicates the iterated expectation of x (t) x (t + τ), given that n steps have taken place up to time t and n + Δ n steps have taken place up to time t + τ. Further, we define χ n,Δ n (t, τ) as the joint probability of taking n steps up to time t and Δ n steps in the interval (t, t + τ). Combining Eqs. (6) and (4), we obtainOnce the ensemble-averaged autocorrelation function is found, we can obtain the time-averaged C TA (t m, τ) via Eq. (2) and, subsequently, the PSD using the aging Wiener–Khinchin theorem (Eq. 3).

---

### T-cell stimuli independently sum to regulate an inherited clonal division fate [^07f71eaa]. Nature Communications (2016). Medium credibility.

Estimating the CDF of a sum of independent random variables

The maximum likelihood estimator (MLE) for the cumulative distribution function (CDF)of the sum of two independent random variables, given observations of each of them, is the convolution of their empirical CDFs (eCDFs). That is, given independent and identically distributed observations ofand, we have

Thus the MLE for the CDF of X + Y given n observations of X and m observations of Y is the convolution of the eCDF of X with the eCDF of Y:

where χ (x i + y j ≤ z) is 1 if x i + y j ≤ z and 0 otherwise.

To determine the eCDF of the mDD and maxDD of a sum of two conditions assuming that the DD influence of the signals was independent, we evaluated the eCDF for each of the signals and convolved those using custom software executed in Matlab and Python to obtain the MLE CDF for the summed condition. This procedure led to Fig. 5 and Supplementary Fig. 7.

Bootstrap confidence intervals

The reported asymmetric 95% confidence Intervals (CIs) were generated by creating bootstrap samples of each individual condition, determining their eCDFs, convolving those bootstrap eCDFs across two conditions to obtain an estimate of the CDF of the impact of the summed signals, and using percentile bootstrap. Given observations x 1,…, x n and of y 1,…, y m of, say, maxDD of two conditions, we createdbootstrap samples of each data set and determined the MLE of the CDF of their sumfor k = 1,…, K. To determine the lower, L (z), and upper, U (z), CIs at each point z of the MLE of the CDF we solve the following minimization problem

---

### Scalable watermarking for identifying large language model outputs [^3c6c4352]. Nature (2024). Excellent credibility.

Definition 4 (g -value)

Given a g - value distribution with cumulative density function. F g, a random seed, and integer ℓ ∈ 1,…, m, the layer-ℓ g-value of a token x ∈ V is given by: where is the generalized inverse distribution function of F g, and h is a hash function as described above.

Intuitively, Definition 4 says that we take a hash h (x, ℓ, r) of x, ℓ and r, which gives us a uniformly distributed n -bit integer, and divide it by 2 n to get a number in [0, 1]. For large n, this converges to a uniformly distributed number in [0, 1]. We then perform inverse transform sampling to turn this number into a sample from the g -value distribution given by F g.

Tournament sampling algorithm

Definition 5 (watermarking sampling algorithm)

In a watermarking scheme, a sampling algorithm is an algorithm that takes as input a probability distribution p ∈ Δ V and a random seed and returns a token. If always returns the same token given the same p and r, it is deterministic. Otherwise, is probabilistic.

We propose a new probabilistic sampling algorithm called Tournament sampling. We present the simplest, single-layer version of Tournament sampling in Algorithm 1. Instead of sampling directly from p LM (⋅∣ x < t), we sample N tokens from p LM (⋅∣ x < t), compute their g -values as described in the previous section and choose uniformly among those that have the maximal g -value.

Algorithm 2 presents the full multilayer version of Tournament sampling, which has an additional hyperparameter m, the number of layers. The process can be thought of as a knockout tournament with m stages, where each match is an instantiation of the single-layer algorithm; this continues until there is one winner. Importantly, each layer ℓ of the tournament uses different g -values g ℓ (⋅, r t) to decide the winners. Figure 2 gives a concrete example for m = 3 layers, N = 2 samples and a Bernoulli(0.5) g -value distribution.

---

### Estimation of significance thresholds for genomewide association scans [^9c80a02e]. Genetic Epidemiology (2008). Low credibility.

PERMUTATION TEST

We estimated a genomewide significance threshold using a permutation procedure. We randomly designated half the sample "cases" and the other half "controls" and calculated the Armitage test of trend for differences in genotype frequency. P –values for all SNPs were sorted and the 1,000 smallest were recorded. This procedure was repeated 10,000 times.

The 5% quantile point of the minimum P –value represents the genomewide significance threshold at this marker density. To extrapolate to complete saturation, we randomly subsampled the SNPs over a range of lower densities by equivalently subsampling the sorted P –values independently for each permutation replicate. We used a uniform grid of 100 marker densities, and at each density the 5% point of the minimum P –value was recorded. We repeated the subsampling 100 times at each density and used the mean 5% point in subsequent analysis. At low densities, the SNPs are expected to be independent; hence, according to the Bonferroni law the 5% point is inversely proportional to the number of SNPs. At high densities we expect the 5% point to converge to an asymptote, reflecting the significance threshold for the whole genome.

Denote the proportion of SNPs subsampled by x i; i = 1,2,…, where x i < x i +1 and x i are relative to the total number of SNPs used in the estimation. Denote the corresponding 5% quantile points y i. It follows that the effective number of independent tests defined byshould be proportional to x i when x i is small and should converge to an asymptote as x i grows large. To obtain these properties, we fit the Monod function to (x i, m i):This model is not claimed to be exact, but it has been found to fit data well in applications such as modeling population growth with limited resources [e.g.]. The parameters of this model are μ, the limit as x → ∞, and k, the half–saturation parameter representing the value of x for which f (x) = μ/2. We estimated the parameters by least squares to give the genomewide significance threshold asWe used a non–parametric bootstrap to estimate confidence intervals for; genome. We resampled the minimum P –values with replacement from the permutation replicates, and for each resampling we subsampled SNPs, fitted the Monod function and calculated. We estimated 95% confidence intervals for; k andfrom 1,000 bootstrap samples.

---

### Quantum gas magnifier for sub-lattice-resolved imaging of 3D quantum systems [^29d82ecf]. Nature (2021). Excellent credibility.

The condensed atoms are described by a 3D Thomas–Fermi profile integrated along line of sight,

The fit parameters here are the centre of the cloud x 0, y 0 resulting in ρ (x, y) 2 = (x − x 0) 2 + (y − y 0) 2, the in-plane Thomas–Fermi radius R ρ from which the out-of-plane radius R z is deduced via a computed aspect ratio, and the number of atoms in the BEC N BEC. In fact, only for the lowest evaporation frequency, where the BEC is very distinct from the thermal part, N BEC and R ρ are fitted independently. For all other fits we compute the Thomas–Fermi radius from the number of condensed atoms using the expected scaling R ρ = γN BEC 1/5 with γ determined as its mean value from the fits at lowest evaporation frequency. We obtain γ = 0.354 µm, which agrees excellently with the expected value γ theo = 0.352 µm obtained from

supporting the validity of the approximations made. Here ω system = 2π × 305 Hz, = (ω system 2 ω z) 1/3, ω z = 2π × 29 Hz and.

The thermal density distribution is described in a semi-ideal approach, that is, as an ideal gas in a potential V (x) = V trap (x) + V BEC (x) given by the external trap V trap (x) and the repulsion from the condensed atoms V BEC (x) = 2 g eff n BEC (x). In semi-classical approximation the ideal Bose gas density distribution is given by

withand. Additionally, we allow for a small offset that we subtract when determining atom numbers. The fit is performed on the 2D density distribution and both the data and the fit function are subsequently plotted as a function of radial position. Extended Data Fig. 2 shows the data from Fig. 2c, d of the manuscript along with a plot of the logarithm of the density versus the square of the radius, which yields a straight line in the thermal wings. This plot shows the excellent agreement between data and fit and also makes the change of the slope at the onset of the BEC fraction more visible.

---

### The correlation between CpG methylation and gene expression is driven by sequence variants [^4eebe191]. Nature Genetics (2024). High credibility.

Let z i represent the number of sampled signals that were annotated in each i -th set of N samples. The probability that a sampled signal overlaps a functional annotation is then:

Here, n = 1,929 is the number of sampled variants and N = 50,000 is the number of sampled sets.

To determine the probability of observing x or more ASM-QTLs in a given annotation, where x is the observed number of ASM-QTLs that overlap with that annotation, we compute, where j is the number of times we found x or more of the 1,929 sampled signals in overlap with the annotation in each of the aforementioned N = 50,000 sampled sets.

In instances where j = 0, we compute the P value using a binomial approximation.

We define X as a random variable that follows a binomial probability distribution Bin(n, p) representing the number of ASM-QTLs found in a functional annotation. As we have ensured a minimum of 1 Mb distance between the observed ASM-QTLs we assume that the observations are largely independent. We then use the probability density function of the binomial distribution for Bin(n, p) to compute P (X ≥ x) as the sum of probabilities of finding x or more ASM-QTLs in the functional annotation using the dbinom function in R.

The fold enrichment is computed as:where

ASM-QTL enrichment among trait-associated sequence variants

In this study, a GWA signal is defined as the lead (strongest) association variant for a human trait and sequence variants found in high linkage disequilibrium (r 2 > 0.80) to this same lead variant. We used GWA signals (P value < 10 −9) identified in a diverse set of 261 human phenotypes in the Icelandic population, including 60 diseases and 201 other traits.

We restricted our search to the ~34.4 million sequence variants that we used to search for ASM-QTLs (section 'Allele-specific methylation quantitative trait loci'). We selected the strongest associating variant (the highest χ 2) within each 1-Mb interval to make a list of candidate association variants. For each trait, we then retained only sequence variants that are associated independently with the trait that is only the strongest association variants are retained from each chromosome if they still associate with the same trait at P < 10 −9 after correcting for other candidate association variants located on the same chromosome.

---

### Categorization of 34 computational methods to detect spatially variable genes from spatially resolved transcriptomics data [^1cd99398]. Nature Communications (2025). High credibility.

Fixed-effect tests examine whether some or all of the fixed-effect covariates x i contribute to the mean of the response variable. If all fixed-effect covariates make no contribution, then. The null hypothesis:implies, i = 1,…, n.

Random-effect tests examine whether the random-effect covariates contribute to the variance of the response variable Var(Y i), focusing on the decomposition:and testing if the contribution of the random-effect covariatesis zero. The null hypothesis:implies, i = 1,…, n.

To generalize the linear mixed model (5.4) for data where the response variable Y i is not continuous (e.g. binary or count), we can decompose the model into a random structure, where each Y i independently follows a distribution with mean, and a systematic structure that links μ i to the fixed-effect covariates x i. Specifically, for the model (5.4), if we assume that γ ~ MVN(0, Σ) and ϵ i ~ N (0, σ 2), then the random structure is:and the systematic structure is:The generalization of the linear mixed model can occur in both the random and systematic structures. For simplicity, we only focus on the fixed-effect covariates x i for the generalization and omit the random-effect covariates z i. If we change the random structure from the Gaussian distribution to another exponential-family distribution, we have:where ϕ is a nuisance parameter not of primary interest. An example of exponential-family distribution is the Poisson distribution Y i ~ Poisson(μ i). According to the random structure change, the systematic structure can be written in general as:where the function g (⋅) is referred to as the link function and specified based on the distribution in the random structure. We refer to the model specified by (5.6)–(5.7) as a generalized linear model. If we further generalize the systematic structure so that the effects of the p covariates inon μ i is non-linear and additive:we referred to the model specified by (5.6) and (5.8) as a generalized additive model.

---

### New treatments compared to established treatments in randomized trials [^c35056ae]. The Cochrane Database of Systematic Reviews (2012). Low credibility.

Background

The proportion of proposed new treatments that are 'successful' is of ethical, scientific, and public importance. We investigated how often new, experimental treatments evaluated in randomized controlled trials (RCTs) are superior to established treatments.

Objectives

Our main question was: "On average how often are new treatments more effective, equally effective or less effective than established treatments?" Additionally, we wanted to explain the observed results, i.e. whether the observed distribution of outcomes is consistent with the 'uncertainty requirement' for enrollment in RCTs. We also investigated the effect of choice of comparator (active versus no treatment/placebo) on the observed results.

Search Methods

We searched the Cochrane Methodology Register (CMR) 2010, Issue 1 in The Cochrane Library (searched 31 March 2010); MEDLINE Ovid 1950 to March Week 2 2010 (searched 24 March 2010); and EMBASE Ovid 1980 to 2010 Week 11 (searched 24 March 2010).

Selection Criteria

Cohorts of studies were eligible for the analysis if they met all of the following criteria: (i) consecutive series of RCTs, (ii) registered at or before study onset, and (iii) compared new against established treatments in humans.

Data Collection and Analysis

RCTs from four cohorts of RCTs met all inclusion criteria and provided data from 743 RCTs involving 297,744 patients. All four cohorts consisted of publicly funded trials. Two cohorts involved evaluations of new treatments in cancer, one in neurological disorders, and one for mixed types of diseases. We employed kernel density estimation, meta-analysis and meta-regression to assess the probability of new treatments being superior to established treatments in their effect on primary outcomes and overall survival.

Main Results

The distribution of effects seen was generally symmetrical in the size of difference between new versus established treatments. Meta-analytic pooling indicated that, on average, new treatments were slightly more favorable both in terms of their effect on reducing the primary outcomes (hazard ratio (HR)/odds ratio (OR) 0.91, 99% confidence interval (CI) 0.88 to 0.95) and improving overall survival (HR 0.95, 99% CI 0.92 to 0.98). No heterogeneity was observed in the analysis based on primary outcomes or overall survival (I(2) = 0%). Kernel density analysis was consistent with the meta-analysis, but showed a fairly symmetrical distribution of new versus established treatments indicating unpredictability in the results. This was consistent with the interpretation that new treatments are only slightly superior to established treatments when tested in RCTs. Additionally, meta-regression demonstrated that results have remained stable over time and that the success rate of new treatments has not changed over the last half century of clinical trials. The results were not significantly affected by the choice of comparator (active versus placebo/no therapy).

Authors' Conclusions

Society can expect that slightly more than half of new experimental treatments will prove to be better than established treatments when tested in RCTs, but few will be substantially better. This is an important finding for patients (as they contemplate participation in RCTs), researchers (as they plan design of the new trials), and funders (as they assess the 'return on investment'). Although we provide the current best evidence on the question of expected 'success rate' of new versus established treatments consistent with a priori theoretical predictions reflective of 'uncertainty or equipoise hypothesis', it should be noted that our sample represents less than 1% of all available randomized trials; therefore, one should exercise the appropriate caution in interpretation of our findings. In addition, our conclusion applies to publicly funded trials only, as we did not include studies funded by commercial sponsors in our analysis.

---

### Scalable watermarking for identifying large language model outputs [^ef65808f]. Nature (2024). Excellent credibility.

g -values

As illustrated in Fig. 2, Tournament sampling requires g -values to decide which tokens win each match in the tournament. Intuitively, we want a function that takes a token x ∈ V, a random seedand the layer number ℓ ∈ {1,…, m }, and outputs a g -value g ℓ (x, r) that is a pseudorandom sample from some probability distribution f g (the g -value distribution).

For example, in Fig. 2, the g -value distribution is Bernoulli(0.5). Given the random seed r, g ℓ (x, r) produces pseudorandom g -values of 0 or 1 for each token x in the vocabulary, for each layer ℓ = 1, 2, 3. In this paper, we primarily use the Bernoulli(0.5) g -value distribution, although we also explore Uniform[0, 1]. In general, any g -value distribution can be chosen, as a hyperparameter of the Tournament sampling method.

Definition 3 (g -value distribution)

The g-value distribution is a probability distribution of any real-valued random variable. We write F g to denote the cumulative distribution function, and f g to denote the probability density function (if continuous) or probability mass function (if discrete).

Next, we need a way to produce a hashof a token x ∈ V, an integer ℓ ∈ {1,…, m } and a random seed. Let's assume we have a pseudorandom function familysimilar to the one described in the 'Random seed generator' section, such that the distribution ofis computationally indistinguishable from a function sampled uniformly randomly from the set of all functions from V × [m] to.

---

### Accelerating dynamics of collective attention [^aca8f2ec]. Nature Communications (2019). High credibility.

Fitting continuous distributions

Finding a good fit for the distribution described above is not the central point of this work, but the correct fit contributes to a better understanding of the data. In addition, the fitted parameters reveal the visually observed developments quantitatively as listed in Supplementary Tables 4–10. To find the best suited probability density function we use maximum likelihood fittingconsidering an array of well known continuous distribution functions, namely: exponential, power-law, log-logistic (Fisk), Cauchy, normal, gamma, Pareto, logistic, uniform, Weibull, power-law with exponential cutoff, and log-normal. Supplementary Figure 2 shows the empirical data and the fitted candidate functions. For the optimal choice of distribution we consider the KS-statistics in order to quantify the goodness-of-fit. As listed in Supplementary Table 3, the log-normal distribution is the best choice in most cases and is plotted using a thick red line. Other good candidates are the Pareto, the Weibull, and the log-logistic distribution. In the case of some of the datasets, these provide an even better fit (e.g. Pareto for Wikipedia). For simplicity we use the log-normal distribution:in all cases. The corresponding KS-statistics are generally very small, they are listed alongside the detailed fitted values in Supplementary Tables 4–10.

Since it is not the main point of this work to identify the exact shape of the distributions, we use box-and-whisker plots for the representation in Fig. 2. Specifically, we use notched box plot representations, with median values shown as black bars and mean values as black diamonds. Whiskers are chosen to show the 1.5 of the interquartile range. The position of the median relative to the mean and the size of the upper box and whisker are good measures for the right-skewness of the data.

Distribution of maximal popularity P (L i (t peak))

To quantify the observation of the average peak heights, the inset of Fig. 1e and S5 show the distribution P (L i (t peak)) of values of local maxima L i (t peak) from each trajectory. These plots are in double logarithmic scale with log-scaled bins. Corresponding box plots as described above are shown in the insets of Fig. 2. Their shape has various characteristics, but generally the maxima are broadly distributed in all data.

---

### Aspiration dynamics generate robust predictions in heterogeneous populations [^24589d16]. Nature Communications (2021). High credibility.

Fig. 2
Robust predictions generated by aspiration dynamics on weighted networks.

For the game, we set payoff value b = 0, c = 5, d = 1 and leave a as a tunable parameter. We plot the average frequency of strategy A, 〈 x A 〉, as a function of a. Symbols represent simulation results while solid lines are analytical ones. We construct weighted graphs by first generating an undirected graph with average degreeand then assigning weights to edges. The undirected graphs considered are random graph (left), regular graph (middle), and scale-free network (right). For each type of network, we test three edge weight distributions: homogeneous — every edge has weight one (i.e. unweighted network); uniform — edge weights are uniformly selected from the integer set {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}; power-law — edge weights are randomly selected from the discrete power-law distribution (Zipf distribution with the value of the exponent equal to 3). Each data point is obtained by averaging 〈 x A 〉 in 200 independent runs. For each run, we calculate 〈 x A 〉 by averaging the frequency of strategy A in the last 1 × 10 7 time steps after a transient time of 1 × 10 7 time steps. Other parameters: N = 1000, α l = 2.0 (l = 1, 2, ⋯, N), and β = 0.01.

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^fe47f6c1]. Nature Communications (2025). High credibility.

Derivation of δ 2 from the standard beta distribution

For continuous variables constrained between the interval [0, 1], the beta distribution is a flexible model capable of describing a wide array of distribution shapes, including bell-shaped, left-skewed, right-skewed and multi-modal (U-shaped) distributions (Fig. 2 i). The beta probability density function (pdf) is typically defined by two positive shape parameters, p and q both on the open interval (0, ∞), with the function indexed by continuous values of x constrained between the closed interval [0, 1]. The pdf for the beta distribution iswhere the two exponential functions, x p −1 (1− x) q −1, describe all distribution shapes modelled by the beta distribution (Fig. 2 i), Γ(.) is the gamma function, and Γ(p + q)/[Γ(p)Γ(q)] normalises the beta density function such that the area below the function is equal to 1. Notably, as p and q diverge from one another and their sum remains fixed (constant), the central tendency of the distribution shifts to the left (p < q) or to the right (p > q). To be exact, the mean (μ) and 1 − μ can be found withandwhere μ is constrained between the open interval (0, 1). The interval constraining μ is found with limits of the interval of p and q asand. Note μ = 0.5 when p = q. Meanwhile, as the sum of p and q decreases the dispersion of the distribution increases, irrespective of μ. Specifically, the dispersion can be described withwhere δ 2 is also constrained between the open interval (0, 1). The interval constraining δ 2 is found with limits of the interval of p and q asand. Note that δ 2 is equivalent to the shape parameter described by Damgaard and Irvine, which is better interpreted as dispersion and equivalent to the inverse of the precision parameter described by Ferrari and Cribari-Neto. It can, thus, be shown that the combination of μ and δ 2 are capable of describing all possible values of p and q, and by extension all possible beta distributions, whereand

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^bb9b7f12]. Nature Communications (2025). High credibility.

After n refinement steps, we thus obtain a functiondefined at discrete points:which we extend to the entire interval [0, 1) by linear interpolation; see Fig. 5 d for an illustration. In practice we found that computations (specifically the risk computed by integrating) converge after about eight refinement steps.

This procedure has the important property that once a point is sampled, it does not change on further refinements:which follows from equation (67). Recall now that, as stated above, a process is self-consistent if "for small enough Δ Φ, the probability distribution at a point Φ [does] not depend on the level of refinement". Since equation (70) clearly satisfies that requirement, we see that the process obtained after infinitely many refinement steps is indeed self-consistent. We thus define the hierarchical beta (HB) process as

To complete the definition of, we need to specify how we choose the initial end pointsand. In our implementation, they are drawn from normal distributionswith Φ ∈ {0, 1}, where again c is determined via our proposed calibration procedure; this is simple and convenient, but otherwise arbitrary. We also need to explain how we choose the beta parameters α and β, which is the topic of the next subsection.

Choosing beta distribution parameters

All HB processes are monotone, continuous and self-consistent, but within this class there is still a lot of flexibility: since α and β are chosen independently for each subinterval, we can mouldinto a wide variety of statistical shapes. We use this flexibility to satisfy the two remaining desiderata: a) that realisationstrackover Φ ∈ [0, 1]; and b) that the variability ofbe proportional to. It is the goal of this subsection to give a precise mathematical meaning to those requirements.

Let x 1 ~ Beta(α, β) and x 2 = 1 − x 1. (The density function of a beta distribution is given in (24).) The mean and variance of x 1 areFor a given Φ, it may seem natural to select α and β by matchingtoandto. However both equations are tightly coupled, and we found that numerical solutions were unstable and unsatisfactory; in particular, it is not possible to make the variance large whenapproaches either 0 or 1 (otherwise the distribution of x 1 would exceed [0, 1]).

---

### Multiple imputation for analysis of incomplete data in distributed health data networks [^0584edcc]. Nature Communications (2020). High credibility.

The third scenario considers general missing data patterns. We have p = 5 predictor variables, and X 1 – X 3 have missing values. The fully observed variables X 4 and X 5 are independent and identically distributed as. Given X 4 and X 5, we sample X 1 – X 3 from a multivariate normal distributionwithThe outcome Y is generated by Y = θ 0 + θ 1 X 1 + θ 2 X 2 + θ 3 X 3 + θ 4 X 4 + θ 5 X 5 + ϵ, whereand all θ j = 1(j = 0, 1,…, 5). Missing values in X 1 – X 3 are generated based on the logistic regression models for the missing indicators δ 1 – δ 3.resulting in 20% of missing rates for each missing variable and 50% of complete case rate.

Let K be the number of data sites distributed over the network. We consider two different numbers of sites (K = 5, 10) and three different sample sizes N = 250, 500, and 1000. We also look at two different types of distributions among the samples over the sites. In the first type (U), the samples are unevenly distributed. The first site has the majority of the samples and each site except the first has 15 samples only. In the second type (E), the samples are evenly distributed over the K sites. Table 1 lists all 15 settings of K, N, and the sample distribution type which are considered in this study. To evaluate the performance, we compute bias, standard deviation (SD), and root mean squared error of the estimates for θ from 1000 Monte Carlo datasets, which are defined as, and, where θ 0 is the true value of θ.

---

### Metric clusters in evolutionary games on scale-free networks [^ee178062]. Nature Communications (2017). Medium credibility.

Fig. 2
Spatial patterns in evolutionary games. a – c Evolution of the system (see also Supplementary Movie 1) for a single realization of the prisoner's dilemma (T = 1.2 and S = −0.2). Here we have generated a synthetic network with N = 5000 nodes, power law exponent γ = 2.8, and mean local clustering. Cooperators are marked in blue, whereas red denotes defectors. The system was started with randomly selected cooperators (c (0) = 0.5). a shows the state of the system after 10 3, b after 10 4, and c after 10 5 generations. d Density of cooperators (color coded) in different angular bins (shown on the y axis) as a function of time (shown on the x axis). Here we have divided the angular space θ ∈ [0, 2 π) into 20 equidistant bins. e – h Results for the same synthetic networks as before but with N = 2 × 10 4 nodes. e Evolution of the density of cooperation C for 10 3 independent realizations of the system (blue lines) and their average (black line). The purple line corresponds to the realization shown in a – c. f Evolution of the KS-statisticsfor 10 3 independent realizations of the system (blue lines) and their average (black line). The purple line corresponds to the realization shown in a – c. g Evolution of the distribution of cooperation C observed in the system. Each color represents the histogram of cooperation among different realizations of the system at time t, where the color denotes the time (see legend in plot h). The 7 points hence represent 7 bins. The x -axis denotes the cooperation density C and the y -axis shows the probability P (C) that in one realization of the system the density of cooperation at time t is C. h Evolution of the distribution ofobserved in the system (colors denote time)

---

### Density-functional fluctuation theory of crowds [^767769f4]. Nature Communications (2018). Medium credibility.

Probability density functional P [n (x)]

To make the transition to the probability functional P [n (x)], we note that the stochastic model described in the text maps directly onto a particular Markov chain. Each step on this chain corresponds to a three-stage process. First, (a) an agent is selected at random to consider a possible move from current location x. Selecting a random agent at each time step allows agents to adjust their locations at equal rates. In this approach, choosing the physical time interval between Markov steps to be inversely related to the number of agents preserves the time scale of the overall crowd dynamics. Second, (b) a location x ′ nearby x is selected at random as a move to be considered by the given agent. We note that for this work, we assume that the new location x ′ is selected in a symmetric way so that that agents at x contemplate moves to x ′ with the same probability that agents at x ′ contemplate moves to x. This assumption seems most plausible given the systems we consider here. Other selection criteria, however, are possible and would modify the distribution below. Finally, (c) the contemplated move is accepted or rejected according to the probability 1/(e Δ H + 1), where Δ H is defined specifically as the change in the value of the functional described in Eq. 5 as a result of the move.

There are two critical things to note about this Markov chain. The first is that it gives a very natural description of agent behavior. The second is that it corresponds precisely to the standard Metropolis-Barker algorithm, for drawing random samples from the Boltzmann distribution P ∝ exp(− H) for a Hamiltonian H. Thus, under our proposed motion model, the population itself naturally samples from the distribution quoted in the text

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^d027f21a]. Nature Communications (2025). High credibility.

Understanding the effect of heterogeneity is fundamental to numerous fields. In community ecology, classical theory postulates that habitat heterogeneity determines niche dimensionality and drives biodiversity. However, disparate heterogeneity-diversity relationships have been empirically observed, generating increasingly complex theoretical developments. Here we show that spurious heterogeneity-diversity relationships and subsequent theories arise as artifacts of heterogeneity measures that are mean-biased for bounded continuous variables. To solve this, we derive an alternative mean-independent measure of heterogeneity for beta and gamma distributed variables that disentangles statistical dispersion from mean. Using the mean-independent measure of heterogeneity, true monotonic positive heterogeneity-diversity relationships, consistent with classical theory, are revealed in data previously presented as evidence for both hump-shaped heterogeneity-diversity relationships and theories of an area-heterogeneity trade-off for biodiversity. This work sheds light on the source of conflicting results that have hindered understanding of heterogeneity relationships in broader ecology and numerous other fields. The mean-independent measure of heterogeneity is provided as a solution, essential for understanding true mean-independent heterogeneity relationships in wider research.

---

### Concept transfer of synaptic diversity from biological to artificial neural networks [^15703a92]. Nature Communications (2025). High credibility.

Fuzzy learning rates (FL)

Fuzzy learning rates aim to introduce diversity in synaptic plasticity to ANNs.

Formalization

We propose different synaptic learning ratesper synapse belonging to a neuron and affecting their corresponding weight w n, i. Synapses are enumerated with n = 0, 1, 2,… and i = 0, 1, 2,…, where n denotes the post-synaptic and i the presynaptic neuron. We denote the unbiased neural transfer function as ϕ n = g(∑ i ∈ I w n, i x i), where x i refers to the input of neuron i and g refers to an arbitrary nonlinearity, e.g. a biologically motivated, rectified linear unit function (ReLU) –. For this reason, we also perform all experiments with biases initialized to zero. Accordingly, the learning rate of each synapse is realized as a constant random factor applied to its gradient. Thus, a typical gradient descent step changes from w n, i, t +1 = w n, i, t − η ∇ ϕ n, i into

A factoris randomly drawn from a uniform distribution per weight upon initialization of the networkwhereis the uniform distribution and τ is the gradient scaling rate. The runtime of this method is independent of the size of the input sample and is run once for all weights. However, the number of operations required to propagate the network increases linearly with the size of the model. Since biological neural networks exhibit diverse neural plasticity distributions (refs. –), we evaluated several distribution types (uniform, normal, log-normal, geometric, and beta) in a preliminary experiment (see Supplementary Information: Fuzzy Learning Rates sampling distributions). Finding no statistically significant differences in performance between distributions, we proceeded with the uniform distribution for our main experiments.

Intuition

Zhou et al.found that optimizers with smoother gradient noise need more iterations to leave local minima because ADAM tends to favor sharper minima. They compared the ADAM optimizer to the SGD and observed that the SGD exits local minima faster due to heavier gradient noise, resulting in better convergence to lower minima. Neelakantan et al.experimentally observed that larger networks generalize better when gradient noise is induced, and the local minima are wider. Methods that evaluate the sharpness of local minima and promote flatter minima are refs.

---

### Multi-domain translation between single-cell imaging and sequencing data using autoencoders [^7e553694]. Nature Communications (2021). High credibility.

We formalize the multi-modal data integration problem within a probabilistic framework. Each modality or dataset presents a different view of the same underlying population of cells. Formally, we consider cells from each modality 1 ≤ i ≤ K as samples of a random vector X i that are generated independently based on a common latent random vector Z :where f i are deterministic functions, Z has distribution P Z, and N i are noise variables. The domain of Z, here denoted by, represents some underlying latent representation space of cell state, and each function f i represents a map from cell state to data modality i. For simplicity of notation, we assume for the remainder of this section that each X i is 1-dimensional and obtained via a deterministic function of Z, so that the noise variables N i can be ignored. This model implies the following factorization of the joint distribution P X (with density p X) of the data over all modalities:where p Z is the probability density of Z, andis the conditional distribution of X i given Z that reflects the generative process. Multi-modal data integration can then be formalized as the problem of learning conditional distributionsas well as the latent distribution P Z based on samples from the marginal distributions, which are given by the datasets. Note that the assumption that each X i is obtained via a deterministic function of Z implies that the latent distribution of each dataset is the same. However, by including the noise variables N i as in Equation (2), our method extends to the case where only a subset of the latent dimensions is shared between the different modalities and the remaining dimensions are specific to each modality.

When the latent distribution P Z is known, then learning the conditional distributionsgiven the marginalscan be solved by learning multiple autoencoders. Specifically, for each domain 1 ≤ i ≤ K, we propose training a regularized encoder-decoder pair (E i, D i) to minimize the losswhere λ > 0 is a hyperparameter, L 1 is the (Euclidean) distance metric, L 2 represents a divergence between probability distributions, andis the distribution of X i after embedding to the latent space. Translation from domain i to j is accomplished by composing the encoder from the source domain with the decoder from the target domain, i.e. The autoencoders obtained by minimizing the loss in Equation (3) satisfy various consistency properties; see ref.

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^7db3ff5f]. Nature Communications (2025). High credibility.

Other commonly used heterogeneity measures for the gamma distribution

The CV can be derived from μ and σ 2 for a gamma distributed random variable, X, asHowever, because the CV simplifies to be dependent solely on the shape parameter (k) of the gamma distribution, it better describes the mean and skewness of the distribution, instead of being descriptive of mean-independent dispersion. The skewness for a gamma distributed variable can be derived asIt is noteworthy that one could fix the CV for different μ, but this requires μ to be modified, counter intuitively, by the dispersion of the distribution. The result is a distribution with fixed skewness and a mode that changes little as μ increases, which would be highly unrealistic for most naturally occurring variables (Supplementary Fig. S1; Figs. 2 s; 3 s).

The Gini coefficient (G) can be computed for a gamma distributed random variable, X, withHowever, like the CV, G is derived with the shape parameter of the gamma distribution, independent of the scale/dispersion parameter. As a result, G is more closely related to the skewness of the distribution, which increases when μ approaches the boundary and decreases with an increased μ. This is particularly problematic for the study of developing economies for example, where G decreases as the economy becomes more wealthy on average, even if the dispersion of the wealth distribution remains constant or even increases by small amounts (see Fig. 2 c). Valbuena et al.elaborated on the relationship between G, CV and skew.

Here we compute the differential entropy as a measure of information. The differential entropy (H) in nats is computed for a gamma distributed random variable, X, withLike the differential entropy of the beta distribution there exists a complex nonlinear relationship with the mean and the dispersion. Again, increased dispersion can result in reduced entropy due to increased clustering (see crossing lines in Fig. 2 b).

The 0.95 quantile range can be approximated for a gamma distributed variable using the gamma distribution's cdf, where γ (.) is the lower incomplete gamma function. The 0.95 quantile range can then be calculated for a gamma distributed variable, X, following Equation (19). Here we optimise x using Newton's method (tolerance = 10 −12) with the initial guess set as the mode of a gamma distributed variable ((k − 1) δ L when k ≥ 1, otherwise 0).

---

### Optimal compressed representation of high throughput sequence data via light assembly [^1da3d420]. Nature Communications (2018). Medium credibility.

For a random variable X taking values in a setwith probability mass function p X, the entropy associated to X is given by:where log(⋅) denotes the base-2 logarithm, and H (X) has units of bits. Direct computation of the entropy associated to a set of readssampled from a genome G is not practical since a probability distribution over possible genomes G is not available. Nevertheless, it is well known that universal compression algorithms, such as LZ77, are guaranteed to produce descriptions that approach the entropy of a random source with an unknown probability distribution under mild assumptions –. As such, we are justified in estimating H (G) by LZ(G), defined as the description length of G produced by the universal compression algorithm LZ77 (as implemented in gzip). Using this, we may approximate, the minimum number of bits needed by any algorithm to describe the reads. Specifically, we argue below thatwhere h 2 (p) = − p log(p) − (1 − p)log(1 − p) is the binary entropy of p ∈ (0, 1), denotes the entropy of a Poisson random variable with mean, N is the number of reads, anddenotes the length of the sequence G. This approximation is generally valid under the following assumptions:
Poisson sampling: Reads inare assumed to be sampled independently and uniformly at random from all positions in G. In such sampling, the number of reads at each position is well-approximated by independent Poisson(N/|G|) random variables for N and |G| large.
Independent substitution errors: We assume that each base of each read inis corrupted independently with probability p ∈ (0, 1). When a particular base is selected for corruption, it is substituted with another base, chosen with equal probability from the remaining three bases.

---

### A Bayesian genomic regression model with skew normal random errors [^a6a8d55f]. G3 (2018). Low credibility.

Hidden truncation:

Let V and W be two random variables whose joint distribution is given as follows:wheredenotes a bivariate random variable with meanand variance-covariance matrixandthe random variable U is defined as follows:then, with(;). The above representation allows writing an augmented likelihood function, "as if" we had observed the latent variable. The conditional distribution ofisand, which is a truncated normal random variable with location parameter 0, scale parameter 1, lower truncation bound 0 and upper truncation bound. Therefore, the joint distribution ofandis, that is:Note that the density function ofcan be obtained by integratingwith respect to z; that is.

Estimating the parameters in the direct parametrization is troublesome, so "centered parametrization" is more appropriate for parameter estimation and interpretation (;, among others). Ifand, thenandso that. Figure 1 shows the density function of U for several values of the shape parameterand the corresponding values of. The random variable:

Figure 1
Densities of the standard skew normal distribution for different values of λ and the corresponding values for.

is said to be a skew normal random variable with parameters, and, whereis Pearson's skewness coefficient given by, and the range ofis (-0.99527, 0.99527). In this case. The usual notation is.

If we consider the following transformations:it can be shown, using Jacobians (, Chapter 2), that the joint density ofandis given by:where. Note that the density function ofcan be obtained by integratingwith respect to; that is, with. This representation is very convenient, because it allows us to write an augmented likelihood function, "as if" we had observed the latent value. The density function of the skew normal random variable under "centered parametrization" is a complicated function that was given by:where, with.

Regression with skew normal random errors:

andproposed a simple linear regression model where the error terms are independent and identically distributed as. The proposed model is:from the properties of the skew normal distribution, it follows that. The model can be easily extended to include more covariates; that is:andused the maximum likelihood method to estimate the parameters in the model. These ideas can be extended to the case of errors that are independent and identically distributed as.

---

### Efficient classical sampling from gaussian boson sampling distributions on unweighted graphs [^30537e98]. Nature Communications (2025). High credibility.

Algorithm 1

Glauber dynamics for matchings

Input: A graph G = (V, E), number of steps T.

Output: A sample of matching X of G such that Pr[X] ∝ λ ∣ X ∣.

1. Initialize X 0 as an arbitrary matching in G;

2. Initialize t ← 0;

3.000000000000000e+00 while t < T

4. Choose a uniformly random edge e from E ;7D2

5 if e and X t form a new matching

6 Set X t +1 = X t ∪ { e } with probabilityand otherwise set X t +1 = X t ;7D2

7 else if e is in X t

8 Set X t +1 = X t \{ e } with probabilityand otherwise set X t +1 = X t;

9 else

10 Set X t +1 = X t;

11 t ← t + 1;

12 Output the matching X T ;7D2

It is straightforward to verify that the Markov chain is ergodic, aperiodic, and irreducible, meaning that it converges to a unique stationary distribution. We can verify that the Glauber dynamics converges to the Gibbs distribution by checking that the detailed balance condition holds: For two matchings X and X ∪ { e }, we haveand thus

Furthermore, the convergence speed of an MCMC to its stationary distribution is characterized by its mixing time, defined as the time required by the Markov chain to have sufficiently small distance to the stationary distribution. Formally, let P t (X 0, ⋅) denote the distribution of matchings after t steps starting from X 0. The total variation distance between P t (X 0, ⋅) and the stationary distribution μ is defined asThus, we can define the mixing time of the Markov chain:

It is known that the Glauber dynamics for matchings has a polynomial mixing time:

---

### Fundamentals of research data and variables: the devil is in the details [^07fcd2d5]. Anesthesia and Analgesia (2017). Low credibility.

Designing, conducting, analyzing, reporting, and interpreting the findings of a research study require an understanding of the types and characteristics of data and variables. Descriptive statistics are typically used simply to calculate, describe, and summarize the collected research data in a logical, meaningful, and efficient way. Inferential statistics allow researchers to make a valid estimate of the association between an intervention and the treatment effect in a specific population, based upon their randomly collected, representative sample data. Categorical data can be either dichotomous or polytomous. Dichotomous data have only 2 categories, and thus are considered binary. Polytomous data have more than 2 categories. Unlike dichotomous and polytomous data, ordinal data are rank ordered, typically based on a numerical scale that is comprised of a small set of discrete classes or integers. Continuous data are measured on a continuum and can have any numeric value over this continuous range. Continuous data can be meaningfully divided into smaller and smaller or finer and finer increments, depending upon the precision of the measurement instrument. Interval data are a form of continuous data in which equal intervals represent equal differences in the property being measured. Ratio data are another form of continuous data, which have the same properties as interval data, plus a true definition of an absolute zero point, and the ratios of the values on the measurement scale make sense. The normal (Gaussian) distribution ("bell-shaped curve") is of the most common statistical distributions. Many applied inferential statistical tests are predicated on the assumption that the analyzed data follow a normal distribution. The histogram and the Q-Q plot are 2 graphical methods to assess if a set of data have a normal distribution (display "normality"). The Shapiro-Wilk test and the Kolmogorov-Smirnov test are 2 well-known and historically widely applied quantitative methods to assess for data normality. Parametric statistical tests make certain assumptions about the characteristics and/or parameters of the underlying population distribution upon which the test is based, whereas nonparametric tests make fewer or less rigorous assumptions. If the normality test concludes that the study data deviate significantly from a Gaussian distribution, rather than applying a less robust nonparametric test, the problem can potentially be remedied by judiciously and openly: (1) performing a data transformation of all the data values; or (2) eliminating any obvious data outlier(s).

---

### Input-output maps are strongly biased towards simple outputs [^50fca0dd]. Nature Communications (2018). Medium credibility.

Many systems in nature can be described using discrete input-output maps. Without knowing details about a map, there may seem to be no a priori reason to expect that a randomly chosen input would be more likely to generate one output over another. Here, by extending fundamental results from algorithmic information theory, we show instead that for many real-world maps, the a priori probability P(x) that randomly sampled inputs generate a particular output x decays exponentially with the approximate Kolmogorov complexity [Formula: see text] of that output. These input-output maps are biased towards simplicity. We derive an upper bound P(x)≲[Formula: see text], which is tight for most inputs. The constants a and b, as well as many properties of P(x), can be predicted with minimal knowledge of the map. We explore this strong bias towards simple outputs in systems ranging from the folding of RNA secondary structures to systems of coupled ordinary differential equations to a stochastic financial trading model.

---

### Advanced methods for gene network identification and noise decomposition from single-cell data [^c21c012b]. Nature Communications (2024). High credibility.

Rao-Blackwell method for solving the filtering problem

With modern time-lapse microscopes, scientists can measure the time trajectory of some intracellular species (e.g. fluorescent reporters) and use these measurements to infer the dynamical states of unobserved species (e.g. the gene state). This process, known as stochastic filtering for intracellular reaction systems, allows scientists to gain insights into unobserved chemical species. This in turn can lead to the development of improved control strategies for the reacting process. Here, we apply the proposed RB-CME solver to this filtering problem.

Mathematically, we can model the observation channels bywhere t i are the observation time points, Y (t i) is a vector of observations with each element corresponding to a particular light frequency, h (⋅) is a vector-valued function indicating the ideal relation between the measurement and the system state, W i are vectors of independent standard Gaussian noise, and Σ is a diagonal matrix indicating observation noise intensities. When the observations Y (t i) are one dimensional, we denote the 1 × 1 matrix Σ as σ. Stochastic filtering aims to compute the conditional probability distribution of the state X (t i) (for every i = 1, 2,…) given the observations up to time t i, i.e. Let us denote. By Bayes' rule, satisfies the following recursive formulaswhereis the density function of the distribution(usually called the likelihood function). We can interpret (8) as the prediction of the state at the next time point t i +1 using the observation up to the current time t i. We interpret (9) as the adjustment of the prediction according to the next observation. Note that the prediction step (8) is actually solving a CME withbeing the initial probability distribution andbeing the final solution. Consequently, the filtering problem can be seen as a combination of the usual CME and an adjustment step.

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^f050c80c]. Nature Communications (2025). High credibility.

Defining the general beta and gamma distribution

One limitation of the method so far is that the beta and gamma distributions are constrained to a given fixed range of values. The gamma distribution is constrained to the positive real numbers, with a zero lower bound. Meanwhile, the beta distribution is constrained to the real numbers within the interval [0, 1]. These constraints limit the variety of variables that can be modelled by the gamma and beta distributions. For this reason, here we define general variants of both distributions such that variables with any lower or upper bound can be modelled and δ can be retrieved. Rather than modifying the pdfs and cdfs directly to allow for any lower or upper bound, we defined methods for indexing the functions and for calculating the variance and mean for the distributions from observations of a variable (V) with any lower (L) or upper (U) bound.

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^cbffc27e]. CDC (2011). Medium credibility.

Appendix G. Mathematical formula for sample size — sample size required to detect the difference of GM between two subpopulation groups defines a z test with null hypothesis GM1 = GM2 or GMmax/GMmin = 1 and alternative GM1 ≠ GM2 or GMmax / GMmin > 1, where GMmax = max(GM1, GM2) and GMmin = min(GM1, GM2). It considers power W (say 80% chance) to detect that one group's GM is at least k fold higher and gives the p-value as p = Prob(Z ≥ z) = 1 − φ−1(z) with Z ~ Normal (0,1). Given α (e.g., 0.05) and target ratio k, power is W = Prob (Z > φ−1(1 − α) − log10(k)/(S√(1/n1 + 1/n2))), where S is the expected standard deviation of log10(VL) in the population of interest; when n1 = n2 = n, W = Prob (Z > φ−1(1 − α) − log10(k)/(S√(2/n))). The required sample size is n = 2[(φ−1(1 − α) − φ−1(1 − W))S/log10(k)]^2, and tables are provided for W = 0.8 and W = 0.9 with α = 0.05.

---

### Evolutionary conservation of within-family biodiversity patterns [^bd2ee6ad]. Nature Communications (2020). High credibility.

Fig. 2
Significant phylogenetic patterns in diversity responses and organismal traits.

a Time-calibrated phylogenetic tree of families, with the number of species and sampling plots in parentheses (the three major taxa are represented in a single tree for representative purposes only). b, d, f Scatterplots representing how dissimilarities in community responses or organismal traits, quantified as effect sizes or raw differences on the y -axis, increase with evolutionary time, represented by patristic distances in million years (Ma) in the x -axis; each point represents a pairwise comparison between families. c, e, g The density plots below each scatterplot represent, for the same variables as scatterplots, the distribution of the likelihood ratio statistics δ for a two model comparison, a null model with random draws independent of the phylogeny (white noise WN model, lighter density distributions) and a Brownian motion BM model of evolution (darker density distributions). The dashed vertical line indicates the observed value of δ when the models are fit to our data set. A total of 1000 replicates were used for each distribution. Peak-α: elevation at which species richness peaks. Edf-α effective number of parameters of the smoothing function, Slope z elevational turnover, R 2 z magnitude of elevational turnover. Source data are provided as a Source Data file.

Fig. 3
The complexity of species richness-elevation relationships increases for certain types of interactions.

a, b, c Scatterplots of the relationships between the parameters of the smoothing function Edf-α of confamilial communities, on the y -axis, and measures of the incidences of green algal endosymbiosis in lichens and of trophic interactions with plants in animals, on the x -axis. Each point represents a family. Source data are provided as a Source Data file.

---

### Prioritizing global conservation efforts [^9356929a]. Nature (2006). Excellent credibility.

One of the most pressing issues facing the global conservation community is how to distribute limited resources between regions identified as priorities for biodiversity conservation. Approaches such as biodiversity hotspots, endemic bird areas and ecoregions are used by international organizations to prioritize conservation efforts globally. Although identifying priority regions is an important first step in solving this problem, it does not indicate how limited resources should be allocated between regions. Here we formulate how to allocate optimally conservation resources between regions identified as priorities for conservation — the 'conservation resource allocation problem'. Stochastic dynamic programming is used to find the optimal schedule of resource allocation for small problems but is intractable for large problems owing to the "curse of dimensionality". We identify two easy-to-use and easy-to-interpret heuristics that closely approximate the optimal solution. We also show the importance of both correctly formulating the problem and using information on how investment returns change through time. Our conservation resource allocation approach can be applied at any spatial scale. We demonstrate the approach with an example of optimal resource allocation among five priority regions in Wallacea and Sundaland, the transition zone between Asia and Australasia.

---

### The distribution of common-variant effect sizes [^e354c84b]. Nature Genetics (2021). High credibility.

The genetic effect-size distribution of a disease describes the number of risk variants, the range of their effect sizes and sample sizes that will be required to discover them. Accurate estimation has been a challenge. Here I propose Fourier Mixture Regression (FMR), validating that it accurately estimates real and simulated effect-size distributions. Applied to summary statistics for ten diseases (average [Formula: see text]), FMR estimates that 100,000–1,000,000 cases will be required for genome-wide significant SNPs to explain 50% of SNP heritability. In such large studies, genome-wide significance becomes increasingly conservative, and less stringent thresholds achieve high true positive rates if confounding is controlled. Across traits, polygenicity varies, but the range of their effect sizes is similar. Compared with effect sizes in the top 10% of heritability, including most discovered thus far, those in the bottom 10–50% are orders of magnitude smaller and more numerous, spanning a large fraction of the genome.

---

### Stable between-subject statistical inference from unstable within-subject functional connectivity estimates [^791af2c6]. Human Brain Mapping (2019). Medium credibility.

2.2 Example case for a single pair of variables

Before coming to a complete description we consider a toy example to make the point above more concrete. We wish to assess if there is a linear relationship between two variables, a and b. The first one, a, with values a n, is Gaussian distributed (mean 0, standard deviation [SD] 1); the second one, b, is a corrupted version of a by the introduction of random noise:

where ε n are independent, Gaussian distributed random variables (mean 0, SD = 1), and κ ≥ 0. We generate replicates of b based on independent realizations of noise ε n and κ, where κ is randomly sampled from a uniform distribution between 0 and c. We choose c to define the expected strength of the relationship between a and b. We then run permutation testing on each data set. We evaluate the power of the permutation combining method to detect a relationship between a and b for different values of c > 0. Even when κ is randomly small on some replicates, it may be large on others (allowing to detect the underlying relationship in these cases).

For the purpose of illustration, we generated 1,000 data sets using N = 100, each with a different value of κ sampled from a uniform distribution and performed permutation testing for each of them. We repeat this for three different values of c: 0.0, 0.1, and 0.2. Figure 1 shows histograms of correlation coefficients between a and b across data sets (top), and histograms of p values (bottom). If the empirical distribution of p values is basically flat, as is the case when c = 0.0, then there is no evidence of a relationship between a and b. However, when c = 0.1 or c = 0.2, then the distribution of p values gets increasingly skewed toward zero despite the generally low correlations. Therefore, if a and b were experimental replications of some pair of unobserved processes, we could intuitively say that there are signs of correlation between these processes in the c = 0.1 and c = 0.2 cases. However, neither p mean or p gmean (data not shown in the figure) are below 0.05; they are higher than 0.2 in all cases, emphasizing again the point that p mean or p gmean are not p values and, thus, the need for a permutation procedure to learn their null distribution.

---

### Identifying domains of applicability of machine learning models for materials science [^72c4ba76]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ν that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by σ lin (x 1, x 2) ≡ −0.3 ≤ x 1 ≤ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: σ rbf ≡ −3.3 ≤ x 11 ≤ 3.1 ∧ x 2 ≤ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA σ poly (x 1, x 2) ≡ −3.5 ≤ x 2 ≤ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### Transient power-law behaviour following induction distinguishes between competing models of stochastic gene expression [^0a60715c]. Nature Communications (2025). High credibility.

Inferring the number of inactive gene states is difficult from steady-state data

In Fig. 2 the steady-state mRNA count distributions of thousands of rate parameter sets for the N = 3, 4, 5 state models are compared with those of their effective telegraph models (computed using the previously detailed three-step procedure). Note that to ensure biological relevance, the rate parameter ranges for each model were chosen to comfortably cover the bounds of the telegraph model rate parameters estimated using a range of eukaryotic data in various published studies (see Table 1 of ref.for these estimates and see Supplementary Note 2 for further details on generating the rate parameter sets).

Fig. 2
The steady-state mRNA count distribution of the N -state model can often be well fit by an effective telegraph model.

Thousands of rate parameter sets of the N -state model (N = 3, 4, 5) were sampled in the range relevant for eukaryotic gene expression. For each parameter set the mapping in Eq. (2) was used to obtain the rate parameters of an effective telegraph model. A – C P-P plots comparing the cumulative distribution function (CDF) of the N -state and effective telegraph models. These are constructed from the 500 rate parameter sets with the largest Wasserstein distance (WD) values between the distributions of the two models. Deviations from the line y = x (red) indicate differences between the distributions. D – G Typical steady-state mRNA count distributions of the 5-state (solid blue lines) and effective telegraph models (histograms) for four classes of distributions (Shape I: Unimodal with Fano factor < 2; Shape II: Unimodal with Fano factor ≥ 2; Shape III: Bimodal with one peak at zero and another at a non-zero value; Shape IV: Bimodal with peaks at non-zero values. H Boxplots displaying summary statistics of the WD between the steady-state distribution of mRNA counts of the N -state and effective telegraph model. The horizontal line and the box show the median and the interquartile range, respectively; whiskers extend to the 10th and 90th percentiles; sample sizes given in Supplementary Table 2. I – K Bar charts displaying the percentage of each distribution shape sampled across the parameter space. The agreement between the N -state and its effective telegraph model becomes worse as N increases and is worst for Shape IV because this shape is qualitatively impossible to capture by the telegraph model. See Supplementary Note 2 for implementation details.

---

### Statistical laws of stick-slip friction at mesoscale [^23513423]. Nature Communications (2023). High credibility.

Figure 2 shows the measured probability density function (PDF) P (f c) of the normalized maximal force, where 〈 F c 〉 andare, respectively, the mean and standard deviation of F c. It is seen that the measured PDFs P (f c) for the quasi-1D probe (black circles) and 2D probe (red triangles) overlap with each other, suggesting that the measured P (f c) is an intrinsic property of the stick-slip friction on the sandpaper and is not sensitive to the shape and dimensions of the scanning probe used. The two sets of data can be well described by the generalized extreme value (GEV) distribution, where z = (f c − μ)/ β is the standardized variable withand μ = β [1 − Γ(1 − ξ)]/ ξ being, respectively, the scale and location parameters of the distribution function, and Γ(x) is the gamma function. Equation (1) is used to model the distribution of extreme values in a sequence of independent and identically distributed random variables with zero mean and unity variance so that the normalized maximal force f c is used as the random variable in Eq. (1) (see SI Section II.D.1 for more details). The solid lines in Fig. 2 show the fits of Eq. (1) to the data points with only one fitting parameter ξ. For the data obtained with the quasi-1D probe (black circles), we find ξ = − 0.13 ± 0.06. For the data obtained with the 2D probe (red triangles), we find ξ = − 0.03 ± 0.05. For ξ = 0, Eq. (1) is reduced to the Gumbel distribution, which has an exponential tail at large values of z without an upper bound for the local maximal force. Evidently, the data shown in Fig. 2 are not far from this limit. Figure 2 thus demonstrates that the maximal force F c is a depinning force for local slips, which follows the extreme value statistics.

---

### Prior ground: selection of prior distributions when analyzing clinical trial data using Bayesian methods [^0094e5c2]. NEJM Evidence (2023). Medium credibility.

Increasingly, investigators are choosing to use Bayesian methods for the analysis of clinical trial data. Unlike classical statistical methods that treat model parameter values (such as treatment effects) as fixed, Bayesian methods view parameters as following a probability distribution. As we have written previously, 1 by analyzing clinical trial data using Bayesian methods one can obtain quantities that may be of interest to clinicians, providers, and patients, such as the probability that a treatment effect is more or less than 0, that is, the probability that a treatment is effective.

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^66f1bd1f]. Nature Communications (2023). High credibility.

CurveCurator yields well-calibrated p -values using a recalibrated F-statistic

The first step to assess the statistical significance of dose–response curves is to find the best possible fit given the measured dose–response values. As the optimization surface for sigmoidal curves is non-convex (i.e. a surface with many local minima), naïve curve fitting algorithms often get stuck in local minima, leading to suboptimal fits and, thereby, overly conservative p -values (Supplementary Fig S1). CurveCurator uses a heuristic that reaches the global minimum in almost all cases in a short period of time (Supplementary Fig. S2). To obtain an empirical null distribution, we simulated 5 million dose–response curves under the null hypothesis, i.e. curves where the response is independent of the dose (i.e. no dose-dependent regulation) for a range of n data points per curve (Fig. 2a). As expected, direct application of the classical F-statistic for linear models to these null dose–response curves yielded poorly calibrated p -values (Supplementary Fig. S3). CurveCurator solved this issue by optimizing both the F-value formula and the parameters of the F-distribution as a function of n to approximate these simulated null distributions accurately (Fig. 2 b, c). The validity of this approach was confirmed by noting that p -values in real experiments in which the vast majority of curves were expected to be unresponsive formed a uniform distribution of truly non-regulated curves plus a small distribution of truly regulated curves enriched at low p -values (Fig. 2d).

---

### Automated design of synthetic microbial communities [^9ce183a3]. Nature Communications (2021). High credibility.

Approximate Bayesian computation

Writing the likelihood function, f (x 0 ∣ θ), in terms of summary statistics can be difficult. We bypass this and approximate the posterior by generating data from a model. We can sample a parameter vector from the prior, θ * ~ π (θ), which is simulated to yield a data vector, x *. This can be written as a conditional, x * ~ f (x ∣ θ *), which also gives the joint density, π (θ, x). In order to obtain the posterior distribution that satisfies our objective behaviour, x 0, we apply a conditional to define whether a generated data vector, x *, belongs to the objective x 0.

If x = x 0 Else

Let ρ (x, x 0) be a distance function that compares a simulation to the objective. Using distance threshold, ϵ, we can define values below which the distance is acceptably small. We can redefine π (θ ∣ x, x 0) in the context of thresholds to obtain an approximation of the posterior.

If ρ (x, x 0) < ϵ Else

The smaller ϵ is and the larger the number of simulations conducted, the more accurate the representation of the true posterior will be. We can write this marginal posterior distribution as:

Model selection with ABC SMC

In this paper, we use a variant of ABC, ABC Sequential Monte Carlo (ABC SMC). Particles are sampled from the prior distributions. Each particle represents a sampled model and sampled parameters for that model. ABC SMC evolves particles sampled from the prior distribution through a series of intermediate distributions and perturbations. Importance weighting is used to define their sample probability for the next distribution. The distance threshold, ϵ, is decreased between distributions, moving the acceptance criteria closer to the objective. These features aim to improve the acceptance rate of particles while maintaining a good approximation of the posterior distribution (see Supplementary Algorithm 1 for more details).

---

### Emergent geometry and duality in the carbon nucleus [^86507865]. Nature Communications (2023). High credibility.

Fig. 2
Nuclear density distributions for several 12 C states using the SU(4) interaction.

a – f Results for the density distribution of the two inner angles of the triangle formed by the three alpha clusters for the, states respectively. The two axes are for the two inner angles θ 1 and θ 2 measured in degrees. g – l Results for the two-dimensional projection of the nuclear density for the, states respectively. In each case the orientation of the shortest root-mean-square direction is aligned with the x axis.

We now define a model-independent two-dimensional projection of the nuclear density for the states of 12 C. In order to construct this projection, we first identify the x axis as the direction with the smallest RMS deviation of the nucleon positions relative to the center of mass. For the nuclear states that we have already identified as having an equilateral triangular shape, we rotate the density configurations along the x axis so that one of the three clusters is pointing along the positive z direction. We then symmetrize with respect to 0°, 120° and 240° rotations about the x axis. For nuclear states that we have already identified as having an obtuse isosceles shape, we identify the z axis as the direction with the longest RMS deviation of the nucleon positions relative to the CM. We then rotate the density configurations along the z axis so that the alpha cluster closest to the CM lies on the positive y axis.

In the right panel of Fig. 2, we show the density distribution of selected states of 12 C for the SU(4) interaction. The density distributions obtained using the N2LO chiral interaction are presented in Fig. 3 and the Supplementary Information and are in excellent agreement with the SU(4) interaction results. The, andstates (see Supplementary Information for the last two) have similar intrinsic equilateral triangular shapes, consistent with an interpretation as members of a rotational band built on top of thestate. The, states (see Supplementary Information for the last one) have similar intrinsic obtuse isosceles triangle shapes and are consistent with belonging to a rotational band built on top of thestate. These findings are consistent with previous studies in the literature based on group theoretical considerations. We note that models where the Hoyle state has an equilateral triangle symmetry predict an additional 3 − and 4 − state in the Hoyle state rotational band.

---

### Frequency spectra and the color of cellular noise [^74505241]. Nature Communications (2022). High credibility.

Estimation of the Padé derivatives

We first consider the case s ℓ < ∞. Appealing to the ergodicity of the CTMC we can express the Padé derivativeaswhereis an independent random variable with Erlang distribution with shape parameter (m + 1) and rate parameter s ℓ. In other words, the probability density function ofis given byand we can viewas the sum of (m + 1) independent and identically distributed exponential random variables with rate parameter s ℓ. Noting that X n (T) andshall have the same mean and variance at stationarity we can rewrite (24) aswhere Var π (X n) is the stationary variance of the output species copy-number andis the steady-state expectation of the squared change in the output state in a time-period of length, i.e.

We now discuss how we can simultaneously estimate the steady-state expectation (25) for each m = 0, 1,…, (ρ ℓ − 1). For this, we augment the CTMC state with ρ ℓ additional state components, denoted by, and an extra reaction, calledthat fires at the constant rate of s ℓ. If this reaction fires at time t, then we reset these additional state components aswhere X n (t −) is the copy-number of the output species X n, just before the reaction firing time. Similarly for j ≥ 2, Y j (t) assumes the value of the previous state component before the jump time, which is Y j −1 (t −). Lettingbe the Erlang-distributed random variable mentioned above, for any T ≫ 1and we can expressas

Suppose we have Q simulated trajectories of the augmented CTMC denoted byfor q = 1,…, Q. Then we can simultaneously estimate eachwith the Monte Carlo (MC) estimatorwhere T c ≪ T f is the cut-off time at which stationarity is assumed to be reached and the initial part of each trajectory in the time-interval [0, T c] is discarded. Observe that if T f is large enough then even a single trajectory (i.e. Q = 1) is sufficient for this estimation due to Birkhoff's Ergodic Theorem. However, using multiple trajectories enhances the MC estimator's statistical accuracy which can be measured by estimating its sample variance. Based on Q CTMC trajectories the output variance Var π (X n) can be estimated as

---

### A fluorescent-protein spin qubit [^ed00d7e8]. Nature (2025). Excellent credibility.

Simulation of magnetic resonance spectrum

Solving for the eigenvalues of the Hamiltonian from equation (1) provides the T x –T z, T y –T z and T x –T y transition frequencies as a function of the magnetic field (B). Importantly the transition frequencies depend not only on the field strength but also on the molecule's orientation relative to the magnetic field. Assuming that the EYFP molecules are randomly oriented, we sample 10,000 uniformly distributed orientations. To simulate the ODMR spectra in Fig. 2b, c, we incorporate the single-molecule ODMR linewidth (γ). The resonance for a single EYFP molecule is then modelled by a Lorentzian of the formwhere a x − z and a y − z denote the ODMR contrast, and ω x – z and ω y – z denote the transition frequencies for the T x –T z and the T y –T z transitions, respectively. It is noted that our model simplifies the fitting by only considering the T x –T z and T y –T z resonance while omitting the T x –T y transition, which has a reduced ODMR contrast and experimental data that overlap with a harmonic of our signal generator. We iteratively optimize the fit parameters D, E, a x − z, a y − z and γ by minimizing the cost function, wheredenotes the experimentally observed ODMR spectra at the following fields: 2.1 mT, 4.5 mT, 6.1 mT, 8.2 mT and 10 mT. We find D = (2π) × (2.356 ± 0.004) GHz, E = (2π) × (0.458 ± 0.003) GHz, a x − z = (0.17 ± 0.02), a y − z = (0.129 ± 0.008) and γ = (2π) × (33 ± 4) MHz.

---

### Bifurcation behaviors shape how continuous physical dynamics solves discrete ising optimization [^5aeb0a89]. Nature Communications (2023). High credibility.

Fig. 3
Trapped nodes and swing nodes that emerge during Ising dynamics and their impact on optimization.

a, b Schematic illustrations of the evolution of the state corresponding to a global minimum point x * (p) following Eq. (5) in Ising problems for which (a) p ✱ = p 0 (exact mapping arises right after first bifurcation), and (b) p * > p 0 (outlier). Outliers are accompanied by swing nodes, which bifurcate at higher pump rates (p 1. p *) during the evolution (retarded bifurcations). In the corresponding graphs blue(red) edges denote G i j = −1(+1), respectively. The nodes are colored according to the sign and magnitude of the steady-state components of the CIM dynamics. c, d Normalized counts of the magnitudes of the components of the maximum eigenvectors for models with (c) p ✱ = p 0 and (d) p * > p 0, computed from statistics of 1000 random instances with n = 5 variables. The brown curves are the kernel density estimations of the probabilities. e The plot shows the probability that a variable with magnitude proportional toat the first bifurcation changes its sign during subsequent evolutions of the CIM, sampled from 100 fully connected graphs with n = 100 variables, and edge weights picked at random from a Gaussian distribution with zero mean and standard deviation. The inset (similar to Fig. 2) visualizes the evolution of several nodes in a typical 100-node graph with trapping and oscillations explicitly shown.

The empirical observations made in Fig. 3 a, b guide us toward rigorous analytical results which we present in the following. Given the first non-trivial stable state, we quantify the degree of synchronization bywhere we denote σ 1 = sign(x 1). Note thatand if the components of x 1 are more centralized away from zero, the larger the value of α 2 (x 1). Then for any Ising Hamiltonian H (σ) (Eq. (1)), we have the following theorem.

---

### Observation and theory of X-ray mirages [^6bb03466]. Nature Communications (2013). Medium credibility.

Numerical simulations

As already noted, the elaborated theory of the mirage and the mirage equation itself are written for an arbitrary-extended optically inhomogeneous amplifying medium, in particular, for any spatial distribution of n e (x, y, z) and G (x, y, z) of the plasma. For the solution of the whole problem, that is, for calculation of mirage radiation at the output of the plasma and for finding the position and shape of the virtual source in a particular physical situation, it is enough to numerically integrate equations (2 and 3) for given n e (x, y) and G (x, y). To do this, we rewrite the original equation (2) as:

whereand the complex amplitude of an incident field A inc (x, y, z) is considered to be given and the complex amplitude of X-ray mirage A M (x, y, z) = 0 at z = 0. In our opinion, the most effective numerical method for solving such an equation is the split-step method, which we applied for our calculations. For numerical solution of this parabolic equation, we used a rectangular grid of N x × N y elements with the spatial step sizes Δ x and Δ y in the x and y directions correspondingly. Decomposition of the total field into two fields, A M (x, y, z) and A inc (x, y, z), has allowed us to have a limited area where A M (x, y, z) is non-zero and also allowed to use the periodic boundary conditions of the split-step method.

In numerical calculations, we used a diffraction limited Gaussian beam A G (x, y, z) = as an incident radiation A inc (x, y, z). As equation (3) is a special case of equation (2), we used the same numerical procedure for its solution.

The calculation scheme is graphically shown in Supplementary Fig. S3. For the main calculations, we used the computational volume of 65 × 68 × 6,000 μm 3 size with 640 × 670 elements in x − y computational grid. The number of steps along z axis was 670. For the calculations according to the simplified model, described in Supplementary Discussion, the numbers were 200 × 200 × 6,000, 1,024 × 1,024 and 1,024 correspondingly.

---

### Resolution limit of image analysis algorithms [^a158c4b0]. Nature Communications (2019). High credibility.

Superresolution image acquisition

Buffer solutions in which the BS-C-1 cell samples were prepared were replaced with an imaging buffer immediately prior to the imaging of each sample. This imaging buffer consisted of 50 mM cysteamine, 0.5 mg mL −1 glucose oxidase, 40 μg mL −1 catalase, and 10% glucose (w/v) in PBS, pH 7.4. (All buffer ingredients were purchased from Thermo Fisher Scientific.) The imaging buffer was freshly prepared for each acquisition. A coverslip was placed over the sample and sealed using vacuum grease. For each acquisition, the sample was briefly illuminated (approximately 5–10 min) using the maximum-power setting of the 635 nm laser to switch off most of the Alexa 647 fluorophores and achieve sparse single-molecule activation conditions. Acquisition was initiated once steady-state, single-molecule blinking was observed with periodic illumination using the 405 nm laser to reactivate fluorophores and maintain sufficient activation density. The activation laser power was kept low throughout the acquisition. For each dataset, typically 50,000 images of stochastically activated fluorophores were acquired with 50 ms exposure time for each image and without EM gain resulting in an acquisition rate of 20 frames per second.

Generating completely spatially random locations

For a completely spatially random distribution of locations in an S μm × S μm region, the (x, y) coordinate for each location was obtained by drawing realizations of independent random variables X and Y, each uniformly distributed with probability density functions p X (x) = p Y (y) = 1/ S, 0 ≤ x, y ≤ S. For the simulated image in Fig. 2, S = 50 resulting in a 50 μm × 50 μm region.

Generating locations for deterministic structures

Deterministic structures consist of D molecules located at evenly spaced points on the circumference of a circle of radius r. The location of the d th pointis given byand, where d = 1,…, D.

---

### Childhood to adulthood: accounting for age dependence in healthy-reference distributions inXe gas-exchange MRI [^843ea1d8]. Magnetic Resonance in Medicine (2023). Medium credibility.

3.4.2 Modeling 129 Xe gas‐exchange distributions

To generate the distributions for the LB and GLB methods (which were used for comparison to the GAMLSS model), 2000 voxel intensities were selected from inside the lung mask of each subject and combined into a single vector(length:) for each gas‐exchange metric. Then, aand adistribution was fit to, corresponding to the LB and GLB methods, respectively. Because these models have no dependence on age or other explanatory variables, the link functions were all set to 1, resulting in a constant distribution for each method.

To generate an age‐dependent GAMLSS model, adistribution was fit to the vector, with the vector of subject ages, set as the explanatory variable. Using the effect sizes of the prior age‐dependence assessment analysis, the link functions versus age were set to: "1" if a distribution parameter had no dependence on age (R 2 < 0.04); or a smoothing function if there was some dependence on age (R 2 0.04). P‐splines (denoted pb‐spline in R) were used as the smoothing functions. To prevent overfitting, the smoothing functions were penalized using the generalized Akaike information criterion, where the applied penalty per degree of freedom was set to 3.84, corresponding to the chi‐square value for a single degree of freedom with a P value of 0.05.

After using GAMLSS to fit, the z‐score bins from the GAMLSS‐binning model were visually compared to those of the LB model:and the GLB model: To do this, z‐scores were collected for all voxels (i.e. all inside the lung mask), in all subjects, and for each gas‐exchange metric. Histograms of the z‐scores were plotted, with an overlaid density curve for a normal distribution. Ideally, the distribution of z‐scores in the density plot would match the shape of the normal curve (e.g. 2.27% of data would distribute into z−2, 13.59% of data would distribute into −2z−1, and 34.13% would distribute into −1z0).

---

### Scalable spatiotemporal prediction with Bayesian neural fields [^fea8687d]. Nature Communications (2024). High credibility.

Prediction queries

We can approximate the posterior (13) using a set of samples, which may be obtained from either MAP ensemble estimation or stochastic variational inference (by sampling from the ensemble of M variational distributions). We can then approximate the posterior-predictive distribution(which marginalizes out the parameters Θ) of Y (r *) at a novel field index r ✱ = (s *, t *) by a mixture model with M equally weighted components:Equipped with Eq. (23), we can directly compute predictive probabilities of events { Y (r *) ≤ y }, predictive probability densities { Y (r *) = y }, or conditional expectationsfor a probe function. Prediction intervals around Y (r *) are estimated by computing the α -quantile y α (r *), which satisfiesFor example, the median estimate is y 0.50 (s *, t *) and 95% prediction interval is [y 0.025 (s *, t *), y 0.975 (s *, t *)]. The quantiles (24) are estimated numerically using Chandrupatla's root finding algorithmon the cumulative distribution function of the mixture (23).

Temporal seasonal features

Including seasonal features (c.f. Eq. (8)), where possible, is often essential for accurate prediction. Example periodic multiples p for datasets with a variety of time units and seasonal components are listed below (Y = Yearly; Q = Quarterly; Mo = Monthly; W = Weekly; D = Daily; H; Hourly; Mi = Minutely; S = Secondly):
Q: Y = 4
Mo: Q = 3, Y = 12
W: Mo = 4.35, Q = 13.045, Y = 52.18
D: W = 7, Mo = 30.44, Q = 91.32, Y = 365.25
H: D = 24, W = 168, Mo = 730.5, Q = 2191.5, Y = 8766
Mi: H = 60, D = 1440, W = 10080, Mo = 43830, Q = 131490, Y = 525960
S: Mi = 60, H = 3600, D = 86400, W = 604800, Mo = 2629800, Q = 7889400, Y = 31557600

---

### Disentangling dispersion from mean reveals true heterogeneity-diversity relationships [^428bde48]. Nature Communications (2025). High credibility.

Derivation of the generalised δ formulation

Lastly, we introduced a generalised formulation of δ that is adaptive to the number of boundaries present for the variable being measured (Equation (1)). When the upper and lower bound are real numbers, δ (Equation (1)) becomes δ 2 (Equation (33)), and when the variable has only a real number lower bound, δ (Equation (1)) becomes δ L (Equation (37)), each with their associated proofs in previous sections. Here we provide proofs of δ for upper bounded variables and unbounded variables.

For upper bounded variables there exists no commonly used model distribution. The solution, however, was simply to use a conceptually flipped gamma distribution, where the lower bound of zero was instead used as an upper bound. This was done using same approach of rescaling the variable as was done for lower bounded variables. The target is, thus, to rescale a given observation (v) of a variable (V) with any real number upper bound (U) to retrieve x for indexing the gamma pdf and cdf (Equation (20); Equation (28)). This can be done withEssentially, v is rescaled such that its maximum is equal to zero. Similarly, the expected value or mean of the variable (E(V)) can be scaled such that its maximum is zero:where E(X) is equivalent to μ for the gamma distribution (defined in Equation (21)). Much like the use of the general gamma distribution for lower bounded variables, the variance of the upper bounded variable (Var(V)) remains unchanged when converted to the variance of a gamma distributed variable. As a result, the mean-independent measure of dispersion for upper bounded (U) variables, δ U, can be retrieved withThus, upper bounded variables can also be modelled using the generalised δ. In the main text Var(V) is denoted σ 2 and E(V) is denoted μ.

For unbounded variables the Gaussian/normal distribution is the most obvious model. The Gaussian distribution is defined by a parameter for central tenancy (μ) and dispersion/scale (σ 2). Where both parameters are estimated independently for an unbounded variable with the mean (E(V)) and variance Var(V). Thus, the distribution's dispersion is the variance itself (δ 0 = Var(V) = σ 2).

---

### Scale-free networks are rare [^9c1c1630]. Nature Communications (2019). High credibility.

Real-world networks are often claimed to be scale free, meaning that the fraction of nodes with degree k follows a power law k -α, a pattern with broad implications for the structure and dynamics of complex systems. However, the universality of scale-free networks remains controversial. Here, we organize different definitions of scale-free networks and construct a severe test of their empirical prevalence using state-of-the-art statistical tools applied to nearly 1000 social, biological, technological, transportation, and information networks. Across these networks, we find robust evidence that strongly scale-free structure is empirically rare, while for most networks, log-normal distributions fit the data as well or better than power laws. Furthermore, social networks are at best weakly scale free, while a handful of technological and biological networks appear strongly scale free. These findings highlight the structural diversity of real-world networks and the need for new theoretical explanations of these non-scale-free patterns.

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^77e75c38]. Nature Communications (2023). High credibility.

Fig. 2
CurveCurator achieves well-calibrated p -values using a recalibrated F-statistic.

a To obtain well-calibrated p -values, we simulated and evaluated 5 million curves for various n doses (here illustrated for n = 10) per curve under the null hypothesis (no dose-dependent response). F- and p -values were obtained for both the standard approach for linear models and the recalibrated F-statistic. b log–log plots of F-values vs. p -values from the recalibrated F-statistic for different numbers of doses n (different colors) based on 5 million random simulations. The recalibrated, parameterized F-distributions (dashed line) approximate the simulated null distributions (solid line) well. c Histogram of p -values for 5 million H 0 -simulations (n = 10) using the recalibrated F-distribution. The red line indicates the expected uniform density at 1.0. d Histogram of p -values from the recalibrated F-statistic for experimental data with few expected regulations (3 Dasatinib replicates in K562, n = 10, from Zecha et al.). The p -value distribution of the true negatives follows a uniform distribution. The curves above the red line are presumably truly regulated by Dasatinib. Source data for (b, d) are provided as a Source Data file.

---

### Trial-history biases in evidence accumulation can give rise to apparent lapses in decision-making [^7ea7aa32]. Nature Communications (2024). High credibility.

Drift diffusion model of decision-making

We use a standard formulation of sequential decision-making, in which an agent is faced with a stream of noisy sensory evidence ϵ 1: t coming from one of two hypotheses H 1 and H 2. The agent has to decide between sampling for longer or choosing one of two actions L, R (reaction time regime) or has to choose one of two actions after a fixed amount of evidence (fixed duration regime). Such a problem can be formulated as one of finding an optimal policy π t in a partially-observable markov decision process, whose solution can be written as a pair of thresholds on the log-posterior ratio, where g (t) = p (H 1 ∣ ϵ 1: t):

The log posterior ratio can be further broken down into a sum of log prior ratios and log-likelihood ratios, using Bayes rule:The optimal policy can equivalently be expressed in terms of the prior and sum of momentary sensory evidence x (t) = ∑ t ϵ t, which are sufficient statistics of the posterior. In the continuous time limit, when the average rate of evidence increments or drift rate is μ, and the standard deviation of sensory noise is σ, this corresponds to a drift-diffusion model that terminates when it reaches one of two boundsand whose initial state I is proportional to the log prior ratio:In this case, the probability of choosing rightward actions, i.e. hitting the upper bound can be written analytically as follows (derived from ref.):

In cases where trial difficulties (and hence drift rates) vary from trial to trial the optimal policy includes time-dependent, collapsing bounds on the posterior. However, under certain circumstances, constant bounds on X t = ∑ t ϵ t implement close-to-optimal collapsing bounds on the posterior, which is the regime we assume for our analysis.

---

### Observation and theory of X-ray mirages [^56122573]. Nature Communications (2013). Medium credibility.

Within the formulated problem, the beam propagation in an extended optically inhomogeneous amplifying medium can be described by the wave equation, which can be rewritten in the paraxial approximation for slowly varying amplitudes E (x, y, z, t) = 1/2{ A (x, y, z) e i (ωt − kz) + c.c. } as:

where A (x, y, z) is a complex amplitude of the X-ray electric field, propagating in z direction; ε (x, y, z) is the dielectric permittivity of a medium at the frequency ω, the imaginary part of which models the gain. For describing the interactions of X-rays with the amplifying plasma, it is enough to use the semi-classical approximation, which leads to a self-consistent system of the Maxwell–Bloch equations, consisting of the Maxwell's wave equation (equation (1) in our case) and the constitutive equations describing the dynamics of a two-level medium. In our model, equation (1) is written for Fourier components of the field at the frequency ω. As it is known, in the Fourier representation constitutive equations become independent on time. As a result, the amplifying medium can be described by the effective dielectric permittivity (see, for example, Dorofeenko et al.). If the radiation frequency coincides with the transition frequency, as in the case of amplification of X-ray radiation in the plasma, the effective dielectric permittivity can be written as ε (x, y, z) = 1−. Here n e (x, y, z) is the electron density of the plasma; n c is the critical electron density of the plasma; G (x, y, z) = G 0 g (x, y, z) is a small-signal gain distribution; x, y, z are Cartesian coordinates; λ is the wavelength; and β is the parameter characterizing the gain saturation, which depends on an off-diagonal element of the transition dipole moment and on the characteristic relaxation times for polarization and population inversion. The form of the real part of ε was chosen in accordance with the data presented in Gasparyan et al. In numerical calculations, we shall also consider the plasma is uniform along z axis. The last assumption means that the functions n e (x, y) and G (x, y) depend only on x, y and do not depend on z. We also assume that x axis is perpendicular to the target surface and y axis is parallel to the target surface.

---

### Massive computational acceleration by using neural networks to emulate mechanism-based biological models [^7944212b]. Nature Communications (2019). High credibility.

To capture stochastic aspects of the Rb-E2F signaling pathway, we adopted the Chemical Langevin Formulation (CLF). We adjusted the units of the molecule concentrations and the parameters so that the molecules are expressed in molecular numbers. Where X i (t) represents the number of molecules of a molecular species I (i = 1,…, N) at time t, andis the state of the entire system at time t. X i (t) interact through a set of M reactions. a j [X (t)]dt (j = 1,…, M) describes the probability that the j th reaction will occur in the next infinitesimal time interval [t, t+dt), and the corresponding change in the number of individual molecules produced by j th reaction is described in v ji.andare temporally uncorrelated, statistically independent Gaussian noises. This formulation retains the deterministic framework (the first term), and intrinsic noise (reaction-dependent) and extrinsic noise (reaction-independent). The concentration units in the deterministic model were converted to molecule numbers, so that the mean molecule number for E2F would be ~1000. We assumed a mean of 0 and variance of 5 for, and a mean of 0 and variance of 50 for. The resulting SDEs were implemented and solved in Matlab. Serum concentration is fixed at.

Twenty-four parameters of the SDE model are generated randomly at the range provided at Supplementary Table 5. The range covers almost all the possible rates that can be found in vivo. For each of the generated combination of parameters, sample 10 4 stochastic simulations and collect the final values of all 10 variables for each of the simulation. Each of the variables are discretized into 1000 intervals. Create a kernel distribution object by fitting it to the data. Then by using Matlab function pdf to get the probability density function of the distribution object, evaluated at the values in each of the discretized interval. (Matlab code can be found on GitHub).

---

### Fast variable density poisson-disc sample generation with directional variation for compressed sensing in MRI [^298c76ee]. Magnetic Resonance Imaging (2021). Medium credibility.

We present a fast method for generating random samples according to a variable density poisson-disc distribution. A minimum parameter value is used to create a background grid array for keeping track of those points that might affect any new candidate point; this reduces the number of conflicts that must be checked before acceptance of a new point, thus reducing the number of computations required. We demonstrate the algorithm's ability to generate variable density poisson-disc sampling patterns according to a parameterized function, including patterns where the variations in density are a function of direction. We further show that these sampling patterns are appropriate for compressed sensing applications. Finally, we present a method to generate patterns with a specific acceleration rate.

---

### Guidelines for validation of next-generation sequencing-based oncology panels: a joint consensus recommendation of the Association for Molecular Pathology and college of American pathologists [^01148c3a]. The Journal of Molecular Diagnostics (2017). Medium credibility.

Nonparametric tolerance intervals for non-normal distributions: The above estimate of the tolerance interval would only be applicable to a population that is normally distributed, but when the underlying population is often not normal (eg, when there is a natural boundary that the data cannot exceed (ie, 0% or 100%)) it is helpful to define tolerance intervals using nonparametric methods; the one-sided nonparametric tolerance interval can be determined by finding the value for k that satisfies the cumulative binomial equation, where CL is the confidence level (eg, 0.95), and by setting k = 0 (ie, 0 failures) the formula can be simplified.

---

### Bayesian strategy selection identifies optimal solutions to complex problems using an example from GP prescribing [^103d3a03]. NPJ Digital Medicine (2020). Medium credibility.

The algorithm begins by assuming a stochastic distribution of reward for each arm, to be initially non-informative — each is a Gaussian with a mean reward of 0 with a standard deviation of 0.3. This is because we expect the reward to lie in the interval [−1,1]. At each iteration, based on the exploitation-exploration trade-off policy and the current reward distributions of arms, an arm is chosen. We choose 13 arms (strategies), one for each clinic. For each clinic, the chosen strategy is implemented and used by all GPs in that clinic. It is important to note that a strategy may be played in more than one clinic and may also be repeated over time to reduce the uncertainty over its reward.

The outcome for each clinic for a particular strategy at the end of the intervention week is the fraction of the number of conversations (for all GPs) that included a discussion of physical activity, y, out of all the conversations where a discussion of physical activity could be relevant, n. Conversations deemed irrelevant included difficulties with a disability, patients who were infants, and patients with a drug use problem. The outcome of strategy i at clinic c, in week τ of the trial is thus given by (see Eq. (1)):The reward for a strategy for a particular clinic in week τ is then computed as the difference between the outcome for the tested strategyand the outcome for the baseline strategy for that clinic,(see Eq. (2)):Formally, the algorithm then proceeds as follows: Let μ i denote the mean reward of the i-th arm (strategy). We use a prior distribution on μ as p(μ i) = , where we select m 0 = 0 for each strategy to start with a prior belief that each strategy is the baseline strategy. The standard deviation s 0 is set to 0.3 to cover the possibilities of mean reward μ i to lie between [−1, 1] with high probability (greater than 0.999). We also assume the reward observation model to be Gaussian distributed as, whereis an i.i.d. Gaussian random variable with zero mean and standard deviation 1. The data collected from all the clinics up to week t is denoted asfor all c, wheredenotes the strategy used in week τ at clinic c. After observing data D t, the posterior distribution of μ i is given aswith mean m t and standard deviation s t given as (see Eq. (3)):

---

### Unbiased estimation of odds ratios: combining genomewide association scans with replication studies [^f6c708fa]. Genetic Epidemiology (2009). Low credibility.

METHODS

TWO-STAGE UNBIASED ESTIMATION

We assume a design consisting of a genomewide association scan (stage 1) followed by a replication study (stage 2) in which only SNPs meeting some selection criteria in stage 1 are of interest. Let X = { X i, i = 1,…, k } be the estimated effect sizes for k SNPs in stage 1, assumed to follow independentdistributions. In the case-control design X i will usually be log odds ratios. μ i is unknown but its variance, although estimated in practice, will be assumed known. Denote the ordered stage 1 effects as X (i) and define μ (i) = μ j when X (i) = X j. Note that we define μ (i) as the mean of the i th ranked effect, not the i th ranked mean, and that therefore μ (i) is a function of X. Define σ 1,(i) similarly. Let ϒ i be the stage 2 estimate of the i th ranked effect in stage 1, so ϒ i. The problem is to estimate μ (i) subject to selection criteria including specified values of i and a P -value threshold.

---

### Observation and theory of X-ray mirages [^02a86493]. Nature Communications (2013). Medium credibility.

To calculate a mirage according to equations (2 and 3), we need to know the functions n e (x, y) and G (x, y). Because these functions cannot be measured experimentally, we used the results of hydrodynamic calculations of these distributions presented in Berrill et al. We digitized the coloured online pictures Fig. 2d from Berrill et al.where two-dimensional spatial distributions of the electron density and small-signal gain at the time delay of 202 ps after the peak of the main prepulse are shown. In our calculations, we used the distribution of the electron density n e (x, y) obtained in this way without any changes. As for the gain distribution G (x, y), we used only the spatial profile g (x, y) and left G 0 as a fitted parameter because the absolute value of the gain obtained in Berrill et al.does not exactly match the plasma used in our experiments. The second fitted parameter in our calculations was the saturation coefficient β. To adjust the parameters, we solved equation (1) that describes the amplification of seeded beam in a plasma amplifier. Varying G 0 and β parameters, as well as X-ray intensity at the input of the amplifier, we sought to achieve consistency between the calculated and experimental energy characteristics of our X-ray amplifier. Coincidence has been achieved at β ~670 nJ cm −2, gain coefficient G 0 ~36 cm −1 and the energy of the seeded beam W 0 ~770 nJ. Comparison of the theoretically calculated saturation curve with the experimental data from Nishikino et al.is shown in Fig. 4c. It is seen that there is good agreement of the theoretical curve and the experimental data in the entire range of output amplifier energy. To ensure with greater certainty that our theory correctly describes the amplification of an injected pulse in the plasma amplifier, we calculated the angular spectrum of the output radiation for the selected above plasma parameters. The idea was to compare the calculated beam divergence, which is determined by the angular spectrum, with the experimentally measured divergence. The angular spectrum is shown in Fig. 4d. The beam divergence over x and y axes calculated as the full width at half-maximum was found to be ~2.4 × 0.6 mrad 2. This is in full accordance with the experimentally measured beam divergence estimated from the beam image profile presented in Fig. 2a. Agreement of calculated and experimentally measured energies, as well as angular characteristics of the amplified radiation, verifies the adequacy of the proposed theoretical model. Besides, the obtained agreement shows that the chosen spatial distributions of the electron density and small-signal gain correctly describe optical properties of our plasma. The adjusted n e (x, y) and G (x, y) distributions are shown in Fig. 4a. For calculation of the mirage radiation, equations (2) and (3) were numerically integrated. The results of such simulations are presented in Fig. 4e, showing the appearance of a virtual X-ray point source in the centre of the plasma amplifier. Its location coincides, to within measurement errors, with its supposed position obtained by processing the experimentally observed fringes. Figure 4e shows the intensity profile of the virtual source beam at the output of the plasma and Fig. 4f shows its x−z cross-section inside the plasma. It is seen that the virtual source is well formed and is close to a point one with a waist diameter of ~2.5 μm. We can see a slight tilt of the beam to the z axis equal to ~1 mrad in the vicinity of the waist. This tilt is caused by the presence of an electron density gradient in the x direction, which leads to deflection of radiation towards the lower electron density. The above results demonstrate the formation of a mirage in a typical plasma with very complex profile. Both refraction and diffraction processes were taken into account. The additional information about the properties of X-ray mirage formation is given in Supplementary Discussion.

---

### A continuous-time maxSAT solver with high analog performance [^87d3d16d]. Nature Communications (2018). Medium credibility.

Algorithm description

Here, we give a simple, nonoptimized variant of the algorithm (see flowchart in Supplementary Fig. 2). Better implementations can be devised, for example with better fitting routines, however the description below is easier to follow and works well. Given a SAT problem, we first determine the b parameter as described previously. Step 1: initially we set, Γ min, = and t max. Unless specified otherwise, in our simulations we used Γ min = 100, Γ max = 2 × 10 6, t max = 50. Step 2: to initialize our statistics, we run Γ min trajectories up to t max, each from a random initial condition. For every such trajectory ω we update the p (E, t) distributions as function of the energies of the orthants visited by ω. We record the lowest energy value found. Step 3: starting from Γ = Γ min + 1 and up to Γ max, we continue running trajectories in the same way and for each one of them check: (a) If, set, update p (E, t) and go to Step 4. (b) If Γ just reached, go to Step 4. (c) If Γ = Γ max, output "Maximum number of steps reached, increase Γ max ", output the lowest energy value found, the predictedand the quality of fit for, then halt. Step 4: using the p (E, t) distributions, estimate the escape rates κ (E) as described in the corresponding Methods section. Step 5: the κ (E) curve is extrapolated to the E − 1 value obtaining κ (E − 1) and then using this we predict(as described in another Methods section). Further extrapolating the κ (E) curve to κ = 0 we obtain(see the corresponding Methods section). Step 6: we check the consistency of the prediction defined here as saturation of the predicted values. We call it consistent, ifhas not changed during the last 5 predictions. If it is not consistent yet, we continue running new trajectories (Step 4). If the prediction is consistent, we check for the following halting conditions: (i) Ifthen we decide the global optimum has been found:and skip to Step 7. (ii) If the fitting is consistently predicting(usually it is very close,) we check the number of trajectories that has attained states with, i.e. = . If it is large enough (e.g. > 100), we decide to stop running new trajectories and setand go to Step 7. (iii) Ifthen we most probably have not found the global optimum yet and we go to Step 4. We added additional stopping conditions that can shorten the algorithm in case of easy problems, see Methods corresponding section, but these are not so relevant. Step 7: the algorithm ends and outputs, values, the Boolean variables corresponding to the optimal state found, along with the quality of fit.

---

### Hippocampal spatial representations exhibit a hyperbolic geometry that expands with experience [^8bc05600]. Nature Neuroscience (2023). High credibility.

Fisher information of model neurons

We modeled the neurons to have Gaussian tuning: with stimulus s = [x, y] as a 2D random variable taking values uniformly from [0, 1] × [0, 1]. For each neuron, we sample A i ∼ unif [5,25], μ i ∼ unif ([0, 1] × [0, 1]) and Σ i = R(θ i)diagR(θ i) T, where R(θ i) is the rotation matrixwith θ i ∼ unif [0, 2 π], and σ i 1 and σ i 2 are i.i.d. ∼ exp(− ζσ), with ζ denoting the curvature of the representation geometry, which dictates the decay rate of the exponential distribution, and ζ −1 is the mean of the distribution. For uniform distribution of place field sizes, σ i 1 and σ i 2 are i.i.d. ∼ unif [0, 2 ζ −1] to keep the same mean as in the exponential case.

We also assume the neurons to have independent Poisson variability. Thus, given a stimulus s, the number of spikes fired by a neuron is r i | s ∼ Poisson (f i (s)). These neurons' responses then encode a posterior distribution on s: p (s | r 1,⋯, r N). For each of 10,000 iterations, a stimulus s is randomly generated ∼ unif ([0, 1] × [0, 1]) with A i, μ i and Σ i all randomly generated again with the above distributions. The Fisher information matrix I (s) is computed by definition:which, for the independent neuron model as we assume here, simplifies to:

So, with Fisher information matrix I (s) computed each time, we can compute the average magnitude of the 10,000 Fisher information matrices, defined as det(I (s)) 0.5. Second-order polynomial is used for fittings, and the optimal size of the hyperbolic representation is determined as the R max under the peak of the fitting polynomial.

---

### Integrating spatial and single-cell transcriptomics data using deep generative models with spatialScope [^21b8b0cd]. Nature Communications (2023). High credibility.

Consider the vanilla score matching problem which aims to find a neural network s θ (x) to approximate, where θ represents the parameter set of the neural network. The challenge of vanilla score matching comes from the fact that high dimensional data x often tends to concentrate on a low dimensional manifold embedded of the entire ambient space. For data points not on the low-dimensional manifold, they would have zero probability, the log of which is undefined. Moreover, the score function can not be estimated accurately in the low density region. Fortunately, these challenges can be addressed by adding multiple levels of Gaussian noise to data. The perturbed data with Gaussian noise will not concentrate on the low-dimensional manifold, and the multiple levels of noise will increase training samples in the low-density region. Specifically, a sequence of data distributions perturbed by L levels of Gaussian noise is given as, where x (l) represents a sample perturbed by the noise level. To learn the score function, we consider the following problem, whereis the weight for noise level l and. Based on Equation (7), the score function can be estimated by stochastic gradient methods. Let x (l, t) be the t -sample at level l. We run Langevin dynamics in Equation (8) from l = L to l = 1 with initialization x (l, t = 1) = x (l +1, t = T). In the meanwhile, we progressive reduce of noise level σ l and decrease the step size η :where. Then the obtained samples x (l, t) at level l = 1, t = 1,…, T, will approximately follow the target distribution p (x) because σ 1 ≈ 0.

---

### A model for the fragmentation kinetics of crumpled thin sheets [^a2bca16c]. Nature Communications (2021). High credibility.

As a preliminary step, we derive the final displacement of the strip when folded at each break, in the absence of confinement. This problem can be mapped to the displacement of a walker performing a one-dimensional random walk with gamma-distributed steps. To enforce the concept of folding, the walker's steps occur in alternating directions. The distribution f Z (z) of position Z after 2 k steps accurate for all k is derived in full in Supplementary Note 2. However, the salient trends may be likewise observed by applying the central limit theorem and considering the position Z valid for large k, or small step size, which givesand describes a normal distribution of zero mean and variance L 0 θ.

If a confinement is now introduced at the locations ∣ z ∣ = w, we next ask with what likelihood the walker steps beyond this confinement. One approach to approximate this probability is to integrate Eq. (16) for all ∣ z ∣ > w, producing a two-sided survival function of Eq. (16). Although this is not equivalent to our initial question, as intermediate steps may also have landed past ∣ z ∣ > w, it proves an acceptable estimate as the last step has the greatest variance. A more accurate calculation would be to evaluate the likelihood that a given walk escapes the confinement at any step; however, looking at the last step is useful for its simplicity in analytical form, and still captures the anticipated behavior. A comparison to the more accurate formulation is made numerically and provided in Supplementary Fig. 9. Once again, we pursue here the simpler form of the survival function valid for large k, and refer to Supplementary Note 2 for the exact derivation valid at all k. The survival function of Eq. (16), S Z (w; θ) = P (∣ Z ∣ > w; w ≥ 0), for a threshold confinement w, is given bywhere erf(z) is the error function. In order for walkers at ∣ z ∣ > w to be restored within the limits of confinement, one or more of their steps must fragment, thereby increasing the number of steps taken and decreasing the overall average, which drives the evolution of fragmentation. This articulates our key claim: considering our original, cylindrically shaped sheets as a statistical ensemble of one-dimensional random walks, we suggest that the progression of fragmentation measured by a change d t, over a single crumpling iteration d n, should be proportional to the fraction of walks in the ensemble which leave the confinement imposed at ∣ z ∣ = w: d t /d n ~ S Z (w; θ). Equivalently, this is the likelihood that a single random walk leaves the critical confinement. We note that this resulting fragmentation rate describes an average fragmentation likelihood given only a confinement w and current temporal parameter t = 1/ θ describing the maturity of the fragmentation process thus far; it does not enforce direct correlations between successive crumpling iterations, whereby new folds should occur preferentially along previous ones. Instead, the decrease in fragmentation rate with n is encoded through the decreasing mean facet area with t. Moreover, while stronger correlation is expected between walks representing nearby transects of the sheet, here we consider the statistical behavior of the sheet as a whole, and account for the increased fragmentation likelihood for facets with larger horizontal extent through the weighting introduced in Eq. (15). At present, Eq. (17) gives the likelihood that new creases will form; however, it does not yet describe how much new damage is created, for which two additional factors should be considered: (1) When the sheet is strongly confined in closely packed layers, the layers tend to collectively fragment, as alluded to by Sultan and Boudaoudand Gottesman et al. thus contributing a factor p ~ 1/ L such that halving the final height doubles the number of additional ridges. (2) In the opposite limit of low compaction, facets are not in close proximity and need not behave cooperatively; thus, new damage scales linearly with the amount of compression L 0 − L, as argued in Gottesman et al. With these additional considerations, we propose that the evolution of the fragmentation process with crumpling iteration behaves aswhere α is a fitted constant of proportionality. We indicate the explicit dependence on t here, as t and θ are inversely related. The critical width w is determined by the geometry of the imposed confinement, as illustrated in Fig. 6 d; a complete derivation is provided in Supplementary Note 4 :where R is the radius of the container. By consequence of Eq. (14) we can directly relate Eqs. (18) and (12) asand obtain a fit to the proportionality constant α. By performing an asymptotic approximation in the limit of large t, detailed in Supplementary Note 3, Eq. (18) may be analytically integrated to provide a scaling relationwhich bears similarity toof Eq. (11):where, and L 0 and R are the sheet length (equivalently the confining container height) and container radius, respectively. Taken together, Eqs. (14) and (21) thereby provide a theoretically motivated expressionbased on properties of fragmentation kinetics and a simple mechanism for re-fragmentation formulated as a random walk. Figure 7 compares the agreement of the empirical relations δ ℓ empir. and ℓ empir. as well as the derived models δ ℓ model and, with the measured quantitiesand ℓ meas. for various n. Collectively, the results of Figs. 4 and 7 demonstrate clear consistency of the fragmentation model with the anticipated logarithmic growth.

---

### Strange data: when the numbers just arent't normal [^b856e8dd]. The Journal of Foot and Ankle Surgery (2015). Low credibility.

Many statistical tests assume that the populations from which we draw our data samples roughly follow a given probability distribution. Here, I review what these assumptions mean, why they are important, and how to deal with situations where the assumptions are not met.

---

### Protein sequence modelling with Bayesian flow networks [^a3673311]. Nature Communications (2025). High credibility.

Methods

Bayesian flow networks for discrete-data

Overview

Let p (x) be a distribution over D discrete variables that we wish to approximate. For some x ~ p (⋅) we have that x = [x 1,…, x D] ∈ V D, where V = [v 1… v K] is a vocabulary with ∣ V ∣ = K. The BFN approximation to p (x) is constructed through the iterative revealing of information about x through a sequence of noisy observations y (1: i) = { y (1)… y (i) }.

A noisy observation at step i, is drawn from the sender distribution as a normally distributed vector for each variable, whereis the accuracy at step i, such that a higher α i reveals more information about the true value of x, andis a one-hot encoding of x,

By starting from some prior distribution θ (0) over the value of x and taking into account all observations so far, the input distribution is derived, providing the optimal estimation of the likelihood of each variable x j independently,

As θ (i) is constructed based on an independent assumption about x 1… x D, it is clearly suboptimal whenever these variables are interdependent. The neural network ϕ is the output distribution, which aims to provide a better estimation by taking into account the interdependency between variables. Specifically,

This approximation implies an equivalent approximation of the distribution of the next noisy observation y (i +1). This approximation, referred to as the receiver distribution, is a mixture of Gaussians for each variable,

The error in ϕ is measured through the KL-divergence between p s and p r at each step. For N steps

---

### LineageOT is a unified framework for lineage tracing and trajectory inference [^57dd942a]. Nature Communications (2021). High credibility.

Our method applies two key steps to recover the lineage coupling spanning a pair of time points t 1, t 2.
We first leverage the lineage tree to adjust the positions of the cells at time t 2 (Fig. 1e).
We then connect them to their ancestors at time t 1 (Fig. 1f) using entropically regularized optimal transport.

The adjustment in the first step can be interpreted as sharing information between closely related cells in order to construct a rough initial estimate of the ancestral states at the earlier time t 1. The rationale behind the second step is based on Schrödinger's discovery that entropically regularized optimal transport gives the maximum-likelihood coupling of diffusing particles. This gives a rigorous interpretation of our methodology, as we explain below.

The core problem involves a single pair of time points, t 1 and t 2, where we are given cells x 1,…, x n sampled at time t 1, and cells y 1,…, y m sampled at time t 2 together with an estimate of their lineage tree. We assume that these data are sampled from trajectories generated by diffusion plus drift through Waddington's landscape (i.e. a stochastic differential equation as described in Methods 2).

Since diffusion dominates drift on short time scales, we can estimate the ancestral state of y i at time t 1 by assuming the dynamics are driven by pure diffusion. However, conditional on the lineage tree, the cells are not diffusing independently. Intuitively, cells with similar lineage should diffuse back toward one another to reach a common ancestral state. The difference in cell state across each of the edges of the lineage tree is given by an independent Gaussian random variable with variance proportional to the time-span along the edge (Methods 4). This implies that the ancestral state at time t 1 for each y i is normally distributed with mean and variance that can be calculated from the lineage tree (Methods 4). Because the ancestral states of each y i are normally distributed, optimal transport will give the maximum-likelihood matching to the observed ancestors x 1,…, x n, when we use an entropy parameter proportional to the inferred variance of ancestral states (Methods 4). This matching, or lineage-resolved coupling, summarizes our knowledge of the ancestral states of cells from t 2 and the hypothetical descendant states of cells from t 1, providing a window onto the embedded lineage tree of each time point.

---

### Markov chain monte carlo methods for hierarchical clustering of dynamic causal models [^fb89bfda]. Human Brain Mapping (2021). Medium credibility.

In order to gain a better understanding of the failure mode of standard Gibbs, we chose one representative dataset and plotted the posterior estimates of subject assignment estimated for each chain individually (Figure 5). This reveals that for standard Gibbs half of the chains (i.e. Chains 1 and 3) are stuck in a local maxima of the posterior density corresponding to an incorrect clustering. At the same time, Figure 6 shows a difference of over 100 in the mean log‐joint probability between the chains. This indicates that Chains 1 and 3 are most likely stuck in low probability regions of the parameter space which are hard to get out of, for example, a local maximum in the posterior distribution. Conversely, for the improved version, all chains converged to the same area of the parameter space (Figure 5, right) with roughly the same mean log‐joint probability (Figure 6, right). In addition, the estimated posterior subject assignment closely matches the ground truth assignment. Since the initial states of the chains are randomized, this result indicates that our improvements to the Gibbs sampler reduce the chance for chains to be trapped in local maxima of the posterior landscape.

---

### The XZZX surface code [^8f6ac9c8]. Nature Communications (2021). High credibility.

This structured noise model thus leads to two distinct regimes, depending on which failure process is dominant. In the first regime where, we expect that the logical failure rate will decay like. We find this behaviour with systems of a finite size and at high bias where error rates are near to threshold. We evaluate logical failure rates using numerical simulations to demonstrate the behavior that characterises this regime; see Fig. 6 (a). Our data show good agreement with the scaling ansatz. In contrast, our data are not well described by a scaling.

Fig. 6
Sub-threshold scaling of the logical failure rate with the XZZX code.

a Logical failure rateat high bias near to threshold plotted as a function of code distance d. We use a lattice with coprime dimensions d × (d + 1) for d ∈ {7, 9, 11, 13, 15} at bias η = 300, assuming ideal measurements. The data were collected usingiterations of Monte-Carlo (MC) samples for each physical rate sampled and for each lattice dimension used. The physical error rates used are, from the bottom to the top curves in the main plot, p = 0.19, 0.20, 0.21, 0.22 and 0.23. Error bars represent one standard deviation for the Monte-Carlo simulations. The solid lines are a fit of the data to, consistent with Eq. (2), and the dashed lines a fit to, consistent with Eq. (3) where we would expect, see Methods. The data fit the former very well; for the latter, the gradients of the best fit dashed lines, as shown on the inset plot as a function of, give a linear slope of 0.61(3). Because this slope exceeds the value of 0.5, we conclude that the sub-threshold scaling is not consistent with. b Logical failure ratesat modest bias far below threshold plotted as a function of the physical error rate p. The data (markers) were collected at bias η = 3 and coprime d × (d + 1) code dimensions of d ∈ {5, 7, 9, 11, 13, 15} assuming ideal measurements. Data is collected using the Metropolis algorithm and splitting method presented in refs. The solid lines represent the prediction of Eq. (3). The data show very good agreement with the single parameter fitting for all system sizes as p tends to zero.

---

### Descriptive statistics: reporting the answers to the 5 basic questions of who, what, why, when, where, and a sixth, so what? [^85777ffe]. Anesthesia and Analgesia (2017). Low credibility.

Descriptive statistics are specific methods basically used to calculate, describe, and summarize collected research data in a logical, meaningful, and efficient way. Descriptive statistics are reported numerically in the manuscript text and/or in its tables, or graphically in its figures. This basic statistical tutorial discusses a series of fundamental concepts about descriptive statistics and their reporting. The mean, median, and mode are 3 measures of the center or central tendency of a set of data. In addition to a measure of its central tendency (mean, median, or mode), another important characteristic of a research data set is its variability or dispersion (ie, spread). In simplest terms, variability is how much the individual recorded scores or observed values differ from one another. The range, standard deviation, and interquartile range are 3 measures of variability or dispersion. The standard deviation is typically reported for a mean, and the interquartile range for a median. Testing for statistical significance, along with calculating the observed treatment effect (or the strength of the association between an exposure and an outcome), and generating a corresponding confidence interval are 3 tools commonly used by researchers (and their collaborating biostatistician or epidemiologist) to validly make inferences and more generalized conclusions from their collected data and descriptive statistics. A number of journals, including Anesthesia & Analgesia, strongly encourage or require the reporting of pertinent confidence intervals. A confidence interval can be calculated for virtually any variable or outcome measure in an experimental, quasi-experimental, or observational research study design. Generally speaking, in a clinical trial, the confidence interval is the range of values within which the true treatment effect in the population likely resides. In an observational study, the confidence interval is the range of values within which the true strength of the association between the exposure and the outcome (eg, the risk ratio or odds ratio) in the population likely resides. There are many possible ways to graphically display or illustrate different types of data. While there is often latitude as to the choice of format, ultimately, the simplest and most comprehensible format is preferred. Common examples include a histogram, bar chart, line chart or line graph, pie chart, scatterplot, and box-and-whisker plot. Valid and reliable descriptive statistics can answer basic yet important questions about a research data set, namely: "Who, What, Why, When, Where, How, How Much?"

---

### Article 5. An introduction to estimation – 2: from z to T [^fb82b806]. Emergency Medicine Journal (2001). Low credibility.

Provided the sample size is large enough (that is, n greater than 100), the z statistic can be used to determine the confidence interval estimation of the population mean even when the sigma is not known. In these cases the estimation of the standard error of the mean is used. The z statistic is also valid when determining the population's proportion based upon a large sample. However, when dealing with smaller samples, the z statistic is replaced by the t statistic. This makes it possible to estimate, in a population with an unknown standard deviation: The probability of getting a sample mean greater than or equal to a particular value The value of a sample mean with a particular probability of occurring The probability of getting a sample mean between two particular values The confidence interval for the estimation of the population mean can also be determined using the t statistic.

---

### Decline of COPD exacerbations in clinical trials over two decades-a systematic review and meta-regression [^6a2c32e7]. Respiratory Research (2019). Medium credibility.

Due to the heterogeneous standards of reporting exacerbation counts, a Bayesian model was developed in order to accommodate different types of reported outcome measures in a coherent model as published previously. This way, endpoints given in terms of (a) rates and standard errors (or confidence intervals), (b) rates (or total event counts) only, (c) numbers (or proportions) of patients with and without an event, or (d) a combination of total count and event-free numbers may be jointly utilized for the analysis. Analysis then primarily aims for the annual exacerbation rate, while at the same time accounting for overdispersion. Study-specific random effects are included for logarithmic rate and overdispersion, and covariables are included in the model by assuming a linear effect on the logarithmic exacerbation rate. For the i th study of duration d i, we assume that the number of exacerbations follows a negative binomial distribution with rate d i λ i and overdispersion φ i, where log (λ i) ~ Normal(β 0 + β 1 x, σ λ 2) and log (φ i) ~ Normal (μ φ, σ φ 2). Here x is our covariable (e.g. publication year, in years since 2000). The parameters β 0 and β 1 here are the intercept and slope of the regression line, respectively, and σ λ 2 and σ φ 2 are the heterogeneity parameters for rate and overdispersion, respectively. Further covariables then are added analogously. In order to fit the Bayesian models, we used the following prior distributions for the parameters: a uniform distribution on the interval log (0.001) to log (1000) for the intercept, i.e. β 0 ~ Uniform (log (0.001), log (1000)); a normal distribution with mean 0 and variance 100 for the slope, i.e. β 1 ~ Normal (0, 10 2); a half-normal distribution with scale 1.0 for the heterogeneity of the rate, i.e. σ λ ~ halfNormal (1.0); a uniform distribution on the interval log (0.0001 to log (10000) for the mean overdispersion, i.e. μ φ ~ Uniform (log (0.0001), log (10000)); and a half-normal distribution with scale 1.0 for the heterogeneity of the overdispersion, i.e. σ φ ~ halfNormal (1.0). The models are fitted via Markov chain Monte Carlo (MCMC) methods using JAGS and the "rjags" R package. Estimates are quoted in terms of posterior medians and central 95% credible intervals (CIs). For the Bayesian analyses, we quote two-sided posterior tail probabilities (p B) for the regression parameters instead of frequentist (two-sided) p -values.

---

### Reconstructing higher-order interactions in coupled dynamical systems [^23239044]. Nature Communications (2024). High credibility.

The variable x i represents the abundance of species i. The local dynamics of x i is governed by the logistic functionwhere r i and k i are the growth rate and the carrying capacity. The pairwise interactions between species are encoded in the real coefficients of the N × N weighted matrixwith at most N (N − 1) non-zero elements, while the three-body interactions in the real coefficients of the N × N × N weighted tensorwith at mostnon-zero elements. This is because when the interaction does not depend on the permutation of the indices, as is the case of Lotka–Volterra type models, the number of non-zero entries of tensoris. Equation (5) is in the form of Eq. (1) with g (1) (x i, x j) = x i x j and g (2) (x i, x j, x k) = x i x j x k. As an example, we consider the system of N = 7 species with four cooperative and four antagonistic pairwise interactions, studied in ref.and shown in Fig. 1 a with blue and red arrows, respectively. In addition to these pairwise interactions, we have included two cooperative three-species interactions, shown as double arrows in the hypergraph in Fig. 1 a. These respectively correspond to a contribution to the dynamics of x 2 given byand one to x 4 given by, withand. The other system parameters, i.e. the values of r i, k i, i = 1,…, 7, and the initial conditions have been chosen as in ref. Namely, growth rates r i for all species have been randomly selected from a uniform distribution in the interval (0, 1), similarly, the carrying capacities k i are sampled from a uniform distribution in the interval (1, 100), and the initial conditions x i (0) are integers sampled in the interval (10, 100).

---

### Extraction of accurate cytoskeletal actin velocity distributions from noisy measurements [^703919c4]. Nature Communications (2022). High credibility.

Displacement distributions were then calculated for twenty timescales (2, 4. 38, and 40 s) from the simulated particles. Within each estimated pdf, bins with zero counts were set to 1/(N ⋅ w), where N is the number of particles in the distribution and w = 30 nm is the width of the bin; this prevents infinite likelihoods but overestimates rare events, as it represents an approximate upper bound on the expected value of that bin. To calculate likelihoods for our experimentally measured displacements, probability densities were linearly interpolated from the two closest points. To keep computational times reasonable, we calculated likelihoods from a randomly sampled subset of our data: for each of the twenty timescales, if there were more than 1000 displacements measured, a random sample of 1000 were chosen, resulting in ~20,000 observations included in the fit sample. Negative log likelihoods were minimized using MATLAB's genetic algorithm global optimization function. Due to the rough nature of the fit landscape (in part due to the stochasticity of the simulations), ten optimizations were completed for each dataset and the best fit (highest likelihood) was chosen for each. For each population, best fit parameters from different experimental replicates were averaged and the standard error of the mean on each parameter is presented (Fig. 4 g).

The resulting velocity distributions from each fit parameter were generated using similar simulations, but without the addition of Gaussian noise to each localization. The discrete probability of zero displacement was calculated analytically as P (s = 0) = e − μ τ, where μ is the average jump rate and τ is the observation timescale. For values > 0, probability distributions were calculated numerically from simulations with 100,000 particles.

For the drug perturbations presented in Supplementary Fig. 10, 95% confidence intervals were estimated using the profile likelihood method as described in ref. In short, each parameter was systematically varied in turn above and below its best fit value. At each sampled value, the other two parameters were again optimized using MATLAB's genetic algorithm, and the log-likelihood of this optimized set of parameters was recorded. These log-likelihood values were then fit to a parabola as a function of the varied parameter with its minimum fixed at the log-likelihood of the best fit parameter. The 95% confidence intervals were then estimated by the domain over which the parabolic fit to the log 10 -likelihoods rises by 0.8338 from the minimum.

---

### Hippocampal spatial representations exhibit a hyperbolic geometry that expands with experience [^05c38acb]. Nature Neuroscience (2023). High credibility.

Fig. 6
Illustration of observation of a log-normal distribution of place field sizes.

Random samples from each of eight different sinh distributions (left) are drawn. Suppressing the probability of observing small place fields according to a sigmoidal function (left inset) leads to a distribution that approximates log-normal (middle). The middle panel shows the resulting observed histogram for one sinh distribution. Inset shows log-scale on the x axis. When sample size is not too large (n = 800 in this simulation), the histogram can be fitted by a log-normal distribution with P = 0.07 (χ 2 GOF test, χ 2 = 11.66 and d.f. = 6). Black curve represents the theoretical density function, and gray line represents the fitted log-normal distribution. The right panel shows the results after pooling from eight distributions where an even better fit with log-normal can be obtained (P = 0.43, χ 2 GOF test, χ 2 = 4.87 and d.f. = 5) at a moderate sample size (n = 800 in this simulation). pdf, probability density function.

Source data

---

### Reporting of statistically significant results at clinicalTrials.gov for completed superiority randomized controlled trials [^3e8d6e10]. BMC Medicine (2016). Low credibility.

Statistical analysis

Categorical variables were described with frequencies (percentages) and quantitative variables with median (Q1-Q3). To assess the statistical significance of the results, we focused on p -values rather than on treatment effect estimates with 95% CI, and graphically represented the density plots of p -values for trials by whether a treatment effect estimate and/or p -value was reported or not. For this, we considered p -values < 0.0001 equal to 0.0001.

We aimed to identify trial characteristics associated with reporting a p -value < 0.05 for the first primary outcome at ClinicalTrials.gov. To do, so, we first compared trial characteristics by whether results were reported to be statistically significant or not using chi-square tests for categorical variables and Wilcoxon tests for continuous variables. Then, we used a multivariate logistic regression model to identify factors independently associated with the reporting of statistically significant results. All variables that were statistically significant on univariate analyses were entered into multivariate analysis except single-center, which was closely related to single-country status.

Statistical analysis involved use of SAS 9.3 (SAS Inst. Cary, NC, USA). The density plot was created with R 3.2.3 (R Foundation for Statistical Computing) using a syntax provided in a previous article.

---

### Statistics review 1: presenting and summarising data [^4c1c82d3]. Critical Care (2002). Low credibility.

The present review is the first in an ongoing guide to medical statistics, using specific examples from intensive care. The first step in any analysis is to describe and summarize the data. As well as becoming familiar with the data, this is also an opportunity to look for unusually high or low values (outliers), to check the assumptions required for statistical tests, and to decide the best way to categorize the data if this is necessary. In addition to tables and graphs, summary values are a convenient way to summarize large amounts of information. This review introduces some of these measures. It describes and gives examples of qualitative data (unordered and ordered) and quantitative data (discrete and continuous); how these types of data can be represented figuratively; the two important features of a quantitative dataset (location and variability); the measures of location (mean, median and mode); the measures of variability (range, interquartile range, standard deviation and variance); common distributions of clinical data; and simple transformations of positively skewed data.

---

### A note on estimating the posterior density of a quantitative trait locus from a Markov chain monte carlo sample [^6b207541]. Genetic Epidemiology (2002). Low credibility.

We provide an overview of the use of kernel smoothing to summarize the quantitative trait locus posterior distribution from a Markov chain Monte Carlo sample. More traditional distributional summary statistics based on the histogram depend both on the bin width and on the sideway shift of the bin grid used. These factors influence both the overall mapping accuracy and the estimated location of the mode of the distribution. Replacing the histogram by kernel smoothing helps to alleviate these problems. Using simulated data, we performed numerical comparisons between the two approaches. The results clearly illustrate the superiority of the kernel method. The kernel approach is particularly efficient when one needs to point out the best putative quantitative trait locus position on the marker map. In such situations, the smoothness of the posterior estimate is especially important because rough posterior estimates easily produce biased mode estimates. Different kernel implementations are available from Rolf Nevanlinna Institute's web page (http://www.rni.helsinki.fi/; fjh).

---

### Foundations of the minimal clinically important difference for imaging [^72f785d9]. The Journal of Rheumatology (2001). Low credibility.

This article develops a generic conceptual framework for defining and validating the concept of minimal clinically important difference. We propose 3 approaches. The first uses statistical descriptions of the population ("distribution based"), the second relies on experts ("opinion based"), and a third is based on sequential hypothesis formation and testing ("predictive/data driven based"). The first 2 approaches serve as proxies for the third, which is an experimentally driven approach, asking such questions as "What carries the least penalty?" or "What imparts the greatest gain?" As an experimental approach, it has the expected drawbacks, including the need for greater resources, and the need to tolerate trial and error en route, compared to the other 2 models.

---

### Graphical data presentation [^8085a383]. Injury (2008). Low credibility.

Figures and charts are the most influential vehicles for distributing scientific information, for affecting decisions as to the acceptance or rejection of a manuscript, and for attracting the attention of the scientific community to study results. Graphical excellence is mainly defined, first, by the highest possible data density (that is, the amount of information provided per graph area); second, by a low ink-to-data ratio (the avoidance of unnecessary shading, three-dimensionality, gridlines and what is often called 'chartjunk'); and third, by clear and unequivocal labelling of axes. The researcher's essential graphical toolbox should contain histograms, bar charts (always with measures of error), box-and-whiskers plots, scatter plots and forest plots.

---

### Confidence reports in decision-making with multiple alternatives violate the Bayesian confidence hypothesis [^b245f51e]. Nature Communications (2020). High credibility.

Inference on a given trial. We assume that the observer knows the mean and standard deviation of each category based on the exemplar dots, and that the observer assumes that the three categories have equal probabilities. The posterior probability of category C given the measurement x is then p (C | x) ∝ p (x | C) = N (x; m C, (σ s 2 + σ 2) I). Instead of the true posterior p (C | x), the observer makes the decisions based on q (C | x), a noisy version of the posterior probability. We obtain a noisy posterior q (C | x) by drawing from a Dirichlet distribution. The Dirichlet distribution is a generalization of the beta distribution. Just like the beta distribution is a continuous distribution over the probability parameter of a Bernoulli random variable, the Dirichlet distribution is a distribution over a vector that represents the probabilities of any number of categories. The Dirichlet distribution is parameterized as

Γ represents the gamma function. p is a vector consisting of the three posterior probabilities, p = (p 1, p 2, p 3) = (p (C = 1 | x), p (C = 2 | x), p (C = 3 | x)). q is a vector consisting of the three posterior probabilities perturbed by decision noise, q = (q 1, q 2, q 3) = (q (C = 1 | x), q (C = 2 | x), q (C = 3 | x)). The expected value of q is p. The concentration parameter α is a scalar whose inverse determines the magnitude of the decision noise; as α increases, the variance of q decreases. To make a category decision, the observer chooses the category that maximizes the posterior probability:

---

### Distributional dual-process model predicts strategic shifts in decision-making under uncertainty [^00a9c8ee]. Communications Psychology (2025). Medium credibility.

Fig. 3
Model simulation results.

a Simulated model surfaces showing the proportion of C choices in CA trials reveal that the dual-process model uniquely demonstrates sensitivity to both increased variance and lower reward ratios. In contrast, the other five models exhibit sharp declines in frequency effects as reward ratio increases but remain largely insensitive to variance. Color represents the difference between the reward ratio and the C choice rate, with blue indicating frequency effects (i.e. a preference for the more frequently rewarded option A) and red indicating a stronger preference for C than predicted by the reward ratio. b Model surfaces for the proportion of simulated agents showing frequency effects further highlight that the dual-process model uniquely predicts a higher prevalence of frequency effects with both increased variance and lower reward ratios. The Delta, risk-sensitive Delta, mean-variance utility, and ACT-R models again show sharp declines in frequency effects as reward ratio increases while remaining unresponsive to changes in variance. The Decay model, while consistently predicting strong frequency effects, also displays limited sensitivity to variance (see also Supplementary Figs. 1, 2 for traditional simulations limited to the three experimental conditions). The color bar is normalized to represent the percentage of simulated agents displaying frequency effects. c The heatmap shows that higher reward variance and lower reward ratios result in a stronger reliance on the Dirichlet distribution. d The scatterplot shows that increased objective Dirichlet weights are positively correlated with greater preference for A in CA trials, supporting the hypothesis that greater uncertainty enhances reliance on frequency-driven processes, which is directly related to more prevalent frequency effects. In these figures, "variance" conceptually refers to reward variance rather than statistical variance. As indicated in the brackets, the values represent the standard deviation of the normal distributions from which the rewards are drawn.

---

### Protein sequence modelling with Bayesian flow networks [^7d765a30]. Nature Communications (2025). High credibility.

Box 2 Reconstructed ODE sampling

1: Input: N steps, output network

2: Output: generated sample x

3: z ~ N (0, I)

4: y (0) ← 0

5: for i = 0… N do

6:

7:

8:

9: β (s) ← β 1 s 2

10:

11: end for

12:

13:

14: return x

Inpainting

Consider the task of inpainting some sequence x according to a binary mask m ∈ [0, 1] D where m i = 0 indicates that the i th element of x should be used as conditioning information, and m i = 1 indicates that the i th element of x should be inpainted. Inspired by SMCDiff, we treat conditional generation of the inpainted regions as a sequential Monte Carlo problem which we may solve with particle filtering. Given an output network ϕ and input distribution θ (t) at time t, the KL divergence of the sender and receiver distributions, with accuracy α, for the conditioning subset x m is, which we will denote D (x, m, θ (t), α). Given q particles at time t with input distributions, we resample each particle with probability proportional to. Combining SMC with our sampling method, we arrive at the algorithm detailed in Box 3.

Box 3 Greedy SMC inpainting

1: Input: N steps, output network, q particles, conditioning information x m

2: Output: conditionally generated sample x

3:

4:

5: for i = 0… N do

6:

7:

8: β (t) ← β 1 t 2

9: β (s) ← β 1 s 2

10: α ← β (s) − β (t)

11:

12:

13:

14:

15: end for

16:

17:

18: return x

Pre-training ProtBFN

---

### Machine learning identifies scale-free properties in disordered materials [^672c0fde]. Nature Communications (2020). High credibility.

Fig. 4
Scale-invariant disordered structures generated by the localization-to-disorder convolutional neural network.

a – f Comparison between seed and machine-learning- (ML-) generated structures for a – c weak and d – f strong disorder. a, d Seed structures that provide the target localizations for the localization-to-disorder (L2D) convolutional neural network (CNN). b, e ML-generated disordered structures obtained from the L2D CNN. c, f Localization of seed ("Truth", red dotted lines) and ML-generated ("ML Truth", black dotted lines) structures, obtained from the tight-binding Hamiltonian in Eq. (1). g Statistical distributions of the strength of the lattice deformation Δ r in the seed (green) and ML-generated (orange) structures for 3200 realizations satisfying 0.20 ≤ w avg ≤ 0.30 in the ML design. The first inset g-1 shows the log–log plot of g for the ML design, illustrating the power-law distribution. The orange line (composed of discretized points) represents the complementary cumulative distribution function (CDF) obtained from the data set in g. The black dashed line represents the best fit to the data using the method in refs. showing the power-law fitting of (Δ r) −3.79. The black dot represents the lower bound Δ r min = 0.432 to the power-law behavior. h The extended plot of the range 0.4 ≤ Δ r ≤ 0.8 in g demonstrating the heavy-tailed distribution of the ML design.

---

### Probabilistic weather forecasting with machine learning [^e674e6e2]. Nature (2025). Excellent credibility.

Methods

Task definition and general approach

A general formulation of the task of probabilistic weather forecasting from the present time t = 0 into the future is to model the joint probability distribution, where T is the forecast horizon, denotes the atmospheric state at time t and O ≤ 0 are observations made up to the forecast initialization time t = 0. This joint distribution can be factored as

Our innovation in this work is an MLWP-based Forecast model, and we adopt a traditional NWP-based State inference approach. We make several approximations to the above general formulation, as follows.

Full atmospheric states are not directly observed, and so we approximate eachwith a best-estimate NWP-based analysis state X t, which has been generated at finite resolution, using a window of observations in a process known as data assimilation. In our case, each X t is an 84 × 720 × 1,440 array, which includes six surface variables and six atmospheric variables at each of 13 vertical pressure levels (Extended Data Table 1), on a 0.25° latitude–longitude grid. We generate 15-day forecasts, at 12 h steps, so T = 30.

As a first, standard approximation, we use analysis X 0: T as evaluation targets. This means we are in effect evaluating the forecast of each model as a predictive distribution,

over sequences of future best-estimate NWP analyses.

Second, we wish to rely on a Markov assumption, but although the underlying atmospheric state sequenceis Markov, it is only partially observed in X 1: T. In our models GenCast and GenCast-Perturbed, we make a weaker second-order Markov approximation, under which we factorize

We found that conditioning on two previous time steps works better than one.

For GenCast, the initialization P (X 0, X −1 | O ≤ 0) is handled by fixing (X 0, X −1) to their values obtained from two consecutive best-estimate analyses from the ERA5 dataset. For GenCast-Perturbed, additional perturbations are added, see Supplementary Information section A.4. With initialization dealt with, the problem is reduced to modelling P (X t | X t −1, X t −2), and samples of X 1: T can be generated autoregressively.

---

### Quantifying the unknown impact of segmentation uncertainty on image-based simulations [^50ae5640]. Nature Communications (2021). High credibility.

EQUIPS then probes the segmentation uncertainty at three probability values: μ − σ, μ, and μ + σ, which we will call the standard segmentations (Fig. 2 h). Here, μ is the mean, 50.0 percentile, and σ is the standard deviation away from the mean, 15.9 and 84.1 percentiles at μ − σ and μ + σ, respectively. We chose these particular percentile values to quickly model the uncertainty distribution in a physics quantity using a characteristic distribution. However, any percentile from the probability map could be used to generate an image segmentation representing that probability value.

For each standard segmentation, we perform a physics simulation to predict the physics quantities (Fig. 2 i). Each inset in Fig. 2 i highlights a region of the domain geometry that undergoes significant alterations. These geometry changes impact the physics quantity behavior and, as a result, these changes manifest in the physics quantity uncertainty distribution. For example, in the μ − σ case, the middle inset shows a material region that is disconnected from the surrounding region and therefore has low heat flux. In contrast, the μ + σ case shows a material region that is fully connected to its neighbors and therefore has a much higher heat flux.

Throughout this work, we focus on a Normal (Gaussian) distribution as the characteristic distribution to rapidly approximate a physics quantity uncertainty distribution using the fewest possible simulations. Only the three standard segmentation simulations are necessary to specify this characteristic distribution. The physics quantity evaluated using the 50.0 percentile segmentation provides the distribution mean, while the 15.9 and 84.1 percentile segmentations provide the standard deviation. The curve in Fig. 2 j is the CDF of the characteristic distribution estimate calculated using this method while the red, black, and green points are the standard segmentation values. This characteristic CDF estimates the uncertainty distribution of a physics quantity as a result of segmentation uncertainty. If the characteristic distribution estimate fits the physics quantity data points well, then the calculation of additional percentile segmentations is likely unnecessary. However, if the characteristic distribution is poor (i.e. the unknown distribution is non-Normal), then additional percentile segmentations are required to capture the underlying physics quantity uncertainty distribution.

In the following discussion, we present three exemplar problems demonstrating the propagation of segmentation uncertainty from image segmentation through physics simulations to physics quantities. In each of these exemplars, simulation conditions and model parameters are held constant to draw attention to the impact that segmentation uncertainty has on the uncertainty distribution of physics quantities.

---

### A practical guide for understanding confidence intervals and P values [^c0a7d4a0]. Otolaryngology — Head and Neck Surgery (2009). Low credibility.

The 95 percent confidence interval about the mean demarcates the range of values in which the mean would fall if many samples from the universal parent population were taken. In other words, if the same observation, experiment, or trial were done over and over with a different sample of subjects, but with the same characteristics as the original sample, 95 percent of the means from those repeated measures would fall within this range. This gives a measure of how confident we are in the original mean. It tells us not only whether the results are statistically significant because the CI falls totally on one side or the other of the no difference marker (0 if continuous variables; 1 if proportions), but also the actual values so that we might determine if the data seem clinically important. In contrast, the P value tells us only whether the results are statistically significant, without translating that information into values relative to the variable that was measured. Consequently, the CI is a better choice to describe the results of observations, experiments, or trials.

---

### Effects of disease severity distribution on the performance of quantitative diagnostic methods and proposal of a novel'V-plot' methodology to display accuracy values [^49f1a3d8]. Open Heart (2018). Low credibility.

Background

Diagnostic accuracy is widely accepted by researchers and clinicians as an optimal expression of a test's performance. The aim of this study was to evaluate the effects of disease severity distribution on values of diagnostic accuracy as well as propose a sample-independent methodology to calculate and display accuracy of diagnostic tests.

Methods and Findings

We evaluated the diagnostic relationship between two hypothetical methods to measure serum cholesterol (Chol rapid and Chol gold) by generating samples with statistical software and (1) keeping the numerical relationship between methods unchanged and (2) changing the distribution of cholesterol values. Metrics of categorical agreement were calculated (accuracy, sensitivity and specificity). Finally, a novel methodology to display and calculate accuracy values was presented (the V-plot of accuracies).

Conclusion

No single value of diagnostic accuracy can be used to describe the relationship between tests, as accuracy is a metric heavily affected by the underlying sample distribution. Our novel proposed methodology, the V-plot of accuracies, can be used as a sample-independent measure of a test performance against a reference gold standard.

---

### Regional, geographic, and racial / ethnic variation in glycemic control in a national sample of veterans with diabetes [^af6494c1]. Diabetes Care (2011). Low credibility.

Statistical analysis

Models for the relationship between HbA 1c and race, urban/rural residence, and VISN were developed in a sequential fashion. We used a generalized linear model approach (lmer function in R) to fit the models for HbA 1c as a response variable with race/ethnicity, location of residence (urban/rural), and region as main independent variables of interest with adjustment for potential confounders. The general model had the following regression format, E(Y i |X i, Z i) = g −1 (X i β + Z i γ i), where g is a monotone link function and Y i is the response for the i th subject with T i (T i = 1,… ,5) repeated measurements, X i and Z i represent vectors of fixed and random effect covariates, respectively, and g is considered as the identity link resulting in a linear mixed model for the continuous HbA 1c outcome and the logit link for the dichotomous HbA 1c outcome. A person-level random effect was included in all models to account for within-individual correlations. This approach accommodates a wide range of distributional assumptions, multilevel data such as longitudinal measurements on subjects, measurement of subjects at different time points, modeling individual level effects, missing data, and time varying or invariant covariates. Model fit was assessed using plots of residuals and predicted values. All statistical tests used a two-tailed α = 0.05 level of significance and were performed in R, version 2.11.0.

---

### Chromatin accessibility landscape and regulatory network of high-altitude hypoxia adaptation [^76da2951]. Nature Communications (2020). High credibility.

Estimate P (S | X, Y; μ) by iteratively updating μ and S

Note that the selection status of each RE is mainly determined by population genetics information, i.e. it is independent of chromatin accessibility and gene expression, the selection status Z (Eq. (4)) is estimated independently from other equations. For estimation problem of P (S | X, Y; μ) denoted by Eq. (4), we implemented the following steps.
Let, where c p denotes the p-value of CMS score of the p th SNP. Initiateand
Estimate μ givenby least-squares:
Estimate P (S k = 1| X, Y) given μ :where f denotes the sigmoid function, i.e.
If not convergent, go to step (3). Convergence means μ does not change between adjacent steps.

After the above iteration, we obtained the selection status P (S k = 1| X, Y) for all REs. We choose a threshold of 0.95. If P (S k = 1| X, Y) > 0.95, P (S k = 1| X, Y) = 1. Otherwise P (S k = 1| X, Y) = 0. This procedure enforces sparsity to the REs under selection.

Estimate

Equation (6) is a linear regression model. given O and S, parameters η, ω, andcan be easily estimated by the least square. Then we used t-distribution to test whether ω is non-zero, with p -value < 0.05.

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^cf66bf7c]. Nature Communications (2023). High credibility.

Dose-response curves are key metrics in pharmacology and biology to assess phenotypic or molecular actions of bioactive compounds in a quantitative fashion. Yet, it is often unclear whether or not a measured response significantly differs from a curve without regulation, particularly in high-throughput applications or unstable assays. Treating potency and effect size estimates from random and true curves with the same level of confidence can lead to incorrect hypotheses and issues in training machine learning models. Here, we present CurveCurator, an open-source software that provides reliable dose-response characteristics by computing p-values and false discovery rates based on a recalibrated F-statistic and a target-decoy procedure that considers dataset-specific effect size distributions. The application of CurveCurator to three large-scale datasets enables a systematic drug mode of action analysis and demonstrates its scalable utility across several application areas, facilitated by a performant, interactive dashboard for fast data exploration.

---

### Estimation of the risk of a qualitative phenotype: dependence on population risk [^4a647c65]. Journal of Human Genetics (2017). Low credibility.

Individual disease risk estimated based on the data from single or multiple genetic loci is generally calculated using the genotypes of a subject, frequencies of alleles of interest, odds ratios and the average population risk. However, it is often difficult to estimate accurately the average population risk, and therefore it is often expressed as an interval. To better estimate the risk of a subject with given genotypes, we built R scripts using the R environment and constructed graphs to examine the change in the estimated risk as well as the relative risk according to the change of the average population risk. In most cases, the graph of the relative risk did not cross the line of y = 1, thereby indicating that the order of the relative risk for given genotypes and the population average risk does not change when the average risk increases or decreases. In rare cases, however, the graph of the relative risk crossed the line of y = 1, thereby indicating that the order of the relative risk for given genotypes and the population average risk does change owing to the change in the population risk. We propose that the relative risk should be estimated for not only the point average population risk but also for an interval of the average population risk. Moreover, when the graph crosses the line of y = 1 within the interval, this information should be reported to the consumer.

---

### Advanced methods for gene network identification and noise decomposition from single-cell data [^3a3d9606]. Nature Communications (2024). High credibility.

Similar to stochastic filtering, this identification problem can also be solved in a recursive manner. Essentially, the parameters Θ can be viewed as the state of some additional special chemical species in the system, which can take non-integer values and remain constant over time. From this perspective, this cell-specific identification problem aims to infer the hidden states of these special species, a task closely aligned with the filtering problem introduced in the preceding section. Therefore, by denoting the condition distributionsand, we can solve this identification problem using the following recursive formulas:Here, (10) represents the prediction of the entire state (Θ, X (t)) at the subsequent time point t i +1 using the observation up to the current time t i. Meanwhile, (11) adjusts this prediction using the next measurement at time t i +1. Finally, (12) obtains the result of parameter identification by marginalization. Since Θ can be viewed as the state of additional chemical species, the probability distribution of (Θ, X (t)) evolves according to an augmented CME (Supplementary Information, Section S5.B), a minor extension of the classical CME. Consequently, the prediction step (10) corresponds to solving this augmented CME withas the initial probability distribution andas the solution at the final time. Thus, this parameter identification problem can also be interpreted as a combination of solving the augmented CMEs and making subsequent adjustments as new measurements arrive.

---

### Solving the where problem and quantifying geometric variation in neuroanatomy using generative diffeomorphic mapping [^fd44f32f]. Nature Communications (2025). High credibility.

The statistical interpretation allows us to accommodate images with non reference signals, such as missing tissue, tracer injection sites, or other anomalies. At each pixel, the identity of the signal type is modeled as missing data, and maximum likelihood estimators are computed using an Expectation Maximization algorithm, which alternates between the E step: compute posterior probability π i (x) that each pixel corresponds to the reference image rather than one of the non-reference types, and the M step: update parameters by solving a posterior weighted version of the above:As an EM algorithm, this approach is guaranteed to be monotonically increasing in likelihood. An example of posterior weights are shown in the right hand column of Fig. 2 b.

Our approach uses mixtures of Gaussians to model variability in data, to allow large outliers to be accommodated by additional components, even though the Gaussian distribution itself does not have long tails. The Gaussian model allows for closed form expression (in terms of matrix inverse) for contrast transformation parameters. Other groups have used long tailed distributions to model variability and outliers in a robust manner, most notably the exponential distribution for l1 optimization. Techniques such as iteratively reweighted least squares can be applied as in Reuter et al. which lead lead to a weighted least squares problem which is similar to ours.

Nonconvex optimization with low to high dimensional subgroups and resolutions

This registration problem is highly nonconvex, and allows for many local minima. To provide robustness in our solution, we solve a sequence of lower dimensional subproblems, initializing the next with the solution to the previous. (i) 2D slice to slice rigid alignment maximizing similarity to neighbors(ii) 3D affine only alignment, registration using the full model at (iii) low (200 μm), (iv) medium (100 μm), and (v) high (50 μm) resolution. Time varying velocity fields are discretized into 5 timesteps and integrated using the Semi Lagrangian method. For most subproblems, spatial transformation parameters are estimated by gradient descent, and intensity transformation parameters are updated by solving a weighted least squares solution at each iteration. For subproblems that include linear registration only, parameters are estimated using Reimannian gradient descent (discussed in ref.and similar to a second order Gauss–Newton optimization scheme).

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^dee6cacc]. CDC (2011). Medium credibility.

Community viral load sample size calculations — sample size to detect differences in geometric mean (GM) viral load depends on power and standard deviation (S). The table specifies "α = 0.05 and W = 0.8", with S columns 1 to 1.5; for k = 3 the minimum sample sizes by S are 54, 66, 78, 92, 106, and 122, and "1.2 is the standard deviation of national VL data", so at S = 1.2 the needed sample size is 78.

---

### Bayesian strategy selection identifies optimal solutions to complex problems using an example from GP prescribing [^07ef380a]. NPJ Digital Medicine (2020). Medium credibility.

Figure 3 shows the cumulative number of trials applied for each strategy over time. The best performing strategies were typically allocated a larger number of trials with the exception of strategy 6, which was trialled more than expected because delayed data submission from some GPs led to higher uncertainty for its efficacy in the algorithm.

Fig. 3
Cumulative number of trials conducted for each strategy vs time.

Stage 1: Weeks 1–2 no strategy is implemented. Stage 2: Weeks 1–7 the strategies are picked by the algorithm. Stage 3: Weeks 1–3 implements only strategies top three strategies determined at end of Stage 2 (3, 7, 5) to improve the power of hypothesis test (Strategy vs baseline).

Strategies seven and three clearly emerge as the best strategies, with low standard error and p -values (Table 1). Mean and standard error of the reward distribution for each strategy were computed with a one-sided t -test at the end of the trial. The trial count is the cumulative count of number of trials for that strategy across all clinics at the termination of the trial.

Table 1
Final statistics for all strategies.

One sided t -test comparing estimates for mean estimate of difference in GP/patient consultations on PA

The strategies are sorted by descending difference in mean effects of strategy and baseline. The strategy 7 and 3 mean differences are statistically significant with p -values < 0.006

---