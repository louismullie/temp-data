# Medical Question & Answer

**Sample ID**: 5363fc25-9f69-d923-3fba-67cd652097fa
**Dataset Index**: 353347

---

## Question

178. Strange Multiplication. I was often asked to explain the following fact, which will undoubtedly interest many readers who were not previously aware of it. If someone correctly performs addition but cannot multiply or divide by numbers greater than 2, it turns out that they can obtain the product of any two numbers using the following strange method. For example, suppose we need to multiply 97 by 23. We form two columns of numbers:

| 97 | 23 |
| ---: | :---: |
| 48 | (46) |
| 24 | (92) |
| 12 | (184) |
| 6 | (368) |
| 3 | 736 |
| 1 | 1472 |
| ∅ |2231 |

We successively divide the numbers in the first column by 2, discarding the remainder, until we get 1, and multiply the numbers in the second column by 2 the same number of times. If we cross out the products that stand opposite even numbers in the left column (we have enclosed them in parentheses) and add the remaining ones, we get the correct answer: 2231

Why?

---

## Answer

> Let's see… What do we have here? The user is asking why the "strange multiplication" algorithm works, specifically how halving one number and doubling the other, then selectively adding the doubled values opposite odd halved values, produces the correct product. Let's break this down step-by-step. First, I need to think about the algorithmic steps and verify the example. Then, I should connect this to binary representation and distributivity. Next, I will prove correctness using induction or a loop invariant. After that, I should consider edge cases and why even entries are excluded. Finally, I will reflect on historical context and practical implications, and I will double-check the arithmetic in the example to ensure there are no hidden errors.

> Let me first confirm the procedure and the concrete example. We start with 97 and 23, repeatedly halve the left column (integer division, discarding remainders) until we reach 1, and simultaneously double the right column the same number of times. Then we add only those right-column values that are opposite odd numbers in the left column, which gives 23 + 736 + 1472 = 2231, matching 97 × 23. Wait, let me verify the additions: 23 + 736 is 759, and 759 + 1472 is indeed 2231, so the example checks out arithmetically.

> Now, I need to think about the underlying principle. This is essentially a manual implementation of ancient binary multiplication, sometimes called Russian peasant multiplication or Ethiopian multiplication, which relies on the distributive law and the fact that any integer can be expressed as a sum of powers of two. Hold on, I should verify that 97 in binary is 64 + 32 + 1, which equals 97, and that corresponds to the odd halved values 97, 3, and 1 in the left column, whose paired right values are 23, 736, and 1472, respectively. That mapping aligns with the binary decomposition, supporting the distributive interpretation [^notfound].

> Next, I should review the mathematical justification. The key insight is that multiplying by 97 is the same as multiplying by the sum of distinct powers of two that compose 97, so 97 × 23 = (64 + 32 + 1) × 23 = 64×23 + 32×23 + 1×23. But wait, I initially thought the right column tracks 23×2^k directly; let me correct that. The right column tracks 23×2^step, and the left column's odd entries mark which powers of two are "active" in the binary expansion of the original multiplier, so we are indeed summing 23×64, 23×32, and 23×1, which is exactly what we get from 736, 1472, and 23, respectively [^notfound].

> I will now examine why we ignore rows where the left column is even. Hold on, let's not jump to conclusions; I should confirm that even left entries correspond to powers of two that are not included in the binary representation of the original multiplier. Because we are effectively building the binary expansion from least significant bit to most significant bit as we halve, an even halved value means the current bit is 0, so that term should not contribute to the sum. Thus, crossing out even-left rows correctly excludes terms that would not appear in the distributive expansion of the product [^notfound].

> Let me consider a formal proof sketch to ensure this holds generally. I can use induction on the multiplier. Base case: if the multiplier is 1, we halt immediately and return the other factor, which is correct. Inductive step: if the multiplier is even, say 2m, then 2m × n = m × (2n), so we halve the left and double the right and proceed inductively. If the multiplier is odd, say 2m + 1, then (2m + 1) × n = m × (2n) + n, so we halve the left, double the right, and add the current right value when the left is odd. This loop invariant preserves the equality of the running total with the desired product, proving correctness for all positive integers [^notfound].

> But wait, what if the original numbers are zero or negative? I should double-check edge cases. If either factor is zero, the product is zero and the algorithm will still work because all right-column values will be zero. If negative numbers are involved, the method can be adapted by handling signs separately and applying the same halving/doubling logic to absolute values, then restoring the sign at the end. Historically, this algorithm is designed for positive integers, so extending it to negatives requires a sign convention, but the core principle remains intact [^notfound].

> Next, I should review the historical and practical context. This method has been used for centuries in various cultures and is a classic example of algorithmic thinking that predates modern computing, illustrating how binary decomposition and distributivity enable multiplication using only halving, doubling, and addition. Hold on, I should verify that no advanced arithmetic is required beyond these operations, which aligns with the constraint that the user can add and divide or multiply by 2, but not perform general multiplication or division by larger numbers [^notfound].

> Finally, let me reconsider the example to ensure there is no subtle error I missed. The left column goes 97, 48, 24, 12, 6, 3, 1, which corresponds to the binary bits of 97 from most significant to least significant, and the right column progresses as 23, 46, 92, 184, 368, 736, 1472. Adding the right values opposite odd left values (23, 736, 1472) gives 2231, which equals 97 × 23. I should confirm once more: 97 × 23 is indeed 2231, so the worked example is consistent and the explanation holds [^notfound].

---

This "strange multiplication" is the **ancient Egyptian (or Russian peasant) method** [^notfound], which works because it decomposes multiplication into repeated doubling and selective addition, mirroring binary representation and the distributive property. The left column halves the first factor, tracking which powers of 2 contribute to the product; the right column doubles the second factor, scaling it to those powers. Only rows with odd left values are added because oddness indicates a 1 in the binary expansion of the first factor, so those rows correspond to terms like (23 × 2ᵏ) that must be included in the sum. The total is the product because (97 × 23 = (64 + 32 + 1) × 23 = 64 × 23 + 32 × 23 + 1 × 23), which equals 1472 + 736 + 23 = 2231 [^notfound].

---

## Step-by-step explanation

### Binary representation and decomposition

Every integer can be expressed as a sum of distinct powers of 2 (binary representation). For example, (97 = 64 + 32 + 1), which corresponds to the binary (1100001₂). The left column's halving sequence exposes these powers: each odd result signals a 1 in that bit position, meaning the corresponding right-column value should be included in the sum [^notfound].

---

### Doubling and halving process

The right column doubles the second factor (23) each time, generating (23 × 2ᵏ) for successive powers of 2. The left column halves the first factor, mapping each step to a bit in its binary form. This pairing aligns each power of 2 in the first factor with a scaled version of the second factor ready to be added if needed [^notfound].

---

### Selective addition based on odd/even status

Only rows with an odd left value are added because oddness indicates a 1 in the binary expansion of the first factor. In the example, the odd left values are 97, 3, and 1, so we add (23 × 1), (23 × 32), and (23 × 64), which are 23, 736, and 1472, respectively. Even left values correspond to 0 bits and are excluded from the sum [^notfound].

---

### Mathematical verification

The sum of the selected right-column values equals the product:

23 + 736 + 1472 = 2231

This matches (97 × 23), confirming the method. Algebraically, the total is:

23 × (1 + 32 + 64) = 23 × 97 = 2231

---

## Generalization and historical context

This method is a **historical algorithm** used in ancient Egypt and other cultures, and it remains a classic example of binary arithmetic and the distributive property. It is efficient because it replaces multiplication with doubling, halving, and addition — operations that are easy to perform manually or with simple tools [^notfound].

---

## Conclusion

The "strange multiplication" works because it **leverages binary decomposition and the distributive property**: the left column identifies which powers of 2 compose the first factor, and the right column provides the second factor scaled to those powers. By adding only the rows with odd left values, we sum exactly the terms needed to reconstruct the product, yielding the correct result with minimal operations [^notfound].

---

## References

### A DNA-based system for selecting and displaying the combined result of two input variables [^afa284ca]. Nature Communications (2015). Medium credibility.

Oligonucleotide-based technologies for biosensing or bio-regulation produce huge amounts of rich high-dimensional information. There is a consequent need for flexible means to combine diverse pieces of such information to form useful derivative outputs, and to display those immediately. Here we demonstrate this capability in a DNA-based system that takes two input numbers, represented in DNA strands, and returns the result of their multiplication, writing this as a number in a display. Unlike a conventional calculator, this system operates by selecting the result from a library of solutions rather than through logic operations. The multiplicative example demonstrated here illustrates a much more general capability — to generate a unique output for any distinct pair of DNA inputs. The system thereby functions as a lookup table and could be a key component in future, more powerful data-processing systems for diagnostics and sensing.

---

### A DNA-based system for selecting and displaying the combined result of two input variables [^325e4170]. Nature Communications (2015). Medium credibility.

Discussion

In this work we have demonstrated examples of a DNA-based lookup table exemplified by the selection of results of a multiplication. It should, however, be clear that we could have chosen any operation that requires two inputs and results in a one- or two-digit output. Since the method can differentiate the order of the inputs, that is, sequence-wise 1 × 2 is different from 2 × 1, the method could also be used for subtraction. In more general terms, it is a method to retrieve a relation between individuals in populations X and Y that has been encoded in the result library. It would also be possible for one combination of inputs X and Y to have selected more results. Since each of inputs X and Y are encoded by 8 nt, the input libraries can principally have 65,536 different members each and the result library can principally have 4.29 billion members. Synthesizing this number of sequences is obviously unrealistic, and cross-reactions between sequences and self-complementarity would be an increasing problem the closer the library numbers come to the principal limits. However, the method has sufficient diversity to cover synthetically realistic numbers of inputs and results.

The translation is also a selection process; however, in contrast to the result selection where two inputs are converted into one result, the translation segregates the result in an array of data that are required for displaying the result. We have demonstrated this concept for the seven-segmented displays, and the mechanism by which the result is translated resembles an electronic calculator. In the translation the result is converted into a binary code that addresses the individual lines of the display (Table 1).

Most of DNA-based logic systems are binary and thus also require binary inputs where a DNA strand represents 0 or 1. The translation process described here would enable the conversion of higher-order information in a DNA input strand to an array of binary numbers.

As the complexity of oligonucleotide-based technologies such as biosensing and regulating systems in synthetic biology increases, more complex and multiplexed output information of such systems becomes available. The method presented here is an example of a method to combine pieces of information encoded in DNA strands and process it to a common result that is displayed in an immediately readable format. For example, two disease-marker oligonucleotide inputs of biologic origin could potentially be combined on a template via the four-way junction method and only if both markers were present they would select a solution from a solution library.

---

### The cost of large numbers of hypothesis tests on power, effect size and sample size [^4b1c0c24]. Molecular Psychiatry (2012). Low credibility.

Results

For a fixed targeted effect size and fixed sample size, power decreases as the number of tests and corresponding critical value increase (Table 1, Figure 2a). If the power for a single test is 80%, the power is approximately 50% for 10; 10% for 1000; and 1% for 100 000 Bonferroni-adjusted tests. To avoid a drop in nominal power without increasing sample size, an investigator may target larger effect sizes (Table 1, Figure 2b) using equation (1). For one million tests, the effect size multiplier is 2.25 at 80% power and 2.08 at 90% power. Suppose it has been determined that a sample of 100 yields 80% power to detect a difference in group means when the mean in group 1 is 10 and the mean in group 2 is 12. The original sample size of 100 would also yield 80% power to detect a mean difference of 2.25 × 2 = 4.50, with Bonferroni adjustment for one million tests. The effect size multiplier works on the scale of the underlying Normally distributed test statistic. For example, effect size multiplication for an OR should be carried out after conversion to the natural log scale. Suppose 500 total cases and controls provide 90% power to detect an OR = 1.2, the effect size multiplier for 90% power and 1000 tests is 1.65. Since log e (1.2) = 0.18, the same sample of 500 yields 90% power in 1000 tests to detect a log e (OR) = 1.65 × 0.18 = 0.30 or, equivalently, an OR = exp(0.30) = 1.35.

---

### Practical guide to understanding multivariable analyses: part a… [^021c7d3a]. Otolaryngology — Head and Neck Surgery (2013). Low credibility.

Multivariable analyses are complex statistical methods to evaluate the impact of multiple variables on outcomes of interest. Books have been written on each of these methods detailing the mathematical and statistical objectives and processes. However, we have found very little in the way of brief reports that help the nonstatistically trained physician obtain a basic understanding of multivariable analyses in order to have some understanding of the increasing literature using these methods. This work is organized in 2 parts. This article, Part A, addresses the "big 4" algebraic methods of multivariable analysis. The primary focus of Part A is to present a brief "primer" to help the reader understand the methods and uses; it expressly avoids the many details of statistical assumptions, calculations, and myriad branching alternatives. Part B will concentrate on conjunctive consolidation and will focus on enough information to allow the interested reader to actually perform the analysis. For the statistical scholar, we have included references to several voluminous serious works.

---

### The cost of large numbers of hypothesis tests on power, effect size and sample size [^d03b4141]. Molecular Psychiatry (2012). Low credibility.

A third way to quantify the cost of multiple tests is the increase in sample size needed to maintain the original level of power at the original effect size. If the sample size is increased by a factor m, it has the effect of dividing the standard error by, narrowing the widths of both the null and alternative curves accordingly (Figure 1d). To offset the addition of Δ, the distance between the two curves expressed in terms of the new standard errorfor the larger sample size and H tests should equal the distance expressed in the original standard errorsfor a single test. Thus, m should satisfyor

For two tests, α = 0.05 and 80% power, the sample size should be multiplied by m = 1.2 to maintain the original power at the original effect size. Again, m depends only on H, α and β and not the effect size or original sample size. Note that the sample size multiplier is the square of the effect size multiplier.

More generally, suppose a study design with H tests at unadjusted significance level α and power 1− β is to be compared to a second study design with H* tests at unadjusted significance level α * and power 1− β *, then the detectable effect size for the second design relative to the effect size of the first design iswhere the numbers in the denominator are based on the initial design and the numbers in the numerator are based on the second. Furthermore, a sample size multiplier for the second design relative to the first is given by

Numerical results were calculated using the Normal distribution and expressions (1) and (2) in R. All tests are two tailed and, if not otherwise specified, at significance level α = 0.05. Logs, unless otherwise indicated, are base 10. A Cost of Multiple Tests calculator that implements equations (1)–(4) in Microsoft Excel 2003 for any choices of H, α and β is also provided (Supplementary Table 1).

---

### Quantitative imaging metrics for the assessment of pulmonary pathophysiology: an official American Thoracic Society and Fleischner society joint workshop report [^6517cbfd]. Annals of the American Thoracic Society (2023). High credibility.

Basic stereological measurements — first-order parameters and probes identify that "First-order parameters are volume (three-dimensional [3D]), surface (two-dimensional [2D]), length (one-dimensional [1D]), and number (zero-dimensional [0D])". Measurements "are performed using geometric test probes, such as points (0D), lines (1D), planes (2D), or volumes (3D)". These probes create countable events in the image, and "Raw counts provide ratios… that are multiplied by the reference space volume to obtain absolute measures for the lung or subcompartment".

---

### Combinatorial summation of feynman diagrams [^86070c11]. Nature Communications (2024). High credibility.

Combinatorial summation for the determinant

A good starting point is the algorithm for division-free calculation of the determinantbased on its permutation cycle decomposition. In terms of the cycle covers, representing an ordered sequence of matrix indices (called elements) grouped into m cycles by the parenthesis, the determinant becomeswhereand. For instance, the cycle coverhasand. In this form, one easily recognises Feynman's rules for constructing the diagrams, with the cycles corresponding to fermionic loops. It is useful to view building each, one element at a time, by an ordered walk of 2 n steps, where at each step l the current element is e and the new elementis selected according to some rules, while the currentis multiplied by, as well as by an additional − 1 when the cycle is closed. An expression like Eq. (2) is then evaluated as a sum over all such walks. The central observationis that, when different walks are executed in parallel, there will be many for which the step l is identical. Thus, before step l the weights of all such walks constructed up to this point can be combined, and the multiplication byapplied to the sum. This suggests linking all walks in a graph, such as that in Fig. 1 c, where the result of the summation before each step is stored in the nodes and the steps are the edges. An optimal structure of the graph minimises the number of times the multiplication byneeds to be performed, and finding it is the task of dynamic programming. In the case of the determinant, the total number of edges can be made only polynomial in n.

---

### Correction to: a method to quantify the "cone of economy" [^c425b20a]. European Spine Journal (2018). Low credibility.

Unfortunately, in the abstract at the results section units have been published incorrectly.

---

### An analytical theory of balanced cellular growth [^a5efc002]. Nature Communications (2020). High credibility.

Proof: The active stoichiometric matrix A may have more rows than columns. In this case, m + 1 > n, and the rows for exactly n metabolites are linearly independent, as row and column rank must equal. As a consequence, the remaining m + 1 − n metabolite concentrations are linearly dependent on the concentrations of the n independent metabolites. These dependent concentrations are not free variables, and hence they can be put aside and dealt with separately.

We decompose the linear system of equations represented by constraint (11) into two parts, rearranging the rows of A into matrices B, C such that B contains the rows for the independent reactants. As A has full column rank, choosing linearly independent rows results in a square matrix B of full rank (#rows(B) = rank(B) = rank(A)). Let b be the subvector of reactant concentrations a that correspond to the rows of B, and let c be the subvector of the remaining reactant concentrations corresponding to the rows of C. We can then split the mass balance constraint (11) into two separate equations: B is a square matrix of full rank, so there is always a unique inverse B −1. Multiplying both sides of the first equation by B −1 from the left, we obtain the desired equation for v. Inserting this result into the second equation results in the desired equation for c. □

---

### A soft photopolymer cuboid that computes with binary strings of white light [^3d3bff3f]. Nature Communications (2019). High credibility.

Fig. 5
Binary arithmetic. Schemes of addition of a 01 + 10 and b 01 + 01. Addends are input as binary strings through Beams Y and Z; place values are indicated. The result is contained in the antidiagonal of the output (yz plane). Carrier configurations 0D (red spots) = 0, 1DV (green spots), 1DH (blue spots) = 1, 2D (gray spots) = 0C1. c Addition is commutative and different addend pairs generate different output patterns but the same sum; examples from the 2² × 2² system are tabulated. Any addend may be added on the grid, provided that the sum does not exceed N digits for a 2ᴺ × 2ᴺ grid. Shorter binary strings are accommodated by appending leading zeroes as shown in d for the sum of 10010 and 1010 performed on a 2⁵ × 2⁵ system. Subtraction is accomplished by adding the two's complement of the minuend. We determine e the difference between 10010 and 1010 by adding 10110 and dropping the leftmost digit. This occurs naturally as the bottom–left voxel signifies a carried digit that cannot be contained within the grid. f A flowchart summarizing the steps of an addition algorithm including the treatment of carried over digits as exemplified in the accompanying output pattern corresponding to the sum of 01011 and 00110. (c – f scale bar = 500 μm)

The system is scalable. For a 2ᴺ × 2ᴺ system, the maximum computable sum = 2ᴺ − 1. For example, in the 2⁵ × 2⁵ system shown in Fig. 5d, the largest possible sum is 31. We use this system to demonstrate the addition of 10010 and 01010 to yield a sum of 11100. Using the same system, we also demonstrate the subtraction of the same numbers (Fig. 5e). In the case of the latter, we employ the radix complement method employed in digital computing (Supplementary Note 2). The addition of 00110 + 01011 to yield 10001 in Fig. 5f illustrates the treatment of carried over digits, which is summarized in the accompanying flowchart. Specifically, a carried over digit can only be deposited at the nearest element of increased place value that possesses a value of 0 or 0C1. When carried over an element possessing a value of 1, a result of 0 is deposited at that place value (further details and additional examples are provided in Supplementary Fig. 6).

---

### Researcher's guide for the preparation of tables [^4c1babbe]. The Journal of Surgical Research (2025). Medium credibility.

Almost all scientific articles include tables, but there is little guidance on how to prepare tables. Authors should enhance the focus on the tables' data-ink, the essential information, and minimizing the no-data-ink, nonessential elements. The six principles of Gestalt on human perception can also be applied to the table to increase readability. Through this review, we provide a practical guide and an overview of how to prepare readable, informative tables. The five steps for the preparation of tables include (1) tables with a clear purpose; (2) using a universal layout; (3) selecting relevant data for Table 1 versus other tables; (4) simplifying variables by categorizing, standardizing, and reducing; and (5) enhancing the readability of numbers and decimals. This results in informative tables that contain data, serving a specific purpose for the reader and increasing readability.

---

### Polygenic risk variants for type 2 diabetes susceptibility modify age at diagnosis in monogenic HNF1A diabetes [^62bef782]. Diabetes (2010). Low credibility.

Statistical methods.

We performed family-based association analyses using the ASSOC program from S. A.G.E. (Statistical Analysis for Genetic Epidemiology) software package, version 5.4.2, for Linux. Assuming randomly sampled independent pedigrees, ASSOC simultaneously tests for associations between a quantitative trait and one or more covariates of interest and estimates familial variance components from the given familial correlations. In our study, the trait of interest was age at diabetes diagnosis, while the main covariate of interest was the number of type 2 diabetes risk alleles. As one of the parameters for the ASSOC program, we set the family effect option to "true", thus including the random nuclear family effect as an additional term in the regression model. Relationships between family members were fully established for most pedigrees. In some of the large pedigrees, we included parents and relatives that had no data for the analyses but were used by the program to accurately connect all related individuals. Singletons and five U.K. family members for whom we could not establish how they were related to other members of their pedigrees were automatically treated as one-person pedigrees and required no special handling in the model.

We assessed the effect of each risk variant on the age at diagnosis individually, jointly and by using an allele counting method to assign a genetic risk score to each patient (the sum of the number of risk alleles a person carries). The allele-counting method assumed equal and additive effects of the individual variants. We repeated this analysis using a weighted allele approach, where the genetic score was based on the previously reported odds ratios (ORs) for type 2 diabetes (obtained from a recent review). For each patient, we first calculated the sum across SNPs of the number of risk alleles at each SNP multiplied by the log of the OR for that SNP (i.e.g.enotypes were coded as 0, log(OR), or 2xlog(OR), rather than 0, 1, 2 in the allele count model). To obtain a rescaled "weighted allele count" score, we multiplied each log(OR) score by 30 (maximum number of risk alleles) and divided the product by 1.81, the sum of the 15 risk homozygote log(OR) weights.

---

### Guidelines for foodborne disease outbreak response. 2nd ed [^fa40728e]. CDC (2014). Medium credibility.

CIFOR performance indicator — outbreak clinical specimen collections — defines the metric as the "Number and % of outbreak investigations with clinical specimens collected and submitted to PHL from two or more people" and defines a foodborne illness outbreak as "The occurrence of two or more similar illnesses resulting from ingestion of a common food". Measurement methods state to "Determine the number of foodborne illness outbreaks that were investigated" for the denominator and to "Determine the number of outbreaks for which clinical specimens were collected and submitted to the PHL from two or more people" for the numerator, then "Divide the numerator by the denominator and multiply by 100". The performance fields label "Denominator (No. outbreaks) = ", "Numerator (No. outbreaks with clinical specimens collected) = " and compute "Rate (Num./Denom. x 100) = ". Feasibility notes that "This metric is associated with CIFOR Indicator 8.2.4 "Foodborne outbreaks investigated". It extends FoodCORE metrics to investigations for all pathogens".

---

### Guidelines for foodborne disease outbreak response. 2nd ed [^6085b2bc]. CDC (2014). Medium credibility.

CIFOR performance measure — confirmed cases with exposure history obtained — uses the metric "Number and % of confirmed cases with exposure history obtained", defining a confirmed case as "Case reported to local or state health department by clinical laboratory with confirmed Salmonella, Shiga toxin-producing E. coli (STEC) or Listeria infection" and defining exposure history as "An interview (of any format) that assesses exposures prior to onset of illness"; "The assessment should go beyond assessment of high risk settings and prevention education to ascertain food consumption/preference or other exposure data", with STEC requiring "disease-specific data elements identified by CSTE" and Listeria requiring "completing the Listeria case form". Measurement methods state to determine the number of confirmed cases reported (denominator) and the number with exposure history obtained (numerator), then "Divide the numerator by the denominator and multiply by 100" to create a standardized rate, and "Measure and report separately for confirmed Salmonella, E. coli (STEC) and Listeria cases". The performance specification lists "Denominator (No. confirmed cases)", "Numerator (No. cases with exposure history)", and "Rate (Num./Denom. x 100)", each for "A. Salmonella", "B. E. coli (STEC)", and "C. Listeria". Feasibility notes this metric is associated with "CIFOR Indicator 8.2.2 'Reported cases with specified foodborne illness interviewed'", is "consistent with FoodCORE common metrics for Salmonella, STEC, and Listeria", and that "Reporting numbers will allow simple comparisons from year to year for the agency, and reporting rates will allow for comparisons across agencies".

---

### Current sample size conventions: flaws, harms, and alternatives [^f34a2bb9]. BMC Medicine (2010). Low credibility.

Value of information methods

Many methods have already been described in the statistical literature for choosing the sample size that maximizes the expected value of the information produced minus the total cost of the study. See for an early discussion, for recent examples, and the introduction of for additional references. These require projecting both value and cost at various different sample sizes, including quantifying cost and value on the same scale (note, however, that this could be avoided by instead maximizing value divided by total cost). They also require formally specifying uncertainty about the state of nature; although this can be criticized as being subjective, it improves vastly on the usual conventional approach of assuming that one particular guess is accurate. These methods can require considerable effort and technical expertise, but they can also produce the sort of thorough and directly meaningful assessment that should be required to justify studies that are very expensive or that put many people at risk.

Simple choices based on cost or feasibility

Recent work has justified two simple choices that are based only on costs, with no need to quantify projected value or current uncertainty about the topic being studied. Because costs can generally be more accurately projected than the inputs for conventional calculations, this avoids the inherent inaccuracy that besets the conventional approach. One choice, called n min, is the sample size that minimizes the total cost per subject studied. This is guaranteed to be more cost-efficient (produce a better ratio of projected value to cost) than any larger sample size. It therefore cannot be validly criticized as inadequate. The other, called n root, is the sample size that minimizes the total cost divided by the square root of sample size. This is smaller than n min and is most justifiable for innovative studies where very little is already known about the issue to be studied, in which case it is also guaranteed to be more cost efficient than any larger sample size. An interactive spreadsheet that facilitates identification of n min and n root is provided as Additional file 2.

A common pragmatic strategy is to use the maximum sample size that is reasonably feasible. When sample size is constrained by cost barriers, such as exhausting the pool of the most easily studied subjects, this strategy may closely approximate use of n min and therefore share its justification. When constraints imposed by funders determine feasibility, doing the maximum possible within those constraints is a sensible choice.

---

### Sample size calculations: basic principles and common pitfalls [^c0e79f8a]. Nephrology, Dialysis, Transplantation (2010). Low credibility.

One of the most common requests that statisticians get from investigators are sample size calculations or sample size justifications. The sample size is the number of patients or other experimental units included in a study, and determining the sample size required to answer the research question is one of the first steps in designing a study. Although most statistical textbooks describe techniques for sample size calculation, it is often difficult for investigators to decide which method to use. There are many formulas available which can be applied for different types of data and study designs. However, all of these formulas should be used with caution since they are sensitive to errors, and small differences in selected parameters can lead to large differences in the sample size. In this paper, we discuss the basic principles of sample size calculations, the most common pitfalls and the reporting of these calculations.

---

### Number representations drive number-line estimates [^6bf63201]. Child Development (2020). Medium credibility.

The number-line task has been extensively used to study the mental representation of numbers in children. However, studies suggest that proportional reasoning provides a better account of children's performance. Ninety 4- to 6-year-olds were given a number-line task with symbolic numbers, with clustered dot arrays that resembled a perceptual scaling task, or with spread-out dot arrays that involved numerical estimation. Children performed well with clustered dot arrays, but poorly with symbolic numbers and spread-out dot arrays. Performances with symbolic numbers and spread-out dot arrays were highly correlated and were related to counting skill; neither was true for clustered dot arrays. Overall, results provide evidence for the role of mental representation of numbers in the symbolic number-line task.

---

### Removing spikes caused by quantization noise from high-resolution histograms [^c5531ba1]. Magnetic Resonance in Medicine (2003). Low credibility.

A novel method is presented for the removal of spikes, caused by the division of two series of integers, from high-resolution histograms. When two series of integers are divided the results take the form of a nonuniform distribution. Such a division is often used in medical imaging, due to the storage of most images as integers. An example of this is the division of the saturated and unsaturated signal intensities to obtain a magnetization transfer ratio. Histograms produced using these methods often contain spikes relating to the nonuniform distribution mentioned above. These spikes can have serious implications for certain histogram characteristics. Most commonly, peak height and location can be seriously distorted by these spikes, which have predictable locations. These spikes can be removed by the addition of uniformly distributed noise to the integer signal intensities before division.

---

### Calculating with light using a chip-scale all-optical abacus [^d88da093]. Nature Communications (2017). Medium credibility.

Machines that simultaneously process and store multistate data at one and the same location can provide a new class of fast, powerful and efficient general-purpose computers. We demonstrate the central element of an all-optical calculator, a photonic abacus, which provides multistate compute-and-store operation by integrating functional phase-change materials with nanophotonic chips. With picosecond optical pulses we perform the fundamental arithmetic operations of addition, subtraction, multiplication, and division, including a carryover into multiple cells. This basic processing unit is embedded into a scalable phase-change photonic network and addressed optically through a two-pulse random access scheme. Our framework provides first steps towards light-based non-von Neumann arithmetic.

---

### Cortical population activity within a preserved neural manifold underlies multiple motor behaviors [^a428039d]. Nature Communications (2018). Medium credibility.

CCA starts with a QR decomposition of the transposed latent activity matrices L A and L B. The first m column vectors ofprovide an orthonormal basis for the column vectors of. We then construct the m by m inner product matrix of Q A and Q B and perform a singular value decomposition to obtain

The elements of the diagonal matrix S are the canonical correlations (CCs), sorted from largest to smallest. The new manifold directions that CCA finds so as to maximize the pairwise correlations between latent activities across the two tasks are the corresponding m by m matrices

To implement this method, the matrices L i, i = A, B included all the concatenated trials for each of the two tasks being compared. To assemble these data matrices, we first equalized the number of trials across all tasks and targets within the corresponding session; we took the first k trials for each task and target, with k being the minimum number of successful trials across all targets and tasks within the session. When comparing tasks with different number of targets, the number of trials per target was adjusted so as to equalize the number of trials per task. For each trial, we used either a 700 ms long (wrist tasks) or a 1000 ms long (reach-to-grasp tasks) window of neural data, starting around target onset or go signal. We then concatenated these k individual trials according to the common sequence of visual targets. When comparing two 1-D wrists tasks, we matched the trials by target location; when comparing the 2-D isometric task to any of the 1-D wrist tasks, we labeled targets according to their projection onto the horizontal axis. No target matching was done for the reach-to-grasp tasks, as the ball task had no targets. We did not exclude trials based on their execution time, or based on the EMG, kinematics, or force patterns; only the few failed trials were excluded.

---

### Computing high-degree polynomial gradients in memory [^c42e580a]. Nature Communications (2024). High credibility.

Fig. 1
Pseudo gradient computation for binary-variable functions.

a Considered 4 th -degree polynomial function. Note that binary-variable high-order polynomials are multilinear, i.e. the degree of any variable in a term is not more than one. b – e Main idea of the proposed approach showing (b) crossbar memory array implementation and (c – e) three in-memory computing operations for parallel gradient computation. c Sums of monomial variables are first computed in the forward vector-by-matrix multiplication pass. These values are compared to monomial orders in backward steps (d) and (e) to identify break and make type monomials. Then, unit inputs, scaled by a monomial factor, are applied for the identified break and make monomials in the backward vector-by-matrix multiplication. Finally, the results are appropriately gated at the periphery to compute make and break terms of the pseudo-partial derivatives for each variable. The partial derivatives in question correspond to a difference between make and break components, as shown in panels (a, d, e). Values shown in blue correspond to specific variable assignments x 1 = 1, x 2 = 0, x 3 = 1, x 4 = 0.

---

### Nonlinear optical components for all-optical probabilistic graphical model [^c57b5600]. Nature Communications (2018). Medium credibility.

Results

Wavelength multiplexing architecture

The multiplier of the message passing algorithm of Eq. (1) can be written with natural logarithmic (ln), summation and exponential operations (Fig. 2a) as, This embodiment of the multiplier is easier to implement optically. In the wavelength multiplexing architecture of Fig. 2b, each node is represented by a different wavelength shown by a different color, since the spectral bandwidth can be equally divided and used for each node.

Fig. 2
Wavelength multiplexing architecture. a ln-sum-exp scheme to multiply two numbers. b Schematic representation of the sum-product message passing algorithm (SPMPA) for node m. The spectral bandwidth is divided equally as a representation of each node (different color indicates different wavelength). The summation unit sums across wavelength for each probability vector that emerges from the natural logarithm (ln) modules

The graph in Fig. 2b has N nodes and the alphabet size is K. To find the updated probability vector of the target node (node m in Fig. 2b), each message from its neighbor nodes is first multiplied with a compatibility matrix whose elements are conditional probabilities. This operation is called vector-matrix-multiplication (VMM). The outputs of the VMM are then multiplied element-wise and normalized to yield the updated probability vector of the target node. The product of all messages is replaced with logarithmic, summation and exponential operations as shown in Eq. (2). These operations are applied to every node in order to determine its updated probability vector. The updated vectors are then used in subsequent iterations until their values reach steady state. Thus, the two most important mathematical operations required to compute the probability vector are multiplication and division for normalization. The natural logarithmic function can be implemented optically by two photon absorption(TPA), while the exponential function can be optically realized through saturable absorption (SA), and the summation function by the fan-in process, respectively. However, using analog optics to implement the mathematical functions and operations can induce noise –, which can affect the performance of the optical solution of the PGMs. Therefore, we performed simulations to determine the effect of noise on the failure rate of the sum-product message passing algorithm. Our results indicate a 99% success rate for a graph with one-million nodes, an alphabet size of 100 and 20% connection density. In other words, the optical implementation of the sum-product message passing algorithm is very tolerant to the noise (Supplementary Note 3).

---

### A high-fidelity cell lineage tracing method for obtaining systematic spatiotemporal gene expression patterns in caenorhabditis elegans [^3b5aaa55]. G3 (2013). Low credibility.

Decoding the Lineage

The hierarchical probabilistic representation allows us to evaluate the probability of any given lineage as the proportional product of the generative, discriminative, general top-down, time to event, and linkage submodels:

This proportional product can be used to determine the most probable configuration for any given 4D image series by employing hybrid simulated annealing and RJMCMC to decode the lineage. RJMCMC is an extension to standard MCMC methods that are often used in trans -dimensional model selection problems such as variable selection, mixture models and factor analysis. In our particular case, the number of nuclei and their link through time is an unknown. As a result, determining the most probable lineage becomes a trans -dimensional model selection problem. Similar work using RJMCMC methods have been applied in computer vision for identifying cells, detecting trees, plant branching, road extraction, and human tracking. Although particle filters can also be used to address the uncertainty in nuclei in the lineage, they are known to have difficulties in high dimensional problems (558 cells at the end of embryonic development), where the number of particles needed to accurately represent the underlying model is both computationally and memory prohibitive.

We decompose the RJMCMC methods into two distinct set of moves: single time point RJMCMC moves, and multi-time point RJMCMC moves. The single time point RJMCMC moves behave similar to traditional decoding approaches such as particle filters, by addressing each individual time point in a sequential fashion t = 1, 2, 3,… through a series of reversible jumps moves such as adding a point, or removing a point within a given time point. The multi time RJMCMC methods are akin to back filtering methods of particle filters in that they can occur over multiple time points and at any current or previous time and are generally defined by moves that affect the tree of the lineage, such as: adding a branch to the lineage, or removing a branch from the lineage. By combining both of these approaches, we are able to address issues that are difficult to account for using standard sequential RJMCMC methods, such as false deaths or lack of detecting a daughter cell during a division. The full algorithmic details on the selection of moves, updates, and all details on empirical proposals are described in.

---

### The cost of large numbers of hypothesis tests on power, effect size and sample size [^9df69432]. Molecular Psychiatry (2012). Low credibility.

To compensate for a greater number of tests, a more realistic strategy may be to increase the sample size (equation 2). Table 1 and Figure 2c give sample sizes needed to maintain the original power at the original targeted effect size. For one million tests, the sample size multiplier is 5.06 for 80% power and 4.33 for 90% power, using equation (2). In the first example above, 506 subjects would be sufficient to reach 80% power to detect that the means differ by 2 in one million Bonferroni-adjusted tests. Although it might appear counterintuitive, the sample size multiplier is smaller for 90% power because the initial sample size is larger. In the same example, 132 subjects would be needed to reach 90% power for one test. For 90% power for one million tests, 4.33 × 132 = 572 subjects are needed. Noting the nearly linear relationship in Figure 2c, we also obtained an approximate rule-of-thumb for the sample size multiplier by fitting zero-intercept linear regression models to the results in Figure 2c. The estimated slopes show that m is approximately, where γ = 1.2 for 50% power, 0.68 for 80% power, 0.55 for 90% power and 0.38 for 99% power.

The rate at which the critical value and, consequently, the effect size and sample size multipliers increase becomes slower and slower as the number of tests becomes larger (Figure 2d), owing to the exponential decline in the tails of the Normal density. For example, effect size multipliers for one million vs ten million tests at 80% power are 2.25 and 2.39, respectively. Sample size multipliers are 5.06 and 5.71. At 80% power, ten million tests require only a 6% increase in the targeted effect size or a 13% increase in the sample size when compared to one million tests. In contrast, 10 tests require a 30% increase in the targeted effect size or a 70% increase in the sample size as compared with a single test. For 80% power and one billion or one trillion tests, the required sample sizes, respectively, are approximately 7 or 9 times that needed for a single test. See Table 1 for some numerical results and the provided Excel calculator (Supplementary Table 1) to explore unreported results and specific study designs.

---

### A fast and unbiased procedure to randomize ecological binary matrices with fixed row and column totals [^16d0e5d0]. Nature Communications (2014). Medium credibility.

A well-known problem in numerical ecology is how to recombine presence-absence matrices without altering row and column totals. A few solutions have been proposed, but all of them present some issues in terms of statistical robustness (that is, their capability to generate different matrix configurations with the same probability) and their performance (that is, the computational effort that they require to generate a null matrix). Here we introduce the 'Curveball algorithm', a new procedure that differs from existing methods in that it focuses rather on matrix information content than on matrix structure. We demonstrate that the algorithm can sample uniformly the set of all possible matrix configurations requiring a computational effort orders of magnitude lower than that required by available methods, making it possible to easily randomize matrices larger than 10(8) cells.

---

### Number needed to treat (or harm) [^d0b683ae]. World Journal of Surgery (2005). Low credibility.

The effect of a treatment versus controls may be expressed in relative or absolute terms. For rational decision-making, absolute measures are more meaningful. The number needed to treat, the reciprocal of the absolute risk reduction, is a powerful estimate of the effect of a treatment. It is particularly useful because it takes into account the underlying risk (what would happen without the intervention?). The number needed to treat tells us not only whether a treatment works but how well it works. Thus, it informs health care professionals about the effort needed to achieve a particular outcome. A number needed to treat should be accompanied by information about the experimental intervention, the control intervention against which the experimental intervention has been tested, the length of the observation period, the underlying risk of the study population, and an exact definition of the endpoint. A 95% confidence interval around the point estimate should be calculated. An isolated number needed to treat is rarely appropriate to summarize the usefulness of an intervention; multiple numbers needed to treat for benefit and harm are more helpful. Absolute risk reduction and number needed to treat should become standard summary estimates in randomized controlled trials.

---

### Input-output maps are strongly biased towards simple outputs [^56a9cad5]. Nature Communications (2018). Medium credibility.

L-systems

All valid L-systems consisting of a single starting letter F, followed by rules made of symbols from {+, −, F } and length ≤ 9 were generated. The outputs were coarse-grained using a method suggested in ref.to associate binary strings to any non-cyclical graph with a distinguished node called the 'root' (these graphs are known as rooted trees). Specifically, by walking along the branches, starting right, and recording whether it is going up (0) or down (1), a binary string representation of the tree is made. The slope a = 0.13 was obtained via Eq. (4), using the values of N O andfrom the full enumeration of inputs, while b = 2.41 was obtained using the modal complexity value, as above.

Matrix map

For the matrix map represented in Fig. 1f, we took a 32 × 32 matrix with all entries chosen uniformly from {−1, 1}, and sampled 10⁹ out of 2³² ≈ 4 × 10⁹ inputs. Further examples of this random map can be found in Supplementary Note 12. For the circulant matrices used in Figs. 1e and 2, we generated the first row, which determines the map, with entries chosen uniformly from {−1, 1}. For the simple map in Fig. 1e, we chose a first row with a low complexity. In this map the estimate of the slope from Eq. (4) did not work well possibly because a large fraction of the inputs map to a single-output vector made up of all 0s. So in Fig. 1e, the slope was simply fit to the data. Further discussion of the circulant matrix map can be found in Supplementary Note 12.

Data availability

The data sets generated during and/or analysed during the current study are available from the corresponding authors on reasonable request.

---

### Current sample size conventions: flaws, harms, and alternatives [^8e4148bc]. BMC Medicine (2010). Low credibility.

Background

The belief remains widespread that medical research studies must have statistical power of at least 80% in order to be scientifically sound, and peer reviewers often question whether power is high enough.

Discussion

This requirement and the methods for meeting it have severe flaws. Notably, the true nature of how sample size influences a study's projected scientific or practical value precludes any meaningful blanket designation of < 80% power as "inadequate". In addition, standard calculations are inherently unreliable, and focusing only on power neglects a completed study's most important results: estimates and confidence intervals. Current conventions harm the research process in many ways: promoting misinterpretation of completed studies, eroding scientific integrity, giving reviewers arbitrary power, inhibiting innovation, perverting ethical standards, wasting effort, and wasting money. Medical research would benefit from alternative approaches, including established value of information methods, simple choices based on cost or feasibility that have recently been justified, sensitivity analyses that examine a meaningful array of possible findings, and following previous analogous studies. To promote more rational approaches, research training should cover the issues presented here, peer reviewers should be extremely careful before raising issues of "inadequate" sample size, and reports of completed studies should not discuss power.

Summary

Common conventions and expectations concerning sample size are deeply flawed, cause serious harm to the research process, and should be replaced by more rational alternatives.

---

### Work-relatedness [^cdc774cb]. Journal of Occupational and Environmental Medicine (2018). Medium credibility.

Work-relatedness — summary states that "The determination of work-relatedness should utilize a reproducible method".

---

### The minimal work cost of information processing [^2ef3664b]. Nature Communications (2015). Medium credibility.

Results

The Framework

We determine a general expression for the minimal amount of work needed to carry out any given logical process. This can be for example an AND gate or any quantum or classical computation; most generallyis defined as any completely positive, trace-preserving map from quantum states on an input Hilbert spaceto quantum states on an output Hilbert space. We assume these spaces to be of finite dimension for simplicity; note that such a space can be a subspace of an infinite-dimensional Hilbert space in which the relevant computation or logical process takes place. The terminology 'logical process' is meant to emphasize that the mathematical objectonly specifies for each input state the corresponding output state and does not prescribe its physical realization, which would consist of a full description of a physical system including the parts of its environment that are relevant to determine its time evolution. Note that in performing a logical process one does not merely transform one quantum state into another; rather, the output must be related to the input in a precisely specified way. In the case where the input is a classical value, this means that the output depends on the particular input value received, and not only on the distribution of inputs. This might be checked in practice, for example, if one keeps a copy of the input as a reference system and observes the correlations between the output and the reference system.

---

### Work-relatedness [^d363de6d]. Journal of Occupational and Environmental Medicine (2018). Medium credibility.

Work-relatedness — evaluating the evidence explains that for epidemiologic surveillance a "highly sensitive, relatively nonspecific case definition is frequently employed", which "may increase the rate of screening yield for study, but generally produces a high rate of false-positives", and that if surveillance suggests an association, "more formal research can be done" with a research-study definition that is "much more rigid, traditionally implying 95% confidence that the purported causal relationship is not statistically spurious", with "The methodology for inferring a causal association" provided in Table 1.

---

### Guidelines for foodborne disease outbreak response. 2nd ed [^355cc092]. CDC (2014). Medium credibility.

CIFOR performance indicators — outbreaks detected from complaints — define a metric and standardized rate for surveillance based on complaint data. The metric counts "Number outbreaks detected as a result of foodborne illness complaints" and calculates the "Rate of outbreaks detected per 1,000 complaints received". Measurement methods specify to "Determine the number of foodborne illness complaints that were received during the year. This will be the denominator for the metric", and to "Determine the number of foodborne illness outbreaks that were detected as a result of a foodborne illness complaint investigation during the year. This will be the numerator for the metric", then "Divide the numerator by the denominator and multiply by 1,000". Definitions include: "Outbreak detected from a complaint: A foodborne illness outbreak that was detected as a result of a foodborne illness complaint investigation", "Foodborne illness outbreak: The occurrence of two or more similar illnesses resulting from ingestion of a common food", and "Foodborne illness complaint: A report of illness experienced by one or more persons following exposure to a specific event or establishment". Feasibility notes that the metric "is associated with CIFOR Indicator 8.2.1 'Foodborne complaints investigated'" and that it "provides a consistent expectation for the use of complaint data system", enabling comparisons "from year to year" and "across agencies".

---

### Using the number needed to treat in clinical practice [^646ae153]. Archives of Physical Medicine and Rehabilitation (2004). Low credibility.

The number needed to treat (NNT) is gaining attention as a method of reporting the results of clinical trails with dichotomous outcome measures. The NNT is defined as the number of patients who would need to be treated, on average, with a specific intervention to prevent 1 additional bad outcome or to achieve 1 desirable outcome in a given time period. Because it reports outcomes in terms of patient numbers, it is extremely useful to clinicians for making decisions about the effort expended with a particular intervention to achieve a single positive outcome. This special communication describes the NNT statistic and its utility for choosing clinical interventions.

---

### Guidelines for foodborne disease outbreak response. 2nd ed [^d3705f5c]. CDC (2014). Medium credibility.

Performance indicators — CIFOR performance measure: foodborne illness outbreak rate defines the metric as "Number foodborne outbreaks reported, all agents" with the rate expressed as "Rate of outbreaks reported / 1,000,000 population". A "Foodborne illness outbreak" is "The occurrence of two or more similar illnesses resulting from ingestion of a common food", and the "Foodborne illness outbreak rate" is "The number of confirmed foodborne illness outbreaks within a jurisdiction during a year, divided by the population of the jurisdiction x 1,000,000". Measurement methods state to determine the population of the jurisdiction as the denominator, determine the number of foodborne illness outbreaks reported during the year as the numerator, and divide the numerator by the denominator and multiply by 1,000,000 to convert to a standardized rate. Feasibility notes association with CIFOR Indicator 8.2.4 "Foodborne outbreaks investigated", aggregation of FoodCORE metrics for outbreak investigations across all pathogens, and that reporting foodborne outbreaks is part of PHEP Performance Measure 13.3 Outbreak Investigation Reports, enabling comparisons from year to year and across agencies.

---

### Supplementary search methods versus bibliographic database searching to identify studies and study reports [^736ccc4a]. The Cochrane Database of Systematic Reviews (2025). Medium credibility.

Measures of the effect of the methods

We assume it will not be possible to estimate the effects of the interventions using risk ratios or odds ratios because, while numbers of identified studies or study reports (i.e. numerators) will be extracted, numbers of studies or study reports that could have been identified (i.e. denominators) are unlikely to be known. The denominators for a study that applies competing search methods to a common body of literature will be identical but unknown. This facilitates computing a point estimate of the risk ratio because the unknown denominator cancels the ratio. However, a standard error or other measure of precision cannot be computed for a risk ratio without making additional assumptions, which would preclude meta‐analysis.

Instead, to facilitate meta‐analysis, we plan to estimate the treatment effect as the percentage of all studies or study reports not identified by bibliographic database searching alone. For example, if a bibliographic database search identified five studies and a supplementary search identified one additional study (as exemplified in Table 1 (see Types of data)), for a total of six studies, we will estimate the effect as (1/6)×100% = 17% (95% CI 0% to 64%). In other words, in this example, the supplementary search yielded 17% more studies than the bibliographic database search alone. We will compute exact (Clopper‐Pearson) 95% confidence intervals on the percentages.

For the cost or resource outcomes (secondary outcomes), where data are amenable, we will undertake a cost comparison, focusing on the cost to identify studies or reports. We will make the following calculation: cost per study or report = (Total Cost)/(Number of reports). We anticipate 'costs' to be salary, or costs of resources. We will source up‐to‐date costs for any cost reported in a study (e.g. research assistant hourly rate, aligning to 2025 costs) and convert published costs to a common currency, adjusted for inflation (e.g. 2025 EUR). We will not adjust data for double‐counting (where the same studies or reports are identified across methods) since the effort to identify the reports is quantifiably proportioned and of interest. We will focus on incremental costs and incremental cost‐effectiveness as opposed to averages.

---

### The maintenance need for water in parenteral fluid therapy [^2bbd8b0b]. Pediatrics (1957). Low credibility.

The clinical calculator "Maintenance Fluids Calculations" for hyponatremia, acute diarrhea and parenteral nutrition.

The Maintenance Fluids Calculations is a clinical calculator used primarily in pediatric medicine. It is designed to estimate the daily fluid requirements of children, based on their weight. The calculator uses the Holliday-Segar method, also known as the "4-2-1 rule", which provides an approximation of the daily maintenance fluid needs of children in milliliters per kilogram per day.

The clinical utility of the Maintenance Fluids Calculations is to guide fluid therapy in children, particularly those who are hospitalized or critically ill. It helps to prevent both dehydration and fluid overload, which can lead to serious complications such as electrolyte imbalances and cardiac or renal dysfunction.

The calculator is generally applicable to children of all ages, from neonates to adolescents. However, it may not be suitable for use in children with certain conditions that affect fluid balance, such as renal or cardiac disease, diabetes insipidus, or conditions causing increased insensible losses. It is also not intended for use in calculating fluid needs for resuscitation or replacement of ongoing losses, which should be calculated separately.

To calculate the necessary fluid requirements for a patient based on their weight, start by obtaining the weight, which can be entered in kilograms or pounds. If given in pounds, convert to kilograms using the formula: 1 kg = 2.20462 lbs.

The program calculates three key outputs:

1. **Fluid Bolus**: Multiply the patient's weight in kilograms by 20 mL/kg. This value gives the volume of fluid needed for rapid administration.

2. Daily Maintenance Fluid Requirement: This is calculated using a tiered method:

- For weights up to 10 kg, multiply the weight by 100 mL.
- For weights between 10 kg and 20 kg, calculate 1000 mL for the first 10 kg, then add 50 mL per kg for each kilogram over 10.
- For weights over 20 kg, calculate 1500 mL for the first 20 kg, then add 20 mL per kg for each kilogram over 20.

3. **Hourly Fluid Rate**: Determine this by dividing the total daily maintenance fluid requirement by 24, giving a per-hour fluid rate.

The results are expressed as whole numbers:

- Fluid Bolus in mL
- Total Daily Fluid Requirement in mL/day
- Hourly Fluid Rate in mL/hour

Here's how the inputs translate into outputs:

| **Input weight** | **Conversion** | **Fluid Bolus (mL)** | **Daily Requirement (mL/day)** | **Hourly Rate (mL/hour)** |
|-|-|-|-|-|
| Weight in kg | None needed | Weight × 20 | As per tiered calculation | Daily Requirement / 24 |
| Weight in lbs | Weight / 2.20462 to get kg | Weight in kg × 20| As per tiered calculation | Daily Requirement / 24 |

This approach provides healthcare providers with a fluid management guideline tailored to the patient's weight, ensuring appropriate hydration levels.

---

### Guidelines for foodborne disease outbreak response. 2nd ed [^82234c6a]. CDC (2014). Medium credibility.

CIFOR performance measure — cluster source identification defines the metric as the "Number and % of clusters with more than five cases in which a source was identified". A cluster is defined as "Two or more isolates with a matching molecular subtype pattern identified in a period of two weeks". "Cluster source identification" is "The number of identified clusters for which a specific food transmission setting, meal, food item or ingredient was identified, leading the cluster to be considered an outbreak". Measurement methods state to "Determine the number of clusters that include five or more cases. This will be the denominator for the metric", and to "Determine the number of clusters for which a source was identified that include five or more cases. This will be the numerator for the metric", then "Divide the numerator by the denominator and multiply by 100". The performance section specifies Denominator "No. clusters with ≥ 5 cases", Numerator "No. clusters with ≥ 5 cases with source identified", and Rate "Num./Denom. x 100". This metric is linked to "CIFOR Indicator 8.2.5 'Case clusters investigated'.

---

### Work-relatedness [^700e045f]. Journal of Occupational and Environmental Medicine (2018). Medium credibility.

Work-relatedness — Table 1 steps for evaluating epidemiological evidence list procedural actions and criteria: "Collect all epidemiological literature reported on that disorder", "Identify the design of each study", and "Assess each study's methods" including "Exposure assessment methods and potential biases", "Disease ascertainment methods and potential biases", "Absence of significant uncontrolled confounders; consideration of residual confounding", "Addressing of other potential biases", "Adequacy of biostatistical methods and analytical techniques", and "Ascertainment of statistical significance — degree to which chance may have produced those results". The table then directs to "Assess the studies using the Updated Hill's Criteria" including "Temporality", "Strength of association", "Dose–response", "Consistency", "Coherence", "Specificity", "Plausibility", "Reversibility", "Prevention/Elimination", "Experiment", and "Predictive Performance", and to make a "Conclusion regarding the degree to which such a causal association is/is not met".

---

### How to more effectively determine what is true: the limits of intuition [^d1252be7]. Journal of Pediatric Urology (2020). Medium credibility.

The plethora of scientific data and explosion of published materials often leave it challenging to develop a clear and concise overview of many scientific topics. A number of factors may contribute to our misunderstanding. It is the focus of this article to describe primary reasons for failure to establish a clear, factual and functional understanding regarding scientific areas of inquiry.

---

### The cost of large numbers of hypothesis tests on power, effect size and sample size [^92bfb057]. Molecular Psychiatry (2012). Low credibility.

Discussion

The observation that the relationship between the number of hypothesis tests and the detectable effect size or required sample size depends only on the choice of significance level and power covers most commonly used statistical tests. This relationship is independent of the original effect size and sample size and other specific characteristics of the data, the statistical model and test procedure. Our results show that most of the cost of multiple testing is incurred when the number of tests is relatively small. The impact of multiple tests on the detectable effect size or required sample size is surprisingly small when the overall number of tests is large. When the number of tests reaches extreme levels, on the order of a million, doubling or tripling the number of tests has an almost negligible effect. This is reassuring in light of continuing developments in methods of high-throughput data collection and the trend toward data mining and exploration of a variety of statistical models entailing greater numbers of tests and comparisons. In addition, we used our results to create a power, effect size and sample size calculator to facilitate the comparison of alternative large-scale study designs.

---

### American Thoracic Society / centers for disease control and prevention / Infectious Diseases Society of America: controlling tuberculosis in the United States [^e3207a60]. American Journal of Respiratory and Critical Care Medicine (2005). Medium credibility.

Grading system for ranking recommendations — ATS/CDC/IDSA TB control statement defines how recommendations are rated: "The recommendations contained in this statement… were rated for their strength by use of a letter grade and for the quality of the evidence… by use of a Roman numeral (Table 1)". Strength A is "Highly recommended in all circumstances", B is "Recommended; implementation might be dependent on resource availability", and C is "Might be considered under exceptional circumstances". Quality of evidence I is "Evidence from at least one randomized, controlled trial", II is "Evidence from (1) at least one well-designed clinical trial, without randomization; (2) cohort or case-control analytic studies; (3) multiple time-series; or (4) dramatic results from uncontrolled experiments", and III is "Evidence from opinions of respected authorities, on the basis of cumulative public health experience, descriptive studies, or reports of expert committees".

---

### Macrosomia: ACOG practice bulletin summary, number 216 [^56865091]. Obstetrics and Gynecology (2020). High credibility.

Macrosomia evidence grading and recommendation categories — studies were "reviewed and evaluated for quality according to the method outlined by the U.S. Preventive Services Task Force", with evidence levels defined as follows: "I Evidence obtained from at least one properly designed randomized controlled trial", "II-1 Evidence obtained from well-designed controlled trials without randomization", "II-2 Evidence obtained from well-designed cohort or case–control analytic studies, preferably from more than one center or research group", and "II-3 Evidence obtained from multiple time series with or without the intervention. Dramatic results in uncontrolled experiments also could be regarded as this type of evidence", plus "III Opinions of respected authorities, based on clinical experience, descriptive studies, or reports of expert committees". "Based on the highest level of evidence found in the data, recommendations are provided and graded according to the following categories:" "Level A — Recommendations are based on good and consistent scientific evidence", "Level B — Recommendations are based on limited or inconsistent scientific evidence", and "Level C — Recommendations are based primarily on consensus and expert opinion".

---

### Methods matter for dietary supplement exposure assessment: comparing prevalence, product types, and amounts of nutrients from dietary supplements in the interactive diet and activity tracking in the American association of retired persons cohort study [^155e7763]. The American Journal of Clinical Nutrition (2025). Medium credibility.

Despite these limitations, research of this nature is critical to advancing our understanding of nutrient exposures and promoting the development of novel techniques with increased accuracy and precision. Currently, no consensus exists on the best method for measuring DS intake. The comparability of 24HRs and FFQs in measuring the prevalence of use of and estimating nutrient intakes from DS varies by the nutrient of interest, the source of intake, and the type of DS product consumed. Research and guidance involving DS exposures must be interpreted with the limitations of the instruments used in mind and utilize multiple assessment strategies capturing both habitual and episodic DS.

In conclusion, DS assessment can be completed using a variety of tools. In general, methods that have the capability to query specific product-level details perform better than a finite list of questions. FFQs are commonly used in epidemiological studies, but they lack the requisite information necessary to estimate mean nutrient exposures, often rely on default amounts, and only query a limited number of products; these limitations may drive the vast differences in estimates obtained between 24HRs and FFQs for some nutrients. Thus, understanding the role of databases, data quality, confounding, and biases in discrepant findings is critical in any intake analysis; however, considering the assessment tool used and its associated measurement error is equally as important. FFQs have good reproducibility for estimating the prevalence of MVM use in some cohorts and, in turn, may be suitable for obtaining basic DS information (e.g. yes/no responses). However, for studies that aim to estimate actual amounts of nutrients, it is vital that novel, robust DS methods (e.g. inventories) beyond an FFQ that have been tested across time in various cohorts are employed. As precision nutrition evolves, quantifying absolute nutrient amounts is necessary for tailored dietary guidance – particularly when episodically consumed nutrients (e.g. vitamin D) that demand precise DS tools for accurate estimates are of interest. This study, and the work of others, illustrate that it is crucial that the selection of the DS assessment instrument is chosen and interpreted with the known limitations of each method kept firmly in mind.

---

### Filters: when, why, and how (not) to use them [^c9bfe740]. Neuron (2019). Medium credibility.

Filters are commonly used to reduce noise and improve data quality. Filter theory is part of a scientist's training, yet the impact of filters on interpreting data is not always fully appreciated. This paper reviews the issue and explains what a filter is, what problems are to be expected when using them, how to choose the right filter, and how to avoid filtering by using alternative tools. Time-frequency analysis shares some of the same problems that filters have, particularly in the case of wavelet transforms. We recommend reporting filter characteristics with sufficient details, including a plot of the impulse or step response as an inset.

---

### How to write a systematic review of reasons [^98954d21]. Journal of Medical Ethics (2012). Low credibility.

Regarding (1): before counting the number of mentions of a specific (broad or narrow) reason type, one should first remove reasons repeated within each publication: reasons in which the codes, alleged implication, person expressing attitude and attitude expressed are identical. Care must be taken particularly when counting the number of mentions when these have been used to support different conclusions. Regarding (2): because types could be made narrower or broader, it is advisable to conduct a sensitivity analysis as follows. One uses finely individuated narrow reason types in the initial analysis and counts the number of types, and then merges similar narrow types and recounts the number of types. One then calculates the difference in the counts of types when they are narrowly versus broadly individuated.

Another essential exhibit is a table of the characteristics of included publications. This enables readers to assess the state of the field and identify gaps. It is a good idea if the review also contains a list of all the publications included when a literature is difficult to track down, as in our case. Furthermore, particularly when users of the review are likely to be interested in the positions taken by individual publications, and it takes specialised skill to extract this information, decision-makers may find useful a table that shows, for each publication, the reasons endorsed by the publication, whether the reason was used for and against, the conclusion drawn from the reason and attitude taken by the author, and the publication's overall conclusion. For examples of all these exhibits, see reference.

Table 1 suggests results to derive and present in the exhibits just mentioned. We stress, however, that the choice of results will depend on the review question and literature reviewed. For further results that could be derived and presented see reference.

---

### Your results, explained: clarity provided by row percentages versus column percentages [^8a915fa6]. The Annals of Thoracic Surgery (2016). Low credibility.

The overwhelming majority of clinical research papers begin their Results section with a series of tables summarizing the distribution of significant demographics, clinical characteristics, and potential outcome variables within their study cohort. However, the presentation of these tables often leaves the reader confused or wanting different information than what is presented. One particularly vexing decision in the presentation of categoric variables is choosing to present the "row percentage" in preference to the "column percentage". This report provides a guide for clinical scientists, some of whom may not have access to statistical support, to summarize their descriptive results in the manner that provides the most clarity for the reader.

---

### An introduction to power and sample size estimation [^6d2fed57]. Emergency Medicine Journal (2003). Low credibility.

The importance of power and sample size estimation for study design and analysis.

---

### Massive yet grossly underestimated global costs of invasive insects [^7df964c3]. Nature Communications (2016). Medium credibility.

Removing potential double counts

We made every effort to eliminate redundant amounts from the monetary values we used to estimate cost sums. First, we removed values that were obvious re-estimates of older values (with the more recent estimates tending to be more reproducible than older ones; for example, Supplementary Data 1, column E). We further separated costs into 'extrapolation' versus 'actual estimate' categories (columns G and H in Supplementary Data 1, respectively). Further removing those estimates already deemed irreproducible (column F), column I indicates with absolute certainty which estimates should be retained to avoid any potential case of double counting (that is, species with reproducible estimates that do not include both extrapolated and actual estimates).

The sum of estimates in column I ($22,629,029,314) versus our sum of the total costs (US$25,166,603,981) reported in the main text is only 10.1%, which suggest that even in the unlikely case of double counting, the bias is minimal, and well within the margin of error expected for a sum of median cost rates across the globe. It is essential to note that even if a species includes both extrapolations and actual values, it does not necessarily equate to double counting because often the different estimates apply to different regions of the insect's distribution or different economic components of their costs. However, this does not exclude the possibility of double counting within the irreproducible category, simply because we cannot verify how the estimates were derived to check for instances of potential double counting.

---

### SHAVE: shrinkage estimator measured for multiple visits increases power in GWAS of quantitative traits [^2a094679]. European Journal of Human Genetics (2013). Low credibility.

Methods

We outline briefly how we test for the association of a single measurement of a trait with a given SNP, and then generalize for multiple replicates. Consider a given quantitative trait and a given SNP. Let y ij (i = 1. n; j = 1, …, k i) denote individual i 's j th repeated measure of the trait, or his/her residual for that trait after adjusting for one or more variables (eg, sex and age). Let G i denote the number of minor alleles of the given SNP for individual i. Let G be the vector (G 1, G 2. G n) containing the number of alleles (0,1 or 2) for all individuals. Similarly, let Y 1 be the vector (y 11, y 21, …, y n 1), containing the first measurement of the trait for all individuals. To test the association between Y 1 and G using an additive model, Y 1 is regressed on G such that y i 1 = β 1 G i + α + e i 1, and an estimate for β 1 is obtained (β * 1). We then we divide β * 1 by its standard error and obtain a z- statistic. Next for each z -statistic a corresponding P -value is obtained. This is done separately for every SNP and every trait.

---

### How to read a scientific research paper [^aaad1254]. Respiratory Care (2009). Low credibility.

Reading is the most common way that adults learn. With the exponential growth in information, no one has time to read all they need. Reading original research, although difficult, is rewarding and important for growth. Building on past knowledge, the reader should select papers about which he already holds an opinion. Rather than starting at the beginning, this author suggests approaching a paper by reading the conclusions in the abstract first. The methods should be next reviewed, then the results — first in the abstract, and then the full paper. For efficiency, at each step, reasons should be sought not to read any further in the paper. By using this approach, new knowledge will be obtained and many papers will be evaluated, read, and considered.

---

### Projected increase in amyotrophic lateral sclerosis from 2015 to 2040 [^75fa275a]. Nature Communications (2016). Medium credibility.

Methods

Literature search

Age- and sex-specific incidence rates were ascertained from published studies. MEDLINE (via PubMed) was searched for English-language articles published after January 1995, when the El Escorial criteria for diagnosis of ALS were implemented. The MeSH terms 'amyotrophic lateral sclerosis' or 'motor neuron disease' along with 'incidence', 'prevalence', or 'epidemiology' were used to identify articles. The search was supplemented by examining the reference sections of the retrieved publications. Only articles reporting quantitative incidence rates separated by age group and sex were selected. For countries with multiple available studies, the most recent population-based, observational study was selected.

The population structure of each country from 2015 to 2040 was obtained from the United States Census Bureau International Programs International Data Base. Two recent reviews citing survival rates of ALS were used to identify survival rates for each of the studied regions.

Case number calculation

The number of ALS cases was estimated in each country based on the product of (a) the age-specific and sex-specific incidence rates for that particular country, (b) the age-specific and sex-specific population of that country and (c) the median disease duration for that country. This provided comparable results to published prevalence data (see Chiò et al. for review). To make this work reproducible, a computer code was written in R programming to automatically obtain the data from the International Data Base website and then to calculate the ALS case numbers for 2015 and 2040.

To estimate the number of cases across the world, we applied the incidence and survival rates obtained for each country to their respective continent. If more than one study existed for a continent, the incidence and survival rates were weighted according to the size of the population covered by the study.

---

### Vaginitis in nonpregnant patients: ACOG practice bulletin, number 215 [^09076443]. Obstetrics and Gynecology (2020). High credibility.

Vaginitis in nonpregnant patients — evidence grading follows the U.S. Preventive Services Task Force method, defining I as "Evidence obtained from at least one properly designed randomized controlled trial", II-1 as "Evidence obtained from well-designed controlled trials without randomization", II-2 as "Evidence obtained from well-designed cohort or case–control analytic studies, preferably from more than one center or research group", II-3 as "Evidence obtained from multiple time series with or without the intervention. Dramatic results in uncontrolled experiments also could be regarded as this type of evidence", and III as "Opinions of respected authorities, based on clinical experience, descriptive studies, or reports of expert committees". Recommendation grades are provided as Level A — "Recommendations are based on good and consistent scientific evidence", Level B — "Recommendations are based on limited or inconsistent scientific evidence", and Level C — "Recommendations are based primarily on consensus and expert opinion".

---

### Guidance on community viral load: a family of measures, definitions, and method for calculation [^31d7d596]. CDC (2011). Medium credibility.

Table 2 — types of viral load (VL) analyses and measures — describes mean-based metrics that average per-case results. One metric is calculated by "Determine the most recent VL for each case, total the values of those VLs, and divide by the number of cases with a VL" and indicates "The average number of virus particles in a population; can be influenced by large outliers; useful for comparisons between subpopulations (e.g., disparities)". A log-transformed variant uses "Log transform the most recent VL for each case, average the log transformed values, total the average values, and divide by the number of cases with a VL" and "The log transformation will provide a stable estimate by reducing the influence of outliers; tightens the association of trend between decline in CVL and new diagnoses". Another metric based on the per-case mean is computed by "Determine the mean VLs for each case, total the values of those mean VLs, and divide by the number of cases with a VL" and "Provides an average of the average number of virus particles per case; this may reflect a greater influence of those individuals with multiple measurements and those who were started on ART and trended towards suppression within 1 year". For starred metrics, " The denominator includes all viral load results, including undetectable viral load".

---

### How to write an effective discussion [^a80ea1d8]. Respiratory Care (2023). Medium credibility.

Explaining the meaning of the results to the reader is the purpose of the discussion section of a research paper. There are elements of the discussion section that should be included and pitfalls that should be avoided. Always write the discussion section for the reader. Remember that the focus is to help the reader understand the study and that the focus should be on the study data.

---

### Understanding statistical terms: 2 [^351bd399]. Drug and Therapeutics Bulletin (2009). Low credibility.

An increasing number of statistical terms appear in journal articles and other medical information. A working knowledge of these is essential in assessing clinical evidence. With this in mind, we are producing a series of explanatory articles covering various statistical terms and their uses. This, the second article in the series, will focus on some of the most common terms used in reporting the results of randomised controlled trials.1.

---

### Newborns' asymmetrical processing of order from sequentially presented magnitudes [^11d61f7e]. Child Development (2025). Medium credibility.

3.1.2 Stimuli

Stimuli were random configurations of white squares on a black background that varied in numerosity, cumulative surface area, and density. As in Experiment 1, there were three sets of habituation stimuli and one set of test stimuli, and each set contained three displays (Figure 4). The numerosities used in both habituation and test were identical to Experiment 1. However, in all sets, item size was positively correlated with number, as it increased (or decreased) three‐fold between displays in each set (4, 12, 36 cm 2). Since both number and item size increased or decreased by a factor of three, the cumulative surface area of the displays consequently increased ninefold within each stimulus set (e.g. 16, 144, 1296 cm 2 for the numerosities 4, 12, 36). In all sets, density also varied systematically with the items' number, because the virtual area of each display was kept constant at 800 × 800 pixels. The items in the smallest displays measured 0.65 × 0.65 cm (after Simion et al.), subtending a visual angle of approximately 1.2° at a viewing distance of 30 cm (corresponding to 0.83 cycles per degree, cpd). Given that newborn acuity is typically around 1 cycle per degree (cpd) (e.g. Acerra et al; Atkinson et al; Banks and Bennett; Brown and Yamamoto; Brown et al.), even these smaller displays were well within the newborn visual acuity threshold.

FIGURE 4
The four sets of stimuli presented to newborns in the increasing (left) and decreasing (right) order condition in the habituation (top) and test (bottom) phase of Experiment 2. In both habituation and test phases, item size increased or decreased proportionally with number according to a 1:3 ratio, and virtual display area remained constant. This resulted in a ninefold difference in cumulative surface area and contour length across numerosities and covarying density.

---

### Using symmetry to elucidate the importance of stoichiometry in colloidal crystal assembly [^dff8d404]. Nature Communications (2019). High credibility.

We demonstrate a method based on symmetry to predict the structure of self-assembling, multicomponent colloidal mixtures. This method allows us to feasibly enumerate candidate structures from all symmetry groups and is many orders of magnitude more computationally efficient than combinatorial enumeration of these candidates. In turn, this permits us to compute ground-state phase diagrams for multicomponent systems. While tuning the interparticle potentials to produce potentially complex interactions represents the conventional route to designing exotic lattices, we use this scheme to demonstrate that simple potentials can also give rise to such structures which are thermodynamically stable at moderate to low temperatures. Furthermore, for a model two-dimensional colloidal system, we illustrate that lattices forming a complete set of 2-, 3-, 4-, and 6-fold rotational symmetries can be rationally designed from certain systems by tuning the mixture composition alone, demonstrating that stoichiometric control can be a tool as powerful as directly tuning the interparticle potentials themselves.

---

### Diagnosis and treatment of osteochondritis dissecans [^7f2b7e80]. AAOS (2023). High credibility.

Appendix VIII — Nominal group technique (NGT) voting for guideline recommendations describes the method and thresholds used to adopt recommendations. Each member ranks agreement on a scale ranging from 1 to 9 (where 1 is "extremely inappropriate" and 9 is "extremely appropriate"), and consensus is obtained if the number of individuals who do not rate a measure as 7, 8, or 9 is statistically non-significant (as determined using the binomial distribution). The number of permissible dissenters varies by work group size: ≤ 3 not allowed, statistical significance cannot be obtained; 4–5: 0; 6–8: 1; 9: 1 or 2. The NGT is conducted by first having members vote on a given recommendation without discussion; if the number of dissenters is "permissible", the recommendation is adopted without further discussion, otherwise there is further discussion; Three rounds of voting are held to attempt to resolve disagreements, and if disagreements are not resolved after three voting rounds, no recommendation is adopted.

---

### Understanding statistical terms: 1 [^50f69b79]. Drug and Therapeutics Bulletin (2009). Low credibility.

An increasing number of statistical terms appear in journal articles and other medical information. A working knowledge of these is essential in assessing clinical evidence. With this in mind, we are producing a series of explanatory articles covering various statistical terms and their uses. This, the first article in the series, will focus on some of the most common terms used in randomised controlled trials.

---

### Exploiting volumetric wave correlation for enhanced depth imaging in scattering medium [^aa21aace]. Nature Communications (2023). High credibility.

The working condition of the proposed approaches is set by the parameter. For our method to work, the single-scattering intensityset by the reflectance and the depth of the target object, multiple scattering intensity, and the complexity of the spectro-angular dispersionshould meet the criteria such thatshould be larger than a certain threshold. The volumetric reflection matrix approach becomes more effective with increasing 2D depth sections in the covered volume. However, this involves taking more images. Since both the wavelength and illumination angle should be scanned for each complex widefield image recording, the required number of images is, whereandare the numbers of scanned wavelengths and angles, respectively. In the presented experiment, the camera's frame rate (300 Hz) is a major limiting factor in the acquisition time. In the case of the experiments using the swept-source laser, thevolume at the sample (0.4 NA) can be obtained withand, which takes ~2 min for obtaining the VRM. The computation time for finding the spectro-angular dispersion was ~10 s when a desktop computer equipped with an intel i9-11900k processor was used. If a high-speed camera with 6000 frames per second is used, the acquisition time can be reduced to 6 s. To make the method compatible with in vivo imaging, the acquisition time can be shortened by downsamplinganddepending on the degrees of spectro-angular dispersion and multiple scattering.

---

### A probabilistic model for the MRMC method, part 1: theoretical development [^4e751f00]. Academic Radiology (2006). Low credibility.

Rationale and Objectives

Current approaches to receiver operating characteristic (ROC) analysis use the MRMC (multiple-reader, multiple-case) paradigm in which several readers read each case and their ratings (or scores) are used to construct an estimate of the area under the ROC curve or some other ROC-related parameter. Standard practice is to decompose the parameter of interest according to a linear model into terms that depend in various ways on the readers, cases, and modalities. Though the methodologic aspects of MRMC analysis have been studied in detail, the literature on the probabilistic basis of the individual terms is sparse. In particular, few articles state what probability law applies to each term and what underlying assumptions are needed for the assumed independence. When probability distributions are specified for these terms, these distributions are assumed to be Gaussians.

Materials and Methods

This article approaches the MRMC problem from a mechanistic perspective. For a single modality, three sources of randomness are included: the images, the reader skill, and the reader uncertainty. The probability law on the reader scores is written in terms of three nested conditional probabilities, and random variables associated with this probability are referred to as triply stochastic.

Results

In this article, we present the probabilistic MRMC model and apply this model to the Wilcoxon statistic. The result is a seven-term expansion for the variance of the figure of merit.

Conclusion

We relate the terms in this expansion to those in the standard, linear MRMC model. Finally, we use the probabilistic model to derive constraints on the coefficients in the seven-term expansion.

---

### High throughput multiple combination extraction from large scale polymorphism data by exact tree method [^af3c17a6]. Journal of Human Genetics (2004). Low credibility.

Single nucleotide polymorphisms (SNPs) are increasingly becoming important in clinical settings as useful genetic markers. For the evaluation of genetic risk factors of multifactorial diseases, it is not sufficient to focus on individual SNPs. It is preferable to evaluate combinations of multiple markers, because it allows us to examine the interactions between multiple factors. If all the combinations possible were evaluated round-robin, the number of calculations would rapidly explode as the number of markers analyzed increased. To overcome this limitation, we devised the exact tree method based on decision tree analysis and applied it to 14 SNP data from 68 Japanese stroke patients and 189 healthy controls. From the obtained tree models, we succeeded in extracting multiple statistically significant combinations that elevate the risk of stroke. From this result, we inferred that this method would work more efficiently in the whole genome study, which handles thousands of genetic markers. This exploratory data mining method will facilitate the extraction of combinations from large-scale genetic data and provide a good foothold for further verificatory research.

---

### Preparing for a meta-analysis of rates: extracting effect sizes and standard errors from studies of count outcomes with person-time denominators [^413712ae]. Injury Prevention (2025). Medium credibility.

Background

Formulas for the extraction of continuous and binary effect sizes that are entered into a meta-analysis are readily available. Only some formulas for the extraction of count outcomes have been presented previously. The purpose of this methodological article is to present formulas for extracting effect sizes and their standard errors for studies of count outcomes with person-time denominators.

Methods

Formulas for the calculation of the number of events in a study and the corresponding person time in which these events occurred are presented. These formulas are then used to estimate the relevant effect sizes and standard errors of interest. These effect sizes are rates, rate ratios and rate differences for a two-group comparison and rate ratios and rate differences for a difference-in-difference design.

Results

Two studies from the field of suicide prevention are used to demonstrate the extraction of the information required to estimate effect sizes and standard errors. In the first example, the rate ratio for a two-group comparison was 0.957 (standard error of the log rate ratio, 0.035), and the rate difference was -0.56 per 100,000 person years (standard error 0.44). In the second example, the rate ratio for a difference-in-difference analysis was 0.975 (standard error of the log rate ratio 0.036) and the rate difference was -0.30 per 100,000 person years (standard error 0.42).

Conclusions

The application of these formulas enables the calculation of effect sizes that may not have been presented in the original study. This reduces the need to exclude otherwise eligible studies from a meta-analysis, potentially reducing one source of bias.

---

### California mugwort [^e23881b2]. FDA (2009). Low credibility.

Volume desired × Concentration desired = Volume needed × Concentration available.

Example 1: If a 1:10 w/v extract is available and it is desired to use a 1:1,000 w/v extract substitute as follows:

Vd × Cd = Vn × Ca

10ml × 0.001 = Vn × 0.1

0.1 mL = Vn

Using a sterile technique, remove 0.10 mL of extract from the 1:10 vial and place it into a vial containing 9.90 mL of sterile diluent. The resulting ratio will be a 10 mL vial of 1:1,000 w/v.

Example 2: If a 10,000 pnu/ml extract is available and it is desired to use a 100 pnu/ml extract substitute as follows:

10ml × 100 = Vn × 10,000

0.1 mL = Vn

Using a sterile technique, remove 0.10 mL of extract from the 10,000 pnu/ml vial and place it into a vial containing 9.90 mL of sterile diluent. The resulting concentration will be a 10 mL vial of 100 pnu/ml.

Example 3: If a 10,000 AU/ml or BAU/ml extract is available and it is desired to use a 100 AU/ml or BAU/ml extract substitute as follows: Vd × Cd = Vn × Ca

---

### Estimated number of people who inject drugs in the United States [^fc5e8909]. Clinical Infectious Diseases (2023). Medium credibility.

Background

Public health data signal increases in the number of people who inject drugs (PWID) in the United States during the past decade. An updated PWID population size estimate is critical for informing interventions and policies aiming to reduce injection-associated infections and overdose, as well as to provide a baseline for assessments of pandemic-related changes in injection drug use.

Methods

We used a modified multiplier approach to estimate the number of adults who injected drugs in the United States in 2018. We deduced the estimated number of nonfatal overdose events among PWID from 2 of our previously published estimates: the number of injection-involved overdose deaths and the meta-analyzed ratio of nonfatal to fatal overdose. The number of nonfatal overdose events was divided by prevalence of nonfatal overdose among current PWID for a population size estimate.

Results

There were an estimated 3 694 500 (95% confidence interval [CI] 1 872 700–7 273 300) PWID in the United States in 2018, representing 1.46% (95% CI, .74–2.87) of the adult population. The estimated prevalence of injection drug use was highest among males (2.1%; 95% CI, 1.1–4.2), non-Hispanic Whites (1.8%; 95% CI, .9–3.6), and adults aged 18–39 years (1.8%; 95% CI, .9–3.6).

Conclusions

Using transparent, replicable methods and largely publicly available data, we provide the first update to the number of people who inject drugs in the United States in nearly 10 years. Findings suggest the population size of PWID has substantially grown in the past decade and that prevention services for PWID should be proportionally increased.

---

### An iterative method for coil sensitivity estimation in multi-coil MRI systems [^dacb61e0]. Magnetic Resonance Imaging (2014). Low credibility.

This paper presents an iterative coil sensitivity estimation method for multi-coil MRI systems. The proposed method works with coil images in the magnitude image domain. It determines a region of support (RoS), a region being composed of the same type of tissues, by a region growing algorithm, which makes use of both intensities and intensity gradients of pixels. By repeating this procedure, it can determine multiple regions of support, which together cover most of the concerned image area. The union of these regions of support provides a rough estimate of the sensitivity of each coil through dividing the intensities of pixels by the average intensity inside every region of support. The obtained rough coil sensitivity estimate is further approached with the product of multiple low-order polynomials, rather than a single one. The product of these polynomials provides a smooth estimate of the sensitivity of each coil. With the obtained sensitivities of coils, it can produce a better reconstructed image, which determines more correct regions of support and yields preciser estimates of the sensitivities of coils. In other words, the method can be iteratively implemented to improve the estimation performance. The proposed method was verified through both simulated data and clinical data from different body parts. The experimental results confirm the superiority of our method to some conventional methods.

---

### The snm procedure guideline for general imaging 6.0 [^85398c18]. SNMMI (2010). Medium credibility.

Nuclear medicine computer system components — The camera head or associated image processing system performs functions including image size, position and zoom; energy correction; spatial distortion correction; other corrections (scatter correction, dead time correction, depth of interaction correction, sensitivity correction); and digital position computation; the interface handles the data in two basic modes: 1) Frame mode: complete images or matrices are available to the attached computer; and 2) List mode: data are passed on to the attached computer as a list of event x, y coordinates, to which time information and energy information may be also attached; for cardiac studies in particular, time lapse averaging is required, such that each image acquired at some specific time within the cardiac cycle is added to other acquired at similar times.

---

### Studying the pulmonary endothelium in health and disease: an official American Thoracic Society workshop report [^dd780042]. American Journal of Respiratory Cell and Molecular Biology (2024). High credibility.

Publications using endothelial cells — cell sourcing, characterization, and replicates reporting should include cell media details (company and catalog number; source and composition of any growth supplements and/or kits added; source and amount of FBS; whether antibiotics are added with source and concentration; whether hydrocortisone is added with source and concentration). Describe cell source (location in lung from where cells are obtained; whether commercially available, a cell line or freshly isolated; commercial source and lot number; method of isolation procedures) and passage information (state passage or range of passages used for each experiment and whether cells undergo multiple freeze and thaw cycles). Include cell characteristics (species and strain; basic demographics for individuals such as age, biological sex, race; diagnosis; relevant clinical parameters; treatment regimens) and validation/testing (state validation procedures; whether mycobacterium testing is performed; antibodies used for validation with source and lot numbers; cellular karyotype for sex as a biological variable research). Specify sample numbers and donors/biological replicates (number of samples per experiment; number of individual cells within a sample; number of fields/locations within a sample; number of total donors from which cells are derived; number of donors used for each experiment; whether cells from different donors are combined) and technical replication (how many separate times an experiment is repeated; how many times individual samples are repeated within an assay; whether technical replicates are averaged to obtain a single mean value).

---

### Infection fatality rate of SARS-CoV2 in a super-spreading event in Germany [^7159373a]. Nature Communications (2020). High credibility.

All CIs presented in this work were computed using the 95% level. CIs are Wald CIs and were not adjusted for multiple comparisons unless otherwise stated. All statistical hypothesis tests were two-sided. The Bonferroni–Holm procedure was applied to adjust p -values for multiple comparisons as indicated.

Infection rates obtained from IgG and IgA measurements were additionally corrected for possible misclassification bias using the matrix method, with sensitivity and specificity values obtained from the ELISA manufacturer's (Euroimmun, Lübeck, Germany) validation data sheet (version: April 7th, 2020). No adjustments were made for age and sex, as these variables were not found to be associated with infection status (Fig. 6a). To account for possible clustering effects due to participants living in the same household, confidence intervals for the corrected infection rate estimates were computed using a cluster bootstrap procedure with 10,000 bootstrap samples. With this procedure, household clusters were sampled with replacement. Within sampled clusters, no additional resampling of household members was carried out. The distributions of the bootstrapped corrected infection rate estimates were symmetrical and close to normality (as indicated by normal quantile-quantile plots, sd = 0.01697), and the percentile method was applied to calculate CI limits. CI limits for the IFR were calculated by dividing the number of deaths (7) by the CI limits of the estimated number of infected. Here, the number 7 was considered fixed, as it corresponded to all recorded SARS-CoV2-associated deaths in Gangelt by the end of the study period and was, therefore, not subject to sampling error. In addition, we computed a Bayesian credibility interval for the IFR that accounted for possible uncertainty in the number of SARS-CoV-2-associated deaths. This CI was defined by the empirical 2.5% and 97.5% quantiles of 100,000 samples drawn from a beta distribution with parameters (7 + 1) and (estimated number of infected — 7 + 1), where in each of the 100,000 samples the estimated number of infected were sampled from a normal distribution with mean 0.1553 and standard deviation 0.01697 (multiplied by 12,597). This approach was motivated by the fact that the beta distribution with parameters (7 + 1) and (estimated number of infected — 7 + 1) is the posterior distribution obtained from a uniform prior distribution on the IFR and a binomial likelihood with estimated number of infected trials and 7 successes. Furthermore, an age-standardized estimate of the IFR in Gangelt was computed. This was done by determining infection rates from the study data in each of the age groups defined in Supplementary Fig. 1 (again corrected for possible misclassification bias using the matrix method) and by calculating an age-standardized estimate of the number of infected in Gangelt (using the proportions of the age groups in Gangelt presented in Supplementary Fig. 1; confidence intervals computed using a cluster bootstrap procedure with 10,000 samples). The age-standardized IFR estimate and its CI limits were calculated by dividing the number of deaths (7) by the age-standardized estimated number of infected and its CI limits, respectively.

---

### Understanding systematic reviews and meta-analyses [formula: see text] [^a0b57635]. JPEN: Journal of Parenteral and Enteral Nutrition (2017). Low credibility.

Systematic reviews should be distinguished from narrative reviews. In the latter, an editor asks an expert to sum up all of the information that is known about a particular topic. However, the expert is under no constraints regarding what he or she does, or does not, choose to include in the review. As a result, his or her bias can influence the final message. A systematic review, which may or may not be written by experts, typically asks a narrower question, and then answers it using the entirety of the medical literature. The systematic review process includes computer searches to identify the pertinent literature, a statement of the inclusion and exclusion criteria for identified studies, a list of items of interest to extract from each study, a method to assess the quality of each study, a summary of the evidence that has been found (which may or may not involve attempts to combine data), a discussion of the evidence and the limitations of the conclusions, and suggestions for future research efforts. If the data are combined, that process is called meta-analysis. In meta-analysis, an estimate of the reliability of each study is made, and those that appear to be more reliable are weighed more heavily when the data are combined. While systematic reviews depend on a more preplanned method and thus, unlike narrative reviews, contain sections on method, they can be easily read once the reader becomes familiar with the vocabulary.

---

### Six suggestions for improving quantitative evaluations [^3e242678]. Injury Prevention (2025). Medium credibility.

This special feature contains some personal suggestions for improving quantitative analyses. I focus on articles that use regressions to determine the connections among variables, a common way for evaluating the effects of public policy. Examples come from the firearms literature. It seems to me that, once they have obtained some data, too many researchers almost immediately start running regressions-before they fully understand the dataset or think deeply about the questions they are trying to answer. I provide six suggestions for researchers: Try to: (1) determine and report a causal theory, including the chain-of-causation; (2) investigate the accuracy of the data; (3) explore the data; (4) disaggregate where possible; (5) determine if the results are plausible and (6) be transparent about the methods and results-let the reader into your 'statistical kitchen'. The evidence about the effectiveness of Child Access Prevention Laws illustrates some of these issues.

---

### How to write a systematic review of reasons [^119f8c22]. Journal of Medical Ethics (2012). Low credibility.

Furthermore, unless conclusion types are extremely narrow, counts of reasons will cover reasons that were used to support slightly different conclusions. However, we note that classic systematic reviews face the analogous problem that the studies reviewed addressed slightly different research questions, and that such systematic reviews have developed appropriate strategies. We hypothesise that systematic reviews of reasons will frequently face this problem; research is needed on adapting existing strategies. Although a comparison between traditional systematic reviews and systematic reviews of reasons is beyond the scope of this paper, we would like to point out that the former are also subject to many types of bias. In any case, we recommend caution when computing counts and the presentation of qualitative results instead of counts whenever appropriate.

Applications of the model are needed to clarify further obstacles, to identify tactics for minimising external biases (eg, publication bias) and internal or review author-related biases (eg, selection bias and coding bias) and to develop means to evaluate the model.

---

### Diagnostic interpretation of genetic studies in patients with primary immunodeficiency diseases: a working group report of the primary immunodeficiency diseases committee of the American Academy of Allergy, Asthma&Immunology [^df7f2aab]. The Journal of Allergy and Clinical Immunology (2020). High credibility.

Variant classification scheme for pathogenicity (Table IV) — a variant is first assessed within the five evidence-type categories and then assigned a class based on the total quantity of criteria met. The table states: "A variant is assessed for evidence of benign or pathogenic effect within the 5 evidence-type categories listed in the left-most column" and that "The variant is then assigned a pathogenic, likely pathogenic, benign, or likely benign designation based on the total quantity of criteria met within the given columns along any individual row". It further provides an example: "For example, a variant that fulfills the criterion for "very strong" pathogenic evidence and at least 1 "strong" pathogenic evidence criterion in any other evidence type categories should be considered "pathogenic". The scheme clarifies that "Numbers in boxes refer to minimum total counts of criteria types fulfilled for each level of evidence within the same column".

---

### Miller moth [^fb68ca60]. FDA (2009). Low credibility.

HOW SUPPLIED

The stock concentrate of allergenic extract is expressed in weight/volume, at 1:50 w/v (2%) or 1:20 w/v (5%). It is supplied in 10, 30 and 50 mL containers. Extracts in 5 mL sterile dropper bottles are available for prick-puncture testing. To insure maximum potency for the entire dating period, all stock concentrates contain 50% glycerine v/v.

---

### Anatomy of a research paper [^77f3b995]. Respiratory Care (2004). Low credibility.

Writing, editing, and publishing the paper is the last step in the research process. The paper tells the story of the project from inception, through the data-collection process, statistical analysis, and discussion of the results. Novice authors often struggle with writing and often find themselves with either nothing on paper or a weighty version of random thoughts. The process of writing the paper should be analogous to the research process. This article describes and provides a template for the essential sections and features of a scientific report (structured abstract, introduction, hypothesis, methods, results, discussion, and conclusions), describes authorship guidelines that have been established by professional societies, and discusses the importance of adequate and correct references.

---

### Effective use of tables and figures in abstracts, presentations, and papers [^9d553efc]. Respiratory Care (2004). Low credibility.

In some situations, tables, graphs, and figures can present certain types of information (including complicated relationships and sequences of events) more clearly and in less space than the same information would require in sentence form. However, do not use tables, graphs, and figures for small amounts of data that could be conveyed clearly and succinctly in a sentence. Also, do not reiterate in sentences the data that are shown in a table, graph, or figure: the point of creating a table or graph or figure is to eliminate that type of sentence from your manuscript. In building a data table you must balance the necessity that the table be complete with the equally important necessity that it not be too complex. Sometimes it is helpful to break a large table into several smaller ones to allow the reader to identify important information easily, but, conversely, it is a common mistake of novice authors to split up into several tables data that belong in one table. In almost all cases, only one table or graph or figure should be included in an abstract, and then only if it can convey essential information in less space and in a more easily interpretable way than the sentence form. For a poster, in almost all instances you should use only one typeface and one font in a table, graph, or figure. In general, do not use bold, italics, or color unless you are presenting a great deal of data and you need to highlight certain data values and you are certain that using bold, italics, or color will improve readability, which is rare. Do not include identical information in a table and a graph/figure. In reporting a clinical trial you will need to include a patient flow chart that identifies the number of patients initially screened for the study, the number of patients who were excluded (and why) after initial screening or in the final analysis, and how many patients entered, exited early, and completed each arm of the study. A treatment protocol should also be described with a flow chart. In preparing a graph the most common error is to include a line that suggests an unsubstantiated extrapolation between or beyond the data points. In selecting the graph's axes, avoid truncating, enlarging, or compressing the axes in ways that might make the graph confusing or misleading. To prepare clear, accurate, easily interpretable tables, graphs, and figures, rely on the rules described in authoritative guides such as the Council of Science Editors' Scientific Style and Format and the American Medical Association's Manual of Style.

---

### Graphical data presentation [^8085a383]. Injury (2008). Low credibility.

Figures and charts are the most influential vehicles for distributing scientific information, for affecting decisions as to the acceptance or rejection of a manuscript, and for attracting the attention of the scientific community to study results. Graphical excellence is mainly defined, first, by the highest possible data density (that is, the amount of information provided per graph area); second, by a low ink-to-data ratio (the avoidance of unnecessary shading, three-dimensionality, gridlines and what is often called 'chartjunk'); and third, by clear and unequivocal labelling of axes. The researcher's essential graphical toolbox should contain histograms, bar charts (always with measures of error), box-and-whiskers plots, scatter plots and forest plots.

---

### Physical activity and exercise during pregnancy and the postpartum period: ACOG committee opinion, number 804 [^7aa306cd]. Obstetrics and Gynecology (2020). High credibility.

Recommended weight limits for lifting at work during pregnancy — an on-page algorithm and stepwise method describe how to determine a recommended weight limit (RWL). The algorithm asks, "At work do you perform lifting tasks more than once every 5 minutes?" and, if yes, "Do you lift more than three times per minute?", then queries "How many hours per day do you spend lifting at work?" with categories "Less than 1 continuous hour" or "1 or more hours per day", corresponding to infrequent, repetitive short duration, or repetitive long duration lifting panels. Steps instruct to "select the one graphic (A, B, or C) that best describes the lifting frequency or frequency/duration pattern", then "When less than 20 weeks pregnant, select the image on the left of the graphic; when pregnant for 20 weeks or more, select the image on the right", to underline all relevant numerical values along the lift path and circle the lowest, and that "The number circled in step 3 is the RWL (in pounds)" with a proviso that the short-duration category can include multiple hours per day provided each continuous bout is less than 1 hour and followed by at least 1 hour of nonlifting before resuming; the figure presents "Provisional recommended weight limits for lifting at work during pregnancy".

---

### Microfluidic multipoles theory and applications [^5123f2af]. Nature Communications (2019). High credibility.

Generalization from microfluidic dipoles to multipoles

Once a solution is known for one particular multipole flow profile, the full power of conformal transforms can be exploited. By using simple functions expressing symmetry operations, such as inversions or power transforms with a suitably placed origin, we can obtain concentration profiles for an infinite family of multipoles. In each case, the transport problem is first solved in the streamline domain (Fig. 2a), then transformed to obtain the flow profile for an asymmetric dipole (Fig. 2b). The dipole solution can then be transformed again to obtain the desired flow patterns. For example, a power law conformal transform generates a polygonal structure with a rotation symmetry whose number of sides is dictated by the exponent of the power transform (as in Fig. 2c). These devices will have injection and aspiration apertures located at new positions, determined by the transform of the initial aperture locations, and can then be fabricated and operated to obtain the predicted patterns. This method gives us a comprehensive toolbox to not only model and explain phenomena in known open-space MFMs in terms of simpler ones, but also to explore new configurations that have not been investigated yet.

While many geometries can be obtained by directly transforming the concentration profile from the microfluidic dipole, an arbitrary placement of injection and aspiration apertures will in general not be reducible to a semi-infinite absorbing leading edge (Fig. 2a). In the more general case, the streamline coordinate problem will exhibit any number of finite and semi-infinite absorbing segments. By solving these streamline coordinate problems, we obtained new families of multipolar devices, such as the two-reagent microfluidic quadrupole (Fig. 2d), which can itself be transformed to obtain new symmetrical patterns (Fig. 2e, f and Supplementary Note 3 for more details).

Table 1 summarizes some common transforms that can be used to obtain the concentration profile for many new devices from the dipole solution without solving any differential equation. A more detailed version, including many more transformation groups is presented in the SI. These configurations include known devices such as the previously published microfluidic quadrupole, but also several geometries that have not yet been investigated such as "flower" multipoles (Fig. 2c, f), alternating multipoles (Fig. 2e), polygonal multipoles and impinging flow multipoles (Table S3),

Table 1
Examples of simple transforms that can be applied to the dipole solution to obtain new geometries

A more exhaustive table (Supplementary Table 3) is presented in the SI

---

### Systematic reviews and meta-analyses: when they are useful and when to be careful [^522d9502]. Kidney International (2009). Low credibility.

Systematic reviews and meta-analyses are increasingly popular study designs in clinical research. A systematic review is a summary of the medical literature that uses explicit and reproducible methods for searching the literature and critical appraisal of individual studies; in contrast, a meta-analysis is a mathematical synthesis of the results of these individual studies. These study designs can be useful tools for summarizing the increasing amount of knowledge that is gained from scientific papers on a certain topic. In addition, combining individual studies in a meta-analysis increases statistical power, resulting in more precise effect estimates. Although the specific methodology of systematic reviews includes steps to minimize bias in all stages of the process, investigators should be aware of potential biases such as poor quality of included studies, heterogeneity between studies, and the presence of publication and outcome reporting bias. This paper explains how systematic reviews and meta-analyses should be performed and how to interpret and implement their results. In addition, we discuss when meta-analyses are useful and when they are not.

---

### RAMESES publication standards: meta-narrative reviews [^50411ff3]. BMC Medicine (2013). Low credibility.

Explanation

A meta-narrative review asks some or all of the following questions:

(1) Which research (or epistemic) traditions have considered this broad topic area?; (2) How has each tradition conceptualized the topic (for example, including assumptions about the nature of reality, preferred study designs and ways of knowing)?; (3) What theoretical approaches and methods did they use?; (4) What are the main empirical findings?; and (5) What insights can be drawn by combining and comparing findings from different traditions?'

Because a meta-narrative review may generate a large number of avenues that might be explored and explained, and because resources and timescale are invariably finite, the expectation is that the review must be 'contained' by progressively focusing both its breadth (how wide an area?) and depth (how much detail?). This important process may involve discussion and negotiation with (for example) content experts, funders and/or users. It is typical and legitimate for the review's objectives, question and/or the breadth and depth of the review to evolve as the review progresses. How and why it evolved is usually worth reporting.

Methods section

The following items should be reported in the methods section.

Item 5: Changes in the review process

Any changes made to the review that were initially planned should be briefly described and justified.

Example

"But as the review unfolded, two things became clear: first, in many areas, the evidence meeting all these criteria was sparse, and second, we could gain critical insights from beyond the parameters we had set. We therefore extended our criteria to a wider range of literature. In particular, we added both overview articles and "landmark" empirical studies from outside the health sector if they had important methodological or theoretical lessons for our research question".

Explanation

A meta-narrative review can (and, in general, should) evolve over the course of the review. For example, changes to the research question or its scope are likely to have an impact on many of the review's subsequent processes. However, this does not mean the review can meander uncontained. An accessible summary of what was originally planned (for example, as described in an initial protocol) and how and why this differed from what was done should be provided as this may assist interpretation.

Item 6: Rationale for using the meta-narrative approach

Explain why meta-narrative review was considered the most appropriate method to use.

---

### How to write an effective discussion [^e4ec4eba]. Respiratory Care (2004). Low credibility.

Explaining the meaning of the results to the reader is the purpose of the discussion section of a research paper. There are elements of the discussion that should be included and other things that should be avoided. Always write the discussion for the reader; remember that the focus should be to help the reader understand the study and that the highlight should be on the study data.

---

### In situ single-atom array synthesis using dynamic holographic optical tweezers [^33aae937]. Nature Communications (2016). Medium credibility.

Single-atom array synthesis demonstration

A proof-of-principle demonstration of the in situ single-atom array synthesis is presented in Fig. 3. In a three-step feedback loop of initial atom loading in N init = 9 sites, readout, and rearrangement, as shown in Fig. 3a, an atom array of N final = 1, 2, 3 or 4, is produced. After the atoms are initially loaded at the sites, the first computer checks the occupancy of each site by reading out the electron multiplying charge-coupled device (CCD) images. A 9-bit binary information that represents the occupancy is then sent to the second computer, which has a look-up table of atom rearrangement trajectories, each stored in DRAM as a sequence of 30 holograms, between all possible initial and final pairs of atom arrays. It takes 0.6 s from the readout to the retrieval of an appropriate trajectory (note that, using a fast graphic-processing unit can greatly reduce this time and also make the look-up table unnecessary). Then, the second computer sequentially loads the holograms to the SLM at a speed of 30 frame-per-second to move the atoms along the trajectory. The initial, and four types of final cumulative images are represented by the atom number histograms in Fig. 3b, c. The individual final images are independent experiments that form one, two, three or four atom arrays out of nine.

Compared with the binomial distribution of the initial histogram, the final histogram in Fig. 3d has non-Poissonian distribution. The loading efficiency curves of the final arrays are presented with red points for the data from the number histograms of at least 500 events. The black dotted line follows 0.5 N, which is the loading efficiency in the collisional blockade regime, and the blue dashed line follows the cumulative binomial distribution given by

and the red line is

where p = 0.48 is the initial loading probability and p s = 0.86 is the experimental (moving) success probability from the fitting. 1− p s is composed of the background collisional and moving losses. During the entire feedback process, the background collisional loss is estimated to be 0.13 and the moving loss is estimated to be 0.01. The moving success rate, as expected, has high fidelity and a deterministic transport has been reliably completed. The N final = 4 case exhibits sixfold enhancement in loading efficiency, compared with the P = 0.5 collisional blockade regime.

---

### Systematic reviews: how they work and how to use them [^349a0f06]. Anaesthesia (2007). Low credibility.

At their best, systematic reviews should be the least biased summaries of the effect of healthcare interventions. However, authors can introduce both intended and unintended biases into systematic reviews. Results presented as odds ratios are often misinterpreted by readers as relative risks, meaning that the effect of the intervention is overestimated. Authors may analyse trials separately having mistaken differences in baseline risk for differences in the effect of an intervention and differences in effect between trials that have been analysed together may go undetected. In this article I discuss how a systematic review should work and how it can go wrong.

---

### Statistical test to assess rank-order imaging studies [^b6665312]. Academic Radiology (2001). Low credibility.

Rationale and Objectives

Rank-order experiments often provide a reasonable method of determining whether a large-scale receiver operating characteristic study can be justified. The authors' purpose was to formalize a proposed method for analyzing rank-order imaging experiments and provide methods that can be used in determining sample sizes for both cases and raters.

Materials and Methods

Simulations were conducted to determine the adequacy of the normal approximation of a statistic used to test the null hypothesis of random ordering. For a multireader experiment, formulas are presented and guidelines are provided to enable investigators to determine the number of required readers (raters) and cases for a specific study.

Results

When there are at least five ordered images per case, 10 cases are sufficient to test a random rank order. When there are only three or four images for a case, 20 cases are required. The authors constructed tables of statistical power for selected numbers of ordered images, numbers of cases, and degrees of trend, and they also provide an approximation for use in situations that are not tabled.

Conclusion

The statistical methods for analyzing rank-order experiments and estimating sample sizes for study planning are relatively simple to implement. The derived formulas for sample size estimation, when applied to typical imaging experiments, indicate that modest numbers of cases and readers are required for rank-order studies.

---

### Research output on primary care in Australia, Canada, Germany, the Netherlands, the United kingdom, and the United States: bibliometric analysis [^723819c0]. BMJ (2011). Excellent credibility.

Sample selection

The records retrieved were loaded into EndNote bibliographic software and deduplicated using multiple approaches. Each country was loaded into a separate EndNote database, and on entry into the database was assigned a sequential EndNote record number. From the result set we removed records relating to countries other than the six of interest. We extracted a pragmatic 15% random sample of the remaining records for each of the six survey years and for each country. Random numbers were generated using the Random.org integer generator facility. We requested a set of numbers between 1 and the total records in an EndNote database. The numbers returned were then matched to EndNote record numbers.

Using the data available in the title, abstract, indexing, and affiliation fields, we categorised the records according to the following questions: Did the publication relate to primary care broadly defined? Did the publication report research findings rather than comment, opinion, or informal review? Was at least one of the authors involved in primary care either as a healthcare professional or as a researcher? We allocated records to a country based on information from an author affiliation field, setting information in the abstract, and subject indexing. Where authors from more than one country were identified we assigned the records to each country. Selection was made by one individual from a team of selectors and queries discussed with a lead selector (JG).

The samples were assessed for the current Research Assessment Exercise census period — namely, whole years for 2001 to 2006 and 10 months for 2007. As the 2007 data did not represent a complete year we excluded such data from the calculation of means, and also to avoid any biases in favour of higher impact journals and English language journals, which may be indexed by databases more quickly than journals with lower impact factors and those in languages other than English.

Adjusting for national differences

To facilitate comparisons across the six countries the research output by primary care researchers or practitioners was adjusted for national differences. We calculated annual estimates based on the 15% sample and then weighted them by the gross domestic product spent on all research and development (gross expenditure on research and development) and the number of full time equivalent researchers (all disciplines). Data were required for a seven year period and the best source of such comparative statistics was the OECD series. We downloaded statistics from OECD. Stat on 25 February 2008. As the statistics had not been published for 2007 at the time of the analysis, we used 2006 values for 2007 calculations.

---

### Rubidium chloride rb-82 (Cardiogen-82) [^b67251a7]. FDA (2025). Medium credibility.

Rubidium Eluate Level Testing:

Set a dose calibrator for Rb 82 as recommended by the manufacturer or use the Co-60 setting and divide the reading obtained by 0.548. Obtain the reading from the instrument in millicuries.
Elute the generator with 50 mL of 0.9% Sodium Chloride Injection and discard the eluate (first elution).
Allow at least 10 minutes for the regeneration of Rb 82, then elute the generator with 50 mL of 0.9% Sodium Chloride Injection at a rate of 50 mL/min and collect the eluate in a stoppered glass vial (plastic containers are not suitable). Note the exact time of end of elution (E.O.E.).
Using the dose calibrator, determine the activity of Rb 82 and note the time of the reading. Correct the reading for decay to the E.O.E. using the appropriate decay factor for Rb 82 (see Table 1). Note: If the reading is taken 2 ½ minutes after end of elution, multiply the dose calibrator reading by 4 to correct for decay.
Using the sample obtained for the Rb 82 activity determination, allow the sample to stand for at least one hour to allow for the complete decay of Rb 82.
Measure the activity of the sample in a dose calibrator at the setting recommended by the manufacturer for Rb 82 and/or Sr 82. As an alternative, use the Co-60 setting and the reading obtained divided by 0.548. Set the instrument to read in microcuries and record in the display.
Calculate the ratio (R) of Sr 85/Sr 82 on the day (post-calibration) of the measurement using the ratio of Sr 85/Sr 82 on the day of calibration provided on the generator label and the Sr 85/Sr 82 Ratio Factor from Table 2. Determine R using the following equation:
Use a correction factor (F) of 0.478 to compensate for the contribution of Sr 85 to the reading.
Calculate the amount of Sr 82 in the sample using the following equation:
Determine if Sr 82 in the eluate exceeds an Alert or Expiration Limit by dividing the microCi of Sr 82 by the mCi of Rb 82 at End of Elution (see below for further instructions based on the Sr 82 level)
Determine if Sr 85 in the eluate exceeds an Alert or Expiration Limit by multiplying the result obtained in step 10 by (R) as calculated in step 7 (above).

---

### Fundamental energy cost of finite-time parallelizable computing [^ac82ab0f]. Nature Communications (2023). High credibility.

Algorithmic parallelization overhead

Another important aspect of real-world algorithms, that ought to be accounted for, is that of parallelization overhead. Parallelization indeed frequently requires the execution of additional overhead operations N ove. For example, an algorithm may need to distribute data to the parallel workers and then, at the end, another one collects data back from the workers. Usually, this overhead is a function of the number of processors used. Because of the constantassumption, the overhead either means that each processor needs to work faster in order to compensate for the overhead (Fig. 4 b inset), or that one might use a stronger degree of parallelization, whereinstead of n (N) = b N. We shall assume that the maximal available parallelization is already used and that overhead may only be compensated by adjusting the calculation speed τ p. We then obtain (Supplementary Information, Sec. S2),

Owing to the time dependence of the dissipated work in Eq. (1), the energy cost of the parallel execution not only increases with the number of additional operations N ove but also because of the necessary increase in processing speed. As a result, we obtain from Eq. (1) the total energetic cost for a general function N ove (n) (Supplementary information, Sec. S2), The overhead thus causes the parallel computer to be less efficient than the serial computer for small problem sizes. This is because the Landauer part adds a fixed cost to Eq. (9), while the dissipative part will only be dominant for large N. However, the parallel implementation exhibits better scaling and becomes more energy efficient for larger problem sizes, even for large overhead, as illustrated, for concreteness and simplicity in Fig. 4 b for a linear overhead, N ove (n) = c n (different overhead functions are analyzed in Supplementary Information, Sec. S2). This fundamental advantage of parallel computers holds as long as N ove (n) scales better than n 3/2, or, equivalently, N 3/2, since n ∝ N because of the fixed finite-time constraint. This scaling is modified to n 5/3 for current electronic computers (Supplementary Information, Sec. S4).

---

### A summary of the methods that the national clinical guideline centre uses to produce clinical guidelines for the national institute for health and clinical excellence [^17347588]. Annals of Internal Medicine (2011). Medium credibility.

NICE study search and retrieval workflow — Search results undergo a First sift: titles (done by information scientist or systematic reviewer) to exclude studies outside the topic; a Second sift: abstracts (done by systematic reviewer) to exclude studies that are not relevant to the review questions; and Assessment of full articles (done by systematic reviewer) to exclude studies on limits set by the GDG (for example, study design or outcomes), after which studies included proceed for data extraction; because of potential bias or error, a second reviewer performs sampling checks, and usually several thousand titles are sifted at the first stage.

---

### Designing screens: how to make your hits a hit [^68fdee95]. Nature Reviews: Drug Discovery (2003). Medium credibility.

The basic goal of small-molecule screening is the identification of chemically 'interesting' starting points for elaboration towards a drug. A number of innovative approaches for pursuing this goal have evolved, and the right approach is dictated by the target class being pursued and the capabilities of the organization involved. A recent trend in high-throughput screening has been to place less emphasis on the number of data points that can be produced, and to focus instead on the quality of the data obtained. Several computational and technological advances have aided in the selection of compounds for screening and widened the variety of assay formats available for screening. The effect on the efficiency of the screening process is discussed.

---

### A neural theory for counting memories [^c99dea2f]. Nature Communications (2022). High credibility.

A count sketch stores a frequency table for items using a 2D matrix C with k rows and v columns, where k is the number of hash functions, and v is the range of the hash functions (Fig. 1 A). Each row is associated with a hash function h: x → [v]; i.e. the function takes as input some item x and maps it to a column index in C. The k hash functions are pairwise independent and random. This means that the inputs are spread uniformly over the range, and two similar inputs could be assigned to arbitrarily far apart indices. In Fig. 1 A, there are three hash functions (k = 3). Each entry in C corresponds to a counter and is initialized to 0.

Fig. 1
The count sketch and corresponding neural circuit implementation.

A The count sketch data structure is a 2D matrix C of counters with k rows and v columns. There is one hash function h per row, each of which determines which column in the row is modified when an item x is observed (dotted arrows). B The neural implementation of a count sketch uses a 1D array w of k × v synapses. When an item is observed, the synapses of the k pre-synaptic neurons that are active for the item (orange highlight) are modified. The neurons activated for the item are determined using a hash function that assigns the item a sparse, high-dimensional representation (z). In this example, each item x is a d -dimensional vector. C To insert an item from the sequence into the count sketch (top), k counters are incremented. For example, after the first time (x 1) is observed, in the first row, the counter in the 1st column (i.e. C [1, 1]) is incremented by 1 since h 1 (x 1) = 1. In the second row, C [2, 2] is incremented by 1 since h 2 (x 1) = 2. In the third row, C [3, 5] is incremented by 1 since h 3 (x 1) = 5. Similarly, in the Hebbian neural count sketch (bottom), the synaptic weights of the k activated pre-synaptic neurons for x 1 (orange highlight) are incremented. In the anti-Hebbian model, all synaptic weights are initialized to 1, and synapses active for the item are decremented with each observation. D To output an estimate of the frequency of item x 1, the count sketch computes: — i.e. the average of the predicted counts over the rows. This results in correct estimates for x 1 and x 2, and a near-correct estimate for x 3. Similarly, the neural count sketch outputs: — i.e. the average of the weights of the activated neurons for the item.

---

### 2025 AHA / ACC statement on cost / value methodology in clinical practice guidelines (update from 2014 statement): a report of the American college of cardiology / American Heart Association joint committee on clinical practice guidelines [^00cf701b]. Journal of the American College of Cardiology (2025). High credibility.

Cost-effectiveness threshold for U.S. clinical guidelines — the writing committee "recommend the use of $120 000 per QALY as the cost-effectiveness threshold for clinical guidelines", with the threshold to be "periodically reexamined", "not be adjusted annually for inflation", and "revised every 10 years (or sooner…)". Supporting estimates include a 2024 Personal Consumption Expenditure update that "suggests a cost-effectiveness threshold of $117 000 per QALY", a welfare economics approach indicating willingness to pay tied to income ("per capita disposable income of $60 300… approximately $142 000 (range: $120 000-$175 000) for 1 QALY"), and a population-level framework using World Bank data estimating "$95 958 ($81 672-$120 181) per QALY", translating to "$112 000 ($96 000-$141 000) per QALY" after GDP growth; while "no single method… can be considered the 'gold standard'", these three approaches "produced similar results".

---

### Inferring effects of mutations on SARS-CoV-2 transmission from genomic surveillance data [^69ee0db0]. Nature Communications (2025). High credibility.

Computational complexity

Here we briefly discuss the computational complexity of our method. The steps in our data processing are:
Clean the data (eliminate sequences with large numbers of Ns or gaps, etc.).
Separate the data by time and region.
Identify SNVs observed above the minimum frequency threshold.
Compute SNV covariance matrices/changes in SNV frequencies in each region and integrate them over time.
Infer the selection coefficients, which involves inverting the total integrated SNV covariance matrix.

Let L be the length of the SARS-CoV-2 sequence (roughly 3 × 10⁴ bps) and let M be the total number of sequences (roughly 10⁷, including data taken up until January 26th, 2024). Then, steps 1 and 2 involve computations that scale as. Step 3 is. This step also introduces a new parameter relevant to the scaling of the problem, which is the fraction of SNVs that are observed at high enough frequencies to be included in our analysis. Let us call this fraction p, which is roughly 0.35 with our current settings. Naively, step 4 then involves a computation that scales like. However, the calculation of the covariance can easily be parallelized across regions. In each individual region, the fraction of SNVs that are observed at high enough frequencies to be included is a different parameter q and the number of sequences in the region is a parameter M r. The largest q that we find in the regions analyzed is around 0.05. For N r separate regions (149 in our analysis), step 4 then involves N r parallel computations that scale-like. Due to the matrix inversion, step 5 requirescomputations to complete.

---

### Obesity in pregnancy: ACOG practice bulletin, number 230 [^2c1e6b7d]. Obstetrics and Gynecology (2021). High credibility.

Obesity in pregnancy — evidence grading and recommendation categories are defined using the U.S. Preventive Services Task Force method, with I defined as evidence obtained from at least one properly designed randomized controlled trial; II-1 as evidence obtained from well-designed controlled trials without randomization; II-2 as evidence obtained from well-designed cohort or case–control analytic studies, preferably from more than one center or research group; II-3 as evidence obtained from multiple time series with or without the intervention, with dramatic results in uncontrolled experiments also regarded as this type of evidence; and III as opinions of respected authorities based on clinical experience, descriptive studies, or reports of expert committees. Based on the highest level of evidence, recommendations are graded as Level A — recommendations are based on good and consistent scientific evidence, Level B — recommendations are based on limited or inconsistent scientific evidence, and Level C — recommendations are based primarily on consensus and expert opinion.

---

### Acitretin [^3c8f6c10]. FDA (2024). Medium credibility.

Females of reproductive potential should also be advised that they must not ingest beverages or products containing ethanol while taking acitretin and for 2 months after acitretin has been discontinued. This allows for elimination of the acitretin which can be converted to etretinate in the presence of alcohol.

Female patients should be advised that any method of birth control can fail, including tubal ligation, and that microdosed progestin "minipill" preparations are not recommended for use with acitretin (see CLINICAL PHARMACOLOGY: Pharmacokinetic Drug Interactions). Data from one patient who received a very low-dosed progestin contraceptive (levonorgestrel 0.03 mg) had a significant increase of the progesterone level after 3 menstrual cycles during acitretin treatment.2

Female patients should be advised to contact their physician, women's health centers, pharmacies, or hospital emergency rooms for information about how to obtain Emergency Contraception if sexual intercourse occurs without using 2 effective forms of contraception simultaneously. A 24-hour, toll-free number (1-800-724-4242) is also available for patients to receive automated birth control and emergency contraception information.

Female patients should sign a consent form prior to beginning therapy with acitretin (see boxed CONTRAINDICATIONS AND WARNINGS).