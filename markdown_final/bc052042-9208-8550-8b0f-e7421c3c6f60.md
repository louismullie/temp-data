# Medical Question & Answer

**Sample ID**: bc052042-9208-8550-8b0f-e7421c3c6f60
**Dataset Index**: 81912

---

## Question

Find all functions f: ℚ → ℚ such that for all x, y in ℚ,

f(x+y) = f(x)+f(y)

---

## Answer

> Let's see… What do we have here? The user is asking to find all functions (f: ℚ → ℚ) satisfying the Cauchy functional equation (f(x+y) = f(x) + f(y)) for all (x, y ∈ ℚ). Let's break this down step-by-step. First, I need to think about the basic properties that follow immediately from additivity. Then, I should verify how induction extends this to integer multiples. Next, I will check how this scales to rational multiples and confirm the form (f(x) = c x). After that, I need to ensure that every function of that form indeed satisfies the equation. Finally, I will consider whether any non-linear solutions could sneak in over (ℚ) and conclude with a concise characterization.

> Let me first confirm the immediate consequences of additivity. Setting (x = y = 0) gives (f(0) = f(0) + f(0)), so (f(0) = 0). Setting (y = -x) gives (f(0) = f(x) + f(-x)), so (f(-x) = -f(x)), which means (f) is odd. These are foundational checks that will anchor later steps.

> Next, I should review integer scaling. For any positive integer (n), induction shows (f(nx) = n f(x)). The base case (n = 1) is trivial, and if (f(kx) = k f(x)), then (f((k+1)x) = f(kx + x) = f(kx) + f(x) = k f(x) + f(x) = (k+1)f(x)), which completes the induction. For negative integers, I can write (f(-nx) = -f(nx) = -n f(x)), so (f(mx) = m f(x)) holds for all (m ∈ ℤ).

> Now, I will extend this to rational scaling. Let (x ∈ ℚ) and write (x = p/q) where (p ∈ ℤ) and (q ∈ ℤ⁺). I need to ensure that (f(rx) = r f(x)) for (r ∈ ℚ). Starting with (f(q × (p/q)) = p f(1/q)), the left side is (f(p)), so (f(p) = p f(1/q)). But wait, I should verify the integer scaling step here; I initially thought (f(p) = p f(1)), but that would only be true if I already knew (f(1/q) = f(1)/q), which is what I'm trying to establish. Hold on, let's not jump to conclusions. I should correct this by using the integer result: (f(p) = p f(1)), so (p f(1) = p f(1/q)), which implies (f(1/q) = f(1)/q). Then for any rational (r = p/q), I get (f(r) = f(p × 1/q) = p f(1/q) = p × f(1)/q = (p/q) f(1)), so (f(r) = c r) where (c = f(1) ∈ ℚ).

> I should double-check that every function of the form (f(x) = c x) with (c ∈ ℚ) satisfies the equation. Indeed, (f(x+y) = c(x+y) = cx + cy = f(x) + f(y)), so the functional equation is satisfied. This confirms that the set of solutions is at least the set of linear functions with rational slope.

> But wait, what if there are non-linear solutions over (ℚ)? Over the reals, pathological solutions exist if we assume the axiom of choice, but those constructions rely on a Hamel basis and non-constructive methods. Over (ℚ), the field structure is too rigid to allow such pathologies, and the inductive scaling argument above forces linearity. I should confirm that no exception arises in (ℚ); none does, because every rational input can be reached by integer multiples and divisions, and the additivity constraint propagates linearity everywhere. Therefore, the only solutions are the linear functions (f(x) = c x) with (c ∈ ℚ).

> Final answer: The functions (f: ℚ → ℚ) satisfying (f(x+y) = f(x) + f(y)) for all (x, y ∈ ℚ) are precisely the linear functions (f(x) = c x) where (c) is a fixed rational constant.

---

The only functions (f: ℚ → ℚ) satisfying (f(x+y) = f(x) + f(y)) for all (x, y ∈ ℚ) are the **linear functions** (f(x) = c x) for some constant (c ∈ ℚ). This follows from the Cauchy functional equation: first show (f(n x) = n f(x)) for integers (n), then extend to rationals (f([m/n] x) = [m/n] f(x)), and finally set (x = 1) to get (f(x) = c x) with (c = f(1) ∈ ℚ).

---

## Step-by-step solution

### Step 1: establish basic properties

From (f(x+y) = f(x) + f(y)), we derive:

- **Additivity**: (f(x+y) = f(x) + f(y)) for all (x, y ∈ ℚ).
- **Homogeneity over integers**: (f(n x) = n f(x)) for all (n ∈ ℤ) and (x ∈ ℚ).

---

### Step 2: prove homogeneity over integers

We prove (f(n x) = n f(x)) by induction for positive integers and extend to negative integers.

- **Base case**: (n = 1) gives (f(1 × x) = 1 × f(x)), which is trivial.
- **Inductive step**: assume (f(k x) = k f(x)). Then
f((k+1) x) = f(k x + x) = f(k x) + f(x) = k f(x) + f(x) = (k+1) f(x).
- **Negative integers**: for (n < 0), let (n = -m) with (m > 0). Then
f(n x) = f(-m x) = -f(m x) = -m f(x) = n f(x).

Thus, (f(n x) = n f(x)) for all (n ∈ ℤ) and (x ∈ ℚ).

---

### Step 3: extend homogeneity to rational scalars

Let (r = [m/n] ∈ ℚ) with (m ∈ ℤ) and (n ∈ ℤ ∖ {0}). We show (f(r x) = r f(x)):

f(n × [m/n] x) = f(m x) = m f(x).

But also, by integer homogeneity,

f(n × [m/n] x) = n f([m/n] x).

Equating the two expressions gives

n f([m/n] x) = m f(x) ⇒ f([m/n] x) = [m/n] f(x).

Hence, (f(r x) = r f(x)) for all (r ∈ ℚ) and (x ∈ ℚ).

---

### Step 4: determine the general form

Set (x = 1) to obtain (f(r) = r f(1)) for all (r ∈ ℚ). Let (c = f(1) ∈ ℚ); then

f(x) = c x for all x ∈ ℚ.

---

### Step 5: verify the solution

We confirm that (f(x) = c x) satisfies the original equation:

f(x+y) = c(x+y) = c x + c y = f(x) + f(y).

Thus, all solutions are of the form (f(x) = c x) for some constant (c ∈ ℚ).

---

## Conclusion

The only functions (f: ℚ → ℚ) satisfying (f(x+y) = f(x) + f(y)) for all (x, y ∈ ℚ) are the **linear functions** (f(x) = c x) for some constant (c ∈ ℚ).

---

## References

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^d55d13ff]. Nature Communications (2023). High credibility.

F-statistics and p -values

The basic idea behind the F-value in classical linear regression problems is to quantify how much better a more complex model (M 1 with k linear parameters) fits the data compared to a simpler model (M 0 with j linear parameters and j < k) given the n observed data points and the corresponding sum-squared errors (SSE) (Eq. 6).

Although not a linear model by nature, the log-logistic function still meets the required assumptions of random sampling, independence of observations, residual normality, and equal variance of the errors. The basic rationality behind CurveCurator's recalibrated F-statistic is similar to the linear F-statistic above. It also quantifies how much better the fitted log-logistic model (M 1) is compared to the mean model (M 0), which describes that there is no relationship between the applied dose and the observed response. We found, however, that n/k was a more appropriate scaling factor for the 4-parameter log-logistic function.

The obtained recalibrated F-value (Eq. 7) can then be used to calculate a p -value that quantifies how often a curve with a similar or bigger F-value can be found by random chance. We observed that these F-values follow a parameterized F-distribution with degrees of freedom that diverged from the case of linear models. Using extensive simulations under the null hypothesis (5 million curves for n = 5… 50), we obtained a simple quasi-linear function to calculate the "effective "degrees of freedom as a function of n (Eqs. 8–10).

---

### Rational strain design with minimal phenotype perturbation [^b17ff651]. Nature Communications (2024). High credibility.

Screening kinetic models

Once the initial set of kinetic models is available, we screen them to find the ones that are representative of the dynamic characteristics of the reference strain. The screening process follows four consecutive criteria to evaluate the models: (i) physiologically relevant linearized dynamics, (ii) robustness to perturbations in a nonlinear context, (iii) ability to reproduce experimentally observed temporal data, and (iv) robustness to enzymatic perturbations. In each step, we increase the robustness and quality of the models that satisfy the requisite criteria. More specifically, during the screening process, we screen for:
Linearized dynamics: We built each kinetic model from the initial set around a steady-state consistent with the integrated experimental data. However, not all of these models necessarily capture the experimentally observed dynamics of the metabolic network. To identify models with physiologically relevant dynamic properties, we assume that: (i) any experimentally observable steady state is locally stable; and (ii) since metabolic reactions occur at a timescale of seconds and milliseconds, metabolic processes should settle before the cell division, which is at a timescale of minutes and hours. To this end, we first linearize the models around their steady states, and estimate the time constants using the eigenvalues of the Jacobian. To compute the Jacobian, we need the kinetic parameters computed by a kinetic modeling technique and the steady-state concentrations in the metabolic network. These concentrations can be obtained by integrating the set of ODEs till they reach a steady state as done by MASSpyor Ensemble Modeling (EM), or directly from the constraint-based models used to build the kinetic models as in pyTFA. We then use the calculated eigenvalues and time constants to screen the models. A necessary condition for local linear stability of the system around the steady state is that its Jacobian has eigenvalues with negative real parts, i.e. implying that all infinitesimal perturbations to the system will eventually return to the original state of the system. However, our interest extends beyond whether the perturbed system will return to its original state but also how quickly it will do so. Therefore, in addition to screening based on local stability, we also use the dominant time constant of the model,(Eq. 1), to screen for models with physiologically relevant dynamics. Metabolic processes occur on different time scales, ranging from milliseconds to minutes, and the dominant time constants depend heavily on the organism. Metabolic responses to perturbations should settle down before cell division. According to the dynamical systems theory, these responses should reach 99.3% of their original steady states within 5 dominant time constants. Therefore, to have a model whose responses settle by the doubling time, its dominant time constants should be five times smaller than the doubling time. The reference strain, E. coli W3110 trpD9923, had a maximal in-silico growth rate of 0.32/h, corresponding to a doubling time of 130 min. For building K_trpD9923 in the first study, we chose kinetic models with a dominant time constant of less than 24 min. Models that meet this condition will likely return to 99.5% of their steady-state value within the doubling time when subjected to small perturbations (Supplementary Note 12). We set the time constant threshold for the enhanced models, eK_trpD9923, at 43 min, indicating that models meeting this criterion would return to 95% of their steady-state values within the doubling time. This relaxed constraint expanded the selection of models available for matching the experimentally observed behavior of the three strains.
Nonlinear response to concentration perturbations: The preceding linear stability analysis provides information on how the network will respond to infinitesimally small perturbations. However, in actual fermentation settings, the cells traverse different phases, such as the lag and exponential phases, during which there are significant fluctuations in concentration profiles. Before using the chosen kinetic models for strain design, we want them to be robust to such large-scale concentration changes. In the presence of experimental fermentation data, we can directly verify the robustness of our models by checking if they can reproduce the fermentation curves. However, in the absence of such temporal data, we can verify the robustness of the models using their nonlinear responses to randomly applied concentration perturbations. To do this, we apply a 'k-fold' perturbation to the steady-state concentrations of each kinetic model and integrate the system of ODEs to verify whether or not the perturbations are damped out before the cell's doubling time. We repeat this 'n-times' and select those models for which all the perturbed models return to the original steady state within the physiological timescale of the cell. This step was unnecessary for both studies as we had ample fermentation data to calibrate our models.
Reproduction of batch fermentation data: NOMAD integrates information on the temporal evolution of key metabolite concentrations, when available, by selecting models that reproduce the data within reasonable bounds. We first integrate all known information about the inoculum and the fermentation medium and run batch fermentation simulations using the models that are selected in the previous step. We then choose those models that can accurately capture experimental fermentation data, which is available in the form of growth curves, secretions, and uptakes. In the first study, we calibrated models to reproduce the behavior of the reference strain, W3110 trpD9923, in batch fermentation experiments. We integrated inoculum information provided in the experimental workand ran nonlinear simulations for each model that passed the screening based on linearized dynamics. We then chose those models within 5% and 10% of the final steady-state values of growth and extracellular anthranilate, respectively, whose fermentation times were less than 20 h. Including fermentation time in the pruning ensured that the models adhered to the experimentally observed dynamics of the strain in addition to the final titers. The kinetic models in the second study were calibrated on data from the reference strain, W3110 trpD9923, and from the two overproducing strains, W3110 trpD9923 /pJL aroG fbr and W3110 trpD9923 /pJL aroG fbr tktA. We integrated inoculum and medium data and simulated all three strains using the models that passed the screening based on linearized dynamics. For the two recombinant strains, we removed the inhibition of DDPA by phenylalanine by setting the inhibition constants to arbitrarily large numbers. Furthermore, we modeled tktA overexpression by applying a 10-fold increase to the activities of TKT1 and TKT2. We first selected models that could reproduce the behavior of W3110 trpD9923 and W3110 trpD9923 /pJL aroG fbr using the following criteria: (i) the simulated final growth and anthranilate titers of the reference strain should be within 10% and 20% of their experimental values (1.29 g/L and 0.31 g/L), (ii) the simulated anthranilate titers of the recombinant strains should be within 12.5% of those recorded experimentally (0.44 g/L). The models that passed this screening were further filtered for those that could produce anthranilate titers that were at least 90% of the experimentally reported values for W3110 trpD9923 /pJL aroG fbr tktA (0.75 g/L) when the double mutant was implemented in-silico.
Robustness to enzymatic interventions: The end goal of the framework is to provide targets for enzymatic interventions that enable us to achieve a given metabolic output. Not all the models that are selected in the previous screening steps are equally robust to enzymatic interventions - some can veer significantly from their behavior, showing little to no growth, while others can retain their reference growth level. Hence, to determine the robustness of each kinetic model to such interventions we apply a 'k-fold' perturbation to the maximal velocities of the different reactions and study the growth of the resulting strain. We repeat this 'n' times and choose those models for which all the perturbed strains demonstrate satisfactory growth. This final screening step ensures that the models are not only representative of the reference strain but also suitable for enzymatic interventions.

---

### Globalizing manifold-based reduced models for equations and data [^837c892a]. Nature Communications (2025). High credibility.

One of the very few mathematically rigorous nonlinear model reduction methods is the restriction of a dynamical system to a low-dimensional, sufficiently smooth, attracting invariant manifold. Such manifolds are usually found using local polynomial approximations and, hence, are limited by the unknown domains of convergence of their Taylor expansions. To address this limitation, we extend local expansions for invariant manifolds via Padé approximants, which re-express the Taylor expansions as rational functions for broader utility. This approach significantly expands the range of applicability of manifold-reduced models, enabling reduced modeling of global phenomena, such as large-scale oscillations and chaotic attractors of finite element models. We illustrate the power of globalized manifold-based model reduction on several equation-driven and data-driven examples from solid mechanics and fluid mechanics.

---

### Globalizing manifold-based reduced models for equations and data [^0798fc0c]. Nature Communications (2025). High credibility.

Discussion

We have presented a method to extend the range of validity of invariant manifold-based reduced-order models by applying Padé approximation, a classic analytic continuation technique. The Taylor coefficients of the parametrization of the SSM and the reduced dynamics were obtained with the robust numerical routines of SSMTool. We then extend their range of validity globally to obtain the gSSM-reduced model, by applying Padé approximation to these mappings that agree with the local Taylor expansions up to a prescribed order.

We have demonstrated the method on high-dimensional examples of dynamical systems exhibiting global nonlinear behaviour, such as transitions between steady states, large-amplitude oscillations, or even chaotic behaviour. In all cases, gSSM models obtained with Padé approximation significantly extend the domain of validity of the reduced model, reaching well beyond the domain of convergence of the classical Taylor series. We have also shown that the sign pattern of the univariate Taylor coefficients can be used to infer the location of the singularity.

To apply our approach, unexpected singularities in the approximants must be checked, and the optimal approximant of the Padé table must be determined according to the steps we have outlined in "Padé approximants". Luckily, the numerical effort required to compute Padé approximants is negligible compared to the computation of the original Taylor coefficients of the SSM and the reduced dynamics. Therefore, even if no singularity-free Padé approximants can be found up to a given order, the benefits of a gSSM model far outweigh the costs of deriving it.

We have shown that a data-driven analog of Padé approximation, rational function regression, can be directly applied to experimental data. Although rational function regression requires more computational power than the standard polynomial approximation implemented in SSMLearn, it can produce more accurate reduced models with fewer unknown coefficients, resulting in ultimately simpler models.

---

### Effective plans for hospital system response to earthquake emergencies [^b4d880be]. Nature Communications (2020). High credibility.

Optimization of performance metric

We evaluate both waiting times and effective use of ambulances as the system performance metric, thus the metric includes two objective functions. The first objective function C 1 (X) measures waiting time across the city as the average time that a patient would take since the earthquake until completing treatment in the operating room.

X represents a vector containing all the decision variables of flow x i, j (t) in edges and the storage y i (t) in the triage nodes. The numerator of C 1 (X) represents the total number of patients passing through the operating rooms (from each triage node, i ∈ Γ to the corresponding discharge node j = i + n h ∈ Λ) multiplied by their respective times to complete treatment, whereas the denominator is the total number of patients arriving to the triage areas. The time horizon t f is carefully chosen to have enough modeling time to treat the patients. However, in few simulations with significant number of patients and not many functional operating rooms, a couple of terms are added to the numerator, one with the remainder patients in the triage areas, t f × ∑ i ∈ Γ y i (t f) × d t, and another with the remainder patients in the ambulances, in order to properly incorporate the unmet demands at the end of the simulation into C 1 (X).

The second objective function measures ambulance usage as the total number of patients transported in ambulances. The objective function C 2 (X) is normalized by the total number of patients analogously to C 1 (X).

We define a system cost C (X) as a weighted sum of C 1 (X) and C 2 (X) to find a Pareto-optimal solution.

After assessing multiple α 1 and α 2 values, we minimized C (X) using values of 0.90 and 0.1, respectively. Smaller α 2 values resulted in inefficient ambulance usage with small reductions in waiting times, requiring some patients to be transferred multiple times in ambulances before being treated. Larger α 2 significantly increased waiting times, thus these α 2 values do not appropriately represent that the priority in the formulation is to minimize waiting times over to use ambulances with efficiency. We find the best set of decisions, vector that contains the values of flow variables x i, j (t) and storage variables y i (t) which minimize C (X).

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^37daae40]. Nature Communications (2025). High credibility.

Fig. 2
The matrix-tree theorem.

A Computing the steady-state occupancy π 1 by summing weights over directed spanning trees. Directed spanning trees are subgraphs containing all graph nodes but no cycles, with edges oriented toward a root node. In each directed spanning tree, the input forces make a positive, negative, or zero contribution to the tree weight. The structural vectorsare shown below each tree; these quantities enter into Equation (3) below. B Schematic illustration of the high-dimensional space of feature vectors ψ (i; θ) and χ (i, F). The depicted arrangement of vectors could solve a binary classification problem.

We define the input multiplicityas the number of edges affected per input variable, which we assume to be the same for each input. To focus on the functional way in which the input driving enters the steady-state probabilities, the driving contributions can be factored out in the algebraic expressions for the numerator and denominator of Equation (1). This has been previously been used to make analytical progress for M = D = 1 in, for example, refs. –. This equivalent formulation of Eq. (1) suggests that steady states of Markov jump processes implement a rational polynomial function of exponentiated input variables. Defining, we rewrite the matrix-tree expression for π i for general D and M We use the multi-index, whereis the set of D input labels and each componentof the multi-index runs over the values, to enumerate themonomials. These monomials y μ (F) in Equation (2) combinatorially depend on the different mixtures μ of input driving, representing a net total μ a of signed contributions from the input force F a, μ b such contributions for F b, and so on for each input. The coefficients, which are functions of the parameters θ, are the sums of weights over all directed spanning trees rooted at node i which have the corresponding mixture μ of signed input contributions. The monomial coefficientsthus represent learnable amplitudes of each polynomial basis function y μ (F). The coefficients in the denominator are defined as. Classification will be successful if, for F ρ drawn from class ρ, the coefficientsand monomials y μ (F ρ) are large for the same μ. In the subsequent sections of the paper and in the Supplementary Information we use the formulation in Equation (2) to show how the classification ability of a non-equilibrium Markov processes may be systematically modulated.

---

### Learning interpretable network dynamics via universal neural symbolic regression [^e181ac56]. Nature Communications (2025). High credibility.

Fortunately, with data acquisition becoming increasingly accessible, the emergence of data-driven technologies has assisted in increasing the frequency with which human experts discover patterns of change in complex systems –. They can provide domain experts with richer and more meaningful inspiration across various fields, thereby accelerating the process of scientific discovery in areas such as mathematics, and physics. Although much excellent work has been developed to reconstruct the symbolic models for low-variate dynamics of complex systems, e.g. bivariate shear flow equation, trivariate metapopulation epidemic model, and up to 9-variate Newton's law of gravitation, inferring governing equations for high-variate network dynamics remains important and challenging. This is primarily because the number of nodes N in network dynamics is usually large, seen as in the epidemic spreading where the number of transmission areas or individuals ranges from tens to billions. Additionally d can be multi-dimensional, resulting in too many free variables (N × d) in the equations and topological interactions with exponential growth, thereby increasing the complexity of inferring symbolic models.

At present, several cutting-edge studies are attempting to deal with the discovery of governing equations from network dynamics. Two-phase sparse symbolic regression (TPSINDy)simply parameterizes f as a learnable linear combination of pre-defined orthogonal or non-orthogonal elementary function terms. Although the efficiency of equation inference is high, the rationality of the pre-defined function terms directly impacts the results, making sufficient and accurate domain expertise essential. Another set of methods involves using graph neural networks (GNNs) to parameterize f overcomes excessive expert knowledge. However, due to the use of genetic programming (GP) to parse neural networks into symbolic equations, it brings the high-cost evolutionary search efficiency issue. Therefore, how to effectively balance expert knowledge and computational costs, while ensuring high computational efficiency, introducing only a small amount or no expert knowledge, lowering the threshold for use, and efficiently discovering governing equations remains an open challenge.

---

### Generating conjectures on fundamental constants with the ramanujan machine [^75e8911d]. Nature (2021). Excellent credibility.

Fundamental mathematical constants such as e and π are ubiquitous in diverse fields of science, from abstract mathematics and geometry to physics, biology and chemistry 1,2. Nevertheless, for centuries new mathematical formulas relating fundamental constants have been scarce and usually discovered sporadically 3–6. Such discoveries are often considered an act of mathematical ingenuity or profound intuition by great mathematicians such as Gauss and Ramanujan 7. Here we propose a systematic approach that leverages algorithms to discover mathematical formulas for fundamental constants and helps to reveal the underlying structure of the constants. We call this approach 'the Ramanujan Machine'. Our algorithms find dozens of well known formulas as well as previously unknown ones, such as continued fraction representations of π, e, Catalan's constant, and values of the Riemann zeta function. Several conjectures found by our algorithms were (in retrospect) simple to prove, whereas others remain as yet unproved. We present two algorithms that proved useful in finding conjectures: a variant of the meet-in-the-middle algorithm and a gradient descent optimization algorithm tailored to the recurrent structure of continued fractions. Both algorithms are based on matching numerical values; consequently, they conjecture formulas without providing proofs or requiring prior knowledge of the underlying mathematical structure, making this methodology complementary to automated theorem proving 8–13. Our approach is especially attractive when applied to discover formulas for fundamental constants for which no mathematical structure is known, because it reverses the conventional usage of sequential logic in formal proofs. Instead, our work supports a different conceptual framework for research: computer algorithms use numerical data to unveil mathematical structures, thus trying to replace the mathematical intuition of great mathematicians and providing leads to further mathematical research.

---

### Bias free multiobjective active learning for materials design and discovery [^71afaf28]. Nature Communications (2021). High credibility.

Introduction

The holy grail of material science is to find the optimal material for a given application. Finding the optimal material requires a metric to rank the materials. In case we have a single objective, our aim is clearly defined: we evaluate the performance indicator of the materials with respect to this objective and we can rank our materials. Developing efficient strategies to find such an optimum with a minimal number of experiments is an active area of research. In many practical applications, scientists and engineers are often faced with the challenge of having to simultaneously optimize multiple objectives. Optimizing one objective alone may come at the cost of penalizing others. For example, in drug discovery, scientists have to balance potency or activity with toxicities and solubility; or in the field of chemical process design, engineers have to optimize yields for several process units yet sacrifice, say, the energy consumption. Likewise, in the field of material science, desirable material properties can be interdependent or even inversely related. For example, one would like a material that is both strong and ductile, and as these are inversely correlated, it is challenging to synthesize new materials that satisfy both criteria at the same time. In these cases, there is no unique way to rank the materials.

If one has multiple objectives, a practical solution is to combine the different performance indicators into a new overall performance indicator. However, unless such an overall performance indicator is a unique, well-defined function of different performance indicators (e.g. costs), the arbitrary combination of performance parameters obscures the true nature of the optimization problem; there simply is no material that simultaneously optimizes all target properties. No single optimum is generally preferred over all the others; hence, the most valuable information any search in the design space can give is the set of all possible materials for which none of the performance indicators can be improved without degrading some of the other indicators. In statistical terms, these materials are referred to as the set of all Pareto-optimal solutions (i.e. the Pareto front). In this study, we address the question of how to efficiently search for this set of materials and with confidence to not discard a good material. Such a methodology is particularly important if, because of limited resources, it is difficult to evaluate an unlimited number of materials.

---

### Quantifying unobserved protein-coding variants in human populations provides a roadmap for large-scale sequencing projects [^3db7b451]. Nature Communications (2016). Medium credibility.

Methods

UnseenEst algorithm

UnseenEst uses the empirical SFS of a given class of variants in a cohort to estimate its frequency distribution in the population. The inputs into the algorithm are the number of alleles in the sample cohort, k, and the SFS, which is a set of counts, { F i }, where F i is the number of variants that are observed in exactly i out of the k alleles. A key challenge for the method is to accurately estimate the frequency distribution of variants that have empirical count of 0 (that is, they are not observed) in the cohort but are likely to have some small, non-zero frequency in the population.

More concretely, let X denote a discrete set of frequencies x in [0, 1] and let h (x) denote the fraction of all the variants with frequency x. UnseenEst estimates h (x) by finding the set of h (x) that jointly minimizes the value of the expression:

where bin(x, k, i) is the binomial probability of observing i heads in k independent flips of a coin with bias x. The intuition for minimizing this objective is as follows:is the number of variants that we expect to find in i out of k alleles in the cohort and F i is the empirical number of variants observed in i alleles. If h (x) is the true frequency distribution, then the expectation should be close to the empirical, which is why we want to find h (x) that minimizes the objective above. Given an estimate of the frequency distribution h (x), the expected number of unique variants in N alleles can be calculated by the formula.

The standard 3rd order jackknife estimator does not estimate the frequency distribution h (x). Instead, it directly estimates that the number of unique variants in N alleles is g 1 (N, k) F 1 +g 2 (N, k) F 2 +g 3 (N, k) F 3, where F 1, F 2, F 3 are the number of variants observed once, twice and three times in k alleles, and g 1, g 2, g 3 are specific functions of N and k derived from self-consistency requirements. Note that the confidence intervals of the jackknife in Supplementary Fig. 3 are very narrow because there is relatively little variation in the counts F 1, F 2, F 3, when the sample size is large.

---

### A rational approach for selection of optimal covariate-based dosing strategies [^ff10e1ca]. Clinical Pharmacology and Therapeutics (2003). Low credibility.

Background

At present, there is no rational approach for choosing a dosing strategy for individualization based on a covariate. An approach to use in establishment of an a priori dosing strategy for individualization is presented. Factors influencing the choice of such a dosing strategy are identified.

Methods

The approach requires definition of the following: target variable, seriousness of deviations from the target (ie, risk function), population model, covariate distributions, and constraints. Minimizing the total risk yields an optimal dosing strategy, estimated as dose sizes for different subpopulations and covariate cutoff values at which doses are increased or decreased. The method was illustrated with the use of simulated and real drug examples for the situation in which clearance is related to creatinine clearance.

Results

The estimated optimal cutoff(s) paralleled the median creatinine clearance in the population. The extent of variability in clearance explained by creatinine clearance was the main factor influencing the optimal ratios between adjacent dose sizes. An optimal dosing strategy was possible to estimate for the real drug.

Conclusions

The method is simple to perform, although one difficulty lies in defining the target variable and risk function. Our results imply that commonly used constraints in dosing strategies based on renal function (ie, dose ratio of 2 and predetermined cutoffs) are nonoptimal in the sense we propose. Because an optimal dosing strategy may not be practical to use, the therapeutic cost that would result with any constraint can be assessed by comparison of the outcome after the desired and the optimal strategy.

---

### Optimal enzyme utilization suggests that concentrations and thermodynamics determine binding mechanisms and enzyme saturations [^26d53043]. Nature Communications (2023). High credibility.

Deciphering the metabolic functions of organisms requires understanding the dynamic responses of living cells upon genetic and environmental perturbations, which in turn can be inferred from enzymatic activity. In this work, we investigate the optimal modes of operation for enzymes in terms of the evolutionary pressure driving them toward increased catalytic efficiency. We develop a framework using a mixed-integer formulation to assess the distribution of thermodynamic forces and enzyme states, providing detailed insights into the enzymatic mode of operation. We use this framework to explore Michaelis-Menten and random-ordered multi-substrate mechanisms. We show that optimal enzyme utilization is achieved by unique or alternative operating modes dependent on reactant concentrations. We find that in a bimolecular enzyme reaction, the random mechanism is optimal over any other ordered mechanism under physiological conditions. Our framework can investigate the optimal catalytic properties of complex enzyme mechanisms. It can further guide the directed evolution of enzymes and fill in the knowledge gaps in enzyme kinetics.

---

### Heuristic identification of biological architectures for simulating complex hierarchical genetic interactions [^1847f692]. Genetic Epidemiology (2015). Low credibility.

An important limitation of the current study is that we did not consider effect size for each genetic variant. Our models included moderate to large genetic effects that are not realistic for common diseases. This can be addressed directly by changing the encoding on the genotypes. For example, we used 0, 1, and 2 to encode the different genotypes. This produces genotypes that contribute zero, one or two units of risk to the liability distribution depending of course on the mathematical function. These numbers could be divided by 10, for example, to reduce the risk contribution relative to the other genotypes. This would naturally reduce the effect size. Alternatively, we could introduce coefficients at the mathematical function layer. There is no reason why the result of any mathematical function could not be combined with a constant using a second function thus modifying its contribution to the liability. Either one or both of these mechanisms would allow the user to alter effect size. The challenge of course is that it might not be apparent how the effect size will change given the hierarchical complexity of the rest of the model. This could be greatly facilitated by perhaps specifying relative effect sizes for each genetic variant that could be used as a fitness objective in a heuristic search of the model space. We address new directions for the heuristic search next.

---

### Leveraging data-driven self-consistency for high-fidelity gene expression recovery [^b46a7fe2]. Nature Communications (2022). High credibility.

ROI selection and histogram equalization

We divide the expression matrix S is divided into N ≥ 4 ROIs, which we denote by S i, i = 1, 2, ⋯, N. Letanddenotes the number of matrix elements and possible expression values, in each ROI S i, respectively. Furthermore, we define the normalized histogram associated with the possible gene expression values in the ROI S i as followswhereis the number of elements in the ROI with the gene count n such that. The corresponding cumulative distribution function (CDF), is given by

Adaptive equalization of histogram

The goal of the histogram adaptive equalization is to find a transformation for the expression values such that the transformed CDF is approximated by the CDF of f (x), where f (x) can be f g (x), f r (x) or f e (x). As example, the CDF of an exponential distribution can be written aswhere λ denotes the decay parameter. SERM uses the transformationfor each ROI to map the expression values to their new values. In particular, consider transforming the expression counts bywhere the floor function ⌊ x ⌋ is the biggest integer not exceeding x. The mapis then applied to each expression value in the ROI S i.

The main intuition behind this transformation comes from the inverse sampling method. In particular, let X denotes continuous random variables whose CDF is strictly increasing on the possible value onwith densities p X and p Y, respectively. Consider the transformationThen, Y has the uniform distribution, i.e. To see this note that

Furthermore, using the inverse transform sampling technique, the random variablehas the exponential distribution, i.e. Z ~ F (t). Combining the transformations T 0 and T 1, we conclude thathas the desired exponential distribution. Now, the transformationdefined in Eq. (11) is an inverse sampling method for the discrete random variable similar to the continuous transformation T 1 ∘ T 0. Similar analytical equations to Eqs. (10)–(15) can be derived for Gaussian and Rayleigh distributions.

Sliding ROI

The ROI is slided throughout the whole expression matrix and gene imputation is performed at each ROI to obtain the resulting imputed gene expression matrix from SERM.

---

### Active learning framework with iterative clustering for bioimage classification [^8e4a2687]. Nature Communications (2012). Medium credibility.

Because the function f is for the display to the user and the optimal form of f is dependent on the properties of the input images and the user's intent, we cannot obtain the optimal f in advance. The properties of the images are represented by feature vectors X through feature extraction. The user's intent for classification is represented by the annotation status Y, which is dynamically updated by active learning. Then we can estimate function f for the specific images and each intent using X and Y. Let a function V be the evaluation function of f, then the optimization of f can be formulated asBecause the function V has the argument Y, which can vary with every cycle of the annotation, CARTA searches for the suboptimal f asymptotically and synchronously to the annotation step.

Implementation of CARTA framework

To implement the CARTA framework, we aimed to combine simple methods in an effort to achieve general versatility and simplicity. In that context, the function f is composed of a feature-selecting function g and SOM function h. At first, function g selects some features from the feature vector x ∈ R D, therefore, an input image is mapped to a vector in lower dimensional feature space as g (x)∈ R S, where S is a number of selected features and D ≥ S ≥ 1. Next, function h tiles the images on the rectangular grid Q 1 × Q 2. Taken together, formula (1) is performed by functions h and g via the subspace R S :Then we focus on the optimization of function g, which compresses the input image onto the feature subspace R S :Function g can be perceived as a feature filter that select S features from D features, and in that respect CARTA can also be used as a solving tool for feature-selection problems (FSPs). We used the genetic algorithm (GA) as an optimization technique for finding the suboptimal solution from multiple candidates: 2 D −1 combinations. The GA, which imitates biological reproduction and natural selection, has the advantages that it is free from the need for parameter tuning and it converges quickly. Because of this, the GA has already been used in FSP solver.

---

### Limits on the computational expressivity of non-equilibrium biophysical processes [^792ad689]. Nature Communications (2025). High credibility.

Storing more classes by increasing input multiplicity

We now generalize the binary classification task and ask how many different classes can be stored as a function of the hyperparameters M and D. Classifying many different classes is crucial in biology. For example, deciphering the glycan code, which specifies one of several hundred different cell states, or recognizing previously encountered antigens during an immune response both require choosing among large numbers of possibilities. How these biochemical systems achieve these complex classification tasks (e.g. through microscopic sensing events like estimating antigen binding affinities) remains an important and open question.

For M = 1, a simple geometric argument suggests that up to 2 D classes could in principle be separated. Each class could be placed in one of the 2 D orthants of the input space, and a classifier with monotonicity along each axis could in principle assign a unique response to each. In Fig. 4 A we attempt to distinguish four classes by placing them in the four quadrants of the (F a, F b) plane, but we find, contrary to this expectation, that no network can separate them all with M = 1.

Fig. 4
Input multiplicity allows overcoming reduced degrees of freedom and increases multi-class capacity.

A Plot of the learned classification functions π 1 (F), π 2 (F), π 3 (F), and π 4 (F) for the network shown in Fig. 1 E, with only the solid arrows used for inputs (M = 1). B Schematic illustration of how the learnable parameters W i j (the edge rates on the left) are first multiplied within directed spanning trees into products called directed tree weights, which are then summed together to yield the polynomial functionsappearing in Equation (2). Although each functionis uniquely defined, there exist equality constraints owing to the physics of Markov networks which reduce the effective number of degrees of freedom below the number needed to solve the four-class classification task in A. C By including driving along the dashed arrows in Fig. 1 E (setting M = 2), there are sufficiently many degrees of freedom to solve the four-class classification task.

---

### Rational strategies for improving the efficiency of design and discovery of nanomedicines [^a91d54ff]. Nature Communications (2024). High credibility.

The rise of rational strategies in nanomedicine development, such as high-throughput methods and computer-aided techniques, has led to a shift in the design and discovery patterns of nanomedicines from a trial-and-error mode to a rational mode. This transition facilitates the enhancement of efficiency in the preclinical discovery pipeline of nanomaterials, particularly in improving the hit rate of nanomaterials and the optimization efficiency of promising candidates. Herein, we describe a directed evolution mode of nanomedicines driven by data to accelerate the discovery of nanomaterials with high delivery efficiency. Computer-aided design strategies are introduced in detail as one of the cutting-edge directions for the development of nanomedicines. Ultimately, we look forward to expanding the tools for the rational design and discovery of nanomaterials using multidisciplinary approaches. Rational design strategies may potentially boost the delivery efficiency of next-generation nanomedicines.

---

### Does temporal irregularity drive prediction failure in schizophrenia? temporal modelling of ERPs [^ba2572e7]. Schizophrenia (2022). Medium credibility.

In electronic circuit analysis, a high-order control system undergoes a transient phase when it receives an input disturbance. Similarly, our "tuned" system has its own TF function, characterized by its settling time until it returns to its steady state, and presents a peak amplitude as well. The Transfer Function is defined as the ratio of Laplace transform of output response to Laplace transform of input (excitation). Equation (1) is a rational function with m poles and n zeros expressed in the complex variable s,

s = σ+ jω as follows:

The transfer function provides a basis for detecting important system response characteristics without solving the complete complex differential equation, as demonstrated below in Eq. 2.

We need high precision, in order to obtain the most realistic simulation of our system by the transfer function. We confirm that the fit of our properly smoothed ERPs is in absolute accordance with the N1 and P2 evoked potentials. The increasing number of poles increases the complexity of our modeling transfer function, and ensures the suitability of the model dynamics. The algorithm is solved repeatedly, until the solution reaches the desired accuracy.

Once the expression of the Transfer Function for each subject is found in s-domain, we decompose the equation, in terms of Partial Fraction Expansion, where z n are the zeros and p m the poles of our transfer function, as shown in Eq. 3.

---

### Knowledge graph-based recommendation framework identifies drivers of resistance in EGFR mutant non-small cell lung cancer [^f7823c7d]. Nature Communications (2022). High credibility.

Results

Re-ranking of CRISPR hits can be approached as multi-objective optimization

We framed re-ranking of CRISPR hits as a multi-objective optimization problem. In this setting, diverse lines of evidence that support gene's relevance are treated as multiple objectives (Fig. 1). In other words, the formal goal is to simultaneously optimize k objectives, reflected in k objective functions: f 1 (x), f 2 (x). f k (x). Individual functions form a vector function F (x):where x = [x 1, x 2. x m] ∈ Ω; x represents the decision variable, Ω-decision variable space. Therefore, multi-objective optimization can be defined as minimization (or maximization) of the objective function set F (x). With multiple competing objectives a singular best solution often cannot be found. However, one can identify a set of optimal solutions based on the notion of Pareto dominance. A solution x 1 dominates solution x 2 if the following two conditions are true:
solution x 1 is not worse than x 2 according to all objectives;
solution x 1 is strictly better than solution x 2 according to at least one objective.

Fig. 1
Recommendation system takes into account diverse types of evidence to suggest promising drivers of drug resistance in NSCLC.

The evidence from specially built knowledge graph, literature, clinical and pre-clinical datasets is aggregated and formalized as objectives in multi-objective optimization (MOO) task. Recommended solutions (genes) represent the optimal trade-offs between the conflicting objectives. A subset of recommended genes is passed for the experimental validation.

If both conditions are true, we can say that x 1 dominates x 2, which is equal to x 2 being dominated by x 1. In other words, dominant solutions can not be improved any further without compromising at least one of the other objectives. A set of such dominant solutions forms a Pareto front, which combines the best trade-off combinations between competing objectives. Therefore, by computing Pareto fronts on diverse sets of objectives defined based on CRISPR screen data and additional supporting evidence we can narrow down the number of promising markers of EGFR TKI resistance (Fig. 1).

---

### Data-driven control of complex networks [^85fc9bea]. Nature Communications (2021). High credibility.

Results

Network dynamics and optimal point-to-point control

We consider networks governed by linear time-invariant dynamicswhere, anddenote, respectively, the state, input, and output of the network at time t. The matrixdescribes the (directed and weighted) adjacency matrix of the network, and the matricesand, respectively, are typically chosen to single out prescribed sets of input and output nodes of the network.

In this work, we are interested in solving point-to-point control problems; that is, designing open-loop control policies that steer the network output y (t) from an initial value y (0) = y 0 to a desired one y (T) = y f in a finite number of steps T. If y f is output controllable in T steps (a standing assumption in this paper; we refer to Supplementary Note 1 for more details), then the latter problem admits a solution and, in fact, there are many ways to accomplish such a control task. Here, we assume that the network is initially relaxed (x (0) = 0), and we seek the control inputthat drives the output of the network to y f in T steps and, at the same time, minimizes a prescribed quadratic combination of the control effort and locality of the controlled trajectories.

Mathematically, we study and solve the following constrained minimization problem:where Q ≽ 0 and R ≻ 0 are tunable (positive semidefinite and positive definite, respectively) matrices that penalize output deviation and input usage, respectively, and y T = y (T). Problem (2) generalizes the classic (open-loop) linear–quadratic control framework by including the possibility of minimizing a linear function of the state (as opposed to the whole state) in addition to the control input. Further, we remark that increasing R in Eq. (2) leads to optimal control inputs that achieve the desired final state with increasingly smaller magnitudes. Similarly, the matrix Q in Eq. (2) weighs the norm of the output (state), so that increasing Q forces the optimization problem to generate inputs that limit the norm of the output (state), at the expenses of using a larger control input. In particular, if Q = 0 and R = I, thencoincides with the minimum-energy control to reach y f in T steps.

---

### Restricting mutualistic partners to enforce trade reliance [^ccbfea6b]. Nature Communications (2016). Medium credibility.

The focal fungus also acquires an amount P f of phosphorus and retains a fraction 1− q f of this, so that it has a total quantity of phosphorus P tf = P f (1− q f). Substituting the expressions for C tf and P tf, we have derived above into the fungus' fitness function given in the main text yields

We use equations (5a) and (5b) to calculate evolutionary maxima. Specifically, we calculate ESSs, strategies where deviations from population mean strategies leads to decreased fitness (mathematically, we calculate:andfor plants, andfor mycorrhizal fungi). We find that the fitness equilibrium is always unique given parameter values and that the second derivatives of fitness with respect to individual strategy are always negative (andfor plants, andfor mycorrhizal fungi). Therefore, the fitness equilibrium is a unique ESS. In this analysis, we do not consider whether the ESS is also convergence stable. The ESSs for plants depend on the strategies used by fungi, and the ESSs for fungi depend on the strategies used by plants. Hence, we need to find the ESSs for plants given fungal strategy, and the ESSs for fungi given plant strategy. Then, we can solve these four resulting expressions to find co-evolved ESSs in terms of model parameters only (x *, q p *, q f *, r*). We first calculate the equilibria of fitness for allocation to direct carbon uptake and trade in a focal plant, ∂w p / ∂x p = ∂w p / ∂q p = 0. At an ESS, any individual receives population mean fitness to first order, so we take the focal plant's strategy to be the population average. Substituting, we find that at equilibrium

---

### General conditions for predictivity in learning theory [^e1e7f28e]. Nature (2004). Excellent credibility.

Developing theoretical foundations for learning is a key step towards understanding intelligence. 'Learning from examples' is a paradigm in which systems (natural or artificial) learn a functional relationship from a training set of examples. Within this paradigm, a learning algorithm is a map from the space of training sets to the hypothesis space of possible functional solutions. A central question for the theory is to determine conditions under which a learning algorithm will generalize from its finite training set to novel examples. A milestone in learning theory was a characterization of conditions on the hypothesis space that ensure generalization for the natural class of empirical risk minimization (ERM) learning algorithms that are based on minimizing the error on the training set. Here we provide conditions for generalization in terms of a precise stability property of the learning process: when the training set is perturbed by deleting one example, the learned hypothesis does not change much. This stability property stipulates conditions on the learning map rather than on the hypothesis space, subsumes the classical theory for ERM algorithms, and is applicable to more general algorithms. The surprising connection between stability and predictivity has implications for the foundations of learning theory and for the design of novel algorithms, and provides insights into problems as diverse as language learning and inverse problems in physics and engineering.

---

### A space-time tradeoff for implementing a function with master equation dynamics [^57fa10b0]. Nature Communications (2019). High credibility.

Master equations are commonly used to model the dynamics of physical systems, including systems that implement single-valued functions like a computer's update step. However, many such functions cannot be implemented by any master equation, even approximately, which raises the question of how they can occur in the real world. Here we show how any function over some "visible" states can be implemented with master equation dynamics-if the dynamics exploits additional, "hidden" states at intermediate times. We also show that any master equation implementing a function can be decomposed into a sequence of "hidden" timesteps, demarcated by changes in what state-to-state transitions have nonzero probability. In many real-world situations there is a cost both for more hidden states and for more hidden timesteps. Accordingly, we derive a "space-time" tradeoff between the number of hidden states and the number of hidden timesteps needed to implement any given function.

---

### Statistical analysis and optimality of neural systems [^bf829ef6]. Neuron (2021). Medium credibility.

Normative theories and statistical inference provide complementary approaches for the study of biological systems. A normative theory postulates that organisms have adapted to efficiently solve essential tasks and proceeds to mathematically work out testable consequences of such optimality; parameters that maximize the hypothesized organismal function can be derived ab initio, without reference to experimental data. In contrast, statistical inference focuses on the efficient utilization of data to learn model parameters, without reference to any a priori notion of biological function. Traditionally, these two approaches were developed independently and applied separately. Here, we unify them in a coherent Bayesian framework that embeds a normative theory into a family of maximum-entropy "optimization priors". This family defines a smooth interpolation between a data-rich inference regime and a data-limited prediction regime. Using three neuroscience datasets, we demonstrate that our framework allows one to address fundamental challenges relating to inference in high-dimensional, biological problems.

---

### Application of combinatorial optimization strategies in synthetic biology [^4b6982ef]. Nature Communications (2020). High credibility.

In the first wave of synthetic biology, genetic elements, combined into simple circuits, are used to control individual cellular functions. In the second wave of synthetic biology, the simple circuits, combined into complex circuits, form systems-level functions. However, efforts to construct complex circuits are often impeded by our limited knowledge of the optimal combination of individual circuits. For example, a fundamental question in most metabolic engineering projects is the optimal level of enzymes for maximizing the output. To address this point, combinatorial optimization approaches have been established, allowing automatic optimization without prior knowledge of the best combination of expression levels of individual genes. This review focuses on current combinatorial optimization methods and emerging technologies facilitating their applications.

---

### Rational strategies for improving the efficiency of design and discovery of nanomedicines [^9a38ded0]. Nature Communications (2024). High credibility.

Fig. 3
The expansion and application of rational strategies for improving the efficiency of the design and discovery of nanomedicines.

Interdisciplinary integration can enrich rational design and discovery strategies for nanoparticles. Rational strategies are conducive to the directed evolution of nanomedicines to obtain nanoparticles with the desired properties. Rational design strategies have the potential to discover nanomedicines capable of precise drug delivery and controllable release for enhancing antitumour therapy.

---

### Rationally inattentive intertemporal choice [^c3e5572f]. Nature Communications (2020). High credibility.

Discounting of future rewards is traditionally interpreted as evidence for an intrinsic preference in favor of sooner rewards. However, temporal discounting can also arise from internal uncertainty in value representations of future events, if one assumes that noisy mental simulations of the future are rationally combined with prior beliefs. Here, we further develop this idea by considering how simulation noise may be adaptively modulated by task demands, based on principles of rational inattention. We show how the optimal allocation of mental effort can give rise to the magnitude effect in intertemporal choice. In a re-analysis of two prior data sets, and in another experiment, we reveal several behavioral signatures of this theoretical account, tying choice stochasticity to the magnitude effect. We conclude that some aspects of temporal discounting may result from a cognitively plausible adaptive response to the costs of information processing.

---

### An event-based architecture for solving constraint satisfaction problems [^57ad565e]. Nature Communications (2015). Medium credibility.

Boolean satisfiability problems

Let X = { x 1, …, x N } be a set of Boolean variables. A literal is either a variable or its negation. The solution to a Boolean satisfiability or K-SAT problem is the variable assignment that satisfies the logical expression involving the variables of X:

where the clause c i is the disjunction of K literals. K-SAT for K ≥ 3 is NP complete.

A highly efficient algorithm for solving random SAT problems is the probSAT algorithm, winner of the parallel random track of the 2014 satcompetition. probSAT iteratively modifies a variable assignment by choosing a random unfulfilled clause c u and changing the assignment of ('flipping') a random variable x f in c u, thereby fulfilling c u. The choice x f is governed by a heuristic function f (m, b), where m (the 'make' heuristic) is the number of clauses that are newly fulfilled when x f is flipped and b (the 'break' heuristic) is the number of clauses that are newly unfulfilled when x f is flipped. The heuristic function is renormalized into a probability over the available choices and x f is chosen according to these probabilities. The heuristic function f (m, b) can take several different forms. In our benchmarks, we use the particularly effective 'exponential' form:

---

### Use of machine learning to differentiate children with Kawasaki disease from other febrile children in a pediatric emergency department [^3960e53a]. JAMA Network Open (2023). High credibility.

Model Establishment

Once obtained, all the cases were randomly split into the training set (80% of the cases) and the testing set (20% of the cases). There was no significant difference in variables between the training set and the testing set. We used Python programming language to build the prediction model. We chose the eXtreme Gradient Boosting (XGBoost) machine learning algorithm. Gradient boosting is a powerful ensemble method that combines base models in a sequential manner to achieve high predictive accuracy. In our study, we used imbalanced XGBoostfor the classification of our KD data set, which is a highly label-imbalanced classification data set. Details of the hyperparameters used and imbalanced XGBoost are available in the eMethods in Supplement 1.

For a given data set D = {(X i, y i), i = 1, 2, 3, …, N, y ∈ (0, 1)} with N samples, the test data with M samples are denoted as T = {(X k, y k), k = 1, 2, 3, …, M, y ∈ (0, 1)}. A loss function is defined as L (Y, Q) = ∑ M k = 1 L (y k, q k), where y k is the label (or outcome) and q k is the predictive value of some X k sample. We can define a binary classification problem as finding a function f that minimizes loss function L (Y, f (X)). XGBoost uses standard cross-entropy (CE) loss:

---

### A general derivation and quantification of the third law of thermodynamics [^314e2820]. Nature Communications (2017). Medium credibility.

Discussion

We hope the present work puts the third law on a footing more in line with those of the other laws of thermodynamics. These have already been long established, although they've recently been reformulated within the context of other resource theories. Namely, as described in ref. the first law (energy conservation), and unitarity (or microscopic reversibility) describe the class of operations which are allowed within thermodynamics. The zeroth law, is the fact that the only state which one can add to the theory without making it trivial, are the equivalence class of thermal states at temperature T. This allows the temperature to emerge naturally. The second law(s), tells us which state transformations are allowed under the class of operations. For macroscopic systems with short-range interactions, there is only one function, the entropy, which tells you whether you can go from one state to another, but in general there are many constraints. The third law quantifies how long it takes to cool a system. We propose to generalize it further: While the second laws tell us which thermodynamical transitions are possible, generalized third laws quantify the time of these transitions. In this context, it would be interesting to explore the time and resource costs of other thermodynamical transitions. It would also be interesting to explore the third law in more restricted physical settings, as well as to other resource theory frameworks, in particular, those discussed in ref.

It is worth noting that scaling we find, for example, the inverse polynomial of equation (1), is more benign than what one might have feared, and does not exclude obtaining lower temperatures with a modest increase of resources. However, it is also stronger than that envisioned when the third law was original formulated. Consider for example, the cooling protocol of Fig. 1 proposed by Nernst. It is unphysical, since it requires an infinite number of heat baths, each one at a lower and lower temperature, and with the final heat baths at close to zero temperature. However, it allows the temperature of the system to decrease exponentially in the number of steps, something which we are able to rule out when one doesn't have colder and colder reservoirs.

---

### What quantile regression does and doesn' T Do: a commentary on petscher and logan (2014) [^220ddc10]. Child Development (2019). Medium credibility.

Petscher and Logan's (2014) description of quantile regression (QR) might mislead readers to believe it would estimate the relation between an outcome, y, and one or more predictors, x, at different quantiles of the unconditional distribution of y. However, QR models the conditional quantile function of y given x just as linear regression models the conditional mean function. This article's contribution is twofold: First, it discusses potential consequences of methodological misconceptions and formulations of Petscher and Logan's (2014) presentation by contrasting features of QR and linear regression. Second, it reinforces the importance of correct understanding of QR in empirical research by illustrating similarities and differences in various QR estimators and linear regression using simulated data.

---

### Resilient cooperators stabilize long-run cooperation in the finitely repeated prisoner's dilemma [^e7acc36b]. Nature Communications (2017). Medium credibility.

In addition to replicating the high-level results of our experiment, the learning model also makes two predictions. First, as shown in Fig. 6a, cooperation in rounds 8, 9 and 10 for the α = 0.4 case remains stable for at least 4,000 games, ten times the length of our experiment. This result suggests that the apparent stabilization of cooperation that we observe in the experiment after 7 days is not simply a slowing down of the unravelling process, but an end to it. In other words, the model predicts that with sufficiently many resilient cooperators present in a population of rational cooperators, cooperation can be sustained indefinitely. Second, the model also makes a prediction about how many resilient cooperators are necessary to sustain cooperation even among rational cooperators. To show this result, we first define r ∞ as the average first round of defection r d for rational players as it approaches its asymptotic limit (in practice we estimate r ∞ by running the simulations for at least 2,000 games). Figure 6b shows estimated r ∞ as a function of α along with the values of α ≈0.4, r ∞ ≈8.2 obtained from our experiment (averaged over the stable phase, days 7–20). In addition to reinforcing the agreement between experiment and simulation noted above, Fig. 6b also predicts the full functional dependency of r ∞ (α). Notably, r ∞ appears to undergo a sharp transition, resembling an epidemic threshold, at some critical value α * ≈0.1: for α < α * unravelling progresses all the way to the beginning of the game (r ∞ = 1), whereas for α > α *, r ∞ increases sharply and nonlinearly, eventually approaching r ∞ = 10 (that is, no unravelling) when α = 1 (see Supplementary Figs 9 and 10 for robustness checks).

---

### Proof-of-concept of a novel structural equation modelling approach for the analysis of functional magnetic resonance imaging data applied to investigate individual differences in human pain responses [^ef8f2793]. Human Brain Mapping (2023). Medium credibility.

2.2.5 Detailed mathematical concepts underlying

In order to describe a complete network of interconnected regions and the signaling between them we can use a set of linear equations. These equations are in the form of Equation 1 for input signaling to every region, and Equation 2 for output signaling for every connection. Note that the method allows for the possibility that a region may have different outputs to each region to which it connects. This is necessary because we are describing the total observed behavior of regions which span multiple voxels and can contain a large number of neurons and synapses. The process of fitting this set of equations to the measured BOLD responses consists of finding the B values and latent inputs for which all of the equations in the set have the least amount of residual (i.e. unexplained signal variation).

---

### Parametric and nonparametric statistical methods for genomic selection of traits with additive and epistatic genetic architectures [^9e95849a]. G3 (2014). Low credibility.

In the remainder, we focus on the ε -insensitive loss function, which needs a more robust representation to account for the noise in the data.

We can add extra "cost" (or allow for additional uncertainty) by introducing non-negative "slack variables" ξ constrained as follows:

ξ 1 i ≥ y i − f (x i) − ε, where i = 1, …, n n is the number of training observations,

ξ 2 i ≥ f (x i) − y i − ε, where i = 1, …, n.

We can now re-write the objective function to be minimized as:The solution to this constrained minimization problem has the form: The solution depends on the training data through the inner product 〈 x i, x j 〉, which is a linear function of the observations.

To take advantage of higher dimensional feature spaces, we can introduce the data via nonlinear functions. For example, we can replace the inner product of the data by a kernel function:Some commonly used kernel functions include:

1 The linear kernel k (x, z) = 〈 x, z 〉
2 The Gaussian radial basis function k (x, z) = exp(−σ‖x − z‖²), where σ is the bandwidth parameter
3 The Laplace radial basis function k (x, z) = exp(−σ‖x − z‖).

The solution to the minimization problem can also be written as a function of the kernel function. The resulting expression is. The choice of the kernel function and of the tuning parameters λ, ε, and σ are not straightforward. Because optimizing SVMs is not the focus of this article, we refer the reader to.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^f5f71204]. Nature Communications (2025). High credibility.

We can invertto obtain the mixed PPF (percent point function, also known as quantile function, or percentile function):which is also a 1-d function, irrespective of the dimensionality ofor. We can then rewrite the risk as a one dimensional integral in:To obtain equation (20), we simply used Fubini's theorem to reorder the integral of (18) and marginalised over all slices of a given loss. The integral form (first line of equation (20)) is equivalent to averaging an infinite number of samples, and therefore to the (true) risk, whereas the average over observed samples (second line of equation (20)) is exactly the empirical risk defined in equation (5).

In practice, to evaluate equation (20), we use the observed samples to compute the sequence of per-sample losses. This provides us with a sequence of losses, which we use as ordinate values. We then sort this sequence so that we havewith Q i ≤ Q i + 1, and assign to each the abscissa Φ i = i / L + 1, such that losses are motonically increasing and uniformly distributed on the [0, 1] interval. This yields the empirical PPF of the loss — the "empirical" qualifier referring to this construction via samples, as opposed to an analytic calculation. Interpolating the points then yields a continuous function which can be used in further calculations. All examples in this paper linearly interpolate the PPF from 2¹⁰ = 1024 points.

In Fig. 4 we show four examples of empirical PPFs, along with their associated empirical CDFs. We see that the statistics of the additive observational noise affects the shape of the PPF: for noise with exponential tails, as we get from Gaussian or Poisson distributions, we have strong concentration around the minimum value of the loss followed by a sharp increase at Φ = 1. For heavier-tailed distributions like Cauchy, loss values are less concentrated and the PPF assigns non-negligible probability mass to a wider range of values. The dimensionality of the data also matters. High-dimensional Gaussians are known to place most of their probability mass in a thin shell centred on the mode, and we see this in the fourth column of Fig. 4: the sharp increase at Φ = 0 indicates that very low probability is assigned to the minimum loss.

---

### Identifying domains of applicability of machine learning models for materials science [^72c4ba76]. Nature Communications (2020). High credibility.

An illustrative example

Before describing the details of DA identification and its integration into the ML process, let us illustrate the concept and its utility via a synthetic example (see Fig. 1). We consider a simple two-dimensional representation consisting of independent features x 1 and x 2 that are each distributed according to a normal distribution with mean 0 and variance 2 (N (0, 2)) and a target property y that is a third-degree polynomial in x 1 with an additive noise component that scales exponentially in x 2 :That is, the y values are almost determined by the third-degree polynomial for low x 2 values but are almost completely random for high x 2 values. Discovering applicable domains reveals how different models cope differently with this setting even if they have a comparable average error. To show this, let us examine the error distributions obtained from three different kernelized regression models of the formwith parameter vector ν that are fitted around a training, or fitting (F), setwith three different choices for the kernel function k. We observe:
When using the linear (lin) kernel, the resulting linear model is globally incapable to trace the variation of the third-order polynomial except for a small stripe on the x 1 -axis where it can be approximated well by a linear function. Consequently, there is a very high error globally that is substantially reduced in the DA described by σ lin (x 1, x 2) ≡ −0.3 ≤ x 1 ≤ 0.3.
When using the Gaussian kernel), the resulting radial basis function (rbf) model is able to represent the target property well locally unless (a) the noise component is too large and (b) the variation of the target property is too high relative to the number of training points. The second restriction is because the rbfs have non-negligible values only within a small region around the training examples. Consequently, the discovered DA is not only restricted in x 2 -direction but also excludes high absolute x 1 -values: σ rbf ≡ −3.3 ≤ x 11 ≤ 3.1 ∧ x 2 ≤ 0.1.
In contrast, when using the non-local third-degree polynomial (poly) kernel, data sparsity does not prevent an accurate modeling of the target property along the x 1 -axis. However, this non-locality is counterproductive along the x 2 -axis where overfitting of the noise component has a global influence that results in higher prediction errors for the almost deterministic data points with low x 2 -values. This is reflected in the identified DA σ poly (x 1, x 2) ≡ −3.5 ≤ x 2 ≤ 0.1, which contains no restriction in x 1 -direction, but excludes both high and low x 2 -values. This highlights an important structural difference between the rbf and the polynomial model that is not reflected in their similar average errors.

---

### Forecasting of the first hour aftershocks by means of the perceived magnitude [^44febb3a]. Nature Communications (2019). High credibility.

The parameter μ M in Eq. (3) represents the maximum value ofand we define it the perceived magnitude since it provides the magnitude of ground shaking felt close to the station. We find that the three mainshocks present roughly the same value of μ M, a result expected for the Kos and Lesvos earthquakes with comparable magnitudes and similar epicentral distance from the recording station. In the case of the Ischia earthquake, this result can be primarily attributed to the smaller epicentral distance from the recording station (see Table 1) but also to soil conditions at the seismic station location. Concerning the functional dependence of the scaling function F (x) in Eq. (3), the shape of the envelope function is dominated by different mechanisms such as source properties, attenuation, scattering effects, etc. The scaling collapse of Fig. 2c suggests that these features are, at a first approximation, incorporated in two seismic source-dependent parameters: the perceived magnitude μ M and the horizontal time rescaling τ M. This result is confirmed by the scaling collapse also obtained for the other mainshocks, when plottingas a function of (t − t 0)/ τ M (Fig. 3). In all cases we find that, for x < 10, F (x) ≃ log 10 (x) − q log 10 (x + x 0) with q ≃ 3.5 (continuous curve in Fig. 2c) and x 0 = (q − 1) q q /(1− q) ≃ 0.43 such that the maximum value of F (x) is equal to zero. We assume the same functional dependence of F (x) for all mainshocks in order to reduce the number of fitting parameters in our method. This fit corresponds to a power law decay of the amplitude of coda-waves of V (t)which anticipates the exponential decay at longer timescales. We wish to emphasize that our results are weakly affected by the specific choice of F (x) and we expect that other choices for F (x) lead to similar results.

---

### Finding spatially variable ligand-receptor interactions with functional support from downstream genes [^9f9f8416]. Nature Communications (2025). High credibility.

Moreover, we apply a nonparametric covariance test for estimating the spatial variance (Supplementary Method 1.3.2). Assumes that if y p is independent of Z, then the spatial distance between two abstract interfaces z i and z j would also be independent of the LRI profile difference between the two abstract interfaces i and j. With y p and Z, we have the centered profile and coordinate covariance matrices Q (y p) and Q (Z). Subsequently, tr(Q (y p) Q (Z))/ k, a correlation measurement between y p and spatial coordinates Z is computed. Assuming that the correlation measurement follows a mixture of χ 2 distributions, we obtain the p -value and the corresponding q -value for y p. SPIDER uses the implementation in SPARK-X.

Given that LRI profiles are often noisy due to sequencing limitations, we further denoise the LRI profiles by discretization, that is, converging the continuous LRI profile to discrete labels. We achieve spatial neighbor-aware discretization with hidden Markov random field by assuming that the observable LRI signal y p are conditionally dependent on the underlying discrete labels f = { f i } whose states are not observable. Initially, we estimate f with a Gaussian mixture model, with which we apply the alpha-expansion graph cut algorithm implemented in scGCOto find a label vector f of interfaces that minimizes an energy function (detailed in Supplementary Method 1.3.3). The optimal label f * defines an active regionfor the LRI signal, on which we test for the spatial non-randomness p -value with the complete spatial randomness framework, as well as the corresponding q -values.

Since we have multiple statistical tests, SPIDER filters the SVIs by applying a strict threshold of p -value ≤ 0.01 across all tests. If the resultant SVIs is less than 10, SPIDER combines the resulting p values using the Cauchy p value combination ruleto obtain a single p value, which is also subject to the threshold of p -value ≤ 0.01. The threshold is also implemented as a hyperparameter for users to adjust.

---

### One-shot learning for solution operators of partial differential equations [^d6bcc900]. Nature Communications (2025). High credibility.

Learning the solution operator for a linear PDE

We first demonstrate the capability of our method with a pedagogical example of a one-dimensional Poisson equationwith the zero Dirichlet boundary condition, and the solution operator is.

As described in the one-shot learning method based on the principle of locality, we first need to select a "canonical" local domain, which is a line segment in 1D case. We choose the simplest local solution operator using 3 nodes (Fig. 2 b)with Δ x = 0.01 from. The training datasetonly has one data point(Fig. 3 a), whereis generated based on. Using the training dataset, we placeat different locations to generate data points and train the local solution operator. For a new PDE condition f, we apply our trainedand enforce the zero Dirichlet boundary condition at x = 0 and x = 1 to find the solution. In Fig. 3 c, we show examples of testing cases f = f 0 + Δ f with different σ test. When σ test is larger, there is a more significant discrepancy between f 0 and f.

Fig. 3
1D Poisson equation.

a The training data includes a randomgenerated from GRF and the corresponding solution. b The convergence of L 2 relative errors of different approaches for various test cases. The shaded regions represent one standard deviation of the L 2 relative errors from 100 test cases. c Testing examples of random f = f 0 + Δ f with Δ f sampled from GRF of σ test = 0.02, 0.05, 0.10, 0.15 and l test = 0.1. d Prediction example of different approaches for various test cases. e Prediction errors (the geometric mean and standard deviation) when training by f random with amplitude σ train and testing on functions with amplitude σ test. The length scales l train = 0.01 and l test = 0.10 are fixed. f Prediction errors (the geometric mean and standard deviation) when training by f random with different length scale l train and amplitude σ train = 0.10, and testing on functions with amplitude σ test = 0.10 and l test = 0.10.

---

### AmpliconReconstructor integrates NGS and optical mapping to resolve the complex structures of focal amplifications [^4ef752f1]. Nature Communications (2020). High credibility.

Next, we defineas the best score of aligning a subsequence of the first j labels on b with a subsequence of the first q labels on segment x, where j and q are included in the subsequences. Given two labels on the assembled contig i, j, and two labels on the reference genome segment p, q where i < j, and p < q, The DP recurrence used by SegAligner iswhere the function Score is the SegAligner scoring function for two OM matching regions.

The function Score is defined in Algorithm 1 and contains four main terms. First, f n which is defined as the number of potentially unmatched contig labels between i and j scaled by the missing label score, c. Second, e ref is the number of potentially unmatched reference labels between p and q, after accounting for labels which are too close together to be measured distinctly. Third, f p is the number of potentially unmatched reference labels scaled by the missing label score. Last, Δ measures the absolute difference in length between j – i and q – p, which is scaled non-linearly (k). Together these penalty terms are combined and subtracted from a base matching score, 2 c. Parameters c and k in this model were identified through a coarse grid search using data where correct OM contig-reference alignments were already known.

Algorithm 1 SegAligner scoring function

function Score(b, x, i, j, p, q, M):

f n = c*(j - (i + 1))

e ref = M(p, q)

f p = c*e ref

Δ = (abs((b[j] - b[i]) - (x[q] - x[p]))) k

return 2c - (f n + f p +Δ)

A backtracking matrix, P is used to record the decision made in filling each cell. The DP Algorithm has complexitywhere m = |b |, n = |x | and d is the width of the band. Backtracking is performed insteps by backtracking through the coordinates stored in P. We find a most-likely path by initializing the backtracking atorfor fitting alignment. Values used to parameterize the scoring function and label collapse map generation function given below are provided in Supplementary Data 5.

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^b49e3a01]. Nature Communications (2023). High credibility.

Several software tools exist that fit dose–response models and estimate, among other parameters, the effect potency –. Surprisingly, none of them addresses the obvious question of whether the observed curve constitutes a significant regulation or is simply the result of experimental error. Assessing the significance of the regulation is especially relevant for applications in which (i) only a small proportion of cases produces significant regulations, (ii) the effect size is close to the measurement variance of the assay, or (iii) the assay is generally not very stable. So far, dose–response curve classification has required (semi-)manual data evaluation by experts –, which suffers from inconsistent assessments among individuals and does not scale to the thousands to millions of curves generated by the aforementioned projects.

Fitting a dose–response curve is, in essence, a regression problem. Hence, we propose that the statistical significance of dose–response curves can be assessed using F-statistics. Unfortunately, the log-logistic function typically used for dose–response curve fitting is non-linear and, therefore, F-distributions using the degrees of freedom appropriate for linear models do not describe the true null distribution exactly. The typical solution for complex non-linear models is to use permutation-based statistics, which has been done for time-series data, where the response is not expected to follow a sigmoidal shapeand thermal stability data, where temperature and dose result in 2-dimensional models. However, we demonstrate here that the simplicity of the log-logistic function allows the estimation of "effective" degrees of freedom for the F-statistic to obtain well-calibrated p -values without the need for permutation strategies. This reduces the computational burden and allows accurate p -value estimation even in the very low p -value region.

---

### Optimal free-surface pumping by an undulating carpet [^d85c2eb5]. Nature Communications (2023). High credibility.

Theoretical modeling

The separation of scales (H ≪ λ) in the thin-film geometry leads to a set of reduced momentum equations and a flow field that is predominantly horizontal. Thus integration of the X -momentum equation with no-slip boundary condition on the undulator (Z = h a) and no shear stress condition at the free surface (Z = h f) results inSimilarly, we integrate the Z -momentum equation and apply the Young-Laplace equation at the free surface, which yields the following expression for the pressure p, Integration of the continuity equation gives the volume flow rate, (per unit depth in this two-dimensional case). Plugging eqs. (11) and (12) into the expression of flow rate gives the following ODE, This equation relates the yet unknown constant q and the unknown free-surface shape h f. We close the problem by imposing the following additional constraint on h f :which states that the mean film thickness over one wavelength remains the same as that of the unperturbed interface, H. In dimensionless form, the above set of equations take the form of eqs. (3) and (4) of the main text.

For a direct comparison with experiments, we transform the flow rate, q back to the lab frame which is an explicit function of time, Q (x, t) = q + V w (h f (x, t) − h a (x, t)). We seek for periodic free-surface shapes, such that h f = H + periodic terms. The time-averaged flow rate thus simplifies towhere the integration is performed at a fixed spatial location. In dimensionless form, this equation becomes, as mentioned in the main text.

To estimate the efficacy of the pumping mechanism, we compare the output, and flow rate to the energy dissipation within the flow, which is given by, Using the expression for the velocity field of eq. (11), we integrate on Z to find that the free-surface shape h f fully determines the amount of dissipation in the flow. In dimensionless form, dissipation becomeswhere.

---

### Demography and the emergence of universal patterns in urban systems [^36ce729d]. Nature Communications (2020). High credibility.

Methods

Probability solution for geometric random growth with boundary conditions

The Fokker-Planck equation for random geometric growth without drift iswhere P = P [x (t), t ∣ x (0), t 0] is the conditional probability of observing state x of the random variable at time t, given the initial state x (t 0) at time t 0. For simplicity of notation, we have dropped the i indices in x and σ and write x (t 0) as x 0.

Equation (17) has some similarities with the diffusion equation in physics and can analogously be solved exactly. First, it is useful to change variables so as to eliminate the non-linear term x 2. We setand. By changing the variables in Eq. (17) we findThis equation is now linear and can be solved in two ways. The first way is using factorization and solve it as a heat equationandThe second way is to solve it directly via a Fourier transform, so thatwhich leads toThis equation can be solved via a separation of variables, P [k, τ] = f (k) T (τ), which leads to an eigenvalue problem:Solving for k we obtain:withwhere P [y, 0] = f (k) T (0). In particular, there are two stationary solutions, for w = 0, with k = k 0 = 2 i and k = k 1 = i. Substituting k 0, we see that the solution corresponds to, which is Zipf's distribution. The other solution corresponds to the existence of a constant probability current up or down the urban hierarchy associated with different boundary conditions, see main text.

---

### Non-linear relationships in clinical research [^668fb19a]. Nephrology, Dialysis, Transplantation (2025). Medium credibility.

Polynomial functions

In Fig. 4, we provide an example of a simple power transformation using the square function, which raises a variable to the power of 2 (x 2), providing a U-shaped curve. Higher powers of x can be also used to determine the shape of the function, such as the cubic function (x 3) which has a distinctive S-shape. These power terms form the core elements of so-called polynomial functions, which along with the coefficients included in the function, offer a flexible way to model various curves. By adjusting the coefficients and power functions, polynomials offer a wide variety of shapes. "Fractional" polynomial functions allow the power terms to be fractions instead of just whole numbers (i.e. x 1/2) [21]. A (fractional) polynomial function often provides sufficient flexibility to follow relatively simple non-linear curves, providing a simpler solution than the more advanced techniques we describe below. However, higher degree polynomials can be sensitive to "noise" in the data, and are not suited for fitting some of the more complex curves (e.g. sudden shifts, discontinuities, or logarithmic curves). They may also suffer from Runge's phenomenon, becoming unstable and oscillating at the edges of the data, and extrapolate poorly beyond the original range of the independent variable.

Regression splines

A powerful approach to dealing with non-linearity is provided by the family of spline functions. Instead of a single function defining the whole curve, such as polynomials or other transformations, splines are constructed by using a series of functions, each defining a different segment of the curve. As splines are constructed segment by segment — or piece by piece — they are often referred to as "piecewise" functions. Segments are connected to each other using so-called "knots", and the spline is restricted to join at these knots so there are no gaps in the curve. The simplest spline function is the linear spline function. This function assumes linearity within each segment, but the overall curve formed by the connected segments can be non-linear. For a small number of segments, linear spline models can be as easy to interpret as linear regression, as each segment can be represented by a single slope. Figure 3 provides an example of a simple linear spline with two knots (in green). These two knots divide the range of the independent variable into three segments, each with its own slope.

---

### Physics-informed learning of governing equations from scarce data [^d1208a9b]. Nature Communications (2021). High credibility.

Harnessing data to discover the underlying governing laws or equations that describe the behavior of complex physical systems can significantly advance our modeling, simulation and understanding of such systems in various science and engineering disciplines. This work introduces a novel approach called physics-informed neural network with sparse regression to discover governing partial differential equations from scarce and noisy data for nonlinear spatiotemporal systems. In particular, this discovery approach seamlessly integrates the strengths of deep neural networks for rich representation learning, physics embedding, automatic differentiation and sparse regression to approximate the solution of system variables, compute essential derivatives, as well as identify the key derivative terms and parameters that form the structure and explicit expression of the equations. The efficacy and robustness of this method are demonstrated, both numerically and experimentally, on discovering a variety of partial differential equation systems with different levels of data scarcity and noise accounting for different initial/boundary conditions. The resulting computational framework shows the potential for closed-form model discovery in practical applications where large and accurate datasets are intractable to capture.

---

### Learnable latent embeddings for joint behavioural and neural analysis [^2236dad0]. Nature (2023). Excellent credibility.

Theoretical guarantees for linear identifiability of CEBRA models

Identifiability describes the property of an algorithm to give a consistent estimate for the model parameters given that the data distributions match. We here apply the relaxed notion of linear identifiability that was previously discussed and used. After training two encoder models f and f ′, the models are linearly identifiable if f (x) = L f(x), where L is a linear map.

When applying CEBRA, three cases are of potential interest. (1) When applying discovery-driven CEBRA, will two models estimated on comparable experimental data agree in their inferred representation? (2) Under which assumptions about the data will we be able to discover the true latent distribution? (3) In the hypothesis-driven or hybrid application of CEBRA, is the algorithm guaranteed to give a meaningful (nonstandard) latent space when we can find signal within the data?

For the first case, we note that the CEBRA objective with a cosine similarity metric follows the canonical discriminative form for which Roeder et al.showed linear identifiability: for sufficiently diverse datasets, two CEBRA models trained to convergence on the same dataset will be consistent up to linear transformations. Note that the consistency of CEBRA is independent of the exact data distribution: it is merely required that the embeddings of reference samples across multiple positive pairs, and the embeddings of negative samples across multiple negative pairs, vary in sufficiently numerous linearly independent directions. Alternatively, we can derive linear identifiability from assumptions about data distribution: if the ground truth latents are sufficiently diverse (that is, vary in all latent directions under distributions p and q), and the model is sufficiently parameterized to fit the data, we will also obtain consistency up to a linear transformation. See Supplementary Note 2 for a full formal discussion and proof.

---

### Optimal diversification strategies in the networks of related products and of related research areas [^60a3dbe8]. Nature Communications (2018). Medium credibility.

We begin by solving the model for three topologies: a wheel network (a network with a central hub surrounded by a ring of nodes) (Fig. 2a), a generalized wheel network (a network with multiple hubs connected to a large number of nodes in a random network) (Fig. 2b), and a scale-free network constructed using the configuration model(Fig. 2 (see Fig. 1c)). We use the wheel network to develop our basic intuition, and then, use the more complex network topologies to show that the intuition developed in the wheel network generalizes to more complex cases. These three networks have heterogeneous degree distributions where few nodes have high degrees while many nodes have low degrees. Their structures are similar to the network structures of research space and product space so we can readily assume that the most effective strategies in these three networks are the most effective ones in the two spaces. Then, we bring the theory to the data by returning to the network of related products and the network of related research activities.

Fig. 2
Strategic diffusion in networks. Network models. a Wheel network, b generalized wheel network, and c scale-free network. d Time needed to activate all nodes in the wheel network as a function of the time in which the hub is targeted, with time expressed as the fraction of active nodes (Eq. 2). Optimal time is indicated with the red line. e optimal strategy for the wheel network described as the degree of the nodes that is optimal to target for each fraction of the total nodes that have been activated. f Total time needed to activate a generalized wheel network as a function of the degree of hubs for the five simple strategies and for the solution of Eq. 4 (in black line, black dots represent a simulation). g optimal strategy for the generalized wheel network described as the degree of the nodes that is optimal to target for each fraction of the total nodes that have been activated. Inset shows optimal targeting time as a function of the degree of the hubs. h Total activation time to activate a scale-free network for the five pure strategies and the optimal mixed strategy as a function of the parameter modeling the importance of relatedness α. Inset shows optimal mix of greedy and majority strategies as a function of α. i Dynamic strategies for the scale-free networks obtained via numerical simulations for the greedy, random, and the optimal mixed strategy. The mixed strategy targets hubs at an intermediate level

---

### Mathematical modeling is more than fitting equations [^994275ea]. The American Psychologist (2014). Low credibility.

Comments on the comments made by Brown et al. (see record 2013-24609-001). The article by Brown et al. regarding the Fredrickson and Losada (see record 2005-11834-001) article discussed the use of differential equations in science and repeated our earlier observation (Luoma, Hämäläinen, & Saarinen, 2008) that there is lack of justification for the use of the Lorenz equations in the latter article. In this comment we want to point out that Brown et al. presented a very narrow view on mathematical modeling in behavioral research. We describe how the conceptual use of mathematical models is essential in many fields.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^284636ad]. Nature Communications (2021). High credibility.

Biological regulatory systems, such as cell signaling networks, nervous systems and ecological webs, consist of complex dynamical interactions among many components. Network motif models focus on small sub-networks to provide quantitative insight into overall behavior. However, such models often overlook time delays either inherent to biological processes or associated with multi-step interactions. Here we systematically examine explicit-delay versions of the most common network motifs via delay differential equation (DDE) models, both analytically and numerically. We find many broadly applicable results, including parameter reduction versus canonical ordinary differential equation (ODE) models, analytical relations for converting between ODE and DDE models, criteria for when delays may be ignored, a complete phase space for autoregulation, universal behaviors of feedforward loops, a unified Hill-function logic framework, and conditions for oscillations and chaos. We conclude that explicit-delay modeling simplifies the phenomenology of many biological networks and may aid in discovering new functional motifs.

---

### Automated in vivo enzyme engineering accelerates biocatalyst optimization [^bbe467ba]. Nature Communications (2024). High credibility.

In addition to the previously discussed methods, ML approaches are now being more commonly employed to identify data patterns that aid in forecasting protein structures, enhancing enzyme stability, solubility, and function, predicting substrate specificity, and facilitating rational de novo protein design.

---

### Power of data in quantum machine learning [^b2a9dd1b]. Nature Communications (2021). High credibility.

Ultimately, to see a prediction advantage in a particular dataset with specific function values/labels, we need a large separation between s C and s Q. This happens when the inputs x i, x j considered close in a quantum ML model are actually close in the target function f (x), but are far in classical ML. This is represented as the final test in Fig. 1 and the methodology here outlines how this result can be achieved in terms of its more essential components.

Projected quantum kernels

In addition to analyzing existing quantum models, the analysis approach introduced also provides suggestions for new quantum models with improved properties, which we now address here. For example, if we start with the original quantum kernel, when the effective dimension d is large, kernel Tr(ρ (x i) ρ (x j)), which is based on a fidelity-type metric, will regard all data to be far from each other and the kernel matrix K Q will be close to identity. This results in a small geometric difference g CQ leading to classical ML models being competitive or outperforming the quantum kernel method. In Supplementary Section 9, we present a simple quantum model that requires an exponential amount of samples to learn using the quantum kernel Tr(ρ (x i) ρ (x j)), but only needs a linear number of samples to learn using a classical ML model.

---

### An additive gaussian process regression model for interpretable non-parametric analysis of longitudinal data [^03f8e4da]. Nature Communications (2019). High credibility.

Methods

Notation

We model target variables (gene/protein/bacteria/etc;) one at a time. Let us assume that there are P individuals and there are n i time-series measurements from the i th individual. The total number of data points is thus. We denote the target variable by a column vector y = (y 1, y 2. y N) T and the covariates by X = (x 1, x 2. x N), where x i = (x i 1, x i 2. x id) T is a d -dimensional column vector and d is the number of covariates. We denote the domain of the j th variable byand the joint domain of all covariates is. In general, we use a bold font letter to denote a vector, an uppercase letter to denote a matrix, and a lowercase letter to denote a scale value.

Gaussian process

GP can be seen as a distribution of nonlinear functions. For inputs, GP is defined aswhere μ (x) is the mean and k (x, x ′) is a positive-semidefinite kernel function that defines the covariance between any two realizations of f (x) and f (x ′) bywhich is called kernel for short. The mean is often assumed to be zero, i.e. and the kernel has parameters θ, i.e. k (x, x ′| θ). For any finite collection of inputs X = (x 1, x 2. x N), the function values f (X) = (f (x 1), f (x 2). f (x N)) T have joint multivariate Gaussian distributionwhere elements of the N -by- N covariance matrix are defined by the kernel [K X, X (θ)] i, j = k (x i, x j | θ).

We use the following hierarchical GP modelwhere π (ϕ) defines a prior for the kernel parameters (including), is the noise variance, and I is the N -by- N identity matrix. For a Gaussian noise model, we can marginalise f analytically

---

### One-shot learning for solution operators of partial differential equations [^78b205a2]. Nature Communications (2025). High credibility.

Methods

In the following sections, we delve into the details of each step in our one-shot learning method for PDE solution operators.

Selecting a local domain and a local solution operator

In this section, we elaborate on the first two steps of our method. To begin with, we consider a virtual background equispaced mesh. This is not a real mesh for the solution prediction in the computational domain; instead, it is a virtual mesh that guides us to select a local domain. Taking a two-dimensional problem as an example, this local equispaced mesh has mesh size Δ x 1 and Δ x 2, where x 1 and x 2 are spatial or temporal dimensions corresponding to the PDE. On this local mesh, we then sketch a polygon with mesh nodes positioned on its edges or within the interior of the polygon. We denote the set of these mesh nodes as, and a general choice ofis represented by black nodes in Fig. 2 a (left).

In the second step, we choose a location x * from the local domainas the target node, and the remaining nodes surrounded x * are denoted as auxiliary points. We define a local solution operator to predict the PDE solution at x * from the information of auxiliary points and the PDE conditionwhich is learned by a neural network. The intuition of this definition is that, for a well-defined PDE, if we know the solution u at the boundary ofand f within, then the solution u at x * insideis determined. We find that only using the value of f at x * is sufficient, so in this study the local solution operator is chosen as

---

### Deep domain adversarial learning for species-agnostic classification of histologic subtypes of osteosarcoma [^e805f5df]. The American Journal of Pathology (2023). Medium credibility.

Domain Adversarial Training of a Histologic Subtype Classification Model for Osteosarcomas

Letbe examples from a source domainandbe examples from a target domainwhere the number of examples available is typically much less than the number of examples available from the source domain. To train a classification model that adapts from the source domain to target domain, we extend the algorithm of Ganin and Lempitskyto the supervised setting. Specifically, letbe the parameter of the feature extraction backbone(ie, the function that takes as input an exampleand maps it to a set of features), letbe the parameter of the subtype classifier(ie, the function that receives input from the feature extractor and predicts class label), and letbe the parameter of the domain classifier(ie, the function that receives input from the feature extractor and predicts the domain label). Furthermore, let:

The first term in Equation 1 represents the subtype classification error, whereas the second term in Equation 1 represents the domain classification error and the hyperparametercontrols the trade-off between the two errors. The goal of a domain adaption algorithm is then to find the saddle point of:

The domain classifier tries to minimize the domain classification error (because of theterm), and the subtype classifier tries to minimize the subtype classification error. To find the saddle point, the domain classifier is trained adversarially with the label classifier. Consequently, the parameters of the feature extractorat the saddle point minimize the subtype classification error (ie, the learned features are discriminative) while maximizing the domain classification error (ie, the learned features are domain invariant). Adversarial training is implemented in practice by simply adding a gradient reversal layer just before the domain classifier and performing standard stochastic gradient descent (Figure 1). The update rule for the parameters after incorporating the gradient reversal layer is given by Equations 4, 5, and 6:

---

### Identifying domains of applicability of machine learning models for materials science [^9a774e75]. Nature Communications (2020). High credibility.

The effect term of the objective function ensures that the model is estimated to be more accurate in the described region than in the global representation space. Thus, selectors with a large effect value describe domains of increased applicability as desired (see also Supplementary Note 4). In addition, promoting large, i.e.g.eneral, DAs through the coverage term is important as those have a higher chance to (a) contain data points of interest and (b) to have an accurate effect estimate, i.e. the empirical error reduction measured by the effect term is likely to generalize to other points in the DA that are not contained in the DA identification set. Thus, the coverage term has a similar role as a regularization term in common objective functions for model fitting. With the above objective function, we reduce the bi-criterial coverage/effect optimization problem to a uni-criterial impact optimization problem where both individual criteria are equally weighted and non-compensatory, i.e. due to the multiplicative combination, very low values of one criterion cannot be compensated by very high values in the other. The relative weight of both criteria can be re-calibrated by introducing a simple exponential weight parameter (see the Supplementary Methods section on Coverage/Effect Trade-off for a detailed discussion).

Optimizing the impact function over all conjunctive selectors that can be formed from a given set of base propositions is an NP-hard problem. This implies that there is no solver for it with worst-case polynomial time complexity (unless P = NP). However, there is a practically efficient branch-and-bound algorithm that turns out to be very fast in practice if the dimensionality of the DA representation is not too high — in particular, substantially faster than the model training process (see Methods and Supplementary Methods).

---

### Fractional response analysis reveals logarithmic cytokine responses in cellular populations [^0d1364aa]. Nature Communications (2021). High credibility.

Calculation of typical fractions

The fractions of cells stimulated with dose i that have responses typical to dose j, v ij, can be easily calculated from data regardless of the number of doses and the type of experimental measurements. We have thatCalculation of typical fractions, v ij, with the above formula requires the possibility to examine the condition P (y | x j) > P (y | x k) for any experimentally observed response, y. The distributions P (y | x j) can be reconstructed from data using a variety of probability density estimators. The use of the available estimators, however, might be problematic for multivariate responses. We, therefore, propose a more convenient strategy. We replace the condition P (y | x j) > P (y | x k) with an equivalent condition that is computationally much simpler to evaluate. Precisely, we propose to use the Bayes formulaIf we set the equiprobable prior distribution, i.e. P (x j) = 1 /m, we have that P (y | x j) is proportional to P (x j | y) and the condition P (y | x j) > P (y | x k) is equivalent toThe above strategy allows avoiding estimation of the response distributions, P (y | x j), from data. For continuous and multivariate variable y the estimation of P (x j | y) is generally simpler than estimation of P (y | x j). Precisely, an estimatorof the distribution P (x j | y) can be built using a variety of Bayesian statistical learning methods. For simplicity and efficiency, here we propose to use logistic regression, which is known to work well in a range of applications. In principle, however, other classifiers could also be considered. The logistic regression estimators of P (x j | Y = y) arise from a simplifying assumption that log-ratio of probabilities, P (x j | Y = y) and P (x m | Y = y) is linear. Precisely, The above formulation allows fitting the logistic regression equations to experimental data, i.e. finding values of the parameters, α j and β j that best represent the data. The fitted logistic regression model allows assigning cellular responses to typical doses based on conditions given by Eq. 8. Formally, the fractions v ij defined by Eq. 6 are calculated aswhere n i is the number of cells measured for the dose x i, y l i denotes response of the l- th cell, andis equal 1 iffor anyand 0 otherwise.

---

### Variable number tandem repeats mediate the expression of proximal genes [^b7ba6b2c]. Nature Communications (2021). High credibility.

Read filtering

For each VNTR locus V, and each read R, consider a binary classification function f: V × R → {0, 1}, where f (R, V) = 1 if and only if read R maps to locus V. For each read and each of N loci V 1, …, V N, the neural recruitment method computes independent classification functions f i (V i, R). Note that a read can be assigned to multiple VNTR loci, or to none. As an initial step toward this task, we perform a fast string matching based on prefix tree (trie) to assign each read to the VNTR loci that share an exact match with the read. For an efficient matching, we generate a separate aho-corasick trieusing every k-mer in VNTR loci as dictionary X. A trie is a rooted tree where each edge is labeled with a symbol and the string concatenation of the edge symbols on the path from the root to a leaf gives a unique word (k-mer) X. We label each leaf with a set of T VNTRs that contain corresponding k-mer. On the other hand, the string concatenation of the edge symbols from the root to a middle node gives a unique substring of X, called the string represented by the node. We add extra internal edges called failure edges to other branches of the trie that share a common prefix which allow fast transitions between failed string matches without the need for backtracking. Testing whether a query q has an exact match in the trie can be done in O (∣ q ∣) and we require additional O (∣ T ∣) time to assign read q to all T VNTR loci that share the keyword. The overall complexity of this algorithm is linear based in the length of original dictionary (VNTRs in the database) to build the trie and recover matches plus the length of queries (sequencing reads). Hence, after construction of the trie, the running time is proportional to just reading in the sequences.

---

### Models of physiology and physiologically based models in clinical pharmacology [^fa104488]. Clinical Pharmacology and Therapeutics (2012). Low credibility.

As science matures, it becomes more mathematical, progressing from enumeration to the use of equations to the formulation of models. Clinical pharmacology has developed to the stage where models play an increasingly important role in predicting and analyzing drug pharmacokinetics and pharmacodynamics, and even in characterizing disease progression and therapeutic response. Useful models have two characteristics that are in ostensible conflict: (i) they must accurately represent the essential features of the underlying system and (ii) the representation must be sufficiently simplified to enable its salient features to be identified and investigated through further experimentation.

---

### Design principles for enhancing phase sensitivity and suppressing phase fluctuations simultaneously in biochemical oscillatory systems [^695a3331]. Nature Communications (2018). Medium credibility.

The linear relation (16) agrees with our numerical results. More importantly, the analytical results reveal that the amplitude-phase coupling constant β 2 has an important role in relating energy dissipation (Eq. (15)) to phase sensitivity (Eq. (13)). We further develop a toy model of limit cycle oscillator (see Supplementary Fig. 6) where β 2 can be explicitly calculated in terms of the microscopic nonequilibrium parameter γ. See Supplementary Note 9 for details. We found, in this simple model, that free-energy cost of increasing forward rates and suppressing backward rates both strengthen the phase-amplitude coupling β 2, therefore enhancing the phase sensitivity.

Design principles for enhancing oscillation functions

Now that we show it is in principle possible to increase phase sensitivity and to suppress phase diffusion simultaneously in a nonequilibrium system that consumes free energy, the next logical question is what are the design principles for a biochemical oscillator to optimize desirable oscillatory behaviors with a fixed energy budget. Here we search for possible design principles by studying the performance of an oscillator, characterized by χ and D, for different combinations of γ 1 and γ 2 that lead to the same free-energy dissipation per period. In particular, we look for rules for designing the kinetic rate parameters in reactions that form a microscopic loop like the one shown in Fig. 2c. Unlike in simplified modelswhere only one reversible reaction exists between two states, these microscopic loops, which are formed by two distinct reaction pathways between two nearby microscopic states, are the basic building blocks in realistic oscillatory biochemical networks.

---

### An additive gaussian process regression model for interpretable non-parametric analysis of longitudinal data [^56514e83]. Nature Communications (2019). High credibility.

Results

Additive GP

Linear models and their mixed effect variants have become a standard tool for longitudinal data analysis. However, a number of challenges still persist in longitudinal analysis, e.g. when data contains nonlinear and ns effects.

GP are a flexible class of models that have become popular in machine learning and statistics. Realizations from a GP correspond to random functions and, consequently, GPs naturally provide a prior for an unknown regression function that is to be estimated from data. Thus, GPs differ from standard regression models in that they define priors for entire nonlinear functions, instead of their parameters. While nonlinear effects can be incorporated into standard linear models by extending the basis functions e.g. with higher order polynomials, GPs can automatically detect any nonlinear as well as ns effects without the need of explicitly defining basis functions. By definition, the prior probability density of GP function values f (X) = (f (x 1), f (x 2), ⋯, f (x N)) T for any finite number of fixed input covariates X = (x 1, x 2. x N) (where) is defined to have a joint multivariate Gaussian distributionwhere elements of the N -by- N covariance matrix are defined by the GP kernel function [K X, X (θ)] i, j = k (x i, x j | θ) with parameters θ. Mean in Eq. (1) can in general depend on X but zero mean is often assumed in practice. Covariance (also called the kernel function) of the normal distribution defines the smoothness of the function f, i.e. how fast the regression function can vary. Intuitively speaking, although GP is defined such that any finite-dimensional marginal has a Gaussian distribution, GP regression is a non-parametric method in the sense that the regression function f has no explicit parametric form. More formally, GP contains countably infinite many parameters that define the regression function, which are the function values f at all possible inputs. For a comprehensive introduction to GPs we refer the reader to the bookand the Methods section.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^22bd51a1]. Nature Communications (2021). High credibility.

Motif III: logic

A general class of functions used to describe natural,– and synthetic, biological networks are logic gates, which have two inputs regulating a single output (Fig. 5). Gates exhibit either high ("on") or low ("off") output depending on whether inputs are on or off. For example, the AND gate specifies high output only if both inputs are on. In this section we provide a specific DDE-based framework that covers 14 out of 16 possible 2-input logic operations, and show that these operations form a continuous 2D parameter space.

Fig. 5
A simple approximation for digital logic using a sum of Hill terms recapitulates all monotonic logic functions in a single parameter space.

a A prototypical regulatory network involving logic where X and Y both regulate Z, which must integrate the two signals using some logic before it can in turn activate a downstream reporter R. b Parameter space showing regions where regulation approximately follows 14 of the 16 possible 2-input logic functions depending on the strength of two single-variable Hill regulation terms (η Z 1: regulation of Z by X, η Z 2: regulation of Z by Y). Network logic can be smoothly altered by varying the parameters (η Z 1, η Z 2), with a change of sign in (n 1, n 2) required to switch quadrants. The bottom-left quadrant shows that very weak regulation in both terms leads to an always-off (FALSE) function, weak regulation in one arm only leads to single-input (X, Y) functions, strong regulation in both arms leads to an OR function, and regulation too weak in either arm alone to activate an output but strong enough in sum leads to an AND function. The other three quadrants are related by applying NOT to one or both inputs, with function names related by de Morgan's lawNOT(X OR Y) = NOT X AND NOT Y. In particular, X IMPLY Y = NOT(X) OR Y, X NIMPLY Y = X AND NOT(Y), X NOR Y = NOT X AND NOT Y, and X NAND Y = NOT X OR NOT Y. Truth tables for all 16 logic gates are provided in Supplementary Table 1 for reference. The two non-monotonic logic functions, X XOR Y and X XNOR Y, are those 2 of 16 not reproduced directly using this summing approximation. They can be produced by layering, e.g. NAND gates. c Representative time traces for AND (η Z 1 = η Z 2 = 0.9) and OR (η Z 1 = η Z 2 = 1.8) gates with n 1 = n 2 = −2, n 3 = −20, η R = η Z 1 + η Z 2. The functionwhen n > 0, when n < 0.

---

### Comparison of parameter optimization methods for quantitative susceptibility mapping [^26a0417b]. Magnetic Resonance in Medicine (2021). Medium credibility.

Purpose

Quantitative Susceptibility Mapping (QSM) is usually performed by minimizing a functional with data fidelity and regularization terms. A weighting parameter controls the balance between these terms. There is a need for techniques to find the proper balance that avoids artifact propagation and loss of details. Finding the point of maximum curvature in the L-curve is a popular choice, although it is slow, often unreliable when using variational penalties, and has a tendency to yield overregularized results.

Methods

We propose 2 alternative approaches to control the balance between the data fidelity and regularization terms: 1) searching for an inflection point in the log-log domain of the L-curve, and 2) comparing frequency components of QSM reconstructions. We compare these methods against the conventional L-curve and U-curve approaches.

Results

Our methods achieve predicted parameters that are better correlated with RMS error, high-frequency error norm, and structural similarity metric-based parameter optimizations than those obtained with traditional methods. The inflection point yields less overregularization and lower errors than traditional alternatives. The frequency analysis yields more visually appealing results, although with larger RMS error.

Conclusion

Our methods provide a robust parameter optimization framework for variational penalties in QSM reconstruction. The L-curve-based zero-curvature search produced almost optimal results for typical QSM acquisition settings. The frequency analysis method may use a 1.5 to 2.0 correction factor to apply it as a stand-alone method for a wider range of signal-to-noise-ratio settings. This approach may also benefit from fast search algorithms such as the binary search to speed up the process.

---

### Universal structures for adaptation in biochemical reaction networks [^61e9f50c]. Nature Communications (2023). High credibility.

Crucially, we extend the mathematical framework for Theorems 3 and 4 (on low-deficiency CRNs,) to a general method for identifying rowspan polynomials in CRNs of arbitrary deficiency. In fact, CRNs can generally be decomposed into a set of algebraically-independent subnetworks (SI Section S1.3), which correspond topologically to independent RPA-capable-modules, each with its own independent RPA polynomial. The deficiency that characterizes each independent subnetwork, and the associated topological module, governs how RPA-relevant subsidiary polynomial invariants are constructed within the rowspan of the CRN. In Section S4.4, we demonstrate these fundamental principles through detailed analyzes of several RPA-capable CRNs withfor a single module (see also Fig. 6). Our method for analyzing these examples makes clear that although an RPA polynomial associated to an RPA-capable CRN generally requires a nonlinear transformation of the reaction equations (Eq. 6), there always exist linear transformations that can extract the special polynomial building blocks, each corresponding one-to-one with a topological feature of the overarching network structure, from the CRN's reaction equations. In particular, all RPA-capable CRNs of Balancer type contain a connector polynomial, corresponding to the connector node, and one or more balancer polynomials, corresponding to balancer node(s), in their rowspans. CRNs of the Opposer type, on the other hand, contain one or more opposer polynomials (corresponding to opposer node(s)) in their rowspans.

---

### CurveCurator: a recalibrated F-statistic to assess, classify, and explore significance of dose-response curves [^cf66bf7c]. Nature Communications (2023). High credibility.

Dose-response curves are key metrics in pharmacology and biology to assess phenotypic or molecular actions of bioactive compounds in a quantitative fashion. Yet, it is often unclear whether or not a measured response significantly differs from a curve without regulation, particularly in high-throughput applications or unstable assays. Treating potency and effect size estimates from random and true curves with the same level of confidence can lead to incorrect hypotheses and issues in training machine learning models. Here, we present CurveCurator, an open-source software that provides reliable dose-response characteristics by computing p-values and false discovery rates based on a recalibrated F-statistic and a target-decoy procedure that considers dataset-specific effect size distributions. The application of CurveCurator to three large-scale datasets enables a systematic drug mode of action analysis and demonstrates its scalable utility across several application areas, facilitated by a performant, interactive dashboard for fast data exploration.

---

### The connectome-based prediction of trust propensity in older adults: a resting-state functional magnetic resonance imaging study [^2c3f95fe]. Human Brain Mapping (2023). Medium credibility.

Based on the neuropsychoeconomic model, trust propensity is shaped by affect, motivation, and cognition (i.e. executive, social), which engages large‐scale domain‐general networks (Krueger & Meyer‐Lindenberg). In a social dilemma created in a one‐shot TG, affect (the emotional arousal of risk of treachery) contrasts with motivation (the anticipation of reward), which produces uncertainty. Affect is anchored in the cingulo‐opercular network (CON, largely overlapping with the salience network), in which the anterior insular (AI) and anterior cingulate cortex (ACC) underlie trust propensity (Bellucci et al; Krueger & Meyer‐Lindenberg). Motivation is related to the reward network, typically involving the ventromedial prefrontal cortex, striatum, and basal ganglia (Krueger & Meyer‐Lindenberg). To resolve the uncertainty in a social dilemma, trustors can apply two types of bounded rationality, that is, economic (executive functions, adopting a context‐based strategy with extrinsic incentives) and social (social cognition, evaluating the relationship‐based trustworthiness with intrinsic incentives) rationality, to transform the risk of treachery (affect) into the positive expectation (motivation) of reciprocity (Krueger & Meyer‐Lindenberg). In a one‐shot TG, trustors reduce the uncertainty based on the calculus‐based trust strategy (i.e. trustors perform rational calculations of the costs and benefits of creating a relationship), which uses economic rationality (executive functions) to perform logical costs‐benefits calculations and social rationality (social cognition) to simulate the trustworthiness of their trustee. In particular, the CON engages the frontoparietal network (FPN, also named the central executive network) that implements a rule‐based strategy to reap personal benefits deliberately (economic rationality) and the default mode network (DMN) that evaluates the relationship‐based trustworthiness of a partner to contribute to a successful relationship (social rationality, Declerck et al; Krueger & Meyer‐Lindenberg). In addition, prior RSFC studies indicate that both the FPN (engaged in economic rationality/executive functions) and DMN (engaged in social rationality/social cognition) contribute to the prediction of trust propensity in younger adults (Feng, Zhu, et al; Lu et al.).

---

### Large-scale mapping and mutagenesis of human transcriptional effector domains [^449366c2]. Nature (2023). Excellent credibility.

Human gene expression is regulated by more than 2,000 transcription factors and chromatin regulators 1,2. Effector domains within these proteins can activate or repress transcription. However, for many of these regulators we do not know what type of effector domains they contain, their location in the protein, their activation and repression strengths, and the sequences that are necessary for their functions. Here, we systematically measure the effector activity of more than 100,000 protein fragments tiling across most chromatin regulators and transcription factors in human cells (2,047 proteins). By testing the effect they have when recruited at reporter genes, we annotate 374 activation domains and 715 repression domains, roughly 80% of which are new and have not been previously annotated 3–5. Rational mutagenesis and deletion scans across all the effector domains reveal aromatic and/or leucine residues interspersed with acidic, proline, serine and/or glutamine residues are necessary for activation domain activity. Furthermore, most repression domain sequences contain sites for small ubiquitin-like modifier (SUMO)ylation, short interaction motifs for recruiting corepressors or are structured binding domains for recruiting other repressive proteins. We discover bifunctional domains that can both activate and repress, some of which dynamically split a cell population into high- and low-expression subpopulations. Our systematic annotation and characterization of effector domains provide a rich resource for understanding the function of human transcription factors and chromatin regulators, engineering compact tools for controlling gene expression and refining predictive models of effector domain function.

---

### A hierarchy of biomolecular proportional-integral-derivative feedback controllers for robust perfect adaptation and dynamic performance [^7222e252]. Nature Communications (2022). High credibility.

Deterministic analysis and properties of the various a PID controllers

It is straightforward to show that the setpoint for the second-order design is given bywith the requirement that β < θ; whereas the set-points for both higher-order designs are given by. Furthermore, the effective PID gains, denoted by (K P, K I, K D), and cutoff frequency ω of the embedded low-pass filter for each of the proposed a PID controllers can be designed by tuning the various biomolecular parameters: β, η, η 0, γ 0, μ 0 and the parameters of the propensity functions h and g. These functions depend on the specific implementation adopted. In particular, they can be picked in a similar fashion to the functions used to realize the three inhibition mechanisms (additive, multiplicative, or degradation) of the a PI controllers shown in Fig. 3. In the subsequent examples, we use degradation inhibitions, but the other mechanisms can also be used.

Next, we demonstrate various properties of the proposed controller designs in the deterministic setting and highlight the benefits of the added complexity. The mappings between the effective PID parameters (K P, K I, K D, ω) and the biomolecular parameters (μ, θ, η, β, γ 0, η 0,.) are given in Supplementary Information Section 4 for each controller. It is fairly straightforward to go back and forth between the two parameter spaces. For control analysis, these mappings can compute the various PID parameters from the biomolecular parameters, whereas for control design, these mappings can compute the various biomolecular parameters that achieve some desired PID gains and cutoff frequency. As a result, one can use existing methods in the literature (e.g.) to carry out the controller tuning in the PID parameter space, and then map them to the actual biomolecular parameter space. Nevertheless, it is of critical importance to note that different controllers yield different coverage over the PID parameter space. For instance, for the fourth-order design, there are enough biomolecular degrees of freedom to design any desired positive. The lower the order of the controller, the fewer the biomolecular degrees of freedom, and hence the more constrained the coverage in the PID parameter space. For instance, for the third-order design, the achievable PID parameters are constrained to satisfy K P ≤ K D ω. For the second-order design, the constraint becomes even stricter. The details are all rigorously reported in Supplementary Information Section 4.

---

### Genetic association mapping leveraging gaussian processes [^987fa797]. Journal of Human Genetics (2024). Medium credibility.

GP regression and its use in genetic association mapping

Because the GP yielding f (x) has various useful properties inherited from the normal distribution, GP can be used to estimate a nonlinear function f (X) from output dataalong continuous factor X. The extended linear model y = f (X) + ε is referred to as the GP regression and widely used in the machine learning framework. This model can be used to map dynamic genetic associations for normalized gene expression or other common complex quantitative traits (e.g. human height) along the continuous factor x (e.g. cellular states or donor's age). Let us denote the genotype vectorand the kinship matrix R among N individuals, the mapping model, as proposed by us or others can be expressed as follows:whereare all GPs with similar covariance matrices, where ⊙ denotes element wise product between two vectors or matrices with the same dimensions, K = k (X, X) denotes the covariance matrix with a kernel function, and ε denotes the residuals. Intuitively, α models the average baseline change of y in relation to x, while β represents the dynamic genetic effect along x. The effect size is multiplied by the genotype vector g, indicating that the output y i varies between different genotype groups (g i ∈ {0, 1, 2}). In fact, the effect size β (x i) is additive to the baseline α (x i) at each x i, which is the same as the standard association mapping. Here statistical hypothesis testing is performed under the null hypothesis of δ g = 0, as the strength of genetic association is determined by δ g.

---

### Non-linear relationships in clinical research [^f94f76fa]. Nephrology, Dialysis, Transplantation (2025). Medium credibility.

CONCLUSION

"Essentially all models are wrong, but some are useful". George Box reminds us that although statistical models are unable to perfectly capture the complexities of the real world, they still offer a convenient approximation, which can be useful to clinical practice. Despite the characteristic of all statistical models inherently being "wrong", in epidemiological research, we strive to produce models that are as truthful to reality as possible. As nonlinear relationships arguably occur more often than linear ones, appropriate methods are required to ensure that the modeled relationship follows the data at hand, and that the shape of the relationship is accurately represented by the model. Here we provide the strengths and limitations of the various tools capable of modeling non-linear relationships, including transformations, polynomials, splines and GAMs, whilst demonstrating how the reporting of results using these techniques can be surprisingly straightforward, despite their perceived complexity.

---

### Higher-order and distributed synergistic functional interactions encode information gain in goal-directed learning [^8b2b74c8]. Nature Communications (2025). High credibility.

Discussion

Information seeking and directed exploration in goal-directed learning

Goal-directed learning is defined as the ability to learn the causal relationship between our behaviours and their outcomes; this supports the selection of actions according to expected outcomes, as well as current goals and motivational state –. Goal-directed learning relies on the balance between exploitation and exploration strategies –. A trade-off between choosing the most valuable option and sampling informative options is central for successful goal-directed learning and it is one of the main focus of reinforcement learning and Bayesian learning theories. Exploration in humans has been shown to be based on a mixture of random and directed strategies. Random exploration is a deviation from the most rewarding option, and it is normally formalised using the softmax choice rule, which models the degree of randomness (decision noise). Directed exploration selectively samples options that are informative (information seeking), that are associated with the highest uncertainty.

---

### Universal structures for adaptation in biochemical reaction networks [^92007991]. Nature Communications (2023). High credibility.

Our transformative step is to prove that all RPA-capable CRNs, regardless of size, complexity, or deficiency, necessarily conform to the general principles encapsulated by these two illustrative examples (see SI Section S4). In fact, all RPA-capable CRNs are characterized by a topological hierarchy of polynomial invariants – a key phenomenon foreshadowed by our earlier framework, before general principles were known as to how these invariants might be achieved by CRN graph structures. Here we show that each such subsidiary polynomial invariant is necessarily identifiable via a linear combination of the CRN's rate equations, and corresponds directly to a topological feature (a balancer node, a connector node, or an opposer node) of the overarching network structure. We further show that the presence of these key invariants in the row span of the system (see SI Sections S3 and S4) is fundamentally controlled by the deficiency of the CRN. Moreover, CRN deficiency is generally distributed across multiple algebraically independent subnetworks, which correspond to a decomposition of the reactions into topological modules (see SI Section S4). Thus, the integral-computing properties of CRNs at the network microscale presented here, together with the topological principles that are known to hold at the network macroscale for all RPA-capable networks, constitute definitive design criteria that unify all possible RPA-capable CRNs.

There are two distinct but interrelated components to this central result, which we delineate in turn in the sections to follow: (i) We identify the universal algebraic condition that is satisfied by all RPA-capable CRNs, and which is codified in what we call the Two-Variable Kinetic Pairing Theorem (SI Section S1.5); (ii) We show that this algebraic condition always admits a decomposition into a collection of linear problems. The discovery of this previously unrecognized fundamental structure of RPA-capable CRNs exploits the fact that CRNs reactions can generally be partitioned into algebraically independent subsets, each with its own linearly independent stoichiometric subspace (SI Section S1.2), and each with its own deficiency (SI Section S1.3) that governs the formation of RPA-promoting algebraic invariants within the associated subnetwork. Together, these two mathematical results reveal an integral control implementation that holds for all possible RPA-capable CRNs, however large, complex or nonlinear in their dynamics, and whatever their deficiency.

---

### The topological requirements for robust perfect adaptation in networks of any size [^fcf9aa3e]. Nature Communications (2018). Medium credibility.

We note that an alternative, but mathematically equivalent, version of the RPA equation has also been developed in the recent work of Tang and McMillen. Those authors refer to the condition as 'the cofactor condition', and apply this approach to the issue of designing novel homeostatic systems. In particular, their design algorithm has been used to generate topologies and parameter constraints that 'will support homeostatic behavior for a given set of network components and a desired set of general regulatory constraints to be applied between them'.

Deducing general mechanisms of RPA from the RPA equation

We provide full details on our solution method for solving the RPA equation in complete generality in our Supplementary Information. As noted in the preceding section, Supplementary Notes 1 and 2 provide a complete set of precise definitions corresponding to our problem, and present detailed derivations of the RPA equation and the RPA constraint, along with mathematical forms for a set of axioms for the reaction kinetics at individual network nodes.

Supplementary Note 3 provides full details on our topological approach to the solution of the RPA problem, identifying conditions whereby the RPA equation may be partitioned into independently adapting subsets. To this end, we begin with the notion of a minimally adaptive subset of the RPA equation, of which there are two basic types — S-sets and M-sets — each with their own type of LAQ. We explore how these mathematical conditions imply novel topological structures within RPA networks — Opposer Modules (employing the novel concept of opposing sets) and Balancer Modules

The constraints on reaction kinetics that are imposed by the creation of S-sets and M-sets from the RPA equation are presented in detail in Supplementary Note 4. Here we also explore a range of parameter constraints that would allow the requisite reaction kinetics to be implemented in RPA networks, and also consider how these reactions implement some form of integral control.

Supplementary Note 5 considers the central matter of this work, namely the relationship between the two fundamental types of RPA Module (Opposer and Balancer) and a topological basis for RPA-capable networks. We consider how these topological basis modules may be interconnected to form larger (multimodular) RPA-capable networks.

To aid in the general delineation of all RPA-capable network topologies through the interconnections of Opposer and/or Balancer Modules we distinguish between the two possible relationships between interconnected modules, as we outline in the next section.

---

### A rational multi-target combination strategy for synergistic improvement of non-ribosomal peptide production [^4aff2ea7]. Nature Communications (2025). High credibility.

In addition to the lack of reliable and universal selection methods, a more pressing concern lies in the selection of targets that exhibit synergetic and additive effects for combinational engineering while discarding the antagonistic targets. This is because simultaneous mutation or interference of two genes usually produces a phenotype that is unexpected in light of each loss-of-function individual effect. Therefore, investigating the underlying functional relationship between individual favorable targets and mapping these interactions would undoubtedly provide a rational guide of multi-target combinations, enabling highly efficient successive strain improvement. However, we still face challenges in effectively combining individual favorable targets to generate synergistic effects in microbial strain improvement. Measurement of the synergy coefficient is a critical metric for evaluating the combined effects of multiple factors, such as in the synergistic screening of drugs, since this parameter quantifies how the interactions between the components either enhances or diminishes their individual effects. Therefore, the determination of synergistic coefficient should be introduced in the development of a rational multi-target combination strategy for synergistic improvement of non-ribosomal peptide producing strains.

---

### Optimal enzyme utilization suggests that concentrations and thermodynamics determine binding mechanisms and enzyme saturations [^440ef9eb]. Nature Communications (2023). High credibility.

Petersen linearization

After approximating elementary displacements from equilibrium, the rate equation contained the bilinear terms arising from the products,… We could approximate these continuous products using the following derivation, which is only shown for the productfor simplicity. The same linearization scheme also applies to the remaining nonlinearities of the formand.

The productis bilinear, though is a product of a binary and a continuous variable. Assuming a constantwe could apply the Petersen linearization scheme, to the bilinearity. Replacingwith another non-negative variable, where s stands for the index of the binary variable and i stands for the elementary step, we could represent the bilinear product by one new variable and three new constraints:

Note that when N e > 3, we need an additional linearization to account for the product of two binary variables. As an example, consider the random-ordered Bi-Uni mechanism given in Eq. (3). By approximating 4 elementary displacement variableswe can express all 6 elementary displacements as explained in the previous section. To describe the reaction rate from the product dissociation step, we can use the following equations:

Note that the product-dissociation stepis not in the branched pathway of the reaction mechanism. We then needed to represent γ 4 from the approximated elementary displacements as.

We can rewrite the constraints above using the following equations:Hereandare the approximations by a piecewise-constant function as described by Eqs. (20)–(24). As both approximations contain binary variables, their product needed to be considered. This product could be linearized by representing it with a new binary variable and three new constraints as follows:where, δ s and λ p are binary variables. δ s and λ p are the binary variables in the binary expansion forandrespectively. After this linearization, the remaining bilinearity was of the form continuous × binary, which could be linearized using Petersen's theorem, (Eqs. (25)–(26)).

Using the change of variables and the piecewise-constant approximation as described above, we translated the nonlinear optimization problem given in Eq. (15) to a MILP, which can be summarized as follows:

Note that, the overall thermodynamic constraint was dropped in the final formulation, as approximations of the independent elementary displacement variableswere performed accordingly as explained before.

---

### Combinatorial code governing cellular responses to complex stimuli [^e91b758e]. Nature Communications (2015). Medium credibility.

Discussion

Previous studies have used independence models to assess synergistic and antagonistic interactions in a quantitative manner based on phenotypic or molecular measurements. For over 80 years, such models have remained a reference, although improvements have been made. Our study revealed an unrecognized diversity in the possible interaction modes between two signals, within categories of behaviour previously thought to be homogeneous. Most importantly, it enabled to precisely quantify the spectrum of possible interactions and their link to particular biological functions.

To reach our objectives, we had to overcome the following four main challenges: (1) the enumeration of theoretically possible interaction profiles, (2) the classification of such profiles into biological interaction modes, (3) the projection of high-throughput data onto the theoretical space and (4) the link between interaction modes and biological functions.

A careful examination of the 82 elementary profiles obtained with combinatorial approaches led us to observe that common terms such as 'inhibition', 'synergy' and 'restoration' could be realized in several ways. Guided by pattern-matching related to biological interpretation, and mathematical formalization, we could define 10 classes of biologically similar interactions termed interaction modes. While the definitions proposed in this work are not the only possible, they encompass all the theoretical space and they are objectively reproducible, which may help to promote a consensus terminology still lacking in the field of signal integration. Another advantage behind the introduction of biological modes is the possibility to gain biological insight, for example, by linking modes to specific functional pathways.

To explore the functional role of the different interaction modes, we performed enrichment analysis separately for each sufficiently represented mode. Our results, obtained from several independent data sets, suggest that each mode preferentially contributes to a particular function or pathway. However, we do not rule out the possibility for more complex scenarios. For example, different modes may participate in the same pathway, and/or the same mode may participate in different pathways.

---

### Selecting fitted models under epistemic uncertainty using a stochastic process on quantile functions [^7bce234b]. Nature Communications (2025). High credibility.

Fig. 4
Loss PPF for different models.

Each column corresponds to a different model. The PPF (percent point function; bottom row) is the inverse of the CDF (cumulative density function; top row). For calculations we interpolate 2¹⁰ = 1024 points (cyan line) to obtain a smooth function; for illustration purposes here only 30 points are shown. The data for the first two columns were generated with the neuron model described at the top of our Results, where the additive noise follows either a Gaussian or Cauchy distribution. The black body radiation data for the third column were generated from a Poisson distribution using equation (38) with s = 2¹⁴ and λ in the range 6 μ m to 20 μ m. Here the true noise is binomial, but the loss assumes a Gaussian. The fourth column shows an example where the data are high-dimensional; the same 30 dimensional, unit variance, isotropic Gaussian is used for both generating the data and evaluating the loss. In all panels the loss function used is the log likelihood under the model.

Since by construction, the abscissae Φ of an empirical PPF are spaced at intervals of 1/ L, the Riemann sum for the integral in equation (20) reduces to the sample average. More importantly, we can interpret the risk as a functional in, which will allow us below to define a generic stochastic process that accounts for epistemic uncertainty.

Up to this point with equation (20) we have simply rewritten the usual definition of the risk. Recall now that in the previous section, we proposed to equate replication uncertainty with misspecification; specifically we are interested in how differences between the candidate modeland the true data-generating processaffect the loss PPF, since this determines the risk. Therefore we also compute the PPF ofunder its own model (recall from equation (1) thatmust be a probabilistic model):from which we obtain the synthetic PPF :The only difference betweenandis the use ofinstead ofin the integral. In practice this integral would also be evaluated by sampling, usingto generate a datasetwith L synth, A samples. Because in this case the candidate model is used for both generating samples and defining the loss, we call the synthetic PPF (CDF).

---

### Neural substrates of cognitive biases during probabilistic inference [^31b931db]. Nature Communications (2016). Medium credibility.

Decision making often requires simultaneously learning about and combining evidence from various sources of information. However, when making inferences from these sources, humans show systematic biases that are often attributed to heuristics or limitations in cognitive processes. Here we use a combination of experimental and modelling approaches to reveal neural substrates of probabilistic inference and corresponding biases. We find systematic deviations from normative accounts of inference when alternative options are not equally rewarding; subjects' choice behaviour is biased towards the more rewarding option, whereas their inferences about individual cues show the opposite bias. Moreover, inference bias about combinations of cues depends on the number of cues. Using a biophysically plausible model, we link these biases to synaptic plasticity mechanisms modulated by reward expectation and attention. We demonstrate that inference relies on direct estimation of posteriors, not on combination of likelihoods and prior. Our work reveals novel mechanisms underlying cognitive biases and contributions of interactions between reward-dependent learning, decision making and attention to high-level reasoning.

---

### Universal structures for adaptation in biochemical reaction networks [^7b21f040]. Nature Communications (2023). High credibility.

Methods

For the purposes of obtaining the greatest possible generality and universality, we define RPA from the most general possible viewpoint in this study. In particular, we consider a CRN to exhibit RPA in the concentration, of some network molecule (directly or indirectly subjected to some disturbance or perturbation) exactly whenmaintains a constant steady-state value, for all steady-states of the system. The setpoint, is a function of some subset of CRN parameters. Moreover, the CRN exhibits RPA (in) in response to any perturbation or disturbance that leaves the collection of setpoint parameters unaltered (see SI Section S1). We make no assumptions a priori as to which type(s) of network perturbation or disturbance might affect a particular CRN, nor do we impose any restrictions on which (or how many) parameters determine the setpoint.

In our Supplementary Information (SI), we provide full details of all mathematical theorems that support our results, along with their corresponding proofs. The central result of our study, the Two-Variable Kinetic Pairing Theorem (Theorem 1), is discussed fully, and proved, in SI Section S1, along with a full discussion of the RPA polynomial – the fundamental object at the heart of Theorem 1 – in addition to a range of other essential concepts, such as CRN deficiency, partitions of CRN reactions into algebraically independent subsets/subnetworks, matrix decomposition of CRN mass-action equations, and boundary variables. The second pivotal result of this study concerns the nature of the nonlinear transformation (of the CRN rate equations) required to yield the all-important RPA polynomial, and the fact that this transformation is always decomposable into a topological hierarchy of linear maps (and hence, linear integral controllers, and their associated internal models). Our analysis (SI Sections S2, S3, S4) highlights the crucial notion that the deficiency of the CRN fundamentally controls the formation of polynomial invariants that are obtainable through linear coordinate changes. All theorems related to these concepts are discussed in detail, along with their corresponding proofs and a set of fully-analyzed illustrative examples (SI Section S4) and annotated Singular code (SI Section S5).

Reporting summary

Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.

---

### Statistics review 7: correlation and regression [^4cb8dd2d]. Critical Care (2003). Low credibility.

The present review introduces methods of analyzing the relationship between two quantitative variables. The calculation and interpretation of the sample product moment correlation coefficient and the linear regression equation are discussed and illustrated. Common misuses of the techniques are considered. Tests and confidence intervals for the population parameters are described, and failures of the underlying assumptions are highlighted.

---

### A robustness-based approach to systems-oriented drug design [^445f051e]. Nature Reviews: Drug Discovery (2007). Medium credibility.

Many potential drugs that specifically target a particular protein considered to underlie a given disease have been found to be less effective than hoped, or to cause significant side effects. The intrinsic robustness of living systems against various perturbations is a key factor that prevents such compounds from being successful. By studying complex network systems and reformulating control and communication theories that are well established in engineering, a theoretical foundation for a systems-oriented approach to more effectively control the robustness of living systems, particularly at the cellular level, could be developed. Here, I use examples that are based on existing drugs to illustrate the concept of robustness, and then discuss how a greater consideration of the importance of robustness could influence the design of new drugs that will be intended to control complex systems.

---

### Universal light-guiding geometry for on-chip resonators having extremely high Q-factor [^1c1c01e6]. Nature Communications (2020). High credibility.

Methods

Obtaining quasi-conformal mapping

We have found the quasi-conformal mapping that maps the boundaries of the virtual domain (trapezoidal or rib shape) to the specified boundaries in the physical domain (flat rectangular shape). Under a general coordinate transformation from virtual space (η = u + iv) to physical space (ζ = x + iy), the form of Maxwell's equations is preserved with the following constitutive parameterswhereis the Jacobian matrix.

Especially for conformal mapping ζ = f (η), the Cauchy–Riemann equations highly simplify the above tensorial parameters to the following refractive index profilefor TE- or TM-polarized electromagnetic waves. In addition, by the Riemann mapping theorem, there exists a unique conformal mapping that maps a simply connected domain to an arbitrary simply connected domain. Therefore, there exists a conformal mapping that maps trapezoidal and rib shape to flat rectangular shape.

The conformal mapping can be numerically found by calculating an extremal value of the Winslow functional with a fully sliding boundary condition.where g is a metric tensor (ΛΛ T). Alternatively, to obtain an extremal value of that, one can solve the following Laplace equationswhich are the Euler–Lagrange equations of the Winslow functional.

In principle, we can find the conformal mapping by solving Laplace equations with a fully sliding boundary condition. However, for the practical purpose, we obtained quasi-conformal mapping with partially sliding boundary condition on the upper and lower boundary of the waveguide while the boundary of left and right sides are fixed (whereis normal derivative at the boundary of interest in the virtual space). As a result, we can find quasi-conformal mapping but the anisotropic factor is almost 1 (≈1.001), which implies that it is almost conformal.

---

### How dieting might make some fatter: modeling weight cycling toward obesity from a perspective of body composition autoregulation [^32b99e3d]. International Journal of Obesity (2020). Medium credibility.

Two exponential fits are shown: a generalized linear model (GLM) with 95% confidence intervals (solid line), and a linear model (LM), with R 2 values (dotted line). The four figures correspond to the four methods presented in the main text for computing the value FAT END at the final time END at which the subjects would have completely recovered their initial FFM. The two red squares correspond to the US Army Ranger data points presented in the section on 'Applications of the Model'.

Their exact values depend on the method used for computing the values FAT END when the FFM has been completely recovered and on the type of statistical regression used. As explained previously, we consider four methods for computing FAT END. In Fig. 4, we show the fits obtained from a generalized linear model (GLM) with 95% confidence intervals (solid line) and from a linear model (LM), with R 2 values (dotted line). Following the performance of diagnostics tests to analyze the residuals, it is found that the GLM satisfies the major assumptions of regression analysis better than the LM, especially concerning the normality assumption of the error terms. Nevertheless, as shown in Fig. 4, both models give curves that are close to each other. In simple terms, the main difference between the above LM and GLM approaches to estimate the best parameter values for the constants a and b is the way the error term is handled. In the LM method, we write y = a × e^(b x) × ε, where ε is a random error variable that follows a log-normal distribution with parameters μ = 0 and σ 2. Taking the logarithm on both sides of the equality gives, which corresponds to the linear model, where, x ′ = x, β 1 = b andis a random error variable that follows a normal distribution with mean μ = 0 and variance σ 2. Note that in the LM method, the error term ε is multiplied with the exponential function y = a × e^(b x). In the GLM method, the error term is instead added to the exponential function. Formally, we write y = a × e^(b x) + ε, where ε is a random error variable that follows a normal distribution with mean 0 and variance σ 2. As a consequence, the GLM admits the possibility of the value y = 0 (which is excluded in LM) and assumes that the variance V (y) is constant, while the LM method assumes that V (y) varies with x. Finally, note that the LM method admits exact analytical solutions for the model parameters a and b, while the GLM method requires numerical optimization algorithms to find the best values (the maximum likelihood estimates) for a and b.

---

### High-throughput screening and rational design of biofunctionalized surfaces with optimized biocompatibility and antimicrobial activity [^a4fdd2c9]. Nature Communications (2021). High credibility.

We employed the orthopedic Ti implants as the substrates to verify the feasibility of our high-throughput strategy. Biomaterial-associated infection (BAI) and the delay of osseointegration are two main clinical reasons for the failure of surgery –. Previous reports and clinical experience showed that overcoming one of these two problems was not enough. Our study also confirmed this phenomenon. For example, a mono-functionalized surface, Ti-RGD-P3, had high risk of bacterial infection due to the lack of AMP, but another one, Ti-AMP-P3, had fibrous connective tissues around it and showed delayed osseointegration due to the lack of RGD. Unfortunately, in the previous study, it is very difficult to explore Ti implants with both excellent antimicrobial activity and the promotion of osseointegration simultaneously by simply mixing two peptides because these two peptides had contradictory biological effects. And finding their best density/ratio turned out to be difficult. However, with our high-throughput strategy to maximize the functions of the two peptides in one biomaterial, we could rationally design the dual-functionalized implant, i.e. Ti-Dual-P4, to show these two contradictory biological activities can be achieved at the same time. Compared with the implants functionalized with a single peptide, the multifunctional implant Ti-Dual-P4 could inhibit BAI and promote osseointegration in both infection and noninfection models in vivo (Figs. 7 and 8 and Supplementary Fig. 32). Also, the reported dual-functionalized implants often required the use of external stimuli, including electricity, light, and temperature, but our rationally designed dual-functionalized implant did not rely on the external stimuli, making our approach more practical in the clinic.

Herein, a high-throughput strategy was developed based on gradient surfaces for rational design and preparation of peptide functionalized surfaces. This strategy provided a series of optimized parameters in a convenient and rapid manner, including the densities and ratios of the peptides, and the related preparation parameters of reaction time and reactant concentrations. With the extracted parameters from the gradient surface, we prepared both mono- and dual-peptide functionalized Ti surfaces with highly similar biofunctions to those of the specific site on the gradient surface, displaying biocompatibility or/and antimicrobial activity both in vitro and in vivo. In addition, our strategy could be extended to functionalize other biomaterials. We believe that this high-throughput method holds great potential for rational design and preparation of biomaterial surface.

---

### Nonlinear delay differential equations and their application to modeling biological network motifs [^488abf14]. Nature Communications (2021). High credibility.

Chaotic behaviors are prevented in monotonic regulation with linear or cyclic network topology

Due to cyclic network topology (no more than one loop) and monotonic regulation, none of the motifs so far can yield chaotic dynamics. Either non-monotonic feedback (as in the Mackey-Glass equation) or non-cyclic topologyis required for chaos. Double feedback (Fig. 8 a) is a minimal motif fulfilling both requirements, although the non-monotonicity is sufficient (Supplementary Note 8).

The governing equation for such a double feedback motif is thus given by setting the output and both inputs of the logic equation (Eq. (19)) to X (and letting K = 1 for simplicity):

Chaotic behavior is possible for non-monotonic feedback with multiple feedback and disparate delays

Simulation of Eq. (37) demonstrates chaos for some parameters of positive/negative mixed feedback, as evidenced by sustained oscillations with variable maxima (Fig. 8 b), occupying an apparently fractal region in phase space (Fig. 8 c), and a large number of peaks in frequency space (Fig. 8 d). To confirm that the dynamics are in fact chaotic, we calculated the dominant Lyapunov exponent using the Wolf method(see "Methods"), finding a positive (i.e. chaotic) value of 0.0040 ± 0.00055 bits (mean ± standard deviation). We also calculated the box dimension of a reconstructed phase space with coordinates (X (T), X (T − 10), X (T − 20)) used by the Wolf method, yielding fractional dimension ~1.81, with 95% confidence interval (1.76, 1.87). These results indicate chaos (see Supplementary Fig. 4).

---

### Mechanics of drosophila wing deployment [^4496d956]. Nature Communications (2024). High credibility.

Note that in the nano-indentation experiments, the solutions of the three-element Maxwell-Jeffrey model reduce to the simpler Kelvin-Voigt model described above. A loading of the formis imposed and we look for solutions of the form ϵ ≈ (t / T) α. For α = 2/3, there are two groups of terms that evolve as ~ t −1/3 and ~ t −4/3. For short times, the dynamics are governed by terms in the power of −4/3. Balancing these terms, we find the expression, withwhere τ e f f = τ 1 τ 2 /(τ 1 + τ 2) which tends toward τ 1 for τ 1 ≪ τ 2. Numerical simulations of Eq. (3) for different values ofconfirm the above scaling.

Model

Wing geometry and force balance

We consider two infinite plates of thickness e, connected with pillars of diameter d, height h, organized in a hexagonal lattice of pitch a. We assume that both plates undergo spatially homogeneous equibiaxial extension in e x and e y, so that the principal stretches in the plates read λ x = λ y = λ. We assume that the material is incompressible, which implies that λ x λ y λ z = 1 and λ z = 1/ λ 2. For the pillars, we consider a uniaxial extension in the e z direction. Incompressibility and isotropy lead to the following equation, superscript p refers to the pillars, and we denote.

A force balance in the plates relates the Cauchy stress in the principal directions σ x x = σ y y = σ to the internal pressure P. As the plates become thinner by a factor of 1/ λ 2 and the pillar height increases by λ p, we obtain the equation 2 σ e / λ 2 = P h λ p and:with Ψ = h /(h + 2 e) the relative height of the pillars in the non-deformed geometry.

Similarly, for the pillars, the longitudinal Cauchy stresscompensates for the increase in internal pressure, so thatwhere A p is the area of the pillars in the plane and A is the total area of the plates in the plane. We introduce the density of the pillars in the plane, such that:

In order to solve these equations and obtain the values of (λ, λ p) as a function of P, it is necessary to define the material's constitutive law.

---

### Classification-based inference of dynamical models of gene regulatory networks [^512ad474]. G3 (2019). Medium credibility.

Inference of kinetic parameters including:

We exploit so-called kink solutions to the gene circuit equations to estimate the kinetic parameters. Let gene g be ON in an anteroposterior domainso that there is net diffusion out of the domain into surrounding OFF nuclei. Then the balance of synthesis, degradation, and diffusion will establish a stable gradient, outside the domain. Here, is the characteristic length scale of the gradient at steady state.

In order to infer the kinetic parameters, we first determine the time class in which a gene is expressed at the highest level, since the spatial gradient would best approximate steady state at that time point. Next, we identify gene-expression domains as local maxima in the spatial pattern in that time class. For genes having multiple expression domains, such asand, the domain with the highest expression is chosen. The gene is expressed at half maximum at the domain borders (Equation 15). Therefore, border positions (l or r) are determined as nuclei where the expression is half of the domain peak. andare then determined by fitting Equation 15 to the observed expression data forandusing MATLAB's lsqnonlin function. This is implemented as the "kink" method of FIGR.

Refinement of parameters

The parameter values estimated using classification and fitting to kink solutions serve as a starting point for an unconstrained local search using the Nelder-Mead algorithm implemented in MATLAB's fminsearch function. The cost function is, whereare data andare solutions of Equation 14.

Gap gene circuit inference and comparison with data and SA:

We inferred a gene circuit for the gap genes, and using both FIGR (Section FIGR for a spatially extended system) and SA (Section Inference of gap gene circuits). The model includes the upstream regulators, and, so that. Our implementation matched previous gap gene models so that the upstream proteins regulated the trunk gap genes but not vice versa. The time-dependent concentrations of Bcd, Cad, and Tll were determined by linear interpolation of the expression data. The gene circuit models the protein expression of these genes between 35–92% egg length during cleavage cycle 14 (Section Inference of gap gene circuits). The dataset is comprised of immunofluorescence protein concentration measurements at the resolution of individual nuclei at eight timepoints during cleavage cycle 14 (Figure 3A).

---

### Classification-based inference of dynamical models of gene regulatory networks [^e5f53bd7]. G3 (2019). Medium credibility.

In the first step of FIGR, the ON/OFF state of each gene is determined by estimating the velocity(Section Determining ON/OFF state). Figure 3B shows that the assignment of ON/OFF states by FIGR correctly recapitulates the extent and dynamics of the gene expression domains. In the next step, the regulatory parametersandwere inferred by supervised classification (Section Determining regulatory parameters). Predictions of the ON/OFF state from the inferred parameter values,(Figure 3C), are in good agreement with the empirical ON/OFF classification and the gene expression domains, suggesting correct inference of the regulatory parameters. In the third step, the kinetic parameters were estimated by fitting the kink equations to gap gene domain borders (Section Inference of kinetic parameters including.). In the presence of diffusion, the correspondence between velocityand the ON/OFF state of the gene will be violated for nuclei abutting gene expression domains due to diffusion of protein from ON regions into OFF regions. The inferred parameters likely deviate slightly from optimal values and were therefore refined using local search (Section Refinement of parameters.) in the fourth step of the algorithm. During refinement, the solutions were computed using gene circuit equations that represent synthesis with the sigmoid regulation-expression function (Equation 14) instead of the Heaviside function. Therefore the final model obtained did not rely on the assumption of binary switch-like regulation, which was utilized only during binary classification while obtaining the initial guesses of parameter values.

---

### Transient power-law behaviour following induction distinguishes between competing models of stochastic gene expression [^bbab52ab]. Nature Communications (2025). High credibility.

The power-law exponent contains useful information on the number of gene states over biologically relevant timescales

To test more directly the practical utility of the theory, we use simulations to mimic the measurement of the mean mRNA count from experimental data. We assumed five measurements of the mean mRNA at regularly spaced time points and that the final measurement was taken at a time much earlier than that of the median half-life of mRNA in mammalian cells (several hours). The parameters of the N -state model were then chosen such that the mean mRNA at the largest time point was within the biologically meaningful range (less than a few tens of mRNA molecules per cell). For further details on the selection procedure see Supplementary Note 2.

For each of the rate parameter sets, we integrated the time-evolution equations for the mean mRNA counts of the N -state model with an initial condition of zero mRNA and starting from inactive state j to calculate the mean mRNA at each of the five-time points. Then we used a linear regression on the log-log plot of the mean versus time to estimate the slope and hence exponent of the power law. In all cases, the coefficient of determination of the best linear fit through the data points was larger than 0.99 thus firmly establishing the existence of a power law — however as clear from the example shown in Fig. 4 A, because of the length of the time interval used, the fitted power law was different than the theoretical one and hence it is of interest to understand the relationship between the two.

---

### Homoscedasticity: an overlooked critical assumption for linear regression [^a3e3e14e]. General Psychiatry (2019). Medium credibility.

Linear regression is widely used in biomedical and psychosocial research. A critical assumption that is often overlooked is homoscedasticity. Unlike normality, the other assumption on data distribution, homoscedasticity is often taken for granted when fitting linear regression models. However, contrary to popular belief, this assumption actually has a bigger impact on validity of linear regression results than normality. In this report, we use Monte Carlo simulation studies to investigate and compare their effects on validity of inference.

---

### One-shot learning for solution operators of partial differential equations [^c2bebd91]. Nature Communications (2025). High credibility.

Learning a local solution operator

To train the local solution operator, we construct a training dataset from. To make the network generalizable to different f, the selection ofinis important. In our study, we use, whereis randomly sampled from a mean-zero GRF with the covariance kernel(σ train: amplitude; l train: length scale). The randomness in f random (x) is used to induce a more "diverse" training dataset so that we can extract more information from it.

Sincelearns for the small local domain, to generate many input-output pairs for training, we have two approaches. In the first approach, we have a structured equispaced global meshwith mesh size Δ x 1 and Δ x 2 (Fig. 2 g). Note that the mesh size of the global mesh matches the local mesh. We placeat different locations of the global mesh, and each placement would lead to one input-output data pair. In the second approach, we randomly sample different locations in Ω (Fig. 2 h). Similarly, each location leads to one data point. These approaches make it possible to learn a network from only one PDE solution by converting one-shot learning to classical learning. For example, for a PDE in a spatio-temporal domain, we may choose the local domain Fig. 2d and generate a datasetwhere (x, t) is sampled from the aforementioned approaches. We discuss the reasoning of using neural networks to learn the local solution operators in Supplementary Section S3.

Since the inputs of the neural networkalso include the solution to be predicted, we cannot predict solution for a new f = f 0 + Δ f directly. To address this issue, we propose the following three approaches to predict the solution of a new f, whose computational efficiency and prediction accuracy depend on the choice of.

---

### Non-linear relationships in clinical research [^43fcf57d]. Nephrology, Dialysis, Transplantation (2025). Medium credibility.

ABSTRACT

True linear relationships are rare in clinical data. Despite this, linearity is often assumed during analyses, leading to potentially biased estimates and inaccurate conclusions. In this introductory paper, we aim to first describe — in a non-mathematical manner — how to identify non-linear relationships. Various methods are then discussed that can be applied to deal with non-linearity, including transformations, polynomials, splines and generalized additive models, along with their strengths and weaknesses. Finally, we illustrate the use of these methods with a practical example from nephrology, providing guidance on how to report the results from non-linear relationships.

---

### Non-linear relationships in clinical research [^ad643ba8]. Nephrology, Dialysis, Transplantation (2025). Medium credibility.

True linear relationships are rare in clinical data. Despite this, linearity is often assumed during analyses, leading to potentially biased estimates and inaccurate conclusions. In this introductory paper, we aim to first describe-in a non-mathematical manner-how to identify non-linear relationships. Various methods are then discussed that can be applied to deal with non-linearity, including transformations, polynomials, splines and generalized additive models, along with their strengths and weaknesses. Finally, we illustrate the use of these methods with a practical example from nephrology, providing guidance on how to report the results from non-linear relationships.

---

### The measurement postulates of quantum mechanics are operationally redundant [^d7941401]. Nature Communications (2019). High credibility.

Understanding the core content of quantum mechanics requires us to disentangle the hidden logical relationships between the postulates of this theory. Here we show that the mathematical structure of quantum measurements, the formula for assigning outcome probabilities (Born's rule) and the post-measurement state-update rule, can be deduced from the other quantum postulates, often referred to as "unitary quantum mechanics", and the assumption that ensembles on finite-dimensional Hilbert spaces are characterized by finitely many parameters. This is achieved by taking an operational approach to physical theories, and using the fact that the manner in which a physical system is partitioned into subsystems is a subjective choice of the observer, and hence should not affect the predictions of the theory. In contrast to other approaches, our result does not assume that measurements are related to operators or bases, it does not rely on the universality of quantum mechanics, and it is independent of the interpretation of probability.

---

### Computational frameworks transform antagonism to synergy in optimizing combination therapies [^17957df9]. NPJ Digital Medicine (2025). Medium credibility.

Introduction

The synergistic and antagonistic interactions between drugs are fundamental concepts in combination drug therapy. When drugs are used in combination, if their combined effect exceeds the sum of the effects of each drug used individually, it is referred to as synergy. Conversely, if the effect of the drug combination is inferior to the sum of the individual drug effects, it is termed antagonism. Identifying synergistic drug combinations can significantly enhance therapeutic efficacy, while antagonistic combinations may lead to adverse consequences. Drug combination therapy has emerged as a promising approach to enhancing treatment outcomes, mitigating toxicity, and surmounting drug resistance, particularly when treating multifaceted diseases such as cancer. However, determining the optimal synergistic drug combinations remains a daunting challenge. Traditional experimental screening methods are laborious and resource-intensive, making it challenging to navigate the ever-expanding combinatorial space. Therefore, there is an imperative need to develop computational methods to predict drug synergy and guide the optimization of drug combinations.

---

### Optimal enzyme utilization suggests that concentrations and thermodynamics determine binding mechanisms and enzyme saturations [^0acc2b4d]. Nature Communications (2023). High credibility.

As we are interested in reactions operating toward the production of products: Then we approximated γ i using the following equation:where, is the resolution of the approximation, N is the number of bins into whichhas been discretized, and p chooses which bin is selected for the solution. Hereand set I c denotes the chosen elementary displacement variables or their combinations to be approximated with a 0th order approximation. To linearize the problem, we expressed p using binary variables. For this, we represent p with its binary expansion.where, indicates the smallest majoring integer to, and δ s is the binary variable. As we performed binary expansion of p, the complexity of the model increased withinstead of, which was also previously used by Salvy and Hatzimanikatis. We also needed to ensure that p does not exceed N:

For simplicity in our formulation, we approximated meaningful combinations of elementary displacements to reduce the number of linearization to be performed. For example, for the reversible Michaelis–Menten reaction given in Eq. (1), we chose two independent displacement variables to linearize asand, where the latter represents the product ofand, which also adds the constraint. In this way, we could represent each elementary displacement as,… Any other combination of elementary displacements from equilibrium works for this mechanism. Overall, we performed the piecewise-constant approximation for the chosen independent elementary displacement variables.

The approximation became more important for the combination of elementary displacements when we studied random-ordered mechanisms. For the random-ordered mechanism given in Eq. (3), by approximating the combination of elementary displacements for the cycle e.g., we could directly satisfy Eq. (9) for each fundamental cycle describing the principle of microscopic reversibility. Thus, for the random-ordered Bi-Uni mechanism, we could express all six elementary displacementsby approximating four independent displacement variables, namely:which yields,… Note that the constraintsandalso need to be considered.

For the results, the resolution of the approximationof elementary displacements was set to 10⁻⁴ for the Michaelis–Menten and 10⁻³ for the random-ordered Bi-Uni mechanisms. If the reaction operates close-to equilibrium, for both mechanisms.